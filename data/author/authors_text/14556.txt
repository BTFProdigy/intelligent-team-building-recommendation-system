Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 1155?1163,
Beijing, August 2010
Exploiting Salient Patterns for Question Detection and Question
Retrieval in Community-based Question Answering
Kai Wang
Department of Computer Science
School of Computing
National University of Singapore
kwang@comp.nus.edu.sg
Tat-Seng Chua
Department of Computer Science
School of Computing
National University of Singapore
chuats@comp.nus.edu.sg
Abstract
Question detection serves great purposes
in the cQA question retrieval task. While
detecting questions in standard language
data corpus is relatively easy, it becomes
a great challenge for online content. On-
line questions are usually long and infor-
mal, and standard features such as ques-
tion mark or 5W1H words are likely to be
absent. In this paper, we explore ques-
tion characteristics in cQA services, and
propose an automated approach to detect
question sentences based on lexical and
syntactic features. Our model is capable
of handling informal online languages.
The empirical evaluation results further
demonstrate that our model significantly
outperforms traditional methods in de-
tecting online question sentences, and it
considerably boosts the question retrieval
performance in cQA.
1 Introduction
Community-based Question Answering services
(cQA) such as Yahoo! Answers have emerged
as popular means of information exchange on the
web. They not only connect a network of people
to freely ask and answer questions, but also allow
information seekers to search for relevant histori-
cal questions in the cQA archive (Agichtein et al,
2008; Xue et al, 2008; Wang et al, 2009).
Many research works have been proposed to
find similar questions in cQA. The state-of-the-art
retrieval models include the vector space model
(Duan et al, 2008), language model (Duan et al,
2008; Jeon et al, 2005), Okapi model (Jeon et al,
2005), translation model (Jeon et al, 2005; Rie-
zler et al, 2007; Xue et al, 2008), and syntac-
tic tree matching model(Wang et al, 2009). Al-
though experimental studies in these works show
that the proposed models are capable of improv-
ing question retrieval, they did not give clear ex-
planation on which portion of the question that
the user query is actually matched against. A
question thread from cQA usually comprises sev-
eral sub-questions conveying different informa-
tion needs, and it is highly desirable to identify
individual sub-questions and match each of them
to the user query. Getting sub-questions clearly
identified not only helps the retrieval system to
match user query to the most desirable content but
also improves the retrieval efficiency.
However, the detection of sub-question is non-
trivial. Question sentences in cQA are usually
mixed with various description sentences, and
they usually employ informal languages, where
standard features such as question mark or ut-
terance are likely to be absent. As such, simple
heuristics using question mark or 5W1H words
(who, what, where, why, how) may become in-
adequate. The demand of special techniques in
detecting question sentences online arises due to
three particular reasons. First, the question mark
could be missing at the end of a question1, or
might be used in cases other than questions such
as ?Really bad toothache??. Second, some ques-
tions such as ?I?d like to know the expense of re-
moving wisdom teeth? are expressed in a declar-
ative form, which neither contains 5W1H words
nor is neccessarily ended with ???. Third, some
question-like sentences do not carry any actual in-
formation need, such as ?Please help me??. Fig-
ure 1 illustrates an example of a question thread
1It is reported (Cong et al, 2008) that 30% of online
questions do not end with question marks.
1155
S1: What do you guys do when you find that the 'plastic 
protection seal' is missing or disturbed. 
S2: Throw it out, buy a new one.. or just use it anyways?
S3: Is it really possible or likely that the item you purchased was 
tampered with??
S4: The box was in a plastic wrap but the item itself inside did 
not having the protection seal (box says it should) so I 
couldn't have inspected it before I bought it.
S5: Please suggest?? thanks!
Figure 1: An example of a question thread ex-
tracted from Yahoo! Answers
from Yahoo! Answers, where sub-questions S1
and S2 are posted in non-standard forms, and S5
is merely a question-like simple sentence. To the
best of our knowledge, none of the existing ques-
tion retrieval systems are equipped with a com-
prehensive question detector module to handle
various question forms online, and limited effort
has been devoted to this direction.
In this paper, we extensively explore character-
istics of questions in cQA, and propose a fully
automated approach to detecting question sen-
tences. In particular, we complement lexical pat-
terns with syntactic patterns, and use them as fea-
tures to train a classification model that is capable
of handling informal online languages. To save
human annotations, we further propose to employ
one-class SVM algorithm for model learning, in
which only positive examples are used as opposed
to requiring both positive and negative examples.
The rest of the paper is organized as follows:
Section 2 presents the lexical and syntactic pat-
terns as used for question detection. Section 3
describes the learning algorithm for the classifi-
cation model. Section 4 shows our experimental
results. Section 5 reviews some related work and
Section 6 concludes this paper.
2 Pattern Mining for Question Detection
As has been discussed, human generated content
on the Web are usually not well formatted, and
naive methods such as the use of question mark
and 5W1H words are not adequate to correctly
detect or capture all online questions. Methods
based on hand-crafted rules also fail to cope with
various question forms as randomly appeared on
the Web. To overcome the shortcomings of these
traditional methods, we propose to extract a set
of salient patterns from online questions and use
them as features to detect question sentences.
In this study, we mainly focus on two kinds
of patterns ? sequential pattern at the lexical
level and syntactic shallow pattern at the syntac-
tic level. Sequential patterns have been well dis-
cussed in many literature, including the identifi-
cation of comparative sentences (Jindal and Liu,
2006), the detection of erroneous sentences (Sun
et al, 2007) and question sentences (Cong et al,
2008) etc. However, works on syntactic patterns
have only been partially explored (Zaki and Ag-
garwal, 2003; Sun et al, 2007; Wang et al, 2009).
Grounded on these previous works, we next ex-
plain our mining approach of the sequential and
syntactic shallow patterns.
2.1 Sequential Pattern Mining
Sequential Pattern is also referred to as Labeled
Sequential Pattern (LSP) in the literature. It is
in the form of S?C , where S is a sequence
{t1, . . . , tn}, and C is the class label that the se-
quence S is classified to. In the problem of ques-
tion detection, a sequence is defined to be a se-
ries of tokens from questions, and the class labels
are {Q,NQ}, which stand for question and non-
question respectively.
The purpose of sequential pattern mining is to
extract a set of frequent subsequence of words
that are indicative of questions. For example,
the word subsequence ?anyone know what . . .
to? could be a good indication to characterize the
question sentence ?anyone know what I can do to
make me less tired.?. Note that the mined sequen-
tial tokens need not to be contiguous as appeared
in the original text.
There is a handful of algorithms available for
frequent subsequence extraction. Pei et al (2001)
observed that all occurrences of a frequent pattern
can be classified into groups (approximated pat-
tern) and proposed a Prefixspan algorithm. The
Prefixspan algorithm quickly finds out all rela-
tive frequent subsequences by a pattern growth
method, and determines the approximated pat-
terns from those subsequences. We adopt this al-
gorithm in our work due to its high reported effi-
ciency. We impose the following additional con-
straints for better control over the significance of
the mined patterns:
1156
1. Maximum Pattern Length: It limits the maxi-
mum number of tokens in a mined sequence.
2. Maximum Token Distance: The two adjacent
tokens tn and tn+1 in the pattern need to be
within a threshold window in the original text.
3. Minimum Support: The minimum percentage
of sentences in Q containing the pattern p.
4. Minimum Confidence: The probability of a
pattern p?Q being true in the whole database.
To overcome the word sparseness problem, we
generalize each sentence by applying the Part-of-
Speech (POS) tags to all tokens except some in-
dicative keywords such as 5W1H words, modal
words, stopwords etc. For instance, the question
sentence ?How can I quickly tell if my wisdom
teeth are coming? is converted to ?How can I RB
VBP if my NN NNS VBP VBG?, on top of which
the pattern mining is conducted. To further cap-
ture online language patterns, we mine a set of
frequent tokens that are unique to cQA such as
?any1?, ?im? and ?whats?, and keep them from
being generalized. The reason to hold back this
set of tokens is twofold. First, conventional POS
taggers are trained from standard English corpus,
and they could mis-tag these non-standard words.
Second, the special online tokens are analogue to
standard stopwords, and having them properly ex-
cluded could help reflect the online users? textual
questioning patterns.
It is expected that the converted patterns pre-
serve the most representative features of online
questions. Each discovered pattern makes up a
binary feature for the classification model that we
will introduce in Section 3.
2.2 Syntactic Shallow Pattern Mining
The sequential patterns represent features at the
lexical level, but we found that lexical patterns
might not always be adequate to categorize ques-
tions. For example, the pattern {when, do} could
presume the non-question ?Levator scapulae is
used when you do the traps workout? to be a ques-
tion, whereas the question ?know someone with
an eating disorder?? could be overlooked due to
the lack of indicative lexical patterns.
These limitations, however, could be allevi-
ated by syntactic features. The syntactic pattern
(SBAR(WHADVP(WRB))(S(NP)(VP))) extracted
S
NP VP
NN VBP NP
Anyone try
NP
weight
NNS
watchers
S
NP VP
NN VBP NP
Someone need
DT
a
NNP
diet
NN
motivator?
.
?
.
Anyone try weight watchers? Someone need a diet motivator?
Figure 2: An example of common syntactic pat-
terns observed in two different question sentences
from the former example has the order of NP
and VP being switched, which could indicate
the sentence to be a non-question, whereas the
pattern (VP(VB)(NP(NP)(PP))) may be evidence
that the latter example is indeed a question,
because this pattern is commonly witnessed in
the archived questions. Figure 2 shows an ex-
ample that two questions bear very different
wordings but share the same questioning pat-
tern (S(NP(NN))(VP(VPB)(NP))) at the syntactic
level. In view of the above, we argue that pat-
terns at the syntactic level could complement lex-
ical patterns in identifying question sentences.
To our knowledge, the mining of salient pat-
terns at the syntactic level was limited to a few
tasks. Zaki and Aggarwal (2003) employed tree
patterns to classify XML data, Sun et al (2007)
extracted all frequent sub-tree structures for erro-
neous sentences detection, and Wang et al (2009)
decomposed the parsing tree into fragments and
used them to match similar questions. Our work
differs from these previous works in that: (1) we
also utilize syntactic patterns for the question de-
tection; and (2) we do not blindly extract all pos-
sible sub-tree structures, but focus only on certain
portions of the parsing tree for better pattern rep-
resentation and extraction efficiency.
Given a syntactic tree T , we define syntac-
tic pattern as a part of sub-structures of T such
that the production rule for each non-leaf node in
the patterns is intact. For example, the pattern
(S(NP(NN))(VP(VPB)(NP))) in Figure 2 is con-
sidered to be a valid syntactic pattern, whereas
(S(NP(NN))(VP(VPB))) is not, since the produc-
tion rule VP?VPB?NP is not strictly complied.
We take the following measures to mine salient
syntactic patterns: First, we limit the depth of
each syntactic pattern to be within a certain range.
1157
SBARQ
WHADVP SQ
WRB MD NP ADVP VP
Q: How can I quickly tell if my wisdom teeth are coming?
SQ
MD NP ADVP VP
RBPRP SBARVB
Generalization
Decomposition
?
?
?
VP
VB SBAR
S
SBAR
IN S
VPNP
SBARQ
WHADVP SQ
WRB MD NP VP
SQ
MD NP VP
PRP SBARVB
?
?
?
VP
VB SBAR
S
SBAR
IN S
VPNP
(a) (b) (c) (d)
(a?) (b?) (c?) (d?)
?
?
Figure 3: Illustration of syntactic pattern extrac-
tion and generalization process
It is believed that the syntax structure will become
too specific if it is extended to a deeper level or
too general if the depth is too shallow, neither of
which produces good representative patterns. We
therefore set the depth D of each syntactic pattern
to be within a reasonable range (2?D?4). Sec-
ond, we prune away all leaf nodes as well as the
production rules at the POS tag level. We believe
that nodes at the bottom levels do not carry much
useful structural information favored by question
detector. For example, the simple grammar rule
NP?DT?NN does not give any insight to use-
ful question structures. Third, we relax the def-
inition of syntactic pattern by allowing the re-
moval of some nodes denoting modifiers, prepo-
sition phrases, conjunctions etc. The reason is
that these nodes are not essential in representing
the syntactic patterns and are better excluded for
generalization purpose. Figure 3 gives an illus-
tration of the process for pattern extraction and
generalization. In this example, several syntac-
tic patterns are generated from the question sen-
tence ?How can I quickly tell if my wisdom teeth
are coming??, and the tree patterns (a) and (b) are
generalized into (a?) and (b?), in which the redun-
dant branch (ADVP(RB)) that represents the ad-
verb ?quickly? is detached.
Contents on the Web are prone to noise, and
most off-the-shelf parsers are not well-trained to
parse online questions. For example, the parsing
tree of the question ?whats the matter with it??
will be very different from that of the question
?what is the matter with it??. It would certainly
be nice to know that ?whats? is a widely used
short form of the phrase ?what is? on the Web,
but we are lack of this kind of thesaurus. Nev-
ertheless, we argue that the parsing errors would
not hurt the question detector performance much
as long as the mining database is large enough.
The reason is that if certain irregular forms fre-
quently occur on the Web, there will be statisti-
cal evidences that the syntactic patterns derived
from it, though not desired, will commonly occur
as well. In other words, we take the wrong pat-
terns and utilize them to detect questions in the
irregular forms. Our approach differs from other
systems in that we do not intentionally try to rec-
tify the grammatical errors, but leave the errors as
they are and use the statistical based approach to
capture those informal patterns.
The pattern extraction process is outlined in Al-
gorithm 1. The overall mining strategy is analo-
gous to the mining of sequential patterns, where
support and confidence measures are taken into
account to control the significance of the mined
patterns. All mined syntactic patterns together
with the lexical patterns will be used as features
for learning the classification model.
Algorithm 1 ExtractPattern(S, D)
Input: A set of syntactic trees for sentences (S); the depth
range (D)
Output: A set of sub-tree patterns extracted from S
1: Patterns = {}
2: for all Syntactic tree T ? S do
3: Nodes ? Top-down level order traversal of T
4: for all node n ? Nodes do
5: Extract subtree p rooted under node n, with depth
within the range D
6: p ? generalize(p)
7: Patterns.add(p)
8: end for
9: end for
10: return Patterns
3 Learning the Classification Model
Although Conditional Random Fields (CRF) is
good sequential learning algorithm and has been
used in other related work (Cong et al, 2008),
here we select Support Vector Machines (SVM)
as an alternative learner. The reason is that our
task not only deals with sequential patterns but
also involves syntactic patterns that possess no
sequential criteria. Additionally, SVM has been
widely shown to provide superior results com-
pared to other classifiers.
1158
The input to a SVM binary classifier normally
consists of both positive and negative examples.
While it is easy to discover certain patterns from
questions, it is unnatural to identify character-
istics for non-questions, as they usually do not
share such common lexical and syntactic patterns.
The lack of good negative examples leads tra-
ditional SVM to perform poorly. To adapt the
imbalanced input data, we proposed to employ
a one-class SVM method (Manevitz and Yousef,
2002) for learning. The basic idea of one-class
SVM is to transform features from only positive
examples via a kernel to a hyper-plane and treats
the origin as the only member of the second class.
It uses relaxation parameters to separate the posi-
tive examples from the origin, and finally applies
the standard two-class SVM techniques to learn
a decision boundary. As a result, anything out-
side the boundary are considered to be outliers
(i.e. non-questions in this problem).
More formally, given n training samples
x1, . . . , xn of one class, the hyperplane separating
them from the origin is constructed by solving
min 12?w?
2 + 1?n
n?
i=1
?i ? ? (1)
subject to: w ? ?(xi) ? ? ? ?i, where ? is a ker-
nel function, ?i is the slack variable, and ? is the
parameter controlling the upper bound percentage
of outliers. If w and ? solve this problem, the de-
cision function f(x) = sign(w ??(x)??) will be
positive for most examples xi in the training set.
Supervised learning methods usually require
training data to be manually annotated. To save
labeling efforts, we take a shortcut by treating all
sentences ending with question marks as an initial
positive examples. This assumption is acceptable,
as Cong et al (2008) reported that the rule-based
method using only question mark achieves a very
high precision of over 97% in detecting questions.
It in turn indicates that questions ending with ???
are highly reliable to be real questions.
However, the initial training data still contain
many sentences ending with ??? but are not true
questions. These possible outliers will shift the
decision boundary away from the optimal one,
and we need to remove them from the training
dataset for better classification. Many prepro-
cessing strategies are available for training data
Good positive examples
(true questions)
Bad positive examples 
(non-questions)
Origin
(i) (ii) (iii)
Iterations for training 
data refinement
(i)
Decision
Boundary
Iterations
Figure 4: Illustration of one-class SVM classifi-
cation with training data refinement (conceptual
only). Three iterations (i) (ii) (iii) are presented.
refinement, including bootstrapping, condensing,
and editing etc. In this work, we employ a SVM-
based data editing and classification method pro-
posed by Song et al (2008), which iteratively sets
a small value to the parameter ? of the one-class
SVM so as to continuously refine the decision
boundary. The algorithm could be better visual-
ized with Figure 4. In each iteration, a new de-
cision boundary will be determined based on the
existing set of data points, and a portion of pos-
sible outliers will be removed from the training
set. It is expected that the learned hyperplane will
eventually be very close to the optimal one.
We use the freely available software LIBSVM2
to conduct the one-class SVM training and test-
ing. A linear kernel is used, as it is shown to be
superior in our experiments. In each refinement
iteration, the parameter ? is conservatively set to
0.02. The number of iteration is dynamically de-
termined according to the algorithm depicted in
(Song et al, 2008). Other parameters are all set to
default. The refined decision boundary from the
training dataset will be applied to classify ques-
tions from non-questions. The question detector
model learned will serve as a component for the
cQA question retrieval system in our experiments.
4 Experiments
In this section, we present empirical evaluation
results to assess the effectiveness of our ques-
tion detection model. In particular, we first ex-
amine the effects of the number of patterns on
question detection performance. We further con-
duct experiments to show that our question de-
2Available at: http://www.csie.ntu.edu.tw/?cjlin/libsvm
1159
# of Lexical Confidence # of Syntactic Confidence
Patterns 60% 65% 70% 75% 80% Patterns 60% 65% 70% 75% 80%
Su
pp
or
t 0.40% 1685 1639 1609 1585 1545
Su
pp
or
t 0.03% 916 758 638 530 453
0.45% 1375 1338 1314 1294 1277 0.04% 707 580 488 402 341
0.50% 1184 1151 1130 1113 1110 0.05% 546 450 375 308 261
0.55% 1037 1007 989 975 964 0.06% 468 379 314 260 218
Table 1: Number of lexical and syntactic patterns mined over different support and confidence values
Lexical
Patterns
Confidence Syntactic
Patterns
Confidence
65% 70% 75% 60% 65% 70%
P R F1 P R F1 P R F1 P R F1 P R F1 P R F1
Su
pp
or
t 0.40% 85.7 90.7 88.1 86.9 88.6 87.7 87.8 86.6 87.2
Su
pp
or
t 0.03% 80.4 83.3 81.9 85.1 77.5 81.1 90.7 70.2 79.1
0.45% 86.6 90.2 88.4 88.9 88.5 88.7 89.6 86.7 88.2 0.04% 79.0 86.1 82.4 90.1 78.2 83.7 90.8 70.8 79.6
0.50% 88.5 91.6 88.4 86.4 89.0 87.7 86.2 87.9 87.0 0.05% 80.3 82.5 81.4 88.8 78.4 83.3 89.9 69.0 78.1
0.55% 86.5 89.9 88.1 88.1 87.5 87.8 88.0 89.2 88.6 0.06% 83.0 83.2 83.1 88.5 77.2 82.4 86.7 75.8 80.9
Table 2: Question detection performance over different sets of lexical patterns and syntactic patterns
tection model combining both lexical and syntac-
tic features outperforms traditional rule-based or
lexical-based methods. We finally demonstrate
that our question detection model gives additional
performance boosting to question matching.
4.1 Performance Variation over Different
Pattern Sets
The performance of the question detection model
can be sensitive to the number of features used for
learning. To find the optimal number of features
used for model training, we examine the perfor-
mance variation over different amount of lexical
and syntactic patterns undertaken for training.
Dataset: We collected a total of around 800k
question threads from Yahoo! Answers Health-
care domain. From the collected data, we gener-
ated the following three datasets:
- Pattern Mining Set: Comprising around 350k
sentences from 60k question threads, where
those ending with ??? are treated as questions
and others as non-questions.
- Training Set: Positive examples comprising
around 130k sentences ending with ??? from
another 60k question threads for the one-class
SVM learning algorithm.
- Testing Set: Two annotators are asked to tag
randomly picked sentences from the remaining
set. A total of 2,004 question sentences and
2,039 non-question sentences are annotated.
Methods & Results: We use different combi-
nations of support and confidence values to gen-
erate different set of patterns. The support value
ranges from 0.40% to 0.55% for lexical patterns
with a step size of 0.05%, and ranges from 0.03%
to 0.06% for syntactic patterns with a step size
of 0.01%. The confidence value for both patterns
ranges from 60% to 80% with a step size of 5%.
These value ranges are empirically determined.
Table 1 presents the number of lexical and syn-
tactic patterns mined against different support and
confidence value combinations.
For each set of lexical or syntactic patterns
mined, we use them as features for model train-
ing. We convert the training sentences into a set
of feature vectors and employ the one-class SVM
algorithm to train a classifier. The classifier will
then be applied to predict the question sentences
in the testing set. To evaluate each question de-
tection model, we employ Precision (P ), Recall
(R), and F1 as performance metrics, and Table 2
presents the results3.
We observe from Table 2 that given a fixed sup-
port level, the precision generally increases with
the confidence level for both lexical and syntactic
patterns, but the recall drops. The lexical feature
set comprising 1,314 sequential patterns as gen-
erated with {sup=0.45%, conf=70%} gives the
best F1 score of 88.7%, and the syntactic feature
set comprising 580 syntactic patterns generated
from {sup=0.04%, conf=65%} gives the best F1
score of 83.7%. It is noted that the sequential
patterns give relatively high recall while the syn-
tactic patterns give relatively high precision. Our
reading is that the sequential patterns are capable
of capturing most questions, but it may also give
wrong predictions to non-questions such as ?Lev-
3The results for certain confidence levels are not very
promising and are not shown in the table due to lack of space.
1160
ator scapulae is used when you do the traps work-
out? that bears the sequential pattern {when, do}.
On the other hand, the syntactic patterns could
give reliable predictions, but its coverage could
suffer due to the limited number of syntactic pat-
terns. We conjecture that a combination of both
features could further improve the performance.
4.2 Performance Comparison with
Traditional Question Detection Methods
We next conduct experiments to compare the per-
formance of our question detection model to tra-
ditional rule-based or lexical-based methods.
Methods & Results: We set up five different
systems for meaningful comparisons:
1. 5W1H (baseline1): a rule-based method using
5W1H to determine a question sentence.
2. Question Mark (baseline2): a method using the
question mark ??? to judge a question.
3. SeqPattern: Using only the set of 1,314 se-
quential patterns as features.
4. SynPattern: Using only the set of 580 syntactic
patterns as features.
5. SeqPattern+SynPattern: Merging both lexical
and syntactic patterns and use them as a set of
features for question detection.
We again employ Precision (P ), Recall (R),
and F1 as performance metrics to evaluate each
question detection system, and tabulate the com-
parison results in Table 3. From the Table, we
observe that 5W1H performs poorly in both preci-
sion and recall, and question mark based method
gives relatively low recall although the precision
is the highest amongst all the methods evaluated.
This is in line with the results as observed in
(Cong et al, 2008). SeqPattern outperforms the
two baseline systems in both R and F1 scores,
and its combination with SynPattern augments
the performance in both precision and recall by
a lot. It also achieves statistically significant im-
proved results (t-test, p-value<0.05) as compared
to other four systems. These results are consistent
with our intuition that syntactic patterns can lever-
age sequential patterns in improving the question
detection performance.
It is noted that SeqPattern+SynPattern exhibits
the highest recall (R) amongst all the systems.
The significance test further suggests that many
System Combination P (%) R(%) F1(%)
(1) 5W1H 75.37 49.50 59.76
(2) Question Mark 94.12 77.50 85.00
(3) SeqPattern 88.92 88.47 88.69
(4) SynPattern 90.06 78.19 83.71
(5) SeqPattern+SynPattern 92.11 89.67 90.87
Table 3: Performance comparisons for question
detection on different system combinations
question sentences miss-detected by 5W1H or
Question Mark method could be properly cap-
tured by our model. This improvement is mean-
ingful, as the question coverage is also an im-
portant factor in the cQA question retrieval task,
where high recall implies that more similar ques-
tions could be matched and returned, hence im-
proving the question retrieval performance.
4.3 Performance Evaluation on Question
Retrieval with Question Detection Model
To further demonstrate that our question detection
model can improve question retrieval, we incor-
porate it into different question retrieval systems.
Methods: We select a simple bag-of-word
(BoW) system retrieving questions at the lexical
level, and a syntactic tree matching (STM) model
matching questions at the syntactic level (Wang et
al., 2009) as two baselines. For each baseline, we
further set up two different combinations:
- Baseline+QM: Using question mark to detect
question sentences, and perform question re-
trieval on top of the detected questions.
- Baseline+QD: Using our proposed model to
detect question sentences, and perform ques-
tion retrieval on top of the detected questions.
This gives rise to additional 4 different system
combinations for comparison.
Dataset: We divide the dataset from Yahoo!
Answers into a question repository set (750k) and
a test set (50k). For the baseline systems, all the
repository sentences containing both questions
and non-questions are indexed, whereas for sys-
tems equipped with QM or QD, only the detected
question sentences are indexed for retrieval. We
randomly select 250 single-sentence questions
from the test set as queries, and for each query, the
retrieval system will return a list of top 10 ques-
tion matches. We combine the retrieved results
from different systems and ask two annotators to
label each result to be either ?relevant? or ?irrel-
1161
System BoW BoW BoW STM STM STM
Combination +QM +QD +QM +QD
MAP (%) 58.07 59.89 60.68 66.53 68.41 69.85
% improvement
of MAP over:
Baseline N.A. +3.13 +4.49 N.A. +2.83 +4.99
Baseline+QM N.A. N.A. +1.32 N.A. N.A. +2.10
P@1 (%) 59.81 61.21 63.55 63.08 64.02 65.42
Table 4: Question retrieval performance on differ-
ent system combinations measured by MAP and
P@1 (Baseline is either BoW or STM)
evant? without telling them which system the re-
sult is generated from. By eliminating some query
questions that have no relevant matches, the final
testing set contains 214 query questions.
Metrics & Results: We evaluate the question
retrieval performance using two metrics: Mean
Average Precision (MAP) and Top One Precision
(P@1). The results are presented in Table 4.
We can see from Table 4 that STM outper-
forms BoW. Applying QM or QD over BoW and
STM boosts the system performance in terms of
both MAP and P@1. They also achieve statis-
tical significance as judged by paired t-test (p-
value<0.05). More specifically, the MAP on
QM coupled systems improves by 3.13% and
2.83% respectively over BoW and STM. This is
evidence that having question sentences clearly
identified could help to retrieve relevant ques-
tions more precisely, as without question detec-
tion, the user query is likely to be matched to ir-
relevant description sentences. Our question de-
tection model (QD) further improves the MAP
by 1.32% and 2.1% respectively over BoW+QM
and STM+QM, and it also yields better top one
precision by correctly retrieving questions at the
first position on 136 and 140 questions respec-
tively, out of a total of 214 questions. These im-
provements are in line with our expectation that
our model incorporating salient features at both
the lexical and syntactic levels is comprehensive
enough to capture various forms of questions on-
line, and hence improve the performance of ques-
tion matching.
5 Related Work
Research on detecting question sentences can
generally be classified into two categories. The
first category simply employs rule-based methods
such as question mark, 5W1H words, or hand-
crafted regular expressions to detect questions.
As discussed, these conventional methods are not
adequate to cope with online questions.
The second category uses machine learning ap-
proaches to detect question sentences. Shrestha
and McKeown (2004) proposed a supervised rule
induction method to detect interrogative questions
in email conversations based on part-of-speech
features. Yeh and Yuan (2003) used a statistical
approach to extract a set of question-related words
and derived some syntax and semantic rules to
detect mandarin question sentences. Cong et al
(2008) extracted labeled sequential patterns and
used them as features to learn a classifier for ques-
tion detection in online forums.
Question pattern mining is also closely related
to the learning of answer patterns. Work on an-
swer patterns includes the web based pattern min-
ing (Zhang and Lee, 2002; Du et al, 2005) and a
combination of syntactic and semantic elements
(Soubbotin and Soubbotin, 2002) etc.
In contrast to previous work, we do not only fo-
cus on standard language corpus, but extensively
explore characteristics of online questions. Our
approach exploits salient question patterns at both
the lexical and syntactic levels for question detec-
tion. In particular, we employ the one-class SVM
algorithm such that the learning process is weakly
supervised and no human annotation is involved.
6 Conclusion
This paper proposed a new approach to detecting
question sentences in cQA.Wemined both lexical
and syntactic question patterns, and used them as
features to build classification models. The min-
ing and leaning process is fully automated and re-
quires no human intervention. Empirical evalua-
tion on the cQA archive demonstrated the effec-
tiveness of our model as well as its usefulness in
improving question retrieval performance.
We are still investigating other features that are
helpful to detect questions. One promising direc-
tion for future work is to also employ lexical and
syntactic patterns to other related areas such as
question type classification etc. It is also interest-
ing to employ a hybrid of CRF and SVM learning
methods to boost the accuracy and scalability of
the classifier.
1162
References
Agichtein, Eugene, Carlos Castillo, Debora Donato,
Aristides Gionis, and Gilad Mishne. 2008. Find-
ing high-quality content in social media. In WSDM,
pages 183?194.
Cong, Gao, Long Wang, Chin-Yew Lin, Young-In
Song, and Yueheng Sun. 2008. Finding question-
answer pairs from online forums. In SIGIR, pages
467?474.
Du, Yongping, Helen Meng, Xuanjing Huang, and
Lide Wu. 2005. The use of metadata, web-
derived answer patterns and passage context to
improve reading comprehension performance. In
HLT, pages 604?611.
Duan, Huizhong, Yunbo Cao, Chin-Yew Lin, and
Yong Yu. 2008. Searching questions by identify-
ing question topic and question focus. In HLT-ACL,
pages 156?164.
Jeon, Jiwoon, W. Bruce Croft, and JoonHo Lee. 2005.
Finding similar questions in large question and an-
swer archives. In CIKM, pages 84?90.
Jindal, Nitin and Bing Liu. 2006. Identifying compar-
ative sentences in text documents. In SIGIR, pages
244?251.
Manevitz, Larry M. and Malik Yousef. 2002. One-
class svms for document classification. J. Mach.
Learn. Res., 2:139?154.
Pei, Jian, Jiawei Han, Behzad Mortazavi-asl, Helen
Pinto, Qiming Chen, Umeshwar Dayal, and Mei
chun Hsu. 2001. Prefixspan: Mining sequen-
tial patterns efficiently by prefix-projected pattern
growth. In ICDE, pages 215?224.
Riezler, Stefan, Alexander Vasserman, Ioannis
Tsochantaridis, Vibhu Mittal, and Yi Liu. 2007.
Statistical machine translation for query expansion
in answer retrieval. In ACL, pages 464?471.
Shrestha, Lokesh and Kathleen McKeown. 2004. De-
tection of question-answer pairs in email conversa-
tions. In COLING, page 889.
Song, Xiaomu, Guoliang Fan, and M. Rao. 2008.
Svm-based data editing for enhanced one-class
classification of remotely sensed imagery. Geo-
science and Remote Sensing Letters, IEEE,
5(2):189?193.
Soubbotin,Martin M. and Sergei M. Soubbotin. 2002.
Use of patterns for detection of likely answer
strings: A systematic approach. In TREC.
Sun, Guihua, Gao Cong, Xiaohua Liu, Chin-Yew Lin,
and Ming Zhou. 2007. Mining sequential patterns
and tree patterns to detect erroneous sentences. In
AAAI, pages 925?930.
Wang, Kai, Zhaoyan Ming, and Tat-Seng Chua. 2009.
A syntactic tree matching approach to finding sim-
ilar questions in community-based qa services. In
SIGIR, pages 187?194.
Xue, Xiaobing, Jiwoon Jeon, and W. Bruce Croft.
2008. Retrieval models for question and answer
archives. In SIGIR, pages 475?482.
Yeh, Ping-Jer and Shyan-MingYuan. 2003. Mandarin
question sentence detection: A preliminary study.
In EPIA, pages 466?478.
Zaki, Mohammed J. and Charu C. Aggarwal. 2003.
Xrules: an effective structural classifier for xml
data. In KDD, pages 316?325.
Zhang, Dell and Wee Sun Lee. 2002. Web based pat-
tern mining and matching approach to question an-
swering. In TREC.
1163
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 140?150,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Domain-Assisted Product Aspect Hierarchy Generation: Towards
Hierarchical Organization of Unstructured Consumer Reviews
Jianxing Yu1, Zheng-Jun Zha1, Meng Wang1, Kai Wang2, Tat-Seng Chua1
1School of Computing, National University of Singapore
2Institute for Infocomm Research, Singapore
{jianxing, zhazj, wangm, chuats}@comp.nus.edu.sg kwang@i2r.a-star.edu.sg
Abstract
This paper presents a domain-assisted ap-
proach to organize various aspects of a prod-
uct into a hierarchy by integrating domain
knowledge (e.g., the product specifications),
as well as consumer reviews. Based on the
derived hierarchy, we generate a hierarchical
organization of consumer reviews on various
product aspects and aggregate consumer opin-
ions on these aspects. With such organiza-
tion, user can easily grasp the overview of
consumer reviews. Furthermore, we apply the
hierarchy to the task of implicit aspect identi-
fication which aims to infer implicit aspects of
the reviews that do not explicitly express those
aspects but actually comment on them. The
experimental results on 11 popular products in
four domains demonstrate the effectiveness of
our approach.
1 Introduction
With the rapidly expanding e-commerce, most retail
Web sites encourage consumers to write reviews to
express their opinions on various aspects of prod-
ucts. Huge collections of consumer reviews are
now available on the Web. These reviews have be-
come an important resource for both consumers and
firms. Consumers commonly seek quality informa-
tion from online consumer reviews prior to purchas-
ing a product, while many firms use online reviews
as an important resource in their product develop-
ment, marketing, and consumer relationship man-
agement. However, the reviews are disorganized,
leading to the difficulty in information navigation
and knowledge acquisition. It is impractical for user
to grasp the overview of consumer reviews and opin-
ions on various aspects of a product from such enor-
mous reviews. Among hundreds of product aspects,
it is also inefficient for user to browse consumer re-
views and opinions on a specific aspect. Thus, there
is a compelling need to organize consumer reviews,
so as to transform the reviews into a useful knowl-
edge structure. Since the hierarchy can improve in-
formation representation and accessibility (Cimiano,
2006), we propose to organize the aspects of a prod-
uct into a hierarchy and generate a hierarchical or-
ganization of consumer reviews accordingly.
Towards automatically deriving an aspect hierar-
chy from the reviews, we could refer to traditional
hierarchy generation methods in ontology learning,
which first identify concepts from the text, then
determine the parent-child relations between these
concepts using either pattern-based or clustering-
based methods (Murthy et al, 2010). However,
pattern-based methods usually suffer from inconsis-
tency of parent-child relationships among the con-
cepts, while clustering-based methods often result
in low accuracy. Thus, by directly utilizing these
methods to generate an aspect hierarchy from con-
sumer reviews, the resulting hierarchy is usually in-
accurate, leading to unsatisfactory review organiza-
tion. On the other hand, domain knowledge of prod-
ucts is now available on the Web. For example,
there are more than 248,474 product specifications
in the product sellingWeb site CNet.com (Beckham,
2005). These product specifications cover some
product aspects and provide coarse-grained parent-
child relations among these aspects. Such domain
knowledge is useful to help organize the product as-
140
Figure 1: Sample hierarchical organization for iPhone 3G
pects into a hierarchy. However, the initial hierarchy
obtained from domain knowledge usually cannot fit
the review data well. For example, the initial hierar-
chy is usually too coarse and may not cover the spe-
cific aspects commented in the reviews, while some
aspects in the hierarchy may not be of interests to
users in the reviews.
Motivated by the above observations, we propose
in this paper to organize the product aspects into a
hierarchy by simultaneously exploiting the domain
knowledge (e.g., the product specification) and con-
sumer reviews. With derived aspect hierarchy, we
generate a hierarchical organization of consumer re-
views on various aspects and aggregate consumer
opinions on these aspects. Figure 1 illustrates a sam-
ple of hierarchical review organization for the prod-
uct ?iPhone 3G?. With such organization, users can
easily grasp the overview of product aspects as well
as conveniently navigate the consumer reviews and
opinions on any aspect. For example, users can find
that 623 reviews, out of 9,245 reviews, are about the
aspect ?price?, with 241 positive and 382 negative
reviews.
Given a collection of consumer reviews on a spe-
cific product, we first automatically acquire an ini-
tial aspect hierarchy from domain knowledge and
identify the aspects from the reviews. Based on the
initial hierarchy, we develop a multi-criteria opti-
mization approach to construct an aspect hierarchy
to contain all the identified aspects. Our approach
incrementally inserts the aspects into the initial hi-
erarchy based on inter-aspect semantic distance, a
metric used to measure the semantic relation among
aspects. In order to derive reliable semantic dis-
tance, we propose to leverage external hierarchies,
sampled from WordNet and Open Directory Project,
to assist semantic distance learning. With resultant
aspect hierarchy, the consumer reviews are then or-
ganized to their corresponding aspect nodes in the
hierarchy. We then perform sentiment classification
to determine consumer opinions on these aspects.
Furthermore, we apply the hierarchy to the task of
implicit aspect identification. This task aims to infer
implicit aspects of the reviews that do not explic-
itly express those aspects but actually comment on
them. For example, the implicit aspect of the review
?It is so expensive? is ?price.? Most existing aspect
identification approaches rely on the appearance of
aspect terms, and thus are not able to handle implicit
aspect problem. Based on our aspect hierarchy, we
can infer the implicit aspects by clustering the re-
views into their corresponding aspect nodes in the
hierarchy. We conduct experiments on 11 popular
products in four domains. More details of the corpus
are discussed in Section 4. The experimental results
demonstrate the effectiveness of our approach.
The main contributions of this work can be sum-
marized as follows:
1) We propose to hierarchically organize con-
sumer reviews according to an aspect hierarchy, so
as to transfer the reviews into a useful knowledge
structure.
2) We develop a domain-assisted approach to
generate an aspect hierarchy by integrating domain
knowledge and consumer reviews. In order to de-
rive reliable semantic distance between aspects, we
propose to leverage external hierarchies to assist se-
mantic distance learning.
3) We apply the aspect hierarchy to the task of im-
plicit aspect identification, and achieve satisfactory
performance.
The rest of this paper is organized as follows. Our
approach is elaborated in Section 2 and applied to
implicit aspect identification in Section 3. Section
4 presents the evaluations, while Section 5 reviews
141
related work. Finally, Section 6 concludes this paper
with future works.
2 Approach
Our approach consists of four components, includ-
ing initial hierarchy acquisition, aspect identifica-
tion, semantic distance learning, and aspect hierar-
chy generation. Next, we first define some prelimi-
nary and notations and then elaborate these compo-
nents.
2.1 Preliminary and Notations
Preliminary 1. An aspect hierarchy is defined as a
tree that consists of a set of unique aspects A and
a set of parent-child relations R between these as-
pects.
Given the consumer reviews of a product, let
A = {a1, ? ? ? , ak} denotes the product aspects com-
mented in the reviews. H0(A0,R0) denotes the ini-
tial hierarchy derived from domain knowledge. It
contains a set of aspects A0 and relations R0. Our
task is to construct an aspect hierarchy H(A,R), to
cover all the aspects in A and their parent-child re-
lations R, so that the consumer reviews are hierar-
chically organized. Note that H0 can be empty.
2.2 Initial Hierarchy Acquisition
As aforementioned, product specifications on prod-
uct selling websites cover some product aspects and
coarse-grained parent-child relations among these
aspects. Such domain knowledge is useful to help
organize aspects into a hierarchy. We here employ
the approach proposed by Ye and Chua (2006) to au-
tomatically acquire an initial aspect hierarchy from
the product specifications. The method first identi-
fies the Web page region covering product descrip-
tions and removes the irrelevant contents from the
Web page. It then parses the region containing the
product information to identify the aspects as well as
their structure. Based on the aspects and their struc-
ture, it generates an aspect hierarchy.
2.3 Aspect Identification
To identify aspects in consumer reviews, we first
parse each review using the Stanford parser 1. Since
the aspects in consumer reviews are usually noun
1http://nlp.stanford.edu/software/lex-parser.shtml
Figure 2: Sample Pros and Cons reviews
or noun phrases (Liu, 2009), we extract the noun
phrases (NP) from the parse tree as aspect candi-
dates. While these candidates may contain much
noise, we leverage Pros and Cons reviews (see Fig-
ure 2), which are prevalent in forum Web sites,
to assist identify aspects from the candidates. It
has been shown that simply extracting the frequent
noun terms from the Pros and Cons reviews can get
high accurate aspect terms (Liu el al., 2005). Thus,
we extract the frequent noun terms from Pros and
Cons reviews as features, then train a one-class SVM
(Manevitz et al, 2002) to identify aspects from the
candidates. While the obtained aspects may con-
tain some synonym terms, such as ?earphone? and
?headphone?, we further perform synonym cluster-
ing to get unique aspects. Specifically, we first ex-
pand each aspect term with its synonym terms ob-
tained from the synonym termsWeb site 2, then clus-
ter them to obtain unique aspects based on unigram
feature.
2.4 Semantic Distance Learning
Our aspect hierarchy generation approach is essen-
tially based on the semantic relations among as-
pects. We here define a metric, Semantic Distance,
d(ax, ay), to quantitatively measure the semantic re-
lation between aspects ax and ay. We formulate
d(ax, ay) as the weighted sum of some underlying
features,
d(ax, ay) =
?
j
wjfj(ax, ay), (1)
where wj is the weight for j-th feature function
fj(?).
Next, we first introduce the linguistic features
used in our work and then present the semantic dis-
tance learning algorithm that aims to find the opti-
mal weights in Eq.(1).
2http://thesaurus.com
142
2.4.1 Linguistic Features
Given two aspects ax and ay, a feature is defined
as a function generating a numeric score f(ax, ay)
or a vector of scores. The features include Contex-
tual, Co-occurrence, Syntactic, Pattern and Lexical
features (Yang and Callan, 2009). These features are
generated based on auxiliary documents collected
from Web.
Specifically, we issue each aspect term and aspect
term pair as queries to Google and Wikipedia, re-
spectively, and collect the top 100 returned docu-
ments of each query. We then split the documents
into sentences. Based on these documents and sen-
tences, the features are generated as follows.
Contextual features. For each aspect, we collect
the documents containing the aspect as context to
build a unigram language model without smoothing.
Given two aspects, the KL-divergence between their
language models is computed as the Global-Context
feature between them. Similarly, we collect the left
two and right two words surrounding each aspect as
context and build a unigram language model. The
KL-divergence between the language models of two
aspects is defined as the Local-Context feature.
Co-occurrence features. We measure the co-
occurrence of two aspects by Pointwise Mutual
Information (PMI): PMI(ax,ay)=log(Count(ax,ay)/
Count(ax) Count(ay)), where Count(?) stands for the
number of documents or sentences containing the
aspect(s), or the number of Google document hits
for the aspect(s). Based on different definitions of
Count(?), we define the features of Document PMI,
Sentence PMI, and Google PMI, respectively.
Syntactic features. We parse the sentences that
contain each aspect pair into syntactic trees via the
Stanford Parser. The Syntactic-path feature is de-
fined as the average length of the shortest syntactic
path between the aspect pair in the tree. In addi-
tion, for each aspect, we collect a set of sentences
containing it, and label the semantic role of the sen-
tences via ASSERT parser 3. Given two aspects,
the number of the Subject terms overlaps between
their sentence sets is computed as the Subject Over-
lap feature. Similarly, for other semantic roles, such
as objects, modifiers, and verbs, we define the fea-
tures of Object Overlap, Modifier Overlap, and Verb
3http://cemantix.org/assert.html
Overlap, respectively.
Pattern features. 46 patterns are used in our
work, including 6 patterns indicating the hypernym
relations of two aspects (Hearst, 1992), and 40 pat-
terns measuring the part-of relations of two aspects
(Girju et al, 2006). These pattern features are
asymmetric, and they take the parent-child relations
among the aspects into consideration. All the pat-
terns are listed in Appendix A (submitted as supple-
mentary material). Based on these patterns, a 46-
dimensional score vector is obtained for each aspect
pair. A score is 1 if two aspects match a pattern, and
0 otherwise.
Lexical features. We take the word length differ-
ence between two aspects, as Length Difference fea-
ture. In addition, we issue the query ?define:aspect?
to Google, and collect the definition of each aspect.
We then count the word overlaps between the defini-
tions of two aspects, as Definition Overlap feature.
2.4.2 Semantic Distance Learning
This section elaborates the learning algorithm
that optimizes the semantic distance metric, i.e.,
the weighting parameters in Eq.(1). Typically, we
can utilize the initial hierarchy as training data.
The ground-truth distance between two aspects
dG(ax, ay) is generated by summing up all the edge
distances along the shortest path between ax and ay,
where every edge weight is assumed as 1. The dis-
tance metric is then obtained by solving the follow-
ing optimization problem,
argmin
wj |mj=1
?
ax,ay?A0
x<y
(dG(ax, ay) ?
m?
j=1
wjfj(ax, ay))2+??
m?
j=1
w2j ,
(2)
where m is the dimension of linguistic feature, ? is
a tradeoff parameter. Eq.(2) can be rewrote to its
matrix form as,
argmin
w
??d? fTw??2 + ? ? ?w?2 , (3)
where vector d contains the ground-truth distance of
all the aspect pairs. Each element corresponds to
the distance of certain aspect pair, and f is the corre-
sponding feature vector. The optimal solution of w
is given as
w? = (fT f + ? ? I)?1(fTd) (4)
143
where I is the identity metric.
The above learning algorithm can perform well
when sufficient training data (i.e., aspect (term)
pairs) is available. However, the initial hierarchy is
usually too coarse and thus cannot provide sufficient
information. On the other hand, abundant hand-
crafted hierarchies are available on the Web, such
as WordNet and Open Directory Project (ODP). We
here propose to leverage these external hierarchies
to assist semantic distance learning. A distance met-
ric w0 is learned from the external hierarchies us-
ing the above algorithm. Since w0 might be biased
to the characteristics of the external hierarchies, di-
rectly using w0 in our task may not perform well.
Alternatively, we use w0 as prior knowledge to as-
sist learning the optimal distance metric w from the
initial hierarchy. The learning problem is formulated
as follows,
argmin
w
??d? fTw??2 + ? ? ?w?2 + ? ? ?w? w0?2 ,
(5)
where ? and ? are tradeoff parameters.
The optimal solution of w can be obtained as
w? = (fT f + (? + ?) ? I)?1(fTd + ? ? w0). (6)
As a result, we can compute the semantic distance
between each two aspects according to Eq.(1).
2.5 Aspect Hierarchy Generation
Given the aspectsA = {a1, ? ? ? , ak} identified from
reviews and the initial hierarchy H0(A0,R0) ob-
tained from domain knowledge, our task is to con-
struct an aspect hierarchy to contain all the aspects
in A. Inspired by Yang and Callan (2009), we adopt
a multi-criteria optimization approach to incremen-
tally insert the aspects into appropriate positions in
the hierarchy based on multiple criteria.
Before going to the details, we first introduce an
information function to measure the amount of in-
formation carried in a hierarchy. An information
function Info(H) is defined as the sum of the se-
mantic distances of all the aspect pairs in the hierar-
chy (Yang and Callan, 2009).
Info(H(A,R)) =
?
x<y;ax,ay?A
d(ax, ay). (7)
Based on this information function, we then intro-
duce the following three criteria for aspect insertion:
minimum Hierarchy Evolution, minimum Hierarchy
Discrepancy and minimum Semantic Inconsistency.
Hierarchy Evolution is designed to monitor the
structure evolution of a hierarchy. The hierarchy is
incrementally hosting more aspects until all the as-
pects are allocated. The insertion of a new aspect a
into different positions in the current hierarchy H(i)
leads to different new hierarchies. Among these new
hierarchies, we here assume that the optimal one
H(i+1) should introduce the least changes of infor-
mation to H(i).
H?(i+1) = argmin
H(i+1)
?Info(H(i+1) ?H(i)). (8)
By plugging in Eq.(7) and using least square to
measure the information changes, we have,
obj1 = argmin
H(i+1)
(?x<y;ax,ay?Ai?{a} d(ax, ay)
??x<y;ax,ay?Ai d(ax, ay))2, (9)
Hierarchy Discrepancy is used to measure the
global changes of the structure. We assume a good
hierarchy should bring the least changes to the initial
hierarchy,
H?(i+1) = argmin
H(i+1)
?Info(H(i+1) ?H(0))
i + 1 . (10)
We then get,
obj2 = argmin
H(i+1)
1
i+1(
?
x<y;ax,ay?Ai?{a} d(ax, ay)
??x<y;ax,ay?A0 d(ax, ay))2. (11)
Semantic Inconsistency is introduced to quantify
the inconsistency between the semantic distance es-
timated via the hierarchy and that computed from
the feature functions. We assume that a good hier-
archy should precisely reflect the semantic distance
between aspects. For two aspects, their semantic
distance reflected by the hierarchy is computed as
the sum of adjacent distances along the shortest path
between them,
dH(ax, ay) =
?
p<q;(ap,aq)?SP (ax,ay)
d(ap, aq),
(12)
where SP (ax, ay) is the shortest path between the
aspects (ax, ay), (ap, aq) are the adjacent nodes
along the path.
144
We then define the following criteria to find the
hierarchy with minimum semantic inconsistency,
obj3 = argmin
H(i+1)
?
x<y;ax,ay?Ai?{a};
(dH(ax, ay)?d(ax, ay))2,
(13)
where d(ax, ay) is the distance computed based on
the feature functions in Section 2.4.
Through integrating the above criteria, the multi-
criteria optimization framework is formulated as,
obj = argmin
H(i+1)
(?1 ? obj1 + ?2 ? obj2 + ?3 ? obj3)
?1 + ?2 + ?3 = 1; 0 ? ?1, ?2, ?3 ? 1.
(14)
where ?1, ?2, ?3 are the tradeoff parameters.
To summarize, our aspect hierarchy generation
process starts from an initial hierarchy and inserts
the aspects into it one-by-one until all the aspects
are allocated. Each aspect is inserted to the op-
timal position found by Eq.(14). It is worth not-
ing that the insertion order may influence the result.
To avoid such influence, we select the aspect with
the least objective function value in Eq.(14) to in-
sert. Based on resultant hierarchy, the consumer re-
views are then organized to their corresponding as-
pect nodes in the hierarchy. We further prune out the
nodes without reviews from the hierarchy.
Moreover, we perform sentiment classification to
determine consumer opinions on various aspects. In
particular, we train a SVM sentiment classifier based
on the Pros and Cons reviews described in Section
2.3. We collect sentiment terms in the reviews as
features and represent reviews as feature vectors us-
ing Boolean weighting. Note that we define senti-
ment terms as those appear in the sentiment lexicon
provided by MPQA project (Wilson et al, 2005).
3 Implicit Aspect Identification
In this section, we apply the aspect hierarchy to the
task of implicit aspect identification. This task aims
to infer the aspects of reviews that do not explic-
itly express those aspects but actually comment on
them (Liu et al 2005). Take the review ?The phone
is too large? as an example, the task is to infer its
implicit aspect ?size.? It has been observed that the
reviews commenting on a same aspect usually use
some same sentiment terms (Su et al, 2008). There-
fore, sentiment term is an effective feature for identi-
fying implicit aspects. We here collect the sentiment
terms as features to represent each review into a fea-
ture vector. For each aspect node in the hierarchy,
we define its centroid as the average of its feature
vectors, i.e., the feature vectors of all the reviews
that are allocated at this node. We then calculate
the cosine similarity of each implicit-aspect review
to the centriods of all the aspect nodes, and allo-
cate the review into the node with maximum sim-
ilarity. As a result, the implicit aspect reviews are
grouped to their related aspect nodes. In other word,
their aspects are identified as the corresponding as-
pect nodes.
4 Evaluations
In this section, we evaluate the effectiveness of our
approach on aspect identification, aspect hierarchy
generation, and implicit aspect identification.
4.1 Data and Experimental Setting
The details of our product review corpus are given
in Table 1. This corpus contains consumer reviews
on 11 popular products in four domains. These
reviews were crawled from several prevalent fo-
rum Web sites, including cnet.com, viewpoints.com,
reevoo.com and gsmarena.com. All of the reviews
were posted between June, 2009 and Sep 2010. The
aspects of the reviews, as well as the opinions on
the aspects were manually annotated. We also in-
vited five annotators to construct the gold-standard
hierarchies for the products by providing them the
initial hierarchies and the aspects in reviews. The
conflicts between annotators were resolved through
their discussions. For semantic distance learning, we
collected 50 hierarchies from WordNet and ODP, re-
spectively. The details are shown in Table 2. We
listed the topics of the hierarchies in Appendix B
(submitted as supplementary material).
Product Name Domain Review# Sentence#
Canon EOS 450D (Canon EOS) camera 440 628
Fujifilm Finepix AX245W (Fujifilm) camera 541 839
Panasonic Lumix DMC-TZ7 (Panasonic) camera 650 1,546
Apple MacBook Pro (MacBook) laptop 552 4,221
Samsung NC10 (Samsung) laptop 2,712 4,946
Apple iPod Touch 2nd (iPod Touch) MP3 4,567 10,846
Sony NWZ-S639 16GB (Sony NWZ) MP3 341 773
BlackBerry Bold 9700 (BlackBerry) phone 4,070 11,008
iPhone 3GS 16GB (iPhone 3GS) phone 12,418 43,527
Nokia 5800 XpressMusic (Nokia 5800) phone 28,129 75,001
Nokia N95 phone 15,939 44,379
Table 1: Statistics of the reviews corpus, # denotes the
size of the reviews/sentences
145
Statistic WordNet ODP
Total # hierarchies 50 50
Total # terms 1,964 2,210
Average # depth 5.5 5.9
Total # related topics 12 16
Table 2: Statistics of the External Hierarchies
Figure 3: Evaluations on Aspect Identification. t-test, p-
values<0.05
We employed F1-measure, which is the combina-
tion of precision and recall, as the evaluation metric
for all the evaluations. For the evaluation on aspect
hierarchy, we defined precision as the percentage of
correctly returned parent-child pairs out of the to-
tal returned pairs, and recall as the percentage of
correctly returned parent-child pairs out of the to-
tal pairs in the gold standard. Throughout the ex-
periments, we empirically set ?1 = 0.4, ?2 = 0.3,
?3 = 0.3, ? = 0.4 and ? = 0.6.
4.2 Evaluations on Aspect Identification
We compared our approach against two state-of-the-
art methods: a) the method proposed by Hu and Liu
(2004), which is based on the association rule min-
ing, and b) the method proposed byWu et al (2009),
which is based on the dependency parser. The re-
sults are presented in Figure 3. On average, our
approach significantly outperforms Hu?s and Wu?s
method in terms of F1-measure by over 5.87% and
3.27%, respectively.
4.3 Evaluations on Aspect Hierarchy
4.3.1 Comparisons with the State-of-the-Arts
We compared our approach against four tra-
ditional hierarchy generation methods in the re-
searches on ontology learning, including a) pattern-
based method (Hearst, 1992) and b) clustering-based
method by Shi et al (2008), c) the method proposed
Figure 4: Evaluations on Aspect Hierarchy Generation. t-
test, p-values<0.05. w/ H denotes the methods with ini-
tial hierarchy, accordingly, w/o H refers to the methods
without initial hierarchy.
by Snow et al (2006) which was based on a proba-
bilistic model, and d) the method proposed by Yang
and Callan (2009). Since our approach and Yang?s
method can utilize initial hierarchy to assist hier-
archy generation, we evaluated their performance
with or without initial hierarchy, respectively. For
the sake of fair comparison, Snow?s, Yang?s and our
methods used the same linguistic features in Section
2.4.1.
Figure 4 shows the performance comparisons
of these five methods. We can see that our ap-
proach without using initial hierarchy outperforms
the pattern-based, clustering-based, Snow?s, and
Yang?s methods by over 17.9%, 19.8%, 2.9% and
6.1% respectively in terms of average F1-measure.
By exploiting initial hierarchy, our approach im-
proves the performance significantly. As compared
to the pattern-based, clustering-based and Snow?s
methods, it improves the average performance by
over 49.4%, 51.2% and 34.3% respectively. Com-
pared to Yang?s method with initial hierarchy, it
achieves 4.7% improvements on the average perfor-
mance.
The results show that pattern-based and
clustering-based methods perform poor. Pattern-
based method may suffer from the problem of low
coverage of patterns, especially when the patterns
are manually pre-defined, while the clustering-
based method (Shi et al, 2008) may sustain to the
bisection clustering mechanism which can only
generate a binary-tree. The results also illustrate
that our approach outperforms Snow?s and Yang?s
methods. By exploiting external hierarchies, our
146
Figure 5: Evaluations on the Impact of Initial Hierarchy.
t-test, p-values<0.05.
approach is able to derive reliable semantic distance
between aspects and thus improve the performance.
4.3.2 Evaluations on Effectiveness of Initial
Hierarchy
In this section, we show that even based on a small
part of the initial hierarchy, our approach can still
generate a satisfactory hierarchy. We explored dif-
ferent proportion of initial hierarchy, including 0%,
20%, 40%, 60% and 80% of the aspect pairs which
are collected top-down from the initial hierarchy. As
shown in Figure 5, the performance increases when
larger proportion of the initial hierarchy is used.
Thus, we can speculate that domain knowledge is
valuable in aspect hierarchy generation.
4.3.3 Evaluations on Effectiveness of
Optimization Criteria
We conducted a leave-one-out study to evaluate
the effectiveness of each optimization criterion. In
particular, we set one of the tradeoff parameters (?1,
?2, ?3) in Eq.(14) to zero, and distributed its weight
to the rest parameters averagely. From Figure 6, we
find that removing any optimization criterion would
degrade the performance on most products. It is in-
teresting to note that removing the third optimiza-
tion criterion, i.e., minimum semantic inconsistency,
slightly increases the performance on two products
(ipad touch and sony MP3). The reason might be
that the values of the three tradeoff parameters (em-
pirically set in Section 4.1) are not suitable for these
two products.
Figure 6: Evaluations of the Optimization Criteria. % of
change in F1-measure when a single criterion is removed.
t-test, p-values<0.05.
Figure 7: Evaluations on the Impact of Linguistic Fea-
tures. t-test, p-values<0.05.
4.3.4 Evaluations on Semantic Distance
Learning
In this section, we evaluated the impact of the fea-
tures and external hierarchies in semantic distance
learning. We investigated five sets of features as de-
scribed in Section 2.4.1, including contextual, co-
occurrence, syntactic, pattern and lexical features.
From Figure 7, we observe that the co-occurrence
and pattern features perform much better than con-
textual and syntactic features. A possible reason
is that co-occurrence and pattern features are more
likely to indicate parent-child aspect relationships,
while contextual and syntactic features are proba-
ble to measure sibling aspect relationships. Among
these features, the lexical features perform the worst.
The combination of all the features achieves the best
performance.
Next, we evaluated the effectiveness of external
hierarchies in semantic distance learning. We com-
pared the performance of our approach with or with-
out the external hierarchies. From Figure 8, we find
that by exploiting the external hierarchies, our ap-
147
Figure 8: Evaluations on the Impact of External Hierar-
chy. t-test, p-values<0.05.
proach improves the performance significantly. The
improvement is over 2.81% in terms of average F1-
measure. This implies that by using external hier-
archies, our approach can obtain effective semantic
distance, and thus improve the performance of as-
pect hierarchy generation.
Additionally, for sentiment classification, our
SVM classifier achieves an average F1-measure of
0.787 in the 11 products.
4.4 Evaluations on Implicit Aspect
Identification
To evaluate the performance of our approach on im-
plicit aspect identification, we collected 29,657 im-
plicit aspect review sentences on the 11 products
from the four forum Web sites introduced in Section
4.1. While most existing approaches for implicit as-
pect identification rely on hand-crafted rules (Liu,
2009), the method proposed in Su et al (2008) can
identify implicit aspects without hand-crafted rules
based on mutual clustering. Therefore, we adopt
Su?s method as the baseline here. Figure 9 illustrates
the performance comparison between Su?s and our
approach. We can see that our approach outperforms
Su?s method by over 9.18% in terms of average F1-
measure. This shows that our approach can iden-
tify the implicit aspects accurately by exploiting the
underlying associations among the sentiment terms
and each aspect in the hierarchy.
5 Related Work
Some researches treated review organization as a
multi-document summarization problem, and gen-
erated a summary by selecting and ordering sen-
tences taken from multiple reviews (Nishikawa et
Figure 9: Evaluations on Implicit Aspects Identification.
t-test, p-values<0.05
al., 2010). These works did not drill down to the
fine-grained level to explore the opinions on the
product aspects. Other researchers proposed to pro-
duce a summary covering consumer opinions on
each aspect. For example, Hu and Liu (2004) fo-
cused on extracting the aspects and determining
opinions on the aspects. However, their gener-
ated summary was unstructured, where the possible
relationships between aspects were not recognized
(Cadilhac et al, 2010). Subsequently, Carenini et
al. (2006) proposed to map the aspect to a user-
defined taxonomy, but the taxonomy was hand-
crafted which was not scalable.
Different from the previous works, we focus on
automatically generating an aspect hierarchy to hi-
erarchically organize consumer reviews. There are
some related works on ontology learning, which
first identify concepts from text, and then determine
parent-child relations between these concepts us-
ing either pattern-based or clustering-based methods
(Murthy et al, 2010). Pattern-based methods usu-
ally defined some lexical syntactic patterns to extract
the relations, while clustering-based methods mostly
utilized the hierarchical clustering methods to build
a hierarchy (Roy et al, 2006). Some works proposed
to integrate the pattern-based and clustering-based
methods in a general model, such as the probabilistic
model (Snow et al, 2006) and metric-based model
(Yang and Callan, 2009).
The researches on aspect identification are also
related to our work. Various aspect identification
methods have been proposed (Popescu et al, 2005),
including supervised methods (Liu el al., 2005), and
unsupervised methods (Mei et al, 2007). Different
148
features have been investigated for this task. For
example, Wu et al (2009) identified aspects based
on the features explored by dependency parser.
For implicit aspect identification, some works pro-
posed to define rules for identification (Liu el al.,
2005), while others suggested to automatically gen-
erate rules via mutual clustering (Su et al, 2008).
On the other hand, there are some related works
on sentiment classification (Pang and Lee, 2008).
These works can be categorized into four granu-
larities: document-level, sentence-level, aspect-level
and word-level sentiment classification (Liu, 2009).
Existing researches have been studied unsupervised
(Kim et al, 2004), supervised (Pang et al, 2002;
Pang et al, 2005) and semi-supervised classification
approaches (Goldberg et al, 2006; Li et al, 2009)
on these four levels.
6 Conclusions and Future Works
In this paper, we have developed a domain-assisted
approach to generate product aspect hierarchy by in-
tegrating domain knowledge and consumer reviews.
Based on the derived hierarchy, we can generate
a hierarchical organization of consumer reviews as
well as consumer opinions on the aspects. With such
organization, user can easily grasp the overview of
consumer reviews, as well as seek consumer reviews
and opinions on any specific aspect by navigating
through the hierarchy. We have further applied the
hierarchy to the task of implicit aspect identification.
We have conducted evaluations on 11 different prod-
ucts in four domains. The experimental results have
demonstrated the effectiveness of our approach. In
the future, we will explore other linguistic features
to learn the semantic distance between aspects, as
well as apply our approach to other applications.
Acknowledgments
This work is supported by NUS-Tsinghua Extreme
Search (NExT) project under the grant number: R-
252-300-001-490. We give warm thanks to the
project and anonymous reviewers for their valuable
comments.
References
P. Beineke, T. Hastie, C. Manning, and S. Vaithyanathan.
An Exploration of Sentiment Summarization. AAAI,
2003.
J. Beckham. The Cnet E-commerce Data set. Technical
Reports, 2005.
G. Carenini, R. Ng, and E. Zwart. Multi-document Sum-
marization of Evaluative Text. ACL, 2006.
A. Cadilhac, F. Benamara, and N. Aussenac-Gilles. On-
tolexical Resources for Feature based OpinionMining:
a Case-study. Ontolex, 2010.
P. Cimiano, A. Madche, S. Staab, and J. Volker. Ontology
Learning. Handbook on Ontologies, Springer, 2004.
P. Cimiano, A. Hotho, and S. Staab. Learning Concept
Hierarchies from Text Corpora using Formal Concept
Analysis. Artificial Intelligence, 2005.
P. Cimiano. Ontology Learning and Population from
Text: Algorithms, Evaluation and Applications.
Springer-Verlag New York, Inc. Secaucus, NJ, USA,
2006.
S. Dasgupta and V. Ng. Mine the Easy, Classify the Hard:
A Semi-supervised Approach to Automatic Sentiment
Classification. ACL, 2009.
O. Etzioni, M. Cafarella, D. Downey, A. Popescu,
T. Shaked, S. Soderland, D. Weld, and A. Yates. Un-
supervised Named-entity Extraction from theWeb: An
Experimental Study. Artificial Intelligence, 2005.
A. Esuli and F. Sebastiani. A Publicly Available Lexical
Resource for Opinion Mining. LREC, 2006.
M. Gamon, A. Aue, S. Corston-Oliver, and E. Ringger.
Pulse: Mining Customer Opinions from Free Text.
IDA, 2005.
R. Girju and A. Badulescu. Automatic Discovery of Part-
whole Relations Computational Linguistics, 2006.
A. Goldberg and X. Zhu. Seeing Stars When There
Aren?t Many Stars: Graph-based Semi-supervised
Learning for Sentiment Categorization. ACL, 2006.
M.A. Hearst. Automatic Acquisition of Hyponyms from
Large Text Corpora. Coling, 1992.
M. Hu and B. Liu. Mining and Summarizing Customer
Reviews. SIGKDD, 2004.
X. Hu, N. Sun, C. Zhang, and T.-S. Chua Exploiting
Internal and External Semantics for the Clustering of
Short Texts Using World Knowledge. CIKM, 2009.
S. Kim and E. Hovy. Determining the Sentiment of Opin-
ions. COLING, 2004.
A. C. Konig and E. Brill. Reducing the Human Overhead
in Text Categorization. KDD, 2006.
Z. Kozareva, E. Riloff, and E. Hovy. Semantic Class
Learning from the Web with Hyponym Pattern Link-
age Graphs. ACL, 2008.
T. Li, Y. Zhang, and V. Sindhwani. A Non-negative Ma-
trix Tri-factorization Approach to Sentiment Classifi-
cation with Lexical Prior Knowledge. ACL, 2009.
B. Liu, M. Hu, and J. Cheng. Opinion Observer: Ana-
lyzing and Comparing Opinions on the Web. WWW,
2005.
149
B. Liu. Handbook Chapter: Sentiment Analysis and Sub-
jectivity. Handbook of Natural Language Processing.
Marcel Dekker, Inc. New York, NY, USA, 2009.
L.M.Manevitz andM. Yousef. One-class SVMs for Doc-
ument Classification. Machine Learning, 2002.
Q.Mei, X. Ling, M.Wondra, H. Su, and C.X. Zhai. Topic
Sentiment Mixture: Modeling Facets and Opinions in
Weblogs. WWW, 2007.
X. Meng and H. Wang. Mining User Reviews: from
Specification to Summarization. ACL-IJCNLP, 2009.
K. Murthy, T.A. Faruquie, L.V. Subramaniam,
K.H. Prasad, and M. Mohania. Automatically
Generating Term-frequency-induced Taxonomies.
ACL, 2010.
H. Nishikawa, T. Hasegawa, Y. Matsuo, and G. Kikui.
Optimizing Informativeness and Readability for Senti-
ment Summarization. ACL, 2010.
B. Pang, L. Lee, and S. Vaithyanathan. Thumbs up? Sen-
timent Classification using Machine Learning Tech-
niques. EMNLP, 2002.
B. Pang and L. Lee. Seeing Stars: Exploiting Class Rela-
tionships for Sentiment Categorization with respect to
Rating Scales. ACL, 2005.
B. Pang and L. Lee. Opinion mining and sentiment anal-
ysis. Foundations and Trends in Information Retrieval,
2008.
HH. Pang, J. Shen, and R. Krishnan Privacy-Preserving,
Similarity-Based Text Retrieval. ACM Transactions
on Internet Technology, 2010.
A.M. Popescu and O. Etzioni. Extracting Product Fea-
tures and Opinions from Reviews. HLT/EMNLP,
2005.
H. Poon and P. Domingos. Unsupervised Ontology In-
duction from Text. ACL, 2010.
G. Qiu, B. Liu, J. Bu, and C. Chen. Expanding Domain
Sentiment Lexicon through Double Propagation. IJ-
CAI, 2009.
S. Roy and L.V. Subramaniam. Automatic Generation
of Domain Models for Call Centers from Noisy Tran-
scriptions. ACL, 2009.
B. Shi and K. Chang. Generating a Concept Hierarchy
for Sentiment Analysis. SMC, 2008.
R. Snow and D. Jurafsky. Semantic Taxonomy Induction
from Heterogenous Evidence. ACL, 2006.
Q. Su, X. Xu, H. Guo, X. Wu, X. Zhang, B. Swen, and
Z. Su. Hidden Sentiment Association in Chinese Web
Opinion Mining. WWW, 2008.
I. Titov and R. McDonald. A Joint Model of Text and
Aspect Ratings for Sentiment Summarization. ACL,
2008.
P. Turney. Thumbs up or thumbs down? Semantic Orien-
tation Applied to Unsupervised Classification of Re-
views. ACL, 2002.
Y. Wu, Q. Zhang, X. Huang, and L. Wu. Phrase Depen-
dency Parsing for Opinion Mining. ACL, 2009.
T. Wilson, J. Wiebe, and P. Hoffmann. Recognizing
Contextual Polarity in Phrase-level Sentiment Analy-
sis. HLT/EMNLP, 2005.
H. Yang and J. Callan A Metric-based Framework for
Automatic Taxonomy Induction. ACL, 2009.
S. Ye and T.-S. Chua. Learning Object Models from
Semi-structured Web Documents. IEEE Transactions
on Knowledge and Data Engineering, 2006.
J. Yi, T. Nasukawa, W. Niblack, and R. Bunescu. Senti-
ment Analyzer: Extracting Sentiments about a Given
Topic using Natural Language Processing Techniques.
ICDM, 2003.
L. Zhuang,F. Jing, and X.Y. Zhu Movie Review Mining
and Summarization CIKM, 2006.
150
