Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 410?419,
Singapore, 6-7 August 2009.
c
?2009 ACL and AFNLP
Perceptron Reranking for CCG Realization
Michael White and Rajakrishnan Rajkumar
Department of Linguistics
The Ohio State University
Columbus, OH, USA
{mwhite,raja}@ling.osu.edu
Abstract
This paper shows that discriminative
reranking with an averaged perceptron
model yields substantial improvements in
realization quality with CCG. The paper
confirms the utility of including language
model log probabilities as features in the
model, which prior work on discrimina-
tive training with log linear models for
HPSG realization had called into question.
The perceptron model allows the combina-
tion of multiple n-gram models to be opti-
mized and then augmented with both syn-
tactic features and discriminative n-gram
features. The full model yields a state-
of-the-art BLEU score of 0.8506 on Sec-
tion 23 of the CCGbank, to our knowledge
the best score reported to date using a re-
versible, corpus-engineered grammar.
1 Introduction
In this paper, we show how discriminative train-
ing with averaged perceptron models (Collins,
2002) can be used to substantially improve surface
realization with Combinatory Categorial Gram-
mar (Steedman, 2000, CCG). Velldal and Oepen
(2005) and Nakanishi et al (2005) have shown that
discriminative training with log-linear (maximum
entropy) models is effective in realization rank-
ing with Head-Driven Phrase Structure Grammar
(Pollard and Sag, 1994, HPSG). Here we show
that averaged perceptron models also perform well
for realization ranking with CCG. Averaged per-
ceptron models are very simple, just requiring a
decoder and a simple update function, yet despite
their simplicity they have been shown to achieve
state-of-the-art results in Treebank and CCG pars-
ing (Huang, 2008; Clark and Curran, 2007a) as
well as on other NLP tasks.
Along the way, we address the question of
whether it is beneficial to incorporate n-gram log
probabilities as baseline features in a discrimina-
tively trained realization ranking model. On a lim-
ited domain corpus, Velldal & Oepen found that
including the n-gram log probability of each can-
didate realization as a feature in their log-linear
model yielded a substantial boost in ranking per-
formance; on the Penn Treebank (PTB), however,
Nakanishi et al found that including an n-gram log
prob feature in their model was of no benefit (with
the use of bigrams instead of 4-grams suggested as
a possible explanation). With these mixed results,
the utility of n-gram baseline features for PTB-
scale discriminative realization ranking has been
unclear. In our particular setting, the question is:
Do n-gram log prob features improve performance
in broad coverage realization ranking with CCG,
where factored language models over words, part-
of-speech tags and supertags have previously been
employed (White et al, 2007; Espinosa et al,
2008)?
We answer this question in the affirmative, con-
firming the results of Velldal & Oepen, despite
the differences in corpus size and kind of lan-
guage model. We show that including n-gram log
prob features in the perceptron model is highly
beneficial, as the discriminative models we tested
without these features performed worse than the
generative baseline. These findings are in line
with Collins & Roark?s (2004) results with incre-
mental parsing with perceptrons, where it is sug-
gested that a generative baseline feature provides
the perceptron algorithm with a much better start-
ing point for learning. We also show that discrim-
inative training allows the combination of multi-
ple n-gram models to be optimized, and that the
best model augments the n-gram log prob fea-
tures with both syntactic features and discrimina-
tive n-gram features. The full model yields a state-
of-the-art BLEU (Papineni et al, 2002) score of
0.8506 on Section 23 of the CCGbank, which is
to our knowledge the best score reported to date
410
using a reversible, corpus-engineered grammar.
The paper is organized as follows. Section 2 re-
views previous work on broad coverage realization
with OpenCCG. Section 3 describes our approach
to realization reranking with averaged perceptron
models. Section 4 presents our evaluation of the
perceptron models, comparing the results of dif-
ferent feature sets. Section 5 compares our results
to those obtained by related systems and discusses
the difficulties of cross-system comparisons. Fi-
nally, Section 6 concludes with a summary and
discussion of future directions for research.
2 Background
2.1 Surface Realization with CCG
CCG (Steedman, 2000) is a unification-based cat-
egorial grammar formalism which is defined al-
most entirely in terms of lexical entries that encode
sub-categorization information as well as syntactic
feature information (e.g. number and agreement).
Complementing function application as the stan-
dard means of combining a head with its argu-
ment, type-raising and composition support trans-
parent analyses for a wide range of phenomena,
including right-node raising and long distance de-
pendencies. An example syntactic derivation ap-
pears in Figure 1, with a long-distance depen-
dency between point and make. Semantic com-
position happens in parallel with syntactic compo-
sition, which makes it attractive for generation.
OpenCCG is a parsing/generation library which
works by combining lexical categories for words
using CCG rules and multi-modal extensions on
rules (Baldridge, 2002) to produce derivations.
Surface realization is the process by which logical
forms are transduced to strings. OpenCCG uses
a hybrid symbolic-statistical chart realizer (White,
2006) which takes logical forms as input and pro-
duces sentences by using CCG combinators to
combine signs. Edges are grouped into equiva-
lence classes when they have the same syntactic
category and cover the same parts of the input log-
ical form. Alternative realizations are ranked us-
ing integrated n-gram or perceptron scoring, and
pruning takes place within equivalence classes of
edges. To more robustly support broad coverage
surface realization, OpenCCG greedily assembles
fragments in the event that the realizer fails to find
a complete realization.
To illustrate the input to OpenCCG, consider
the semantic dependency graph in Figure 2. In
aa1
heh3
he h2
<Det>
<Arg0> <Arg1>
<TENSE>pres
<NUM>sg
<Arg0>
w1 want.01
m1
<Arg1>
<GenRel>
<Arg1>
<TENSE>pres
p1point
h1have.03
make.03
<Arg0>
s[b]\np/np
np/n
np
n
s[dcl]\np/np
s[dcl]\np/(s[to]\np)
np
Figure 2: Semantic dependency graph from the
CCGbank for He has a point he wants to make
[. . . ], along with gold-standard supertags (cate-
gory labels)
the graph, each node has a lexical predication
(e.g. make.03) and a set of semantic features
(e.g. ?NUM?sg); nodes are connected via depen-
dency relations (e.g. ?ARG0?). (Gold-standard su-
pertags, or category labels, are also shown; see
Section 2.4 for their role in hypertagging.) In-
ternally, such graphs are represented using Hy-
brid Logic Dependency Semantics (HLDS), a
dependency-based approach to representing lin-
guistic meaning (Baldridge and Kruijff, 2002). In
HLDS, each semantic head (corresponding to a
node in the graph) is associated with a nominal
that identifies its discourse referent, and relations
between heads and their dependents are modeled
as modal relations.
2.2 Realization from an Enhanced CCGbank
Our starting point is an enhanced version of the
CCGbank (Hockenmaier and Steedman, 2007)?a
corpus of CCG derivations derived from the Penn
Treebank?with Propbank (Palmer et al, 2005)
roles projected onto it (Boxwell and White, 2008).
To engineer a grammar from this corpus suitable
for realization with OpenCCG, the derivations are
first revised to reflect the lexicalized treatment
of coordination and punctuation assumed by the
multi-modal version of CCG that is implemented
in OpenCCG (White and Rajkumar, 2008). Fur-
ther changes are necessary to support semantic de-
pendencies rather than surface syntactic ones; in
411
He has a point he wants to make
np s
dcl
\np/np np/n n np s
dcl
\np/(s
to
\np) s
to
\np/(s
b
\np) s
b
\np/np
> >T >B
np s/(s\np) s
to
\np/np
>B
s
dcl
\np/np
>B
s
dcl
/np
np\np
<
np
>
s
dcl
\np
<
s
dcl
Figure 1: Syntactic derivation from the CCGbank for He has a point he wants to make [. . . ]
particular, the features and unification constraints
in the categories related to semantically empty
function words such complementizers, infinitival-
to, expletive subjects, and case-marking preposi-
tions are adjusted to reflect their purely syntactic
status.
In the second step, a grammar is extracted from
the converted CCGbank and augmented with log-
ical forms. Categories and unary type chang-
ing rules (corresponding to zero morphemes) are
sorted by frequency and extracted if they meet the
specified frequency thresholds. A separate trans-
formation then uses a few dozen generalized tem-
plates to add logical forms to the categories, in a
fashion reminiscent of (Bos, 2005). As shown in
Figure 2, numbered semantic roles are taken from
PropBank when available, and more specific rela-
tions are introduced in the categories for closed-
class items such as determiners.
After logical form insertion, the extracted and
augmented grammar is loaded and used to parse
the sentences in the CCGbank according to the
gold-standard derivation. If the derivation can
be successfully followed, the parse yields a log-
ical form which is saved along with the corpus
sentence in order to later test the realizer. Cur-
rently, the algorithm succeeds in creating logical
forms for 98.85% of the sentences in the develop-
ment section (Sect. 00) of the converted CCGbank,
and 97.06% of the sentences in the test section
(Sect. 23). Of these, 95.99% of the development
LFs are semantic dependency graphs with a sin-
gle root, while 95.81% of the test LFs have a sin-
gle root. The remaining cases, with multiple roots,
are missing one or more dependencies required to
form a fully connected graph. Such missing de-
pendencies usually reflect remaining inadequacies
in the logical form templates.
An error analysis of OpenCCG output by Ra-
jkumar et al (2009) recently revealed that out of
2331 named entities (NEs) annotated by the BBN
corpus (Weischedel and Brunstein, 2005), 238
were not realized correctly. For example, multi-
word NPs like Texas Instruments Japan Ltd. were
realized as Japan Texas Instruments Ltd. Accord-
ingly, inspired by Hogan et al?s (2007)?s Experi-
ment 1, Rajkumar et al used the BBN corpus NE
annotation to collapse certain classes of NEs. But
unlike Hogan et al?s experiment where all the NEs
annotated by the BBN corpus were collapsed, Ra-
jkumar et al chose to collapse into single tokens
only NEs whose exact form can be reasonably ex-
pected to be specified in the input to the realizer.
For example, while some quantificational or com-
paratives phrases like more than $ 10,000 are an-
notated as MONEY in the BBN corpus, Rajkumar
et al only collapse $ 10,000 into an atomic unit,
with more than handled compositionally accord-
ing to the semantics assigned to it by the gram-
mar. Thus, after transferring the BBN annotations
to the CCGbank corpus, Rajkumar et al (partially)
collapsed NEs which are CCGbank constituents
according to the following rules: (1) completely
collapse the PERSON, ORGANIZATION, GPE,
WORK OF ART major class type entitites; (2) ig-
nore phrases like three decades later, which are
annotated as DATE entities; and (3) collapse all
phrases with POS tags CD or NNP(S) or lexical
items % or $, ensuring that all prototypical named
entities are collapsed.
It is worth noting that improvements in our
corpus-based grammar engineering process?
including a more precise treatment of punctuation,
better named entity handling and the addition of
catch-all logical form templates?have resulted in
a 13.5 BLEU point improvement in our baseline
realization scores on Section 00 of the CCGbank,
from a score of 0.6567 in (Espinosa et al, 2008)
to 0.7917 in (Rajkumar et al, 2009), contribut-
ing greatly to the state-of-the-art results reported
412
in Section 4. A further 4.5 point improvement is
obtained from the use of named entity classes in
language modeling and hypertagging (Rajkumar
et al, 2009), as described next, and from our per-
ceptron reranking model, described in Section 3.
2.3 Factored Language Models
As in (White et al, 2007; Rajkumar et al, 2009),
we use factored language models (Bilmes and
Kirchhoff, 2003) over words, part-of-speech tags
and supertags
1
to score partial and complete real-
izations. The trigram models were created using
the SRILM toolkit (Stolcke, 2002) on the standard
training sections (02?21) of the CCGbank, with
sentence-initial words (other than proper names)
uncapitalized. While these models are consider-
ably smaller than the ones used in (Langkilde-
Geary, 2002; Velldal and Oepen, 2005), the train-
ing data does have the advantage of being in the
same domain and genre. The models employ in-
terpolated Kneser-Ney smoothing with the default
frequency cutoffs. The best performing model
interpolates three component models using rank-
order centroid weights: (1) a word trigram model;
(2) a word model with semantic classes replac-
ing named entities; and (3) a trigram model that
chains a POS model with a supertag model, where
the POS model (P ) conditions on the previous two
POS tags, and the supertag model (S) conditions
on the previous two POS tags as well as the current
one, as shown below:
p
PS
(
~
F
i
|
~
F
i?1
i?2
) = p(P
i
| P
i?1
i?2
)p(S
i
| P
i
i?2
) (1)
Training data for the semantic class?replaced
model was created by replacing (collapsed) words
with their NE classes, in order to address data spar-
sity issues caused by rare words in the same se-
mantic class. For example, the Section 00 sen-
tence Pierre Vinken , 61 years old , will join the
board as a nonexecutive director Nov. 29 . be-
comes PERSON , DATE:AGE DATE:AGE old ,
will join the ORG DESC:OTHER as a nonexecu-
tive PER DESC DATE:DATE DATE:DATE . Dur-
ing realization, word forms are generated, but are
then replaced by their semantic classes for scoring
using the semantic class?replaced model, similar
to Oh and Rudnicky (2002).
Note that the use of supertags in the factored
language model to score possible realizations is
1
With CCG, supertags (Bangalore and Joshi, 1999) are
lexical categories considered as fine-grained syntactic labels.
distinct from the prediction of supertags for lexical
category assignment: the former takes the words
in the local context into account (as in supertag-
ging for parsing), while the latter takes features of
the logical form into account. This latter process
we call hypertagging, to which we now turn.
2.4 Hypertagging
A crucial component of the OpenCCG realizer is
the hypertagger (Espinosa et al, 2008), or su-
pertagger for surface realization, which uses a
maximum entropy model to assign the most likely
lexical categories to the predicates in the input log-
ical form, thereby greatly constraining the real-
izer?s search space.
2
Figure 2 shows gold-standard
supertags for the lexical predicates in the graph;
such category labels are predicted by the hyper-
tagger at run-time. As in recent work on using
supertagging in parsing, the hypertagger operates
in a multitagging paradigm (Curran et al, 2006),
where a variable number of predictions are made
per input predicate. Instead of basing category as-
signment on linear word and POS context, how-
ever, the hypertagger predicts lexical categories
based on contexts within a directed graph structure
representing the logical form (LF) of the sentence
to be realized. The hypertagger generalizes Ban-
galore and Rambow?s (2000) method of using su-
pertags in generation by using maximum entropy
models with a larger local context.
During realization, the hypertagger returns a ?-
best list of supertags in order of decreasing prob-
ability. Increasing the number of categories re-
turned clearly increases the likelihood that the
most-correct supertag is among them, but at a cor-
responding cost in chart size. Accordingly, the hy-
pertagger begins with a highly restrictive value for
?, and backs off to progressively less-restrictive
values if no complete realization can be found us-
ing the set of supertags returned. Clark and Curran
(2007b) have shown this iterative relaxation strat-
egy to be highly effective in CCG parsing.
3 Perceptron Reranking
As Collins (2002) observes, perceptron training
involves a simple, on-line algorithm, with few it-
erations typically required to achieve good perfor-
mance. Moreover, averaged perceptrons?which
2
The approach has been dubbed hypertagging since it op-
erates at a level ?above? the syntax, moving from semantic
representations to syntactic categories.
413
Input: training examples (x
i
, y
i
)
Initialization: set ? = 0, or use optional input
model
Algorithm:
for t = 1 . . . T , i = 1 . . . N
z
i
= argmax
y?GEN(x
i
)
?(x
i
, y) ? ?
if z
i
6= y
i
? = ? + ?(x
i
, y
i
) ? ?(x
i
, z
i
)
Output: ? =
?
T
t=1
?
N
i=1
?
ti
/TN
Figure 3: Averaged perceptron training algorithm
approximate voted perceptrons, a maximum-
margin method with attractive theoretical
properties?seem to work remarkably well in
practice, while adding little further complexity.
Additionally, since features only take on non-
zero values when they appear in training items
requiring updates, perceptrons integrate feature
selection with, and often produce quite small
models, especially when starting with a good
baseline.
The generic averaged perceptron training algo-
rithm appears in Figure 3. In our case, the algo-
rithm trains a model for reranking the n-best real-
izations generated using our existing factored lan-
guage model for scoring, with the oracle-best re-
alization considered the correct answer. Accord-
ingly, the input to the algorithm is a list of pairs
(x
i
, y
i
), where x
i
is a logical form, GEN(x
i
) are
the n-best realizations for x
i
, and y
i
is the oracle-
best member of GEN(x
i
). The oracle-best realiza-
tion is determined using a 4-gram precision metric
(approximating BLEU) against the reference sen-
tence.
We have followed Huang (2008) in using
oracle-best targets for training, rather than gold
standard ones, in order to better approximate test
conditions during training. However, following
Clark & Curran (2007a), during training we seed
the realizer with the gold-standard supertags, aug-
menting the hypertagger?s ?-best list, in order to
ensure that the n-best realizations are generally of
high quality; consequently, the gold standard real-
ization (i.e., the corpus sentence) usually appears
in the n-best list.
3
In addition, we use a hyper-
tagger trained on all the training data, to improve
hypertagger performance, while excluding the cur-
3
As in Clark & Curran?s approach, we use a single ? value
during training, rather than iteratively loosening the ? value;
the chosen ? value determines the size of the discrimation
space.
rent training section (in jack-knifed fashion) from
the word-based parts of the language model, in or-
der to make the language model scores more re-
alistic. It remains for future work to determine
whether using a different compromise between en-
suring high-quality training data and remaining
faithful to the test conditions would yield better
results.
Since realization of the n-best lists for train-
ing is the most time-consuming part of the pro-
cess, in our current implementation we perform
this step once, generating event files along the way
containing feature vectors for each candidate real-
ization. The event files are used to calculate the
frequency distribution for the features, and mini-
mum cutoffs are chosen to trim the feature alpha-
bet to a reasonable size. Training then takes place
by iterating over the event files, ignoring features
that do not appear in the alphabet. As Figure 3
indicates, training consists of calculating the top-
ranked realization according to the current model
?, and performing an update when the top-ranked
realization does not match the oracle-best realiza-
tion. Updates to the model add the feature vec-
tor ?(x
i
, y
i
) for the missed oracle-best realiza-
tion, and subtract the feature vector ?(x
i
, z
i
) for
the mistakenly top-ranked realization. The final
model averages the models across the T iterations
over the training data, andN test cases within each
iteration.
Note that while training the perceptron model
involves n-best reranking, realization with the re-
sulting model can be viewed as forest rescoring,
since scoring of all partial realizations is integrated
into the realizer?s beam search. In future work, we
intend to investigate saving the realizer?s packed
charts, rather than event files, and integrating the
unpacking of the charts with the perceptron train-
ing algorithm.
The features we employ in our perceptron mod-
els are of three kinds. First, as in the log-linear
models of Velldal & Oepen and Nakanishi et al,
we incorporate the log probability of the candidate
realization?s word sequence according to our fac-
tored language model as a single feature in the per-
ceptron model. Since our language model linearly
interpolates three component models, we also in-
clude the log prob from each component language
model as a feature, so that the combination of
these components can be optimized.
Second, we include syntactic features in our
414
Feature Type Example
LexCat + Word s/s/np + before
LexCat + POS s/s/np + IN
Rule s
dcl
? np s
dcl
\np
Rule + Word s
dcl
? np s
dcl
\np + bought
Rule + POS s
dcl
? np s
dcl
\np + VBD
Word-Word ?company, s
dcl
? np s
dcl
\np, bought?
Word-POS ?company, s
dcl
? np s
dcl
\np, VBD?
POS-Word ?NN, s
dcl
? np s
dcl
\np, bought?
Word + ?
w
?bought, s
dcl
? np s
dcl
\np? + d
w
POS + ?
w
?VBD, s
dcl
? np s
dcl
\np? + d
w
Word + ?
p
?bought, s
dcl
? np s
dcl
\np? + d
p
POS + ?
p
?VBD, s
dcl
? np s
dcl
\np? + d
p
Word + ?
v
?bought, s
dcl
? np s
dcl
\np? + d
v
POS + ?
v
?VBD, s
dcl
? np s
dcl
\np? + d
v
Table 1: Basic and dependency features from
Clark & Curran?s (2007b) normal form model;
distances are in intervening words, punctuation
marks and verbs, and are capped at 3, 3 and 2,
respectively
model by implementing Clark & Curran?s (2007b)
normal form model in OpenCCG.
4
The features of
this model are listed in Table 1; they are integer-
valued, representing counts of occurrences in a
derivation. These syntactic features are quite com-
parable to the dominance-oriented features in the
union of the Velldal & Oepen and Nakanishi et
al. models, except that our feature set does not
include grandparenting, which has been found to
have limited utility in CCG parsing. Our syntac-
tic features also include ones that measure the dis-
tance between headwords in terms of intervening
words, punctuation marks or verbs; these features
generalize the ones in Nakanishi et al?s model.
Note that in contrast to parsing, in realization dis-
tance features are non-local, since different partial
realizations in the same equivalence class typically
differ in word order; as we are working in a rerank-
ing paradigm though, the non-local nature of these
features is unproblematic.
Third, we include discriminative n-gram fea-
tures in our model, following Roark et al?s (2004)
approach to discriminative n-gram modeling for
speech recognition. By discriminative n-gram fea-
tures, we mean features counting the occurrences
of each n-gram that is scored by our factored lan-
guage model, rather than a feature whose value is
the log prob determined by the language model.
As Roark et al note, discriminative training with
n-gram features has the potential to learn to nega-
4
We have omitted Clark & Curran?s root features, since
the category we use for the full stop ensures that it must ap-
pear at the root of any complete derivation.
Model #Alph-feats #Feats Acc Time
full-model 2402173 576176 96.40% 08:53
lp-ngram 1127437 342025 94.52% 05:19
lp-syn 1274740 291728 85.03% 05:57
Table 2: Perceptron Training Details?number of
features in the alphabet, number of features in the
model, training accuracy and training time (hours)
for 10 iterations on a single commodity server
tively weight n-grams that appear in some of the
GEN(x
i
) candidates, but which never appear in
the naturally occurring corpus used to train a stan-
dard, generative language model. Since our fac-
tored language model considers words, semantic
classes, part-of-speech tags and supertags, our n-
gram features represent a considerable generaliza-
tion of the sequence-oriented features in Velldal
& Oepen?s model, which never contain more than
one word and do not include semantic classes.
4 Evaluation
4.1 Experimental Conditions
For the experiments reported below, we used a
lexico-grammar extracted from Sections 02?21 of
our enhanced CCGbank, a hypertagging model in-
corporating named entity class features, and a tri-
gram factored language model over words, named
entity classes, part-of-speech tags and supertags,
as described in the preceding section. BLEU
scores were calculated after removing the under-
scores between collapsed NEs.
Events were generated for each training section
separately. As already noted, the hypertagger and
POS/supertag language model was trained on all
the training sections, while separate word-based
models were trained excluding each of the train-
ing sections in turn. Event files for 26530 training
sentences with complete realizations were gener-
ated in 7 hours and 16 minutes on a cluster us-
ing one commodity server per section, with an av-
erage n-best list size of 18.2. Perceptron models
were trained on single machines; details for three
of the models appear in Table 2. The complete set
of models is listed in Table 3.
4.2 Results
Realization results on the development section are
given in Table 4. As the first block of rows af-
ter the baseline shows, of the models incorporating
a single kind of feature, only the one with the n-
gram log prob features beats the baseline BLEU
415
Model Description
baseline-w3 No perceptron (3g wd only)
baseline No perceptron
syn-only-nodist All syntactic features except distance
ngram-only Just ngram features
syn-only Just syntactic features
lp-only Just log prob features
lp-ngram Log prob + Ngram features
lp-syn Log prob + Syntactic features
full-model Log prob + Ngram +Syntactic features
Table 3: Legend for Experimental Conditions
score, with the other models falling well below
the baseline (though faring better than the trigram-
word LM baseline). This result confirms the im-
portance of including n-gram log prob features in
discriminative realization ranking models, in line
with Velldal & Oepen?s findings, and contra those
of Nakanishi et al, even though it was Nakanishi
et al who experimented with the Penn Treebank
corpus, while Velldal &Oepen?s experiments were
on a much smaller, limited domain corpus. The
second block of rows shows that both the discrim-
inative n-gram features and the syntactic features
provide a substantial boost when used with the n-
gram log prob features, with the syntactic features
yielding a more than 3 BLEU point gain. The
final row shows that the full model works best,
though the boosts provided by the syntactic and
discriminative n-gram features are clearly not in-
dependent. The BLEU point trends are mirrored in
the percentage of exact match realizations, which
goes up by more than 10% from the baseline. The
percentage of complete (i.e., non-fragmentary) re-
alizations, however, goes down; we expect that
this is due to the time taken up by our current
naive method of feature extraction, which does not
cache the features calculated for partial realiza-
tions. Realization results on the standard test sec-
tion appear in Table 5, confirming the gains made
by the full model over the baseline.
5
We calculated statistical significance for the
main results on the development section using
bootstrap random sampling.
6
After re-sampling
1000 times, significance was calculated using a
paired t-test (999 d.f.). The results indicated that
lp-only exceeded the baseline, lp-ngram and lp-
5
Note that the baseline for Section 23 uses 4-grams and a
filter for balanced punctuation (White and Rajkumar, 2008),
unlike the other reported configurations, which would explain
the somewhat smaller increase seen with this section.
6
Scripts for running these tests are available at
http://projectile.sv.cmu.edu/research/
public/tools/bootStrap/tutorial.htm
Model %Exact %Compl. BLEU Time
baseline-w3 26.00 83.15 0.7646 1.8
baseline 29.00 83.28 0.7963 2.0
syn-only-nodist 26.02 82.69 0.7754 3.2
ngram-only 27.67 82.95 0.7777 3.0
syn-only 28.34 82.74 0.7838 3.4
lp-only 32.01 83.02 0.8009 2.1
lp-ngram 36.31 80.47 0.8183 3.1
lp-syn 39.47 79.74 0.8323 3.5
full-model 40.11 79.63 0.8373 3.6
Table 4: Section 00 Results (98.9% coverage)?
percentage of exact match and grammatically
complete realizations, BLEU scores and average
times, in seconds
Model %Exact %Complete BLEU
baseline 33.74 85.04 0.8173
full-model 40.45 83.88 0.8506
Table 5: Section 23 Results (97.06% coverage)
syn exceeded lp-only, and the full model exceeded
lp-syn, with p < 0.0001 in each case.
4.3 Examples
Table 6 presents four examples where the full
model improves upon the baseline. Example sen-
tence wsj 0020.10 in Table 6 is a case where the
perceptron successfully weights the component
ngram models, as the lp-ngram model and those
that build on it get it right. Note that here, the mod-
ifier ordering in small video-viewing is not speci-
fied in the logical form and either ordering is pos-
sible syntactically. In wsj 0024.2, number agree-
ment between the conjoined subject noun phrase
and verb is obtained only with the full model. This
suggests that the full model is more robust to cases
where the grammar is insufficiently precise (num-
ber agreement is enforced by the grammar in only
the simplest cases). Example wsj 0034.9 corrects
a VP ordering mismatch, where the corpus sen-
tence is clearly preferred to the one where into
oblivion is shifted to the end. Finally, wsj 0047.13
corrects an animacy mismatch on the wh-pronoun,
in large part due to the high negative weight as-
signed to the discriminative n-gram feature PER-
SON , which. Note that the full model still dif-
fers from the original sentence in its placement of
the adverb reportedly, choosing the arguably more
natural position following the auxiliary.
4.4 Comparison to Other Systems
Table 7 lists our results in the context of those re-
ported for other systems on PTB Section 23. The
416
Ref-wsj 0020.10 that measure could compel Taipei ?s growing number of small video-viewing parlors to pay ...
baseline,syn-only,ngram-only that measure could compel Taipei ?s growing number of video-viewing small parlors to ...
lp-only, lp-ngram, full-model that measure could compel Taipei ?s growing number of small video-viewing parlors to ...
Ref-wsj 0024.2 Esso Australia Ltd. , a unit of new york-based Exxon Corp. , and Broken Hill Pty. operate the fields ...
all except full-model Esso Australia Ltd. , a unit of new york-based Exxon Corp. , and Broken Hill Pty. operates the fields ...
full-model Esso Australia Ltd. , a unit of new york-based Exxon Corp. , and Broken Hill Pty. operate the fields ...
Ref-wsj 0034.9 they fell into oblivion after the 1929 crash .
baseline, lp-ngram they fell after the 1929 crash into oblivion .
lp-only, ngram-only, syn-only, full-model they fell into oblivion after the 1929 crash .
Ref-wsj 0047.13 Antonio Novello , whom Mr. Bush nominated to serve as surgeon general , reportedly has assured . . .
baseline,baseline-w3, lp-syn, lp-only Antonio Novello , which Mr. Bush nominated to serve as surgeon general , has reportedly assured . . .
full-model, lp-ngram, syn-only, ngram-syn Antonio Novello , whom Mr. Bush nominated to serve as surgeon general , has reportedly assured . . .
Table 6: Examples of realized output
System Coverage BLEU %Exact
Callaway (05) 98.5% 0.9321 57.5
OpenCCG (09) 97.1% 0.8506 40.5
Ringger et al (04) 100.0% 0.836 35.7
Langkilde-Geary (02) 83% 0.757 28.2
Guo et al (08) 100.0% 0.7440 19.8
Hogan et al (07) ?100.0% 0.6882
OpenCCG (08) 96.0% 0.6701 16.0
Nakanishi et al (05) 90.8% 0.7733
Table 7: PTB Section 23 BLEU scores and exact
match percentages in the NLG literature (Nakan-
ishi et al?s results are for sentences of length 20 or
less)
most similar systems to ours are those of Nakan-
ishi et al (2005) and Hogan et al (2007), as they
both involve chart realizers for reversible gram-
mars engineered from the Penn Treebank. While
direct comparisons across systems cannot really
be made when inputs vary in their semantic depth
and specificity, we observe that our all-sentences
BLEU score of 0.8506 exceeds that of Hogan et
al., who report a top score of 0.6882 (though with
coverage near 100%), and also surpasses Nakan-
ishi et al?s score of 0.7733, despite their results be-
ing limited to sentences of length 20 or less (with
91% coverage). Velldal & Oepen?s (2005) system
is also closely related, as noted in the introduc-
tion, but as their experiments are on a limited do-
main corpus, their results cannot be compared at
all meaningfully.
5 Related Work and Discussion
As alluded to above, realization systems cannot be
easily compared, even on the same corpus, when
their inputs are not the same. This point is dra-
matically illustrated in Langkilde-Geary?s (2002)
system, where a BLEU score of 0.514 is reported
for minimally specified inputs on PTB Section 23,
while a score of 0.757 is reported for the ?Per-
mute, no dir? case (which perhaps most closely
resembles our inputs), and a score of 0.924 is re-
ported for the most fully specified inputs; note,
however, that in the latter case word order is deter-
mined by sibling order in the inputs, an assump-
tion not commonly made. As another example,
Guo et al (2008) report a competitive result of
0.7440 (with 100% coverage) using a dependency-
based approach; however, their inputs, like those
of Hogan et al, include more surface syntactic in-
formation than ours, as they specify case-marking
prepositions, wh-pronouns and complementizers.
In a recent experiment to assess the impact of
input specificity, we found that including pred-
icates for all prepositions in our logical forms
boosted our baseline results by more than 3 BLEU
points, with complete realizations found in more
than 90% of the test cases, indicating that generat-
ing from a more surfacy input is indeed an easier
task than generating from a deeper representation.
Given the current lack of consensus on realizer in-
put specificity, we believe it is important to keep
in mind that within-system comparisons (such as
those in the preceding section) are the ones that
should be given the most credence.
Returning to our cross-system comparison, it is
perhaps surprising that Callaway (2005) reports
the best PTB BLEU score to date, 0.9321, with
98.5% coverage, using a purely symbolic, hand-
crafted grammar augmented to handle the most
frequent coverage issues for the PTB. While Call-
away?s inputs are unordered, word order is often
determined by positional features (e.g. front) or
by the type of modification (e.g. describer vs.
qualifier), and parts-of-speech are included
for lexical items. Additionally, in contrast to our
approach, Callaway makes use of a generation-
only grammar, rather than a reversible one, and his
approach is less well-suited to producing n-best
417
outputs. Nevertheless, his high scores do suggest
the potential for precise grammar engineering to
improve realization quality.
While we have yet to perform a thorough er-
ror analysis, our impression is that although the
current set of syntactic features substantially im-
proves clausal constituent ordering, a variety of
disfluent cases remain. More thorough inves-
tigations of features for constituent ordering in
English have been performed by Ringger et al
(2004), Filippova and Strube (2009) and Zhong
and Stent (2009), all of whom develop classifiers
for determining linear order. In future work, we
plan to investigate whether features inspired by
these approaches can be usefully integrated into
our perceptron reranker.
Also related to the present work is discrimina-
tive training in syntax-based MT (Turian et al,
2007; Watanabe et al, 2007; Blunsom et al, 2008;
Chiang et al, 2009). Not surprisingly, since MT is
a harder problem than surface realization, syntax-
based MT systems have made use of less precise
grammars and more impoverished (target-side)
feature sets than those tackling realization rank-
ing. With progress on discriminative training with
large numbers of features in syntax-based MT, the
features found to be useful for high-quality sur-
face realization may become increasingly relevant
for MT as well.
6 Conclusions
In this paper, we have shown how discriminative
reranking with an averaged perceptron model can
be used to achieve substantial improvements in re-
alization quality with CCG. Using a comprehen-
sive feature set, we have also confirmed the util-
ity of including language model log probabilities
as features in the model, which prior work on
discriminative training with log linear models for
HPSG realization had called into question. The
perceptron model allows the combination of mul-
tiple n-gram models to be optimized and then aug-
mented with both syntactic features and discrim-
inative n-gram features, inspired by related work
in discriminative parsing and language modeling
for speech recognition. The full model yields a
state-of-the-art BLEU score of 0.8506 on Section
23 of the CCGbank, to our knowledge the best
score reported to date using a reversible, corpus-
engineered grammar, despite our use of deeper,
less specific inputs. Finally, the perceptron model
paves the way for exploring the utility of richer
feature spaces in statistical realization, including
the use of linguistically-motivated and non-local
features, a topic which we plan to investigate in
future work.
Acknowledgements
This work was supported in part by NSF grant IIS-
0812297 and by an allocation of computing time
from the Ohio Supercomputer Center. Our thanks
also to the OSU Clippers group and the anony-
mous reviewers for helpful comments and discus-
sion.
References
Jason Baldridge and Geert-Jan Kruijff. 2002. Cou-
pling CCG and Hybrid Logic Dependency Seman-
tics. In Proc. ACL-02.
Jason Baldridge. 2002. Lexically Specified Deriva-
tional Control in Combinatory Categorial Gram-
mar. Ph.D. thesis, University of Edinburgh.
Srinivas Bangalore and Aravind K. Joshi. 1999. Su-
pertagging: An Approach to Almost Parsing. Com-
putational Linguistics, 25(2):237?265.
Srinivas Bangalore and Owen Rambow. 2000. Ex-
ploiting a probabilistic hierarchical model for gener-
ation. In Proc. COLING-00.
Jeff Bilmes and Katrin Kirchhoff. 2003. Factored lan-
guage models and general parallelized backoff. In
Proc. HLT-03.
Phil Blunsom, Trevor Cohn, and Miles Osborne. 2008.
A discriminative latent variable model for statistical
machine translation. In Proc. ACL-08: HLT.
Johan Bos. 2005. Towards wide-coverage semantic
interpretation. In Proc. IWCS-6.
Stephen Boxwell andMichaelWhite. 2008. Projecting
Propbank roles onto the CCGbank. In Proc. LREC-
08.
Charles Callaway. 2005. The types and distributions of
errors in a wide coverage surface realizer evaluation.
In Proceedings of the 10th European Workshop on
Natural Language Generation.
David Chiang, Kevin Knight, and Wei Wang. 2009.
11,001 new features for statistical machine transla-
tion. In Proc. NAACL HLT 2009.
Stephen Clark and James Curran. 2007a. Perceptron
training for a wide-coverage lexicalized-grammar
parser. In ACL 2007 Workshop on Deep Linguistic
Processing.
418
Stephen Clark and James R. Curran. 2007b. Wide-
Coverage Efficient Statistical Parsing with CCG and
Log-Linear Models. Computational Linguistics,
33(4):493?552.
Michael Collins and Brian Roark. 2004. Incremen-
tal parsing with the perceptron algorithm. In Proc.
ACL-04.
Michael Collins. 2002. Discriminative training meth-
ods for hidden Markov models: theory and ex-
periments with perceptron algorithms. In Proc.
EMNLP-02.
James R. Curran, Stephen Clark, and David Vadas.
2006. Multi-tagging for lexicalized-grammar pars-
ing. In Proc. COLING/ACL-06.
Dominic Espinosa, Michael White, and Dennis Mehay.
2008. Hypertagging: Supertagging for surface real-
ization with CCG. In Proc. ACL-08: HLT.
Katja Filippova and Michael Strube. 2009. Tree lin-
earization in English: Improving language model
based approaches. In Proc. NAACL HLT 2009 Short
Papers.
Yuqing Guo, Josef van Genabith, and Haifeng Wang.
2008. Dependency-based n-gram models for
general purpose sentence realisation. In Proc.
COLING-08.
Julia Hockenmaier and Mark Steedman. 2007. CCG-
bank: A Corpus of CCG Derivations and Depen-
dency Structures Extracted from the Penn Treebank.
Computational Linguistics, 33(3):355?396.
Deirdre Hogan, Conor Cafferkey, Aoife Cahill, and
Josef van Genabith. 2007. Exploiting multi-word
units in history-based probabilistic generation. In
Proc. EMNLP-CoNLL.
Liang Huang. 2008. Forest reranking: Discriminative
parsing with non-local features. In Proc. ACL-08:
HLT.
Irene Langkilde-Geary. 2002. An empirical verifi-
cation of coverage and correctness for a general-
purpose sentence generator. In Proc. INLG-02.
Hiroko Nakanishi, Yusuke Miyao, and Jun?ichi Tsujii.
2005. Probabilistic methods for disambiguation of
an HPSG-based chart generator. In Proc. IWPT-05.
Alice H. Oh and Alexander I. Rudnicky. 2002.
Stochastic natural language generation for spoken
dialog systems. Computer, Speech & Language,
16(3/4):387?407.
Martha Palmer, Dan Gildea, and Paul Kingsbury. 2005.
The proposition bank: A corpus annotated with se-
mantic roles. Computational Linguistics, 31(1).
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proceedings
of the 40th Annual Meeting of the Association for
Computational Linguistics (ACL), Philadelphia, PA.
Carl Pollard and Ivan Sag. 1994. Head-Driven Phrase
Structure Grammar. University Of Chicago Press.
Rajakrishnan Rajkumar, Michael White, and Dominic
Espinosa. 2009. Exploiting named entity classes in
CCG surface realization. In Proc. NAACL HLT 2009
Short Papers.
Eric Ringger, Michael Gamon, Robert C. Moore,
David Rojas, Martine Smets, and Simon Corston-
Oliver. 2004. Linguistically informed statistical
models of constituent structure for ordering in sen-
tence realization. In Proc. COLING-04.
Brian Roark, Murat Saraclar, Michael Collins, and
Mark Johnson. 2004. Discriminative language
modeling with conditional random fields and the
perceptron algorithm. In Proc. ACL-04.
Mark Steedman. 2000. The syntactic process. MIT
Press, Cambridge, MA, USA.
Andreas Stolcke. 2002. SRILM ? An extensible lan-
guage modeling toolkit. In Proc. ICSLP-02.
Joseph Turian, Benjamin Wellington, and I. Dan
Melamed. 2007. Scalable discriminative learn-
ing for natural language parsing and translation. In
Proc. NIPS 19.
Erik Velldal and Stephan Oepen. 2005. Maximum en-
tropy models for realization ranking. In Proc. MT
Summit X.
Taro Watanabe, Jun Suzuki, Hajime Tsukada, and
Hideki Isozaki. 2007. Online large-margin training
for statistical machine translation. In Proc. EMNLP-
CoNLL-07.
RalphWeischedel and Ada Brunstein. 2005. BBN pro-
noun coreference and entity type corpus. Technical
report, BBN.
Michael White and Rajakrishnan Rajkumar. 2008.
A more precise analysis of punctuation for broad-
coverage surface realization with CCG. In Proc.
of the Workshop on Grammar Engineering Across
Frameworks (GEAF08).
Michael White, Rajakrishnan Rajkumar, and Scott
Martin. 2007. Towards broad coverage surface re-
alization with CCG. In Proc. of the Workshop on
Using Corpora for NLG: Language Generation and
Machine Translation (UCNLG+MT).
Michael White. 2006. Efficient Realization of Coor-
dinate Structures in Combinatory Categorial Gram-
mar. Research on Language and Computation,
4(1):39?75.
Huayan Zhong and Amanda Stent. 2009. Determining
the position of adverbial phrases in English. In Proc.
NAACL HLT 2009 Short Papers.
419
Proceedings of NAACL HLT 2009: Short Papers, pages 161?164,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Exploiting Named Entity Classes in CCG Surface Realization
Rajakrishnan Rajkumar Michael White
Department of Linguistics
The Ohio State University
Columbus, OH, USA
{raja,mwhite,espinosa}@ling.osu.edu
Dominic Espinosa
Abstract
This paper describes how named entity (NE)
classes can be used to improve broad cover-
age surface realization with the OpenCCG re-
alizer. Our experiments indicate that collaps-
ing certain multi-word NEs and interpolating
a language model where NEs are replaced by
their class labels yields the largest quality in-
crease, with 4-grams adding a small additional
boost. Substantial further benefit is obtained
by including class information in the hyper-
tagging (supertagging for realization) compo-
nent of the system, yielding a state-of-the-
art BLEU score of 0.8173 on Section 23 of
the CCGbank. A targeted manual evaluation
confirms that the BLEU score increase corre-
sponds to a significant rise in fluency.
1 Introduction
Hogan et al (2007) have recently shown that better
handling of named entities (NEs) in broad coverage
surface realization with LFG can lead to substan-
tial improvements in BLEU scores. In this paper,
we confirm that better NE handling can likewise im-
prove broad coverage surface realization with CCG,
even when employing a more restrictive notion of
named entities that better matches traditional real-
ization practice. Going beyond Hogan et al (2007),
we additionally show that NE classes can be used
to improve realization quality through better lan-
guage models and better hypertagging (supertagging
for realization) models, yielding a state-of-the-art
BLEU score of 0.8173 on Section 23 of the CCG-
bank.
A question addressed neither by Hogan et al
nor anyone else working on broad coverage surface
realization recently is whether reported increases
in BLEU scores actually correspond to observable
improvements in quality. We view this situation
as problematic, not only because Callison-Burch
et al (2006) have shown that BLEU does not al-
ways rank competing systems in accord with hu-
man judgments, but also because surface realiza-
tion scores are typically much higher than those in
MT?where BLEU?s performance has been repeat-
edly assessed?even when using just one reference.
Thus, in this paper, we present a targeted manual
evaluation confirming that our BLEU score increase
corresponds to a significant rise in fluency, a practice
we encourage others to adopt.
2 CCG Surface Realization
CCG (Steedman, 2000) is a unification-based cat-
egorial grammar formalism defined almost en-
tirely in terms of lexical entries that encode sub-
categorization as well as syntactic features (e.g.
number and agreement). OpenCCG is a pars-
ing/generation library which includes a hybrid
symbolic-statistical chart realizer (White, 2006). A
vital component of the realizer is the hypertagger
(Espinosa et al, 2008), which predicts lexical cat-
egory assignments using a maxent model trained on
contexts within a directed graph structure represent-
ing the logical form (LF) input; features and rela-
tions in the graph as well as parent child relation-
ships are the main features used to train the model.
The realizer takes as input an LF description (see
Figure 1 of Espinosa et al, 2008), but here we also
161
use LFs with class information on some elementary
predications (e.g. @x:MONEY($ 10,000)). Chart re-
alization proceeds in iterative beta-best fashion, with
a progressively wider hypertagger beam width. If no
complete realization is found within the time limit,
fragments are greedily assembled. Alternative real-
izations are ranked using integrated n-gram scoring;
n-gram models help in choosing word order and, to
a lesser extent, making lexical choices.
3 Collapsing Named Entities
An error analysis of the OpenCCG baseline output
reveals that out of 2331 NEs annotated by the BBN
corpus, 238 are not realized correctly. For exam-
ple, multi-word NPs like Texas Instruments Japan
Ltd. are realized as Japan Texas Instruments Ltd..
Inspired by Hogan et al?s (2007)?s Experiment 1,
we decided to use the BBN corpus NE annotation
(Weischedel and Brunstein, 2005) to collapse cer-
tain classes of NEs. But unlike their experiment
where all the NEs annotated by the BBN corpus are
collapsed, we chose to collapse into single tokens
only NEs whose exact form can be reasonably ex-
pected to be specified in the input to the realizer.
For example, while some quantificational or com-
paratives phrases like more than $ 10,000 are anno-
tated as MONEY in the BBN corpus, in our view
only $ 10,000 should be collapsed into an atomic
unit, with more than handled compositionally ac-
cording to the semantics assigned to it by the gram-
mar. Thus, after transferring the BBN annotations to
the CCGbank corpus, we (partially) collapsed NEs
which are CCGbank constituents according to the
following rules: (1) completely collapse the PER-
SON, ORGANIZATION, GPE, WORK OF ART
major class type entitites; (2) ignore phrases like
three decades later, which are annotated as DATE
entities; and (3) collapse all phrases with POS tags
CD or NNP(S) or lexical items % or $, ensuring that
all prototypical named entities are collapsed.
4 Exploiting NE Classes
Going beyond Hogan et al (2007) and collaps-
ing experiments, we also experiment with NE
classes in language models and hypertagging mod-
els. BBN annotates both major types and subtypes
(DATE:AGE, DATE:DATE etc). For all our experi-
ments, we use both of these.
4.1 Class replaced n-gram models
For both the original CCGbank as well as the col-
lapsed corpus, we created language model training
data with semantic classes replacing actual words,
in order to address data sparsity issues caused by
rare words in the same semantic class. For exam-
ple, in the collapsed corpus, the Section 00 sen-
tence Pierre Vinken , 61 years old , will join the
board as a nonexecutive director Nov. 29 . be-
comes PERSON , DATE:AGE DATE:AGE old ,
will join the ORG DESC:OTHER as a nonexecutive
PER DESC DATE:DATE DATE:DATE . During re-
alization, word forms are generated, but are then re-
placed by their semantic classes and scored using
the semantic class replaced n-gram model, similar
to (Oh and Rudnicky, 2002). As the specific words
may still matter, the class replaced model is interpo-
lated at the word level with an ordinary, word-based
language model, as well as with a factored language
model over POS tags and supertags.
4.2 Class features in hypertagging
We also experimented with a hypertagging model
trained over the collapsed corpus, where the seman-
tic classes of the elementary lexical predications,
along with the class features of their adjacent nodes,
are added as features.
5 Evaluation
5.1 Hypertagger evaluation
As Table 2 indicates, the hypertagging model does
worse in terms of per-logical predication accuracy
& per-whole-graph accuracy on the collapsed cor-
pus. To some extent this is not surprising, as collaps-
ing eliminates many easy tagging cases; however, a
full explanation is still under investigation. Note that
class information does improve performance some-
what on the collapsed corpus.
5.2 Realizer evaluation
For a both the original CCGbank and the col-
lapsed corpus, we extracted a section 02?21 lexico-
grammars and used it to derive LFs for the devel-
opment and test sections. We used the language
models in Table 1 to score realizations and for the
162
Condition Expansion
LM baseline-LM: word 3g+ pos 3g*stag 3g
HT baseline Hypertagger
LM4 LM with 4g word
LMC LM with class-rep model interpolated
LM4C LM with both
HTC HT with classes on nodes as extra feats
Table 1: Legend for Experimental Conditions
Corpus Condition Tags/pred Pred Graph
Uncollapsed HT 1.0 93.56% 39.14%
HT 1.5 98.28% 78.06%
Partly HT 1.0 92.22% 35.04%
Collapsed HTC 1.0 92.89% 38.31%
HT 1.5 97.87% 73.14%
HTC 1.5 98.02% 75.30%
Table 2: Hypertagger testing on Section 00 of the uncol-
lapsed corpus (1896 LFs & 38104 predicates) & partially
collapsed corpus (1895 LFs & 35370 predicates)
collapsed corpus, we also tried a class-based hyper-
tagging model. Hypertagger ?-values were set for
each corpus and for each hypertagging model such
that the predicted tags per pred was the same at each
level. BLEU scores were calculated after removing
the underscores between collapsed NEs.
5.3 Results
Our baseline results are much better than those pre-
viously reported with OpenCCG in large part due to
improved grammar engineering efforts and bug fix-
ing. Table 3 shows development set results which
indicate that collapsing appears to improve realiza-
tion on the whole, as evidenced by the small increase
in BLEU scores. The class-replaced word model
provides a big boost on the collapsed corpus, from
0.7917 to 0.7993, much more than 4-grams. Adding
semantic classes to the hypertagger improves its ac-
curacy and gives us another half BLEU point in-
crease. Standard test set results, reported in Table 4,
confirm the overall increase, from 0.7940 to 0.8173.
In analyzing the Section 00 results, we found that
with the collapsed corpus, NE errors were reduced
from 238 to 99, which explains why the BLEU
score increases despite the drop in exact matches and
grammatically complete realizations from the base-
line. A semi-automatic analysis reveals that most
of the corrections involve proper names that are no
longer mangled. Correct adjective ordering is also
achieved in some cases; for example, Dutch publish-
Corpus Condition %Exact %Complete BLEU
Uncollapsed LM+HT 29.27 84.02 0.7900
(98.6% LM4+HT 29.14 83.61 0.7899
coverage) LMC+HT 30.64 83.70 0.7937
LM4C+HT 30.85 83.65 0.7946
Partly collapsed LM+HT 28.28 82.48 0.7917
(98.6% LM4+HT 28.68 82.54 0.7929
coverage) LMC+HT 30.74 82.33 0.7993
LM4C+HT 31.06 82.33 0.7995
LM4C+HTC 32.01 83.17 0.8042
Table 3: Section 00 blind testing results
Condition %Exact %Complete BLEU
LM+HT 29.38 82.53 0.7940
LM4C+HTC 33.74 85.04 0.8173
Table 4: Section 23 results: LM+HT baseline on origi-
nal corpus (97.8% coverage), LM4C+HTC best case on
collapsed corpus (94.8% coverage)
ing group is enforced by the class-replaced models,
while all the other models realize this as publishing
Dutch group. Additionally, the class-replaced model
sometimes helps with animacy marking on relative
pronouns, as in Mr. Otero , who . . . instead of Mr.
Otero , which . . . . (Note that our input LFs do not
directly specify the choice of function words such
as case-marking prepositions, relative pronouns and
complementizers, and thus class-based scoring can
help to select the correct surface word form.)
5.4 Targeted manual evaluation
While the language models employing NE classes
certainly improve some examples, others are made
worse, and some are just changed to different, but
equally acceptable paraphrases. For this reason, we
carried out a targeted manual evaluation to confirm
the BLEU results.
5.4.1 Procedure
Along the lines of (Callison-Burch et al, 2006),
two native speakers (two of the authors) provided
ratings for a random sample of 49 realizations that
differed between the baseline and best conditions on
the collapsed corpus. Note that the selection pro-
cedure excludes exact matches and thus focuses on
sentences whose realization quality may be lower
on average than in an arbitrary sample. Sentences
were rated in the context of the preceding sentence
(if any) for both fluency and adequacy in compari-
son to the original sentence. The judges were not
163
LEU scoreB
 2
 2.5
 3
 3.5
 4
 4.5
 5
 0.66  0.68  0.7  0.72  0.74  0.76  0.78  0.8
AdequacyFluency
Baseline
BestH
um
an 
Sco
re
Figure 1: BLEU scores plotted against human judge-
ments of fluency and adequacy
aware of the condition (best/baseline) while doing
the rating. Ratings of the two judges were averaged
for each item.
5.4.2 Results
In the human evaluation, the best system?s mean
scores were 4.4 for adequacy and 3.61 for fluency,
compared with the baseline?s scores of 4.35 and 3.36
respectively. Figure 1 shows these results including
the standard error for each measurement, with the
BLEU scores for this specific test set. The sample
size was sufficient to show that the increase in flu-
ency from 3.36 to 3.61 represented a significant dif-
ference (paired t-test, 1-tailed, p = 0.015), while the
adequacy scores did not differ significantly.
5.4.3 Brief comparison to related systems
While direct comparisons cannot really be made
when inputs vary in their semantic depth and speci-
ficity, we observe that our all-sentences BLEU score
of 0.8173 exceeds that of Hogan et al (2007), who
report a top score of 0.6882 (though with coverage
near 100%). Nakanishi et al (2005) and Langkilde-
Geary (2002) report scores of 0.7733 and 0.7570, re-
spectively, though the former is limited to sentences
of length 20 or less, and the latter?s coverage is much
lower.
6 Conclusion and Future Work
In this paper, we have shown how named entity
classes can be used to improve the OpenCCG re-
alizer?s language models and hypertagging models,
helping to achieve a state-of-the-art BLEU score of
0.8173 on CCGbank Section 23. We have also con-
firmed the increase in quality through a targeted
manual evaluation, a practice we encourage others
working on surface realization to adopt. In future
work, we plan to investigate the unexpected drop in
hypertagger performance on our NE-collapsed cor-
pus, which we conjecture may be resolved by taking
advantage of Vadas and Curran?s (2008) corrections
to the CCGbank?s NP structures.
7 Acknowledgements
This work was supported in part by NSF IIS-
0812297 and by an allocation of computing time
from the Ohio Supercomputer Center. Our thanks
also to Josef Van Genabith, the OSU Clippers group
and the anonymous reviewers for helpful comments
and discussion.
References
Chris Callison-Burch, Miles Osborne, and Philipp
Koehn. 2006. Re-evaluating the role of BLEU in ma-
chine translation research. In Proc. EACL.
Dominic Espinosa, Michael White, and Dennis Mehay.
2008. Hypertagging: Supertagging for surface real-
ization with CCG. In Proc. ACL-08:HLT.
Deirdre Hogan, Conor Cafferkey, Aoife Cahill, and Josef
van Genabith. 2007. Exploiting multi-word units
in history-based probabilistic generation. In Proc.
EMNLP-CoNLL.
Irene Langkilde-Geary. 2002. An empirical verification
of coverage and correctness for a general-purpose sen-
tence generator. In Proc. INLG-02.
Hiroko Nakanishi, Yusuke Miyao, and Jun?ichi Tsujii.
2005. Probabilistic methods for disambiguation of an
HPSG-based chart generator. In Proc. IWPT-05.
Alice H. Oh and Alexander I. Rudnicky. 2002. Stochas-
tic natural language generation for spoken dialog sys-
tems. Computer, Speech & Language, 16(3/4):387?
407.
Mark Steedman. 2000. The syntactic process. MIT
Press, Cambridge, MA, USA.
David Vadas and James R. Curran. 2008. Parsing noun
phrase structure with CCG. In Proc. ACL-08:HLT.
Ralph Weischedel and Ada Brunstein. 2005. BBN pro-
noun coreference and entity type corpus. Technical
report, BBN.
Michael White. 2006. Efficient Realization of Coordi-
nate Structures in Combinatory Categorial Grammar.
Research on Language and Computation, 4(1):39?75.
164
Proceedings of the NAACL HLT Workshop on Software Engineering, Testing, and Quality Assurance for Natural Language Processing, pages 45?46,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Grammar Engineering for CCG using Ant and XSLT?
Scott Martin, Rajakrishnan Rajkumar, and Michael White
Ohio State University
Department of Linguistics
{scott,raja,mwhite}@ling.ohio-state.edu
Overview
Corpus conversion and grammar extraction have
traditionally been portrayed as tasks that are
performed once and never again revisited (Burke
et al, 2004). We report the successful imple-
mentation of an approach to these tasks that
facilitates the improvement of grammar engi-
neering as an evolving process. Taking the
standard version of the CCGbank (Hocken-
maier and Steedman, 2007) as input, our sys-
tem then introduces greater depth of linguis-
tic insight by augmenting it with attributes
the original corpus lacks: Propbank roles and
head lexicalization for case-marking preposi-
tions (Boxwell and White, 2008), derivational
re-structuring for punctuation analysis (White
and Rajkumar, 2008), named entity annotation
and lemmatization. Our implementation ap-
plies successive XSLT transforms controlled by
Apache Ant (http://ant.apache.org/) to an
XML translation of this corpus, finally produc-
ing an OpenCCG grammar (http://openccg.
sourceforge.net/). This design is beneficial
to grammar engineering both because of XSLT?s
unique suitability to performing arbitrary trans-
formations of XML trees and the fine-grained
control that Ant provides. The resulting system
enables state-of-the-art BLEU scores for surface
realization on section 23 of the CCGbank.
1 Design
Rather than transforming the corpus, it would
be simple to introduce several of the corpus aug-
?This work was supported in part by NSF grant no.
IIS-0812297.
mentations that we make (e.g. punctuation re-
structuring) during grammar extraction. How-
ever, machine learning applications (e.g., real-
ization ranking) benefit when the corpus and
extracted grammar are consistent. A case in
point: annotating the corpus with named en-
tities, then using n-gram models with words re-
placed by their class labels to score realization.
Accordingly, our pipeline design starts by gen-
erating an XML version of the CCGbank us-
ing JavaCC (http://javacc.dev.java.net/)
from the original corpus. Next, conversion and
extraction transforms are applied to create a
converted corpus (also in XML) and extracted
grammar (in OpenCCG format).
We refactored our original design to separate
the grammar engineering task into several con-
figurable processes using Ant tasks. This sim-
plifies process management, speeds experiment
iterations, and facilitates the comparison of dif-
ferent grammar engineering strategies.
2 Implementation
It seemed natural to implement our pipeline pro-
cedure in XSLT since both OpenCCG grammars
and our CCGbank translation are represented in
XML. Aside from its inherent attributes, XSLT
requires no re-compilation as a result of being an
interpreted language. Also, because both con-
version and extraction use a series of transforms
in a chain, each required sub-step can be split
into as many XSLT transforms as desired.
Both the conversion and extraction steps
were implemented by extending Ant with cus-
tom tasks as configuring Ant tasks requires no
45
source editing or compilation. Ant is partic-
ularly well-suited to this process because, like
OpenCCG (whose libraries are used in the ex-
traction phase), it is written in Java. Our sys-
tem also employs the Ant-provided javacc task,
invoking the JavaCC parser to translate the
CCGbank to XML. This approach is preferable
to a direct Java implementation because it keeps
source code and configuration separate, allowing
for more rapid grammar engineering iterations.
Our particular implementation harnesses
Ant?s built-in FileSet (for specification of
groups of corpus files) and FileList (for re-
use of series of XSLT transforms) data types.
The first of our extension tasks, convert, encap-
sulates the conversion process while the second
task, extract, implements the grammar extrac-
tion procedure for a previously-converted cor-
pus.
3 Experimental Impact
Our conversion process currently supports var-
ious experiments by including only specified
transforms. We gain the ability to cre-
ate corpora with various combinations of at-
tributes, among them punctuation annotation,
semantic class information, and named entities
(lack of space precludes inclusion of examples
here; see http://www.ling.ohio-state.edu/
~scott/publications/grammareng/). In ad-
dition to extracting grammars, the extraction
task employs a constrained parser to create log-
ical forms (LFs) for surface realization and ex-
tracts SRILM training data for realization scor-
ing. This task also enables feature extraction
from LF graphs for training during supertagging
for realization (Espinosa et al, 2008).
Our design supports comprehensive experi-
mentation and has helped facilitate recent ef-
forts to investigate factors impacting surface re-
alization, such as semantic classes and named
entities. Our initial results reported in (White et
al., 2007) record 69.7% single-rooted LFs with a
BLEU score of 0.5768. But current figures stand
at 95.8% single-rooted LFs and a state-of-the
art BLEU score of 0.8506 on section 23 of the
CCGbank. (Fragmentary LFs result when at
least one semantic dependency is missing from
the LF graph.) In achieving these results, im-
provements in the grammar engineering process
have been at least as important as improvements
in the statistical models.
4 Conclusions and Future Work
We designed and implemented a system that fa-
cilitates the process of grammar engineering by
separating conversion and extraction steps into
a pipeline of XSLT transforms. Our Ant imple-
mentation is highly configurable and has posi-
tive effects on our grammar engineering efforts,
including increased process control and a short-
ened testing cycle for different grammar engi-
neering approaches. Future work will focus on
increasing the number of single-rooted LFs and
integrating this system with OpenCCG.
References
[Boxwell and White2008] Stephen Boxwell and
Michael White. 2008. Projecting Propbank roles
onto the CCGbank. In Proc. LREC-08.
[Burke et al2004] Michael Burke, Aoife Cahill,
Mairead Mccarthy, Ruth O?Donovan, Josef
Genabith, and Andy Way. 2004. Evaluating
automatic LFG F-structure annotation for the
Penn-II treebank. Research on Language and
Computation, 2:523?547, December.
[Espinosa et al2008] Dominic Espinosa, Michael
White, and Dennis Mehay. 2008. Hypertagging:
Supertagging for surface realization with CCG.
In Proc. ACL-08: HLT.
[Hockenmaier and Steedman2007] Julia Hockenmaier
and Mark Steedman. 2007. CCGbank: A Corpus
of CCG Derivations and Dependency Structures
Extracted from the Penn Treebank. Computa-
tional Linguistics, 33(3):355?396.
[White and Rajkumar2008] Michael White and Ra-
jakrishnan Rajkumar. 2008. A more precise
analysis of punctuation for broad-coverage sur-
face realization with CCG. In Proc. of the Work-
shop on Grammar Engineering Across Frame-
works (GEAF08).
[White et al2007] Michael White, Rajakrishnan Ra-
jkumar, and Scott Martin. 2007. Towards broad
coverage surface realization with CCG. In Proc.
of the Workshop on Using Corpora for NLG: Lan-
guage Generation and Machine Translation (UC-
NLG+MT).
46
Coling 2010: Poster Volume, pages 1032?1040,
Beijing, August 2010
Designing Agreement Features for Realization Ranking
Rajakrishnan Rajkumar and Michael White
Department of Linguistics
The Ohio State University
{raja,mwhite}@ling.osu.edu
Abstract
This paper shows that incorporating lin-
guistically motivated features to ensure
correct animacy and number agreement in
an averaged perceptron ranking model for
CCG realization helps improve a state-of-
the-art baseline even further. Tradition-
ally, these features have been modelled us-
ing hard constraints in the grammar. How-
ever, given the graded nature of grammat-
icality judgements in the case of animacy
we argue a case for the use of a statisti-
cal model to rank competing preferences.
Though subject-verb agreement is gener-
ally viewed to be syntactic in nature, a pe-
rusal of relevant examples discussed in the
theoretical linguistics literature (Kathol,
1999; Pollard and Sag, 1994) points to-
ward the heterogeneous nature of English
agreement. Compared to writing gram-
mar rules, our method is more robust and
allows incorporating information from di-
verse sources in realization. We also show
that the perceptron model can reduce bal-
anced punctuation errors that would other-
wise require a post-filter. The full model
yields significant improvements in BLEU
scores on Section 23 of the CCGbank and
makes many fewer agreement errors.
1 Introduction
In recent years a variety of statistical models for
realization ranking that take syntax into account
have been proposed, including generative mod-
els (Bangalore and Rambow, 2000; Cahill and
van Genabith, 2006; Hogan et al, 2007; Guo et
al., 2008), maximum entropy models (Velldal and
Oepen, 2005; Nakanishi et al, 2005) and averaged
perceptron models (White and Rajkumar, 2009).
To our knowledge, however, none of these mod-
els have included features specifically designed to
handle grammatical agreement, an important task
in surface realization. In this paper, we show that
incorporating linguistically motivated features to
ensure correct animacy and verbal agreement in
an averaged perceptron ranking model for CCG
realization helps improve a state-of-the-art base-
line even further. We also demonstrate the utility
of such an approach in ensuring the correct pre-
sentation of balanced punctuation marks.
Traditionally, grammatical agreement phenom-
ena have been modelled using hard constraints
in the grammar. Taking into consideration the
range of acceptable variation in the case of ani-
macy agreement and facts about the variety of fac-
tors contributing to number agreement, the ques-
tion arises: tackle agreement through grammar
engineering, or via a ranking model? In our
experience, trying to add number and animacy
agreement constraints to a grammar induced from
the CCGbank (Hockenmaier and Steedman, 2007)
turned out to be surprisingly difficult, as hard con-
straints often ended up breaking examples that
were working without such constraints, due to ex-
ceptions, sub-regularities and acceptable variation
in the data. With sufficient effort, it is conceiv-
able that an approach incorporating hard agree-
ment constraints could be refined to underspec-
ify cases where variation is acceptable, but even
so, one would want a ranking model to capture
preferences in these cases, which might vary de-
pending on genre, dialect or domain. Given that
1032
a ranking model is desirable in any event, we in-
vestigate here the extent to which agreement phe-
nomena can be more robustly and simply handled
using a ranking model alone, with no hard con-
straints in the grammar.
We also show here that the perceptron model
can reduce balanced punctuation errors that would
otherwise require a post-filter. As White and Ra-
jkumar (2008) discuss, in CCG it is not feasible
to use features in the grammar to ensure that bal-
anced punctuation (e.g. paired commas for NP ap-
positives) is used in all and only the appropriate
places, given the word-order flexibility that cross-
ing composition allows. While a post-filter is a
reasonably effective solution, it can be prone to
search errors and does not allow balanced punctu-
ation choices to interact with other choices made
by the ranking model.
The starting point for our work is a CCG re-
alization ranking model that incorporates Clark &
Curran?s (2007) normal-form syntactic model, de-
veloped for parsing, along with a variety of n-
gram models. Although this syntactic model plays
an important role in achieving top BLEU scores
for a reversible, corpus-engineered grammar, an
error analysis nevertheless revealed that many er-
rors in relative pronoun animacy agreement and
subject-verb number agreement remain with this
model. In this paper, we show that features specif-
ically designed to better handle these agreement
phenomena can be incorporated into a realization
ranking model that makes many fewer agreement
errors, while also yielding significant improve-
ments in BLEU scores on Section 23 of the CCG-
bank. These features make use of existing corpus
annotations ? specifically, PTB function tags and
BBN named entity classes (Weischedel and Brun-
stein, 2005) ? and thus they are relatively easy to
implement.
1.1 The Graded Nature of Animacy
Agreement
To illustrate the variation that can be found with
animacy agreement phenomena, consider first an-
imacy agreement with relative pronouns. In En-
glish, an inanimate noun can be modified by a rel-
ative clause introduced by that or which, while an
animate noun combines with who(m). With some
nouns though ? such as team, group, squad, etc.
? animacy status is uncertain, and these can be
found with all the three relative pronouns (who,
which and that). Google counts suggest that all
three choices are almost equally acceptable, as the
examples below illustrate:
(1) The groups who protested against plans to
remove asbestos from the nuclear subma-
rine base at Faslane claimed victory when
it was announced the government intends
to dispose of the waste on site. (The Glas-
gow Herald; Jun 25, 2010)
(2) Mr. Dorsch says the HIAA is work-
ing on a proposal to establish a privately
funded reinsurance mechanism to help
cover small groups that ca n?t get insur-
ance without excluding certain employees
. (WSJ0518.35)
1.2 The Heterogeneous Nature of Number
Agreement
Subject-verb agreement can be described as a con-
straint where the verb agrees with the subject in
terms of agreement features (number and person).
Agreement has often been considered to be a syn-
tactic phenomenon and grammar implementations
generally use syntactic features to enforce agree-
ment constraints (e.g. Velldal and Oepen, 2005).
However a closer look at our data and a survey
of the theoretical linguistics literature points to-
ward a more heterogeneous conception of English
agreement. Purely syntactic accounts are prob-
lematic when the following examples are consid-
ered:
(3) Five miles is a long distance to walk.
(Kim, 2004)
(4) King prawns cooked in chili salt and pep-
per was very much better, a simple dish
succulently executed. (Kim, 2004)
(5) ? I think it will shake confidence one more
time , and a lot of this business is based on
client confidence . ? (WSJ1866.10)
(6) It ?s interesting to find that a lot of the ex-
pensive wines are n?t always walking out
the door . (WSJ0071.53)
1033
In Example (3) above, the subject and deter-
miner are plural while the verb is singular. In
(4), the singular verb agrees with the dish, rather
than with individual prawns. Measure nouns such
as lot, ton, etc. exhibit singular agreement with
the determiner a, but varying agreement with the
verb depending on the head noun of the measure
noun?s of -complement. As is also well known,
British and American English differ in subject-
verb agreement with collective nouns. Kathol
(1999) proposes an explanation where agreement
is determined by the semantic properties of the
noun rather than by its morphological properties.
This accounts for all the cases above. In the light
of this explanation, specifying agreement features
in the logical form for realization could perhaps
solve the problem. However, the semantic view
of agreement is not completely convincing due to
counterexamples like the following discussed in
the literature (reported in Kim (2004)):
(7) Suppose you meet someone and they are
totally full of themselves
(8) Those scissors are missing.
In Example (7), the pronoun they used in a
generic sense is linked to the singular antecedent
someone, but its plural feature triggers plural
agreement with the verb. Example (8) illustrates a
situation where the subject scissors is arguably se-
mantically singular, but exhibits plural morphol-
ogy and plural syntactic agreement with both the
determiner as well as the verb. Thus this suggests
that English has a set of heterogeneous agree-
ment patterns rather than purely syntactic or se-
mantic ones. This is also reflected in the pro-
posal for a hybrid agreement system for English
(Kim, 2004), where the morphology tightly in-
teracts with the system of syntax, semantics, or
even pragmatics to account for agreement phe-
nomena. Our machine learning-based approach
approximates the insights discussed in the theoret-
ical linguistics literature. Writing grammar rules
to get these facts right proved to be surprisingly
difficult (e.g. discerning the actual nominal head
contributing agreement feature in cases like areas
of the factory were/*was vs. a lot of wines are/*is)
and required a list of measure nouns and parti-
tive quantifiers. We investigate here the extent
to which a machine learning?based approach is a
simpler, practical alternative for acquiring the rel-
evant generalizations from the data by combining
information from various information sources.
The paper is structured as follows. Section 2
provides CCG background. Section 3 describes
the features we have designed for animacy and
number agreement as well as for balanced punc-
tuation. Section 4 presents our evaluation of the
impact of these features in averaged perceptron re-
alization ranking models, tabulating specific kinds
of errors in the CCGbank development section as
well as overall automatic metric scores on Sec-
tion 23. Section 5 compares our results to those
obtained with related systems. Finally, Section 6
concludes with a summary of the paper?s contri-
butions.
2 Background
2.1 Surface Realization with Combinatory
Categorial Grammar (CCG)
CCG (Steedman, 2000) is a unification-based cat-
egorial grammar formalism which is defined al-
most entirely in terms of lexical entries that en-
code sub-categorization information as well as
syntactic feature information (e.g. number and
agreement). Complementing function application
as the standard means of combining a head with its
argument, type-raising and composition support
transparent analyses for a wide range of phenom-
ena, including right-node raising and long dis-
tance dependencies. An example syntactic deriva-
tion appears in Figure 1, with a long-distance
dependency between point and make. Seman-
tic composition happens in parallel with syntactic
composition, which makes it attractive for gener-
ation.
OpenCCG is a parsing/generation library which
works by combining lexical categories for words
using CCG rules and multi-modal extensions on
rules (Baldridge, 2002) to produce derivations.
Conceptually these extensions are on lexical cate-
gories. Surface realization is the process by which
logical forms are transduced to strings. OpenCCG
uses a hybrid symbolic-statistical chart realizer
(White, 2006) which takes logical forms as in-
put and produces sentences by using CCG com-
1034
He has a point he wants to make
np sdcl\np/np np/n n np sdcl\np/(sto\np) sto\np/(sb\np) sb\np/np
> >T >B
np s/(s\np) sto\np/np
>B
sdcl\np/np
>B
sdcl/np
np\np
<np
>
sdcl\np
<sdcl
Figure 1: Syntactic derivation from the CCGbank for He has a point he wants to make [. . . ]
aa1
heh3
he h2
<Det>
<Arg0> <Arg1>
<TENSE>pres
<NUM>sg
<Arg0>
w1 want.01
m1
<Arg1>
<GenRel>
<Arg1>
<TENSE>pres
p1point
h1have.03
make.03
<Arg0>
s[b]\np/np
np/n
np
n
s[dcl]\np/np
s[dcl]\np/(s[to]\np)
np
Figure 2: Semantic dependency graph from the
CCGbank for He has a point he wants to make
[. . . ], along with gold-standard supertags (cate-
gory labels)
binators to combine signs. Edges are grouped
into equivalence classes when they have the same
syntactic category and cover the same parts of
the input logical form. Alternative realizations
are ranked using integrated n-gram or perceptron
scoring, and pruning takes place within equiva-
lence classes of edges. To more robustly support
broad coverage surface realization, OpenCCG
greedily assembles fragments in the event that the
realizer fails to find a complete realization.
To illustrate the input to OpenCCG, consider
the semantic dependency graph in Figure 2. In
the graph, each node has a lexical predication
(e.g. make.03) and a set of semantic features
(e.g. ?NUM?sg); nodes are connected via depen-
dency relations (e.g. ?ARG0?). (Gold-standard su-
pertags, or category labels, are also shown; see
Section 2.2 for their role in hypertagging.) In-
ternally, such graphs are represented using Hy-
brid Logic Dependency Semantics (HLDS), a
dependency-based approach to representing lin-
guistic meaning (Baldridge and Kruijff, 2002). In
HLDS, each semantic head (corresponding to a
node in the graph) is associated with a nominal
that identifies its discourse referent, and relations
between heads and their dependents are modeled
as modal relations.
For our experiments, we use an enhanced ver-
sion of the CCGbank (Hockenmaier and Steed-
man, 2007)?a corpus of CCG derivations derived
from the Penn Treebank?with Propbank (Palmer
et al, 2005) roles projected onto it (Boxwell and
White, 2008). Additionally, certain multi-word
NEs were collapsed using underscores so that they
are treated as atomic entities in the input to the
realizer. To engineer a grammar from this cor-
pus suitable for realization with OpenCCG, the
derivations are first revised to reflect the lexical-
ized treatment of coordination and punctuation as-
sumed by the multi-modal version of CCG that is
implemented in OpenCCG (White and Rajkumar,
2008). Further changes are necessary to support
semantic dependencies rather than surface syntac-
tic ones; in particular, the features and unifica-
tion constraints in the categories related to seman-
tically empty function words such complemen-
tizers, infinitival-to, expletive subjects, and case-
marking prepositions are adjusted to reflect their
purely syntactic status.
1035
2.2 Hypertagging
A crucial component of the OpenCCG realizer is
the hypertagger (Espinosa et al, 2008), or su-
pertagger for surface realization, which uses a
maximum entropy model to assign the most likely
lexical categories to the predicates in the input
logical form, thereby greatly constraining the real-
izer?s search space.1 Category label prediction is
done at run-time and is based on contexts within
the directed graph structure as shown in Figure 2,
instead of basing category assignment on linear
word and POS context as in the parsing case.
3 Feature Design
The features we employ in our baseline perceptron
ranking model are of three kinds. First, as in the
log-linear models of Velldal & Oepen and Nakan-
ishi et al, we incorporate the log probability of the
candidate realization?s word sequence according
to our linearly interpolated language models as a
single feature in the perceptron model. Since our
language model linearly interpolates three com-
ponent models, we also include the log prob from
each component language model as a feature so
that the combination of these components can be
optimized. Second, we include syntactic features
in our model by implementing Clark & Curran?s
(2007) normal form model in OpenCCG. The fea-
tures of this model are listed in Table 1; they
are integer-valued, representing counts of occur-
rences in a derivation. Third, we include dis-
criminative n-gram features (Roark et al, 2004),
which count the occurrences of each n-gram that
is scored by our factored language model, rather
than a feature whose value is the log probability
determined by the language model. Table 2 de-
picts the new animacy, agreement and punctuation
features being introduced as part of this work. The
next two sections describe these features in more
detail.
3.1 Animacy and Number Agreement
Underspecification as to the choice of pronoun in
the input leads to competing realizations involv-
ing the relative pronouns who, that, which etc. The
1The approach has been dubbed hypertagging since it op-
erates at a level ?above? the syntax, moving from semantic
representations to syntactic categories.
Feature Type Example
LexCat + Word s/s/np + before
LexCat + POS s/s/np + IN
Rule sdcl ? np sdcl\np
Rule + Word sdcl ? np sdcl\np + bought
Rule + POS sdcl ? np sdcl\np + VBD
Word-Word ?company, sdcl ? np sdcl\np, bought?
Word-POS ?company, sdcl ? np sdcl\np, VBD?
POS-Word ?NN, sdcl ? np sdcl\np, bought?
Word + ?w ?bought, sdcl ? np sdcl\np? + dw
POS + ?w ?VBD, sdcl ? np sdcl\np? + dw
Word + ?p ?bought, sdcl ? np sdcl\np? + dp
POS + ?p ?VBD, sdcl ? np sdcl\np? + dp
Word + ?v ?bought, sdcl ? np sdcl\np? + dv
POS + ?v ?VBD, sdcl ? np sdcl\np? + dv
Table 1: Baseline features: Basic and dependency
features from Clark & Curran?s (2007) normal
form model; distances are in intervening words,
punctuation marks and verbs, and are capped at 3,
3 and 2, respectively
Feature Example
Animacy features
Noun Stem + Wh-pronoun researcher + who
Noun Class + Wh-pronoun PER DESC + who
Number features
Noun + Verb people + are
NounPOS + Verb NNS + are
Noun + VerbPOS people + VBP
NounPOS + VerbPOS NNS + VBP
Noun of + Verb lot of + are
Noun of + VerbPOS lot of + VBP
NounPOS of + Verb NN of + are
NounPOS of + VerbPOS NN of + VBP
Noun of + of-complementPOS + VerbPOS lot of + NN + VBZ
NounPOS of + of-complementPOS + VerbPOS NN of + NN + VBZ
Noun of + of-complementPOS + Verb lot of + NN + is
NounPOS of + of-complementPOS + Verb NN of + NN + is
Punctuation feature
Balanced Punctuation Indicator $unbalPunct=1
Table 2: New features introduced
existing ranking models (n-gram models as well
as perceptron) often allow the top-ranked output
to have the relative pronoun that associated with
animate nouns. The existing normal form model
uses the word forms as well as part-of-speech tag
based features. Though this is useful for associ-
ating proper nouns (tagged NNP or NNPS) with
who, for other nouns (as in consumers who vs.
consumers that/which), the model often prefers
the infelicitous pronoun. So here we designed fea-
tures which also took into account the named en-
tity class of the head noun as well as the stem of
the head noun. These features aid the discrimi-
native n-gram features (PERSON, which has high
negative weight). As the results section discusses,
1036
NE classes like PER DESC contribute substan-
tially towards animacy preferences.
For number agreement, we designed three
classes of features (c.f. Number Agr row in Table
2). Each of these classes results in 4 features. Dur-
ing feature extraction, subjects of the verbs tagged
VBZ and VBP and verbs was, were were iden-
tified using the PTB NP-SBJ function tag anno-
tation projected on to the appropriate arguments
of lexical categories of verbs. The first class
of features encoded all possible combinations of
subject-verb word forms and parts of speech tags.
In the case of NPs involving of-complements like
a lot of ... (Examples 5 and 6), feature classes 2
and 3 were extracted (class 1 was excluded). Class
2 features encode the fact that the syntactic head
has an associated of-complement, while class 3
features also include the part of speech tag of the
complement. In the case of conjunct/disjunct VPs
and subject NPs, the feature specifically looked
at the parts of speech of both the NPs/VPs form-
ing the conjunct/disjunct. The motivation behind
such a design was to glean syntactic and semantic
generalizations from the data. During feature ex-
traction, from each derivation, counts of animacy
and agreement features were obtained.
3.2 Balanced Punctuation
A complex issue that arises in the design of bi-
directional grammars is ensuring the proper pre-
sentation of punctuation. Among other things, this
involves the task of ensuring the correct realiza-
tion of commas introducing noun phrase apposi-
tives.
(9) John, CEO of ABC, loves Mary.
(10) * John, CEO of ABC loves Mary.
(11) Mary loves John, CEO of ABC.
(12) * Mary loves John, CEO of ABC,.
(13) Mary loves John, CEO of ABC, madly.
(14) * Mary loves John, CEO of ABC madly.
As of now, n-gram models rule out examples
like 12 above. All the other unacceptable ex-
amples are ruled out using a post-filter on real-
ized derivations. As described in White and Ra-
jkumar (2008), the need for the filter arises be-
cause a feature-based approach appears to be in-
adequate for dealing with the class of examples
presented above in CCG. This approach involves
the incorporation of syntactic features for punctu-
ation into atomic categories so that certain combi-
nations are blocked. To ensure proper appositive
balancing sentence finally, the rightmost element
in the sentence should transmit a relevant fea-
ture to the clause level, which the sentence-final
period can then check for the presence of right-
edge punctuation. However, the feature schema
does not constrain cases of balanced punctuation
in cases involving crossing composition and ex-
traction. However, in this paper we explore a sta-
tistical approach to ensure proper balancing of NP
apposition commas. The first step in this solution
is the introduction of a feature in the grammar
which indicates balanced vs. unbalanced marks.
We modified the result categories of unbalanced
appositive commas and dashes to include a fea-
ture marking unbalanced punctuation, as follows:
(15) , ` np?1?unbal=comma\?np?1?/?np?2?
Then, during feature extraction, derivations
were examined to detect categories such as
npunbal=comma , and checked to make sure this NP
is followed by another punctuation mark in the
string such as a full stop. The feature indicates the
presence or absence of unbalanced punctuation in
the derivation.
4 Evaluation
4.1 Experimental Conditions
For the experiments reported below, we used a
lexico-grammar extracted from Sections 02?21 of
our enhanced CCGbank with collapsed NEs, a
hypertagging model incorporating named entity
class features, and a trigram factored language
model over words, named entity classes, part-of-
speech tags and supertags. Perceptron training
events were generated for each training section
separately. The hypertagger and POS/supertag
language model were trained on all the training
sections, while separate word-based models were
trained excluding each of the training sections in
turn. Event files for 26530 training sentences with
complete realizations were generated, with an av-
erage n-best list size of 18.2. The complete set of
models is listed in Table 3.
1037
Model Description
full-model All the feats from models below
agr-punct Baseline Feats + Punct + Num-Agr
wh-punct Baseline Feats + Punct + Animacy-Agr
baseline-punct Baseline Feats + Punct
baseline Log prob + n-gram +Syntactic features
Table 3: Legend for experimental conditions
4.2 Results
Realization results on the development and test
sections are given in Table 4. For the develop-
ment section, in terms of both exact matches and
BLEU scores, the model with all the three features
discussed above (agreement, animacy and punc-
tuation) performs better than the baseline which
does not have any of these features. However, us-
ing these criteria, the best performing model is ac-
tually the model which has agreement and punc-
tuation features. The model containing all the
features does better than the punctuation-feature
only model, but performs slightly worse than the
agreement-punctuation model. Section 23, the
test section, confirms that the model with all the
features performs better than the baseline model.
We calculated statistical significance for the main
results using bootstrap random sampling.2 Af-
ter re-sampling 1000 times, significance was cal-
culated using a paired t-test (999 d.f.). The re-
sults indicated that the model with all the fea-
tures in it (full-model) exceeded the baseline with
p < 0.0001 . However, exact matches and
BLEU scores do not necessarily reflect the extent
to which important grammatical flaws have been
reduced. So to judge the effectiveness of the new
features, we computed the percentage of errors of
each type that were present in the best Section 00
realization selected by each of these models. Also
note that our baseline results differ slightly from
the corresponding results reported in White and
Rajkumar (2009) in spite of using the same feature
set because quotes were introduced into the cor-
pus on which these experiments were conducted.
Previous results were based on the original CCG-
bank text where quotation marks are absent.
Table 6 reports results of the error analysis. It
2Scripts for running these tests are available at
http://projectile.sv.cmu.edu/research/
public/tools/bootStrap/tutorial.htm
Section Model %Exact %Compl. BLEU
00 baseline 38.18 82.47 0.8341
baseline-punct 37.97 82.47 0.8340
wh-punct 38.93 82.53 0.8360
full-model 40.47 82.53 0.8403
agr-punct 40.84 82.53 0.8414
23 baseline 38.98 83.39 0.8442
full-model 40.09 83.35 0.8446
Table 4: Results (98.9% coverage)?percentage
of exact match and grammatically complete real-
izations and BLEU scores
Model METEOR TERP
baseline 0.9819 0.0939
baseline-punct 0.9819 0.0939
wh-punct 0.9827 0.0923
agr-punct 0.9821 0.0902
full-model 0.9826 0.0909
Table 5: Section 00 METEOR and TERP scores
can be seen that the punctuation-feature is effec-
tive in reducing the number of sentences with un-
balanced punctuation marks. Similarly, the full
model has fewer animacy mismatches and just
about the same number of errors of the other two
types, though it performs slightly worse than the
agreement-only model in terms of BLEU scores
and exact matches. We also manually examined
the remaining cases of animacy agreement errors
in the output of the full model here. Of the remain-
ing 18 errors, 14 were acceptable paraphrases in-
volving object relative clauses (eg. wsj 0083.40 ...
the business that/? a company can generate). We
also provide METEOR and TERP scores for these
models (Table 5). In recently completed work on
the creation of a human-rated paraphrase corpus
to evaluate NLG systems, our analyses showed
that BLEU, METEOR and TERP scores correlate
moderately with human judgments of adequacy
and fluency, and that the most reliable system-
level comparisons can be made only by looking
at all three metrics.
4.3 Examples
Table 7 presents four examples where the
full model differs from the baseline. Example
wsj 0003.8 illustrates an example where the NE
tag PER DESC for researchers helps the percep-
tron model enforce the correct animacy agree-
ment, while the two baseline models prefer the
1038
Ref-wsj 0003.8 full,agr,wh neither Lorillard nor the researchers who studied the workers were aware of any research on
smokers of the Kent cigarettes
baseline,baseline-punct neither Lorillard nor the researchers that studied the workers were aware of any research on
smokers of the Kent cigarettes .
Ref-wsj 0003.18 agr-punct, full the plant , which is owned by Hollingsworth & Vose Co. , was under contract with lorillard to make the cigarette filters .
baselines, wh the plant , which is owned by Hollingsworth & Vose Co. , were under contract with lorillard to make the cigarette filters .
Ref-wsj 0018.6 agr-punct, full model while many of the risks were anticipated when minneapolis-based Cray Research first announced the spinoff ...
agr-punct, full while many of the risks were anticipated when minneapolis-based Cray Research first announced the spinoff ...
baselines while many of the risks was anticipated when minneapolis-based Cray Research announced the spinoff ...
Ref-wsj 0070.4 agr-punct, full Giant Group is led by three Rally ?s directors , Burt Sugarman , James M. Trotter III and William E. Trotter II that last
month indicated that they hold a 42.5 % stake in Rally ?s and plan to seek a majority of seats on ...
all others Giant Group is led by three Rally ?s directors , Burt Sugarman , James M. Trotter III and William E. Trotter II that last
month indicated that they holds a 42.5 % stake in Rally ?s and plans to seek a majority of seats on ...
Ref-wsj 0047.5 ... the ban wo n?t stop privately funded tissue-transplant research or federally funded fetal-tissue research
that does n?t involve transplants .
agr, full ... the ban wo n?t stop tissue-transplant privately funded research or federally funded fetal-tissue research
that does n?t involve transplants .
baselines, wh ... the ban wo n?t stop tissue-transplant privately funded research or federally funded fetal-tissue research
that do n?t involve transplants .
Table 7: Examples of realized output
Model #Punct-Errs %Agr-Errs %WH-Errs
baseline 39 11.05 22.44
baseline-punct 0 10.79 20.77
wh-punct 11 10.87 13.53
agr-punct 8 4.0 21.84
full-model 10 4.31 15.53
Table 6: Error analysis of Section 00 complete re-
alizations (total of 1554 agreement cases; total of
207 WH-pronoun cases)
that realization. Example wsj 0003.18 illustrates
an instance of simple subject-verb agreement be-
ing enforced by the models containing the agree-
ment features. Example wsj 0070.4 presents a
more complex situation where a single subject
has to agree with both verbs in a conjoined verb
phrase. The last example in Table 7 shows the
case of a NP subject which is a disjunction of two
individual NPs. In both these cases, while the
baseline models do not enforce the correct choice,
the models with the agreement features do get this
right. This is because our agreement features are
sensitive to the properties of both NP and VP con-
juncts/disjuncts. In addition, most of the realiza-
tions involving of -complements are also ranked
correctly. In the final example sentence provided
(i.e. wsj 0018.6), the models with the agreement
features are able to enforce the correct the agree-
ment constraints in the phrase many of the risks
were in contrast to the baseline models.
5 Conclusion
In this paper, we have shown for the first time
that incorporating linguistically motivated fea-
tures to ensure correct animacy and number agree-
ment in a statistical realization ranking model
yields significant improvements over a state-of-
the-art baseline. While agreement has tradition-
ally been modelled using hard constraints in the
grammar, we have argued that using a statistical
ranking model is a simpler and more robust ap-
proach that is capable of learning competing pref-
erences and cases of acceptable variation. Our
approach also approximates insights about agree-
ment which have been discussed in the theoret-
ical linguistics literature. We have also shown
how a targeted error analysis can reveal substan-
tial reductions in agreement errors, whose impact
on quality no doubt exceeds what is suggested
by the small BLEU score increases. As future
work, we also plan to learn such patterns from
large amounts of unlabelled data and use models
learned thus to rank paraphrases.
Acknowledgements
This work was supported in part by NSF grant IIS-
0812297 and by an allocation of computing time
from the Ohio Supercomputer Center. Our thanks
also to Robert Levine and the anonymous review-
ers for helpful comments and discussion.
1039
References
Baldridge, Jason and Geert-Jan Kruijff. 2002. Cou-
pling CCG and Hybrid Logic Dependency Seman-
tics. In Proc. ACL-02.
Baldridge, Jason. 2002. Lexically Specified Deriva-
tional Control in Combinatory Categorial Gram-
mar. Ph.D. thesis, University of Edinburgh.
Bangalore, Srinivas and Owen Rambow. 2000. Ex-
ploiting a probabilistic hierarchical model for gen-
eration. In Proc. COLING-00.
Boxwell, Stephen and Michael White. 2008. Project-
ing Propbank roles onto the CCGbank. In Proc.
LREC-08.
Cahill, Aoife and Josef van Genabith. 2006. Ro-
bust PCFG-based generation using automatically
acquired LFG approximations. In Proc. COLING-
ACL ?06.
Clark, Stephen and James R. Curran. 2007. Wide-
Coverage Efficient Statistical Parsing with CCG and
Log-Linear Models. Computational Linguistics,
33(4):493?552.
Espinosa, Dominic, Michael White, and Dennis
Mehay. 2008. Hypertagging: Supertagging for sur-
face realization with CCG. In Proc. ACL-08: HLT.
Guo, Yuqing, Josef van Genabith, and Haifeng Wang.
2008. Dependency-based n-gram models for
general purpose sentence realisation. In Proc.
COLING-08.
Hockenmaier, Julia and Mark Steedman. 2007. CCG-
bank: A Corpus of CCG Derivations and Depen-
dency Structures Extracted from the Penn Treebank.
Computational Linguistics, 33(3):355?396.
Hogan, Deirdre, Conor Cafferkey, Aoife Cahill, and
Josef van Genabith. 2007. Exploiting multi-word
units in history-based probabilistic generation. In
Proc. EMNLP-CoNLL.
Kathol, Andreas. 1999. Agreement and the
Syntax-Morphology Interface in HPSG. In Levine,
Robert D. and Georgia M. Green, editors, Studies
in Contemporary Phrase Structure Grammar, pages
223?274. Cambridge University Press, Cambridge.
Kim, Jong-Bok. 2004. Hybrid Agreement in English.
Linguistics, 42(6):1105?1128.
Nakanishi, Hiroko, Yusuke Miyao, and Jun?ichi Tsujii.
2005. Probabilistic methods for disambiguation of
an HPSG-based chart generator. In Proc. IWPT-05.
Palmer, Martha, Dan Gildea, and Paul Kingsbury.
2005. The proposition bank: A corpus annotated
with semantic roles. Computational Linguistics,
31(1).
Pollard, Carl and Ivan Sag. 1994. Head-Driven
Phrase Structure Grammar. University Of Chicago
Press.
Roark, Brian, Murat Saraclar, Michael Collins, and
Mark Johnson. 2004. Discriminative language
modeling with conditional random fields and the
perceptron algorithm. In Proc. ACL-04.
Steedman, Mark. 2000. The syntactic process. MIT
Press, Cambridge, MA, USA.
Velldal, Erik and Stephan Oepen. 2005. Maximum
entropy models for realization ranking. In Proc. MT
Summit X.
Weischedel, Ralph and Ada Brunstein. 2005. BBN
pronoun coreference and entity type corpus. Tech-
nical report, BBN.
White, Michael and Rajakrishnan Rajkumar. 2008.
A more precise analysis of punctuation for broad-
coverage surface realization with CCG. In Proc.
of the Workshop on Grammar Engineering Across
Frameworks (GEAF08).
White, Michael and Rajakrishnan Rajkumar. 2009.
Perceptron reranking for CCG realization. In Pro-
ceedings of the 2009 Conference on Empirical
Methods in Natural Language Processing, pages
410?419, Singapore, August. Association for Com-
putational Linguistics.
White, Michael. 2006. Efficient Realization of Coor-
dinate Structures in Combinatory Categorial Gram-
mar. Research on Language and Computation,
4(1):39?75.
1040
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 564?574,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Further Meta-Evaluation of Broad-Coverage Surface Realization
Dominic Espinosa and Rajakrishnan Rajkumar and Michael White and Shoshana Berleant
Department of Linguistics
The Ohio State University
Columbus, Ohio, USA
{espinosa,raja,mwhite,berleant}@ling.ohio-state.edu
Abstract
We present the first evaluation of the utility of
automatic evaluation metrics on surface real-
izations of Penn Treebank data. Using outputs
of the OpenCCG and XLE realizers, along
with ranked WordNet synonym substitutions,
we collected a corpus of generated surface re-
alizations. These outputs were then rated and
post-edited by human annotators. We eval-
uated the realizations using seven automatic
metrics, and analyzed correlations obtained
between the human judgments and the auto-
matic scores. In contrast to previous NLG
meta-evaluations, we find that several of the
metrics correlate moderately well with human
judgments of both adequacy and fluency, with
the TER family performing best overall. We
also find that all of the metrics correctly pre-
dict more than half of the significant system-
level differences, though none are correct in
all cases. We conclude with a discussion of the
implications for the utility of such metrics in
evaluating generation in the presence of varia-
tion. A further result of our research is a cor-
pus of post-edited realizations, which will be
made available to the research community.
1 Introduction and Background
In building surface-realization systems for natural
language generation, there is a need for reliable
automated metrics to evaluate the output. Unlike
in parsing, where there is usually a single gold-
standard parse for a sentence, in surface realization
there are usually many grammatically-acceptable
ways to express the same concept. This parallels
the task of evaluating machine-translation (MT) sys-
tems: for a given segment in the source language,
there are usually several acceptable translations into
the target language. As human evaluation of trans-
lation quality is time-consuming and expensive, a
number of automated metrics have been developed
to evaluate the quality of MT outputs. In this study,
we investigate whether the metrics developed for
MT evaluation tasks can be used to reliably evaluate
the outputs of surface realizers, and which of these
metrics are best suited to this task.
A number of surface realizers have been devel-
oped using the Penn Treebank (PTB), and BLEU
scores are often reported in the evaluations of these
systems. But how useful is BLEU in this con-
text? The original BLEU study (Papineni et al,
2001) scored MT outputs, which are of generally
lower quality than grammar-based surface realiza-
tions. Furthermore, even for MT systems, the
usefulness of BLEU has been called into question
(Callison-Burch et al, 2006). BLEU is designed to
work with multiple reference sentences, but in tree-
bank realization, there is only a single reference sen-
tence available for comparison.
A few other studies have investigated the use of
such metrics in evaluating the output of NLG sys-
tems, notably (Reiter and Belz, 2009) and (Stent et
al., 2005). The former examined the performance of
BLEU and ROUGE with computer-generated weather
reports, finding a moderate correlation with human
fluency judgments. The latter study applied sev-
eral MT metrics to paraphrase data from Barzilay
and Lee?s corpus-based system (Barzilay and Lee,
2003), and found moderate correlations with human
adequacy judgments, but little correlation with flu-
ency judgments. Cahill (2009) examined the perfor-
mance of six MT metrics (including BLEU) in evalu-
ating the output of a LFG-based surface realizer for
564
German, also finding only weak correlations with
the human judgments.
To study the usefulness of evaluation metrics such
as BLEU on the output of grammar-based surface
realizers used with the PTB, we assembled a cor-
pus of surface realizations from three different re-
alizers operating on Section 00 of the PTB. Two
human judges evaluated the adequacy and fluency
of each of the realizations with respect to the ref-
erence sentence. The realizations were then scored
with a number of automated evaluation metrics de-
veloped for machine translation. In order to investi-
gate the correlation of targeted metrics with human
evaluations, and gather other acceptable realizations
for future evaluations, the judges manually repaired
each unacceptable realization during the rating task.
In contrast to previous NLG meta-evaluations, we
found that several of the metrics correlate moder-
ately well with human judgments of both adequacy
and fluency, with the TER family performing best.
However, when looking at statistically significant
system-level differences in human judgments, we
found that some of the metrics get some of the rank-
ings correct, but none get them all correct, with dif-
ferent metrics making different ranking errors. This
suggests that multiple metrics should be routinely
consulted when comparing realizer systems.
Overall, our methodology is similar to that of
previous MT meta-evaluations, in that we collected
human judgments of system outputs, and com-
pared these scores with those assigned by auto-
matic metrics. A recent alternative approach to para-
phrase evaluation is ParaMetric (Callison-Burch et
al., 2008); however, it requires a corpus of annotated
(aligned) paraphrases (which does not yet exist for
PTB data), and is arguably focused more on para-
phrase analysis than paraphrase generation.
The plan of the paper is as follows: Section 2 dis-
cusses the preparation of the corpus of surface real-
izations. Section 3 describes the human evaluation
task and the automated metrics applied. Sections 4
and 5 present and discuss the results of these evalua-
tions. We conclude with some general observations
about automatic evaluation of surface realizers, and
some directions for further research.
2 Data Preparation
We collected realizations of the sentences in Sec-
tion 00 of the WSJ corpus from the following three
sources:
1. OpenCCG, a CCG-based chart realizer (White,
2006)
2. The XLE Generator, a LFG-based system de-
veloped by Xerox PARC (Crouch et al, 2008)
3. WordNet synonym substitutions, to investigate
how differences in lexical choice compare to
grammar-based variation.1
Although all three systems used Section 00 of
the PTB, they were applied with various parame-
ters (e.g., language models, multiple-output versus
single-output) and on different input structures. Ac-
cordingly, our study does not compare OpenCCG to
XLE, or either of these to the WordNet system.
2.1 OpenCCG realizations
OpenCCG is an open source parsing/realization
library with multimodal extensions to CCG
(Baldridge, 2002). The OpenCCG chart realizer
takes logical forms as input and produces strings
by combining signs for lexical items. Alternative
realizations are scored using integrated n-gram
and perceptron models. For robustness, fragments
are greedily assembled when necessary. Realiza-
tions were generated from 1,895 gold standard
logical forms, created by constrained parsing of
development-section derivations. The following
OpenCCG models (which differ essentially in the
way the output is ranked) were used:
1. Baseline 1: Output ranked by a trigram word
model
2. Baseline 2: Output ranked using three language
models (3-gram words + 3-gram words with
named entity class replacement + factored lan-
guage model of words, POS tags and CCG su-
pertags)
1Not strictly surface realizations, since they do not involve
an abstract input specification, but for simplicity we refer to
them as realizations throughout.
565
3. Baseline 3: Perceptron with syntax features and
the three LMs mentioned above
4. Perceptron full-model: n-best realizations
ranked using perceptron with syntax features
and the three n-gram models, as well as dis-
criminative n-grams
The perceptron model was trained on sections 02-
21 of the CCGbank, while a grammar extracted from
section 00-21 was used for realization. In addition,
oracle supertags were inserted into the chart during
realization. The purpose of such a non-blind test-
ing strategy was to evaluate the quality of the output
produced by the statistical ranking models in isola-
tion, rather than focusing on grammar coverage, and
avoid the problems associated with lexical smooth-
ing, i.e. lexical categories in the development sec-
tion not being present in the training section.
To enrich the variation in the generated realiza-
tions, dative-alternation was enforced during real-
ization by ensuring alternate lexical categories of the
verb in question, as in the following example:
(1) the executives gave [the chefs] [a stand-
ing ovation]
(2) the executives gave [a standing ovation]
[to the chefs]
2.2 XLE realizations
The corpus of realizations generated by the XLE
system contained 42,527 surface realizations of ap-
proximately 1,421 section 00 sentences (an aver-
age of 30 per sentence), initially unranked. The
LFG f-structures used as input to the XLE genera-
tor were derived from automatic parses, as described
in (Riezler et al, 2002). The realizations were
first tokenized using Penn Treebank conventions,
then ranked using perplexities calculated from the
same trigram word model used with OpenCCG. For
each sentence, the top 4 realizations were selected.
The XLE generator provides an interesting point
of comparison to OpenCCG as it uses a manually-
developed grammar with inputs that are less abstract
but potentially noisier, as they are derived from au-
tomatic parses rather than gold-standard ones.
2.3 WordNet synonymizer
To produce an additional source of variation, the
nouns and verbs of the sentences in section 00 of
the PTB were replaced with all of their WordNet
synonyms. Verb forms were generated using verb
stems, part-of-speech tags, and the morphg tool.2
These substituted outputs were then filtered using
the n-gram data which Google Inc. has made avail-
able.3 Those without any 5-gram matches centered
on the substituted word (or 3-gram matches, in the
case of short sentences) were eliminated.
3 Evaluation
From the data sources described in the previous sec-
tion, a corpus of realizations to be evaluated by the
human judges was constructed by randomly choos-
ing 305 sentences from section 00, then selecting
surface realizations of these sentences using the fol-
lowing algorithm:
1. Add OpenCCG?s best-scored realization.
2. Add other OpenCCG realizations until all four
models are represented, to a maximum of 4.
3. Add up to 4 realizations from either the XLE
system or the WordNet pool, chosen randomly.
The intent was to give reasonable coverage of all
realizer systems discussed in Section 2 without over-
loading the human judges. ?System? here means
any instantiation that emits surface realizations, in-
cluding various configurations of OpenCCG (using
different language models or ranking systems), and
these can be multiple-output, such as an n-best list,
or single-output (best-only, worst-only, etc.). Ac-
cordingly, more realizations were selected from the
OpenCCG realizer because 5 different systems were
being represented. Realizations were chosen ran-
domly, rather than according to sentence types or
other criteria, in order to produce a representative
sample of the corpus. In total, 2,114 realizations
were selected for evaluation.
2http://www.informatics.sussex.ac.uk/
research/groups/nlp/carroll/morph.html
3http://www.ldc.upenn.edu/Catalog/docs/
LDC2006T13/readme.txt
566
3.1 Human judgments
Two human judges evaluated each surface realiza-
tion on two criteria: adequacy, which represents the
extent to which the output conveys all and only the
meaning of the reference sentence; and fluency, the
extent to which it is grammatically acceptable. The
realizations were presented to the judges in sets con-
taining a reference sentence and the 1-8 outputs se-
lected for that sentence. To aid in the evaluation of
adequacy, one sentence each of leading and trailing
context were displayed. Judges used the guidelines
given in Figure 1, based on the scales developed
by the NIST Machine Translation Evaluation Work-
shop.
In addition to rating each realization on the two
five-point scales, each judge also repaired each out-
put which he or she did not judge to be fully ade-
quate and fluent. An example is shown in Figure 2.
These repairs resulted in new reference sentences for
a substantial number of sentences. These repaired
realizations were later used to calculate targeted ver-
sions of the evaluation metrics, i.e., using the re-
paired sentence as the reference sentence. Although
targeted metrics are not fully automatic, they are of
interest because they allow the evaluation algorithm
to focus on what is actually wrong with the input,
rather than all textual differences. Notably, targeted
TER (HTER) has been shown to be more consistent
with human judgments than human annotators are
with one another (Snover et al, 2006).
3.2 Automatic evaluation
The realizations were also evaluated using seven au-
tomatic metrics:
? IBM?s BLEU, which scores a hypothesis by
counting n-gram matches with the reference
sentence (Papineni et al, 2001), with smooth-
ing as described in (Lin and Och, 2004)
? The NIST n-gram evaluation metric, similar to
BLEU, but rewarding rarer n-gram matches, and
using a different length penalty
? METEOR, which measures the harmonic mean
of unigram precision and recall, with a higher
weight for recall (Banerjee and Lavie, 2005)
? TER (Translation Edit Rate), a measure of the
number of edits required to transform a hy-
pothesis sentence into the reference sentence
(Snover et al, 2006)
? TERP, an augmented version of TER which
performs phrasal substitutions, stemming, and
checks for synonyms, among other improve-
ments (Snover et al, 2009)
? TERPA, an instantiation of TERP with edit
weights optimized for correlation with ade-
quacy in MT evaluations
? GTM (General Text Matcher), a generaliza-
tion of the F-measure that rewards contiguous
matching spans (Turian et al, 2003)
Additionally, targeted versions of BLEU, ME-
TEOR, TER, and GTM were computed by using the
human-repaired outputs as the reference set. The
human repair was different from the reference sen-
tence in 193 cases (about 9% of the total), and we
expected this to result in better scores and correla-
tions with the human judgments overall.
4 Results
4.1 Human judgments
Table 1 summarizes the dataset, as well as the mean
adequacy and fluency scores garnered from the hu-
man evaluation. Overall adequacy and fluency judg-
ments were high (4.16, 3.63) for the realizer sys-
tems on average, and the best-rated realizer systems
achieved mean fluency scores above 4.
4.2 Inter-annotator agreement
Inter-annotator agreement was measured using the
?-coefficient, which is commonly used to measure
the extent to which annotators agree in category
judgment tasks. ? is defined as P (A)?P (E)1?P (E) , where
P (A) is the observed agreement between annota-
tors and P (E) is the probability of agreement due
to chance (Carletta, 1996). Chance agreement for
this data is calculated by the method discussed in
Carletta?s squib. However, in previous work in
MT meta-evaluation, Callison-Burch et al (2007),
assume the less strict criterion of uniform chance
agreement, i.e. 15 for a five-point scale. They also
567
Score Adequacy Fluency
5 All the meaning of the reference Perfectly grammatical
4 Most of the meaning Awkward or non-native; punctuation errors
3 Much of the meaning Agreement errors or minor syntactic problems
2 Meaning substantially different Major syntactic problems, such as missing words
1 Meaning completely different Completely ungrammatical
Figure 1: Rating scale and guidelines
Ref. It wasn?t clear how NL and Mr. Simmons would respond if Georgia Gulf spurns them again
Realiz. It weren?t clear how NL and Mr. Simmons would respond if Georgia Gulf again spurns them
Repair It wasn?t clear how NL and Mr. Simmons would respond if Georgia Gulf again spurns them
Figure 2: Example of repair
introduce the notion of ?relative? ?, which measures
how often two or more judges agreed that A > B,
A = B, or A < B for two outputs A and B, irre-
spective of the specific values given on the five-point
scale; here, uniform chance agreement is taken to be
1
3 . We report both absolute and relative ? in Table 2,
using actual chance agreement rather than uniform
chance agreement.
The ? scores of 0.60 for adequacy and 0.63 for flu-
ency across the entire dataset represent ?substantial?
agreement, according to the guidelines discussed in
(Landis and Koch, 1977), better than is typically
reported for machine translation evaluation tasks;
for example, Callison-Burch et al (2007) reported
?fair? agreement, with ? = 0.281 for fluency and
? = 0.307 for adequacy (relative). Assuming the
uniform chance agreement that the previously cited
work adopts, our inter-annotator agreements (both
absolute and relative) are still higher. This is likely
due to the generally high quality of the realizations
evaluated, leading to easier judgments.
4.3 Correlation with automatic evaluation
To determine how well the automatic evaluation
methods described in Section 3 correlate with the
human judgments, we averaged the human judg-
ments for adequacy and fluency, respectively, for
each of the rated realizations, and then computed
both Pearson?s correlation coefficient and Spear-
man?s rank correlation coefficient between these
scores and each of the metrics. Spearman?s corre-
lation makes fewer assumptions about the distribu-
tion of the data, but may not reflect a linear rela-
tionship that is actually present. Both are frequently
reported in the literature. Due to space constraints,
we show only Spearman?s correlation, although the
TER family scored slightly better on Pearson?s coef-
ficient, relatively.
The results for Spearman?s correlation are given
in Table 3. Additionally, the average scores for ad-
equacy and fluency were themselves averaged into
a single score, following (Snover et al, 2009), and
the Spearman?s correlation of each of the automatic
metrics with these scores are given in Table 4. All
reported correlations are significant at p < 0.001.
4.4 Bootstrap sampling of correlations
For each of the sub-corpora shown in Table 1, we
computed confidence intervals for the correlations
between adequacy and fluency human scores with
selected automatic metrics (BLEU, HBLEU, TER,
TERP, and HTER) as described in (Koenh, 2004). We
sampled each sub-corpus 1000 times with replace-
ment, and calculated correlations between the rank-
ings induced by the human scores and those induced
by the metrics for each reference sentence. We then
used these coefficients to estimate the confidence in-
terval, after excluding the top 25 and bottom 25 co-
efficients, following (Lin and Och, 2004). The re-
sults of this for the BLEU metric are shown in Table
5. We determined which correlations lay within the
95% confidence interval of the best performing met-
ric in each row of Table Table 3; these figures are
italicized.
568
5 Discussion
5.1 Human judgments of systems
The results for the four OpenCCG perceptron mod-
els mostly confirm those reported in (White and Ra-
jkumar, 2009), with one exception: the B-3 model
was below B-2, though the P-B (perceptron-best)
model still scored highest. This may have been due
to differences in the testing scenario. None of the
differences in adequacy scores among the individ-
ual systems are significant, with the exception of the
WordNet system. In this case, the lack of word-
sense disambiguation for the substituted words re-
sults in a poor overall adequacy score (e.g., wage
floor ? wage story). Conversely, it scores highest
for fluency, as substituting a noun or verb with a syn-
onym does not usually introduce ungrammaticality.
5.2 Correlations of human judgments with MT
metrics
Of the non-human-targeted metrics evaluated, BLEU
and TER/TERP demonstrate the highest correla-
tions with the human judgments of fluency (r =
0.62, 0.64). The TER family of evaluation metrics
have been observed to perform very well in MT-
evaluation tasks, and although the data evaluated
here differs from typical MT data in some impor-
tant ways, the correlation of TERP with the human
judgments is substantial. In contrast with previous
MT evaluations where TERP performs considerably
better than TER, these scored close to equal on our
data, possibly because TERP?s stem, synonym, and
paraphrase matching are less useful when most of
the variation is syntactic.
The correlations with BLEU and METEOR are
lower than those reported in (Callison-Burch et al,
2007); in that study, BLEU achieved adequacy and
fluency correlations of 0.690 and 0.722, respec-
tively, and METEOR achieved 0.701 and 0.719. The
correlations for these metrics might be expected to
be lower for our data, since overall quality is higher,
making the metrics? task more difficult as the out-
puts involve subtler differences between acceptable
and unacceptable variation.
The human-targeted metrics (represented by the
prefixed H in the data tables) correlated even more
strongly with the human judgments, compared to the
non-targeted versions. HTER demonstrated the best
correlation with realizer fluency (r = 0.75).
For several kinds of acceptable variation involv-
ing the rearrangement of constituents (such as da-
tive shift), TERP gives a more reasonable score than
BLEU, due to its ability to directly evaluate phrasal
shifts. The following realization was rated 4.5 for
fluency, and was more correctly ranked by TERP
than BLEU:
(3) Ref: The deal also gave Mitsui access to
a high-tech medical product.
(4) Realiz.: The deal also gave access to a
high-tech medical product to Mitsui.
For each reference sentence, we compared the
ranking of its realizations induced from the human
scores to the ranking induced from the TERP score,
and counted the rank errors by the latter, infor-
mally categorizing them by error type (see Table
7). In the 50 sentences with the highest numbers of
rank errors, 17 were affected by punctuation differ-
ences, typically involving variation in comma place-
ment. Human fluency judgments of outputs with
only punctuation problems were generally high, and
many realizations with commas inserted or removed
were rated fully fluent by the annotators. However,
TERP penalizes such insertions or deletions. Agree-
ment errors are another frequent source of rank-
ing errors for TERP. The human judges tended to
harshly penalize sentences with number-agreement
or tense errors, whereas TERP applies only a single
substitution penalty for each such error. We expect
that with suitable optimization of edit weights to
avoid over-penalizing punctuation shifts and under-
penalizing agreement errors, TERP would exhibit an
even stronger correlation with human fluency judg-
ments.
None of the evaluation metrics can distinguish an
acceptable movement of a word or constituent from
an unacceptable movement, with only one reference
sentence. A substantial source of error for both
TERP and BLEU is variation in adverbial placement,
as shown in (7).
Similar errors are seen with prepositional phrases
and some commonly-occurring temporal adverbs,
which typically admit a number of variations in
placement. Another important example of accept-
able variation which these metrics do not generally
rank correctly is dative alternation:
569
(7)
Ref. We need to clarify what exactly is wrong with it.
Realiz. Flu. TERP BLEU
We need to clarify exactly what is wrong with it. 5 0.1 0.5555
We need to clarify exactly what ?s wrong with it. 5 0.2 0.4046
We need to clarify what , exactly , is wrong with it. 5 0.2 0.5452
We need to clarify what is wrong with it exactly. 4.5 0.1 0.6756
We need to clarify what exactly , is wrong with it. 4 0.1 0.7017
We need to clarify what , exactly is wrong with it. 4 0.1 0.7017
We needs to clarify exactly what is wrong with it. 3 0.103 0.346
(5) Ref. When test booklets were passed
out 48 hours ahead of time, she says she
copied questions in the social studies sec-
tion and gave the answers to students.
(6) Realiz. When test booklets were passed
out 48 hours ahead of time , she says she
copied questions in the social studies sec-
tion and gave students the answers.
The correlations of each of the metrics with the
human judgments of fluency for the realizer systems
indicate at least a moderate relationship, in contrast
with the results reported in (Stent et al, 2005) for
paraphrase data, which found an inverse correlation
for fluency, and (Cahill, 2009) for the output of a sur-
face realizer for German, which found only a weak
correlation. However, the former study employed
a corpus-based paraphrase generation system rather
than grammar-driven surface realizers, and the re-
sulting paraphrases exhibited much broader varia-
tion. In Cahill?s study, the outputs of the realizer
were almost always grammatically correct, and the
automated evaluation metrics were ranking marked-
ness instead of grammatical acceptability.
5.3 System-level comparisons
In order to investigate the efficacy of the metrics
in ranking different realizer systems, or competing
realizations from the same system generated using
different ranking models, we considered seven dif-
ferent ?systems? from the whole dataset of realiza-
tions. These consisted of five OpenCCG-based re-
alizations (the best realization from three baseline
models, and the best and the worst realization from
the full perceptron model), and two XLE-based sys-
tems (the best and the worst realization, after rank-
ing the outputs of the XLE realizer with an n-gram
model). The mean of the combined adequacy and
fluency scores of each of these seven systems was
compared with that of every other system, result-
ing in 21 pairwise comparisons. Then Tukey?s HSD
test was performed to determine the systems which
differed significantly in terms of the average ade-
quacy and fluency rating they received.4 The test
revealed five pairwise comparisons where the scores
were significantly different.
Subsequently, for each of these systems, an over-
all system-level score for each of the MT metrics
was calculated. For the five pairwise comparisons
where the adequacy-fluency group means differed
significantly, we checked whether the metric ranked
the systems correctly. Table 8 shows the results of
a pairwise comparison between the ranking induced
by each evaluation metric, and the ranking induced
by the human judgments. Five of the seven non-
targeted metrics correctly rank more than half of the
systems. NIST, METEOR, and GTM get the most
comparisons right, but neither NIST nor GTM cor-
rectly rank the OpenCCG-baseline model 1 with re-
spect to the XLE-best model. TER and TERP get two
of the five comparisons correct, and they incorrectly
rank two of the five OpenCCG model comparisons,
as well as the comparison between the XLE-worst
and OpenCCG-best systems.
For the targeted metrics, HNIST is correct for all
five comparisons, while neither HBLEU nor HME-
TEOR correctly rank all the OpenCCG models. On
the other hand, HTER and HGTM incorrectly rank the
XLE-best system versus OpenCCG-based models.
In summary, some of the metrics get some of the
rankings correct, but none of the non-targeted met-
rics get al of them correct. Moreover, different met-
rics make different ranking errors. This argues for
4This particular test was chosen since it corrects for multiple
post-hoc analyses conducted on the same data-set.
570
the use of multiple metrics in comparing realizer
systems.
6 Conclusion
Our study suggests that although the task of evalu-
ating the output from realizer systems differs from
the task of evaluating machine translations, the au-
tomatic metrics used to evaluate MT outputs deliver
moderate correlations with combined human fluency
and adequacy scores when used on surface realiza-
tions. We also found that the MT-evaluation met-
rics are useful in evaluating different versions of the
same realizer system (e.g., the various OpenCCG re-
alization ranking models), and finding cases where
a system is performing poorly. As in MT-evaluation
tasks, human-targeted metrics have the highest cor-
relations with human judgments overall. These re-
sults suggest that the MT-evaluation metrics are use-
ful for developing surface realizers. However, the
correlations are lower than those reported for MT
data, suggesting that they should be used with cau-
tion, especially for cross-system evaluation, where
consulting multiple metrics may yield more reliable
comparisons. In our study, the targeted version of
TERP correlated most strongly with human judg-
ments of fluency.
In future work, the performance of the TER family
of metrics on this data might be improved by opti-
mizing the edit weights used in computing its scores,
so as to avoid over-penalizing punctuation move-
ments or under-penalizing agreement errors, both
of which were significant sources of ranking errors.
Multiple reference sentences may also help mitigate
these problems, and the corpus of human-repaired
realizations that has resulted from our study is a step
in this direction, as it provides multiple references
for some cases. We expect the corpus to also prove
useful for feature engineering and error analysis in
developing better realization models.5
Acknowledgements
We thank Aoife Cahill and Tracy King for providing
us with the output of the XLE generator. We also
thank Chris Callison-Burch and the anonymous re-
viewers for their helpful comments and suggestions.
5The corpus can be downloaded from http://www.
ling.ohio-state.edu/?mwhite/data/emnlp10/.
This material is based upon work supported by
the National Science Foundation under Grant No.
0812297.
References
Jason Baldridge. 2002. Lexically Specified Derivational
Control in Combinatory Categorial Grammar. Ph.D.
thesis, University of Edinburgh.
S. Banerjee and A. Lavie. 2005. METEOR: An auto-
matic metric for MT evaluation with improved corre-
lation with human judgments. In Proceedings of the
ACL Workshop on Intrinsic and Extrinsic Evaluation
Measures for Machine Translation and/or Summariza-
tion, pages 65?72.
R. Barzilay and L. Lee. 2003. Learning to paraphrase:
An unsupervised approach using multiple-sequence
alignment. In proceedings of HLT-NAACL, volume
2003, pages 16?23.
Aoife Cahill. 2009. Correlating human and automatic
evaluation of a german surface realiser. In Proceed-
ings of the ACL-IJCNLP 2009 Conference Short Pa-
pers, pages 97?100, Suntec, Singapore, August. Asso-
ciation for Computational Linguistics.
C. Callison-Burch, M. Osborne, and P. Koehn. 2006. Re-
evaluating the role of BLEU in machine translation re-
search. In Proceedings of EACL, volume 2006, pages
249?256.
Chris Callison-Burch, Cameron Fordyce, Philipp Koehn,
Christof Monz, and Josh Schroeder. 2007. (meta-)
evaluation of machine translation. In StatMT ?07: Pro-
ceedings of the Second Workshop on Statistical Ma-
chine Translation, pages 136?158, Morristown, NJ,
USA. Association for Computational Linguistics.
C. Callison-Burch, T. Cohn, and M. Lapata. 2008. Para-
metric: An automatic evaluation metric for paraphras-
ing. In Proceedings of the 22nd International Con-
ference on Computational Linguistics-Volume 1, pages
97?104. Association for Computational Linguistics.
J. Carletta. 1996. Assessing agreement on classification
tasks: the kappa statistic. Computational linguistics,
22(2):249?254.
Dick Crouch, Mary Dalrymple, Ron Kaplan, Tracy King,
John Maxwell, and Paula Newman. 2008. Xle docu-
mentation. Technical report, Palo Alto Research Cen-
ter.
Philip Koenh. 2004. Statistical significance tests for ma-
chine translation evaluation. In Proceedings of the
2009 Conference on Empirical Methods in Natural
Language Processing.
J.R. Landis and G.G. Koch. 1977. The measurement of
observer agreement for categorical data. Biometrics,
33(1):159?174.
571
Chin-Yew Lin and Franz Josef Och. 2004. Orange: a
method for evaluating automatic evaluation metrics for
machine translation. In COLING ?04: Proceedings
of the 20th international conference on Computational
Linguistics, page 501, Morristown, NJ, USA. Associ-
ation for Computational Linguistics.
K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2001.
Bleu: a method for automatic evaluation of machine
translation. Technical report, IBM Research.
E. Reiter and A. Belz. 2009. An investigation into the
validity of some metrics for automatically evaluating
natural language generation systems. Computational
Linguistics, 35(4):529?558.
Stefan Riezler, Tracy H. King, Ronald M. Kaplan,
Richard Crouch, John T. III Maxwell, and Mark John-
son. 2002. Parsing the wall street journal using
a lexical-functional grammar and discriminative esti-
mation techniques. In Proceedings of 40th Annual
Meeting of the Association for Computational Lin-
guistics, pages 271?278, Philadelphia, Pennsylvania,
USA, July. Association for Computational Linguistics.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea
Micciulla, and John Makhoul. 2006. A study of trans-
lation edit rate with targeted human annotation. In In
Proceedings of Association for Machine Translation in
the Americas, pages 223?231.
M. Snover, N. Madnani, B.J. Dorr, and R. Schwartz.
2009. Fluency, adequacy, or HTER?: exploring dif-
ferent human judgments with a tunable MT metric.
In Proceedings of the Fourth Workshop on Statistical
Machine Translation, pages 259?268. Association for
Computational Linguistics.
Amanda Stent, Matthew Marge, and Mohit Singhai.
2005. Evaluating evaluation methods for generation in
the presence of variation. In Proceedings of CICLing.
J.P. Turian, L. Shen, and I.D. Melamed. 2003. Evalua-
tion of machine translation and its evaluation. recall
(C? R), 100:2.
Michael White and Rajakrishnan Rajkumar. 2009. Per-
ceptron reranking for CCG realization. In Proceedings
of the 2009 Conference on Empirical Methods in Nat-
ural Language Processing, pages 410?419, Singapore,
August. Association for Computational Linguistics.
Michael White. 2006. Efficient Realization of Coordi-
nate Structures in Combinatory Categorial Grammar.
Research on Language and Computation, 4(1):39?75.
572
Type System #Refs #Paraphrases Average Paraphrases/Ref #Exact Matches Adq Flu
Single output OpenCCG Baseline 1 296 296 1.0 72 4.17 3.65
OpenCCG Baseline 2 296 296 1.0 82 4.34 3.94
OpenCCG Baseline 3 296 296 1.0 76 4.31 3.86
OpenCCG Perceptron Best 296 1.0 1.0 112 4.37 4.09
OpenCCG Perceptron Worst 117 117 1.0 5 4.34 3.36
XLE Best 154 154 1.0 24 4.41 4.07
XLE Worst 157 157 1.0 13 4.08 3.73
Multiple output OpenCCG-Perceptron All 296 767 2.6 158 4.45 3.91
OpenCCG All 296 1131 3.8 162 4.20 3.61
XLE All 174 557 3.2 54 4.17 3.81
Wordnet Subsitutions 162 486 3.0 0 3.66 4.71
Realizer All 296 1628 5.0 169 4.16 3.63
All 296 2114 7.1 169 4.05 3.88
Table 1: Descriptive statistics
System Adq Flu
p(A) p(E) ? p(A) p(E) ?
OpenCCG-Abs 0.73 0.47 0.48 0.70 0.24 0.61
OpenCCG-Rel 0.76 0.47 0.54 0.76 0.34 0.64
XLE-Abs 0.68 0.42 0.44 0.69 0.27 0.58
XLE-Rel 0.73 0.45 0.50 0.69 0.37 0.50
Wordnet-Abs 0.57 0.25 0.43 0.77 0.66 0.33
Wordnet-Rel 0.74 0.34 0.61 0.73 0.60 0.33
Realizer-Abs 0.70 0.44 0.47 0.69 0.24 0.59
Realizer-Rel 0.74 0.41 0.56 0.73 0.33 0.60
All-Abs 0.67 0.38 0.47 0.71 0.29 0.59
All-Rel 0.74 0.36 0.60 0.75 0.34 0.63
Table 2: Corpora-wise inter-annotator agreement (absolute and relative ? values shown)
Sys N B M G TP TA T HT HN HB HM HG
OpenCCG-Adq 0.27 0.39 0.35 0.18 0.39 0.34 0.4 0.43 0.3 0.43 0.43 0.23
OpenCCG-Flu 0.49 0.55 0.4 0.42 0.6 0.46 0.6 0.72 0.58 0.69 0.57 0.53
XLE-Adq 0.52 0.51 0.55 0.31 0.5 0.5 0.5 0.52 0.47 0.51 0.61 0.4
XLE-Flu 0.56 0.56 0.48 0.37 0.55 0.5 0.55 0.61 0.54 0.61 0.51 0.51
Wordnet-Adq 0.17 0.14 0.24 0.15 0.37 0.26 0.22 0.64 0.52 0.56 0.32 0.6
Wordnet-Flu 0.26 0.21 0.24 0.24 0.22 0.27 0.26 0.34 0.32 0.34 0.3 0.34
Realizer-Adq 0.47 0.6 0.57 0.42 0.59 0.57 0.6 0.62 0.49 0.62 0.65 0.48
Realizer-Flu 0.51 0.62 0.52 0.5 0.63 0.53 0.64 0.75 0.59 0.73 0.65 0.63
All-Adq 0.37 0.37 0.33 0.32 0.42 0.31 0.43 0.53 0.44 0.48 0.44 0.45
All-Flu 0.21 0.62 0.51 0.32 0.61 0.55 0.6 0.7 0.33 0.71 0.62 0.48
Table 3: Spearman?s correlations among NIST (N), BLEU (B), METEOR (M), GTM (G), TERp (TP), TERpa (TA),
TER (T), human variants (HN, HB, HM, HT, HG) and human judgments (-Adq: adequacy and -Flu: Fluency); Scores
which fall within the 95 %CI of the best are italicized.
Sys N B M G TP TA T HT HN HB HM HG
OpenCCG 0.49 0.57 0.42 0.4 0.61 0.46 0.62 0.73 0.58 0.7 0.59 0.51
XLE 0.63 0.64 0.59 0.39 0.62 0.58 0.63 0.69 0.6 0.68 0.63 0.54
Wordnet 0.21 0.14 0.21 0.19 0.38 0.25 0.23 0.65 0.56 0.57 0.31 0.63
Realizer 0.55 0.68 0.57 0.5 0.68 0.58 0.69 0.78 0.61 0.77 0.7 0.63
All 0.34 0.58 0.47 0.38 0.61 0.48 0.61 0.75 0.48 0.73 0.61 0.58
Table 4: Spearman?s correlations among NIST (N), BLEU (B), METEOR (M), GTM (G), TERp (TP), TERpa (TA),
TER (T), human variants (HN, HB, HM, HT, HG) and human judgments (combined adequacy and fluency scores)
573
System Adq Flu
Sp 95%L 95%U Sp 95%L 95%U
Realizer 0.60 0.58 0.63 0.62 0.59 0.65
XLE 0.51 0.47 0.56 0.56 0.51 0.61
OpenCCG 0.39 0.35 0.42 0.55 0.52 0.59
All 0.37 0.34 0.4 0.62 0.6 0.64
Wordnet 0.14 0.06 0.21 0.21 0.13 0.28
Table 5: Spearman?s correlation analysis (bootstrap sampling) of the BLEU scores of various systems with human
adequacy and fluency scores
Sys HJ N B M G TP TA T HT HN HB HM HG HJ1-HJ2
OpenCCG HJ-1 0.44 0.52 0.39 0.36 0.56 0.43 0.58 0.75 0.58 0.72 0.62 0.52 0.76
HJ-2 0.5 0.58 0.43 0.4 0.62 0.46 0.63 0.7 0.55 0.68 0.56 0.49
XLE HJ-1 0.6 0.6 0.55 0.37 0.57 0.55 0.58 0.69 0.63 0.68 0.64 0.54 0.75
HJ-2 0.6 0.6 0.56 0.39 0.6 0.55 0.61 0.64 0.54 0.61 0.57 0.51
Wordnet HJ-1 0.2 0.18 0.26 0.16 0.37 0.28 0.24 0.7 0.59 0.64 0.35 0.65 0.72
HJ-2 0.25 0.16 0.23 0.19 0.37 0.25 0.25 0.59 0.52 0.51 0.32 0.56
Realizer HJ-1 0.51 0.65 0.56 0.49 0.64 0.56 0.66 0.8 0.62 0.78 0.72 0.64 0.82
HJ-2 0.55 0.68 0.56 0.5 0.67 0.57 0.68 0.74 0.58 0.73 0.66 0.6
All HJ-1 0.32 0.53 0.45 0.37 0.57 0.44 0.57 0.77 0.5 0.74 0.62 0.59 0.79
HJ-2 0.35 0.58 0.46 0.37 0.61 0.47 0.6 0.71 0.44 0.69 0.57 0.54
Table 6: Spearman?s correlations of NIST (N), BLEU (B), METEOR (M), GTM (G), TERp (TP), TERpa (TA), human
variants (HT, HN, HB, HM, HG), and individual human judgments (combined adq. and flu. scores)
Factor Count
Punctuation 17
Adverbial shift 16
Agreement 14
Other shifts 8
Conjunct rearrangement 8
Complementizer ins/del 5
PP shift 4
Table 7: Factors influencing TERP ranking errors for 50 worst-ranked realization groups
Metric Score Errors
nist 4 C1-XB
bleu 3 XB-PW C1-XB
meteor 4 XW-PB
ter 2 PW-PB XW-PB C1-PB
terp 2 PW-PB XW-PB C1-PB
terpa 3 XW-PB C1-PB
gtm 4 C1-XB
hnist 5
hbleu 3 PW-PB XW-PB
hmeteor 2 PW-PB XW-PB C1-PB
hter 3 XB-PW C1-XB
hgtm 3 XB-PW C1-XB
Table 8: Metric-wise ranking performance in terms of agreement with a ranking induced by combined adequacy and
fluency scores; each metric gets a score out of 5 (i.e. number of system-level comparisons that emerged significant as
per the Tukey?s HSD test)
Legend: Perceptron Best (PB); Perceptron Worst (PW); XLE Best (XB); XLE Worst (XW); OpenCCG baseline mod-
els 1 to 3 (C1 ... C3)
574
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 486?496,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
A Word Reordering Model for Improved Machine Translation
Karthik Visweswariah
IBM Research India
Bangalore, India
v-karthik@in.ibm.com
Rajakrishnan Rajkumar
Dept. of Linguistics
Ohio State University
raja@ling.osu.edu
Ankur Gandhe
IBM Research India
Bangalore, India
ankugand@in.ibm.com
Ananthakrishnan Ramanathan
IBM Research India
Bangalore, India
aramana2@in.ibm.com
Jiri Navratil
IBM T.J. Watson Research Center
Yorktown Heights, New York
jiri@us.ibm.com
Abstract
Preordering of source side sentences has
proved to be useful in improving statistical
machine translation. Most work has used a
parser in the source language along with rules
to map the source language word order into
the target language word order. The require-
ment to have a source language parser is a ma-
jor drawback, which we seek to overcome in
this paper. Instead of using a parser and then
using rules to order the source side sentence
we learn a model that can directly reorder
source side sentences to match target word or-
der using a small parallel corpus with high-
quality word alignments. Our model learns
pairwise costs of a word immediately preced-
ing another word. We use the Lin-Kernighan
heuristic to find the best source reordering ef-
ficiently during training and testing and show
that it suffices to provide good quality reorder-
ing.
We show gains in translation performance
based on our reordering model for translating
from Hindi to English, Urdu to English (with
a public dataset), and English to Hindi. For
English to Hindi we show that our technique
achieves better performance than a method
that uses rules applied to the source side En-
glish parse.
1 Introduction
Languages differ in the way they order words to pro-
duce sentences representing the same meaning. Ma-
chine translation systems need to reorder words in
the source sentence to produce fluent output in the
target language that preserves the meaning of the
source sentence.
Current phrase based machine translation systems
can capture short range reorderings via the phrase
table. Even the capturing of these local reordering
phenomena is constrained by the amount of training
data available. For example, if adjectives precede
nouns in the source language and follow nouns in the
target language we still need to see a particular ad-
jective noun pair in the parallel corpus to handle the
reordering via the phrase table. Phrase based sys-
tems also rely on the target side language model to
produce the right target side order. This is known
to be inadequate (Al-Onaizan and Papineni, 2006),
and this inadequacy has spurred various attempts to
overcome the problem of handling differing word
order in languages.
One approach is through distortion models, that
try to model which reorderings are more likely
than others. The simplest models just penalize
long jumps in the source sentence when producing
the target sentence. These models have also been
generalized (Al-Onaizan and Papineni, 2006; Till-
man, 2004) to allow for lexical dependencies on the
source. While these models are simple, and can
be integrated with the decoder they are insufficient
to capture long-range reordering phenomena espe-
cially for language pairs that differ significantly.
The weakness of these simple distortion models
has been overcome using syntax of either the source
or target sentence (Yamada and Knight, 2002; Gal-
ley et al, 2006; Liu et al, 2006; Zollmann and Venu-
gopal, 2006). While these methods have shown to
be useful in improving machine translation perfor-
486
mance they generally involve joint parsing of the
source and target language which is significantly
more computationally expensive when compared to
phrase based translation systems. Another approach
that overcomes this weakness, is to to reorder the
source sentence based on rules applied on the source
parse (either hand written or learned from data) both
when training and testing (Collins et al, 2005; Gen-
zel, 2010; Visweswariah et al, 2010).
In this paper we propose a novel method for deal-
ing with the word order problem that is efficient and
does not rely on a source or target side parse being
available. We cast the word ordering problem as a
Traveling Salesman Problem (TSP) based on previ-
ous work on word-based and phrased-based statis-
tical machine translation (Tillmann and Ney, 2003;
Zaslavskiy et al, 2009). Words are the cities in the
TSP and the objective is to learn the distance be-
tween words so that the shortest tour corresponds to
the ordering of the words in the source sentence in
the target language. We show that the TSP distances
for reordering can be learned from a small amount
of high-quality word alignment data by means of
pairwise word comparisons and an informative fea-
ture set involving words and part-of-speech (POS)
tags adapted and extended from prior work on de-
pendency parsing (McDonald et al, 2005b). Ob-
taining high-quality word alignments that we require
for training is fairly easy compared with obtaining a
treebank required to obtain parses for use in syntax
based methods.
We show experimentally that our reordering
model, even when used to reorder sentences for
training and testing (rather than being used as an
additional score in the decoder) improves machine
translation performance for: Hindi ? English, En-
glish?Hindi, and Urdu? English. Although Urdu
is similar to Hindi from the point of reordering phe-
nomena we include it in our experiments since there
are publicly available datasets for Urdu-English. For
English ? Hindi we obtained better machine trans-
lation performance with our reordering model as
compared to a method that uses reordering rules ap-
plied to the source side parse.
The rest of the paper is organized as follows. Sec-
tion 2 reviews related work and places our work in
context. Section 3 outlines reordering issues due
to syntactic differences between Hindi and English.
Section 4 presents our reordering model, Section 5
presents experimental results and Section 6 presents
our conclusions and possible future work.
2 Related work
There have been several studies demonstrating im-
proved machine translation performance by reorder-
ing source side sentences based on rules applied to
the source side parse during training and decoding.
Much of this work has used hand written rules and
several language pairs have been studied e.g German
to English (Collins et al, 2005), Chinese to English
(Wang et al, 2007), English to Hindi (Ramanathan
et al, 2009), English to Arabic (Badr et al, 2009)
and Japanese to English (Lee et al, 2010). There
have also been some studies where the rules are
learned from the data (Genzel, 2010; Visweswariah
et al, 2010; Xia and McCord, 2004). In addition
there has been work (Yamada and Knight, 2002;
Zollmann and Venugopal, 2006; Galley et al, 2006;
Liu et al, 2006) which uses source and/or target
side syntax in a Context Free Grammar framework
which results in machine translation decoding being
considered as a parsing problem. In this paper we
propose a model that does not require either source
or target side syntax while also preserving the effi-
ciency of reordering techniques based on rules ap-
plied to the source side parse.
In work that is closely related to ours, (Tromble
and Eisner, 2009) formulated word reordering as a
Linear Ordering Problem (LOP), an NP-hard permu-
tation problem. They learned LOP model weights
capable of assigning a score to every possible per-
mutation of the source language sentence from an
aligned corpus by using a averaged perceptron learn-
ing model. The key difference between our model
and the model in (Tromble and Eisner, 2009) is that
while they learn costs of a word wi appearing any-
where before wj , we learn costs of wi immediately
preceding wj . This results in more compact models
and (as we show in Section 5) better models.
Our model results in us having to solve a TSP
instance. The relation between the TSP and ma-
chine translation decoding has been explored before.
(Knight, 1999) showed that TSP is a sub-class ofMT
decoding and thus established that the latter is NP-
hard. (Zaslavskiy et al, 2009) casts phrase-based
487
decoding as a TSP and they show favorable speed
performance trade-offs compared with Moses, an
existing state-of-the-art decoder. In (Tillmann and
Ney, 2003), a beam-search algorithm used for TSP
is adapted to work with an IBM-4 word-based model
and phrase-based model respectively. As opposed
to calculating TSP distances from existing machine
translation components ( viz. the translation, dis-
tortion and language model probabilities) we learn
model weights to reorder source sentences to match
target word order using an informative feature set
adapted from graph-based dependency parsing (Mc-
Donald et al, 2005a).
3 Hindi-English reordering issues
This section provides a brief survey of constructions
that the two languages in question differ as well as
have in common. (Ramanathan et al, 2009) notes
the following divergences:
? English follows SVO order while Hindi follows
SOV order
? English uses prepositions while Hindi uses
post-positions
? Hindi allows greater word order freedom
? Hindi has a relatively richer case-marking sys-
tem
In addition to these differences, (Visweswariah et
al., 2010) mention the similarity in word order in
the case of adjective noun sequences (some books
vs. kuch kitab).
4 Reordering model
Consider a source sentence w consisting of a se-
quence of n words w1, w2, ... wn that we would
like to reorder into the target language order. Given
a permutation pi of the indices 1..n, let the candi-
date reordering be wpi1 , wpi2 , ..., wpin . Thus, pii de-
notes the index of the word in the source sentence
that maps to position i in the candidate reordering.
Clearly there are n! such permutations. Our reorder-
ing model assigns costs to candidate permutations
as:
C(pi|w) =
?
i
c(pii?1, pii).
The cost c(m,n) can be thought of as the cost of the
word at index m immediately preceding the word
with index n in the candidate reordering. In this pa-
per, we parametrize the costs as:
c(m,n) = ?T?(w,m, n),
where ? is a learned vector of weights and ? is a
vector of feature functions.
Given a source sentence w we reorder it accord-
ing to the permutation pi that minimizes the cost
C(pi|w). Thus, we would like our cost function
C(pi|w) to be such that the correct reordering pi? has
the lowest cost of all possible reorderings pi. In Sec-
tion 4.1 we describe the features ? that we use, and
in Section 4.2 we describe how we train the weights
? to obtain a good reordering model.
Given our model structure, the minimization
problem that we need to solve is identical to solving
a Asymmetric Traveling Salesman Problem (ATSP)
with each word corresponding to a city, and the costs
c(m,n) representing the pairwise distances between
the cities. Consider the following example:
English input: John eats apples
Hindi: John seba(apples) khaataa hai(eats)
Desired reordered English: John apples eats
The ATSP that we need to solve is represented
pictorially in Figure 1 with sample costs. Note that
we have one extra node numbered 0. We start and
end the tour at node 0, and this determines the first
word in the reordered sentence. In this example the
minimum cost tour is:
Start ? John ? apple ? eats
recovering the right reordering for translation into
Hindi.
Solving the ATSP (which is a well known NP hard
problem) efficiently is crucial for the efficiency of
our reordering model. To solve the ATSP, we first
convert the ATSP to a symmetric TSP and then use
the Lin-Kernighan heuristic as implemented in Con-
corde, a state-of-the-art TSP solver (Applegate et al,
2005). We also experimented with using the exact
TSP solver in Concorde but since it was slower and
did not improve performance we preferred using the
Lin-Kernighan heuristic. To convert the ATSP to
a symmetric TSP we double the size of the orig-
inal problem creating a node N ? for every node
N in the original graph. Following (Hornik and
488
3apples
2eats1John
0Start
c(2,3)=3c(3,2)=1
c(0,3)=5c(3,0)=2
c(0,1)=-1c(1,0)= 5 c(1,3)=0
c(0,2)=5
c(1,2)=3c(2,1)=5
c(3,1)=5
c(2,0)=-2
Figure 1: Example of an ATSP for reordering the sen-
tence: John eats apples.
Hahsler, 2009), we then set new costs as follows:
c?(A,B) = ?, c?(A,B?) = c?(B? , A) = c(A,B)
and C(A,A?) = ??. Even with this doubling of
the number of nodes, we observed that solving the
TSPs with the Lin-Kernighan heuristic is very fast,
taking roughly 10 milliseconds per sentence on av-
erage. Overall, this means that our reordering model
is as fast as parsing and hence our model is compara-
ble in performance to techniques based on applying
rules to the parse tree.
4.1 Features
Since we would like to model reordering phenomena
which are largely related to analyzing the syntax of
the source sentence, we chose to use features based
on those that have in the past been used for parsing
(McDonald et al, 2005a). A subset of the features
we use was also used for reordering in (Tromble and
Eisner, 2009).
To be able to generalize from relatively small
amounts of data, we use features that in addition to
depending on the words in the input sentence w de-
pend on the part-of-speech (POS) tags of the words
in the input sentence. All features ?(w, i, j) we use
are binary features, that fire based on the identities
of the words and POS tags at or surrounding posi-
tions i and j in the source sentence. The first set of
feature templates we use are given in Table 1. These
features depend only on the identities of the word
and POS tag of the two positions i and j and we call
wi pi wj pj
? ? ? ?
?
?
?
?
? ?
? ?
? ?
? ?
? ?
? ?
? ? ?
Table 1: Bigram feature templates used to calculate the
cost that word at position i immediately precedes word at
position j in the target word order. wi (pi) denotes the
word (POS tag) at position i in the source sentence. Each
of the templates is also conjoined with i-j the signed dis-
tance between the two words in the source sentence.
these Bigram features.
The second set of feature templates we use are
given in Table 2. These features, in addition to ex-
amining positions i and j examine the surround-
ing positions. We instantiate these feature templates
separately for the POS tag sequence and for the
word sequence. We call these two feature sets Con-
textPOS and ContextWord respectively. When in-
stantiated with POS tags, the first row of Table 2
looks at all POS tags between positions i and j.
(Tromble and Eisner, 2009) use Bigram and Con-
textPOS features, while we extend their feature set
with the use of ContextWord features. Since Hindi
is verb final, in Hindi sentences with multiple verb
groups it is rare for words with a verb in between
to be placed together in the reordering to match En-
glish. Looking at the POS tags of words between
positions i and j allows us to penalize such reorder-
ings.
Each of the templates described in Table 1 and
Table 2 is also conjoined with i-j the signed dis-
tance between the two words in the source sentence.
The values of i-j between 5 and 10, and greater than
10 are quantized (negative values are similarly quan-
tized).
In Section 5.2 we report on experiments showing
the relative performance of these different feature
489
oi?1 oi oi+1 ob oj?1 oj oj+1
? ? ?
? ? ? ?
? ? ?
? ? ?
? ? ?
? ? ?
? ? ? ?
? ? ?
? ? ?
? ? ?
? ? ?
? ? ? ?
? ? ?
? ? ?
? ? ? ?
? ? ?
? ? ?
Table 2: Context feature templates used to calculate the
cost that word at position i immediately precedes word
at position j in the target word order. oi denotes the ob-
servation at position i in the source sentence and ob de-
notes an observation at a position between i and j (i.e
i + 1 ? b ? j ? 1). Each of the templates is instan-
tiated with the observation sequence o taken to be the
word sequence w and the POS tag sequence p. Each of
the templates is also conjoined with i-j the signed dis-
tance between the two positions in the source sentence.
types for the task of reordering Hindi sentences to
be in English word order.
4.2 Training
To train the weights ? in our model, we need a
collection of sentences, where we have the desired
reference reordering pi?(x) for each input sentence
x. To obtain these reference reorderings we use
word aligned source-target sentence pairs. The qual-
ity and consistency of these reference reorderings
will depend on the quality of the word alignments
that we use. Given word aligned source and tar-
get sentences, we drop the source words that are not
aligned. Let mi be the mean of the target word po-
sitions that the source word at index i is aligned to.
We then sort the source indices in increasing order
of mi. If mi = mj (for example, because wi and wj
are aligned to the same set of words) we keep them
in the same order that they occurred in the source
sentence. Obtaining the target ordering in this man-
ner, is certainly not the only possible way and we
would like to explore better treatment of this in fu-
ture work.
We used the single best Margin Infused Re-
laxed Algorithm (MIRA) ((McDonald et al, 2005b),
(Crammer and Singer, 2003)) with the online up-
dates to our parameters being given by
?i+1 = argmin
?
||? ? ?i||
s.t. C(pi?|w) < C(p?i|w)? L(pi?, p?i).
In the equation above,
p?i = argmin
pi
C(pi|x)
is the best reordering based on the current parameter
value and L is a loss function. We take the loss of
a reordering to be the number of words for which
the preceding word is wrong relative to the reference
target order.
We also experimented with the averaged percep-
tron algorithm (Collins, 2002), but found single best
MIRA to work slightly better and hence used MIRA
for all our experiments.
5 Experiments
In this section we report on experiments to evalu-
ate our reordering model. The first method we use
for evaluation (monolingual BLEU) is by generat-
ing the desired reordering of the source sentence (as
described in Section 4.2) and compare the reordered
output to this desired reordered sentence using the
BLEU metric. In addition, to these monolingual
BLEU results, we also evaluate (in Section 5.5) the
reordering by its effect on eventual machine transla-
tion performance.
We note that our reordering techniques uses POS
information for the input sentence. The POS taggers
used in this paper are Maximum Entropy Markov
models trained using manually annotated POS cor-
pora. For Hindi, we used roughly fifty thousand
words with twenty six tags from the corpus de-
scribed in (Dalal et al, 2007). For Urdu we used
roughly fifty thousand words and forty six tags from
the CRULP corpus (Hussain, 2008) and for English
we used the Wall Street Journal section of the Penn
Treebank.
490
5.1 Reordering model training data and
alignment quality
To train our reordering models we need training data
where we have the input source language sentence
and the desired reordering in the target language.
As described in Section 4.2 we derive the refer-
ence reordered sentence using word alignments. Ta-
ble 3 presents our monolingual BLEU results for
Hindi to English reordering as the source of the
word alignments is varied. All results in Table 3
are with Bigram and ContextPOS features. We have
word alignments from three sources: A small set
of hand aligned sentences, HMM alignments (Vo-
gel et al, 1996) and alignments obtained using a su-
pervised Maximum Entropy aligner (Ittycheriah and
Roukos, 2005) trained on the hand alignments. The
F-measure for the HMM alignments were 65% and
78% for the Maximum Entropy model alignments.
We see that the quality of the alignments is an im-
portant determiner of reordering performance. Row
1 shows the BLEU for unreordered (baseline) Hindi
compared with the Hindi sentences reordered in En-
glish Order. Using just HMM alignments to train
our model we do worse than unreordered Hindi. Al-
though using the Maximum Entropy alignments is
better than using HMM alignments, we do not im-
prove upon a small number of hand alignments by
using all the Maximum Entropy alignments.
To improve upon the model trained with only
hand alignments we selected a small number of snip-
pets of sentences from our Maximum Entropy align-
ments. The goal was to pick parts of sentences
where the alignment is reliable enough to use for
training. The heuristic we used in the selection of
snippets was to pick maximal snippets of at least
7 consecutive Hindi words with all Hindi words
aligned to a consecutive span of English words,
with no unaligned English words in the span and no
English words aligned to Hindi words outside the
span. Adding snippets selected with this heuristic
improves the reordering performance of our model
as seen in the last row of Table 3.
5.2 Feature set comparison
In this section we report on experiments to deter-
mine the performance of the different classes of fea-
tures (Bigram, ContextPos and ContextWord) dis-
HMM MaxEnt Hand BLEU
- - - 35.9
220K - - 35.4
- 220K - 47.0
- 220K 6K 48.4
- - 6K 49.0
- Good 17K 6K 51.3
Table 3: Monolingual BLEU scores for Hindi to English
reordering using models trained on different alignment
types and tested on a development set of 280 Hindi sen-
tences (5590 tokens).
Feature template
Bigram ContextPOS ContextWord BLEU
- - - 35.9
? - - 43.8
? ? - 49.0
? ? ? 51.3
Table 4: Monolingual BLEU scores for Hindi to En-
glish reordering using models trained with different fea-
ture sets and tested on a development set of 280 Hindi
sentences (5590 tokens).
cussed in Section 4.1. Table 4 shows monolingual
BLEU results for training with different features sets
for Hindi to English reordering. In all cases, we
use a set of 6000 sentence pairs which were hand
aligned to generate the training data. It is clear that
all three sets of features contribute to performance of
the reordering model, however the number of Con-
textWord features is larger than the number of Bi-
gram and ContextPOS features put together, and it
may be desirable to select from this set of features
especially when training on large amounts of data.
5.3 Monolingual reordering comparisons
Table 5 compares our reordering model with a reim-
plementation of the reordering model proposed in
(Tromble and Eisner, 2009). Both the models use
exactly the same features (bigram features and Con-
textPOS features) and are trained on the same data.
To generate our training data, for Hindi to English
and English to Hindi we use a set of 6000 hand
aligned sentences, for Urdu to English we use a set
of 8500 hand aligned sentences and for English to
French we use a set of 10000 hand aligned sentences
(a subset of Europarl and Hansards corpus). Our
491
Language pair Monolingual BLEU
Source Target Unreordered LOP TSP
Hindi English 35.9 36.6 49.0
English Hindi 34.4 48.4 56.7
Urdu English 35.6 39.5 49.9
English French 64.4 78.2 81.2
Table 5: Monolingual BLEU scores comparing the orig-
inal source order with desired target reorder without re-
ordering, and reordering using our model (TSP) and the
model proposed in (Tromble and Eisner, 2009) (LOP).
test data consisted of 280 sentences for Hindi to En-
glish and 400 sentences for all other language pairs
generated from hand aligned sentences. We include
English-French here to compare on a fairly similar
language pair with local reordering phenomena (the
main difference being that in French adjectives gen-
erally follow nouns). We note that our model outper-
forms the model proposed in (Tromble and Eisner,
2009) in all cases.
5.4 Analysis of reordering performance
To get a feel for the qualitative performance of our
reordering algorithm and the kind of phenomena it
is able to capture, we analyze the reordering per-
formance in terms of (i) whether the clause restruc-
turing is done correctly ? these can be thought of
as medium-to-long range reorderings, (ii) whether
clause boundaries are respected, and (iii) whether lo-
cal (short range) reordering is performed correctly.
The following analysis is for Hindi to English re-
ordering with the best model (this is also the model
used for Machine Translation experiments reported
on in Section 5.5).
? Clause structure: As discussed in Section 3,
the canonical clause order in Hindi is SOV,
while in English it is SVO. However, variations
on this structure are possible and quite frequent
(e.g., clauses with two objects). To evaluate
clause restructuring, we compared sequences
of subjects, objects and verbs in the output and
reference reorderings.
We had a set of 70 sentences annotated with
subject, direct object, indirect object and verb
information ? these annotations were made on
the head word of each phrase, and the compar-
isons were on sequences of these words alone
and not the entire constituent phrase. 52 sen-
tences were reordered by the model to match
the order of the corresponding reference. Eight
sentences were ordered correctly but differently
from the reference, because the reference was
expressed in non-canonical fashion (e.g., in the
passive) ? note that these cases negatively im-
pact the monolingual BLEU score. The follow-
ing example shows a sentence being reordered
correctly, where, however, the reference is ex-
pressed differently (note the position of the
subject ?policy? (niiti) in the reference and the
reordered output) 1:
Input: aba1 (now) taka2 (till) aisii3 (this) niiti4
(policy) kabhii5 (ever) nahii6 (not) rahii7 (has)
hai8 (been)
Reordered: taka2 (till) aba1 (now) aisii3 (this)
niiti4 (policy) hai8 (been) kabhii5 (ever) nahii6
(not) rahii7 (has)
Reference: taka2 (till) aba1 (now) aisii3 (this)
kabhii5 (ever) nahii6 (not) rahii7 (has) hai8
(been) niiti4 (policy)
English: Till now this never has been the policy
The remaining ten sentences were reordered in-
correctly. These errors are largely in clauses
which deviate from the SVO order in some
way ? clauses with multiple subjects or objects,
clauses with no object, etc.. For example, the
following sentence with two subjects and ob-
jects corresponding to the verb wearing has not
been reordered correctly.
Input: sabhii1 (all) purusha2 (men) safeda3
(white) evama4 (and) mahilaaen5 (women)
kesariyaa6 (saffron) vastra7 (clothes) dhaarana8
(wear) kiye9 hue10 (-ing) thiin11 (were)
Reordered: sabhii1 (all) purusha2 (men)
safeda3 (white) evama4 (and) mahilaaen5
(women) kesariyaa6 (saffron) vastra7 (clothes)
dhaarana8 (wear) thiin11 (were) kiye9 hue10 (-
ing)
Reference: sabhii1 (all) purusha2 (men)
thiin11 (were) dhaarana8 (wear) kiye9 hue10 (-
1The numeric subscripts in the examples indicate word po-
sitions in the input.
492
ing) safeda3 (white) evama4 (and) mahilaaen5
(women) kesariyaa6 (saffron)
English: All men were wearing white and the
women saffron
The model possibly needs more data with pat-
terns that deviate from the standard SOV order
to learn to reorder them correctly. We could
also add to the model, features pertaining to
subject, object, etc.
? Clause boundaries: Measured on a set of
844 sentences which were marked with clause
boundaries, 37 sentences (4.4 %) had reorder-
ings that violated these boundaries. An exam-
ple of such a clause-boundary violation is be-
low:
Input: main1 (I) sarakaara2 (government) kaa3
(of) dhyaana4 (attention) maananiiya5 (hon-
ourable) pradhaana6 (prime) mantri7 (min-
ister) dvaaraa8 (by) isa9 (this) sabhaa10
(house) me11 (in) kiye12 gaye13 (made) isa14
(this) vaade15 (promise) ki16 ora17 (towards)
dilaanaa18 (to bring) chaahuungaa19 (would
like)
Reordered: main1 (I) chahuungaa19 (would
like) dilaanaa18 (to bring) kii16 ora17 (to-
wards) isa9 (this) vaade15 (promise) kiye12
gaye13 (made) dvaaraa8 (by) maananiiya5
(honourable) mantri7 (minister) pradhaana6
(prime) dhyaana4 (attention) kaa3 (of)
sarakaara2 (government) men11 (in) isa14 (this)
sabhaa10 (house)
Reference: main1 (I) chahuungaa19 (would
like) dilaanaa18 (to bring) dhyaana4 (attention)
kaa3 (of) sarakaara2 (government) kii16 ora17
(towards) isa9 (this) vaade15 (promise) kiye12
gaye13 (made) dvaaraa8 (by) maananiiya5
(honourable) mantri7 (minister) pradhaana6
(prime) men11 (in) isa9 (this) sabhaa10 (house)
English I would like to bring the attention of
the government towards this promise made by
the honourable prime minister in this house.
Note how the italicized clause, which is kept
together in the reference, is split up incorrectly
in the reordered output. The proportion of such
boundary violations is, however, quite low, be-
cause Hindi being a verb-final language, most
clauses end with a verb and it is probably quite
straightforward for the model to keep clauses
separate. A clause boundary detection program
should make it possible to eliminate the re-
maining errors.
? Local reordering: To estimate the short range
reordering performance, we consider how of-
ten different POS bigrams in the input are re-
ordered correctly. Here, we expect the model
to reorder prepositions correctly, and to avoid
any reordering that moves apart nouns and their
adjectival pre-modifiers or components of com-
pound nouns (see Section 3). Table 6 sum-
marizes the reordering performance for these
categories for a set of 280 sentences (same as
the test set used in Section 5.1). Each row
in Table 6 indicates the total number of cor-
rect instances for the pair, i.e., the number of
instances of the pair in the reference (column
titled Total), the number of instances that al-
ready appear in the correct order in the input
(column Input), and the number that are or-
dered correctly by the reordering model (col-
umn Reordered). The first two rows show that
adjective-noun and noun-noun (compounds)
are in most cases correctly retained in the orig-
inal order by the model. The final row shows
that while many prepositions have been moved
into their correct positions, there are still quite a
few mismatches with the reference. An impor-
tant reason why this happens is that nouns mod-
ified by prepositional phrases can often also be
expressed as noun compounds. For example,
vidyuta (electricity) kii (of) aavashyakataaen
(requirements) in Hindi can be expressed either
as ?requirements of electricity? or ?electricity
requirements?. The latter expression results in
a match with the input (explaining many of the
104 correct orders in the input) and a mismatch
with the model?s reordering. The same problem
in the training data would also adversely impact
the learning of the preposition reordering rule.
493
POS pair Total Input Reordered
adj-noun 234 192 196
noun-noun 46 44 42
prep-noun 436 104 250
Table 6: An analyis of reordering for a few POS bigrams
5.5 Machine translation results
We now present experiments in incorporating the re-
ordering model in machine translation systems. For
all results presented here, we reorder the training and
test data using the single best reordering based on
our reordering model for each sentence. For each of
the language pairs we evaluated, we trained Direct
Translation Model 2 (DTM) systems (Ittycheriah
and Roukos, 2007) with and without reordering and
compared performance on test data. We note that the
DTM system includes features that allow it to model
lexicalized reordering phenomena. The reordering
window size was set to +/-8 words for both the base-
line and our reordered input. In our experiments, we
left the word alignments fixed, i.e we reordered the
existing word alignments rather than realigning the
sentences after reordering. Redoing the word align-
ments with the reordered data could potentially give
further small improvements. We note that we ob-
tained better baseline performance using DTM sys-
tems than the standard Moses/Giza++ pipeline (e.g
we obtained a BLEU of 14.9 for English to Hindi
with a standard Moses/Giza++ pipeline). For all of
our systems we used a combination of HMM (Vo-
gel et al, 1996) and MaxEnt alignments (Ittycheriah
and Roukos, 2005).
For our Hindi-English experiments we use a train-
ing set of roughly 250k sentences (5.5Mwords) con-
sisting of the Darpa-TIDES dataset (Bojar et al,
2010) and an internal dataset from several domains
but dominated by news. Our test set was roughly
1.2K sentences from the news domain with a sin-
gle reference. To train our reordering model, we
used roughly 6K alignments plus 17K snippets se-
lected from MaxEnt alignments as described in Sec-
tion 5.1 with bigram, ContextPOS and ContextWord
features. The monolingual reordering BLEU (on the
same data reported on in Section 5.3) was 54.0 for
Hindi to English and 60.8 for English to Hindi.
For our Urdu-English experiments we used 70k
Language pair BLEU
Source Target Unreordered Reordered
Hindi English 14.7 16.7
Urdu English 23.3 24.8
English Hindi 20.7 22.5
Table 7: Translation performance without reordering
(baseline) compared with performance after preordering
with our reordering model.
sentences from the NIST MT-08 training corpus
and used the MT-08 eval set for testing. We note
that the MT-08 eval set has four references as com-
pared to one reference for our Hindi-English test
set. This largely explains the improved baseline per-
formance for Urdu-English as compared to Hindi-
English. We present averaged results for the Web
and News part of the test sets. To train the reorder-
ing model we used 9K hand alignments and 11K
snippets extracted from MaxEnt alignments as de-
scribed in Section 5.1 with bigram, ContextPOS and
ContextWord context feature. The monolingual re-
ordering BLEU for the reordering model thus ob-
tained (on the same data reported on in Section 5.3)
was 52.7.
Table 7 shows that for Hindi to English, English
to Hindi and for Urdu to English we see a gain
of 1.5 - 2 BLEU points. For English ? Hindi
we also experimented with a system that uses rules
(learned from the data using the methods described
in (Visweswariah et al, 2010)) applied to a parse to
reorder source side English sentences. This system
had a BLEU score of 21.2, which is an improvement
over the baseline, but our reordering model is better
by 1.3 BLEU points.
An added benefit of our reordering model is that
the decoder can be run with a smaller search space
exploring only a small amount of reordering with-
out losing accuracy but running substantially faster.
Table 8 shows the variation in machine Hindi to En-
glish translation performance with varying skip size
(this parameter sets the maximum number of words
skipped during decoding, lower values are associ-
ated with a restricted decoder search space and in-
creased speed).
494
skip Unreordered Reordered
2 12.2 16.7
4 13.4 16.7
8 14.7 16.4
Table 8: Translation performance with/without reorder-
ing with varying decoder search space.
6 Conclusion and future work
In this paper we presented a reordering model to
reorder source language data to make it resemble
the target language word order without using either
a source or target parser. We showed consistent
gains of up to 2 BLEU points in machine transla-
tion performance using this model to preorder train-
ing and test data. We show better performance com-
pared to syntax based reordering rules for English
to Hindi translation. Our model used only a part of
speech tagger (sometimes trained with fairly small
amounts of data) and a small corpus of word align-
ments. Considering the fact that treebanks required
to build high quality parsers are costly to obtain, we
think that our reordering model is a viable alterna-
tive to using syntax for reordering. We also note,
that with the preordering based on our reordering
model we can achieve the best BLEU scores with
a much tighter search space in the decoder. Even ac-
counting for the cost of finding the best reordering
according to our model, this usually results in faster
processing than if we did not have the reordering in
place.
In future work we plan to explore using more data
from automatic alignments, perhaps by considering
a joint model for aligning and reordering. We would
also like to explore doing away with the requirement
of having a POS tagger, using completely unsuper-
vised methods to class words. We currently only
look at word pairs in calculating the loss function
used in MIRA updates. We would like to investigate
the use of other loss functions and their effect on re-
ordering performance. We also would like to explore
whether the use of scores from our reordering model
directly in machine translation systems can improve
performance relative to using just the single best re-
ordering.
References
Yaser Al-Onaizan and Kishore Papineni. 2006. Dis-
tortion models for statistical machine translation. In
Proceedings of ACL, ACL-44, pages 529?536, Mor-
ristown, NJ, USA. Association for Computational Lin-
guistics.
David L. Applegate, Robert E. Bixby, Vasek Chvatal, and
William J. Cook. 2005. Concorde tsp solver. In
http://www.tsp.gatech.edu/.
Ibrahim Badr, Rabih Zbib, and James Glass. 2009. Syn-
tactic phrase reordering for English-to-Arabic statisti-
cal machine translation. In Proceedings of EACL.
Ondrej Bojar, Pavel Stranak, and Daniel Zeman. 2010.
Data issues in English-to-Hindi machine translation.
In LREC.
Michael Collins, Philipp Koehn, and Ivona Kuc?erova?.
2005. Clause restructuring for statistical machine
translation. In Proceedings of ACL, pages 531?540,
Morristown, NJ, USA. Association for Computational
Linguistics.
Michael Collins. 2002. Discriminative training meth-
ods for hidden Markov models: theory and experi-
ments with perceptron algorithms. In Proceedings of
EMNLP.
K. Crammer and Y. Singer. 2003. Ultraconservative on-
line algorithms for multiclass problems. Journal of
Machine Learning Research.
Aniket Dalal, Kumar Nagaraj, Uma Sawant, Sandeep
Shelke, and Pushpak Bhattacharyya. 2007. Building
feature rich pos tagger for morphologically rich lan-
guages: Experiences in Hindi. In Proceedings of In-
ternational Conference on Natural Language Process-
ing.
M. Galley, J. Graehl, K. Knight, D. Marcu, S. DeNeefe,
W. Wang, and I. Thayer. 2006. Scalable inference and
training of context-rich syntactic translation models.
In Proceedings of ACL.
D. Genzel. 2010. Automatically learning source-side re-
ordering rules for large scale machine translation. In
Proceedings of the 23rd International Conference on
Computational Linguistics.
Kurt Hornik and Michael Hahsler. 2009. TSP?
infrastructure for the traveling salesperson problem.
Journal of Statistical Software, 23(i02).
Sarmad Hussain. 2008. Resources for Urdu language
processing. In Proceedings of the 6th Workshop on
Asian Language Resources, IJCNLP?08.
Abraham Ittycheriah and Salim Roukos. 2005. A max-
imum entropy word aligner for Arabic-English ma-
chine translation. In Proceedings of HLT/EMNLP,
HLT ?05, pages 89?96, Stroudsburg, PA, USA. Asso-
ciation for Computational Linguistics.
495
Abraham Ittycheriah and Salim Roukos. 2007. Direct
translation model 2. In Proceedings of HLT-NAACL,
pages 57?64.
Kevin Knight. 1999. Decoding complexity in word-
replacement translation models. Comput. Linguist.,
25:607?615, December.
Young-Suk Lee, Bing Zhao, and Xiaoqian Luo. 2010.
Constituent reordering and syntax models for English-
to-Japanese statistical machine translation. In COL-
ING.
Y. Liu, Q. Liu, and S. Lin. 2006. Tree-to-String align-
ment template for statistical machine translation. In
Proceedings of ACL.
R. McDonald, K. Crammer, and F. Pereira. 2005a. On-
line large-margin training of dependency parsers. In
Proceedings of ACL.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Hajic?. 2005b. Non-projective dependency pars-
ing using spanning tree algorithms. In Proceedings of
HLT.
Ananthakrishnan Ramanathan, Hansraj Choudhary,
Avishek Ghosh, and Pushpak Bhattacharyya. 2009.
Case markers and morphology: addressing the crux
of the fluency problem in English-Hindi smt. In
Proceedings of ACL-IJCNLP.
Christoph Tillman. 2004. A unigram orientation model
for statistical machine translation. In Proceedings of
HLT-NAACL.
Christoph Tillmann and Hermann Ney. 2003. Word re-
ordering and a dynamic programming beam search al-
gorithm for statistical machine translation. Computa-
tional Linguistics, 29(1):97?133.
Roy Tromble and Jason Eisner. 2009. Learning linear or-
dering problems for better translation. In Proceedings
of EMNLP.
Karthik Visweswariah, Jiri Navratil, Jeffrey Sorensen,
Vijil Chenthamarakshan, and Nandakishore Kamb-
hatla. 2010. Syntax based reordering with automat-
ically derived rules for improved statistical machine
translation. In Proceedings of the 23rd International
Conference on Computational Linguistics.
Stephan Vogel, Hermann Ney, and Christoph Tillmann.
1996. HMM-based word alignment in statistical trans-
lation. In Proceedings of the 16th conference on Com-
putational Linguistics.
Chao Wang, Michael Collins, and Philipp Koehn. 2007.
Chinese syntactic reordering for statistical machine
translation. In Proceedings of EMNLP-CoNLL.
Fei Xia and Michael McCord. 2004. Improving a sta-
tistical MT system with automatically learned rewrite
patterns. In COLING.
Kenji Yamada and Kevin Knight. 2002. A decoder for
syntax-based statistical mt. In Proceedings of ACL.
Mikhail Zaslavskiy, Marc Dymetman, and Nicola Can-
cedda. 2009. Phrase-based statistical machine transla-
tion as a traveling salesman problem. In Proceedings
of ACL-IJCNLP.
Andreas Zollmann and Ashish Venugopal. 2006. Syntax
augmented machine translation via chart parsing. In
Proceedings on the Workshop on Statistical Machine
Translation.
496
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 244?255, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Minimal Dependency Length in Realization Ranking
Michael White and Rajakrishnan Rajkumar
Department of Linguistics
The Ohio State University
Columbus, OH, USA
{mwhite,raja}@ling.osu.edu
Abstract
Comprehension and corpus studies have found
that the tendency to minimize dependency
length has a strong influence on constituent or-
dering choices. In this paper, we investigate
dependency length minimization in the con-
text of discriminative realization ranking, fo-
cusing on its potential to eliminate egregious
ordering errors as well as better match the dis-
tributional characteristics of sentence order-
ings in news text. We find that with a state-
of-the-art, comprehensive realization rank-
ing model, dependency length minimization
yields statistically significant improvements
in BLEU scores and significantly reduces
the number of heavy/light ordering errors.
Through distributional analyses, we also show
that with simpler ranking models, dependency
length minimization can go overboard, too of-
ten sacrificing canonical word order to shorten
dependencies, while richer models manage to
better counterbalance the dependency length
minimization preference against (sometimes)
competing canonical word order preferences.
1 Introduction
In this paper, we show that for the constituent or-
dering problem in surface realization, incorporating
insights from the minimal dependency length the-
ory of language production (Temperley, 2007) into a
discriminative realization ranking model yields sig-
nificant improvements upon a state-of-the-art base-
line. We demonstrate empirically using OpenCCG,
our CCG-based (Steedman, 2000) surface realiza-
tion system, the utility of a global feature encoding
the total dependency length of a given derivation.
Although other works in the realization literature
have used phrase length or head-dependent distances
in their models (Filippova and Strube, 2009; Velldal
and Oepen, 2005; White and Rajkumar, 2009, i.a.),
to the best of our knowledge, this paper is the first
to use insights from the minimal dependency length
theory directly and study their effects, both qualita-
tively and quantitatively.
The impetus for this paper was the discovery
that despite incorporating a sophisticated syntac-
tic model borrowed from the parsing literature?
including features with head-dependent distances at
various scales?White & Rajkumar?s (2009) real-
ization ranking model still often performed poorly
on weight-related decisions such as when to em-
ploy heavy-NP shift. Table 1 illustrates this point.
In wsj 0034.9, the full model (incorporating numer-
ous syntactic features) succeeds in reproducing the
reference sentence, which is clearly preferable to
the rather awkward variant selected by the base-
line model (using various n-gram models). How-
ever, in wsj 0013.16, the full model fails to shift the
temporal modifier for now next to the phrasal verb
turned down, leaving it at the end of its very long
verb phrase where it is highly ambiguous (with mul-
tiple intervening attachment sites). Conversely, in
wsj 0044.3, the full model shifts before next to the
verb, despite the NP cheating being very light, yield-
ing a very confusing ordering given that before is
meant to be intransitive.
The syntactic features in White & Rajku-
mar?s (2009) realization ranking model are taken
from Clark & Curran?s (2007) normal form model
244
wsj 0034.9 they fell into oblivion after the 1929 crash .
FULL [same]
BASELINE they fell after the 1929 crash into oblivion .
wsj 0013.16 separately , the Federal Energy Regulatory Commission [V P turned down for now [NP a request
by Northeast [V P seeking approval of [NP its possible purchase of PS of New Hampshire]]]] .
FULL separately , the Federal Energy Regulatory Commission [V P turned down [NP a request by North-
east [V P seeking approval of [NP its possible purchase of PS of New Hampshire]]] for now] .
wsj 0044.3 she had seen cheating before , but these notes were uncanny .
FULL she had seen before cheating , but these notes were uncanny .
Table 1: Examples of OpenCCG output with White & Rajkumar?s (2009) models?the first represents a successful
case, the latter two egregious ordering errors
(Table 3; see Section 3). In this model, head-
dependenct distances are considered in conjunc-
tion with lexicalized and unlexicalized CCG deriva-
tion steps, thereby appearing in numerous features.
As such, the model takes into account the inter-
action of dependency length with derivation steps,
but in essence does not consider the main ef-
fect of dependency length itself. In this light,
our investigation of dependency length minimiza-
tion can be viewed as examining the question of
whether realization ranking models can be made
more accurate?and in particular, avoid egregious
ordering errors?by incorporating a feature to ac-
count for the main effect of dependency length.
It is important to observe at this point that de-
pendency length minimization is more of a prefer-
ence than an optimization objective, which must be
balanced against other order preferences at times.
A closer reading of Temperley?s (2007) study re-
veals that dependency length can sometimes run
counter to many canonical word order choices. A
case in point is the class of examples involving
pre-modifying adjunct sequences that precede both
the subject and the verb. Assuming that their par-
ent head is the main verb of the sentence, a long-
short sequence would minimize overall dependency
length. However, in 613 examples found in the Penn
Treebank, the average length of the first adjunct was
3.15 words while the second adjunct was 3.48 words
long, thus reflecting a short-long pattern, as illus-
trated in the Temperley p.c. example in Table 2.
Apart from these, Hawkins (2001) shows that argu-
ments are generally located closer to the verb than
adjuncts. Gildea and Temperley (2007) also suggest
that adverb placement might involve cases which go
against dependency length minimization. An exam-
ination of 295 legitimate long-short post-verbal con-
stituent orders (counter to dependency length) from
Section 00 of the Penn Treebank revealed that tem-
poral adverb phrases are often involved in long-short
orders, as shown in wsj 0075.13 in Table 2. In our
setup, the preference to minimize dependency length
can be balanced by features capturing preferences
for alternate choices (e.g. the argument-adjunct dis-
tinction in our dependency ordering model, Table 4).
Via distributional analyses, we show that while sim-
pler realization ranking models can go overboard
in minimizing dependency length, richer models
largely succeed in overcoming this issue, while still
taking advantage of dependency length minimiza-
tion to avoid egregious ordering errors.
2 Background
2.1 Minimal Dependency Length
Comprehension and corpus studies (Gibson, 1998;
Gibson, 2000; Temperley, 2007) point to the ten-
dency of production and comprehension systems to
adhere to principles of dependency length minimiza-
tion. The idea of dependency length minimization
is based on Gibson?s (1998) Dependency Locality
Theory (DLT) of comprehension, which predicts
that longer dependencies are more difficult to pro-
cess. DLT predictions have been further validated
using comprehension studies involving eye-tracking
corpora (Demberg and Keller, 2008). DLT metrics
also correlate reasonably well with activation de-
cay over time expressed in computational models of
245
Temperley (p.c.) [In 1976], [as a film student at the Purchase campus of the State University of New York], Mr.
Lane, shot ...
wsj 0075.13 The Treasury also said it plans to sell [$ 10 billion] [in 36-day cash management bills] [on
Thursday].
Table 2: Counter-examples to dependency length minimization
comprehension (Lewis et al2006; Lewis and Va-
sishth, 2005).
Extending these ideas from comprehension, Tem-
perley (2007) poses the question: Does language
production reflect a preference for shorter dependen-
cies as well so as to facilitate comprehension? By
means of a study of Penn Treebank data, Temperley
shows that English sentences do display a tendency
to minimize the sum of all their head-dependent
distances as illustrated by a variety of construc-
tions. Further, Gildea and Temperley (2007) report
that random linearizations have higher dependency
lengths compared to actual English, while an ?opti-
mal? algorithm (from the perspective of dependency
length minimization), which places dependents on
either sides of a head in order of increasing length,
is closer to actual English. Tily (2010) also applies
insights from the above cited papers to show that
dependency length constitutes a significant pressure
towards language change. For head-final languages
(e.g., Japanese), dependency length minimization
results in the ?long-short? constituent ordering in
language production (Yamashita and Chang, 2001).
More generally, Hawkins?s (1994; 2000) processing
domains, dependency length minimization and end-
weight effects in constituent ordering (Wasow and
Arnold, 2003) are all very closely related. The de-
pendency length hypothesis goes beyond the predic-
tions made by Hawkins? Minimize Domains princi-
ple in the case of English clauses with three post-
verbal adjuncts: Gibson?s DLT correctly predicts
that the first constituent tends to be shorter than the
second, while Hawkins? approach does not make
predictions about the relative orders of the first two
constituents.
However, it would be very reductive to consider
dependency length minimization as the sole factor
in language production. In fact, a large body of
prior work discusses a variety of other factors in-
volved in language production. These other prefer-
ences are either correlated with dependency length
or can override the minimal dependency length pref-
erence. Complexity (Wasow, 2002; Wasow and
Arnold, 2003), animacy (Snider and Zaenen, 2006;
Branigan et al2008), information status consid-
erations (Wasow and Arnold, 2003; Arnold et al
2000), the argument-adjunct distinction (Hawkins,
2001) and lexical bias (Wasow and Arnold, 2003;
Bresnan et al2007) are a few prominent factors.
More recently, Anttila et al2010) argued that the
principle of end weight can be revised by calculat-
ing weight in prosodic terms to provide more ex-
planatory power. As Temperley (2007) suggests,
a satisactory model should combine insights from
multiple approaches, a theme which we investigate
in this work by means of a rich feature set adapted
from the parsing and realization literature. Our fea-
ture design has been inspired by the conclusions of
the above-cited works pertaining to the role of de-
pendency length minimization in syntactic choice
in conjuction with other factors influencing con-
stituent order. However, going beyond Temper-
ley?s corpus study, we confirm the utility of incor-
porating a feature for minimizing dependency length
into machine-learned models with hundreds of thou-
sands of features found to be useful in previous pars-
ing and realization work, and investigate the extent
to which these features can counterbalance a de-
pendency length minimization preference in cases
where canonical word order considerations should
prevail.
2.2 Surface Realization with Combinatory
Categorial Grammar (CCG)
We provide here a brief overview of CCG and the
OpenCCG realizer; for further details, see the works
cited below.
CCG (Steedman, 2000) is a unification-based
categorial grammar formalism defined almost en-
tirely in terms of lexical entries that encode sub-
246
Feature Type Example
LexCat + Word s/s/np + before
LexCat + POS s/s/np + IN
Rule sdcl ? np sdcl\np
Rule + Word sdcl ? np sdcl\np + bought
Rule + POS sdcl ? np sdcl\np + VBD
Word-Word ?company, sdcl ? np sdcl\np, bought?
Word-POS ?company, sdcl ? np sdcl\np, VBD?
POS-Word ?NN, sdcl ? np sdcl\np, bought?
Word + ?w ?bought, sdcl ? np sdcl\np? + dw
POS + ?w ?VBD, sdcl ? np sdcl\np? + dw
Word + ?p ?bought, sdcl ? np sdcl\np? + dp
POS + ?p ?VBD, sdcl ? np sdcl\np? + dp
Word + ?v ?bought, sdcl ? np sdcl\np? + dv
POS + ?v ?VBD, sdcl ? np sdcl\np? + dv
Table 3: Basic and dependency features from Clark &
Curran?s (2007) normal form model; distances are in in-
tervening words, punctuation marks and verbs, and are
capped at 3, 3 and 2, respectively
categorization as well as syntactic features (e.g.
number and agreement). OpenCCG is a pars-
ing/generation library which includes a hybrid
symbolic-statistical chart realizer (White, 2006;
White and Rajkumar, 2009). The input to the
OpenCCG realizer is a semantic graph, where each
node has a lexical predication and a set of seman-
tic features; nodes are connected via dependency re-
lations. Internally, such graphs are represented us-
ing Hybrid Logic Dependency Semantics (HLDS),
a dependency-based approach to representing lin-
guistic meaning (Baldridge and Kruijff, 2002). Al-
ternative realizations are ranked using integrated n-
gram or averaged perceptron scoring models. In the
experiments reported below, the inputs are derived
from the gold standard derivations in the CCGbank
(Hockenmaier and Steedman, 2007), and the outputs
are the highest-scoring realizations found during the
realizer?s chart-based search.1
3 Feature Design
In the realm of paraphrasing using tree lineariza-
tion, Kempen and Harbusch (2004) explore features
which have later been appropriated into classifica-
tion approaches for surface realization (Filippova
and Strube, 2007). Prominent features include in-
1The realizer can also be run using inputs derived from
OpenCCG?s parser, though informal experiments suggest that
parse errors tend to decrease generation quality.
formation status, animacy and phrase length. In the
case of ranking models for surface realization, by far
the most comprehensive experiments involving lin-
guistically motivated features are reported in work
of Cahill for German realization ranking (Cahill et
al., 2007; Cahill and Riester, 2009). Apart from
language model and Lexical Functional Grammar
(LFG) c-structure and f -structure based features,
Cahill also designed and incorporated features mod-
eling information status considerations.
The feature sets explored in this paper ex-
tend those in previous work on realization ranking
with OpenCCG using averaged perceptron models
(White and Rajkumar, 2009; Rajkumar et al2009;
Rajkumar and White, 2010) to include more com-
prehensive ordering features. The feature classes
are listed below, where DEPLEN, HOCKENMAIER
and DEPORD are novel, and the rest are as in ear-
lier OpenCCG models. The inclusion of the DE-
PORD features is intended to yield a model with a
similarly rich set of ordering features as Cahill and
Forster?s (2009) realization ranking model for Ger-
man. Except where otherwise indicated, features are
integer-valued, representing counts of occurrences
in a derivation.
DEPLEN The total of the length between all se-
mantic heads and dependents for a realization,
where length is in intervening words2 exclud-
ing punctuation. For length purposes, collapsed
named entities were counted as a single word in
the experiments reported here.
NGRAMS The log probabilities of the word se-
quence scored using three different n-gram
models: a trigram word model, a trigram
word model with named entity classes replac-
ing words, and a trigram model over POS tags
and supertags.
HOCKENMAIER As an extra component of the
generative baseline, the log probability of the
derivation according to (a reimplementation
2We also experimented with two other definitions of depen-
dency length described in the literature, namely (1) counting
only nouns and verbs to approximate counting by discourse ref-
erents (Gibson, 1998) and (2) omitting function words to ap-
proximate prosodic weight (Anttila et al2010); however, re-
alization ranking accuracy was slightly worse than counting all
non-punctuation words.
247
Feature Type Example
HeadBroadPos + Rel + Precedes + HeadWord + DepWord ?VB, Arg0, dep, wants, he?
. . . + HeadWord + DepPOS ?VB, Arg0, dep, wants, PRP?
. . . + HeadPOS + DepWord ?VB, Arg0, dep, VBZ, he?
. . . + HeadWord + DepPOS ?VB, Arg0, dep, VBZ, PRP?
HeadBroadPos + Side + DepWord1 + DepWord2 ?NN, left, an, important?
. . . + DepWord1 + DepPOS2 ?NN, left, an, JJ?
. . . + DepPOS1 + DepWord2 ?NN, left, DT, important?
. . . + DepPOS1 + DepPOS2 ?NN, left, DT, JJ?
. . . + Rel1 + Rel2 ?NN, left, Det, Mod?
Table 4: Basic head-dependent and sibling dependent ordering features
of) Hockenmaier?s (2003) generative syntactic
model.
DISCRIMINATIVE NGRAMS Sequences from each
of the n-gram models in the perceptron model.
AGREEMENT Features for subject-verb and ani-
macy agreement as well as balanced punctua-
tion.
C&C NF BASE The features from Clark & Cur-
ran?s (2007) normal form model, listed in Ta-
ble 3, minus the distance features.
C&C NF DISTANCE The distance features from
the C&C normal form model, where the dis-
tance between a head and its dependent is mea-
sured in intervening words, punctuation marks
or verbs; caps of 3, 3 and 2 (resp.) on the
distances have the effect of binning longer dis-
tances.
DEPORD Several classes of features for ordering
heads and dependents as well as sibling depen-
dents on the same side of the head. The ba-
sic features?using words, POS tags and de-
pendency relations, grouped by the broad POS
tag of the head?are shown in Table 4. There
are also similar features using words and a
word class (instead of words and POS tags),
where the class is either the named entity class,
COLOR for color words, PRO for pronouns,
one of 60-odd suffixes culled from the web, or
HYPHEN or CAP for hyphenated or capital-
ized words. Additionally, there are features for
detecting definiteness of an NP or PP (where
the definiteness value is used in place of the
POS tag).
Model # Alph Feats # Model Feats
GLOBAL 4 4
DEPLEN-GLOBAL 5 5
DEPORD-NONF 790,887 269,249
DEPORD-NODIST 1,035,915 365,287
DEPLEN-NODIST 1,035,916 366,094
DEPORD-NF 1,173,815 431,226
DEPLEN 1,173,816 428,775
Table 6: Model sizes?number of features in alphabet for
each model (satisfying count cutoff of 5) along with num-
ber active in model after 5 training epochs
4 Evaluation
4.1 Experimental Conditions
We followed the averaged perceptron training proce-
dure of White and Rajkumar (2009) with a couple of
updates. First, as noted earlier, we used a reimple-
mentation of Hockenmaier?s (2003) generative syn-
tactic model as an extra component of our genera-
tive baseline; and second, only five epochs of train-
ing were used, which was found to work as well as
using additional epochs on the development set. As
in the earlier work, the models were trained on the
standard training sections (02?21) of an enhanced
version of the CCGbank, using a lexico-grammar
extracted from these sections.
The models tested in the experiments reported be-
low are summarized in Table 5. The three groups
of models are designed to test the impact of the
dependency length feature when added to feature
sets of increasing complexity. In more detail,
the GLOBAL and DEPLEN-GLOBAL models contain
dense features on entire derivations; their values
are the log probabilities of the three n-gram mod-
248
Model Dep Ngram Hocken- Discr Agree- C&C NF C&C NF Dep
Len Mods maier Ngrams ment Base Dist Ord
GLOBAL N Y Y N N N N N
DEPLEN-GLOBAL Y Y Y N N N N N
DEPORD-NONF N Y Y Y Y N N Y
DEPORD-NODIST N Y Y Y Y Y N Y
DEPLEN-NODIST Y Y Y Y Y Y N Y
DEPORD-NF N Y Y Y Y Y Y Y
DEPLEN Y Y Y Y Y Y Y Y
Table 5: Legend for experimental conditions
els used in the earlier work along with the Hock-
enmaier model (and the dependency length feature,
in DEPLEN-GLOBAL). The second group is cen-
tered on DEPORD-NODIST, which contains all fea-
tures except the dependency length feature and the
distance features in Clark & Curran?s normal form
model, which may indirectly capture some depen-
dency length minimization preferences. In addition
to DEPLEN-NODIST?where the dependency length
feature is added?this group also contains DEPORD-
NONF, which is designed to test (as a side compari-
son) whether the Clark & Curran normal form base
features are still useful even when used in conjunc-
tion with the new dependency ordering features. In
the final group, DEPORD-NF contains all the features
examined in this paper except the dependency length
feature, while DEPLEN contains all the features in-
cluding the dependency length feature. Note that the
learned weight of the total dependency length fea-
ture was negative in each case, as expected.
Table 6 shows the sizes of the various models. For
each model, the alphabet?whose size increases to
over a million features?is the set of applicable fea-
tures found to have discriminative value in at least 5
training examples; from these, a subset are made ac-
tive (i.e., take on a non-zero weight) through percep-
tron updates when the feature value differs between
the model-best and oracle-best realization.
4.2 BLEU Results
Following the usual practice in the realization rank-
ing, we first evaluate our results quantitatively us-
ing exact matches and BLEU (Papineni et al2002),
a corpus similarity metric developed for MT evalu-
ation. Realization results for the development and
Model % Exact BLEU Signif
Sect 00
GLOBAL 33.03 0.8292 -
DEPLEN-GLOBAL 34.73 0.8345 ***
DEPORD-NONF 42.33 0.8534 **
DEPORD-NODIST 43.12 0.8560 -
DEPLEN-NODIST 43.87 0.8587 ***
DEPORD-NF 43.44 0.8590 -
DEPLEN 44.56 0.8610 **
Sect 23
GLOBAL 34.75 0.8302 -
DEPLEN-GLOBAL 34.70 0.8330 ***
DEPORD-NODIST 41.42 0.8561 -
DEPLEN-NODIST 42.95 0.8603 ***
DEPORD-NF 41.32 0.8577 -
DEPLEN 42.05 0.8596 **
Table 7: Development (Section 00) & test (Section 23)
set results?exact match percentage and BLEU scores,
along with statistical significance of BLEU compared to
the unmarked model in each group (* = p < 0.1, ** =
p < 0.05, *** = p < 0.01); significant within-group
winners (at p < 0.05) are shown in bold
test sections appear in Table 7. For all three model
groups, the dependency length feature yields signif-
icant increases in BLEU scores, even in compar-
ison to the model (DEPORD-NF) containing Clark
& Curran?s distance features in addition to the new
dependency ordering features (as well as all other
features but total dependency length). The second
group additionally shows that the Clark & Curran
normal form base features do indeed have a signif-
icant impact on BLEU scores even when used with
249
Model % DL % DL DL Signif
Lower Greater Mean
GOLD n.a. n.a. 41.02 -
GLOBAL 17.23 21.59 42.40 ***
DEPLEN-GLOBAL 24.37 12.81 40.29 ***
DEPORD-NONF 15.76 19.34 42.34 ***
DEPORD-NODIST 14.58 19.06 42.03 ***
DEPLEN-NODIST 17.75 14.82 40.87 n.s.
DEPORD-NF 14.96 17.65 41.58 ***
DEPLEN 16.28 14.78 40.97 n.s.
Table 8: Dependency length compared to corpus?
percentage of realizations with dependency length less
than and greater than gold standard, along with mean
dependency length, whose significance is tested against
gold; 1671 development set (Section 00) complete real-
izations analyzed
the new dependency ordering model, as DEPORD-
NONF is significantly worse than DEPORD-NODIST
(the impact of the distance features is evident in the
increases from the second group to the third group).
As with the dev set, the dependency length feature
yielded a significant increase in BLEU scores for
each comparison on the test set al
For each group, the statistical significance of the
difference in BLEU scores between a model and the
unmarked model (-) is determined by bootstrap re-
sampling (Koehn, 2004).3 Note that although the
differences in BLEU scores are small, they end
up being statistically significant because the mod-
els frequently yield the same top scoring realiza-
tion, and reliably deliver improvements in the cases
where they differ. In particular, note that DEPLEN
and DEPORD-NF agree on the best realization 81%
of the time, while DEPLEN-NODIST and DEPORD-
NODIST have 78.1% agreement, and DEPLEN-
GLOBAL and GLOBAL show 77.4% agreement; by
comparison, DEPORD-NODIST and GLOBAL only
agree on the best realization 51.1% of the time.
4.3 Detailed Analyses
The effect of the dependency length feature on the
distribution of dependency lengths is illustrated in
Table 8. The table shows the mean of the total de-
pendency length of each realized derivation com-
3Kudos to Kevin Gimpel for making his resampling
scripts available from http://www.ark.cs.cmu.edu/
MT/paired_bootstrap_v13a.tar.gz.
Model % Short % Long % Eq % Single
/ Long / Short Constit
GOLD 25.25 4.87 4.08 65.79
GLOBAL 23.15 7.86 3.94 65.04
DEPLEN-GLOBAL 24.58 5.57 4.09 65.76
DEPORD-NONF 23.13 6.61 4.03 66.23
DEPORD-NODIST 23.38 6.52 3.94 66.15
DEPLEN-NODIST 24.03 5.38 4.01 66.58
DEPORD-NF 23.74 5.92 3.96 66.40
DEPLEN 24.36 5.36 4.07 66.21
Table 9: Distribution of various kinds of post-verbal con-
stituents in the development set (Section 00); 4692 gold
cases considered
pared to the corresponding gold standard derivation,
as well as the number of derivations with greater and
lower dependency length. According to paired t-
tests, the mean dependency lengths for the DEPLEN-
NODIST and DEPLEN models do not differ signifi-
cantly from the gold standard. In contrast, the mean
dependency length of all the models that do not in-
clude the dependency length feature does differ sig-
nificantly (p < 0.001) from the gold standard. Ad-
ditionally, all these models have more realizations
with dependency length greater than the gold stan-
dard, in comparison to the dependency length min-
imizing models; this shows the efficacy of the de-
pendency length feature in approximating the gold
standard. Interestingly, the DEPLEN-GLOBAL model
significantly undershoots the gold standard on mean
dependency length, and has the most skewed dis-
tribution of sentences with greater vs. lesser depen-
dency length than the gold standard.
Apart from studying dependency length directly,
we also looked at one of the attested effects of de-
pendency length minimization, viz. the tendency to
prefer short-long post-verbal constituents in produc-
tion (Temperley, 2007). The relative lengths of ad-
jacent post-verbal constituents were computed and
their distribution is shown in Table 9. While cal-
culating length, punctuation marks were excluded.
Four kinds of constituents were found in the post-
verbal domain. For every verb, apart from single
constituents and equal length constituents, short-
long and long-short sequences were also observed.
Table 9 demonstrates that for both the gold standard
corpus as well as the realizer models, short-long
constituents were more frequent than long-short or
equal length constituents. This follows the trend re-
250
Model % Light % Heavy Signif
/ Heavy / Light
GOLD 8.60 0.36 -
GLOBAL 7.73 2.02 ***
DEPLEN-GLOBAL 8.35 0.75 **
DEPORD-NONF 7.98 1.15 ***
DEPORD-NODIST 8.04 1.12 ***
DEPLEN-NODIST 8.23 0.45 n.s.
DEPORD-NF 8.26 0.71 **
DEPLEN 8.36 0.51 n.s.
Table 10: Distribution of heavy unequal constituents
(length difference > 5) in Section 00; 4692 gold cases
considered and significance tested against the gold stan-
dard using a ?-square test
ported by previous corpus studies of English (Tem-
perley, 2007; Wasow and Arnold, 2003). The figures
reported here show the tendency of the DEPLEN*
models to be closer to the gold standard than the
other models, especially in the case of short-long
constituents.
We also performed an analysis of relative con-
stituent lengths focusing on light-heavy and heavy-
light cases; specifically, we examined unequal
length constituent sequences where the length dif-
ference of the constituents was greater than 5, and
the shorter constituent was under 5 words. Table 10
shows the results. Using a ?-square test, the distri-
bution of heavy unequal length constituent counts in
the DEPLEN-NODIST and DEPLEN models does not
significantly differ from that of the gold standard. In
contrast, for all the other models, the counts do dif-
fer significantly from the gold standard.
4.4 Examples
Table 11 shows examples of how the dependency
length feature (DEPLEN) affects the output even in
comparison to a model (DEPORD) with a rich set
of discriminative syntactic and dependency order-
ing features, but no features directly targeting rel-
ative weight. In wsj 0015.7, the dependency length
model produces an exact match, while the DEPORD
model fails to shift the short temporal adverbial next
year next to the verb, leaving a confusingly repeti-
tive this year next year at the end of the sentence.
In wsj 0020.1, the dependency length model pro-
duces a nearly exact match with just an equally ac-
ceptable inversion of closely watching. By contrast,
the DEPORD model mistakenly shifts the direct ob-
ject South Korea, Taiwan and Saudia Arabia to the
end of the sentence where it is difficult to under-
stand following two very long intervening phrases.
In wsj 0021.8, both models mysteriously put not in
front of the auxiliary and leave out the complemen-
tizer, but DEPORD also mistakenly leaves before at
the end of the verb phrase where it is again apt to
be interpreted as modifying the preceding verb. In
wsj 0075.13, both models put the temporal modi-
fier on Thursday in its canonical VP-final position,
despite this order running counter to dependency
length minimization. Finally, wsj 0014.2 shows a
case where DEPORD is nearly an exact match (except
for a missing comma), but the dependency length
model fronts the PP on the 12-member board, where
it is grammatical but rather marked (and not moti-
vated in the discourse context).
4.5 Interim Discussion
The experiments show a consistent positive effect of
the dependency length feature in improving BLEU
scores and achieving a better match with the corpus
distributions of dependency length and short/long
constituent orders. The results in Table 10 are partic-
ulary encouraging, as they show that minimizing de-
pendency length reduces the number of realizations
in which a heavy constituent precedes a light one
down to essentially the level of the corpus, thereby
eliminating many realizations that can be expected
to have egregious errors like those shown in Ta-
ble 11.
Intriguingly, there is some evidence that a nega-
tively weighted total dependency length feature can
go too far in minimizing dependency length, in the
absence of other informative features to counterbal-
ance it. In particular, the DEPLEN-GLOBAL model in
Table 8 has significantly lower dependency length
than the corpus, but in the richer models with dis-
criminative synactic and dependency ordering fea-
tures, there are no significant differences. It may still
be though that additional features are necessary to
counteract the tendency towards dependency length
minimization, for example to ensure that initial con-
stituents play their intended role in establishing and
continuing topics in discourse, as also observed in
Table 11.
251
wsj 0015.7 the exact amount of the refund will be determined next year based on actual collections made until
Dec. 31 of this year .
DEPLEN [same]
DEPORD the exact amount of the refund will be determined based on actual collections made until Dec. 31
of this year next year .
wsj 0020.1 the U.S. , claiming some success in its trade diplomacy , removed South Korea , Taiwan and Saudi
Arabia from a list of countries it is closely watching for allegedly failing to honor U.S. patents ,
copyrights and other intellectual-property rights .
DEPLEN the U.S. claiming some success in its trade diplomacy , removed South Korea , Taiwan and Saudi
Arabia from a list of countries it is watching closely for allegedly failing to honor U.S. patents ,
copyrights and other intellectual-property rights .
DEPORD the U.S. removed from a list of countries it is watching closely for allegedly failing to honor U.S.
patents , copyrights and other intellectual-property rights , claiming some success in its trade diplo-
macy , South Korea , Taiwan and Saudi Arabia .
wsj 0021.8 but he has not said before that the country wants half the debt forgiven .
DEPLEN but he not has said before ? the country wants half the debt forgiven .
DEPORD but he not has said ? the country wants half the debt forgiven before .
wsj 0075.13 The Treasury also said it plans to sell [$ 10 billion] [in 36-day cash management bills] [on Thurs-
day].
DEPLEN [same]
DEPORD [same]
wsj 0014.2 they succeed Daniel M. Rexinger , retired Circuit City executive vice president , and Robert R.
Glauber , U.S. Treasury undersecretary , on the 12-member board .
DEPORD they succeed Daniel M. Rexinger , retired Circuit City executive vice president , and Robert R.
Glauber , U.S. Treasury undersecretary ? on the 12-member board .
DEPLEN on the 12-member board they succeed Daniel M. Rexinger , retired Circuit City executive vice
president , and Robert R. Glauber , U.S. Treasury undersecretary .
Table 11: Examples of realized output for full models with and without the dependency length feature
4.6 Targeted Human Evaluation
To determine whether heavy-light ordering differ-
ences often represent ordering errors (including
egregious ones), rather than simply representing ac-
ceptable variation, we conducted a targeted human
evaluation on examples of this kind. Specifically,
for each of the DEPLEN* models and their corre-
sponding models without the dependency length fea-
ture, we chose the 25 sentences from the develop-
ment section whose realizations exhibited the great-
est difference in dependency length between sibling
constituents appearing in opposite orders, and asked
two judges (not the authors) to choose which of the
two realizations best expressed the meaning of the
reference sentence in a grammatical and fluent way,
with the choice forced (2AFC). Table 12 shows the
results. Agreement between the judges was high,
Model % Preferred % Agr Signif
GLOBAL 22 - -
DEPLEN-GLOBAL 78 84 ***
DEPORD-NODIST 24 - -
DEPLEN-NODIST 76 92 ***
DEPORD-NF 26 - -
DEPLEN 74 96 ***
Table 12: Targeted human evaluation?percentage of re-
alizations preferred by two human judges in a 2AFC test
among the 25 development set sentences with the great-
est differences in dependency length, with a binomial test
for significance
252
with only one disagreement on the realizations from
the DEPLEN and DEPORD-NF models (involving an
acceptable paraphrase in our judgment), and only
four disagreements on the DEPLEN-GLOBAL and
GLOBAL realizations. Pooling the judgments, the
preference for the DEPLEN* models was well above
the chance level of 50% according to a binomial test
(p < 0.001 in each case). Inspecting the data our-
selves, we found that many of the items did indeed
involve egregious ordering errors that the DEPLEN*
models managed to avoid.
5 Related Work
As noted in the introduction, to the best of our
knowledge this paper is the first to examine the im-
pact of dependency length minimization on realiza-
tion ranking. While there have been quite a few
papers to date reporting results on Penn Treebank
data, since the various systems make different as-
sumptions regarding the specificity of their inputs,
all but the most broad-brushed comparisons remain
impossible at present, and thus detailed studies such
as the present one can only be made within the con-
text of different models for the same system. Some
progress on this issue has been made in the con-
text of the Generation Challenges Surface Realiza-
tion Shared Task (Belz et al2011), but it remains
to be seen to what extent fair cross-system compar-
isons using common inputs can be achieved.
For (very) rough comparison purposes, Table 13
lists our results in the context of those reported for
various other systems on PTB Section 23. As the
table shows, the OpenCCG scores are quite com-
petitive, exceeded only by Callaway?s (2005) ex-
tensively hand-crafted system as well as Bohnet et
al.?s (2011) system on shared task shallow inputs
(-S), which performs much better than their sys-
tem on deep inputs (-D) that more closely resemble
OpenCCG?s.
6 Conclusions
In this paper, we have investigated dependency
length minimization in the context of realization
ranking, focusing on its potential to eliminate egre-
gious ordering errors as well as better match the dis-
tributional characteristics of sentence orderings in
news text. When added to a state-of-the-art, com-
System Coverage BLEU % Exact
Callaway (05) 98.5% 0.9321 57.5
Bohnet et al (11) 100% 0.8911
OpenCCG (12) 97.1% 0.8596 42.1
OpenCCG (09) 97.1% 0.8506 40.5
Ringger et al04) 100% 0.836 35.7
Bohnet et al (11) 100% 0.7943
Langkilde-Geary (02) 83% 0.757 28.2
Guo et al08) 100% 0.7440 19.8
Hogan et al07) ?100% 0.6882
OpenCCG (08) 96.0% 0.6701 16.0
Nakanishi et al05) 90.8% 0.7733
Table 13: PTB Section 23 BLEU scores and exact match
percentages in the NLG literature (Nakanishi et al re-
sults are for sentences of length 20 or less)
prehensive realization ranking model, we showed
that including a dense, global feature for minimiz-
ing total dependency length yields statistically sig-
nificant improvements in BLEU scores and signif-
icantly reduces the number of heavy-light ordering
errors. Going beyond the BLEU metric, we also
conducted a targeted human evaluation to confirm
the utility of the dependency length feature in mod-
els of varying richness. Interestingly, even with the
richest model, in some cases we found that the de-
pendency length feature still appears to go too far in
minimizing dependency length, suggesting that fur-
ther counter-balancing features?especially ones for
the sentence-initial position (Filippova and Strube,
2009)?warrant investigation in future work.
Acknowledgments
This work was supported in part by NSF grants no.
IIS-1143635 and IIS-0812297. We thank the anony-
mous reviewers for helpful comments and discus-
sion, and Scott Martin and Dennis Mehay for their
participation in the targeted human evaluation.
253
References
Arto Anttila, Matthew Adams, and Mike Speriosu. 2010.
The role of prosody in the English dative alternation.
Language and Cognitive Processes.
Jennifer E. Arnold, Thomas Wasow, Anthony Losongco,
and Ryan Ginstrom. 2000. Heaviness vs. newness:
The effects of structural complexity and discourse sta-
tus on constituent ordering. Language, 76:28?55.
Jason Baldridge and Geert-Jan Kruijff. 2002. Coupling
CCG and Hybrid Logic Dependency Semantics. In
Proc. ACL-02.
Anja Belz, Mike White, Dominic Espinosa, Eric Kow,
Deirdre Hogan, and Amanda Stent. 2011. The
first surface realisation shared task: Overview and
evaluation results. In Proceedings of the Genera-
tion Challenges Session at the 13th European Work-
shop on Natural Language Generation, pages 217?
226, Nancy, France, September. Association for Com-
putational Linguistics.
Bernd Bohnet, Simon Mille, Beno??t Favre, and Leo Wan-
ner. 2011. <stumaba >: From deep representation to
surface. In Proceedings of the Generation Challenges
Session at the 13th European Workshop on Natural
Language Generation, pages 232?235, Nancy, France,
September. Association for Computational Linguis-
tics.
H Branigan, M Pickering, and M Tanaka. 2008. Con-
tributions of animacy to grammatical function assign-
ment and word order during production. Lingua,
118(2):172?189.
Joan Bresnan, Anna Cueni, Tatiana Nikitina, and R. Har-
ald Baayen. 2007. Predicting the Dative Alternation.
Cognitive Foundations of Interpretation, pages 69?94.
Aoife Cahill and Arndt Riester. 2009. Incorporating in-
formation status into generation ranking. In Proceed-
ings of, ACL-IJCNLP ?09, pages 817?825, Morris-
town, NJ, USA. Association for Computational Lin-
guistics.
Aoife Cahill, Martin Forst, and Christian Rohrer. 2007.
Designing features for parse disambiguation and real-
isation ranking. In Miriam Butt and Tracy Holloway
King, editors, Proceedings of the 12th International
Lexical Functional Grammar Conference, pages 128?
147. CSLI Publications, Stanford.
Charles Callaway. 2005. The types and distributions
of errors in a wide coverage surface realizer evalua-
tion. In Proceedings of the 10th European Workshop
on Natural Language Generation.
Stephen Clark and James R. Curran. 2007. Wide-
Coverage Efficient Statistical Parsing with CCG and
Log-Linear Models. Computational Linguistics,
33(4):493?552.
Vera Demberg and Frank Keller. 2008. Data from eye-
tracking corpora as evidence for theories of syntactic
processing complexity. Cognition, 109(2):193?210.
Katja Filippova and Michael Strube. 2007. Generating
constituent order in German clauses. In ACL 2007,
Proceedings of the 45th Annual Meeting of the As-
sociation for Computational Linguistics, June 23-30,
2007, Prague, Czech Republic. The Association for
Computer Linguistics.
Katja Filippova and Michael Strube. 2009. Tree lin-
earization in English: Improving language model
based approaches. In Proceedings of Human Lan-
guage Technologies: The 2009 Annual Conference of
the North American Chapter of the Association for
Computational Linguistics, Companion Volume: Short
Papers, pages 225?228, Boulder, Colorado, June. As-
sociation for Computational Linguistics.
Edward Gibson. 1998. Linguistic complexity: Locality
of syntactic dependencies. Cognition, 68:1?76.
Edward Gibson. 2000. Dependency locality theory:
A distance-based theory of linguistic complexity. In
Alec Marantz, Yasushi Miyashita, and Wayne O?Neil,
editors, Image, Language, brain: Papers from the First
Mind Articulation Project Symposium. MIT Press,
Cambridge, MA.
Daniel Gildea and David Temperley. 2007. Optimizing
grammars for minimum dependency length. In Pro-
ceedings of the 45th Annual Meeting of the Association
of Computational Linguistics, pages 184?191, Prague,
Czech Republic, June. Association for Computational
Linguistics.
Yuqing Guo, Josef van Genabith, and Haifeng Wang.
2008. Dependency-based n-gram models for general
purpose sentence realisation. In Proc. COLING-08.
John A. Hawkins. 1994. A Performance Theory of Order
and Constituency. Cambridge University Press, New
York.
John A. Hawkins. 2000. The relative order of
prepositional phrases in English: Going beyond
manner-place-time. Language Variation and Change,
11(03):231?266.
John A. Hawkins. 2001. Why are categories adjacent?
Journal of Linguistics, 37:1?34.
Julia Hockenmaier and Mark Steedman. 2007. CCG-
bank: A Corpus of CCG Derivations and Dependency
Structures Extracted from the Penn Treebank. Com-
putational Linguistics, 33(3):355?396.
Julia Hockenmaier. 2003. Data and models for statis-
tical parsing with Combinatory Categorial Grammar.
Ph.D. thesis, University of Edinburgh.
Deirdre Hogan, Conor Cafferkey, Aoife Cahill, and Josef
van Genabith. 2007. Exploiting multi-word units
in history-based probabilistic generation. In Proc.
EMNLP-CoNLL.
254
Gerard Kempen and Karin Harbusch. 2004. Generat-
ing natural word orders in a semi-free word order lan-
guage: Treebank-based linearization preferences for
German. In Alexander F. Gelbukh, editor, CICLing,
volume 2945 of Lecture Notes in Computer Science,
pages 350?354. Springer.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Dekang Lin and
Dekai Wu, editors, Proceedings of EMNLP 2004,
pages 388?395, Barcelona, Spain, July. Association
for Computational Linguistics.
Irene Langkilde-Geary. 2002. An empirical verification
of coverage and correctness for a general-purpose sen-
tence generator. In Proc. INLG-02.
R. L. Lewis and S. Vasishth. 2005. An activation-based
model of sentence processing as skilled memory re-
trieval. Cognitive Science, 29:1?45, May.
Richard L. Lewis, Shravan Vasishth, and Julie Van Dyke.
2006. Computational principles of working memory
in sentence comprehension. Trends in Cognitive Sci-
ences, 10(10):447?454.
Hiroko Nakanishi, Yusuke Miyao, and Jun?ichi Tsujii.
2005. Probabilistic methods for disambiguation of an
HPSG-based chart generator. In Proc. IWPT-05.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic eval-
uation of machine translation. In Proc. ACL-02.
Rajakrishnan Rajkumar and Michael White. 2010. De-
signing agreement features for realization ranking.
In Coling 2010: Posters, pages 1032?1040, Beijing,
China, August. Coling 2010 Organizing Committee.
Rajakrishnan Rajkumar, Michael White, and Dominic
Espinosa. 2009. Exploiting named entity classes in
CCG surface realization. In Proceedings of Human
Language Technologies: The 2009 Annual Conference
of the North American Chapter of the Association for
Computational Linguistics, Companion Volume: Short
Papers, pages 161?164, Boulder, Colorado, June. As-
sociation for Computational Linguistics.
Eric Ringger, Michael Gamon, Robert C. Moore, David
Rojas, Martine Smets, and Simon Corston-Oliver.
2004. Linguistically informed statistical models of
constituent structure for ordering in sentence realiza-
tion. In Proc. COLING-04.
Neal Snider and Annie Zaenen. 2006. Animacy and syn-
tactic structure: Fronted NPs in English. In M. Butt,
M. Dalrymple, and T.H. King, editors, Intelligent Lin-
guistic Architectures: Variations on Themes by Ronald
M. Kaplan. CSLI Publications, Stanford.
Mark Steedman. 2000. The Syntactic Process. MIT
Press.
David Temperley. 2007. Minimization of dependency
length in written English. Cognition, 105(2):300 ?
333.
Harry Tily. 2010. The Role of Processing Complexity
in Word Order Variation and Change. Ph.D. thesis,
Stanford University.
Erik Velldal and Stefan Oepen. 2005. Maximum entropy
models for realization ranking. In Proc. MT-Summit
X.
Thomas Wasow and Jennifer Arnold. 2003. Post-verbal
Constituent Ordering in English. Mouton.
Tom Wasow. 2002. Postverbal Behavior. CSLI Publica-
tions, Stanford.
Michael White and Rajakrishnan Rajkumar. 2009. Per-
ceptron reranking for CCG realization. In Proceedings
of the 2009 Conference on Empirical Methods in Nat-
ural Language Processing, pages 410?419, Singapore,
August. Association for Computational Linguistics.
Michael White. 2006. Efficient Realization of Coordi-
nate Structures in Combinatory Categorial Grammar.
Research on Language & Computation, 4(1):39?75.
Hiroko Yamashita and Franklin Chang. 2001. ?Long
before short? preference in the production of a head-
final language. Cognition, 81.
255
Coling 2008: Proceedings of the workshop on Grammar Engineering Across Frameworks, pages 17?24
Manchester, August 2008
A More Precise Analysis of Punctuation for
Broad-Coverage Surface Realization with CCG
Michael White and Rajakrishnan Rajkumar
Department of Linguistics
The Ohio State University
Columbus, OH, USA
{mwhite,raja}@ling.osu.edu
Abstract
This paper describes a more precise anal-
ysis of punctuation for a bi-directional,
broad coverage English grammar extracted
from the CCGbank (Hockenmaier and
Steedman, 2007). We discuss various ap-
proaches which have been proposed in
the literature to constrain overgeneration
with punctuation, and illustrate how as-
pects of Briscoe?s (1994) influential ap-
proach, which relies on syntactic features
to constrain the appearance of balanced
and unbalanced commas and dashes to ap-
propriate sentential contexts, is unattrac-
tive for CCG. As an interim solution
to constrain overgeneration, we propose
a rule-based filter which bars illicit se-
quences of punctuation and cases of im-
properly unbalanced apposition. Using
the OpenCCG toolkit, we demonstrate
that our punctuation-augmented grammar
yields substantial increases in surface re-
alization coverage and quality, helping to
achieve state-of-the-art BLEU scores.
1 Introduction
In his pioneering monograph, Nunberg (1990) ar-
gues that punctuation is a systematic module of the
grammar of written text and is governed by princi-
ples and constraints like other sub-systems such as
syntax or phonology. Since then, others including
Briscoe (1994) and Doran (1998) have explored
ways of including rules and representations for
punctuation marks in broad coverage grammars. In
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
computational systems, punctuation provides dis-
ambiguation cues which can help parsers arrive at
the correct parse. From a natural language gener-
ation standpoint, text without punctuation can be
difficult to comprehend, or even misleading.
In this paper, we describe a more precise analy-
sis of punctuation for a bi-directional, broad cover-
age English grammar extracted from the CCGbank
(Hockenmaier and Steedman, 2007). In contrast to
previous work, which has been primarily oriented
towards parsing, our goal has been to develop an
analysis of punctuation that is well suited for both
parsing and surface realization. In addition, while
Briscoe and Doran have simply included punctu-
ation rules in their manually written grammars,
our approach has been to revise the CCGbank it-
self with punctuation categories and more precise
linguistic analyses, and then to extract a grammar
from the enhanced corpus.
In developing our analysis, we illustrate how as-
pects of Briscoe?s (1994) approach, which relies
on syntactic features to constrain the appearance
of balanced and unbalanced commas and dashes to
appropriate sentential contexts, is unattractive for
CCG, with its more flexible handling of word or-
der. Consequently, as an interim solution, we have
chosen to identify and filter undesirable configu-
rations when scoring alternative realizations. We
also point to other ways in which punctuation con-
straints could be incorporated into the grammar,
for exploration in future work.
Using the OpenCCG toolkit, we demonstrate
that our punctuation-enhanced grammar yields
substantial increases in surface realization quality,
helping to achieve state-of-the-art BLEU scores.
We use non-blind testing to evaluate the efficacy
of the grammar, and blind-testing to evaluate its
performance on unseen data. The baseline models
17
are (1) a grammar which has lexicalized punctu-
ation categories only for conjunction and apposi-
tion, and (2) one which has punctuation categories
corresponding to the existing treatment of punctua-
tion in the corpus. Non-blind testing results shown
a nearly 9-point increase in BLEU scores com-
pared to the best baseline model using oracle n-
grams, as well as a 40% increase in exact matches.
Blind testing results show a more than 5.5-point
increase in BLEU scores, contributing to an all-
sentences score of 0.7323 on Section 23 with over
96% coverage.
2 Background
CCG (Steedman, 2000) is a unification-based cat-
egorial grammar formalism which is defined al-
most entirely in terms of lexical entries that encode
sub-categorization information as well as syntactic
feature information (e.g. number and agreement).
Complementing function application as the stan-
dard means of combining a head with its argument,
type-raising and composition support transparent
analyses for a wide range of phenomena, includ-
ing right-node raising and long distance dependen-
cies. Semantic composition happens in parallel
with syntactic composition, which makes it attrac-
tive for generation.
OpenCCG is a parsing/generation library which
works by combining lexical categories for words
using CCG rules and multi-modal extensions on
rules (Baldridge, 2002) to produce derivations.
Surface realization is the process by which logical
forms are transduced to strings. OpenCCG uses
a hybrid symbolic-statistical chart realizer (White,
2006) which takes logical forms as input and pro-
duces sentences by using CCG combinators to
combine signs. Alternative realizations are ranked
using integrated n-gram scoring.
To illustrate the input to OpenCCG, consider
the semantic dependency graph in Figure 1. In
the graph, each node has a lexical predication
(e.g. make.03) and a set of semantic features (e.g.
?NUM?sg); nodes are connected via dependency
relations (e.g. ?ARG0?). Internally, such graphs
are represented using Hybrid Logic Dependency
Semantics (HLDS), a dependency-based approach
to representing linguistic meaning (Baldridge and
Kruijff, 2002). In HLDS, each semantic head (cor-
responding to a node in the graph) is associated
with a nominal that identifies its discourse referent,
and relations between heads and their dependents
he h2
aa1
heh3
<Det>
<Arg0> <Arg1>
<TENSE>pres
<NUM>sg
<Arg0>
w1 want.01
m1
<Arg1>
<GenRel>
<Arg1>
<TENSE>pres
p1point
h1have.03
make.03
<Arg0>
Figure 1: Semantic dependency graph from the
CCGbank for He has a point he wants to make
[. . . ]
are modeled as modal relations.
3 The need for an OpenCCG analysis of
punctuation
The linguistic analysis aims to make a broad cover-
age OpenCCG grammar extracted from the CCG-
bank (White et al, 2007) more precise by adding
lexicalized punctuation categories to deal with
constructions involving punctuation. The origi-
nal CCGbank corpus does not have lexical cate-
gories for punctuation; instead, punctuation marks
carry categories derived from their part of speech
tags and form part of a binary rule. It is as-
sumed that there are no dependencies between
words and punctuation marks and that the re-
sult of punctuation rules is the same as the non-
punctuation category. OpenCCG does not support
non-combinatory binary rules, as they can be re-
placed by equivalent lexicalized categories with
application-only slashes. For example, a binary
rule of the form , s ? s can be replaced by the
equivalent category s
?1?
/
?
s
?1?
for the comma. In
fact, this would work reasonably well for parsing,
but is inadequate for generation. To illustrate, con-
sider (1):
(1) Despite recent declines in yields, in-
vestors continue to pour cash into
money funds. (wsj 0004.10)
A comma category like the one shown above
would end up overgenerating, as sentences and
18
sentential complements would be generated with
a comma preceding them. Also, the result of the
above function application rule could act as its own
argument, producing a string of commas. More
generally, binary rules miss out on many linguis-
tic generalizations, such as the presence of manda-
tory balancing marks in sentence-medial comma
or dash adjuncts.
The literature discusses various means to ad-
dress the issue of overgeneration: absorption rules
(Nunberg, 1990), syntactic features (Doran, 1998)
and (Briscoe, 1994) and semantic features (White,
2006). Section 5 explains these approaches in de-
tail, and considers a possible system of syntactic
features for a multi-modal CCG grammar imple-
mentation. We show how such a system is inade-
quate to constrain all possible cases of overgener-
ation, motivating our decision to employ semantic
features in our bi-directional grammar.
4 Integrating an analysis of punctuation
into the grammar
As our starting point, we used an XML repre-
sentation of an enhanced version of the CCGbank
with Propbank roles projected onto it (Boxwell and
White, 2008). Contexts and constructions in which
punctuation marks occur were isolated and the cor-
pus was then restructured by inserting new cate-
gories and modified derivations using XSL trans-
forms. In many cases this also involved modify-
ing the gold standard derivations substantially and
adding semantic representations to syntactic cat-
egories using logical form templates. Currently,
the algorithm succeeds in creating logical forms
for 98.01% of the sentences in the development
section (Sect. 00) of the converted CCGbank, and
96.46% of the sentences in the test section (Sect.
23). Of these, 92.10% of the development LFs
are semantic dependency graphs with a single root,
while 92.12% of the test LFs have a single root.
The remaining cases, with multiple roots, are miss-
ing one or more dependencies required to form a
fully connected graph. These missing dependen-
cies usually reflect inadequacies in the current log-
ical form templates. In Section 00, 89 punctuation
categories were created (66 commas, 14 dashes
and 3 each for the rest) out of 54 classes of binary
rules (37 comma, 8 dash, 3 apiece of colon, paren-
thesis and dots). Three high frequency comma cat-
egories are explained below.
4.1 Sentential Adjuncts
The comma in example (1) has been analysed
as selecting a sentential modifier to its left,
Despite recent declines in yields, to result in a
sentential modifier which then selects the rest of
the sentence. This results in the following lexical
category and semantics for the comma category:
(2) , ` s
?1?ind=X1 ,mod=M
/s
?1?
\
?
(s
?1?
/s
?1?
)
: @
M
(?EMPH-INTRO?+)
Syntactic categories and their semantics are linked
by index variables in the feature structures of cat-
egories. Index variables for semantic heads (e.g.
X1) are conventionally named X plus the number
of the feature structure. To support modifier modi-
fiers, as in (2), semantic heads of modifiers are also
made available through a modifier index feature,
with a variable conventionally named M .
1
Here,
the effect of combining the comma with the phrase
headed by despite is to add the ?EMPH-INTRO?+
feature to the despite-phrase?s semantics. Follow-
ing (Bayraktar et al, 1998), this feature indicates
that the comma has the discourse function of em-
phasizing an introductory clause or phrase. Dur-
ing realization, the feature triggers the look-up of
the category in (2), and prevents the re-application
of the category to its own output (as the feature
should only be realized once).
The category in (2) illustrates our approach,
which is to assign to every punctuation mark (other
than balancing marks) a category whose LF in-
cludes a feature or relation which represents its
discourse semantic function in broad-brush terms
such as emphasis, elaboration and apposition.
4.2 Verbs of reported speech
In (3), the comma which follows Neverthless and
sets off the phrase headed by said has the category
in (4):
(3) Nevertheless, said Brenda Malizia Ne-
gus, editor of Money Fund Report,
yields may blip up again before they
blip down because of recent rises in
short-term interest rates. (wsj 0004.8)
(4) , ` s
?2?
/s
?2?
/
?
punct[, ]/
?
(s
?1?dcl
\s
?2?dcl
)
: @
X2
(?ELABREL? ?X1)
1
A limited form of default unification is used in the im-
plementation to keep multiple modifiers from conflicting. As
the names of index variables are entirely predictable, they are
suppressed in the remainder of the paper.
19
In the genre of newswire text, this construction
occurs frequently with verbs of reported speech.
The CCGbank derivation of (3) assigns the cate-
gory s
?1?dcl
\s
?2?dcl
to the phrase headed by said,
the same category that is used when the phrase
follows the missing sentential complement. The
comma category in (4) selects for this category
and a balancing comma and then converts it to
a pre-sentential modifier, s
?2?
/s
?2?
. Semantically,
an elaboration relation is added between the main
clause and the reported speech phrase.
Category (4) overgenerates to some extent in
that it will allow a comma at the beginning of the
sentence. To prevent this, an alternative would be
to make the comma explicitly select for lexical ma-
terial to its left (in this case for the category of Nev-
erthless). Another possibility would be to follow
Doran (1998) in analyzing the above construction
by using the verb itself to select for the comma.
However, since our method involves changing the
gold standard derivations, and since making the
verb select extra commas or having the comma se-
lect leftward material would entail substantial fur-
ther changes to the derivations, we have opted to
go with (4), balancing adequacy and convenience.
4.3 NP appositives
Neither the Penn Tree Bank nor the CCGbank
distinguishes between NP appositives and NP
conjunctions. We wrote a set of simple heuristic
rules to enforce this distinction, which is vital
to generation. Appositives can occur sentence
medially or finally. The conventions of writing
mandate that sentence medial appositives should
be balanced?i.e., the appositive NP should
be surrounded by commas or dashes on both
sides?while sentence final appositives should
be unbalanced?i.e., they should only have one
preceding comma or dash. The categories and
semantics for unbalanced and balanced appositive
commas are, respectively:
(5) a. , ` np
?1?
\np
?1?
/
?
np
?3?
: @
X1
(?APPOSREL? ?X3)
b. , ` np
?1?
\np
?1?
/
?
punct[, ]/
?
np
?3?
: @
X1
(?APPOSREL? ?X3)
Here, the unbalanced appositive has a category
where the comma selects as argument the apposi-
tive NP and converts it to a nominal modifier. For
balanced appositives, the comma selects the ap-
positive NP and the balancing comma to form a
nominal modifier (examples are given in the next
section).
5 Constraining overgeneration in
bi-directional grammars
A complex issue that arises in the design of bi-
directional grammars is ensuring the proper pre-
sentation of punctuation. Among other things, this
involves the task of ensuring the correct realization
of commas introducing noun phrase appositives?
in our case, choosing when to use (5a) vs. (5b). In
this section, we consider and ultimately reject a so-
lution that follows Briscoe (1994) in using syntac-
tic features. As an alternative, interim solution, we
then describe a rule-based filter which bars illicit
punctuation sequences and improperly unbalanced
apposition. The paradigm below helps illustrate
the issues:
(6) John, CEO of ABC, loves Mary.
(7) * John, CEO of ABC loves Mary.
(8) Mary loves John, CEO of ABC.
(9) * Mary loves John, CEO of ABC,.
(10) Mary loves John, CEO of ABC, madly.
(11) * Mary loves John, CEO of ABC madly.
5.1 Absorption vs. syntactic features
Nunberg (1990) argues that text adjuncts intro-
duced by punctuation marks have an underlying
representation where these adjuncts have marks on
either side. They attain their surface form when
a set of presentation rules are applied. This ap-
proach ensures that all sentence medial cases like
(6) and (10) above are generated correctly, while
unacceptable examples (7) and (11) would not be
generated at all. Example (8) would at first be
generated as (9): to deal with such sentences,
where two points happen to coincide, Nunberg
posits an implicit point which is absorbed by the
adjacent point. Absorption occurs according to
the ?strength? of the two points. Strength is de-
termined according to the Point Absorption Hi-
erarchy, which ranks commas lower than dashes,
semi-colons, colons and periods. As White (1995)
observes, from a generation-only perspective, it
makes sense to generate text adjuncts which are
always balanced and post-process the output to
delete lower ranked points, as absorption uses rel-
atively simple rules that operate independently of
20
the hierarchy of the constituents. However, us-
ing this approach for parsing would involve a pre-
processing step which inserts commas into possi-
ble edges of possible constituents, as described in
(Forst and Kaplan, 2006). To avoid this consider-
able complication, Briscoe (1994) has argued for
developing declarative approaches involving syn-
tactic features, with no deletions or insertions of
punctuation marks.
5.2 Features for punctuation in CCG?
Unfortunately, the feature-based approach appears
to be inadequate for dealing with the class of ex-
amples presented above in CCG. This approach in-
volves the incorporation of syntactic features for
punctuation into atomic categories so that certain
combinations are blocked. To ensure proper ap-
positive balancing sentence finally, the rightmost
element in the sentence should transmit a relevant
feature to the clause level, which the sentence-final
period can then check for the presence of right-
edge punctuation. Possible categories for a tran-
sitive verb and the full stop appear below:
(12) loves ` s
?1?bal=BAL,end=PE
\np
?2?bal=+
/np
?3?bal=BAL,end=PE
(13) . ` sent\
?
s
end=nil
Here the feature variables BAL and PE of the right-
most argument of the verb would unify with the
corresponding result category feature values to re-
alize the main clauses of (8) and (9) with the fol-
lowing feature values:
(14) Mary loves John, CEO of ABC `
s
?1?bal=?,end=nil
(15) Mary loves John, CEO of ABC, `
s
?1?bal=+,end=comma
Thus, in (15), the sentence-final period would not
combine with s
?1?bal=+,end=comma
and the deriva-
tion would be blocked.
2
5.2.1 Issue: Extraction cases
The solution sketched above is not adequate to
deal with extraction involving ditransitive verbs in
cases like (16) and (17):
2
It is worth noting than an n-gram scorer would highly
disprefer example (9), as a comma period sequence would not
be attested in the training data. However, an n-gram model
cannot be relied upon to eliminate examples like (11), which
would likely be favored as they are shorter than their balanced
counterparts.
(16) Mary loves a book that John gave Bill,
his brother.
(17) * Mary loves a book that John gave Bill,
his brother,.
As Figure 2 shows, an unacceptable case like (17)
is not blocked. Even when the sentence final NP is
balanced, the end=comma value is not propagated
to the root level. This is because the end feature
for the relative clause should depend on the first
(indirect) object of gave, rather than the second
(direct) object as in a full ditransitive clause. A
possible solution would be to introduce more fea-
tures which record the presence of punctuation in
the leftward and rightward arguments of complex
categories; this would be rather baroque, however.
5.2.2 Issue: Crossing composition
Another issue is how crossing composition, used
with adverbs in heavy NP shift contructions, inter-
acts with appositives, as in the following examples:
(18) Mary loves madly John, CEO of ABC.
(19) * Mary loves madly John, CEO of ABC,.
For examples (10) and (11), which do not involve
crossing composition, the category for the adverb
should be the one in (20):
(20) madly ` s
?1?end=nil
\np
?2?
\
(s
?1?bal=+
\np
?2?
)
Here the bal=+ feature on the argument of the ad-
verb madly ensures that the direct object of the
verb is balanced, as in (10); otherwise, the deriva-
tion fails, as in (11). Irrespective of the value
of the end feature of the argument, the result of
the adverb has the feature end=nil as the post-
modifier is lexical material which occurs after the
VP. With crossing composition, however, category
(20) would licence an erroneous derivation for ex-
ample (19), as the end=nil feature on the result of
the adverb category would prevent the percolation
of the end feature at the edge of the phrase to the
clausal root, as Figure 3 shows.
To block such derivations, one might consider
giving the adverb another category for use with
crossing composition:
(21) madly ` s
?1?
\np
?2?
\
?
(s
?1?
\np
?2?
)
The use of the non-associative, permutative modal-
ity ? on the main slash allows the crossing com-
position rule to be applied, and feature inheritance
21
that John gave Bill, his brother,
(n
end=PE
\n)/(s
end=PE
/np) np s
end=PE
\np/np
end=PE
/np np
end=comma
>T >
s/(s\np) s
end=PE
\np/np
end=PE
>B
s
end=PE
/np
end=PE
>
n
end=PE
\n
Figure 2: Object extraction
Mary loves madly John, CEO, .
np s
end=PE
\np/np
end=PE
s 1
end=nil
\np 1\(s 1
bal=+
\np 1) np
bal=+,end=comma
sent\
?
s
end=nil
<B
?
s
end=nil
\np/np
end=PE
>
s
end=nil
\np
<
s
end=nil
<
sent
Figure 3: Crossing composition
ensures that the end feature from the verb loves
is also copied over. Thus, in example (19), the
punctuation at the edge of the phrase would be
percolated to the clausal root, where the sentence-
final period would block the derivation. However,
in the slash modality inheritance hierarchy pro-
posed by Baldridge (2002), the ? modality inher-
its the properties of function application. Conse-
quently, this category could also lead to the erro-
neous derivation of example (11). In such a deriva-
tion, category (21) will not require the direct ob-
ject to have a balanced appositive; meanwhile, the
end=nil feature on the direct object will propagate
to the clausal root, where it will happily combine
with the category for the full stop. Finally, having
two distinct categories for the adverb would off-
set the advantage of multi-modal categorial gram-
mar in dealing with word order variation, where it
is possible to use one category in situations where
otherwise several categories would be required.
5.3 A rule-based filter to constrain
overgeneration
For the reasons discussed in the preceding section,
we decided not to use syntactic features to con-
strain overgeneration. Instead, we have employed
semantic features in the logical form together with
a rule-based filter, as an interim solution. Dur-
ing realization, the generated output is examined
and fragments where two marks appear in a row
are eliminated. Additionally, to handle improp-
erly unbalanced punctuation, we modified the re-
sult categories of unbalanced appositive commas
and dashes to include a feature marking unbal-
anced punctuation, as follows:
(22) , ` np
?1?unbal=comma
\
?
np
?1?
/
?
np
?2?
Then, during realization, a filter on derivations
looks for categories such as np
unbal=comma
, and
checks to make sure this NP is followed by a an-
other punctuation mark in the string. We report on
the effects of the filter in our results section.
6 Evaluation
We extracted a grammar from the restructured cor-
pus and created testbeds of logical forms under the
following conditions:
1. Baseline 1: A CCGbank version which has no
lexicalized categories corresponding to any
of the punctuation marks except sentence fi-
nal marks and commas which conjoin ele-
ments or introduce NP appositives. Conjunc-
tion and apposition are frequent in the corpus
and if excluded, logical forms for many sen-
tences are not produced, weakening the base-
line considerably.
2. Baseline 2: A CCGbank version where
all punctuation marks (except conjunc-
tion/apposition commas and sentence-final
marks, which have proper categories) have
lexicalized MMCCG categories with no se-
mantics, corresponding to binary rules in the
original CCGbank.
3. The CCGbank augmented with punctuation
categories.
22
Testing was done under four conditions:
1. Non-blind testing with oracle n-gram scoring.
This condition tests the grammar most di-
rectly, as it avoids the issue of lexical smooth-
ing and keeps the combinatorial search man-
ageable. A grammar extracted from the de-
velopment section (Section 00) of the CCG-
bank was applied to the LF testbed of that
section, using oracle n-gram scoring (along
with FLMs, see next) to generate the sen-
tences back. For each logical form, the gener-
ated output sentence was compared with the
actual gold standard sentence corresponding
to that logical form.
2. Blind testing with factored language mod-
els (FLM) and lexical smoothing, following
(White et al, 2007). Blind testing naturally
provides a more realistic test of performance
on unseen data. Here logical forms of Sec-
tions 00 and 23 were created using gram-
mars of those sections respectively and then
a grammar was extracted from the standard
training sections (02-21). This grammar was
used to generate from the LFs of the develop-
ment and test sections; for space reasons, we
only report the results on the test section.
3. Blind testing with hypertagging. Hypertag-
ging (Espinosa et al, 2008) is supertagging
for surface realization; it improves realizer
speed and coverage with large grammars by
predicting lexical category assignments with
a maximum entropy model.
4. The punctuation-enhanced grammars were
tested in the three conditions above with and
without the balanced punctuation filter.
7 Results
Non-blind testing results in Table 1 indicate that
both exact match figures as well BLEU scores in-
crease substantially in comparison to the baselines
when a punctuation augmented grammar is used.
The difference is especially notable when oracle
n-gram scoring is used. The punctuation filter im-
proves performance as exact matches increase by
1.66% and BLEU scores also show a slight in-
crease. Complete realizations are slightly worse
for the augmented grammar than Baseline 1, but
the coverage of the baseline grammar is lower.
Table 1: Non-blind testing on Section 00 (Gram-
mar coverage: Baseline 1, 95.8%; Baseline 2,
95.03%; Punct grammar, 98.0%)
N-grams Grammar Exact Complete BLEU
Oracle Baseline 1 35.8% 86.2% 0.8613
Baseline 2 39.10% 53.58% 0.8053
Punct 75.9% 85.3% 0.9503
FLM w/o Baseline 1 17.7% 83.0% 0.7293
filter Baseline 2 5.72% 4.18% 0.4470
Punct 29.7% 80.6% 0.7984
FLM w/ filt. Punct 31.3% 80.6% 0.8062
Table 2: Blind testing on Section 23 with FLM
(Grammar coverage: Baseline 1, 94.8%; Base-
line 2, 95.06%; Punct grammar, 96.5%)
Hyp., Filt. Grammar Exact Complete BLEU
no, w/o Baseline 1 11.1% 46.4% 0.6297
Baseline 2 2.97% 3.97% 0.3104
Punct 18.0% 43.2% 0.6815
no, w/ Punct 19.3% 43.3% 0.6868
yes, w/o Punct 20.4% 61.5% 0.7270
yes, w/ Punct 21.6% 61.5% 0.7323
Blind testing results shown in Table 2 also demon-
strate that the augmented grammar does better than
the baseline in terms of BLEU scores and ex-
act matches, with the hypertagger further boosting
BLEU scores and the number of complete realiza-
tions. The use of the filter yields a further 1.2?
1.3% increase in exact match figures as well as a
half a BLEU point improvement; a planned col-
lection of human judgments may reveal that these
improvements are more meaningful than the scores
would indicate.
Baseline 2, which models all punctuation, per-
forms very badly with FLM scoring though it does
better than the minimal punctuation Baseline 1
with oracle scoring. The main reason for this is
that, without any semantic or syntactic features to
constrain punctuation categories, they tend to re-
apply to their own output, clogging up the chart.
This results in a low number of complete realiza-
tions as well as exact matches.
While direct comparisons cannot really be made
across grammar frameworks, as inputs vary in
their semantic depth and specificity, we observe
that our all-sentences BLEU score of 0.7323 ex-
ceeds that of Hogan et al (2007), who report a
top score of 0.6882 including special treatment of
multi-word units (though their coverage is near
100%). Nakanishi et al (2005) and Langkilde-
23
Geary (2002) report scores several points higher,
though the former is limited to sentences of length
20 or less, and the latter?s coverage is much lower.
8 Conclusion
We have shown that incorporating a more pre-
cise analysis of punctuation into a broad-coverage
reversible grammar extracted from the CCGbank
yields substantial increases in the number of ex-
act matches and BLEU scores when performing
surface realization with OpenCCG, contributing to
state-of-the-art results. Our discussion has also
highlighted the inadequacy of using syntactic fea-
tures to control punctuation placement in CCG,
leading us to develop a filter to ensure appro-
priately balanced commas and dashes. In fu-
ture work, we plan to investigate a more satisfac-
tory grammatical treatment involving constraints
in independent orthographic derivations, perhaps
along the lines of the autonomous prosodic deriva-
tions which Steedman and Prevost (1994) discuss.
An evaluation of parsing side performance is also
planned.
Acknowledgments
We thank the anonymous reviewers, Detmar Meur-
ers and the Clippers and Synners groups at OSU
for helpful comments and discussion.
References
Baldridge, Jason and Geert-Jan Kruijff. 2002. Cou-
pling CCG and Hybrid Logic Dependency Seman-
tics. In Proc. ACL-02.
Baldridge, Jason. 2002. Lexically Specified Deriva-
tional Control in Combinatory Categorial Grammar.
Ph.D. thesis, University of Edinburgh.
Bayraktar, Murat, Bilge Say, and Varol Akman. 1998.
An Analysis of English Punctuation: The Special
Case of Comma. International Journal of Corpus
Linguistics, 3(1):33?58.
Boxwell, Stephen and Michael White. 2008. Pro-
jecting Propbank roles onto the CCGbank. In Proc.
LREC-08. To appear.
Briscoe, Ted. 1994. Parsing (with) punctuation. Tech-
nical report, Xerox, Grenoble, France.
Doran, Christine. 1998. Incorporating Punctuation
into the Sentence Grammar: A Lexicalized Tree Ad-
joining Grammar Perspective. Ph.D. thesis, Univer-
sity of Pennsylvania.
Espinosa, Dominic, Michael White, and Dennis Mehay.
2008. Hypertagging: Supertagging for surface real-
ization with CCG. In Proc. ACL-08:HLT. To appear.
Forst, Martin and Ronald M. Kaplan. 2006. The im-
portance of precise tokenizing for deep grammars.
In Proc. LREC-06.
Hockenmaier, Julia and Mark Steedman. 2007. CCG-
bank: A Corpus of CCG Derivations and Depen-
dency Structures Extracted from the Penn Treebank.
Computational Linguistics, 33(3):355?396.
Hogan, Deirdre, Conor Cafferkey, Aoife Cahill, and
Josef van Genabith. 2007. Exploiting multi-word
units in history-based probabilistic generation. In
Proc. EMNLP-CoNLL-07.
Langkilde-Geary, Irene. 2002. An empirical veri-
fication of coverage and correctness for a general-
purpose sentence generator. In Proc. INLG-02.
Nakanishi, Hiroko, Yusuke Miyao, and Jun?ichi Tsujii.
2005. Probabilistic methods for disambiguation of
an HPSG-based chart generator. In Proc. IWPT-05.
Nunberg, Geoffrey. 1990. The Linguistics of Punctua-
tion. CSLI Publications, Stanford, CA.
Steedman, Mark and S. Prevost. 1994. Specifying in-
tonation from context for speech synthesis. Speech
Communication, 15(1?2):139?153.
Steedman, Mark. 2000. The syntactic process. MIT
Press, Cambridge, MA, USA.
White, Michael, Rajakrishnan Rajkumar, and Scott
Martin. 2007. Towards broad coverage surface re-
alization with CCG. In Proc. of the Workshop on
Using Corpora for NLG: Language Generation and
Machine Translation (UCNLG+MT).
White, Michael. 1995. Presenting punctuation. In Pro-
ceedings of the Fifth European Workshop on Natural
Language Generation, pages 107?125.
White, Michael. 2006. Efficient realization of coordi-
nate structures in combinatory categorial grammar.
Research on Language and Computation, 4(1):39?
75.
24
Proceedings of the UCNLG+Eval: Language Generation and Evaluation Workshop, pages 39?44,
Edinburgh, Scotland, UK, July 31, 2011. c?2011 Association for Computational Linguistics
Linguistically Motivated Complementizer Choice in Surface Realization
Rajakrishnan Rajkumar and Michael White
Department of Linguistics
The Ohio State University
Columbus, OH, USA
{raja,mwhite}@ling.osu.edu
Abstract
This paper shows that using linguis-
tically motivated features for English
that-complementizer choice in an averaged
perceptron model for classification can
improve upon the prediction accuracy of a
state-of-the-art realization ranking model.
We report results on a binary classification
task for predicting the presence/absence of a
that-complementizer using features adapted
from Jaeger?s (2010) investigation of the
uniform information density principle in the
context of that-mentioning. Our experiments
confirm the efficacy of the features based
on Jaeger?s work, including information
density?based features. The experiments also
show that the improvements in prediction
accuracy apply to cases in which the presence
of a that-complementizer arguably makes a
substantial difference to fluency or intelli-
giblity. Our ultimate goal is to improve the
performance of a ranking model for surface
realization, and to this end we conclude with
a discussion of how we plan to combine the
local complementizer-choice features with
those in the global ranking model.
1 Introduction
Johnson (2009) observes that in developing statis-
tical parsing models, ?shotgun? features ? that is,
myriad scattershot features that pay attention to su-
perficial aspects of structure ? tend to be remark-
ably useful, while features based on linguistic the-
ory seem to be of more questionable utility, with
the most basic linguistic insights tending to have the
greatest impact.1 Johnson also notes that feature
design is perhaps the most important but least un-
derstood aspect of statistical parsing, and thus the
disappointing impact of linguistic theory on pars-
ing models is of real consequence. In this paper,
by contrast, we show that in the context of sur-
face realization, using linguistically motivated fea-
tures for English that-complementizer choice can
improve upon the prediction accuracy of a state-of-
the-art realization ranking model, arguably in ways
that make a substantial difference to fluency and in-
telligiblity.2 In particular, we report results on a bi-
nary classification task for predicting the presence
or absence of a that-complementizer using features
adapted from Jaeger?s (2010) investigation of the
uniform information density principle in the con-
text of that-mentioning. This information-theoretic
principle predicts that language production is af-
fected by a preference to distribute information uni-
formly across the linguistic signal. In Jaeger?s study,
uniform information density emerges as an impor-
tant predictor of speakers? syntactic reduction pref-
erences even when taking a sizeable variety of con-
trols based on competing hypotheses into account.
Our experiments confirm the efficacy of the fea-
tures based on Jaeger?s work, including information
density?based features.
1The term ?shotgun? feature appears in the slides for
Johnson?s talk (http://www.cog.brown.edu/?mj/
papers/johnson-eacl09-workshop.pdf), rather
than in the paper itself.
2For German surface realization, Cahill and Riester (2009)
show that incorporating information status features based on
the linguistics literature improves performance on realization
ranking.
39
That-complementizers are optional words that in-
troduce sentential complements in English. In the
Penn Treebank, they are left out roughly two-thirds
of the time, thereby enhancing conciseness. This
follows the low complementizer rates reported in
previous work (Tagliamonte and Smith, 2005; Ca-
coullos and Walker, 2009). While some surface re-
alizers, such as FUF/SURGE (Elhadad, 1991), have
made use of input features to control the choice of
whether to include a that-complementizer, for many
applications the decision seems best left to the real-
izer, since multiple surface syntactic factors appear
to govern the choice, rather than semantic ones. In
our experiments, we use the OpenCCG3 surface re-
alizer with logical form inputs underspecified for the
presence of that in complement clauses. While in
many cases, adding or removing that results in an
acceptable paraphrase, in the following example, the
absence of that in (2) introduces a local ambiguity,
which the original Penn Treebank sentence avoids
by including the complementizer.
(1) He said that for the second month in a
row, food processors reported a shortage
of nonfat dry milk. (WSJ0036.61)
(2) ? He said for the second month in a row,
food processors reported a shortage of
nonfat dry milk.
The starting point for this paper is White and Ra-
jkumar?s (2009) realization ranking model, a state-
of-the-art model employing shotgun features ga-
lore. An error analysis of this model, performed
by comparing CCGbank Section 00 realized deriva-
tions with their corresponding gold standard deriva-
tions, revealed that out of a total of 543 that-
complementizer cases, the realized output did not
match the gold standard choice 82 times (see Table 3
in Section 5 for details). Most of these mismatches
involved cases where a clause originally containing
a that-complementizer was realized in reduced form,
with no that. This under-prediction of that-inclusion
is not surprising, since the realization ranking model
makes use of baseline n-gram model features, and
n-gram models are known to have a built-in bias for
strings with fewer words.
3openccg.sf.net
We report here on experiments comparing this
global model to ones that employ local features
specifically designed for that-choice in complement
clauses. As a prelude to incorporating these fea-
tures into a model for realization ranking, we study
the efficacy of these features in isolation by means
of a binary classification task to predict the pres-
ence/absence of that in complement clauses. In
a global realization ranking setting, the impact of
these phenomenon-specific features might be less
evident, as they would interact with other features
for lexical selection and ordering choices that the
ranker makes. Note that a comprehensive ranking
model is desirable, since linear ordering and that-
complementizer choices may interact. For exam-
ple, Hawkins (2003) reports examples where explic-
itly marked phrases can occur either close to or far
from their heads as in (3) and (4), whereas zero-
marked phrases are only rarely attested at some dis-
tance from their heads and prefer adjacency, as (5)
and (6) show.
(3) I realized [that he had done it] with sad-
ness in my heart.
(4) I realized with sadness in my heart [that
he had done it].
(5) I realized [he had done it] with sadness in
my heart.
(6) ? I realized with sadness in my heart [he
had done it].
2 Background
CCG (Steedman, 2000) is a unification-based cat-
egorial grammar formalism defined almost en-
tirely in terms of lexical entries that encode sub-
categorization as well as syntactic features (e.g.
number and agreement). OpenCCG is a pars-
ing/generation library which includes a hybrid
symbolic-statistical chart realizer (White, 2006).
The chart realizer takes as input logical forms rep-
resented internally using Hybrid Logic Dependency
Semantics (HLDS), a dependency-based approach
to representing linguistic meaning (Baldridge and
Kruijff, 2002). To illustrate the input to OpenCCG,
consider the semantic dependency graph in Figure 1.
In the graph, each node has a lexical predication
(e.g. make.03) and a set of semantic features (e.g.
40
aa1
heh3
he h2
<Det>
<Arg0> <Arg1>
<TENSE>pres
<NUM>sg
<Arg0>
w1 want.01
m1
<Arg1>
<GenRel>
<Arg1>
<TENSE>pres
p1point
h1have.03
make.03
<Arg0>
s[b]\np/np
np/n
np
n
s[dcl]\np/np
s[dcl]\np/(s[to]\np)
np
Figure 1: Semantic dependency graph from the CCGbank
for He has a point he wants to make [. . . ], along with
gold-standard supertags (category labels)
?NUM?sg); nodes are connected via dependency re-
lations (e.g. ?ARG0?). In HLDS, each semantic head
(corresponding to a node in the graph) is associated
with a nominal that identifies its discourse referent,
and relations between heads and their dependents
are modeled as modal relations. We extract HLDS-
based quasi logical form graphs from the CCG-
bank and semantically empty function words such as
complementizers, infinitival-to, expletive subjects,
and case-marking prepositions are adjusted to reflect
their purely syntactic status. Alternative realizations
are ranked using an averaged perceptron model de-
scribed in the next section.
3 Feature Design
White and Rajkumar?s (2009) realization ranking
model serves as the baseline for this paper. It is
a global, averaged perceptron ranking model using
three kinds of features: (1) the log probability of the
candidate realization?s word sequence according to
three linearly interpolated language models (as well
as a feature for each component model), much as
in the log-linear models of Velldal & Oepen (2005)
and Nakanishi et al (2005); (2) integer-valued syn-
tactic features, representing counts of occurrences in
a derivation, from Clark & Curran?s (2007) normal
form model; and (3) discriminative n-gram features
(Roark et al, 2004), which count the occurrences of
each n-gram in the word sequence.
Table 1 shows the new complementizer-choice
features investigated in this paper. The example fea-
tures mentioned in the table are taken from the two
complement clause (CC) forms (with-that CC vs.
that-less CC) of the sentence below:
(7) The finding probably will support those
who argue [ that/? the U.S. should regu-
late the class of asbestos including croci-
dolite more stringently than the common
kind of asbestos, chrysotile, found in most
schools and other buildings], Dr. Talcott
said. (WSJ0003.19)
The first class of features, dependency length and
position of CC, have been adapted from the related
control features in Jaeger?s (2010) study. For the
above example, the position of the matrix verb with
respect to the start of the sentence (feature name
mvInd and having the value 7.0), the distance be-
tween the matrix verb and the onset of the CC (fea-
ture name mvCCDist with the value 1.0) and fi-
nally the length of the CC (feature ccLen with value
of 29.0 for the that-CC and 28.0 for the that-less
CC) are encoded as features. The second class of
features includes various properties of the matrix
verb viz. POS tag, form, stem and supertag (fea-
ture names mv Pos, mvStem, mvForm, mvSt, respec-
tively). These features were motivated by the fact
that Jaeger controls for the per-verb bias of this con-
struction, as attested in the earlier literature. The
third class of features are related to information den-
sity. Jaeger (2010) estimates information density at
the CC onset by using matrix verb subcategorization
frequency. In our case, more like the n-gram fea-
tures employed by Levy and Jaeger (2007), we used
log probabilities from two existing n-gram models,
viz. a trigram word model and trigram word model
with semantic class replacement. For each CC, two
features (one per language model) were extracted by
calculating the average of the log probs of individual
words from the beginning of the complement clause.
In the that-CC version of the example above, lo-
cal CC-features having the prefix $uidCCMean were
calculated by averaging the individual log probs of
the 3 words that the U.S. to get feature values of
-0.8353556 and -2.0460036 per language model (see
41
Feature Example for that-CCs Example for that-less CCs
Dependency length and position of CC
Position of matrix verb thatCC:mvInd 7.0 noThatCC:mvInd 7.0
Dist between matrix verb & CC thatCC:mvCCDist 1.0 noThatCC:mvCCDist 1.0
Length of CC thatCC:ccLen 29.0 noThatCC:ccLen 28.0
Matrix verb features
POS-tag thatCC:mvPos:VBP 1.0 noThatCC:mvPos:VBP 1.0
Stem thatCC:mvStem:argue 1.0 noThatCC:mvStem:argue 1.0
Form thatCC:mvForm:argue 1.0 noThatCC:mvForm:argue 1.0
CCG supertag thatCC:mvSt:s[dcl]\np/s[em] 1.0 noThatCC:mvSt:s[dcl]\np/s[dcl] 1.0
uniform information density (UID)
Average n-gram log probs thatCC:$uidCCMean1 -0.8353556 noThatCC:$uidCCMean1 -2.5177214
of first 2 words of that-less CCs thatCC:$uidCCMean2 -2.0460036 noThatCC:$uidCCMean2 -3.6464245
or first 3 words of that-CCs
Table 1: New features introduced (the prefix of each feature encodes the type of CC; subsequent parts supply the
feature name)
last part of Table 1). In the that-less CC version,
$uidCCMean features were calculated by averaging
the log probs of the first two words in the comple-
ment clause, i.e. the U.S.
4 Classification Experiment
To train a local classification model to predict the
presence of that in complement clauses, we used
an averaged perceptron ranking model with the
complementizer-specific features listed in Table 1
to rank alternate with-that vs. that-less CC choices.
For each CC classification instance in CCGbank
Sections 02?21, the derivation of the competing al-
ternate choice was created; i.e., in the case of a that-
CC, the corresponding that-less CC was created and
vice versa. Table 2 illustrates classification results
on Sections 00 (development) using models contain-
ing different feature sets & Section 23 (final test) for
the best-performing classification and ranking mod-
els. For both the development as well as test sec-
tions, the local classification model performed sig-
nificantly better than the global realization ranking
model according to McNemar?s ?2 test (p = 0.005,
two-tailed). Feature ablation tests on the develop-
ment data (Section 00) revealed that removing the
information density features resulted in a loss of ac-
curacy of around 1.8%.
5 Discussion
As noted in the introduction, in many cases, adding
or removing that to/from the corpus sentence results
in an acceptable paraphrase, while in other cases
the presence of that appears to make a substantial
Model Features % 00 % 23
Most Frequent Baseline 68.7 66.8
Global Realization Ranking 78.45 77.0
Local That-Classification
Only UID feats 74.77
Table 1 features except UID ones 81.4
Both feature sets above 83.24 83.02
Table 2: Classification accuracy results (Section 00 has
170/543 that-CCs; Section 23 has 192/579 that-CCs)
Construction %that % that / %Accuracy
Gold Classification Ranking
Gerundive (26) 53.8 61.5 / 92.3 26.9 / 57.7
Be-verb (21) 71.4 95.2 / 66.7 47.6 / 57.1
Non-adjacent CCs (53) 49.1 54.7 / 67.9 30.2 / 66.0
Total (543) 31.3 29.3 / 83.2 21.9 / 78.5
Table 3: Section 00 construction-wise that-CC propor-
tions and model accuracies (total CC counts given in
brackets alongside labels); gold standard obviously has
100% accuracy; models are local that-classification and
White and Rajkumar?s (2009) global realization ranking
model
difference to intelligibility or fluency. In order to
better understand the effect of the complementizer-
specific features, we examined three construction
types in the development data, viz. non-adjacent
complement clauses, gerundive matrix verbs and a
host of sub-cases involving a matrix be-verb (wh-
clefts, be+adjective etc.), where the presence of that
seemed to make the most difference. The results are
provided in Table 3. As is evident, the global realiza-
tion ranking model under-proposes the that-choice,
most likely due to the preference of n-gram mod-
els towards fewer words, while the local classifica-
42
WSJ0049.64 Observing [that/?? the judge has never exhibited any bias or prejudice], Mr. Murray concluded that he would be impartial
in any case involving a homosexual or prostitute as a victim.
WSJ0020.16 ? what this tells us is [that/?? U.S. trade law is working] ?, he said .
WSJ0010.5 The idea, of course: to prove to 125 corporate decision makers [that/?? the buckle on the Rust Belt is n?t so rusty after all ,
that it ?s a good place for a company to expand].
WSJ0044.118 Editorials in the Greenville newspaper allowed [that/?? Mrs. Yeargin was wrong], but also said the case showed how testing
was being overused.
WSJ0060.7 Viacom denies [?/?that it ?s using pressure tactics].
WSJ0018.4 The documents also said [that/?? although the 64-year-old Mr. Cray has been working on the project for more than six years ,
the Cray-3 machine is at least another year away from a fully operational prototype].
Table 4: Examples from model comparison
tion model is closer to the gold standard in terms of
that-choice proportions. For all the three construc-
tion types as well as overall, classifier performance
was better than ranker performance. The difference
in performance between the local classification and
global ranking models in the case of gerundive ma-
trix verbs is statistically significant according to the
McNemar?s ?2 test (Bonferroni corrected, two tailed
p = 0.001). The performance difference was not
significant with the other two constructions, how-
ever, using only the cases in Section 00.
Table 4 lists relevant examples where the classi-
fication model?s that-choice prediction matched the
gold standard while a competing model?s predic-
tion did not. Example WSJ0049.64 is one such
instance of classifier success involving a gerun-
dive matrix verb (in contrast to the realization
ranking model), Example WSJ0020.16 exemplifies
success with a wh-cleft construction and Exam-
ple WSJ0010.5 contains a non-adjacent CC. Apart
from these construction-based analyses, examples
like WSJ0044.118 indicate that the classification
model prefers the that-CC choice in cases that sub-
stantially improve intelligiblity, as here the overt
complementizer helps to avoid a local syntactic am-
biguity where the NP in allowed NP is unlikely to be
interpreted as the start of an S.
Finally, we also studied the effect of the uniform
information density features by comparing the full
classification model to a model without the UID
features. The full classification model exhibited a
trend towards significantly outperforming the ab-
lated model (McNemar?s p = 0.10, 2-tailed); more
test data would be needed to establish significance
conclusively. Examples are shown at the bottom of
Table 4. In WSJ0060.7, the full classification model
predicted a that-less clause (matching the gold stan-
dard), while the ablated classification model pre-
dicted a clause with that. In all such examples ex-
cept one, the information density features helped the
classification model avoid predicting that-inclusion
when not necessary. Example WSJ0018.4 is the
only instance where the best classification model
differed in predicting the that-choice.
6 Conclusions and Future Work
In this paper, we have shown that using linguistically
motivated features for English that-complementizer
choice in a local classifier can improve upon the
prediction accuracy of a state-of-the-art global re-
alization ranking model employing myriad shotgun
features, confirming the efficacy of features based
on Jaeger?s (2010) investigation of the uniform in-
formation density principle in the context of that-
mentioning. Since that-complementizer choice in-
teracts with other realization decisions, in future
work we plan to investigate incorporating these fea-
tures into the global realization ranking model. This
move will require binning the real-valued features,
as multiple complement clauses can appear in a sin-
gle sentence. Should feature-level integration prove
ineffective, we also plan to investigate alternative ar-
chitectures, such as using the local classifier outputs
as features in the global model.
Acknowledgements
This work was supported in part by NSF IIS-
0812297 and by an allocation of computing time
from the Ohio Supercomputer Center. Our thanks
also to Florian Jaeger, William Schuler, Peter Culi-
cover and the anonymous reviewers for helpful com-
ments and discussion.
43
References
Jason Baldridge and Geert-Jan Kruijff. 2002. Coupling
CCG and Hybrid Logic Dependency Semantics. In
Proc. ACL-02.
Rena Torres Cacoullos and James A. Walker. 2009. On
the persistence of grammar in discourse formulas: A
variationist study of ?that?. Linguistics, 47(1):1?43.
Stephen Clark and James R. Curran. 2007. Wide-
Coverage Efficient Statistical Parsing with CCG and
Log-Linear Models. Computational Linguistics,
33(4):493?552.
Michael Elhadad. 1991. FUF: The universal unifier user
manual version 5.0. Technical Report CUCS-038-91,
Dept. of Computer Science, Columbia University.
John A. Hawkins. 2003. Why are zero-marked phrases
close to their heads? In Gu?nter Rohdenburg and Britta
Mondorf, editors, Determinants of Grammatical Vari-
ation in English, Topics in English Linguistics 43. De
Gruyter Mouton, Berlin.
T. Florian Jaeger. 2010. Redundancy and reduction:
Speakers manage information density. Cognitive Psy-
chology, 61(1):23?62, August.
Mark Johnson. 2009. How the statistical revolution
changes (computational) linguistics. In Proceedings of
the EACL 2009 Workshop on the Interaction between
Linguistics and Computational Linguistics: Virtuous,
Vicious or Vacuous?, pages 3?11, Athens, Greece,
March. Association for Computational Linguistics.
Roger Levy and T. Florian Jaeger. 2007. Speakers opti-
mize information density through syntactic reduction.
Advances in Neural Information Processing Systems,
19:849.
Hiroko Nakanishi, Yusuke Miyao, and Jun?ichi Tsujii.
2005. Probabilistic methods for disambiguation of an
HPSG-based chart generator. In Proc. IWPT-05.
Brian Roark, Murat Saraclar, Michael Collins, and Mark
Johnson. 2004. Discriminative language modeling
with conditional random fields and the perceptron al-
gorithm. In Proc. ACL-04.
Mark Steedman. 2000. The syntactic process. MIT
Press, Cambridge, MA, USA.
S. Tagliamonte and J. Smith. 2005. No momentary
fancy! the zero ?complementizer? in English dialects.
English Language and Linguistics, 9(2):289?309.
Erik Velldal and Stephan Oepen. 2005. Maximum en-
tropy models for realization ranking. In Proc. MT
Summit X.
Michael White and Rajakrishnan Rajkumar. 2009. Per-
ceptron reranking for CCG realization. In Proceedings
of the 2009 Conference on Empirical Methods in Nat-
ural Language Processing, pages 410?419, Singapore,
August. Association for Computational Linguistics.
Michael White. 2006. Efficient Realization of Coordi-
nate Structures in Combinatory Categorial Grammar.
Research on Language and Computation, 4(1):39?75.
44
