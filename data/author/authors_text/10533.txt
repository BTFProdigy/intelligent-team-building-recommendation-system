Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 841?848
Manchester, August 2008
Modeling Latent-Dynamic in Shallow Parsing:
A Latent Conditional Model with Improved Inference
Xu Sun? Louis-Philippe Morency? Daisuke Okanohara? Jun?ichi Tsujii??
?Department of Computer Science, The University of Tokyo, Hongo 7-3-1, Tokyo, Japan
?USC Institute for Creative Technologies, 13274 Fiji Way, Marina del Rey, USA
?School of Computer Science, The University of Manchester, 131 Princess St, Manchester, UK
?{sunxu, hillbig, tsujii}@is.s.u-tokyo.ac.jp ?morency@ict.usc.edu
Abstract
Shallow parsing is one of many NLP tasks
that can be reduced to a sequence la-
beling problem. In this paper we show
that the latent-dynamics (i.e., hidden sub-
structure of shallow phrases) constitutes a
problem in shallow parsing, and we show
that modeling this intermediate structure
is useful. By analyzing the automatically
learned hidden states, we show how the
latent conditional model explicitly learn
latent-dynamics. We propose in this paper
the Best Label Path (BLP) inference algo-
rithm, which is able to produce the most
probable label sequence on latent condi-
tional models. It outperforms two existing
inference algorithms. With the BLP infer-
ence, the LDCRF model significantly out-
performs CRF models on word features,
and achieves comparable performance of
the most successful shallow parsers on the
CoNLL data when further using part-of-
speech features.
1 Introduction
Shallow parsing identifies the non-recursive cores
of various phrase types in text. The paradigmatic
shallow parsing problem is noun phrase chunking,
in which the non-recursive cores of noun phrases,
called base NPs, are identified. As the represen-
tative problem in shallow parsing, noun phrase
chunking has received much attention, with the de-
velopment of standard evaluation datasets and with
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
extensive comparisons among methods (McDon-
ald 2005; Sha & Pereira 2003; Kudo & Matsumoto
2001).
Syntactic contexts often have a complex under-
lying structure. Chunk labels are usually far too
general to fully encapsulate the syntactic behavior
of word sequences. In practice, and given the lim-
ited data, the relationship between specific words
and their syntactic contexts may be best modeled
at a level finer than chunk tags but coarser than
lexical identities. For example, in the noun phrase
(NP) chunking task, suppose that there are two lex-
ical sequences, ?He is her ?? and ?He gave her
? ?. The observed sequences, ?He is her? and
?He gave her?, would both be conventionally la-
beled by ?BOB?, where B signifies the ?beginning
NP?, and O the ?outside NP?. However, this label-
ing may be too general to encapsulate their respec-
tive syntactic dynamics. In actuality, they have dif-
ferent latent-structures, crucial in labeling the next
word. For ?He is her ??, the NP started by ?her? is
still incomplete, so the label for ? is likely to be I,
which conveys the continuation of the phrase, e.g.,
?[He] is [her brother]?. In contrast, for ?He gave
her ??, the phrase started by ?her? is normally self-
complete, and makes the next label more likely to
be B, e.g., ?[He] gave [her] [flowers]?.
In other words, latent-dynamics is an interme-
diate representation between input features and la-
bels, and explicitly modeling this can simplify the
problem. In particular, in many real-world cases,
when the part-of-speech tags are not available, the
modeling on latent-dynamics would be particu-
larly important.
In this paper, we model latent-dynamics in
shallow parsing by extending the Latent-Dynamic
Conditional Random Fields (LDCRFs) (Morency
et al 2007), which offer advantages over previ-
841
y y y1 2 m h h h1 2 m
y y y1 2 m
CRF LDCRF
x x x1 2 mx x x1 2 m
Figure 1: Comparison between CRF and LDCRF.
In these graphical models, x represents the obser-
vation sequence, y represents labels and h repre-
sents hidden states assigned to labels. Note that
only gray circles are observed variables. Also,
only the links with the current observation are
shown, but for both models, long range dependen-
cies are possible.
ous learning methods by explicitly modeling hid-
den state variables (see Figure 1). We expect LD-
CRFs to be particularly useful in those cases with-
out POS tags, though this paper is not limited to
this.
The inference technique is one of the most im-
portant components for a structured classification
model. In conventional models like CRFs, the op-
timal label path can be directly searched by using
dynamic programming. However, for latent condi-
tional models like LDCRFs, the inference is kind
of tricky, because of hidden state variables. In this
paper, we propose an exact inference algorithm,
the Best Label Path inference, to efficiently pro-
duce the optimal label sequence on LDCRFs.
The following section describes the related
work. We then review LDCRFs, and propose the
BLP inference. We further present a statistical
interpretation on learned hidden states. Finally,
we show that LDCRF-BLP is particularly effective
when pure word features are used, and when POS
tags are added, as existing systems did, it achieves
comparable results to the best reported systems.
2 Related Work
There is a wide range of related work on shallow
parsing. Shallow parsing is frequently reduced to
sequence labeling problems, and a large part of
previous work uses machine learning approaches.
Some approaches rely on k-order generative proba-
bilistic models of paired input sequences and label
sequences, such as HMMs (Freitag & McCallum
2000; Kupiec 1992) or multilevel Markov mod-
els (Bikel et al 1999). The generative model
provides well-understood training and inference
but requires stringent conditional independence as-
sumptions.
To accommodate multiple overlapping features
on observations, some other approaches view the
sequence labeling problem as a sequence of clas-
sification problems, including support vector ma-
chines (SVMs) (Kudo & Matsumoto 2001) and a
variety of other classifiers (Punyakanok & Roth
2001; Abney et al 1999; Ratnaparkhi 1996).
Since these classifiers cannot trade off decisions at
different positions against each other (Lafferty et
al. 2001), the best classifier based shallow parsers
are forced to resort to heuristic combinations of
multiple classifiers.
A significant amount of recent work has shown
the power of CRFs for sequence labeling tasks.
CRFs use an exponential distribution to model the
entire sequence, allowing for non-local dependen-
cies between states and observations (Lafferty et
al. 2001). Lafferty et al (2001) showed that CRFs
outperform classification models as well as HMMs
on synthetic data and on POS tagging tasks. As for
the task of shallow parsing, CRFs also outperform
many other state-of-the-art models (Sha & Pereira
2003; McDonald et al 2005).
When the data has distinct sub-structures, mod-
els that exploit hidden state variables are advanta-
geous in learning (Matsuzaki et al 2005; Petrov
et al 2007). Sutton et al (2004) presented an
extension to CRF called dynamic conditional ran-
dom field (DCRF) model. As stated by the authors,
training a DCRF model with unobserved nodes
(hidden variables) makes their approach difficult
to optimize. In the vision community, the LD-
CRF model was recently proposed by Morency et
al. (2007), and shown to outperform CRFs, SVMs,
and HMMs for visual sequence labeling.
In this paper, we introduce the concept of latent-
dynamics for shallow parsing, showing how hid-
den states automatically learned by the model
present similar characteristics. We will also pro-
pose an improved inference technique, the BLP,
for producing the most probable label sequence in
LDCRFs.
3 Latent-Dynamic Conditional Random
Fields
The task is to learn a mapping between a sequence
of observations x = x
1
, x
2
, . . . , x
m
and a sequence
of labels y = y
1
, y
2
, . . . , y
m
. Each y
j
is a class la-
842
bel for the j?th token of a word sequence and is a
member of a set Y of possible class labels. For
each sequence, the model also assumes a vector of
hidden state variables h = {h
1
, h
2
, . . . , h
m
}, which
are not observable in the training examples.
Given the above definitions, we define a latent
conditional model as follows:
P(y|x,?) =
?
h
P(y|h, x,?)P(h|x,?), (1)
where ? are the parameters of the model. The LD-
CRF model can seem as a natural extension of the
CRF model, and the CRF model can seem as a spe-
cial case of LDCRFs employing one hidden state
for each label.
To keep training and inference efficient, we re-
strict the model to have disjointed sets of hidden
states associated with each class label. Each h
j
is
a member of a set H
y
j
of possible hidden states for
the class label y
j
. We define H, the set of all pos-
sible hidden states to be the union of all H
y
j
sets.
Since sequences which have any h
j
< H
y
j
will by
definition have P(y|x,?) = 0, we can express our
model as:
P(y|x,?) =
?
h?H
y
1
?...?H
y
m
P(h|x,?), (2)
where P(h|x,?) is defined using the usual con-
ditional random field formulation: P(h|x,?) =
exp ??f(h|x)/
?
?h
exp ??f(h|x), in which f(h|x) is
the feature vector. Given a training set consisting
of n labeled sequences (x
i
, y
i
) for i = 1 . . . n, train-
ing is performed by optimizing the objective func-
tion to learn the parameter ?
?
:
L(?) =
n
?
i=1
log P(y
i
|x
i
,?) ? R(?). (3)
The first term of this equation is the conditional
log-likelihood of the training data. The second
term is the regularizer.
4 BLP Inference on Latent Conditional
Models
For testing, given a new test sequence x, we want
to estimate the most probable label sequence (Best
Label Path), y
?
, that maximizes our conditional
model:
y
?
= argmax
y
P(y|x,?
?
). (4)
In the CRF model, y
?
can be simply searched by
using the Viterbi algorithm. However, for latent
conditional models like LDCRF, the Best Label
Path y
?
cannot directly be produced by the Viterbi
algorithm because of the incorporation of hidden
states.
In this paper, we propose an exact inference al-
gorithm, the Best Label Path inference (BLP), for
producing the most probable label sequence y
?
on
LDCRF. In the BLP schema, top-n hidden paths
HP
n
= {h
1
,h
2
. . . h
n
} over hidden states are effi-
ciently produced by using A
?
search (Hart et al,
1968), and the corresponding probabilities of hid-
den paths P(h
i
|x,?) are gained. Thereafter, based
on HP
n
, the estimated probabilities of various la-
bel paths, P(y|x,?), can be computed by summing
the probabilities of hidden paths, P(h|x,?), con-
cerning the association between hidden states and
each class label:
P(y|x,?) =
?
h: h?H
y
1
?...?H
y
m
?h?HP
n
P(h|x,?). (5)
By using the A
?
search, HP
n
can be extended in-
crementally in an efficient manner, until the algo-
rithm finds that the Best Label Path is ready, and
then the search stops and ends the BLP inference
with success. The algorithm judges that y
?
is ready
when the following condition is achieved:
P(y
1
|x,?) ? P(y
2
|x,?) +
?
h<H
y
1
?...?H
y
m
P(h|x,?), (6)
where y
1
is the most probable label sequence, and
y
2
is the second ranked label sequence estimated
by using HP
n
. It would be straightforward to prove
that y
?
= y
1
, and further search is unnecessary, be-
cause in this case, the unknown probability mass
can not change the optimal label path. The un-
known probability mass can be computed by using
?
h<H
y
1
?...?H
y
m
P(h|x,?) = 1 ?
?
h?H
y
1
?...?H
y
m
P(h|x,?). (7)
The top-n hidden paths of HP
n
produced by the
A
?
-search are exact, and the BLP inference is ex-
act. To guarantee HP
n
is exact in our BLP in-
ference, an admissible heuristic function should
be used in A
?
search (Hart et al, 1968). We use
a backward Viterbi algorithm (Viterbi, 1967) to
compute the heuristic function of the forward A
?
search:
Heu
i
(h
j
) = max
h
?
i
=h
j
?h
?
i
?HP
|h|
i
P
?
(h
?
|x,?
?
), (8)
843
where h
?
i
= h
j
represents a partial hidden path
started from the hidden state h
j
, and HP
|h|
i
rep-
resents all possible partial hidden paths from the
position i to the ending position |h| . Heu
i
(h
j
) is
an admissible heuristic function for the A
?
search
over hidden paths, therefore HP
n
is exact and BLP
inference is exact.
The BLP inference is efficient when the prob-
ability distribution among the hidden paths is in-
tensive. By combining the forward A
?
with the
backward Viterbi algorithm, the time complexity
of producing HP
n
is roughly a linear complexity
concerning its size. In practice, on the CoNLL test
data containing 2,012 sentences, the BLP infer-
ence finished in five minutes when using the fea-
ture set based on both word and POS information
(see Table 3). The memory consumption is also
relatively small, because it is an online style algo-
rithm and it is not necessary to preserve HP
n
.
In this paper, to make a comparison, we also
study the Best Hidden Path inference (BHP):
y
BHP
= argmax
y
P(h
y
|x,?
?
), (9)
where h
y
? H
y
1
? . . . ?H
y
m
. In other words, the
Best Hidden Path is the label sequence that is di-
rectly projected from the most probable hidden
path h
?
.
In (Morency et al 2007), y
?
is estimated by us-
ing the Best Point-wise Marginal Path (BMP). To
estimate the label y
j
of token j, the marginal prob-
abilities P(h
j
= a|x,?) are computed for possible
hidden states a ? H. Then, the marginal probabili-
ties are summed and the optimal label is estimated
by using the marginal probabilities.
The BLP produces y
?
while the BHP and the
BMP perform an estimation on y
?
. We will make
an experimental comparison in Section 6.
5 Analyzing Latent-Dynamics
The chunks in shallow parsing are represented with
the three labels shown in Table 1, and shallow pars-
ing is treated as a sequence labeling task with those
three labels. A challenge for most shallow parsing
approaches is to determine the concepts learned by
the model. In this section, we show how we can
analyze the latent-dynamics.
5.1 Analyzing Latent-Dynamics
In this section, we show how to analyze the charac-
teristics of the hidden states. Our goal is to find the
words characterizing a specific hidden state, and
B words beginning a chunk
I words continuing a chunk
O words being outside a chunk
Table 1: Shallow parsing labels.
then look at the selected words with their associ-
ated POS tags to determine if the LDCRF model
has learned meaningful latent-dynamics.
In the experiments reported in this section, we
did not use the features on POS tags in order to
isolate the model?s capability of learning latent dy-
namics. In other words, the model could simply
learn the dynamics of POS tags as the latent dy-
namics if the model is given the information about
POS tags. The features used in the experiments are
listed on the left side (Word Features) in Table 3.
The main idea is to look at the marginal proba-
bilities P(h
j
= a|x,?) for each word j, and select
the hidden state a
?
with the highest probability. By
counting how often a specific word selected a as
the optimal hidden state, i.e., ?(w, a), we can cre-
ate statistics about the relationship between hidden
states and words. We define relative frequency as
the number of times a specific word selected a hid-
den state while normalized by the global frequency
of this word:
RltFreq(w, h
j
) =
Freq( ?(w, h
j
) )
Freq(w)
. (10)
5.2 Learned Latent-Dynamics from CoNLL
In this subsection, we show the latent-dynamics
learned automatically from the CoNLL dataset.
The details of these experiments are presented in
the following section.
The most frequent three words corresponding to
the individual hidden states of the labels, B and O,
are shown in Table 2. As shown, the automati-
cally learned hidden states demonstrate prominent
characteristics. The extrinsic label B, which begins
a noun phrase, is automatically split into 4 sub-
categories: wh-determiners (WDT, such as ?that?)
together with wh-pronouns (WP, such as ?who?),
the determiners (DT, such as ?any, an, a?), the per-
sonal pronouns (PRP, such as ?they, we, he?), and
the singular proper nouns (NNP, such as ?Nasdaq,
Florida?) together with the plural nouns (NNS,
such as ?cities?). The results of B1 suggests that
the wh-determiners represented by ?that?, and the
wh-pronouns represented by ?who?, perform simi-
844
Labels HidStat Words POS RltFreq
B
That WDT 0.85
B1 who WP 0.49
Who WP 0.33
any DT 1.00
B2 an DT 1.00
a DT 0.98
They PRP 1.00
B3 we PRP 1.00
he PRP 1.00
Nasdaq NNP 1.00
B4 Florida NNP 0.99
cities NNS 0.99
O
But CC 0.88
O1 by IN 0.73
or IN 0.67
4.6 CD 1.00
O2 1 CD 1.00
11 CD 0.62
were VBD 0.94
O3 rose VBD 0.93
have VBP 0.92
been VBN 0.97
O4 be VB 0.94
to TO 0.92
Table 2: Latent-dynamics learned automatically by
the LDCRF model. This table shows the top three
words and their gold-standard POS tags for each
hidden states.
lar roles in modeling the dynamics in shallow pars-
ing. Further, the singular proper nouns and the
plural nouns are grouped together, suggesting that
they may perform similar roles. Moreover, we can
notice that B2 and B3 are highly consistent.
The label O is automatically split into the coordi-
nating conjunctions (CC) together with the prepo-
sitions (IN) indexed by O1, the cardinal numbers
(CD) indexed by O2, the past tense verbs (VBD)
together with the personal verbs (VBP) indexed by
O3, and another sub-category, O4. From the results
we can find that gold-standard POS tags may not
be adequate in modeling latent-dynamics in shal-
low parsing, as we can notice that three hidden
states out of four (O1, O3 and O4) contains relat-
ing but different gold-standard POS tags.
6 Experiments
Following previous studies on shallow parsing, our
experiments are performed on the CoNLL 2000
Word Features:
{w
i?2
, w
i?1
, w
i
, w
i+1
, w
i+2
, w
i?1
w
i
, w
i
w
i+1
}
?{h
i
, h
i?1
h
i
, h
i?2
h
i?1
h
i
}
POS Features:
{t
i?1
, t
i
, t
i+1
, t
i?2
t
i?1
, t
i?1
t
i
, t
i
t
i+1
, t
i+1
t
i+2
,
t
i?2
t
i?1
t
i
, t
i?1
t
i
t
i+1
, t
i
t
i+1
t
i+2
}
?{h
i
, h
i?1
h
i
, h
i?2
h
i?1
h
i
}
Table 3: Feature templates used in the experi-
ments. w
i
is the current word; t
i
is current POS
tag; and h
i
is the current hidden state (for the case
of latent models) or the current label (for the case
of conventional models).
data set (Sang & Buchholz 2000; Ramshow &
Marcus 1995). The training set consists of 8,936
sentences, and the test set consists of 2,012 sen-
tences. The standard evaluation metrics for this
task are precision p (the fraction of output chunks
matching the reference chunks), recall r (the frac-
tion of reference chunks returned), and the F-
measure given by F = 2pr/(p + r).
6.1 LDCRF for Shallow Parsing
We implemented LDCRFs in C++, and optimized
the system to cope with large scale problems, in
which the feature dimension is beyond millions.
We employ similar predicate sets defined in Sha
& Pereira (2003). We follow them in using predi-
cates that depend on words as well as POS tags in
the neighborhood of a given position, taking into
account only those 417,835 features which occur
at least once in the training data. The features are
listed in Table 3.
As for numerical optimization (Malouf 2002;
Wallach 2002), we performed gradient decent with
the Limited-Memory BFGS (L-BFGS) optimiza-
tion technique (Nocedal & Wright 1999). L-BFGS
is a second-order Quasi-Newton method that nu-
merically estimates the curvature from previous
gradients and updates. With no requirement on
specialized Hessian approximation, L-BFGS can
handle large-scale problems in an efficient manner.
We implemented an L-BFGS optimizer in C++ by
modifying the OWLQN package (Andrew & Gao
2007) developed by Galen Andrew. In our exper-
iments, storing 10 pairs of previous gradients for
the approximation of the function?s inverse Hes-
sian worked well, making the amount of the ex-
tra memory required modest. Using more pre-
vious gradients will probably decrease the num-
845
ber of iterations required to reach convergence,
but would increase memory requirements signifi-
cantly. To make a comparison, we also employed
the Conjugate-Gradient (CG) optimization algo-
rithm. For details of CG, see Shewchuk (1994).
Since the objective function of the LDCRF
model is non-convex, it is suggested to use the ran-
dom initialization of parameters for the training.
To reduce overfitting, we employed an L
2
Gaus-
sian weight prior (Chen & Rosenfeld 1999). Dur-
ing training and validation, we varied the number
of hidden states per label (from 2 to 6 states per
label), and also varied the L
2
-regularization term
(with values 10
k
, k from -3 to 3). Our experiments
suggested that using 4 or 5 hidden states per label
for the shallow parser is a viable compromise be-
tween accuracy and efficiency.
7 Results and Discussion
7.1 Performance on Word Features
As discussed in Section 4, it is preferred to not
use the features on POS tags in order to isolate
the model?s capability of learning latent dynam-
ics. In this sub-section, we use pure word fea-
tures with their counts above 10 in the training data
to perform experimental comparisons among dif-
ferent inference algorithms on LDCRFs, including
BLP, BHP, and existing BMP.
Since the CRF model is one of the success-
ful models in sequential labeling tasks (Lafferty et
al. 2001; Sha & Pereira 2003; McDonald et al
2005), in this section, we also compare LDCRFs
with CRFs. We tried to make experimental results
more comparable between LDCRF and CRF mod-
els, and have therefore employed the same fea-
tures set, optimizer and fine-tuning strategy be-
tween LDCRF and CRF models.
The experimental results are shown in Table 4.
In the table, Acc. signifies ?label accuracy?, which
is useful for the significance test in the follow-
ing sub-section. As shown, LDCRF-BLP outper-
forms LDCRF-BHP and LDCRF-BMP, suggesting
that BLP inference
1
is superior. The superiority
of BLP is statistically significant, which will be
shown in next sub-section. On the other side, all
the LDCRF models outperform the CRF model. In
particular, the gap between LDCRF-BLP and CRF
is 1.53 percent.
1
In practice, for efficiency, we approximated the BLP on a
few sentences by limiting the number of search steps.
Models: WF Acc. Pre. Rec. F
1
LDCRF-BLP 97.01 90.33 88.91 89.61
LDCRF-BHP 96.52 90.26 88.21 89.22
LDCRF-BMP 97.26 89.83 89.06 89.44
CRF 96.11 88.12 88.03 88.08
Table 4: Experimental comparisons among differ-
ent inference algorithms on LDCRFs, and the per-
formance of CRFs using the same feature set on
pure word features. The BLP inference outper-
forms the BHP and BMP inference. LDCRFs out-
perform CRFs.
Models F
1
Gap Acc. Gap Sig.
BLP vs. BHP 0.39 0.49 1e-10
BLP vs. CRF 1.53 0.90 5e-13
Table 5: The significance tests. LDCRF-BLP is
significantly more accurate than LDCRF-BHP and
CRFs.
7.2 Labeling Accuracy and Significance Test
As shown in Table 4, the accuracy rate for individ-
ual labeling decisions is over-optimistic as a mea-
sure for shallow parsing. Nevertheless, since test-
ing the significance of shallow parsers? F-measures
is tricky, individual labeling accuracy provides a
more convenient basis for statistical significance
tests (Sha & Pereira 2003). One such test is the
McNemar test on paired observations (Gillick &
Cox 1989). As shown in Table 5, for the LD-
CRF model, the BLP inference schema is sta-
tistically more accurate than the BHP inference
schema. Also, Evaluations show that the McNe-
mar?s value on labeling disagreement between the
LDCRF-BLP and CRF models is 5e-13, suggest-
ing that LDCRF-BLP is significantly more accu-
rate than CRFs.
On the other hand, the accuracy rate of BMP in-
ference is a special case. Since the BMP inference
is essentially an accuracy-first inference schema,
the accuracy rate and the F-measure have a differ-
ent relation in BMP. As we can see, the individual
labeling accuracy achieved by the LDCRF-BMP
model is as high as 97.26%, but its F-measure is
still lower than LDCRF-BLP.
7.3 Convergence Speed
It would be interesting to compare the convergence
speed between the objective loss function of LD-
CRFs and CRFs. We apply the L-BFGS optimiza-
846
 150
 200
 250
 300
 350
 400
 450
 500
 0  50  100  150  200  250
P
en
al
iz
ed
 L
os
s
Passes
LDCRF
CRF
Figure 2: The value of the penalized loss based on
the number of iterations: LDCRFs vs. CRFs.
 160
 180
 200
 220
 240
 260
 280
 0  50  100  150  200  250
P
en
al
iz
ed
 L
os
s
Passes
L-BFGS
CG
Figure 3: Training the LDCRF model: L-BFGS
vs. CG.
tion algorithm to optimize the loss function of LD-
CRF and CRF models, making a comparison be-
tween them. We find that the iterations required
for the convergence of LDCRFs is less than for
CRFs (see Figure 2). Normally, the LDCRF model
arrives at the plateau of convergence in 120-150
iterations, while CRFs require 210-240 iterations.
When we replace the L-BFGS optimizer by the CG
optimization algorithm, we observed as well that
LDCRF converges faster on iteration numbers than
CRF does.
On the contrary, however, the time cost of the
LDCRF model in each iteration is higher than the
CRF model, because of the incorporation of hid-
den states. The time cost of the LDCRF model
in each iteration is roughly a quadratic increase
concerning the increase of the number of hidden
states. Therefore, though the LDCRF model re-
quires less passes for the convergence, it is practi-
cally slower than the CRF model. Improving the
scalability of the LDCRF model would be a inter-
esting topic in the future.
Furthermore, we make a comparison between
Models: WF+POS Pre. Rec. F
1
LDCRF-BLP 94.65 94.03 94.34
CRF
N/A N/A 93.6
(Vishwanathan et al 06)
CRF
94.57 94.00 94.29
(McDonald et al 05)
Voted perceptron
N/A N/A 93.53
(Collins 02)
Generalized Winnow
93.80 93.99 93.89
(Zhang et al 02)
SVM combination
94.15 94.29 94.22
(Kudo & Matsumoto 01)
Memo. classifier
93.63 92.89 93.26
(Sang 00)
Table 6: Performance of the LDCRF-BLP model,
and the comparison with CRFs and other success-
ful approaches. In this table, all the systems have
employed POS features.
the L-BFGS and the CG optimizer for LDCRFs.
We observe that the L-BFGS optimizer is slightly
faster than CG on LDCRFs (see Figure 3), which
echoes the comparison between the L-BFGS and
the CG optimizing technique on the CRF model
(Sha & Pereira 2003).
7.4 Comparisons to Other Systems with POS
Features
Performance of the LDCRF-BLP model and some
of the best results reported previously are summa-
rized in Table 6. Our LDCRF model achieved
comparable performance to those best reported
systems in terms of the F-measure.
McDonald et al (2005) achieved an F-measure
of 94.29% by using a CRF model. By employing a
multi-model combination approach, Kudo & Mat-
sumoto (2001) also achieved a good performance.
They use a combination of 8 kernel SVMs with
a heuristic voting strategy. An advantage of LD-
CRFs over max-margin based approaches is that
LDCRFs can output N-best label sequences and
their probabilities using efficient marginalization
operations, which can be used for other compo-
nents in an information extraction system.
8 Conclusions and Future Work
In this paper, we have shown that automatic model-
ing on ?latent-dynamics? can be useful in shallow
parsing. By analyzing the automatically learned
847
hidden states, we showed how LDCRFs can natu-
rally learn latent-dynamics in shallow parsing.
We proposed an improved inference algorithm,
the BLP, for LDCRFs. We performed experiments
using the CoNLL data, and showed how the BLP
inference outperforms existing inference engines.
When further employing POS features as other
systems did, the performance of the LDCRF-BLP
model is comparable to those best reported results.
The LDCRF model demonstrates a significant ad-
vantage over other models on pure word features
in this paper. We expect it to be particularly useful
in the real-world tasks without rich features.
The latent conditional model handles latent-
dynamics naturally, and can be easily extended to
other labeling tasks. Also, the BLP inference algo-
rithm can be extended to other latent conditional
models for producing optimal label sequences. As
a future work, we plan to further speed up the BLP
algorithm.
Acknowledgments
Many thanks to Yoshimasa Tsuruoka for helpful
discussions on the experiments and paper-writing.
This research was partially supported by Grant-
in-Aid for Specially Promoted Research 18002007
(MEXT, Japan). The work at the USC Institute for
Creative Technology was sponsored by the U.S.
Army Research, Development, and Engineering
Command (RDECOM), and the content does not
necessarily reflect the position or the policy of the
Government, and no official endorsement should
be inferred.
References
Abney, S. 1991. Parsing by chunks. In R. Berwick, S. Ab-
ney, and C. Tenny, editors, Principle-based Parsing. Kluwer
Academic Publishers.
Abney, S.; Schapire, R. E. and Singer, Y. 1999. Boosting
applied to tagging and PP attachment. In Proc. EMNLP/VLC-
99.
Andrew, G. and Gao, J. 2007. Scalable training of L1-
regularized log-linear models. In Proc. ICML-07.
Bikel, D. M.; Schwartz, R. L. and Weischedel, R. M. 1999.
An algorithm that learns what?s in a name. Machine Learning,
34: 211-231.
Chen, S. F. and Rosenfeld, R. 1999. A Gaussian prior
for smooth-ing maximum entropy models. Technical Report
CMU-CS-99-108, CMU.
Collins, M. 2002. Discriminative training methods for hid-
den Markov models: Theory and experiments with perceptron
algorithms. In Proc. EMNLP-02.
Freitag, D. and McCallum, A. 2000. Information extrac-
tion with HMM structures learned by stochastic optimization.
In Proc. AAAI-00.
Gillick, L. and Cox, S. 1989. Some statistical issues in
the comparison of speech recognition algorithms. In Interna-
tional Conference on Acoustics Speech and Signal Process-
ing, v1, pages 532-535.
Hart, P.E.; Nilsson, N.J.; and Raphael, B. 1968. A formal
basis for the heuristic determination of minimum cost path.
IEEE Trans. On System Science and Cybernetics, SSC-4(2):
100-107.
Kudo, T. and Matsumoto, Y. 2001. Chunking with support
vector machines. In Proc. NAACL-01.
Kupiec, J. 1992. Robust part-of-speech tagging using a
hidden Markov model. Computer Speech and Language.
6:225-242.
Lafferty, J.; McCallum, A. and Pereira, F. 2001. Condi-
tional random fields: Probabilistic models for segmenting and
labeling sequence data. In Proc. ICML-01, pages 282-289.
Malouf, R. 2002. A comparison of algorithms for maxi-
mum entropy parameter estimation. In Proc. CoNLL-02.
Matsuzaki, T.; Miyao Y. and Tsujii, J. 2005. Probabilistic
CFG with Latent Annotations. In Proc. ACL-05.
McDonald, R.; Crammer, K. and Pereira, F. 2005. Flexible
Text Segmentation with Structured Multilabel Classification.
In Proc. HLT/EMNLP-05, pages 987- 994.
Morency, L.P.; Quattoni, A. and Darrell, T. 2007. Latent-
Dynamic Discriminative Models for Continuous Gesture
Recognition. In Proc. CVPR-07, pages 1- 8.
Nocedal, J. and Wright, S. J. 1999. Numerical Optimiza-
tion. Springer.
Petrov, S.; Pauls, A.; and Klein, D. 2007. Discriminative
log-linear grammars with latent variables. In Proc. NIPS-07.
Punyakanok, V. and Roth, D. 2001. The use of classifiers
in sequential inference. In Proc. NIPS-01, pages 995-1001.
MIT Press.
Ramshaw, L. A. and Marcus, M. P. 1995. Text chunking
using transformation-based learning. In Proc. Third Work-
shop on Very Large Corpora. In Proc. ACL-95.
Ratnaparkhi, A. 1996. A maximum entropy model for
part-of-speech tagging. In Proc. EMNLP-96.
Sang, E.F.T.K. 2000. Noun Phrase Representation by Sys-
tem Combination. In Proc. ANLP/NAACL-00.
Sang, E.F.T.K and Buchholz, S. 2000. Introduction to the
CoNLL-2000 shared task: Chunking. In Proc. CoNLL-00,
pages 127-132.
Sha, F. and Pereira, F. 2003. Shallow Parsing with Condi-
tional Random Fields. In Proc. HLT/NAACL-03.
Shewchuk, J. R. 1994. An introduction to the
conjugate gradient method without the agonizing pain.
http://www.2.cs.cmu.edu/jrs/jrspapers.html/#cg.
Sutton, C.; Rohanimanesh, K. and McCallum, A. 2004.
Dynamic conditional random fields: Factorized probabilistic
models for labeling and segmenting sequence data. In Proc.
ICML-04.
Viterbi, A.J. 1967. Error bounds for convolutional codes
and an asymptotically optimum decoding algorithm. IEEE
Transactions on Information Theory. 13(2):260-269.
Vishwanathan, S.; Schraudolph, N. N.; Schmidt, M.W. and
Murphy, K. 2006. Accelerated training of conditional random
fields with stochastic meta-descent. In Proc. ICML-06.
Wallach, H. 2002. Efficient training of conditional random
fields. In Proc. 6th Annual CLUK Research Colloquium.
Zhang, T.; Damerau, F. and Johnson, D. 2002. Text chunk-
ing based on a generalization of winnow. Journal of Machine
Learning Research, 2:615-637.
848
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 860?868,
Beijing, August 2010
Latent Mixture of Discriminative Experts
for Multimodal Prediction Modeling
Derya Ozkan, Kenji Sagae and Louis-Philippe Morency
USC Institute for Creative Technologies
{ozkan,sagae,morency}@ict.usc.edu
Abstract
During face-to-face conversation, people
naturally integrate speech, gestures and
higher level language interpretations to
predict the right time to start talking or
to give backchannel feedback. In this
paper we introduce a new model called
Latent Mixture of Discriminative Experts
which addresses some of the key issues
with multimodal language processing: (1)
temporal synchrony/asynchrony between
modalities, (2) micro dynamics and (3) in-
tegration of different levels of interpreta-
tion. We present an empirical evaluation
on listener nonverbal feedback prediction
(e.g., head nod), based on observable be-
haviors of the speaker. We confirm the im-
portance of combining four types of mul-
timodal features: lexical, syntactic struc-
ture, eye gaze, and prosody. We show
that our Latent Mixture of Discriminative
Experts model outperforms previous ap-
proaches based on Conditional Random
Fields (CRFs) and Latent-Dynamic CRFs.
1 Introduction
Face-to-face communication is highly interactive.
Even when only one person speaks at a time,
other participants exchange information continu-
ously amongst themselves and with the speaker
through gestures, gaze and prosody. These differ-
ent channels contain complementary information
essential to interpretation and understanding of
human behaviors (Oviatt, 1999). Psycholinguistic
studies also suggest that gesture and speech come
from a single underlying mental process, and they
Pitch
Words
Gaze
TimeP
(no
d)
Look  at listener
Speaker
Listener
Prediction
Figure 1: Example of multimodal prediction
model: listener nonverbal backchannel prediction
based on speaker?s speech and eye gaze. As the
speaker says the word her, which is the end of the
clause (her is also the object of the verb bother-
ing), and lowers the pitch while looking back at
the listener and eventually pausing, the listener
is then very likely to head nod (i.e., nonverbal
backchannel).
are related both temporally and semantically (Mc-
Neill, 1992; Cassell and Stone, 1999; Kendon,
2004).
A good example of such complementarity is
how people naturally integrate speech, gestures
and higher level language to predict when to give
backchannel feedback. Building computational
models of such a predictive process is challeng-
ing since it involves micro dynamics and temporal
relationship between cues from different modali-
ties (Quek, 2003). Figure 1 shows an example of
backchannel prediction where a listener head nod
860
is more likely. For example, a temporal sequence
from the speaker where he/she reaches the end of
segment (syntactic feature) with a low pitch and
looks at the listener before pausing is a good op-
portunity for the listener to give nonverbal feed-
back (e.g., head nod). These prediction models
have broad applicability, including the improve-
ment of nonverbal behavior recognition, the syn-
thesis of natural animations for robots and virtual
humans, the training of cultural-specific nonver-
bal behaviors, and the diagnoses of social disor-
ders (e.g., autism spectrum disorder).
In this paper we introduce a new model
called Latent Mixture of Discriminative Experts
(LMDE) which addresses some of the key issues
with multimodal language processing: (1) tempo-
ral synchrony/asynchrony between modalities, (2)
micro dynamics and (3) integration of different
levels of interpretation. We present an empirical
evaluation on nonverbal feedback prediction (e.g.,
head nod) confirming the importance of combin-
ing different types of multimodal features. We
show that our LMDE model outperforms previ-
ous approaches based Conditional Random Fields
(CRFs) and Latent-Dynamic CRFs.
2 Related Work
Earlier work in multimodal language processing
focused on multimodal dialogue systems where
the gestures and speech may be constrained (John-
ston, 1998; Jurafsky et al, 1998). Most of
the research in multimodal language processing
over the past decade fits within two main trends
that have emerged: (1) recognition of individ-
ual multimodal actions such as speech and ges-
tures (e.g, (Eisenstein et al, 2008; Frampton et
al., 2009; Gravano et al, 2007)), and (2) recog-
nition/summarization of the social interaction be-
tween more than one participants (e.g., meeting
analysis (Heylen and op den Akker, 2007; Moore,
2007; Murray and Carenini, 2009; Jovanovic et
al., 2006)).
The work described in this paper can be seen
from a third intermediate category where multi-
modal cues from one person is used to predict
the social behavior of another participant. This
type of predictive models has been mostly stud-
ied in the context of embodied conversational
agents (Nakano et al, 2003; Nakano et al, 2007).
In particular, backchannel feedback (the nods and
paraverbals such as ?uh-hu? and ?mm-hmm? that
listeners produce as someone is speaking) has re-
ceived considerable interest due to its pervasive-
ness across languages and conversational contexts
and this paper addresses the problem of how to
predict and generate this important class of dyadic
nonverbal behavior.
Several researchers have developed models to
predict when backchannel should happen. In gen-
eral, these results are difficult to compare as they
utilize different corpora and present varying eval-
uation metrics. Ward and Tsukahara (2000) pro-
pose a unimodal approach where backchannels
are associated with a region of low pitch last-
ing 110ms during speech. Models were pro-
duced manually through an analysis of English
and Japanese conversational data. Nishimura
et al (2007) present a unimodal decision-tree
approach for producing backchannels based on
prosodic features. Cathcart et al (2003) propose a
unimodal model based on pause duration and tri-
gram part-of-speech frequency. The model was
constructed by identifying, from the HCRC Map
Task Corpus (Anderson et al, 1991), trigrams
ending with a backchannel. Fujie et al (2004)
used Hidden Markov Models to perform head nod
recognition. In their paper, they combined head
gesture detection with prosodic low-level features
from the same person to determine strongly pos-
itive, weak positive and negative responses to
yes/no type utterances.
In recent years, great research has shown the
strength of latent variable models for natural lan-
guage processing (Blunsom et al, 2008). One of
the most relevant works is that of Eisenstein and
Davis (2007), which presents a latent conditional
model for fusion of multiple modalities (speech
and gestures). One of the key difference of our
work is that we are explicitly modeling the mi-
cro dynamics and temporal relationship between
modalities.
3 Multimodal Prediction Models
Human face-to-face communication is a little like
a dance, in that participants continuously adjust
their behaviors based on verbal and nonverbal dis-
861
plays and signals. A topic of central interest in
modeling such behaviors is the patterning of in-
terlocutor actions and interactions, moment-by-
moment, and one of the key challenges is iden-
tifying the patterns that best predict specific ac-
tions. Thus we are interested in developing pre-
dictive models of communication dynamics that
integrate previous and current actions from all in-
terlocutors to anticipate the most likely next ac-
tions of one or all interlocutors. Humans are good
at this: they have an amazing ability to predict, at
a micro-level, the actions of an interlocutor (Bave-
las et al, 2000); and we know that better predic-
tions can correlate with more empathy and better
outcomes (Goldberg, 2005; Fuchs, 1987).
With turn-taking being perhaps the best-known
example, we now know a fair amount about some
aspects of communication dynamics, but much
less about others. However, recent advances in
machine learning and experimental methods, and
recent findings from a variety of perspectives, in-
cluding conversation analysis, social signal pro-
cessing, adaptation, corpus analysis and model-
ing, perceptual experiments, and dialog systems-
building and experimentation, mean that the time
is ripe to start working towards more comprehen-
sive predictive models.
The study of multimodal prediction models
bring a new series of research challenges:
MULTIMODAL ASYNCHRONY While speech
and gestures seem to come from a single under-
lying mental process (McNeill, 1992), they not
always happen at the same time, making it hard
for earlier multimodal fusion approaches based
on synchrony. A multimodal prediction model
needs to be able to learn automatically the tempo-
ral relationship (and relative importance) between
modalities.
MICRO DYNAMICS The dynamic between mul-
timodal signals should be taken at a micro level
since many of the interactions between speech and
gesture happen at the sub-gesture level or sub-
word level (Quek, 2003). Typical word-based
sampling may not be sufficient and instead a
higher sampling rate should be used.
LIMITED ANNOTATED DATA Given the time re-
quirement to correctly annotate multimodal data,
Figure 2: Latent Mixture of Discriminative Ex-
perts: a new dynamic model for multimodal fu-
sion. In this graphical model, xj represents the
jth multimodal observation, hj is a hidden state
assigned to xj , and yj the class label of xj . Gray
circles are latent variables. The micro dynamics
and multimodal temporal relationships are auto-
matically learned by the hidden states hj during
the learning phase.
most multimodal datasets contain only a limited
number of labeled examples. Since many ma-
chine learning algorithms rely on a large training
corpus, effective training of a predictive model on
multimodal datasets is challenging.
4 Latent Mixture of Discriminative
Experts
In this paper we present a multimodal fusion al-
gorithm, called Latent Mixture of Discriminative
Experts (shown in Figure 2), that addresses the
three challenges discussed in the previous section.
The hidden states of LMDE automatically learn
the temporal asynchrony between modalities. By
using a constant sample rate of 30Hz in our ex-
periments, we can model the micro dynamics of
speech and prosody (e.g., change of intonation
in the middle of a word). And finally, by train-
ing separate experts for each modalities, we im-
prove the prediction performance even with lim-
ited datasets.
The task of our LMDE model is to learn a map-
ping between a sequence of multimodal observa-
tions x = {x1, x2, ..., xm} and a sequence of la-
bels y = {y1, y2, ..., ym}. Each yj is a class la-
bel for the jth frame of a video sequence and is a
member of a set Y of possible class labels, for ex-
ample, Y = {head-nod,other-gesture}.
862
Each frame observation xj is represented by a fea-
ture vector ?(xj) ? Rd, for example, the prosodic
features at each sample. For each sequence, we
also assume a vector of ?sub-structure? variables
h = {h1, h2, ..., hm}. These variables are not ob-
served in the training examples and will therefore
form a set of hidden variables in the model.
Following Morency et al (2007), we define our
LMDE model as follows:
P (y | x, ?) = ?
h
P (y | h, x, ?)P (h | x, ?) (1)
where ? is the model parameters that is to be esti-
mated from training data.
To keep training and inference tractable,
Morency et al (2007) restrict the model to have
disjoint sets of hidden states associated with each
class label. Each hj is a member of a set Hyj
of possible hidden states for the class label yj .
H, the set of all possible hidden states, is defined
to be the union of all Hy sets. Since sequences
which have any hj /? Hyj will by definition have
P (y | h, x, ?) = 0, latent conditional model be-
comes:
P (y | x, ?) = ?
h:?hj?Hyj
P (h | x, ?). (2)
What differentiates our LMDE model from the
original work of Morency et al is the definition of
P (h|x, ?):
P (h| x, ?) =
exp
( ?
l ?l ? Tl(h, x)+?
? ?? ? P?(y|x, ??)
)
Z(x, ?) ,
(3)
whereZ is the partition function and P?(y|x) is
the conditional distribution of the expert indexed
by ?. The expert conditional distributions are de-
fined P?(y|x, ??) using the usual conditional ran-
dom field formulation:
P?(y| x, ??) = exp (
?
k ??,k ? F?,k(y, x))
Z?(x, ??) , (4)
F?,k is defined as
F?,k(y, x) =
m?
j=1
f?,k(yj?1, yj , x, j),
and each feature function f?,k(yj?1, yj , x, j) is
either a state function sk(yj , x, j) or a transition
function tk(yj?1, yj , x, j). State functions sk de-
pend on a single hidden variable in the model
while transition functions tk can depend on pairs
of hidden variables. Tl(h, x), defined in Equa-
tion 3, is a special case, summing only over
the transition feature functions tl(hl?1, hl, x, l).
Each expert ? contains a different subset of
f?,k(yj?1, yj , x, j). These feature functions are
defined in Section 5.2.
4.1 Learning Model Parameters
Given a training set consisting of n labeled se-
quences (xi,yi) for i = 1...n, training is done in
a two step process. First each expert ? is trained
following (Kumar and Herbert., 2003; Lafferty et
al., 2001) objective function to learn the parame-
ter ???:
L(??) =
n?
i=1
logP?(yi | xi, ??)? 12?2 ||??||
2
(5)
The first term in Eq. 5 is the conditional log-
likelihood of the training data. The second term
is the log of a Gaussian prior with variance ?2,
i.e., P (??) ? exp
( 1
2?2 ||??||2
).
Then the marginal probabilities P?(yj =
a | y, x, ???), are computed using belief prop-
agation and used as input for Equation 3. The
optimal parameter ?? was learned using the log-
likelyhood of the conditional probability defined
in Equation 2 (i.e., no regularization).
4.2 Inference
For testing, given a new test sequence x, we want
to estimate the most probable sequence of labels
y? that maximizes our LMDE model:
y? = argmaxy
?
h:?hi?Hyi
P (h | x, ??) (6)
5 Experimental Setup
We evaluate our Latent Mixture of Discrimina-
tive Experts on the multimodal task of predicting
listener nonverbal backchannel (i.e., head nods).
Backchannel feedback (the nods and paraverbals
such as ?uh-hu? and ?mm-hmm? that listeners
863
produce as some is speaking) has received con-
siderable interest due to its pervasiveness across
languages and conversational contexts.
5.1 Dataset
We are using the RAPPORT dataset from (Maat-
man et al, 2005), which contains 47 dyadic inter-
actions between a speaker and a listener. Data is
drawn from a study of face-to-face narrative dis-
course (?quasi-monologic? storytelling). In this
dataset, participants in groups of two were told
they were participating in a study to evaluate a
communicative technology. Subjects were ran-
domly assigned the role of speaker and listener.
The speaker viewed a short segment of a video
clip taken from the Edge Training Systems, Inc.
Sexual Harassment Awareness video. After the
speaker finished viewing the video, the listener
was led back into the computer room, where the
speaker was instructed to retell the stories por-
trayed in the clips to the listener. The listener
was asked to not talk during the story retelling.
Elicited stories were approximately two minutes
in length on average. Participants sat approxi-
mately 8 feet apart. Video sequences were manu-
ally annotated to determine the ground truth head
nod labels. A total of 587 head nods occured over
all video sequences.
5.2 Multimodal Features
This section describes the different multimodal
features used to create our five experts.
PROSODY Prosody refers to the rhythm, pitch and
intonation of speech. Several studies have demon-
strated that listener feedback is correlated with a
speaker?s prosody (Nishimura et al, 2007; Ward
and Tsukahara, 2000; Cathcart et al, 2003). For
example, Ward and Tsukahara (2000) show that
short listener backchannels (listener utterances
like ?ok? or ?uh-huh? given during a speaker?s ut-
terance) are associated with a lowering of pitch
over some interval. Listener feedback often fol-
lows speaker pauses or filled pauses such as
?um? (see (Cathcart et al, 2003)). Using openS-
MILE (Eyben et al, 2009) toolbox, we extract the
following prosodic features, including standard
linguistic annotations and the prosodic features
suggested by Ward and Tsukhara: downslopes in
pitch continuing for at least 40ms, regions of pitch
lower than the 26th percentile continuing for at
least 110ms (i.e., lowness), drop or rise in energy
of speech (i.e., energy edge), Fast drop or rise in
energy of speech (i.e., energy fast edge), vowel
volume (i.e., vowels are usually spoken softer)
and Pause in speech (i.e., no speech).
VISUAL GESTURES Gestures performed by the
speaker are often correlated with listener feed-
back (Burgoon et al, 1995). Eye gaze, in particu-
lar, has often been implicated as eliciting listener
feedback. Thus, we manually annotate the follow-
ing contextual feature: speaker looking at the lis-
tener.
LEXICAL Some studies have suggested an asso-
ciation between lexical features and listener feed-
back (Cathcart et al, 2003). Using the transcrip-
tions, we included all individual words (i.e., uni-
grams) spoken by the speaker during the interac-
tions.
SYNTACTIC STRUCTURE Finally, we attempt
to capture syntactic information that may pro-
vide relevant cues by extracting four types of fea-
tures from a syntactic dependency structure cor-
responding to the utterance. The syntactic struc-
ture is produced automatically using a CRF part-
of-speech (POS) tagger and a data-driven left-to-
right shift-reduce dependency parser (Sagae and
Tsujii, 2007), both trained on POS tags and de-
pendency trees extracted from the Switchboard
section of the Penn Treebank (Marcus et al,
1994), converted to dependency trees using the
Penn2Malt tool1. The four syntactic features are:
? Part-of-speech tags for each word (e.g. noun,
verb, etc.), taken from the output of the POS
tagger
? Grammatical function for each word (e.g.
subject, object, etc.), taken directly from the
dependency labels produced by the parser
? Part-of-speech of the syntactic head of each
word, taken from the dependency links pro-
duced by the parser
? Distance and direction from each word to its
syntactic head, computed from the depen-
dency links produced by the parser
1http://w3.msi.vxu.se/ nivre/research/Penn2Malt.html
864
Figure 3: Baseline Models: a) Conditional Random Fields (CRF), b) Latent Dynamic Conditional
Random Fields(LDCRF), c) CRF Mixture of Experts (no latent variable)
Although our current method for extracting
these features requires that the entire utterance
be available for processing, this provides us with
a first step towards integrating information about
syntactic structure in multimodal prediction mod-
els. Many of these features could in principle be
computed incrementally with only a slight degra-
dation in accuracy, with the exception of features
that require dependency links where a word?s syn-
tactic head is to the right of the word itself. We
leave an investigation that examines only syntac-
tic features that can be produced incrementally in
real time as future work.
5.3 Baseline Models
INDIVIDUAL EXPERTS Our first baseline model
consists of a set of CRF chain models, each
trained with different set of multimodel features
(as described in the previous section). In other
words, only visual, prosodic, lexical or syntactic
features are used to train a single CRF expert. In
one CRF chain model, each gesture class corre-
sponds to a state label. (See Figure 3a).
MULTIMODAL CLASSIFIERS (EARLY FUSION)
Our second baseline consists of two models: CRF
and LDCRF (Morency et al, 2007). To train these
models, we concatenate all multimodal features
(lexical, syntactic, prosodic and visual) in one in-
put vector. Graphical representation of these base-
line models are given in Figure 3.
CRF MIXTURE OF EXPERTS To show the im-
portance of latent variable in our LMDE model,
we trained a CRF-based mixture of discriminative
experts. This model is similar to the Logarithmic
Opinion Pool (LOP) CRF suggested by Smith et
al. (2005). The training is performed in two steps.
A graphical representation of a CRF Mixture of
experts is given in the last graph of Figure 3.
5.4 Methodology
We performed held-out testing by randomly se-
lecting a subset of 11 interactions (out of 47) for
the test set. The training set contains the remain-
ing 36 dyadic interactions. All models in this pa-
per were evaluated with the same training and test
sets. Validation of all model parameters (regular-
ization term and number of hidden states) was per-
formed using a 3-fold cross-validation strategy on
the training set. The regularization term was vali-
dated with values 10k, k = ?1..3. Three different
number of hidden states were tested for the LMDE
models: 2, 3 and 4.
The performance is measured by using the F-
measure. This is the weighted harmonic mean
of precision and recall. Precision is the proba-
bility that predicted backchannels correspond to
actual listener behavior. Recall is the probabil-
ity that a backchannel produced by a listener in
our test set was predicted by the model. We use
the same weight for both precision and recall, so-
called F1. During validation we find all the peaks
(i.e., local maxima) from the marginal probabil-
ities. These backchannel hypotheses are filtered
using the optimal threshold from the validation
set. A backchannel (i.e., head nod) is predicted
correctly if a peak happens during an actual lis-
tener backchannel with high enough probability.
The same evaluation measurement is applied to all
models.
The training of all CRFs and LDCRFs were
done using the hCRF library2. The LMDE model
was implemented in Matlab3 based on the hCRF
2http://sourceforge.net/projects/hrcf/
3The source code is available at:
http://projects.ict.usc.edu/multicomp/.
865
Table 1: Comparison of individual experts with
our Latent Mixture of Discriminative Experts
(LMDE).
Expert Precision Recall f1
Lexical 0.1647 0.3305 0.2198
Prosody 0.1396 0.9112 0.2421
Syntactic 0.1833 0.4663 0.2632
POS 0.1935 0.4514 0.2709
Eye Gaze 0.1573 0.1741 0.1653
LMDE 0.2295 0.5677 0.3268
0 0.2 0.4 0.6 0.8 10
0.1
0.2
0.3
0.4
0.5
0.6
0.7
Recall
Pre
cis
ion
Latent Mixture
Lexical
Prosody
Syntactic
POS
EyeGaze
Figure 4: Comparison of individual experts with
our LMDE model.
library.
6 Results and Discussion
In this section we present the results of our empiri-
cal evaluation designed to test the three main char-
acteristics of the LMDE model: (1) integration of
multiple sources of information, (2) late fusion ap-
proach and (3) latent variable which models the
hidden dynamic between experts. We also present
an analysis of the output probabilities from the
LMDE model and individual experts.
INDIVIDUAL EXPERTS We trained one individ-
ual expert for each feature types: visual, prosodic,
lexical and syntactic features (both part-of speech
and syntactic structure). Precision, recall and F1
values for each individual expert and our LMDE
model are shown in Table 1 and Figure 4.
Pairwise two-tailed t-test comparison between
our LMDE model and individual experts shows a
Table 2: Comparison of our Latent Mixture of
Discriminative Experts (LMDE) with two early
fusion technique (CRF vs LDCRF) and the CRF
Mixture of Experts (Smith et al, 2005).
model Precision Recall f1
LMDE 0.2295 0.5677 0.3268
Early CRF 0.13958 0.9245 0.2425
Early LDCRF 0.1826 0.2484 0.2105
Mixture CRF 0.1502 0.2712 0.1934
significant difference for Lexical, Prosody, Syn-
tactic and Eye gaze, with respective p-values of
0.0037, 0.0379, 0.0400 and 0.0233. Even though
some experts may not perform well individually
(e.g., eye gaze), they can bring important informa-
tion once merged with others. Table 1 shows that
our LMDE model was able to take advantage of
the complementary information from each expert.
LATE FUSION We compare our approach with
two early fusion models: CRF and Latent-
dynamic CRF (see Figure 3). Table 2 summarizes
the results. The CRF model learns direct weights
between input features and the gesture labels. The
LDCRF is able to model more complex dynam-
ics between input features with the latent variable.
We can see that our LMDE model outperforms
both early fusion approaches because of its late
fusion approach. Pairwise two-tailed t-test analy-
sis gives p-values of 0.0481 and 0.0748, for CRF
and LDCRF respectively.
LATENT VARIABLE The CRF Mixture of Ex-
perts (2005) directly merges the expert outputs
while our model uses a latent variable to model the
hidden dynamic between experts (see Figure 3).
Table 2 summarizes the results. Pairwise two-
tailed t-test comparison between these two mod-
els shows a significant difference with a p-value
of 0.0062. This result is important since it shows
that our LMDE model does learn the hidden inter-
action between experts.
MODEL ANALYSIS To understand the multi-
modal integration which happens at the latent
variable level in our LMDE model, Figure 5
shows the output probabilities for all five individ-
ual experts as well as our model. The strength of
the latent variable is to enable different weigting
866
Speak
er inp
ut fea
tures
Exper
ts
Laten
t
Mixtu
re Ground truth labels
Word
s
Gaze ...
Time
(a) (b)
5.6s 7.7s 10.3s
...
15.6s 17.0s 18.7s 20.5s
Figure 5: Output probabilities from LMDE and individual experts for two different sub-sequences. The
gray areas in the graph corresponds to ground truth backchannel feedbacks of the listener.
of the experts at different point in time.
By analyzing the sequence (a), we observe that
both the POS and Syntactic experts learned that
when no words are present (i.e., pause) there is
a high likelihood of backchennel feedback from
the listener (shown at 5.6s and 10.3s). These two
experts are highly weighted (by one of the hid-
den state) during this part of the sequence. Also,
both the Lexical and POS experts learned that the
word ??that?? (and its part-of-speech) are impor-
tant but since the speaker is not looking at the
listener when saying it, the output from LMDE
model is low (see Figure 5, Sequence (a), 7.7s).
By analyzing sequence (b), we see that the Lex-
ical and POS experts learned the importance of the
??and?? at 15.6s and 20.5s. More importantly, we
can see at 17.0s and 18.7s that the influence of
the POS and Syntactic experts have been reduced
in the LMDE output probability. This difference
of weighting shows that a different hidden state is
active during Sequence (b).
7 Conclusion
In this paper we introduced a new model
called Latent Mixture of Discriminative Experts
(LMDE) for learning predictive models of human
communication behaviors. Many of the interac-
tions between speech and gesture happen at the
sub-gesture or sub-word level. LMDE learns au-
tomatically the temporal relationship between dif-
ferent modalities. Since, we train separate experts
for each modality, LMDE is capable of improv-
ing the prediction performance even with limited
datasets.
We evaluated our model on the task of non-
verbal feedback prediction (e.g., head nod). Our
experiments confirm the importance of combin-
ing the four types of multimodal features: lexical,
syntactic structure, eye gaze, and prosody. LMDE
is a generic model that can be applied to a wide
range of problems. As future work, we are plan-
ning to test our model on dialog act classification
and multimodal behavior recognition tasks.
Acknowledgements
This material is based upon work supported by
the National Science Foundation under Grant No.
0917321 and the U.S. Army Research, Develop-
ment, and Engineering Command (RDECOM).
The content does not necessarily reflect the posi-
tion or the policy of the Government, and no offi-
cial endorsement should be inferred.
References
Anderson, H., M. Bader, E.G. Bard, G. Doherty, S. Garrod,
S. Isard, J. Kowtko, J. McAllister, J. Miller, C. Sotillo,
867
H. Thompson, and R. Weinert. 1991. The mcrc map task
corpus. Language and Speech, 34(4):351?366.
Bavelas, J.B., L. Coates, and T. Johnson. 2000. Listeners as
co-narrators. JPSP, 79(6):941?952.
Blunsom, P., T. Cohn, and M. Osborne. 2008. A discrimi-
native latent variable model for statistical machine trans-
lation. In ACL: HLT, pages 200?208.
Burgoon, Judee K., Lesa A. Stern, and Leesa Dillman. 1995.
Interpersonal adaptation: Dyadic interaction patterns.
Cambridge University Press, Cambridge.
Cassell, J. and M. Stone. 1999. Living hand to mouth: Psy-
chological theories about speech and gesture in interactive
dialogue systems. In AAAI.
Cathcart, N., Jean Carletta, and Ewan Klein. 2003. A shal-
low model of backchannel continuers in spoken dialogue.
In EACL, pages 51?58.
Eisenstein, J., R. Barzilay, and R. Davis. 2008. Gestural co-
hesion for topic segmentation. In ACL: HLT, pages 852?
860.
Eisentein, J. and R. Davis. 2007. Conditional modality fu-
sion for coreference. In ACL, pages 352?359.
Eyben, Florian, Martin Wo?llmer, and Bjo?rn Schuller. 2009.
openEAR - Introducing the Munich Open-Source Emo-
tion and Affect Recognition Toolkit. In ACII, pages 576?
581.
Frampton, M., J. Huang, T. Bui, and S. Peters. 2009.
Real-time decision detection in multi-party dialogue. In
EMNLP, pages 1133?1141.
Fuchs, D. 1987. Examiner familiarity effects on test perfor-
mance: implications for training and practice. Topics in
Early Childhood Special Education, 7:90?104.
Fujie, Shinya, Yasuhi Ejiri, Kei Nakajima, Yosuke Mat-
susaka, and Tetsunori Kobayashi. 2004. A conversation
robot using head gesture recognition as para-linguistic in-
formation. In RO-MAN, pages 159?164.
Goldberg, S.B. 2005. The secrets of successful mediators.
Negotiation Journal, 21(3):365?376.
Gravano, A., S. Benus, H. Chavez, J. Hirschberg, and
L. Wilcox. 2007. On the role of context and prosody
in the interpretation and ?okay?. In ACL, pages 800?807.
Heylen, D. and R. op den Akker. 2007. Computing
backchannel distributions in multi-party conversations. In
ACL:EmbodiedNLP, pages 17?24.
Johnston, M. 1998. Multimodal language processing. In
ICSLP.
Jovanovic, N., R. op den Akker, and A. Nijholt. 2006.
Adressee identification in face-to-face meetings. In
EACL.
Jurafsky, D., E. Shriberg, B. Fox, and T. Curl. 1998. Lexical,
prosodic and syntactic cures for dialog acts. In Workshop
on Discourse Relations, pages 114?120.
Kendon, A. 2004. Gesture: Visible Action as Utterance.
Cambridge University Press.
Kumar, S. and M. Herbert. 2003. Discriminative random
fields: A framework for contextual interaction in classifi-
cation. In ICCV.
Lafferty, J., A. McCallum, and F. Pereira. 2001. Conditional
random fields: probabilistic models for segmenting and
labelling sequence data. In ICML.
Maatman, M., J. Gratch, and S. Marsella. 2005. Natural
behavior of a listening agent. In IVA.
Marcus, Mitchell, Grace Kim, Mary Ann Marcinkiewicz,
Robert MacIntyre, Ann Bies, Mark Ferguson, Karen Katz,
and Britta Schasberger. 1994. The penn treebank: anno-
tating predicate argument structure. In ACL:HLT, pages
114?119.
McNeill, D. 1992. Hand and Mind: What Gestures Reveal
about Thought. Univ. Chicago Press.
Moore, P.-Y. Hsueh J. 2007. What decisions have you made:
Automatic decision detection in conversational speech. In
NAACL-HLT, pages 25?32.
Morency, Louis-Philippe, Ariadna Quattoni, and Trevor Dar-
rell. 2007. Latent-dynamic discriminative models for
continuous gesture recognition. In CVPR.
Murray, G. and G. Carenini. 2009. Predicting subjectivity in
multimodal conversations. In EMNLP, pages 1348?1357.
Nakano, Reinstein, Stocky, and Justine Cassell. 2003. To-
wards a model of face-to-face grounding. In ACL.
Nakano, Y., K. Murata, M. Enomoto, Y. Arimoto, Y. Asa,
and H. Sagawa. 2007. Predicting evidence of understand-
ing by monitoring user?s task manipulation in multimodal
conversations. In ACL, pages 121?124.
Nishimura, Ryota, Norihide Kitaoka, and Seiichi Nakagawa.
2007. A spoken dialog system for chat-like conversations
considering response timing. LNCS, 4629:599?606.
Oviatt, S. 1999. Ten myths of multimodal interaction. Com-
munications of the ACM.
Quek, F. 2003. The catchment feature model for multimodal
language analysis. In ICCV.
Sagae, Kenji and Jun?ichi Tsujii. 2007. Dependency parsing
and domain adaptation with LRmodels and parser ensem-
bles. In ACL, pages 1044?1050.
Smith, A., T. Cohn, and M. Osborne. 2005. Logarithmic
opinion pools for conditional random fields. In ACL,
pages 18?25.
Ward, N. and W. Tsukahara. 2000. Prosodic features which
cue back-channel responses in english and japanese.
Journal of Pragmatics, 23:1177?1207.
868
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 335?340,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Modeling Wisdom of Crowds Using
Latent Mixture of Discriminative Experts
Derya Ozkan and Louis-Philippe Morency
Institute for Creative Technologies
University of Southern California
{ozkan,morency}@ict.usc.edu
Abstract
In many computational linguistic scenarios,
training labels are subjectives making it nec-
essary to acquire the opinions of multiple an-
notators/experts, which is referred to as ?wis-
dom of crowds?. In this paper, we propose a
new approach for modeling wisdom of crowds
based on the Latent Mixture of Discrimina-
tive Experts (LMDE) model that can automat-
ically learn the prototypical patterns and hid-
den dynamic among different experts. Experi-
ments show improvement over state-of-the-art
approaches on the task of listener backchannel
prediction in dyadic conversations.
1 Introduction
In many real life scenarios, it is hard to collect
the actual labels for training, because it is expen-
sive or the labeling is subjective. To address this
issue, a new direction of research appeared in the
last decade, taking full advantage of the ?wisdom of
crowds? (Surowiecki, 2004). In simple words, wis-
dom of crowds enables parallel acquisition of opin-
ions from multiple annotators/experts.
In this paper, we propose a new method to fuse
wisdom of crowds. Our approach is based on the
Latent Mixture of Discriminative Experts (LMDE)
model originally introduced for multimodal fu-
sion (Ozkan et al, 2010). In our Wisdom-LMDE
model, a discriminative expert is trained for each
crowd member. The key advantage of our compu-
tational model is that it can automatically discover
the prototypical patterns of experts and learn the dy-
namic between these patterns. An overview of our
approach is depicted in Figure 1.
We validate our model on the challenging task of
listener backchannel feedback prediction in dyadic
conversations. Backchannel feedback includes the
nods and paraverbals such as ?uh-huh? and ?mm-
hmm? that listeners produce as they are speaking.
Backchannels play a significant role in determining
the nature of a social exchange by showing rapport
and engagement (Gratch et al, 2007). When these
signals are positive, coordinated and reciprocated,
they can lead to feelings of rapport and promote
beneficial outcomes in diverse areas such as nego-
tiations and conflict resolution (Drolet and Morris,
2000), psychotherapeutic effectiveness (Tsui and
Schultz, 1985), improved test performance in class-
rooms (Fuchs, 1987) and improved quality of child
care (Burns, 1984). Supporting such fluid interac-
tions has become an important topic of virtual hu-
man research. In particular, backchannel feedback
has received considerable interest due to its perva-
siveness across languages and conversational con-
texts. By correctly predicting backchannel feed-
back, virtual agent and robots can have stronger
sense of rapport.
What makes backchannel prediction task well-
suited for our model is that listener feedback varies
between people and is often optional (listeners can
always decide to give feedback or not). A successful
computational model of backchannel must be able
to learn these variations among listeners. Wisdom-
LMDE is a generic approach designed to integrate
opinions from multiple listeners.
In our experiments, we validate the performance
of our approach using a dataset of 43 storytelling
dyadic interactions. Our analysis suggests three pro-
335
Latent Mixture of 
Discriminative Experts
h1 h2 h3 hn
y2y1 y3 yn
x1
x1
Wisdom of crowds
(listener backchannel)
Speaker
x1 x2 x3 xnPitch
Words
Gaze Look  at listener
h1
Time
Figure 1: Left: Our approach applied to backchannel prediction: (1) multiple listeners experience the same series of
stimuli (pre-recorded speakers) and (2) a Wisdom-LMDE model is learned using this wisdom of crowds, associating
one expert for each listener. Right: Baseline models used in our experiments: a) Conditional Random Fields (CRF),
b) Latent Dynamic Conditional Random Fields (LDCRF), c) CRF Mixture of Experts (no latent variable)
totypical patterns for backchannel feedback. By
automatically identifying these prototypical pat-
terns and learning the dynamic, our Wisdom-LMDE
model outperforms the previous approaches for lis-
tener backchannel prediction.
1.1 Previous Work
Several researchers have developed models to pre-
dict when backchannel should happen. Ward and
Tsukahara (2000) propose a unimodal approach
where backchannels are associated with a region of
low pitch lasting 110ms during speech. Nishimura et
al. (2007) present a unimodal decision-tree approach
for producing backchannels based on prosodic fea-
tures. Cathcart et al (2003) propose a unimodal
model based on pause duration and trigram part-of-
speech frequency.
Wisdom of crowds was first defined and used in
business world by Surowiecki (2004). Later, it has
been applied to other research areas as well. Raykar
et. al. (2010) proposed a probabilistic approach for
supervised learning tasks for which multiple annota-
tors provide labels but not an absolute gold standard.
Snow et. al. (2008) show that using non-expert la-
bels for training machine learning algorithms can be
as effective as using a gold standard annotation.
In this paper, we present a computational ap-
proach for listener backchannel prediction that ex-
ploits multiple listeners. Our model takes into ac-
count the differences in people?s reactions, and au-
tomatically learns the hidden structure among them.
The rest of the paper is organized as follows. In
Section 2, we present the wisdom acquisition pro-
cess. Then, we describe our Wisdom-LMDE model
in Section 3. Experimentals are presented in Sec-
tion 4. Finally, we conclude with discussions and
future works in Section 5.
2 Wisdom Acquisition
It is known that culture, age and gender affect peo-
ple?s nonverbal behaviors (Linda L. Carli and Loe-
ber, 1995; Matsumoto, 2006). Therefore, there
might be variations among people?s reactions even
when experiencing the same situation. To effi-
ciently acquire responses from multiple listeners, we
employ the Parasocial Consensus Sampling (PCS)
paradigm (Huang et al, 2010), which is based on the
theory that people behave similarly when interact-
ing through a media (e.g., video conference). Huang
et al (2010) showed that a virtual human driven by
PCS approach creates significantly more rapport and
is perceived as more believable than the virtual hu-
man driven by face-to-face interaction data (from ac-
tual listener). This result indicates that the parasocial
paradigm is a viable source of information for wis-
dom of crowds.
In practice, PCS is applied by having participants
watch pre-recorded speaker videos drawn from a
336
Listener1 Listener2 Listener3 Listener4 Listener5 Listener6 Listener7 Listener8 Listener9
pauselabel:subPOS:NN
POS:NNpauselabel:pmod
pausePOS:NNlabel:nmod
pausePOS:NNlow pitch
pausedirdist:L1low pitch
POS:NNpauselow pitch
Eyebrow updirdist:L8+POS:NN
eye gazedirdist:R1POS:JJ
lownesseye gazepause
Table 1: Most predictive features for each listener from our wisdom dataset. This analysis suggests three prototypical
patterns for backchannel feedback.
dyadic story-telling dataset. In our experiments,
we used 43 video-recorded dyadic interactions from
the RAPPORT1 dataset (Gratch et al, 2006). This
dataset was drawn from a study of face-to-face
narrative discourse (?quasi-monologic? storytelling).
The videos of the actual listeners were manually an-
notated for backchannel feedback. For PCS wis-
dom acquisition, we recruited 9 participants, who
were told to pretend they are an active listener and
press the keyboard whenever they felt like provid-
ing backchannel feedback. This provides us the re-
sponses from multiple listeners all interacting with
the same speaker, hence the wisdom necessary to
model the variability among listeners.
3 Modeling Wisdom of Crowds
Given the wisdom of multiple listeners, our goal is to
create a computational model of backchannel feed-
back. Although listener responses vary among indi-
viduals, we expect some patterns in these responses.
Therefore, we first analyze the most predictive fea-
tures for each listener and search for prototypical
patterns (in Section 3.1). Then, we present our
Wisdom-LMDE that allows to automatically learn
the hidden structure within listener responses.
3.1 Wisdom Analysis
We analyzed our wisdom data to see the most rel-
evant speaker features when predicting responses
from each individual listener. (The complete list of
speaker features are described in Section 4.1.) We
used a feature ranking scheme based on a sparse
regularization technique, as described in (Ozkan and
Morency, 2010). It allows us to identify the speaker
features most predictive of each listener backchan-
nel feedback. The top 3 features for all 9 listeners
are listed in Table 1.
This analysis suggests three prototypical patterns.
For the first 3 listeners, pause in speech and syntac-
1http://rapport.ict.usc.edu/
tic information (POS:NN) are more important. The
next 3 experts include a prosodic feature, low pitch,
which is coherent with earlier findings (Nishimura
et al, 2007; Ward and Tsukahara, 2000). It is inter-
esting to see that the last 3 experts incorporate visual
information when predicting backchannel feedback.
This is in line with Burgoon et al (Burgoon et al,
1995) work showing that speaker gestures are of-
ten correlated with listener feedback. These results
clearly suggest that variations be present among lis-
teners and some prototypical patterns may exist.
Based on these observations, we propose new com-
putational model for listener backchannel.
3.2 Computational Model: Wisdom-LMDE
The goals of our computational model are to au-
tomatically discover the prototypical patterns of
backchannel feedback and learn the dynamic be-
tween these patterns. This will allow the compu-
tational model to accurately predict the responses of
a new listener even if he/she changes her backchan-
nel patterns in the middle of the interaction. It will
also improve generalization by allowing mixtures of
these prototypical patterns.
To achieve these goals, we propose a variant of the
Latent Mixture of Discriminative Experts (Ozkan et
al., 2010) which takes full advantage of the wisdom
of crowds. Our Wisdom-LMDE model is based on
a two step process: a Conditional Random Field
(CRF, see Figure 1a) is learned for each wisdom
listener, and the outputs of these expert models are
used as input to a Latent Dynamic Conditional Ran-
dom Field (LDCRF, see Figure 1b) model, which is
capable of learning the hidden structure within the
experts. In our Wisdom-LMDE, each expert cor-
responds to a different listener from the wisdom of
crowds. More details about training and inference of
LMDE can be found in Ozkan et al (2010).
337
4 Experiments
To confirm the validity of our Wisdom-LMDE
model, we compare its performance with compu-
tational models previously proposed. As motivated
earlier, we focus our experiments on predicting lis-
tener backchannel since it is a well-suited task where
variability exists among listeners.
4.1 Multimodal Speaker Features
The speaker videos were transcribed and annotated
to extract the following features:
Lexical: Some studies have suggested an asso-
ciation between lexical features and listener feed-
back (Cathcart et al, 2003). Therefore, we use all
the words (i.e., unigrams) spoken by the speaker.
Syntactic structure: Using a CRF part-of-speech
(POS) tagger and a data-driven left-to-right shift-
reduce dependency parser (Sagae and Tsujii, 2007)
we extract four types of features from a syntactic de-
pendency structure corresponding to the utterance:
POS tags and grammatical function for each word,
POS tag of the syntactic head, distance and direction
from each word to its syntactic head.
Prosody: Prosody refers to the rhythm, pitch and
intonation of speech. Several studies have demon-
strated that listener feedback is correlated with
a speaker?s prosody (Ward and Tsukahara, 2000;
Nishimura et al, 2007). Following this, we use
downslope in pitch, pitch regions lower than 26th
percentile, drop/rise and fast drop/rise in energy of
speech, vowel volume, pause.
Visual gestures: Gestures performed by the speaker
are often correlated with listener feedback (Burgoon
et al, 1995). Eye gaze, in particular, has often been
implicated as eliciting listener feedback. Thus, we
encode the following contextual features: speaker
looking at listener, smiling, moving eyebrows up
and frowning.
Although our current method for extracting these
features requires that the entire utterance to be avail-
able for processing, this provides us with a first
step towards integrating information about syntac-
tic structure in multimodal prediction models. Many
of these features could in principle be computed in-
crementally with only a slight degradation in accu-
racy, with the exception of features that require de-
pendency links where a word?s syntactic head is to
the right of the word itself. We leave an investiga-
tion that examines only syntactic features that can be
produced incrementally in real time as future work.
4.2 Baseline Models
Consensus Classifier In our first baseline model, we
use consensus labels to train a CRF model, which
are constructed by a similar approach presented
in (Huang et al, 2010). The consensus threshold is
set to 3 (at least 3 listeners agree to give feedback at
a point) so that it contains approximately the same
number of head nods as the actual listener. See Fig-
ure 1 for a graphical representation of CRF model.
CRF Mixture of Experts To show the importance
of latent variable in our Wisdom-LMDE model, we
trained a CRF-based mixture of discriminative ex-
perts. This model is similar to the Logarithmic
Opinion Pool (LOP) CRF suggested by Smith et
al. (2005). Similar to our Wisdom-LMDE model,
the training is performed in two steps. A graphical
representation of a CRF Mixture of experts is given
in the Figure 1.
Actual Listener (AL) Classifiers This baseline model
consists of two models: CRF and LDCRF chains
(See Figure 1). To train these models, we use the
labels of the ?Actual Listeners? (AL) from the RAP-
PORT dataset.
Multimodal LMDE In this baseline model, we com-
pare our Wisdom LMDE to a multimodal LMDE,
where each expert refers to one of 5 different set of
multimodal features as presented in (Ozkan et al,
2010): lexical, prosodic, part-of-speech, syntactic,
and visual.
Random Classifier Our last baseline model is a ran-
dom backchannel generator as desribed by Ward
and Tsukahara (2000). This model randomly gener-
ates backchannels whenever some pre-defined con-
ditions in the prosody of the speech is purveyed.
4.3 Methodolgy
We performed hold-out testing on a randomly se-
lected subset of 10 interactions. The training set
contains the remaining 33 interactions. Model pa-
rameters were validated by using a 3-fold cross-
validation strategy on the training set. Regulariza-
338
Table 2: Comparison of our Wisdom-LMDE model with previously proposed models. The last column shows the
paired one tailed t-test results comparing Wisdom LMDE to each model.
tion values used are 10k for k = -1,0,..,3. Numbers
of hidden states used in the LDCRF models were
2, 3 and 4. We use the hCRF library2 for training
of CRFs and LDCRFs. Our Wisdom-LMDE model
was implemented in Matlab based on the hCRF li-
brary. Following (Morency et al, 2008), we use
an encoding dictionary to represent our features.
The performance is measured by using the F-score,
which is the weighted harmonic mean of precision
and recall. A backchannel is predicted correctly if
a peak happens during an actual listener backchan-
nel with high enough probability. The threshold was
selected automatically during validation.
4.4 Results and Discussion
Before reviewing the prediction results, is it impor-
tant to remember that backchannel feedback is an
optional phenomena, where the actual listener may
or may not decide on giving feedback (Ward and
Tsukahara, 2000). Therefore, results from predic-
tion tasks are expected to have lower accuracies as
opposed to recognition tasks where labels are di-
rectly observed (e.g., part-of-speech tagging).
Table 2 summarizes our experiments comparing
our Wisdom-LMDE model with state-of-the-art ap-
proaches for behavior prediction (see Section 4.2).
Our Wisdom-LMDE model achieves the best F1
score. Statistical t-test analysis show that Wisdom-
LMDE is significantly better than Consensus Clas-
sifier, AL Classifier (LDCRF), Multimodel LMDE
and Random Classifier.
The second best F1 score is achieved by CRF
Mixture of experts, which is the only model among
other baseline models that combines different lis-
tener labels in a late fusion manner. This result
2http://sourceforge.net/projects/hrcf/
supports our claim that wisdom of clouds improves
learning of prediction models. CRF Mixture model
is a linear combination of the experts, whereas
Wisdom-LMDE enables different weighting of ex-
perts at different point in time. By using hidden
states, Wisdom-LMDE can automatically learn the
prototypical patterns between listeners.
One really interesting result is that the optimal
number of hidden states in the Wisdom-LMDE
model (after cross-validation) is 3. This is coherent
with our qualitative analysis in Section 3.1, where
we observed 3 prototypical patterns.
5 Conclusions
In this paper, we proposed a new approach called
Wisdom-LMDE for modeling wisdom of crowds,
which automatically learns the hidden structure in
listener responses. We applied this method on
the task of listener backchannel feedback predic-
tion, and showed improvement over previous ap-
proaches. Both our qualitative analysis and exper-
imental results suggest that prototypical patterns ex-
ist when predicting listener backchannel feedback.
The Wisdom-LMDE is a generic model applicable
to multiple sequence labeling tasks (such as emotion
analysis and dialogue intent recognition), where la-
bels are subjective (i.e. small inter-coder reliability).
Acknowledgements
This material is based upon work supported by
the National Science Foundation under Grant No.
0917321 and the U.S. Army Research, Develop-
ment, and Engineering Command (RDE-COM).
The content does not necessarily reflect the position
or the policy of the Government, and no official en-
dorsement should be inferred.
339
References
Judee K. Burgoon, Lesa A. Stern, and Leesa Dillman.
1995. Interpersonal adaptation: Dyadic interaction
patterns. Cambridge University Press, Cambridge.
M. Burns. 1984. Rapport and relationships: The basis of
child care. Journal of Child Care, 4:47?57.
N. Cathcart, Jean Carletta, and Ewan Klein. 2003. A
shallow model of backchannel continuers in spoken
dialogue. In European Chapter of the Association for
Computational Linguistics. 51?58.
Aimee L. Drolet and Michael W. Morris. 2000. Rap-
port in conflict resolution: Accounting for how face-
to-face contact fosters mutual cooperation in mixed-
motive conflicts. Journal of Experimental Social Psy-
chology, 36(1):26?50.
D. Fuchs. 1987. Examiner familiarity effects on test per-
formance: Implications for training and practice. Top-
ics in Early Childhood Special Education, 7:90?104.
J. Gratch, A. Okhmatovskaia, F. Lamothe, S. Marsella,
M. Morales, R.J. Werf, and L.-P. Morency. 2006. Vir-
tual rapport. Proceedings of International Conference
on Intelligent Virtual Agents (IVA), Marina del Rey,
CA.
Jonathan Gratch, Ning Wang, Jillian Gerten, and Edward
Fast. 2007. Creating rapport with virtual agents. In
IVA.
L. Huang, L.-P. Morency, and J. Gratch:. 2010. Paraso-
cial consensus sampling: combining multiple perspec-
tives to learn virtual human behavior. In Interna-
tional Conference on Autonomous Agents and Multi-
agent Systems (AAMAS).
Suzanne J. LaFleur Linda L. Carli and Christopher C.
Loeber. 1995. Nonverbal behavior, gender, and influ-
ence. Journal of Personality and Social Psychology.
68, 1030-1041.
D. Matsumoto. 2006. Culture and Nonverbal Behav-
ior. The Sage Handbook of Nonverbal Communica-
tion, Sage Publications Inc.
L.-P. Morency, I. de Kok, and J. Gratch. 2008. Predict-
ing listener backchannels: A probabilistic multimodal
approach. In Proceedings of the Conference on Intel-
ligent Virutal Agents (IVA).
Ryota Nishimura, Norihide Kitaoka, and Seiichi Naka-
gawa. 2007. A spoken dialog system for chat-like
conversations considering response timing. Interna-
tional Conference on Text, Speech and Dialog. 599-
606.
D. Ozkan and L.-P. Morency. 2010. Concensus of self-
features for nonverbal behavior analysis. In Human
Behavior Understanding in conjucion with Interna-
tional Conference in Pattern Recognition.
D. Ozkan, K. Sagae, and L.-P. Morency. 2010. La-
tent mixture of discriminative experts for multimodal
prediction modeling. In International Conference on
Computational Linguistics (COLING).
Vikas C. Raykar, Shipeng Yu, Linda H. Zhao, Ger-
ardo Hermosillo Valadez, Charles Florin, Luca Bo-
goni, Linda Moy, and David Blei. 2010. Learning
from crowds.
Kenji Sagae and Jun?ichi Tsujii. 2007. Dependency pars-
ing and domain adaptation with LR models and parser
ensembles. In Proceedings of the CoNLL Shared Task
Session of EMNLP-CoNLL 2007, pages 1044?1050,
Prague, Czech Republic, June. Association for Com-
putational Linguistics.
A. Smith, T. Cohn, and M. Osborne. 2005. Logarithmic
opinion pools for conditional random fields. In ACL,
pages 18?25.
Rion Snow, Daniel Jurafsky, and Andrew Y. Ng. 2008.
Cheap and fast - but is it good? Evaluating non-expert
annotations for natural language tasks.
James Surowiecki. 2004. The Wisdom of Crowds: Why
the Many Are Smarter Than the Few and How Col-
lective Wisdom Shapes Business, Economies, Societies
and Nations. Doubleday.
P. Tsui and G.L. Schultz. 1985. Failure of rapport: Why
psychotheraputic engagement fails in the treatment of
asian clients. American Journal of Orthopsychiatry,
55:561?569.
N. Ward and W. Tsukahara. 2000. Prosodic fea-
tures which cue back-channel responses in english and
japanese. Journal of Pragmatics. 23, 1177?1207.
340
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 973?982,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Utterance-Level Multimodal Sentiment Analysis
Vero?nica Pe?rez-Rosas and Rada Mihalcea
Computer Science and Engineering
University of North Texas
veronicaperezrosas@my.unt.edu, rada@cs.unt.edu
Louis-Philippe Morency
Institute for Creative Technologies
University of Southern California
morency@ict.usc.edu
Abstract
During real-life interactions, people are
naturally gesturing and modulating their
voice to emphasize specific points or to
express their emotions. With the recent
growth of social websites such as YouTube,
Facebook, and Amazon, video reviews are
emerging as a new source of multimodal
and natural opinions that has been left al-
most untapped by automatic opinion anal-
ysis techniques. This paper presents a
method for multimodal sentiment classi-
fication, which can identify the sentiment
expressed in utterance-level visual datas-
treams. Using a new multimodal dataset
consisting of sentiment annotated utter-
ances extracted from video reviews, we
show that multimodal sentiment analysis
can be effectively performed, and that the
joint use of visual, acoustic, and linguistic
modalities can lead to error rate reductions
of up to 10.5% as compared to the best
performing individual modality.
1 Introduction
Video reviews represent a growing source of con-
sumer information that gained increasing interest
from companies, researchers, and consumers. Pop-
ular web platforms such as YouTube, Amazon,
Facebook, and ExpoTV have reported a signifi-
cant increase in the number of consumer reviews
in video format over the past five years. Compared
to traditional text reviews, video reviews provide a
more natural experience as they allow the viewer to
better sense the reviewer?s emotions, beliefs, and
intentions through richer channels such as intona-
tions, facial expressions, and body language.
Much of the work to date on opinion analysis has
focused on textual data, and a number of resources
have been created including lexicons (Wiebe and
Riloff, 2005; Esuli and Sebastiani, 2006) or large
annotated datasets (Maas et al, 2011). Given the
accelerated growth of other media on the Web and
elsewhere, which includes massive collections of
videos (e.g., YouTube, Vimeo, VideoLectures), im-
ages (e.g., Flickr, Picasa), audio clips (e.g., pod-
casts), the ability to address the identification of
opinions in the presence of diverse modalities is be-
coming increasingly important. This has motivated
researchers to start exploring multimodal clues for
the detection of sentiment and emotions in video
content (Morency et al, 2011; Wagner et al, 2011).
In this paper, we explore the addition of speech
and visual modalities to text analysis in order to
identify the sentiment expressed in video reviews.
Given the non homogeneous nature of full-video
reviews, which typically include a mixture of posi-
tive, negative, and neutral statements, we decided
to perform our experiments and analyses at the ut-
terance level. This is in line with earlier work on
text-based sentiment analysis, where it has been
observed that full-document reviews often contain
both positive and negative comments, which led to
a number of methods addressing opinion analysis
at sentence level. Our results show that relying
on the joint use of linguistic, acoustic, and visual
modalities allows us to better sense the sentiment
being expressed as compared to the use of only one
modality at a time.
Another important aspect of this paper is the in-
troduction of a new multimodal opinion database
annotated at the utterance level which is, to our
knowledge, the first of its kind. In our work, this
dataset enabled a wide range of multimodal senti-
ment analysis experiments, addressing the relative
importance of modalities and individual features.
The following section presents related work
in text-based sentiment analysis and audio-visual
emotion recognition. Section 3 describes our new
multimodal datasets with utterance-level sentiment
annotations. Section 4 presents our multimodal sen-
973
timent analysis approach, including details about
our linguistic, acoustic, and visual features. Our
experiments and results on multimodal sentiment
classification are presented in Section 5, with a
detailed discussion and analysis in Section 6.
2 Related Work
In this section we provide a brief overview of re-
lated work in text-based sentiment analysis, as well
as audio-visual emotion analysis.
2.1 Text-based Subjectivity and Sentiment
Analysis
The techniques developed so far for subjectivity
and sentiment analysis have focused primarily on
the processing of text, and consist of either rule-
based classifiers that make use of opinion lexicons,
or data-driven methods that assume the availability
of a large dataset annotated for polarity. These tools
and resources have been already used in a large
number of applications, including expressive text-
to-speech synthesis (Alm et al, 2005), tracking
sentiment timelines in on-line forums and news
(Balog et al, 2006), analysis of political debates
(Carvalho et al, 2011), question answering (Oh et
al., 2012), conversation summarization (Carenini et
al., 2008), and citation sentiment detection (Athar
and Teufel, 2012).
One of the first lexicons used in sentiment anal-
ysis is the General Inquirer (Stone, 1968). Since
then, many methods have been developed to auto-
matically identify opinion words and their polarity
(Hatzivassiloglou and McKeown, 1997; Turney,
2002; Hu and Liu, 2004; Taboada et al, 2011), as
well as n-gram and more linguistically complex
phrases (Yang and Cardie, 2012).
For data-driven methods, one of the most widely
used datasets is the MPQA corpus (Wiebe et al,
2005), which is a collection of news articles manu-
ally annotated for opinions. Other datasets are also
available, including two polarity datasets consist-
ing of movie reviews (Pang and Lee, 2004; Maas et
al., 2011), and a collection of newspaper headlines
annotated for polarity (Strapparava and Mihalcea,
2007).
While difficult problems such as cross-domain
(Blitzer et al, 2007; Li et al, 2012) or cross-
language (Mihalcea et al, 2007; Wan, 2009; Meng
et al, 2012) portability have been addressed, not
much has been done in terms of extending the ap-
plicability of sentiment analysis to other modalities,
such as speech or facial expressions.
The only exceptions that we are aware of are the
findings reported in (Somasundaran et al, 2006;
Raaijmakers et al, 2008; Mairesse et al, 2012;
Metze et al, 2009), where speech and text have
been analyzed jointly for the purpose of subjectiv-
ity or sentiment identification, without, however,
addressing other modalities such as visual cues;
and the work reported in (Morency et al, 2011;
Perez-Rosas et al, 2013), where multimodal cues
have been used for the analysis of sentiment in
product reviews, but where the analysis was done
at the much coarser level of full videos rather than
individual utterances as we do in our work.
2.2 Audio-Visual Emotion Analysis.
Also related to our work is the research done on
emotion analysis. Emotion analysis of speech sig-
nals aims to identify the emotional or physical
states of a person by analyzing his or her voice
(Ververidis and Kotropoulos, 2006). Proposed
methods for emotion recognition from speech fo-
cus both on what is being said and how is be-
ing said, and rely mainly on the analysis of the
speech signal by sampling the content at utterance
or frame level (Bitouk et al, 2010). Several re-
searchers used prosody (e.g., pitch, speaking rate,
Mel frequency coefficients) for speech-based emo-
tion recognition (Polzin and Waibel, 1996; Tato et
al., 2002; Ayadi et al, 2011).
There are also studies that analyzed the visual
cues, such as facial expressions and body move-
ments (Calder et al, 2001; Rosenblum et al, 1996;
Essa and Pentland, 1997). Facial expressions are
among the most powerful and natural means for
human beings to communicate their emotions and
intentions (Tian et al, 2001). Emotions can be
also expressed unconsciously, through subtle move-
ments of facial muscles such as smiling or eyebrow
raising, often measured and described using the
Facial Action Coding System (FACS) (Ekman et
al., 2002).
De Silva et. al. (De Silva et al, 1997) and Chen
et. al. (Chen et al, 1998) presented one of the
early works that integrate both acoustic and visual
information for emotion recognition. In addition to
work that considered individual modalities, there
is also a growing body of work concerned with
multimodal emotion analysis (Silva et al, 1997;
Sebe et al, 2006; Zhihong et al, 2009; Wollmer et
al., 2010).
974
Utterance transcription Label
En este color, creo que era el color frambuesa. neu
In this color, I think it was raspberry
Pinta hermosisimo. pos
It looks beautiful.
Sinceramente, con respecto a lo que pinta y a que son hidratante, si son muy hidratantes. pos
Honestly, talking about how they looks and hydrates, yes they are very hydrant.
Pero el problema de estos labiales es que cuando uno se los aplica, te dejan un gusto asqueroso en la boca. neg
But the problem with those lipsticks is that when you apply them, they leave a very nasty taste
Sinceramente, es no es que sea el olor sino que es mas bien el gusto. neg
Honestly, is not the smell, it is the taste.
Table 1: Sample utterance-level annotations. The labels used are: pos(itive), neg(ative), neu(tral).
More recently, two challenges have been or-
ganized focusing on the recognition of emotions
using audio and visual cues (Schuller et al,
2011a; Schuller et al, 2011b), which included sub-
challenges on audio-only, video-only, and audio-
video, and drew the participation of many teams
from around the world. Note however that most of
the previous work on audio-visual emotion analy-
sis has focused exclusively on the audio and video
modalities, and did not consider textual features, as
we do in our work.
3 MOUD: Multimodal Opinion
Utterances Dataset
For our experiments, we created a dataset of ut-
terances (named MOUD) containing product opin-
ions expressed in Spanish.1 We chose to work with
Spanish because it is a widely used language, and
it is the native language of the main author of this
paper.
We started by collecting a set of videos from
the social media web site YouTube, using several
keywords likely to lead to a product review or rec-
ommendation. Starting with the YouTube search
page, videos were found using the following key-
words: mis products favoritos (my favorite prod-
ucts), products que no recomiendo (non recom-
mended products), mis perfumes favoritos (my fa-
vorite perfumes), peliculas recomendadas (recom-
mended movies), peliculas que no recomiendo (non
recommended movies) and libros recomendados
(recommended books), libros que no recomiendo
(non recommended books). Notice that the key-
words are not targeted at a specific product type;
rather, we used a variety of product names, so that
the dataset has some degree of generality within
the broad domain of product reviews.
1Publicly available from the authors webpage.
Among all the videos returned by the YouTube
search, we selected only videos that respected the
following guidelines: the speaker should be in front
of the camera; her face should be clearly visible,
with a minimum amount of face occlusion during
the recording; there should not be any background
music or animation. The final video set includes 80
videos randomly selected from the videos retrieved
from YouTube that also met the guidelines above.
The dataset includes 15 male and 65 female speak-
ers, with their age approximately ranging from 20
to 60 years.
All the videos were first pre-processed to elimi-
nate introductory titles and advertisements. Since
the reviewers often switched topics when express-
ing their opinions, we manually selected a 30 sec-
onds opinion segment from each video to avoid
having multiple topics in a single review.
3.1 Segmentation and Transcription
All the video clips were manually processed to
transcribe the verbal statements and also to extract
the start and end time of each utterance. Since the
reviewers utter expressive sentences that are nat-
urally segmented by speech pauses, we decided
to use these pauses (>0.5seconds) to identify the
beginning and the end of each utterance. The tran-
scription and segmentation were performed using
the Transcriber software.
Each video was segmented into an average of
six utterances, resulting in a final dataset of 498
utterances. Each utterance is linked to the corre-
sponding audio and video stream, as well as its
manual transcription. The utterances have an aver-
age duration of 5 seconds, with a standard deviation
of 1.2 seconds.
975
Figure 1: Multimodal feature extraction
3.2 Sentiment Annotation
To enable the use of this dataset for sentiment de-
tection, we performed sentiment annotations at ut-
terance level. Annotations were done using Elan,2
which is a widely used tool for the annotation of
video and audio resources. Two annotators indepen-
dently labeled each utterance as positive, negative,
or neutral. The annotation was done after seeing
the video corresponding to an utterance (along with
the corresponding audio source). The transcription
of the utterance was also made available. Thus, the
annotation process included all three modalities: vi-
sual, acoustic, and linguistic. The annotators were
allowed to watch the video segment and their cor-
responding transcription as many times as needed.
The inter-annotator agreement was measured at
88%, with a Kappa of 0.81, which represents good
agreement. All the disagreements were reconciled
through discussions.
Table 1 shows the five utterances obtained from a
video in our dataset, along with their corresponding
2http://tla.mpi.nl/tools/tla-tools/elan/
sentiment annotations. As this example illustrates,
a video can contain a mix of positive, negative, and
neutral utterances. Note also that sentiment is not
always explicit in the text: for example, the last
utterance ?Honestly, it is not the smell, it is the
taste? has an implicit reference to the ?nasty taste?
expressed in the previous utterance, and thus it was
also labeled as negative by both annotators.
4 Multimodal Sentiment Analysis
The main advantage that comes with the analysis of
video opinions, as compared to their textual coun-
terparts, is the availability of visual and speech cues.
In textual opinions, the only source of information
consists of words and their dependencies, which
may sometime prove insufficient to convey the ex-
act sentiment of the user. Instead, video opinions
naturally contain multiple modalities, consisting of
visual, acoustic, and linguistic datastreams. We hy-
pothesize that the simultaneous use of these three
modalities will help create a better opinion analysis
model.
976
4.1 Feature Extraction
This section describes the process of automatically
extracting linguistic, acoustic and visual features
from the video reviews. First, we obtain the stream
corresponding to each modality, followed by the
extraction of a representative set of features for
each modality, as described in the following sub-
sections. These features are then used as cues to
build a classifier of positive or negative sentiment.
Figure 1 illustrates this process.
4.1.1 Linguistic Features
We use a bag-of-words representation of the video
transcriptions of each utterance to derive unigram
counts, which are then used as linguistic features.
First, we build a vocabulary consisting of all the
words, including stopwords, occurring in the tran-
scriptions of the training set. We then remove
those words that have a frequency below 10 (value
determined empirically on a small development
set). The remaining words represent the unigram
features, which are then associated with a value
corresponding to the frequency of the unigram in-
side each utterance transcription. These simple
weighted unigram features have been successfully
used in the past to build sentiment classifiers on
text, and in conjunction with Support Vector Ma-
chines (SVM) have been shown to lead to state-of-
the-art performance (Maas et al, 2011).
4.1.2 Acoustic Features
Acoustic features are automatically extracted from
the speech signal of each utterance. We used the
open source software OpenEAR (Schuller, 2009)
to automatically compute a set of acoustic features.
We include prosody, energy, voicing probabilities,
spectrum, and cepstral features.
? Prosody features. These include intensity,
loudness, and pitch that describe the speech
signal in terms of amplitude and frequency.
? Energy features. These features describe the
human loudness perception.
? Voice probabilities. These are probabilities
that represent an estimate of the percentage of
voiced and unvoiced energy in the speech.
? Spectral features. The spectral features are
based on the characteristics of the human ear,
which uses a nonlinear frequency unit to simu-
late the human auditory system. These fea-
tures describe the speech formants, which
model spoken content and represent speaker
characteristics.
? Cepstral features. These features emphasize
changes or periodicity in the spectrum fea-
tures measured by frequencies; we model
them using 12 Mel-frequency cepstral coeffi-
cients that are calculated based on the Fourier
transform of a speech frame.
Overall, we have a set of 28 acoustic features.
During the feature extraction, we use a frame sam-
pling of 25ms. Speaker normalization is performed
using z-standardization. The voice intensity is
thresholded to identify samples with and without
speech, with the same threshold being used for all
the experiments and all the speakers. The features
are averaged over all the frames in an utterance, to
obtain one feature vector for each utterance.
4.1.3 Facial Features
Facial expressions can provide important clues for
affect recognition, which we use to complement
the linguistic and acoustic features extracted from
the speech stream.
The most widely used system for measuring and
describing facial behaviors is the Facial Action
Coding System (FACS), which allows for the de-
scription of face muscle activities through the use
of a set of Action Units (AUs). According with
(Ekman, 1993), there are 64 AUs that involve the
upper and lower face, including several face posi-
tions and movements.3 AUs can occur either by
themselves or in combination, and can be used to
identify a variety of emotions. While AUs are fre-
quently annotated by certified human annotators,
automatic tools are also available. In our work, we
use the Computer Expression Recognition Toolbox
(CERT) (Littlewort et al, 2011), which allows us to
automatically extract the following visual features:
? Smile and head pose estimates. The smile
feature is an estimate for smiles. Head pose
detection consists of three-dimensional esti-
mates of the head orientation, i.e., yaw, pitch,
and roll. These features provide information
about changes in smiles and face positions
while uttering positive and negative opinions.
? Facial AUs. These features are the raw es-
timates for 30 facial AUs related to muscle
movements for the eyes, eyebrows, nose, lips,
3http://www.cs.cmu.edu/afs/cs/project/face/www/facs.htm
977
and chin. They provide detailed information
about facial behaviors from which we expect
to find differences between positive and nega-
tive states.
? Eight basic emotions. These are estimates
for the following emotions: anger, contempt,
disgust, fear, joy, sad, surprise, and neutral.
These features describe the presence of two or
more AUs that define a specific emotion. For
example, the unit A12 describes the pulling
of lip corners movement, which usually sug-
gests a smile but when associated with a
check raiser movement (unit A6), represents
a marker for the emotion of happiness.
We extract a total of 40 visual features, each
of them obtained at frame level. Since only one
person is present in each video clip, most of the
time facing the camera, the facial tracking was
successfully applied for most of our data. For the
analysis, we use a sampling rate of 30 frames per
second. The features extracted for each utterance
are averaged over all the valid frames, which are
automatically identified using the output of CERT.4
Segments with more than 60% of invalid frames
are simply discarded.
5 Experiments and Results
We run our sentiment classification experiments
on the MOUD dataset introduced earlier. From
the dataset, we remove utterances labeled as neu-
tral, thus keeping only the positive and negative
utterances with valid visual features. The removal
of neutral utterances is done for two main reasons.
First, the number of neutral utterances in the dataset
is rather small. Second, previous work in subjec-
tivity and sentiment analysis has demonstrated that
a layered approach (where neutral statements are
first separated from opinion statements followed
by a separation between positive and negative state-
ments) works better than a single three-way classifi-
cation. After this process, we are left with an exper-
imental dataset of 412 utterances, 182 of which are
labeled as positive, and 231 are labeled as negative.
From each utterance, we extract the linguis-
tic, acoustic, and visual features described above,
which are then combined using the early fusion
(or feature-level fusion) approach (Hall and Llinas,
4There is a small number of frames that CERT could not
process, mostly due to the brief occlusions that occur when
the speaker is showing the product she is reviewing.
Modality Accuracy
Baseline 55.93%
One modality at a time
Linguistic 70.94%
Acoustic 64.85%
Visual 67.31%
Two modalities at a time
Linguistic + Acoustic 72.88%
Linguistic + Visual 72.39%
Acoustic + Visual 68.86%
Three modalities at a time
Linguistic+Acoustic+Visual 74.09%
Table 2: Utterance-level sentiment classification
with linguistic, acoustic, and visual features.
1997; Atrey et al, 2010). In this approach, the fea-
tures collected from all the multimodal streams are
combined into a single feature vector, thus result-
ing in one vector for each utterance in the dataset
which is used to make a decision about the senti-
ment orientation of the utterance.
We run several comparative experiments, using
one, two, and three modalities at a time. We use
the entire set of 412 utterances and run ten fold
cross validations using an SVM classifier, as imple-
mented in the Weka toolkit.5 In line with previous
work on emotion recognition in speech (Haq and
Jackson, 2009; Anagnostopoulos and Vovoli, 2010)
where utterances are selected in a speaker depen-
dent manner (i.e., utterances from the same speaker
are included in both training and test), as well as
work on sentence-level opinion classification where
document boundaries are not considered in the split
performed between the training and test sets (Wil-
son et al, 2004; Wiegand and Klakow, 2009), the
training/test split for each fold is performed at ut-
terance level regardless of the video they belong
to.
Table 2 shows the results of the utterance-level
sentiment classification experiments. The baseline
is obtained using the ZeroR classifier, which as-
signs the most frequent label by default, averaged
over the ten folds.
6 Discussion
The experimental results show that sentiment clas-
sification can be effectively performed on multi-
modal datastreams. Moreover, the integration of
5http://www.cs.waikato.ac.nz/ml/weka/
978
Figure 2: Visual and acoustic feature weights. This
graph shows the relative importance of the infor-
mation gain weights associated with the top most
informative acoustic-visual features.
visual, acoustic, and linguistic features can improve
significantly over the use of one modality at a time,
with incremental improvements observed for each
added modality.
Among the individual classifiers, the linguistic
classifier appears to be the most accurate, followed
by the classifier that relies on visual clues, and by
the audio classifier. Compared to the best indi-
vidual classifier, the relative error rate reduction
obtained with the tri-modal classifier is 10.5%.
The results obtained with this multimodal utter-
ance classifier are found to be significantly better
than the best individual results (obtained with the
text modality), with significance being tested with
a t-test (p=0.05).
Feature analysis.
To determine the role played by each of the vi-
sual and acoustic features, we compare the fea-
ture weights assigned by the learning algorithm,
as shown in Figure 2. Interestingly, a distressed
brow is the strongest indicator of sentiment, fol-
lowed, this time not surprisingly, by the smile fea-
ture. Other informative features for sentiment clas-
sification are the voice probability, representing the
energy in speech, the combined visual features that
represent an angry face, and two of the cepstral
coefficients.
To reach a better understanding of the relation
between features, we also calculate the Pearson
correlation between the visual and acoustic fea-
tures. Table 3 shows a subset of these correlation
figures. As we expected, correlations between fea-
tures of the same type are higher. For example,
the correlation between features AU6 and AU12
or the correlation between intensity and loudness
is higher than the correlation between AU6 and in-
tensity. Nonetheless, we still find some significant
correlations between features of different types, for
instance AU12 and AU45 which are both signifi-
cantly correlated with the intensity and loudness
features. This give us confidence about using them
for further analysis.
Video-level sentiment analysis.
To understand the role played by the size of the
video-segments considered in the sentiment classi-
fication experiments, as well as the potential effect
of a speaker-independence assumption, we also run
a set of experiments where we use full videos for
the classification.
In these experiments, once again the sentiment
annotation is done by two independent annotators,
using the same protocol as in the utterance-based
annotations. Videos that were ambivalent about
the general sentiment were either labeled as neu-
tral (and thus removed from the experiments), or
labeled with the dominant sentiment. The inter-
annotator agreement for this annotation was mea-
sured at 96.1%. As before, the linguistic, acoustic,
and visual features are averaged over the entire
video, and we use an SVM classifier in ten-fold
cross validation experiments.
Table 4 shows the results obtained in these
video-level experiments. While the combination of
modalities still helps, the improvement is smaller
than the one obtained during the utterance-level
classification. Specifically, the combined effect of
acoustic and visual features improves significantly
over the individual modalities. However, the com-
bination of linguistic features with other modalities
does not lead to clear improvements. This may be
due to the smaller number of feature vectors used
in the experiments (only 80, as compared to the
412 used in the previous setup). Another possi-
ble reason is the fact that the acoustic and visual
modalities are significantly weaker than the lin-
guistic modality, most likely due to the fact that
the feature vectors are now speaker-independent,
which makes it harder to improve over the linguis-
tic modality alone.
7 Conclusions
In this paper, we presented a multimodal approach
for utterance-level sentiment classification. We
introduced a new multimodal dataset consisting
979
AU6 AU12 AU45 AUs 1,1+4 Pitch Voice probability Intensity Loudness
AU6 1.00 0.46* -0.03 -0.05 0.06 -0.14* -0.04 -0.02
AU12 1.00 -0.23* -0.33* 0.04 0.05 0.15* 0.16*
AU45 1.00 0.05 -0.05 -0.11* -.163* 0.16*
AUs 1,1+4 1.00 -0.11* -0.16* 0.06 0.07
Pitch 1.00 -0.04 -0.01 -0.08
Voice probability 1.00 0.19* 0.38*
Intensity 1.00 0.85*
Loudness 1.00
Table 3: Correlations between several visual and acoustic features. Visual features: AU6 Cheek raise,
AU12 Lip corner pull, AU45 Blink eye and closure, AU1,1+4 Distress brow. Acoustic features: Pitch,
Voice probability, Intensity, Energy. *Correlation is significant at the 0.05 level (1-tailed)
.
Modality Accuracy
Baseline 55.93%
One modality at a time
Linguistic 73.33%
Acoustic 53.33%
Visual 50.66%
Two modalities at a time
Linguistic + Acoustic 72.00%
Linguistic + Visual 74.66%
Acoustic + Visual 61.33%
Three modalities at a time
Linguistic+Acoustic+Visual 74.66%
Table 4: Video-level sentiment classification with
linguistic, acoustic, and visual features.
of sentiment annotated utterances extracted from
video reviews, where each utterance is associated
with a video, acoustic, and linguistic datastream.
Our experiments show that sentiment annotation
of utterance-level visual datastreams can be ef-
fectively performed, and that the use of multiple
modalities can lead to error rate reductions of up to
10.5% as compared to the use of one modality at a
time. In future work, we plan to explore alternative
multimodal fusion methods, such as decision-level
and meta-level fusion, to improve the integration
of the visual, acoustic, and linguistic modalities.
Acknowledgments
We would like to thank Alberto Castro for his help
with the sentiment annotations. This material is
based in part upon work supported by National Sci-
ence Foundation awards #0917170 and #1118018,
by DARPA-BAA-12-47 DEFT grant #12475008,
and by a grant from U.S. RDECOM. Any opinions,
findings, and conclusions or recommendations ex-
pressed in this material are those of the authors
and do not necessarily reflect the views of the Na-
tional Science Foundation, the Defense Advanced
Research Projects Agency, or the U.S. Army Re-
search, Development, and Engineering Command.
References
C. Alm, D. Roth, and R. Sproat. 2005. Emotions
from text: Machine learning for text-based emotion
prediction. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing,
pages 347?354, Vancouver, Canada.
C. Anagnostopoulos and E. Vovoli. 2010. Sound pro-
cessing features for speaker-dependent and phrase-
independent emotion recognition in berlin database.
In Information Systems Development, pages 413?
421. Springer.
A. Athar and S. Teufel. 2012. Context-enhanced cita-
tion sentiment detection. In Proceedings of the 2012
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies, Montre?al, Canada, June.
P. K. Atrey, M. A. Hossain, A. El Saddik, and
M. Kankanhalli. 2010. Multimodal fusion for mul-
timedia analysis: a survey. Multimedia Systems, 16.
M. El Ayadi, M. Kamel, and F. Karray. 2011. Survey
on speech emotion recognition: Features, classifica-
tion schemes, and databases. Pattern Recognition,
44(3):572 ? 587.
K. Balog, G. Mishne, and M. de Rijke. 2006. Why are
they excited? identifying and explaining spikes in
blog mood levels. In Proceedings of the 11th Meet-
ing of the European Chapter of the As sociation for
Computational Linguistics (EACL-2006).
Dmitri Bitouk, Ragini Verma, and Ani Nenkova. 2010.
Class-level spectral features for emotion recognition.
Speech Commun., 52(7-8):613?625, July.
980
J. Blitzer, M. Dredze, and F. Pereira. 2007. Biogra-
phies, bollywood, boom-boxes and blenders: Do-
main adaptation for sentiment classification. In As-
sociation for Computational Linguistics.
A. J. Calder, A. M. Burton, P. Miller, A. W. Young, and
S. Akamatsu. 2001. A principal component analysis
of facial expressions. Vision research, 41(9):1179?
1208, April.
G. Carenini, R. Ng, and X. Zhou. 2008. Summarizing
emails with conversational cohesion and subjectivity.
In Proceedings of the Association for Computational
Linguistics: Human Language Technologies (ACL-
HLT 2008), Columbus, Ohio.
P. Carvalho, L. Sarmento, J. Teixeira, and M. Silva.
2011. Liars and saviors in a sentiment annotated
corpus of comments to political debates. In Proceed-
ings of the Association for Computational Linguis-
tics (ACL 2011), Portland, OR.
L. S. Chen, T. S. Huang, T. Miyasato, and R. Nakatsu.
1998. Multimodal human emotion/expression recog-
nition. In Proceedings of the 3rd. International Con-
ference on Face & Gesture Recognition, pages 366?,
Washington, DC, USA. IEEE Computer Society.
L C De Silva, T Miyasato, and R Nakatsu, 1997. Facial
emotion recognition using multi-modal information,
volume 1, page 397401. IEEE Signal Processing So-
ciety.
P. Ekman, W. Friesen, and J. Hager. 2002. Facial ac-
tion coding system.
P. Ekman. 1993. Facial expression of emotion. Ameri-
can Psychologist, 48:384?392.
I.A. Essa and A.P. Pentland. 1997. Coding, analy-
sis, interpretation, and recognition of facial expres-
sions. Pattern Analysis and Machine Intelligence,
IEEE Transactions on, 19(7):757 ?763, jul.
A. Esuli and F. Sebastiani. 2006. SentiWordNet: A
publicly available lexical resource for opinion min-
ing. In Proceedings of the 5th Conference on Lan-
guage Resources and Evaluation (LREC 2006), Gen-
ova, IT.
D.L. Hall and J. Llinas. 1997. An introduction to mul-
tisensor fusion. IEEE Special Issue on Data Fusion,
85(1).
S. Haq and P. Jackson. 2009. Speaker-dependent
audio-visual emotion recognition. In International
Conference on Audio-Visual Speech Processing.
V. Hatzivassiloglou and K. McKeown. 1997. Predict-
ing the semantic orientation of adjectives. In Pro-
ceedings of the Conference of the European Chap-
ter of the Association for Computational Linguistics,
pages 174?181.
M. Hu and B. Liu. 2004. Mining and summariz-
ing customer reviews. In Proceedings of the tenth
ACM SIGKDD international conference on Knowl-
edge discovery and data mining, Seattle, Washing-
ton.
F. Li, S. J. Pan, O. Jin, Q. Yang, and X. Zhu. 2012.
Cross-domain co-extraction of sentiment and topic
lexicons. In Proceedings of the 50th Annual Meet-
ing of the Association for Computational Linguistics,
Jeju Island, Korea.
G. Littlewort, J. Whitehill, Tingfan Wu, I. Fasel,
M. Frank, J. Movellan, and M. Bartlett. 2011. The
computer expression recognition toolbox (cert). In
Automatic Face Gesture Recognition and Workshops
(FG 2011), 2011 IEEE International Conference on,
pages 298 ?305, march.
A. Maas, R. Daly, P. Pham, D. Huang, A. Ng, and
C. Potts. 2011. Learning word vectors for sentiment
analysis. In Proceedings of the Association for Com-
putational Linguistics (ACL 2011), Portland, OR.
F. Mairesse, J. Polifroni, and G. Di Fabbrizio. 2012.
Can prosody inform sentiment analysis? experi-
ments on short spoken reviews. In Acoustics, Speech
and Signal Processing (ICASSP), 2012 IEEE Inter-
national Conference on, pages 5093 ?5096, march.
X. Meng, F. Wei, X. Liu, M. Zhou, G. Xu, and H. Wang.
2012. Cross-lingual mixture model for sentiment
classification. In Proceedings of the 50th Annual
Meeting of the Association for Computational Lin-
guistics, Jeju Island, Korea.
F. Metze, T. Polzehl, and M. Wagner. 2009. Fusion
of acoustic and linguistic features for emotion detec-
tion. In Semantic Computing, 2009. ICSC ?09. IEEE
International Conference on, pages 153 ?160, sept.
R. Mihalcea, C. Banea, and J. Wiebe. 2007. Learning
multilingual subjective language via cross-lingual
projections. In Proceedings of the Association for
Computational Linguistics, Prague, Czech Republic.
L.P. Morency, R. Mihalcea, and P. Doshi. 2011. To-
wards multimodal sentiment analysis: Harvesting
opinions from the web. In Proceedings of the In-
ternational Conference on Multimodal Computing,
Alicante, Spain.
J. Oh, K. Torisawa, C. Hashimoto, T. Kawada,
S. De Saeger, J. Kazama, and Y. Wang. 2012.
Why question answering using sentiment analysis
and word classes. In Proceedings of the 2012 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning, Jeju Island, Korea.
B. Pang and L. Lee. 2004. A sentimental education:
Sentiment analysis using subjectivity summarization
based on minimum cuts. In Proceedings of the 42nd
Meeting of the Association for Computational Lin-
guistics, Barcelona, Spain, July.
981
V. Perez-Rosas, R. Mihalcea, and L.-P. Morency. 2013.
Multimodal sentiment analysis of spanish online
videos. IEEE Intelligent Systems.
T. Polzin and A. Waibel. 1996. Recognizing emotions
in speech. In In ICSLP.
S. Raaijmakers, K. Truong, and T. Wilson. 2008. Mul-
timodal subjectivity analysis of multiparty conversa-
tion. In Proceedings of the Conference on Empiri-
cal Methods in Natural Language Processing, pages
466?474, Honolulu, Hawaii.
M. Rosenblum, Y. Yacoob, and L.S. Davis. 1996. Hu-
man expression recognition from motion using a ra-
dial basis function network architecture. Neural Net-
works, IEEE Transactions on, 7(5):1121 ?1138, sep.
B. Schuller, M. Valstar, R. Cowie, and M. Pantic, edi-
tors. 2011a. Audio/Visual Emotion Challenge and
Workshop (AVEC 2011).
B. Schuller, M. Valstar, F. Eyben, R. Cowie, and
M. Pantic, editors. 2011b. Audio/Visual Emotion
Challenge and Workshop (AVEC 2011).
F. Eyben M. Wollmer B. Schuller. 2009. Openear in-
troducing the munich open-source emotion and af-
fect recognition toolkit. In ACII.
N. Sebe, I. Cohen, T. Gevers, and T.S. Huang. 2006.
Emotion recognition based on joint visual and audio
cues. In ICPR.
D. Silva, T. Miyasato, and R. Nakatsu. 1997. Facial
emotion recognition using multi-modal information.
In Proceedings of the International Conference on
Information and Communications Security.
S. Somasundaran, J. Wiebe, P. Hoffmann, and D. Lit-
man. 2006. Manual annotation of opinion cate-
gories in meetings. In Proceedings of the Work-
shop on Frontiers in Linguistically Annotated Cor-
pora 2006.
P. Stone. 1968. General Inquirer: Computer Approach
to Content Analysis. MIT Press.
C. Strapparava and R. Mihalcea. 2007. Semeval-2007
task 14: Affective text. In Proceedings of the 4th In-
ternational Workshop on the Semantic Evaluations
(SemEval 2007), Prague, Czech Republic.
M. Taboada, J. Brooke, M. Tofiloski, K. Voli, and
M. Stede. 2011. Lexicon-based methods for sen-
timent analysis. Computational Linguistics, 37(3).
R. Tato, R. Santos, R. Kompe, and J. M. Pardo. 2002.
Emotional space improves emotion recognition. In
In Proc. ICSLP 2002, pages 2029?2032.
Y.-I. Tian, T. Kanade, and J.F. Cohn. 2001. Recogniz-
ing action units for facial expression analysis. Pat-
tern Analysis and Machine Intelligence, IEEE Trans-
actions on, 23(2):97 ?115, feb.
P. Turney. 2002. Thumbs up or thumbs down? seman-
tic orientation applied to unsupervised classification
of reviews. In Proceedings of the 40th Annual Meet-
ing of the Association for Computational Linguistics
(ACL 2002), pages 417?424, Philadelphia.
D. Ververidis and C. Kotropoulos. 2006. Emotional
speech recognition: Resources, features, and meth-
ods. Speech Communication, 48(9):1162?1181,
September.
J. Wagner, E. Andre, F. Lingenfelser, and Jonghwa
Kim. 2011. Exploring fusion methods for multi-
modal emotion recognition with missing data. Af-
fective Computing, IEEE Transactions on, 2(4):206
?218, oct.-dec.
X. Wan. 2009. Co-training for cross-lingual sentiment
classification. In Proceedings of the Joint Confer-
ence of the Association of Computational Linguistics
and the International Joint Conference on Natural
Language Processing, Singapore, August.
J. Wiebe and E. Riloff. 2005. Creating subjective and
objective sentence classifiers from unannotated texts.
In Proceedings of the 6th International Conference
on Intelligent Text Processing and Computational
Linguistics (CICLing-2005) (invited paper), Mexico
City, Mexico.
J. Wiebe, T. Wilson, and C. Cardie. 2005. Annotating
expressions of opinions and emotions in language.
Language Resources and Evaluation, 39(2-3):165?
210.
M. Wiegand and D. Klakow. 2009. The role of
knowledge-based features in polarity classification
at sentence level. In Proceedings of the Interna-
tional Conference of the Florida Artificial Intelli-
gence Research Society.
T. Wilson, J. Wiebe, and R. Hwa. 2004. Just how mad
are you? finding strong and weak opinion clauses.
In Proceedings of the American Association for Arti-
ficial Intelligence.
M. Wollmer, B. Schuller, F. Eyben, and G. Rigoll.
2010. Combining long short-term memory and dy-
namic bayesian networks for incremental emotion-
sensitive artificial listening. IEEE Journal of Se-
lected Topics in Signal Processing, 4(5), October.
B. Yang and C. Cardie. 2012. Extracting opinion
expressions with semi-markov conditional random
fields. In Proceedings of the 2012 Joint Conference
on Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning,
Jeju Island, Korea.
Z. Zhihong, M. Pantic G.I. Roisman, and T.S. Huang.
2009. A survey of affect recognition methods: Au-
dio, visual, and spontaneous expressions. PAMI,
31(1).
982
Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL), pages 118?125,
Seoul, South Korea, 5-6 July 2012. c?2012 Association for Computational Linguistics
Dialogue Act Recognition using Reweighted Speaker Adaptation
Congkai Sun
Institute for Creative
Technologies
12015 Waterfront Drive
Playa Vista, CA 90094-2536
csun@ict.usc.edu
Louis-Philippe Morency
Institute for Creative
Technologies
12015 Waterfront Drive
Playa Vista, CA 90094-2536
morency@ict.usc.edu
Abstract
In this work we study the effectiveness of
speaker adaptation for dialogue act recogni-
tion in multiparty meetings. First, we analyze
idiosyncracy in dialogue verbal acts by quali-
tatively studying the differences and conflicts
among speakers and by quantitively compar-
ing speaker-specific models. Based on these
observations, we propose a new approach for
dialogue act recognition based on reweighted
domain adaptation which effectively balance
the influence of speaker specific and other
speakers? data. Our experiments on a real-
world meeting dataset show that with even
only 200 speaker-specific annotated dialogue
acts, the performances on dialogue act recog-
nition are significantly improved when com-
pared to several baseline algorithms. To our
knowledge, this work is the first 1 to tackle this
promising research direction of speaker adap-
tation for dialogue act recogntion.
1 Introduction
By representing a higher level intention of utterances
during human conversation, dialogue act labels are
being used to enrich the information provided by
spoken words (Stolcke et al, 2000). Dialogue act
recognition is a preliminary step towards deep dia-
logue understanding. It plays a key role in the de-
sign of dialogue systems. Besides, Fernandez et al
(2008) find certain dialogue acts are important cues
for detecting decisions in Multi-party dialogue. In
1This paper is an extended version of a poster presented at
SemDial 2011, with new experiments and deeper analysis.
Ranganath et al (2009), dialogue acts are used as
important features for flirt detection.
Automatic dialogue act recognition is still an ac-
tive research topic. The conventional approach is to
train one generic classifier using a large corpus of
annotated utterances. One aspect that makes it so
challenging is that people can express the same idea
(or speech act) using a very different set of spoken
words. Even more, people can mean different things
with the exact same spoken words. These idiosyn-
cratic differences in dialogue acts make the learning
of generic classifiers extremely challenging. Luck-
ily, in many applications such as face-to-face meet-
ings or tele-immersion, we have access to archives
of previous interactions with the same participants.
From these archives, a small subset of spoken utter-
ances can be efficiently annotated. As we will later
show in our experiments, even a small number of an-
notated utterances can make a significant difference.
In this paper, we propose a new approach for
dialogue act recognition based on reweighted do-
main adaptation which effectively balance the influ-
ence of speaker specific and other speakers? data.
By treating each speaker as one domain, we point
out the connection between training speaker spe-
cific dialogue act classifier and supervised domain
adaptation problem. We analyze idiosyncracy in
dialogue verbal acts by qualitatively studying the
differences and conflicts among speakers and by
quantitively comparing speaker-specific models. We
present an extensive set of experiments studying the
effect of speaker adaptation on dialogue act recogn-
tion in multi-party meetings using the ICSI-MRDA
dataset (Shriberg, 2004).
118
The following section presents related work on di-
alogue act recognition and domain adaptation. Sec-
tion 3 describes the ICSI-MRDA (Shriberg, 2004)
dataset which is used in all our experiments. Sec-
tion 4 analyze idiosyncracy in dialogue acts, both
qualitatively and quantitatively. Section 5 ex-
plains our reweighting-based speaker adaptation al-
gorithm. Section 6 contains all experiments to prove
the applicability of speaker adaptation to dialogue
act recognition. Finally, inspired by the promising
results, Section 8 describes some future directions.
2 Previous Work
Automatic dialogue act recognition has been an im-
portant problem in the past decades. Different dia-
logue act labeling standards and datasets have been
provided, including Switchboard-DAMSL (Stolcke
et al, 2000), ICSI-MRDA (Shriberg, 2004) and
AMI (Carletta, 2007). Stolcke et al(2000) is one
of the first work using machine learning technique
(HMM) to automatically segment and recognize di-
alogue acts. Rangarajan et al (2009) demonstrated
well-designed prosodic n-gram features are very
helpful for Dialogue Act recognition in Maximum
Entropy model. And Ang et al(2005) explored
joint segmentation and dialogue act classification for
speech from ICSI.
Domain adaptation is a popular problem in natu-
ral language processing community due to the spar-
sity of labeled data. Jiang (Jiang, 2007) breaks
the analysis of domain adaptation problem into dis-
tributional differences in instances and classifica-
tion functions between source and target data. In
Daume?s work (2007) several domain adaptation al-
gorithms are described. Our speaker adaptation al-
gorithm is inspired by the reweighting-based adap-
tation algorithm introduced in this paper.
Recently, dialogue act adaptation has been getting
a lot of attention. Tur et al (2006) successfully use
Switchboard-DAMSL to help dialogue act recogni-
tion in ICSI-MRDA. Promising results have been
obtained by using a regression model to combine the
model weights obtained by training on Switchboard-
DAMSL and ICSI-MRDA respectively. Following
the work by Tur et al (2006), Guz et al (2009) fur-
ther studied the effectiveness of dialogue act domain
adaptation in cascaded dialogue act segmentation
and recognition system, their results prove adapta-
tion in the intermediate step (segmentation) are also
very helpful for the final output (recognition). Jeong
et al(2009) use semi-supervised boosting algorithm
to leverage labeled data from Switchboard-DAMSL
and ICSI-MRDA to help dialogue act recognition in
email and forums. Margolis et.al (2010) use a struc-
tural correspondence learning technique to adapt di-
alogue act recognition on automatic translated Span-
ish genre with the help of Switchboard-DAMSL and
ICSI-MRDA. Kolar et al (2007) explores the dif-
ference among speakers for dialogue act segmenta-
tion in ICSI-MRDA dataset. Similar to the approach
taken in Tur et al (2006), adaptation is performed
through the combination of generic speaker inde-
pendent Language Model and other speakers? Lan-
guage Model. Significant improvements have been
obtained for most of the selected speakers.
All these previous papers focused on adapting di-
alogue act models between domains and did not
address the person-specific adaptation. The only
exception was Kolar et al (2007) who explored
speaker-specific dialogue act segmentation. To our
knowledge, this paper is the first work to analyze the
effectiveness of speaker adaptation for dialogue act
recognition.
3 ICSI-MRDA Corpus
Different Dialogue Act labeling standards and
datasets have been provided in recent years, in-
cluding Switchboard-DAMSL (Stolcke et al, 2000),
ICSI-MRDA (Shriberg, 2004) and AMI (Carletta,
2007). ICSI-MRDA is the dataset for our exper-
iments because many of its meetings contain the
same speakers, thus making it more suitable for our
speaker adaptation study. The tagset in ICSI-MRDA
is adapted from DAMSL standard (damsl, 1997) by
allowing multiple tags per dialogue act. Each dia-
logue act in ICSI-MRDA has one general tag and
multiple specific tags.
ICSI-MRDA consists of 75 meetings, each
roughly an hour long. There are five categories of
meetings (three of which we are actively using in
our experiments) : Bed is about the discussion of
natural language processing and neural theories of
language, Bmr is for the discussion on ICSI meeting
corpus, Bro is on speech recognition topics and Bns
119
ID Tag Type Nb. Meetings Nb. DAs
1 mn015 Bed 15 6228
2 me010 Bed 11 5309
3 me013 Bmr 25 9753
4 mn017 Bmr 15 4059
5 fe016 Bmr 18 5500
6 me018 Bro 20 4263
7 me013 Bro 22 11928
Table 1: The 7 speakers from ICSI-MRDA dataset used
in our experiments. The table lists: the Speaker ID, orig-
inal speaker tag, the type of meeting selected for this
speaker, the number of meetings this speaker participated
and the total number of dialogue acts by this speaker.
is about network and architecture. The last category
is varies which contains all other topics.
From these 75 meetings, there are 53 unique
speakers in total, and an average of about 6 speakers
per meeting. 7 speakers2 having more than 4, 000
dialogue acts are selected for our adaptation experi-
ments. Table 1 shows the details of our 7 selected
speakers. From the word transcriptions, we cre-
ated an extended list of linguistic features per ut-
terance. From the 7 selected speakers, we com-
puted 14653 unigram features, 158884 bigram fea-
tures and 400025 trigram features.
Following the work of Shriberg et al (2004), we
use the 5 general tags in our experiments:
? Disruption indicates the current Dialogue Act
is interrupted.
? Back Channel are utterances which are not
made directly by a speaker as a response and
do not function in a way that elicits a response
either.
? Floor Mechanism are dialogue acts for grab-
bing or maintaining the floor.
? Question is for eliciting listener feed back.
? And finally, unless an utterance is completely
indecipherable or else can be further described
by a general tag, then its default status is State-
ment.
Our dataset consisted of 47040 dialogue acts. The
distribution of Dialogue Act is shown in Table 2.
2speaker me013 is split into me013-Bmr and me013-Bro to
avoid the difference introduced by meeting types.
Tag proportion
Disruption 14.73%
Back Channel 10.20%
Floor Mechanism 12.40%
Question 7.20%
Statement 55.46%
Table 2: Distribution of dialogue acts in our dataset.
4 Idiosyncrasy in Dialogue Acts
Our goal is to create a dialogue act recognition al-
gorithm that can adapt to specific speakers. Some
important questions must be studied before creat-
ing such algorithm. The first obvious one is: do
speakers really differ in their choice of words and
associated dialogue acts? Do we really see a vari-
ability on how people express their dialogue in-
tent? If the answers are yes, then we will expect
that learning a dialogue act recognizer from speaker-
specific utterances should always outperform a rec-
ognizer learned from someone else data. Section 4.1
presents a comparative experiment addressing these
questions.
To better understand the results from this com-
parative experiment, we also performed a quali-
tative analysis presented in Section 4.2 where we
look more closely at the differences between speak-
ers. These two qualitative and quantitative analysis
are building block for our adaptation algorithm pre-
sented in Section 5.
4.1 Speaker-Specific Recognizers
An important assumption when performing speaker
adaptation (or more generally domain adaptation)
is that data coming from the same speaker should
be similar than data coming from another person.
In other words, a recognizer trained on a speaker
should perform better (when tested on the same per-
son) than a recognizer trained on another speaker.
We designed an experiment to test this hypothesis.
We learned 7 speaker-specific recognizers, one
for each speaker (see Table 1). We then tested all
these recognizers on new utterances from the same
7 speakers. We looked the recognition performance
when (1) the recognizer was trained on the same
person and (2) when the recognizer was trained on
a different person. This experiments quantitatively
120
Figure 1: Effect of same-speaker data on dialogue act
recognition. We compare two approaches: (1) when a
recognizer is trained on the same person and tested on
new utterances from the same person, and (2) when the
recognizer was trained on another speaker (same test set).
We vary the amount of training data to be 200, 500,
1000, 1500 and 2000 dialogue acts. In all cases, using
speaker-specific recognizer outperforms recognizer from
other speakers.
analyze the the difference among speakers. The ex-
perimental methodology used in this experiment is
the same as the other experiments described in this
paper (see Section 6). We use the Maximum En-
tropy model(MaxEnt) for all dialogue act recogniz-
ers (Ratnaparkhi, 1996). Please refer to Section 6.2
for more details about the experimental methodol-
ogy.
Figure 1 compares the average performances
when testing on the same speaker or on some other
speaker. We vary the number of training data for
each speaker to be 200, 500, 1000, 1500 and 2000
dialogue acts. For all five cases, the recognizers
trained on the same speaker outperforms the aver-
age performance when using a recognizer from an
other person. Thus speaker specific dialogue acts
adaptation fits the assumption of domain adaptation
problems.
4.2 Speakers Differences
To better understand the problem, we look more
closely at the differences among speakers and their
use of dialogue acts. We analyze the problem
induced by speaker idiosyncrasy in dialogue acts.
During our qualitative analysis of the ICSI-MRDA
dataset, we identified three major differences ex-
plaining the performances observed in the previous
sections: dialogue act conflicts, word distribution
and dialogue act label distribution. We describe
these three differences with some examples:
Conflicts: These differences happen when two
speakers intended to express different meanings
while speaking the exact same utterance. To exam-
plify these conflicts, we computed mutual informa-
tion between a specific utterance and all dialogue act
labels. We find interesting examples where for ex-
emple the word right is the most important cue for
dialogue act question when spoken by me013-Bmr,
while right is also an important cue for dialogue act
back-channel for speaker me010-Bed. These exam-
ples suggest that conflicts exist among speakers and
simply trying to learn one generic model may not
be able to handle these conflicts. The generic model
will learn what most people mean with this utter-
ance, which may be the wrong prediction for our
specific speaker.
Word distribution: People have their own vocab-
ulary. Although many words are the same, how of-
ten one person use each word will vary. Although we
may not have direct conflict here, the problem can
also be serious. The learning algorithm may mis-
leadingly focus on optimizing the weights for certain
words which are not important(e.g., words that oc-
cur more often in other speakers? dialogue acts than
his/her own) while under-estimating the important
words for this speaker. This observation suggests
that our adaptation should take into account word
distribution.
Label Distribution: Another interesting observa-
tion is to look at the distribution of dialogue act la-
bels for different speakers. Table 2 shows the aver-
age distribution over all 7 speakers. When looking
more closely at each speaker, we find some interest-
ing differences. For example, speaker 1 made state-
ments 61% of the time while speaker 4 made 49% of
the time. While this difference may not look signif-
icant, these changes can definitely affect the recog-
nition performance. So the adaptation model should
also take into account the dialogue act label distri-
bution.
5 Reweighted Speaker Adaptation
Based on the observations described in the previous
sections, we implement a simple reweighting-based
121
domain adaptation algorithm mentioned in (Daume,
2007) based on Maximum Entropy model (MaxEnt)
(Ratnaparkhi, 1996). MaxEnt model is a popular
and efficient discriminative model which can effec-
tively accommodate large numbers of features. All
the unigram, bigram and trigram features are used
as input to the maxEnt model, the output is the di-
alogue act label. MaxEnt model maximizes the log
conditional likelihood of all samples:
Loss =
N?
1
log(p(yn|xn)) (1)
where N is the number of samples for the training
data. xn represents the feature of the nth sample and
yn is the label. The conditional likelihood is defined
as
p(y|x) = exp(
?
i
?ifi(x, y))/Z(x) (2)
where Z(x) is the normalization factor and fi(x, y)
are the n-gram features described in Section 3.
When applied to our problem of speaker adapta-
tion, the reweighting adaptation model can be for-
mally defined as
Loss = w
S?
n=1
log(p(yn|xn))+
O?
m=1
log(p(ym|xm))
(3)
where S is the number of labeled speaker-specific
dialogue acts, O is the number for other speakers?
labeled dialogue acts. For each speaker, we train
one speaker-specific classifier by varying the distri-
bution of training data. We reweight the importance
of speaker specific dialogue acts versus other speak-
ers? labeled dialogue acts in the training data. The
optimal weight parameter w is automatically esti-
mated through validation.
It is worth mentioning a specific instance of the
reweighting adaptation algorithm. When w is set to
1, the reweighting adaptation algorithm is equivalent
to simply training a MaxEnt model by putting the
speaker-specific and generic data samples together
as training data. In our experiments, we will com-
pare the reweighting adaptation approach with this
simpler approach, referred as constant adaptation.
6 Experiments
Our goal is to get one model specifically adapted
for each speaker. We first describes 4 different ap-
proaches to be compared in the experiments, and
section 6.2 explains our experimental methodology.
6.1 4 Approaches
In these experiments, we compare our approach,
called reweighted adaptation, with three more
conventional approaches: speaker-specific only,
Generic and Constant adaptation.
? Speaker Specific Only For this approach, we
train the dialogue act recognizer using training
sentences from the same speaker used during
testing.
? Generic In this case, we train the dialogue act
recognizer using utterances from all speakers
other than the speaker used during testing.
? Constant Adaptation For this approach, we
train the dialogue act recognizer using all
speakers, including the speaker who will later
be used for testing. All utterances have the
same weight in this case.
? Reweighted Adaptation This is our proposed
approach. As described in Section 5, we train
our dialogue act recognizer using all speakers
but reweight the utterances from the speaker
who will later be used for testing.
6.2 Methodology
In all the following experiments we use MaxEnt
models as defined in Section 5. L2 regularization
is used for MaxEnt to avoid overfitting. The optimal
regularization parameter was automatically selected
during validation. The following regularization pa-
rameters were used: 0.01, 0.1, 1, 10 , 100, 1000 and
0 (no regularization). All the unigram, bigram and
trigram features are used in the maxEnt model. The
labels are the five dialogue act tags described in Sec-
tion 3.
All experiments were performed using hold-out
testing and hold-out validation. Both validation
and test sets consisted of 1000 dialogue acts. The
training sets contained only utterances from meet-
ings that were not in the validation set of test set.
122
Train Data 200 500 1000 1500 2000
Speaker-specific
Only 64.07 65.99 68.51 69.99 71.06
Constant
adaptation model 76.81 76.96 77.00 77.23 77.53
Our reweighted
adaptation model 78.17 78.29 78.67 78.74 78.47
Table 3: Average results among all 7 speakers when train
with different combinations of speaker specific data and
other speakers? data. The number of speaker specific data
is varied from 200, 500, 1000, 1500 to 2000.
In many of our experiments, we analyzed the ef-
fect of training set size on the recognition perfor-
mance. The speaker-specific data size varied from
200, 500, 1000, 1500 and 2000 dialogue acts respec-
tively. When training our reweighting adaptation al-
gorithm described in Section 5, we used the follow-
ing weights: 10, 30, 50, 75, and 100. The optimal
weight factor was selected automatically during val-
idation.
7 Results
In this section we present our approaches to study
the importance of speaker adaptation for dialogue
act recognition. All following results are calculated
based on the overall tag accuracies. We designed
three series of experiments for this study:
? Generic Recognizer (Section 7.1)
? Sparsity in speaker-specific data (Section 7.2)
? Effectiveness of Constant Adaptation (Sec-
tion 7.3)
? Performance of the reweighting algorithm
(Section 7.4)
7.1 Generic Recognizer
The first result we get is on average, for each speaker
when we use all other speaker?s data for training,
then test on speaker- specific test data. The perfor-
mance of this generic recognizer is 76.76% is the
baseline we try to improve when adding speaker-
specific data into consideration. 3
3The performance of our generic model is comparable to the
results from Ang et al(2005) when you take into consideration
that we used only 47,040 dialogue acts in our experiments (i.e.,
dialogue acts from our 7 speakers) which is a small fraction
compared with Ang et al(2005) .
7.2 Sparsity of speaker-specific data
A second result is the performance when only us-
ing speaker-specific data. The row Speaker Specific
Only in Table 3 shows the average results among
all speakers when for each speaker, we train us-
ing only data from the same speaker. The number
of speaker-specific training data we tried are 200,
500, 1000, 1500, and 2000 respectively. Even with
2000 speaker-specific dialogue acts for training, the
best accuracy is 71.06% which is lower than 76.76%
when using generic recognizer. Given the challenge
in getting 2000 speaker-specific annotated dialogue
acts, we are looking at a different approach where
we need less speaker-specific data.
7.3 Results of Constant Adaptation
The most straightforward way to combine other
speakers? data is to directly add them with speaker-
specific data as train. We refer to this approach
as constant adaptation. The row Constant Adap-
tation in Table 3 shows the average results among
all speakers when for each speaker, we combine
the speaker-specific data directly with the all other
speaker?s data. In our experiments, we varied the
amount of speaker-specific data included to be 200,
500, 1000, 1500, and 2000 respectively. For all
7 speakers, the performance can always been im-
proved by including speaker-specific data with all
other speakers? data for training. Furthermore, the
more speaker specific data added, the better perfor-
mance we get.
7.4 Results of Reweighting Algorithm
Finally, in this section we describe the results for
a simple adaptation algorithm based on reweight-
ing, as described in Section 5. Following the same
methodology as previous experiments, we vary the
amount of speaker-specific data to be 200, 500,
1000, 1500 and 2000. The best reweighting factor is
selected through validation on speaker-specific val-
idation data described in section 6.2. The results of
all 7 speakers from Reweighting algorithm when we
vary the amount of speaker-specific data are shown
in Figure 3.
We analyze the influence of the weighting factor
on our speaker adaptation by plotting the recogni-
tion performance for different weights. Figure 4 il-
123
Figure 2: The average results among all 7 speakers when
train with different combinations of speaker specific data
and other speakers? data are displayed. In both Constant
adaptation and Reweighted adaptation models the num-
ber of speaker specific data are varied from 200, 500,
1000, 1500 to 2000. In Generic model, only all other
speakers? data are used for training data.
Figure 3: Reweighting algorithm for all 7 Individual
Speakers when varying the amount of training data to be
0, 200, 500, 1000, 1500 and 2000.
lustrates the influence of the weight factor on three
speaker adaptation cases: None, 500 and 2000. In
this case, None represent the Constant Adaptation.
We observe the following trend: with more speaker-
specific data, the optimal reweighting factor is also
lower. This confirms that our reweighting algorithm
finds the right balance between speaker-specific data
and generic data.
Figure 2 and the row Reweighted Adaptation
from Table 3 shows the effectiveness of reweight-
ing algorithm. Results shows that even this sim-
ple algorithm can efficiently balance the influence
of speaker specific data and other speakers? data and
0 20 40 60 80 1000.765
0.77
0.775
0.78
0.785
0.79
 
 None5002000
Figure 4: Average results of Reweighting among all 7
speakers when the amount of speaker specific data is 0,
500, 2000
give significantly improved results. And most sur-
prisingly, even with only 200 speaker specific data
the reweighting algorithm can give very promising
results.
8 Conclusion
In this work we analyze the effectiveness of speaker
adaptation for dialogue act recognition. A simple
reweighting algorithm is shown to give promising
improvement on several baseline algorithms even
with only 200 speaker-specific dialogue acts. This
paper is a first step toward automatic adaptation for
dialogue act recognition. Inspired by the promising
results from the simple reweighting algorithm, we
plan to evaluate other domain adaptation techniques
such as Daume?s feature-based approach (2007). It
will also be interesting to consider the unlabeled
data from each speaker when performing dialogue
act recognition.
Acknowledgments
This material is based upon work supported by
the National Science Foundation under Grant No.
1118018 and the U.S. Army Research, Develop-
ment, and Engineering Command (RDECOM). The
content does not necessarily reflect the position or
the policy of the Government, and no official en-
dorsement should be inferred.
124
References
Jeremy Ang, Yang Liu, Elizabeth Shriberg 2005. Au-
tomatic Dialog Act Segmentation and Classification in
Multiparty Meetings. ICASSP.
Jean Carletta. 2007. Unleashing the killer corpus: expe-
riences in creating the multi-everything AMI Meeting
Corpus. Language Resources and Evaluation, 41(2):
181-190
Mark Core and James Allen. 1997. Working Notes:
AAAI Fall Symposium. HLT-NAACL SIGDIAL Work-
shop.
Hal Daume? III. 2007. Frustratingly Easy Domain Adap-
tation. Proceedings of the 45th Annual Meeting of the
Association for Computational Linguistics.
Raquel Fernandez, Matthew Frampton, Patrick Ehlen,
Matthew Purver and Stanley Peters. 2008. Mod-
elling and Detecting Decisions in Multi-Party Dia-
logue. Proceedings of the 9th SIGdial Workshop on
Discourse and Dialogue.
Umit Guz, Gokhan Tur, Dilek Hakkani-Tur, and Se-
bastien Cuendet. 2009. Cascaded model adaptation
for dialog act segmentation and tagging. Computer
Speech & Language, 24(2):289?306.
Minwoo Jeong, Chin-Yew Lin and Gary Lee. 2009.
Semi-supervised speech act recognition in emails and
forums. The 2009 Conference on Empirical Methods
on Natural Language Processing.
Jing Jiang and ChengXiang Zhai. 2007. Instance weight-
ing for domain adaptation in NLP. Proceedings of the
45th Annual Meeting of the Association for Computa-
tional Linguistics.
Jachym Kolar, Yang Liu, and Elizabeth Shriberg. 2007.
Speaker Adaptation of Language Models for Auto-
matic Dialog Act Segmentation of Meetings. Inter-
speech, 339?373.
Anna Margolis, Karen Livescu, Mari Ostendorf. 2010.
Semi-supervised domain adaptation for automatic di-
alog act tagging. ACL 2010 Workshop on Domain
Adaptation for Natural Language Processing.
Rajesh Ranganath, Dan Jurafsky, and Dan McFarland.
2009. It?s Not You, it?s Me: Detecting Flirting and its
Misperception in Speed-Dates. The 2009 Conference
on Empirical Methods on Natural Language Process-
ing.
Vivek Rangarajan, Srinivas Bangaloreb and Shrikanth
Narayanana. 2009. Combining lexical, syntactic and
prosodic cues for improved online dialog act tagging.
Computer Speech and Language, 23(4): 407-422
Adwait Ratnaparkhi. 1996. A maximum entropy model
for part-of-speech tagging. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing.
Elizabeth Shriberg, Raj Dhillon, Sonali Bhagat, Jeremy
Ang and Hannah Carvey. 2004. The ICSI Meeting
Recorder Dialog Act (MRDA) Corpus. HLT-NAACL
SIGDIAL Workshop.
Andreas Stolcke, Klaus Ries, Noah Coccaro, Elizabeth
Shriberg, Rebecca Bates, Daniel Jurafsky, Paul Tay-
lor, Rachel Martin, Carol V. Ess-dykema and Marie
Meteer. 2000. Dialogue Act Modeling for Automatic
Tagging and Recognition of Conversational Speech.
Computational Linguistics, 26:339-373.
Gokhan Tur, Umit Guz and Dilek Hakkani-Tur. 2006.
Model Adaptation For Dialogue Act Tagging. Spoken
Language Technology Workshop.
125
Proceedings of the SIGDIAL 2013 Conference, pages 193?202,
Metz, France, 22-24 August 2013. c?2013 Association for Computational Linguistics
Verbal indicators of psychological distress in interactive dialogue with a
virtual human
David DeVault, Kallirroi Georgila, Ron Artstein, Fabrizio Morbini, David Traum,
Stefan Scherer, Albert (Skip) Rizzo, Louis-Philippe Morency
University of Southern California, Institute for Creative Technologies
Playa Vista, CA
devault@ict.usc.edu
Abstract
We explore the presence of indicators of
psychological distress in the linguistic be-
havior of subjects in a corpus of semi-
structured virtual human interviews. At
the level of aggregate dialogue-level fea-
tures, we identify several significant dif-
ferences between subjects with depres-
sion and PTSD when compared to non-
distressed subjects. At a more fine-grained
level, we show that significant differences
can also be found among features that
represent subject behavior during specific
moments in the dialogues. Finally, we
present statistical classification results that
suggest the potential for automatic assess-
ment of psychological distress in individ-
ual interactions with a virtual human dia-
logue system.
1 Introduction
One of the first steps toward dealing with psy-
chological disorders such as depression and PTSD
is diagnosing the problem. However, there is of-
ten a shortage of trained health care professionals,
or of access to those professionals, especially for
certain segments of the population such as mili-
tary personnel and veterans (Johnson et al, 2007).
One possible partial remedy is to use virtual hu-
man characters to do a preliminary triage screen-
ing, so that mental healthcare providers can focus
their attention on those who are most likely to need
help. The virtual human would engage an indi-
vidual in an interview and analyze some of their
behavioral characteristics. In addition to serving
a triage function, this automated interview could
produce valuable information to help the health-
care provider make their expert diagnosis.
In this paper, we investigate whether features
in the linguistic behavior of participants in a con-
versation with a virtual human could be used
for recognizing psychological distress. We focus
specifically on indicators of depression and post-
traumatic stress disorder (PTSD) in the verbal be-
havior of participants in a Wizard-of-Oz corpus.
The results and analysis presented here are part
of a broader effort to create an automated, interac-
tive virtual human dialogue system that can detect
indicators of psychological distress in the multi-
modal communicative behavior of its users. Re-
alizing this vision requires a careful and strate-
gic design of the virtual human?s dialogue behav-
ior, and in concert with the system?s behavior, the
identification of robust ?indicator? features in the
verbal and nonverbal responses of human intervie-
wees. These indicators should be specific behavior
patterns that are empirically correlated with spe-
cific psychological disorders, and that can inform
a triage screening process or facilitate the diagno-
sis or treatment performed by a clinician.
In this paper, we report on several kinds of such
indicators we have observed in a corpus of 43
Wizard-of-Oz interactions collected with our pro-
totype virtual human, Ellie, pictured in Figure 1.
We begin in Section 2 with a brief discussion of
background and related work on the communica-
tive behavior associated with psychological dis-
tress. In Section 3, we describe our Wizard-of-Oz
data set. Section 4 presents an analysis of indicator
features we have explored in this data set, identi-
fying several significant differences between sub-
jects with depression and PTSD when compared
to non-distressed subjects. In Section 5 we present
statistical classification results that suggest the po-
tential for automatic assessment of psychological
distress based on individual interactions with a vir-
tual human dialogue system. We conclude in Sec-
tion 6.
2 Background and Related Work
There has been a range of psychological and clin-
ical research that has identified differences in the
193
Figure 1: Ellie.
communicative behavior of patients with specific
psychological disorders such as depression. In this
section, we briefly summarize some closely re-
lated work.
Most work has observed the behavior of patients
in human-human interactions, such as clinical in-
terviews and doctor-patient interactions. PTSD is
generally less well studied than depression.
Examples of the kinds of differences that have
been observed in non-verbal behavior include dif-
ferences in rates of mutual gaze and other gaze
patterns, downward angling of the head, mouth
movements, frowns, amount of gesturing, fidget-
ing, emotional expressivity, and voice quality; see
Scherer et al (2013) for a recent review.
In terms of verbal behavior, our exploration of
features here is guided by several previous obser-
vations in the literature. Cohn and colleagues have
identified increased speaker-switch durations and
decreased variability of vocal fundamental fre-
quency as indicators of depression, and have ex-
plored the use of these features for classification
(Cohn et al, 2009). That work studied these fea-
tures in human-human clinical interviews, rather
than in virtual human interactions as reported here.
In clinical studies, acute depression has been as-
sociated with decreased speech, slow speech, de-
lays in delivery, and long silent pauses (Hall et al,
1995). Aggregate differences in lexical frequen-
cies have also been observed. For example, in
written essays, Rude et al (2004) observed that
depressed participants used more negatively va-
lenced words and used the first-person pronoun ?I?
more frequently than never-depressed individuals.
Heeman et al (2010) observed differences in chil-
dren with autism in how long they pause before
speaking and in their use of fillers, acknowledg-
ments, and discourse markers. Some of these fea-
tures are similar to those studied here, but looked
at children communicating with clinicians rather
than a virtual human dialogue system.
Recent work on machine classification has
demonstrated the ability to discriminate between
schizophrenic patients and healthy controls based
on transcriptions of spoken narratives (Hong et al,
2012), and to predict patient adherence to med-
ical treatment from word-level features of dia-
logue transcripts (Howes et al, 2012). Automatic
speech recognition and word alignment has also
been shown to give good results in scoring narra-
tive recall tests for identification of cognitive im-
pairment (Prud?hommeaux and Roark, 2011; Lehr
et al, 2012).
3 Data Set
In this section, we introduce the Wizard-of-Oz
data set that forms the basis for this paper. In
this virtual human dialogue system, the charac-
ter Ellie depicted in Figure 1 carries out a semi-
structured interview with a single user. The sys-
tem was designed after a careful analysis of a
set of face-to-face interviews in the same do-
main. The face-to-face interviews make up the
large human-human Distress Assessment Inter-
view Corpus (DAIC) that is described in Scherer
et al (2013). Drawing on observations of inter-
viewer behavior in the face-to-face dialogues, El-
lie was designed to serve as an interviewer who
is also a good listener, providing empathetic re-
sponses, backchannels, and continuation prompts
to elicit more extended replies to specific ques-
tions. The data set used in this paper is the re-
sult of a set of 43 Wizard-of-Oz interactions where
the virtual human interacts verbally and nonver-
bally in a semi-structured manner with a partici-
pant. Excerpts from the transcripts of two interac-
tions in this Wizard-of-Oz data set are provided in
the appendix in Figure 5.1
3.1 Procedure
The participants were recruited via Craigslist and
were recorded at the USC Institute for Creative
1A sample demonstration video of an interaction be-
tween the virtual agent and a human actor can be seen here:
http://www.youtube.com/watch?v=ejczMs6b1Q4
194
Technologies. In total 64 participants interacted
with the virtual human. All participants who met
requirements (i.e. age greater than 18, and ad-
equate eyesight) were accepted. In this paper,
we focus on a subset of 43 of these participants
who were told that they would be interacting with
an automated system. (The other participants,
which we exclude from our analysis, were aware
that they were interacting with a human-controlled
system.) The mean age of the 43 participants in
our data set was 36.6 years, with 23 males and 20
females.
We adhered to the following procedure for data
collection: After a short explanation of the study
and giving consent, participants completed a series
of questionnaires. These questionnaires included
the PTSD Checklist-Civilian version (PCL-C) and
the Patient Health Questionnaire, depression mod-
ule (PHQ-9) (Scherer et al, 2013) along with other
questions. Then participants engage in an inter-
view with the virtual human, Ellie. After the di-
alogue concludes, participants are then debriefed
(i.e. the wizard control is revealed), paid $25 to
$35, and escorted out.
The interaction between the participants and El-
lie was designed as follows: Ellie explains the pur-
pose of the interaction and that she will ask a series
of questions. She then tries to build rapport with
the participant in the beginning of the interaction
with a series of casual questions about Los Ange-
les. Then the main interview begins, including a
range of questions such as:
What would you say are some of your
best qualities?
What are some things that usually put
you in a good mood?
Do you have disturbing thoughts?
What are some things that make you re-
ally mad?
How old were you when you enlisted?
What did you study at school?
Ellie?s behavior was controlled by two human
?wizards? in a separate room, who used a graph-
ical user interface to select Ellie?s nonverbal be-
havior (e.g. head nods, smiles, back-channels)
and verbal utterances (including the interview
questions, verbal back-channels, and empathy re-
sponses). This Wizard-of-Oz setup allows us to
prove the utility of the protocol and collect training
data for the eventual fully automatic interaction.
The speech for each question was pre-recorded us-
ing an amateur voice actress (who was also one of
the wizards). The virtual human?s performance of
these utterances is animated using the SmartBody
animation system (Thiebaux et al, 2008).
3.2 Condition Assessment
The PHQ-9 and PCL-C scales provide researchers
with guidelines on how to assess the participants?
conditions based on the responses. Among the
43 participants, 13 scored above 10 on the PHQ-
9, which corresponds to moderate depression and
above (Kroenke et al, 2001). We consider these
13 participants as positive for depression in this
study. 20 participants scored positive for PTSD,
following the PCL-C classification. The two pos-
itive conditions overlap strongly, as the evalu-
ated measurements PHQ-9 and PCL-C correlate
strongly (Pearson?s r > 0.8, as reported in Scherer
et al (2013)).
4 Feature Analysis
4.1 Transcription and timing of speech
We have a set D = {d1, ..., d43} of 43 dialogues.
The user utterances in each dialogue were tran-
scribed using ELAN (Wittenburg et al, 2006),
with start and end timestamps for each utterance.2
At each pause of 300ms or longer in the user?s
speech, a new transcription segment was started.
The resulting speech segments were subsequently
reviewed and corrected for accuracy.
For each dialogue di ? D, this process resulted
in a sequence of user speech segments. We repre-
sent each segment as a tuple ?s, e, t?, where s and e
are the starting and ending timestamps in seconds,
and t is the manual text transcription of the corre-
sponding audio segment. The system speech seg-
ments, including their starting and ending times-
tamps and verbatim transcripts of system utter-
ances, were recovered from the system log files.
To explore aggregate statistical features based
on user turn-taking behavior in the dialogues, we
employ a simple approach to identifying turns
within the dialogues. First, all user and system
speech segments are sorted in increasing order of
2ELAN is a tool that supports annotation of
video and audio, from the Max Planck Insti-
tute for Psycholinguistics, The Language Archive,
Nijmegen, The Netherlands. It is available at
http://tla.mpi.nl/tools/tla-tools/elan/.
195
Segment level features
(a) mean speaking rate of each user segment
(b) mean onset time of first segment in each user turn
(c) mean onset time of non-first segments in user turns
(d) mean length of user segments
(e) mean minimum valence in user segments
(f) mean mean valence in user segments
(g) mean maximum valence in user segments
(h) mean number of filled pauses in user segments
(i) mean filled pause rate in user segments
Dialogue level features
(j) total number of user segments
(k) total length of all user segments
Figure 2: List of context-independent features.
their starting timestamps. All consecutive seg-
ments with the same speaker are then designated
as constituting a single turn. While this simple
scheme does not provide a detailed treatment of
relevant phenomena such as overlapping speech,
backchannels, and the interactive process of ne-
gotiating the turn in dialogue (Yang and Heeman,
2010), it provides a conceptually simple model for
the definition of features for aggregate statistical
analysis.
4.2 Context-independent feature analysis
We begin by analyzing a set of shallow features
which we describe as context-independent, as they
apply to user speech segments independently of
what the system has recently said. Most of these
are features that apply to many or all user speech
segments. We describe our context-independent
features in Section 4.2.1, and present our results
for these features in Section 4.2.2.
4.2.1 Context-independent features
We summarize our context-independent features
in Figure 2.
Speaking rate and onset times Based on previ-
ous clinical observations related to slowed speech
and increased onset time for depressed individuals
(Section 2), we defined features for speaking rate
and onset time of user speech segments.
We quantify the speaking rate of a user speech
segment ?s, e, t?, where t = ?w1, ..., wN ?, as
N/(e ? s). Feature (a) is the mean value of
this feature across all user speech segments within
each dialogue.
Onset time is calculated using the notion of user
turns. For each user turn, we extracted the first
user speech segment in the turn fu = ?su, eu, tu?,
and the most recent system speech segment ls =
?ss, es, ts?. We define the onset time of such a first
user segment as su ? es, and for each dialogue,
feature (b) is the intra-dialogue mean of these on-
set times.
In order to also quantify pause length between
user speech segments within a turn, we define fea-
ture (c), a similar feature that measures the mean
onset time between non-first user speech segments
within a user turn in relation to the preceding user
speech segment.
Length of user segments As one way to quan-
tify the amount of speech, feature (d) reports the
mean length of all user speech segments within a
dialogue (measured in words).
Valence features for user speech Features (e)-
(g) are meant to explore the idea that distressed
users might use more negative or less positive vo-
cabulary than non-distressed subjects. As an ex-
ploratory approach to this topic, we used Senti-
WordNet 3.0 (Baccianella and Sebastiani, 2010),
a lexical sentiment dictionary, to assign valence
to individual words spoken by users in our study.
The dictionary contains approximately 117,000
entries. In general, each word w may appear in
multiple entries, corresponding to different parts
of speech and word senses. To assign a single va-
lence score v(w) to each word in the dictionary, in
our features we compute the average score across
all parts of speech and word senses:
v(w) =
?
e?E(w) PosScoree(w)?NegScoree(w)
|E(w)|
where E(w) is the set of entries for the word w,
PosScoree(w) is the positive score for w in entry
e, and NegScoree(w) is the negative score for w
in entry e. This is similar to the ?averaging across
senses? method described in Taboada et al (2011).
We use several different measures of the va-
lence of each speech segment with transcript t =
?w1, ..., wn?. We compute the min, mean, and max
valence of each transcript:
minimum valence of t = minwi?t v(wi)
mean valence of t = 1n
?
wi?t v(wi)
maximum valence of t = maxwi?t v(wi)
Features (e)-(f) then are intra-dialogue mean
196
values for these three segment-level valence mea-
sures.
Filled pauses Another feature that we explored
is the presence of filled pauses in user speech seg-
ments. To do so, we counted the number of times
any of the tokens uh, um, uhh, umm, mm, or mmm
appeared in each speech segment. For each dia-
logue, feature (h) is the mean number of these to-
kens per user speech segment. In order to account
for the varying length of speech segments, we also
normalize the raw token counts in each segment
by dividing them by the length of the segment, to
produce a filled pause rate for the segment. Fea-
ture (i) is the mean value of the filled pause rate
for all speech segments in the dialogue.
Dialogue level features We also included two
dialogue level measures of how ?talkative? the
user is. Feature (j) is the total number of user
speech segments throughout the dialogue. Feature
(k) is the total length (in words) of all speech seg-
ments throughout the dialogue.
Standard deviation features For the classifica-
tion experiments reported in Section 5, we also in-
cluded a standard deviation variant of each of the
features (a)-(i) in Figure 2. These variants are de-
fined as the intra-dialogue standard deviation of
the underlying value, rather than the mean. We
discuss examples of standard deviation features
further in Section 5.
4.2.2 Results for context-independent
features
We summarize the observed significant effects in
our Wizard-of-Oz corpus in Table 1.
Onset time We report our findings for individu-
als with and without depression and PTSD for fea-
ture (b) in Table 1 and in Figure 3. The units are
seconds. While an increase in onset time for in-
dividuals with depression has previously been ob-
served in human-human interaction (Cohn et al,
2009; Hall et al, 1995), here we show that this
effect transfers to interactions between individuals
with depression and virtual humans. We find that
mean onset time is significantly increased for indi-
viduals with depression in their interactions with
our virtual human Ellie (p = 0.018, Wilcoxon
rank sum test).
Additionally, while to our knowledge onset time
for individuals with PTSD has not been reported,
we also found a significant increase in onset time
Me
an
on
se
td
ela
yo
ffi
rst
pa
rtic
ipa
nt
se
gm
en
t(s
ec
on
ds
)
0
1
2
3
4
No depr.
??
Depr.
?
Me
an
on
se
td
ela
yo
ffi
rst
pa
rtic
ipa
nt
se
gm
en
t(s
ec
on
ds
)
0
1
2
3
4
?PTSD
?
PTSD
?
Figure 3: Onset time.
for individuals with PTSD (p = 0.019, Wilcoxon
rank sum test).
Filled pauses We report our findings for individ-
uals with and without depression and PTSD under
feature (h) in Table 1 and in Figure 4. We observed
a significant reduction in this feature for both in-
dividuals with depression (p = 0.012, Wilcoxon
rank sum test) and PTSD (p = 0.014, Wilcoxon
rank sum test). We believe this may be related
to the trend we observed toward shorter speech
segments from distressed individuals (though this
trend did not reach significance). There is a pos-
itive correlation, ? = 0.55 (p = 0.0001), be-
tween mean segment length (d) and mean number
of filled pauses in segments (h).
Other features We did not observe significant
differences in the values of the other context-
independent features in Figure 2.
4.3 Context-dependent features
Our data set alows us to zoom in and look at
specific contextual moments in the dialogues, and
observe how users respond to specific Ellie ques-
tions. As an example, one of Ellie?s utterances,
which has system ID happy lasttime, is:
happy lasttime = Tell me about the last
time you felt really happy.
In our data set of 43 dialogues, this question was
asked in 42 dialogues, including 12 users positive
for depression and 19 users positive for PTSD.
197
Feature Depression (13 yes, 30 no) PTSD (20 yes, 23 no)
(b) mean onset time of first
segment in each user turn
?
Depr.: 1.72 (0.89)
No Depr.: 1.08 (0.56)
p = 0.018
?
PTSD.: 1.56 (0.80)
No PTSD.: 1.03 (0.57)
p = 0.019
(h) mean number of filled pauses
in user segments
?
Depr.: 0.32 (0.19)
No Depr.: 0.48 (0.23)
p = 0.012
?
PTSD: 0.36 (0.24)
No PTSD: 0.49 (0.21)
p = 0.014
Table 1: Results for context-independent features. For each feature and condition, we provide the mean
(standard deviation) for individuals with and without the condition. P-values for individual Wilcoxon
rank sum tests are provided. An up arrow (?) indicates a significant trend toward increased feature values
for positive individuals. A down arrow (?) indicates a significant trend toward decreased feature values
for positive individuals.
Me
an
fille
dp
au
se
si
np
art
icip
an
ts
eg
me
nt
(to
ke
ns
)
0
0.2
0.4
0.6
0.8
1.0
1.2
No depr.
?
Depr.
?
Me
an
fille
dp
au
se
si
np
art
icip
an
ts
eg
me
nt
(to
ke
ns
)
0
0.2
0.4
0.6
0.8
1.0
1.2
?PTSD PTSD
?
Figure 4: Number of filled pauses per speech seg-
ment.
This question is one of 95 topic setting utter-
ances in Ellie?s repertoire. (Ellie has additional
utterances that serve as continuation prompts,
backchannels, and empathy responses, which can
be used as a topic is discussed.)
To define context-dependent features, we asso-
ciate with each user segment the most recent of
Ellie?s topic setting utterances that has occurred in
the dialogue. We then focus our analysis on those
user segments and turns that follow specific topic
setting utterances. In Table 2, we present some ex-
amples of our findings for context-dependent fea-
tures for happy lasttime.3
3While we provide significance test results here at the p <
0.05 level, it should be noted that because of the large number
of context-dependent features that may be defined in a small
corpus such as ours, we report these merely as observations in
our corpus. We do not claim that these results transfer beyond
The contextual feature labeled (g?) in Table 2 is
the mean of the maximum valence feature across
all segments for which happy lasttime is the most
recent topic setting system utterance. We provide
a full example of this feature calculation in Fig-
ure 5 in the appendix.
As the figure shows, we find that users with
both PTSD and depression show a sharp reduc-
tion in the mean maximum valence in their speech
segments that respond to this question. This sug-
gests that in these virtual human interactions, this
question plays a useful role in eliciting differen-
tial responses from subjects with these psycholog-
ical disorders. We observed three additional ques-
tions which showed a significant difference in the
mean maximum valence feature. One example is
the question, How would your best friend describe
you?.
With feature (b?) in Table 2, we find an in-
creased onset time in responses to this question for
subjects with depression.4 Feature (d?) shows that
subjects with PTSD exhibit shorter speech seg-
ments in their responses to this question.
We observed a range of findings of this sort for
various combinations of Ellie?s topic setting utter-
ances and specific context-dependent features. In
future work, we would like to study the optimal
combinations of context-dependent questions that
yield the most information about the user?s distress
status.
this data set.
4In comparing Table 2 with Table 1, this question seems
to induce a higher mean onset time for distressed users than
the average system utterance does. This does not seem to be
the case for non-distressed users.
198
Feature Depression (12 yes, 30 no) PTSD (19 yes, 23 no)
(g?) mean maximum valence
in user segments following
happy lasttime
?
Depr.: 0.15 (0.07)
No Depr.: 0.26 (0.12)
p = 0.003
?
PTSD: 0.16 (0.08)
No PTSD: 0.28 (0.11)
p = 0.0003
(b?) mean onset time of first
segments in user turns
following happy lasttime
?
Depr.: 2.64 (2.70)
No Depr.: 0.94 (1.80)
p = 0.030
n.s.
PTSD: 2.18 (2.48)
No PTSD: 0.80 (1.76)
p = 0.077
(d?) mean length of user
segments following
happy lasttime
n.s.
Depr.: 5.95 (1.80)
No Depr.: 10.03 (6.99)
p = 0.077
?
PTSD: 6.82 (5.12)
No PTSD: 10.55 (6.68)
p = 0.012
Table 2: Example results for context-dependent features. For each feature and condition, we provide
the mean (standard deviation) for individuals with and without the condition. P-values for individual
Wilcoxon rank sum tests are provided. An up arrow (?) indicates a significant trend toward increased
feature values for positive individuals. A down arrow (?) indicates a significant trend toward decreased
feature values for positive individuals.
5 Classification Results
In this section, we present some suggestive clas-
sification results for our data set. We construct
three binary classifiers that use the kinds of fea-
tures described in Section 4 to predict the pres-
ence of three conditions: PTSD, depression, and
distress. For the third condition, we define dis-
tress to be present if and only if PTSD, depres-
sion, or both are present. Such a notion of distress
that collapses distinctions between disorders may
be the most appropriate type of classification for a
potential application in which distressed users of
any type are prioritized for access to health care
professionals (who will make a more informed as-
sessment of specific conditions).
For each individual dialogue, each of the three
classifiers emits a single binary label. We train
and evaluate the classifiers in a 10-fold cross-
validation using Weka (Hall et al, 2009).
While our data set of 43 dialogues is perhaps
of a typical size for a study of a research proto-
type dialogue system, it remains very small from
a machine learning perspective. We report here
two kinds of results that help provide perspective
on the prospects for classification of these condi-
tions. The first kind looks at classification based
on all the context-independent features described
in Section 4.2.1. The second looks at classifica-
tion based on individual features from this set.
In the first set of experiments, we trained a
Na??ve Bayes classifier for each condition using
all the context-independent features. We present
our results in Table 3, comparing each classifier to
a baseline that always predicts the majority class
(i.e. no condition for PTSD, no condition for de-
pression, and with condition for distress).
We note first that the trained classifiers all out-
perform the baseline in terms of weighted F-score,
weighted precision, weighted recall, and accuracy.
The accuracy improvement over baseline is sub-
stantial for PTSD (20.9% absolute improvement)
and distress (23.2% absolute improvement). The
accuracy improvement over baseline is more mod-
est for depression (2.3% absolute). We believe
one factor in the relative difficulty of classifying
depression more accurately is the relatively small
number of depressed participants in our study
(13).
While it has been shown in prior work (Cohn et
al., 2009) that depression can be classified above
baseline performance using features observed in
clinical human-human interactions, here we have
shown that classification above baseline perfor-
mance is possible in interactions between human
participants and a virtual human dialogue system.
Further, we have shown classification results for
PTSD and distress as well as depression.
We tried incorporating context-dependent fea-
tures, and also unigram features, but found that
neither improved performance. We believe our
data set is too small for effective training with
these very large extended feature sets.
199
Disorder Model Weighted F-score Weighted Precision Weighted Recall Accuracy
PTSD Na??ve Bayes 0.738 0.754 0.744 74.4%
Majority Baseline 0.373 0.286 0.535 53.5%
Depression Na??ve Bayes 0.721 0.721 0.721 72.1%
Majority Baseline 0.574 0.487 0.698 69.8%
Distress Na??ve Bayes 0.743 0.750 0.744 74.4%
Majority Baseline 0.347 0.262 0.512 51.2%
Table 3: Classification results.
In our second set of experiments, we sought to
gain understanding of which features were pro-
viding the greatest value to classification perfor-
mance. We therefore retrained Na??ve Bayes classi-
fiers using only one feature at a time. We summa-
rize here some of the highest performing features.
For depression, we found that the feature stan-
dard deviation in onset time of first segment in
each user turn yielded very strong performance
by itself. In our corpus, we observed that de-
pressed individuals show a greater standard devia-
tion in the onset time of their responses to Ellie?s
questions (p = 0.024, Wilcoxon rank sum test).
The value of this feature in classification comple-
ments the clinical finding that depressed individu-
als show greater onset times in their responses to
interview questions (Cohn et al, 2009).
For distress, we found that the feature mean
maximum valence in user segments was the most
valuable. We discussed findings for a context-
dependent version of this feature in Section 4.3.
This finding for distress can be related to previ-
ous observations that individuals with depression
use more negatively valenced words (Rude et al,
2004).
For PTSD, we found that the feature mean num-
ber of filled pauses in user segments was among
the most informative.
Based on our observation of classification per-
formance using individual features, we believe
there remains much room for improvement in fea-
ture selection and training. A larger data set would
enable feature selection approaches that use held
out data, and would likely result in both increased
performance and deeper insights into the most
valuable combination of features for classification.
6 Conclusion
In this paper, we have explored the presence of in-
dicators of psychological distress in the linguistic
behavior of subjects in a corpus of semi-structured
virtual human interviews. In our data set, we
have identified several significant differences be-
tween subjects with depression and PTSD when
compared to non-distressed subjects. Drawing on
these features, we have presented statistical classi-
fication results that suggest the potential for auto-
matic assessment of psychological distress in indi-
vidual interactions with a virtual human dialogue
system.
Acknowledgments
This work is supported by DARPA under con-
tract (W911NF-04-D-0005) and the U.S. Army
Research, Development, and Engineering Com-
mand. The content does not necessarily reflect the
position or the policy of the Government, and no
official endorsement should be inferred.
References
Andrea Esuli Stefano Baccianella and Fabrizio Sebas-
tiani. 2010. Sentiwordnet 3.0: An enhanced lexi-
cal resource for sentiment analysis and opinion min-
ing. In Proceedings of the Seventh conference on
International Language Resources and Evaluation
(LREC?10), Valletta, Malta, May. European Lan-
guage Resources Association (ELRA).
Jeffery F. Cohn, Tomas Simon Kruez, Iain Matthews,
Ying Yang, Minh Hoai Nguyen, Margara Tejera
Padilla, Feng Zhou, and Fernando De la Torre.
2009. Detecting depression from facial actions and
vocal prosody. In Affective Computing and Intelli-
gent Interaction (ACII), September.
Judith A. Hall, Jinni A. Harrigan, and Robert Rosen-
thal. 1995. Nonverbal behavior in clinician-patient
interaction. Applied and Preventive Psychology,
4(1):21 ? 37.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The weka data mining software: an update.
SIGKDD Explor. Newsl., 11(1):10?18, November.
Peter A Heeman, Rebecca Lunsford, Ethan Selfridge,
Lois Black, and Jan Van Santen. 2010. Autism and
200
interactional aspects of dialogue. In Proceedings
of the 11th Annual Meeting of the Special Interest
Group on Discourse and Dialogue, pages 249?252.
Association for Computational Linguistics.
Kai Hong, Christian G. Kohler, Mary E. March, Am-
ber A. Parker, and Ani Nenkova. 2012. Lexi-
cal differences in autobiographical narratives from
schizophrenic patients and healthy controls. In Pro-
ceedings of the 2012 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning, pages 37?
47, Jeju Island, Korea, July. Association for Compu-
tational Linguistics.
Christine Howes, Matthew Purver, Rose McCabe,
Patrick G. T. Healey, and Mary Lavelle. 2012.
Predicting adherence to treatment for schizophrenia
from dialogue transcripts. In Proceedings of the
13th Annual Meeting of the Special Interest Group
on Discourse and Dialogue, pages 79?83, Seoul,
South Korea, July. Association for Computational
Linguistics.
Shannon J Johnson, Michelle D Sherman, Jeanne S
Hoffman, Larry C James, Patti L Johnson, John E
Lochman, Thomas N Magee, David Riggs, Jes-
sica Henderson Daniel, Ronald S Palomares, et al
2007. The psychological needs of US military ser-
vice members and their families: A preliminary re-
port. American Psychological Association Presi-
dential Task Force on Military Deployment Services
for Youth, Families and Service Members.
Kurt Kroenke, Robert L. Spitzer, and Janet B. W.
Williams. 2001. The phq-9. Journal of General
Internal Medicine, 16(9):606?613.
Maider Lehr, Emily Prud?hommeaux, Izhak Shafran,
and Brian Roark. 2012. Fully automated neuropsy-
chological assessment for detecting mild cognitive
impairment. In Interspeech 2012: 13th Annual Con-
ference of the International Speech Communication
Association, Portland, Oregon, September.
Emily Prud?hommeaux and Brian Roark. 2011. Ex-
traction of narrative recall patterns for neuropsycho-
logical assessment. In Interspeech 2011: 12th An-
nual Conference of the International Speech Com-
munication Association, pages 3021?3024, Flo-
rence, Italy, August.
Stephanie Rude, Eva-Maria Gortner, and James Pen-
nebaker. 2004. Language use of depressed and
depression-vulnerable college students. Cognition
& Emotion, 18(8):1121?1133.
S. Scherer, G. Stratou, M. Mahmoud, J. Boberg,
J. Gratch, A. Rizzo, and L.-P. Morency. 2013. Au-
tomatic behavior descriptors for psychological dis-
order analysis. In IEEE Conference on Automatic
Face and Gesture Recognition.
Maite Taboada, Julian Brooke, Milan Tofiloski, Kim-
berly Voll, and Manfred Stede. 2011. Lexicon-
based methods for sentiment analysis. Computa-
tional Linguistics, 37(2):267?307, June.
Marcus Thiebaux, Stacy Marsella, Andrew N. Mar-
shall, and Marcelo Kallmann. 2008. Smartbody:
behavior realization for embodied conversational
agents. In Proceedings of the 7th international
joint conference on Autonomous agents and multi-
agent systems - Volume 1, AAMAS ?08, pages 151?
158, Richland, SC. International Foundation for Au-
tonomous Agents and Multiagent Systems.
Peter Wittenburg, Hennie Brugman, Albert Russel,
Alex Klassmann, and Han Sloetjes. 2006. Elan: a
professional framework for multimodality research.
In Proceedings of LREC 2006, Fifth International
Conference on Language Resources and Evaluation.
Fan Yang and Peter A. Heeman. 2010. Initiative con-
flicts in task-oriented dialogue. Computer Speech &
Language, 24(2):175 ? 189.
201
Appendix A. Wizard-of-Oz Dialogue Excerpts
Example user with PTSD and depression Example non-distressed user
max
valence
transcript
Ellie (happy lasttime) tell me
about the last time you
felt really happy
User 0.014 when i play with my dog
Ellie when was the last time
that happened?
User 0.000 this morning
Ellie aw
Ellie tell me more about that
User 0.312 i don?t know i just
User 0.010 whenever i get
User 0.312 super bored or i just
need some time to
User 0.312 for myself i just play
with the dog
Ellie that?s good
User 0.000 yeah
0.1371 = mean
maximum valence in
user segments following
happy lasttime
Ellie (ideal weekendC) tell
me how you spend your
ideal weekend
max
valence
transcript
Ellie (happy lasttime) tell me
about the last time you
felt really happy
User 0.562 um the last time i felt
really happy was
User 0.000 hm
User 0.000 today
Ellie tell me more about that
User 0.688 uh just from the moment
i woke up it was a
beautiful sunny day
User -0.062 i
User 0.565 went to see some friends
we had a good time
went to school
User 0.565 had some good grades
on some papers um
wrote a good essay
User 0.292 feel pretty accomplished
and
User 0.312 i feel like my day is just
User 0.565 a good day
Ellie that?s so good to hear
0.3487 = mean
maximum valence in
user segments following
happy lasttime
Ellie (BF describe) how
would your best friend
describe you?
Figure 5: Examples of maximum valence feature.
202
Proceedings of the SIGDIAL 2014 Conference, pages 254?256,
Philadelphia, U.S.A., 18-20 June 2014.
c?2014 Association for Computational Linguistics
A Demonstration of Dialogue Processing in SimSensei Kiosk
Fabrizio Morbini, David DeVault, Kallirroi Georgila,
Ron Artstein, David Traum, Louis-Philippe Morency
USC Institute for Creative Technologies
12015 Waterfront Dr., Playa Vista, CA 90094
{morbini,devault,kgeorgila,artstein,traum,morency}@ict.usc.edu
Abstract
This demonstration highlights the dia-
logue processing in SimSensei Kiosk, a
virtual human dialogue system that con-
ducts interviews related to psychologi-
cal distress conditions such as depression,
anxiety, and post-traumatic stress disorder
(PTSD). The dialogue processing in Sim-
Sensei Kiosk allows the system to con-
duct coherent spoken interviews of human
users that are 15-25 minutes in length,
and in which users feel comfortable talk-
ing and openly sharing information. We
present the design of the individual dia-
logue components, and show examples of
natural conversation flow between the sys-
tem and users, including expressions of
empathy, follow-up responses and contin-
uation prompts, and turn-taking.
1 Introduction
This demonstration highlights the dialogue pro-
cessing in SimSensei Kiosk, a virtual human di-
alogue system that conducts interviews related to
psychological distress conditions such as depres-
sion, anxiety, and post-traumatic stress disorder
(PTSD) (DeVault et al., 2014). SimSensei Kiosk
has two main functions ? a virtual human called
Ellie (pictured in Figure 1), who converses with a
user in a spoken, semi-structured interview, and a
multimodal perception system which analyzes the
user?s behavior in real time to identify indicators
of psychological distress.
The system has been designed and devel-
oped over two years using a series of face-to-
face, Wizard-of-Oz, and automated system stud-
ies involving more than 350 human participants
(Scherer et al., 2013; DeVault et al., 2013; DeVault
et al., 2014). Agent design has been guided by
two overarching goals: (1) the agent should make
Figure 1: Ellie, the virtual human interviewer in
SimSensei Kiosk.
the user feel comfortable talking and openly shar-
ing information, and at the same time (2) the agent
should create interactional situations that support
the automatic assessment of verbal and nonver-
bal behaviors correlated with psychological dis-
tress. During an interview, the agent presents a
set of questions which have been shown in user
testing to support these goals. Since the main in-
terview questions and their order are mostly fixed,
dialogue management concentrates on providing
appropriate verbal feedback behaviors to keep the
user engaged, maintain a natural and comfort-
able conversation flow, and elicit continuations
and elaborations from the user.
The agent is implemented using a modular ar-
chitecture (Hartholt et al., 2013). Dialogue pro-
cessing, which is the focus of this demonstration,
is supported by individual modules for speech
recognition, language understanding and dialogue
management (see Section 2). The agent?s lan-
guage and speech are executed by selecting from
pre-recorded audio clips. Additional agent mod-
ules include nonverbal behavior generation, which
matches appropriately timed body movements to
the agent?s speech; character animation in a vir-
tual 3D environment; and rendering in a game en-
254
gine. The perception system analyzes audio and
video in real time to identify features such as head
position, gaze direction, smile intensity, and voice
quality. DeVault et al. (2014) provides details on
all the agent?s modules.
2 Overview of Dialogue Processing
2.1 ASR and NLU components
Unlike many task-oriented dialogue domains, in-
terview dialogues between SimSensei Kiosk and
participants are naturally open-ended. People tend
to respond to interview stimuli such as ?what?s
one of your most memorable experiences?? with
idiosyncratic stories and events from their lives.
The variability in the vocabulary and content of
participants? answers to such questions is so large
that it makes the ASR task very challenging. Fur-
thermore, continuous ASR is employed to ensure
that participants feel comfortable interacting with
the system without being distracted by having to
use a push-to-talk microphone. The use of con-
tinuous ASR necessitates the development of spe-
cific policies for turn-taking (see Section 2.2). In
this demonstration, voice activity detection and
speech recognition are performed using Pocket-
Sphinx (Huggins-Daines et al., 2006).
Because of the open-ended participants? re-
sponses, for NLU, we cannot simply construct a
small semantic ontology and expect to cover the
majority of meanings that will be expressed by
users. Thus, this is an application in which the
dialogue policy needs to be able to create a sense
of engagement, continuation, and empathy despite
relatively shallow and limited understanding of
user speech. SimSensei Kiosk currently uses 4
statistically trained utterance classifiers to capture
different aspects of user utterance meaning.
The first NLU classifier identifies generic di-
alogue act types, including statements, yes-no
questions, wh-questions, yes and no answers, and
several others. This classifier is trained using
the Switchboard DAMSL corpus (Jurafsky et al.,
1997) using a maximum entropy model.
The second NLU classifier assigns positive,
negative, or neutral valence to utterances, in or-
der to guide Ellie?s expression of empathy. We
use SentiWordNet 3.0 (Baccianella et al., 2010), a
lexical sentiment dictionary, to assign valence to
individual words spoken by users (as recognized
by the ASR); the valence assigned to an utterance
is based primarily on the mean valence scores of
Opening Rapport Building Phase
Ellie What are some things you really like about LA?
(top level question)
User I love the weather, I love the palm trees, I love the
beaches, there?s a lot to do here.
Diagnostic Phase
Ellie Have you noticed any changes in your behavior or
thoughts lately? (top level question)
User Yes.
Ellie Can you tell me about that? (continuation prompt)
User I?m having a lot more nightmares now uh can?t
sleep have haven?t really been eating uh trying to
eat. . . I have to force down food um just feeling
like an emotional wreck.
Ellie I?m sorry to hear that. (empathy response)
Ellie What are you like when you don?t sleep well?
(follow-up question)
User Irritable, emotional, it just adds to my overall
stress um [long pause]
Ellie What. . . (Ellie speaks after the participant?s long
pause)
User Can?t concentrate uh I uh. . . (the participant starts
speaking while Ellie is speaking)
Ellie I?m sorry please continue. (Ellie realizes that she
has interrupted the participant and apologizes)
Positive Closing Phase
Ellie How would your best friend describe you? (top
level question)
User As caring, as fun because most of the time when
I?m around my best friends I?m happy and I?m fun
loving. I joke around with them a lot and uh I do
better when I?m around my friends. . .
Figure 2: Examples of Ellie?s interview phases.
the individual words in the utterance.
The third NLU classifier supports domain-
specific small talk by recognizing a handful of
specific anticipated responses to Ellie?s rapport-
building questions. For example, when Ellie asks
users where they are from, this classifier detects
the names of commonly mentioned cities and re-
gions using keyphrase spotting.
The fourth NLU classifier identifies domain-
specific dialogue acts, and supports Ellie?s follow-
up responses to specific questions, such as ?how
close are you to your family??. This maximum
entropy classifier is trained using face-to-face and
Wizard-of-Oz data to detect specific responses
such as assertions of closeness.
2.2 Dialogue Management
Ellie currently uses about 100 fixed utterances in
total in the automated system. She employs 60 top
level interview questions (e.g., ?do you travel a
255
lot??), plus some follow-up questions (e.g., ?what
do you enjoy about traveling??) and a range of
backchannels (e.g., ?uh huh?), empathy responses
(e.g., ?that?s great?, ?I?m sorry?), and continua-
tion prompts (e.g., ?tell me more about that?).
The dialogue policy is implemented using the
FLoReS dialogue manager (Morbini et al., 2012).
The policy groups interview questions into three
phases (opening rapport building, diagnostic, pos-
itive closing ? ensuring that the participant leaves
with positive feelings). Questions are generally
asked in a fixed order, with some branching based
on responses to specific questions.
Rule-based subpolicies determine what Ellie?s
follow-up responses will be for each of her top-
level interview questions. The rules for follow-ups
are defined in relation to the four NLU classifiers
and the duration of user speech (measured in sec-
onds). These rules trigger continuation prompts
and empathy responses under specific conditions.
The turn-taking policy supports our design goal
to encourage users to openly share information
and to speak at length in response to Ellie?s open-
ended questions. In this domain, users often pause
before or during their responses to think about
their answers to Ellie?s personal questions. The
turn-taking policy is designed to provide ample
time for users to consider their responses, and to
let users take and keep the initiative as much as
possible. Ellie?s turn-taking decisions are based
on thresholds for user pause duration, i.e., how
much time the system should wait after the user
has stopped speaking before Ellie starts speaking.
These thresholds are tuned to the face-to-face and
Wizard-of-Oz data to minimize Ellie?s interrup-
tion rate, and are extended dynamically when El-
lie detects that she has interrupted the participant.
This is to take into account the fact that some peo-
ple tend to use longer pauses than others.
Examples of the three interview phases and of
Ellie?s subdialogue policies (top level and follow-
up questions, continuation prompts, empathy re-
sponses, and turn-taking) are given in Figure 2.
3 Demonstration Summary
The demonstration will feature a live interac-
tion between Ellie and a participant, showing El-
lie?s real-time understanding and consequent pol-
icy actions. Live dialogues will highlight Ellie?s
strategies for questioning, follow-up continuation
prompts, displays of empathy, and turn-taking,
similar to the example in Figure 2. The demon-
stration will illustrate how these elements work to-
gether to enable Ellie to carry out extended inter-
views that also provide information relevant to the
automatic assessment of indicators of distress.
Acknowledgments
The effort described here is supported by DARPA
under contract W911NF-04-D-0005 and the U.S.
Army. Any opinion, content or information pre-
sented does not necessarily reflect the position or
the policy of the United States Government, and
no official endorsement should be inferred.
References
S. Baccianella, A. Esuli, and F. Sebastiani. 2010. Sen-
tiWordNet 3.0: An enhanced lexical resource for
sentiment analysis and opinion mining. In Proceed-
ings of LREC.
D. DeVault, K. Georgila, R. Artstein, F. Morbini, D.
Traum, S. Scherer, A. Rizzo, and L.-P. Morency.
2013. Verbal indicators of psychological distress in
interactive dialogue with a virtual human. In Pro-
ceedings of SIGDIAL.
D. DeVault, R. Artstein, G. Benn, T. Dey, E. Fast,
A. Gainer, K. Georgila, J. Gratch, A. Hartholt, M.
Lhommet, G. Lucas, S. Marsella, F. Morbini, A.
Nazarian, S. Scherer, G. Stratou, A. Suri, D. Traum,
R. Wood, Y. Xu, A. Rizzo, and L.-P. Morency. 2014.
SimSensei Kiosk: A virtual human interviewer for
healthcare decision support. In Proceedings of AA-
MAS.
A. Hartholt, D. Traum, S. Marsella, A. Shapiro, G.
Stratou, A. Leuski, L.-P. Morency, and J. Gratch.
2013. All together now, introducing the virtual hu-
man toolkit. In Proceedings of IVA.
D. Huggins-Daines, M. Kumar, A. Chan, A.W. Black,
M. Ravishankar, and A.I. Rudnicky. 2006. Pocket-
Sphinx: A free, real-time continuous speech recog-
nition system for hand-held devices. In Proceedings
of ICASSP.
D. Jurafsky, E. Shriberg, and D. Biasca. 1997. Switch-
board SWBD-DAMSL Shallow-Discourse-Function
Annotation Coders Manual, Draft 13.
F. Morbini, D. DeVault, K. Sagae, J. Gerten, A. Nazar-
ian, and D. Traum. 2012. FLoReS: A forward look-
ing reward seeking dialogue manager. In Proceed-
ings of IWSDS.
S. Scherer, G. Stratou, M. Mahmoud, J. Boberg,
J. Gratch, A. Rizzo, and L.-P. Morency. 2013. Au-
tomatic behavior descriptors for psychological dis-
order analysis. In Proceedings of IEEE Conference
on Automatic Face and Gesture Recognition.
256
Proceedings of the Second Workshop on Natural Language Processing for Social Media (SocialNLP), pages 50?58,
Dublin, Ireland, August 24 2014.
 Verbal Behaviors and Persuasiveness in Online Multimedia Content 
 Moitreya Chatterjee, Sunghyun Park*, Han Suk Shim*, Kenji Sagae and Louis-Philippe Morency USC Institute for Creative Technologies Los Angeles, CA 90094 metro.smiles@gmail.com,  { park, hshim, sagae, morency }@ict.usc.edu  Abstract Persuasive communication is an essential component of our daily lives, whether it is negotiat-ing, reviewing a product, or campaigning for the acceptance of a point of view. With the rapid expansion of social media websites such as YouTube, Vimeo and ExpoTV, it is becoming ev-er more important and useful to understand persuasiveness in social multimedia content. In this paper we present a novel analysis of verbal behavior, based on lexical usage and para-verbal markers of hesitation, in the context of predicting persuasiveness in online multimedia content. Toward the end goal of predicting perceived persuasion, this work also explores the potential differences in verbal behavior of people expressing a positive opinion (e.g., a posi-tive movie review) versus a negative one. The analysis is performed on a multimedia corpus of 1,000 movie review videos annotated for persuasiveness. Our results show that verbal be-havior can be a significant predictor of persuasiveness in such online multimedia content.  1 Introduction A message that is ?intended to shape, reinforce or change the responses of another or others? is cate-gorized as persuasive communication (Miller, 1980), and it is particularly important for the role it plays in creating social influence and altering other people?s opinions (Reardon, 1991; Zimbardo and Leippe, 1991). For instance, a persuasive advertisement could be a potential profit churner. The growth of social networking sites on the Internet has resulted in an explosion of online content with the purpose of delivering persuasive messages. Websites such as YouTube, Vimeo and ExpoTV are examples of online media in which these messages propagate mainly in the form of videos. ExpoTV, in particular, is a repository of a large number of videos dedicated for product reviews in which people try to convince others in favor of or against the use of various products. This raises an interesting research problem as to what it is that makes certain speakers have a substantial impact on others? opinions while other speakers are ignored. In this paper, we present a novel analysis of spoken persuasion in online multimedia content. Our work is motivated by prior research findings in psychology indicating that verbal behavior is a prom-ising indicator for persuasive communication (Chaiken and Eagly, 1979; Werner, 1982). Such prior findings allow us to hypothesize that two primary types of verbal features will be predictive of per-suasion: lexical features and paraverbal markers of hesitation. Additionally we explore the relation-ship of the sentiment of the content and perceived persuasion, by hypothesizing that speakers? exhibit different verbal behavior when expressing a positive opinion versus a negative one and taking into account these differences will improve prediction performance. We conduct several experiments in order to validate these hypotheses using a multimedia corpus of 1,000 movie review videos obtained from ExpoTV.com, which is a great source of online reviews. Our experiments followed by a detailed analysis also reveal a set of predictive features which characterize persuasive online presentations. In the following section, we present an overview of related work. Section 3 elaborates on our re-
* Both authors contributed equally to this work. This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/  
50
search hypotheses. In Section 4, we present our multimedia corpus. The details of the experiments with computational descriptors and methodology are described in Section 5. We discuss the results and findings in Section 6, and finally we conclude our paper and present some future directions of research in Section 7.  2 Related Work Content in the form of written text are omnipresent in our society. Starting from books, magazines and newspapers to the now prevalent emails and blog posts, text-based content are an invaluable component for effective communication. Prior research reports possibly greater persuasiveness in written messages compared to visual or acoustic modalities in certain situations (Chaiken and Eagly, 1979; Werner, 1982). Past research has also revealed that for sophisticated messages, such as those used in a martial setting, written messages are more persuasive (Chaiken and Eagly, 1979).  Although the importance of studying verbal behavior for determining persuasiveness has been un-derscored in prior work in the field of communication sciences (O?Keefe, 2002) and this line of re-search gives us useful pointers to the factors that contribute to persuasiveness in text or verbal com-munication, they present no computational aspect, which is where we put our emphasis in the paper. In the field of natural language processing, text classification based on bag-of-words has been a long standing approach (Lewis and Gale, 1994; Mitchell, 1997; Dave et al., 2003). In fact, Young et al. (2011) have explored lexical features in the specific context of predicting persuasion, but they fo-cus their attention on studying persuasion in dialogue. Our work draws inspiration from such ap-proaches but explores it in the specific context of predicting persuasiveness in online multimedia con-tent using lexical and paraverbal features. 3 Research Hypotheses Motivated by prior works and theoretical background, we designed our experiments to validate three hypotheses. Since multiple prior works point to the usefulness of the text modality in persuasive communication and also to the power of text classification with lexical features in various tasks, we explored the fea-sibility of capturing the difference in verbal behavior between persuasive and unpersuasive expres-sions of opinions in online social multimedia content (specifically, movie reviews). The following is the hypothesis that we specifically tested with our experiments: Hypothesis 1: Verbal behavior, as captured by lexical usage, is indicative of persuasiveness in online social multimedia content, irrespective of whether the opinion expressed is positive or negative. Paraverbal behaviors indicative of hesitation can constitute important information for predicting persuasiveness. For instance, a speaker?s stuttering or breaking his/her speech with filled pauses (such as um and uh) has influence on how other people perceive his/her persuasiveness. Although previous work (DeVault et al, 2013) suggests paraverbal behavior may be indicative of depression, another work on emotion prediction however, (Devillers et al., 2006) raised questions about its predictive power when compared to using standard cues derived from lexical usage. This leads us to our second hypothesis on paraverbal behaviors in the context of predicting persuasiveness: Hypothesis 2: Paraverbal behaviors related to hesitation are indicative of persuasiveness in online social multimedia content.  Past research highlights the importance of the knowledge of the affective state of a document towards its perceived persuasiveness (Murphy, 2001). We therefore hypothesize the following: Hypothesis 3: Knowledge of the sentiment polarity of a movie review improves classification of the speaker?s perceived persuasiveness. 4 Dataset ExpoTV.com is a popular website housing videos of product reviews. Each product review has a vid-eo of a speaker talking about a particular product as well as the speaker?s direct rating of the product on an integral scale from 1 star (for most negative review) to 5 stars (for most positive review). This direct rating is useful for the purpose of our study because this allows us to study perceived persua-sion under different directions of persuasion (in favor of or against). For instance, the speaker in a 5-
51
star movie review video would most likely try to persuade his/her audience in favor of watching the movie while the speaker in a 1-star movie review video would argue against watching the movie. We therefore collected a total of 1,000 movie review videos that were either highly positive or negative. The dataset consists of the following:  ? Positive Reviews: 500 movie review videos with 5-star rating (315 males and 185 females). ? Negative Reviews: 500 movie review videos with 1 or 2-star rating, consisting of 216 1-star videos (151 males and 65 females) and 284 2-star videos (212 males and 72 females). We included 2-star videos due to the lack of enough 1-star videos on the website.  Each video in the corpus has a frontal view of one person talking about a particular movie, and the average length of the videos is about 94 seconds. The corpus contains 372 unique speakers and 600 unique movie titles and is available to the community for purposes of academic research1. 4.1 Evaluation of Persuasiveness Amazon Mechanical Turk (AMT), which is a popular online crowdsourcing platform, was used to obtain subjective evaluation of each speaker?s perceived persuasiveness, following a similar annota-tion scheme as (Mohammadi et al., 2013). For each video in the corpus, we obtained 3 repeated eval-uations on the level of persuasiveness of the speaker by asking the workers to give direct rating on each speaker?s persuasiveness on a Likert scale from 1 (very unpersuasive) to 7 (very persuasive). A total of 50 native English-speaking workers based in the United States participated in the evaluation process online, and the task was evenly distributed among the 50 workers. To minimize gender influ-ence, the task was distributed such that the workers only evaluated speakers of the same gender. The correlation between the mean score of every movie and the individual ratings was found to be 0.7 on the average (Pearson?s Correlation Coefficient). Once the evaluation was complete, we used the mean persuasiveness score for each video as the ground-truth measure of the speaker?s perceived persuasiveness. In this initial effort, we focused on videos that were extremely persuasive or not persuasive at all. Hence, videos with a mean score of equal to or greater than 5.5 were taken as persuasive while those with a mean score of equal to or less than 2.5 were taken as unpersuasive. After this, we ended up with a total of 300 videos, specifically 157 videos of positive reviews (75 persuasive and 82 unpersuasive) and 143 videos of negative re-views (62 persuasive and 81 unpersuasive). 4.2 Transcriptions Using AMT and 18 participants from the same worker pool for persuasiveness evaluation, we ob-tained verbatim transcriptions of these filtered 300 videos, including transcriptions for filled pauses and stutters. Each transcription was reviewed and edited by multiple in-house experienced transcribers for accuracy. We do not use automatic speech recognition techniques in order to avoid noisy tran-scriptions. 5 Experiments In this section, we give details on the design of our computational descriptors followed by the experi-mental methodology. 5.1 Computational Descriptors In our experiments, our main focus was on devising computational descriptors for verbal behaviors in terms of lexical usage and also in terms of paraverbal markers of hesitation that can capture indica-tions of persuasiveness of the speaker.  Verbal (Lexical) Descriptors: As in many text classification tasks, we designed our verbal de-scriptors based on the bag-of-words representation using term frequency of both unigrams and bi-                                                1 Dataset available online: http://multicomp.ict.usc.edu/  
52
grams. Using the 300 filtered videos (see Section 4.1) and without feature selection, the numbers of unigrams reach around 4,500 and bigrams around 24,000. We did not proceed further with higher or-der n-grams because empirical evidence has shown that trigrams and other higher order n-grams do not always show improvement because they introduce problems related to the sparsity of features (Dave et al., 2003).   Paraverbal Descriptors of Hesitation: From the verbatim transcriptions of our corpus, we observed a set of frequent paraverbal cues that could potentially be associated with the level of persuasiveness. The set of descriptors is inspired from the findings of DeVault et al. (2013), who explored a similar set of generic paraverbal features in an interactive dialogue setting. However, we are interested specifically in the ones that capture signs of hesitation. The following were the descriptors that were used:  ? Pause-Fillers: The verbal behaviors of reviewers are often characterized with various pause-fillers, such as um or uh. In order to account for the varying length of each review, we normalized the count of all instances of filled pauses by the number of words spoken in the video. ? Disfluency Markers: A prominent marker of disfluency in human speech is stuttering. To capture this disfluency, we counted all instances of stuttering in each video and normalized them by the number of words spoken in the video. ? Articulation Rate: Articulation rate is defined as the rate of speaking in which all pauses are excluded from calculation (Dankovicova, 1999). This descriptor was computed by taking the ratio of the number of spoken words in each video to the actual time spent speaking.  ? Mean Span of Silence: Human speech is often interspersed with pauses. We therefore computed this descriptor, by measuring the total duration of silence during speech, normalized by the total length of the video. 5.2 Methodology We processed all the videos in our dataset and automatically extracted the indicated lexical and paraverbal features. The extracted features were then used for several classification experiments under three different settings to test our hypotheses: only positive reviews, only negative reviews (called  the sentiment-dependent classifiers) and a combined set of positive and negative reviews (called  the sentiment-independent classifiers). For each such setting, we divided the set of samples (transcription of movie reviews) into 5 balanced folds that were both speaker-independent and movie-independent. In other words, in all our experiments, no 2 folds contained samples from the same speaker or movie title. This was done to remove any form of bias in the classifier based on either the speaker or the movie.  We then performed classification experiments using 5-fold cross-validation using the lexical fea-tures (unigrams and bigrams) on this combined set of reviews (positive and negative reviews togeth-er), each time leaving 1 fold for hold-out testing. Here, we note that for constructing the dictionary, only data from the training set was used. On average across 5-fold cross-validation, the number of unigrams was around 4,560 and bigrams around 23,701 for the combined set of movie reviews. However, since such a feature design typically suffers from problems arising out of the sparsity of the entries of the dictionary in the dataset, we employed a feature selection step. For feature selection and analysis, we used Information Gain (IG), which is a measure of the number of bits of information obtained for category prediction by knowing the presence of a term in a document. Prior evaluation of feature-selection methods for text classification has revealed the superiority of IG as a metric over other ones such as Mutual Information, Term Strength or a simple Document Frequency thresholding for document classification tasks (Yang and Pedersen, 1997). This serves as an inspiring basis for us-ing IG as a metric for feature selection. The gain score G(t) obtained from IG is a non-zero positive value for features that are strongly in-dicative of the extent of persuasiveness of the document, while ones that are not so informative have a value of 0. We therefore select only those lexical features (unigrams and bigrams) which have an IG > 0 based on the distribution obtained from the training set. This allows us to trim the dictionary signifi-cantly and use only meaningful features for classification. 
53
This was then followed up by a 5-fold cross-validation using only the paraverbal features (no fea-ture selection was used here since they were too few in number). The accuracy of classification based on paraverbal features was then compared with that obtained by classification using only the lexical descriptors and by a majority baseline classifier. Furthermore, we also tried an early-fusion approach, where we simply use both lexical and para-verbal features together. Such an approach to fusion seemed more promising here than a decision-level fusion approach because of the few categories of features used (just lexical and paralinguistic, as motivated by the findings of (Gunes and Piccardi, 2005)). 5.3  Classification Model For performing classification experiments we used the Na?ve Bayes classifier. A well-known issue with using the Na?ve Bayes classifier is its incapability of handling new features, which is handled by performing a conditional uniform smoothing (Puurula, 2012). 6 Results and Discussion Table 1 shows the results for our classification experiments, which confirm the predictive power of lexical features. Hypothesis 1: The lexical features (unigrams and bigrams) are predictive of persuasiveness. This is manifested by the fact that they perform significantly better than a majority baseline, which is only 51.04% accurate on the combined set of positive and negative reviews, while the lexical features achieved an accuracy of around 77% (Figure 1). Considering the positive and the negative reviews individually, we note that the lexical features were accurate for nearly 82% of the test samples for the positive reviews and for 86% of the test samples for the negative reviews, again outperforming a sim-ple majority baseline classifier (Table 1). An analysis of the features (Table 2) reveals that certain lexical features contribute to the predicta-bility of the persuasiveness of a speaker. The presence of unigrams such as character or make or bi-grams such as to make or this movie for instance, contributes to the predictability of persuasiveness of the speaker, even though they are not emotionally salient terms. The high IG scores of such features irrespective of the setting we conduct our experiments in (positive reviews only, negative reviews on-ly or a combined set of positive and negative reviews), highlights their importance. Moreover, a (+) sign for most of these unigrams or bigrams show that their presence contributes favorably to the speaker being perceived as persuasive. On the other hand a (-) sign for an informative bigram such as it says is indicative of lack of speaker?s persuasiveness. This can be explained by the context of the usage of such features. For instance, the bigram it says in it says that the movie duration is? is a bi-
 Feature Group Sentiment Dependent Classifier Sentiment Independent Classifier Mean Positive Reviews Negative Reviews Lexical Features (Unigrams and Bigrams) 83.92% 81.74% 86.09% 76.73% ? Unigrams Only 77.70% 74.78% 80.62% 73.77% ? Bigrams  Only 84.05% 81.64% 86.46% 75.81% Para-Linguistic Features 64.23% 65.22% 63.23% 63.04% Early Fusion 84.54% 82.61% 86.46% 78.56% Majority Baseline 52.14% 50.43% 53.85% 51.09%  Table 1: Accuracies for our experiments using a Na?ve Bayes classifier. The scores in bold indi-cate the dominance of the sentiment-dependent classifier under all circumstances. 
54
gram that is uttered by the reviewers when they refer to the DVD cover of the movie to give some more detailed information about it. This is identified as a sign of an unpersuasive reviewer. Such re-sults confirm that the verbal behaviors, as captured by lexical usage, are extremely predictive of per-suasiveness irrespective of whether the opinion expressed is positive or negative, which validates Hy-pothesis 1.  Hypothesis 2: Moreover, our experiments show that while the designed paraverbal features that are markers of hesitation can classify only about 63% of the speakers correctly (see Table 1), however 
  Figure 1: Bar graph visualization of the classification accuracies using different types of fea-tures on the combined set of reviews (i.e. sentiment-independent classifier). ** indicates 2-samples t-test results with p < 0.01 and *** indicates p < 0.001. The error bars show 1 SD.   Feature Positive Reviews Negative Reviews Both Combined Word IG Score Word IG Score Word IG Score 
Unigrams 
The (+) 0.1183 Even (+) 0.11 Make  (+) 0.1117 Make  (+) 0.0816 Make  (-) 0.1082 Just  (+) 0.0728 Everything  (+) 0.0806 Movie  (+) 0.0969 Very  (+) 0.0669 Just  (+) 0.0806 Real  (+) 0.0873 Character  (+) 0.0573 Dollars (+) 0.0722 Not  (+) 0.0867 Becomes  (+) 0.0558 Character  (+) 0.0685 Big  (+) 0.0858 Even (+) 0.0524 Can  (+) 0.0685 One  (+) 0.0817 One  (+) 0.051 Product  (+) 0.0685 Avoid  (+) 0.079 Yourself  (+) 0.05 Famous (+) 0.0609 Feel  (+) 0.079 You  (+) 0.04571 Enjoy  (+) 0.0566 Character  (+) 0.0773 Lot  (+) 0.0456 
Bigrams       
There are  (+) 0.1183 This movie  (+) 0.1083 To make  (+) 0.0905 This movie  (+) 0.0816 Do not  (+) 0.1032 A lot  (+) 0.0617 I can?t  (+) 0.0806 I think  (+) 0.1032 This movie  (+) 0.0578 To make  (+) 0.0806 To make  (+) 0.0989 Lot of  (+) 0.0443 Good movie  (+) 0.0722 Not even  (+) 0.091 It says (-) 0.0417 Buy it  (+) 0.0685 Don?t even  (+) 0.091 You will (-) 0.0417 Really a  (+) 0.0685 The story  (+) 0.079 Twenty dollars (+) 0.0368 Definitely one  (+) 0.0685 The film  (+) 0.0672 The character (+) 0.0386 Best movies  (+) 0.0609 At all  (+) 0.0672 So many (+) 0.033 It?s awesome  (+) 0.0566 It?s so (+) 0.0672 See it (+) 0.033  Table 2: Important unigrams and bigrams when they are used individually as lexical features. (+) indicates that it increases persuasiveness while (-) indicates it contributes to the lack of per-suasiveness.   
55
they are statistically significant features, in terms of their p-values (Figure 2). While classification performance is lower than that obtained with purely lexical features, it is still far above a majority baseline, and thus confirms our second hypothesis. Additionally, it is interesting to note from Table 1 that, although a feature-level fusion of the lexical features and paraverbal features gives us an improvement in classification performance, the difference between the results obtained with fusion and those with lexical features alone was minor and was not statistically significant.  Hypothesis 3: We also observe that a sentiment-dependent classifier trained individually on positive reviews or on negative reviews outperforms one that is trained on a combined set of reviews. This is supported by our empirical results in Table 1 which show that when classification is performed with any of the lexical features, the accuracies are significantly higher for the classifier trained only on the positive or only on the negative reviews (sentiment-dependent classifiers) than for the classifier trained on the combined set of reviews (sentiment-independent classifiers). For instance, when unigrams and bigrams were both used as our lexical features, we observed that for a sentiment-dependent classifier the classification accuracy jumps to over 84% on average. This is significantly better than the scenario where the classifier is not aware of the sentiment of the review. Figure 3 demonstrates this phenomenon.     We resort to feature analysis for an explanation of such an observation (Table 2). The analysis re-veals that certain sentiment-based lexical features, i.e. emotionally salient terms, assume an important role in magnifying the discriminative power of language use in persuasiveness prediction, when prior knowledge about the speaker?s opinion is known. For instance, in the case of a classifier trained only on the positive reviews, unigrams such as enjoy and famous and bigrams such as good movie or it?s awesome become significant. In the context of persuading against watching the movie prominent sen-timent-based unigrams are not and avoid while bigrams are do not, don?t even and at all. This pro-vides empirical support for our third hypothesis. 7 Conclusion and Future Work This work presents several interesting findings about perceived persuasiveness prediction in online social multimedia content by analyzing the verbal behavior of the speaker, modeled using lexical fea-tures and paraverbal features of hesitation. We conducted experiments and showed that verbal behav-ior as captured by lexical descriptors is a strong indicator of persuasiveness, irrespective of whether we persuade in favor of or against something. Much of this is due to the presence of certain unigrams and bigrams that are either indicative of strong persuasiveness or of lack of persuasiveness. Our ex-periments further reveal the superiority of classifying with lexical features as compared to with para-
  Figure 2: Boxplots for the paralinguistic hesitation markers for a classifier trained on the para-linguistic features only. * and *** indicate p <= 0.05 and 0.001, respectively.   
56
verbal features alone.  Moreover we empirically validate the hypothesis that a sentiment-aware classi-fier outperforms a sentiment-independent one. As future work, we intend to explore more paraverbal features for persuasiveness prediction and also try more sophisticated prediction models which explic-itly model the temporal dynamic.  Acknowledgments This work was supported by the National Science Foundation under Grant IIS-1118018 and the U.S. Army. The content does not necessarily reflect the position or the policy of the Government, and no official endorsement should be inferred. References Shelly Chaiken and Alice H. Eagly. 1979. Communication modality as a determinant of message persuasiveness and message comprehensibility. Journal of Personality and Social Psychology, 37:1387-1397. Jana Dankovicova. 1999. Articulation rate variation within the intonation phrase in Czech and English. 14th Int. Congress of Phonetic Sciences, San Francisco, Vol. 1, pp. 269-272.  Kushal Dave, Steve Lawrence, and David M. Pennck. Mining the Peanut Gallery: Opinion Extraction and semantic Classification of Product Reviews, 2003. 2003 Association for Computational Linguistics (ACL ?03). David DeVault, Kallirroi Georgila, Ron Artstein, Fabrizio Morbini, David Traum, Stefan, Scherer, Albert (Skip) Rizzo, and Louis-Philippe Morency. 2013. Verbal indicators of psychological distress in interactive dialogue with a virtual human. SIGDIAL 2013 Conf, 2013 Association for Computational Linguistics (ACL ?13). Laurence Devillers and Laurence Vidrascu. 2006. Real-life emotions detection with lexical and paralinguistric cues on Human-Human call center dialogs. Interspeech 2006. Hatice Gunes, and Massimo Piccardi. 2005. Affect Recognition from face and body: Early fusion vs. Late fusion. IEEE Int?l Conf. on Systems, Man and Cybernnetic. Daniel  J. O?Keefe. 2002. Persuasion: Theory and research. (2nd Edition).  Sage Publications, Thousand Oaks, CA. David D. Lewis and William A. Gale. 1994. A Sequential Algorithm for Training Text Classifiers. Special Interest Group in Information Retieval (SIGIR?94 ). Gerald R. Miller (1980). On being persuaded: Some basic distinctions. In M. Roloff, & G. R. Miller (Eds.), Persuasion: New directions in theory and research, 11?28. Beverly Hills, CA: Sage. 
  Figure 3: Bar graph visualization of the classification accuracies of lexical features using a senti-ment-dependent classifier (mean) and a sentiment independent one. ** indicates 2-sample t-test results with p < 0.01 and the error bars show 1 SD.  
57
Tom M. Mitchell. 1997. Machine Learning. McGraw-Hill. Gelareh Mohammadi, Sunghyun Park, Kenji Sagae, Alessandro Vinciarelli, and Lois-Phillippe Morency. 2013. Who is persuasive? The role of perceived personality and Communication modality in social multimedia. Int?l Conf. on Multimodal Interfaces (ICMI ?13). P. Karen Murphy. 2001. What makes a text persuasive? Comparing students? and experts? conceptions of persuasiveness. Int?l Journal of Education Research, 35 (2001) 675-698. Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan. 2002. Thumbs up? Sentiment Classification using Machine Learning Techniques. Conf. on Empirical Methods in Natural Language Processing. (EMNLP ?02). Antti Puurula. 2012. Combining Modifications to Multinomial Naive Bayes for Text Classification. Springer, LNCS. Kathleen Kelley Reardon. 1991. Persuasion in practice. Sage Publication, Inc. Carol Werner. 1982. Intrusiveness and persuasive impact of three communication media. Journal of Applied Social Psychology, 89:155-181. Yiming Yang and Jan O. Pedersen. 1997. A comparative study on feature selection in text categorization. Int?l Conf. on Machine Learning (ICML ?97). Joel Young, Craig Martell, Pranav Anand, Pedro Ortiz and Henry T. Gilbert IV. 2011. A Microtext Corpus for Persuasion Detection in Dialog. Analyzing Microtext: AAAI Workshop (AAAI-Workshop ?11). Phillip G. Zimbardo and Michael R. Leippe. 1991. The psychology of attitude change and social influence. McGrew-Hill New York.   
58
