Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 102?107,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
BRAT: a Web-based Tool for NLP-Assisted Text Annotation
Pontus Stenetorp1? Sampo Pyysalo2,3? Goran Topic?1
Tomoko Ohta1,2,3 Sophia Ananiadou2,3 and Jun?ichi Tsujii4
1Department of Computer Science, The University of Tokyo, Tokyo, Japan
2School of Computer Science, University of Manchester, Manchester, UK
3National Centre for Text Mining, University of Manchester, Manchester, UK
4Microsoft Research Asia, Beijing, People?s Republic of China
{pontus,smp,goran,okap}@is.s.u-tokyo.ac.jp
sophia.ananiadou@manchester.ac.uk
jtsujii@microsoft.com
Abstract
We introduce the brat rapid annotation tool
(BRAT), an intuitive web-based tool for text
annotation supported by Natural Language
Processing (NLP) technology. BRAT has
been developed for rich structured annota-
tion for a variety of NLP tasks and aims
to support manual curation efforts and in-
crease annotator productivity using NLP
techniques. We discuss several case stud-
ies of real-world annotation projects using
pre-release versions of BRAT and present
an evaluation of annotation assisted by se-
mantic class disambiguation on a multi-
category entity mention annotation task,
showing a 15% decrease in total annota-
tion time. BRAT is available under an open-
source license from: http://brat.nlplab.org
1 Introduction
Manually-curated gold standard annotations are
a prerequisite for the evaluation and training of
state-of-the-art tools for most Natural Language
Processing (NLP) tasks. However, annotation is
also one of the most time-consuming and finan-
cially costly components of many NLP research
efforts, and can place heavy demands on human
annotators for maintaining annotation quality and
consistency. Yet, modern annotation tools are
generally technically oriented and many offer lit-
tle support to users beyond the minimum required
functionality. We believe that intuitive and user-
friendly interfaces as well as the judicious appli-
cation of NLP technology to support, not sup-
plant, human judgements can help maintain the
quality of annotations, make annotation more ac-
cessible to non-technical users such as subject
?These authors contributed equally to this work
Figure 1: Visualisation examples. Top: named en-
tity recognition, middle: dependency syntax, bot-
tom: verb frames.
domain experts, and improve annotation produc-
tivity, thus reducing both the human and finan-
cial cost of annotation. The tool presented in
this work, BRAT, represents our attempt to realise
these possibilities.
2 Features
2.1 High-quality Annotation Visualisation
BRAT is based on our previously released open-
source STAV text annotation visualiser (Stene-
torp et al 2011b), which was designed to help
users gain an understanding of complex annota-
tions involving a large number of different se-
mantic types, dense, partially overlapping text an-
notations, and non-projective sets of connections
between annotations. Both tools share a vector
graphics-based visualisation component, which
provide scalable detail and rendering. BRAT in-
tegrates PDF and EPS image format export func-
tionality to support use in e.g. figures in publica-
tions (Figure 1).
102
Figure 2: Screenshot of the main BRAT user-interface, showing a connection being made between the
annotations for ?moving? and ?Citibank?.
2.2 Intuitive Annotation Interface
We extended the capabilities of STAV by imple-
menting support for annotation editing. This was
done by adding functionality for recognising stan-
dard user interface gestures familiar from text ed-
itors, presentation software, and many other tools.
In BRAT, a span of text is marked for annotation
simply by selecting it with the mouse by ?drag-
ging? or by double-clicking on a word. Similarly,
annotations are linked by clicking with the mouse
on one annotation and dragging a connection to
the other (Figure 2).
BRAT is browser-based and built entirely using
standard web technologies. It thus offers a fa-
miliar environment to annotators, and it is pos-
sible to start using BRAT simply by pointing a
standards-compliant modern browser to an instal-
lation. There is thus no need to install or dis-
tribute any additional annotation software or to
use browser plug-ins. The use of web standards
also makes it possible for BRAT to uniquely iden-
tify any annotation using Uniform Resource Iden-
tifiers (URIs), which enables linking to individual
annotations for discussions in e-mail, documents
and on web pages, facilitating easy communica-
tion regarding annotations.
2.3 Versatile Annotation Support
BRAT is fully configurable and can be set up to
support most text annotation tasks. The most ba-
sic annotation primitive identifies a text span and
assigns it a type (or tag or label), marking for e.g.
POS-tagged tokens, chunks or entity mentions
(Figure 1 top). These base annotations can be
connected by binary relations ? either directed or
undirected ? which can be configured for e.g. sim-
ple relation extraction, or verb frame annotation
(Figure 1 middle and bottom). n-ary associations
of annotations are also supported, allowing the an-
notation of event structures such as those targeted
in the MUC (Sundheim, 1996), ACE (Doddington
et al 2004), and BioNLP (Kim et al 2011) In-
formation Extraction (IE) tasks (Figure 2). Addi-
tional aspects of annotations can be marked using
attributes, binary or multi-valued flags that can
be added to other annotations. Finally, annotators
can attach free-form text notes to any annotation.
In addition to information extraction tasks,
these annotation primitives allow BRAT to be
configured for use in various other tasks, such
as chunking (Abney, 1991), Semantic Role La-
beling (Gildea and Jurafsky, 2002; Carreras
and Ma`rquez, 2005), and dependency annotation
(Nivre, 2003) (See Figure 1 for examples). Fur-
ther, both the BRAT client and server implement
full support for the Unicode standard, which al-
low the tool to support the annotation of text us-
ing e.g. Chinese or Devana?gar?? characters. BRAT
is distributed with examples from over 20 cor-
pora for a variety of tasks, involving texts in seven
different languages and including examples from
corpora such as those introduced for the CoNLL
shared tasks on language-independent named en-
tity recognition (Tjong Kim Sang and De Meul-
der, 2003) and multilingual dependency parsing
(Buchholz and Marsi, 2006).
BRAT also implements a fully configurable sys-
tem for checking detailed constraints on anno-
tation semantics, for example specifying that a
TRANSFER event must take exactly one of each
of GIVER, RECIPIENT and BENEFICIARY argu-
ments, each of which must have one of the types
PERSON, ORGANIZATION or GEO-POLITICAL
ENTITY, as well as a MONEY argument of type
103
Figure 3: Incomplete TRANSFER event indicated
to the annotator
MONEY, and may optionally take a PLACE argu-
ment of type LOCATION (LDC, 2005). Constraint
checking is fully integrated into the annotation in-
terface and feedback is immediate, with clear vi-
sual effects marking incomplete or erroneous an-
notations (Figure 3).
2.4 NLP Technology Integration
BRAT supports two standard approaches for inte-
grating the results of fully automatic annotation
tools into an annotation workflow: bulk anno-
tation imports can be performed by format con-
version tools distributed with BRAT for many
standard formats (such as in-line and column-
formatted BIO), and tools that provide standard
web service interfaces can be configured to be in-
voked from the user interface.
However, human judgements cannot be re-
placed or based on a completely automatic analy-
sis without some risk of introducing bias and re-
ducing annotation quality. To address this issue,
we have been studying ways to augment the an-
notation process with input from statistical and
machine learning methods to support the annota-
tion process while still involving human annotator
judgement for each annotation.
As a specific realisation based on this approach,
we have integrated a recently introduced ma-
chine learning-based semantic class disambigua-
tion system capable of offering multiple outputs
with probability estimates that was shown to be
able to reduce ambiguity on average by over 75%
while retaining the correct class in on average
99% of cases over six corpora (Stenetorp et al
2011a). Section 4 presents an evaluation of the
contribution of this component to annotator pro-
ductivity.
2.5 Corpus Search Functionality
BRAT implements a comprehensive set of search
functions, allowing users to search document col-
Figure 4: The BRAT search dialog
lections for text span annotations, relations, event
structures, or simply text, with a rich set of search
options definable using a simple point-and-click
interface (Figure 4). Additionally, search results
can optionally be displayed using keyword-in-
context concordancing and sorted for browsing
using any aspect of the matched annotation (e.g.
type, text, or context).
3 Implementation
BRAT is implemented using a client-server ar-
chitecture with communication over HTTP using
JavaScript Object Notation (JSON). The server is
a RESTful web service (Fielding, 2000) and the
tool can easily be extended or adapted to switch
out the server or client. The client user interface is
implemented using XHTML and Scalable Vector
Graphics (SVG), with interactivity implemented
using JavaScript with the jQuery library. The
client communicates with the server using Asyn-
chronous JavaScript and XML (AJAX), which
permits asynchronous messaging.
BRAT uses a stateless server back-end imple-
mented in Python and supports both the Common
Gateway Interface (CGI) and FastCGI protocols,
the latter allowing response times far below the
100 ms boundary for a ?smooth? user experience
without noticeable delay (Card et al 1983). For
server side annotation storage BRAT uses an easy-
to-process file-based stand-off format that can be
converted from or into other formats; there is no
need to perform database import or export to in-
terface with the data storage. The BRAT server in-
104
Figure 5: Example annotation from the BioNLP Shared Task 2011 Epigenetics and Post-translational
Modifications event extraction task.
stallation requires only a CGI-capable web server
and the set-up supports any number of annotators
who access the server using their browsers, on any
operating system, without separate installation.
Client-server communication is managed so
that all user edit operations are immediately sent
to the server, which consolidates them with the
stored data. There is no separate ?save? operation
and thus a minimal risk of data loss, and as the
authoritative version of all annotations is always
maintained by the server, there is no chance of
conflicting annotations being made which would
need to be merged to produce an authoritative ver-
sion. The BRAT client-server architecture also
makes real-time collaboration possible: multiple
annotators can work on a single document simul-
taneously, seeing each others edits as they appear
in a document.
4 Case Studies
4.1 Annotation Projects
BRAT has been used throughout its development
during 2011 in the annotation of six different cor-
pora by four research groups in efforts that have
in total involved the creation of well-over 50,000
annotations in thousands of documents compris-
ing hundreds of thousands of words.
These projects include structured event an-
notation for the domain of cancer biology,
Japanese verb frame annotation, and gene-
mutation-phenotype relation annotation. One
prominent effort making use of BRAT is the
BioNLP Shared Task 2011,1 in which the tool was
used in the annotation of the EPI and ID main
task corpora (Pyysalo et al 2012). These two
information extraction tasks involved the annota-
tion of entities, relations and events in the epige-
netics and infectious diseases subdomains of biol-
ogy. Figure 5 shows an illustration of shared task
annotations.
Many other annotation efforts using BRAT are
still ongoing. We refer the reader to the BRAT
1http://2011.bionlp-st.org
Mode Total Type Selection
Normal 45:28 13:49
Rapid 39:24 (-6:04) 09:35 (-4:14)
Table 1: Total annotation time, portion spent se-
lecting annotation type, and absolute improve-
ment for rapid mode.
website2 for further details on current and past an-
notation projects using BRAT.
4.2 Automatic Annotation Support
To estimate the contribution of the semantic class
disambiguation component to annotation produc-
tivity, we performed a small-scale experiment in-
volving an entity and process mention tagging
task. The annotation targets were of 54 dis-
tinct mention types (19 physical entity and 35
event/process types) marked using the simple
typed-span representation. To reduce confound-
ing effects from annotator productivity differ-
ences and learning during the task, annotation was
performed by a single experienced annotator with
a Ph.D. in biology in a closely related area who
was previously familiar with the annotation task.
The experiment was performed on publication
abstracts from the biomolecular science subdo-
main of glucose metabolism in cancer. The texts
were drawn from a pool of 1,750 initial candi-
dates using stratified sampling to select pairs of
10-document sets with similar overall statistical
properties.3 Four pairs of 10 documents (80 in to-
tal) were annotated in the experiment, with 10 in
each pair annotated with automatic support and 10
without, in alternating sequence to prevent learn-
ing effects from favouring either approach.
The results of this experiment are summarized
in Table 1 and Figure 6. In total 1,546 annotations
were created in normal mode and 1,541 annota-
2http://brat.nlplab.org
3Document word count and expected annotation count,
were estimated from the output of NERsuite, a freely avail-
able CRF-based NER tagger: http://nersuite.nlplab.org
105
0500
1000
1500
2000
2500
3000
Normal Mode Rapid Mode
Tim
e(
se
co
nd
s)
Figure 6: Allocation of annotation time. GREEN
signifies time spent on selecting annotation type
and BLUE the remaining annotation time.
tions in rapid mode; the sets are thus highly com-
parable. We observe a 15.4% reduction in total
annotation time, and, as expected, this is almost
exclusively due to a reduction in the time the an-
notator spent selecting the type to assign to each
span, which is reduced by 30.7%; annotation time
is otherwise stable across the annotation modes
(Figure 6). The reduction in the time spent in se-
lecting the span is explained by the limiting of the
number of candidate types exposed to the annota-
tor, which were decreased from the original 54 to
an average of 2.88 by the semantic class disam-
biguation component (Stenetorp et al 2011a).
Although further research is needed to establish
the benefits of this approach in various annotation
tasks, we view the results of this initial experi-
ment as promising regarding the potential of our
approach to using machine learning to support an-
notation efforts.
5 Related Work and Conclusions
We have introduced BRAT, an intuitive and user-
friendly web-based annotation tool that aims to
enhance annotator productivity by closely inte-
grating NLP technology into the annotation pro-
cess. BRAT has been and is being used for several
ongoing annotation efforts at a number of aca-
demic institutions and has so far been used for
the creation of well-over 50,000 annotations. We
presented an experiment demonstrating that inte-
grated machine learning technology can reduce
the time for type selection by over 30% and over-
all annotation time by 15% for a multi-type entity
mention annotation task.
The design and implementation of BRAT was
informed by experience from several annotation
tasks and research efforts spanning more than
a decade. A variety of previously introduced
annotation tools and approaches also served to
guide our design decisions, including the fast an-
notation mode of Knowtator (Ogren, 2006), the
search capabilities of the XConc tool (Kim et al
2008), and the design of web-based systems such
as MyMiner (Salgado et al 2010), and GATE
Teamware (Cunningham et al 2011). Using ma-
chine learning to accelerate annotation by sup-
porting human judgements is well documented in
the literature for tasks such as entity annotation
(Tsuruoka et al 2008) and translation (Mart??nez-
Go?mez et al 2011), efforts which served as in-
spiration for our own approach.
BRAT, along with conversion tools and exten-
sive documentation, is freely available under the
open-source MIT license from its homepage at
http://brat.nlplab.org
Acknowledgements
The authors would like to thank early adopters of
BRAT who have provided us with extensive feed-
back and feature suggestions. This work was sup-
ported by Grant-in-Aid for Specially Promoted
Research (MEXT, Japan), the UK Biotechnology
and Biological Sciences Research Council (BB-
SRC) under project Automated Biological Event
Extraction from the Literature for Drug Discov-
ery (reference number: BB/G013160/1), and the
Royal Swedish Academy of Sciences.
106
References
Steven Abney. 1991. Parsing by chunks. Principle-
based parsing, 44:257?278.
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-
X shared task on multilingual dependency parsing.
In Proceedings of the Tenth Conference on Com-
putational Natural Language Learning (CoNLL-X),
pages 149?164.
Stuart K. Card, Thomas P. Moran, and Allen Newell.
1983. The psychology of human-computer interac-
tion. Lawrence Erlbaum Associates, Hillsdale, New
Jersey.
Xavier Carreras and Llu??s Ma`rquez. 2005. Introduc-
tion to the CoNLL-2005 shared task: Semantic Role
Labeling. In Proceedings of the 9th Conference on
Natural Language Learning, pages 152?164. Asso-
ciation for Computational Linguistics.
Hamish Cunningham, Diana Maynard, Kalina
Bontcheva, Valentin Tablan, Niraj Aswani, Ian
Roberts, Genevieve Gorrell, Adam Funk, Angus
Roberts, Danica Damljanovic, Thomas Heitz,
Mark A. Greenwood, Horacio Saggion, Johann
Petrak, Yaoyong Li, and Wim Peters. 2011. Text
Processing with GATE (Version 6).
George Doddington, Alexis Mitchell, Mark Przybocki,
Lance Ramshaw, Stephanie Strassel, and Ralph
Weischedel. 2004. The Automatic Content Extrac-
tion (ACE) program: Tasks, data, and evaluation. In
Proceedings of the 4th International Conference on
Language Resources and Evaluation, pages 837?
840.
Roy Fielding. 2000. REpresentational State Trans-
fer (REST). Architectural Styles and the Design
of Network-based Software Architectures. Univer-
sity of California, Irvine, page 120.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic
labeling of semantic roles. Computational Linguis-
tics, 28(3):245?288.
Jin-Dong Kim, Tomoko Ohta, and Jun?ichi Tsujii.
2008. Corpus annotation for mining biomedi-
cal events from literature. BMC Bioinformatics,
9(1):10.
Jin-Dong Kim, Sampo Pyysalo, Tomoko Ohta, Robert
Bossy, Ngan Nguyen, and Jun?ichi Tsujii. 2011.
Overview of BioNLP Shared Task 2011. In Pro-
ceedings of BioNLP Shared Task 2011 Workshop,
pages 1?6, Portland, Oregon, USA, June. Associa-
tion for Computational Linguistics.
LDC. 2005. ACE (Automatic Content Extraction) En-
glish Annotation Guidelines for Events. Technical
report, Linguistic Data Consortium.
Pascual Mart??nez-Go?mez, Germa?n Sanchis-Trilles,
and Francisco Casacuberta. 2011. Online learn-
ing via dynamic reranking for computer assisted
translation. In Alexander Gelbukh, editor, Compu-
tational Linguistics and Intelligent Text Processing,
volume 6609 of Lecture Notes in Computer Science,
pages 93?105. Springer Berlin / Heidelberg.
Joakim Nivre. 2003. An Efficient Algorithm for Pro-
jective Dependency Parsing. In Proceedings of the
8th International Workshop on Parsing Technolo-
gies, pages 149?160.
Philip V. Ogren. 2006. Knowtator: A prote?ge? plug-in
for annotated corpus construction. In Proceedings
of the Conference of the North American Chapter of
the Association for Computational Linguistics: Hu-
man Language Technologies, Companion Volume:
Demonstrations, pages 273?275, New York City,
USA, June. Association for Computational Linguis-
tics.
Sampo Pyysalo, Tomoko Ohta, Rafal Rak, Dan Sul-
livan, Chunhong Mao, Chunxia Wang, Bruno So-
bral, Junichi Tsujii, and Sophia Ananiadou. 2012.
Overview of the ID, EPI and REL tasks of BioNLP
Shared Task 2011. BMC Bioinformatics, 13(suppl.
8):S2.
David Salgado, Martin Krallinger, Marc Depaule,
Elodie Drula, and Ashish V Tendulkar. 2010.
Myminer system description. In Proceedings of the
Third BioCreative Challenge Evaluation Workshop
2010, pages 157?158.
Pontus Stenetorp, Sampo Pyysalo, Sophia Ananiadou,
and Jun?ichi Tsujii. 2011a. Almost total recall: Se-
mantic category disambiguation using large lexical
resources and approximate string matching. In Pro-
ceedings of the Fourth International Symposium on
Languages in Biology and Medicine.
Pontus Stenetorp, Goran Topic?, Sampo Pyysalo,
Tomoko Ohta, Jin-Dong Kim, and Jun?ichi Tsujii.
2011b. BioNLP Shared Task 2011: Supporting Re-
sources. In Proceedings of BioNLP Shared Task
2011 Workshop, pages 112?120, Portland, Oregon,
USA, June. Association for Computational Linguis-
tics.
Beth M. Sundheim. 1996. Overview of results of
the MUC-6 evaluation. In Proceedings of the Sixth
Message Understanding Conference, pages 423?
442. Association for Computational Linguistics.
Erik F. Tjong Kim Sang and Fien De Meulder.
2003. Introduction to the CoNLL-2003 shared
task: Language-independent named entity recogni-
tion. In Proceedings of the Seventh Conference on
Natural Language Learning at HLT-NAACL 2003,
pages 142?147.
Yoshimasa Tsuruoka, Jun?ichi Tsujii, and Sophia Ana-
niadou. 2008. Accelerating the annotation of
sparse named entities by dynamic sentence selec-
tion. BMC Bioinformatics, 9(Suppl 11):S8.
107
Proceedings of BioNLP Shared Task 2011 Workshop, pages 112?120,
Portland, Oregon, USA, 24 June, 2011. c?2011 Association for Computational Linguistics
BioNLP Shared Task 2011: Supporting Resources
Pontus Stenetorp: Goran Topic? Sampo Pyysalo
Tomoko Ohta Jin-Dong Kim; and Jun?ichi Tsujii$
Tsujii Laboratory, Department of Computer Science, University of Tokyo, Tokyo, Japan
:Aizawa Laboratory, Department of Computer Science, University of Tokyo, Tokyo, Japan
; Database Center for Life Science,
Research Organization of Information and Systems, Tokyo, Japan
$Microsoft Research Asia, Beijing, People?s Republic of China
{pontus,goran,smp,okap}@is.s.u-tokyo.ac.jp
jdkim@dbcls.rois.ac.jp
jtsujii@microsoft.com
Abstract
This paper describes the supporting resources
provided for the BioNLP Shared Task 2011.
These resources were constructed with the
goal to alleviate some of the burden of sys-
tem development from the participants and al-
low them to focus on the novel aspects of con-
structing their event extraction systems. With
the availability of these resources we also seek
to enable the evaluation of the applicability of
specific tools and representations towards im-
proving the performance of event extraction
systems. Additionally we supplied evaluation
software and services and constructed a vi-
sualisation tool, stav, which visualises event
extraction results and annotations. These re-
sources helped the participants make sure that
their final submissions and research efforts
were on track during the development stages
and evaluate their progress throughout the du-
ration of the shared task. The visualisation
software was also employed to show the dif-
ferences between the gold annotations and
those of the submitted results, allowing the
participants to better understand the perfor-
mance of their system. The resources, evalu-
ation tools and visualisation tool are provided
freely for research purposes and can be found
at http://sites.google.com/site/bionlpst/
1 Introduction
For the BioNLP?09 Shared Task (Kim et al, 2009),
the first in the ongoing series, the organisers pro-
vided the participants with automatically generated
syntactic analyses for the sentences from the anno-
tated data. For evaluation purposes, tools were made
publicly available as both distributed software and
online services. These resources were well received.
A majority of the participants made use of one or
more of the syntactic analyses, which have remained
available after the shared task ended and have been
employed in at least two independent efforts study-
ing the contribution of different tools and forms of
syntactic representation to the domain of informa-
tion extraction (Miwa et al, 2010; Buyko and Hahn,
2010). The evaluation software for the BioNLP?09
Shared Task has also been widely adopted in subse-
quent studies (Miwa et al, 2010; Poon and Vander-
wende, 2010; Bjo?rne et al, 2010).
The reception and research contribution from pro-
viding these resources encouraged us to continue
providing similar resources for the BioNLP Shared
Task 2011 (Kim et al, 2011a). Along with the
parses we also encouraged the participants and ex-
ternal groups to process the data with any NLP (Nat-
ural Language Processing) tools of their choice and
make the results available to the participants.
We provided continuous verification and evalua-
tion of the participating systems using a suite of in-
house evaluation tools. Lastly, we provided a tool
for visualising the annotated data to enable the par-
ticipants to better grasp the results of their experi-
ments and to help gain a deeper understanding of
the underlying concepts and the annotated data. This
paper presents these supporting resources.
2 Data
This section introduces the data resources provided
by the organisers, participants and external groups
for the shared task.
112
Task Provider Tool
CO University of Utah Reconcile
CO University of Zu?rich UZCRS
CO University of Turku TEES
REL University of Turku TEES
Table 1: Supporting task analyses provided, TEES
is the Turku Event Extraction System and UZCRS
is the University of Zu?rich Coreference Resolution
System
2.1 Supporting task analyses
The shared task included three Supporting Tasks:
Coreference (CO) (Nguyen et al, 2011), Entity re-
lations (REL) (Pyysalo et al, 2011b) and Gene re-
naming (REN) (Jourde et al, 2011). In the shared
task schedule, the supporting tasks were carried out
before the main tasks (Kim et al, 2011b; Pyysalo
et al, 2011a; Ohta et al, 2011; Bossy et al, 2011)
in order to allow participants to make use of analy-
ses from the systems participating in the Supporting
Tasks for their main task event extraction systems.
Error analysis of BioNLP?09 shared task sub-
missions indicated that coreference was the most
frequent feature of events that could not be cor-
rectly extracted by any participating system. Fur-
ther, events involving statements of non-trivial rela-
tions between participating entities were a frequent
cause of extraction errors. Thus, the CO and REL
tasks were explicitly designed to support parts of
the main event extraction tasks where it had been
suggested that they could improve the system per-
formance.
Table 1 shows the supporting task analyses pro-
vided to the participants. For the main tasks, we
are currently aware of one group (Emadzadeh et al,
2011) that made use of the REL task analyses in their
system. However, while a number of systems in-
volved coreference resolution in some form, we are
not aware of any teams using the CO task analyses
specifically, perhaps due in part to the tight sched-
ule and the somewhat limited results of the CO task.
These data will remain available to allow future re-
search into the benefits of these resources for event
extraction.
2.2 Syntactic analyses
For syntactic analyses we provided parses for all
the task data in various formats from a wide range
of parsers (see Table 2). With the exception of
the Pro3Gres1 parser (Schneider et al, 2007), the
parsers were set up and run by the task organisers.
The emphasis was put on availability for research
purposes and variety of parsing models and frame-
works to allow evaluation of their applicability for
different tasks.
In part following up on the results of Miwa et al
(2010) and Buyko and Hahn (2010) regarding the
impact on performance of event extraction systems
depending on the dependency parse representation,
we aimed to provide several dependency parse for-
mats. Stanford Dependencies (SD) and Collapsed
Stanford Dependencies (SDC), as described by de
Marneffe et al (2006), were generated by convert-
ing Penn Treebank (PTB)-style (Marcus et al, 1993)
output using the Stanford CoreNLP Tools2 into the
two dependency formats. We also provided Confer-
ence on Computational Natural Language Learning
style dependency parses (CoNLL-X) (Buchholz and
Marsi, 2006) which were also converted from PTB-
style output, but for this we used the conversion
tool3 from Johansson and Nugues (2007). While
this conversion tool was not designed with convert-
ing the output from statistical parsers in mind (but
rather to convert between treebanks), it has previ-
ously been applied successfully for this task (Miyao
et al, 2008; Miwa et al, 2010).
The text from all documents provided were split
into sentences using the Genia Sentence Splitter4
(S?tre et al, 2007) and then postprocessed using a
set of heuristics to correct frequently occurring er-
rors. The sentences were then tokenised using a to-
kenisation script created by the organisers intended
to replicate the tokenisation of the Genia Tree Bank
(GTB) (Tateisi et al, 2005). This tokenised and
sentence-split data was then used as input for all
parsers.
We used two deep parsers that provide phrase
structure analysis enriched with deep sentence struc-
1https://files.ifi.uzh.ch/cl/gschneid/parser/
2http://nlp.stanford.edu/software/corenlp.shtml
3http://nlp.cs.lth.se/software/treebank converter/
4http://www-tsujii.is.s.u-tokyo.ac.jp/y-matsu/geniass/
113
Name Format(s) Model Availability BioNLP?09
Berkeley PTB, SD, SDC, CoNLL-X News Binary, Source No
C&C CCG, SD Biomedical Binary, Source Yes
Enju HPSG, PTB, SD, SDC, CoNLL-X Biomedical Binary No
GDep CoNLL-X Biomedical Binary, Source Yes
McCCJ PTB, SD, SDC, CoNLL-X Biomedical Source Yes
Pro3Gres Pro3Gres Combination ? No
Stanford PTB, SD, SDC, CoNLL-X Combination Binary, Source Yes
Table 2: Parsers, the formats for which their output was provided and which type of model that was used. The
availability column signifies public availability (without making an explicit request) for research purposes
tures, for example predicate-argument structure for
Head-Driven Phrase Structure Grammar (HPSG).
First we used the C&C Combinatory Categorial
Grammar (CCG) parser5 (C&C) by Clark and Cur-
ran (2004) using the biomedical model described in
Rimell and Clark (2009) which was trained on GTB.
Unlike all other parsers for which we supplied SD
and SDC dependency parses, the C&C output was
converted from its native format using a separate
conversion script provided by the C&C authors. Re-
grettably we were unable to provide CoNLL-X for-
mat output for this parser due to the lack of PTB-
style output. The other deep parser used was the
HPSG parser Enju6 by Miyao and Tsujii (2008), also
trained on GTB.
We also applied the frequently adopted Stanford
Parser7 (Klein and Manning, 2003) using a mixed
model which includes data from the biomedical do-
main, and the Charniak Johnson re-ranking parser8
(Charniak and Johnson, 2005) using the self-trained
biomedical model from McClosky (2009) (McCCJ).
For the BioNLP?09 shared task it was observed
that the Bikel parser9 (Bikel, 2004), which used a
non-biomedical model and can be argued that it uses
the somewhat dated Collins? parsing model (Collins,
1996), did not contribute towards event extraction
performance as strongly as other parses supplied for
the same data. We therefore wanted to supply a
parser that can compete with the ones above in a do-
main which is different from the biomedical domain
to see whether conclusions could be drawn as to the
5http://svn.ask.it.usyd.edu.au/trac/candc/
6http://www-tsujii.is.s.u-tokyo.ac.jp/enju/
7http://nlp.stanford.edu/software/lex-parser.shtml
8ftp://ftp.cs.brown.edu/pub/nlparser/
9http://www.cis.upenn.edu/dbikel/software.html
importance of using a biomedical model. For this
we used the Berkeley parser10 (Petrov et al, 2006).
Lastly we used a native dependency parser, the GE-
NIA Dependency parser (GDep) by Sagae and Tsujii
(2007).
At least one team (Choudhury et al, 2011) per-
formed experiments on some of the provided lexi-
cal analyses and among the 14 submissions for the
EPI and ID tasks, 13 submissions utilised tools for
which resources were provided by the organisers of
the shared task. We intend to follow up on whether
or not the majority of the teams ran the tools them-
selves or used the provided analyses.
2.3 Other analyses
The call for analyses was open to all interested par-
ties and all forms of analysis. In addition to the Sup-
porting Task analyses (CO and REL) and syntactic
analyses provided by various groups, the University
of Antwerp CLiPS center (Morante et al, 2010) re-
sponded to the call providing negation/speculation
analyses in the BioScope corpus format (Szarvas et
al., 2008).
Although this resource was not utilised by the par-
ticipants for the main task, possibly due to a lack of
time, it is our hope that by keeping the data available
it can lead to further development of the participat-
ing systems and analysis of BioScope and BioNLP
ST-style hedging annotations.
3 Tools
This section presents the tools produced by the or-
ganisers for the purpose of the shared task.
10http://code.google.com/p/berkeleyparser/
114
1 10411007-E1 Regulation <Exp>regulate[26-34] <Theme>TNF-alpha[79-88] ?
?<Excerpt>[regulate] an enhancer activity in the third intron of [TNF-alpha]
2 10411007-E2 Gene_expression <Exp>activity[282-290] <Theme>TNF-alpha[252-261] ?
?<Excerpt>[TNF-alpha] gene displayed weak [activity]
3 10411007-E3 +Regulation <Exp>when[291-295] <Theme>E2 <Excerpt>[when]
Figure 1: Text output from the BioNLP?09 Shared Event Viewer with line numbering and newline markings
Figure 2: An illustration of collective (sentence 1)
and distributive reading (sentence 2). ?Theme? is
abbreviated as ?Th? and ?Protein? as ?Pro? when
there is a lack of space
3.1 Visualisation
The annotation data in the format specified by the
shared task is not intended to be human-readable ?
yet researchers need to be able to visualise the data
in order to understand the results of their experi-
ments. However, there is a scarcity of tools that can
be used for this purpose. There are three available
for event annotations in the BioNLP ST format that
we are aware of.
One is the BioNLP?09 Shared Task Event
Viewer11, a simple text-based annotation viewer: it
aggregates data from the annotations, and outputs it
in a format (Figure 1) that is meant to be further pro-
cessed by a utility such as grep.
Another is What?s Wrong with My NLP12, which
visualises relation annotations (see Figure 3a) ? but
is unable to display some of the information con-
tained in the Shared Task data. Notably, the distribu-
tive and collective readings of an event are not dis-
tinguished (Figure 2). It also displays all annotations
on a single line, which makes reading and analysing
longer sentences, let alne whole documents, some-
what difficult.
The last one is U-Compare13 (Kano et al, 2009),
11http://www-tsujii.is.s.u-tokyo.ac.jp/GENIA/SharedTask/
downloads.shtml
12http://code.google.com/p/whatswrong/
13http://u-compare.org/bionlp2009.html
which is a comprehensive suite of tools designed for
managing NLP workflows, integrating many avail-
able services. However, the annotation visualisation
component, illustrated in Figure 3b, is not optimised
for displaying complex event structures. Each anno-
tation is marked by underlining its text segment us-
ing a different colour per annotation type, and a role
in an event is represented by a similarly coloured arc
between the related underlined text segments. The
implementation leaves some things to be desired:
there is no detailed information added in the display
unless the user explicitly requests it, and then it is
displayed in a separate panel, away from the text it
annotates. The text spacing makes no allowance for
the annotations, with opaque lines crossing over it,
with the effect of making both the annotations and
the text hard to read if the annotations are above a
certain degree of complexity.
As a result of the difficulties of these existing
tools, in order to extract a piece of annotated text
and rework it into a graph that could be embedded
into a publication, users usually read off the annota-
tions, then create a graph from scratch using vector
drawing or image editing software.
To address these issues, we created a visualisa-
tion tool named stav (stav Text Annotation Visual-
izer), that can read the data formatted according to
the Shared Task specification and aims to present it
to the user in a form that can be grasped at a glance.
Events and entities are annotated immediately above
the text, and the roles within an event by labelled
arcs between them (Figure 3c). In a very complex
graph, users can highlight the object or association
of interest to follow it even more easily. Special fea-
tures of annotations, such as negation or speculation,
are shown by unique visual cues, and more in-depth,
technical information that is usually not required can
be requested by floating the mouse cursor over the
annotation (as seen in Figure 5).
We took care to minimise arc crossovers, and to
115
(a) Visualisation using What?s Wrong with My NLP
(b) Visualisation using U-Compare
(c) Visualisation using stav
Figure 3: Different visualisations of complex textual annotations of Dickensheets et al (1999)
116
Figure 4: A screenshot of the stav file-browser
keep them away from the text itself, in order to main-
tain text readability. The text is spaced to accommo-
date the annotations between the rows. While this
does end up using more screen real-estate, it keeps
the text legible, and annotations adjacent to the text.
The text is broken up into lines, and each sentence
is also forced into a new line, and given a numer-
ical identifier. The effect of this is that the text is
laid out vertically, like an article would be, but with
large spacing to accomodate the annotations. The
arcs are similarly continued on successive lines, and
can easily be traced ? even in case of them spanning
multiple lines, by the use of mouseover highlight-
ing. To preserve the distributionality information of
the annotation, any event annotations are duplicated
for each event, as demonstrated in the example in
Figure 2.
stav is not limited to the Shared Task datasets with
appropriate configuration settings, it could also vi-
sualise other kinds of relational annotations such as:
frame structures (Fillmore, 1976) and dependency
parses (de Marneffe et al, 2006).
To achieve our objectives above, we use the Dy-
namic Scalable Vector Graphics (SVG) functional-
ity (i.e. SVG manipulated by JavaScript) provided
by most modern browsers to render the WYSIWYG
(What You See Is What You Get) representation of
the annotated document. An added benefit from
this technique is that the installation process, if any,
is very simple: although not all browsers are cur-
rently supported, the two that we specifically tested
against are Safari14 and Google Chrome15; the for-
mer comes preinstalled with the Mac OS X oper-
ating system, while the latter can be installed even
by relatively non-technical users. The design is kept
modular using a dispatcher pattern, in order to al-
low the inclusion of the visualiser tool into other
JavaScript-based projects. The client-server archi-
tecture also allows centralisation of data, so that ev-
ery user can inspect an uploaded dataset without the
hassle of downloading and importing into a desktop
application, simply by opening an URL which can
uniquely identify a document, or even a single an-
notation. A screenshot of the stav file browser can
be seen in Figure 4.
3.2 Evaluation Tools
The tasks of BioNLP-ST 2011 exhibit very high
complexity, including multiple non-trivial subprob-
lems that are partially, but not entirely, independent
of each other. With such tasks, the evaluation of par-
ticipating systems itself becomes a major challenge.
Clearly defined evaluation criteria and their precise
implementation is critical not only for the compari-
son of submissions, but also to help participants fol-
low the status of their development and to identify
the specific strengths and weaknesses of their ap-
proach.
A further challenge arising from the complexity
of the tasks is the need to process the relatively in-
tricate format in which annotations are represented,
which in turn carries a risk of errors in submissions.
To reduce the risk of submissions being rejected or
the evaluation showing poor results due to format-
ting errors, tools for checking the validity of the file
format and annotation semantics are indispensable.
For these reasons, we placed emphasis in the or-
ganisation of the BioNLP-ST?11 on making tools for
format checking, validation and evaluation available
to the participants already during the early stages of
system development. The tools were made avail-
able in two ways: as downloads, and as online ser-
vices. With downloaded tools, participants can per-
form format checking and evaluation at any time
without online access, allowing more efficient op-
timisation processes. Each task in BioNLP-ST also
14http://www.apple.com/safari
15http://www.google.com/chrome
117
Figure 5: An example of a false negative illustrated by the evaluation tools in co-ordination with stav
maintained an online evaluation tool for the develop-
ment set during the development period. The online
evaluation is intended to provide an identical inter-
face and criteria for submitted data as the final on-
line submission system, allowing participants to be
better prepared for the final submission. With on-
line evaluation, the organisers could also monitor
submissions to ensure that there were no problems
in, for example, the evaluation software implemen-
tations.
The system logs of online evaluation systems
show that the majority of the participants submit-
ted at least one package with formatting errors, con-
firming the importance of tools for format checking.
Further, most of the participants made use of the on-
line development set evaluation at least once before
their final submission.
To enhance the evaluation tools we drew upon the
stav visualiser to provide a view of the submitted re-
sults. This was done by comparing the submitted
results and the gold data to produce a visualisation
where errors are highlighted, as illustrated in Fig-
ure 5. This experimental feature was available for
the EPI and ID tasks and we believe that by doing so
it enables participants to better understand the per-
formance of their system and work on remedies for
current shortcomings.
4 Discussion and Conclusions
Among the teams participating in the EPI and ID
tasks, a great majority utilised tools for which re-
sources were made available by the organisers. We
hope that the continued availability of the parses will
encourage further investigation into the applicability
of these and similar tools and representations.
As for the analysis of the supporting analyses pro-
vided by external groups and the participants, we are
so far aware of only limited use of these resources
among the participants, but the resources will re-
main available and we are looking forward to see
future work using them.
To enable reproducibility of our resources, we
provide a publicly accessible repository containing
the automated procedure and our processing scripts
used to produce the released data. This repository
also contains detailed instructions on the options and
versions used for each parser and, if the software li-
cense permits it, includes the source code or binary
that was used to produce the processed data. For the
cases where the license restricts redistribution, in-
structions and links are provided on how to obtain
the same version that was used. We propose that us-
ing a multitude of parses and formats can benefit not
just the task of event extraction but other NLP tasks
as well.
We have also made our evaluation tools and visu-
alisation tool stav available along with instructions
on how to run it and use it in coordination with the
shared task resources. The responses from the par-
ticipants in relation to the visualisation tool were
very positive, and we see this as encouragement to
advance the application of visualisation as a way to
better reach a wider understanding and unification
of the concept of events for biomedical event extrac-
tion.
All of the resources described in this paper are
available at http://sites.google.com/site/bionlpst/.
118
Acknowledgements
We would like to thank Jari Bjo?rne of the Uni-
versity of Turku BioNLP group; Gerold Schneider,
Fabio Rinaldi, Simon Clematide and Don Tuggener
of the Univerity of Zurich Computational Linguis-
tics group; Roser Morante of University of Antwerp
CLiPS center; and Youngjun Kim of the Univer-
sity of Utah Natural Language Processing Research
Group for their generosity with their time and exper-
tise in providing us with supporting analyses.
This work was supported by Grant-in-Aid for
Specially Promoted Research (MEXT, Japan) and
the Royal Swedish Academy of Sciences.
References
Daniel M. Bikel. 2004. Intricacies of Collins? Parsing
Model. Computational Linguistics, 30(4):479?511.
J. Bjo?rne, F. Ginter, S. Pyysalo, J. Tsujii, and
T. Salakoski. 2010. Complex event extraction at
PubMed scale. Bioinformatics, 26(12):i382.
Robert Bossy, Julien Jourde, Philippe Bessie`res, Marteen
van de Guchte, and Claire Ne?dellec. 2011. BioNLP
Shared Task 2011 - Bacteria Biotope. In Proceedings
of the BioNLP 2011 Workshop Companion Volume for
Shared Task, Portland, Oregon, June. Association for
Computational Linguistics.
S. Buchholz and E. Marsi. 2006. CoNLL-X shared
task on multilingual dependency parsing. In Proceed-
ings of the Tenth Conference on Computational Nat-
ural Language Learning, pages 149?164. Association
for Computational Linguistics.
E. Buyko and U. Hahn. 2010. Evaluating the impact
of alternative dependency graph encodings on solv-
ing event extraction tasks. In Proceedings of the
2010 Conference on Empirical Methods in Natural
Language Processing, pages 982?992. Association for
Computational Linguistics.
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-Fine n-Best Parsing and MaxEnt Discriminative
Reranking. In Proceedings of the 43rd Annual Meet-
ing of the Association for Computational Linguistics
(ACL?05), pages 173?180.
Pallavi Choudhury, Michael Gamon, Chris Quirk, and
Lucy Vanderwende. 2011. MSR-NLP entry in
BioNLP Shared Task 2011. In Proceedings of
the BioNLP 2011 Workshop Companion Volume for
Shared Task, Portland, Oregon, June. Association for
Computational Linguistics.
S. Clark and J.R. Curran. 2004. Parsing the WSJ us-
ing CCG and log-linear models. In Proceedings of
the 42nd Annual Meeting on Association for Compu-
tational Linguistics, page 103. Association for Com-
putational Linguistics.
Michael John Collins. 1996. A new statistical parser
based on bigram lexical dependencies. In Proceed-
ings of the 34th Annual Meeting of the Association
for Computational Linguistics, pages 184?191, Santa
Cruz, California, USA, June. Association for Compu-
tational Linguistics.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating Typed
Dependency Parses from Phrase Structure Parses. In
Proceedings of the Fifth International Conference
on Language Resources and Evaluation (LREC?06),
pages 449?454.
H.L. Dickensheets, C. Venkataraman, U. Schindler, and
R.P. Donnelly. 1999. Interferons inhibit activation of
STAT6 by interleukin 4 in human monocytes by in-
ducing SOCS-1 gene expression. Proceedings of the
National Academy of Sciences of the United States of
America, 96(19):10800.
Ehsan Emadzadeh, Azadeh Nikfarjam, and Graciela
Gonzalez. 2011. A generalizable and efficient ma-
chine learning approach for biological event extraction
from text. In Proceedings of the BioNLP 2011 Work-
shop Companion Volume for Shared Task, Portland,
Oregon, June. Association for Computational Linguis-
tics.
Charles J. Fillmore. 1976. Frame semantics and the na-
ture of language. Annals of the New York Academy of
Sciences, 280(1):20?32.
R. Johansson and P. Nugues. 2007. Extended
constituent-to-dependency conversion for English. In
Proc. of the 16th Nordic Conference on Computational
Linguistics (NODALIDA).
Julien Jourde, Alain-Pierre Manine, Philippe Veber,
Kare?n Fort, Robert Bossy, Erick Alphonse, and
Philippe Bessie`res. 2011. BioNLP Shared Task 2011
- Bacteria Gene Interactions and Renaming. In Pro-
ceedings of the BioNLP 2011 Workshop Companion
Volume for Shared Task, Portland, Oregon, June. As-
sociation for Computational Linguistics.
Yoshinobu Kano, William Baumgartner, Luke McCro-
hon, Sophia Ananiadou, Kevin Cohen, Larry Hunter,
and Jun?ichi Tsujii. 2009. U-Compare: share and
compare text mining tools with UIMA. Bioinformat-
ics, 25(15):1997?1998, May.
Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yoshi-
nobu Kano, and Jun?ichi Tsujii. 2009. Overview
of BioNLP?09 Shared Task on Event Extraction.
In Proceedings of Natural Language Processing in
Biomedicine (BioNLP) NAACL 2009 Workshop, pages
1?9.
119
Jin-Dong Kim, Sampo Pyysalo, Tomoko Ohta, Robert
Bossy, and Jun?ichi Tsujii. 2011a. Overview
of BioNLP Shared Task 2011. In Proceedings of
the BioNLP 2011 Workshop Companion Volume for
Shared Task, Portland, Oregon, June. Association for
Computational Linguistics.
Jin-Dong Kim, Yue Wang, Toshihisa Takagi, and Aki-
nori Yonezawa. 2011b. Overview of the Genia Event
task in BioNLP Shared Task 2011. In Proceedings
of the BioNLP 2011 Workshop Companion Volume for
Shared Task, Portland, Oregon, June. Association for
Computational Linguistics.
D. Klein and C.D. Manning. 2003. Fast exact infer-
ence with a factored model for natural language pars-
ing. Advances in neural information processing sys-
tems, pages 3?10.
M.P Marcus, B. Santorini, and M.A Marcinkiewicz.
1993. Building a large annotated corpus of English:
The Penn Tree Bank. Computational Linguistics,
pages 313?318.
D. McClosky. 2009. Any Domain Parsing: Automatic
Domain Adaptation for Natural Language Parsing.
Ph.D. thesis, Ph. D. thesis, Department of Computer
Science, Brown University.
M. Miwa, S. Pyysalo, T. Hara, and J. Tsujii. 2010. Eval-
uating Dependency Representation for Event Extrac-
tion. In In the 23rd International Conference on Com-
putational Linguistics (COLING 2010), pages 779?
787.
Y. Miyao and J. Tsujii. 2008. Feature forest models for
probabilistic HPSG parsing. Computational Linguis-
tics, 34(1):35?80.
Yusuke Miyao, Rune S?tre, Kenji Sagae, Takuya Mat-
suzaki, and Jun?ichi Tsujii. 2008. Task-oriented eval-
uation of syntactic parsers and their representations. In
Proceedings of ACL-08: HLT, pages 46?54, Colum-
bus, Ohio, June. Association for Computational Lin-
guistics.
R. Morante, V. Van Asch, and W. Daelemans. 2010.
Memory-based resolution of in-sentence scopes of
hedge cues. CoNLL-2010: Shared Task, page 40.
Ngan Nguyen, Jin-Dong Kim, and Jun?ichi Tsujii. 2011.
Overview of the Protein Coreference task in BioNLP
Shared Task 2011. In Proceedings of the BioNLP 2011
Workshop Companion Volume for Shared Task, Port-
land, Oregon, June. Association for Computational
Linguistics.
Tomoko Ohta, Sampo Pyysalo, and Jun?ichi Tsujii. 2011.
Overview of the Epigenetics and Post-translational
Modifications (EPI) task of BioNLP Shared Task
2011. In Proceedings of the BioNLP 2011 Workshop
Companion Volume for Shared Task, Portland, Oregon,
June. Association for Computational Linguistics.
S. Petrov, L. Barrett, R. Thibaux, and D. Klein. 2006.
Learning accurate, compact, and interpretable tree an-
notation. In Proceedings of the 21st International
Conference on Computational Linguistics and the 44th
annual meeting of the Association for Computational
Linguistics, pages 433?440. Association for Computa-
tional Linguistics.
H. Poon and L. Vanderwende. 2010. Joint inference
for knowledge extraction from biomedical literature.
In Human Language Technologies: The 2010 Annual
Conference of the North American Chapter of the As-
sociation for Computational Linguistics, pages 813?
821. Association for Computational Linguistics.
Sampo Pyysalo, Tomoko Ohta, Rafal Rak, Dan Sul-
livan, Chunhong Mao, Chunxia Wang, Bruno So-
bral, Jun?ichi Tsujii, and Sophia Ananiadou. 2011a.
Overview of the Infectious Diseases (ID) task of
BioNLP Shared Task 2011. In Proceedings of
the BioNLP 2011 Workshop Companion Volume for
Shared Task, Portland, Oregon, June. Association for
Computational Linguistics.
Sampo Pyysalo, Tomoko Ohta, and Jun?ichi Tsujii.
2011b. Overview of the Entity Relations (REL) sup-
porting task of BioNLP Shared Task 2011. In Pro-
ceedings of the BioNLP 2011 Workshop Companion
Volume for Shared Task, Portland, Oregon, June. As-
sociation for Computational Linguistics.
Laura Rimell and Stephen Clark. 2009. Porting a
lexicalized-grammar parser to the biomedical domain.
Journal of Biomedical Informatics, 42(5):852 ? 865.
Biomedical Natural Language Processing.
R. S?tre, K. Yoshida, A. Yakushiji, Y. Miyao, Y. Matsub-
yashi, and T. Ohta. 2007. AKANE system: protein-
protein interaction pairs in BioCreAtIvE2 challenge,
PPI-IPS subtask. In Proceedings of the Second
BioCreative Challenge Workshop, pages 209?212.
Kenji Sagae and Jun?ichi Tsujii. 2007. Dependency pars-
ing and domain adaptation with LR models and parser
ensembles. In Proceedings of the CoNLL 2007 Shared
Task.
G. Schneider, M. Hess, and P. Merlo. 2007. Hybrid
long-distance functional dependency parsing. Unpub-
lished PhD thesis, Institute of Computational Linguis-
tics, University of Zurich.
G. Szarvas, V. Vincze, R. Farkas, and J. Csirik. 2008.
The BioScope corpus: annotation for negation, uncer-
tainty and their scope in biomedical texts. In Proceed-
ings of the Workshop on Current Trends in Biomedical
Natural Language Processing, pages 38?45. Associa-
tion for Computational Linguistics.
Y. Tateisi, A. Yakushiji, T. Ohta, and J. Tsujii. 2005.
Syntax Annotation for the GENIA corpus. In Proceed-
ings of the IJCNLP, pages 222?227.
120
Proceedings of the Workshop on Open Infrastructures and Analysis Frameworks for HLT, pages 44?52,
Dublin, Ireland, August 23rd 2014.
Significance of Bridging Real-world Documents and NLP Technologies
Tadayoshi Hara Goran Topic? Yusuke Miyao Akiko Aizawa
National Institute of Informatics
2-1-2 Hitotsubashi, Chiyoda-ku, Tokyo 101-8430, Japan
{harasan, goran topic, yusuke, aizawa}@nii.ac.jp
Abstract
Most conventional natural language processing (NLP) tools assume plain text as their input,
whereas real-world documents display text more expressively, using a variety of layouts, sentence
structures, and inline objects, among others. When NLP tools are applied to such text, users
must first convert the text into the input/output formats of the tools. Moreover, this awkwardly
obtained input typically does not allow the expected maximum performance of the NLP tools to
be achieved. This work attempts to raise awareness of this issue using XML documents, where
textual composition beyond plain text is given by tags. We propose a general framework for
data conversion between XML-tagged text and plain text used as input/output for NLP tools and
show that text sequences obtained by our framework can be much more thoroughly and efficiently
processed by parsers than naively tag-removed text. These results highlight the significance of
bridging real-world documents and NLP technologies.
1 Introduction
Recent advances in natural language processing (NLP) technologies have allowed us to dream about
applying these technologies to large-scale text, and then extracting a wealth of information from the text
or enriching the text itself with various additional information. When actually considering the realization
of this dream, however, we are faced with an inevitable problem. Conventional NLP tools usually assume
an ideal situation where each input text consists of a plain word sequence, whereas real-world documents
display text more expressively using a variety of layouts, sentence structures, and inline objects, among
others. This means that obtaining valid input for a target NLP tool is left completely to the users, who
have to program pre- and postprocessors for each application to convert their target text into the required
format and integrate the output results into their original text. This additional effort reduces the viability
of technologies, while the awkwardly obtained input does not allow the expected maximum benefit of
the NLP technologies to be realized.
In this research, we raise awareness of this issue by developing a framework that simplifies this con-
version and integration process. We assume that any textual composition beyond plain text is captured by
tags in XML documents, and focus on the data conversion between XML-tagged text and the input/output
formats of NLP tools. According to our observations, the data conversion process is determined by the
textual functions of the XML-tags utilized in the target text, of which there seem to be only four types.
We therefore devise a conversion strategy for each of the four types. After all tags in the XML tagset of
the target text have been classified by the user into the four types, data conversion and integration can be
executed automatically using our strategies, regardless of the size of the text (see Figure 1).
In the experiments, we apply our framework to several types of XML documents, and the results show
that our framework can extract plain text sequences from the target XML documents by classifying only
20% or fewer of the total number of tag types. Furthermore, with the obtained sequences, two typical
parsers succeed in processing entire documents with a much greater coverage rate and using much less
parsing time than with naively tag-removed text.
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/44
Formatted Input(e.g. plain text)
Output analysis(e.g. parse trees)
<p> A user task is a scenario of use of the UI, ? the UML 
notation.  In our case, we use the CTT (Concur Task Tree) 
<cite>[ <bibref bibrefs="paterno-ctte-2001,paterno-ctt-2001" separator="," show="Number" yyseparator=","/>]</cite></p>
A user task is a scenario of use of the UI, ? the UML notation.In our case, we use the CTT (Concur Task Tree) [1]
<sentence id=?s0?><cons>?</cons></sentence>
<sentence id=?s1?>?<cons><tok>[</tok>
</cons><cons><cons><tok>1</tok></cons> ? 
<cons><tok>]</tok></cons>?</sentence>
Independent
Decoration
Object
Meta-info
Tag1
Tag2
?
(Classifyinto 4 types)
TextstructuredbyXML
TextstructuredbyXML
Textstructuredby XML
Tagset
Dataconversion NLP tools(Parser)
Figure 1: Proposed data conversion framework for applying NLP tools to text structured as XML
Tag type Criteria for classification Strategies for data conversion
Independent To represent a region syntactically Remove the tag and tagged region
independent from the surrounding text ? (apply tools to the tagged region independently)
? recover the (analyzed) tagged region after applying the tools
Decoration To set the display style of Remove only the tag
the tagged region at the same level as ? recover the tag after applying the tools
the surrounding text
Object To represent the minimal object Replace the tag (and the tagged region) with a plain word
unit that should be handled in the ? (do not process the tagged region further)
the same level as the surrounding text ? recover the tag (and region) after applying the tools
Meta-info To describe the display style Remove the tag and tagged region
setting or additional information ? (do not process the tagged region further)
? recover the tag and region after applying the tools
Table 1: Four types of tags and the data conversion strategy for each type
The contribution of this work is to demonstrate the significance of bridging real-world documents and
NLP technologies in practice. We show that, if supported by a proper framework, conventional NLP tools
already have the ability to process real-world text without significant loss of performance. We expect the
demonstration to promote further discussion on real-world document processing.
In Section 2, some related research attempts are introduced. In Section 3, the four types of textual
functions for XML tags and our data conversion strategies for each of these are described and imple-
mented. In Section 4, the efficiency of our framework and the adequacy of the obtained text sequences
for use in NLP tools are examined using several types of documents.
2 Related Work
To the best of our knowledge, no significant work on a unified methodology for data conversion between
target text and the input/output formats of NLP tools has been published. Some NLP tools provide
scripts for extracting valid input text for the tools from real-world documents; however, even these scripts
assume specific formats for the documents. For example, deep syntactic parsers such as the C&C Parser
(Clark and Curran, 2007) and Enju (Ninomiya et al., 2007) assume POS-tagged sentences as input, and
therefore the distributed packages for the parsers1 contain POS-taggers together with the parsers. The
POS-taggers assume plain text sentences as their input.
As the work most relevant to our study, UIMA (Ferruci et al., 2006) deals with various annotations
in an integrated framework. However, in this work, the authors merely proposed the framework and did
1[C&C Parser]: http://svn.ask.it.usyd.edu.au/trac/candc/wiki / [Enju]: http://kmcs.nii.ac.jp/enju/45
New UI is shown. The UI is more useful  than XYZ          , and ... .
text indexmark
Cite1
Notice that ? . notecite
<text>New UI</text> is shown. The UI is more useful than XYZ <indexmark>
? </indexmark> in <cite>[ ? ]</cite><note>Notice that ? .</note>, and ? .
sentence sentence
New UI is shown. The UI is more useful  than XYZ          , and ... .Cite1 sentence
<sentence><text>New UI</text> is shown.</sentence> <sentence>The UI is more useful  than XYZ<indexmark> 
? </indexmark> in <cite>[?]</cite><note><sentence>Notice that ? . </sentence></note>, and ? .</sentence> 
(a)
(c)
(d)
(b)
text indexmark notecite Notice that ? . 
Meta-infoDecoration
IndependentObject
Figure 2: Example of executing our strategy
not explain how the given text can be used in a target annotation process such as parsing. Some projects
based on the UIMA framework, such as RASP4UIMA (Andersen et al., 2008), U-compare (Kano et
al., 2011), and Kachako (Kano, 2012)2, have developed systems where the connections between various
documents and various tools are already established. Users, however, can utilize only the text and tool
pairs that have already been integrated into the systems. GATE (Cunningham et al., 2013) is based on
a similar concept to UIMA; it supports XML documents as its input, while the framework also requires
integration of tools into the systems.
In our framework, although availability of XML documents is assumed, a user can apply NLP tools to
the documents without modifying the tools; instead, this is achieved by merely classifying the XML-tags
in the documents into a small number of functional types.
3 Data Conversion Framework
We designed a framework for data conversion between tagged text and the input/output formats of NLP
tools based on the four types of textual functions of tags. First, we introduce the four tag types and
the data conversion strategy for each. Then, we introduce the procedure for managing the entire data
conversion process using the strategies.
3.1 Strategies for the Four Tag Types
The functions of the tags are classified into only four types, namely, Independent, Decoration, Object,
and Meta-info, and for each of these types, a strategy for data conversion is described, as given in Table
1. This section explains the types and their strategies using a simple example where we attempt to apply
a sentence splitter to the text given in Figure 2(a). The target text has four tags, ?<note>?, ?<text>?,
?<cite>?, and ?<indexmark>?, denoting, respectively, Independent, Decoration, Object, and Meta-
info tags. We now describe each of the types.
Regions enclosed by Independent tags contain syntactically independent text, such as titles, sections,
and so on. In some cases, a region of this type is inserted into the middle of another sentence, like
the ?<note>? tags in the example, which represent footnote text. The data conversion strategy for text
containing these tags is to split the enclosed region into multiple subregions and apply the NLP tools
separately to each subregion.
2[U-compare]: http://u-compare.org/ / [Kachako]: http://kachako.org/kano/46
<?xml ?><document ?><title>Formal approaches ? </title><creator> ? </creator><abstract>This research ? </abstract><section><title>Introduction</title><para><p><text>New UI</text> is shown. The UI is more useful than XYZ<indexmark> ? </indexmark> in <cite>[ ? ]</cite><note>Notice that ? </note> and ? .</p></para></section><bibliography> ? </bibliography></document>
(a) XML document
?xml document
creator section bibliographytitle abstractThis research ? Formal ?
paratitleIntroduction
p
textNew UI indexmark? cite[?] noteNotice ? . and ? .inis ? XYZ
(b) Structure of XML document
Figure 3: Example XML document
Decoration tags, on the other hand, do not necessarily guarantee the independence of the enclosed
text regions, and are utilized mainly for embossing the regions visually, such as changing the font or
color of the text (?<text>? in the example), paragraphing sections3, and so on. The data conversion
strategy for text containing these tags is to remove the tags before inputting the text into the NLP tools,
and then to recover the tags afterwards4.
Regions enclosed by Object tags contain special descriptions for representing objects treated as single
syntactic components in the surrounding text. The regions do not consist of natural language text, and
therefore cannot be analyzed by NLP tools5. The data conversion strategy for text containing this tag is
to replace the enclosed region with some proper character sequence before inputting the text into NLP
tools, and then to recover the replaced region afterwards.
Regions enclosed by Meta-info tags are not targets of NLP tools, mainly because the regions are not
displayed, but utilized for other purposes, such as creating index pages (like this ?<indexmark>?)6. The
data conversion strategy for text containing these tags is to delete the tagged region before inputting the
text into NLP tools, and then to recover the region afterwards.
According to the above strategies, which are summarized in Table 1, conversion of the example text
in Figure 2(a) is carried out as follows. In the first step, tags are removed from the text, whilst retaining
their offsets in the resulting tag-less sequence shown in (b). ?Cite1? in the sequence is a plain word
utilized to replace the ?<cite>? tag region. For the ?<note>? tag, we recursively apply our strategies
to its inner region, with two plain text regions ?New UI ...? and ?Notice that ...? consequently input into
the sentence splitter. Thereafter, sentence boundary information is returned as shown in (c), and finally,
using the retained offset information of the tags, the obtained analysis and original tag information are
integrated to produce the XML-tagged sequence shown in (d).
3.2 Procedure for Efficient Tag Classification and Data Conversion
In actual XML documents as shown in Figure 3(a), a number of tags are introduced and tagged regions
are multi-layered as illustrated in Figure 3(b) (where black and white boxes represent, respectively, XML
tags and plain text regions, and regions enclosed by tags are placed below the tags in the order they
appear.). We implemented a complete data conversion procedure for efficiently classifying tags in text
documents into the four types and simultaneously obtaining plain text sequences from such documents,
3In some types of scientific articles, one sentence can be split into two paragraph regions. It depends on the target text
whether a paragraph tag is classified as Independent or Decoration.
4The tags may imply that the enclosed regions constitute chunks of text, which may be suitable for use in NLP tools.
5In some cases, natural language text is used for parts of the descriptions, for example, itemization or tables in scientific
articles. How the inner textual parts are generally associated with the surrounding text would be discussed in our future work.
For the treatment of list structures, we can learn more from A??t-Mokhtar et al. (2003).
6If the tagged region contains analyzable text, it depends on the user policy whether NLP tools should be applied to the
region, that is, whether to classify the tag as Independent.47
@plain_text_sequences = ();   # plain text sequences input to NLP tools@recovery_info = ();                 # information for recovering original document after applying NLP tools@unknown = ();                        # unknown tags
function data_convert ($target_sequence, $seq_ID) {
if ($target_sequence contains any tags) {   # process one instance of tag usage in a target sequence$usage = (pick one instance of top-level tag usage in $target_sequence);$tag = (name of the top-level tag in $usage);@attributes = (attributes and their values for the top-level tag in $usage);$region = (region in $target_sequence enclosed by tag $tag in $usage); $tag_and_region =  (region in $target_sequence consisting of $region & tag $tag enclosing it);
if ($tag ?@independent)             { remove $tag_and_region from $target_sequence;add [?independent?, $tag, @attributes,  $seq_ID, $seq_ID + 1,(offset in $target_sequence where $tag_and_region should be inserted) ] to @recovery_info;data_convert($region,  $seq_ID + 1);  } # process the tagged region separatelyelse if ($tag ?@decoration)         { remove only tag $tag enclosing $region from $target_sequence; add [?decoration?, $tag, @attributes,(offsets in $target_sequence where $region begans and ends)] to @recovery_info; }else if ($tag ?@object)                 { replace $tag_and_region in $target_sequence with a unique plain word $uniq;add [?object?, $uniq, $tag_and_region] to @recovery_info; }else if ($tag ?@meta_info)          { remove $tag_and_region from $target_sequence;add [?meta_info?, $tag_and_region,(offset in $target_sequence where $tag_and_region should be inserted)] to @recovery_info; }else                                                     { replace $tag_and_region in $target_sequence with a unique plain word $uniq;add [?unknown?, $uniq, $tag_and_region] to @recovery_info;if ($tag ?@unknown) { add $tag to @unknown; }  }
data_convert($target_sequence, $seq_ID);  # process the remaining tags}else {  # a plain text sequence is obtainedadd [$seq_id,  $target_sequence] to @plain_text_sequences;} }
function main ($XML_document) {data_convert ($XML_document, 0);return @plain_text_sequences, @recovery_info, @unknown;}
Figure 4: Pseudo-code algorithm for data conversion from XML text to plain text for use in NLP tools
as given by the pseudo-code algorithm in Figure 4. In the remainder of this section, we explain how the
algorithm works.
Our data conversion procedure applies the strategies for the four types of tags recursively from the
top-level tags to the lower tags. The @independent, @decoration, @object and @meta info lists
contain, respectively, Independent, Decoration, Object, and Meta-info tags, which have already been
classified by the user. When applied to a target document, the algorithm uses the four lists and strategies
given in the previous section in its first attempt at converting the document into plain text sequences,
storing unknown (and therefore unprocessed) tags, if any, in @unknown. After the entire document has
been processed for the first time, the user classifies any reported unknown tags. This process is repeated
until no further unknown tags are encountered.
In the first iteration of processing the document in Figure 3(a) the algorithm is applied to the target
document with the four tag lists empty. In the function ?data convert?, top-level tags in the document,
?<?xml>? and ?<document>?, are detected as yet-to-be classified tags and added to @unknown.
The tags and their enclosed regions in the target document are replaced with unique plain text such as
?UN1? and ?UN2?, and the input text thus becomes a sequence consisting of only plain words like ?UN1
UN2?. The algorithm then adds the sequence to @plain text sequences and terminates. The user then
classifies the reported yet-to-be classified tags in @unknown into the four tag lists, and the algorithm48
Article # ar- # total tags # classified tags (# types) #obtain-
type ticles (# types) I D O M Total ed seq.
PMC 1,000 1,357,229( 421) 32,109(12) 62,414( 8) 48205( 9) 33,953(56) 176,681( 85) 25,679
ArX. 300 1,969,359(210?) 5,888(15) 46,962(12) 60,194( 8) 7,960(17) 121,004( 52) 4,167
ACL 67 130,861( 66?) 3,240(24) 14,064(29) 4,589(15) 2,304(19) 24,197( 87) 2,293
Wiki. 300 223,514( 60?) 3,530(12) 11,197( 8) 1,470(28) 11,360(67) 27,557(115) 2,286
(ArX.: arXiv.org, Wiki.: Wikipedia, I: Independent, D: Decoration, O: Object, M: Meta-info)
Table 2: Classified tags and obtained sequences for each type of article
Treat- Parsing with Enju parser Parsing with Stanford parserArticle ed tag * # sen- ** Time Avg. # failures * # sen- ** Time Avg. # failures
type classes tences (s) (**/*) (rate) tences (s) (**/*) (rate)
None 159,327 209,783 1.32 4,721 ( 2.96%) 170,999 58,865 0.39 18,621 (10.89%)
PMC O/M 112,285 135,752 1.21 810 ( 0.72%) 126,176 50,741 0.44 11,881 ( 9.42%)
All 126,215 132,250 1.05 699 ( 0.55%) 139,805 63,295 0.49 11,338 ( 8.11%)
None 74,762 108,831 1.46 2,047 ( 2.74%) 75,672 27,970 0.43 10,590 (13.99%)
ArX. O/M 41,265 89,200 2.16 411 ( 1.00%) 48,666 24,630 0.57 5,457 (11.21%)
All 43,208 87,952 2.04 348 ( 0.81%) 50,504 26,360 0.58 5,345 (10.58%)
None 19,571 15,142 0.77 115 ( 0.59%) 17,166 5,047 0.29 1,095 ( 6.38%)
ACL O/M 9,819 9,481 0.97 63 ( 0.64%) 11,182 4,157 0.37 616 ( 5.51%)
All 11,136 8,482 0.76 39 ( 0.35%) 12,402 4,871 0.39 587 ( 4.73%)
None 10,561 14,704 1.39 1,161 (10.99%) 14,883 3,114 0.24 1,651 (11.09%)
Wiki. O/M 5,026 6,743 1.34 67 ( 1.33%) 6,173 2,248 0.38 282 ( 4.57%)
All 6,893 6,058 0.88 61 ( 0.88%) 8,049 2,451 0.31 258 ( 3.21%)
(ArX.: arXiv, Wiki.: Wikipedia, O/M: Object and Meta-info)
Table 3: Impact on parsing performance of plain text sequences extracted using classified tags
starts its second iteration7.
In the case of Independent/Decoration tags, the algorithm splits the regions enclosed by the
tags/removes only the tags from the target text, and recursively processes the obtained text sequence(s)
according to our strategies. In the splitting/removal operation, the algorithm stores in @recovery info,
the locations (offsets) in the obtained text where the tags should be inserted in order to recover the tags
and textual structures after applying the NLP tools. In the case of Object/Meta-info tags, regions en-
closed by these tags are replaced with unique plain text/omitted from the target text, which means that the
inner regions are not unpacked and processed (with relevant information about the replacement/omitting
process also stored in @recovery info). This avoids unnecessary classification tasks for tags that are
utilized only in the regions, and therefore minimizes user effort.
When no further unknown tags are reported, sufficient tag classification has been done to obtain plain
text sequences for input into NLP tools, with the sequences already stored in @plain text sequences.
After applying NLP tools to the obtained sequences, @recovery info is used to integrate the anno-
tated output from the tools into the original XML document by merging the offset information8, and
consequently to recover the structure of the original document.
4 Experiments
We investigated whether the algorithm introduced in Section 3.2 is robustly applicable to different types
of XML documents and whether the obtained text sequences are adequate for input into NLP tools. The
results of this investigation highlight the significance of bridging real-world text and NLP technologies.
4.1 Target Documents
Our algorithm was applied to four types of XML documents: three types of scientific ar-
ticles, examples of which were, respectively, downloaded from PubMed Central (PMC)
(http://www.ncbi.nlm.nih.gov/pmc/tools/ftp/), arXiv.org (http://arxiv.org/) and ACL Anthology
7The user can delay the classification for some tags to later iterations.
8When crossover of tag regions occur, the region in the annotated output is divided into subregions at the crossover point.49
(http://anthology.aclweb.org/)9, and a web page type, examples of which were downloaded from
Wikipedia (http://www.wikipedia.org/). The articles obtained from PMC were originally given in an
XML format, while those from arXiv.org and ACL Anthology were given in XHTML (based on XML),
and those from Wikipedia were given in HTML, with the HTML articles generated via intermediate
XML files. These four types of articles were therefore more or less based on valid XML (or XML-like)
formats. For our experiments, we randomly selected 1,000 PMC articles, randomly selected 300
arXiv.org articles, collected 67 (31 long and 36 short) ACL 2014 conference papers without any
conversion errors (see Footnote 9), and randomly downloaded 300 Wikipedia articles.
Each of the documents contained a variety of textual parts; we decided to apply the NLP tools to the
titles of the articles and sections, abstracts, and body text of the main sections in the scientific articles,
and to the titles of the articles, body text headings, and the actual body text of the Wikipedia articles.
According to these policies, we classified the tags appearing in all articles of each type.
4.2 Efficiency of Tag Classification
Table 2 summarizes the classified tags and obtained sequences for each type of document. The second
to ninth columns give the numbers of utilized articles, tags (in tokens and types) in the documents, each
type of tag actually classified and processed, and obtained text sequences, respectively10. Using simple
regular-expression matching, we found no remaining tagged regions in the obtained sequences. From
this we concluded that our framework at least succeeded in converting XML-tagged text into plain text.
For the PMC articles, we obtained plain text sequences by classifying only a fifth or less of the total
number of tag types, that is, focusing on less than 15% of the total tag occurrences in the documents
(comparing the third and eighth columns). This is because the tags within the regions enclosed by
Object and Meta-info tags were not considered by our procedure. For each of the arXiv.org, ACL and
Wikipedia articles, a similar effect was implied by the fact that the number of classified tags was less
than 20% of the total occurrences of all tags.
4.3 Adequacy of Obtained Sequences for Use in NLP Tools
We randomly selected several articles from each article type, and confirmed that the obtained text se-
quences consisted of valid sentences, which could be directly input into NLP tools and which thoroughly
covered the content of the original articles. Then, to evaluate the impact of this adequacy in a more prac-
tical situation, we input the obtained sequences (listed in Table 2) into two typical parsers, namely, the
Enju parser for deep syntactic/semantic analysis, and the Stanford parser (de Marneffe et al., 2006)11 for
phrase structure and dependency analysis12. Table 3 compares the parsing performance on three types
of plain text sequences obtained by different strategies: simply removing all tags, processing Object
and Meta-info tags using our framework and removing the remaining tags, and processing all the tags
using our framework. For each combination of parser and article type, we give the number of detected
sentences13, the total parsing time, the average parsing time per sentence14, and the number/ratio of
sentences that could not be parsed15.
For all article types, the parsers, especially the Enju parser, succeeded in processing the entire article
with much higher coverage (see the fourth column for each parser) and in much less time (see the third
9The XHTML version of 178 ACL 2014 conference papers were available at ACL Anthology. Each of the XHTML files
was generated by automatic conversion of the original article using LaTeXML (http://dlmf.nist.gov/LaTeXML/).
10For Wikipedia, arXiv.org and ACL articles, since HTML/XHTML tag names represent more abstract textual functions,
the number of different tag types was much smaller than for PMC articles (see ? in the table). To better capture the textual
functions of the tagged regions, we used the combination of the tag name and its selected attributes as a single tag. The number
of classified tags for Wikipedia, arXiv.org and ACL given in the table reflects this decision.
11http://nlp.stanford.edu/software/lex-parser.shtml
12The annotated output from the parsers was integrated into the original XML documents by merging the offset information,
and the structures of the original documents were consequently recovered. The recovered structures were input to xmllint, a
UNIX tool for parsing XML documents, and the tool succeeded in parsing the structures without detecting any error.
13For the Enju parser, we split each text sequence into sentences using GeniaSS [http://www.nactem.ac.uk/y-matsu/geniass/].
14For the Enju parser, the time spent parsing failed sentences was also considered.
15For the Stanford parser, the maximum sentence length was limited to 50 words using the option settings because several
sentences caused parsing failures, even after increasing the memory size from 150 MB to 2 GB, which terminated the whole
process. 50
column for each parser) using the text sequences obtained by treating some (Object and Meta-info)
or all tags with our framework than with those sequences obtained by merely removing the tags. This
is mainly because the text sequences obtained by merely removing the tags contained some embedded
inserted sentences (specified by Independent tags), bare expressions consisting of non natural language
(non-NL) principles (specified by Object tags), and sequences not directly related to the displayed text
(specified by Meta-info tags), which confused the parsers. In particular, treating Object and Meta-info
tags drastically improved parsing performance, since non-NL tokens were excluded from the analysis.
Compared with treating Object/Meta-info tags, treating all tags, that is, additionally treating Inde-
pendent tags and removing the remaining tags as Decoration tags, increased the number of detected
sentences. This is because Independent tags provide solid information for separating text sequences
into shorter sequences and thus prompting the splitting of sequences into shorter sentences, which de-
creased parsing failure by preventing a lack of search space for the Enju parser and by increasing target
(? 50 word) sentences for the Stanford parser. Treating all tags increased the total time for the Stanford
parser since a decrease in failed (> 50 word) sentences directly implied an increase in processing cost,
whereas, for the Enju parser, the total time decreased since the shortened sentences drastically narrowed
the required search space.
4.4 Significance of Bridging Real-world Documents and NLP Technologies
As demonstrated above, the parsers succeeded in processing the entire article with much higher coverage
and in much less time with the text sequences obtained by our framework than with those sequences
obtained by merely removing the tags. Then, what does such thorough and efficient processing bring
about? If our target is shallow analysis of documents which can be achieved by simple approaches such
as counting words, removing tags will suffice; embedded sentences do not affect word count, and non-NL
sequences can be canceled by a large amount of valid sequences in the target documents.
Such shallow approaches, however, cannot satisfy the demands on more detailed or precise analysis
of documents: discourse analysis, translation, grammar extraction, and so on. In order to be sensitive to
subtle signs from the documents, information uttered even in small parts of text cannot be overlooked,
under the condition that sequences other than body text are excluded.
This process of plain text extraction is a well-established procedure in NLP research; in order to
concentrate on precise analysis of natural language phenomena, datasets have been arranged in the format
of plain text sequences, and, using those datasets, plenty of remarkable achievements have been reported
in various NLP tasks while brand-new tasks have been found and tackled.
But what is the ultimate goal of these challenges? Is it to just parse carefully arranged datasets? We all
know this to be just a stepping stone to the real goal: to parse real-world, richly-formatted documents. As
we demonstrated, if supported by a proper framework, conventional NLP tools already have the ability
to process real-world text without significant loss of performance. Adequately bridging target real-world
documents and NLP technologies is thus a crucial task for taking advantage of full benefit brought by
NLP technologies in ubiquitous application of NLP.
5 Conclusion
We proposed a framework for data conversion between XML-tagged text and input/output formats of
NLP tools. In our framework, once each tag utilized in the XML-tagged text has been classified as one
of the four types of textual functions, the conversion is automatically done according to the classification.
In the experiments, we applied our framework to several types of documents, and succeeded in obtaining
plain text sequences from these documents by classifying only a fifth of the total number of tag types
in the documents. We also observed that with the obtained sequences, the target documents were much
more thoroughly and efficiently processed by parsers than with naively tag-removed text. These results
emphasize the significance of bridging real-world documents and NLP technologies.
We are now ready for public release of a tool for conversion of XML documents into plain text se-
quences utilizing our framework. We would like to share further discussion on applying NLP tools to
various real-world documents for increased benefits from NLP.51
Acknowledgements
This research was partially supported by ?Data Centric Science: Research Commons? at the Research
Organization of Information and Systems (ROIS), Japan.
References
Salah A??t-Mokhtar, Veronika Lux, and ?Eva Ba?nik. 2003. Linguistic parsing of lists in structured documents. In
Proceedings of Language Technology and the Semantic Web: 3rd Workshop on NLP and XML (NLPXML-2003),
Budapest, Hungary, April.
?. Andersen, J. Nioche, E.J. Briscoe, and J. Carroll. 2008. The BNC parsed with RASP4UIMA. In Proceedings
of the 6th Language Resources and Evaluation Conference (LREC 2008), pages 865?869, Marrakech, Morocco,
May.
Stephen Clark and James R. Curran. 2007. Wide-coverage efficient statistical parsing with CCG and log-linear
models. Computational Linguistics, 33(4):493?552.
H. Cunningham, V. Tablan, A. Roberts, and K. Bontcheva. 2013. Getting more out of biomedical documents with
GATE?s full lifecycle open source text analytics. PLoS Comput Biol, 9(2).
Marie-Catherine de Marneffe, Bill MacCartney, and Christopher D. Manning. 2006. Generating typed dependency
parses from phrase structure parses. In Proceedings of the 5th Language Resources and Evaluation Conference
(LREC 2006), pages 449?454, Genoa, Italy, May.
David Ferruci, Adam Lally, Daniel Gruhl, Edward Epstein, Marshall Schor, J. William Murdock, Andy Frenkiel,
Eric W. Brown, Thomas Hampp, Yurdaer Doganata, Christopher Welty, Lisa Amini, Galina Kofman, Lev Koza-
kov, and Yosi Mass. 2006. Towards an interoperability standard for text and multi-modal analytics. Technical
Report RC24122, IBM Research Report.
Yoshinobu Kano, Makoto Miwa, Kevin Cohen, Larry Hunter, Sophia Ananiadou, and Jun?ichi Tsujii. 2011.
U-Compare: a modular NLP workflow construction and evaluation system. IBM Journal of Research and
Development, 55(3):11:1?11:10.
Yoshinobu Kano. 2012. Kachako: a hybrid-cloud unstructured information platform for full automation of service
composition, scalable deployment and evaluation. In Proceedings in the 1st International Workshop on Analyt-
ics Services on the Cloud (ASC), the 10th International Conference on Services Oriented Computing (ICSOC
2012), Shanghai, China, November.
Takashi Ninomiya, Takuya Matsuzaki, Yusuke Miyao, and Jun?ichi Tsujii. 2007. A log-linear model with an
n-gram reference distribution for accurate HPSG parsing. In Proceedings of the 10th International Conference
on Parsing Technologies (IWPT?07), Prague, Czech Republic, June.
52
