Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 977?984,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Empirical Lower Bounds on the Complexity of Translational Equivalence ?
Benjamin Wellington
Computer Science Dept.
New York University
New York, NY 10003
{lastname}@cs.nyu.edu
Sonjia Waxmonsky
Computer Science Dept.
University of Chicago?
Chicago, IL, 60637
wax@cs.uchicago.edu
I. Dan Melamed
Computer Science Dept.
New York University
New York, NY, 10003
{lastname}@cs.nyu.edu
Abstract
This paper describes a study of the pat-
terns of translational equivalence exhib-
ited by a variety of bitexts. The study
found that the complexity of these pat-
terns in every bitext was higher than sug-
gested in the literature. These findings
shed new light on why ?syntactic? con-
straints have not helped to improve statis-
tical translation models, including finite-
state phrase-based models, tree-to-string
models, and tree-to-tree models. The
paper also presents evidence that inver-
sion transduction grammars cannot gen-
erate some translational equivalence rela-
tions, even in relatively simple real bi-
texts in syntactically similar languages
with rigid word order. Instructions
for replicating our experiments are at
http://nlp.cs.nyu.edu/GenPar/ACL06
1 Introduction
Translational equivalence is a mathematical rela-
tion that holds between linguistic expressions with
the same meaning. The most common explicit rep-
resentations of this relation are word alignments
between sentences that are translations of each
other. The complexity of a given word alignment
can be measured by the difficulty of decomposing
it into its atomic units under certain constraints de-
tailed in Section 2. This paper describes a study
of the distribution of alignment complexity in a
variety of bitexts. The study considered word
alignments both in isolation and in combination
with independently generated parse trees for one
or both sentences in each pair. Thus, the study
? Thanks to David Chiang, Liang Huang, the anonymous
reviewers, and members of the NYU Proteus Project for help-
ful feedback. This research was supported by NSF grant #?s
0238406 and 0415933.
? SW made most of her contribution while at NYU.
is relevant to finite-state phrase-based models that
use no parse trees (Koehn et al, 2003), tree-to-
string models that rely on one parse tree (Yamada
and Knight, 2001), and tree-to-tree models that
rely on two parse trees (Groves et al, 2004, e.g.).
The word alignments that are the least complex
on our measure coincide with those that can be
generated by an inversion transduction grammar
(ITG). Following Wu (1997), the prevailing opin-
ion in the research community has been that more
complex patterns of word alignment in real bitexts
are mostly attributable to alignment errors. How-
ever, the experiments in Section 3 show that more
complex patterns occur surprisingly often even in
highly reliable alignments in relatively simple bi-
texts. As discussed in Section 4, these findings
shed new light on why ?syntactic? constraints have
not yet helped to improve the accuracy of statisti-
cal machine translation.
Our study used two kinds of data, each con-
trolling a different confounding variable. First,
we wanted to study alignments that contained as
few errors as possible. So unlike some other stud-
ies (Zens and Ney, 2003; Zhang et al, 2006), we
used manually annotated alignments instead of au-
tomatically generated ones. The results of our ex-
periments on these data will remain relevant re-
gardless of improvements in technology for auto-
matic word alignment.
Second, we wanted to measure how much of
the complexity is not attributable to systematic
translation divergences, both in the languages as
a whole (SVO vs. SOV), and in specific construc-
tions (English not vs. French ne. . . pas). To elim-
inate this source of complexity of translational
equivalence, we used English/English bitexts. We
are not aware of any previous studies of word
alignments in monolingual bitexts.
Even manually annotated word alignments vary
in their reliability. For example, annotators some-
times link many words in one sentence to many
977
(a) that
,
I
believe
we
all
find
unacceptable
,
regardless
of
political
party
,
je
pense
que
,
independamment
de
notre
parti
,
nous
trouvons
tous
cela
inacceptable
(b)
(Y / Y,Y) ??> (D C / D,C)
*
(S / S) ??> (X A  / X A X) (X / X,X) ??> (Y B / B Y,Y)
X A Y B A D C B A
B
D
A
CY
A
Y
B
X
A
X
S
S
believe
party
pense
unacc
that
celaparti inacc
Figure 1: (a) Part of a word alignment. (b) Derivation of this word alignment using only binary and nullary productions
requires one gap per nonterminal, indicated by commas in the production rules.
words in the other, instead of making the effort to
tease apart more fine-grained distinctions. A study
of such word alignments might say more about
the annotation process than about the translational
equivalence relation in the data. The inevitable
noise in the data motivated us to focus on lower
bounds, complementary to Fox (2002), who wrote
that her results ?should be looked on as more of an
upper bound.? (p. 307) As explained in Section 3,
we modified all unreliable alignments so that they
cannot increase the complexity measure. Thus, we
arrived at complexity measurements that were un-
derestimates, but reliably so. It is almost certain
that the true complexity of translational equiva-
lence is higher than what we report.
2 A Measure of Alignment Complexity
Any translation model can memorize a training
sentence pair as a unit. For example, given a sen-
tence pair like (he left slowly / slowly he left) with
the correct word alignment, a phrase-based trans-
lation model can add a single 3-word biphrase to
its phrase table. However, this biphrase would not
help the model predict translations of the individ-
ual words in it. That?s why phrase-based models
typically decompose such training examples into
their sub-biphrases and remember them too. De-
composing the translational equivalence relations
in the training data into smaller units of knowledge
can improve a model?s ability to generalize (Zhang
et al, 2006). In the limit, to maximize the chances
of covering arbitrary new data, a model should de-
compose the training data into the smallest pos-
sible units, and learn from them.1 For phrase-
based models, this stipulation implies phrases of
length one. If the model is a synchronous rewrit-
ing system, then it should be able to generate ev-
ery training sentence pair as the yield of a binary-
1Many popular models learn from larger units at the same
time, but the size of the smallest learnable unit is what?s im-
portant for our purposes.
branching synchronous derivation tree, where ev-
ery word-to-word link is generated by a different
derivation step. For example, a model that uses
production rules could generate the previous ex-
ample using the synchronous productions
(S, S) ? (X Y / Y X); (X, X) ? (U V / U V);
(Y, Y) ? (slowly, slowly); (U, U) ? (he, he);
and (V, V) ? (left, left).
A problem arises when this kind of decomposi-
tion is attempted for the alignment in Figure 1(a).
If each link is represented by its own nonterminal,
and production rules must be binary-branching,
then some of the nonterminals involved in gener-
ating this alignment need discontinuities, or gaps.
Figure 1(b) illustrates how to generate the sen-
tence pair and its word alignment in this manner.
The nonterminals X and Y have one discontinuity
each.
More generally, for any positive integer k, it is
possible to construct a word alignment that cannot
be generated using binary production rules whose
nonterminals all have fewer than k gaps (Satta and
Peserico, 2005). Our study measured the com-
plexity of a word alignment as the minimum num-
ber of gaps needed to generate it under the follow-
ing constraints:
1. Each step of the derivation generates no more
than two different nonterminals.
2. Each word-to-word link is generated from a
separate nonterminal.2
Our measure of alignment complexity is analo-
gous to what Melamed et al (2004) call ?fan-
out.?3 The least complex alignments on this mea-
sure ? those that can be generated with zero gaps
? are precisely those that can be generated by an
2If we imagine that each word is generated from a sep-
arate nonterminal as in GCNF (Melamed et al, 2004), then
constraint 2 becomes a special case of constraint 1.
3For grammars that generate bitexts, fan-out is equal to
the maximum number of allowed gaps plus two.
978
bitext # SPs min median max 95% C.I.
Chinese/English 491 4 24 52 .02
Romanian/English 200 2 19 76 .03
Hindi/English 90 1 10 40 .04
Spanish/English 199 4 23 49 .03
French/English 447 2 15 29 .01
Eng/Eng MTEval 5253 2 26 92 .01
Eng/Eng fiction 6263 2 15 97 .01
Table 1: Number of sentence pairs and mini-
mum/median/maximum sentence lengths in each bitext.
All failure rates reported later have a 95% confidence
interval that is no wider than the value shown for each bitext.
ITG. For the rest of the paper, we restrict our atten-
tion to binary derivations, except where explicitly
noted otherwise.
To measure the number of gaps needed to gener-
ate a given word alignment, we used a bottom-up
hierarchical alignment algorithm to infer a binary
synchronous parse tree that was consistent with
the alignment, using as few gaps as possible. A
hierarchical alignment algorithm is a type of syn-
chronous parser where, instead of constraining in-
ferences by the production rules of a grammar, the
constraints come from word alignments and possi-
bly other sources (Wu, 1997; Melamed and Wang,
2005). A bottom-up hierarchical aligner begins
with word-to-word links as constituents, where
some of the links might be to nothing (?NULL?). It
then repeatedly composes constituents with other
constituents to make larger ones, trying to find a
constituent that covers the entire input.
One of the important design choices in this kind
of study is how to treat multiple links attached to
the same word token. Word aligners, both hu-
man and automatic, are often inconsistent about
whether they intend such sets of links to be dis-
junctive or conjunctive. In accordance with its
focus on lower bounds, the present study treated
them as disjunctive, to give the hierarchical align-
ment algorithm more opportunities to use fewer
gaps. This design decision is one of the main dif-
ferences between our study and that of Fox (2002),
who treated links to the same word conjunctively.
By treating many-to-one links disjunctively, our
measure of complexity ignored a large class of dis-
continuities. Many types of discontinuous con-
stituents exist in text independently of any trans-
lation. Simard et al (2005) give examples such
as English verb-particle constructions, and the
French negation ne. . . pas. The disparate elements
of such constituents would usually be aligned to
the same word in a translation. However, when
PP NP
b)
V
S leftGeorgeFriday
George left on Friday
VP
S
NP
V PPleftGeorgeFriday
George left on Friday
on
ona)
Figure 2: a) With a parse tree constraining the top sentence,
a hierarchical alignment is possible without gaps. b) With a
parse tree constraining the bottom sentence, no such align-
ment exists.
our hierarchical aligner saw two words linked to
one word, it ignored one of the two links. Our
lower bounds would be higher if they accounted
for this kind of discontinuity.
3 Experiments
3.1 Data
We used two monolingual bitexts and five
bilingual bitexts. The Romanian/English and
Hindi/English data came from Martin et al (2005).
For Chinese/English and Spanish/English, we
used the data from Ayan et al (2005). The
French/English data were those used by Mihalcea
and Pedersen (2003). The monolingual bitext la-
beled ?MTEval? in the tables consists of multiple
independent translations from Chinese to English
(LDC, 2002). The other monolingual bitext, la-
beled ?fiction,? consists of two independent trans-
lations from French to English of Jules Verne?s
novel 20,000 Leagues Under the Sea, sentence-
aligned by Barzilay and McKeown (2001).
From the monolingual bitexts, we removed all
sentence pairs where either sentence was longer
than 100 words. Table 1 gives descriptive statis-
tics for the remaining data. The table also shows
the upper bound of the 95% confidence intervals
for the coverage rates reported later. The results
of experiments on different bitexts are not directly
comparable, due to the varying genres and sen-
tence lengths.
3.2 Constraining Parse Trees
One of the main independent variables in our ex-
periments was the number of monolingual parse
trees used to constrain the hierarchical alignments.
To induce models of translational equivalence,
some researchers have tried to use such trees to
constrain bilingual constituents: The span of ev-
ery node in the constraining parse tree must coin-
cide with the relevant monolingual span of some
979
crew astronautsincluded
S
NP VP
NP
VP
VP
S
NP
PP
theinare crewincludedastronauts
the
Figure 3: A word alignment that cannot be generated with-
out gaps in a manner consistent with both parse trees.
node in the bilingual derivation tree. These ad-
ditional constraints can thwart attempts at hierar-
chical alignment that might have succeeded oth-
erwise. Figure 2a shows a word alignment and a
parse tree that can be hierarchically aligned with-
out gaps. George and left can be composed in both
sentences into a constituent without crossing any
phrase boundaries in the tree, as can on and Fri-
day. These two constituents can then be composed
to cover the entire sentence pair. On the other
hand, if a constraining tree is applied to the other
sentence as shown in Figure 2b, then the word
alignment and tree constraint conflict. The projec-
tion of the VP is discontinuous in the top sentence,
so the links that it covers cannot be composed into
a constituent without gaps. On the other hand, if a
gap is allowed, then the VP can compose as on Fri-
day . . . left in the top sentence, where the ellipsis
represents a gap. This VP can then compose with
the NP complete a synchronous parse tree. Some
authors have applied constraining parse trees to
both sides of the bitext. The example in Figure 3
can be hierarchically aligned using either one of
the two constraining trees, but gaps are necessary
to align it with both trees.
3.3 Methods
We parsed the English side of each bilingual bitext
and both sides of each English/English bitext us-
ing an off-the-shelf syntactic parser (Bikel, 2004),
which was trained on sections 02-21 of the Penn
English Treebank (Marcus et al, 1993).
Our bilingual bitexts came with manually anno-
tated word alignments. For the monolingual bi-
texts, we used an automatic word aligner based
on a cognate heuristic and a list of 282 function
words compiled by hand. The aligner linked two
words to each other only if neither of them was on
the function word list and their longest common
subsequence ratio (Melamed, 1995) was at least
0.75. Words that were not linked to another word
in this manner were linked to NULL. For the pur-
poses of this study, a word aligned to NULL is
a non-constraint, because it can always be com-
posed without a gap with some constituent that is
adjacent to it on just one side of the bitext. The
number of automatically induced non-NULL links
was lower than what would be drawn by hand.
We modified the word alignments in all bi-
texts to minimize the chances that alignment errors
would lead to an over-estimate of alignment com-
plexity. All of the modifications involved adding
links to NULL. Due to our disjunctive treatment
of conflicting links, the addition of a link to NULL
can decrease but cannot increase the complexity of
an alignment. For example, if we added the links
(cela, NULL) and (NULL, that) to the alignment
in Figure 1, the hierarchical alignment algorithm
could use them instead of the link between cela
and that. It could thus generate the modified align-
ment without using a gap. We added NULL links
in two situations. First, if a subset of the links
in an alignment formed a many-to-many mapping
but did not form a bipartite clique (i.e. every word
on one side linked to every word on the other side),
then we added links from each of these words to
NULL. Second, if n words on one side of the bi-
text aligned to m words on the other side with
m > n then we added NULL links for each of
the words on the side with m words.
After modifying the alignments and obtaining
monolingual parse trees, we measured the align-
ment complexity of each bitext using a hierarchi-
cal alignment algorithm, as described in Section 2.
Separate measurements were taken with zero, one,
and two constraining parse trees. The synchronous
parser in the GenPar toolkit4 can be configured for
all of these cases (Burbank et al, 2005).
Unlike Fox (2002) and Galley et al (2004), we
measured failure rates per corpus rather than per
sentence pair or per node in a constraining tree.
This design was motivated by the observation that
if a translation model cannot correctly model a cer-
tain word alignment, then it is liable to make incor-
rect inferences about arbitrary parts of that align-
ment, not just the particular word links involved in
a complex pattern. The failure rates we report rep-
resent lower bounds on the fraction of training data
4http://nlp.cs.nyu.edu/GenPar
980
# of gaps allowed ? 0/0 0/1 or 1/0
Chinese/English 26 = 5% 0 = 0%
Romanian/English 1 = 0% 0 = 0%
Hindi/English 2 = 2% 0 = 0%
Spanish/English 3 = 2% 0 = 0%
French/English 3 = 1% 0 = 0%
Table 2: Failure rates for hierarchical alignment of bilingual
bitexts under word alignment constraints only.
# of gaps allowed on
non-English side ? 0 1 2
Chinese/English 298 = 61% 28 = 6% 0 = 0%
Romanian/English 82 = 41% 6 = 3% 1 = 0%
Hindi/English 33 = 37% 1 = 1% 0 = 0%
Spanish/English 75 = 38% 4 = 2% 0 = 0%
French/English 67 = 15% 2 = 0% 0 = 0%
Table 3: Failure rates for hierarchical alignment of bilin-
gual bitexts under the constraints of a word alignment and a
monolingual parse tree on the English side.
that is susceptible to misinterpretation by overcon-
strained translation models.
3.4 Summary Results
Table 2 shows the lower bound on alignment fail-
ure rates with and without gaps for five languages
paired with English. This table represents the
case where the only constraints are from word
alignments. Wu (1997) has ?been unable to find
real examples? of cases where hierarchical align-
ment would fail under these conditions, at least
in ?fixed-word-order languages that are lightly in-
flected, such as English and Chinese.? (p. 385).
In contrast, we found examples in all bitexts that
could not be hierarchically aligned without gaps,
including at least 5% of the Chinese/English sen-
tence pairs. Allowing constituents with a single
gap on one side of the bitext decreased the ob-
served failure rate to zero for all five bitexts.
Table 3 shows what happened when we used
monolingual parse trees to restrict the composi-
tions on the English side. The failure rates were
above 35% for four of the five language pairs, and
61% for Chinese/English! Again, the failure rate
fell dramatically when one gap was allowed on the
unconstrained (non-English) side of the bitext. Al-
lowing two gaps on the non-English side led to al-
most complete coverage of these word alignments.
Table 3 does not specify the number of gaps al-
lowed on the English side, because varying this pa-
rameter never changed the outcome. The only way
that a gap on that side could increase coverage is if
there was a node in the constraining parse tree that
# of gaps ? 0/0 0/1 0/2
0 CTs 171 = 3% 0 = 0% 0 = 0%
1 CTs 1792 = 34% 143 = 3% 7 = 0%
2 CTs 3227 = 61% 3227 = 61% 3227 = 61%
Table 4: Failure rates for hierarchical alignment of the
MTEval bitext, over varying numbers of gaps and constrain-
ing trees (CTs).
# of gaps ? 0/0 0/1 0/2
0 CTs 23 = 0% 0 = 0% 0 = 0%
1 CTs 655 = 10% 22 = 0% 1 = 0%
2 CTs 1559 = 25% 1559 = 25% 1559 = 25%
Table 5: Failure rates for hierarchical alignment of the fic-
tion bitext, over varying numbers of gaps and constraining
trees (CTs).
had at least four children whose translations were
in one of the complex permutations. The absence
of such cases in the data implies that the failure
rates under the constraints of one parse tree would
be identical even if we allowed production rules of
rank higher than two.
Table 4 shows the alignment failure rates for the
MTEval bitext. With word alignment constraints
only, 3% of the sentence pairs could not be hierar-
chically aligned without gaps. Allowing a single
gap on one side decreased this failure rate to zero.
With a parse tree constraining constituents on one
side of the bitext and with no gaps, alignment fail-
ure rates rose from 3% to 34%, but allowing a
single gap on the side of the bitext that was not
constrained by a parse tree brought the failure rate
back down to 3%. With two constraining trees the
failure rate was 61%, and allowing gaps did not
lower it, for the same reasons that allowing gaps
on the tree-constrained side made no difference in
Table 3.
The trends in the fiction bitext (Table 5) were
similar to those in the MTEval bitext, but the cov-
erage was always higher, for two reasons. First,
the median sentence size was lower in the fiction
bitext. Second, the MTEval translators were in-
structed to translate as literally as possible, but the
fiction translators paraphrased to make the fiction
more interesting. This freedom in word choice re-
duced the frequency of cognates and thus imposed
fewer constraints on the hierarchical alignment,
which resulted in looser estimates of the lower
bounds. We would expect the opposite effect with
hand-aligned data (Galley et al, 2004).
To study how sentence length correlates with
the complexity of translational equivalence, we
took subsets of each bitext while varying the max-
981
 0
 0.01
 0.02
 0.03
 0.04
 0.05
 0.06
 0.07
 0.08
 10  20  30  40  50  60  70  80  90  100
fa
ilu
re
 ra
te
maximum length of shortest sentence
0 constraining trees
Chinese/Eng
MTeval
fiction
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 10  20  30  40  50  60  70  80  90  100
fa
ilu
re
 ra
te
maximum length of shorter sentence
1 constraining tree
Chinese/Eng
Romanian/Eng
Hindi/Eng
Spanish/Eng
MTeval
French/Eng
fiction
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 10  20  30  40  50  60  70  80  90  100
fa
ilu
re
 ra
te
maximum length of shorter sentence
2 constraining trees
MTeval
fiction
Figure 4: Failure rates for hierarchical alignment without gaps vs. maximum length of shorter sentence.
category ? 1 2 3
valid reordering 12 10 5
parser error n/a 16 25
same word used differently 15 4 0
erroneous cognates 3 0 0
total sample size 30 30 30
initial failure rate (%) 3.25 31.9 38.4
% false negatives 60?7 66?7 84?3
adjusted failure rate (%) 1.3?.22 11?2.2 6?1.1
Table 6: Detailed analysis of hierarchical alignment failures
in MTEval bitext.
imum length of the shorter sentence in each pair.5
Figure 4 plots the resulting alignment failure rates
with and without constraining parse trees. The
lines in these graphs are not comparable to each
other because of the variety of genres involved.
3.5 Detailed Failure Analysis
We examined by hand 30 random sentence pairs
from the MTEval bitext in each of three different
categories: (1) the set of sentence pairs that could
not be hierarchically aligned without gaps, even
without constraining parse trees; (2) the set of sen-
tence pairs that could not be hierarchically aligned
without gaps with one constraining parse tree, but
that did not fall into category 1; and (3) the set
of sentence pairs that could not be hierarchically
aligned without gaps with two constraining parse
trees, but that did not fall into category 1 or 2. Ta-
ble 6 shows the results of this analysis.
In category 1, 60% of the word alignments that
could not be hierarchically aligned without gaps
were caused by word alignment errors. E.g.:
1a GlaxoSmithKline?s second-best selling drug may have
to face competition.
1b Drug maker GlaxoSmithKline may have to face com-
petition on its second best selling product.
The word drug appears in both sentences, but for
different purposes, so drug and drug should not
5The length of the shorter sentence is the upper bound on
the number of non-NULL word alignments.
have been linked.6 Three errors were caused by
words like targeted and started, which our word
alignment algorithm deemed cognates. 12 of the
hierarchical alignment failures in this category
were true failures. For example:
2a Cheney denied yesterday that the mission of his trip
was to organize an assault on Iraq, while in Manama.
2b Yesterday in Manama, Cheney denied that the mis-
sion of his trip was to organize an assault on Iraq.
The alignment pattern of the words in bold is
the familiar (3,1,4,2) permutation, as in Figure 1.
Most of the 12 true failures were due to movement
of prepositional phrases. The freedom of move-
ment for such modifiers would be greater in bitexts
that involve languages with less rigid word order
than English.
Of the 30 sentence pairs in category 2, 16 could
not be hierarchically aligned due to parser errors
and 4 due to faulty word alignments. 10 were due
to valid word reordering. In the following exam-
ple, a co-referring pronoun causes the word align-
ment to fail with a constraining tree on the second
sentence:
3a But Chretien appears to have changed his stance after
meeting with Bush in Washington last Thursday.
3b But after Chretien talked to Bush last Thursday in
Washington, he seemed to change his original stance.
25 of the 30 sentence pairs in category 3 failed
to align due to parser error. 5 examples failed be-
cause of valid word reordering. 1 of the 5 reorder-
ings was due to a difference between active voice
and passive voice, as in Figure 3.
The last row of Table 6 takes the various rea-
sons for alignment failure into account. It esti-
mates what the failure rates would be if the mono-
lingual parses and word alignments were perfect,
with 95% confidence intervals. These revised rates
emphasize the importance of reliable word align-
ments for this kind of study.
6This sort of error is likely to happen with other word
alignment algorithms too, because words and their common
translations are likely to be linked even if they?re not transla-
tionally equivalent in the given sentence.
982
4 Discussion
Figure 1 came from a real bilingual bitext,
and Example 2 in Section 3.5 came from a
real monolingual bitext.7 Neither of these ex-
amples can be hierarchically aligned correctly
without gaps, even without constraining parse
trees. The received wisdom in the literature
led us to expect no such examples in bilin-
gual bitexts, let alne in monolingual bitexts.
See http://nlp.cs.nyu.edu/GenPar/ACL06 for
more examples. The English/English lower
bounds are very loose, because the automatic word
aligner would not link words that were not cog-
nates. Alignment failure rates on a hand aligned
bitext would be higher. We conclude that the ITG
formalism cannot account for the ?natural? com-
plexity of translational equivalence, even when
translation divergences are factored out.
Perhaps our most surprising results were those
involving one constraining parse tree. These re-
sults explain why constraints from independently
generated monolingual parse trees have not im-
proved statistical translation models. For exam-
ple, Koehn et al (2003) reported that ?requiring
constituents to be syntactically motivated does not
lead to better constituent pairs, but only fewer con-
stituent pairs, with loss of a good amount of valu-
able knowledge.? This statement is consistent with
our findings. However, most of the knowledge
loss could be prevented by allowing a gap. With
a parse tree constraining constituents on the En-
glish side, the coverage failure rate was 61% for
the Chinese/English bitext (top row of Table 3),
but allowing a gap decreased it to 6%. Zhang and
Gildea (2004) found that their alignment method,
which did not use external syntactic constraints,
outperformed the model of Yamada and Knight
(2001). However, Yamada and Knight?s model
could explain only the data that would pass the no-
gap test in our experiments with one constraining
tree (first column of Table 3). Zhang and Gildea?s
conclusions might have been different if Yamada
and Knight?s model were allowed to use discon-
tinuous constituents. The second row of Ta-
ble 4 suggests that when constraining parse trees
are used without gaps, at least 34% of training sen-
tence pairs are likely to introduce noise into the
model, even if systematic syntactic differences be-
tween languages are factored out. We should not
7The examples were shortened for the sake of space and
clarity.
 0
 10
 20
 30
 40
 50
 60
 70
 80
 90
 100
 0  10  20  30  40  50  60  70
cu
m
ul
at
ive
 %
ag
e 
of
 s
en
te
nc
es
span length
Figure 5: Lengths of spans covering words in (3,1,4,2) per-
mutations.
be surprised when such constraints do more harm
than good.
To increase the chances that a translation model
can explain complex word alignments, some au-
thors have proposed various ways of extending
a model?s domain of locality. For example,
Callison-Burch et al (2005) have advocated for
longer phrases in finite-state phrase-based transla-
tion models. We computed the phrase length that
would be necessary to cover the words involved
in each (3,1,4,2) permutation in the MTEval bi-
text. Figure 5 shows the cumulative percentage of
these cases that would be covered by phrases up to
a certain length. Only 9 of the 171 cases (5.2%)
could be covered by phrases of length 10 or less.
Analogous techniques for tree-structured transla-
tion models involve either allowing each nonter-
minal to generate both terminals and other non-
terminals (Groves et al, 2004; Chiang, 2005), or,
given a constraining parse tree, to ?flatten? it (Fox,
2002; Zens and Ney, 2003; Galley et al, 2004).
Both of these approaches can increase coverage of
the training data, but, as explained in Section 2,
they risk losing generalization ability.
Our study suggests that there might be some
benefits to an alternative approach using discontin-
uous constituents, as proposed, e.g., by Melamed
et al (2004) and Simard et al (2005). The large
differences in failure rates between the first and
second columns of Table 3 are largely indepen-
dent of the tightness of our lower bounds. Syn-
chronous parsing with discontinuities is computa-
tionally expensive in the worst case, but recently
invented data structures make it feasible for typi-
cal inputs, as long as the number of gaps allowed
per constituent is fixed at a small maximum (Wax-
monsky and Melamed, 2006). More research is
needed to investigate the trade-off between these
costs and benefits.
983
5 Conclusions
This paper presented evidence of phenomena that
can lead to complex patterns of translational
equivalence in bitexts of any language pair. There
were surprisingly many examples of such patterns
that could not be analyzed using binary-branching
structures without discontinuities. Regardless of
the languages involved, the translational equiva-
lence relations in most real bitexts of non-trivial
size cannot be generated by an inversion trans-
duction grammar. The low coverage rates without
gaps under the constraints of independently gen-
erated monolingual parse trees might be the main
reason why ?syntactic? constraints have not yet in-
creased the accuracy of SMT systems. Allowing a
single gap in bilingual phrases or other types of
constituent can improve coverage dramatically.
References
Necip Ayan, Bonnie J. Dorr, and Christof Monz. 2005.
Alignment link projection using transformation-
based learning. In EMNLP.
Regina Barzilay and Kathleen McKeown. 2001. Ex-
tracting paraphrases from a parallel corpus. In ACL.
Andrea Burbank, Marine Carpuat, Stephen Clark,
Markus Dreyer and Pamela Fox, Declan Groves,
Keith Hall, Mary Hearne, I. Dan Melamed,
Yihai Shen, Andy Way, Ben Wellington, and
Dekai Wu. 2005. Final Report on Statistical
Machine Translation by Parsing. JHU CLSP.
http://www.clsp.jhu.edu/ws2005
/groups/statistical/report.html
Dan Bikel. 2004. A distributional analysis of a lexical-
ized statistical parsing model. In EMNLP.
Chris Callison-Burch, Colin Bannard, and Josh
Scroeder. 2005. Scaling phrase-based statistical
machine translation to larger corpora and longer
phrases. In ACL.
David Chiang. 2005. A hierarchical phrase-based
model for statistical machine translation. In ACL.
Bonnie Dorr. 1994. Machine translation divergences:
A formal description and proposed solution. Com-
putational Linguistics 20(4):597?633.
Heidi Fox. 2002. Phrasal cohesion and statistical ma-
chine translation. In EMNLP.
Michel Galley, Mark Hopkins, Kevin Knight, and
Daniel Marcu. 2004. What?s in a translation rule?
In HLT-NAACL.
Declan Groves, Mary Hearne, and Andy Way. 2004.
Robust sub-sentential alignment of phrase-structure
trees. In COLING.
Philipp Koehn, Franz Och, and Daniel Marcu. 2003.
Statistical phrase-based translation. In NAACL.
Mitchell Marcus, Beatrice Santorini, and Mary-Ann
Marcinkiewicz. 1993. Building a large annotated
corpus of English: The Penn Treebank. Computa-
tional Linguistics, 19(2):313?330.
Joel Martin, Rada Mihalcea, and Ted Pedersen. 2005.
Word alignments for languages with scarce re-
sources. In ACL Workshop on Building and Using
Parallel Texts.
I. Dan Melamed. 1995. Automatic evaluation and uni-
form filter cascades for inducing N -best translation
lexicons. In ACL Workshop on Very Large Corpora.
I. Dan Melamed, Giorgio Satta, and Benjamin Welling-
ton. 2004. Generalized multitext grammars. In
ACL.
I. Dan Melamed and Wei Wang. 2005. Gen-
eralized Parsers for Machine Translation.
NYU Proteus Project Technical Report 05-001
http://nlp.cs.nyu.edu/pubs/.
Rada Mihalcea and Ted Pedersen. 2003. An evalua-
tion exercise for word alignment. In HLT-NAACL
Workshop on Building and Using Parallel Texts.
LDC. 2002. NIST MT evaluation data, Linguistic
Data Consortium catalogue # LDC2002E53.
http://projects.ldc.upenn.edu
/TIDES/mt2003.html.
Giorgio Satta and Enoch Peserico. 2005. Some
computational complexity results for synchronous
context-free grammars. In EMNLP.
Michel Simard, Nicola Cancedda, Bruno Cavestro,
Marc Dymetman, Eric Guassier, Cyril Goutte, and
Kenji Yamada. 2005. Translating with non-
contiguous phrases. In EMNLP.
Sonjia Waxmonsky and I. Dan Melamed. 2006. A dy-
namic data structure for parsing with discontinuous
constituents. NYU Proteus Project Technical Report
06-001 http://nlp.cs.nyu.edu/pubs/.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377?404.
Kenji Yamada and Kevin Knight. 2001. A syntax-
based statistical translation model. In ACL.
Richard Zens and Hermann Ney. 2003. A comparative
study on reordering constraints in statistical machine
translation. In ACL.
Hao Zhang and Daniel Gildea. 2004. Syntax-based
alignment: Supervised or unsupervised? In COL-
ING.
Hao Zhang, Liang Huang, Daniel Gildea, and Kevin
Knight. 2006. Synchronous binarization for ma-
chine translation. In HLT-NAACL.
984
Proceedings of the 2009 Named Entities Workshop, ACL-IJCNLP 2009, pages 92?95,
Suntec, Singapore, 7 August 2009. c?2009 ACL and AFNLP
Substring-based Transliteration with Conditional Random Fields
Sravana Reddy and Sonjia Waxmonsky
Department of Computer Science
The University of Chicago
Chicago, IL 60637
{sravana, wax}@cs.uchicago.edu
Abstract
Motivated by phrase-based translation research,
we present a transliteration system where char-
acters are grouped into substrings to be mapped
atomically into the target language. We show how
this substring representation can be incorporated
into a Conditional Random Field model that uses
local context and phonemic information.
1 Introduction
We present a transliteration system that is moti-
vated by research in phrase-based machine trans-
lation. In particular, we borrow the concept of
phrases, which are groups of words that are trans-
lated as a unit. These phrases correspond to multi-
character substrings in our transliteration task.
That is, source and target language strings are
treated not as sequences of characters but as se-
quences of non-overlapping substrings.
We model transliteration as a sequential label-
ing task where substring tokens in the source lan-
guage are labeled with tokens in the target lan-
guage. This is done using Conditional Random
Fields (CRFs), which are undirected graphical
models that maximize the posterior probabilities
of the label sequence given the input sequence. We
use as features both local contexts and phonemic
information acquired from an English pronuncia-
tion dictionary.
2 The Transliteration Process
Our transliteration system has the following steps:
1. Pre-processing of the target language.
2. Substring alphabet generation for both the
source and target. This step also generates
training data for the CRFs in Step 3 and 4.
3. CRF training on aligned data from Step 2.
4. Substring segmentation and translitera-
tion of source language input.
Our training and test data consists of three sets ?
English to Hindi, English to Kannada, and English
to Tamil (Kumaran and Kellner, 2007) ? from the
NEWS 2009 Machine Transliteration Shared Task
(Li et al, 2009).
2.1 Step 1: Pre-Processing
The written words of Hindi, Tamil, and Kannada
correspond almost perfectly to their phonological
forms, with each character mapping to a phoneme.
The only exception to this arises from the implicit
vowel (which may be a schwa /@/ or a central
vowel /5/) that is inserted after consonants that
are not followed by the halanta or ?killer stroke?.
Hence, any mappings of an English vowel to a
target language schwa will not be reflected in the
alignment of the named entity pair.
To minimize misalignments of target language
strings with the English strings during training,
we convert the Indic abugida strings to an in-
ternal phonemic representation. The conversion
maps each unicode character to its correspond-
ing phonemic character and inserts a single sym-
bol (representing the schwa/central vowel) after all
consonants that are not followed by the halanta.
These phoneme sequences are used as the in-
ternal representation of Indic character strings for
all later steps in our system. Once transliteration
is complete, the phonemic symbols are converted
back to unicode by reversing the above process.
2.2 Step 2: Substring alphabet generation
Our decision to use substrings in the transliteration
task is motivated by the differences in orthography
and phonology between the target and source lan-
guages, which prevent trivial one-to-one character
level alignment. We first discuss the cause of the
poor character alignment between English and the
92
Indic languages, and then describe how we trans-
form the input into substring representation.
English uses several digraphs for phonemes
that are represented by single characters in Indic
scripts, which are either part of standard ortho-
graphic convention (oo, ch, etc.), or necessitated
by the lack of a single phoneme that approximates
an Indic one (as in the case of aspirated conso-
nants). Conversely, English sometimes uses a sin-
gle character for a biphone (such as x for /ks/, or
u for /ju/ as in museum), which is represented by
two characters in the target languages. In certain
cases, a digraph in English is transliterated to a di-
graph in the target, as a result of metathesis (le ?
/@l/, in words like temple). Further, all three tar-
get languages often insert vowels between English
consonant clusters; for instance, Hindi inserts a
schwa between s and p in ?transport?, transliter-
ated as ?rAns@por? (V~ A\spoV).
To handle these cases, we borrow the concept of
phrases from machine translation (Och and Ney,
2004), where groups of words are translated as a
unit. In the case of transliteration, the ?phrases?
are commonly occurring substrings ? sequences
of characters ? in one language that map to a
character or a substring in the other. We use the
term ?substrings? after a previous work (Sherif and
Kondrak, 2007) that employs it in a noisy channel
transliteration system. Zhao et al (2007) also use
substrings (which they call ?blocks?) in a bi-stream
HMM.
We bootstrap the induction of substrings by
aligning all named entity pairs in the training data,
using the GIZA++ toolkit (Och and Ney, 2003).
The toolkit performs unidirectional one-to-many
alignments, meaning that a single symbol in its
source string can be aligned to at most one sym-
bol in its target. In order to induce many-to-many
alignments, GIZA++ is run on the data in both di-
rections (source language to target language and
target language to source), and the bidirectional
alignment of a named entity pair is taken to be the
union of the alignments in each direction. Any
inserted characters (maps within the alignment
where the source or target character is null) are
combined with the preceding character within the
string. For example, the initial bidirectional align-
ment of shivlal ? Siv@lAl (E?vlAl) contains
the maps [sh ? S, i ? i, v ? v, null ? @, l ? l,
a ? A, and l ? l]. The null ? @ map is combined
with the preceding map to give v ? v@, and hence
a one-to-one alignment.
Multicharacter units formed by bidirectional
alignments are added to source and target alha-
bets. The above example would add the substrings
?sh? to the source alphabet, and v@ to the target.
Very low frequency substrings in both languages
are removed, giving the final substring alphabets
of single and multicharacter tokens. These alpha-
bets (summarized in Table 1) are used as the token
set for the CRF in Step 3.
We now transform our training data into a
substring-based representation. The original
named entity pairs are replaced by their bidirec-
tional one-to-one alignments described earlier. For
example, the ?s h i v l a l? ? ?S i v @ l
A l? training pair is replaced by ?sh i v l a l? ?
?S i v@ l A l?. A few (less than 3%) of the
pairs are not aligned one-to-one, since their bidi-
rectional alignments contain low-frequency sub-
strings that have not been included in the alpha-
bet.1 These pairs are removed from the training
data, since only one-to-one alignments can be han-
dled by the CRF.
2.3 Step 3: CRF transliteration
With the transformed training data in hand, we can
now train a CRF sequential model that uses sub-
strings rather than characters as the basic token
unit. The CRF algorithm is chosen for its ability
to handle non-independent features of the source
language input sequence. We use the open-source
CRF++ software package (Kudo, 2005).
Ganesh et al (2008) also apply a CRF to the
transliteration task (Hindi to English) but with
different alignment methods than those presented
here. In particular, multicharacter substrings are
only used as tokens on the target (English) side,
and a null token is used to account for deletion.
We train our CRF using unigram, bigram, and
trigram features over the source substrings, as well
as pronunciation information described in ?2.3.1.
Table 2 describes these feature sets.
2.3.1 Phonetic information
Since the CRF model allows us to incorporate non-
independent features, we add pronunciation data
as a token-level feature. Doing so allows the CRF
to use phonetic information for local decision-
making. Word pronunciations were obtained from
1Note that if we did not filter out any of the substrings,
every pair would be aligned one-to-one.
93
Target Language Source Target
# of Tokens Longest Token # of Tokens Longest Token
Hindi 196 augh, ough 141 Aj@ (aAy), ks@ (?s)
Kannada 197 aine 137 Aj@, mjA
Tamil 179 cque 117 mij, Aj@
Table 1: Overview of the substring alphabets generated in Step 2.
Feature Set Description
U Unigram: s?1, s0, and s1
B Bigram: s?1+s0
T Trigram: s?2+s?1+s0,
s?1+s0+s1 and s0+s1+s2
P Phoneme assigned to s0
from dictionary lookup
Table 2: Feature sets used for CRF in Step 3. si is
the substring relative to the current substring s0.
the CMU Pronouncing Dictionary2. Just over a
third of the English named entities have pronun-
ciation information available for some or all the
constituent words.
The CMU dictionary provides a sequence of
phoneme symbols for an English word. We in-
clude these phonemes as CRF features if and
only if a one-to-one correspondence exists be-
tween phonemes and substring tokens. For exam-
ple, the English word simon has the segmentation
?s i m o n? and pronunciation ?S AY M AH N?,
both of length five. Additionally, a check is done
to ensure that vowel phonemes do not align with
consonant characters and vice-versa.
2.4 Step 4: Substring segmentation
In order to apply our trained model to unseen
data, we must segment named entities into non-
overlapping substrings that correspond to tokens
in the source alphabet generated in Step 2. For in-
stance, we need to convert the four character desh
to the three token sequence ?d e sh?.
This is a non-trivial task. We must allow for
the fact that substrings are not inserted every time
the component character sequence appears. For
instance, in our English/Hindi training set, the bi-
gram ti always reduces to a single substring token
when it occurs in the -tion suffix, but does not re-
duce in any other contexts (like martini). There
are also cases where more than one non-trivial seg-
mentation is possible. For example, two possible
2The CMU Pronouncing Dictionary (v0.7a). Available at
http://www.speech.cs.cmu.edu/cgi-bin/cmudict
segmentations of desh are ?d es h? and ?d e sh?,
with the latter being the one that best corresponds
to the three-character Hindi d?eS (d?).
One solution is to greedily choose the most
likely multi-character substring ? in the example
cited, we can choose ?d e sh? because sh reduces
more frequently than es. However, this creates the
problem in cases where no reduction should occur,
as with the ti in martini. Since contextual informa-
tion is necessary to determine the correct substring
segmentation, we model segmentation with a CRF,
using a combination of character unigram, bigram,
and trigram features.
We use an approach motivated by the In-
side/Outside representation of NP-chunking
which treats segmentation as a tagging process
over words (Ramshaw and Marcus, 1995). As
in NP-chunking, our goal is to identify non-
overlapping, non-recursive segments in our input
sequence. Our tagset is {I, O, B} where I
indicates that a character is inside a substring, O
indicates a character is outside a substring, and B
marks a right boundary.
After the test data has been segmented into its
substring representation, it can be passed as input
to the CRF model trained in Step 3 to produce our
final transliteration output.
3 Results
We first report our results on the development data
provided by the NEWS task, for different feature
sets and segmentation methods. We then present
the performance of our system on the test data.3
3.1 Development Data
Table 3 shows the results across feature sets.
Noting that the trigram feature T provides a
sizable improvement, we compare results from
U+B+T+P and U+B+P feature sets. Of the im-
proved cases, 75-84% are a single vowel-to-vowel
3For the development runs, we use the training set for
training, and the development for testing. For the final test
runs, we use both the training and development sets for train-
ing, and the test set for evaluation.
94
Language Feature Set ACC F-Score
U+P 24.6 86.2
Hindi U+B+P 26.2 86.5
U+B+T+P 34.5 88.6
U+B+T 34.2 88.3
U+P 26.7 87.8
Tamil U+B+P 27.6 88.0
U+B+T+P 34.9 89.8
U+B+T 33.1 89.7
U+P 22.5 86.0
Kannada U+B+P 22.6 86.2
U+B+T+P 28.7 88.0
U+B+T 27.5 87.9
Table 3: Accuracy (ACC) and F-score results (in
%) for CRF model on the development data.
Language Feature Set ACC F-Score
Hindi U+B+T+P 34.4 90.2
U+B+T 33.6 89.5
Tamil U+B+T+P 29.1 91.1
U+B+T 25.5 90.6
Kannada U+B+T+P 27.2 89.8
U+B+T 23.4 89.2
Table 4: Results on development data, restricted to
NEs where P is included as a feature.
change, with the majority of the changes involving
a schwa/central vowel.
We see small gains from using the phonetic fea-
ture in both accuracy and F-Score. We further ex-
amine only those named entities where dictionary
information is applied, and as expected, this subset
shows greater improvement (Table 4).
Table 5 compares our the Inside/Outside tag-
ging approach with a greedy approach described
earlier. The greedy approach only inserts a multi-
character substring when that substring reduces
more than 50% of the time in the overall train-
ing corpus. Since the Greedy method uses no
local contextual information, results are signifi-
cantly lower given the same feature set.
Language Segmentation ACC F-Score
Hindi I-O-B 34.5 88.6
Greedy 30.3 86.7
Tamil I-O-B 34.9 89.8
Greedy 28.2 87.5
Kannada I-O-B 28.7 88.0
Greedy 25.0 86.7
Table 5: Comparison of segmentation methods
on development data, using the U+B+T+P feature
set.
3.2 Test Data
Our model produces 10 candidates for each named
entity in the test data, ranked by the probability
that the model assigns the candidate. We filter out
candidates below the rank of 5 whose scores are
less than 0.5 lower than that of the highest rank-
ing candidate. Table 6 shows our results on the
test data, using a CRF trained on the training and
development data, with the feature set U+B+T+P.
Hindi Kannada Tamil
Accuracy 41.8 36.3 43.5
F-Score 87.9 87.0 90.2
MRR 54.6 48.2 57.2
MAPref 41.2 35.5 43.0
MAP10 18.3 16.4 19.5
MAPsys 24.0 21.8 26.5
Table 6: Final results on the test data (in %).
References
Surya Ganesh, Sree Harsh, Prasad Pingali, and Va-
sudeva Varma. 2008. Statistical transliteration for
cross language information retrieval using HMM
alignment model and CRF. In Proceedings of the
2nd Workshop on Cross Lingual Information Access.
Taku Kudo. 2005. CRF++: Yet another CRF toolkit.
Available at http://chasen.org/ taku/software/crf++/.
A. Kumaran and Tobias Kellner. 2007. A generic
framework for machine transliteration. In Proceed-
ings of SIGIR-07.
Haizhou Li, A Kumaran, Min Zhang, and Vladimir
Pervouchine. 2009. Whitepaper of NEWS 2009
machine transliteration shared task. In Proceed-
ings of ACL-IJCNLP 2009 Named Entities Work-
shop (NEWS 2009).
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19?51.
Franz Josef Och and Hermann Ney. 2004. The align-
ment template approach to statistical machine trans-
lation. Computational Linguistics, 30(4):417?449.
Lance Ramshaw and Mitch Marcus. 1995. Text
chunking using transformation-based learning. In
Proceedings of WVLC-3.
Tarek Sherif and Grzegorz Kondrak. 2007. Substring-
based transliteration. In Proceedings of ACL-07.
Bing Zhao, Nguyen Bach, Ian Lane, and Stephan Vo-
gel. 2007. A log-linear block transliteration model
based on bi-stream HMMs. In Proceedings of
NAACL HLT 2007.
95
2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 367?371,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
G2P Conversion of Proper Names Using Word Origin Information
Sonjia Waxmonsky and Sravana Reddy
Department of Computer Science
The University of Chicago
Chicago, IL 60637
{wax, sravana}@cs.uchicago.edu
Abstract
Motivated by the fact that the pronuncia-
tion of a name may be influenced by its
language of origin, we present methods to
improve pronunciation prediction of proper
names using word origin information. We
train grapheme-to-phoneme (G2P) models on
language-specific data sets and interpolate the
outputs. We perform experiments on US sur-
names, a data set where word origin variation
occurs naturally. Our methods can be used
with any G2P algorithm that outputs poste-
rior probabilities of phoneme sequences for a
given word.
1 Introduction
Speakers can often associate proper names with their
language of origin, even when the words have not
been seen before. For example, many English speak-
ers will recognize that Makowski and Masiello are
Polish and Italian respectively, without prior knowl-
edge of either name. Such recognition is important
for language processing tasks since the pronuncia-
tions of out-of-vocabulary (OOV) words may de-
pend on the language of origin. For example, as
noted by Llitjo?s (2001), ?sch? is likely to be pro-
nounced as /sh/ for German-origin names (Schoe-
nenberg) and /sk/ for Italian-origin words (Schi-
avone).
In this work, we apply word origin recognition
to grapheme-to-phoneme (G2P) conversion, the task
of predicting the phonemic representation of a word
given its written form. We specifically study G2P
conversion for personal surnames, a domain where
OOVs are common and expected.
Our goal is to show how word origin information
can be used to train language-specific G2P models,
and how output from these models can be combined
to improve prediction of the best pronunciation of a
name. We deal with data sparsity in rare language
classes by re-weighting the output of the language-
specific and language-independent models.
2 Previous Work
Llitjo?s (2001) applies word origin information to
pronunciation modeling for speech synthesis. Here,
a CART decision tree system is presented for G2P
conversion that maps letters to phonemes using local
context. Experiments use a data set of US surnames
that naturally draws from a diverse set of origin lan-
guages, and show that the inclusion of word origin
features in the model improves pronunciation accu-
racy. We use similar data, as described in ?4.1.
Some works on lexical modeling for speech
recognition also make use of word origin. Here,
the focus is on expanding the vocabulary of an ASR
system rather than choosing a single best pronunci-
ation. Maison et al (2003) train language-specific
G2P models for eight languages and output pronun-
ciations to augment a baseline lexicon. This aug-
mented lexicon outperforms a handcrafted lexicon
in ASR experiments; error reduction is highest for
foreign names spoken by native speakers of the ori-
gin language. Cremelie and ten Bosch (2001) carry
out a similar lexicon augmentation, and make use of
penalty weighting, with different penalties for pro-
nunciations generated by the language-specific and
language-independent G2P models.
The problem of machine transliteration is closely
related to grapheme-to-phoneme conversion. Many
367
transliteration systems (Khapra and Bhattacharyya,
2009; Bose and Sarkar, 2009; Bhargava and Kon-
drak, 2010) use word origin information. The
method described by Hagiwara and Sekine (2011)
is similar to our work, except that (a) we use a data
set where multiple languages of origin occur nat-
urally, rather than creating language-specific lists
and merging them into a single set, and (b) we
consider methods of smoothing against a language-
independent model to overcome the problems of
data sparsity and errors in word origin recognition.
3 Language-Aware G2P
Our methods are designed to be used with any
statistical G2P system that produces the posterior
probability Pr(??|g?) of a phoneme sequence ?? for
a word (grapheme sequence) g? (or a score that
can be normalized to give a probability). The
most likely pronunciation of a word is taken to be
arg max?? Pr(??|g?).
Our baseline is a single G2P model that is trained
on all available training data. We train additional
models on language-specific training subsets and in-
corporate the output of these models to re-estimate
Pr(??|g?), which involves the following steps:
1. Train a supervised word origin classifier to pre-
dict Pr(l|w) for all l ? L, the set of languages
in our hand-labeled word origin training set.
2. Train G2P models for each l ? L. Each model
ml is trained on words with Pr(l|w) greater
than some threshold ?. Here, we use ? = 0.7.
3. For each word w in the test set, generate can-
didate transcriptions from model ml for each
language with nonzero Pr(l|w). Re-estimate
Pr(??|g?) by interpolating the outputs of the
language-specific models. We may also use the
output of the language-independent model.
We elaborate on our approaches to Steps 1 and 3.
3.1 Step 1: Word origin modeling
We apply a sequential conditional model to predict
Pr(l|w), the probability of a language class given
the word. A similar Maximum Entropy model is
presented by Chen and Maison (2003), where fea-
tures are the presence or absence of a given charac-
ter n-gram in w. In our approach, feature functions
are defined at character positions rather than over the
entire word. Specifically, for word wj composed of
character sequence c1 . . . cm of length m (including
start and end symbols), binary features test for the
presence or absence of an n-gram context at each
position m. A context is the presence of a charac-
ter n-gram starting or ending at position m. Model
features are represented as:
fi(w,m, lk) =
?
??
??
1, if lang(w) = lk and context
i is present at position m
0, otherwise
(1)
Then, for wj = ci . . . cm:
Pr(lk|wj) =
exp
?
m
?
i ?ifi(cm, lk)
Z
(2)
where Z =
?
j exp
?
m
?
i ?ifi(cm, lk) is a nor-
malization factor. In practice, we can implement this
model as a CRF, where a language label is applied
at each character position rather than for the word.
While all the language labels in a sequence need
not be the same, we find only a handful of words
where a transition occurs from one language label to
another within a word. For these cases, we take the
label of the last character in the word as the language
of origin. Experiments comparing this sequential
Maximum Entropy method with other word origin
classifiers are described by Waxmonsky (2011).
3.2 Step 3: Re-weighting of G2P output
We test two methods of re-weighting Pr(??|g?) us-
ing the word origin estimation and the output of
language-specific G2P models.
Method A uses only language-specific models:
P?r(??|g?) =
?
l?L
Pr(??|g?, l) Pr(l|g) (3)
where Pr(??|g?, l) is estimated by model ml.
Method B With the previous method, names from
infrequent classes suffer from data sparsity. We
therefore smooth with the output PI of the baseline
language-independent model.
P?r(??|g?) = ?Pr
I
(??|g?)+(1??)
?
l?L
Pr(??|g?, l) Pr(l|g)
(4)
The factor ? is tuned on a development set.
368
Language Train Test Base (A) (B)
Class Count Count -line
British 16.1k 2111 71.8 73.1 73.9
German 8360 1109 75.8 74.2 78.2
Italian 3358 447 61.7 66.2 65.1
Slavic 1658 232 50.9 49.6 51.7
Spanish 1460 246 44.7 41.5 48.0
French 1143 177 42.9 42.4 45.2
Dutch 468 82 70.7 52.4 68.3
Scandin. 393 61 77.1 60.7 72.1
Japanese 116 23 73.9 52.2 78.3
Arabic 68 18 33.3 11.1 38.9
Portug. 34 4 25.0 25.0 50.0
Hungarian 28 3 100.0 66.7 100.0
Other 431 72 55.6 54.2 59.7
All 67.8 67.4 70.0
Table 1: G2P word accuracy for various weighting meth-
ods using a character-based word origin model.
4 Experiments
4.1 Data
We assemble a data set of surnames that occur fre-
quently in the United States. Since surnames are
often ?Americanized? in their written and phone-
mic forms, our goal is to model how a name is
most likely to be pronounced in standard US English
rather than in its language of origin.
We consider the 50,000 most frequent surnames
in the 1990 census1, and extract those entries that
also appear in the CMU Pronouncing Dictionary2,
giving us a set of 45,841 surnames with their
phoneme representations transcribed in the Arpabet
symbol set. We divide this data 80/10/10 into train,
test, and development sets.
To build a word origin classification training set,
we randomly select 3,000 surnames from the same
census lists, and label by hand the most likely lan-
guage of origin of each name when it occurs in the
US. Labeling was done primarily using the Dictio-
nary of American Family Names (Hanks, 2003) and
Ellis Island immigration records.3 We find that, in
many cases, a surname cannot be attributed to a sin-
gle language but can be assigned to a set of lan-
1http://www.census.gov/genealogy/names/
2http://www.speech.cs.cmu.edu/cgi-bin/
cmudict
3http://www.ellisisland.org
guages related by geography and language family.
For example, we discovered several surnames that
could be ambiguously labeled as English, Scottish,
or Irish in origin. For languages that are frequently
confusable, we create a single language group to be
used as a class label. Here, we use groups for British
Isles, Slavic, and Scandinavian languages. Names
of undetermined origin are removed, leaving a final
training set of 2,795 labeled surnames and 33 dif-
ferent language classes. We have made this anno-
tated word origin data publicly available for future
research.4
In these experiments, we use surnames from the
12 language classes that contain at least 10 hand-
labeled words, and merge the remaining languages
into an ?Other? class. Table 1 shows the final lan-
guage classes used. Unlike the training sets, we do
not remove names with ambiguous or unknown ori-
gin from the test set, so our G2P system is also eval-
uated on the ambiguous names.
4.2 Results
The Sequitur G2P algorithm (Bisani and Ney, 2008)
is used for all our experiments.
We use the CMU Dictionary as the gold stan-
dard, with the assumption that it contains the stan-
dard pronunciations in US English. While surnames
may have multiple valid pronunciations, we make
the simplifying assumption that a name has one best
pronunciation. Evaluation is done on the test set of
4,585 names from the CMU Dictionary.
Table 1 shows G2P accuracy for the baseline sys-
tem and Methods A and B. Test data is partitioned
by the most likely language of origin.
We see that Method A, which uses only language-
specific G2P models, has lower overall accuracy
than the baseline. We attribute this to data spar-
sity introduced by dividing the training set by lan-
guage. With the exception of British and German,
language-specific training set sizes are less than 10%
the size of the baseline training set of 37k names.
Another cause of the lowered performance is likely
due to errors made by our word origin model.
Examining results for individual language classes
for Method A, we see that Italian and British are
4The data may be downloaded from http://people.
cs.uchicago.edu/?wax/wordorigin/.
369
Language Surname Baseline Method B
Carcione K AA R S IY OW N IY K AA R CH OW N IY
Cuttino K AH T IY N OW K UW T IY N OW
Italian Lubrano L AH B R AA N OW L UW B R AA N OW
Pesola P EH S AH L AH P EH S OW L AH
Kotula K OW T UW L AH K AH T UW L AH
Slavic Jaworowski JH AH W ER AO F S K IY Y AH W ER AO F S K IY
Lisak L IY S AH K L IH S AH K
Wasik W AA S IH K V AA S IH K
Bencivenga B EH N S IH V IH N G AH B EH N CH IY V EH NG G AH
Spanish Vivona V IH V OW N AH V IY V OW N AH
Zavadil Z AA V AA D AH L Z AA V AA D IY L
Table 2: Sample G2P output from the Baseline (language-independent) and Method B systems. Language labels
shown here are the arg maxl P (l|w) using the character-based word origin model. Phoneme symbols are from an
Arpabet-based alphabet, as used in the CMU Pronouncing Dictionary.
the only language classes where accuracy improves.
For Italian, we attribute this to two factors: high
divergence in pronunciation from US English, and
the availability of enough training data to build a
successful language-specific model. In the case of
British, a language-specific model removes foreign
words but leaves enough training data to model the
language sufficiently.
Method B shows accuracy gains of 2.2%, with
gains for almost all language classes except Dutch
and Scandinavian. This is probably because names
in these two classes have almost standard US En-
glish pronunciations, and are already well-modeled
by a language-independent model.
We next look at some sample outputs from our
G2P systems. Table 2 shows names where Method
B generated the gold standard pronunciation and the
baseline system did not. For the Italian and Span-
ish sets, we see that the letter-to-phoneme mappings
produced by Method B are indicative of the lan-
guage of origin: (c ? /CH/) in Carcione, (u ?
/UW/) in Cuttino, (o ? /OW/) in Pesola, and (i ?
/IY/) in Zavadil and Vivona. Interestingly, the name
Bencivenga is categorized as Spanish but appears
with the letter-to-phoneme mapping (c ? /CH/),
which corresponds to Italian as the language of ori-
gin. We found other examples of the (c ? /CH/)
mappings, indicating that Italian-origin names have
been folded into Spanish data. This is not surprising
since Spanish and Italian names have high confusion
with each other. Effectively, our word origin model
produced a noisy Spanish G2P training set, but the
re-weighted G2P system is robust to these errors.
We see examples in the Slavic set where the gold
standard dictionary pronunciation is partially but not
completely Americanized. In Jaworowski, we have
the mappings (j? /Y/) and (w? /F/), both of which
are derived from the original Polish pronunciation.
But for the same name, we also have (w ? /W/)
rather than (w? /V/), although the latter is truer to
the original Polish. This illustrates one of the goals
of our project, which is is to capture these patterns
of Americanization as they occur in the data.
5 Conclusion
We apply word origin modeling to grapheme-
to-phoneme conversion, interpolating between
language-independent and language-specific proba-
bilistic grapheme-to-phoneme models. We find that
our system outperforms the baseline in predicting
Americanized surname pronunciations and captures
several letter-to-phoneme features that are specific
to the language of origin.
Our method operates as a wrapper around G2P
output without modifying the underlying algorithm,
and therefore can be applied to any state-of-the-art
G2P system that outputs posterior probabilities of
phoneme sequences for a word.
Future work will consider unsupervised or semi-
supervised approaches to word origin recognition
for this task, and methods to tune the smoothing
weights ? at the language rather than the global
level.
370
References
Aditya Bhargava and Grzegorz Kondrak. 2010. Lan-
guage identification of names with SVMs. In Proceed-
ings of NAACL.
Maximilian Bisani and Hermann Ney. 2008. Joint-
sequence models for grapheme-to-phoneme conver-
sion. Speech Communication.
Dipankar Bose and Sudeshna Sarkar. 2009. Learning
multi character alignment rules and classification of
training data for transliteration. In Proceedings of the
ACL Named Entities Workshop.
Stanley F. Chen and Beno??t Maison. 2003. Using place
name data to train language identification models. In
Proceedings of Eurospeech.
Nick Cremelie and Louis ten Bosch. 2001. Improv-
ing the recognition of foreign names and non-native
speech by combining multiple grapheme-to-phoneme
converters. In Proceedings of ITRW on Adaptation
Methods for Speech Recognition.
Masato Hagiwara and Satoshi Sekine. 2011. Latent class
transliteration based on source language origin. In
Proceedings of ACL.
Patrick Hanks. 2003. Dictionary of American family
names. New York : Oxford University Press.
Mitesh M. Khapra and Pushpak Bhattacharyya. 2009.
Improving transliteration accuracy using word-origin
detection and lexicon lookup. In Proceedings of the
ACL Named Entities Workshop.
Ariadna Font Llitjo?s. 2001. Improving pronunciation
accuracy of proper names with language origin classes.
Master?s thesis, Carnegie Mellon University.
Beno??t Maison, Stanley F. Chen, and Paul S. Cohen.
2003. Pronunciation modeling for names of foreign
origin. In Proceedings of ASRU.
Sonjia Waxmonsky. 2011. Natural language process-
ing for named entities with word-internal information.
Ph.D. thesis, University of Chicago.
371
