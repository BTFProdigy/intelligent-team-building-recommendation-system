Unsupervised Discovery of Scenario-Level Patterns for 
Information Extraction 
Roman Yangarber  Ra lph  Gr i shman 
roman?as, nyu. edu grishman?cs, nyu. edu 
Courant Inst i tute of Courant Inst i tute of 
Mathematical  Sciences Mathematical  Sciences 
New York University New York University 
Pas i  Tapana inen  )~ Si l ja  Hut tunen $ 
tapanain?conexor, fi sihuttun~ling.helsinki, fi 
t Conexor Oy :~ University of Helsinki 
Helsinki, F inland F in land 
Abst rac t  
Information Extraction (IE) systems are com- 
monly based on pattern matching. Adapting 
an IE system to a new scenario entails the 
construction of a new pattern base---a time- 
consuming and expensive process. We have 
implemented a system for finding patterns au- 
tomatically from un-annotated text. Starting 
with a small initial set of seed patterns proposed 
by the user, the system applies an incremental 
discovery procedure to identify new patterns. 
We present experiments with evaluations which 
show that the resulting patterns exhibit high 
precision and recall. 
0 I n t roduct ion  
The task of Information Extraction (I-E) is 
the selective extraction of meaning from free 
natural language text. I "Meaning" is under- 
stood here in terms of a fixed set of semantic 
objects--entities, relationships among entities, 
and events in which entities participate. The 
semantic objects belong to a small number of 
types, all having fixed regular structure, within 
a fixed and closely circumscribed subject do- 
main. The extracted objects are then stored in 
a relational database. In this paper, we use the 
nomenclature accepted in current IE literature; 
the term subject domain denotes a class of tex- 
tual documents to be processed, e.g., "business 
news," and scenario denotes the specific topic 
of interest within the domain, i.e., the set of 
facts to be extracted. One example of a sce- 
nario is "management succession," the topic of 
MUC-6 (the Sixth Message Understanding Con- 
ference); in this scenario the system seeks to 
identify events in which corporate managers left 
1For general references on IE, cf., e.g., (Pazienza, 
1997; muc, 1995; muc, 1993). 
their posts or assumed new ones. We will con- 
sider this scenario in detail in a later section 
describing experiments. 
IE systems today are commonly based on pat- 
tern matching. The patterns are regular ex- 
pressions, stored in a "pattern base" containing 
a general-purpose component and a substantial 
domain- and scenario-specific component. 
Portability and performance are two major 
problem areas which are recognized as imped- 
ing widespread use of IE. This paper presents a
novel approach, which addresses both of these 
problems by automatically discovering good 
patterns for a new scenario. The viability of 
our approach is tested and evaluated with an 
actual IE system. 
In the next section we describe the problem in 
more detail in the context of our IE system; sec- 
tions 2 and 3 describe our algorithm for pattern 
discovery; section 4 describes our experimental 
results, followed by comparison with prior work 
and discussion, in section 5. 
1 The  IE  Sys tem 
Our IE system, among others, contains a a back- 
end core engine, at the heart of which is a 
regular-e~xpression pattern matcher. The engine 
draws on attendant knowledge bases (KBs) of 
varying degrees of domain-specificity. The KB 
components are commonly factored out to make 
the systems portable to new scenarios. There 
are four customizable knowledge bases in our IE 
system: the Lexicon contains general dictionar- 
ies and scenario-specific terms; the concept base 
groups terms into classes; the predicate base de- 
scribes the logical structure of events to be ex- 
tracted, and the pattern base contains patterns 
that catch the events in text. 
Each KB has a. substantial domain-specific 
component, which must be modified when mov-  
282 
ing to new domains and scenarios. The system 
allows the user (i.e. scenario developer) to start 
with example sentences in text which contain 
events of interest, the candidates, and general- 
ize them into patterns. However, the user is 
ultimately responsible for finding all the can- 
didates, which amounts to manually processing 
example sentences in a very large training cor- 
pus. Should s/he fail to provide an example 
of a particular class of syntactic/semantic con- 
struction, the system has no hope of recovering 
the corresponding events. Our experience has 
shown that (1) the process of discovering candi- 
dates is highly expensive, and (2) gaps in pat- 
terns directly translate into gaps in coverage. 
How can the system help automate the pro- 
cess of discovering new good candidates? The 
system should find examples of all common lin- 
guistic constructs relevant o a scenario. While 
there has been prior research on identifying the 
primary lexical patterns of a sub-language or 
corpus (Grishman et al, 1986; Riloff, 1996), the 
task here is more complex, since we are typi- 
cally not provided in advance with a sub-corpus 
of relevant passages; these passages must them- 
selves be found as part of the discovery process. 
The difficulty is that one of the best indications 
of the relevance of the passages i precisely the 
presence of these constructs. Because of this 
circularity, we propose to acquire the constructs 
and passages in tandem. 
2 So lu t ion  
We outline our procedure for automatic ac- 
quisition of patterns; details are elaborated in 
later sections. The procedure is unsupervised 
in that it does not require the training corpus 
to be manually annotated with events of inter- 
est, nor a pro-classified corpus with relevance 
judgements, nor any feedback or intervention 
from the user 2. The idea is to combine IR-style 
document selection with an iterative relaxation 
process; this is similar to techniques used else- 
where in NLP, and is inspired in large part, if 
remotely, by the work of (Kay and RSscheisen, 
1993) on automatic alignment of sentences and 
words in a bilingual corpus. There, the reason- 
ing was: sentences that are translations of each 
2however, it may be supervised after each iteration, 
where the user can answer yes/no questions to improve 
the quality of the results 
other are good indicators that words they con- 
tain are translation pairs; conversely, words that 
are translation pairs indicate that the sentences 
which contain them correspond to one another. 
In our context, we observe that documents 
that are relevant to the scenario will neces- 
sarily contain good patterns; conversely, good 
patterns are strong indicators of relevant docu- 
ments. The outline of our approach is as follows. 
. 
. 
Given: (1) a large corpus of un-annotated 
and un-classified documents in the domain; 
(2) an initial set of trusted scenario pat- 
terns, as chosen ad hoc by the user--the 
seed; as will be seen, the seed can be quite 
small--two or three patterns eem to suf- 
fice. (3) an initial (possibly empty) set of 
concept classes 
The pattern set induces a binary partition 
(a split) on the corpus: on any document, 
either zero or more than zero patterns will 
match. Thus the universe of documents, U, 
is partitioned into the relevant sub-corpus, 
R, vs. the non-relevant sub-corpus, R = 
U - R, with respect o the given pattern 
set. Actually, the documents are assigned 
weights which are 1 for documents matched 
by the trusted seed, and 0 otherwise. 3 
2. Search for new candidate patterns: 
(a) Automatically convert each sentence 
in the corpus,into a set of candidate 
patterns, 4 
(b) Generalize each pattern by replacing 
each lexical item which is a member of 
a concept class by the class name. 
(c) Working from the relevant documents, 
select those patterns whose distribu- 
tion is strongly correlated with other 
relevant documents (i.e., much more 
3R represents he trusted truth through the discovery 
iterations, since it was induced by the manually-selected 
seed. 
4Here, for each clause in the sentence we extract a 
tuple of its major roles: the head of the subject, the 
verb group, the object, object complement, asdescribed 
below. This tuple is considered to be a pattern for the 
present purposes of discovery; it is a skeleton for the 
rich, syntactically transformed patterns our system uses 
in the extraction phase. 
283 
densely distributed among the rele- 
vant documents than among the non- 
relevant ones). The idea is to consider 
those candidate patterns, p, which 
meet the density, criterion: 
IHnRI IRI - - > >  
IHnUI IUI 
where H = H(p) is the set of docu- 
ments where p hits. 
(d) Based on co-occurrence with the cho- 
sen patterns, extend the concept 
classes. 
3. Optional: Present he new candidates and 
classes to the user for review, retaining 
those relevant o the scenario. 
4. The new pattern set induces a new parti- 
tion on the corpus. With this pattern set, 
return to step 1. Repeat he procedure un- 
til no more patterns can be added. 
3 Methodo logy  
3.1 Pre-proeess ing:  Normal i za t ion  
Before applying the discovery procedure, we 
subject the corpus to several stages o f  pre- 
processing. First, we apply a name recognition 
module, and replace each name with a token 
describing its class, e.g. C-Person, C-Company, 
etc. We collapse together all numeric expres- 
sions, currency values, dates, etc., using a single 
token to designate ach of these classes. 
3.2 Syntact ic  Analys is  
We then apply a parser to perform syntactic 
normalization to transform each clause into a 
common predicate-argument structure. We use 
the general-purpose d pendency parser of En- 
glish, based on the FDG formalism (Tapanainen 
and J~rvinen, 1997) and developed by the Re- 
search Unit for Multilingual Language Technol- 
ogy at the University of Helsinki, and Conexor 
Oy. The parser (modified to understand the 
name labels attached in the previous step) is 
used for reducing such variants as passive and 
relative clauses to a tuple, consisting of several 
elements. 
1. For each claus, the first element is the sub- 
ject, a "semantic" subject of a non-finite 
sentence or agent of the passive. 5 
2. The second element is the verb. 
3. The third element is the object, certain 
object-like adverbs, subject of the passive 
or subject complement 6 
4. The fourth element is a phrase which 
refers to the object or the subject. A 
typical example of such an argument is 
an object complement, such as Com- 
pany named John Smith pres ident .  An- 
other instance is the so-called copredica- 
tire (Nichols, 1978), in the parsing system 
(J~irvinen and Tapanainen, 1997). A co- 
predicative refers to a subject or an object, 
though this distinction is typically difficult 
to resolve automatically/ 
Clausal tuples also contain a locative modifier, 
and a temporal modifier. We used a corpus of 
5,963 articles from the Wall Street Journal, ran- 
domly chosen. The parsed articles yielded a to- 
tal of 250,000 clausal tuples, of which 135,000 
were distinct. 
3.3 Genera l i za t ion  and  Concept  Classes 
Because tuples may not repeat with sufficient 
frequency to obtain reliable statistics, each tu- 
ple is reduced to a set of pairs: e.g., a verb- 
object pair, a subject-object pair, etc. Each 
pair is used as a generalized pattern during 
the candidate selection stage. Once we have 
identified pairs which are relevant o the sce- 
nario, we use them to construct or augment con- 
cept classes, by grouping together the missing 
roles, (for example, a class of verbs which oc- 
cur with a relevant subject-object pair: "com- 
pany (hire/fire/expel...} person"). This is sim- 
ilar to work by several other groups which 
aims to induce semantic lasses through syn- 
tactic co-occurrence analysis (Riloff and Jones, 
1999; Pereira et al, 1993; Dagan et al, 1993; 
Hirschman et al, 1975), although in .our case 
the contexts are limited to selected patterns, 
relevant o the scenario. 
SE.g., " John sleeps", "John is appointed by 
Company" ,  "I saw a dog which sleeps", "She asked 
John  to buy a car". 
6E.g., " John is appointed by Company", "John is the 
pres ident  of Company", "I saw a dog which sleeps", 
The dog  which I saw sleeps. 
7For example, "She gave us our coffee black",  "Com- 
pany appointed John Smith as pres ident" .  
284 
3.4 Pattern Discovery 
Here we present he results from experiments 
we conducted on the MUC-6 scenario, "man- 
agement succession". The discovery procedure 
was seeded with a small pattern set, namely: 
Subject Verb Direct Object 
C-Company C-Appoint C-Person 
C-Person C-Resign 
Documents are assigned relevance scores on 
a scale between 0 and 1. The seed patterns 
are accepted as ground truth; thus the docu- 
ments they match have relevance 1. On sub- 
sequent iterations, the newly accepted patterns 
are not trusted as absolutely. On iteration um- 
ber i q- 1, each pattern p is assigned a precision 
measure, based on the relevance of the docu- 
ments it matches: 
Here C-Company and C-Person denote se- 
mantic classes containing named entities of the 
corresponding semantic types. C-Appoirlt de- 
notes a class of verbs, containing four verbs 
{ appoint, elect, promote, name}; C-Resign = 
{ resign, depart, quit, step-down }. 
During a single iteration, we compute the 
score s, L(p), for each candidate pattern p: 
L(p) = Pc(P)" log {H A R\] (1) 
where R denotes the relevant subset, and H -- 
H(p) the documents matching p, as above, and 
\[gnR\[ Pc(P) -- Igl is the conditional probability of 
relevance. We further impose two support cri- 
teria: we distrust such frequent patterns where 
\[HA U{ > a\[U\[ as uninformative, and rare pat- 
terns for which \[H A R\[ </3  as noise. ? At the 
end of each iteration, the system selects the pat- 
tern with the highest score, L(p), and adds it to 
the seed set. The documents which the winning 
pattern hits are added to the relevant set. The 
pattern search is then restarted. 
3.5 Re-computat lon  of  Document  
Relevance 
The above is a simplification of the actual pro- 
cedure, in several important respects. 
Only generalized patterns are considered for 
candidacy, with one or more slots filled with 
wild-cards. In computing the score of the gen- 
eralized pattern, we do not take into considera- 
tion all possible values of the wild-card role. We 
instead constrain the wild-card to those values 
which themselves in turn produce patterns with 
high scores. These values then become members 
of a new class, which is output in tandem with 
the winning pattern 1? 
Ssimilarly to (Riloff, 1996) 
?U denotes the universe of documents. We used c~ = 
0.i and ~----- 2. 
1?The classes are currently unused by subsequent i er- 
ations; this important issue is considered in future work. 
Preci+l(p) = 1 {H(p){ ~ Reli(d) (2) 
dEH(p) 
where Reli(d) is the relevance of the document 
from the previous iteration, and H(p) is the set 
of documents where p matched. More generally, 
if K is a classifier consisting of a set of patterns, 
we can define H(K) as the set of documents 
where all of patterns p E K match, and the 
"cumulative" precision 11 of K as 
Preci+l(K) = 1 ~ Reli(d) (3) 
IH(K)\[ riCH(K) 
Once the new winning pattern is accepted, 
the relevance scores of the documents are re- 
adjusted as follows. For each document d which 
is matched by some (non-empty) subset of the 
currently accepted patterns, we can view that 
subset of patterns as a classifier K d = {py}. 
These patterns determine the new relevance 
score of the document 
Reli+l(d) = max (Rel~(d),Prec~+l(Kd)) (4) 
This ensures that the relevance score grows 
monotonically, and only when there is sufficient 
positive evidence, as the patterns in effect vote 
"conjunctively" on the documents. The results 
which follow use this measure. 
Thus in the formulas above, R is not sim- 
ply the count of the relevant documents, but 
is rather their cumulative relevance. The two 
formulas, (3) and (4), capture the mutual de- 
pendency of patterns and documents; this re- 
computation and growing of precision and rele- 
vance scores is at the heart of the procedure. 
11Of course, this measure is defined only when 
H(K) # 0. 
285 
4 Resu l ts  1 
An objective measure of goodness of a pattern o. 9 
is not trivial to establish since the patterns can- 
not be used for extraction directly, without be- o. s 
ing properly incorporated into the knowledge 
base. Thus, the discovery procedure does not o. v 
lend itself easily to MUC-style evaluations, ince 
0.6  a pattern lacks information about which events 
it induces and which slots its arguments should 0.5  
fill. 
However, it is possible to apply some objec- o. a 
tive measures of performance. One way we eval- 
uated the system is by noting that in addition o.  
to growing the pattern set, the procedure also 
grows the relevance of documents. The latter o. 2 
can be objectively evaluated. 
0.1  
We used a test corpus of 100 MUC-6 formal- 
training documents (which were included in the o 
main development corpus of about 6000 docu- 
ments) plus another 150 documents picked at 
random from the main corpus and judged by 
hand. These judgements constituted the ground 
truth and were used only for evaluation, (not in 
the discovery procedure). 
4.1 Text  F i l ter ing 
Figure 1 shows the recall/precision measures 
with respect to the test corpus of 250 docu- 
ments, over a span of 60 generations, tarting 
with the seed set in table 3.4. The Seed pat- 
terns matched 184 of the 5963 documents, yield- 
ing an initial recall of .11 and precision of .93; 
by the last generation it searched through 982 
documents with non-zero relevance, and ended 
with .80 precision and .78 recall. This facet of 
the discovery procedure is closely related to the 
MUC '%ext-filtering" sub-task, where the sys- 
tems are judged at the level of documents rather 
than event slots. It is interesting to compare the 
results with other MUC-6 participants, shown 
anonymously in figure 2. Considering recall and 
precision separately, the discovery procedure at- 
tains values comparable to those achieved by 
some of the participants, all of which were ei- 
ther heavily-supervised or manually coded sys- 
tems. It is important o bear in mind that the 
discovery procedure had no benefit of training 
material, or any information beyond the seed 
pattern set. 
I I I I I I 
......i"{+X'N+~v i P ~ e c i s i o n  ' i 
" "  ... .  i . .~ .  i Re~aa - - -?- - -  
. . . . . . . . . .  ~ . . . . . . . . . .  ~ . . . . . . . . .  ~ . . . . . . . . . . .  ~ . . . . . . . . . .  ~ . . . . . . . . . .  ~ . . . . . . . . .  
...... iiiiiiiiiiiiiilEi   ........... ........ 
/... 
. . . . . . . . . .  ~ . . . . . . . . . .  '." . . . . . . . .  ": . . . . . . . . . .  ~ . . . . . . . . . . .  r . . . . . . . . . .  ~ .. . . . . . . . .  ! . . . . . . . . .  
2111111ji.. iii121122;1211111;ii122221ilSiiii12112121SiiiiSiii: . . . . . .
0 I0  20  30  40  50  60  70  
G e n e r a t i o n  # 
80 
Figure h Recall/Precision curves for Manage- 
ment Succession 
4.2 Cho ice  of  Test  Corpus  
Figure 2 shows two evaluations of our discovery 
procedure, tested against the original MUC-6 
corpus of 100 documents, and against our test 
corpus, which consists of an additional 150 doc- 
uments judged manually. The two plots in the 
figure show a slight difference in results, indi- 
cating that in some sense, the MUC corpus was 
more "random", or that our expanded corpus 
was somewhat skewed in favor of more common 
patterns that the system is able to find more 
easily. 
4.3 Cho ice  of  Eva luat ion  Met r i c  
The graphs shown in Figures 1 and 2 are based 
on an "objective" measure we adopted during 
the experiments. This is the same measure of 
relevance used internally by the discovery proce- 
dure on each iteration (relative to the "truth" of 
relevance scores of the previous iteration), and 
is not quite the standard measure used for text 
filtering in IR. According to this measure, the 
system gets a score for each document based on 
the relevance which it assigned to the document. 
Thus if the system .assigned relevance of X per- 
cent to a relevant document, it only received X 
286 
0 
( I )  
0 . 9  
0 . 8  
0 . 7  
I I  I I I I I I : : . , . : ? 
. . . . . . .  . . . . . . . .  . . . . . . . . . . . . . . .  - - -  ! . . . . . . . .  . . . . . . .  
i i i :: i i i i i ~ 
. . . . . .  i . . . . . . . .  i . . . . . . . .  \[ . . . . . . .  ? .. . . . . . .  f . . . . . . . . . . . .  T . . . . . . .  
. . . . . .  J . . . . . . . .  i . . . . . . . .  i .  . . . . . . .  i . . . . . . . .  ~ . . . . . . .  .; . . . . . . . . . . .  ..: . . . . . . . .  i . . . . :  
0 6 . . . . . . . . . . . . . . . . . . . . . . . .  .'7 . . . . . . .  ' . . . . . . . .  ~ . . . . . . .  ~ . . . . . . . .  ~ . . . . . . . . . . . . . . . . . . . . . . .  
i 
i 
i 
0 . 5 . . . . . . .  '. . . . . . . .  , . . . . . . . .  ~" . . . . . . .  , . . . . . . . .  ~ . . . . . . .  ~ . . . . . . . .  * . . . . . . .  "=. . . . . . . .  , . . . . . . .  
i z 
0 . 4  i I 
0 0 . 1 0 . 2 0 . 3 0 . 4 0 . 5 0 . 6 0 . 7 0 . 8 0 . 9  1 
R e c a l l  
Figure 2: Precision vs. Recall 
I 
i . i ~ iB  i 
. . . . . .  e . . . . . . . .  i . . . . . . . .  ! . . . . . . . .  ~ . . . . . . . .  ~ . . .  ' . . . . . . . .  ! . . . . . . . .  i . . . . . . .  0 . 9  
i i i i i im~ ! ! 
! i i D iE  c 
! i i i i i i i i 
0 . '7  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  ~" . . . . . . . . . . . . . . .  
C o n ~ i n ~ o u -  ? ~ut i - -o  f 
0 . 6 . . . . . .  ~ . . . . . . . .  ~ . . . . . . . .  ~ . . . . . . .  ' . . . . . . . .  ~ . . . . . . .  ~ . . . . . . . .  ; . . . . . . .  ~ . . . . . . . .  ~ . . . . . . .  
0 . 5 ...... ~ ........ , ........ ~ ....... ~ ........ ~ ....... ~ ........ ! ........ : ........ 4 ....... 
0 . 4  
0 0 . i 0 . 2 0 . 3 0 . 4 0 . 5 0 . 6 0 . 7 0 . 8 0 . 9  1 
R e c a l l  
Figure 3: Results on the MUC corpus 
percent on the recall score for classifying that 
document correctly. Similarly, if the system as- 
signed relevance Y to an irrelevant document, 
it was penalized only for the mis-classified Y
percent on the precision score. To make our re- 
sults more comparable to those of other MUC 
competitors, we chose a cut-off point and force 
the system to make a binary relevance decision 
on each document. The cut-off of 0.5 seemed 
optimal from empirical observations. Figure 3 
shows a noticeable improvement in scores, when 
using our continuous, "objective" measure, vs. 
the cut-off measure, with the entire graph essen- 
tially translated to the right for a gain of almost 
10 percentage points of recall. 
4.4 Eva luat ing  Pat terns  
Another effective, if simple, measure of perfor- 
mance is  how many of the patterns the pro- 
cedure found, and comparing them with those 
used by an extraction engine which was manu- 
ally constructed for the same task. Our MUC-6 
system used approximately 75 clause level pat- 
terns, with 30 distinct verbal heads. In one 
conservative experiment, we observed that the 
discovery procedure found 17 of these verbs, or 
57%. However, it also found at least 8 verbs the 
manual system lacked, which seemed relevant to 
the scenario: 
company-bring-person-\[as?officer\] 12 
person-come-\[to+eompanv\]-\[as+oZScer\] 
person-rejoin- company-\[as + o25cer\] 
person-{ ret  , conti,  e, remai, ,stay}-\[as + o25cer\] 
person-pursue-interest 
At the risk of igniting a philosophical de- 
bate over what is or is not relevant o a sce- 
nario, we note that the first four of these verbs 
are evidently essential to the scenario in the 
strictest definition, since they imply changes of 
post. The next three are "staying" verbs, and 
are actually also needed, since higher-level infer- 
ences required in tracking events for long-range 
merging over documents, require knowledge of 
persons occupying posts, rather than only as- 
suming or leaving them. The most curious one 
is "person-pursue-interesf'; urprisingly, it too 
is useful, even in the strictest MUC sense, cf., 
(muc, 1995). Systems are judged on filling a 
slot called "other-organization", i dicating from 
or to which company the person came or went. 
This pattern is consistently used in text to indi- 
nbracketed  const i tuents  a re  outs ide  o f  the  cent ra l  
SVO t r ip le t ,  inc luded here  fo r  c la r i ty .  
287 
cate that the person left to pursue other, undis- 
closed interests, the knowledge of which would 
relieve the system from seeking other informa- 
tion in order to fill this slot. This is to say that 
here strict evaluation is elusive. 
5 D iscuss ion  and  Cur rent  Work  
Some of the prior research as emphasized in- 
teractive tools to convert examples to extraction 
patterns, cf. (Yangarber and Grishman, 1997), 
while others have focused on methods for au- 
tomatically converting a corpus annotated with 
extraction examples into such patterns (Lehn- 
ert et al, 1992; Fisher et al, 1995; Miller et 
al., 1998). These methods, however, do not re- 
duce the burden of finding the examples to an- 
notate. With either approach, the portability 
bottleneck is shifted from the problem of build- 
ing patterns to that of finding good candidates. 
The prior work most closely related to this 
study is (Riloff, 1996), which, along with (Riloff, 
1993), seeks automatic methods for filling slots 
in event templates. However, the prior work 
differs from that presented here in several cru- 
cial respects; firstly, the prior work does not at- 
tempt to find entire events, after the fashion 
of MUC's highest-level scenario-template task. 
Rather the patterns produced by those systems 
identify NPs that fill individual slots, without 
specifying how these slots may be combined 
at a later stage into complete vent templates. 
The present work focuses on directly discovering 
event-level, multi-slot relational patterns. Sec- 
ondly, the prior work either relies on a set of 
documents with relevance judgements to find 
slot fillers where they are relevant o events, 
(Riloff, 1996), or utilizes an un-classified cor- 
pus containing a very high proportion of rele- 
vant documents o find all instances of a seman- 
tic class, (Riloff and Jones, 1999). By contrast, 
our procedure requires no relevance judgements, 
and works on the assumption that the corpus is 
balanced and the proportion of relevant docu- 
ments is small. Classifying documents by hand, 
although admittedly easier than tagging event 
instances in text for automatic training, is still 
a formidable task. When we prepared the test 
corpus, it took 5 hours to mark 150 short doc- 
uments. 
The presented results indicate that our 
method of corpus analysis can be used to rapidly 
identify a large number of relevant patterns 
without pre-classifying a large training corpus. 
We are at the early stages of understanding 
how to optimally tune these techniques, and 
there are number of areas that need refinement. 
We are working on capturing the rich informa- 
tion about concept classes which is currently re- 
turned as part of our pattern discovery proce- 
dure, to build up a concept dictionary in tandem 
with the pattern base. We are also consider- 
ing the proper selection of weights and thresh- 
olds for controlling the rankings of patterns and 
documents, criteria for terminating the itera- 
tion process, and for dynamic adjustments of 
these weights. We feel that the generalization 
technique in pattern discovery offers a great 
opportunity for combating sparseness of data, 
though this requires further research. Lastly, 
we are studying these algorithms under several 
unrelated scenarios to determine to what extent 
scenario-specific phenomena affect their perfor- 
mance. 
References 
Ido Dagan, Shaul Marcus, and Shaul 
Markovitch. 1993. Contextual word simi- 
larity and estimation from sparse data. In 
Proceedings of the 31st Annual Meeting of 
the Assn. for Computational Linguistics, 
pages 31-37, Columbus, OH, June. 
David Fisher, Stephen Soderland, Joseph Mc- 
Carthy, Fang-fang Feng, and Wendy Lehnert. 
1995. Description of the UMass system as 
used for MUC-6. In Proc. Si;zth Message Un- 
derstanding Conf. (MUC-6), Columbia, MD, 
November. Morgan Kaufmann. 
R. Grishman, L. Hirschman, and N.T. Nhan. 
1986. Discovery procedures for sublanguage 
selectional patterns: Initial experiments. 
Computational Linguistics, 12(3):205-16. 
Lynette Hirschman, Ralph Grishman, and 
Naomi Sager. 1975. Grammatically-based 
automatic word class formation. Information 
Processing and Management, 11(1/2):39-57. 
Timo J/irvinen and Pasi Tapanainen. 1997. A 
dependency parser for English. Technical Re- 
port TR-1, Department of General Linguis- 
tics, University of Helsinki, Finland, Febru- 
ary. 
Martin Kay and Martin RSscheisen. 1993. 
288 
Text-translation alignment. Computational 
Linguistics, 19(1). 
W. Lehnert, C. Cardie, D. Fisher, J. McCarthy, 
E. Riloff, and S. Soderland. 1992. Univer- 
sity of massachusetts: MUC-4 test results 
and analysis. In Proc. Fourth Message Un- 
derstanding Conf., McLean, VA, June. Mor- 
gan Kaufmann. 
Scott Miller, Michael Crystal, Heidi Fox, 
Lance Ramshaw, Richard Schwartz, Rebecca 
Stone, Ralph Weischedel, and the Annota- 
tion Group. 1998. Algorithms that learn to 
extract information; BBN: Description of the 
SIFT system as used for MUC-7. In Proc. of 
the Seventh Message Understanding Confer- 
ence, Fairfax, VA. 
1993. Proceedings of the Fifth Message Un- 
derstanding Conference (MUC-5), Baltimore, 
MD, August. Morgan Kaufmann. 
1995. Proceedings of the Sixth Message Un- 
derstanding Conference (MUC-6), Columbia, 
M_D, November. Morgan Kaufmann. 
Johanna Nichols. 1978. Secondary predicates. 
Proceedings of the 4th Annual Meeting of 
Berkeley Linguistics Society, pages 114-127. 
Maria Teresa Pazienza, editor. 1997. Infor- 
mation Extraction. Springer-Verlag, Lecture 
Notes in Artificial Intelligence, Rome. 
Fernando Pereira, Naftali Tishby, and Lillian 
Lee. 1993. Distributional clustering of En- 
glish words. In Proceedings of the 31st An- 
nual Meeting of the Assn. for Computational 
Linguistics, pages 183-190, Columbus, OH, 
June. 
Ellen Riloff and Rosie Jones. 1999. Learn- 
ing dictionaries for information extraction by 
multi-level bootstrapping. In Proceedings of
Sixteenth National Conference on Artificial 
Intelligence (AAAI-99), Orlando, Florida, 
Ellen Riloff. 1993. Automatically construct- 
ing a dictionary for information extraction 
tasks. In Proceedings of Eleventh National 
Conference on Artificial Intelligence (AAAI- 
93), pages 811-816. The AAAI Press/MIT 
Press. 
Ellen Riloff. 1996. Automatically generating 
extraction patterns from untagged text. In 
Proceedings of Thirteenth National Confer- 
ence on Artificial Intelligence (AAAL96), 
pages 1044-1049. The AAAI Press/MIT 
Press. 
Pasi Tapanainen and Timo J~rvinen. 1997. A 
non-projective dependency parser. In Pro- 
ceedings of the 5th Conference on Applied 
Natural Language Processing, pages 64-71, 
Washington, D.C., April. ACL. 
Roman Yangarber and Ralph Grishman. 1997. 
Customization of information extraction sys- 
tems. In Paola Velardi, editor, International 
Workshop on Lexically Driven Information 
Extraction, pages 1-11, Frascati, Italy, July. 
Universit?~ di Roma. 
289 
Chart-Based Transfer Rule Application in Machine Translation 
Adam Meyers 
New York University 
meyers@cs.nyu.edu 
Mich iko  Kosaka 
Monlnouth University 
kosaka@monmouth.edu 
Ralph GrishInan 
New York University 
gr ishman@cs.nyu.edu 
Abstract 
35"ansfer-based Machine Translation systems re- 
quire a procedure for choosing the set; of transfer 
rules for generating a target language transla- 
tion from a given source language sentence. In 
an MT system with many comI)eting transfer 
rules, choosing t;he best, set of transfer ules for 
translation may involve the evaluation of an ex- 
plosive number of competing wets. We propose a
sohltion t;o this problem l)ased on current best- 
first chart parsing algorithms. 
1 Introduct ion 
ri~'ansfer-based Machine 'Kanslation systenls re- 
quire a procedure for choosing the set of trans- 
tier rules for generating a target  language I;rans- 
lation from a given source language sentence. 
This procedure is trivial tbr a system if, given 
a (:ontext, one transtb.r ule. can l)e selected un- 
~mfl)iguously. O|;herwise, choosing the besl; set; 
of transfer ules may involve the. evaluation of 
mmmrous competing sets. In fact, the number 
of l)ossible transfer ule combinations increas- 
es exponentially with the length of the source, 
language sentence,. This situation mirrors the 
t)roblem of choosing productions in a nondeter- 
ministic parser, in this paI)er, we descril)e a 
system for choosing transfer ules, based on s- 
tatistical chart parsing (Bol)row, 1990; Chitrao 
and Grishman, 1990; Caraballo and Charniak, 
1997; Charniak et al, 1998). 
In our Machine %'anslation system, transfer 
rules are generated automatically from parsed 
parallel text along the lines of (Matsulnoto el; 
al,, 1993; Meyers et al, 1996; Meyers et al, 
1998b). Our system tends to acquire a large 
nmnber of transt~r rules, due lnainly to 3,1terna- 
tive ways of translating the same sequences of 
words, non-literal translations in parallel text 
and parsing e, rrors. It is therefore crucial that 
our system choose the best set of rules efficient- 
ly. While the technique discussed he.re obviously 
applies to similar such systems, it could also ap- 
ply to hand-coded systems in which each word 
or group of words is related to more than one 
transfer ule. D)r example, both Multra (Hein, 
1996) and the Eurotra system described in (Way 
el; al., 1997) require components for deciding 
which combination of transtbr ules to use. The 
proi).osed technique may 1)e used with syst;ems 
like these, t)rovided that all transfer ules are as- 
signed initial scores rating thcqr at)propriateness 
for translation. These al)t)rol)riateness ratings 
couhl be dependent or independent of context. 
2 Previous Work 
The MT literature deserib(;s everal techniques 
tbr deriving the appropriate translation. Statis- 
tical systems l;hal; do not incorporate linguistic 
analysis (Brown el: al., 1993) typically choose 
the most likely translation based on a statis- 
tical mode.l, i.e.., translation probability deter- 
mines the translation. (Hein, 1996) reports a set; 
of (hand-coded) fea|;llre structure based prefi~r- 
ence rules to choose among alternatives in Mu\]- 
tra. There is some discussion about adding 
some transtbr ules automatically acquired flom 
corpora to Multra? Assuming that they over- 
generate rules (as we did), a system like the one 
we propose should 1)e beneficial. In (Way et al, 
1997), many ditDrent criteria are used to dloose 
trmlsi~;r ules to execute including: pretbrmlces 
for specific rules over general ones, and comt)lex 
rule nol, ation that insures that tb.w rules can 21)- 
ply to the same set, of words. 
The Pangloss Mark III system (Nirenburg 
~This translatioll procedm'e would probably comple- 
menI~ not; replace exist, ing procedures in these systelns. 
2http : / / s tp .  l i ng .  uu. se /~corpora /p lug / repor t  s / 
ansk_last/ is a report on this 1)reject; for Multra. 
537 
and Frederking, 1995) uses a chart-walk algo- 
rithm to combine the results of three MT en- 
gines: an example-based ngine, a knowledge- 
based engine, and a lexical-transfer engine. 
Each engine contributes its best edges and tile 
chart-walk algorithm uses dynamic program- 
ruing to find the combination of edges with the 
best overall score that covers the input string. 
Scores of edges are normalized so that the scores 
fi'om the different engines are comparable and 
weighted to favor engines which tend to produce 
better results. Pangloss's algorithm combines 
whole MT systems. In contrast, our algorith- 
m combines output of individual transfer ules 
within a single MT system. Also, we use a best- 
first search that incorporates a probabilistic- 
based figure of merit, whereas Pangloss uses an 
empirically based weighting scheme and what 
appears to be a top-down search. 
Best-first probabilistic chart parsers (Bo- 
brow, 1990; Chitrao and Grishman, 1990; Cara- 
ballo and Charniak, 1997; Charniak et al, 1998) 
strive to find the best parse, without exhaus- 
tively trying a l l  possible productions. A proba- 
bilistic figure of merit (Caraballo and Charniak, 
1997; Charniak et al, 1998) is devised for rank- 
ing edges. The highest ranking edges are pur- 
sued first and the parser halts after it produces 
a complete parse. We propose an algorithm for 
choosing and applying transthr ules based on 
probability. Each final translation is derived 
from a specific set of transfer ules. If the pro- 
cedure immediately selected these transfer rules 
and applied them in tile correct order, we would 
arrive at tile final translation while creating the 
minimum number of edges. Our procedure uses 
about 4 tinms this minimum number of edges. 
With respect o chart parsing, (Charniak et al, 
1998) report that their parser can achieve good 
results while producing about three times tile 
mininmm number of edges required to produce 
the final parse. 
3 Test  Data  
We conducted two experiments. For experimen- 
t1, we parsed a sentence-aligned pair of Span- 
ish and English corpora, each containing 1155 
sentences of Microsoft Excel Help Text. These 
pairs of parsed sentences were divided into dis- 
tinct training and test sets, ninety percent for 
training and ten percent fbr test. The training 
Source Tree Target Tree 
D = vo lvcr  D' = reca lcu late  
s,,I,J  
A = Exce l  E = ca lcu la r  
Obj~en A' = Excel I C' = workbook 
B' = va lues  
/ C = l ibro 
k , 
B =va lores  \ae  
F = t raba jo  
Excel vuelve a calcular Excel recalculates 
valores en libro de trabajo values iu workbook 
Figure 1: Spanish and English I{egularized 
Parse 2?ees 
set was used to acquire transfer ules (Meyers 
et al, 1998b) which were then used to translate 
tile sentences in tile test set. This paper focus- 
es on our technique for applying these transfer 
rules in order to translate the test sentences. 
The test and training sets in experiment1 
were rotated, assigning a different enth of the 
sentences to the test set in each rotation. In this 
ww we tested tile program on the entire corpus. 
Only one test set (one tenth of the corpus) was 
used for tuning the system (luring development. 
~:ansfer rules, 11.09 on average, were acquired 
t'rom each training set and used for translation 
of the corresponding test set. For Experiment 
2, we parsed 2617 pairs of aligned sentences and 
used the same rotation procedure for dividing 
test and training corpora. The Experiment 2
corpus included the experinlentl corpus. An av- 
erage of 2191 transfer ules were acquired from 
a given set of Experinmnt 2 training sentences. 
Experimentl isorchestrated in a carefld man- 
ner that may not be practical for extremely 
large corpora, and Experiment 2 shows how the 
program performs if we scale up and elilniuate 
some of the fine-tuning. Apart from corpus size, 
there are two main difference between the two 
experiments: (1) the experimentl corpus was 
aligned completely by hand, whereas the Exper- 
iment 2 corpus was aligned automatically using 
the system described ill (Meyers et al, 1998a); 
and (2) the parsers were tuned to the experi- 
mentl sentences, but not the Experiment 2 sen- 
tences (that did not overlap with experinmntl). 
538 
1) A = Excel  
2) B =va lores  
C = l ibro 
v 
A' = Excel 
B'  = values 
.~) 
r 
C' = workbook  
F = trabajo 
D = volvcr S.IJ.i~ 
4) 
1 E = ealcular  
O b. \ ]~en l 
2 3 
1)' = recalculate 
1 2 3 
Figure 2: A S('t of %-ansfer Rules 
4 Parses  and  Trans fer  Ru les  
Figure 1 is a pair of "regularized" parses t br a 
corresi)onding pair of Spanish and Fmglish sen- 
tences fi'om Microsoft Excel hell) text. These 
at'(; F-structure-like dependency analyses of sen- 
tences that represent 1)redicate argument struc- 
ture. This representation serves to neutralize 
some ditfbrences between related sentence tyt)es, 
e.g., the regularized parse of related active and 
t)a,~sive senten(:es are identical, except tbr the 
{i'.ature value pair {Mood, Passive}. Nodes (wfl- 
ues) are labeled with head words and arcs (fea- 
tures) are labeled with gramma~;ical thnetions 
(subject, object), 1)repositions (in) and subor- 
dinate conjunctions (beNre). a For demonstra- 
tion purposes, the source tree in Figure 1 is the 
input to our translation system and the target 
tree is the outl)ut. 
The t;ransfer rules in Figure 2 can be 
used to convert the intmt; tree into the out- 
1)at tree. These transtbr rules are pairs of 
corresponding rooted substructures, where a 
substructure (Matsumoto et al, 1993) is a 
connected set of arcs and nodes. A rule 
aMorphologieal features and their values (Gram- 
Number: plural) are also represented as ares and nodes. 
consists of o, ither a pair of "open" substructures 
(rule 4) or a pair of "closed" substructures (rules 
1, 2 and 3). Closed substructures consist of s- 
ingle nodes (A,A',B,B',C') or subtrees (the left 
hand side of rule 3). Open substructures con- 
tain one or more open arcs, arcs without heads 
(both sul)structures in rule 4). 
5 Simplif ied Translat ion with 
Tree-based Transfer Rules 
The rules in Figure 2 could combine by filling 
in the open arcs in rule 4 with the roots of the 
substructures in rules 1, 2 and 3. The result 
would be a closed edge which maps the left; tree 
in l,'igure, 1 into the right tree. Just as edges of a 
chart parser are based on the context free rules 
used by the chart parser, edges of our trans- 
lation system are, based on these trans~L'r ules. 
Initial edges are identical to transtb, r rules. Oth- 
er edges result from combining one closed edge 
with one open edge. Figure 3 lists the sequence 
of edges which wouhl result from combining the 
initial edges based (m Rules 1-4 to replicate, the 
trees in Figure 1. The translation proceeds by 
incrementally matching the left hand sides of 
Rules 1-4 with the intmt tree (and insuring that 
the tree is completely covered by these rules). 
The right-hand sides of these comt)atil)le rules 
are also (:ombined t;o 1)reduce the translal;iolL 
This is an idealized view of our system in which 
each node in the input tree matches the left;- 
hand side of exactly one transfer rule: there is 
no ambiguity and no combinatorial explosion. 
The reality is that more than one transfer ules 
may be activated tbr each node, as suggested 
in Figure 4. 4 If each of the six nodes of the 
source tree corresponded to five transfer rules, 
there are 56 = 15625 possible combinations of 
rules to consider. To produce t lm output  in Fig- 
ure 3, a minimum of seven edges would be re- 
quired: four initial edges derived ti'om the o- 
riginal transfer ules plus three additional edges 
representing the combination of edges (steps 2, 
3 and 4 in Figure 3). The speed of our system is 
measured by the number of actual edges divided 
by this minimuln. 
4The third example listed would actually involve two 
trm~sfer rules, one translating "volver" to "ret)cat" and 
the second translating "calcular" to "calculal;e". 
539 
1) 
2) 
D = vo lver  
S u ~  
1 E = ca lcu la r  
Obj~n 
2 3 
D = vo lver  
A = Exce l  E = ca lcu la r  
Obj~n 
2 3 
v 
v 
D' = reca lcu la te  
I 2 3 
D' = reca lcu la te  
A' = Excel 2 3 
3) 
D = volver 
A = Exce l  E = ca leu la r  
B = valores 3 
D'  = reca lcu la te  
A' = Excel / 3 
g' = values 
4) 
D = volver 
A = Excel E = ca lcu la r  
Ob/~n 
B = va io res  C = l ib ro  
de 
F = t raba jo  
v 
D'  = reca lcu la te  
A' = Excel \ C' = workbook 
B' = va lues  
Figure 3: An Idealized Translation Procedure 
6 Best  F i r s t  T rans la t ion  Procedure  
The following is an outline of our best first 
search procedure for finding a single translation: 
1. For each node N, find TN, the set of com- 
patible transfer ules 
2. Create initial edges for all TN 
3. Repeat until a "finished" edge is tbund or 
an edge limit is reached: 
(a) Find the highest scoring edge E 
(b) If complete, combine E with compati- 
ble incoml)lete dges 
(c) If incomplete, combine E with com- 
patible complete dges 
(d) Incomplete dge + complete edge = 
new edge 
The procedure creates one initial edge 
for each matching transfer rule in the 
database 5 and puts these edges in a 
'~The left-hand side of a matching transfer rule is com- 
patible with a substructure in the input source tree. 
540 
D'  = recalculate 
D = velvet 1 2 3 / %  
Sub, i / ~ a !)' = calculate 
/ \ 
/ E = \ '4 .  '+" 
3 again 
D = repeat 
Sabj ~ b j  
1 E = calculation 
Figure 4: Multiple \[lYansfer Rules for Each Sub- 
structm:e 
queue prioritized by score. The pro- 
cedure iteratively combines the best 
s(:oring edge with some other comt)al;ilfle 
edge to t)roduce a new edge. and inserts the new 
edge in the queu('.. The score for each new edge 
is a function of the scores of the edges used to 
produce it:. The process contimms m~til either 
an edge limit is reache(l (the system looks like 
it; will take too long to terminate) or a complete 
edge is t)roduced whose left-hand side is the 
input tree: we (:all this edge a "finished edge". 
We use the tbllowing technique for calculating 
the score tbr initial edges. 6 The score tbr each 
initial edge E rooted at N, based on rule/~, is 
calculated as follows: 
1. SCO17.F=I(S) " " F,.c.,~(n) = ~'?.q'~D~(~a  ~t N~) 
Where the fl'equency (Freq) of a rule is the 
nmnber of times it matched an exmnple in 
the training corpus, during rule ~cquisition. 
The denominator is the combined fl'equen- 
cies of all rules that match N. 
aThis is somewhat det)cndent on the way these |;rans- 
fer rules are derived. Other systems would t)robably have 
to use some other scoring system. 
Ezperiment 1:1155 sentences 
Norm No Norm 
Total Translations 
Over Edge Limit 
Actual Edges 
Miniature Edges 
Edge Ratio 
Accuracy 
1153 
2 
93,719 
22,125 
3.3 
70.9 
1127 
28 
579,278 
20,125 
1.4.8 
70.9 
Ezpcriment 2:2617 sentences 
Norm No Norm 
Total Translations 
Over Edge Limit 
Actual Edges 
Minimum Edges 
Edge Ratio 
A(:curacy 
2610 
7 
262,172 
48,570 
4.0 
62.6 
2544 
73 
1,398,796 
42,770 
15.5 
61.5 
Figure 5: Result:s 
2, S s ) = s ,o,.(;.l ( S ) - No,., , , ,  
Where the Norm (normalization) t~ctor is 
equal to the highest SCORE1 for any rule 
matching N. 
Since the log.2 of probabilities are necessarily 
negative, this has the effect of setting the E of 
each of the most t)rol)able initial edges to zero. 
The scores tbr non-initial edges are calculated 
by ad(ling u I) the scores of the initial e(tges of 
which they are comt)osed. 7 
Without any normMization (Score(S) = 
SCORE1 (,9)), small trees are favored over large 
trees. This slows down the process of finding the 
final result. The normalization we use insures 
that the most probable set; of transihr ules are 
considered early on. 
7 Resu l ts  
Figure 5 gives our results for both experiments 
1 and 2, both with normalization (Norm) and 
without (No Norm). "Total Translations" refer 
to the number of sen|;ences which were translat- 
ed successfully 1)y the system and "Over Edge 
Limit" refers to the numl)er of sentences which 
caused the system to exceed the edge limit, i.e., 
once the system produces over 10,000 edges, 
trm~slation failure is assmned. The system cur- 
7Scoring for special cases is not; included in this paper. 
These cases include rules for conjunctions and rules ibr 
words that do not match any transfer ules in a given 
context (we currently leave the word untranslated.) 
541 
rently will only fail to produce some transla- 
tion for any input if the edge limit is exceed- 
ed. "Actual Edges" reibrs to the total number 
of edges used tbr attempting to translate very 
sentence in the corpus. "Minimum Edges" refer 
to the total minimum number of edges required 
for successful translations. The "Edge Ratio" 
is a ratio between: (1) "Total Edges" less the 
mnnber of edges used in failed translations; and 
(2) The "Minimum Edges". This ratio, in com- 
l)ination with, the number of "Over Edge Limit" 
measures the efficiency of a given system. "Ac- 
curacy" is an assessment of translation quality 
which we will discuss in the next section. 
Normalization caused significant speed-up for 
both experiments. If you compare the total 
number of edges used with and without nor- 
malization, speed-up is a factor of 6.2 for Ex- 
periment I and 5.3 for Experiment 2. If you 
compare actual edge ratios, speed-up is a factor 
of 4:.5 tbr Experiment 1 and 3.9 tbr Experiment 
2. In addition, the number of failed parses went 
down by a fhctor of 10 for both experiments. As 
should be expected, accuracy was virtually the 
same with and without normalization, although 
normalization <lid cause a slight improvemen- 
t. Normalization should produce the essentially 
the same result in less time. 
These results suggest that we can probably 
count on a speed-up of at least 4 and a signif 
icant decline in failed parses by using normM- 
ization. The ditferences in performance on the 
two corpora are most likely due to the degree of 
hand-tuning for Experiment 1. 
7.1 Our  Accuracy  Measure  
"Accuracy" in Figure 5 is the average of the 
tbllowing score for each translated sentence: 
ITNYu ~ TMSI 
1/2 x (ITNYuI + ITMsl) 
TNZU is the set of words in NYU's translation 
and TMS is the set of words in the original Mi- 
crosoft translation. If TNYU = "A B C D E" 
and TMS = "A B C F", then the intersection 
set "A B C" is length 3 (the numerator) and 
the average length of TNZU and TMS is 4 1/2 
(the denominator). The accuracy score equals 
3 + 4 1/2 = 2/3. This is a Dice coefficient com- 
parison of our translation with the original. It is 
an inexpensive nmthod of measuring the pertbr- 
mance of a new version of our system, hnprove- 
ments in the average accuracy score for our san> 
ple set; of sentences usually reflect an improve- 
ment in overall translation quality. While it is 
significant hat the accuracy scores in Figure 5 
did not go down when we normalized the scores, 
the slight improvement in accuracy should not 
be given nmch weight. Our accuracy score is 
flawed in that it cannot account for the follow- 
ing facts: (1) good paraphrases are perfectly ac- 
ceptable; (2) some diflbrences in word selection 
are more significant han others; and (3) errors 
in syntax are not directly accounted tbr. 
NYU's system translates the Spanish sen- 
tence "1. Selection la celda en la que desea 
introducir una rethrencia" as "1. select the cel- 
l that you want to enter a reference in". Mi- 
crosoft translates this sentence as "1. Select the 
cell in which you want; to enter the reference". 
Our system gives NYU's translation an accu- 
racy score of .75 due to the degree of overlap 
with Microsoft's translation. A truman reviewer 
wouhl probably rate NYU's translation as com- 
pletely acceptable. In contrast, NYU's system 
produced the following unacceptable translation 
which also received a score of .75: the Spanish 
sentence "Elija la funcidn que desea pegar en la 
f6rmula en el cuadro de di~logo Asistente para 
flmciones" is translated as " "Choose the flmc- 
tion that wants to paste Function Wizard in the 
formula in the dialog box", in contr,~st with Mi- 
crosoft's translation "Choose the flmction you 
want to paste into the tbrmula fl'om the Func- 
tion Wizard dialog box". In fact, some good 
translations will get worse scores than some 
bad ones, e.g., an acceptable one word trans- 
lation can even get a score of 0, e.g.,"SUPR" 
was translated as "DEL" by Microsoft and as 
"Delete" by NYU. Nevertheless, by averaging 
this accuracy score over many examples, it has 
proved a valuable measure for comparing differ- 
ent versions of a particular system: better sys- 
tems get better results. Similarly, after tweak- 
ing the system, a better translation of a partic- 
ular sentence will usually yield a better score. 
8 Future  Work  
Fnture work should address two limitations of 
our current system: (1) Bad parses yield bad 
transihr rules; and (2) sparse data limits the size 
of our transfer rule database and our options for 
542 
applying transfer ules selectively. To nttack the 
"bad parse" problem, we are eonsideriug using 
our MT system with less-detailed parsers, since 
these parsers typically produce less error-prone 
output. We will have to conduct exl)erimcnts 
to determine the minimum level of detM1 that 
is needed, a 
Previous to the work reported in this paper, 
we ran our MT system on bilinguM corpora in 
which the sentences were Migned manuMly. The 
cost of manuM aligmnent limited the size of the 
corpora we could use. A lot of our recent MT 
research as bo.en tbcused on solving this sparse 
data prol)lem through our develoi)ment of a sen- 
tence alignment progrmn (Meyers et al, 1998a). 
We now have 300,000 automaticMly aligned sen- 
tences in the Microsoft help text domain tbr fu- 
ture experiineni;s. In addition to provi(ting us 
with many more transfer ules, this shouhl Mlow 
us to colh'.ct transfer rule co-occurrence infor- 
mation which we c~m then use to apply tr;mstbr 
rules more effectively, perhaps improving trans- 
b~tion quality. In a preliminary experime, nt a- 
hmg these lines using the Experiment 1. tort)us, 
co-occurrence information had no noticeable f  
feet. However, we are hot)eflfl that flltm'e ex- 
t)eriments with 300,000 Migned sentences (300 
tinies as nnlch data) will 1)e more successful. 
Re ferences  
Robert J. Bobrow. 1990. S1;~Ltistical agenda 
parsing. In I)ARPA Speech and Lang'uagc 
Workshop, pages 222-224. 
Peter Brown~ Stephen A. Delb~ t)ietra, Vin- 
cent J. Della Pietra, and Robert L. Mer- 
cer. 1993. The Mathematics of Statistical 
M~zchine 'h'anslation: 1)arametcr Estimation. 
Computational Lin.quistics, 19:263-312. 
Sh;~ron A. Caraballo and Eugene Chm'niak. 
1997. New figures of merit tbr best-tirst prot)- 
M)ilistie chart parsing. Computational Lin- 
guistics, 24:275-298. 
Eugene Ctmrniak, Sharon Goldwater, and M~rk 
Johnson. 1998. Edge-Based Best-First Chart 
Parsing. In Proceedings of the Sixth Annual 
Workshop for Very Lawc Corpora, Montreal. 
SOne could set u 1) a contimmm from detailed parser- 
s like Proteus down to shallow verb-group/noun-grouI) 
recognizers, with the Penn treetmnk based parsers ly- 
ing somewhere in the middle. As one travels down t, he 
eonLinlmIn t;o t;he lower detail parsers, tim error rate nat- 
urally decreases. 
Mahesh V. Chitrao and RMph Gris}unan. 1990. 
St;~tisti('al pnrsing of messages. In \])AIIPA 
Speech and La'n,g'uagc Workshop, pages 263 
266. 
Annn Sggvall ltein. 1996. Pretbrence Mecha- 
nisms of the Multra Machine %'ansb~tion Sys- 
tem. In Barbara H. Partee and Petr Sgall, 
editors, Discourse and Meaning: Papers in 
11onor of Eva 11aji~ovd. John Benja.mins Pub- 
lishing Company, Amsterdam. 
Y. Matsumoto, H. Ishimoto, T. Utsuro, and 
M. Nagao. 1993. Structural Matching of 
Parallel Texts. In 31st Annual Meeting of 
the Association for Computational Linguis- 
tics: "Proceedings of the Uo~@rencc". 
Adam Meyers, Roman Ymlgm'ber, a.nd Ralph 
Grishman. 1996. Alignment of Shared 
Forests fi)r BilinguM Corpora. In Proceedings 
of Coliw.I 1996: The 16th International Con- 
fercncc on Computational Linguistics, l)ages 
460 465. 
Adam Meyers, Miehiko Kosak~, and Ralph Gr- 
ishman. 1998m A Multilingual Procedure 
for Dict;ionary-B;~sed Sentence Aligmnent. In 
Proceedings of AMTA '98: Machine Transla- 
tion and th, c ht:fo'rmation Soup, t)~ges 187. 
198. 
Adam Meyers, R,om~m Ym~g~rber, Ralph Gr- 
ishmml, Cntherine Macleod, mM Antonio 
Moreno-S~mdow~l. 1998|). l)eriving ~l~:a.ns- 
fin: Rules from Domimmce-Preserving Align- 
ments. In I)'rocccdim.ls o.f Coling-A CL98: Th.c 
171h International Conference on Computa- 
tional Ling,uistics and the 36th, Meeting of the 
Association for Computational Linguistics. 
Sergei Nirenlmrg mM Robert E. l~:ederking. 
1995. The Pangloss Mark III Machine 'l?nms- 
lt~tion System: Multi-Engine System Archi- 
tecture. Te(:hnical report, NMSU Oil,L, USC 
ISI, ;rod CMU CMT. 
Andrew Way, Ian Crookston, and Jane Shell;on. 
1997. A Typology of ~IYanslation Prol)lems 
for Eurotra Translation Machines. Machine 
\[l}'anslation, 12:323 374. 
543 
Automatic Acquisition of Domain Knowledge for Information 
Extraction 
Roman Yangarber, Ralph Grishman Past Tapanainen 
Courant  Inst i tute of Conexor oy 
Mathemat ica l  Sciences Helsinki, F in land 
New York University 
{roman \[ grishman}@cs, nyu. edu Pasi. Tapanainen@conexor. fi 
Si!ja Ituttunen 
University of Helsinki 
F inland 
sihuttun@ling.helsinki.fi 
Abstract  
In developing an Infbrmation Extraction tIE) 
system tbr a new class of events or relations, one 
of the major tasks is identifying the many ways 
in which these events or relations may be ex- 
pressed in text. This has generally involved the 
manual analysis and, in some cases, the anno- 
tation of large quantities of text involving these 
events. This paper presents an alternative ap- 
proach, based on an automatic discovery pro- 
cedure, ExDIsCO, which identifies a set; of rele- 
wmt documents and a set of event patterns from 
un-annotated text, starting from a small set of 
"seed patterns." We evaluate ExDIScO by com- 
paring the pertbrmance of discovered patterns 
against that of manually constructed systems 
on actual extraction tasks. 
0 Introduct ion 
Intbrmation Extraction is the selective xtrac- 
tion of specified types of intbrmation from nat- 
ural language text. The intbrmation to be 
extracted may consist of particular semantic 
classes of objects (entities), relationships among 
these entities, and events in which these entities 
participate. The extraction system places this 
intbrmation into a data base tbr retrieval and 
subsequent processing. 
In this paper we shall be concerned primar- 
ily with the extraction of intbrmation about 
events. In the terminology which has evolved 
ti'om the Message Understanding Conferences 
(muc, 1995; muc, 1993), we shall use the term 
subject domain to refer to a broad class of texts, 
such as business news, and tile term scenario to 
refer to tile specification of tile particular events 
to be extracted. For example, the "Manage- 
ment Succession" scenario for MUC-6, which we 
shall refer to throughout this paper, involves in- 
formation about corporate executives tarting 
and leaving positions. 
The fundamental problem we face in port- 
ing an extraction system to a new scenario is 
to identify the many ways in which intbrmation 
about a type of event may be expressed in the 
text;. Typically, there will be a few common 
tbrms of expression which will quickly come to 
nfind when a system is being developed. How- 
ever, the beauty of natural language (and the 
challenge tbr computational linguists) is that 
there are many variants which an imaginative 
writer cast use, and which the system needs to 
capture. Finding these variants may involve 
studying very large amounts of text; in the sub- 
ject domain. This has been a major impediment 
to the portability and performance of event ex- 
traction systems. 
We present; in this paper a new approach 
to finding these variants automatically fl'om a 
large corpus, without the need to read or amLo- 
tate the corpus. This approach as been evalu- 
ated on actual event extraction scenarios. 
In the next section we outline the strncture of 
our extraction system, and describe the discov- 
ery task in the context of this system. Sections 
2 and 3 describe our algorithm for pattern dis- 
covery; section 4 describes our experimental re- 
sults. This is tbllowed by comparison with prior 
work and discussion in section 5. 
1 The Extract ion System 
In the simplest terms, an extraction system 
identifies patterns within the text, and then 
mat)s some constituents of these patterns into 
data base entries. (This very simple descrip- 
lion ignores the problems of anaphora nd in- 
tersentential inference, which must be addressed 
by any general event extraction system.) AI- 
though these l)atterns could in principle be 
stated in terms of individual words, it is much 
940 
easier to state them in terms of larger SylltaC- 
tic constituents, uch as noun phrases and verb 
groups. Consequently, extraction ormally con- 
sists of an analysis of the l;e.xt in terms of general 
linguistic structures and dolnain-specifio con- 
structs, tbllowed by a search for the scenario- 
specific patterns. 
It is possible to build these constituent struc- 
tures through a flfll syntactic analysis of the 
text, and the discovery procedure we describe 
below woul(1 be applicable to such an architec- 
ture. Howe, ver, for re&sellS of slme,(t , coverage, 
and system rolmstness, the more (:ommon ap- 
t)roa(:h at present is to peribrni a t)artial syn- 
tactic analysis using a cascade of finite-state 
transducers. This is the at)t)roa(:h used by our 
e.xtraction system (Grishman, 1995; Yangarber 
and Grishman, 1998). 
At; the heart of our syslx'an is a regular ex- 
pression pattern matcher which is Cal)al)le of 
matching a set of regular exl)ressions against 
a partially-analyzed text and producing addi- 
tional annotations on the text. This core draws 
on a set of knowledge bases of w~rying degrees 
of domain- and task-specificity. The lexicon in- 
cludes both a general English dictionary and 
definitions of domain and scenario terms. The 
concept base arranges the domain terms into 
a semantic hierarchy. The predicate base. de- 
s('ribes the, logical structure of I;he events to be 
extracl;od. 'Fire pattern \])ase consists of sets of 
patterns (with associated actions), whi(;h make 
r(;ferollCO to information Kern the other knowl- 
e(lge bases. Some t)attorn sots, su(:h as those for 
n(mn and verb groups, are broadly apl)licable , 
wlfile other sets are spe(:ifio to the scenario. 
V~Ze, have previously (Yangarl)er and Grish- 
man, 1.997) (lescrit)ed a user interface which 
supt)orts the rapid cust;omization of the extrac- 
tion system to a new scenario. This interface 
allows the user to provide examples of role- 
wmt events, which are automatically converted 
into the appropriate patterns and generalized to 
cover syntactic variants (passive, relative clause, 
etc.). Through this internee, the user can also 
generalize l;he pattern semanti('ally (to (:over a 
broader class of words) and modify the concet)t 
base and lexicon as needed. Given an appro- 
priate set; of examples, thereibre, it; has become 
possible to adapt the extraction system quite 
ral)idly. 
However, the burden is still on the user to 
find the appropriate set of examples, which may 
require a painstaldng and expensive search of a 
large corpus. Reducing this cost is essential for 
enhanced system portability; this is the problem 
addressed by the current research. 
Ilow can we automatically discover a suitable 
set; of candidate patterns or examples (patterns 
which at least have a high likelihood of being 
relevant to the scenario)? The basic idea is to 
look for linguistic patterns which apt)ear with 
relatively high frequency in relevant documents. 
While there has been prior research oll idea|i- 
lying the primary lexical t)atterns of a sublan- 
guage or cortms (Orishman et al 1986; Riloff, 
1996), the task here is more complex, since we 
are tyt)ically not provided in advance with a 
sub-corpus of relevmlt passages; these passages 
must themselves be tbund as part of t;t1(; discov- 
ery i)rocedure. The difficulty is that one of the 
l)est imlic~tions of the relevance of the passages 
is t)recisely the t)resence of these constructs. Bo- 
(:ause of this (:ircularity, we l)ropose to a(:quire. 
the constructs and t)assagos in tandem. 
2 ExDISCO: the  D iscovery  P rocedure  
We tirst outline ExDIsco ,  our procedure for 
discovery of oxl,raction patterns; details of some 
of the stops arc l)rcse, nted in the section which 
follows, and an earlier t)~q)er on our at)l)roach 
(Yang~u:bcr ot al., 2000). ExDIscO is mi ml- 
supervised 1)rocedure: the training (:ortms does 
not need to t)e amlotated with the specific event 
intbrmatkm to be. e.xtracted, or oven with infor- 
mation as to whi(;h documents in the ('orpus are 
relevant o the scenario. 'i7tlo only intbrmation 
the user must provide, as described below, is a 
small set of seed patterns regarding the s(:enario. 
Starting with this seed, the system automati- 
(:ally pertbnns a repeated, automatic expansion 
of the pattern set. This is analogous to the pro- 
cess of automatic t;enn expansion used in s()me 
information retrieval systems, where, the terlns 
Dora the most relewmt doculncnts are added 
to the user query and then a new retriewfl is 
imrformed. However, by expanding in terms of 
1)atl;erns rather than individual terms, a more 
precise expansion is possit)le. This process pro- 
coeds as tbllows: 
0. We stm:t with a large, corlms of documents 
in the domain (which have not been anne- 
941 
tared or classified in any way) and an initial 
"seed" of scenario patterns selected by the 
user - -  a small set of patterns whose pres- 
ence reliably indicates thai; the document 
is relevant o the scenario. 
. The pattern set is used to divide the cor- 
tins U into a set of relewmt documents, R
(which contain at; least one instance of one 
of the patterns), and a set of non-relevant 
documents R = U - R. 
2. Search tbr new candidate patterns: 
? automatically convert each document 
in the eorIms into a set of candidate 
patterns, one for each clause 
? rank patterns by the degree to which 
their distribution is correlated with 
docmnent relevance (i.e., appears with 
higher frequency in relevant docu- 
ments than in non-relewmt ones). 
3. Add the highest ranking pattern to the pat- 
tern set. (Optionally, at this point, we may 
present he pattern to the user for review.) 
4. Use the new pattern set; to induce a new 
split of the corpus into relevant and non- 
relevant documents. More precisely, docu- 
ments will now be given a relevance confi- 
dence measure; documents containing one 
of the initial seed patterns will be given 
a score of 1, while documents which arc 
added to the relevant cortms through newly 
discovered patterns will be given a lower 
score. I/,epeat the procedure (from step 1) 
until some iteration limit is reached, or no 
more patterns can be added. 
3 Methodo logy  
3.1 Pre-processing: Syntact ic Analysis 
Before at)plying ExDIsco ,  we pre-proeessed 
the cortms using a general-purpose d pendency 
parser of English. The parser is based on 
the FDG tbrmalism (Tapanainen and Jgrvi- 
hen, 1997) and developed by the Research Unit 
for Multilingual Language Technology at the 
University of Helsinki, and Conexor Oy. The 
parser is used ibr reducing each clause or noun 
phrase to a tuple, consisting of the central ar- 
guments, ms described in detail in (Yangarber 
et al, 2000). We used a corlms of 9,224 articles 
from the Wall Street; Journal. The parsed arti- 
cles yielded a total of 440,000 clausal tuples, of 
which 215,000 were distinct. 
3.2 Normal izat ion 
We applied a name recognition module prior to 
parsing, and replaced each name with a token 
describing its (:lass, e.g. C-Person, C-Company, 
etc. We collapsed together all numeric expres- 
sions, currency wflues, dates, etc., using a single 
token to designate ach of these classes. Lastly, 
the parser performed syntactic normalization to 
transtbrm such variants ms the various passive 
and relative clauses into a common tbrm. 
3.3 General izat ion and Concept Classes 
Because tuples may not repeat with sufficient 
frequency to obtain reliable statistics, each tu- 
ple is reduced to a set of pints: e.g., a verb- 
object pair, a subject-object pair, etc. Each pair 
is used as a generalized pattern during the can- 
didate selection stage. Once we have identitied 
pairs which are relevant o the scenario, we use 
them to gather the set; of words for the miss- 
ing role(s) (tbr example, a class of verbs which 
occur with a relevant subject-ot@ct pair: "com- 
pany {hire/fire/expel...} person"). 
3.4 Pat tern  Discovery 
We (-onducte(1 exi)eriments in several scenarios 
within news domains such as changes in cor- 
porate ownership, and natural disasters. Itere 
we present results on the "Man~geme.nt Suc- 
cession" and "Mergers/Acquisitions" cenarios. 
ExDIsco  was seeded with lninimal pattern sets, 
namely: 
Subject Verb Direct Object 
C-Company C-At)point C-Person 
C-Person C-Resign 
ibr the Mmmgement task, and 
Subject Verb Direct Object 
* C-Buy C-Conlt)any 
C-Company merge * 
for Acquisitions. Here C-Company and C- 
Person denote semantic classes containing 
named entities of the corresponding types. C- 
Appoint denotes the list of verbs { appoint, elect, 
promote, name, nominate}, C-Resign = { re- 
sign, depart, quit }, and C-Buy = { buy , pur- 
chase }. 
942 
\ ])uring ~ single iter~tion, we conqmt(; the 
score, See're(p), for each cm~(lidate 1)attern p, 
using (;he fornmla~: 
S, :o ' , ' ,@)  = IH n l~l 
IHI - 1,,~ IHn  ~.1 (:t) 
where 12. (Icnotes (;h(', l'clewmt subsc(; of docu- 
ments, mid I t=  It(p) the, ( locmnents imttching 
p, as above; the Iirst (;erm a(:(:ounts for the con- 
(lition~fl t)robabil ity of relev;m('e oil p~ and |;11(; 
second tbr its support .  We further impose two 
support criteria: we distrust such frequent pat- 
(;,~.,-.~ w\]le,:e I1~ n UI > ,~IUI, ,~ uninforn,,~tive, 
mid rare patte.rns \['or which I1\] r-I \]~.1 < fl as 
noise. 2 At the end ot' (.aeh il;eratiol~, the sysl;em 
selects the pal;tern with the highest Sco'/'d(p)~ 
and adds it (;o (;lie seed scl;. The (to(:un~enl;s 
which t;he winning t)~(;t;ern hits are added (;o 
t;111(; relevant set. The  t)al;l;(;rn s(;areh is then 
r(;sl;m:l;(;d. 
3.5 Document  Re- rank ing  
Th(: above is a simt)lifi(';~l;ion of (;he a,(:tual pro- 
cedlll'(}~ in severa\] r(',st)e('(;s. 
Only generalized t)ntl;erns are (:onsidered fi)r 
(:audi(t~my, with one or mot(', slol;s fill(:(1 wi(;h 
wihl-cm'ds. In comput ing the score of th(', ge, n- 
(;raliz(:d \]);tttern, w(: do not take into ('onsi(h:r- 
;i,1;i()11 all possible va,hw, s of the, wil(1-('m:d role. 
\?e instea.d (:()llS(;raJll (;he wild-(:ar(l to thos(~ wd- 
u(:s wlli(:h l;ht',llls(;lv(;s ill (;llrH \]l;tV(: high scores. 
Th(:se v~du(:s l;lw, n |)e(:on~e lllClll\])(;l'S of }/. II(:W 
(:lass, whi(:h is l)rOdu(:ed in (;:tlldClll with the 
wimfing 1)att(:rn. 
\])o('umel~tS reh:wm('e is s(-ored (m ~ s(;ah: l)e- 
(;ween 0 and 1. Tlm seed t)atterns a.re a.(:cet)ted 
~,s trut\]~; the do('mlw, nts (;hey mat(:\]1 hnve rcle- 
vmme 1. On i(;er;~tion i + 1, e~mh t)a(;tern p is 
assigned a precision measure, t)ase(l on the rel- 
(':Vall(;e of |;11(; (locllnlelfl;s i|; 111a, l;(;ll(',,q: 
~ ".d~(d) (~) 
f f , , :d  +~ (v)  - -  IH(v) l ,~.(,,) 
where l~,eli(d) is the re, levmlce of' 1;11(: doeunmn(; 
fi'om t;t1(', previous iteration, ~md l I(p) is the set 
of documents where p matched, in general, if K 
is a classifier (:onsisting of ~ set of l)al;terns, w(', 
define H (K)  as the st:l; of documents  where all 
~similar to that used in (liiloff, 1996) 
~W(: used ,:-- 0.1 and fl = 2. 
of t)~d;terns p C K m~l;(:h, mid the "cunmlative" 
precision of K as 
1 ~ 1~4~(a,) (3) P~.~d +~(1() = IH U()I <.(K)  
Once the wimfing pa,l;l;ern is accepted, the rel- 
ewmee of the documents is re-adjusted. For 
(;~mh document  d which is matched by some 
subset of l;he currently accet)t('d pntterns, we 
can view thai; sul)s(',t; of  l)~tterns as ~ classitier 
Kd = {pj}. These  patterns (tel;ermilm the new 
reh;wmce score of the document  as 
J~, "~l,~ " ( ,0  : 111~x (:tc,,.1,*(,O,v,.,;, .~" (K , ) )  (~:) 
This ensures tha.(; l;he rclewmce score grows 
monotonical ly, and only when there is sufliei(mt 
positive evidence, as (;he i)ntterns in etl'e(:I; vote 
"conjmmtively" on the (loculncnl;s. 
We also tr ied an alternative, ::disjun(:tive" 
voting scheme, with weights wlfich accounts tbr 
vm:intion in support of the p~ttterns, 
J,.,.1, (d) . . . .  ~ "~ I I  (1 - ~',.~,.c~(p))"",' (5) 
~c K(d) 
where t;11(', weights ,wp arc (tetint;d using the tel- 
ewm(:(: of the (loeuments, a,s the total  SUl)l)or(; 
which the pa, I;I;ern p receives: 
% = log ~ l;.d,(d) 
dE 11 (p) 
and ;,7 is (;11(' largest weight. The  r(',cursive for- 
nmb~s ('apl;m:e (;he mul;u~fl dependency of t)~t- 
terns ~md documents;  this re-computat ion ~md 
growing of precision and relevmlce rmlks is the 
core of the t)rocedure. :~ 
4 Resu l ts  
4 .1  Event  Ext ract ion  
'l'he, most nal;m'a.l measm'e of efl'ecl;iveness of our 
discovery procedure is the performmme of ml ex- 
tract ion systmn using the, discovered t)~tterns. 
However, il; is not 1)ossil)le to apply this reel;- 
rio direei;ly because the discovered t)al;terns lack 
some of the information required tbr entries ill 
:{\V('. did not el)serve a significam; difl'erencc in 1)crfi)r- 
lIiHl\[CO, bet, ween the two tormulas 4 alt(t 5 in o111" experi- 
in(mrs; the results whit:h tbllow use 5. 
943 
the pattern base: information about the event 
type (predicate) associated with the pattern, 
and the mapping from pattern elements to pred- 
icate arguments. We have evaluated ExDIsco  
by manually incorporating the discovered pat- 
terns into the Proteus knowledge bases and run- 
ning a full MUC-style evaluation. 
We started with our extraction system, Pro- 
tens, which was used in MUC-6 in 1995, and 
has undergone continual improvements since 
the MUC evaluation. We removed all the 
scenario-specific clause and nominalization pat- 
terns. 4 We then reviewed all the patterns which 
were generated by the ExDIsco,  deleting those 
which were not relewmt to the task, or which 
did not correspond irectly to a predicate al- 
ready implemented tbr this task)  The remain- 
ing pat;terns were augmented with intbnnation 
about the corresponding predicate, and the re- 
lation between the pattern and the predicate 
al'guments, a The resulting variants of Proteus 
were applied to the formal training corpus and 
the (hidden) formal test corpus for MUC-6, and 
the output evaluated with the MUC scorer. 
The results on the training corpus are: 
Pattern Base Recall Precision 
Seed 38 83 
Ex I ) Isco 62 80 
Union 69 __79 
Manual-MUC ~ 71 L~1.9~ 
Manual-NOW 6(3~ 79 L7!~z\[)_t_j 
and on the test cortms: 
4There are also a few noun phrase patterns which can 
give rise to scenario events. For example, "Mr Smith, 
former president of IBM", may produce an event record 
where l%ed Smith left IBM. These patterns were left in 
Proteus for all the runs, and they make some contribu- 
tion to the relatively high baseline scores obtained using 
just the seed event patterns. 
~ExD~sco f und patterns which were relevant to the 
task lint could not be easily aceomodated in Proteus. 
For instance "X remained as president" could be rele- 
vant, particularly in the case of a merger creating anew 
corporate ntity, but Proteus was not equipped to trun- 
dle such iIfformation, and has not yet been extended to 
incorporate such patterns. 
6As with all clause-level patterns in Proteus, these 
patterns m-e automatically generalized tohandle syntac- 
tic wn'iants uch as passive, relative clause, etc. 
Pattern Base Recall Precision F 
Seed 27 74 39.58 
ExDIsco 52 72 60.16 
Union 57 73 63.56 
Manual-NOW -- 56 75 6404. 
The tables show the recall and precision mea- 
sures for the patterns, with F-measure being 
the harmonic mean of the two. The Seed pat- 
tern base consists of just the initial pattern set, 
given in the table on the previous page. ~ib this 
we added the patterns which the system discov- 
ered automatically after about 100 iterations, 
producing the pattern set called ExDIsco.  For 
comparison, M anual-MUC is the pattern base 
lnanually develot)ed on the MUC-6 training 
corpus-1)repared over the course of 1 month 
of full-time work by at least one computational 
linguist (during which the 100-document train- 
ing corpus was studied in detail). The last row, 
Manual-now, shows the current pertbrmance of
the Proteus system. The base called Ultiolt con- 
tains the union of ExDIScO and Manual-No'w. 
We find these results very encouraging: Pro- 
teus performs better with the patterns discov- 
ered by ExI)IscO than it did after one month 
of manual tinting and development; in fact, this 
perfi)rmance is close to current levels, which 
are the result of substantial additional devel- 
opmeut. These results umst be interpreted, 
however, with several caveats. First, Proteus 
performance depends on many fimtors besides 
the event patterns, such as the quality of name 
re, cognition, syntactic mmlysis, anaphora reso~ 
lution, inferencing, etc. Several of these were 
improved since the MUC formal evaluation, so 
some of the gain over the MUC formal evalua- 
tion score is attritmtable to these factors. How~ 
ever, all of the other scores are comparable in 
these regards. Second, as we noted above, the 
patterns were reviewed and augmented manu- 
ally, so the overall procedure is not entirely au- 
tomatic. However, the review and augmenta- 
tion process took little time, as compared to 
the manual corpus analysis and development of
the pattern base. 
4.2 Text  f i l ter ing 
We can obtain a second measure of pertbr- 
mance by noting that, in addition to growing 
the tmttern set, ExDIsco  also grows the rele- 
944 
0.9 
0.8 
0.7 
0.6 
0.5 
_ . r -~H . . . . . . . . . . . . . . . . .  r . . . . . . .  T ~ ~ : : ~  T 
;!\ >. g~t : 
- ' il 
%i 
\[!\] 
7 
\[!J 
Legend: 
Management/Test ? .-{~ ...... 
ManagemenVl-raie - :*: -- 
MUC-6 ? 
0.2 0.4 0.6 0.8 
Recall 
Figure l: Management Suc('cssion 
0.9 
0.8 
0.7 
0.6 
0.5 
L_~/r 
Legend: 
Acquisition 
0.2 0.4 0.6 
Recall 
0.8 
Figme 2: Mergers/A(:quisitions 
vance rankings of documents. The latter cnn be 
evahlated irectly, wil;hollt human intervention. 
We tested Exl)IsC, o ~tgainst wo cor\])orn: th(; 
100 documents from MUC-6 tbrmal training, 
a:nd the 100 documents from the MUC-6 for- 
mal test (both are contained anlong the 10,000 
ExDIsoO training set) r. Figure 1 shows recall 
t)\]otted against precision on the two corpora, 
over 100 iterations, starting with the seed pat- 
te, nls in section 3.d. This view on the discovery 
procedure is closely related to the MUC %ext- 
till;ering" task, in which the systems are jlulged 
at the \]evel of doc,wm, e,'nt.s rather thmt event slots. 
It; is interesting to (:omt)m:e Exl)IsCO's results 
with how other MUC-6 part\]tit)ants performed 
on the MUC-b '  test cortms , shown anonymously. 
ExDIscO attains values within the range of 
the MUC participald;S, all of which were either 
heavily-supervised or m~mually coded systems. 
II; is important to bear in mind that Ex I ) I sco  
had no benefit of training material, or any in- 
tbrmation beyond the seed pattern set. 
Figure 2 shows the 1)ertbrmance, of text fil- 
tering on the Acquisition task, again, given the 
seed in section 3.4. ExDisco  was trained on 
|;lie same WSJ eorlms, and tested against a set 
of 200 documents. We retrieved this set using 
keyword-based IR, search, and judged their rel- 
evance by halId. 
rThesc judgements constituted the truth which was 
used only for evaluation, not visible to ExDISCO 
5 Discuss ion  
The development of a w~riety of information 
extra(:tion systems over the last decade has 
demonstrated their feasibility but also the lim- 
itations on their portability and t)erformance. 
Prcl)aring good t)atterns tbr these syste, ms re- 
quires (:onsiderable skill, and achieving good 
(:overage requires |;lie analysis of a large amount 
of text. These t)rol)lems h~ve t)een impedinmnts 
to the -wide\].' use of extraction systenls. 
These dit\[iculties have stimulate.d resear('h on 
1)attel . 'n a ( : ( lu i s i t ion .  So lne  o f  th i s  work  has  en l -  
i)hasized il\]teractive tools to (:onvert examples 
to extractioi~ t)atterlls (Yangarber and Grish- 
man, 1997); nmch ot:' the re, search has focused on 
methods for automatically converting a cortms 
annotated with extraction examples into pat- 
terns (Lehnert et al, 1992; Fisher et al, 1995; 
Miller el; al., 1998). These techniques may re- 
duce the level of systeln expertise required to 
develop a new extraction N)plieation, but they 
do not lessen the lmrden of studying a large cor- 
lms in order to .find relevant candidates. 
The prior work most closely related to our 
own is that of (R.ilotf, 1996), who also seeks to 
lmild pattenls automatically without the need 
to annotate a corpus with the information to 
be extracted. Itowever, her work ditfers t'rom 
01217 own in several i lnportant respects. First, 
her patterns identit~y phrases that fill individual 
slots in the template, without specifying how 
these slots may be combined at a later stage 
into complete templates. In contrast, our pro- 
cedure discovers complete, multi-slot event pat- 
945 
terns. Second, her procedure relies on a cort)us 
in which |;tie documents have been classified for 
relevance by hand (it was applied to the MUC-3 
task, tbr which over 1500 classified documents 
are available), whereas ExDIsco requires no 
manual relevance judgements. While classify- 
ing documents tbr relevance is much easier than 
annotating docunlents with the information to 
be extracted, it; is still a significant ask, and 
places a limit on |:tie size of the training corpus 
that can be effectively used. 
Our research as demonstrated that for the 
studied scenarios automatic pattern discovery 
Call yield extraction perfi)rmance colnt)arabh~ to
that obtained through extensive corpus anal- 
ysis. There are many directions in which the 
work reported here needs to be extended: 
? nsing larger training corpora, in order to 
find less frequent exanlplcs, and in that way 
hopefully exceeding the i)erfornlancc of our 
best hand-trained system 
? cat)luring the word classes which are gen- 
erated as a by-product of our pattern dis- 
covery 1)rocedure (in a manner similar to 
(Riloff and ,Jones, 1999)) and using them 
to discover less frequent )atterns in subse- 
quent iterations 
- evaluating the effectiveness of the discov- 
cry procedure on other scenarios. In par- 
titular, we need to be able to identi\[y top- 
its which cast be most effbctively charac- 
terized by clause-level patterns (as was the 
case tbr the business domain), and topics 
which can be better characterized by other 
means. We. wouM also like to understand 
how the topic clusters (of documents and 
patterns) which are developed by our pro- 
cedure line up with pre-specified scenarios. 
References 
David Fisher, Stephen Soderland, Joseph Mc- 
Carthy, Fangfang Feng, and Wendy Lelmert. 
1995. Description of the UMass system as 
used fbr MUC-6. In Prec. Sixth Message Un- 
dcrstandin9 Conf. (MUC-6), Columbia, MD, 
November. Morgan Kauflnann. 
R.alph Grishman. 1995. The NYU systenl tbr 
MUC-6, or where's the syntax? Ill Prec. 
Sixth Message Understanding Conf. (MUC- 
6), pages 167 176, Columl)ia, MD, Novem- 
ber. Morgan Kauflnann. 
W. Lehnert, C. Cardie, D. Fisher, J. McCarthy, 
E. Riloff, and S. Soderland. 1992. Univer- 
sity of nlassachusetts: MUC-4 test results 
and analysis. Ill P,'oe. Fourth Message Un- 
der.standing Con.\[., McLean, VA, June. Mor- 
gan Kauflnaml. 
Scott Miller, Michael Crystal, Heidi Fox, 
Lance II,amshaw, R,ichard Schwartz, Rebecca 
Stone, Rall)h Weischedel, and the Annota- 
tion Group. 1998. Algorithms that learn to 
extract intbrmation; BBN: Description of the 
SIFT systenl as used for MUC-7. In PTve. 7th 
Mc.ssagc Understanding Co~:f., FMrfax, VA. 
1993. Proceedings of the F'~ifth Message UTz.- 
derstanding Confer(race (MUC-5), Baltimore, 
MD, August. Morgan Kauflnann. 
1995. PTveeedings of the Sixth Message U~I,- 
derstav, ding Conference (MUC-6), Colmnt)ia, 
MD, November. Morgan Kauflnaml. 
Ellen Rilotf and Rosie Jones. 1999. Learn- 
ing dictionaries for infbrmation extraction by 
multi-level bootstrat)ping. In Prec. 16th Nat'l 
Cord'erenee on Art'~i\[icial Intelli9enee (AAA I 
99), Orlando, Florida. 
Ellen Riloff. 1996. Automatically generating 
extraction patterns from m~tagged text. In 
Prec. I3th Nat'l Co~~:f. on Art~ificial Intel- 
ligence (AAAI-96). The AAAI Press/MIT 
Press. 
l?asi '\])~panainen a d Time .J/h:vinen. 1997. A 
non-t)rojectivc dependency parser. In P'mc. 
5th Conf. on Applied Nat'aral Language P~v- 
cessiu9, pages 64-71, Washington, D.C. ACL. 
Roman Yangarber and RalI)h Grishman. 1997. 
Customization of intbrmation extraction sys- 
tems. In Paola Velardi, editor, I~tt'l Work- 
shop on Lexically Driven I~7:forrnation Extrac- 
tion, Frascati, Italy. Universith di Roma. 
Roman Yangarl)er and Ralph Grishman. 1998. 
NYU: Description of thc Protens/PET sys- 
tem as used tbr MUC-7 ST. In 7th Message 
Understanding Conference, Columbia, MD. 
Roman Yangarl)er, Ralph Grishman, Past 
Tapanainen, and Silja Huttunen. 2000. Un- 
supervised discovery of scenario-level pat- 
terns tbr information extraction. Ill PTve. 
Co~@ on Applied Nat'aral Lang'aage Pr'ocess- 
tug (ANLP-NAACL), Seattle, WA. 
946 
Unsupervised Learning of Generalized Names
Roman Yangarber, Winston Lin, Ralph Grishman
Courant Institute of Mathematical Sciences
New York University
froman|winston|grishmang@cs.nyu.edu
Abstract
We present an algorithm, Nomen, for learning
generalized names in text. Examples of these
are names of diseases and infectious agents, such
as bacteria and viruses. These names exhibit
certain properties that make their identica-
tion more complex than that of regular proper
names. Nomen uses a novel form of bootstrap-
ping to grow sets of textual instances and of
their contextual patterns. The algorithm makes
use of competing evidence to boost the learning
of several categories of names simultaneously.
We present results of the algorithm on a large
corpus. We also investigate the relative merits
of several evaluation strategies.
1 Introduction
This research grew out of the Integrated Feasi-
bility Experiment on Biological Infectious Out-
breaks (IFE-BIO), a project to build an Infor-
mation Extraction (IE) system for identifying
events related to outbreaks and epidemics of in-
fectious disease, (Grishman et al, 2002).
IE generally relies on knowledge bases of sev-
eral kinds, and the most fundamental of these is
the domain-specic lexicon|lexical items that
are not likely to be found in general-purpose
dictionaries. This particular scenario requires
a comprehensive list of disease names. Other
requisite classes of names include: biological
agents causing disease, such as viruses and bac-
teria; vectors|organisms or animals capable of
transmitting infection; and possibly names of
drugs, used in treatment.
1.1 Generalized Names
Names of these kinds, generalized names (GNs),
dier from conventional proper names (PNs)
that have been studied extensively in the lit-
erature, e.g., as part of the traditional Named
Entity (NE) categorization task, which evolved
out of the MUC NE evaluation, (Wakao et al,
1996; Bikel et al, 1997; Borthwick et al, 1998;
Collins and Singer, 1999). The three main-
stream NE kinds are location, person, and or-
ganization, and much research has centered on
these \classical" kinds of proper names.
On the other hand, the vast eld of termi-
nology has traditionally dealt with identifying
single- and multi-word domain-specic expres-
sions, for various NLP tasks, and recent years
have seen a growing convergence between the
two elds.
In fact, good identication of names of both
kinds is essential for IE in general. In IFE-BIO,
for example, the text:
National Veterinary Services Director Dr.
Gideon Bruckner said no cases of mad cow
disease have been found in South Africa.
exhibits more than one problem of name identi-
cation and classication. We focus on general-
ized names, which pose numerous challenges.
The classication process usually starts with
identication, but the primary cue for a proper
name|capitalization (in English text)|is un-
available for generalized names. GNs are not al-
ways capitalized (\mad cow disease" or \tuber-
culosis") or may be partially capitalized (\Ebola
haemorrhagic fever", \E. coli"). GNs often have
multiple pre- and post-modiers|\(new) vari-
ant Creutzfeldt-Jacob disease," or may modify
the head of a noun group|\Bacillus anthracis
infection." Locating the boundaries of GNs is
much harder than for PNs.
The problem of ambiguity aects generalized
names, as it does proper names. E. coli can
refer to the organism or to the disease it causes;
encephalitis can mean a disease or a symptom.
1.2 Why Learning?
Why is it undesirable to rely on xed, special-
ized, domain-specic lists or gazetteers?
1. Comprehensive lists are not easy to ob-
tain.
2. Lists are never complete, since new names
(locations, diseases) periodically enter into ex-
istence and literature.
3. A typical text contains all the information
that is necessary for a human to infer the cate-
gory. This makes discovering names in text an
interesting research problem in its own right.
The following section introduces the learning
algorithm; Section 3 compares our approach to
related prior work; Section 4 presents an evalu-
ation of results; we conclude with a discussion
of evaluation and current work, in Section 5.
2 Nomen: The Learning Algorithm
Nomen is based on a bootstrapping approach,
similar in essence to that employed in (Yangar-
ber et al, 2000).
1
The algorithm is trained on
a large corpus of medical text, as described in
Section 4.
2.1 Pre-processing
A large text corpus is passed through a zoner,
a tokenizer/lemmatizer, and a part-of-speech
(POS) tagger. The zoner is a rule-based
program to extract textual content from the
mailing-list messages, i.e., stripping headers and
footers. The tokenizer produces lemmas for the
inected surface forms. The statistical POS tag-
ger is trained on the Wall Street Journal (pos-
sibly sub-optimal for texts about infectious dis-
ease). Unknown or foreign words are not lem-
matized and marked noun by the tagger.
2.2 Unsupervised Learning
0. Seeds: The user provides several trusted
seeds of each category we intend to learn. E.g.,
we selected the 10 most common diseases as
seeds for the disease category; the same for lo-
cations and several other categories.
2
1
For a detailed comparison of the algorithms,
cf. (Yangarber, 2002).
2
Frequency counts are computed from a large IE
database, of more than 10,000 records. The most com-
mon disease names: cholera, dengue, anthrax, BSE, ra-
bies, JE, Japanese encephalitis, inuenza, Nipah virus,
FMD (for foot-and-mouth disease).
For each category, the set of accepted names,
AcceptName, is initialized with the seeds.
1. Tagging: For each accepted name in each
category C to be learned, Nomen tags the lem-
matized, POS-tagged training corpus, placing
left and right tags around each occurrence of
the name|e.g., <disease> and </disease>.
2. Pattern Generation: For each tag T
inserted in the corpus on Step 1, Nomen gener-
ates a literal pattern p using a context window
of width w around the tag, e.g.,
p = [ l
 3
l
 2
l
 1
<T> l
+1
l
+2
l
+3
]
where l
i
are the context of p|the lemmas of
the surrounding words.
Note, the tag of the pattern, Tag(p) = T , in-
dicates both a direction, either \left" or \right,"
Dir(p) 2 fleft; rightg, and a category, Cat(p).
E.g., if Tag(p) = </disease>, then Dir(p) =
right and Cat(p) = disease.
Then p is transformed replacing each element
in the w-window by its generalization; in the
current simple scheme, the only generalization
can be a wildcard. These patterns form the set
of potential patterns, . Note that each pattern
matches on only one side of an instance, either
its beginning or its end.
3. Pattern Matching: Match every pat-
tern p 2  against the entire training corpus.
In a place where the context of p matches, p
predicts where one boundary of a name in text
would occur. Let pos
a
be the position of this
boundary. Then use a noun group (NG) regu-
lar expression
3
to search for the other, partner
boundary, say, at position pos
b
. For example,
suppose p matches in the text
the
  
z }| {
h
1
yellow feveri
2
vaccinei
3
| {z }
 !
to villagers
at pos
a
= 2 and Dir(p) = right; then pos
b
= 1.
However, if pos
a
= 1 and Dir(p) = left then
pos
b
= 3. (Note, the search proceeds in the
opposite direction of Dir(p).) Next, we check
whether the NG between positions pos
a
and
pos
b
has already been accepted as a name in
some category; the result can be:
3
Using heuristics, as in terminology discovery,
(Frantzi et al, 2000); we use a simple NG regular ex-
pression, [Adj* Noun+].
 positive: The NG has already been ac-
cepted as a name in the same category as
Cat(p);
 negative: The NG has already been ac-
cepted as a name in a dierent category,
C
0
6= Cat(p);
 unknown: The NG has not yet been ac-
cepted as a name in any category.
The unknown case is where a new candidate of
the category Cat(p) may potentially be discov-
ered.
4. Pattern Acquisition: For each pat-
tern p 2 , this gives us instance-based lists of
positive pos(p), negative neg(p) and unknown
unk(p) NGs. To compute Score(p), we rst de-
ne the corresponding type-based sets:
 pos

(p) = set of distinct names of category
Cat(p) from AcceptName that p matched.
 neg

(p) = set of distinct names of a wrong
category.
 unk

(p) = set of distinct NGs of unknown
type.
To score the patterns in , we currently use
the accuracy and condence measures:
acc

(p) =
jpos

j
jpos

j + jneg

j
conf

(p) =
jpos

j
jpos

j + jneg

j + junk

j
Patterns with accuracy below a precision
threshold acc

(p) < 
prec
, are removed from .
The remaining patterns are ranked as follows.
The score is computed as:
Score(p) = conf

(p)  log jpos

(p)j (1)
Add the n{best patterns for each target cate-
gory to the set of accepted patterns, AcceptPat.
In the rst term of the scoring function,
higher condence implies that we take less risk if
we acquire the pattern, since acquiring the pat-
tern aects the unknown population. The sec-
ond term favors patterns which select a greater
number of distinct names in AcceptName.
5. Application: Apply each pattern p 2
AcceptPat to the entire corpus.
The noun groups in the set unk

(p) are the
candidates for being added to the category
Cat(p). Let 	 be the list of candidate types:
	 =
[
p 2AcceptPat
unk

(p)
6. Candidate Acquisition: Compute a
score for each candidate type t 2 	, based on
 how many dierent patterns in AcceptPat
match an instance of type t,
 how reliable these patterns are.
To rank a candidate type t 2 	 consider the set
of patterns in AcceptPat which match on some
instance of t; let's call this set M
t
. If jM
t
j < 2,
the candidate is discarded.
4
Otherwise, com-
pute Rank(t) based on the quality of M
t
:
Rank(t) = 1  
Y
p2M
t

1   conf

(p)

(2)
This formula combines evidence by favoring
candidates matched by a greater number of pat-
terns; on the other hand, the term conf

(p) as-
signs more credit to the more reliable patterns.
For each target category, add the m best-
scoring candidate types to the set AcceptName.
7. Repeat: from Step 1, until no more
names can be learned.
3 Prior Work
The Nomen algorithm builds on some ideas
in previous research. Initially, NE classi-
cation centered on supervised methods, sta-
tistically learning from tagged corpora, using
Bayesian learning, ME, etc., (Wakao et al,
1996; Bikel et al, 1997; Borthwick et al,
1998). (Cucerzan and Yarowsky., 1999) present
an unsupervised algorithms for learning proper
names. AutoSlog-TS, (Rilo and Jones, 1999),
learns \concepts" (general NPs) for lling slots
in events, which in principle can include gen-
eralized names. The algorithm does not use
competing evidence. It uses syntactic heuristics
which mark whole noun phrases as candidate in-
stances, whereas Nomen also attempts to learn
names that appear as modiers within a NP.
4
Note, this means that the algorithm is unlikely to
learn a candidate which occurs only once in the corpus.
It can happen if the unique occurrence is anked by ac-
cepted patterns on both sides.
In the area of NE learning, (LP)
2
, (Ciravegna,
2001), is a recent high-performance, supervised
algorithm that learns contextual surface-based
rules separately for the left and the right side
of an instance in text. Separating the two sides
allows the learner to accept weaker rules, and
several correction phases compensate in cases
of insu?cient evidence by removing uncertain
items, and preventing them from polluting the
set of good seeds.
Research in automatic terminology acquisi-
tion initially focused more on the problem of
identication and statistical methods for this
task, e.g., (Justeson and Katz, 1995), the C-
Value/NC-Value method, (Frantzi et al, 2000).
Separately, the problem of classication or clus-
tering is addressed in, e.g., (Ushioda, 1996)
(Strzalkowski and Wang, 1996) presents an
algorithm for learning \universal concepts,"
which in principle includes both PNs and
generic NPs|a step toward our notion of gen-
eralized names. The \spotter" proceeds itera-
tively from a handful of seeds and learns names
in a single category.
DL-CoTrain, (Collins and Singer, 1999),
learns capitalized proper name NEs from a syn-
tactically analyzed corpus. This allows the rules
to use deeper, longer-range dependencies, which
are di?cult to express with surface-level infor-
mation alone. However, a potential problem
with using this approach for our task is that
the Penn-Treebank-based parser does not assign
structure to noun groups, so it is unclear that it
could discover generalized names, as these often
occur within a noun group, e.g., \the 4 yellow
fever cases." Our approach does not have this
limitation.
The salient features of Nomen: it learns
 generalized names, with no reliance on cap-
italization cues, as would be possible in the
case of proper names (in English).
 from an un-annotated corpus, bootstrap-
ping from a few manually-selected seeds
 rules for left and right contexts indepen-
dently (as (LP)
2
to boost coverage).
 several categories simultaneously, and uses
additional categories for negative evidence
to reduce overgeneration.
4 Results
The algorithm was developed using a corpus
drawn from the ProMed mailing list. ProMed is
a global forum where medical professionals post
information regarding outbreaks of infectious
disease (using at times informal language).
Our full training corpus contains 100,000 sen-
tences from 5,100 ProMed articles, from the be-
ginning of 1999 to mid-2001. A subset of that,
used for development, contains 26,000 sentences
from 1,400 documents (3.2Mb) from January to
July 1999.
Our evaluation strategy diers from those in
some of the prior work. We discuss the compet-
ing evaluation strategies in detail in Section 5.2.
To measure performance, we constructed sev-
eral reference lists as follows. First, a manual
list of disease names was hand-compiled from
multiple sources.
5
The manual list consists of
2,492 disease names.
The recall list is automatically derived from
the manual list by searching the training cor-
pus for disease names that surface more than
once.
6
The recall list for the 26,000-sentence
corpus contains 322 disease names, including
some aliases and common acronyms.
The precision list is constructed as the union
of the manual list with an automatically gener-
ated list of acronyms (made by collecting rst
letters of all multi-token names in the manual
list). We applied the same procedure to gener-
ate recall and precision lists for locations.
Then, we judge the recall of Nomen against
the recall lists, and precision against the preci-
sion lists. The list sizes are shown in Table 1.
We focus on two categories, diseases and lo-
cations, while learning several categories simul-
5
Using a disease IE database (Grishman et al, 2002),
the Gideon disease database, and Web search. The list
includes some common acronyms, like HIV and FMD.
6
This is justied because the current algorithm is un-
likely to discover a name that occurs only once.
Reference List Disease Location
Manual 2492 1785
Recall (26K corpus) 322 641
Recall (100K corpus) 616 1134
Precision 3588 2404
Table 1: Reference Lists
00.2
0.4
0.6
0.8
1
0 0.2 0.4 0.6 0.8 1
 
Pr
ec
is
io
n
 Diseases & Locations:  Recall 
Dis + Loc + Sym + Other:
Locations (100k)
Locations  (26k)
Diseases  (26k)
Diseases (100k)
Figure 1: Names: Recall vs. Precision
taneously.
7
We introduce a category for symp-
toms, discussed in the next section.
We also introduce a negative category for
learning terms belonging to none of the classes.
As seeds, we use the 10 most frequent NGs
in the corpus, excluding disease and location
names, and generic words for diseases or loca-
tions (\virus," \outbreak," \area").
8
The parameters in these experiments are:
number of seeds = 10 per category; pattern ac-
curacy threshold 
prec
= 0:80; n = m = 5 for
the number of retained patterns and candidates.
The learning curves in Figure 1 show how re-
call and precision for diseases and locations vary
across the iterations. The bold curves show the
result for diseases and locations on the devel-
opment corpus (26K); e.g., by the end, 70% of
diseases (from the recall list of 322 items) were
learned, at 50% precision|half of the learned
names were not on the precision list. On the
100K corpus (with 641 diseases on the recall
list) the precision was only slightly lower.
The precision measures, however, are under-
stated. Because it is not possible to get a full list
for measuring precision, we nd that Nomen is
penalized for nding correct answers. This is a
general problem of type-based evaluation.
To quantify this eect, we manually examined
the disease names learned by Nomen on the de-
velopment corpus and re-introduced those that
7
Locations seeds: United States, Malaysia, Australia,
Belgium, China, Europe, Taiwan, Hong Kong, Singa-
pore, France.
8
The negative seeds were: case, health, day, people,
year, patient, death, number, report, farm.
0
0.2
0.4
0.6
0.8
1
0 0.2 0.4 0.6 0.8 1
 
Pr
ec
is
io
n
 Disease Names:  Recall 
Dis + Loc + Sym + Other
Diseases (26K), as Figure 1
Enhanced precision list
Figure 2: Eect of Understated Precision
were incorrectly marked as errors, into the pre-
cision list only. The updated graph is shown in
Figure 2; at 70% recall the true precision is 65%.
Note that precision is similarly understated for
all type-based curves in this paper.
Among the re-introduced names there were
99 new diseases which were missed in the man-
ual compilation of reference lists.
9
This is an
encouraging result, since this is ultimately how
Nomen is intended to be used: for discovering
new, previously unknown names.
5 Discussion
5.1 Competing Categories
Figure 3 demonstrates the usefulness of com-
petition among target categories. All curves
show the performance of Nomen on the dis-
ease category, when the algorithm is seeded
only with diseases (the curve labeled Dis), when
seeded with diseases and locations (Dis+Loc),
and with symptoms, and the \other" category.
The curves Dis and Dis+Loc are very similar.
However, when more categories are added, pre-
cision and recall increase dramatically.
When only one category is being learned,
acc(p) = 1:0 for all patterns p. The lack of
an eective accuracy measure causes us to ac-
quire unselective disease name patterns that of-
ten also match non-diseases (e.g., \... X has
been conrmed"). This hurts precision.
9
Examples of new diseases: rinderpest, konzo,
Mediterranean spotted fever, coconut cadang-cadang,
swamp fever, lathyrism, PRRS (for \porcine reproduc-
tive and respiratory syndrome"); locations: Kinta, Ulu
Piah, Melilla, Anstohihy, etc.
00.2
0.4
0.6
0.8
1
0 0.2 0.4 0.6 0.8 1
 
Pr
ec
is
io
n
 Disease Names:  Recall 
Disease names:
Dis + Loc + Sym + Other
Dis + Loc + Other
Dis + Loc
Dis
Figure 3: Diseases: Eect of Competition
Recall also suers, (a) because some patterns
that are more selective (but have lower con-
dence or coverage) are neglected, and (b) be-
cause non-diseases contaminate the seed set and
generate useless patterns.
(Collins and Singer, 1999) also makes use of
competing categories (person, organization, and
location), which cover 96% of all the instances it
set out to classify. In our case, the sought cat-
egories, (diseases and locations), do not cover
the bulk of potential candidates for generalized
names|word sequences matching [ADJ* N+].
Introducing the \negative" category helps us
cover more of the potential candidates. This in
turn boosts the utility of the accuracy measure.
Additional competing categories may help to
prevent a category from \creeping" into an over-
lapping concept. E.g., we had mentioned that
the disease and symptom classes may overlap.
When the target categories include diseases but
not symptoms, Nomen learns some names that
can function as either. This leads to learning of
some patterns which tend to occur with symp-
toms only, resulting in precision errors. Figure 3
shows the improvement in precision from adding
the symptom category.
On the other hand, there may be disadvan-
tages to splitting categories too nely. For ex-
ample, one problem is metonymy among classes
of generalized names. It appears to be distinct
from the problem of ambiguity in PNs, e.g.,
when \Washington" may refer to a person, or a
location. In the case of PNs, there are usually
clues in the context to aord disambiguation.
0
0.2
0.4
0.6
0.8
1
0 0.2 0.4 0.6 0.8 1
 
Pr
ec
is
io
n
 Recall 
Dis+Sym+Loc+Other:
Locations (100k)
Locations  (26k)
Diseases  (26k)
Diseases (100k)
Figure 4: Token-based, MUC-style Evaluation
In the case of GNs, rather, the nature of ambi-
guity may be related to regular metonymy. For
example, names of agents regularly function as
the name of the disease they cause: \E. coli."
Therefore, in learning agents and diseases sepa-
rately, the algorithm will naturally confound the
two classes, which will inhibit learning. In these
experiments, we learn them as a single class.
It may then be more appropriate to apply an-
other procedure to separate the classes based on
a measure of prevalence of co-occurrence with
the respectively characteristic contexts.
5.2 Evaluation
The results in the preceding gures are not di-
rectly commensurate with those in the men-
tioned literature, e.g., (Strzalkowski and Wang,
1996; Collins and Singer, 1999). This relates to
the token-type dichotomy.
The evaluation in the prior work is token-
based, where the learner gets credit|recall
points|for identifying an instance correctly, for
every time it occurs in the corpus. In our type-
based evaluation, it gets credit only once per
name, no matter how many times it occurs.
We also conducted an instance-based evalua-
tion, more compatible with the mentioned prior
work. We manually tagged all diseases and lo-
cations in a 500-sentence test sub-corpus. Using
the output from the runs in Figure 1 we mea-
sured recall and precision using the standard
MUC NE scoring scheme, shown in Figure 4.
10
10
The sharp dip in the \diseases (100K)" curve is
due to several generic terms that were learned early on;
generics were not tagged in the test corpus.
Iteration Type-Based Instance-Based
0 0.03 0.35
20 0.18 0.68
40 0.31 0.85
60 0.42 0.85
300 0.69 0.86
Table 2: Evaluation of Disease Recall
Table 2 contrasts type-based and instance-
based recall across the iterations. The instance-
based evaluation can hardly distinguish between
an algorithm that learns 31% of the types vs.
one that learns 69% of the types. The algorithm
keeps learning lots of new, infrequent types until
iteration 340, but the instance-based evaluation
does not demonstrate this.
5.3 Current Work
Nomen can be improved in several respects.
The current regular-expression NG pattern is
very simplistic. In its present form, it does not
allow \foot and mouth disease" to be learned,
nor \legionnaires' disease"; this introduces in-
accuracy, since parts of these names are learned
and contaminate the pool.
The current pattern generalization scheme
could be expanded. (LP)
2
generalizes on sur-
face form, case, and semantic information. We
could use, e.g., parts of speech from the tagger,
as a level of generalization between lemmas and
wildcards. A complementary approach would
be to use a NP chunker, to capture longer-
distance relations, in the heads and prepositions
of adjacent phrases. ((Collins and Singer, 1999)
achieves this eect by full parsing.)
We are exploring acquisition of more types of
generalized names|agents and vectors, as well
as people and organizations. What is the eect
of learning possibly related classes simultane-
ously, what happens to the items in their inter-
section, and to what extent they inhibit learn-
ing, remains a practical question.
Acknowledgements
This research is supported by the Defense Advanced
Research Projects Agency as part of the Translin-
gual Information Detection, Extraction and Sum-
marization (TIDES) program, under Grant N66001-
001-1-8917 from the Space and Naval Warfare Sys-
tems Center San Diego, and by the National Science
Foundation under Grant IIS-0081962.
References
D. Bikel, S. Miller, R. Schwartz, and R. Weischedel.
1997. Nymble: a high-performance learning
name-nder. In Proc. 5th Applied Natural Lan-
guage Processing Conf., Washington, DC.
A. Borthwick, J. Sterling, E. Agichtein, and R. Gr-
ishman. 1998. Exploiting diverse knowledge
sources via maximum entropy in named entity
recognition. In Proc. 6th Workshop on Very Large
Corpora, Montreal, Canada.
F. Ciravegna. 2001. Adaptive information extrac-
tion from text by rule induction and generalisa-
tion. In Proc. 17th Intl. Joint Conf. on AI (IJCAI
2001), Seattle, WA.
M. Collins and Y. Singer. 1999. Unsupervised mod-
els for named entity classication. In Proc. Joint
SIGDAT Conf. on EMNLP/VLC.
S. Cucerzan and D. Yarowsky. 1999. Language in-
dependent named entity recognition combining
morphological and contextual evidence. In Proc.
Joint SIGDAT Conf. on EMNLP/VLC.
K. Frantzi, S. Ananiadou, and H. Mima. 2000. Au-
tomatic recognition of multi-word terms: the C-
value/NC-value method. Intl. Journal on Digital
Libraries, 2000(3):115{130.
R. Grishman, S. Huttunen, and R. Yangarber. 2002.
Event extraction for infectious disease outbreaks.
In Proc. 2nd Human Lang. Technology Conf.
(HLT 2002), San Diego, CA.
J.S. Justeson and S.M. Katz. 1995. Technical ter-
minology: Some linguistic properties and an algo-
rithm for identication in text. Natural Language
Engineering, 1(1):9{27.
E. Rilo and R. Jones. 1999. Learning dictio-
naries for information extraction by multi-level
bootstrapping. In Proc. 16th Natl. Conf. on AI
(AAAI-99), Orlando, FL.
T. Strzalkowski and J. Wang. 1996. A self-learning
universal concept spotter. In Proc. 16th Intl.
Conf. Computational Linguistics (COLING-96).
A. Ushioda. 1996. Hierarchical clustering of words.
In Proc. 16th Intl. Conf. Computational Linguis-
tics (COLING-96), Copenhagen, Denmark.
T. Wakao, R. Gaizauskas, and Y. Wilks. 1996.
Evaluation of an algorithm for the recognition
and classication of proper names. In Proc. 16th
Int'l Conf. on Computational Linguistics (COL-
ING 96), Copenhagen, Denmark.
R. Yangarber, R. Grishman, P. Tapanainen, and
S. Huttunen. 2000. Automatic acquisition of do-
main knowledge for information extraction. In
Proc. 18th Intl. Conf. Computational Linguistics
(COLING 2000), Saarbrucken, Germany.
R. Yangarber. 2002. Acquisition of domain knowl-
edge. In M.T. Pazienza, editor, Information Ex-
traction. Springer-Verlag, LNAI, Rome.
Complexity of Event Structure in IE Scenarios
Silja Huttunen, Roman Yangarber, Ralph Grishman
Courant Institute of Mathematical Sciences
New York University
fsilja,roman,grishmang@cs.nyu.edu
Abstract
This paper presents new Information Extrac-
tion scenarios which are linguistically and struc-
turally more challenging than the traditional
MUC scenarios. Traditional views on event
structure and template design are not adequate
for the more complex scenarios.
The focus of this paper is to show the com-
plexity of the scenarios, and propose a way to
recover the structure of the event. First we
identify two structural factors that contribute
to the complexity of scenarios: the scattering
of events in text, and inclusion relationships
between events. These factors cause di?culty
in representing the facts in an unambiguous
way. Then we propose a modular, hierarchi-
cal representation where the information is split
in atomic units represented by templates, and
where the inclusion relationships between the
units are indicated by links. Lastly, we discuss
how we may recover this representation from
text, with the help of linguistic cues linking the
events.
1 Introduction
Information Extraction (IE) is a technology
used for locating and extracting specic pieces
of information from texts. The knowledge bases
are customized for each new topic or scenario,
as dened by ll rules that state which facts are
needed for constitution of an extractable event.
A scenario is a set of predened facts to be ex-
tracted from a large text corpus, such as news
articles, and organized in output templates.
Our experience with customizing our IE sys-
tem called Proteus (Grishman, 1997; Grishman
et al, 2002) to new scenarios suggests that the
lexical and structural properties of the scenario
aect the performance of the system. To make
an IE system exible for tasks of varying com-
plexity, it is essential to conduct a linguistic
analysis of the texts relating to dierent sce-
narios.
In this paper, we focus on the Infectious Dis-
ease Outbreak scenario (Grishman et al, 2002),
and the Natural Disaster scenario (Hirschman
et al, 1999) collectively called the \Nature" sce-
narios. During the customization of the IE sys-
tem to the Nature scenarios, we encountered
problems that did not arise in the traditional
scenarios of the Message Understanding Con-
ferences (MUCs). This included, in particular,
delimiting the scope of a single event and orga-
nizing the events into templates.
We identify two structural factors that con-
tribute to the complexity of a scenario: rst, the
scattering of events in text, and second, inclu-
sion relationships between events. These factors
cause di?culty in representing the facts in an
unambiguous way. We proposed that such event
relationships can be described with a modular,
hierarchical model (Huttunen et al, 2002).
The phenomenon of inclusion is widespread in
the Nature scenarios, and the types of inclusions
are numerous. In this paper we present prelim-
inary results obtained from our corpus analysis,
with a classication and distribution of inclu-
sion relationships. We discuss the potential for
recovery of these inclusions from text with the
help of the linguistic cues, of which we show
some examples.
This paper will argue that a thorough linguis-
tic analysis of the corpus is needed to help recov-
ery of the complex event structure in the text.
In the next section we give a brief description
of the scenarios we are investigating. In section
3 we review the problems of scattering, inclusion
and event denition, and propose a method for
representing template structure. In section 4
we present examples of the linguistic cues to
Disaster Date Location VictimDead Damage
tornado Sunday night Georgia one person motel
Disease Date Location VictimDead VictimSick
Ebola since September Uganda 156 people -
Table 1: Disaster Event and Disease Event
recover the complex event structure, followed
by discussion in section 5.
2 Background
2.1 Information Extraction
Our IE system has been previously customized
for several news topics, as part of the MUC
program, such as Terrorist Attacks (MUC,
1991; MUC, 1992) and Management Succession
(MUC, 1995; Grishman, 1995). Subsequently to
the MUCs, we customized Proteus to extract,
among other scenarios, Corporate Mergers and
Acquisitions, Natural Disasters and Infectious
Disease Outbreaks.
We contrasted the Nature scenarios with the
earlier MUC scenarios (Huttunen et al, 2002).
The \traditional" template structure is such
that all the information about the main event
can be presented within a single template. The
main events form separate instances, and there
are no links between them. Management Suc-
cession scenario presents a slightly more com-
plicated template structure, but it is still possi-
ble to present in one template. The traditional
representation is not adequate to represent the
complex structure of the Nature scenarios.
In the next section, we give a short descrip-
tion of the Nature scenarios.
2.2 Scenarios
For the Natural Disaster scenario, the task
is to nd occurrences of disasters (earthquakes,
storms, etc.) around the world, as reported in
newspaper articles. The information extracted
for each disaster should include the type of dis-
aster, date and location of the occurrence, and
the amount of human or material damage.
An example of a Natural Disaster template
is in table 1, extracted from the following news
fragment:
\[...] tornadoes that destroyed a Geor-
gia motel and killed one person in a
mobile home Sunday night."
For the Infectious Disease Outbreak sce-
nario, the task is to track the spread of epi-
demics of infectious diseases around the world.
The system has to nd the name of the disease,
the time and location of the outbreak, the num-
ber of victims (infected and dead), and type of
victims (e.g., human or animal). The next ex-
ample is a fragment of a disease outbreak report,
and the extracted facts are shown in table 1.
\Ebola fever has killed 156 people, [...],
in Uganda since September."
3 Structure of Events
The complex event structure in Nature scenar-
ios is partly due to the fact that the events are
reported in a scattered manner in the text.
By scattering of events we mean that their
components are not close to each other in the
text, and a typical text contains several related
events. This is partly because the articles are
often in a form of an update, where the latest
reported damages contribute to the total dam-
ages reported earlier, over several locations and
over dierent time spans.
The example in table 2 illustrates scattering
in the Disease scenario. It is a fragment of an
update about a cholera epidemic in Sudan, from
the World Health Organization's (WHO) web
report. The locations are highlighted in italics
and the victim counts are in boldface, to show
the scattering. In this example there are six
separate mentions|partial descriptions of the
event in text|giving the number of infected
and dead victims, in Sudan, and in two loca-
tions within Sudan. Paragraph (1) reports the
number of victims in Sudan, 2549 infected, and
186 dead. In paragraph (2), the focus is shifted
to another location in Sudan, and new numbers
are reported. Paragraph (3) gives the respective
(0) Meningococcal in Sudan
(1) A total of 2 549 cases of meningococcal disease, of which 186 were fatal, was reported to the
national health authorities between 1 January and 31 March 2000.
(2) Bahar aj Jabal State has been most aected to date, with 1 437 cases (including 99 deaths)
reported in the Juba city area.
(3) Other States aected include White Nile (197 cases, 15 deaths), [...]
Table 2: Example of a Disease Outbreak Report
Disease Location Infected Dead
Meningococcal Sudan 2549 186
Bahar aj Jabal State 1437 99
White Nile 197 15
Table 3: Facts from Disease Outbreak Report
numbers for yet another location in Sudan. The
mentions are summarized in table 3.
3.1 Inclusion Relationships
As we frequently observe in the Nature scenar-
ios, the information in the various mentions in
table 2 is overlapping, and the mentions par-
tially include each other.
For example, the numbers for infected victims
in paragraph (2) and (3), contribute to the total
number of infected cases in paragraph (1). The
extraction system should be able to extract all
the numbers for this text. The problem is how
to group these mentions into a template in an
unambiguous and coherent way. It is impossi-
ble to represent an event with overlapping in-
formation in a single template, since it consists
of multiple numbers of victims in several areas
and several time intervals.
For the purpose of handling this phenomenon,
we rst introduce a distinction between out-
breaks and incidents. An incident is a short de-
scription, or a mention, of one occurrence that
relates to an outbreak. It covers a single specic
span of time in a single specic area. An out-
break takes place over a longer period of time,
and possibly over wider geographical area: it
consists of multiple incidents.
In general, one incident may include others,
which give further detailed information.
Therefore, we analyze the news fragment in
table 2 as containing six incidents, with two
types of inclusions: rst, inclusion by status,
where the dead count contributes to the infected
count of the same area, and second, inclusion by
location, where the numbers of infected cases in
Bahar aj Jabal State, in paragraph (2), and in
White Nile, in (3), contribute to the infected
count in Sudan, in paragraph (1).
The Natural Disaster scenario poses further
complications for this schema. The scattering
is complicated by the relationship of causation:
the main disaster triggers derivative disasters
(sub-disasters), which in turn may cause dam-
ages that contribute to the overall damage. This
is illustrated by the news fragment in table 4,
from the New York Times. Names of disasters
are in bold, and the damages are italicized.
In table 4, paragraph (1), a disaster includes
rain and winds, which cause ooding. In para-
graph (3), the human damages caused by snow
are included in the total human damages caused
by the storm in (2). The derivative disasters
and their damages often take place in several lo-
cations, appearing relatively far in the text from
the rst mention of the main disaster. The -
nal logical representation of the event should be
such that the eects of the sub-disasters could
be traced back to the main event.
The following is a summary of the inclusion
relationships found in the two Nature scenarios:
 location: e.g, victim count in one city con-
tributes to the victim count in the whole
country.
 time: e.g. victim count for an update re-
port contributes to the overall victim count
since the beginning of the outbreak.
 status: dead or sick count is included in
(1) A brutal northeaster thrashed the Eastern Seaboard again Thursday with cold, slicing rain
and strong winds that caused ooding in coastal areas of New Jersey and Long Island. [...]
(2) Elsewhere along the East Coast, 19 deaths have been attributed to the storm since it began
on Monday.
(3) The 19 deaths include ve in accidents on snowy roads in Kentucky and two in Indiana. [...]
Table 4: Example of Disaster Reporting
the infected count, as in paragraph (2) of
table 2.
 victim type or descriptor: e.g., \people" in-
cludes \health workers", and \children".
 disease name (Disease scenario): e.g., the
number of Hepatitis C cases may be in-
cluded in the number of Hepatitis cases.
 disaster (Disaster scenario): e.g., damages
caused by rain may be included in the dam-
ages caused by rain and winds.
 causation (Disaster scenario): a disaster
can trigger derivative disasters.
3.2 Type and Distribution of Inclusions
To investigate the extent of inclusions and their
distribution by type, we analyzed 40 documents
related to Nature scenarios.
1
To conrm the feasibility and applicability of
this approach, we manually tagged the inclu-
sion relationships present in these documents.
Table 5 shows the number of incidents found in
the documents, as well as the number and the
types of inclusion. There are also multiple in-
clusions: e.g., infected health workers in a town
in Uganda are included in the total number of
infected people in the whole country: this is in-
clusion by both case-descriptor and location.
Multiple inheritance also occurs: in table 2,
the deaths in Bahar aj Jabal State contribute
to the infected count in that state, as well as to
the total number of deaths in Sudan. However,
in table 5, we show only the inclusion in the
immediately preceding parent.
3.3 Hierarchical Template Structure
Our proposed solution is to have a separate tem-
plate for each incident. Once we have broken
1
The training corpus was used to evaluate the per-
formance of our IE system on these tasks. For the Dis-
aster scenario we analyzed a total of 14 reports from
NYT, ABC, APW, CNN, VOA and WSJ. For Disease
Outbreaks, a total of 26 documents from NYT, Promed,
WHO, and ABC.
Scenario Disease Disaster
Documents 26 14
Words 9 500 6500
Incidents 125 112
Inclusions 57 81
time 6 6
location 19 20
status 19 1
case-descriptor 6 1
case-desc/location 3 {
disease 1 {
causation { 19
causation/location { 11
causation/time { 3
time/location { 7
disaster { 5
disaster/location { 2
damage { 4
others 3 2
Table 5: Type and Number of Inclusion
down the information into smaller incident tem-
plates, the inclusion relationship between them
is indicated by event pointers. This approach
makes it possible to represent the information
in a natural and intuitive way.
The nal template for the Infectious Disease
scenario is shown in table 6. Note that there is
a separate slot indicating the parent incident.
Disease Name
Date
Location
Victim Number
Victim Descriptor
Victim Status
Victim Type
Parent Event
Table 6: Infectious Disease Template
Figure 1: Infectious Disease Outbreak
Figure 2: Natural Disaster
Figure 1 is a graphical representation of the
inclusion relationships among the incidents ex-
tracted from the Disease report in table 2. The
gure shows the main incident with several sub-
incidents. Two of the sub-incidents have, in
turn, sub-incidents. The types of inclusions are
shown in the last row.
Figure 2 shows a graphical representation of
inclusion by causation in Natural Disaster sce-
nario. The incidents are extracted from ta-
ble 4.
2
There is a causation relationship be-
tween the incidents. It is important to recover
the long causation chains from the text.
As a result, the templates are simple, but
2
Note that the northeaster is not in causation rela-
tionship with storm, which began on Monday. The dam-
ages that the synonymous northeaster caused, are from
the following Thursday.
there are typically many templates per docu-
ment. The separation of incidents aects the
process of extraction, since we can now focus
on looking for smaller atomic pieces rst. Then
we must address the problem of linking together
related incidents as a separate problem in the
overall process of IE.
4 Linguistic cues
The process of tracking the inclusion relation-
ships between the incidents is not trivial. A
human reader uses the cohesive devices in the
text to construct the connections between parts
of text (see e.g., (Halliday and Hasan, 1976; Hal-
liday, 1985)). Finding the relationship between
incidents may be a less complex task than track-
ing cohesion through an entire text or discourse.
Our task is limited to nding the cohesive de-
vices connecting a small set of pre-dened facts,
that may occur nearby within one sentence, or
are separated by one or more sentence bound-
aries. Our goal is to locate the cues in the text,
and use them to automatically recover these re-
lationships.
An example of a linguistic cue is in the fol-
lowing fragment of an update from table 4:
Elsewhere along the East Coast, 19
deaths have been attributed to the
storm [...]
Elsewhere indicates a shift in the focus from
one location to another and there is probably
no inclusion between the following and immedi-
ately preceding mention of the damages.
We have identied several linguistic cues that
signal the presence or absence of an inclusion
relationship between two incidents. These cues
can be one of following types:
 Specic lexical items, which can be e.g.,
adverbs, verbs, prepositions, connectives.
Elsewhere in the previous example implies
that damages caused by the following dis-
aster do not contribute to the damages of
the immediately preceding disaster.
 Two expressions in separate incidents
which are related in the scenario-specic
concept hierarchy, may indicate the pres-
ence and also the direction of an inclusion,
e.g., health worker is included in people;
names of plants, animals and terms refer-
ring to human beings, are hyponyms of vic-
tim.
 Locative or temporal expressions that are
in a hierarchical relationship in a location
hierarchy or in the implicit time hierarchy,
often indicate presence or direction of in-
clusion.
 Elliptical elements create cohesion. Ellipsis
indicates the presence of a parent incident
earlier in the text. In paragraph (3) of table
4, in the parent incident we observe a case
descriptor, deaths, which is elided in the
two sub-incidents.
 Anaphora: anaphoric reference usually in-
dicates the absence of an inclusion between
two incidents, merging into one. For exam-
ple, in table 4, paragraph (3), the 19 deaths
is coreferential with 19 deaths caused by the
storm in paragraph (2).
 Coordination tends to indicate the absence
of inclusion relationship. For example,
when two incidents are conjoined by and
and do not share information about loca-
tion or time, there is typically no inclusion.
However, there are cases where other cues
override this general tendency.
These cues often do not appear in isolation,
and they may interact.
We give an example of three lexical items and
their role as an indicator of inclusion in the In-
fectious Disease Outbreak Scenario. Consider
the preposition with
3
, the participle including
and the nite verb include.
\More than 500 cases of dengue hem-
orrhagic fever were reported in Mexico
last year, with 30 deaths, Ruiz said."
The 30 deaths are included in the 500 cases.
The direction of the inclusion is reversed in the
following example:
"Disease has killed 10 persons, with
242 cases having already been re-
ported."
The latter incident includes the former. Here
additional cues are provided by the concept hi-
erarchy, and the numbers: a smaller number
cannot include a larger one.
The following illustrates the participle includ-
ing as cue:
Ebola fever has killed 156 people, in-
cluding 14 health workers, in Uganda
since September.
The incidents are connected by including,
which also indicates the direction explicitly. Ad-
ditional information is obtained from the case-
descriptors, related in the concept hierarchy.
The context for such \trigger" words as they
indicate inclusion, is that the trigger appears
between two incidents, preceding and preceded
3
In the case of with we look only at free prepositions,
that is, those not bound to a preceding verb (Biber et
al., 1999).
by a quantied NP
4
and optional phrases or
items from the concept hierarchy.
Q fcase-descriptor j statusg [reported
j get sick j time j location j disease] [,]
trigger Q fcase-descriptor j statusg
These triggers can indicate inclusion also in-
side a parenthetical phrase, preceding a quanti-
ed NP, as in table 2 in paragraph (2).
The trigger include (as a nite verb) functions
similarly, but can also occur between sentences:
[...] the Ugandan Ministry of Health
has reported [...] 370 cases and 140
deaths. This gure includes 16 new
conrmed cases in Gulu [...]
In our training corpus, when these cue words
occurred in this context, they consistently indi-
cated an event inclusion relation.
5 Discussion
Complexity of a scenario seems to depend of
multiple factors. The notion of complexity,
however, has not been investigated in great
depth. Some research on this was done by
(Bagga and Biermann, 1997; Bagga, 1997),
classifying scenarios according to di?culty by
counting distances between \components" of an
event in the text. In this way it attempts to ac-
count for variation in performance across the
MUC scenarios.
Our analysis suggests that the type and
amount of inclusion relationships depend on
the nature of the topic. In such scenarios as
Management Succession and Corporate Acqui-
sitions, an event usually occurs at one specic
point in time. By contrast, the Nature events
typically take place across a span of time and
space. As the event \travels" and evolves, its
manifestations are reported in a piecewise fash-
ion, sometimes on an hour-by-hour basis.
An extensive linguistic analysis of the cor-
pus is necessary to resolve these complex is-
sues. For evaluation and training, we are build-
ing test and training corpora, totaling 70 doc-
uments and annotated with inclusion relation-
ships.
4
Here the case descriptor or status can be elided:
however, one of quantiers should have a case descriptor
or a status.
Acknowledgments
This research is supported by the Defense Advanced
Research Projects Agency as part of the Translin-
gual Information Detection, Extraction and Sum-
marization (TIDES) program, under Grant N66001-
001-1-8917 from the Space and Naval Warfare Sys-
tems Center San Diego, and by the National Science
Foundation under Grant IIS-0081962.
This paper does not necessarily reect the posi-
tion or the policy of the U.S. Government.
References
A. Bagga and A. W. Biermann. 1997. Analyzing
the complexity of a domain with respect to an
information extraction task. In Proc. 10th Intl.
Conf. on Research on Computational Linguistics
(ROCLING X).
A. Bagga. 1997. Analyzing the performance of
message understanding systems. In Proc. Natu-
ral Language Processing Pacic Rim Symposium
(NLPRS'97).
D. Biber, S. Johansson, G. Leech, S. Conrad, and
E. Finegan. 1999. Longman Grammar of Spoken
and Written English. Longman.
R. Grishman, S. Huttunen, and R. Yangarber. 2002.
Real-time event extraction for infectious disease
outbreaks. In Proc. HLT 2002: Human Language
Technology Conf., San Diego, CA.
R. Grishman. 1995. The NYU system for MUC-
6, or where's the syntax? In Proc. 6th Message
Understanding Conf. (MUC-6), Columbia, MD.
Morgan Kaufmann.
R. Grishman. 1997. Information extraction: Tech-
niques and challenges. In M. T. Pazienza, editor,
Information Extraction. Springer-Verlag, Lecture
Notes in Articial Intelligence, Rome.
M.A.K. Halliday and R. Hasan. 1976. Cohesion in
English. Longman, London.
M.A.K. Halliday. 1985. Introduction to Functional
Grammar. Edward Arnold, London.
L. Hirschman, E. Brown, N. Chinchor, A. Douthat,
L. Ferro, R. Grishman, P. Robinson, and B. Sund-
heim. 1999. Event99: A proposed event indexing
task for broadcast news. In Proc. DARPA Broad-
cast News Workshop, Herndon, VA.
S. Huttunen, R. Yangarber, and R. Grishman. 2002.
Diversity of scenarios in information extraction.
In Proc. 3rd Intl. Conf. of Language Resources
and Evaluation, LREC-2002, Las Palmas de Gran
Canaria, Spain.
1991. Proc. 3th Understanding Conf. (MUC-3).
Morgan Kaufmann.
1992. Proc. 4th Message Understanding Conf.
(MUC-4). Morgan Kaufmann.
1995. Proc. 6th Message Understanding Conf.
(MUC-6). Morgan Kaufmann.
 1 
Discriminative Slot Detection Using Kernel Methods 
Shubin Zhao, Adam Meyers, Ralph Grishman 
Department of Computer Science 
New York University 
715 Broadway, New York, NY 10003 
shubinz, meyers, grishman@cs.nyu.edu 
 
Abstract 
Most traditional information extraction 
approaches are generative models that assume 
events exist in text in certain patterns and these 
patterns can be regenerated in various ways. 
These assumptions limited the syntactic clues 
being considered for finding an event and 
confined these approaches to a particular 
syntactic level. This paper presents a 
discriminative framework based on kernel SVMs 
that takes into account different levels of 
syntactic information and automatically 
identifies the appropriate clues. Kernels are used 
to represent certain levels of syntactic structure 
and can be combined in principled ways as input 
for an SVM. We will show that by combining a 
low level sequence kernel with a high level 
kernel on a GLARF dependency graph, the new 
approach outperformed a good rule-based 
system on slot filler detection for MUC-6. 
1 Introduction 
The goal of Information Extraction (IE) is to 
extract structured facts of interest from text and 
present them in databases or templates. Much of 
the IE research was promoted by the US 
Government-sponsored MUCs (Message 
Understanding Conferences). The techniques used 
by Information Extraction depend greatly on the 
sublanguage used in a domain, such as financial 
news or medical records. The training data for an 
IE system is often sparse since the target domain 
changes quickly. Traditional IE approaches try to 
generate patterns for events by various means 
using training data. For example, the FASTUS 
(Appelt et al, 1996) and Proteus (Grishman, 1996) 
systems, which performed well for MUC-6, used 
hand-written rules for event patterns. The symbolic 
learning systems, like AutoSlog (Riloff, 1993) and 
CRYSTAL (Fisher et al, 1996), generated patterns 
automatically from specific examples (text 
segments) using generalization and predefined 
pattern templates. There are also statistical 
approaches (Miller et al, 1998) (Collins et al, 
1998) trying to encode event patterns in statistical 
CFG grammars. All of these approaches assume 
events occur in text in certain patterns. However 
this assumption may not be completely correct and 
it limits the syntactic information considered by 
these approaches for finding events, such as 
information on global features from levels other 
than deep processing. This paper will show that a 
simple bag-of-words model can give us reliable 
information about event occurrence. When training 
data is limited, these other approaches may also be 
less effective in their ability to generate reliable 
patterns.    
  The idea for overcoming these problems is to 
avoid making any prior assumption about the 
syntactic structure an event may assume; instead, 
we should consider all syntactic features in the 
target text and use a discriminative classifier to 
decide that automatically. Discriminative 
classifiers make no attempt to resolve the structure 
of the target classes. They only care about the 
decision boundary to separate the classes. In our 
case, we only need criteria to predict event 
elements from text using the syntactic features 
provided. This seems a more suitable solution for 
IE where training data is often sparse. 
 This paper presents an approach that uses kernel 
functions to represent different levels of syntactic 
structure (information). With the properties of 
kernel functions, individual kernels can be 
combined freely into comprehensive kernels that 
cross syntactic levels. The classifier we chose to 
use is SVM (Support Vector Machine), mostly due 
to its ability to work in high dimensional feature 
spaces. The experimental results of this approach 
show that it can outperform a hand-crafted rule 
system for the MUC-6 management succession 
domain. 
2 Background 
2.1 Information Extraction 
The major task of IE is to find the elements of an 
event from text and combine them to form 
templates or populate databases.  Most of these 
elements are named entities (NEs) involved in the 
event. To determine which entities in text are 
involved, we need to find reliable clues around 
each entity. The extraction procedure starts with 
 2 
text preprocessing, ranging from tokenization and 
part-of-speech tagging to NE identification and 
parsing. Traditional approaches would use various 
methods of analyzing the results of deep 
preprocessing to find patterns. Here we propose to 
use support vector machines to identify clues 
automatically from the outputs of different levels 
of preprocessing. 
2.2 Support Vector Machine 
For a two-class classifier, with separable training 
data, when given a set of n labeled vector examples 
    }1,1{),,(),...,,(),,( 2211 ?+?inn yyXyXyX ,  
a support vector machine (Vapnik, 1998) produces 
the separating hyperplane with largest margin 
among all the hyperplanes that successfully 
classify the examples. Suppose that all the 
examples satisfy the following constraint:  
             1),( ?+><? bXWy ii  
It is easy to see that the margin between the two  
bounding hyperplanes 1, ?=+>< bXW i is 
2/||W||. So maximizing the margin is equivalent to 
minimizing ||W||2 subject to the separation 
constraint above. In machine learning theory, this 
margin relates to the upper bound of the VC-
dimension of a support vector machine. Increasing 
the margin reduces the VC-dimension of the 
learning system, thus increasing the generalization 
capability of the system.  So a support vector 
machine produces a classifier with optimal 
generalization capability. This property enables 
SVMs to work in high dimensional vector spaces. 
2.3 Kernel SVM 
The vectors in SVM are usually feature vectors 
extracted by a certain procedure from the original 
objects, such as images or sentences. Since the 
only operator used in SVM is the dot product 
between two vectors, we can replace this operator 
by a function ),( ji SS?  on the object domain. In 
our case, Si and Sj are sentences. Mathematically 
this is still valid as long as ),( ji SS?  satisfies 
Mercer?s condition 1 . Function ),( ji SS?  is often 
referred to as a kernel function or just a kernel.  
Kernel functions provide a way to compute the 
similarity between two objects without 
transforming them into features.  
   
The kernel set has the following properties: 
                                                     
1
 The matrix must be positive semi-definite 
1. If ),(1 yxK  and ),(2 yxK are kernels on YX ? , 
0, >?? , then ),(),( 21 yxKyxK ?? +  is a kernel 
on YX ? . 
2. If ),(1 yxK  and ),(2 yxK are kernels on YX ? , 
then ),(),( 21 yxKyxK ?  is a kernel on YX ? . 
3. If ),(1 yxK  is a kernel on YX ? and 
),(2 vuK  is a kernel on VU ? , then 
),(),()),(),,(( 21 vuKyxKvyuxK += is a kernel 
on )()( VYUX ??? . 
When we have kernels representing information 
from different sources, these properties enable us 
to incorporate them into one kernel. The general 
kernels such as RBF or polynomial kernels (M?ller 
et al, 2001), which extend features nonlinearly 
into higher dimensional space, can also be applied 
to either the combination kernel or to each 
component kernel individually. 
2.4 Related Work 
  There have been a number of SVM applications 
in NLP using particular levels of syntactic 
information. (Lodhi et al, 2002) compared a word-
based string kernel and n-gram kernels at the 
sequence level for a text categorization task. The 
experimental results showed that the n-gram 
kernels performed quite well for the task. Although 
string kernels can capture common word 
subsequences with gaps, its geometric penalty 
factor may not be suitable for weighting the long 
distance features. (Collins et al, 2001) suggested 
kernels on parse trees and other structures for 
general NLP tasks. These kernels count small 
subcomponents multiple times so that in practice 
one has to be careful to avoid overfitting. This can 
be achieved by limiting the matching depth or 
using a penalty factor to downweight large 
components.  
(Zelenko et al, 2003) devised a kernel on 
shallow parse trees to detect relations between 
named entities, such as the person-affiliation 
relation between a person name and an 
organization name. The so-called relation kernel 
matches from the roots of two trees and continues 
recursively to the leaf nodes if the types of two 
nodes match.  
All the kernels used in these works were applied 
to a particular syntactic level. This paper presents 
an approach for information extraction that uses 
kernels to combine information from different 
levels and automatically identify which 
information contributes to the task. This 
framework can also be applied to other NLP tasks. 
 
 3 
3 A Discriminative Framework 
  The discriminative framework proposed here is 
called ARES (Automated Recognition of Event 
Slots). It makes no assumption about the text 
structure of events. Instead, kernels are used to 
represent syntactic information from various 
syntactic sources. The structure of ARES is shown 
in Fig 1. The preprocessing modules include a 
part-of-speech tagger, name tagger, sentence parser 
and GLARF parser, but are not limited to these. 
Other general tools can also be included, which are 
not shown in the diagram. The triangles in the 
diagram are kernels that encode the corresponding 
syntactic processing result. In the training phase, 
the target slot fillers are labeled in the text so that 
SVM slot detectors can be trained through the 
kernels to find fillers for the key slots of events. In 
the testing phase, the SVM classifier will predict 
the slot fillers from unlabeled text and a merging 
procedure will merge slots into events if necessary. 
The main kernel we propose to use is on GLARF 
(Meyers et al, 2001) dependency graphs. 
 
 
Fig 1. Structure of the discriminative model 
 
  The idea is that an IE model should not commit 
itself to any syntactic level. The low level 
information, such as word collocations, may also 
give us important clues. Our experimentation will 
show that for the MUC-6 management succession 
domain, even bag-of-words or n-grams can give us 
helpful information about event occurrence. 
3.1 Syntactic Kernels 
  To make use of syntactic information from 
different levels, we can develop kernel functions or 
syntactic kernels to represent a certain level of 
syntactic structure. The possible syntactic kernels 
include 
? Sequence kernels: representing sequence 
level information, such as bag-of-words, n-
grams, string kernel, etc. 
? Phrase kernel: representing information at 
an intermediate level, such as kernels 
based on multiword expressions, chunks or 
shallow parse trees. 
? Parsing kernel: representing detailed 
syntactic structure of a sentence, such as 
kernels based on parse trees or dependency 
graphs. 
 
  These kernels can be used alone or combined 
with each other using the properties of kernels. 
They can also be combined with high-order kernels 
like polynomial or RBF kernels, either individually 
or on the resulting kernel. 
As the depth of analysis of the preprocessing 
increases, the accuracy of the result decreases. 
Combining the results of deeper processing with 
those of shallower processing (such as n-grams) 
can also give us a back-off ability to recover from 
errors in deep processing. 
In practice each kernel can be tested for the task 
as the sole input to an SVM to determine if this 
level of information is helpful or not. After 
figuring out all the useful kernels, we can try to 
combine them to make a comprehensive kernel as 
final input to the classifier. The way to combine 
them and the parameters in combination can be 
determined using validation data. 
4 Introduction to GLARF 
GLARF (Grammatical and Logical Argument 
Regularization Framework) [Meyers et al, 2001] is 
a  hand-coded system that produces comprehensive 
word dependency graphs from Penn TreeBank-II 
(PTB-II) parse trees to facilitate applications like 
information extraction. GLARF is designed to 
enhance PTB-II parsing to produce more detailed 
information not provided by parsing, such as 
information about object, indirect object and 
appositive relations. GLARF can capture more 
regularization in text by transforming non-
canonical (passive, filler-gap) constructions into 
their canonical forms (simple declarative clauses). 
This is very helpful for information extraction 
where training data is often sparse. It also 
represents all syntactic phenomena in uniform 
typed PRED-ARG structures, which is convenient 
for computational purposes. For a sentence, 
GLARF outputs depencency triples derived 
automatically from the GLARF typed feature 
structures [Meyers et al, 2001]. A directed 
dependency graph of the sentence can also be 
constructed from the depencency triples. The 
following is the output of GLARF for the sentence 
?Tom Donilon, who also could get a senior job 
??. 
<SBJ,   get,  Tom Donilon> 
<OBJ,  get,   job> 
<ADV,  get,  also> 
<AUX,  get,  could> 
<T-POS,  job, a> 
  
 
 
 
 
Texts 
Input 
 
 
Output 
Templates 
POS      
Tagger 
Sent      
Parser 
Glarf      
Parser 
Name      
Tagger 
SGML     
Parser Event      
Merger 
Slot
 D
etecto
r
 
Documents 
 4 
<A-POS,  job,  senior> 
  . . . 
GLARF can produce logical relations in addition 
to surface relations, which is helpful for IE tasks. It 
can also generate output containing the base form 
of words so that different tenses of verbs can be 
regularized. Because of all these features, our main 
kernels are based on the GLARF dependency 
triples or dependency graphs.  
5 Event and Slot Kernels 
Here we will introduce the kernels used by ARES 
for event occurrence detection (EOD) and slot 
filler detection (SFD).  
5.1 EOD Kernels 
  In Information Extraction, one interesting issue 
is event occurrence detection, which is determining 
whether a sentence contains an event occurrence or 
not. If this information is given, it would be much 
easier to find the relevant entities for an event from 
the current sentence or surrounding sentences. 
Traditional approaches do matching (for slot 
filling) on all sentences, even though most of them 
do not contain any event at all. Event occurrence 
detection is similar to sentence level information 
retrieval, so simple models like bag-of-words or n-
grams could work well. We tried two kernels to do 
this, one is a sequence level n-gram kernel and the 
other is a GLARF-based kernel that matches 
syntactic details between sentences. In the 
following formulae, we will use an identity 
function ),( yxI that gives 1 when yx ?  and 0 
otherwise, where x and y are strings or vectors of 
strings.  
 
1. N-gram kernel ),( 21 SSN?  that counts common 
n-grams between two sentences. Given two 
sentence: >=<
1
,..., 211 NwwwS , and >=< 2,..., 211 NwwwS ,  
a bigram kernel ),( 21 SSbi?  is 
??
?
=
++
?
=
><><
1
1
11
1
1
21
),,,(
N
j
jjii
N
i
wwwwI .   
Kernels can be inclusive, in other words, the 
trigram kernel includes bigrams and unigrams. For 
the unigram kernel a stop list is used that removes 
words other than nouns, verbs, adjectives and 
adverbs. 
2. Glarf kernel ),( 21 GGg? : this kernel is based 
on the GLARF dependency result. Given the triple 
outputs of two sentences produced by  
GLARF: },,{1 ><= iii aprG , 11 Ni ??  and 
},,{2 ><= jjj aprG , 21 Nj ?? , where ri, pi, ai 
correspond to the role label, predicate word and 
argument word respectively in GLARF output, it 
matches the two triples, their predicates and 
arguments respectively. So ),( 21 GGg?  equals 
)),(),(),,,,,((
21
11
??
==
++><><
N
j
jijijjjiii
N
i
aaIppIapraprI ??  
In our experiments, ? and ?  were set to 1. 
5.2 SFD Kernels 
 Slot filler detection (SFD) is the task of 
determining which named entities fill a slot in 
some event template.  Two kernels were proposed 
for SFD: the first one matches local contexts of 
two target NEs, while the second one combines the 
first one with an n-gram EOD kernel.  
  1. ),(1 jiSFD GG? : This kernel was also defined 
on a GLARF dependency graph (DG), a directed 
graph constructed from its typed PRED-ARG 
outputs. The arcs labeled with roles go from 
predicate words to argument words. This kernel 
matches local context surrounding a name in a 
GLARF dependency graph. In preprocessing, all 
the names of the same type are translated into one 
symbol (a special word). The matching starts from 
two anchor nodes (NE nodes of the same type) in 
the two DG?s and recursively goes from these 
nodes to their successors and predecessors, until 
the words associated with nodes do not match. In 
our experiment, the matching depth was set to 2. 
Each node n contains a predicate word w
 
and  
relation pairs },{ >< ii ar , pi ??1  representing 
its p arguments and the roles associated with them.  
A matching function ),( 21 nnC  is defined as 
??
==
+><><
21
11
)),(),,,((
p
j
jijjii
p
i
rrIararI . 
Then ),(1 jiSFD GG? : can be written as 
??
?
?
?
?
?
?
++
ji
jj
ii
ji
jj
ii
nn
Eedn
Eedn
ji
nn
ESuccn
ESuccn
jiji nnCnnCEEC
)(Pr
)(Pr
)(
)(
),(),(),(  
where Ei and Ej are the anchor nodes in the two 
DG?s; ji nn ? is true if the predicate words 
associated with them match. Functions Succ(n) and 
Pred(n) give the successor and predecessor node 
set of a node n. The reason for setting a depth limit 
is that it covers most of the local syntax of a node 
(before matching stops); another reason is that the 
cycles currently present in GLARF dependency 
graph prohibit unbounded recursive matching. 
  2. ),(2 jiSFD SS? : This kernel combines linearly 
the n-gram event kernel and the slot kernel above, 
 5 
in the hope that the general event occurrence 
information provided by EOD kernel can help the 
slot kernel to ignore NEs in sentences that do not 
contain any event occurrence.  
),(),(),( 12 jiSFDjiNjiSFD GGSSSS ????? += , 
where ?? , were set to be 1 in our experiments. 
The Glarf event kernel was not used, simply 
because it uses information from the same source 
as ),(1 jiSFD GG? . The n-gram kernel was chosen 
to be the trigram kernel, which gives us the best 
EOD performance among n-gram kernels. 
We also tried the dependency graph kernel 
proposed by (Collins et al, 2001), but it did not 
give us better result. 
6 Experiments 
6.1 Corpus 
  The experiments of ARES were done on the 
MUC-6 corporate management succession domain 
using the official training data and, for the final 
experiment, the official test data as well. The 
training data was split into a training set (80%) and 
validation set (20%). In ARES, the text was 
preprocessed by the Proteus NE tagger and 
Charniak sentence parser. Then the GLARF 
processor produced dependency graphs based on 
the parse trees and NE results. All the names were 
transformed into symbols representing their types, 
such as #PERSON# for all person names. The 
reason is that we think the name itself does not 
provide a significant clue; the only thing that 
matters is what type of name occurs at certain 
position. 
  Two tasks have been tried: one is EOD (event 
occurrence detection) on sentences; the other is 
SFD (slot filler detection) on named entities, 
including person names and job titles. EOD is to 
determine whether a sentence contains an event or 
not. This would give us general information about 
sentence-level event occurrences. SFD is to find 
name fillers for event slots. The slots we 
experimented with were the person name and job 
title slots in MUC-6. We used the SVM package 
SVMlight in our experiments, embedding our own 
kernels as custom kernels. 
6.2 EOD Experiments 
  In this experiment, ARES was trained on the 
official MUC-6 training data to do event 
occurrence detection. The data contains 1940 
sentences, of which 158 are labeled as positive 
instances (contain an event). Five-fold cross 
validation was used so that the training and test set 
contain 80% and 20% of the data respectively. 
Three kernels defined in the previous section were 
tried. Table 1 shows the performance of each 
kernel. Three n-gram kernels were tested: unigram, 
bigram and trigram. Subsequences longer than 
trigrams were also tried, but did not yield better 
results. 
  The results show that the trigram kernel 
performed the best among n-gram kernels. GLARF 
kernel did better than n-gram kernels, which is 
reasonable because it incorporates detailed syntax 
of a sentence. But generally speaking, the n-gram 
kernels alone performed fairly well for this task, 
which indicates that low level text processing can 
also provide useful information. The mix kernel 
that combines the trigram kernel with GLARF 
kernel gave the best performance, which might 
indicate that the low level information provides 
additional clues or helps to overcome errors in 
deep processing. 
 
Kernel Precision Recall F-score 
Unigram 66.0% 66.5% 66.3% 
Bigram 73.9% 60.3% 66.4% 
Trigram 77.5% 61.5% 68.6% 
GLARF 77.5% 63.9% 70.1% 
Mix 81.5% 66.4% 73.2% 
 
Table 1. EOD performance of ARES using 
different kernels. The Mix kernel is a linear 
combination of the trigram kernel and the Glarf 
kernel. 
6.3 SFD Experiments 
The slot filler detection (SFD) task is to find the 
named entities in text that can fill the 
corresponding slots of an event.2 We treat job title 
as a named entity throughout this paper, although it 
is not included in the traditional MUC named 
entity set. The slots we used for evaluation were 
PERSON_IN (the person who took a position),  
PERSON_OUT (the person who left a position) 
and POST (the position involved). We generated 
the two person slots from the official MUC-6 
templates and the corresponding filler strings in 
text were labeled. Three SVM predictors were 
trained to find name fillers of each slot. Two 
experiments have been tried on MUC-6 training 
data using five-fold cross validation. 
  The first experiment of ARES used slot kernel 
),(1 jiSFD GG?  alone, relying solely on local 
                                                     
2
 We used this task for evaluation, rather than the 
official MUC template-filling task, in order to assess the 
system?s ability to identify slot fillers separately from its 
ability to combine them into templates. 
 6 
context around a NE. From the performance table 
(Table 2), we can see that local context can give a 
fairly good clue for finding PERSON_IN and 
POST, but not for PERSON_OUT. The main 
reason is that local context might be not enough to 
determine a PERSON_OUT filler. It often requires 
inference or other semantic information. For 
example, the sentence ?Aaron Spelling, the 
company's vice president, was named president.?, 
indicates that ?Aaron Spelling? left the position of 
vice president, therefore it should be a 
PERSON_OUT. But the sentence ?Aaron Spelling, 
the company's vice president, said ??, which is 
very similar to first one in syntax, has no such 
indication at all. In complicated cases, a person can 
even hold two positions at the same time. 
 
Accuracy Precision Recall F-score 
PER_IN 63.6% 62.5% 63.1% 
PER_OUT 54.8% 54.2% 54.5% 
POST 64.4% 55.2% 59.4% 
 
Table 2. SFD performance of ARES using kernel 
),(1 jiSFD GG? . 
 
  In this experiment, the SVM predictor 
considered all the names identified by the NE 
tagger; however, most of the sentences do not 
contain an event occurrence at all, so NEs in these 
sentences should be ignored no matter what their 
local context is. To achieve this we need general 
information about event occurrence, and this is just 
what the EOD kernel can provide. In our second 
experiment, we tested the kernel ),(2 jiSFD SS? , 
which is a linear combination of the trigram EOD 
kernel and the SFD kernel ),(1 jiSFD GG? . Table 3 
shows the performance of the combination kernel, 
from which we can see that there is clear 
performance improvement for all three slots. We 
also tried to use the mix kernel which gave us the 
best EOD performance, but it did not yield a better 
result. The reason we think is that the GLARF 
EOD kernel and SFD kernel are from the same 
syntactic source, so the information was repeated. 
 
Accuracy Precision Recall F-score 
PER_IN 86.6% 60.5% 71.2% 
PER_OUT 69.2% 58.2% 63.2% 
POST 68.5% 68.9% 68.7% 
 
Table 3. SFD performance of ARES using kernel 
),(2 jiSFD SS? . It combines the Glarf SFD kernel 
with trigram EOD kernel. For PER_OUT,  
unigram EOD kernel was used. 
 
Since five-fold cross validation was used, ARES 
was trained on 80% of the MUC-6 training data in 
these two experiments.  
6.4 Comparison with MUC-6 System 
This experiment was done on the official MUC-
6 training and test data, which contain 50K words 
and 40K words respectively. ARES used the 
official corpora as training and test sets, except that 
in the training data, all the slot fillers were 
manually labeled. We compared the performance 
of ARES with the NYU Proteus system, a rule-
based system that performed well for MUC-6. To 
score the performance for these three slots, we 
generated the slot-filler pairs as keys for a 
document from the official MUC-6 templates and 
removed duplicate pairs. The scorer matches the 
filler string in the response file of ARES to the 
keys.  The response result for Proteus was 
extracted in the same way from its template output. 
Table 4. shows the result of ARES using the 
combination kernel in the previous experiment. 
 
Accuracy Precision Recall F-score 
PER_IN 77.3% 62.2% 68.9% 
PER_OUT 58.9% 69.7% 63.9% 
POST 77.1% 71.5% 73.6% 
 
Table 4. Slot performance ARES using kernel 
),(2 jiSFD SS?  on MUC-6 test data.  
 
Table 5 shows the test result of the Proteus 
system. Comparing the numbers we can see that 
for slot PERSON_IN and POST, ARES 
outperformed the Proteus system by a few points. 
The result is promising considering that this model 
is fully automatic and does not involve any post-
processing. As for the PERSON_OUT slot, the 
performance of ARES was not as good. As we 
have discussed before, relying purely on syntax 
might not help us much;  we may need an 
inference model to resolve this problem. 
 
Accuracy Precision Recall F-score 
PER_IN 85.7% 51.2% 64.1% 
PER_OUT 78.4% 58.6% 67.1% 
POST 83.3% 59.7% 69.5% 
 
Table 5. Slot performance of the rule-based 
Proteus system for MUC-6. 
7 Related Work 
(Chieu et al, 2003) reported a feature-based 
SVM system (ALICE) to extract MUC-4 events of 
 7 
terrorist attacks. The Alice-ME system 
demonstrated competitive performance with rule-
based systems. The features used by Alice are 
mainly from parsing. Comparing with ALICE, our 
system uses kernels on dependency graphs to 
replace explicit features, an approach which is 
fully automatic and requires no enumeration of 
features. The model we proposed can combine 
information from different syntactic levels in 
principled ways. In our experiments, we used both 
word sequence  information and parsing level 
syntax information. The training data for ALICE 
contains 1700 documents, while for our system it 
is just 100 documents. When data is sparse, it is 
more difficult for an automatic system to 
outperform a rule-based system that incorporates 
general knowledges. 
8 Discussion and Further Works 
    This paper describes a discriminative approach 
that can use syntactic clues automatically for slot 
filler detection. It outperformed a hand-crafted 
system on sparse data by considering different 
levels of syntactic clues. The result also shows that 
low level syntactic information can also come into 
play in finding events, thus it should not be ignored 
in the IE framework. 
    For slot filler detection, several classifiers were 
trained to find names for each slot and there is no 
correlation among these classifiers. However, 
entity slots in events are often strongly correlated, 
for example the PER_IN and POST slots for 
management succession events. Since these 
classifiers take the same input and produce 
different results, correlation models can be used to 
integrate these classifiers so that the identification 
of slot fillers might benefit each other.  
    It would also be interesting to experiment with 
the tasks that are more difficult for pattern 
matching, such as determining the on-the-job 
status property in MUC-6. Since events often span 
multiple sentences, another direction is to explore 
cross-sentence models, which is difficult for 
traditional approaches. For our approach it is 
possible to extend the kernel from one sentence to 
multiple sentences, taking into account the 
correlation between NE?s in adjacent sentences. 
9 Acknowledgements 
This research was supported in part by the 
Defense Advanced Research Projects Agency as 
part of the TIDES program, under Grant N66001-
001-1-8917 from the Space and Naval Warfare 
Systems Center, San Diego, and by the National 
Science Foundation under Grant ITS-0325657.  
This paper does not necessarily reflect the position 
of the U.S. Government. 
References  
D. Appelt, J. Hobbs, J. Bear, D. Israel, M. Kameyama,  
A. Kehler, D. Martin, K. Meyers, and M. Tyson 
1996. SRI International FASTUS system: MUC-6 test 
results and analysis. In Proceedings of the Sixth 
Message Understanding Conference.  
H. L. Chieu, H. T. Ng, & Y. K. Lee. 2003. Closing the 
Gap: Learning-Based Information Extraction 
Rivaling Knowledge-Engineering Methods. In 
Proceedings of the 41st Annual Meeting of the 
Association for Computational Linguistics.  
M. Collins and S. Miller. 1998. Semantic Tagging using 
a Probabilistic Context Free Grammar, In 
Proceedings of the Sixth Workshop on Very Large 
Corpora. 
M. Collins and N. Duffy. 2001. Convolution Kernels for 
Natural Language, Advances in Neural Information 
Processing Systems 14, MIT Press. 
D. Fisher, S. Soderland, J. McCarthy, F. Feng and W. 
Lehnert. 1996. Description of The UMass System As 
Used For MUC-6. In Proceedings of the Sixth 
Message Understanding Conference. 
R. Grishman. 1996. The NYU System for MUC-6 or 
Where's the Syntax?. In Proceedings of the Sixth 
Message Understanding Conference.  
H. Lodhi, C. Sander, J. Shawe-Taylor, N. Christianini 
and C. Watkins. 2002. Text Classification using 
String Kernels. Journal of Machine Learning 
Research. 
A. Meyers, R. Grishman, M. Kosaka and S. Zhao. 2001. 
Covering Treebanks with GLARF. In Proceedings of 
of the ACL Workshop on Sharing Tools and 
Resources. 
S. Miller, M. Crystal, H. Fox, L. Ramshaw, R. 
Schwartz, R. Stone, and R. Weischedel. 1998. BBN: 
Description of The SIFT System As Used For MUC-
7, In Proceedings of the Seventh Message 
Understanding Conference. 
K.-R. M?ller, S. Mika, G. Ratsch, K. Tsuda, B. 
Scholkopf. 2001. An introduction to kernel-based 
learning algorithms, IEEE Trans. Neural Networks, 
12, 2, pages 181-201. 
E. Riloff. 1993. Automatically constructing a dictionary 
for information extraction tasks. In Proceedings of 
the 11th National Conference on Artificial 
Intelligence, 811-816. 
V. N. Vapnik. 1998. Statistical Learning Theory. Wiley-
Interscience Publication. 
D. Zelenko, C. Aone and A. Richardella. 2003. Kernel 
methods for relation extraction. Journal of Machine 
Learning Research. 
 
Cross-lingual Information Extraction System Evaluation
Kiyoshi Sudo, Satoshi Sekine, and Ralph Grishman
Department of Computer Science
New York University
715 Broadway, 7th Floor,
New York, NY 10003
 
sudo,sekine,grishman  @cs.nyu.edu
Abstract
In this paper, we discuss the performance of cross-
lingual information extraction systems employing
an automatic pattern acquisition module. This mod-
ule, which creates extraction patterns starting from
a user?s narrative task description, allows rapid cus-
tomization to new extraction tasks. We compare two
approaches: (1) acquiring patterns in the source lan-
guage, performing source language extraction, and
then translating the resulting templates to the tar-
get language, and (2) translating the texts and per-
forming pattern discovery and extraction in the tar-
get language. We demonstrate an average of 8-10%
more recall using the first approach. We discuss
some of the problems with machine translation and
their effect on pattern discovery which lead to this
difference in performance.
1 Introduction
Research in information extraction (IE) and its re-
lated fields has led to a wide range of applications
in many domains. The portability issue of IE sys-
tems across different domains, however, remains a
serious challenge. This problem is being addressed
through automatic knowledge acquisition methods,
such as unsupervised learning for domain-specific
lexicons (Lin et al, 2003) and extraction patterns
(Yangarber, 2003), which require the user to pro-
vide only a small set of lexical items of the target
classes or extraction patterns for the target domain.
The idea of a self-customizing IE system emerged
recently with the improvement of pattern acquisi-
tion techniques (Sudo et al, 2003b), where the IE
system customizes itself across domains given by
the user?s query.
Furthermore, there are demands for access to in-
formation in languages different from the user?s
own. However, it is more challenging to provide
an IE system where the target language (here, En-
glish) is different from the source language (here,
Japanese): a cross-lingual information extraction
(CLIE) system.
In this research, we explore various methods
for efficient automatic pattern acquisition for the
CLIE system, including the translation of the en-
tire source document set into the target language.
To achieve efficiency, the resulting CLIE system
should (1) provide a reasonable level of extraction
performance (both accuracy and coverage) and (2)
require little or no knowledge on the user?s part of
the source language. Today, there are basic linguis-
tics tools available for many major languages. We
show how we can take advantage of the tools avail-
able for the source language to boost extraction per-
formance.
The rest of this paper is organized as follows.
Section 2 and 3 discuss the self-adaptive CLIE sys-
tem we assess throughout the paper. In Section 4,
we show the experimental result for entity detec-
tion. Section 5 discusses the problems in translation
that affect the pattern acquisition and Section 6 dis-
cusses related work. Finally, we conclude the paper
in Section 7 with future work.
2 Query-Driven Information Extraction
One approach to IE portability is to have a system
that takes the description of the event type from the
user as input and acquires extraction patterns for the
given scenario. Throughout the paper, we call this
kind of IE system QDIE (Query-Driven Information
Extraction) system, whose typical procedure is il-
lustrated in Figure 1.
QDIE (e.g. (Sudo et al, 2003a)) consists of three
phases to learn extraction patterns from the source
documents for a scenario specified by the user.
First, it applies morphological analysis, depen-
dency parsing and Named Entity (NE) tagging to the
entire source document set, and converts all the sen-
tences in the source document set into dependency
trees. The NE tagging replaces named entities by
their class, so the resulting dependency trees con-
tain some NE class names as leaf nodes. This is
crucial to identifying common patterns, and to ap-
plying these patterns to new text.
Second, the user provides a set of narrative sen-
Figure 1: QDIE Pattern Acquisition
tences describing the scenario (the events of inter-
est). Using these sentences as a retrieval query, the
information retrieval component of QDIE retrieves
representative documents of the scenario specified
by the user (relevant documents).
Then from among all the possible connected sub-
trees of all the sentences in the relevant documents,
the system calculates the score for each pattern can-
didate. The scoring function is based on TF/IDF
scoring in IR literature; a pattern is more relevant
when it appears more in the relevant documents
and less across the entire collection of source docu-
ments. The final output is the ordered list of pattern
candidates.
Note that a pattern candidate contains at least one
NE, so that it can be used to match a portion of a
sentence which contains an instance of the same NE
type. The matched NE instance is then extracted.
The pattern candidates may be simple predicate-
argument structures (e.g. (resign from   C-POST  )
in business domain) or even a complicated subtree
of a sentence which commonly appears in the rel-
evant documents (e.g. (   C-ORG  report personnel
affair (that   C-PERSON  resigns)) ).
3 Cross-lingual Information Extraction
(Riloff et al, 2002) present several approaches to
cross-lingual information extraction (CLIE). They
describe the use of ?cross-language projection? for
CLIE, exploiting the word alignment of documents
in one language and the same documents translated
into a different language by a machine translation
(MT) system. They conducted experiments between
two relatively close languages, English and French.
In the experiment reported here, we will explore
CLIE for two more disparate languages, English
and Japanese.
The QDIE system can be used in a cross-lingual
setting, and thus, the resulting cross-lingual version
of the QDIE system can minimize the requirement
of the user?s knowing the source language. Figure 2
shows two possible ways to achieve this goal.
It may be realized by translating all the docu-
ments of the source language into the target lan-
guage, and then running the monolingual ver-
sion of the QDIE system for the target language
(Translation-based QDIE). In our experiment, we
translated all the source Japanese documents into
English. Then we ran English-QDIE system to get
the extraction patterns, which are used to extract the
entities by pattern matching.
On the other hand, one can first translate the sce-
nario description into the source language and use
it for the monolingual QDIE system for the source
language, assuming that we have access to the tools
for pattern acquisition in the source language. Each
entity in the extracted table is translated into the
target language (Crosslingual-QDIE). In Figure 2,
we implemented this procedure by first translating
the English query into Japanese. 1 Then we ran
Japanese-QDIE system to identify Japanese extrac-
tion patterns. The extraction patterns are used to
extract items to fill the Japanese table. Finally, each
item in the extracted table is separately translated
into English. Note that translating names is easier
than translating the whole sentences.
As we shall demonstrate, the errors introduced by
the MT system impose a significant cost in extrac-
tion performance both in accuracy and coverage of
the target event. However, if basic linguistic anal-
ysis tools are available for the source language, it
is possible to boost CLIE performance by learning
patterns in the source language. In the next section,
we describe an experiment which compares these
two approaches. In the following section, we assess
the difficulty of learning extraction patterns from the
translated source language document set caused by
the errors of the MT system and/or the differences
of grammatical structure of the translated sentences.
We address specifically:
1. The accuracy of NE tagging on MT-ed source
documents and the use of cross-language pro-
jection.
2. How the structural difference in source and tar-
get language affects the extracted patterns.
3. The reduced frequency of the extracted pat-
terns, which makes it difficult for any mea-
surement of pattern relevance to distinguish the
1Note that our current implementation uses the output from
query translation by the MT system. As we note in Section 7,
we plan to investigate the possibility of additional performance
gain by using current crosslingual information retrieval tech-
niques.
Figure 2: Translation-based QDIE System(A) vs Crosslingual QDIE System(B): The user?s query (English),
the source document (Japanese) and the target extracted table (English) are highlighted.
effective patterns of low frequency from the
noise patterns.
4 Experiments
To evaluate the relevance of extraction patterns au-
tomatically learned for CLIE, we conducted exper-
iments for the Translation-based QDIE system and
the Cross-lingual QDIE system on the entity extrac-
tion task, which is to identify all the entities partic-
ipating in relevant events in a given set of Japanese
texts.
4.1 Experimental Setting
Since general NE taggers either are trained on En-
glish sentences or use manually created rules for
English sentences, the deterioration of NE tagger?s
performance cannot be avoided if it is applied to
the MT-ed English sentences. This causes the
Translation-based QDIE system to identify fewer
pattern candidates from the relevant documents
since a pattern candidate must contain at least one
of the NE types.
To remedy this problem, we incorporated ?cross-
language projection? (Riloff et al, 2002) only for
Named Entities. We used word alignment obtained
by using Giza++ (Och and Ney, 2003) to get names
in the English translation from names in the original
Japanese sentences. Note that it is extremely diffi-
cult to make an alignment of case markers where
one language explicitly renders a marker as a word
and the other does not. So, direct application of
(Riloff et al, 2002) is not suitable for this experi-
ment.
We compare the following three systems in this
experiment.
1. Crosslingual QDIE system
2. Translation-based QDIE system with word
alignment
3. Translation-based QDIE system without word
alignment
4.2 Data
The scenario for this experiment is the Management
Succession scenario of MUC-6(muc, 1995), where
corporate managers assumed and/or left their posts.
We used a much simpler template structure than the
one used in MUC-6, with Person, Organization, and
Post slots. To assess system performance, we mea-
sure the accuracy of the system at identifying the
participating entities in a management succession
event. This task does not involve grouping entities
associated with the same event into a single tem-
plate, in order to avoid possible effects of merging
failure on extraction performance for entities.
The source document set from which the extrac-
tion patterns are learned consists of 132,996 Yomi-
uri Newspaper articles from 1998. For our Crosslin-
gual QDIE system, all the documents are morpho-
logically analyzed by JUMAN (Kurohashi, 1997)
and converted into dependency trees by KNP (Kuro-
hashi and Nagao, 1994). For the Translation-based
QDIE system, all the documents are translated into
English by a commercial machine translation sys-
tem (IBM ?King of Translation?), and converted
into dependency trees by a corpus-based parser. We
retrieved 1500 documents as relevant documents.
We accumulated the test set of documents by a
simple keyword search. The test set consists of
100 Yomiuri Newspaper articles from 1999, out of
which only 61 articles contain at least one manage-
ment succession event. Note that all NE in the test
documents both in the original Japanese and in the
translated English sentences were identified man-
ually, so that the task can measure only how well
extraction patterns can distinguish the participating
entities from the entities that are not related to any
succession events. Table 1 shows the details of the
test data.
4.3 Results
Each pattern acquisition system outputs a list of the
pattern candidates ordered by the ranking function.
The resulting performance is shown as a precision-
Documents 100
(relevant + irrelevant)  
	
Names Person: 173 + 651
(relevant + irrelevant) Org: 111 + 709
Post: 210 + 626
Table 1: Statistics of Test Data
recall graph for each subset of top-  ranked patterns
where  ranges from 1 to the number of pattern can-
didates. The parameters for each system are tuned
to maximize the performance on separate validation
data.
The association of NE classes in the matched pat-
terns and slots in the template is made automati-
cally; Person, Organization, Post (slots) correspond
to C-PERSON, C-ORG, C-POST (NE-classes), re-
spectively, in the Management Succession scenario.
Figure 3 shows the precision-recall curve for the
top 1000 patterns acquired by each system on the
entity extraction task. Crosslingual QDIE system
reaches a maximum recall of 60%, which is sig-
nificantly better than Translation-based QDIE with
word alignment (52%) and Translation-based QDIE
without word alignment (41%). Within the high re-
call range, Crosslingual QDIE system generally had
better precision at the same recall than Translation-
based QDIE systems. At the low recall range ( 
 ), the performance is rather noisy.
Translation-based QDIE without word align-
ment performs similarly to Translation-based QDIE
with word alignment up to its maximum recall
(41%). Translation-based QDIE with word align-
ment reached 10% higher maximum recall (52%).
5 Problems in Translation
The detailed analysis of the result revealed the effect
of several problems caused by the MT system. The
current off-the-shelf MT system?s output resulted in
difficulty in using it as a source of extraction pat-
terns. In this section we will discuss the types of dif-
ferences between the source and target languages,
and their effect on pattern discovery.
Lexical differences Abbreviations in the source
language may not have their corresponding short
form in the target language. For example, ?Kei-
Dan-Ren? is an abbreviation of ?Keizai Dantai
Rengo-kai? which is an organization whose English
translation is ?Japan Federation of Economic Orga-
nizations?. Such abbreviations may not be listed in
the dictionary of the MT system. In such cases, the
literal translation of the abbreviation may be diffi-
cult to recognize as a name and is likely to be treated
as a common noun phrase.
Structural differences Some phrases in the
source language may have more than one relevant
translation. Depending upon the context where
a phrase appears, the MT system has to choose
one among the possible translations. Moreover,
the MT system may make a mistake, of course,
and output an erroneous translation. This results
in a diverse distribution of extraction patterns in
the target language. Figure 4 shows an exam-
ple of such a case. Suppose an extraction pattern
((   C-POST  -ni) shuninsuru) appears 20 times in
the original Japanese document set, out of which
it may be translated 10 times as (be appointed (to
(   C-POST  ))), 5 times as (assume (   C-POST  )),
3 times as (be inaugurated (as (   C-POST  ))), and
2 times as an erroneous translation. Some of the
lower frequency translated patterns will be ranked
lower by the scoring function and so will be hard to
distinguish from noise.
Figure 4: Example of Structural Difference in
Translation: The translation of a Japanese expression
into several English different expressions including erro-
neous ones.
Figure 5 shows an example of the case where
the context around the name did not seem to be
translated properly, so the dependency tree for the
sentence was not correct. The right translation is
?Okajima announced that President Hiroyuki Oka-
jima, 40 years old, resigned formally ...? which re-
sults in the dependency between the main verb ?an-
nounce? and the company ?Okajima?. The trans-
lation shown in Figure 5 not only shows incor-
rect word-translations, but also shows ungrammat-
ical structure, including too many relative clauses.
The structural error causes the errors in the depen-
dency parse tree including having ?end? as a root of
the entire tree and the wrong dependency from ?an-
nounced? to ?the major department? in Figure 5 2.
Thus, the accumulation of the errors resulted in
missing the organization name ?Okajima?.
Also, the conjunctions in Japanese sentences
could not be translated properly, and therefore, the
2The head is ?the major department? and ?announced? is
modifying the head.
020
40
60
80
100
0 10 20 30 40 50 60 70 80
Pr
ec
is
io
n 
(%
)
Recall (%)
Precision-Recall
Crosslingual
Translation with WA
Translation without WA
Figure 3: Performance Comparison on Entity Extraction Task
English dependency parser?s output is significantly
deteriorated. The example in Figure 6 shows the
case where both ?Mr. Suzuki? and ?Mr. Asada?
were inaugurated. In the original Japanese sentence,
?Mr. Suzuki? is closer to the verb ?be inaugurated?.
So, it seems that the MT system tries to find another
verb for ?Mr. Asada?, and attaches it (incorrectly)
to ?unofficially arranged?.
Out-of-Vocabulary Words The MT system may
not have a word in the source language dictionary, in
which case some MT systems output it in the origi-
nal script in the source language. This happens not
only for names but also for sentences which are er-
roneously segmented into words. Such problems, of
course, may make it hard to detect Named Entities
and get a correct dependency tree of the sentence.
However, translation of names is easier than
translation of contexts; the MT system can output
the transliteration of an unknown word. In fact,
name translation of the MT system we used for
this experiment is better than the sentence transla-
tion of the same MT system. The names appro-
priately extracted from Japanese documents by the
Crosslingual QDIE system, in most cases, are cor-
rectly translated or transliterated if no equivalent
translation exists.
6 Related Work
The work closest to ours is (Riloff et al, 2002).
They showed how IE learning tools, bitext align-
ment, and an MT system can be combined to cre-
ate CLIE systems between English and French.
They evaluated a variety of methods, including one
similar to our Translation-based QDIE. Their ap-
proaches were less reliant on language tools for the
?source? language (in their case, French) than our
Crosslingual-QDIE system. On the other hand, their
tests were made on a closer language pair (English
- French). We expect that the performance gap be-
tween Translation-based IE and Crosslingual IE is
more pronounced with a more divergent language
pair like Japanese and English.
There are interesting parallels between our work
and that of (Douzidia and Lapalme, 2004), who dis-
cussed the role of machine translation in a cross-
lingual summarization system which produces an
English summary from Arabic text. Their system
took the same path as our Crosslingual QDIE: sum-
marizing the Arabic text directly and only translat-
ing the summary, rather than translating the entire
Arabic text and summarizing the translation. They
had similar motivations: different translations pro-
duced by the MT system for the same word in dif-
ferent contexts, as well as translation errors, would
interfere with the summarization process.
The trade-offs, however, are not the same for the
two applications. For summarization either path
requires an MT system which can translate entire
sentences (either the original text or the summary).
Translation-based QDIE has a similar requirement,
Output of MT system:
From Muika the term settlement of accounts ended February , 99 having become
the prospect of the first deficit settlement of accounts after the war etc. ,
six of President Hiroyuki Okajima ( 40 ) , two managing directors , one man-
aging directors , the full-time directors that are 13 persons submitted the
resignation report , ?Okajima? of Marunouchi , Kofu-shi who is the major de-
partment store within the prefecture announced that he resigns formally by
the fixed general meeting of shareholders of the company planned at the end of
this month .
Output of Dependency Tree (part):
Figure 5: Example of Translation Errors: Figure also contains a part of the dependency parser?s output of the
sentence. Dashed lines show the correct dependencies.
but Crosslingual QDIE reduces the demands on MT:
only query translation and name translation are re-
quired.
7 Conclusion
We discussed the difficulty in cross-lingual infor-
mation extraction caused by the translation of the
source documents using an MT system. The ex-
perimental result for entity extraction suggests that
exploiting some basic tools available for the source
language will boost the performance of the whole
CLIE system.
We intend to investigate whether further perfor-
mance gain may be obtained by introducing ad-
ditional techniques for query translation. These
techniques, including query translation on expanded
queries and building a translation dictionary from
parallel corpora, are currently used in crosslingual
information retrieval (Larkey and Connell, 2003).
Acknowledgments
This research was supported in part by the De-
fense Advanced Research Projects Agency as part
of the Translingual Information Detection, Extrac-
tion and Summarization (TIDES) program, un-
der Grant N66001-001-1-8917 from the Space and
Naval Warfare Systems Center, San Diego, and by
the National Science Foundation under Grant ITS-
00325657. This paper does not necessarily reflect
the position of the U.S. Government.
References
Fouad Soufiane Douzidia and Guy Lapalme. 2004.
Lakhas, an Arabic summarization system. In
Proceedings of DUC2004.
Sadao Kurohashi and Makoto Nagao. 1994. KN
parser : Japanese dependency/case structure an-
alyzer. In Proceedings of the Workshop on
Sharable Natural Language Resources.
Sadao Kurohashi, 1997. Japanese Morphological
Analyzing System: JUMAN. http://www.kc.t.u-
tokyo.ac.jp/nl-resource/juman-e.html.
Leah Larkey and Margaret Connell. 2003. Struc-
tured Queries, Language Modeling, and Rele-
vance Modeling in Cross-Language Information
Retrieval. In Information Processing and Man-
agement, Special Issue on Cross Language Infor-
mation Retrieval.
Winston Lin, Roman Yangarber, and Ralph Grish-
man. 2003. Bootstrapped Learning of Seman-
tic Classes from Positive and Negative Examples.
In Proceedings of the ICML-2003 Workshop on
The Continuum from Labeled to Unlabeled Data,
Washington, D.C.
1995. Proceedings of the Sixth Message Under-
Output of MT system:
The personnel affairs to which managing director Shosei Suzuki ( 57 ) is in-
augurated as the Nippon Telegraph and Telephone president of the holding com-
pany which NTT will be the board of directors on the seventh , is inaugu-
rated by NTT reorganization on July 1 at President Jun-ichiro Miyatsu ( 63
) the Nippon Telegraph Telephone East Corporation ( NTT East Japan ) presi-
dent of a local communication company , he is inaugurated as Vice President
Shuichi Inoue ( 61 ) Nippon Telegraph Telephone West Corporation ( NTT west-
ern part of Japan ) were unofficially arranged Vice President Kazuo Asada
( 59 ) the NTT Communications president of a long distance international-
telecommunications company .
Output of Dependency Tree (part):
Figure 6: Example of Erroneous Conjunction Phrase: Figure also contains a part of the dependency parser?s
output of the sentence. Dashed lines show the correct dependencies.
standing Conference (MUC-6), Columbia, MD,
November. Morgan Kaufmann.
Franz Josef Och and Hermann Ney. 2003. A
Systematic Comparison of Various Statistical
Alignment Models. Computational Linguistics,
29(1):19?51.
Ellen Riloff, Charles Schafer, and David Yarowsky.
2002. Inducing Information Extraction Systems
for New Languages via Cross-Language Projec-
tion. In Proceedings of the 19th International
Conference on Computational Linguistics (COL-
ING 2002).
Kiyoshi Sudo, Satoshi Sekine, and Ralph Grishman.
2003a. An Improved Extraction Pattern Repre-
sentation Model for Automatic IE Pattern Acqui-
sition. In Proceedings of the 41st Annual Meet-
ing of Association of Computational Linguistics
(ACL 2003), Sapporo, Japan.
Kiyoshi Sudo, Satoshi Sekine, and Ralph Grish-
man. 2003b. Pre-CODIE ? Crosslingual On-
Demand Information Extraction. In Proceedings
of HLT/NAACL 2003, Edmonton, Canada.
Roman Yangarber. 2003. Counter-Training in Dis-
covery of Semantic Patterns. In Proceedings
of the 41st Annual Meeting of Association of
Computational Linguistics (ACL 2003), Sapporo,
Japan.
Automatic Pattern Acquisition
for Japanese Information Extraction
Kiyoshi Sudo
Computer Science
Department
New York University
715 Broadway, 7th floor,
New York, NY 10003 USA
sudo@cs.nyu.edu
Satoshi Sekine
Computer Science
Department
New York University
715 Broadway, 7th floor,
New York, NY 10003 USA
sekine@cs.nyu.edu
Ralph Grishman
Computer Science
Department
New York University
715 Broadway, 7th floor,
New York, NY 10003 USA
grishman@cs.nyu.edu
ABSTRACT
One of the central issues for information extraction is the cost of
customization from one scenario to another. Research on the auto-
mated acquisition of patterns is important for portability and scala-
bility. In this paper, we introduce Tree-Based Pattern representation
where a pattern is denoted as a path in the dependency tree of a sen-
tence. We outline the procedure to acquire Tree-Based Patterns in
Japanese from un-annotated text. The system extracts the relevant
sentences from the training data based on TF/IDF scoring and the
common paths in the parse tree of relevant sentences are taken as
extracted patterns.
Keywords
Information Extraction, Pattern Acquisition
1. INTRODUCTION
Information Extraction (IE) systems today are commonly based
on pattern matching. New patterns need to be written when we
customize an IE system for a new scenario (extraction task); this is
costly if done by hand. This has led to recent research on automated
acquisition of patterns from text with minimal pre-annotation. Riloff
[4] reported a successful result for her procedure that needs only
a pre-classified corpus. Yangarber [6] developed a procedure for
unannotated natural language texts.
One of their common assumption is that the relevant documents
include good patterns. Riloff implemented this idea by applying the
pre-defined heuristic rules to pre-classified (relevant) documents
and Yangarber advanced further so that the system can classify the
documents by itself given seed patterns specific to a scenario and
then find the best patterns from the relevant document set.
Considering how they represent the patterns, we can see that,
in general, Riloff and Yangarber relied on the sentence structure
of English. Riloff?s predefined heuristic rules are based on syn-
tactic structures, such as ?<subj> active-verb? and ?active-verb
.
<dobj>?. Yangarber used triples of a predicate and some of its
arguments, such as ?<pred> <subj> <obj>?.
The Challenges
Our careful examination of Japanese revealed some of the chal-
lenges for automated acquisition of patterns and information ex-
traction on Japanese(-like) language and other challenges which
arise regardless of the languages.
Free Word-ordering
Free word order is one of the most significant problems in analyz-
ing Japanese. To capture all the possible patterns given a predicate
and its arguments, we need to permute the arguments and list all the
patterns separately. For example, for ?<subj> <dobj> <iobj>
<predicate>? with the constraint that the predicate comes last in
the sentence, there would be six possible patterns (permutations of
three arguments). The number of patterns to cover even simple
facts would rise unacceptably high.
Flexible case marking system
There is also a difficulty in a language with a flexible case marking
system, like Japanese. In particular, we found that, in Japanese,
some of the arguments that are usually marked as object in En-
glish were variously marked by different post-positions, and some
case markers (postpositions) are used for marking more than one
grammatical category in different situations. For example, the topic
marker in Japanese, ?wa?, can mark almost any entity that would
have been variously marked in English. It is difficult to deal with
this variety by simply fixing the number of arguments of a predicate
for creating patterns in Japanese.
Relationships beyond direct predicate-argument
Furthermore, we may want to capture the relationship between a
predicate and a modifier of one of its arguments. In previous ap-
proaches, one had to introduce an ad hoc frame for such a relation-
ship, such as ?verb obj [PP <head-noun>]?, to extract the relation-
ship between ?to assume? and ?<organization>? in the sentence
?<person> will assume the <post> of <organization>?.
Relationships beyond clausal boundaries
Another problem lies in relationships beyond clause boundaries, es-
pecially if the event is described in a subordinate clause. For exam-
ple, for a sentence like ?<organization> announced that <person>
retired from <post>,? it is hard to find a relationship between
<organization> and the event of retiring without the global view
from the predicate ?announce?.
These problems lead IE systems to fail to capture some of the ar-
guments needed for filling the template. Overcoming the problems
above makes the system capable of finding more patterns from the
training data, and therefore, more slot-fillers in the template.
In this paper, we introduce Tree-based pattern representation and
consider how it can be acquired automatically.
2. TREE-BASED PATTERN REPRESENTA-
TION (TBP)
Definition
Tree-based representation of patterns (TBP) is a representation of
patterns based on the dependency tree of a sentence. A pattern is
defined as a path in the dependency tree passing through zero or
more intermediate nodes within the tree. The dependency tree is a
directed tree whose nodes are bunsetsus or phrasal units, and whose
directed arcs denote the dependency between two bunsetsus: A!B
denotes A?s dependency on B (e.g. A is a subject and B is a pred-
icate.) Here dependency relationships are not limited to just those
between a case-marked element and a predicate, but also include
those between a modifier and its head element, which covers most
relationships within sentences. 1
TBP for Information Extraction
Figure 2 shows how TBP is used in comparison with the word-
order based pattern, where A...F in the left part of the figure is a
sequence of the phrasal units in a sentence appearing in this or-
der and the tree in the right part is its dependency tree. To find
the relationship between B!F, a word-order based pattern needs a
dummy expression to hold C, D and E, while TBF can denote the
direct relationship as B!F. TBP can also represent a complicated
pattern for a node which is far from the root node in the depen-
dency tree, like C!D!E, which is hard to represent without the
sentence structure.
For matching with TBP, the target sentence should be parsed into
a dependency tree. Then all the predicates are detected and the
subtrees which have a predicate node as a root are traversed to find
a match with a pattern.
Benefit of TBP
TBP has some advantages for pattern matching over the surface
word-order based patterns in addressing the problems mentioned
in the previous section:
 Free word-order problem
TBP can offer a direct representation of the dependency re-
lationship even if the word-order is different.
 Free case-marking problem
TBP can freely traverse the whole dependency tree and find
any significant path as a pattern. It does not depend on pre-
defined case-patterns as Riloff [4] and Yangarber [6] did.
 Indirect relationships
TBP can find indirect relationships, such as the relationship
between a predicate and the modifier of the argument of the
1In this paper, we used the Japanese parser KNP [1] to obtain the
dependency tree of a sentence.
predicate. For example, the pattern
?<organization> of!<post> to!appoint? can capture the rela-
tionship between ?<organization>? and ?to be appointed?
in the sentence
?<person> was appointed to <post> of <organization>.?
 Relationships beyond clausal boundaries
TBP can capture relationships beyond clausal boundaries.
The pattern ?<post> to!appointCOMP! announce? can find the
relationship between ?<post>? and ?to announce?. This re-
lationship, later on, can be combined with the relationship
?<organization>? and ?to announce? and merged into one
event.
3. ALGORITHM
In this section, we outline our procedure for automatic acquisi-
tion of patterns. We employ a cascading procedure, as is shown
in Figure 3. First, the original documents are processed by a mor-
phological analyzer and NE-tagger. Then the system retrieves the
relevant documents for the scenario as a relevant document set. The
system, further, selects a set of relevant sentences as a relevant sen-
tence set from those in the relevant document set. Finally, all the
sentences in the relevant sentence set are parsed and the paths in
the dependency tree are taken as patterns.
3.1 Document Preprocessing
Morphological analysis and Named Entity (NE) tagging is per-
formed on the training data at this stage. We used JUMAN [2] for
the former and a NE-system which is based on a decision tree algo-
rithm [5] for the latter. Also the part-of-speech information given
by JUMAN is used in the later stages.
3.2 Document Retrieval
The system first retrieves the documents that describe the events
of the scenario of interest, called the relevant document set. A set
of narrative sentences describing the scenario is selected to create
a query for the retrieval. For this experiment, we set the size of
the relevant document set to 300 and retrieved the documents us-
ing CRL?s stochastic-model-based IR system [3], which performed
well in the IR task in IREX, Information Retrieval and Extraction
evaluation project in Japan 2. All the sentences used to create the
patterns are retrieved from this relevant document set.
3.3 Sentence Retrieval
The system then calculates the TF/IDF-based score of relevance
to the scenario for each sentence in the relevant document set and
retrieves the n most relevant sentences as the source of the patterns,
where n is set to 300 for this experiment. The retrieved sentences
will be the source for pattern extraction in the next subsection.
First, the TF/IDF-based score for every word in the relevant doc-
ument set is calculated. TF/IDF score of word w is:
score(w) =
(
TF (w) 
log(N+0:5)
DF (w)
log(N+1)
if w is Noun, Verb or Named Entity
0 otherwise
where N is the number of documents in the collection, TF(w) is
the term frequency of w in the relevant document set and DF(w) is
the document frequency of w in the collection.
Second, the system calculates the score of each sentence based
on the score of its words. However, unusually short sentences and
2IREX Homepage: http://cs.nyu.edu/cs/projects/proteus/irex
Dependency Tree Tree-Based Pattern
f
f f
f f
 
 
 
 
 
 
 
@
@
@
@
@
@
@
 
 
 
 
 
 
 
@
@
@
@
@
@
@
<organization>-wa
<organization>-TOPIC
<person>-ga
<person>-SUBJ
<post>-kara
<post>-FROM
taininsuru(retire)
happyosuru(announce)
 
 
 
 	
1
@
@
@
@R
3
@
@
@
@
@
 
 
 
 
 	
2
f
f
f
f
f
f
f
-
- -
-
happyosuru(announce)
taininsuru(retire)
<organization>-wa(<organization>-TOPIC)
<person>-ga(<person>-SUBJ)
<post>-kara(<post>-FROM)
1
2
3
Figure 1: Tree-Based Pattern Representation
Word-order Pattern
A B C D E F
6
Pattern [B * F]
F
E
BA
D
C


3
P
P
P
Pq


3
Q
Q
s
-
Pattern [B!F]
P
P
P
P
P
P
P
P
Pq
Pattern [C!E!F]TBP
Figure 2: Extraction using Tree-Based Pattern Representation
unusually long sentences will be penalized. The TF/IDF score of
sentence s is:
score(s) =
P
w2s
score(w)
length(s) + jlength(s) AVEj
where length(s) is the number of words in s, and AVE is the av-
erage number of words in a sentence.
3.4 Pattern Extraction
Based on the dependency tree of the sentences, patterns are ex-
tracted from the relevant sentences retrieved in the previous sub-
section. Figure 4 shows the procedure. First, the retrieved sentence
is parsed into a dependency tree by KNP [1] (Stage 1). This stage
also finds the predicates in the tree. Second, the system takes all
the predicates in the tree as the roots of their own subtrees, as is
shown in (Stage 2). Then each path from the root to a node is
extracted, and these paths are collected and counted across all the
relevant sentences. Finally, the system takes those paths with fre-
quency higher than some threshold as extracted patterns. Figure 5
shows examples of the acquired patterns.
4. EXPERIMENT
It is not a simple task to evaluate how good the acquired pat-
terns are without incorporating them into a complete extraction sys-
tem with appropriate template generation, etc. However, finding a
match of the patterns and a portion of the test sentences can be a
good measure of the performance of patterns.
The task for this experiment is to find a bunsetsu, a phrasal unit,
that includes slot-fillers by matching the pattern to the test sentence.
The performance is measured by recall and precision in terms of the
number of slot-fillers that the matched patterns can find; these are
calculated as follows.
Recall =
# of Matched Relevant SlotF illers
# of All Relevant SlotF illers
Original Document Set
Document
Preprocessing
-
Preprocessed Document Set
Document
Retrieval
-
Relevant Document Set
Sentence Retrieval
 
 
 
 
 
 
 
	
Relevant Sentence Set
(Tree representation)
f
f
f
f
f



P
P
P
P




P
P
P
P
Pattern
Extraction
-
f f f
f f f
f f
Extracted Patterns
Figure 3: Pattern Acquisition Procedure Overall Process
Precision =
# of Matched Relevant SlotF illers
# of All Matched SlotF illers
The procedure proposed in this paper is based on bunsetsus, and
an individual bunsetsu may contain more than one slot filler. In
such cases the procedure is given credit for each slot filler.
Strictly speaking, we don?t know how many entities in a matched
pattern might be slot-fillers when, actually, the pattern does not
contain any slot-fillers (in the case of over-generating). We ap-
proximate the potential number of slot-fillers by assigning 1 if the
(falsely) matched pattern does not contain any Named-Entities, or
assigning the number of Named-Entities in the (falsely) matched
pattern. For example, if we have a pattern ?go to dinner? for a
management succession scenario and it matches falsely in some
part of the test sentences, this match will gain one at the number
of All Matched Slot-fillers (the denominator of the precision). On
the other hand, if the pattern is ?<post> <person> laugh? and it
falsely matches ?President Clinton laughed?, this will gain two, the
number of the Named Entities in the pattern.
For the sake of comparison, we defined the baseline system with
the patterns acquired by the same procedure but only from the di-
rect relationships between a predicate and its arguments (PA in Fig-
ure 6 and 7).
We chose the following two scenarios.
 Executive Management Succession: events in which corpo-
rate managers left their positions or assumed new ones re-
gardless of whether it was a present (time of the report) or
past event.
Items to extract: Date, person, organization, title.
 Robbery Arrest: events in which robbery suspects were ar-
rested.
Items to extract: Date, suspect, suspicion.
4.1 Data
Management Succession
Documents 15
Sentences 79
DATE 43
PERSON 41
ORGANIZATION 22
OLD-ORGANIZATION 2
NEW-POST 30
OLD-POST 39
Table 1: Test Set for Management Succession scenario
Robbery Arrest
Documents 28
Sentences 182
DATE 26
SUSPICION 34
SUSPECT 50
Table 2: Test Set for Robbery Arrest scenario
For all the experiments, we used the Mainichi-Newspaper-95
corpus for training. As described in the previous section, the system
retrieved 300 articles for each scenario as the relevant document set
from the training data and it further retrieved 300 sentences as the
relevant sentence set from which all the patterns were extracted.
Test data was taken from Mainichi-Newspaper-94 by manually
reviewing the data for one month. The statistics of the test data are
shown in Table 1 and 2.
4.2 Results
Figure 6 and Figure 7 illustrates the precision-recall curve of this
Stage 1 (Dependency Tree)
<org>-wa
<psn>-ga
<post>-ni
shuninsuru(p)-to
happyoshita(p)
"
"
b
b
"
"
b
b
f
f
f
f
f
-
Stage 2 (Separated Trees)
<org>-wa
<psn>-ga
<post>-ni
happyoshita(p)


Q
Q
k
"
"
b
b
"
"
b
b
Q
Q
k




+


+
1
2
3
4
f
f
f
f
f
<psn>-ga
<post>-ni
shuninsuru(p)
b
b
"
"
Q
Q
k


+
5
6
f
f
f
(p indicates the node is a predicate.)
Extracted Patterns
1 <organization>-wa ! happyosuru
2 <person>-ga ! shuninsuru-to ! happyosuru
3 <post>-ni ! shuninsuru-to ! happyosuru
4 shuninsuru-to ! happyosuru
5 <person>-ga ! shuninsuru
6 <post>-ni ! shuninsuru
Japanese sentence :
English Translation :
<organization>-wa
<organization>-TOPIC
<person>-ga
<person>-SBJ
<post>-ni
<post>-TO
shuninsuru-to
start-COMP
happyoshita.
announced.
(<organization> announced that <person> was appointed to <post>.)
Figure 4: Pattern Acquisition from ?<org>-wa <psn>-ga <pst>-ni shuninsuru-to happyoshita.?
experiment for the executive management succession scenario and
robbery arrest scenario, respectively. We ranked all the acquired
patterns by calculating the sum of the TF/IDF-based score (same
as for sentence retrieval in Section 3.3) for each word in the pattern
and sorting them on this basis. Then we obtained the precision-
recall curve by changing the number of the top-ranked patterns in
the list.
Figure 6 shows that TBP is superior to the baseline system both
in recall and precision. The highest recall for TBP is 34% while the
baseline gets 29% at the same precision level. On the other hand,
at the same level of recall, TBP got higher precision (75%) than the
baseline (70%).
We can also see from Figure 6 that the curve has a slightly anoma-
lous shape where at lower recall (below 20%) the precision is also
low for both TBP and the baseline. This is due to the fact that
the pattern lists for both TBP and the baseline contains some non-
reliable patterns which get a high score because each word in the
patterns gets higher score than others.
Figure 7 shows the result of this experiment on the Robbery Ar-
rest scenario. Although the overall recall is low, TBP achieved
higher precision and recall (as high as 30% recall at 40% of pre-
cision) than the baseline except at the anomalous point where both
TBP and the baseline got a small number of perfect slot-fillers by a
highly ranked pattern, namely ?gotoyogi-de ! taihosuru (to arrest
0 20 40 60 80 100
Precision
0
20
40
60
Recall
+ +
+
+ +
+ +
++
++
+++
+
* *
*
* *
*
*
**
*
**
+ ... TBP
* ... Baseline (PA)
Figure 6: Result on Management Succession Scenario
Scenario Patterns
Executive Succession : <post>-ni ! shokakusuru (to be promoted to <post>)
<post>-ni ! shuninsuru (to assume <post>)
<post>-ni ! shokakusuru ! (to announce an informal decision of promoting
<jinji>-o ! happyosuru somebody to <post>)
Robbery Arrest : satsujin-yogi-de ! taihosuru (to arrest in suspicion of murder)
<date> ! taihosuru (to arrest on <date>)
satsujin-yogi-de ! taihosuru (to arrest in suspicion of murder)
<person>-yogisha ! #-o ! taihosuru (to arrest the suspect, <person>, age #)
Figure 5: Acquired Patterns
0 20 40 60 80 100
Precision
0
20
40
60
Recall
+
+
+
+++
+
++++
*
*
*
**
******
****
+ ... TBP
* ... Baseline (PA)
Figure 7: Result on Robbery Arrest Scenario
on suspicion of robbery)? for the baseline and ?<person> yogisha
! <number>-o ! taihosuru (to arrest the suspect, <person>,
age <number>)?.
5. DISCUSSION
Low Recall
It is mostly because we have not made a class of types of crimes
that the recall on the robbery arrest scenario is low. Once we have
a classifier as reliable as Named-Entity tagger, we can make a sig-
nificant gain in the recall of the system. And in turn, once we have
a class name for crimes in the training data (automatically anno-
tated by the classifier) instead of a separate name for each crime,
it becomes a good indicator to see if a sentence should be used to
acquire patterns. And also, incorporating the classes in patterns can
reduce the noisy patterns which do not carry any slot-fillers of the
template.
For example on the management succession scenario, all the
slot-fillers defined there were able to be tagged by the Named-
Entity tagger [5] we used for this experiment, including the title.
Since we knew all the slot-fillers were in one of the classes, we
also knew those patterns whose argument was not classified any
of the classes would not likely capture slot-fillers. So we could
put more weight on those patterns which contained <person>,
<organization>, <post> and <date> to collect the patterns with
higher performance, and therefore we could achieve high precision.
Erroneous Case Analysis
We also investigated other scenarios, namely train accident and air-
plane accident scenario, which we will not report in this paper.
However, some of the problems which arose may be worth men-
tioning since they will arise in other, similar scenarios.
 Results or Effects of the Target Event
Especially for the airplane accident scenario, most errors were
identified as matching the effect or result of the incident. A
typical example is ?Because of the accident, the airport had
been closed for an hour.? In the airplane accident scenario,
the performance of the document retrieval and the sentence
retrieval is not as good as the other two scenarios, and there-
fore, the frequency of relevant acquired patterns is rather low
because of the noise. Further improvement in retrieval and a
more robust approach is necessary.
 Related but Not-Desired Sentences
If the scenario is specific enough to make it difficult as an IR
task, the result of the document retrieval stage may include
many documents related to the scenario in a broader sense
but not specific enough for IE tasks. In this experiment, this
was the case for the airplane accident scenario. The result of
document retrieval included documents about other accidents
in general, such as traffic accidents. Therefore, the sentence
retrieval and pattern acquisition for these scenarios were af-
fected by the results of the document retrievals.
6. FUTURE WORK
Information Extraction
To apply the acquired patterns to an information extraction task,
further steps are required besides those mentioned above. Since
the patterns are a set of the binary relationships of a predicate and
another element, it is necessary to merge the matched elements into
a whole event structure.
Necessity for Generalization
We have not yet attempted any (lexical) generalization of pattern
candidates. The patterns can be expanded by using a thesaurus
and/or introducing a new (lexical) class suitable for a particular
domain. For example, the class of expressions of flight number
clearly helps the performance on the airplane accident scenario.
Especially, the generalized patterns will help improve recall.
Robust Pattern Extraction
As is discussed in the previous section, the performance of our sys-
tem relies on each component. If the scenario is difficult for the IR
task, for example, the whole result is affected. The investigation of
a more conservative approach would be necessary.
Translingualism
The presented results show that our procedure of automatic pat-
tern acquisition is promising. The procedure is quite general and
addresses problems which are not specific to Japanese. With an ap-
propriate morphological analyzer, a parser that produces a depen-
dency tree and an NE-tagger, our procedure should be applicable to
almost any language.
7. ACKNOWLEDGMENTS
This research is supported by the Defense Advanced Research
Projects Agency as part of the Translingual Information Detec-
tion, Extraction and Summarization (TIDES) program, under Grant
N66001-00-1-8917 from the Space and Naval Warfare Systems Cen-
ter San Diego. This paper does not necessarily reflect the position
or the policy of the U.S. Government.
8. REFERENCES
[1] S. Kurohashi and M. Nagao. Kn parser : Japanese
dependency/case structure analyzer. In the Proceedings of the
Workshop on Sharable Natural Language Resources, 1994.
[2] Y. Matsumoto, S. Kurohashi, O. Yamaji, Y. Taeki, and
M. Nagano. Japanese morphological analyzing system:
Juman. Kyoto University and Nara Institute of Science and
Technology, 1997.
[3] M. Murata, K. Uchimoto, H. Ozaku, and Q. Ma. Information
retrieval based on stochastic models in irex. In the
Proceedings of the IREX Workshop, 1994.
[4] E. Riloff. Automatically generating extraction patterns from
untagged text. In the Proceedings of Thirteenth National
Conference on Artificial Intelligence (AAAI-96), 1996.
[5] S. Sekine, R. Grishman, and H. Shinnou. A decision tree
method for finding and classifying names in japanese texts. In
the Proceedings of the Sixth Workshop on Very Large
Corpora, 1998.
[6] R. Yangarber, R. Grishman, P. Tapanainen, and S. Huttunen.
Unsupervised discovery of scnario-level patterns for
information extraction. In the Proceedings of the Sixth Applied
Natural Language Processing Conference, 2000.
Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 17?24, Vancouver, October 2005. c?2005 Association for Computational Linguistics
Using Semantic Relations to Refine Coreference Decisions 
 
 
Heng Ji David Westbrook Ralph Grishman 
Department of Computer Science 
New York University 
New York, NY, 10003, USA 
hengji@cs.nyu.edu westbroo@cs.nyu.edu grishman@cs.nyu.edu 
 
 
 
 
Abstract 
We present a novel mechanism for im-
proving reference resolution by using the 
output of a relation tagger to rescore 
coreference hypotheses. Experiments 
show that this new framework can im-
prove performance on two quite different 
languages -- English and Chinese. 
1 Introduction 
Reference resolution has proven to be a major 
obstacle in building robust systems for information 
extraction, question answering, text summarization 
and a number of other natural language processing 
tasks.  
Most reference resolution systems use represen-
tations built out of the lexical and syntactic attrib-
utes of the noun phrases (or ?mentions?) for which 
reference is to be established. These attributes may 
involve string matching, agreement, syntactic dis-
tance, and positional information, and they tend to 
rely primarily on the immediate context of the 
noun phrases (with the possible exception of sen-
tence-spanning distance measures such as Hobbs 
distance). Though gains have been made with such 
methods (Tetreault 2001; Mitkov 2000; Soon et al 
2001; Ng and Cardie 2002), there are clearly cases 
where this sort of local information will not be suf-
ficient to resolve coreference correctly. 
Coreference is by definition a semantic 
relationship: two noun phrases corefer if they both 
refer to the same real-world entity. We should 
therefore expect a successful coreference system to 
exploit world knowledge, inference, and other 
forms of semantic information in order to resolve 
hard cases. If, for example, two nouns refer to 
people who work for two different organizations, 
we want our system to infer that these noun 
phrases cannot corefer. Further progress will likely 
be aided by flexible frameworks for representing 
and using the information provided by this kind of 
semantic relation between noun phrases.  
This paper tries to make a small step in that di-
rection. It describes a robust reference resolver that 
incorporates a broad range of semantic information 
in a general news domain. Using an ontology that 
describes relations between entities (the Auto-
mated Content Extraction program1 relation ontol-
ogy) along with a training corpus annotated for 
relations under this ontology, we first train a classi-
fier for identifying relations. We then apply the 
output of this relation tagger to the task of refer-
ence resolution.  
The rest of this paper is structured as follows. 
Section 2 briefly describes the efforts made by 
previous researchers to use semantic information in 
reference resolution.  Section 3 describes our own 
method for incorporating document-level semantic 
context into coreference decisions. We propose a 
representation of semantic context that isolates a 
particularly informative structure of interaction 
between semantic relations and coreference. Sec-
tion 4 explains in detail our strategies for using 
relation information to modify coreference deci-
sions, and the linguistic intuitions behind these 
strategies. Section 5 then presents the system archi-
tectures and algorithms we use to incorporate rela-
tional information into reference resolution. 
                                                          
1
 The ACE task description can be found at 
http://www.itl.nist.gov/iad/894.01/tests/ace/  and the ACE guidelines at 
http://www.ldc.upenn.edu/Projects/ACE/ 
17
Section 6 presents the results of experiments on 
both English and Chinese test data. Section 7 pre-
sents our conclusions and directions for future 
work.  
2 Prior Work 
Much of the earlier work in anaphora resolution 
(from the 1970?s and 1980?s, in particular) relied 
heavily on deep semantic analysis and inference 
procedures (Charniak 1972; Wilensky 1983; 
Carbonell and Brown 1988; Hobbs et al 1993).  
Using these methods, researchers were able to give 
accounts of some difficult examples, often by 
encoding quite elaborate world knowledge.  
Capturing sufficient knowledge to provide 
adequate coverage of even a limited but realistic 
domain was very difficult. Applying these 
reference resolution methods to a broad domain 
would require a large scale knowledge-engineering 
effort. 
The focus for the last decade has been primarily 
on broad coverage systems using relatively shallow 
knowledge, and in particular on corpus-trained sta-
tistical models.  Some of these systems attempt to 
apply shallow semantic information. (Ge et al 
1998) incorporate gender, number, and animaticity 
information into a statistical model for anaphora 
resolution by gathering coreference statistics on 
particular nominal-pronoun pairs. (Tetreault and 
Allen 2004) use a semantic parser to add semantic 
constraints to the syntactic and agreement con-
straints in their Left-Right Centering algorithm. 
(Soon et al 2001) use WordNet to test the seman-
tic compatibility of individual noun phrase pairs. In 
general these approaches do not explore the possi-
bility of exploiting the global semantic context 
provided by the document as a whole. 
Recently Bean and Riloff (2004) have sought to 
acquire automatically some semantic patterns that 
can be used as contextual information to improve 
reference resolution, using techniques adapted 
from information extraction.  Their experiments 
were conducted on collections of texts in two topic 
areas (terrorism and natural disasters). 
3 Relational Model of Semantic Context 
Our central goal is to model semantic and corefer-
ence structures in such a way that we can take ad-
vantage of a semantic context larger than the 
individual noun phrase when making coreference 
decisions. Ideally, this model should make it possi-
ble to pick out important features in the context 
and to distinguish useful signals from background 
noise. It should, for example, be able to represent 
such basic relational facts as whether the (possibly 
identical) people referenced by two noun phrases 
work in the same organization, whether they own 
the same car, etc.  And it should be able to use this 
information to resolve references even when sur-
face features such as lexical or grammatical attrib-
utes are imperfect or fail altogether.  
In this paper we present a Relational Corefer-
ence Model (abbreviated as RCM) that makes pro-
gress toward these goals.  To represent semantic 
relations, we use an ontology (the ACE 2004 rela-
tion ontology) that describes 7 main types of rela-
tions between entities and 23 subtypes (Table 1).2 
These relations prove to be more reliable guides 
for coreference than simple lexical context or even 
tests for the semantic compatibility of heads and 
modifiers. The process of tagging relations implic-
itly selects relevant items of context and abstracts 
raw lists of modifiers into a representation that is 
deeper, but still relatively lightweight.  
 
Relation Type Example 
Agent-Artifact 
(ART) 
Rubin Military Design, the 
makers of the Kursk 
Discourse (DISC) each of whom 
Employment/ 
Membership 
(EMP-ORG) 
Mr. Smith, a senior pro-
grammer at Microsoft 
Place-Affiliation 
(GPE-AFF) 
Salzburg Red Cross offi-
cials 
Person-Social  
(PER-SOC) 
relatives of the dead 
 
Physical 
(PHYS) 
a town some 50 miles south 
of Salzburg 
Other-Affiliation 
(Other-AFF) 
Republican senators 
 
Table 1. Examples of the ACE Relation Types 
 
Given these relations we can define a semantic 
context for a candidate mention coreference pair 
(Mention 1b and Mention 2b) using the structure 
                                                          
2
 See http://www.ldc.upenn.edu/Projects/ACE/docs/Eng-
lishRDCV4-3-2.PDF for a more complete description of ACE 
2004 relations. 
18
depicted in Figure 1. If both mentions participate 
in relations, we examine the types and directions of 
their respective relations as well as whether or not 
their relation partners (Mention 1a and Mention 
2a) corefer. These values (which correspond to the 
edge labels in Figure 1) can then be factored into a 
coreference prediction. This RCM structure 
assimilates relation information into a coherent 
model of semantic context. 
 
 
 
 
 
 
 
 
Figure 1. The RCM structure 
4 Incorporating Relations into Reference 
Resolution 
Given an instance of the RCM structure, we need 
to convert it into semantic knowledge that can be 
applied to a coreference decision. We approach 
this problem by constructing a set of RCM patterns 
and evaluating the accuracy of each pattern as 
positive or negative evidence for coreference. The 
resulting knowledge sources fall into two catego-
ries: rules that improve precision by pruning incor-
rect coreference links between mentions, and rules 
that improve recall by recovering missed links.  
To formalize these relation patterns, based on 
Figure 1, we define the following clauses: 
 
A: RelationType1 = RelationType2 
B: RelationSubType1 = RelationSubType2 
C: Two Relations have the same direction 
Same_Relation: CBA ??  
CorefA: Mention1a and Mention2a corefer 
CorefBMoreLikely: Mention1b and Mention2b are 
more likely to corefer 
CorefBLessLikely: Mention1b and Mention2b are 
less likely to corefer 
 
From these clauses we can construct the follow-
ing plausible inferences: 
 
Rule (1) 
LikelyCorefBLessCorefAlationSame ???Re_  
Rule (2) 
LikelyCorefBLessCorefAlationSame ??? Re_  
Rule (3) 
LikelyCorefBMoreCorefAlationSame ??Re_   
 
Rule (1) and (2) can be used to prune corefer-
ence links that simple string matching might incor-
rectly assert; and (3) can be used to recover missed 
mention pairs.  
The accuracy of Rules (1) and (3) varies depend-
ing on the type and direction of the particular rela-
tion shared by the two noun phrases. For example, 
if Mention1a and Mention 2a both refer to the 
same nation, and Mentions 1b and 2b participate in 
citizenship relations (GPE-AFF) with Mentions 1a 
and 2a respectively, we should not necessarily 
conclude that 1b and 2b refer to the same person.  
If 1a and 2a refer to the same person, however, and 
1b and 2b are nations in citizenship relations with 
1a and 2a, then it would indeed be the rare case in 
which 1b and 2b refer to two different nations. In 
other words, the relation of a nation to its citizens 
is one-to-many.  
Our system learns broad restrictions like these 
by evaluating the accuracy of Rules (1) and (3) 
when they are instantiated with each possible rela-
tion type and direction and used as weak classifi-
ers. For each such instantiation we use cross-
validation on our training data to calculate a reli-
ability weight defined as: 
 
| Correct decisions by rule for given instance | 
 
| Total applicable cases for given instance | 
 
  We count the number of correct decisions for a 
rule instance by taking the rule instance as the only 
source of information for coreference resolution 
and making only those decisions suggested by the 
rule?s implication (interpreting CorefBMoreLikely 
as an assertion that mention 1b and mention 2b do 
in fact corefer, and interpreting CorefBLessLikely 
as an assertion that they do not corefer). 
Every rule instance with a reliability weight of 
70% or greater is retained for inclusion in the final 
system. Rule (2) cannot be instantiated with a 
single type because it requires that the two relation 
types be different, and so we do not perform this 
filtering for Rule (2) (Rule (2) has 97% accuracy 
across all relation types). 
This procedure yields 58 reliable (reliability 
weight > 70%) type instantiations of Rule (1) and 
(3), in addition to the reliable Rule 2. We can 
Relation? 
Type2/Subtype2 
Mention1a 
Mention2a 
 
 
 
Candidate 
Mention1b 
Mention2b 
Relation? 
Type1/Subtype1
  Contexts: Corefer?  
19
recover an additional 24 reliable rules by 
conjoining additional boolean tests to less reliable 
rules. Tests include equality of mention heads, 
substring matching, absence of temporal key words 
such as ?current? and ?former,? number 
agreement, and high confidence for original 
coreference decisions (Mention1b and Mention2b). 
For each rule below the reliability threshold, we 
search for combinations of 3 or fewer of these 
restrictions until we achieve reliability of 70% or 
we have exhausted the search space.  
We give some examples of particular rule 
instances below. 
 
Example for Rule (1) 
 
Bush campaign officials ... decided to tone down a 
post-debate rally, and were even considering can-
celing it. 
? 
The Bush and Gore campaigns did not talk to each 
other directly about the possibility of postpone-
ment, but went through the debate commission's di-
rector, Janet Brown...Eventually, Brown 
recommended that the debate should go on, and 
neither side objected, according to campaign offi-
cials. 
 
Two mentions that do not corefer share the same 
nominal head (?officials?). We can prune the 
coreference link by noting that both occurrences of 
?officials? participate in an Employee-
Organization (EMP-ORG) relation, while the Or-
ganization arguments of these two relation in-
stances do not corefer (because the second 
occurrence refers to officials from both cam-
paigns). 
 
Example for Rule (2) 
 
Despite the increases, college remains affordable 
and a good investment, said College Board Presi-
dent Gaston Caperton in a statement with the sur-
veys. ? 
A majority of students need grants or loans -- or 
both -- but their exact numbers are unknown, a 
College Board spokesman said. 
 
  ?Gaston Caperton? stands in relation EMP-
ORG/Employ-Executive with ?College Board?, 
while "a College Board spokesman" is in relation 
EMP-ORG/Employ-Staff with the same organiza-
tion. We conclude that ?Gaston Caperton? does not 
corefer with "spokesman." 
 
Example for Rule (3) 
 
In his foreign policy debut for Syria, this Sunday 
Bashar Assad met Sunday with Egyptian President 
Hosni Mubarak in talks on Mideast peace and the 
escalating violence in the Palestinian territories. 
? 
The Syrian leader's visit came on a fourth day of 
clashes that have raged in the West Bank, Gaza 
Strip and Jerusalem??  
 
  If we have detected a coreference link between 
?Syria? and ?Syrian,? as well as EMP-ORG/ 
Employ-Executive relations between this country 
and two noun phrases ?Bashar Assad? and 
?leader?, it is likely that the two mentions both 
refer to the same person. Without this inference, a 
resolver might have difficulty detecting this 
coreference link. 
5 Algorithms 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 2.  System Pipeline (Test Procedure) 
 
 
 
Coreference 
Rules 
Baseline Maxent 
Coref Classifiers 
Relation 
Tagger 
Final coreference decisions 
Entities 
Relation Features 
Rescoring Coreference Decisions 
 
Mentions 
20
In this section we will describe our algorithm for 
incorporating semantic relation information from 
the RCM into the reference resolver. In a nutshell, 
the system applies a baseline statistical resolver to 
generate multiple coreference hypotheses, applies a 
relation tagger to acquire relation information, and 
uses the relation information to rescore the 
coreference hypotheses. This general system archi-
tecture is shown in Figure 2.  
In section 5.1 below we present our baseline 
coreference system. In Section 5.2 we describe a 
system that combines the output of this baseline 
system with relation information to improve per-
formance. 
5.1 Baseline System 
Baseline reference resolver 
As the first stage in the resolution process we 
apply a baseline reference resolver that uses no 
relation information at all. This baseline resolver 
goes through two successive stages.  
First, high-precision heuristic rules make some 
positive and negative reference decisions. Rules 
include simple string matching (e.g., names that 
match exactly are resolved), agreement constraints 
(e.g., a nominal will never be resolved with an en-
tity that doesn't agree in number), and reliable syn-
tactic cues (e.g., mentions in apposition are 
resolved). When such a rule applies, it assigns a 
confidence value of 1 or 0 to a candidate mention-
antecedent pair. 
The remaining pairs are assigned confidence 
values by a collection of maximum entropy mod-
els. Since different mention types have different 
coreference problems, we separate the system into 
different models for names, nominals, and pro-
nouns. Each model uses a distinct feature set, and 
for each instance only one of these three models is 
used to produce a probability that the instance 
represents a correct resolution of the mention. 
When the baseline is used as a standalone system, 
we apply a threshold to this probability: if some 
resolution has a confidence above the  threshold, 
the highest confidence resolution will be made. 
Otherwise the mention is assumed to be the first 
mention of an entity. When the baseline is used as 
a component of the system depicted in figure 2, the 
confidence value is passed on to the rescoring 
stage described in 5.2 below. 
Both the English and the Chinese coreference 
models incorporate features representing agree-
ment of various kinds between noun phrases 
(number, gender, humanness), degree of string 
similarity, synonymy between noun phrase heads, 
measures of distance between noun phrases (such 
as the number of intervening sentences), the pres-
ence or absence of determiners or quantifiers, and 
a wide variety of other properties. 
Relation tagger 
The relation tagger uses a K-nearest-neighbor algo-
rithm. We consider a mention pair as a possible 
instance of a relation only when: (1) there is at 
most one other mention between their heads, and 
(2) the coreference probability produced for the 
pair by the baseline resolver is lower than a thresh-
old.  Each training / test example consists of the 
pair of mentions and the sequence of intervening 
words. We defined a distance metric between two 
examples based on: 
 whether the heads of the mentions match 
 whether the ACE types of the heads of the 
mentions match (for example, both are people 
or both are organizations) 
 whether the intervening words match 
To tag a test example, we find the k nearest 
training examples, use the distance to weight each 
neighbor, and then select the most heavily 
weighted class in the weighted neighbor set. 
Name tagger and noun phrase chunker  
Our baseline name tagger consists of a HMM 
tagger augmented with a set of post-processing 
rules.  The HMM tagger generally follows the 
Nymble model (Bikel et al 1997), but with a larger 
number of states (12 for Chinese, 30 for English) 
to handle name prefixes and suffixes, and, for 
Chinese, transliterated foreign names separately.  
For Chinese it operates on the output of a word 
segmenter from Tsinghua University. Our nominal 
mention tagger (noun phrase chunker) is a 
maximum entropy tagger trained on treebanks 
from the University of Pennsylvania. 
5.2 Rescoring stage 
To incorporate information from the relation tagger 
into the final coreference decision, we split the 
maxent classification into two stages. The first 
21
stage simply applies the baseline maxent models, 
without any relation information, and produces a 
probability of coreference. This probability 
becomes a feature in the second (rescoring) stage 
of maxent classification, together with features 
representing the relation knowledge sources. If a 
high reliability instantiation of one of the RCM 
rules (as defined in section 4 above) applies to a 
given mention-antecedent pair, we include the 
following features for that pair: the type of the 
RCM rule, the reliability of the rule instantiation, 
the relation type and subtype, the direction of the 
relation, and the tokens for the two mentions. 
The second stage helps to increase the margin 
between correct and incorrect links and so effects 
better disambiguation. See figure 3 below for a 
more detailed description of the training and test-
ing processes. 
 
 
 Training  
1. Calculate reliability weights of relation knowl-
edge sources using cross-validation (for each of k 
divisions of training data, train relation tagger on k 
? 1 divisions, tag relations in remaining division 
and compute reliability of each relation knowledge 
source using this division). 
2. Use high reliability relation knowledge sources 
to generate relation features for 2nd stage Maxent 
training data. 
3. Apply baseline coreference resolver to 2nd stage 
training data. 
4. Using output of both 2 and 3 as features, train 
2nd stage Maxent resolver. 
 
Test 
1. Tag relations. 
2. Convert relation knowledge sources into fea-
tures for second stage Maxent models. 
3. Use baseline Maxent models to get coreference 
probabilities for use as features in second stage 
Maxent models. 
4. Using output of 2 and 3 as features for 2nd stage 
Maxent model, apply 2nd stage resolver to make 
final coreference decisions. 
 
Figure 3.  Training and Testing Processes 
 
 
 
6 Evaluation Results 
6.1 Corpora 
We evaluated our system on two languages: 
English and Chinese. The following are the 
training corpora used for the components in these 
two languages. 
English 
For English, we trained the baseline maxent 
coreference model on 311 newswire and 
newspaper texts from the ACE 2002 and ACE 
2003 training corpora. We trained the relation 
tagger on 328 ACE 2004 texts. We used 126 
newswire texts from the ACE 2004 data to train the 
English second-stage model, and 65 newswire 
texts from the ACE 2004 evaluation set as a test set 
for the English system.  
Chinese 
For Chinese, the baseline reference resolver was 
trained on 767 texts from ACE 2003 and ACE 
2004 training data. Both the baseline relation 
tagger and the rescoring model were trained on 646 
texts from ACE 2004 training data. We used 100 
ACE texts for a final blind test. 
6.2 Experiments 
We used the MUC coreference scoring metric 
(Vilain et al1995) to evaluate3 our systems.  
To establish an upper limit for the possible 
improvement offered by our models, we first did 
experiments using perfect (hand-tagged) mentions 
and perfect relations as inputs. The algorithms for 
                                                          
3
 In our scoring, we use the ACE keys and only score mentions which appear in 
both the key and system response.  This therefore includes only mentions identi-
fied as being in the ACE semantic categories by both the key and the system 
response.  Thus these scores cannot be directly compared against coreference 
scores involving all noun phrases. (Ng 2005) applies another variation on the 
MUC metric to several systems tested on the ACE data by scoring all response 
mentions against all key mentions. For coreference systems that don?t restrict 
themselves to mentions in the ACE categories (or that don?t succeed in so re-
stricting themselves), this scoring method could lead to some odd effects. For 
example, systems that recover more correct links could be penalized for this 
greater recall because all links involving non-ACE mentions will be incorrect 
according to the ACE key. For the sake of comparison, however, we present 
here English system results measured according to this metric: On newswire 
data, our baseline had an F of 62.8 and the rescoring method had an F of 64.2. 
Ng?s best F score (on newspaper data) is 69.3. The best F score of  the (Ng and 
Cardie 2002)  system (also on newspaper data) is 62.1. On newswire data the 
(Ng 2005) system had an F score of 54.7 and the (Ng and Cardie 2002) system 
had an F score of 50.1. Note that Ng trained and tested these systems on differ-
ent ACE data sets than those we used for our experiments. 
22
these experiments are identical to those described 
above except for the omission of the relation tagger 
training. Tables 2 and 3 show the performance of 
the system for English and Chinese.  
 
Performance Recall Precision F-measure 
Baseline 74.5 86.6 80.1 
Rescoring 78.3 87.0 82.4 
 
Table 2. Performance of English system 
with perfect mentions and perfect relations 
 
 
Performance Recall Precision F-measure 
Baseline 87.5 83.2 85.3 
Rescoring 88.8 84.7 86.7 
 
Table 3. Performance of Chinese system 
with perfect mentions and perfect relations 
 
We can see that the relation information 
provided some improvements for both languages. 
Relation information increased both recall and 
precision in both cases. 
We then performed experiments to evaluate the 
impact of coreference rescoring when used with 
mentions and relations produced by the system. 
Table 4 and Table 5 list the results.4 
 
 
Performance Recall Precision F-measure 
Baseline 77.2 87.3 81.9 
Rescoring 80.3 87.5 83.7 
 
Table 4. Performance of English system 
with system mentions and system relations 
 
 
Performance Recall Precision F-measure 
Baseline 75.0 76.3 75.6 
Rescoring 76.1 76.5 76.3 
 
Table 5. Chinese system performance with 
system mentions and system relations 
 
                                                          
4
 Note that, while English shows slightly less relative gain from rescoring when 
using system relations and mentions, all of these scores are higher than the 
perfect mention/perfect relation scores. This increase may be a byproduct of the 
fact that the system mention tagger output contains almost 8% fewer scoreable 
mentions than the perfect mention set (see footnote 3). With a difference of this 
magnitude, the particular mention set selected can be expected to have a sizable 
impact on the final scores. 
The improvement provided by rescoring in trials 
using mentions and relations detected by the 
system is considerably less than the improvement 
in trials using perfect mentions and relations, 
particularly for Chinese. The performance of our 
relation tagger is the most likely cause for this 
difference. We would expect further gain after 
improving the relation tagger. 
A sign test applied to a 5-way split of each of the 
test corpora indicated that for both languages, for 
both perfect and system mentions/relations, the 
system that exploited relation information signifi-
cantly outperformed the baseline (at the 95% con-
fidence level, judged by F measure). 
6.3 Error Analysis 
Errors made by the RCM rules reveal both the 
drawbacks of using a lightweight semantic 
representation and the inherent difficulty of 
semantic analysis. Consider the following instance: 
 
Card's interest in politics began when he became 
president of the class of 1965 at Holbrook High 
School?In 1993, he became president and chief 
executive of the American Automobile Manufac-
turers Association, where he oversaw the lobbying 
against tighter fuel-economy and air pollution regu-
lations for automobiles? 
 
The two occurrences of ?president? should core-
fer even though they have EMP-ORG/Employ-
Executive relations with two different organiza-
tions. The relation rule (Rule 1) fails here because 
it doesn't take into account the fact that relations 
change over time (in this case, the same person 
filling different positions at different times). In 
these and other cases, a little knowledge is a dan-
gerous thing: a more complete schema might be 
able to deal more thoroughly with temporal and 
other essential semantic dimensions. 
Nevertheless, performance improvements indi-
cate that the rewards of the RCM?s simple seman-
tic representation outweigh the risks. 
7 Conclusion and Future Work 
We have outlined an approach to improving refer-
ence resolution through the use of semantic rela-
tions, and have described a system which can 
exploit these semantic relations effectively. Our 
experiments on English and Chinese data showed 
23
that these small inroads into semantic territory do 
indeed offer performance improvements. Further-
more, the method is low-cost and not domain-
specific. 
  These experiments also suggest that some gains 
can be made through the exploration of new archi-
tectures for information extraction applications. 
The ?resolve coreference, tag relations, resolve 
coreference? procedure described above could be 
seen as one and a half iterations of a ?resolve 
coreference then tag relations? loop. Seen in this 
way, the system poses the question of whether fur-
ther gains could be made by pushing the iterative 
approach further. Perhaps by substituting an itera-
tive procedure for the pipeline architecture?s linear 
sequence of stages we can begin to address the 
knotty, mutually determining nature of the interac-
tion between semantic relations and coreference 
relations. This approach could be applied more 
broadly, to different NLP tasks, and also more 
deeply, going beyond the simple one-and-a-half-
iteration procedure we present here. Ultimately, we 
would want this framework to boost the perform-
ance of each component automatically and signifi-
cantly. 
We also intend to extend our method both to 
cross-document relation detection and to event de-
tection. 
Acknowledgements  
This research was supported by the Defense Ad-
vanced Research Projects Agency under Grant 
N66001-04-1-8920 from SPAWAR San Diego, 
and by the National Science Foundation under 
Grant 03-25657. This paper does not necessarily 
reflect the position or the policy of the U.S. Gov-
ernment. 
References  
David Bean, Ellen Riloff. 2004. Unsupervised learning 
of contextual role knowledge for coreference resolu-
tion. Proc. HLT-NAACL 2004, pp. 297-304. 
Daniel M. Bikel, Scott Miller, Richard Schwartz, and 
Ralph Weischedel. 1997. Nymble: A high-
performance learning name-finder. Proc. Fifth Conf. 
on Applied Natural Language Processing, Washing-
ton, D.C., pp. 194-201. 
Carbonell, Jaime and Ralf Brown. 1988. Anaphora reso-
lution: A multi-strategy approach. Proc. COLING 
1988, pp.96-101 
Eugene Charniak. 1972. Toward a model of children's 
story comprehension. Ph.D. thesis, Massachusetts Insti-
tute of Technology, Cambridge, MA. 
Niyu Ge, John Hale and Eugene Charniak. 1998. A sta-
tistical approach to anaphora resolution. Proc. the 
Sixth Workshop on Very Large Corpora. 
Jerry Hobbs, Mark Stickel, Douglas Appelt and Paul 
Martin. 1993.  Interpretation as abduction.  Artificial 
Intelligence, 63, pp. 69-142. 
Ruslan Mitkov. 2000. Towards a more consistent and 
comprehensive evaluation of anaphora resolution al-
gorithms and systems. Proc. 2nd Discourse Anaph-
ora and Anaphora Resolution Colloquium, pp. 96-
107 
Vincent Ng and Claire Cardie. 2002. Improving ma-
chine learning approaches to coreference resolution. 
Proc. ACL 2002, pp.104-111 
Wee Meng Soon, Hwee Tou Ng, and Daniel Chung 
Yong Lim. 2001. A machine learning approach to 
coreference resolution of noun phrases. Computa-
tional Linguistics, Volume 27, Number 4, pp. 521-
544 
Joel R. Tetreault. 2001. A corpus-based evaluation of 
centering and pronoun resolution. Computational 
Linguistics, Volume 27, Number 4, pp. 507-520 
Joel  R. Tetreault and James Allen. 2004. Semantics, 
Dialogue, and Pronoun Resolution. Proc. CATALOG 
'04 Barcelona, Spain. 
Marc Vilain, John Burger, John Aberdeen, Dennis Con- 
     nolly, and Lynette Hirschman. 1995. A model-
theoretic coreference scoring scheme. Proc. the 6th 
Message Understanding Conference (MUC-6). San 
Mateo, Cal. Morgan Kaufmann. 
Robert Wilensky.  1983.  Planning and Understanding.  
Addison-Wesley. 
24
Event Detection and Summarization in Weblogs with Temporal Collocations 
Chun-Yuan Teng and Hsin-Hsi Chen 
Department of Computer Science and Information Engineering 
National Taiwan University 
Taipei, Taiwan 
{r93019, hhchen}@csie.ntu.edu.tw 
Abstract 
 
This paper deals with the relationship between weblog content and time. With the proposed temporal mutual information, we analyze 
the collocations in time dimension, and the interesting collocations related to special events. The temporal mutual information is 
employed to observe the strength of term-to-term associations over time. An event detection algorithm identifies the collocations that 
may cause an event in a specific timestamp. An event summarization algorithm retrieves a set of collocations which describe an event. 
We compare our approach with the approach without considering the time interval. The experimental results demonstrate that the 
temporal collocations capture the real world semantics and real world events over time. 
 
1. 
2. 
Introduction 
Compared with traditional media such as online news 
and enterprise websites, weblogs have several unique 
characteristics, e.g., containing abundant life experiences 
and public opinions toward different topics, highly 
sensitive to the events occurring in the real world, and 
associated with the personal information of bloggers. 
Some works have been proposed to leverage these 
characteristics, e.g., the study of the relationship between 
the content and bloggers? profiles (Adamic & Glance, 
2005; Burger & Henderson, 2006; Teng & Chen, 2006), 
and content and real events (Glance, Hurst & Tornkiyo, 
2004; Kim, 2005; Thelwall, 2006; Thompson, 2003). 
In this paper, we will use temporal collocation to 
model the term-to-term association over time.  In the past, 
some useful collocation models (Manning & Sch?tze, 
1999) have been proposed such as mean and variance, 
hypothesis test, mutual information, etc. Some works 
analyze the weblogs from the aspect of time like the 
dynamics of weblogs in time and location (Mei, et al, 
2006), the weblog posting behavior (Doran, Griffith & 
Henderson, 2006; Hurst, 2006), the topic extraction (Oka, 
Abe & Kato, 2006), etc. The impacts of events on social 
media are also discussed, e.g., the change of weblogs after 
London attack (Thelwall, 2006), the relationship between 
the warblog and weblogs (Kim, 2005; Thompson, 2003), 
etc. 
This paper is organized as follows. Section 2 defines 
temporal collocation to model the strength of term-to-term 
associations over time.  Section 3 introduces an event 
detection algorithm to detect the events in weblogs, and 
an event summarization algorithm to extract the 
description of an event in a specific time with temporal 
collocations. Section 4 shows and discusses the 
experimental results.  Section 5 concludes the remarks. 
Temporal Collocations 
We derive the temporal collocations from Shannon?s 
mutual information (Manning & Sch?tze, 1999) which is 
defined as follows (Definition 1). 
Definition 1 (Mutual Information) The mutual 
information of two terms x and y is defined as: 
)()(
),(log),(),(
yPxP
yxPyxPyxI =  
where P(x,y) is the co-occurrence probability of x and y, 
and P(x) and P(y) denote the occurrence probability of x 
and y, respectively. 
Following the definition of mutual information, we 
derive the temporal mutual information modeling the 
term-to-term association over time, and the definition is 
given as follows.  
 Definition 2 (Temporal Mutual Information) Given 
a timestamp t and a pair of terms x and y, the temporal 
mutual information of x and y in t is defined as: 
)|()|(
)|,(log)|,()|,(
tyPtxP
tyxPtyxPtyxI =
where P(x,y|t) is the probability of co-occurrence of terms 
x and y in timestamp t, P(x|t) and P(y|t) denote the 
probability of occurrences of x and y in timestamp t, 
respectively. 
To measure the change of mutual information in time 
dimension, we define the change of temporal mutual 
information as follows. 
Definition 3 (Change of Temporal Mutual 
Information) Given time interval [t1, t2], the change of 
temporal mutual information is defined as: 
12
12
21
)|,()|,(),,,(
tt
tyxItyxIttyxC ?
?=  
where C(x,y,t1,t2) is the change of temporal mutual 
information of terms x and y in time interval [t1, t2], I(x,y| 
t1) and I(x,y| t2) are the temporal mutual information in 
time t1 and t2, respectively. 
3. Event Detection 
Event detection aims to identify the collocations 
resulting in events and then retrieve the description of 
events. Figure 1 sketches an example of event detection. 
The weblog is parsed into a set of collocations. All 
collocations are processed and monitored to identify the 
plausible events.  Here, a regular event ?Mother?s day? 
and an irregular event ?Typhoon Chanchu? are detected.  
The event ?Typhoon Chanchu? is described by the words  
 
 
 
 
 
 
 
 
 
 
 
 
Figure 1: An Example of Event Detection
?Typhoon?, ?Chanchu?, ?2k?, ?Eye?, ?Path? and 
?chinaphillippine?.  
The architecture of an event detection system includes 
a preprocessing phase for parsing the weblogs and 
retrieving the collocations; an event detection phase 
detecting the unusual peak of the change of temporal 
mutual information and identifying the set of collocations 
which may result in an event in a specific time duration; 
and an event summarization phase extracting the 
collocations related to the seed collocations found in a 
specific time duration. 
The most important part in the preprocessing phase is 
collocation extraction. We retrieve the collocations from 
the sentences in blog posts. The candidates are two terms 
within a window size. Due to the size of candidates, we 
have to identify the set of tracking terms for further 
analysis. In this paper, those candidates containing 
stopwords or with low change of temporal mutual 
information are removed. 
In the event detection phase, we detect events by 
using the peak of temporal mutual information in time 
dimension.  However, the regular pattern of temporal 
mutual information may cause problems to our detection. 
Therefore, we remove the regular pattern by seasonal 
index, and then detect the plausible events by measuring 
the unusual peak of temporal mutual information. 
If a topic is suddenly discussed, the relationship 
between the related terms will become higher. Two 
alternatives including change of temporal mutual 
information and relative change of temporal mutual 
information are employed to detect unusual events. Given 
timestamps t1 and t2 with temporal mutual information 
MI1 and MI2, the change of temporal mutual information 
is calculated by (MI2-MI1). The relative change of 
temporal mutual information is calculated by (MI2-
MI1)/MI1. 
For each plausible event, there is a seed collocation, 
e.g., ?Typhoon Chanchu?. In the event description 
retrieval phase, we try to select the collocations with the 
highest mutual information with the word w in a seed 
collocation. They will form a collocation network for the 
event.  Initially, the seed collocation is placed into the 
network.  When a new collocation is added, we compute 
the mutual information of the multiword collocations by 
the following formula, where n is the number of 
collocations in the network up to now. 
?= n iMInInformatioMutualMultiwo  
If the multiword mutual information is lower than a 
threshold, the algorithm stops and returns the words in the 
collocation network as a description of the event.  Figure 
2 sketches an example.  The collocations ?Chanchu?s 
path?, ?Typhoon eye?, and ?Chanchu affects? are added 
into the network in sequence based on their MI. 
We have two alternatives to add the collocations to 
the event description. The first method adds the 
collocations which have the highest mutual information 
as discussed above. In contrast, the second method adds 
the collocations which have the highest product of mutual 
information and change of temporal mutual information. 
 
 
 
 
 
 
Figure 2: An Example of Collocation network 
4. 
4.1. 
Experiments and Discussions 
Temporal Mutual Information versus 
Mutual Information 
In the experiments, we adopt the ICWSM weblog data 
set (Teng & Chen, 2007; ICWSM, 2007). This data set 
collected from May 1, 2006 through May 20, 2006 is 
about 20 GB. Without loss of generality, we use the 
English weblog of 2,734,518 articles for analysis. 
To evaluate the effectiveness of time information, we 
made the experiments based on mutual information 
(Definition 1) and temporal mutual information 
(Definition 2). The former called the incremental 
approach measures the mutual information at each time 
point based on all available temporal information at that 
time. The latter called the interval-based approach 
considers the temporal mutual information in different 
time stamps.  Figures 3 and 4 show the comparisons 
between interval-based approach and incremental 
approach, respectively, in the event of Da Vinci Code.   
We find that ?Tom Hanks? has higher change of 
temporal mutual information compared to ?Da Vinci 
Code?. Compared to the incremental approach in Figure 4, 
the interval-based approach can reflect the exact release 
date of ?Da Vinci Code.? 
 rd
=i 1 4.2. Evaluation of Event Detection 
We consider the events of May 2006 listed in 
wikipedia1 as gold standard. On the one hand, the events 
posted in wikipedia are not always complete, so that we 
adopt recall rate as our evaluation metric.  On the other 
hand, the events specified in wikipedia are not always 
discussed in weblogs.  Thus, we search the contents of 
blog post to verify if the events were touched on in our 
blog corpus. Before evaluation, we remove the events 
listed in wikipedia, but not referenced in the weblogs. 
 
 
 
 
 
 
 
 
 
 
 
Figure 3: Interval-based Approach in Da Vinci Code  
 
 
 
 
 
 
 
 
Figure 4: Incremental Approach in Da Vinci Code 
gure 5 sketches the idea of evaluation.  The left side 
of t s figure shows the collocations detected by our event 
dete tion system, and the right side shows the events 
liste  in wikipedia.  After matching these two lists, we 
can find that the first three listed events were correctly 
identified by our system.  Only the event ?Nepal Civil 
War? was listed, but not found. Thus, the recall rate is 
75% in this case. 
 
 
 
 
 
 
 
Figure 5: Evaluation of Event Detection Phase 
As discussed in Section 3, we adopt change of 
temporal mutual information, and relative change of 
temporal mutual information to detect the peak. In Figure 
6, we compare the two methods to detect the events in 
weblogs. The relative change of temporal mutual 
information achieves better performance than the change 
of temporal mutual information. 
                                                     
1 http://en.wikipedia.org/wiki/May_2006 
Table 1 and Table 2 list the top 20 collocations based 
on these two approaches, respectively. The results of the 
first approach show that some collocations are related to 
the feelings such as ?fell left? and time such as ?Saturday 
night?. In contrast, the results of the second approach 
show more interesting collocations related to the news 
events at that time, such as terrorists ?zacarias 
moussaoui? and ?paramod mahajan.? These two persons 
were killed in May 3. Besides, ?Geena Davis? got the 
golden award in May 3. That explains why the 
collocations detected by relative change of temporal 
mutual information are better than those detected by 
change of temporal mutual information. 
-20
-15
-10
-5
0
5
10
1 3 5 7 9 11 13 15 17 19
Time (day)
M
ut
ua
l i
nf
or
m
at
io
n
Da-Vinci Tom Hanks
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 6: Performance of Event Detection Phase 
-15
-10
-5
0
5
10
1 3 5 7 9 11 13 15 17 19
Time (day)
M
ut
ua
l i
nf
or
m
at
io
n
Da-Vinci Tom Hanks
Collocations CMI Collocations CMI 
May 03 9276.08 Current music 1842.67
Illegal immigrants 5833.17 Hate studying 1722.32
Feel left 5411.57 Stephen Colbert 1709.59
Saturday night 4155.29 Thursday night 1678.78
Past weekend 2405.32 Can?t believe 1533.33
White house 2208.89 Feel asleep 1428.18
Red sox 2208.43 Ice cream 1373.23
Album tool 2120.30 Oh god 1369.52
Sunday morning 2006.78 Illegalimmigration 1368.12
16.56
f 
CMI
32.50
31.63
29.09
28.45
28.34
28.13Sunday night 1992.37 Pretty cool 13
Table 1: Top 20 collocations with highest change o
temporal mutual information 
Collocations CMI Collocations 
casinos online 618.36 Diet sodas 
zacarias moussaoui 154.68 Ving rhames 
Tsunami warning 107.93 Stock picks 
Conspirator zacarias 71.62 Happy hump 
Artist formerly 57.04 Wong kan 
Federal  
Jury 
41.78 Sixapartcom 
movabletype Wed 3 39.20 Aaron echolls 27.48
Pramod mahajan 35.41 Phnom penh 25.78
BBC  
Version 
35.21 Livejournal 
sixapartcom 
23.83  Fi
hi
c
dGeena davis 33.64 George yeo 20.34
Table 2: Top 20 collocations with highest relative change 
of mutual information 
4.3. Evaluation of Event Summarization 
As discussed in Section 3, we have two methods to 
include collocations to the event description. Method 1 
employs the highest mutual information, and Method 2 
utilizes the highest product of mutual information and 
change of temporal mutual information. Figure 7 shows 
the performance of Method 1 and Method 2. We can see 
that the performance of Method 2 is better than that of 
Method 1 in most cases. 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 7: Overall Performance of Event Summarization 
The results of event summarization by Method 2 are 
shown in Figure 8. Typhoon Chanchu appeared in the 
Pacific Ocean on May 10, 2006, passed through 
Philippine and China and resulted in disasters in these 
areas on May 13 and 18, 2006.  The appearance of the 
typhoon Chanchu cannot be found from the events listed 
in wikipedia on May 10.  However, we can identify the 
appearance of typhoon Chanchu from the description of 
the typhoon appearance such as ?typhoon named? and 
?Typhoon eye.  In addition, the typhoon Chanchu?s path 
can also be inferred from the retrieved collocations such 
as ?Philippine China? and ?near China?. The response of 
bloggers such as ?unexpected typhoon? and ?8 typhoons? 
is also extracted.   
 
 
 
 
 
 
 
 
 
 
Figure 8: Event Summarization for Typhoon Chanchu 
5. Concluding Remarks 
This paper introduces temporal mutual information to 
capture term-term association over time in weblogs. The 
extracted collocation with unusual peak which is in terms 
of relative change of temporal mutual information is 
selected to represent an event.  We collect those 
collocations with the highest product of mutual 
information and change of temporal mutual information 
to summarize the specific event.  The experiments on 
ICWSM weblog data set and evaluation with wikipedia 
event lists at the same period as weblogs demonstrate the 
feasibility of the proposed temporal collocation model 
and event detection algorithms. 
Currently, we do not consider user groups and 
locations. This methodology will be extended to model 
the collocations over time and location, and the 
relationship between the user-preferred usage of 
collocations and the profile of users. 
Acknowledgments 
Research of this paper was partially supported by 
National Science Council, Taiwan (NSC96-2628-E-002-
240-MY3) and Excellent Research Projects of National 
Taiwan University (96R0062-AE00-02). 
References 
Adamic, L.A., Glance, N. (2005). The Political 
Blogosphere and the 2004 U.S. Election: Divided 
They Blog. In: Proceedings of the 3rd International 
Workshop on Link Discovery, pp. 36--43. 
Burger, J.D., Henderson J.C. (2006). An Exploration of 
Observable Features Related to Blogger Age. In: 
Proceedings of AAAI 2006 Spring Symposium on 
Computational Approaches to Analysing Weblogs, pp. 
15--20. 
Doran, C., Griffith, J., Henderson, J. (2006). Highlights 
from 12 Months of Blogs. In: Proceedings of AAAI 
2006 Spring Symposium on Computational 
Approaches to Analysing Weblogs, pp. 30--33. 
Glance, N., Hurst, M., Tornkiyo, T. (2004). Blogpulse: 
Automated Trend Discovery for Weblogs. In: 
Proceedings of WWW 2004 Workshop on the 
Weblogging Ecosystem: Aggregation, Analysis, and 
Dynamics. 
Hurst, M. (2006). 24 Hours in the Blogosphere. In: 
Proceedings of AAAI 2006 Spring Symposium on 
Computational Approaches to Analysing Weblogs, pp. 
73--77. 
ICWSM (2007). http://www.icwsm.org/data.html 
Kim, J.H. (2005). Blog as an Oppositional Medium? A 
Semantic Network Analysis on the Iraq War Blogs. In: 
Internet Research 6.0: Internet Generations. 
 
Manning, C.D., Sch?tze, H. (1999). Foundations of 
Statistical Natural Language Processing, The MIT 
Press, London England. 
Mei, Q., Liu, C., Su, H., Zhai, C. (2006). A Probabilistic 
Approach to Spatiotemporal Theme Pattern Mining on 
Weblogs. In: Proceedings of the 15th International 
Conference on World Wide Web, Edinburgh, Scotland, 
pp. 533--542. 
Oka, M., Abe, H., Kato, K. (2006). Extracting Topics 
from Weblogs Through Frequency Segments. In: 
Proceedings of WWW 2006 Annual Workshop on the 
Weblogging Ecosystem: Aggregation, Analysis, and 
Dynamics. 
Teng, C.Y., Chen, H.H. (2006). Detection of Bloggers? 
Interest: Using Textual, Temporal, and Interactive 
Features. In: Proceeding of IEEE/WIC/ACM 
International Conference on Web Intelligence, pp. 
366--369. 
Teng, C.Y., Chen, H.H. (2007). Analyzing Temporal 
Collocations in Weblogs. In: Proceeding of 
International Conference on Weblogs and Social 
Media, 303--304. 
Thelwall, M. (2006). Blogs During the London Attacks: 
Top Information Sources and Topics. In: Proceedings 
of 3rd Annual Workshop on the Weblogging 
Ecosystem: Aggregation, Analysis and Dynamics. 
Thompson, G. (2003). Weblogs, Warblogs, the Public 
Sphere, and Bubbles. Transformations, 7(2). 
pre-CODIE ? Crosslingual On-Demand Information Extraction
Kiyoshi Sudo Satoshi Sekine Ralph Grishman
Computer Science Department
New York University
715 Broadway, 7th Floor,
New York, NY
 
sudo,sekine,grishman  @cs.nyu.edu
1 Introduction
Our research addresses two central issues of informa-
tion extraction ? portability and multilinguality. We
are creating information extraction (IE) systems that take
foreign-language input and generate English tables of ex-
tracted information, and that can be easily adapted to new
extraction tasks. We want to minimize the human in-
tervention required for customization to a new scenario
(type of facts or events of interest), and allow the user
to interact with the system entirely in English. As a
prototype, we have developed the pre-CODIE system,
an experimental Crosss-lingual On-Demand Information
Extraction system that extracts facts or events of interest
from Japanese source text without requiring user knowl-
edge of Japanese.
2 Overview
To minimize the customization of the IE system across
scenarios, the extraction procedure of pre-CODIE is
driven by the query from the user. The user starts the
procedure by specifying the type of facts or events of
interest in the form of a narrative description, and then
pre-CODIE customizes itself to the topic by acquiring
extraction patterns based on the user?s description. Pre-
CODIE, as an early attempt at a fully-automated system,
still needs user interaction for template definition and slot
assignment; automating these steps is left as future work.
Pre-CODIE interacts with its user entirely in English;
even for slot assignment of the extraction patterns, the
system translates the Japanese extraction patterns, which
are based on subtrees of a dependency tree (Sudo et al,
2001), by word-to-word translation of each lexical item in
the patterns. For ease of use, the Japanese extraction pat-
terns are not only translated into English, but also shown
with translated example sentences which match the pat-
tern.
3 System Architecture
Pre-CODIE is implemented as an integration of several
modules, as shown in Figure 1: translation, information
retrieval, pattern acquisition, and extraction.
Figure 1: system architecture
First, the system takes the narrative description of
the scenario of interest as an English query, and the
Translation module (off-the-shelf IBM King of Transla-
tion system) generates a Japanese query. The IR mod-
ule retrieves a set of relevant documents from Japanese
Mainichi Newspaper from 1995. Then, the Pattern Ac-
quisition module produces a list of extraction patterns
from the relevant document set sorted by their relevance
to the scenario (Sudo et al, 2001). Pre-CODIE asks the
user to assign each placeholder in the patterns to one of
the slots in the template. Finally, the Extraction module
performs the pattern matching with slot-assigned patterns
to each text in the relevant document set and generates a
filled Japanese template, which is translated slot-by-slot
into English for the user.
                                                               Edmonton, May-June 2003
                                                            Demonstrations , pp. 25-26
                                                         Proceedings of HLT-NAACL 2003
4 An Example procedure: Management
Succession
From the user?s point of view, pre-CODIE works as fol-
lows with the screenshots in Figure 2.
1. Query: The user types in the narrative descrip-
tion of the scenario of interest, one phrase in ?de-
scription? text-box and more detail optionally given
in ?narrative? text-box: ?Management Succession:
...?.
2. Configuration: The user adds and/or deletes the
slots in the template; Add ?Person-In?, ?Person-
Out?, ?Post-In?, ?Post-Out?, and ?Organization?.
3. Slot Assignment: The user assigns a slot to each
placeholder in the pattern by choosing one of
the slots defined in step 2; Assign ?(be-promoted
(PERSON-SBJ))? to ?Person-In?.
Also, the user can see the example sentences with
the match of the pattern highlighted. This will make
it easier for the user to understand what each pattern
aims to extract.
4. Extraction: The user gets the extracted template
and repeats this procedure until the user gets the
right template by going back to step 3 to change
and/or add slot assignments, and by going back to
step 2 to delete and/or add slots in the template.
Acknowledgments
This research is supported by the Defense Advanced Re-
search Projects Agency under Grant N66001-00-1-8917
from the Space and Naval Warfare Systems Center San
Diego. This paper does not necessarily reflect the posi-
tion or the policy of the U.S. Government.
References
Kiyoshi Sudo, Satoshi Sekine, and Ralph Grishman.
2001. Automatic pattern acquisition for japanese in-
formation extraction. In Proceedings of the Human
Language Technology Conference (HLT2001), San
Diego, California.
(1)
(2)
(3)
(4)
Figure 2: Screenshots of an Example procedure: Each
image corresponds to the procedure in Section 4.
Proceedings of NAACL HLT 2007, pages 532?539,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
Question Answering using Integrated Information Retrieval and
Information Extraction
Barry Schiffman and Kathleen R. McKeown
Department of Computer Science
Columbia University
New York, NY 10027
bschiff,kathy@cs.columbia.edu
Ralph Grishman
Department of Computer Science
New York University
New York, NY 10003
grishman@cs.nyu.edu
James Allan
University of Massachusetts
Department of Computer Science
Amherst, MA 01003
allan@cs.umass.edu
Abstract
This paper addresses the task of provid-
ing extended responses to questions re-
garding specialized topics. This task is an
amalgam of information retrieval, topical
summarization, and Information Extrac-
tion (IE). We present an approach which
draws on methods from each of these ar-
eas, and compare the effectiveness of this
approach with a query-focused summa-
rization approach. The two systems are
evaluated in the context of the prosecution
queries like those in the DARPA GALE
distillation evaluation.
1 Introduction
As question-answering systems advance from han-
dling factoid questions to more complex requests,
they must be able to determine how much informa-
tion to include while making sure that the informa-
tion selected is indeed relevant. Unlike factoid ques-
tions, there is no clear criterion that defines the kind
of phrase that answers the question; instead, there
may be many phrases that could make up an answer
and it is often unclear in advance, how many. As
system developers, our goal is to yield high recall
without sacrificing precision.
In response to questions about particular events of
interest that can be enumerated in advance, it is pos-
sible to perform a deeper semantic analysis focusing
on the entities, relations, and sub-events of interest.
On the other hand, the deeper analysis may be error-
ful and will also not always provide complete cov-
erage of the information relevant to the query. The
challenge, therefore, is to blend a shallower, robust
approach with the deeper approach in an effective
way.
In this paper, we show how this can be achieved
through a synergistic combination of information re-
trieval and information extraction. We interleave in-
formation retrieval (IR) and response generation, us-
ing IR in high precision mode in the first stage to
return a small number of documents that are highly
likely to be relevant. Information extraction of enti-
ties and events within these documents is then used
to pinpoint highly relevant sentences and associated
words are selected to revise the query for a sec-
ond pass of retrieval, improving recall. As part of
this process, we approximate the relevant context by
measuring the proximity of the target name in the
query and extracted events.
Our approach has been evaluated in the frame-
work of the DARPA GALE1 program. One of the
GALE evaluations involves responding to questions
based on a set of question templates, ranging from
broad questions like ?Provide information on X?,
where X is an organization, to questions focused on
particular classes of events. For the experiments pre-
sented here, we used the GALE program?s prosecu-
tion class of questions. These are given in the fol-
lowing form: ?Describe the prosecution of X for Y,?
where X is a person and Y is a crime or charge. Our
results show that we are able to achieve higher accu-
1Global Autonomous Language Exploitation
532
racy with a system that exploits the justice events
identified by IE than with an approach based on
query-focused summarization alone.
In the following sections, we first describe the
task and then review related work in question-
answering. Section 3 details our procedure for find-
ing answers as well as performing the information
retrieval and information extraction tasks. Section 4
compares the results of the two approaches. Finally,
we present our conclusion and plans for future work.
1.1 The Task
The language of the question immediately raises the
question of what is meant by prosecution. Unlike a
question such as ?When was X born??, which is ex-
pected to be answered by a clear, concrete phrase,
the prosecution question asks for a much greater
range of material. The answer is in no way limited
to the statements and activities of the prosecuting at-
torney, although these would certainly be part of a
comprehensive answer.
In the GALE relevance guidelines2 , the answer
can include many facets of the case:
? Descriptions of the accused?s involvement in
the crime.
? Descriptions of the activities, motivations, and
involvement in the crime.
? Descriptions of the person as long as they are
related to the trial.
? Information about the defense of the suspect.
? Information about the sentencing of the person.
? Information about similar cases involving the
person.
? Information about the arrest of the person and
statements made by him or her.
? Reactions of people involved in the trial, as
well as statements by officials or reactions by
the general public.
2BAE Systems Advanced Information Technologies, ?Rele-
vance Guidelines for Distillation Evaluation for GALE: Global
Autonomous Language Exploitation?, Version 2.2, January 25,
2007
The guidelines also provide a catchall instruction
to ?include reported information believed to be rele-
vant to the case, but deemed inadmissible in a court
of law.?
It is easy to see that the use of a few search terms
alone will be insufficient to locate a comprehensive
answer.
We took a broad view of the question type and
consider that any information about the investiga-
tion, accusation, pursuit, capture, trial and punish-
ment of the individual, whether a person or organi-
zation, would be desireable in the answer.
1.2 Overview
The first step in our procedure sends a query tai-
lored to this question type to the IR system to ob-
tain a small number of high-quality documents with
which we can determine what name variations are
used in the corpus and estimate how many docu-
ments contain references to the individual. In the
future we will expand the type of information we
want to glean from this small set of documents. A
secondary search is issued to find additional docu-
ments that refer to the individual, or individuals.
Once we have the complete document retrieval,
the foundation for finding these types of events
lies in the Proteus information extraction compo-
nent (Grishman et al, 2005). We employ an IE sys-
tem trained for the tasks of the 2005 Automatic Con-
tent Extraction evaluation, which include entity and
event extraction. ACE defines a number of general
event types, including justice events, which cover in-
dictments, accusations, arrests, trials, and sentenc-
ings. The union of all these specific categories gives
us many of the salient events in a criminal justice
case from beginning to end. The program uses the
events, as well as the entities, to help identify the
passages that respond to the question.
The selection of sentences is based on the as-
sumption that the co-occurrence of the target indi-
vidual and a judicial event indicates that the target
is indeed involved in the event, but these two do not
necesssarily occur in the same sentence.
2 Related Work
A large body of work in question-answering has fol-
lowed from the opening of the Text Retrieval Con-
533
ference?s Q&A track in 1999. The task started as a
group of factoid questions and expanded from there
into more sophisticated problems. TREC provides
a unique testbed of question-answer pairs for re-
searchers and this data has been influential in fur-
thering progress.
In TREC 2006, there was a new secondary task
called ?complex, interactive Question Answering,?
(Dang et al, 2006) which is quite close to the GALE
problem, though it incorporated interaction to im-
prove results. Questions are posed in a canonical
form plus a narrative elaborating on the kind of in-
formation requested. An example question (from the
TREC guidelines) asks, ?What evidence is there for
transport of [drugs] from [Bonaire] to the [United
States]?? Our task is most similar to the fully-
automatic baseline runs of the track, which typically
took the form of passage retrieval with query ex-
pansion (Oard et al, 2006) or synonym processing
(Katz et al, 2006), and not the deeper processing
employed in this work.
Within the broader QA task, the other question
type is closest to the requirements in GALE, but it
is too open ended. In TREC, the input for other
questions is the name or description of the target,
and the response is supposed to be all information
that did not fit in the answers to the previous ques-
tions. While a few GALE questions have similar in-
put, most, including the prosecution questions, pro-
vide more detail about the topic in question.
A number of systems have used techniques in-
spired by information extraction. One of the top sys-
tems in the other questions category at the 2004 and
2005 evaluations generated lexical-syntactic pat-
terns and semantic patterns (Schone et al, 2004).
But they build these patterns from the question. In
our task, we took advantage of the structured ques-
tion format to make use of extensive work on the
semantics of selected domains. In this way we
hope to determine whether we can obtain better per-
formance by adding more sophisticated knowledge
about these domains. The Language Computer Cor-
poration (LCC) has long experimented with incorpo-
rating information extraction techniques. Recently,
in its system for the other type questions at TREC
2005, LCC developed search patterns for 33 target
classes (Harabagiu et al, 2005). These patterns were
learned with features from WordNet, stemming and
named entity recognition.
More and more systems are exploiting the size
and redundancy of the Web to help find answers.
Some obtain answers from the Web and then
project the answer back to the test corpus to find
a supporting document (Voorhees and Dang, 2005).
LCC used ?web boosting features? to add to key
words (Harabagiu et al, 2005). Rather than go to
the Web and enhance the question terms, we made
a beginning at examining the corpus for specific bits
of information, in this prototype, to determine alter-
native realizations of names.
3 Implementation
As stated above, the system takes a query in the
XML format required by the GALE program. The
query templates allow users to amplify their requests
by specifying a timeframe for the information and/or
a locale. In addition, there are provisions for en-
tering synonyms or alternate terms for either of the
main arguments, i.e. the accused and the crime, and
for related but less important terms.
Since this system is a prototype written especially
for the GALE evaluation in July 2006, we paid close
attention to the way example questions were given,
as well as to the evaluation corpus, which consisted
of more than 600,000 short news articles. The goal
in GALE was to offer comprehensive results to the
user, providing all snippets, or segments of texts,
that responded to the information request. This re-
quired us to develop a strategy that balanced pre-
cision against recall. A system that reported only
high-confidence answers was in danger of having no
answers or far fewer answers than other systems,
while a system that allowed lower confidence an-
swers risked producing answers with a great deal of
irrelevant material. Another way to look at this bal-
ancing act was that it was necessary for a system to
know when to quit. For this reason, we sought to
obtain a good estimate of the number of documents
we wanted to scan for answers.
Answer selection focused first on the name of the
suspect, which was always given in the query tem-
plate. In many of the training cases, the suspect was
in the news only because of a criminal charge against
him; and in most, the charge specified was the only
accusation reported in the news. Both location and
534
date constraints seemed to be largely superfluous,
and so we ignored these. But we did have a mecha-
nism for obtaining supplementary answers keyed to
the brief description of the crime and other related
words
The first step in the process is to request a seed
collection of 10 documents from the IR system.
This number was established experimentally. The
IR query combines terms tailored to the prosecution
template and the specific template parameters for a
particular question. The 10 documents returned are
then examined to produce a list of name variations
that substantially match the name as rendered in the
query template. The IR system is then asked for the
number of times that the name appears in the cor-
pus. This figure is adjusted by the frequency per
document in the seed collection and a new query is
submitted, set to obtain the N documents in which
we expect to find the target?s name.
3.1 Information Retrieval
The goal of the information retrieval component of
the system was to locate relevant documents that the
summarization system could then use to construct an
answer. All search, whether high-precision or high-
recall, was performed using the Indri retrieval sys-
tem 3 (Strohman et al, 2005).
Indri provides a powerful query language that
is used here to combine numerous aspects of the
query. The Indri query regarding Saddam Hus-
sein?s prosecution for crimes against humanity in-
cludes the following components: source restric-
tions, prosecution-related words, mentions of Sad-
dam Hussein, justice events, dependence model
phrases (Metzler and Croft, 2005) regarding the
crime, and a location constraint.
The first part of the query located references to
prosecutions by looking for the keywords prosecu-
tion, defense, trial, sentence, crime, guilty, or ac-
cuse, all of which were determined on training data
to occur in descriptions of prosecutions. These
words were important to have in documents for them
to be considered relevant, but the individual?s name
and the description of the crime were far more im-
portant (by a factor of almost 19 to 1).
The more heavily weighted part of the query,
3http://lemurproject.org/indri
then, was a ?justice event? marker found using in-
formation extraction (Section 3.2) and the more de-
tailed description of that event based on phrases ex-
tracted from the crime (here crimes against human-
ity). Those phrases give more probability of rele-
vance to documents that use more terms from the
crime. It also included a location constraint (here,
Iraq) that boosted documents referring to that lo-
cation. And it captured user-provided equivalent
words such as Saddam Hussein being a synonym for
former President of Iraq.
The most complex part of the query handled ref-
erences to the individual. The extraction system had
annotated all person names throughout the corpus.
We used the IR system to index all names across
all documents and used Indri to retrieve any name
forms that matched the individual. As a result, we
were able to find references to Saddam, Hussein,
and so on. This task could have also been accom-
plished with cross-document coreference technol-
ogy but our approach appeared to compensate for
incorrectly translated names slightly better than the
coreference system we had available at the time. For
example, Present rust Hussein was one odd form
that was matched by our simple approach.The final query looked like the following:
#filreq( #syn( #1(AFA).source ... #1(XIE).source )
#weight(
0.05 #combine( prosecution defense trial sentence
crime guilty accuse )
0.95 #combine(
#any:justice
#weight(1.0 #combine(humanity against crimes)
1.0 #combine(
#1(against humanity)
#1(crimes against)
#1(crimes against humanity))
1.0 #combine
#uw8(against humanity)
#uw8(crimes humanity)
#uw8(crimes against)
#uw12(crimes against humanity)))
Iraq
#syn( #1(saddam hussein)
#1(former president iraq))
#syn( #equals( entity 126180 ) ...))))
The actual query is much longer because it con-
tains 100 possible entities and numerous sources.
The processing is described in more detail else-
where (Kumaran and Allan, 2007).
3.2 Information Extraction
The Proteus system produces the full range of anno-
tations as specified for the ACE 2005 evaluation, in-
cluding entities, values, time expressions, relations,
535
and events. We focus here on the two annotations,
entities and events, most relevant to our question-
answering task. The general performance on entity
and event detection in news articles is within a few
percentage points of the top-ranking systems from
the evaluation.
The extraction engine identifies seven semantic
classes of entities mentioned in a document, of
which the most frequent are persons, organizations,
and GPE?s (geo-political entities ? roughly, regions
with a government). Each entity will have one or
more mentions in the document; these mentions in-
clude names, nouns and noun phrases, and pro-
nouns. Text processing begins with an HMM-based
named entity tagger, which identifies and classifies
the names in the document. Nominal and pronomi-
nal mentions are identified either with a chunker or
a full Penn-Treebank parser. A rule-based coref-
erence component identifies coreference relations,
forming entities from the mentions. Finally, a se-
mantic classifier assigns a class to each entity based
on the type of the first named mention (if the entity
includes a named mention) or the head of the first
nominal mention (using statistics gathered from the
ACE training corpus).
The ACE annotation guidelines specify 33 differ-
ent event subtypes, organized into 8 major types.
One of the major types is justice events, which in-
clude arrest, charge, trial, appeal, acquit, convict,
sentence, fine, execute, release, pardon, sue, and ex-
tradite subtypes. In parallel to entities, the event
tagger first identifies individual event mentions and
then uses event coreference to form events. For the
ACE evaluation, an annotated corpus of approxi-
mately 300,000 words is used to train the event tag-
ger.
For each event mention in the corpus, we collect
the trigger word (the main word indicating the event)
and a pattern recording the path from the trigger
to each event argument. These paths are recorded
in two forms: as the sequence of heads of maxi-
mal constituents between the trigger and the argu-
ment, and as the sequence of predicate-argument re-
lations connecting the trigger to the argument4 . In
4These predicate argument relations are based on a repre-
sentation called GLARF (Grammatical-Logical Argument Rep-
resentation Framework), which incorporates deep syntactic re-
lations and the argument roles from PropBank and NomBank.
addition, a set of maximum-entropy classifiers are
trained: to distinguish events from non-events, to
classify events by type and subtype, to distinguish
arguments from non-arguments, and to classify ar-
guments by argument role. In tagging new data, we
first match the context of each instance of a trig-
ger word against the collected patterns, thus iden-
tifying some arguments. The argument classifier is
then used to collect additional arguments within the
sentence. Finally, the event classifier (which uses
the proposed arguments as features) is used to re-
ject unlikely events. The patterns provide somewhat
more precise matching, while the argument classi-
fiers improve recall, yielding a tagger with better
performance than either strategy separately.
3.3 Answer Generation
Once the final batch of documents is received,
the answer generator module selects candidate pas-
sages. The names, with alternate renderings, are lo-
cated through the entity mentions by the IE system.
All sentences that contain a justice event and that
fall within a mention of a target by no more than
n sentences, where n is a settable parameter, which
was put at 5 for this evaluation, form the core of the
system?s answer.
The tactic takes the place of topic segmentation,
which we used for other question types in GALE
that did not have the benefit of the sophisticated
event recognition offered by the IE system. Segmen-
tation is used to give users sufficient context in the
answer without needing a means of identifying dif-
ficult definite nominal resolution cases that are not
handled by extraction.
In order to increase recall, in keeping with the
need for a comprehensive answer in the GALE eval-
uation, we added sentences that contain the name of
the target in documents that have justice events and
sentences that contain words describing the crime.
However, we imposed a limitation on the growth of
the answer size. When the target individual is well-
known, he or she will be mentioned in numerous
contexts, reducing the likelihood that this additional
mention will be relevant. Thus, when the size of the
answer grew too rapidly, we stopped including these
additional sentences, and produced sentences only
from the justice events. The threshold for triggering
this shift was 200 sentences.
536
3.4 Summarization
As a state-of-the-art baseline, we used a generic
multidocument summarization system that has been
tested in numerous contexts. It is, indeed, the
backup answer generator for several question types,
including the prosecution questions, in our GALE
system, and has been been tested in the topic-based
tasks of the 2005 and 2006 Document Understand-
ing Conferences.
A topic statement is formed by collapsing the
template arguments into one list, e.g., ?saddam hus-
sein crimes against humanity prosecution?, and the
answer generation module proceeds by using a hy-
brid approach that combines top-down strategies
based on syntactic patterns, alongside a suite of
summarization methods which guide content in a
bottom-up manner that clusters and combines the
candidate sentences (Blair-Goldensohn and McKe-
own, 2006).
4 Evaluation
The results of our evaluation are shown in Table 1.
We increased the number of test questions over the
number used in the official GALE evaluation and we
used only previously unseen questions. Documents
for the baseline system were selected without use of
the event annotations from Proteus.
We paired the 25 questions for judges, so that both
the system?s answer and the baseline answer were
assigned to the same person. We provided explicit
instructions on the handling on implicit references,
allowing the judges to use the context of the ques-
tion and other answer sentences to determine if a
sentence was relevant ? following the practice of the
GALE evaluation.
Our judges were randomly assigned questions
and asked whether the snippets, which in our case
were individual sentences, were relevant or not;
they could respond Relevant, Not Relevant or Don?t
Know. In cases where references were unclear, the
judges were asked to choose Don?t Know and these
were removed from the scoring.5
5In the GALE evaluation, the snippets are broken down by
hand into nuggets ? discrete pieces of information ? and the
answers are scored on that basis. However, we scored our re-
sponses on the basis of snippets (sentences) only, as it is much
more efficient, and therefore more feasible to repeat in the fu-
ture.
Our system using IE event detection and en-
tity tracking outperformed the summarization-based
baseline, with average precision of 68% compared
with 57%. Moreover, the specialized system sus-
tained that level of precision although it returned a
much larger number of snippets, totaling 2,086 over
the 25 questions, compared with 363 for the base-
line system. We computed a relative recall score, us-
ing the union of the sentences found by the systems
and judged relevant as the ground truth. For recall,
the specialized system scored an average 89% ver-
sus 17% for the baseline system. Computing an F-
measure weighting precision and recall equally, the
specialized system outperformed the baseline sys-
tem 75% to 23%. The difference in relative recall
and F-measure are both statisticaly significant under
a two-tailed, paired t-test, with p < 0.001.
5 Conclusion and Future Work
Our results show that the specialized system statis-
tically outperforms the baseline, a well-tested query
focused summarization approach, on precision. The
specialized system produced a much larger answer
on average (Table 1). Moreover, our answer gener-
ator seemed to adapt well to information in the cor-
pus. Of the six cases where it returned fewer than
10 sentences, the baseline found no additional sen-
tences four times (Questions B006, B011, B015 and
B022). We regard this as an important property in
the question-answering task.
A major challenge is to ascertain whether the
mention of the target is indeed involved in the rec-
ognized justice event. Our event recognition system
was developed within the ACE program and only
seeks to assigns roles within the local context of a
single sentence. We currently use a threshold to con-
sider whether an entity mention is reliable, but we
will experiment with ways to measure the likelihood
that a particular sentence is about the prosecution or
some other issue. We are planning to obtain vari-
ous pieces of information from additional secondary
queries to the search engine. Within the GALE pro-
gram, we are limited to the defined corpus, but in the
general case, we could add more varied resources.
In addition, we are working to produce answers
using text generation, to bring more sophisticated
summarization techniques to make a better presen-
537
QID System with IE Baseline System
Precision Recall F-meas Count Precision Recall F-meas Count
B001 0.728 0.905 0.807 92 0.818 0.122 0.212 11
B002 0.713 0.906 0.798 108 0.889 0.188 0.311 18
B003 0.770 0.942 0.848 148 0.875 0.058 0.109 8
B004 0.930 0.879 0.904 86 1.000 0.154 0.267 14
B005 0.706 0.923 0.800 34 0.400 0.231 0.293 15
B006 1.000 1.000 1.000 3 0.000 0.000 0.000 17
B007 0.507 1.000 0.673 73 0.421 0.216 0.286 19
B008 0.791 0.909 0.846 201 0.889 0.091 0.166 18
B009 0.759 0.960 0.848 158 0.941 0.128 0.225 17
B010 1.000 0.828 0.906 24 0.500 0.276 0.356 16
B011 0.500 1.000 0.667 6 0.000 0.000 0.000 18
B012 0.338 0.714 0.459 74 0.765 0.371 0.500 17
B013 0.375 0.900 0.529 120 0.700 0.280 0.400 20
B014 0.571 0.800 0.667 7 0.062 0.200 0.095 16
B015 0.500 1.000 0.667 2 0.000 0.000 0.000 10
B016 1.000 0.500 0.667 5 0.375 0.600 0.462 16
B017 1.000 1.000 1.000 13 0.125 0.077 0.095 7
B018 0.724 0.993 0.837 199 0.875 0.048 0.092 8
B019 0.617 0.954 0.749 201 0.684 0.100 0.174 19
B020 0.923 0.727 0.814 26 0.800 0.364 0.500 15
B021 0.562 0.968 0.711 162 0.818 0.096 0.171 11
B022 0.667 1.000 0.800 6 0.000 0.000 0.000 18
B023 0.684 0.950 0.795 196 0.778 0.050 0.093 9
B024 0.117 0.636 0.197 60 0.714 0.455 0.556 7
B025 0.610 0.943 0.741 82 0.722 0.245 0.366 18
Aver 0.684 0.893 0.749 83 0.566 0.174 0.229 14
Table 1: The table compares results of our answer generator combining the Indri and the Proteus ACE sys-
tem, against the focused-summarization baseline. This experiment is over 25 previously unseen questions.
The differences between the two systems are statistically significant (p < 0.001) for recall and f-measure by
a two-tailed, paired t-test. A big difference between the two systems is that the answer generator produces
a total of 2,086 answer sentences while sustaining an average precision of 0.684. In only three cases, does
the precision fall below 0.5. In contrast, the baseline system produced only 362, one-sixth the number of
answer sentences. While its average precision was not significantly worse than the answer-generator?s, its
precision varied widely, failing to find any correct sentences four times.
538
tation than an unordered list of sentences.
Finally, we will look into applying the techniques
used here on other topics. The first test would rea-
sonably be Conflict events, for which the ACE pro-
gram has training data. But ultimately, we would
like to adapt our system to arbitrary topic areas.
Acknowledgements
This material is based in part upon work supported
by the Defense Advanced Research Projects Agency
(DARPA) under Contract No. HR0011-06-C-0023.
Any opinions, findings and conclusions or recom-
mendations expressed in this material are those of
the authors and do not necessarily reflect the views
of the Defense Advanced Research Projects Agency
(DARPA).
References
Sasha Blair-Goldensohn and Kathleen McKeown. 2006.Integrating rhetorical-semantic relation models forquery-focused summarization. In Proceedings of 6th
Document Understanding Conference (DUC2006).
Hoa Trang Dang, Jimmy Lin, and Diane Kelly. 2006.Overview of the TREC 2006 question answering track.In Proceedings TREC. Forthcoming.
Ralph Grishman, David Westbrook, and Adam Meyers.2005. NYU?s english ACE 2005 system descrip-tion. In ACE 05 Evaluation Workshop. On-line athttp://nlp.cs.nyu.edu/publication.
Sanda Harabagiu, Dan Moldovan, Christine Clark,Mitchell Bowden, Andrew Hickl, and Patrick Wang.2005. Employing two question answering systems in
TREC 2005. In Proceedings of the Fourteenth Text
Retrieval Conference.
B. Katz, G. Marton, G. Borchardt, A. Brownell,S. Felshin, D. Loreto, J. Louis-Rosenberg, B. Lu,
F. Mora, S. Stiller, O. Uzuner, and A. Wilcox.2006. External knowledge sources for question an-swering. In Proceedings of TREC. On-line at
http://www.trec.nist.gov.
Giridhar Kumaran and James Allan. 2007. Informationretrieval techniques for templated queries. In Proceed-
ings of RIAO. Forthcoming.
D. Metzler and W.B. Croft. 2005. A Markov random
field model for term dependencies. In Proceedings of
ACM SIGIR, pages 472?479.
D. Oard, T. Elsayed, J. Wang, Y. Wu, P. Zhang, E. Abels,
J. Lin, and D. Soergel. 2006. Trec 2006 at maryland:Blog, enterprise, legal and QA tracks. In Proceedings
of TREC. On-line at http://www.trec.nist.gov.
Patrick Schone, Gary Ciany, Paul McNamee, James
Mayeld, Tina Bassi, and Anita Kulman. 2004. Ques-tion answering with QACTIS at TREC-2004. In Pro-
ceedings of the Thirteenth Text Retrieval Conference.
T. Strohman, D. Metzler, H. Turtle, and W.B. Croft.
2005. Indri: A language-model based search enginefor complex queries (extended version). Technical Re-port IR-407, CIIR, UMass Amherst.
Ellen M. Voorhees and Hoa Trang Dang. 2005.
Overview of the TREC 2005 question answering track.In Proceedings of the Fourteenth Text Retrieval Con-
ference.
539
An Improved Extraction Pattern Representation Model
for Automatic IE Pattern Acquisition
Kiyoshi Sudo, Satoshi Sekine, and Ralph Grishman
Department of Computer Science
New York University
715 Broadway, 7th Floor,
New York, NY 10003 USA
 
sudo,sekine,grishman  @cs.nyu.edu
Abstract
Several approaches have been described
for the automatic unsupervised acquisi-
tion of patterns for information extraction.
Each approach is based on a particular
model for the patterns to be acquired, such
as a predicate-argument structure or a de-
pendency chain. The effect of these al-
ternative models has not been previously
studied. In this paper, we compare the
prior models and introduce a new model,
the Subtree model, based on arbitrary sub-
trees of dependency trees. We describe
a discovery procedure for this model and
demonstrate experimentally an improve-
ment in recall using Subtree patterns.
1 Introduction
Information Extraction (IE) is the process of identi-
fying events or actions of interest and their partici-
pating entities from a text. As the field of IE has de-
veloped, the focus of study has moved towards au-
tomatic knowledge acquisition for information ex-
traction, including domain-specific lexicons (Riloff,
1993; Riloff and Jones, 1999) and extraction pat-
terns (Riloff, 1996; Yangarber et al, 2000; Sudo
et al, 2001). In particular, methods have recently
emerged for the acquisition of event extraction pat-
terns without corpus annotation in view of the cost
of manual labor for annotation. However, there has
been little study of alternative representation models
of extraction patterns for unsupervised acquisition.
In the prior work on extraction pattern acquisition,
the representation model of the patterns was based
on a fixed set of pattern templates (Riloff, 1996), or
predicate-argument relations, such as subject-verb,
and object-verb (Yangarber et al, 2000). The model
of our previous work (Sudo et al, 2001) was based
on the paths from predicate nodes in dependency
trees.
In this paper, we discuss the limitations of prior
extraction pattern representation models in relation
to their ability to capture the participating entities in
scenarios. We present an alternative model based on
subtrees of dependency trees, so as to extract enti-
ties beyond direct predicate-argument relations. An
evaluation on scenario-template tasks shows that the
proposed Subtree model outperforms the previous
models.
Section 2 describes the Subtree model for extrac-
tion pattern representation. Section 3 shows the
method for automatic acquisition. Section 4 gives
the experimental results of the comparison to other
methods and Section 5 presents an analysis of these
results. Finally, Section 6 provides some concluding
remarks and perspective on future research.
2 Subtree model
Our research on improved representation models for
extraction patterns is motivated by the limitations of
the prior extraction pattern representations. In this
section, we review two of the previous models in
detail, namely the Predicate-Argument model (Yan-
garber et al, 2000) and the Chain model (Sudo et al,
2001).
The main cause of difficulty in finding entities by
extraction patterns is the fact that the participating
entities can appear not only as an argument of the
predicate that describes the event type, but also in
other places within the sentence or in the prior text.
In the MUC-3 terrorism scenario, WEAPON entities
occur in many different relations to event predicates
in the documents. Even if WEAPON entities appear
in the same sentence with the event predicate, they
rarely serve as a direct argument of such predicates.
(e.g., ?One person was killed as the result of a bomb
explosion.?)
Predicate-Argument model The Predicate-
Argument model is based on a direct syntactic rela-
tion between a predicate and its arguments1 (Yan-
garber et al, 2000). In general, a predicate provides
a strong context for its arguments, which leads to
good accuracy. However, this model has two major
limitations in terms of its coverage, clausal bound-
aries and embedded entities inside a predicate?s
arguments.
Figure 12 shows an example of an extraction task
in the terrorism domain where the event template
consists of perpetrator, date, location and victim.
With the extraction patterns based on the Predicate-
Argument model, only perpetrator and victim can
be extracted. The location (downtown Jerusalem) is
embedded as a modifier of the noun (heart) within
the prepositional phrase, which is an adjunct of the
main predicate, triggered3. Furthermore, it is not
clear whether the extracted entities are related to the
same event, because of the clausal boundaries.4
1Since the case marking for a nominalized predicate is sig-
nificantly different from the verbal predicate, which makes it
hard to regularize the nominalized predicates automatically, the
constraint for the Predicate-Argument model requires the root
node to be a verbal predicate.
2Throughout this paper, extraction patterns are defined as
one or more word classes with their context in the dependency
tree, where the actual word matched with the class is associ-
ated to one of the slots in the template. The notation of the
patterns in this paper is based on a dependency tree where (  
(  -  )..(  -  )) denotes   is the head, and, for each 	 in 
  ,
 is its argument and the relation between   and  is labeled
with   . The labels introduced in this paper are SBJ (subject),
OBJ (object), ADV (adverbial adjunct), REL (relative), APPOS
(apposition) and prepositions (IN, OF, etc.). Also, we assume
that the order of the arguments does not matter. Symbols begin-
ning with C- represent NE (Named Entity) types.
3Yangarber refers this as a noun phrase pattern in (Yangar-
ber et al, 2000).
4This is the problem of merging the result of entity extrac-
tion. Most IE systems have hard-coded inference rules, such
Chain model Our previous work, the Chain
model (Sudo et al, 2001)5 attempts to remedy the
limitations of the Predicate-Argument model. The
extraction patterns generated by the Chain model
are any chain-shaped paths in the dependency tree.6
Thus it successfully avoids the clausal boundary
and embedded entity limitation. We reported a 5%
gain in recall at the same precision level in the
MUC-6 management succession task compared to
the Predicate-Argument model.
However, the Chain model also has its own weak-
ness in terms of accuracy due to the lack of context.
For example, in Figure 1(c), (triggered (  C-DATE  -
ADV)) is needed to extract the date entity. However,
the same pattern is likely to be applied to texts in
other domains as well, such as ?The Mexican peso
was devalued and triggered a national financial cri-
sis last week.?
Subtree model The Subtree model is a general-
ization of previous models, such that any subtree of
a dependency tree in the source sentence can be re-
garded as an extraction pattern candidate. As shown
in Figure 1(d), the Subtree model, by its defini-
tion, contains all the patterns permitted by either the
Predicate-Argument model or the Chain model. It
is also capable of providing more relevant context,
such as (triggered (explosion-OBJ)(  C-DATE  -ADV)).
The obvious advantage of the Subtree model is
the flexibility it affords in creating suitable patterns,
spanning multiple levels and multiple branches. Pat-
tern coverage is further improved by relaxing the
constraint that the root of the pattern tree be a pred-
icate node. However, this flexibility can also be a
disadvantage, since it means that a very large num-
ber of pattern candidates ? all possible subtrees of
the dependency tree of each sentence in the corpus
? must be considered. An efficient procedure is re-
quired to select the appropriate patterns from among
the candidates.
Also, as the number of pattern candidates in-
creases, the amount of noise and complexity in-
as ?triggering an explosion is related to killing or injuring and
therefore constitutes one terrorism action.?
5Originally we called it ?Tree-Based Representation of Pat-
terns?. We renamed it to avoid confusion with the proposed
approach that is also based on dependency trees.
6(Sudo et al, 2001) required the root node of the chain to be
a verbal predicate, but we have relaxed that constraint for our
experiments.
(a)
JERUSALEM, March 21 ? A smiling Palestinian suicide bomber triggered a mas-
sive explosion in the heavily policed heart of downtown Jerusalem today, killing
himself and three other people and injuring scores.
(b)
(c)
Predicate-Argument Chain model
(triggered (  C-PERSON  -SBJ)(explosion-OBJ)(  C-DATE  -ADV)) (triggered (  C-PERSON  -SBJ))
(killing (  C-PERSON  -OBJ)) (triggered (heart-IN (  C-LOCATION  -OF)))
(injuring (  C-PERSON  -OBJ)) (triggered (killing-ADV (  C-PERSON  -OBJ)))
(triggered (injuring-ADV (  C-PERSON  -OBJ)))
(triggered (  C-DATE  -ADV))
(d)
Subtree model
(triggered (  C-PERSON  -SBJ)(explosion-OBJ)) (triggered (explosion-OBJ)(  C-DATE  -ADV))
(killing (  C-PERSON  -OBJ)) (triggered (  C-DATE  -ADV)(killing-ADV))
(injuring (  C-PERSON  -OBJ)) (triggered (  C-DATE  -ADV)(killing-ADV(  C-PERSON  -OBJ)))
(triggered (heart-IN (  C-LOCATION  -OF))) (triggered (  C-DATE  -ADV)(injuring-ADV))
(triggered (killing-ADV (  C-PERSON  -OBJ))) (triggered (explosion-OBJ)(killing (  C-PERSON  -OBJ)))
(triggered (  C-DATE  -ADV)) ...
Figure 1: (a) Example sentence on terrorism scenario. (b) Dependency Tree of the example sentence (The entities to be extracted
are shaded in the tree). (c) Predicate-Argument patterns and Chain-model patterns that contribute to the extraction task. (d) Subtree
model patterns that contribute the extraction task.
creases. In particular, many of the pattern candidates
overlap one another. For a given set of extraction
patterns, if pattern A subsumes pattern B (say, A is
(shoot (  C-PERSON  -OBJ)(to death)) and B is (shoot (  C-
PERSON  -OBJ))), there is no added contribution for
extraction by pattern matching with A (since all the
matches with pattern A must be covered with pattern
B). Therefore, we need to pay special attention to the
ranking function for pattern candidates, so that pat-
terns with more relevant contexts get higher score.
3 Acquisition Method
This section discusses an automatic procedure to
learn extraction patterns. Given a narrative descrip-
tion of the scenario and a set of source documents,
the following three stages obtain the relevant extrac-
tion patterns for the scenario; preprocessing, docu-
ment retrieval, and ranking pattern candidates.
3.1 Stage 1: Preprocessing
Morphological analysis and Named Entities (NE)
tagging are performed at this stage.7 Then all the
sentences are converted into dependency trees by an
appropriate dependency analyzer.8 The NE tagging
7We used Extended NE hierarchy based on (Sekine et al,
2002), which is structured and contains 150 classes.
8Any degree of detail can be chosen through entire proce-
dure, from lexicalized dependency to chunk-level dependency.
For the following experiment in Japanese, we define a node in
replaces named entities by their class, so the result-
ing dependency trees contain some NE class names
as leaf nodes. This is crucial to identifying common
patterns, and to applying these patterns to new text.
3.2 Stage 2: Document Retrieval
The procedure retrieves a set of documents that de-
scribe the events of the scenario of interest, the rel-
evant document set. A set of narrative sentences de-
scribing the scenario is selected to create a query
for the retrieval. Any IR system of sufficient accu-
racy can be used at this stage. For this experiment,
we retrieved the documents using CRL?s stochastic-
model-based IR system (Murata et al, 1999).
3.3 Stage 3: Ranking Pattern Candidates
Given the dependency trees of parsed sentences in
the relevant document set, all the possible subtrees
can be candidates for extraction patterns. The rank-
ing of pattern candidates is inspired by TF/IDF scor-
ing in IR literature; a pattern is more relevant when
it appears more in the relevant document set and less
across the entire collection of source documents.
The right-most expansion base subtree discovery
algorithm (Abe et al, 2002) was implemented to cal-
culate term frequency (raw frequency of a pattern)
and document frequency (the number of documents
where a pattern appears) for each pattern candidate.
The algorithm finds the subtrees appearing more fre-
quently than a given threshold by constructing the
subtrees level by level, while keeping track of their
occurrence in the corpus. Thus, it efficiently avoids
the construction of duplicate patterns and runs al-
most linearly in the total size of the maximal tree
patterns contained in the corpus.
The following ranking function was used to rank
each pattern candidate. The score of subtree   ,
	

, is
	


	Discovering Relations among Named Entities from Large Corpora
Takaaki Hasegawa  
Cyberspace Laboratories
Nippon Telegraph and Telephone Corporation
1-1 Hikarinooka, Yokosuka,
Kanagawa 239-0847, Japan
hasegawa.takaaki@lab.ntt.co.jp
Satoshi Sekine and Ralph Grishman
Dept. of Computer Science
New York University
715 Broadway, 7th floor,
New York, NY 10003, U.S.A.

sekine,grishman  @cs.nyu.edu
Abstract
Discovering the significant relations embedded in
documents would be very useful not only for infor-
mation retrieval but also for question answering and
summarization. Prior methods for relation discov-
ery, however, needed large annotated corpora which
cost a great deal of time and effort. We propose
an unsupervised method for relation discovery from
large corpora. The key idea is clustering pairs of
named entities according to the similarity of con-
text words intervening between the named entities.
Our experiments using one year of newspapers re-
veals not only that the relations among named enti-
ties could be detected with high recall and precision,
but also that appropriate labels could be automati-
cally provided for the relations.
1 Introduction
Although Internet search engines enable us to ac-
cess a great deal of information, they cannot eas-
ily give us answers to complicated queries, such as
?a list of recent mergers and acquisitions of com-
panies? or ?current leaders of nations from all over
the world?. In order to find answers to these types
of queries, we have to analyze relevant documents
to collect the necessary information. If many rela-
tions such as ?Company A merged with Company
B? embedded in those documents could be gathered
and structured automatically, it would be very useful
not only for information retrieval but also for ques-
tion answering and summarization. Information Ex-
traction provides methods for extracting informa-
tion such as particular events and relations between
entities from text. However, it is domain depen-
dent and it could not give answers to those types of
queries from Web documents which include widely
various domains.
Our goal is automatically discovering useful re-
lations among arbitrary entities embedded in large

This work is supported by Nippon Telegraph and Telephone
(NTT) Corporation?s one-year visiting program at New York
University.
text corpora. We defined a relation broadly as an af-
filiation, role, location, part-whole, social relation-
ship and so on between a pair of entities. For ex-
ample, if the sentence, ?George Bush was inaugu-
rated as the president of the United States.? exists in
documents, the relation, ?George Bush?(PERSON)
is the ?President of? the ?United States? (GPE1),
should be extracted. In this paper, we propose
an unsupervised method of discovering relations
among various entities from large text corpora. Our
method does not need the richly annotated corpora
required for supervised learning ? corpora which
take great time and effort to prepare. It also does
not need any instances of relations as initial seeds
for weakly supervised learning. This is an advan-
tage of our approach, since we cannot know in ad-
vance all the relations embedded in text. Instead, we
only need a named entity (NE) tagger to focus on
the named entities which should be the arguments
of relations. Recently developed named entity tag-
gers work quite well and are able to extract named
entities from text at a practically useful level.
The rest of this paper is organized as follows. We
discuss prior work and their limitations in section 2.
We propose a new method of relation discovery in
section 3. Then we describe experiments and eval-
uations in section 4 and 5, and discuss the approach
in section 6. Finally, we conclude with future work.
2 Prior Work
The concept of relation extraction was introduced
as part of the Template Element Task, one of the
information extraction tasks in the Sixth Message
Understanding Conference (MUC-6) (Defense Ad-
vanced Research Projects Agency, 1995). MUC-7
added a Template Relation Task, with three rela-
tions. Following MUC, the Automatic Content Ex-
traction (ACE) meetings (National Institute of Stan-
dards and Technology, 2000) are pursuing informa-
1GPE is an acronym introduced by the ACE program to rep-
resent a Geo-Political Entity ? an entity with land and a gov-
ernment.
tion extraction. In the ACE Program2, Relation De-
tection and Characterization (RDC) was introduced
as a task in 2002. Most of approaches to the ACE
RDC task involved supervised learning such as ker-
nel methods (Zelenko et al, 2002) and need richly
annotated corpora which are tagged with relation in-
stances. The biggest problem with this approach is
that it takes a great deal of time and effort to prepare
annotated corpora large enough to apply supervised
learning. In addition, the varieties of relations were
limited to those defined by the ACE RDC task. In
order to discover knowledge from diverse corpora,
a broader range of relations would be necessary.
Some previous work adopted a weakly super-
vised learning approach. This approach has the ad-
vantage of not needing large tagged corpora. Brin
proposed the bootstrapping method for relation dis-
covery (Brin, 1998). Brin?s method acquired pat-
terns and examples by bootstrapping from a small
initial set of seeds for a particular relation. Brin
used a few samples of book titles and authors, col-
lected common patterns from context including the
samples and finally found new examples of book
title and authors whose context matched the com-
mon patterns. Agichtein improved Brin?s method
by adopting the constraint of using a named entity
tagger (Agichtein and Gravano, 2000). Ravichan-
dran also explored a similar method for question an-
swering (Ravichandran and Hovy, 2002). These ap-
proaches, however, need a small set of initial seeds.
It is also unclear how initial seeds should be selected
and how many seeds are required. Also their meth-
ods were only tried on functional relations, and this
was an important constraint on their bootstrapping.
The variety of expressions conveying the same re-
lation can be considered an example of paraphrases,
and so some of the prior work on paraphrase ac-
quisition is pertinent to relation discovery. Lin pro-
posed another weakly supervised approach for dis-
covering paraphrase (Lin and Pantel, 2001). Firstly
Lin focused on verb phrases and their fillers as sub-
ject or object. Lin?s idea was that two verb phrases
which have similar fillers might be regarded as para-
phrases. This approach, however, also needs a sam-
ple verb phrase as an initial seed in order to find
similar verb phrases.
3 Relation Discovery
3.1 Overview
We propose a new approach to relation discovery
from large text corpora. Our approach is based on
2A research and evaluation program in information extrac-
tion organized by the U.S. Government.
context based clustering of pairs of entities. We as-
sume that pairs of entities occurring in similar con-
text can be clustered and that each pair in a cluster
is an instance of the same relation. Relations be-
tween entities are discovered through this clustering
process. In cases where the contexts linking a pair
of entities express multiple relations, we expect that
the pair of entities either would not be clustered at
all, or would be placed in a cluster corresponding
to its most frequently expressed relation, because
its contexts would not be sufficiently similar to con-
texts for less frequent relations. We assume that use-
ful relations will be frequently mentioned in large
corpora. Conversely, relations mentioned once or
twice are not likely to be important.
Our basic idea is as follows:
1. tagging named entities in text corpora
2. getting co-occurrence pairs of named entities
and their context
3. measuring context similarities among pairs of
named entities
4. making clusters of pairs of named entities
5. labeling each cluster of pairs of named entities
We show an example in Figure 1. First, we find the
pair of ORGANIZATIONs (ORG) A and B, and the
pair of ORGANIZATIONs (ORG) C and D, after we
run the named entity tagger on our newspaper cor-
pus. We collect all instances of the pair A and B
occurring within a certain distance of one another.
Then, we accumulate the context words interven-
ing between A and B, such as ?be offer to buy?, ?be
negotiate to acquire?.3 In same way, we also ac-
cumulate context words intervening between C and
D. If the set of contexts of A and B and those of C
and D are similar, these two pairs are placed into
the same cluster. A ? B and C ? D would be in the
same relation, in this case, merger and acquisition
(M&A). That is, we could discover the relation be-
tween these ORGANIZATIONs.
3.2 Named entity tagging
Our proposed method is fully unsupervised. We
do not need richly annotated corpora or any ini-
tial manually selected seeds. Instead of them, we
use a named entity (NE) tagger. Recently devel-
oped named entity taggers work quite well and ex-
tract named entities from text at a practically usable
3We collect the base forms of words which are stemmed
by a POS tagger (Sekine, 2001). But verb past participles are
distinguished from other verb forms in order to distinguish the
passive voice from the active voice.
  
	
	



	


		
		
	
	
	



 
	Proceedings of the 43rd Annual Meeting of the ACL, pages 411?418,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Improving Name Tagging by  
Reference Resolution and Relation Detection 
 
 
Heng Ji Ralph Grishman 
Department of Computer Science 
New York University 
New York, NY, 10003, USA 
hengji@cs.nyu.edu grishman@cs.nyu.edu 
 
 
 
 
Abstract 
Information extraction systems incorpo-
rate multiple stages of linguistic analysis.  
Although errors are typically compounded 
from stage to stage, it is possible to re-
duce the errors in one stage by harnessing 
the results of the other stages.  We dem-
onstrate this by using the results of 
coreference analysis and relation extrac-
tion to reduce the errors produced by a 
Chinese name tagger.  We use an N-best 
approach to generate multiple hypotheses 
and have them re-ranked by subsequent 
stages of processing.  We obtained 
thereby a reduction of 24% in spurious 
and incorrect name tags, and a reduction 
of 14% in missed tags. 
1 Introduction 
Systems which extract relations or events from a 
document typically perform a number of types of 
linguistic analysis in preparation for information 
extraction.  These include name identification and 
classification, parsing (or partial parsing), semantic 
classification of noun phrases, and coreference 
analysis.  These tasks are reflected in the evalua-
tion tasks introduced for MUC-6 (named entity, 
coreference, template element) and MUC-7 (tem-
plate relation). 
In most extraction systems, these stages of 
analysis are arranged sequentially, with each stage 
using the results of prior stages and generating a 
single analysis that gets enriched by each stage.  
This provides a simple modular organization for 
the extraction system.  
Unfortunately, each stage also introduces a cer-
tain level of error into the analysis.  Furthermore, 
these errors are compounded ? for example, errors 
in name recognition may lead to errors in parsing.  
The net result is that the final output (relations or 
events) may be quite inaccurate. 
This paper considers how interactions between 
the stages can be exploited to reduce the error rate. 
For example, the results of coreference analysis or 
relation identification may be helpful in name clas-
sification, and the results of relation or event ex-
traction may be helpful in coreference. 
Such interactions are not easily exploited in a 
simple sequential model ? if name classification 
is performed at the beginning of the pipeline, it 
cannot make use of the results of subsequent stages. 
It may even be difficult to use this information im-
plicitly, by using features which are also used in 
later stages, because the representation used in the 
initial stages is too limited. 
To address these limitations, some recent sys-
tems have used more parallel designs, in which a 
single classifier (incorporating a wide range of fea-
tures) encompasses what were previously several 
separate stages (Kambhatla, 2004; Zelenko et al, 
2004).  This can reduce the compounding of errors 
of the sequential design.  However, it leads to a 
very large feature space and makes it difficult to 
select linguistically appropriate features for par-
ticular analysis tasks.  Furthermore, because these 
decisions are being made in parallel, it becomes 
much harder to express interactions between the 
levels of analysis based on linguistic intuitions. 
411
In order to capture these interactions more ex-
plicitly, we have employed a sequential design in 
which multiple hypotheses are forwarded from 
each stage to the next, with hypotheses being rer-
anked and pruned using the information from later 
stages. We shall apply this design to show how 
named entity classification can be improved by 
?feedback? from coreference analysis and relation 
extraction.  We shall show that this approach can 
capture these interactions in a natural and efficient 
manner, yielding a substantial improvement in 
name identification and classification. 
2 Prior Work 
A wide variety of trainable models have been ap-
plied to the name tagging task, including HMMs 
(Bikel et al, 1997), maximum entropy models 
(Borthwick, 1999), support vector machines 
(SVMs), and conditional random fields.  People 
have spent considerable effort in engineering ap-
propriate features to improve performance; most of 
these involve internal name structure or the imme-
diate local context of the name. 
Some other named entity systems have explored 
global information for name tagging. (Borthwick,  
1999) made a second tagging pass which uses in-
formation on token sequences tagged in the first 
pass; (Chieu and Ng, 2002) used as features infor-
mation about features assigned to other instances 
of the same token. 
Recently, in (Ji and Grishman, 2004) we pro-
posed a name tagging method which applied an 
SVM based on coreference information to filter the 
names with low confidence, and used coreference 
rules to correct and recover some names. One limi-
tation of this method is that in the process of dis-
carding many incorrect names, it also discarded 
some correct names. We attempted to recover 
some of these names by heuristic rules which are 
quite language specific. In addition, this single-
hypothesis method placed an upper bound on recall. 
Traditional statistical name tagging methods 
have generated a single name hypothesis. BBN 
proposed the N-Best algorithm for speech recogni-
tion in (Chow and Schwartz, 1989). Since then N-
Best methods have been widely used by other re-
searchers (Collins, 2002; Zhai et al, 2004). 
In this paper, we tried to combine the advan-
tages of the prior work, and incorporate broader 
knowledge into a more general re-ranking model. 
3 Task and Terminology 
Our experiments were conducted in the context of 
the ACE Information Extraction evaluations, and 
we will use the terminology of these evaluations: 
entity:  an object or a set of objects in one of the 
semantic categories of interest 
mention:  a reference to an entity (typically, a noun 
phrase) 
name mention:  a reference by name to an entity 
nominal mention:  a reference by a common noun 
or noun phrase to an entity 
relation:  one of a specified set of relationships be-
tween a pair of entities 
The 2004 ACE evaluation had 7 types of entities, 
of which the most common were PER (persons), 
ORG (organizations), and GPE (?geo-political enti-
ties? ? locations which are also political units, such 
as countries, counties, and cities).  There were 7 
types of relations, with 23 subtypes.  Examples of 
these relations are ?the CEO of Microsoft? (an em-
ploy-exec relation), ?Fred?s wife? (a family rela-
tion), and ?a military base in Germany? (a located 
relation). 
In this paper we look at the problem of identify-
ing name mentions in Chinese text and classifying 
them as persons, organizations, or GPEs.  Because 
Chinese has neither capitalization nor overt word 
boundaries, it poses particular problems for name 
identification. 
4 Baseline System 
4.1 Baseline Name Tagger 
Our baseline name tagger consists of a HMM tag-
ger augmented with a set of post-processing rules.  
The HMM tagger generally follows the Nymble 
model (Bikel et al 1997), but with multiple hy-
potheses as output and a larger number of states 
(12) to handle name prefixes and suffixes, and 
transliterated foreign names separately.  It operates 
on the output of a word segmenter from Tsinghua 
University.   
Within each of the name class states, a statistical 
bigram model is employed, with the usual one-
word-per-state emission. The various probabilities 
involve word co-occurrence, word features, and 
class probabilities. Then it uses A* search decod-
ing to generate multiple hypotheses. Since these 
probabilities are estimated based on observations 
412
seen in a corpus, ?back-off models? are used to 
reflect the strength of support for a given statistic, 
as for the Nymble system. 
We also add post-processing rules to correct 
some omissions and systematic errors using name 
lists (for example, a list of all Chinese last names; 
lists of organization and location suffixes) and par-
ticular contextual patterns (for example, verbs oc-
curring with people?s names).  They also deal with 
abbreviations and nested organization names. 
The HMM tagger also computes the margin ? 
the difference between the log probabilities of the 
top two hypotheses.  This is used as a rough meas-
ure of confidence in the top hypothesis (see sec-
tions 5.3 and 6.2, below). 
The name tagger used for these experiments 
identifies the three main ACE entity types: Person 
(PER), Organization (ORG), and GPE (names of 
the other ACE types are identified by a separate 
component of our system, not involved in the ex-
periments reported here). 
4.2 Nominal Mention Tagger 
Our nominal mention tagger (noun group recog-
nizer) is a maximum entropy tagger trained on the 
Chinese TreeBank from the University of Pennsyl-
vania, supplemented by list matching. 
4.3  Reference Resolver  
Our baseline reference resolver goes through two 
successive stages: first, coreference rules will iden-
tify some high-confidence positive and negative 
mention pairs, in training data and test data; then 
the remaining samples will be used as input of a 
maximum entropy tagger. The features used in this 
tagger involve distance, string matching, lexical 
information, position, semantics, etc. We separate 
the task into different classifiers for different men-
tion types (name / noun / pronoun). Then we in-
corporate the results from the relation tagger to 
adjust the probabilities from the classifiers. Finally 
we apply a clustering algorithm to combine them 
into entities (sets of coreferring mentions). 
4.4 Relation Tagger 
The relation tagger uses a k-nearest-neighbor algo-
rithm. For both training and test, we consider all 
pairs of entity mentions where there is at most one 
other mention between the heads of the two men-
tions of interest1.  Each training / test example con-
sists of the pair of mentions and the sequence of 
intervening words. Associated with each training 
example is either one of the ACE relation types or 
no relation at all. We defined a distance metric be-
tween two examples based on 
? whether the heads of the mentions match 
? whether the ACE types of the heads of the mentions 
match (for example, both are people or both are or-
ganizations) 
? whether the intervening words match 
To tag a test example, we find the k nearest 
training examples (where k = 3) and use the dis-
tance to weight each neighbor, then select the most 
common class in the weighted neighbor set. 
To provide a crude measure of the confidence of 
our relation tagger, we define two thresholds, Dnear 
and Dfar.  If the average distance d to the nearest 
neighbors d < Dnear, we consider this a definite re-
lation.  If Dnear < d < Dfar, we consider this a possi-
ble relation.  If d > Dfar, the tagger assumes that no 
relation exists (regardless of the class of the nearest 
neighbor). 
5 Information from Coreference and Re-
lations 
Our system is processing a document consisting of 
multiple sentences.  For each sentence, the name 
recognizer generates multiple hypotheses, each of 
which is an NE tagging of the entire sentence. The 
names in the hypothesis, plus the nouns in the 
categories of interest constitute the mention set for 
that hypothesis. Coreference resolution links these 
mentions, assigning each to an entity.  In symbols: 
 Si  is the i-th sentence in the document. 
Hi  is the hypotheses set for Si  
 hij  is the j-th hypothesis in Si  
Mij  is the mention set for hij  
mijk  is the k-th mention in Mij  
eijk  is the entity which mijk belongs to according to 
the current reference resolution results 
5.1 Coreference Features 
For each mention we compute seven quantities 
based on the results of name tagging and reference 
resolution: 
                                                          
1 This constraint is relaxed for parallel structures such as ?mention1, mention2, 
[and] mention3?.?; in such cases there can be more than one intervening men-
tion. 
413
CorefNumijk  is the number of mentions in eijk  
WeightSumijk  is the sum of all the link weights be-
tween mijk and other mentions in eijk , 0.8 for 
name-name coreference; 0.5 for apposition;  
0.3 for other name-nominal coreference 
FirstMentionijk  is 1 if mijk is the first name mention 
in the entity; otherwise 0 
Headijk  is 1 if mijk includes the head word of name; 
otherwise 0 
Withoutidiomijk  is 1 if mijk is not part of an idiom; 
otherwise 0 
PERContextijk  is the number of PER context words 
around a PER name such as a title or an ac-
tion verb involving a PER 
ORGSuffixijk  is 1 if ORG mijk includes a suffix word; 
otherwise 0 
The first three capture evidence of the correct-
ness of a name provided by reference resolution; 
for example, a name which is coreferenced with 
more other mentions is more likely to be correct.  
The last four capture local or name-internal evi-
dence; for instance, that an organization name in-
cludes an explicit, organization-indicating suffix. 
We then compute, for each of these seven quan-
tities, the sum over all mentions k in a sentence, 
obtaining values for CorefNumij, WeightSumij, etc.: 
CorefNum CorefNumij ijk
k
= ?   etc. 
Finally, we determine, for a given sentence and 
hypothesis, for each of these seven quantities, 
whether this quantity achieves the maximum of its 
values for this hypothesis: 
BestCorefNumij ?  
 CorefNumij = maxq CorefNumiq   etc. 
We will use these properties of the hypothesis as 
features in assessing the quality of a hypothesis.  
5.2 Relation Word Clusters 
In addition to using relation information for 
reranking name hypotheses, we used the relation 
training corpus to build word clusters which could 
more directly improve name tagging.  Name tag-
gers rely heavily on words in the immediate con-
text to identify and classify names; for example, 
specific job titles, occupations, or family relations 
can be used to identify people names.  Such words 
are learned individually from the name tagger?s 
training corpus.  If we can provide the name tagger 
with clusters of related words, the tagger will be 
able to generalize from the examples in the training 
corpus to other words in the cluster. 
The set of ACE relations includes several in-
volving employment, social, and family relations.  
We gathered the words appearing as an argument 
of one of these relations in the training corpus, 
eliminated low-frequency terms and manually ed-
ited the ten resulting clusters to remove inappro-
priate terms.  These were then combined with lists 
(of titles, organization name suffixes, location suf-
fixes) used in the baseline tagger. 
5.3 Relation Features 
Because the performance of our relation tagger 
is not as good as our coreference resolver, we have 
used the results of relation detection in a relatively 
simple way to enhance name detection.  The basic 
intuition is that a name which has been correctly 
identified is more likely to participate in a relation 
than one which has been erroneously identified. 
For a given range of margins (from the HMM), 
the probability that a name in the first hypothesis is 
correct is shown in the following table, for names 
participating and not participating in a relation: 
 
Margin In Relation(%) Not in Relation(%)
<4 90.7 55.3 
<3 89.0 50.1 
<2 86.9 42.2 
<1.5 81.3 28.9 
<1.2 78.8 23.1 
<1 75.7 19.0 
<0.5 66.5 14.3 
Table 1 Probability of a name being correct 
 
Table 1 confirms that names participating in re-
lations are much more likely to be correct than 
names that do not participate in relations.  We also 
see, not surprisingly, that these probabilities are 
strongly affected by the HMM hypothesis margin 
(the difference in log probabilities) between the 
first hypothesis and the second hypothesis.  So it is 
natural to use participation in a relation (coupled 
with a margin value) as a valuable feature for re-
ranking name hypotheses. 
Let mijk be the k-th name mention for hypothe-
sis hij of sentence; then we define: 
414
Inrelationijk  = 1 if mijk  is in a definite relation 
   = 0 if mijk is in a possible relation 
   = -1 if mijk is not in a relation  
 Inrelation Inrelationij ijk
k
= ?  
Mostrelated Inrelation Inrelationij ij q iq? =( max )
  Finally, to capture the interaction with the margin, 
we let zi  = the margin for sentence Si and divide 
the range of values of zi into six intervals Mar1, ? 
Mar6.  And we define the hypothesis ranking in-
formation: FirstHypothesisij = 1 if j =1; otherwise 0. 
We will use as features for ranking hij the con-
junction of Mostrelatedij, zi ? Marp (p = 1, ?, 6), 
and FirstHypothesisij . 
6 Using the Information from Corefer-
ence and Relations 
6.1 Word Clustering based on Relations 
As we described in section 5.2, we can generate 
word clusters based on relation information. If a 
word is not part of a relation cluster, we consider it 
an independent (1-word) cluster.  
The Nymble name tagger (Bikel et al, 1999) re-
lies on a multi-level linear interpolation model for 
backoff. We extended this model by adding a level 
from word to cluster, so as to estimate more reli-
able probabilities for words in these clusters. Table 
2 shows the extended backoff model for each of 
the three probabilities used by Nymble.  
 
Transition  
Probability 
First-Word 
Emission  
Probability 
Non-First-Word
Emission  
Probability 
P(NC2|NC1, 
 <w1, f1>) 
P(<w2,f2>| 
NC1, NC2) 
P(<w2,f2>| 
<w1,f1>, NC2) 
 P(<Cluster2,f2>| 
NC1, NC2) 
P(<Cluster2,f2>|
<w1,f1>, NC2) 
P(NC2|NC1,  
<Cluster1, 
f1>) 
P(<Cluster2,f2>| 
<+begin+, other>, 
NC2) 
P(<Cluster2,f2>|
<Cluster1,f1>, 
NC2) 
P(NC2|NC1) P(<Cluster2, f2>|NC2) 
P(NC2)  P(Cluster2|NC2) * P(f2|NC2) 
1/#(name 
classes) 
1/#(cluster)  *  1/#(word features) 
Table2 Extended Backoff Model 
 
6.2 Pre-pruning by Margin 
The HMM tagger produces the N best hypotheses 
for each sentence.2  In order to decide when we 
need to rely on global (coreference and relation) 
information for name tagging, we want to have 
some assessment of the confidence that the name 
tagger has in the first hypothesis.  In this paper, we 
use the margin for this purpose. A large margin 
indicates greater confidence that the first hypothe-
sis is correct.3  So if the margin of a sentence is 
above a threshold, we select the first hypothesis, 
dropping the others and by-passing the reranking. 
6.3 Re-ranking based on Coreference 
We described in section 5.1, above, the coreference 
features which will be used for reranking the hy-
potheses after pre-pruning. A maximum entropy 
model for re-ranking these hypotheses is then 
trained and applied as follows: 
 
Training 
1. Use K-fold cross-validation to generate multi-
ple name tagging hypotheses for each docu-
ment in the training data Dtrain (in each of the K 
iterations, we use K-1 subsets to train the 
HMM and then generate hypotheses from the 
Kth subset). 
2. For each document d in Dtrain, where d includes 
n sentences S1?Sn 
For i = 1?n, let m = the number of hy-
potheses for Si 
(1) Pre-prune the candidate hypotheses us-
ing the HMM margin 
(2) For each hypothesis hij, j = 1?m 
(a) Compare hij with the key, set the 
prediction Valueij ?Best? or ?Not 
Best? 
(b) Run the Coreference Resolver on 
hij and the best hypothesis for each 
of the other sentences, generate 
entity results for each candidate 
name in hij 
(c) Generate a coreference feature vec-
tor Vij for hij 
(d) Output Vij and Valueij 
                                                          
2 We set different N = 5, 10, 20 or 30 for different margin ranges, by cross-
validation checking the training data about the ranking position of the best 
hypothesis for each sentence.  With this N, optimal reranking (selecting the best 
hypothesis among the N best) would yield Precision = 96.9 Recall = 94.5 F = 
95.7 on our test corpus. 
3 Similar methods based on HMM margins were used by (Scheffer et al, 2001). 
415
3. Train Maxent Re-ranking system on all Vij and 
Valueij 
 
Test 
1. Run the baseline name tagger to generate mul-
tiple name tagging hypotheses for each docu-
ment in the test data Dtest 
2. For each document d in Dtest, where d includes 
n sentences S1?Sn 
(1) Initialize: Dynamic input of coreference re-
solver H = {hi-best | i = 1?n, hi-best is the 
current best hypothesis for Si} 
(2) For i = 1?n, assume m = the number of 
hypotheses  for Si 
(a) Pre-prune the candidate hypotheses us-
ing the HMM margin 
(b) For each hypothesis hij, j = 1?m 
? hi-best = hij  
? Run the Coreference Resolver on H, 
generate entity results for each name 
candidate in hij 
? Generate a coreference feature vec-
tor Vij for hij 
? Run Maxent Re-ranking system on 
Vij, produce Probij of ?Best? value 
(c) hi-best = the hypothesis with highest 
Probij of ?Best? value, update H and 
output hi-best 
6.4 Re-ranking based on Relations 
From the above first-stage re-ranking by corefer-
ence, for each hypothesis we got the probability of 
its being the best one. By using these results and 
relation information we proceed to a second-stage 
re-ranking. As we described in section 5.3, the in-
formation of ?in relation or not? can be used to-
gether with margin as another important measure 
of confidence. 
  In addition, we apply the mechanism of weighted 
voting among hypotheses (Zhai et al, 2004) as an 
additional feature in this second-stage re-ranking. 
This approach allows all hypotheses to vote on a 
possible name output. A recognized name is con-
sidered correct only when it occurs in more than 30 
percent of the hypotheses (weighted by their prob-
ability).  
In our experiments we use the probability pro-
duced by the HMM, probij , for hypothesis hij . We 
normalize this probability weight as: 
W
prob
probij
ij
iq
q
= ?
exp( )
exp( )
 
For each name mention mijk in hij , we define:  
Occur mq ijk( )  = 1 if mijk occurs in hq  
   = 0 otherwise 
Then we count its voting value as follows: 
Votingijk  is 1 if W Occur miq q ijk
q
?? ( ) >0.3;  
  otherwise 0. 
The voting value of hij is:  
Voting Votingij ijk
k
= ?  
Finally we define the following voting feature: 
BestVoting Voting Votingij ij q iq? =( max )  
This feature is used, together with the features 
described at the end of section 5.3 and the prob-
ability score from the first stage, for the second-
stage maxent re-ranking model. 
One appeal of the above two re-ranking algo-
rithms is its flexibility in incorporating features 
into a learning model: essentially any coreference 
or relation features which might be useful in dis-
criminating good from bad structures can be in-
cluded.  
7 System Pipeline 
Combining all the methods presented above, the 
flow of our final system is shown in figure 1.  
8 Evaluation Results 
8.1 Training and Test Data 
We took 346 documents from the 2004 ACE train-
ing corpus and official test set, including both 
broadcast news and newswire, as our blind test set. 
To train our name tagger, we used the Beijing Uni-
versity Insititute of Computational Linguistics cor-
pus ? 2978 documents from the People?s Daily in 
1998 ? and 667 texts in the training corpus for the 
2003 & 2004 ACE evaluation. Our reference re-
solver is trained on these 667 ACE texts. The rela-
tion tagger is trained on 546 ACE 2004 texts, from 
which we also extracted the relation clusters. The 
test set included 11715 names: 3551 persons, 5100 
GPEs and 3064 organizations. 
 
416
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 1  System Flow 
8.2 Overall Performance Comparison 
Table 3 shows the performance of the baseline sys-
tem; Table 4 is the system with relation word clus-
ters; Table 5 is the system with both relation 
clusters and re-ranking based on coreference fea-
tures; and Table 6 is the whole system with sec-
ond-stage re-ranking using relations. 
The results indicate that relation word clusters 
help to improve the precision and recall of most 
name types. Although the overall gain in F-score is 
small (0.7%), we believe further gain can be 
achieved if the relation corpus is enlarged in the 
future. The re-ranking using the coreference fea-
tures had the largest impact, improving precision 
and recall consistently for all types. Compared to 
our system in (Ji and Grishman, 2004), it helps to 
distinguish the good and bad hypotheses without 
any loss of recall. The second-stage re-ranking us-
ing the relation participation feature yielded a 
small further gain in F score for each type, improv-
ing precision at a slight cost in recall. 
The overall system achieves a 24.1% relative re-
duction on the spurious and incorrect tags, and 
14.3% reduction in the missing rate over a state-of-
the-art baseline HMM trained on the same material. 
Furthermore, it helps to disambiguate many name 
type errors: the number of cases of type confusion 
in name classification was reduced from 191 to 
102. 
 
Name Precision Recall F 
PER 88.6 89.2 88.9 
GPE 88.1 84.9 86.5 
ORG 88.8 87.3 88.0 
ALL 88.4 86.7 87.5 
Table 3 Baseline Name Tagger 
 
Name Precision Recall F 
PER 89.4 90.1 89.7 
GPE 88.9 85.8 89.4 
ORG 88.7 87.4 88.0 
ALL 89.0 87.4 88.2 
Table 4 Baseline + Word Clustering by Relation 
 
Name Precision Recall F 
PER 90.1 91.2 90.5 
GPE 89.7 86.8 88.2 
ORG 90.6 89.8 90.2 
ALL 90.0 88.8 89.4 
Table 5 Baseline + Word Clustering by Relation + 
Re-ranking by Coreference 
 
Name Precision Recall F 
PER 90.7 91.0 90.8 
GPE 91.2 86.9 89.0 
ORG 91.7 89.1 90.4 
ALL 91.2 88.6 89.9 
Table 6 Baseline + Word Clustering by Relation +   
Re-ranking by Coreference +  
Re-ranking by Relation 
 
In order to check how robust these methods are, 
we conducted significance testing (sign test) on the 
346 documents. We split them into 5 folders, 70 
documents in each of the first four folders and 66 
in the fifth folder. We found that each enhance-
ment (word clusters, coreference reranking, rela-
tion reranking) produced an improvement in F 
score for each folder, allowing us to reject the hy-
pothesis that these improvements were random at a 
95% confidence level. The overall F-measure im-
provements (using all enhancements) for the 5 
folders were: 2.3%, 1.6%, 2.1%, 3.5%, and 2.1%. 
 
HMM Name Tagger, word 
clustering based on rela-
tions, pruned by margin 
Multiple name 
hypotheses 
Maxent Re-ranking
by coreference 
Single name
 hypothesis 
Post-processing  
by heuristic rules
Input 
Nominal 
Mention 
Tagger 
Nominal 
Mentions
Relation 
Tagger 
Maxent Re-ranking
by relation 
Coreference 
Resolver 
417
9 Conclusion 
This paper explored methods for exploiting the 
interaction of analysis components in an informa-
tion extraction system to reduce the error rate of 
individual components.  The ACE task hierarchy 
provided a good opportunity to explore these inter-
actions, including the one presented here between 
reference resolution/relation detection and name 
tagging. We demonstrated its effectiveness for 
Chinese name tagging, obtaining an absolute im-
provement of 2.4% in F-measure (a reduction of 
19% in the (1 ? F) error rate). These methods are 
quite low-cost because we don?t need any extra 
resources or components compared to the baseline 
information extraction system. 
Because no language-specific rules are involved 
and no additional training resources are required, 
we expect that the approach described here can be 
straightforwardly applied to other languages.  It 
should also be possible to extend this re-ranking 
framework to other levels of analysis in informa-
tion extraction ?- for example, to use event detec-
tion to improve name tagging; to incorporate 
subtype tagging results to improve name tagging; 
and to combine name tagging, reference resolution 
and relation detection to improve nominal mention 
tagging.  For Chinese (and other languages without 
overt word segmentation) it could also be extended 
to do character-based name tagging, keeping mul-
tiple segmentations among the N-Best hypotheses.  
Also, as information extraction is extended to cap-
ture cross-document information, we should expect 
further improvements in performance of the earlier 
stages of analysis, including in particular name 
identification. 
For some levels of analysis, such as name tag-
ging, it will be natural to apply lattice techniques to 
organize the multiple hypotheses, at some gain in 
efficiency. 
Acknowledgements 
This research was supported by the Defense Ad-
vanced Research Projects Agency under Grant 
N66001-04-1-8920 from SPAWAR San Diego, 
and by the National Science Foundation under 
Grant 03-25657. This paper does not necessarily 
reflect the position or the policy of the U.S. Gov-
ernment. 
References 
Daniel M. Bikel, Scott Miller, Richard Schwartz, and 
Ralph Weischedel. 1997. Nymble: a high-
performance Learning Name-finder.  Proc. Fifth 
Conf. on Applied Natural Language Processing, 
Washington, D.C. 
Andrew Borthwick. 1999. A Maximum Entropy Ap-
proach to Named Entity Recognition.  Ph.D. Disser-
tation, Dept. of Computer Science, New York 
University. 
Hai Leong Chieu and Hwee Tou Ng. 2002.  Named En-
tity Recognition: A Maximum Entropy Approach Us-
ing Global Information.  Proc.: 17th Int?l Conf. on 
Computational Linguistics (COLING 2002), Taipei, 
Taiwan. 
Yen-Lu Chow and Richard Schwartz. 1989. The N-Best 
Algorithm: An efficient Procedure for Finding Top N 
Sentence Hypotheses. Proc. DARPA Speech and 
Natural Language Workshop 
Michael Collins. 2002. Ranking Algorithms for Named-
Entity Extraction: Boosting and the Voted Percep-
tron. Proc. ACL 2002 
Heng Ji and Ralph Grishman. 2004. Applying Corefer-
ence to Improve Name Recognition. Proc. ACL 2004 
Workshop on Reference Resolution and Its Applica-
tions, Barcelona, Spain 
N. Kambhatla. 2004. Combining Lexical, Syntactic, and 
Semantic Features with Maximum Entropy Models 
for Extracting Relations. Proc. ACL 2004. 
Tobias Scheffer, Christian Decomain, and Stefan 
Wrobel. 2001. Active Hidden Markov Models for In-
formation Extraction. Proc. Int?l Symposium on In-
telligent Data Analysis (IDA-2001). 
Dmitry Zelenko, Chinatsu Aone, and Jason Tibbets. 
2004.  Binary Integer Programming for Information 
Extraction.  ACE Evaluation Meeting, September 
2004, Alexandria, VA. 
Lufeng Zhai, Pascale Fung, Richard Schwartz, Marine 
Carpuat, and Dekai Wu. 2004. Using N-best Lists for 
Named Entity Recognition from Chinese Speech. 
Proc. NAACL 2004 (Short Papers) 
418
Proceedings of the 43rd Annual Meeting of the ACL, pages 419?426,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
 
Extracting Relations with Integrated Information Using Kernel Methods 
 
 
                                        Shubin Zhao               Ralph Grishman 
Department of Computer Science 
New York University 
715 Broadway, 7th Floor, New York, NY 10003 
              shubinz@cs.nyu.edu     grishman@cs.nyu.edu 
 
 
 
 
Abstract 
Entity relation detection is a form of in-
formation extraction that finds predefined 
relations between pairs of entities in text. 
This paper describes a relation detection 
approach that combines clues from differ-
ent levels of syntactic processing using 
kernel methods. Information from three 
different levels of processing is consid-
ered: tokenization, sentence parsing and 
deep dependency analysis. Each source of 
information is represented by kernel func-
tions. Then composite kernels are devel-
oped to integrate and extend individual 
kernels so that processing errors occurring 
at one level can be overcome by informa-
tion from other levels. We present an 
evaluation of these methods on the 2004 
ACE relation detection task, using Sup-
port Vector Machines, and show that each 
level of syntactic processing contributes 
useful information for this task. When 
evaluated on the official test data, our ap-
proach produced very competitive ACE 
value scores. We also compare the SVM 
with KNN on different kernels.  
1 Introduction 
Information extraction subsumes a broad range of 
tasks, including the extraction of entities, relations 
and events from various text sources, such as 
newswire documents and broadcast transcripts. 
One such task, relation detection, finds instances 
of predefined relations between pairs of entities, 
such as a Located-In relation between the entities 
Centre College and Danville, KY in the phrase 
Centre College in Danville, KY. The ?entities? are 
the individuals of selected semantic types (such as 
people, organizations, countries, ?) which are re-
ferred to in the text. 
    Prior approaches to this task (Miller et al, 2000; 
Zelenko et al, 2003) have relied on partial or full 
syntactic analysis. Syntactic analysis can find rela-
tions not readily identified based on sequences of 
tokens alone. Even ?deeper? representations, such 
as logical syntactic relations or predicate-argument 
structure, can in principle capture additional gener-
alizations and thus lead to the identification of ad-
ditional instances of relations. However, a general 
problem in Natural Language Processing is that as 
the processing gets deeper, it becomes less accu-
rate. For instance, the current accuracy of tokeniza-
tion, chunking and sentence parsing for English is 
about 99%, 92%, and 90% respectively. Algo-
rithms based solely on deeper representations in-
evitably suffer from the errors in computing these 
representations. On the other hand, low level proc-
essing such as tokenization will be more accurate, 
and may also contain useful information missed by 
deep processing of text. Systems based on a single 
level of representation are forced to choose be-
tween shallower representations, which will have 
fewer errors, and deeper representations, which 
may be more general. 
    Based on these observations, Zhao et al (2004) 
proposed a discriminative model to combine in-
formation from different syntactic sources using a 
kernel SVM (Support Vector Machine). We 
showed that adding sentence level word trigrams 
as global information to local dependency context 
boosted the performance of finding slot fillers for 
419
 management succession events. This paper de-
scribes an extension of this approach to the identi-
fication of entity relations, in which syntactic 
information from sentence tokenization, parsing 
and deep dependency analysis is combined using 
kernel methods. At each level, kernel functions (or 
kernels) are developed to represent the syntactic 
information. Five kernels have been developed for 
this task, including two at the surface level, one at 
the parsing level and two at the deep dependency 
level. Our experiments show that each level of 
processing may contribute useful clues for this 
task, including surface information like word bi-
grams. Adding kernels one by one continuously 
improves performance. The experiments were car-
ried out on the ACE RDR (Relation Detection and 
Recognition) task with annotated entities. Using 
SVM as a classifier along with the full composite 
kernel produced the best performance on this task. 
This paper will also show a comparison of SVM 
and KNN (k-Nearest-Neighbors) under different 
kernel setups. 
2 Kernel Methods  
Many machine learning algorithms involve only 
the dot product of vectors in a feature space, in 
which each vector represents an object in the ob-
ject domain. Kernel methods (Muller et al, 2001) 
can be seen as a generalization of feature-based 
algorithms, in which the dot product is replaced by 
a kernel function (or kernel) ?(X,Y) between two 
vectors, or even between two objects. Mathemati-
cally, as long as ?(X,Y) is symmetric and the ker-
nel matrix formed by ? is positive semi-definite, it 
forms a valid dot product in an implicit Hilbert 
space. In this implicit space, a kernel can be bro-
ken down into features, although the dimension of 
the feature space could be infinite. 
   Normal feature-based learning can be imple-
mented in kernel functions, but we can do more 
than that with kernels. First, there are many well-
known kernels, such as polynomial and radial basis 
kernels, which extend normal features into a high 
order space with very little computational cost. 
This could make a linearly non-separable problem 
separable in the high order feature space. Second, 
kernel functions have many nice combination 
properties: for example, the sum or product of ex-
isting kernels is a valid kernel. This forms the basis 
for the approach described in this paper. With 
these combination properties, we can combine in-
dividual kernels representing information from 
different sources in a principled way.  
   Many classifiers can be used with kernels. The 
most popular ones are SVM, KNN, and voted per-
ceptrons. Support Vector Machines (Vapnik, 1998; 
Cristianini and Shawe-Taylor, 2000) are linear 
classifiers that produce a separating hyperplane 
with largest margin. This property gives it good 
generalization ability in high-dimensional spaces, 
making it a good classifier for our approach where 
using all the levels of linguistic clues could result 
in a huge number of features. Given all the levels 
of features incorporated in kernels and training 
data with target examples labeled, an SVM can 
pick up the features that best separate the targets 
from other examples, no matter which level these 
features are from. In cases where an error occurs in 
one processing result (especially deep processing) 
and the features related to it become noisy, SVM 
may pick up clues from other sources which are 
not so noisy. This forms the basic idea of our ap-
proach. Therefore under this scheme we can over-
come errors introduced by one processing level; 
more particularly, we expect accurate low level 
information to help with less accurate deep level 
information. 
3 Related Work  
Collins et al (1997) and Miller et al (2000) used 
statistical parsing models to extract relational facts 
from text, which avoided pipeline processing of 
data. However, their results are essentially based 
on the output of sentence parsing, which is a deep 
processing of text. So their approaches are vulner-
able to errors in parsing. Collins et al (1997) ad-
dressed a simplified task within a confined context 
in a target sentence.  
Zelenko et al (2003) described a recursive ker-
nel based on shallow parse trees to detect person-
affiliation and organization-location relations, in 
which a relation example is the least common sub-
tree containing two entity nodes. The kernel 
matches nodes starting from the roots of two sub-
trees and going recursively to the leaves. For each 
pair of nodes, a subsequence kernel on their child 
nodes is invoked, which matches either contiguous 
or non-contiguous subsequences of node. Com-
pared with full parsing, shallow parsing is more 
reliable. But this model is based solely on the out-
420
 put of shallow parsing so it is still vulnerable to 
irrecoverable parsing errors. In their experiments, 
incorrectly parsed sentences were eliminated.  
Culotta and Sorensen (2004) described a slightly 
generalized version of this kernel based on de-
pendency trees. Since their kernel is a recursive 
match from the root of a dependency tree down to 
the leaves where the entity nodes reside, a success-
ful match of two relation examples requires their 
entity nodes to be at the same depth of the tree. 
This is a strong constraint on the matching of syn-
tax so it is not surprising that the model has good 
precision but very low recall. In their solution a 
bag-of-words kernel was used to compensate for 
this problem. In our approach, more flexible ker-
nels are used to capture regularization in syntax, 
and more levels of syntactic information are con-
sidered. 
Kambhatla (2004) described a Maximum En-
tropy model using features from various syntactic 
sources, but the number of features they used is 
limited and the selection of features has to be a 
manual process.1 In our model, we use kernels to 
incorporate more syntactic information and let a 
Support Vector Machine decide which clue is cru-
cial. Some of the kernels are extended to generate 
high order features. We think a discriminative clas-
sifier trained with all the available syntactic fea-
tures should do better on the sparse data. 
4 Kernel Relation Detection 
4.1 ACE Relation Detection Task 
ACE (Automatic Content Extraction)2 is a research 
and development program in information extrac-
tion sponsored by the U.S. Government. The 2004 
evaluation defined seven major types of relations 
between seven types of entities. The entity types 
are PER (Person), ORG (Organization), FAC (Fa-
cility), GPE (Geo-Political Entity: countries, cities, 
etc.), LOC (Location), WEA (Weapon) and VEH 
(Vehicle). Each mention of an entity has a mention 
type: NAM (proper name), NOM (nominal) or 
                                                          
1 Kambhatla also evaluated his system on the ACE relation 
detection task, but the results are reported for the 2003 task, 
which used different relations and different training and test 
data, and did not use hand-annotated entities, so they cannot 
be readily compared to our results. 
2Task description: http://www.itl.nist.gov/iad/894.01/tests/ace/ 
  ACE guidelines: http://www.ldc.upenn.edu/Projects/ACE/ 
PRO (pronoun); for example George W. Bush, the 
president and he respectively. The seven relation 
types are EMP-ORG (Employ-
ment/Membership/Subsidiary), PHYS (Physical), 
PER-SOC (Personal/Social), GPE-AFF (GPE-
Affiliation), Other-AFF (Person/ORG Affiliation), 
ART (Agent-Artifact) and DISC (Discourse). 
There are also 27 relation subtypes defined by 
ACE, but this paper only focuses on detection of 
relation types. Table 1 lists examples of each rela-
tion type. 
 
Type Example 
EMP-ORG the CEO of Microsoft 
PHYS a military base in Germany 
GPE-AFF U.S.  businessman 
PER-SOC a spokesman for the senator 
DISC many of these people 
ART the makers of the Kursk 
Other-AFF Cuban-American  people 
 
Table 1. ACE relation types and examples. The 
heads of the two entity arguments in a relation are 
marked. Types are listed in decreasing order of 
frequency of occurrence in the ACE corpus. 
 
  Figure 1 shows a sample newswire sentence, in 
which three relations are marked. In this sentence, 
we expect to find a PHYS relation between Hez-
bollah forces and areas, a PHYS relation between 
Syrian troops and areas and an EMP-ORG relation 
between Syrian troops and Syrian. In our ap-
proach, input text is preprocessed by the Charniak 
sentence parser (including tokenization and POS 
tagging) and the GLARF (Meyers et al, 2001) de-
pendency analyzer produced by NYU. Based on 
treebank parsing, GLARF produces labeled deep 
dependencies between words (syntactic relations 
such as logical subject and logical object). It han-
dles linguistic phenomena like passives, relatives, 
reduced relatives, conjunctions, etc.  
 
Figure 1. Example sentence from newswire text  
4.2 Definitions 
In our model, kernels incorporate information from 
PHYS PHYS EMP-ORG
That's because Israel was expected to retaliate against 
Hezbollah forces in areas controlled by Syrian troops. 
421
 tokenization, parsing and deep dependency analy-
sis. A relation candidate R is defined as 
 R = (arg1, arg2, seq, link, path), 
where arg1 and arg2 are the two entity arguments 
which may be related; seq=(t1, t2, ?, tn) is a token 
vector that covers the arguments and intervening 
words; link=(t1, t2, ?, tm) is also a token vector, 
generated from seq and the parse tree; path is a 
dependency path connecting arg1 and arg2 in the 
dependency graph produced by GLARF. path can 
be empty if no such dependency path exists. The 
difference between link and seq is that link only 
retains the ?important? words in seq in terms of 
syntax. For example, all noun phrases occurring in 
seq are replaced by their heads. Words and con-
stituent types in a stop list, such as time expres-
sions, are also removed. 
  A token T is defined as a string triple, 
T = (word, pos, base), 
where word, pos and base are strings representing 
the word, part-of-speech and morphological base 
form of T. Entity is a token augmented with other 
attributes, 
             E = (tk, type, subtype, mtype), 
where tk is the token associated with E; type, sub-
type and mtype are strings representing the entity 
type, subtype and mention type of E. The subtype 
contains more specific information about an entity. 
For example, for a GPE entity, the subtype tells 
whether it is a country name, city name and so on. 
Mention type includes NAM, NOM and PRO. 
  It is worth pointing out that we always treat an 
entity as a single token: for a nominal, it refers to 
its head, such as boys in the two boys; for a proper 
name, all the words are connected into one token, 
such as Bashar_Assad. So in a relation example R 
whose seq is (t1, t2, ?, tn), it is always true that 
arg1=t1 and arg2=tn. For names, the base form of 
an entity is its ACE type (person, organization, 
etc.). To introduce dependencies, we define a de-
pendency token to be a token augmented with a 
vector of dependency arcs, 
           DT=(word, pos, base, dseq),     
where dseq = (arc1, ... , arcn ). A dependency arc is 
            ARC = (w, dw, label, e),  
where w is the current token; dw is a token con-
nected by a dependency to w; and label and e are 
the role label and direction of this dependency arc 
respectively. From now on we upgrade the type of 
tk in arg1 and arg2 to be dependency tokens. Fi-
nally, path is a vector of dependency arcs, 
     path = (arc1 , ... , arcl ),  
where l is the length of the path and arci (1?i?l) 
satisfies arc1.w=arg1.tk, arci+1.w=arci.dw and 
arcl.dw=arg2.tk. So path is a chain of dependencies 
connecting the two arguments in R. The arcs in it 
do not have to be in the same direction. 
 
 
 
Figure 2. Illustration of a relation example R. The 
link sequence is generated from seq by removing 
some unimportant words based on syntax. The de-
pendency links are generated by GLARF. 
 
  Figure 2 shows a relation example generated from 
the text ?? in areas controlled by Syrian troops?. 
In this relation example R, arg1 is ((?areas?, 
?NNS?, ?area?, dseq), ?LOC?, ?Region?, 
?NOM?), and arg1.dseq is ((OBJ, areas, in, 1), 
(OBJ, areas, controlled, 1)). arg2 is ((?troops?, 
?NNS?, ?troop?, dseq), ?ORG?, ?Government?, 
?NOM?) and arg2.dseq = ((A-POS, troops, Syrian, 
0), (SBJ, troops, controlled, 1)). path is ((OBJ, ar-
eas, controlled, 1), (SBJ, controlled, troops, 0)). 
The value 0 in a dependency arc indicates forward 
direction from w to dw, and 1 indicates backward 
direction. The seq and link sequences of R are 
shown in Figure 2. 
  Some relations occur only between very restricted 
types of entities, but this is not true for every type 
of relation. For example, PER-SOC is a relation 
mainly between two person entities, while PHYS 
can happen between any type of entity and a GPE 
or LOC entity. 
4.3 Syntactic Kernels 
In this section we will describe the kernels de-
signed for different syntactic sources and explain 
the intuition behind them. 
  We define two kernels to match relation examples 
at surface level. Using the notation just defined, we 
can write the two surface kernels as follows: 
1) Argument kernel 
troopsareas controlled by 
A-POS OBJ 
arg1 arg2 SBJ 
OBJ
path 
in
seq 
link 
areas controlled by Syrian troops
COMP 
422
  
 
where KE is a kernel that matches two entities, 
 
 
 
 
 
 
KT is a kernel that matches two tokens. I(x, y) is a 
binary string match operator that gives 1 if x=y 
and 0 otherwise. Kernel ?1 matches attributes of 
two entity arguments respectively, such as type, 
subtype and lexical head of an entity. This is based 
on the observation that there are type constraints 
on the two arguments. For instance PER-SOC is a 
relation mostly between two person entities. So the 
attributes of the entities are crucial clues. Lexical 
information is also important to distinguish relation 
types. For instance, in the phrase U.S. president 
there is an EMP-ORG relation between president 
and U.S., while in a U.S. businessman there is a 
GPE-AFF relation between businessman and U.S. 
2)  Bigram kernel 
 
 
where  
 
 
 
Operator <t1, t2> concatenates all the string ele-
ments in tokens t1 and t2 to produce a new token. 
So ?2 is a kernel that simply matches unigrams and 
bigrams between the seq sequences of two relation 
examples. The information this kernel provides is 
faithful to the text. 
3) Link sequence kernel 
 
 
 
 
where min_len is the length of the shorter link se-
quence in R1 and R2. ?3 is a kernel that matches 
token by token between the link sequences of two 
relation examples. Since relations often occur in a 
short context, we expect many of them have simi-
lar link sequences. 
4) Dependency path kernel 
 
 
where  
 
 
 
 
             ).',.()).',.( earcearcIdwarcdwarcK jijiT ?  
  Intuitively the dependency path connecting two 
arguments could provide a high level of syntactic 
regularization. However, a complete match of two 
dependency paths is rare. So this kernel matches 
the component arcs in two dependency paths in a 
pairwise fashion. Two arcs can match only when 
they are in the same direction. In cases where two 
paths do not match exactly, this kernel can still tell 
us how similar they are. In our experiments we 
placed an upper bound on the length of depend-
ency paths for which we computed a non-zero ker-
nel. 
5) Local dependency 
 
 
where 
 
 
 
 
         ).',.()).',.( earcearcIdwarcdwarcK jijiT ?  
  This kernel matches the local dependency context 
around the relation arguments. This can be helpful 
especially when the dependency path between ar-
guments does not exist. We also hope the depend-
encies on each argument may provide some useful 
clues about the entity or connection of the entity to 
the context outside of the relation example.  
4.4 Composite Kernels 
Having defined all the kernels representing shallow 
and deep processing results, we can define com-
posite kernels to combine and extend the individ-
ual kernels.  
1) Polynomial extension  
 
 
  This kernel combines the argument kernel ?1 and 
link kernel ?3 and applies a second-degree poly-
nomial kernel to extend them. The combination of 
?1 and ?3 covers the most important clues for this 
task: information about the two arguments and the 
word link between them. The polynomial exten-
sion is equivalent to adding pairs of features as 
),arg.,arg.(),( 21
2,1
211 ii
i
E RRKRR ?
=
=?
++= ).,.().,.(),( 212121 typeEtypeEItkEtkEKEEK TE
).,.().,.( 2121 mtypeEmtypeEIsubtypeEsubtypeEI +
+= ).,.(),( 2121 wordTwordTITTKT
).,.().,.( 2121 baseTbaseTIposTposTI +
),.,.(),( 21212 seqRseqRKRR seq=?
? ?
<? <?
+=
lenseqi lenseqj
jiTseq tktkKseqseqK
.0 .'0
)',(('),(
))',',,( 11 ><>< ++ jjiiT tktktktkK
).,.(),( 21213 linkRlinkRKRR link=?
,)..,..( 21
min_0
ii
leni
T ktlinkRktlinkRK?
<?
=
),.,.(),( 21214 pathRpathRKRR path=?
)',( pathpathK path
? ?
<? <?
+=
lenpathi lenpathj
ji labelarclabelarcI
.0 .'0
).',.(((
,).arg.,.arg.(),(
2,1
21215 ?
=
=
i
iiD dseqRdseqRKRR?
)',( dseqdseqK D
? ?
<? <?
+=
lendseqi lendseqj
ji labelarclabelarcI
.0 .'0
).',.((
4/)()(),( 23131211 ???? +++=? RR
423
 new features. Intuitively this introduces new fea-
tures like: the subtype of the first argument is a 
country name and the word of the second argument 
is president, which could be a good clue for an 
EMP-ORG relation. The polynomial kernel is 
down weighted by a normalization factor because 
we do not want the high order features to over-
whelm the original ones. In our experiment, using 
polynomial kernels with degree higher than 2 does 
not produce better results. 
2) Full kernel 
 
 
This is the final kernel we used for this task, which 
is a combination of all the previous kernels. In our 
experiments, we set al the scalar factors to 1. Dif-
ferent values were tried, but keeping the original 
weight for each kernel yielded the best results for 
this task. 
  All the individual kernels we designed are ex-
plicit. Each kernel can be seen as a matching of 
features and these features are enumerable on the 
given data. So it is clear that they are all valid ker-
nels. Since the kernel function set is closed under 
linear combination and polynomial extension, the 
composite kernels are also valid. The reason we 
propose to use a feature-based kernel is that we can 
have a clear idea of what syntactic clues it repre-
sents and what kind of information it misses. This 
is important when developing or refining kernels, 
so that we can make them generate complementary 
information from different syntactic processing 
results. 
5 Experiments  
Experiments were carried out on the ACE RDR 
(Relation Detection and Recognition) task using 
hand-annotated entities, provided as part of the 
ACE evaluation. The ACE corpora contain docu-
ments from two sources: newswire (nwire) docu-
ments and broadcast news transcripts (bnews). In 
this section we will compare performance of dif-
ferent kernel setups trained with SVM, as well as 
different classifiers, KNN and SVM, with the same 
kernel setup. The SVM package we used is 
SVMlight. The training parameters were chosen us-
ing cross-validation. One-against-all classification 
was applied to each pair of entities in a sentence. 
When SVM predictions conflict on a relation ex-
ample, the one with larger margin will be selected 
as the final answer. 
5.1 Corpus 
The ACE RDR training data contains 348 docu-
ments, 125K words and 4400 relations. It consists 
of both nwire and bnews documents. Evaluation of 
kernels was done on the training data using 5-fold 
cross-validation. We also evaluated the full kernel 
setup with SVM on the official test data, which is 
about half the size of the training data. All the data 
is preprocessed by the Charniak parser and 
GLARF dependency analyzer. Then relation ex-
amples are generated based these results. 
5.2 Results 
  Table 2 shows the performance of the SVM on 
different kernel setups. The kernel setups in this 
experiment are incremental. From this table we can 
see that adding kernels continuously improves the 
performance, which indicates they provide 
additional clues to the previous setup. The argu-
ment kernel treats the two arguments as 
independent entities. The link sequence kernel 
introduces the syntactic connection between 
arguments, so adding it to the argument kernel 
boosted the performance. Setup F shows the 
performance of adding only dependency kernels to 
the argument kernel. The performance is not as 
good as setup B, indicating that dependency 
information alone is not as crucial as the link 
sequence.  
 
 Kernel           Performance   prec       recall    F-score 
A Argument (?1) 52.96%    58.47%   55.58% 
B A + link (?1+?3) 58.77%    71.25%   64.41%* 
C B-poly (?1) 66.98%    70.33%   68.61%* 
D C + dep (?1+?4+?5) 69.10%    71.41%   70.23%* 
E D + bigram (?2) 69.23%    70.50%   70.35% 
F A + dep (?1+?4+?5) 57.86%    68.50%   62.73% 
 
Table 2. SVM performance on incremental kernel 
setups. Each setup adds one level of kernels to the 
previous one except setup F. Evaluated on the 
ACE training data with 5-fold cross-validation. F-
scores marked by * are significantly better than the 
previous setup (at 95% confidence level). 
 
2541212 ),( ?????? +++?=? RR
424
   Another observation is that adding the bigram 
kernel in the presence of all other level of kernels 
improved both precision and recall, indicating that 
it helped in both correcting errors in other 
processing results and providing supplementary 
information missed by other levels of analysis. In 
another experiment evaluated on the nwire data 
only (about half of the training data), adding the 
bigram kernel improved F-score 0.5% and this 
improvement is statistically significant.  
   
Type KNN (?1+?3) KNN (?2) SVM (?2) 
EMP-ORG 75.43% 72.66% 77.76% 
PHYS 62.19 % 61.97% 66.37% 
GPE-AFF 58.67% 56.22% 62.13% 
PER-SOC 65.11% 65.61% 73.46% 
DISC 68.20% 62.91% 66.24% 
ART 69.59% 68.65% 67.68% 
Other-AFF 51.05% 55.20% 46.55% 
Total 67.44% 65.69% 70.35% 
 
Table 3. Performance of SVM and KNN (k=3) on 
different kernel setups. Types are ordered in de-
creasing order of frequency of occurrence in the 
ACE corpus. In SVM training, the same 
parameters were used for all 7 types.  
 
  Table 3 shows the performance of SVM and 
KNN (k Nearest Neighbors) on different kernel 
setups. For KNN, k was set to 3. In the first setup 
of KNN, the two kernels which seem to contain 
most of the important information are used. It 
performs quite well when compared with the SVM 
result. The other two tests are based on the full 
kernel setup. For the two KNN experiments, 
adding more kernels (features) does not help. The 
reason might be that all kernels (features) were 
weighted equally in the composite kernel ?2 and 
this may not be optimal for KNN. Another reason 
is that the polynomial extension of kernels does not 
have any benefit in KNN because it is a monotonic 
transformation of similarity values. So the results 
of KNN on kernel (?1+?3) and ?1 would be ex-
actly the same. We also tried different k for KNN 
and k=3 seems to be the best choice in either case.  
  For the four major types of relations SVM does 
better than KNN, probably due to SVM?s 
generalization ability in the presence of large 
numbers of features. For the last three types with 
many fewer examples, performance of SVM is not 
as good as KNN. The reason we think is that 
training of SVM on these types is not sufficient. 
We tried different training parameters for the types 
with fewer examples, but no dramatic 
improvement obtained. 
  We also evaluated our approach on the official 
ACE RDR test data and obtained very competitive 
scores.3 The primary scoring metric4 for the ACE 
evaluation is a 'value' score, which is computed by 
deducting from 100 a penalty for each missing and 
spurious relation; the penalty depends on the types 
of the arguments to the relation. The value scores 
produced by the ACE scorer for nwire and bnews 
test data are 71.7 and 68.0 repectively. The value 
score on all data is 70.1.5 The scorer also reports an 
F-score based on full or partial match of relations 
to the keys. The unweighted F-score for this test 
produced by the ACE scorer on all data is 76.0%. 
For this evaluation we used nearest neighbor to 
determine argument ordering and relation 
subtypes. 
  The classification scheme in our experiments is 
one-against-all. It turned out there is not so much 
confusion between relation types. The confusion 
matrix of predictions is fairly clean. We also tried 
pairwise classification, and it did not help much. 
6 Discussion 
In this paper, we have shown that using kernels to 
combine information from different syntactic 
sources performed well on the entity relation 
detection task. Our experiments show that each 
level of syntactic processing contains useful 
information for the task. Combining them may 
provide complementary information to overcome 
errors arising from linguistic analysis. Especially, 
low level information obtained with high reliability 
helped with the other deep processing results. This 
design feature of our approach should be best 
employed when the preprocessing errors at each 
level are independent, namely when there is no 
dependency between the preprocessing modules. 
The model was tested on text with annotated 
entities, but its design is generic. It can work with 
                                                          
3 As ACE participants, we are bound by the participation 
agreement not to disclose other sites? scores, so no direct 
comparison can be provided. 
4 http://www.nist.gov/speech/tests/ace/ace04/software.htm 
5 No comparable inter-annotator agreement scores are avail-
able for this task, with pre-defined entities.  However, the 
agreement scores across multiple sites for similar relation 
tagging tasks done in early 2005, using the value metric, 
ranged from about 0.70 to 0.80. 
425
 noisy entity detection input from an automatic 
tagger. With all the existing information from other 
processing levels, this model can be also expected 
to recover from errors in entity tagging. 
7 Further Work 
Kernel functions have many nice properties. There 
are also many well known kernels, such as radial 
basis kernels, which have proven successful in 
other areas. In the work described here, only linear 
combinations and polynomial extensions of kernels 
have been evaluated. We can explore other kernel 
properties to integrate the existing syntactic 
kernels. In another direction, training data is often 
sparse for IE tasks. String matching is not 
sufficient to capture semantic similarity of words. 
One solution is to use general purpose corpora to 
create clusters of similar words; another option is 
to use available resources like WordNet. These 
word similarities can be readily incorporated into 
the kernel framework.  To deal with sparse data, 
we can also use deeper text analysis to capture 
more regularities from the data. Such analysis may 
be based on newly-annotated corpora like 
PropBank (Kingsbury and Palmer, 2002) at the 
University of Pennsylvania and NomBank (Meyers 
et al, 2004) at New York University. Analyzers 
based on these resources can generate regularized 
semantic representations for lexically or 
syntactically related sentence structures. Although 
deeper analysis may even be less accurate, our 
framework is designed to handle this and still 
obtain some improvement in performance. 
8 Acknowledgement 
This research was supported in part by the Defense 
Advanced Research Projects Agency under Grant 
N66001-04-1-8920 from SPAWAR San Diego, 
and by the National Science Foundation under 
Grant ITS-0325657. This paper does not necessar-
ily reflect the position of the U.S. Government. We 
wish to thank Adam Meyers of the NYU NLP 
group for his help in producing deep dependency 
analyses. 
References  
M. Collins and S. Miller. 1997. Semantic tagging using 
a probabilistic context free grammar. In Proceedings 
of the 6th Workshop on Very Large Corpora. 
N. Cristianini and J. Shawe-Taylor. 2000. An introduc-
tion to support vector machines. Cambridge Univer-
sity Press. 
A. Culotta and J. Sorensen. 2004. Dependency Tree 
Kernels for Relation Extraction. In Proceedings of 
the 42nd Annual Meeting of the Association for 
Computational Linguistics. 
D. Gildea and M. Palmer. 2002. The Necessity of Pars-
ing for Predicate Argument Recognition. In Proceed-
ings of the 40th Annual Meeting of the Association 
for Computational Linguistics. 
N. Kambhatla. 2004. Combining Lexical, Syntactic, and 
Semantic Features with Maximum Entropy Models 
for Extracting Relations. In Proceedings of the 42nd 
Annual Meeting of the Association for Computa-
tional Linguistics. 
P. Kingsbury and M. Palmer. 2002. From treebank to 
propbank. In Proceedings of the 3rd International 
Conference on Language Resources and Evaluation 
(LREC-2002). 
C. D. Manning and H. Schutze 2002. Foundations of 
Statistical Natural Language Processing. The MIT 
Press, page 454-455. 
A. Meyers, R. Grishman, M. Kosaka and S. Zhao. 2001. 
Covering Treebanks with GLARF. In Proceedings of 
the 39th Annual Meeting of the Association for 
Computational Linguistics. 
A. Meyers, R. Reeves, Catherine Macleod, Rachel 
Szekeley, Veronkia Zielinska, Brian Young, and R. 
Grishman. 2004. The Cross-Breeding of Dictionar-
ies. In Proceedings of the 5th International Confer-
ence on Language Resources and Evaluation (LREC-
2004).  
S. Miller, H. Fox, L. Ramshaw, and R. Weischedel. 
2000. A novel use of statistical parsing to extract in-
formation from text. In 6th Applied Natural Lan-
guage Processing Conference. 
K.-R. M?ller, S. Mika, G. Ratsch, K. Tsuda and B. 
Scholkopf. 2001. An introduction to kernel-based 
learning algorithms, IEEE Trans. Neural Networks, 
12, 2, pages 181-201. 
V. N. Vapnik. 1998. Statistical Learning Theory. Wiley-
Interscience Publication. 
D. Zelenko, C. Aone and A. Richardella. 2003. Kernel 
methods for relation extraction. Journal of Machine 
Learning Research. 
Shubin Zhao, Adam Meyers, Ralph Grishman. 2004. 
Discriminative Slot Detection Using Kernel Methods. 
In the Proceedings of the 20th International Confer-
ence on Computational Linguistics. 
426
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 420?427,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Analysis and Repair of Name Tagger Errors 
 
 
Heng Ji Ralph Grishman 
Department of Computer Science 
New York University 
New York, NY, 10003, USA 
hengji@cs.nyu.edu grishman@cs.nyu.edu 
 
  
Abstract 
Name tagging is a critical early stage in 
many natural language processing pipe-
lines. In this paper we analyze the types 
of errors produced by a tagger, distin-
guishing name classification and various 
types of name identification errors.  We 
present a joint inference model to im-
prove Chinese name tagging by incorpo-
rating feedback from subsequent stages in 
an information extraction pipeline: name 
structure parsing, cross-document 
coreference, semantic relation extraction 
and event extraction. We show through 
examples and performance measurement 
how different stages can correct different 
types of errors.  The resulting accuracy 
approaches that of individual human an-
notators.   
1 Introduction 
High-performance named entity (NE) tagging is 
crucial in many natural language processing tasks, 
such as information extraction and machine 
translation. In 'traditional' pipelined system archi-
tectures, NE tagging is one of the first steps in 
the pipeline. NE errors adversely affect subse-
quent stages, and error rates are often com-
pounded by later stages. 
However, (Roth and Yi 2002, 2004) and our 
recent work have focused on incorporating richer 
linguistic analysis, using the ?feedback? from 
later stages to improve name taggers. We ex-
panded our last year?s model (Ji and Grishman, 
2005) that used the results of coreference analy-
sis and relation extraction, by adding ?feedback? 
from more information extraction components ? 
name structure parsing, cross-document corefer-
ence, and event extraction ? to incrementally re-
rank the multiple hypotheses from a baseline 
name tagger.  
While together these components produced a 
further improvement on last year?s model, our 
goal in this paper is to look behind the overall 
performance figures in order to understand how 
these varied components contribute to the im-
provement, and compare the remaining system 
errors with the human annotator?s performance. 
To this end, we shall decompose the task of name 
tagging into two subtasks 
? Name Identification ? The process of iden-
tifying name boundaries in the sentence. 
? Name Classification ? Given the correct 
name boundaries, assigning the appropri-
ate name types to them. 
and observe the effects that different components 
have on errors of each type.  Errors of identifica-
tion will be further subdivided by type (missing 
names, spurious names, and boundary errors).  
We believe such detailed understanding of the 
benefits of joint inference is a prerequisite for 
further improvements in name tagging perform-
ance. 
After summarizing some prior work in this 
area, describing our baseline NE tagger, and ana-
lyzing its errors, we shall illustrate, through a 
series of examples, the potential for feedback to 
improve NE performance. We then present some 
details on how this improvement can be achieved 
through hypothesis reranking in the extraction 
pipeline, and analyze the results in terms of dif-
ferent types of identification and classification 
errors. 
2 Prior Work 
Some recent work has incorporated global infor-
mation to improve the performance of name tag- 
gers.  
For mixed case English data, name identifica-
tion is relatively easy. Thus some researchers 
have focused on the more challenging task ? 
classifying names into correct types. In (Roth and 
420
Yi 2002, 2004), given name boundaries in the 
text, separate classifiers are first trained for name 
classification and semantic relation detection. 
Then, the output of the classifiers is used as a 
conditional distribution given the observed data. 
This information, along with the constraints 
among the relations and entities (specific rela-
tions require specific classes of names), is used to 
make global inferences by linear programming 
for the most probable assignment. They obtained 
significant improvements in both name classifi-
cation and relation detection. 
In (Ji and Grishman 2005) we generated N-
best NE hypotheses and re-ranked them after 
coreference and semantic relation identification; 
we obtained a significant improvement in Chi-
nese name tagging performance. In this paper we 
shall use a wider range of linguistic knowledge 
sources, and integrate cross-document techniques. 
3 Baseline Name Tagger 
We apply a multi-lingual (English / Chinese) 
bigram HMM tagger to identify four named 
entity types: Person, Organization, GPE (?geo-
political entities? ? locations which are also 
political units, such as countries, counties, and 
cities) and Location. The HMM tagger generally 
follows the Nymble model (Bikel et al 1997), 
and uses best-first search to generate N-Best 
hypotheses for each input sentence. 
In mixed-case English texts, most proper 
names are capitalized. So capitalization provides 
a crucial clue for name boundaries.  
In contrast, a Chinese sentence is composed of 
a string of characters without any word bounda-
ries or capitalization. Even after word segmenta-
tion there are still no obvious clues for the name 
boundaries. However, we can apply the following 
coarse ?usable-character? restrictions to reduce 
the search space. 
Standard Chinese family names are generally 
single characters drawn from a set of 437 family 
names (there are also 9 two-character family 
names, although they are quite infrequent) and 
given names can be one or two characters (Gao et 
al., 2005). Transliterated Chinese person names 
usually consist of characters in three relatively 
fixed character lists (Begin character list, Middle 
character list and End character list). Person ab-
breviation names and names including title words 
match a few patterns. The suffix words (if there 
are any) of Organization and GPE names belong 
to relatively fixed lists too. 
However, this ?usable-character? restriction is 
not as reliable as the capitalization information 
for English, since each of these special characters 
can also be part of common words. 
3.1 Identification and Classification Errors 
We begin our error analysis with an investigation 
of the English and Chinese baseline taggers, de-
composing the errors into identification and clas-
sification errors. In Figure 1 we report the 
identification F-Measure for the baseline (the 
first hypothesis), and the N-best upper bound, the 
best of the N hypotheses1, using different models: 
English MonoCase (EN-Mono, without capitali-
zation), English Mixed Case (EN-Mix, with capi-
talization), Chinese without the usable character 
restriction (CH-NoRes) and Chinese with the 
usable character restriction (CH-WithRes). 
 
Figure 1. Baseline and Upper Bound of 
Name Identification 
 
Figure 1 shows that capitalization is a crucial 
clue in English name identification (increasing 
the F measure by 7.6% over the monocase score). 
We can also see that the best of the top N (N <= 
30) hypotheses is very good, so reranking a small 
number of hypotheses has the potential of pro-
ducing a very good tagger. 
The ?usable? character restriction plays a ma-
jor role in Chinese name identification, increas-
ing the F-measure 4%.  With this restriction, the 
performance of the best-of-N-best is again very 
good. However, it is evident that, even with this 
restriction, identification is more challenging for 
Chinese, due to the absence of capitalization and 
word boundaries. 
Figure 2 shows the classification accuracy of 
the above four models. We can see that capitali-
zation does not help English name classification; 
                                                          
1 These figures were obtained using training and test corpora 
described later in this paper, and a value of N ranging from 
1 to 30 depending on the margin of the HMM tagger, as also 
described below.  All figures are with respect to the official 
ACE keys prepared by the Linguistic Data Consortium. 
421
and the difficulty of classification is similar for 
the two languages. 
 
Figure 2. Baseline and Upper Bound of 
Name Classification 
3.2 Identification Errors in Chinese 
For the remainder of this paper we shall focus on 
the more difficult problems of Chinese tagging, 
using the HMM system with character restric-
tions as our baseline.  The name identification 
errors of this system can be divided into missed 
names (21%), spurious names (29%), and bound-
ary errors, where there is a partial overlap be-
tween the names in the key and the system 
response (50%).  Confusion between names and 
nominals (phrases headed by a common noun) is 
a major source of both missed and spurious 
names (56% of missed, 24% of spurious).  In a 
language without capitalization, this is a hard 
task even for people; one must rely largely on 
world knowledge to decide whether a phrase 
(such as the "criminal-processing team") is an 
organization name or merely a description of an 
organization.  The other major source of missed 
names is words not seen in the training data, gen-
erally representing minor cities or other locations 
in China (28%).  For spurious names, the largest 
source of error is names of a type not included in 
the key (44%) which are mistakenly tagged as 
one of the known name types.2  As we shall see, 
different types of knowledge are required for cor-
recting different types of errors. 
4 Mutual Inferences between Informa-
tion Extraction Stages  
4.1 Extraction Pipeline 
Name tagging is typically one of the first stages 
                                                          
2 If the key included an 'other' class of names, these would 
be classification errors; since it does not -- since these names 
are not tagged in the key -- the automatic scorer treats them 
as spurious names. 
in an information extraction pipeline. Specifically, 
we will consider a system which was developed 
for the ACE (Automatic Content Extraction) 
task 3  and includes the following stages: name 
structure parsing, coreference, semantic relation 
extraction and event extraction (Ji et al, 2006). 
All these stages are performed after name tag-
ging since they take names as input ?objects?. 
However, the inferences from these subsequent 
stages can also provide valuable constraints to 
identify and classify names.  
Each of these stages connects the name candi-
date to other linguistic elements in the sentence, 
document, or corpus, as shown in Figure 3.   
 
                                                       Sentence    Document 
                                                             Boundary  Boundary 
 
 
 
 
 
 
Name        Local    Related   Event              Coreferring  
Candidate Context Mention  trigger&arg     Mentions 
 
                  Linguistic Elements Supporting Inference 
 
Figure 3. Name candidate and its global context 
 
The baseline name tagger (HMM) uses very 
local information; feedback from later extraction 
stages allows us to draw from a wider context in 
making final name tagging decisions. 
In the following we use two related (translated) 
texts as examples, to give some intuition of how 
these different types of linguistic evidence im-
prove name tagging.4 
 
Document 1: Yugoslav election 
 
[?] More than 300,000 people rushed the <bei 
er ge le>0 congress building, forcing <yugo-
slav>1 president <mi lo se vi c>2 to admit 
frankly that in the Sept. 24 election he was 
beaten by his opponent <ke shi tu ni cha>3. 
    <mi lo se vi c>4 was forced to flee <bei er ge 
le>5; the winning opposition party's <sai er wei 
ya>6 <anti-democracy committee>7 on the 
morning of the 6th formed a <crisis-handling 
                                                          
3 The ACE task description can be found at 
http://www.itl.nist.gov/iad/894.01/tests/ace/  and the ACE 
guidelines at http://www.ldc.upenn.edu/Projects/ACE/ 
4 Rather than offer the most fluent translation, we have pro-
vided one that more closely corresponds to the Chinese text 
in order to more clearly illustrate the linguistic issues.  
Transliterated names are rendered phonetically, character by 
character. 
supporting  inference 
information 
422
committee>8, to deal with transfer-of-power is-
sues. 
        This crisis committee includes police, supply,  
economics and other important departments. 
In such a crisis, people cannot think through 
this question: has the <yugoslav>9 president <mi 
lo se vi c>10 used up his skills? 
        According to the official voting results in the 
first round of elections, <mi lo se vi c>11 was 
beaten by <18 party opposition committee>12 
candidate <ke shi tu ni cha>13. [?] 
 
Document 2: Biography of these two leaders 
 
[?]<ke shi tu ni cha>14 used to pursue an aca-
demic career, until 1974, when due to his opposi-
tion position he was fired by <bei er ge le>15 
<law school>16 and left the academic community. 
    <ke shi tu ni cha>17 also at the beginning of the 
1990s joined the opposition activity, and in 1992 
founded <sai er wei ya>18 <opposition party>19. 
This famous new leader and his previous 
classmate at law school, namely his wife <zuo li 
ka>20 live in an apartment in <bei er ge le>21. 
The vanished <mi lo se vi c>22 was born in 
<sai er wei ya>23 ?s central industrial city. [?] 
 
4.1 Inferences for Correcting Name Errors 
4.2.1 Internal Name Structure 
Constraints and preferences on the structure of 
individual names can capture local information 
missed by the baseline name tagger. They can 
correct several types of identification errors, in-
cluding in particular boundary errors.  For exam-
ple, ?<ke shi tu ni cha>3? is more likely to be 
correct than ?<shi tu ni cha>3? since ?shi? (?) 
cannot be the first character of a transliterated 
name. 
Name structures help to classify names too. 
For example, ?anti-democracy committee7? is 
parsed as ?[Org-Modifier anti-democracy] [Org-
Suffix committee]?, and the first character is not 
a person last name or the first character of a 
transliterated person name, so it is more likely to 
be an organization than a person name.  
4.2.2 Patterns 
Information about expected sequences of con-
stituents surrounding a name can be used to cor-
rect name boundary errors.  In particular, event 
extraction is performed by matching patterns in-
volving a "trigger word" (typically, the main verb 
or nominalization representing the event) and a 
set of arguments.  When a name candidate is in-
volved in an event, the trigger word and other 
arguments of the event can help to determine the 
name boundaries.  For example, in the sentence 
?The vanished mi lo se vi c was born in sai er wei 
ya ?s central industrial city?, ?mi lo se vi c? is 
more likely to be a name than ?mi lo se?, ?sai er 
wei ya? is more likely be a name than ?er wei?, 
because these boundaries will allow us to match 
the event pattern ?[Adj] [PER-NAME] [Trigger 
word for 'born' event] in [GPE-NAME]?s [GPE-
Nominal]?. 
4.2.3 Selection 
Any context which can provide selectional con-
straints or preferences for a name can be used to 
correct name classification errors.  Both semantic 
relations and events carry selectional constraints 
and so can be used in this way. 
For instance, if the ?Personal-Social/Business? 
relation (?opponent?) between ?his? and ?<ke shi 
tu ni cha>3? is correctly identified, it can help to 
classify ?<ke shi tu ni cha>3? as a person name. 
Relation information is sometimes crucial to 
classifying names. ?<mi lo se vi c>10? and ?<ke 
shi tu ni cha>13? are likely person names because 
they are ?employees? of ?<yugoslav>9? and 
?<18 party opponent committee>12?. Also the 
?Personal-Social/Family? relation (?wife?) be-
tween ?his? and ?<zuo li ka>20? helps to classify 
<zuo li ka>20 as a person name.   
Events, like relations, can provide effective se-
lectional preferences to correctly classify names. 
For example, ?<mi lo se vi c>2,4,10,11,22? are likely 
person names because they are involved in the 
following events: ?claim?, ?escape?, ?built?, 
?beat?, ?born?, while ?<sai er wei ya>23?can be 
easily tagged as GPE because it?s a ?birth-place? 
in the event ?born?.  
4.2.4 Coreference 
Names which are introduced in an article are 
likely to be referred to again, either by repeating 
the same name or describing it with nominal 
mentions (phrases headed by common nouns).  
These mentions will have the same spelling 
(though if a name has several parts, some may be 
dropped) and same semantic type.  So if the 
boundary or type of one mention can be deter-
mined with some confidence, coreference can be 
used to disambiguate other mentions.  
For example, if ?< mi lo se vi c>2? is con-
firmed as a name, then ?< mi lo se vi c>10? is 
more likely to be a name than ?< mi lo se>10?, by 
423
refering to ?< mi lo se vi c>2?. Also ?This crisis 
committee? supports the analysis of ?<crisis-
handling committee>8? as an organization name 
in preference to the alternative name candidate 
?<crisis-handling>8?. 
For a name candidate, high-confidence infor-
mation about the type of one mention can be used 
to determine the type of other mentions. For ex-
ample, for the repeated person name ?< mi lo se 
vi c>2,4,10,11,22? type information based on the 
event context of one mention can be used to clas-
sify or confirm the type of the others. The person 
nominal ?This famous new leader? confirms 
?<ke shi tu ni cha>17? as a person name.  
5 Incremental Re-Ranking Algorithm 
5.1 Overall Architecture 
In this section we will present the algorithms to 
capture the intuitions described in Section 4. The 
overall system pipeline is presented in Figure 4.  
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 4.  System Architecture 
 
 
 
The baseline name tagger generates N-Best 
multiple hypotheses for each sentence, and also 
computes the margin ? the difference between 
the log probabilities of the top two hypotheses.  
This is used as a rough measure of confidence in 
the top hypothesis. A large margin indicates 
greater confidence that the first hypothesis is cor-
rect.5 It generates name structure parsing results 
too, such as the family name and given name of 
person, the prefixes of the abbreviation names, 
the modifiers and suffixes of organization names. 
Then the results from subsequent components 
are exploited in four incremental re-rankers. 
From each re-ranking step we output the best 
name hypothesis directly if the re-ranker has high 
confidence in its decisions. Otherwise the sen-
tence is forwarded to the next re-ranker, based on 
other features. In this way we can adjust the rank-
ing of multiple hypotheses and select the best 
tagging for each sentence gradually. 
The nominal mention tagger (noun phrase 
chunker) uses a maximum entropy model. Entity 
type assignment for the nominal heads is done by 
table look-up. The coreference resolver is a com-
bination of high-precision heuristic rules and 
maximum entropy models. In order to incorpo-
rate wider context we use cross-document 
coreference for the test set. We cluster the docu-
ments using a cross-entropy metric and then treat 
the entire cluster as a single document. 
The relation tagger uses a K-nearest-neighbor 
algorithm. 
   We extract event patterns from the ACE05 
training corpus for personnel, contact, life, busi-
ness, and conflict events. We also collect addi-
tional event trigger words that appear frequently 
in name contexts, from a syntactic dictionary, a 
synonym dictionary and Chinese PropBank V1.0. 
Then the patterns are generalized and tested 
semi-automatically. 
5.2 Supervised Re-Ranking Model 
In our name re-ranking model, each hypothesis is 
an NE tagging of the entire sentence, for example, 
?The vanished <PER>mi lo se vi c</PER> was 
born in <GPE>sai er wei ya</GPE>?s central 
industrial city?; and each pair of hypotheses (hi, 
hj) is called a ?sample?.  
 
                                                          
5 The margin also determines the number of hypotheses (N) 
generated by the baseline tagger.  Using cross-validation on 
the training data, we determine the value of N required to 
include the best hypothesis, as a function of the margin.  We 
then divide the margin into ranges of values, and set a value 
of N for each range, with a maximum of 30. 
High-
Confidence 
Ranking 
Best Name 
Hypothesis 
Event based 
Re-Ranking 
Cross-document 
Coreference based 
Re-Ranking 
Coref  
Resolver
Event 
Patterns
Raw Sentence 
HMM Name 
Tagger and Name 
Structure Parser 
Multiple name 
hypotheses 
Name Structure 
based Re-Ranking 
Relation
Tagger
Mentions
Relation based 
Re-Ranking 
Nominal 
Tagger
424
Re-Ranker Property for comparing names Nik and Njk 
HMMMargin scaled margin value from HMM 
Idiomik -1 if Nik is part of an idiom; otherwise 0 
PERContextik the number of PER context words if Nik and Njk  are both PER; otherwise 0 
ORGSuffixik 1 if Nik is tagged as ORG and it includes a suffix word; otherwise 0 
PERCharac-
terik 
-1 if Nik is tagged as PER without family name, and it does not consist entirely of 
transliterated person name characters; otherwise 0 
Titlestructureik -1 if Nik = title word + family name while Njk = title word + family name + given 
name; otherwise 0 
Digitik -1 if Nik is  PER or GPE and it includes digits or punctuation; otherwise 0 
AbbPERik -1 if Nik = little/old + family name + given name while Njk = little/old + family 
name; otherwise 0 
SegmentPERik -1 if Nik is GPE (PER)* GPE , while Njk is PER*; otherwise 0 
Votingik the voting rate among all the candidate hypotheses6 
 
 
 
 
Name  
Structure 
Based 
Famous-
Nameik 
1 if Nik is tagged as the same type in one of the famous name lists7; otherwise 0 
Probability1i scaled ranking probability for (hi, hj) from name structure based re-ranker 
Relation 
Constraintik 
If Nik is in relation R (Nik = EntityType1, M2 = EntityType2), compute 
Prob(EntityType1|EntityType2, R) from training data and scale it; otherwise 0 
 
Relation 
Based 
 Conjunction of 
InRelation i & 
Probability1i 
Inrelationik is 1 if Nik and Njk  have different name types, and Nik is in a definite re-
lation while Njk  is not; otherwise 0. ?
k
iki InrelationInrelation?  
Probability2i scaled ranking probability for (hi, hj) from relation based re-ranker 
Event 
Constrainti 
1 if all entity types in hi match event pattern, -1 if some do not match, and 0 if the 
argument slots are empty 
Event 
Based 
EventSubType Event subtype if the patterns are extracted from ACE data, otherwise?None? 
Probability3i scaled ranking probability for (hi, hj) from event based re-ranker 
Headik 1 if ikN includes the head word of name; otherwise 0 
CorefNumik the number of mentions corefered to Nik  
WeightNumik the sum of all link weights between Nik and its corefered mentions, 0.8 for name-
name coreference; 0.5 for apposition;  0.3 for other name-nominal coreference 
Cross- 
document 
Corefer-
ence 
Based 
NumHigh-
Corefi 
the number of mentions which corefer to Nik and output by previous re-rankers with 
high confidence 
 
Table 3. Re-Ranking Properties 
 
 
Component Data 
Baseline name tagger 2978 texts from the People?s Daily in 1998 and 1300 texts from 
ACE03, 04, 05 training data 
Nominal tagger Chinese Penn TreeBank V5.1 
Coreference resolver 1300 texts from ACE03, 04, 05 training data 
Relation tagger 633 ACE 05 texts, and 546 ACE 04 texts with types/subtypes 
mapped into 05 set 
Event pattern 376 trigger words, 661 patterns 
Name structure, coreference 
and relation based re-rankers 
1,071,285 samples (pairs of hypotheses) from ACE 03, 04 and 
05 training data 
 
 
 
 
 
Training 
Event based re-ranker 325,126 samples from ACE sentences including event trigger 
words 
Test 100 texts from ACE 04 training corpus, includes 2813 names: 
1126 persons, 712 GPEs, 785 organizations and 190 locations. 
 
Table 4. Data Description 
                                                          
6 The method of counting the voting rate refers to (Zhai, 04) and (Ji and Grishman, 05) 
7 Extracted from the high-frequency name lists from the training corpus, and country/province/state/ city lists from Chinese 
wikipedia. 
  
425
The goal of each re-ranker is to learn a ranking 
function f of the following form: for each pair of 
hypotheses (hi, hj), f : H ? H ? {-1, 1}, such that 
f(hi, hj) = 1 if hi is better than hj; f (hi, hj) = -1 if hi 
is worse than hj. In this way we are able to con-
vert ranking into a classification problem. And 
then a maximum entropy model for re-ranking 
these hypotheses can be trained and applied.  
During training we use F-measure to measure 
the quality of each name hypothesis against the 
key. During test we get from the MaxEnt classi-
fier the probability (ranking confidence) for each 
pair: Prob (f (hi, hj) = 1). Then we apply a dy-
namic decoding algorithm to output the best hy-
pothesis. More details about the re-ranking 
algorithm are presented in (Ji et al, 2006). 
5.3 Re-Ranking Features 
For each sample (hi, hj), we construct a feature 
set for assessing the ranking of hi and hj. Based 
on the information obtained from inferences, we 
compute (for each property) the property score 
PSik for each individual name candidate Nik in hi; 
some of these properties depend also on the cor-
responding name tags in hj.  Then we sum over 
all names in each hypothesis hi: ?=
k
iki PSPS  
Finally we use the quantity (PSi?PSj) as the 
feature value for the sample (hi, hj).  Table 3 
summarizes the property scores PSik used in the 
different re-rankers; space limitations prevent us 
from describing them in further detail. 
6 Experimental Results and Analysis 
Table 4 shows the data used to train each stage, 
drawn from the ACE training data and other 
sources. The training samples of the re-rankers 
are obtained by running the name tagger in cross-
validation. 100 ACE 04 documents were held out 
for use as test data. 
In the following we evaluate the contributions 
of re-rankers in name identification and classifi-
cation separately.   
 
Identification Model 
Precision Recall F-Measure
Baseline 93.2 93.4 93.3 
+name structure 94.0 93.5 93.7 
+relation 93.9 93.7 93.8 
+event 94.1 93.8 93.9 
+cross-doc  
coreference 
95.1 93.9 94.5 
 
Table 5. Name Identification 
Identification 
+Classification 
 
Model 
Classifi-
cation 
Accuracy P R F 
Baseline 93.8 87.4 87.6 87.5
+name structure 94.3 88.7 88.2 88.4
+relation 95.2 89.4 89.2 89.3
+event 95.7 90.1 89.8 89.9
+cross-doc 
coreference 
96.5 91.8 90.6 91.2
 
Table 6. Name Classification 
 
Tables 5 and 6 show the performance on iden-
tification, classification, and the combined task as 
we add each re-ranker to the system.  
The gain is greater for classification (2.7%) 
than for identification (1.2%).  Furthermore, we 
can see that the gain in identification is produced 
primarily by the name structure and coreference 
components. As we noted earlier, the name struc-
ture analysis can correct boundary errors by pre-
ferring names with complete internal components, 
while coreference can resolve a boundary ambi-
guity for one mention of a name if another men-
tion is unambiguous. The greatest gains were 
therefore obtained in boundary errors: the stages 
together eliminated over 1/3 of boundary errors 
and about 10% of spurious names; only a few 
missing names were corrected, and some correct 
names were deleted. 
Both relations and events contribute substan-
tially to classification performance through their 
selectional constraints.  The lesser contribution of 
events is related to their lower frequency.  Only 
11% of the sentences in the test data contain in-
stances of the original ACE event types.  To in-
crease the impact of the event patterns, we 
broadened their coverage to include additional 
frequent event types, so that finally 35% of sen-
tences contain event "trigger words".  
We used a simple cross-document coreference 
method in which the test documents were clus-
tered based on their cross-entropy and documents 
in the same cluster were treated as a single 
document for coreference. This produced small 
gains in both identification (0.6% vs. 0.4%) and 
classification (0.8% vs. 0.4%) over single- 
document coreference. 
7 Discussion 
The use of 'feedback' from subsequent stages of 
analysis has yielded substantial improvements in 
name tagging accuracy, from F=87.5 with the 
baseline HMM to F=91.2. This performance 
compares quite favorably with the performance 
of the human annotators who prepared the ACE 
426
2005 training data.  The annotator scores (when 
measured against a final key produced by review 
and adjudication of the two annotations) were 
F=92.5 for one annotator and F=92.7 for the 
other. 
As in the case of the automatic tagger, human 
classification accuracy (97.2 - 97.6%) was better 
than identification accuracy (F = 95.0 - 95.2%).   
In Figure 5 we summarize the error rates for 
the baseline system, the improved system without 
coreference based re-ranker, the final system 
with re-ranking, and a single annotator.8 
 
 
 
Figure 5.  Error Distribution 
 
Figure 5 shows that the performance im-
provement reflects a reduction in classification 
and boundary errors. Compared to the system, 
the human annotator?s identification accuracy 
was much more skewed (52.3% missing, 13.5% 
spurious), suggesting that a major source of iden-
tification error was not difference in judgement 
but rather names which were simply overlooked 
by one annotator and picked up by the other.  
This further suggests that through an extension of 
our joint inference approach we may soon be able 
to exceed the performance of a single manual 
annotator. 
Our analysis of the types of errors, and the per-
formance of our knowledge sources, gives some 
indication of how these further gains may be 
achieved.  The selectional force of event extrac-
tion was limited by the frequency of event pat-
terns ? only about 1/3 of sentences had a pattern 
                                                          
8  Here spurious errors are names in the system response 
which do not overlap names in the key; missing errors are 
names in the key which do not overlap names in the system 
response; and boundary errors are names in the system re-
sponse which partially overlap names in the key plus names 
in the key which partially overlap names in the system re-
sponse. 
instance.  Even with this limitation, we obtained 
a gain of 0.5% in name classification.  Capturing 
a broader range of selectional patterns should 
yield further improvements.  Nearly 70% of the 
spurious names remaining in the final output 
were in fact instances of 'other' types of names, 
such as book titles and building names; creating 
explicit models of such names should improve 
performance. Finally, our cross-document 
coreference is currently performed only within 
the (small) test corpus.  Retrieving related articles 
from a large collection should increase the likeli-
hood of finding a name instance with a disam-
biguating context. 
Acknowledgment 
This material is based upon work supported by 
the Defense Advanced Research Projects Agency 
under Contract No. HR0011-06-C-0023, and the 
National Science Foundation under Grant IIS-
00325657.  Any opinions, findings and conclu-
sions expressed in this material are those of the 
authors and do not necessarily reflect the views 
of the U. S. Government. 
References  
Daniel M. Bikel, Scott Miller, Richard Schwartz, and 
Ralph Weischedel. 1997. Nymble: a high-
performance Learning Name-finder. Proc. 
ANLP1997. pp. 194-201., Washington, D.C.  
Jianfeng Gao, Mu Li, Andi Wu and Chang-Ning 
Huang. 2005. Chinese Word Segmentation and 
Named Entity Recognition: A Pragmatic Approach. 
Computational Linguistics 31(4). pp. 531-574 
Heng Ji and Ralph Grishman. 2005. Improving Name 
Tagging by Reference Resolution and Relation De-
tection. Proc. ACL2005. pp. 411-418. Ann Arbor, 
USA. 
Heng Ji, Cynthia Rudin and Ralph Grishman. 2006. 
Re-Ranking Algorithms for Name Tagging. Proc. 
HLT/NAACL 06 Workshop on Computationally 
Hard Problems and Joint Inference in Speech and 
Language Processing. New York, NY, USA 
Dan Roth and Wen-tau Yih. 2004. A Linear Pro-
gramming Formulation for Global Inference in 
Natural Language Tasks. Proc. CONLL2004. 
Dan Roth and Wen-tau Yih. 2002. Probabilistic Rea-
soning for Entity & Relation Recognition. Proc. 
COLING2002. 
Lufeng Zhai, Pascale Fung, Richard Schwartz, Marine 
Carpuat, and Dekai Wu. 2004. Using N-best Lists 
for Named Entity Recognition from Chinese 
Speech. Proc. NAACL 2004 (Short Papers) 
427
Proceedings of ACL-08: HLT, pages 254?262,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Refining Event Extraction through Cross-document Inference 
 
 
Heng Ji Ralph Grishman 
Computer Science Department 
New York University 
New York, NY 10003, USA 
(hengji, grishman)@cs.nyu.edu 
 
 
 
 
 
 
Abstract 
We apply the hypothesis of ?One Sense Per 
Discourse? (Yarowsky, 1995) to information 
extraction (IE), and extend the scope of ?dis-
course? from one single document to a cluster 
of topically-related documents. We employ a 
similar approach to propagate consistent event 
arguments across sentences and documents. 
Combining global evidence from related doc-
uments with local decisions, we design a sim-
ple scheme to conduct cross-document 
inference for improving the ACE event ex-
traction task1. Without using any additional 
labeled data this new approach obtained 7.6% 
higher F-Measure in trigger labeling and 6% 
higher F-Measure in argument labeling over a 
state-of-the-art IE system which extracts 
events independently for each sentence. 
1 Introduction 
Identifying events of a particular type within indi-
vidual documents ? ?classical? information extrac-
tion ? remains a difficult task. Recognizing the 
different forms in which an event may be ex-
pressed, distinguishing events of different types, 
and finding the arguments of an event are all chal-
lenging tasks. 
Fortunately, many of these events will be re-
ported multiple times, in different forms, both 
within the same document and within topically- 
related documents (i.e. a collection of documents 
sharing participants in potential events). We can 
                                                          
1 http://www.nist.gov/speech/tests/ace/ 
take advantage of these alternate descriptions to 
improve event extraction in the original document, 
by favoring consistency of interpretation across 
sentences and documents. Several recent studies 
involving specific event types have stressed the 
benefits of going beyond traditional single-
document extraction; in particular, Yangarber 
(2006) has emphasized this potential in his work 
on medical information extraction. In this paper we 
demonstrate that appreciable improvements are 
possible over the variety of event types in the ACE 
(Automatic Content Extraction) evaluation through 
the use of cross-sentence and cross-document evi-
dence. 
As we shall describe below, we can make use of 
consistency at several levels: consistency of word 
sense across different instances of the same word 
in related documents, and consistency of argu-
ments and roles across different mentions of the 
same or related events. Such methods allow us to 
build dynamic background knowledge as required 
to interpret a document and can compensate for the 
limited annotated training data which can be pro-
vided for each event type. 
2 Task and Baseline System 
2.1 ACE Event Extraction Task 
The event extraction task we are addressing is that 
of the Automatic Content Extraction (ACE) evalu-
ations2. ACE defines the following terminology: 
                                                          
2 In this paper we don?t consider event mention coreference 
resolution and so don?t distinguish event mentions and events. 
254
entity: an object or a set of objects in one of the 
semantic categories of interest 
mention: a reference to an entity (typically, a 
noun phrase) 
event trigger: the main word which most clearly 
expresses an event occurrence 
event arguments: the mentions that are in-
volved in an event (participants) 
event mention: a phrase or sentence within 
which an event is described, including trigger 
and arguments 
 
The 2005 ACE evaluation had 8 types of events, 
with 33 subtypes; for the purpose of this paper, we 
will treat these simply as 33 distinct event types. 
For example, for a sentence: 
 
Barry Diller on Wednesday quit as chief of Vivendi 
Universal Entertainment. 
 
the event extractor should detect a ?Person-
nel_End-Position? event mention, with the trigger 
word, the position, the person who quit the posi-
tion, the organization, and the time during which 
the event happened: 
 
Trigger Quit 
 
 
Arguments 
Role = Person Barry Diller 
Role =  
Organization 
Vivendi Universal 
Entertainment 
Role = Position Chief 
Role =  
Time-within Wednesday 
 
Table 1. Event Extraction Example 
 
We define the following standards to determine 
the correctness of an event mention: 
? A trigger is correctly labeled if its event type 
and offsets match a reference trigger. 
? An argument is correctly identified if its event 
type and offsets match any of the reference ar-
gument mentions. 
? An argument is correctly identified and classi-
fied if its event type, offsets, and role match 
any of the reference argument mentions. 
2.2 A Baseline Within-Sentence Event Tagger 
We use a state-of-the-art English IE system as our 
baseline (Grishman et al, 2005). This system ex-
tracts events independently for each sentence. Its 
training and test procedures are as follows.  
The system combines pattern matching with sta-
tistical models. For every event mention in the 
ACE training corpus, patterns are constructed 
based on the sequences of constituent heads sepa-
rating the trigger and arguments. In addition, a set 
of Maximum Entropy based classifiers are trained: 
? Trigger Labeling: to distinguish event men-
tions from non-event-mentions, to classify 
event mentions by type;  
? Argument Classifier: to distinguish arguments 
from non-arguments; 
? Role Classifier: to classify arguments by ar-
gument role.  
? Reportable-Event Classifier: Given a trigger, 
an event type, and a set of arguments, to de-
termine whether there is a reportable event 
mention. 
In the test procedure, each document is scanned 
for instances of triggers from the training corpus. 
When an instance is found, the system tries to 
match the environment of the trigger against the set 
of patterns associated with that trigger. This pat-
tern-matching process, if successful, will assign 
some of the mentions in the sentence as arguments 
of a potential event mention. The argument clas-
sifier is applied to the remaining mentions in the 
sentence; for any argument passing that classifier, 
the role classifier is used to assign a role to it. Fi-
nally, once all arguments have been assigned, the 
reportable-event classifier is applied to the poten-
tial event mention; if the result is successful, this 
event mention is reported. 
3 Motivations 
In this section we shall present our motivations 
based on error analysis for the baseline event tag-
ger. 
3.1 One Trigger Sense Per Cluster 
Across a heterogeneous document corpus, a partic-
ular verb can sometimes be trigger and sometimes 
not, and can represent different event types. How-
ever, for a collection of topically-related docu-
ments, the distribution may be much more 
convergent. We investigate this hypothesis by au-
tomatically obtaining 25 related documents for 
each test text. The statistics of some trigger exam-
ples are presented in table 2. 
255
 
Candidate Triggers 
 
Event Type 
Perc./Freq. as 
trigger in ACE 
training corpora 
Perc./Freq. as 
trigger in test  
document 
Perc./Freq. as 
trigger in test + 
related  
documents 
 
Correct 
Event 
Triggers 
advance Movement_Transport 31% of 16 50% of 2 88.9% of 27 
fire Personnel_End-Position 7% of 81 100% of 2 100% of 10 
fire Conflict_Attack 54% of 81 100% of 3 100% of 19 
replace Personnel_End-Position 5% of 20 100% of 1 83.3% of 6 
form Business_Start-Org 12% of 8 100% of 2 100% of 23 
talk Contact_Meet 59% of 74 100% of 4 100% of 26 
Incorrect 
Event 
Triggers 
hurt Life_Injure 24% of 33 0% of 2 0% of 7 
execution Life_Die 12% of 8 0% of 4 4% of 24 
 
Table 2. Examples: Percentage of a Word as Event Trigger in Different Data Collections 
 
As we can see from the table, the likelihood of a 
candidate word being an event trigger in the test 
document is closer to its distribution in the collec-
tion of related documents than the uniform training 
corpora. So if we can determine the sense (event 
type) of a word in the related documents, this will 
allow us to infer its sense in the test document. In 
this way related documents can help recover event 
mentions missed by within-sentence extraction.  
For example, in a document about ?the advance 
into Baghdad?: 
 
Example 1:  
[Test Sentence]  
Most US army commanders believe it is critical to 
pause the breakneck advance towards Baghdad to se-
cure the supply lines and make sure weapons are oper-
able and troops resupplied?. 
[Sentences from Related Documents]  
British and US forces report gains in the advance on 
Baghdad and take control of Umm Qasr, despite a 
fierce sandstorm which slows another flank. 
? 
 
The baseline event tagger is not able to detect 
?advance? as a ?Movement_Transport? event trig-
ger because there is no pattern ?advance towards 
[Place]? in the ACE training corpora (?advance? 
by itself is too ambiguous). The training data, 
however, does include the pattern ?advance on 
[Place]?, which allows the instance of ?advance? in 
the related documents to be successfully identified 
with high confidence by pattern matching as an 
event. This provides us much stronger ?feedback? 
confidence in tagging ?advance? in the test sen-
tence as a correct trigger. 
On the other hand, if a word is not tagged as an 
event trigger in most related documents, then it?s 
less likely to be correct in the test sentence despite 
its high local confidence. For example, in a docu-
ment about ?assessment of Russian president Pu-
tin?: 
 
Example 2:  
[Test Sentence]  
But few at the Kremlin forum suggested that Putin's 
own standing among voters will be hurt by Russia's 
apparent diplomacy failures. 
[Sentences from Related Documents]  
Putin boosted ties with the United States by throwing 
his support behind its war on terrorism after the Sept. 
11 attacks, but the Iraq war has hurt the relationship. 
? 
 
The word ?hurt? in the test sentence is mistaken-
ly identified as a ?Life_Injure? trigger with high 
local confidence (because the within-sentence ex-
tractor misanalyzes ?voters? as the object of ?hurt? 
and so matches the pattern ?[Person] be hurt?). 
Based on the fact that many other instances of 
?hurt? are not ?Life_Injure? triggers in the related 
documents, we can successfully remove this wrong 
event mention in the test document. 
3.2 One Argument Role Per Cluster 
Inspired by the observation about trigger distribu-
tion, we propose a similar hypothesis ? one argu-
ment role per cluster for event arguments. In other 
words, each entity plays the same argument role, or 
no role, for events with the same type in a collec-
tion of related documents. For example, 
 
256
Example 3:  
[Test Sentence]  
Vivendi earlier this week confirmed months of press 
speculation that it planned to shed its entertainment 
assets by the end of the year. 
[Sentences from Related Documents]  
Vivendi has been trying to sell assets to pay off huge 
debt, estimated at the end of last month at more than 
$13 billion. 
Under the reported plans, Blackstone Group would 
buy Vivendi's theme park division, including Universal 
Studios Hollywood, Universal Orlando in Florida... 
? 
   
The above test sentence doesn?t include an ex-
plicit trigger word to indicate ?Vivendi? as a ?sel-
ler? of a ?Transaction_Transfer-Ownership? event 
mention, but ?Vivendi? is correctly identified as 
?seller? in many other related sentences (by match-
ing patterns ?[Seller] sell? and ?buy [Seller]?s?). 
So we can incorporate such additional information 
to enhance the confidence of ?Vivendi? as a ?sel-
ler? in the test sentence. 
  On the other hand, we can remove spurious ar-
guments with low cross-document frequency and 
confidence. In the following example,  
 
Example 4:  
[Test Sentence]  
The Davao Medical Center, a regional government 
hospital, recorded 19 deaths with 50 wounded. 
 
?the Davao Medical Center? is mistakenly 
tagged as ?Place? for a ?Life_Die? event mention. 
But the same annotation for this mention doesn?t 
appear again in the related documents, so we can 
determine it?s a spurious argument. 
4 System Approach Overview 
Based on the above motivations we propose to in-
corporate global evidence from a cluster of related 
documents to refine local decisions. This section 
gives more details about the baseline within-
sentence event tagger, and the information retrieval 
system we use to obtain related documents. In the 
next section we shall focus on describing the infe-
rence procedure. 
4.1 System Pipeline 
Figure 1 depicts the general procedure of our ap-
proach. EMSet represents a set of event mentions 
which is gradually updated. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 1. Cross-doc Inference for Event Extraction 
4.2 Within-Sentence Event Extraction 
For each event mention in a test document t , the 
baseline Maximum Entropy based classifiers pro-
duce three types of confidence values: 
 
? LConf(trigger,etype): The probability of a 
string trigger indicating an event mention with 
type etype; if the event mention is produced by 
pattern matching then assign confidence 1. 
? LConf(arg, etype): The probability that a men-
tion arg is an argument of some particular 
event type etype. 
? LConf(arg, etype, role): If arg is an argument 
with event type etype, the probability of arg 
having some particular role. 
 
We apply within-sentence event extraction to get 
an initial set of event mentions 0
tEMSet , and con-
duct cross-sentence inference (details will be pre-
sented in section 5) to get an updated set of event 
mentions 1
tEMSet . 
4.3 Information Retrieval 
We then use the INDRI retrieval system (Strohman 
et al, 2005) to obtain the top N (N=25 in this pa-
Test doc
Within-sent 
Event Extraction
Query 
Construction 
Cross-sent 
Inference
Query 
Unlabeled 
Corpora 
Information 
Retrieval 
Related 
docs
Within-sent 
Event Extraction
Cross-sent 
Inference
1
rEMSet
Cross-doc 
Inference
0
tEMSet
0
rEMSet
1
tEMSet
2
tEMSet
257
per3) related documents. We construct an INDRI 
query from the triggers and arguments, each 
weighted by local confidence and frequency in the 
test document. For each argument we also add oth-
er names coreferential with or bearing some ACE 
relation to the argument. 
For each related document r returned by INDRI, 
we repeat the within-sentence event extraction and 
cross-sentence inference procedure, and get an ex-
panded event mention set 1
t rEMSet + . Then we apply 
cross-document inference to 1
t rEMSet +  and get the 
final event mention output 2
tEMSet . 
5 Global Inference 
The central idea of inference is to obtain docu-
ment-wide and cluster-wide statistics about the 
frequency with which triggers and arguments are 
associated with particular types of events, and then 
use this information to correct event and argument 
identification and classification.  
For a set of event mentions we tabulate the fol-
lowing document-wide and cluster-wide confi-
dence-weighted frequencies: 
? for each trigger string, the frequency with 
which it appears as the trigger of an event of a 
particular type; 
? for each event argument string and the names 
coreferential with or related to the argument, 
the frequency of the event type; 
? for each event argument string and the names 
coreferential with or related to the argument, 
the frequency of the event type and role. 
Besides these frequencies, we also define the 
following margin metric to compute the confi-
dence of the best (most frequent) event type or role: 
 
Margin = 
   (WeightedFrequency (most frequent value) 
    ? WeightedFrequency (second most freq value))/ 
   WeightedFrequency (second most freq value) 
A large margin indicates greater confidence in 
the most frequent value. We summarize the fre-
quency and confidence metrics in Table 3. 
Based on these confidence metrics, we designed 
the inference rules in Table 4. These rules are ap-
plied in the order (1) to (9) based on the principle 
of improving ?local? information before global 
                                                          
3 We tested different N ? [10, 75] on dev set; and N=25 
achieved best gains. 
propagation. Although the rules may seem com-
plex, they basically serve two functions:    
? to remove triggers and arguments with low 
(local or cluster-wide) confidence; 
? to adjust trigger and argument identification 
and classification to achieve (document-wide 
or cluster-wide) consistency. 
6 Experimental Results and Analysis 
In this section we present the results of applying 
this inference method to improve ACE event ex-
traction. 
6.1 Data 
We used 10 newswire texts from ACE 2005 train-
ing corpora (from March to May of 2003) as our 
development set, and then conduct blind test on a 
separate set of 40 ACE 2005 newswire texts. For 
each test text we retrieved 25 related texts from 
English TDT5 corpus which in total consists of 
278,108 texts (from April to September of 2003). 
6.2 Confidence Metric Thresholding 
We select the thresholds (?k with k=1~13) for vari-
ous confidence metrics by optimizing the F-
measure score of each rule on the development set, 
as shown in Figure 2 and 3 as follows. 
Each curve in Figure 2 and 3 shows the effect on 
precision and recall of varying the threshold for an 
individual rule.  
 
 
Figure 2. Trigger Labeling Performance with  
Confidence Thresholding on Dev Set 
258
 
 
Figure 3. Argument Labeling Performance with 
Confidence Thresholding on Dev Set 
 
The labeled point on each curve shows the best 
F-measure that can be obtained on the develop-
ment set by adjusting the threshold for that rule. 
The gain obtained by applying successive rules can 
be seen in the progression of successive points to-
wards higher recall and, for argument labeling, 
precision4. 
6.3 Overall Performance 
Table 5 shows the overall Precision (P), Recall (R) 
and F-Measure (F) scores for the blind test set. In 
addition, we also measured the performance of two 
human annotators who prepared the ACE 2005 
training data on 28 newswire texts (a subset of the 
blind test set). The final key was produced by re-
view and adjudication of the two annotations. 
Both cross-sentence and cross-document infe-
rences provided significant improvement over the 
baseline with local confidence thresholds con-
trolled. 
We conducted the Wilcoxon Matched-Pairs 
Signed-Ranks Test on a document basis. The re-
sults show that the improvement using cross-
sentence inference is significant at a 99.9% confi-
dence level for both trigger and argument labeling; 
adding cross-document inference is significant at a 
99.9% confidence level for trigger labeling and 
93.4% confidence level for argument labeling. 
                                                          
4 We didn?t show the classification adjusting rules (2), (6) and 
(8) here because of their relatively small impact on dev set. 
6.4 Discussion 
From table 5 we can see that for trigger labeling 
our approach dramatically enhanced recall (22.9% 
improvement) with some loss (7.4%) in precision. 
This precision loss was much larger than that for 
the development set (0.3%). This indicates that the 
trigger propagation thresholds optimized on the 
development set were too low for the blind test set 
and thus more spurious triggers got propagated. 
The improved trigger labeling is better than one 
human annotator and only 4.7% worse than anoth-
er. 
For argument labeling we can see that cross-
sentence inference improved both identification 
(3.7% higher F-Measure) and classification (6.1% 
higher accuracy); and cross-document inference 
mainly provided further gains (1.9%) in classifica-
tion. This shows that identification consistency 
may be achieved within a narrower context while 
the classification task favors more global back-
ground knowledge in order to solve some difficult 
cases. This matches the situation of human annota-
tion as well: we may decide whether a mention is 
involved in some particular event or not by reading 
and analyzing the target sentence itself; but in or-
der to decide the argument?s role we may need to 
frequently refer to wider discourse in order to infer 
and confirm our decision. In fact sometimes it re-
quires us to check more similar web pages or even 
wikipedia databases. This was exactly the intuition 
of our approach. We should also note that human 
annotators label arguments based on perfect entity 
mentions, but our system used the output from the 
IE system. So the gap was also partially due to 
worse entity detection. 
Error analysis on the inference procedure shows 
that the propagation rules (3), (4), (7) and (9) pro-
duced a few extra false alarms. For trigger labe-
ling, most of these errors appear for support verbs 
such as ?take? and ?get? which can only represent 
an event mention together with other verbs or 
nouns. Some other errors happen on nouns and 
adjectives. These are difficult tasks even for human 
annotators. As shown in table 5 the inter-annotator 
agreement on trigger identification is only about 
40%. Besides some obvious overlooked cases (it?s 
probably difficult for a human to remember 33 dif-
ferent event types during annotation), most diffi-
culties were caused by judging generic verbs, 
nouns and adjectives.
259
             Performance 
 
System/Human 
Trigger  
Identification 
+Classification
Argument  
Identification 
Argument 
Classification 
Accuracy 
Argument  
Identification 
+Classification
P R F P R F P R F 
Within-Sentence IE with  
Rule (1) (Baseline) 67.6 53.5 59.7 47.8 38.3 42.5 86.0 41.2 32.9 36.6 
Cross-sentence Inference 64.3 59.4 61.8 54.6 38.5 45.1 90.2 49.2 34.7 40.7 
Cross-sentence+ 
Cross-doc Inference 60.2 76.4 67.3 55.7 39.5 46.2 92.1 51.3 36.4 42.6 
Human Annotator1 59.2 59.4 59.3 60.0 69.4 64.4 85.8 51.6 59.5 55.3 
Human Annotator2 69.2 75.0 72.0 62.7 85.4 72.3 86.3 54.1 73.7 62.4 
Inter-Annotator Agreement 41.9 38.8 40.3 55.2 46.7 50.6 91.7 50.6 42.9 46.4 
 
Table 5. Overall Performance on Blind Test Set (%) 
 
In fact, compared to a statistical tagger trained on 
the corpus after expert adjudication, a human an-
notator tends to make more mistakes in trigger 
classification. For example it?s hard to decide 
whether ?named? represents a ?Person-
nel_Nominate? or ?Personnel_Start-Position? 
event mention; ?hacked to death? represents a 
?Life_Die? or ?Conflict_Attack? event mention 
without following more specific annotation guide-
lines. 
7 Related Work 
The trigger labeling task described in this paper is 
in part a task of word sense disambiguation 
(WSD), so we have used the idea of sense consis-
tency introduced in (Yarowsky, 1995), extending 
it to operate across related documents.  
Almost all the current event extraction systems 
focus on processing single documents and, except 
for coreference resolution, operate a sentence at a 
time (Grishman et al, 2005; Ahn, 2006; Hardy et 
al., 2006).  
We share the view of using global inference to 
improve event extraction with some recent re-
search. Yangarber et al (Yangarber and Jokipii, 
2005; Yangarber, 2006; Yangarber et al, 2007) 
applied cross-document inference to correct local 
extraction results for disease name, location and 
start/end time. Mann (2007) encoded specific infe-
rence rules to improve extraction of CEO (name, 
start year, end year) in the MUC management 
succession task. In addition, Patwardhan and Ri-
loff (2007) also demonstrated that selectively ap-
plying event patterns to relevant regions can 
improve MUC event extraction. We expand the 
idea to more general event types and use informa-
tion retrieval techniques to obtain wider back-
ground knowledge from related documents. 
8 Conclusion and Future Work 
One of the initial goals for IE was to create a da-
tabase of relations and events from the entire input 
corpus, and allow further logical reasoning on the 
database. The artificial constraint that extraction 
should be done independently for each document 
was introduced in part to simplify the task and its 
evaluation. In this paper we propose a new ap-
proach to break down the document boundaries 
for event extraction. We gather together event ex-
traction results from a set of related documents, 
and then apply inference and constraints to en-
hance IE performance. 
In the short term, the approach provides a plat-
form for many byproducts. For example, we can 
naturally get an event-driven summary for the col-
lection of related documents; the sentences includ-
ing high-confidence events can be used as 
additional training data to bootstrap the event tag-
ger; from related events in different timeframes 
we can derive entailment rules; the refined consis-
tent events can serve better for other NLP tasks 
such as template based question-answering. The 
aggregation approach described here can be easily 
extended to improve relation detection and corefe-
rence resolution (two argument mentions referring 
to the same role of related events are likely to 
corefer). Ultimately we would like to extend the 
system to perform essential, although probably 
lightweight, event prediction. 
 
260
XSent-Trigger-Freq(trigger, etype) The weighted frequency of string trigger appearing as the trigger of an event of type etype across all sentences within a document 
XDoc-Trigger-Freq (trigger, etype) The weighted frequency of string trigger appearing as the trigger of an event of type etype across all documents in a cluster  
XDoc-Trigger-BestFreq (trigger) Maximum over all etypes of XDoc-Trigger-Freq (trigger, etype) 
XDoc-Arg-Freq(arg, etype) The weighted frequency of arg appearing as an argument of an event of type etype across all documents in a cluster 
XDoc-Role-Freq(arg, etype, role)  The weighted frequency of arg appearing as an argument of an event of type etype with role role across all documents in a cluster 
XDoc-Role-BestFreq(arg)  Maximum over all etypes and roles of XDoc-Role-Freq(arg, etype, role) 
XSent-Trigger-Margin(trigger) The margin value of trigger in XSent-Trigger-Freq 
XDoc-Trigger-Margin(trigger) The margin value of trigger in XDoc-Trigger-Freq 
XDoc-Role-Margin(arg) The margin value of arg in XDoc-Role-Freq 
 
Table 3. Global Frequency and Confidence Metrics 
 
Rule (1): Remove Triggers and Arguments with Low Local Confidence 
If LConf(trigger, etype) < ?1, then delete the whole event mention EM; 
If LConf(arg, etype) < ?2 or LConf(arg, etype, role) < ?3, then delete arg. 
Rule (2): Adjust Trigger Classification to Achieve Document-wide Consistency 
If XSent-Trigger-Margin(trigger) >?4, then propagate the most frequent etype to all event mentions with  trigger in 
the document; and correct roles for corresponding arguments. 
Rule (3): Adjust Trigger Identification to Achieve Document-wide Consistency 
If LConf(trigger, etype) > ?5, then propagate etype to all unlabeled strings trigger in the document. 
Rule (4): Adjust Argument Identification to Achieve Document-wide Consistency 
If LConf(arg, etype) > ?6, then in the document, for each sentence containing an event mention EM with etype, add 
any unlabeled mention in that sentence with the same head as arg as an argument of EM with role. 
Rule (5): Remove Triggers and Arguments with Low Cluster-wide Confidence 
If XDoc-Trigger-Freq (trigger, etype) < ?7, then delete EM;  
If XDoc-Arg-Freq(arg, etype) < ?8 or XDoc-Role-Freq(arg, etype, role) < ?9, then delete arg. 
Rule (6): Adjust Trigger Classification to Achieve Cluster-wide Consistency 
If XDoc-Trigger-Margin(trigger) >?10, then propagate most frequent etype to all event mentions with trigger in the 
cluster; and correct roles for corresponding arguments. 
Rule (7): Adjust Trigger Identification to Achieve Cluster-wide Consistency 
If XDoc-Trigger-BestFreq (trigger) >?11, then propagate etype to all unlabeled strings trigger in the cluster, override 
the results of Rule (3) if conflict. 
Rule (8): Adjust Argument Classification to Achieve Cluster-wide Consistency 
If XDoc-Role-Margin(arg) >?12, then propagate the most frequent etype and role to all arguments with the same 
head as arg in the entire cluster. 
Rule (9): Adjust Argument Identification to Achieve Cluster-wide Consistency 
If XDoc-Role-BestFreq(arg) > ?13, then in the cluster, for each sentence containing an event mention EM with etype, 
add any unlabeled mention in that sentence with the same head as arg as an argument of EM with role. 
 
Table 4. Probabilistic Inference Rule 
 
Acknowledgments 
This material is based upon work supported by the 
Defense Advanced Research Projects Agency un-
der Contract No. HR0011-06-C-0023, and the Na-
tional Science Foundation under Grant IIS-
00325657. Any opinions, findings and conclusions 
expressed in this material are those of the authors 
and do not necessarily reflect the views of the U. S. 
Government. 
261
References  
David Ahn. 2006. The stages of event extraction. Proc. 
COLING/ACL 2006 Workshop on Annotating and 
Reasoning about Time and Events. Sydney, Aus-
tralia. 
Ralph Grishman, David Westbrook and Adam Meyers. 
2005. NYU?s English ACE 2005 System Descrip-
tion. Proc. ACE 2005 Evaluation Workshop. Wash-
ington, US. 
Hilda Hardy, Vika Kanchakouskaya and Tomek Strzal-
kowski. 2006. Automatic Event Classification Us-
ing Surface Text Features. Proc. AAAI06 Workshop 
on Event Extraction and Synthesis. Boston, Massa-
chusetts. US. 
Gideon Mann. 2007. Multi-document Relationship Fu-
sion via Constraints on Probabilistic Databases. 
Proc. HLT/NAACL 2007. Rochester, NY, US. 
Siddharth Patwardhan and Ellen Riloff. 2007. Effective 
Information Extraction with Semantic Affinity Pat-
terns and Relevant Regions. Proc. EMNLP 2007. 
Prague, Czech Republic. 
Trevor Strohman, Donald Metzler, Howard Turtle and 
W. Bruce Croft. 2005. Indri: A Language-model 
based Search Engine for Complex Queries (ex-
tended version). Technical Report IR-407, CIIR, 
Umass Amherst, US. 
Roman Yangarber, Clive Best, Peter von Etter, Flavio 
Fuart, David Horby and Ralf Steinberger. 2007. 
Combining Information about Epidemic Threats 
from Multiple Sources. Proc. RANLP 2007 work-
shop on Multi-source, Multilingual Information Ex-
traction and Summarization. Borovets, Bulgaria. 
Roman Yangarber. 2006. Verification of Facts across 
Document Boundaries. Proc. International Work-
shop on Intelligent Information Access. Helsinki, 
Finland. 
Roman Yangarber and Lauri Jokipii. 2005.   Redundan-
cy-based Correction of Automatically Extracted 
Facts. Proc. HLT/EMNLP 2005. Vancouver, Cana-
da. 
David Yarowsky. 1995. Unsupervised Word Sense Dis-
ambiguation Rivaling Supervised Methods. Proc. 
ACL 1995. Cambridge, MA, US. 
 
 
262
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 423?431,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Who, What, When, Where, Why?  
Comparing Multiple Approaches to the Cross-Lingual 5W Task 
Kristen Parton*, Kathleen R. McKeown*, Bob Coyne*, Mona T. Diab*,  
Ralph Grishman?, Dilek Hakkani-T?r?, Mary Harper?, Heng Ji?, Wei Yun Ma*,  
Adam Meyers?, Sara Stolbach*, Ang Sun?, Gokhan Tur?, Wei Xu? and Sibel Yaman? 
 
*Columbia University 
New York, NY, USA 
{kristen, kathy, 
coyne, mdiab, ma, 
sara}@cs.columbia.edu 
 
?New York University 
New York, NY, USA 
{grishman, meyers, 
asun, xuwei} 
@cs.nyu.edu 
?International Computer 
Science Institute 
Berkeley, CA, USA 
{dilek, sibel} 
@icsi.berkeley.edu 
 
?Human Lang. Tech. Ctr. of 
Excellence, Johns Hopkins 
and U. of Maryland, 
College Park  
mharper@umd.edu 
?City University of  
New York 
New York, NY, USA 
hengji@cs.qc.cuny.edu 
 
 
?SRI International 
Palo Alto, CA, USA 
gokhan@speech.sri.com 
 
 
  
Abstract 
Cross-lingual tasks are especially difficult 
due to the compounding effect of errors in 
language processing and errors in machine 
translation (MT). In this paper, we present an 
error analysis of a new cross-lingual task: the 
5W task, a sentence-level understanding task 
which seeks to return the English 5W's (Who, 
What, When, Where and Why) corresponding 
to a Chinese sentence. We analyze systems 
that we developed, identifying specific prob-
lems in language processing and MT that 
cause errors. The best cross-lingual 5W sys-
tem was still 19% worse than the best mono-
lingual 5W system, which shows that MT 
significantly degrades sentence-level under-
standing. Neither source-language nor target-
language analysis was able to circumvent 
problems in MT, although each approach had 
advantages relative to the other. A detailed 
error analysis across multiple systems sug-
gests directions for future research on the 
problem. 
1 Introduction 
In our increasingly global world, it is ever more 
likely for a mono-lingual speaker to require in-
formation that is only available in a foreign lan-
guage document. Cross-lingual applications ad-
dress this need by presenting information in the 
speaker?s language even when it originally ap-
peared in some other language, using machine 
translation (MT) in the process. In this paper, we 
present an evaluation and error analysis of a 
cross-lingual application that we developed for a 
government-sponsored evaluation, the 5W task. 
The 5W task seeks to summarize the informa-
tion in a natural language sentence by distilling it 
into the answers to the 5W questions: Who, 
What, When, Where and Why. To solve this 
problem, a number of different problems in NLP 
must be addressed: predicate identification, ar-
gument extraction, attachment disambiguation, 
location and time expression recognition, and 
(partial) semantic role labeling. In this paper, we 
address the cross-lingual 5W task: given a 
source-language sentence, return the 5W?s trans-
lated (comprehensibly) into the target language. 
Success in this task requires a synergy of suc-
cessful MT and answer selection.  
The questions we address in this paper are: 
? How much does machine translation (MT) 
degrade the performance of cross-lingual 
5W systems, as compared to monolingual 
performance? 
? Is it better to do source-language analysis 
and then translate, or do target-language 
analysis on MT? 
? Which specific problems in language 
processing and/or MT cause errors in 5W 
answers?  
In this evaluation, we compare several differ-
ent approaches to the cross-lingual 5W task, two 
that work on the target language (English) and 
one that works in the source language (Chinese). 
423
A central question for many cross-lingual appli-
cations is whether to process in the source lan-
guage and then translate the result, or translate 
documents first and then process the translation. 
Depending on how errorful the translation is, 
results may be more accurate if models are de-
veloped for the source language. However, if 
there are more resources in the target language, 
then the translate-then-process approach may be 
more appropriate. We present a detailed analysis, 
both quantitative and qualitative, of how the ap-
proaches differ in performance.  
We also compare system performance on hu-
man translation (which we term reference trans-
lations) and MT of the same data in order to de-
termine how much MT degrades system per-
formance. Finally, we do an in-depth analysis of 
the errors in our 5W approaches, both on the 
NLP side and the MT side. Our results provide 
explanations for why different approaches suc-
ceed, along with indications of where future ef-
fort should be spent. 
2 Prior Work 
The cross-lingual 5W task is closely related to 
cross-lingual information retrieval and cross-
lingual question answering (Wang and Oard 
2006; Mitamura et al 2008). In these tasks, a 
system is presented a query or question in the 
target language and asked to return documents or 
answers from a corpus in the source language. 
Although MT may be used in solving this task, it 
is only used by the algorithms ? the final evalua-
tion is done in the source language. However, in 
many real-life situations, such as global business, 
international tourism, or intelligence work, users 
may not be able to read the source language. In 
these cases, users must rely on MT to understand 
the system response. (Parton et al 2008) exam-
ine the case of ?translingual? information re-
trieval, where evaluation is done on translated 
results in the target language. In cross-lingual 
information extraction (Sudo et al 2004) the 
evaluation is also done on MT, but the goal is to 
learn knowledge from a large corpus, rather than 
analyzing individual sentences.  
The 5W task is also closely related to Seman-
tic Role Labeling (SRL), which aims to effi-
ciently and effectively derive semantic informa-
tion from text. SRL identifies predicates and 
their arguments in a sentence, and assigns roles 
to each argument. For example, in the sentence 
?I baked a cake yesterday.?, the predicate 
?baked? has three arguments. ?I? is the subject of 
the predicate, ?a cake? is the object and ?yester-
day? is a temporal argument.  
Since the release of large data resources anno-
tated with relevant levels of semantic informa-
tion, such as the FrameNet (Baker et al, 1998) 
and PropBank corpora (Kingsbury and Palmer, 
2003), efficient approaches to SRL have been 
developed (Carreras and Marquez, 2005). Most 
approaches to the problem of SRL follow the 
Gildea and Jurafsky (2002) model. First, for a 
given predicate, the SRL system identifies its 
arguments' boundaries. Second, the Argument 
types are classified depending on an adopted 
lexical resource such as PropBank or FrameNet. 
Both steps are based on supervised learning over 
labeled gold standard data. A final step uses heu-
ristics to resolve inconsistencies when applying 
both steps simultaneously to the test data.  
Since many of the SRL resources are English, 
most of the SRL systems to date have been for 
English. There has been work in other languages 
such as German and Chinese (Erk 2006; Sun 
2004; Xue and Palmer 2005). The systems for 
the other languages follow the successful models 
devised for English, e.g. (Gildea and Palmer, 
2002; Chen and Rambow, 2003; Moschitti, 2004; 
Xue and Palmer, 2004; Haghighi et al, 2005). 
3 The Chinese-English 5W Task 
3.1 5W Task Description 
We participated in the 5W task as part of the 
DARPA GALE (Global Autonomous Language 
Exploitation) project. The goal is to identify the 
5W?s (Who, What, When, Where and Why) for a 
complete sentence. The motivation for the 5W 
task is that, as their origin in journalism suggests, 
the 5W?s cover the key information nuggets in a 
sentence. If a system can isolate these pieces of 
information successfully, then it can produce a 
pr?cis of the basic meaning of the sentence. Note 
that this task differs from QA tasks, where 
?Who? and ?What? usually refer to definition 
type questions. In this task, the 5W?s refer to se-
mantic roles within a sentence, as defined in Ta-
ble 1.  
In order to get al 5W?s for a sentence correct, 
a system must identify a top-level predicate, ex-
tract the correct arguments, and resolve attach-
ment ambiguity. In the case of multiple top-level 
predicates, any of the top-level predicates may be 
chosen. In the case of passive verbs, the Who is 
the agent (often expressed as a ?by clause?, or 
not stated), and the What should include the syn-
tactic subject.  
424
Answers are judged Correct1 if they identify a 
correct null argument or correctly extract an ar-
gument that is present in the sentence. Answers 
are not penalized for including extra text, such as 
prepositional phrases or subordinate clauses, 
unless the extra text includes text from another 
answer or text from another top-level predicate. 
In sentence 2a in Table 2, returning ?bought and 
cooked? for the What would be Incorrect. Simi-
larly, returning ?bought the fish at the market? 
for the What would also be Incorrect, since it 
contains the Where. Answers may also be judged 
Partial, meaning that only part of the answer was 
returned. For example, if the What contains the 
predicate but not the logical object, it is Partial.  
Since each sentence may have multiple correct 
sets of 5W?s, it is not straightforward to produce 
a gold-standard corpus for automatic evaluation. 
One would have to specify answers for each pos-
sible top-level predicate, as well as which parts 
of the sentence are optional and which are not 
allowed. This also makes creating training data 
for system development problematic. For exam-
ple, in Table 2, the sentence in 2a and 2b is the 
same, but there are two possible sets of correct 
answers. Since we could not rely on a gold-
standard corpus, we used manual annotation to 
judge our 5W system, described in section 5. 
3.2 The Cross-Lingual 5W Task 
In the cross-lingual 5W task, a system is given a 
sentence in the source language and asked to 
produce the 5W?s in the target language. In this 
task, both machine translation (MT) and 5W ex-
traction must succeed in order to produce correct 
answers. One motivation behind the cross-lingual 
5W task is MT evaluation. Unlike word- or 
phrase-overlap measures such as BLEU, the 5W 
evaluation takes into account ?concept? or ?nug-
get? translation. Of course, only the top-level 
predicate and arguments are evaluated, so it is 
not a complete evaluation. But it seeks to get at 
the understandability of the MT output, rather 
than just n-gram overlap. 
Translation exacerbates the problem of auto-
matically evaluating 5W systems. Since transla-
tion introduces paraphrase, rewording and sen-
tence restructuring, the 5W?s may change from 
one translation of a sentence to another transla-
tion of the same sentence. In some cases, roles 
may swap. For example, in Table 2, sentences 1a 
and 1b could be valid translations of the same 
                                                 
1
 The specific guidelines for determining correctness 
were formulated by BAE.  
Chinese sentence. They contain the same infor-
mation, but the 5W answers are different. Also, 
translations may produce answers that are textu-
ally similar to correct answers, but actually differ 
in meaning. These differences complicate proc-
essing in the source followed by translation. 
 
Example: On Tuesday, President Obama met with 
French President Sarkozy in Paris to discuss the 
economic crisis. 
W Definition Example  
answer 
WHO Logical subject of the 
top-level predicate in 
WHAT, or null. 
President 
Obama 
WHAT One of the top-level 
predicates in the sen-
tence, and the predi-
cate?s logical object. 
met with 
French Presi-
dent Sarkozy 
WHEN ARGM-TMP of the 
top-level predicate in 
WHAT, or null. 
On Tuesday 
WHERE ARGM-LOC of the 
top-level predicate in 
WHAT, or null. 
in Paris 
WHY ARGM-CAU of the 
top-level predicate in 
WHAT, or null. 
to discuss the 
economic crisis 
Table 1. Definition of the 5W task, and 5W answers 
from the example sentence above. 
4 5W System 
We developed a 5W combination system that 
was based on five other 5W systems. We se-
lected four of these different systems for evalua-
tion: the final combined system (which was our 
submission for the official evaluation), two sys-
tems that did analysis in the target-language 
(English), and one system that did analysis in the 
source language (Chinese). In this section, we 
describe the individual systems that we evalu-
ated, the combination strategy, the parsers that 
we tuned for the task, and the MT systems.  
 Sentence WHO WHAT 
1a Mary bought a cake 
from Peter. 
Mary bought a 
cake 
1b Peter sold Mary a 
cake. 
Peter sold Mary 
2a I bought the fish at 
the market yesterday 
and cooked it today. 
I bought the 
fish 
[WHEN: 
yesterday] 
2b I bought the fish at 
the market yesterday 
and cooked it today. 
I cooked it 
[WHEN: 
today] 
Table 2. Example 5W answers. 
425
4.1 Latent Annotation Parser 
For this work, we have re-implemented and en-
hanced the Berkeley parser (Petrov and Klein 
2007) in several ways: (1) developed a new 
method to handle rare words in English and Chi-
nese; (2) developed a new model of unknown 
Chinese words based on characters in the word; 
(3) increased robustness by adding adaptive 
modification of pruning thresholds and smooth-
ing of word emission probabilities. While the 
enhancements to the parser are important for ro-
bustness and accuracy, it is even more important 
to train grammars matched to the conditions of 
use. For example, parsing a Chinese sentence 
containing full-width punctuation with a parser 
trained on half-width punctuation reduces accu-
racy by over 9% absolute F. In English, parsing 
accuracy is seriously compromised by training a 
grammar with punctuation and case to process 
sentences without them.  
We developed grammars for English and Chi-
nese trained specifically for each genre by sub-
sampling from available treebanks (for English, 
WSJ, BN, Brown, Fisher, and Switchboard; for 
Chinese, CTB5) and transforming them for a 
particular genre (e.g., for informal speech, we 
replaced symbolic expressions with verbal forms 
and remove punctuation and case) and by utiliz-
ing a large amount of genre-matched self-labeled 
training parses. Given these genre-specific 
parses, we extracted chunks and POS tags by 
script. We also trained grammars with a subset of 
function tags annotated in the treebank that indi-
cate case role information (e.g., SBJ, OBJ, LOC, 
MNR) in order to produce function tags.   
4.2 Individual 5W Systems 
The English systems were developed for the 
monolingual 5W task and not modified to handle 
MT. They used hand-crafted rules on the output 
of the latent annotation parser to extract the 5Ws.  
English-function used the function tags from 
the parser to map parser constituents to the 5Ws. 
First the Who, When, Where and Why were ex-
tracted, and then the remaining pieces of the sen-
tence were returned as the What. The goal was to 
make sure to return a complete What answer and 
avoid missing the object. 
English-LF, on the other hand, used a system 
developed over a period of eight years (Meyers 
et al 2001) to map from the parser?s syntactic 
constituents into logical grammatical relations 
(GLARF), and then extracted the 5Ws from the 
logical form. As a back-up, it also extracted 
GLARF relations from another English-treebank 
trained parser, the Charniak parser (Charniak 
2001). After the parses were both converted to 
the 5Ws, they were then merged, favoring the 
system that: recognized the passive, filled more 
5W slots or produced shorter 5W slots (provid-
ing that the WHAT slot consisted of more than 
just the verb). A third back-up method extracted 
5Ws from part-of-speech tag patterns. Unlike 
English-function, English-LF explicitly tried to 
extract the shortest What possible, provided there 
was a verb and a possible object, in order to 
avoid multiple predicates or other 5W answers.  
Chinese-align uses the latent annotation 
parser (trained for Chinese) to parse the Chinese 
sentences. A dependency tree converter (Johans-
son and Nuges 2007) was applied to the constitu-
ent-based parse trees to obtain the dependency 
relations and determine top-level predicates. A 
set of hand-crafted dependency rules based on 
observation of Chinese OntoNotes were used to 
map from the Chinese function tags into Chinese 
5Ws.  Finally, Chinese-align used the alignments 
of three separate MT systems to translate the 
5Ws: a phrase-based system, a hierarchical 
phrase-based system, and a syntax augmented 
hierarchical phrase-based system. Chinese-align 
faced a number of problems in using the align-
ments, including the fact that the best MT did not 
always have the best alignment. Since the predi-
cate is essential, it tried to detect when verbs 
were deleted in MT, and back-off to a different 
MT system. It also used strategies for finding 
and correcting noisy alignments, and for filtering 
When/Where answers from Who and What.  
4.3 Hybrid System 
A merging algorithm was learned based on a de-
velopment test set. The algorithm selected all 
5W?s from a single system, rather than trying to 
merge W?s from different systems, since the 
predicates may vary across systems. For each 
document genre (described in section 5.4), we 
ranked the systems by performance on the devel-
opment data. We also experimented with a vari-
ety of features (for instance, does ?What? include 
a verb). The best-performing features were used 
in combination with the ranked list of priority 
systems to create a rule-based merger. 
4.4 MT Systems 
The MT Combination system used by both of the 
English 5W systems combined up to nine sepa-
rate MT systems. System weights for combina-
tion were optimized together with the language 
426
model score and word penalty for a combination 
of BLEU and TER (2*(1-BLEU) + TER). Res-
coring was applied after system combination us-
ing large language models and lexical trigger 
models. Of the nine systems, six were phrased-
based systems (one of these used chunk-level 
reordering of the Chinese, one used word sense 
disambiguation, and one used unsupervised Chi-
nese word segmentation), two were hierarchical 
phrase-based systems, one was a string-to-
dependency system, one was syntax-augmented, 
and one was a combination of two other systems. 
Bleu scores on the government supplied test set 
in December 2008 were 35.2 for formal text, 
29.2 for informal text, 33.2 for formal speech, 
and 27.6 for informal speech. More details may 
be found in (Matusov et al 2009). 
5 Methods 
5.1 5W Systems 
For the purposes of this evaluation2, we com-
pared the output of 4 systems: English-Function, 
English-LF, Chinese-align, and the combined 
system. Each English system was also run on 
reference translations of the Chinese sentence. 
So for each sentence in the evaluation corpus, 
there were 6 systems that each provided 5Ws. 
5.2 5W Answer Annotation 
For each 5W output, annotators were presented 
with the reference translation, the MT version, 
and the 5W answers. The 5W system names 
were hidden from the annotators. Annotators had 
to select ?Correct?, ?Partial? or ?Incorrect? for 
each W. For answers that were Partial or Incor-
rect, annotators had to further specify the source 
of the error based on several categories (de-
scribed in section 6). All three annotators were 
native English speakers who were not system 
developers for any of the 5W systems that were 
being evaluated (to avoid biased grading, or as-
signing more blame to the MT system). None of 
the annotators knew Chinese, so all of the judg-
ments were based on the reference translations. 
After one round of annotation, we measured 
inter-annotator agreement on the Correct, Partial, 
or Incorrect judgment only. The kappa value was 
0.42, which was lower than we expected. An-
other surprise was that the agreement was lower 
                                                 
2
 Note that an official evaluation was also performed by 
DARPA and BAE. This evaluation provides more fine-
grained detail on error types and gives results for the differ-
ent approaches. 
for When, Where and Why (?=0.31) than for 
Who or What (?=0.48). We found that, in cases 
where a system would get both Who and What 
wrong, it was often ambiguous how the remain-
ing W?s should be graded. Consider the sentence: 
?He went to the store yesterday and cooked lasa-
gna today.? A system might return erroneous 
Who and What answers, and return Where as ?to 
the store? and When as ?today.? Since Where 
and When apply to different predicates, they 
cannot both be correct. In order to be consistent, 
if a system returned erroneous Who and What 
answers, we decided to mark the When, Where 
and Why answers Incorrect by default. We added 
clarifications to the guidelines and discussed ar-
eas of confusion, and then the annotators re-
viewed and updated their judgments.  
After this round of annotating, ?=0.83 on the 
Correct, Partial, Incorrect judgments. The re-
maining disagreements were genuinely ambigu-
ous cases, where a sentence could be interpreted 
multiple ways, or the MT could be understood in 
various ways. There was higher agreement on 
5W?s answers from the reference text compared 
to MT text, since MT is inherently harder to 
judge and some annotators were more flexible 
than others in grading garbled MT. 
5.3 5W Error Annotation 
In addition to judging the system answers by the 
task guidelines, annotators were asked to provide 
reason(s) an answer was wrong by selecting from 
a list of predefined errors. Annotators were asked 
to use their best judgment to ?assign blame? to 
the 5W system, the MT, or both. There were six 
types of system errors and four types of MT er-
rors, and the annotator could select any number 
of errors. (Errors are described further in section 
6.) For instance, if the translation was correct, 
but the 5W system still failed, the blame would 
be assigned to the system. If the 5W system 
picked an incorrectly translated argument (e.g., 
?baked a moon? instead of ?baked a cake?), then 
the error would be assigned to the MT system. 
Annotators could also assign blame to both sys-
tems, to indicate that they both made mistakes.  
Since this annotation task was a 10-way selec-
tion, with multiple selections possible, there were 
some disagreements. However, if categorized 
broadly into 5W System errors only, MT errors 
only, and both 5W System and MT errors, then 
the annotators had a substantial level of agree-
ment (?=0.75 for error type, on sentences where 
both annotators indicated an error).  
427
5.4 5 W Corpus 
The full evaluation corpus is 350 documents, 
roughly evenly divided between four genres: 
formal text (newswire), informal text (blogs and 
newsgroups), formal speech (broadcast news) 
and informal speech (broadcast conversation). 
For this analysis, we randomly sampled docu-
ments to judge from each of the genres. There 
were 50 documents (249 sentences) that were 
judged by a single annotator. A subset of that set, 
with 22 documents and 103 sentences, was 
judged by two annotators. In comparing the re-
sults from one annotator to the results from both 
annotators, we found substantial agreement. 
Therefore, we present results from the single an-
notator so we can do a more in-depth analysis. 
Since each sentence had 5W?s, and there were 6 
systems that were compared, there were 7,500 
single-annotator judgments over 249 sentences. 
6 Results 
Figure 1 shows the cross-lingual performance 
(on MT) of all the systems for each 5W. The best 
monolingual performance (on human transla-
tions) is shown as a dashed line (% Correct 
only). If a system returned Incorrect answers for 
Who and What, then the other answers were 
marked Incorrect (as explained in section 5.2). 
For the last 3W?s, the majority of errors were due 
to this (details in Figure 1), so our error analysis 
focuses on the Who and What questions. 
6.1 Monolingual 5W Performance 
To establish a monolingual baseline, the Eng-
lish 5W system was run on reference (human) 
translations of the Chinese text. For each partial 
or incorrect answer, annotators could select one 
or more of these reasons: 
? Wrong predicate or multiple predicates. 
? Answer contained another 5W answer. 
? Passive handled wrong (WHO/WHAT). 
? Answer missed. 
? Argument attached to wrong predicate. 
Figure 1 shows the performance of the best 
monolingual system for each 5W as a dashed 
line. The What question was the hardest, since it 
requires two pieces of information (the predicate 
and object). The When, Where and Why ques-
tions were easier, since they were null most of 
the time. (In English OntoNotes 2.0, 38% of sen-
tences have a When, 15% of sentences have a 
Where, and only 2.6% of sentences have a Why.) 
The most common monolingual system error on 
these three questions was a missed answer, ac-
counting for all of the Where errors, all but one 
Why error and 71% of the When errors. The re-
maining When errors usually occurred when the 
system assumed the wrong sense for adverbs 
(such as ?then? or ?just?). 
 Missing Other 
5W 
Wrong/Multiple 
Predicates 
Wrong 
REF-func 37 29 22 7 
REF-LF 54 20 17 13 
MT-func 18 18 18 8 
MT-LF 26 19 10 11 
Chinese 23 17 14 8 
Hybrid 13 17 15 12 
Table 3. Percentages of Who/What errors attributed to 
each system error type. 
The top half of Table 3 shows the reasons at-
tributed to the Who/What errors for the reference 
corpus. Since English-LF preferred shorter an-
swers, it frequently missed answers or parts of 
Figure 1. System performance on each 5W. ?Partial? indicates that part of the answer was missing. Dashed lines 
show the performance of the best monolingual system (% Correct on human translations). For the last 3W?s, the 
percent of answers that were Incorrect ?by default? were: 30%, 24%, 27% and 22%, respectively, and 8% for the 
best monolingual system 
60 60 56 66
36 40 38 42
56 59 59 64 63
70 66 73 68 75 71 78
19201914
0
10
20
30
40
50
60
70
80
90
100
En
g-
fu
nc
En
g-
LF
Ch
in
es
e
Hy
br
id
En
g-
fu
nc
En
g-
LF
Ch
in
es
e
Hy
br
id
En
g-
fu
nc
En
g-
LF
Ch
in
es
e
Hy
br
id
En
g-
fu
nc
En
g-
LF
Ch
in
es
e
Hy
br
id
En
g-
fu
nc
En
g-
LF
Ch
in
es
e
Hy
br
id
WHO WHAT WHEN WHERE WHY
Partia l
Correct
90
75 81
83 90
Best 
mono-
lingual
428
answers. English-LF also had more Partial an-
swers on the What question: 66% Correct and 
12% Partial, versus 75% Correct and 1% Partial 
for English-function. On the other hand, English-
function was more likely to return answers that 
contained incorrect extra information, such as 
another 5W or a second predicate. 
6.2 Effect of MT on 5W Performance 
The cross-lingual 5W task requires that systems 
return intelligible responses that are semantically 
equivalent to the source sentence (or, in the case 
of this evaluation, equivalent to the reference).  
As can be seen in Figure 1, MT degrades the 
performance of the 5W systems significantly, for 
all question types, and for all systems. Averaged 
over all questions, the best monolingual system 
does 19% better than the best cross-lingual sys-
tem. Surprisingly, even though English-function 
outperformed English-LF on the reference data, 
English-LF does consistently better on MT. This 
is likely due to its use of multiple back-off meth-
ods when the parser failed.  
6.3 Source-Language vs. Target-Language 
The Chinese system did slightly worse than ei-
ther English system overall, but in the formal 
text genre, it outperformed both English systems.  
Although the accuracies for the Chinese and 
English systems are similar, the answers vary a 
lot. Nearly half (48%) of the answers can be an-
swered correctly by both the English system and 
the Chinese system. But 22% of the time, the 
English system returned the correct answer when 
the Chinese system did not. Conversely, 10% of 
the answers were returned correctly by the Chi-
nese system and not the English systems. The 
hybrid system described in section 4.2 attempts 
to exploit these complementary advantages. 
After running the hybrid system, 61% of the 
answers were from English-LF, 25% from Eng-
lish-function, 7% from Chinese-align, and the 
remaining 7% were from the other Chinese 
methods (not evaluated here). The hybrid did 
better than its parent systems on all 5Ws, and the 
numbers above indicate that further improvement 
is possible with a better combination strategy.  
6.4 Cross-Lingual 5W Error Analysis 
For each Partial or Incorrect answer, annotators 
were asked to select system errors, translation 
errors, or both. (Further analysis is necessary to 
distinguish between ASR errors and MT errors.) 
The translation errors considered were: 
? Word/phrase deleted. 
? Word/phrase mistranslated. 
? Word order mixed up. 
? MT unreadable. 
Table 4 shows the translation reasons attrib-
uted to the Who/What errors. For all systems, the 
errors were almost evenly divided between sys-
tem-only, MT-only and both, although the Chi-
nese system had a higher percentage of system-
only errors. The hybrid system was able to over-
come many system errors (for example, in Table 
2, only 13% of the errors are due to missing an-
swers), but still suffered from MT errors. 
Table 4. Percentages of Who/What errors by each 
system attributed to each translation error type. 
Mistranslation was the biggest translation 
problem for all the systems. Consider the first 
example in Figure 3. Both English systems cor-
rectly extracted the Who and the When, but for 
Mistrans-
lation 
Deletion Word 
Order 
Unreadable 
MT-func 34 18 24 18 
MT-LF 29 22 21 14 
Chinese 32 17 9 13 
Hybrid 35 19 27 18 
MT: After several rounds of reminded, I was a little bit 
Ref: After several hints, it began to come back to me. 
 ??????,?????????? 
MT: The Guizhou province, within a certain bank robber, under the watchful eyes of a weak woman, and, with a 
knife stabbed the woman. 
Ref: I saw that in a bank in Guizhou Province, robbers seized a vulnerable young woman in front of a group of 
onlookers and stabbed the woman with a knife. 
 ?????????,?????????,???????,??,???????? 
MT: Woke up after it was discovered that the property is not more than eleven people do not even said that the 
memory of the receipt of the country into the country. 
Ref: Well, after waking up, he found everything was completely changed. Apart from having additional eleven 
grandchildren, even the motherland as he recalled has changed from a socialist country to a capitalist country. 
 ?????????????,?????????,????????????????????????? 
Figure 3 Example sentences that presented problems for the 5W systems. 
 
429
What they returned ?was a little bit.? This is the 
correct predicate for the sentence, but it does not 
match the meaning of the reference. The Chinese 
5W system was able to select a better translation, 
and instead returned ?remember a little bit.? 
Garbled word order was chosen for 21-24% of 
the target-language system Who/What errors, but 
only 9% of the source-language system 
Who/What errors. The source-language word 
order problems tended to be local, within-phrase 
errors (e.g., ?the dispute over frozen funds? was 
translated as ?the freezing of disputes?). The tar-
get-language system word order problems were 
often long-distance problems. For example, the 
second sentence in Figure 3 has many phrases in 
common with the reference translation, but the 
overall sentence makes no sense. The watchful 
eyes actually belong to a ?group of onlookers? 
(deleted). Ideally, the robber would have 
?stabbed the woman? ?with a knife,? rather than 
vice versa. Long-distance phrase movement is a 
common problem in Chinese-English MT, and 
many MT systems try to handle it (e.g., Wang et 
al. 2007). By doing analysis in the source lan-
guage, the Chinese 5W system is often able to 
avoid this problem ? for example, it successfully 
returned ?robbers? ?grabbed a weak woman? for 
the Who/What of this sentence. 
Although we expected that the Chinese system 
would have fewer problems with MT deletion, 
since it could choose from three different MT 
versions, MT deletion was a problem for all sys-
tems. In looking more closely at the deletions, 
we noticed that over half of deletions were verbs 
that were completely missing from the translated 
sentence. Since MT systems are tuned for word-
based overlap measures (such as BLEU), verb 
deletion is penalized equally as, for example, 
determiner deletion. Intuitively, a verb deletion 
destroys the central meaning of a sentence, while 
a determiner is rarely necessary for comprehen-
sion. Other kinds of deletions included noun 
phrases, pronouns, named entities, negations and 
longer connecting phrases.  
Deletion also affected When and Where. De-
leting particles such as ?in? and ?when? that in-
dicate a location or temporal argument caused 
the English systems to miss the argument. Word 
order problems in MT also caused attachment 
ambiguity in When and Where. 
The ?unreadable? category was an option of 
last resort for very difficult MT sentences. The 
third sentence in Figure 3 is an example where 
ASR and MT errors compounded to create an 
unparseable sentence.  
7 Conclusions 
In our evaluation of various 5W systems, we dis-
covered several characteristics of the task. The 
What answer was the hardest for all systems, 
since it is difficult to include enough information 
to cover the top-level predicate and object, with-
out getting penalized for including too much. 
The challenge in the When, Where and Why 
questions is due to sparsity ? these responses 
occur in much fewer sentences than Who and 
What, so systems most often missed these an-
swers. Since this was a new task, this first 
evaluation showed clear issues on the language 
analysis side that can be improved in the future. 
The best cross-lingual 5W system was still 
19% worse than the best monolingual 5W sys-
tem, which shows that MT significantly degrades 
sentence-level understanding. A serious problem 
in MT for systems was deletion. Chinese con-
stituents that were never translated caused seri-
ous problems, even when individual systems had 
strategies to recover. When the verb was deleted, 
no top level predicate could be found and then all 
5Ws were wrong.  
One of our main research questions was 
whether to extract or translate first. We hypothe-
sized that doing source-language analysis would 
be more accurate, given the noise in Chinese 
MT, but the systems performed about the same. 
This is probably because the English tools (logi-
cal form extraction and parser) were more ma-
ture and accurate than the Chinese tools.  
Although neither source-language nor target-
language analysis was able to circumvent prob-
lems in MT, each approach had advantages rela-
tive to the other, since they did well on different 
sets of sentences. For example, Chinese-align 
had fewer problems with word order, and most 
of those were due to local word-order problems.  
Since the source-language and target-language 
systems made different kinds of mistakes, we 
were able to build a hybrid system that used the 
relative advantages of each system to outperform 
all systems. The different types of mistakes made 
by each system suggest features that can be used 
to improve the combination system in the future. 
Acknowledgments 
This work was supported in part by the Defense 
Advanced Research Projects Agency (DARPA) 
under contract number HR0011-06-C-0023. Any 
opinions, findings and conclusions or recom-
mendations expressed in this material are the 
authors' and do not necessarily reflect those of 
the sponsors. 
430
References  
Collin F. Baker, Charles J. Fillmore, and John B. 
Lowe. 1998. The Berkeley FrameNet project. In 
COLING-ACL '98: Proceedings of the Conference, 
held at the University of Montr?al, pages 86?90. 
Xavier Carreras and Llu?s M?rquez. 2005. Introduc-
tion to the CoNLL-2005 shared task: Semantic role 
labeling. In Proceedings of the Ninth Conference 
on Computational Natural Language Learning 
(CoNLL-2005), pages 152?164. 
Eugene Charniak. 2001. Immediate-head parsing for 
language models. In Proceedings of the 39th An-
nual Meeting on Association For Computational 
Linguistics (Toulouse, France, July 06 - 11, 2001).   
John Chen and Owen Rambow. 2003. Use of deep 
linguistic features for the recognition and labeling 
of semantic arguments. In Proceedings of the 2003 
Conference on Empirical Methods in Natural Lan-
guage Processing, Sapporo, Japan. 
Katrin Erk and Sebastian Pado. 2006. Shalmaneser ? 
a toolchain for shallow semantic parsing. Proceed-
ings of LREC. 
Daniel Gildea and Daniel Jurafsky. 2002. Automatic 
labeling of semantic roles. Computational Linguis-
tics, 28(3):245?288. 
Daniel Gildea and Martha Palmer. 2002. The neces-
sity of parsing for predicate argument recognition. 
In Proceedings of the 40th Annual Conference of 
the Association for Computational Linguistics 
(ACL-02), Philadelphia, PA, USA. 
Mary Harper and Zhongqiang Huang. 2009. Chinese 
Statistical Parsing, chapter to appear. 
Aria Haghighi, Kristina Toutanova, and Christopher 
Manning. 2005. A joint model for semantic role la-
beling. In Proceedings of the Ninth Conference on 
Computational Natural Language Learning 
(CoNLL-2005), pages 173?176.  
Paul Kingsbury and Martha Palmer. 2003. Propbank: 
the next level of treebank. In Proceedings of Tree-
banks and Lexical Theories. 
Evgeny Matusov, Gregor Leusch, & Hermann Ney: 
Learning to combine machine translation systems.  
In: Cyril Goutte, Nicola Cancedda, Marc Dymet-
man, & George Foster (eds.) Learning machine 
translation. (Cambridge, Mass.: The MIT Press, 
2009); pp.257-276. 
Adam Meyers, Ralph Grishman, Michiko Kosaka and 
Shubin Zhao.  2001. Covering Treebanks with 
GLARF. In Proceedings of the ACL 2001 Work-
shop on Sharing Tools and Resources. Annual 
Meeting of the ACL. Association for Computa-
tional Linguistics, Morristown, NJ, 51-58. 
Teruko Mitamura, Eric Nyberg, Hideki Shima, 
Tsuneaki Kato, Tatsunori Mori, Chin-Yew Lin, 
Ruihua Song, Chuan-Jie Lin, Tetsuya Sakai, 
Donghong Ji, and Noriko Kando. 2008. Overview 
of the NTCIR-7 ACLIA Tasks: Advanced Cross-
Lingual Information Access. In Proceedings of the 
Seventh NTCIR Workshop Meeting. 
Alessandro Moschitti, Silvia Quarteroni, Roberto 
Basili, and Suresh Manandhar. 2007. Exploiting 
syntactic and shallow semantic kernels for question 
answer classification. In Proceedings of the 45th 
Annual Meeting of the Association of Computa-
tional Linguistics, pages 776?783.  
Kristen Parton, Kathleen R. McKeown, James Allan, 
and Enrique Henestroza. Simultaneous multilingual 
search for translingual information retrieval. In 
Proceedings of ACM 17th Conference on Informa-
tion and Knowledge Management (CIKM), Napa 
Valley, CA, 2008. 
Slav Petrov and Dan Klein. 2007. Improved Inference 
for Unlexicalized Parsing. North American Chapter 
of the Association for Computational Linguistics 
(HLT-NAACL 2007). 
Sudo, K., Sekine, S., and Grishman, R. 2004. Cross-
lingual information extraction system evaluation. 
In Proceedings of the 20th international Confer-
ence on Computational Linguistics. 
Honglin Sun and Daniel Jurafsky. 2004. Shallow Se-
mantic Parsing of Chinese. In Proceedings of 
NAACL-HLT. 
Cynthia A. Thompson, Roger Levy, and Christopher 
Manning. 2003. A generative model for semantic 
role labeling. In 14th European Conference on Ma-
chine Learning. 
Nianwen Xue and Martha Palmer. 2004. Calibrating 
features for semantic role labeling. In Dekang Lin 
and Dekai Wu, editors, Proceedings of EMNLP 
2004, pages 88?94, Barcelona, Spain, July. Asso-
ciation for Computational Linguistics. 
Xue, Nianwen and Martha Palmer. 2005. Automatic 
semantic role labeling for Chinese verbs. InPro-
ceedings of the Nineteenth International Joint Con-
ference on Artificial Intelligence, pages 1160-1165.  
Chao Wang, Michael Collins, and Philipp Koehn. 
2007. Chinese Syntactic Reordering for Statistical 
Machine Translation. Proceedings of the 2007 Joint 
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning (EMNLP-CoNLL), 737-745. 
Jianqiang Wang and Douglas W. Oard, 2006. "Com-
bining Bidirectional Translation and Synonymy for 
Cross-Language Information Retrieval," in 29th 
Annual International ACM SIGIR Conference on 
Research and Development in Information Re-
trieval, pp. 202-209. 
431
Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 353?356,
Suntec, Singapore, 4 August 2009. c?2009 ACL and AFNLP
Updating a Name Tagger Using Contemporary Unlabeled Data
Cristina Mota
L2F (INESC-ID) & IST & NYU
Rua Alves Redol 9
1000-029 Lisboa Portugal
cmota@ist.utl.pt
Ralph Grishman
New York University
Computer Science Department
New York NY 10003 USA
grishman@cs.nyu.edu
Abstract
For many NLP tasks, including named en-
tity tagging, semi-supervised learning has
been proposed as a reasonable alternative
to methods that require annotating large
amounts of training data. In this paper,
we address the problem of analyzing new
data given a semi-supervised NE tagger
trained on data from an earlier time pe-
riod. We will show that updating the unla-
beled data is sufficient to maintain quality
over time, and outperforms updating the
labeled data. Furthermore, we will also
show that augmenting the unlabeled data
with older data in most cases does not re-
sult in better performance than simply us-
ing a smaller amount of current unlabeled
data.
1 Introduction
Brill (2003) observed large gains in performance
for different NLP tasks solely by increasing the
size of unlabeled data, but stressed that for other
NLP tasks, such as named entity recognition
(NER), we still need to focus on developing tools
that help to increase the size of annotated data.
This problem is particularly crucial when pro-
cessing languages, such as Portuguese, for which
the labeled data is scarce. For instance, in the first
NER evaluation for Portuguese, HAREM (San-
tos and Cardoso, 2007), only two out of the nine
participants presented systems based on machine
learning, and they both argued they could have
achieved significantly better results if they had
larger training sets.
Semi-supervised methods are commonly cho-
sen as an alternative to overcome the lack of an-
notated resources, because they present a good
trade-off between amount of labeled data needed
and performance achieved. Co-training is one of
those methods, and has been extensively studied in
NLP (Nigam and Ghani, 2000; Pierce and Cardie,
2001; Ng and Cardie, 2003; Mota and Grishman,
2008). In particular, we showed that the perfor-
mance of a name tagger based on co-training de-
cays as the time gap between training data (seeds
and unlabeled data) and test data increases (Mota
and Grishman, 2008). Compared to the original
classifier of Collins and Singer (1999) that uses
seven seeds, we used substantially larger seed sets
(more than 1000), which raises the question of
which of the parameters (seeds or unlabeled data)
are causing the performance deterioration.
In the present study, we investigated two main
questions, from the point of view of a developer
who wants to analyze a new data set, given an NE
tagger trained with older data. First, we studied
whether it was better to update the seeds or the
unlabeled data; then, we analyzed whether using
a smaller amount of current unlabeled data could
be better than increasing the amount of unlabeled
data drawn from older sources. The experiments
show that using contemporary unlabeled data is
the best choice, outperforming most experiments
with larger amounts of older unlabeled data and
all experiments with contemporary seeds.
2 Contemporary labeled data in NLP
The speech community has been defending for
some time now the idea of having similar tem-
poral data for training and testing automatic
speech recognition systems for broadcast news.
Most works focus on improving out-of-vocabulary
(OOV) rates, to which new names contribute
significantly. For instance, Palmer and Osten-
dorf (2005) aiming at reducing the error rate due
to OOV names propose to generate offline name
lists from diverse sources, including temporally
relevant news texts; Federico and Bertoldi (2004),
and Martins et al (2006) propose to daily adapt
the statistical language model of a broadcast
353
news transcription system, exploiting contempo-
rary newswire texts available on the web; Auzanne
et al (2000) proposed a time-adaptive language
model, studying its impact over a period of five
months on the reduction of OOV rate, word error
rate and retrieval accuracy on a spoken document
retrieval system.
Concerning variations over longer periods of
time, we observed that the performance of a semi-
supervised name tagger decays over a period of
eight years, which seems to be directly related
with the fact that the texts used to train and test the
tagger also show a tendency to become less simi-
lar over time (Mota and Grishman, 2008); Batista
et al (2008) also observed a decaying tendency in
the performance of a system for recovering capi-
talization over a period of six years, proposing to
retrain a MaxEnt model using additional contem-
porary written texts.
3 Name tagger overview
We assessed the name tagger described in Mota
and Grishman (2008) to recognize names of peo-
ple, organizations and locations. The tagger is
based on the co-training NE classifier proposed
by Collins and Singer (1999), and is comprised
of several components organized sequentially (cf.
Figure 1).
!"#$%$"&$
'()*#%+,-./01$"&$2
3(4"5"6%'()*#
%!"&$%7)$8%/5(##)9"6%,-
!"&$%7)$8%:1/5(##)9"6%,-;6"1$)9/($)01
<5(##)9/($)01
'*0=(>($)01
?"($:*"%"&$*(/$)01
'()*#%+#="55)1>%@"($:*"#.%
/01$"&$:(5%@"($:*"#2
<0A$*()1)1>
B="55)1>%C%
/01$"&$:(5%*:5"#
B""6#%
D15(4"5"6%$"&$
!"#$%&'!()%&%&'
Figure 1: NE tagger architecture
4 Data sets
CETEMPu?blico (Rocha and Santos, 2000) is a
Portuguese journalistic corpus with 180 million
words that spans eight years of news, from 1991
to 1998. The minimum size of epoch (time span
of data set) available for analysis is a six-month
period, corresponding either to the first half of the
year or the second.
The data sets were created using the first 8256
extracts1 within each six-month period of the pol-
itics section of the corpus: the first 192 are used to
collect seeds, the next 208 extracts are used as test
sets and the remaining 7856 are used to collect the
unlabeled examples. The seeds correspond to the
first 1150 names occurring in those extracts. From
the list of unlabeled examples obtained after the
NE identification stage, only the first 41226 exam-
ples of each epoch were used to bootstrap in the
classification stage.
5 Experiments
We denote by S, U and T , respectively, the seed,
unlabeled and test texts, and by (S
i
, U
j
, T
k
) a
training-test configuration, where 91a ? i, j, k ?
98b, i.e., epochs i, j and k vary between the first
half of 1991 (91a) and the second half of 1998
(98b). For instance, the training-test configuration
(S
i=91a...98b
, U
i=91a...98b
, T
j=98b
) represents the
training-test configuration where the test set was
drawn from epoch 98b, and the tagger was trained
in turn with seeds and unlabeled data drawn from
the same epoch i that varied from 91a to 98b.
5.1 Do we need contemporary labeled data?
In order to understand whether it is better to label
examples falling within the epoch of the test set
or to keep using old labeled data while bootstrap-
ping with contemporary unlabeled data, we fixed
the test set to be within the last epoch of the inter-
val (98b), and performed backward experiments,
i.e., we varied the epoch of either the seeds or the
unlabeled data backwards. The choice of fixing
the test within the last epoch of the interval is the
one that most approximates a real situation where
one has a tagger trained with old data and wants to
process a more recent text.
Figure 2 shows the results for both experiments,
where (S
j=98b
, U
i=91a...98b
, T
j=98b
) represents the
experiment where the test was within the same
epoch as the seeds and the unlabeled data were
drawn from a single, variable, epoch in turn, and
(S
i=91a...98b
, U
j=98b
, T
j=98b
) represents the exper-
iment where the test was within the epoch of the
1Extracts are typically two paragraphs.
354
unlabeled data and the seeds were drawn in turn
from each of the epochs; the graphic also shows
the baseline backward training (varying the epoch
of both the seeds and the unlabeled data together).
0.
74
0.
76
0.
78
0.
80
0.
82
Training epoch
F?
m
ea
su
re
(i,i,98b)
(98b,i,98b)
(i,98b,98b)
91
a
91
b
92
a
92
b
93
a
93
b
94
a
94
b
95
a
95
b
96
a
96
b
97
a
97
b
98
a
98
b
Figure 2: F-measure over time for test set 98b with
configurations: (S
i=91a...98b
, U
i=91a...98b
, T
j=98b
),
(S
j=98b
, U
i=91a...98b
, T
j=98b
), and (S
i=91a...98b
,
U
j=98b
, T
j=98b
)
As can be seen, there is a small gain in perfor-
mance by using seeds within the epoch of the test
set, but the decay is still observable as we increase
the time gap between the unlabeled data and the
test set. On the contrary, if we use unlabeled data
within the epoch of the test set, we hardly see
a degradation trend as the time gap between the
epochs of seeds and test set is increased.
An examination of the results shows that, for
instance, Sendero Luminoso received the correct
classification of organization when the tagger is
trained with unlabeled data drawn from the same
epoch, but is incorretly classified as person when
trained with data that is not contemporary with the
test set. Even though that name is not a seed in any
of the cases, it occurs twice in good contexts for
organization in unlabeled data contemporary with
the test set (l??der do Sendero Luminoso/leader of
the Shining Path and acc?o?es do Sendero Lumi-
noso/actions of the Shining Path), while it does
not occur in the unlabeled data that is not contem-
porary. Given that both the name spelling and the
context in the test set, o messianismo do peruano
Sendero Luminoso/the messianism of the Peruvian
Shining Path, are insufficient to assign a correct la-
bel, the occurrence of the name in the contempo-
rary unlabeled data contributes to its correct clas-
sification in the test set.
5.2 Is more older unlabeled data better?
The second question we addressed was whether
having more older unlabeled data could result in
better performance than less data but within the
epoch of the test set. In this case, we conducted
two backward experiments, augmenting the un-
labeled data backwards with older data than the
test set (98b), starting in the previous epoch (98a):
in the first experiment, the seeds were within the
same epoch as the test set, and in the second ex-
periment the seeds were within the same epoch as
the unlabeled set being added. This corresponds to
configurations (S
j=98b
, U ?
i=91a...98a
, T
j=98b
) and
(S
i=91a...98a
, U ?
i=91a...98a
, T
j=98b
), respectively,
where U ?
i
=
?
98a
k=i
U
k
.
In Figure 3, we show the result of these con-
figurations together with the result of the back-
ward experiment corresponding to configuration
(S
i=91a...98b
, U
j=98b
, T
j=98b
), also represented in
Figure 2. We note that, in the case of the former
experiments, the size of the unlabeled examples is
increasing in the direction 98a to 91a.
0.
77
0.
78
0.
79
0.
80
0.
81
0.
82
0.
83
Training epoch
F?
m
ea
su
re
(i,98b,98b)
(i,u[i,...,98a],98b)
(98b,u[i,...,98a],98b)
91
a
91
b
92
a
92
b
93
a
93
b
94
a
94
b
95
a
95
b
96
a
96
b
97
a
97
b
98
a
98
b
Figure 3: F-measure for test set 98b with
configurations (S
i=91a...98b
, U
j=98b
, T
j=98b
),
(S
j=98b
, U ?
i=91a...98a
, T
j=98b
) and (S
i=91a...98a
,
U
?
i=91a...98a
, T
j=98b
), where U ?
i
=
?
98a
k=i
U
k
As can be observed, increasing the size of the
unlabeled data does not necessarily result in bet-
ter performance: for both choices of seeds, perfor-
mance sometimes improves, sometimes worsens,
as the unlabeled data grows (following the curves
355
from right to left).
Furthermore, the tagger trained with more unla-
beled data in most cases did not outperform the
tagger trained with less unlabeled data selected
from the epoch of the test set.
6 Discussion and future directions
We conducted experiments varying the epoch of
seeds and unlabeled data of a named entity tagger
based on co-training. We observed that the per-
formance decay resulting from increasing the time
gap between training data (seeds and unlabeled ex-
amples) and the test set can be slightly attenuated
by using the seeds contemporary with the test set.
The gain is larger if one uses older seeds and con-
temporary unlabeled data, a strategy that, in most
of the experiments, results in better performance
than using increasing sizes of older unlabeled data.
These results suggest that we may not need to
label new data nor train our tagger with increasing
sizes of data, as long as we are able to train it with
unlabeled data time compatible with the test set.
In the future, one issue that needs clarification is
why bootstraping from contemporary labeled data
had so little influence on the performance of co-
training, and if other semi-supervised approches
are also sensitive to this question.
Acknowledgment
The first author?s research work was funded by
Fundac?a?o para a Cie?ncia e a Tecnologia through a
doctoral scholarship (ref.: SFRH/BD/3237/2000).
References
Ce?dric Auzanne, John S. Garofolo, Jonathan G. Fiscus,
and William M. Fisher. 2000. Automatic language
model adaptation for spoken document retrieval. In
Proceedings of RIAO 2000 Conference on Content-
Based Multimedia Information Access.
Fernando Batista, Nuno Mamede, and Isabel Trancoso.
2008. Language dynamics and capitalization using
maximum entropy. In Proceedings of ACL-08: HLT,
Short Papers, pages 1?4, Columbus, Ohio, June. As-
sociation for Computational Linguistics.
Eric Brill. 2003. Processing natural language with-
out natural language processing. In CICLing, pages
360?369.
Michael Collins and Yoram Singer. 1999. Unsuper-
vised models for named entity classification. In
Proceedings of the Joint SIGDAT Conference on
EMNLP.
Marcello Federico and Nicola Bertoldi. 2004. Broad-
cast news lm adaptation over time. Computer
Speech & Language, 18(4):417?435.
Ciro Martins, Anto?nio Teixeira, and Joa?o Neto. 2006.
Dynamic vocabulary adaptation for a daily and
real-time broadcast news transcription system. In
IEEE/ACL Workshop on Spoken Language Technol-
ogy, Aruba.
Cristina Mota and Ralph Grishman. 2008. Is this NE
tagger getting old? In Proceedings of the Sixth
International Language Resources and Evaluation
(LREC?08), Marrakech, Morocco, may.
Vincente Ng and Claire Cardie. 2003. Weakly super-
vised natural language learning without redundant
views. In NAACL?03: Proceedings of the 2003 Con-
ference of the North American Chapter of the As-
sociation for Computational Linguistics on Human
Language Technology, pages 94?101, Morristown,
NJ, USA. ACL.
Kamal Nigam and Rayid Ghani. 2000. Analyzing
the effectiveness and applicability of co-training. In
Proceedings of CIKM, pages 86?93.
David D. Palmer and Mari Ostendorf. 2005. Improv-
ing out-of-vocabulary name resolution. Computer
Speech & Language, 19(1):107?128.
David Pierce and Claire Cardie. 2001. Limitations of
co-training for natural language learning from large
datasets. In Proceedings of the 2001 Conference on
Empirical Methods in Natural Language Processing
(EMNLP-2001).
Paulo Rocha and Diana Santos. 2000. Cetempu?blico:
Um corpus de grandes dimenso?es de linguagem
jornal??stica portuguesa. In Maria das Grac?as
Volpe Nunes, editor, Actas do V Encontro para o
processamento computacional da l??ngua portuguesa
escrita e falada PROPOR 2000, pages 131?140, At-
ibaia, Sa?o Paulo, Brasil.
Diana Santos and Nuno Cardoso, editors. 2007. Re-
conhecimento de entidades mencionadas em por-
tugue?s: Documentac?a?o e actas do HAREM, a
primeira avaliac?a?o conjunta na a?rea. Linguateca,
12 de Novembro.
356
Covering Treebanks with GLARF
Adam Meyers
 
and Ralph Grishman   and Michiko Kosaka  and Shubin Zhao  
 
New York University, 719 Broadway, 7th Floor, NY, NY 10003 USA
 Monmouth University, West Long Branch, N.J. 07764, USA
meyers/grishman/shubinz@cs.nyu.edu, kosaka@monmouth.edu
Abstract
This paper introduces GLARF, a frame-
work for predicate argument structure.
We report on converting the Penn Tree-
bank II into GLARF by automatic
methods that achieved about 90% pre-
cision/recall on test sentences from the
Penn Treebank. Plans for a corpus
of hand-corrected output, extensions of
GLARF to Japanese and applications
for MT are also discussed.
1 Introduction
Applications using annotated corpora are often,
by design, limited by the information found in
those corpora. Since most English treebanks pro-
vide limited predicate-argument (PRED-ARG)
information, parsers based on these treebanks do
not produce more detailed predicate argument
structures (PRED-ARG structures). The Penn
Treebank II (Marcus et al, 1994) marks sub-
jects (SBJ), logical objects of passives (LGS),
some reduced relative clauses (RRC), as well as
other grammatical information, but does not mark
each constituent with a grammatical role. In our
view, a full PRED-ARG description of a sen-
tence would do just that: assign each constituent
a grammatical role that relates that constituent to
one or more other constituents in the sentence.
For example, the role HEAD relates a constituent
to its parent and the role OBJ relates a constituent
to the HEAD of its parent. We believe that the
absence of this detail limits the range of appli-
cations for treebank-based parsers. In particu-
lar, they limit the extent to which it is possible
to generalize, e.g., marking IND-OBJ and OBJ
roles allows one to generalize a single pattern to
cover two related examples (?John gave Mary a
book? = ?John gave a book to Mary?). Distin-
guishing complement PPs (COMP) from adjunct
PPs (ADV) is useful because the former is likely
to have an idiosyncratic interpretation, e.g., the
object of ?at? in ?John is angry at Mary? is not
a locative and should be distinguished from the
locative case by many applications.
In an attempt to fill this gap, we have begun
a project to add this information using both au-
tomatic procedures and hand-annotation. We are
implementing automatic procedures for mapping
the Penn Treebank II (PTB) into a PRED-ARG
representation and then we are correcting the out-
put of these procedures manually. In particular,
we are hoping to encode information that will en-
able a greater level of regularization across lin-
guistic structures than is possible with PTB.
This paper introduces GLARF, the Grammati-
cal and Logical Argument Representation Frame-
work. We designed GLARF with four objec-
tives in mind: (1) capturing regularizations ?
noncanonical constructions (e.g., passives, filler-
gap constructions, etc.) are represented in terms
of their canonical counterparts (simple declara-
tive clauses); (2) representing all phenomena us-
ing one simple data structure: the typed feature
structure (3) consistently labeling all arguments
and adjuncts for phrases with clear heads; and (4)
producing clear and consistent PRED-ARGs for
phrases that do not have heads, e.g., conjoined
structures, named entities, etc. ? rather than try-
ing to squeeze these phrases into an X-bar mold,
we customized our representations to reflect their
head-less properties. We believe that a framework
for PRED-ARG needs to satisfy these objectives
to adequately cover a corpus like PTB.
We believe that GLARF, because of its uni-
form treatment of PRED-ARG relations, will be
valuable for many applications, including ques-
tion answering, information extraction, and ma-
chine translation. In particular, for MT, we ex-
pect it will benefit procedures which learn trans-
lation rules from syntactically analyzed parallel
corpora, such as (Matsumoto et al, 1993; Mey-
ers et al, 1996). Much closer alignments will
be possible using GLARF, because of its multi-
ple levels of representation, than would be pos-
sible with surface structure alone (An example is
provided at the end of Section 2). For this reason,
we are currently investigating the extension of our
mapping procedure to treebanks of Japanese (the
Kyoto Corpus) and Spanish (the UAM Treebank
(Moreno et al, 2000)). Ultimately, we intend to
create a parallel trilingual treebank using a com-
bination of automatic methods and human correc-
tion. Such a treebank would be valuable resource
for corpus-trained MT systems.
The primary goal of this paper is to discuss the
considerations for adding PRED-ARG informa-
tion to PTB, and to report on the performance of
our mapping procedure. We intend to wait until
these procedures are mature before beginning an-
notation on a larger scale. We also describe our
initial research on covering the Kyoto Corpus of
Japanese with GLARF.
2 Previous Treebanks
There are several corpora annotated with PRED-
ARG information, but each encode some dis-
tinctions that are different. The Susanne Cor-
pus (Sampson, 1995) consists of about 1/6 of the
Brown Corpus annotated with detailed syntactic
information. Unlike GLARF, the Susanne frame-
work does not guarantee that each constituent be
assigned a grammatical role. Some grammatical
roles (e.g., subject, object) are marked explicitly,
others are implied by phrasetags (Fr corresponds
to the GLARF node label SBAR under a REL-
ATIVE arc label) and other constituents are not
assigned roles (e.g., constituents of NPs). Apart
from this concern, it is reasonable to ask why
we did not adapt this scheme for our use. Su-
sanne?s granularity surpasses PTB-based GLARF
in many areas with about 350 wordtags (part of
speech) and 100 phrasetags (phrase node labels).
However, GLARF would express many of the de-
tails in other ways, using fewer node and part of
speech (POS) labels and more attributes and role
labels. In the feature structure tradition, GLARF
can represent varying levels of detail by adding
or subtracting attributes or defining subsumption
hierarchies. Thus both Susanne?s NP1p word-
tag and Penn?s NNP wordtag would correspond
to GLARF?s NNP POS tag. A GLARF-style
Susanne analysis of ?Ontario, Canada? is (NP
(PROVINCE (NNP Ontario)) (PUNCTUATION
(, ,)) (COUNTRY (NNP Canada)) (PATTERN
NAME) (SEM-FEATURE LOC)). A GLARF-
style PTB analysis uses the roles NAME1 and
NAME2 instead of PROVINCE and COUNTRY,
where name roles (NAME1, NAME2) are more
general than PROVINCE and COUNTRY in a
subsumption hierarchy. In contrast, attempts to
convert PTB into Susanne would fail because de-
tail would be unavailable. Similarly, attempts to
convert Susanne into the PTB framework would
lose information. In summary, GLARF?s ability
to represent varying levels of detail allows dif-
ferent types of treebank formats to be converted
into GLARF, even if they cannot be converted into
each other. Perhaps, GLARF can become a lingua
franca among annotated treebanks.
The Negra Corpus (Brants et al, 1997) pro-
vides PRED-ARG information for German, simi-
lar in granularity to GLARF. The most significant
difference is that GLARF regularizes some phe-
nomena which a Negra version of English would
probably not, e.g., control phenomena. Another
novel feature of GLARF is the ability to represent
paraphrases (in the Harrisian sense) that are not
entirely syntactic, e.g., nominalizations as sen-
tences. Other schemes seem to only regularize
strictly syntactic phenomena.
3 The Structure of GLARF
In GLARF, each sentence is represented by a
typed feature structure. As is standard, we
model feature structures as single-rooted directed
acyclic graphs (DAGs). Each nonterminal is la-
beled with a phrase category, and each leaf is la-
beled with either: (a) a (PTB) POS label and a
word (eat, fish, etc.) or (b) an attribute value (e.g.,
singular, passive, etc.). Types are based on non-
terminal node labels, POSs and other attributes
(Carpenter, 1992). Each arc bears a feature label
which represents either a grammatical role (SBJ,
OBJ, etc.) or some attribute of a word or phrase
(morphological features, tense, semantic features,
etc.).1 For example, the subject of a sentence is
the head of a SBJ arc, an attribute like SINGU-
LAR is the head of a GRAM-NUMBER arc, etc.
A constituent involved in multiple surface or log-
ical relations may be at the head of multiple arcs.
For example, the surface subject (S-SBJ) of a pas-
sive verb is also the logical object (L-OBJ). These
two roles are represented as two arcs which share
the same head. This sort of structure sharing anal-
ysis originates with Relational Grammar and re-
lated frameworks (Perlmutter, 1984; Johnson and
Postal, 1980) and is common in Feature Structure
frameworks (LFG, HPSG, etc.). Following (John-
son et al, 1993)2, arcs are typed. There are five
different types of role labels:
 Attribute roles: Gram-Number (grammati-
cal number), Mood, Tense, Sem-Feature (se-
mantic features like temporal/locative), etc.
 Surface-only relations (prefixed with S-),
e.g., the surface subject (S-SBJ) of a passive.
 Logical-only Roles (prefixed with L-), e.g.,
the logical object (L-OBJ) of a passive.
 Intermediate roles (prefixed with I-) repre-
senting neither surface, nor logical positions.
In ?John seemed to be kidnapped by aliens?,
?John? is the surface subject of ?seem?, the
logical object of ?kidnapped?, and the in-
termediate subject of ?to be?. Intermedi-
ate arcs capture are helpful for modeling the
way sentences conform to constraints. The
intermediate subject arc obeys lexical con-
straints and connect the surface subjects of
?seem? (COMLEX Syntax class TO-INF-
RS (Macleod et al, 1998a)) to the subject
of the infinitive. However, the subject of the
infinitive in this case is not a logical sub-
ject due to the passive. In some cases, in-
termediate arcs are subject to number agree-
ment, e.g., in ?Which aliens did you say
were seen??, the I-SBJ of ?were seen? agrees
with ?were?.
 Combined surface/logical roles (unprefixed
arcs, which we refer to as SL- arcs). For ex-
1A few grammatical roles are nonfunctional, e.g., a con-
stituent can have multiple ADV constituents. We number
these roles (ADV1, ADV2,  ) to preserve functionality.
2That paper uses two arc types: category and relational.
ample, ?John? in ?John ate cheese? would be
the target of a SBJ subject arc.
Logical relations, encoded with SL- and L-
arcs, are defined more broadly in GLARF than
in most frameworks. Any regularization from a
non-canonical linguistic structure to a canonical
one results in logical relations. Following (Harris,
1968) and others, our model of canonical linguis-
tic structure is the tensed active indicative sen-
tence with no missing arguments. The following
argument types will be at the head of logical (L-)
arcs based on counterparts in canonical sentences
which are at the head of SL- arcs: logical argu-
ments of passives, understood subjects of infini-
tives, understood fillers of gaps, and interpreted
arguments of nominalizations (In ?Rome?s de-
struction of Carthage?, ?Rome? is the logical sub-
ject and ?Carthage? is the logical object). While
canonical sentence structure provides one level
of regularization, canonical verb argument struc-
tures provide another. In the case of argument al-
ternations (Levin, 1993), the same role marks an
alternating argument regardless of where it occurs
in a sentence. Thus ?the man? is the indirect ob-
ject (IND-OBJ) and ?a dollar? is the direct object
(OBJ) in both ?She gave the man a dollar? and
?She gave a dollar to the man? (the dative alter-
nation). Similarly, ?the people? is the logical ob-
ject (L-OBJ) of both ?The people evacuated from
the town? and ?The troops evacuated the people
from the town?, when we assume the appropriate
regularization. Encoding this information allows
applications to generalize. For example, a single
Information Extraction pattern that recognizes the
IND-OBJ/OBJ distinction would be able to han-
dle these two examples. Without this distinction,
2 patterns would be needed.
Due to the diverse types of logical roles, we
sub-type roles according to the type of regu-
larization that they reflect. Depending on the
application, one can apply different filters to a
detailed GLARF representation, only looking at
certain types of arcs. For example, one might
choose all logical (L- and SL-) roles for an
application that is trying to acquire selection
restrictions, or all surface (S- and SL-) roles
if one was interested in obtaining a surface
parse. For other applications, one might want to
choose between subtypes of logical arcs. Given
(S (NP-SBJ (PRP they))
(VP (VP (VBD spent)
(NP-2 ($ $)
(CD 325,000)
(-NONE- *U*))
(PP-TMP-3 (IN in)
(NP (CD 1989))))
(CC and)
(VP (NP=2 ($ $)
(CD 340,000)
(-NONE- *U*))
(PP-TMP=3 (IN in)
(NP (CD 1990))))))
Figure 1: Penn representation of gapping
a trilingual treebank, suppose that a Spanish
treebank sentence corresponds to a Japanese
nominalization phrase and an English nominal-
ization phrase, e.g.,
Disney ha comprado Apple Computers
Disney?s acquisition of Apple Computers
Furthermore, suppose that the English treebank
analyzes the nominalization phrase both as an
NP (Disney = possessive, Apple Computers =
object of preposition) and as a paraphrase of a
sentence (Disney = subject, Apple Computers
= object). For an MT system that aligns the
Spanish and English graph representation, it
may be useful to view the nominalization phrase
in terms of the clausal arguments. However,
in a Japanese/English system, we may only
want to look at the structure of the English
nominalization phrase as an NP.
4 GLARF and the Penn Treebank
This section focuses on some characteristics of
English GLARF and how we map PTB into
GLARF, as exemplified by mapping the PTB rep-
resentation in Figure 1 to the GLARF representa-
tion in Figure 2. In the process, we will discuss
how some of the more interesting linguistic phe-
nomena are represented in GLARF.
4.1 Mapping into GLARF
Our procedure for mapping PTB into GLARF
uses a sequence of transformations. The first
transformation applies to PTB, and the out-
put of each 	

 is the input of
Applying Coreference to Improve Name Recognition 
Heng JI and Ralph GRISHMAN 
Department of Computer Science 
New York University 
715 Broadway, 7th Floor 
New York, NY 10003, U.S.A. 
   hengji@cs.nyu.edu,  grishman@cs.nyu.edu 
 
Abstract 
We present a novel method of applying the 
results of coreference resolution to improve 
Name Recognition for Chinese.  We consider 
first some methods for gauging the confidence 
of individual tags assigned by a statistical 
name tagger.  For names with low confidence, 
we show how these names can be filtered 
using coreference features to improve 
accuracy.  In addition, we present rules which 
use coreference information to correct some 
name tagging errors.  Finally, we show how 
these gains can be magnified by clustering 
documents and using cross-document 
coreference in these clusters.  These combined 
methods yield an absolute improvement of 
about 3.1% in tagger F score. 
 
1 Introduction 
The problem of name recognition and 
classification has been intensively studied since 
1995, when it was introduced as part of the MUC-
6 Evaluation (Grishman and Sundheim, 1996).  A 
wide variety of machine learning methods have 
been applied to this problem, including Hidden 
Markov Models (Bikel et al 1997), Maximum 
Entropy methods (Borthwick et al 1998, Chieu 
and Ng 2002), Decision Trees (Sekine et al 1998), 
Conditional Random Fields (McCallum and Li 
2003), Class-based Language Model (Sun et al 
2002), Agent-based Approach (Ye et al 2002) and 
Support Vector Machines. However, the 
performance of even the best of these models1 has 
been limited by the amount of labeled training data 
available to them and the range of features which 
they employ.  In particular, most of these methods 
classify an instance of a name based on the 
information about that instance alone, and very 
local context of that instance ? typically, one or 
                                                   
1
  The best results reported for Chinese named entity 
recognition, on the MET-2 test corpus, are 0.92 to 0.95   
F-measure for the different name types (Ye et al 2002). 
two words preceding and following the name.  If a 
name has not been seen before, and appears in a 
relatively uninformative context, it becomes very 
hard to classify. 
We propose to use more global information to 
improve the performance of name recognition.  
Some name taggers have incorporated a name 
cache or similar mechanism which makes use of 
names previously recognized in the document.  In 
our approach, we perform coreference analysis and 
then use detailed evidence from other phrases in 
the document which are co-referential with this 
name in order to disambiguate the name.  This 
allows us to perform a richer set of corrections 
than with a name cache.  We then go one step 
further and process similar documents containing 
instances of the same name, and combine the 
evidence from these additional instances.  At each 
step we are able to demonstrate a small but 
consistent improvement in named entity 
recognition. 
The rest of the paper is organized as follows. 
Section 2 briefly describes the baseline name 
tagger and coreference resolver used in this paper. 
Section 3 considers methods for assessing the 
confidence of name tagging decisions.  Section 4 
examines the distribution of name errors, as a 
motivation for using coreference information. 
Section 5 shows the coreference features we use 
and how they are incorporated into a statistical 
name filter.  Section 6 describes additional rules 
using coreference to improve name recognition. 
Section 7 provides the flow graph of the improved 
system.  Section 8 reports and discusses the 
experimental results while Section 9 summarizes 
the conclusions. 
2 Baseline Systems 
The task we consider in this paper is to identify 
three classes of names in Chinese text:  persons 
(PER), organizations (ORG), and geo-political 
entities (GPE).  Geo-political entities are locations 
which have an associated government, such as 
cities, states, and countries.2  Name recognition in 
Chinese poses extra challenges because neither 
capitalization nor word segmentation clues are 
explicitly provided, although most of the 
techniques we describe are more generally 
applicable. 
Our study builds on an extraction system 
developed for the ACE evaluation, a multi-site 
evaluation of information extraction organized by 
the U.S. Government.  Following ACE 
terminology, we will use the term mention to refer 
to a name or noun phrase of one of the types of 
interest, and the term entity for a set of coreferring 
mentions.  We briefly describe in this section the 
baseline Chinese named entity tagger, as well as 
the coreference system, used in our experiments. 
2.1 Chinese Name Tagger 
Our baseline name tagger consists of an HMM 
tagger augmented with a set of post-processing 
rules.  The HMM tagger generally follows the 
NYMBLE model (Bikel et al 1997), but with a 
larger number of states (12) to handle name 
prefixes and suffixes, and transliterated foreign 
names separately.  It operates on the output of a 
word segmenter from Tsinghua University.  It uses 
a trigram model with dynamic backoff.  The post-
processing rules correct some omissions and 
systematic errors using name lists (for example, a 
list of all Chinese last names; lists of organization 
and location suffixes) and particular contextual 
patterns (for example, verbs occurring with 
people?s names).  They also deal with 
abbreviations and nested organization names. 
 
2.2 Chinese Coreference Resolver 
For this study we have used a rule-based 
coreference resolver.  Table 1 lists the main rules 
and patterns used.  We have extensive rules for 
name-name coreference, including rules specific to 
the particular name types.  For these experiments, 
we do not attempt to resolve pronouns, and we 
only resolve names with nominals when the name 
and nominal appear in close proximity in a specific 
structure, as listed in Table 1. 
We have used the MUC coreference scoring 
metric (Vilain et al 1995) to evaluate this resolver, 
excluding all pronouns and limiting ourselves to 
noun phrases of semantic type PER, ORG, and 
GPE.  Using a perfect (hand-generated) set of 
mentions, we obtain a recall of 82.7% and 
precision of 95.1%, for an F score of 88.47%.  
                                                   
2
 This class is used in the U.S. Government?s ACE 
evaluations;  it excludes locations without governments, 
such as bodies of water and mountains. 
Using the mentions generated by our extraction 
system, we obtain a recall of 74.3%, a precision of 
84.5%, and an F score of 79.07%.3  
3 Confidence Measures 
In order to decide when we need to rely on 
global (coreference) information for name tagging, 
we want to have some assessment of the 
confidence that the name tagger has in individual 
tagging decisions.  In this paper, we use two tools 
to reach this goal.  The first method is to use three 
manually built proper name lists which include 
common names of each type (selected from the 
high frequency names in the user query blog of 
COMPASS, a Chinese search engine, and name 
lists provided by Linguistic Data Consortium; the 
PER list includes 147 names, the GPE list 226 
names, and the ORG list 130 names).  Names on 
these lists are accepted without further review. 
The second method is to have the HMM tagger 
compute a probability margin for the identification 
of a particular name as being of a particular type.  
Scheffer et al (2001) used a similar method to 
identify good candidates for tagging in an active 
learner.  During decoding, the HMM tagger seeks 
the path of maximal probability through the Viterbi 
lattice.  Suppose we wish to evaluate the 
confidence with which words wi, ?, wj are 
identified as a name of type T.  We compute 
 
Margin (wi,?, wj; T) =  log P1 ? log P2 
 
Here P1 is the maximum path probability and P2 is 
the maximum probability among all paths for 
which some word in wi, ?, wj is assigned a tag 
other than T. 
A large margin indicates greater confidence in 
the tag assignment.  If we exclude names tagged 
with a margin below a threshold, we can increase 
the precision of name tagging at some cost in recall.  
Figure 1 shows the trade-off between margin 
threshold and name recognition performance.  
Names with a margin over 3.0 are accepted on this 
basis. 
                                                   
3
 In our scoring, we use the ACE keys and only score 
mentions which appear in both the key and system 
response.  This therefore includes only mentions 
identified as being in the ACE semantic categories by 
both the key and the system response.  Thus these 
scores cannot be directly compared against coreference 
scores involving all noun phrases. 
85
87
89
91
93
95
97
99
0 1 2 3 4 5 6 7 8 9 10 11 12
Threshold
Pr
ec
isi
o
n
(%
)
 
Figure 1: Tradeoff between Margin Threshold and 
name recognition performance 
 
4   Distribution of Name Errors 
We consider now names which did not pass the 
confidence measure tests: names not on the 
common name list, which were tagged with a 
margin below the threshold.  We counted the 
accuracy of these ?obscure? names as a function of 
the number of mentions in an entity; the results are 
shown in Table 2. 
The table shows that the accuracy of name 
recognition increases as the entity includes more 
mentions.  In other words, if a name has more 
coref-ed mentions, it is more likely to be correct. 
This also provides us a linguistic intuition: if 
people mention an obscure name in a text, they 
tend to emphasize it later by repeating the same 
name or describe it with nominal mentions. 
The table also indicates that the accuracy of 
single name entities (singletons) is much lower 
than the overall accuracy.  So, although they 
constitute only about 10% of all names, increasing 
their accuracy can significantly improve overall 
performance.  Coreference information can play a 
great role here.  Take the 157 PER singletons as an 
example; 56% are incorrect names. Among these 
incorrect names, 73% actually belong to the other 
two name types.  Many of these can be easily fixed 
by searching for coreference to other mentions 
without type restriction.  Among the correct names, 
71% can be confirmed by the presence of a title 
word or a Chinese last name.  From these 
observations we can conclude that without strong 
confirmation features, singletons are much less 
likely to be correct names. 
5 Incorporating Coreference Information 
into Name Recognition 
We make use of several features of the 
coreference relations a name is involved in; the 
features are listed in Table 3.  Using these features, 
we built an independent classifier to predict if a 
name identified by the baseline name tagger is 
correct or not. (Note that this classifier is trained 
on all name mentions, but during test only 
?obscure? names which failed the tests in section 3 
are processed by this classifier.) Each name 
corresponds to a feature vector which consists of 
the factors described in Table 3.  The PER context 
words are generated from the context patterns 
described in  (Ji and Luo, 2001).  We used a 
Support Vector Machine to implement the 
classifier, because of its state-of-the-art 
performance and good generalization ability.  We 
used a polynomial kernel of degree 3. 
6 Name Rules based on Coreference 
Besides the factors in the above statistical model, 
additional coreference information can be used to 
filter and in some cases correct the tagging 
produced by the HMM.  We developed the 
following rules to correct names generated by the 
baseline tagger. 
6.1 Name Structure Errors 
Sometimes the Name tagger outputs names 
which are too short (incomplete) or too long.  We 
can make use of the relation among mentions in 
the same entity to fix them.  For example, nested 
ORGs are traditionally difficult to recognize 
correctly.  Errors in ORG names can take the 
following forms: 
 
(1) Head Missed. Examples: ????????/ 
Chinese Art (Group)?, ????????/ Chinese 
Student (Union)?, ??????????/ Russian 
Nuclear Power (Instituition)? 
 
Rule 1: If an ORG name x is coref-ed with other 
mentions with head y (an ORG suffix), and in the 
original text x is immediately followed by y, then 
tag xy instead of x; otherwise discard x. 
 
(2) Modifier Missed. Rule 1 can also be used to 
restore missed modifiers. For example, ????
???? / (Edinburgh) University?; ??????
??? / (Peng Cheng) Limited Corporation?, and 
some incomplete translated PER names such as 
??????? / (Pa)lestine?. 
 
(3) Name Too Long 
Rule 2: If a name x has no coref-ed mentions 
but part of it, x', is identical to a name in another 
entity y, and y includes at least two mentions; then 
tag x' instead of x. 
.
 Rule Type Rule Description 
Ident(i, j) Mentioni and Mentionj are identical 
Abbrev(i, j) Mentioni is an abbreviation of Mentionj 
Modifier(i, j) Mentionj = Modifier + ?de? + Mentioni 
 
 
All 
Formal(i, j) Formal and informal ways of referring to the same entity 
(Ex. ?????? / American Defense Dept. &  
????/ Pentagon?) 
Substring(i, j) Mentioni is a substring of Mentionj   
PER Title(i, j) Mentionj = Mentioni + title word; or 
Mentionj = LastName + title word 
ORG Head(i, j) Mentioni and Mentionj have the same head 
Head(i, j) Mentioni and Mentionj have the same head 
Capital(i, j) Mentioni: country name; 
Mentionj: name of the capital of this country 
Applied in restricted context. 
 
 
 
 
 
 
 
 
Name & 
Name 
 
 
 
GPE 
Country(i, j) Mentioni and Mentionj are different names referring to the same 
country.  
(Ex. ??? / China & ?? / Huaxia & ??? / Republic?) 
RSub(i, j) Namei is a right substring of Nominalj 
Apposition(i, j) Nominalj is the apposite of Namei 
 
All 
Modifier2(i, j) Nominalj = Determiner/Modifier + Namei/ head 
 
 
Name & 
Nominal 
 
GPE 
Ref(i, j) Nominalj = Namei + GPE Ref Word  
(examples of GPE Ref Word: ??? / Side?, ???/Government?, 
???? / Republic?, ?????/ Municipality?) 
IdentN(i, j) Nominali and Nominalj are identical Nominal& 
Nominal 
All 
Modifier3(i, j) Nominalj = Determiner/Modifier + Nominali 
 
Table1: Main rules used in the Coreference Resolver 
 
Number of mentions 
per entity 
Name Type 
1 
 
2 3 4 5 6 7 8 >8 
PER 43.94 87.07 91.23 87.95 91.57 91.92 94.74 92.31 97.36 
GPE 55.81 88.8 96.07 100 100 100 100 95.83 97.46 
ORG 64.71 80.59 89.47 94.29 100 100 -- -- 100 
 
Table 2 Accuracy(%) of ?obscure? name recognition 
 
 Factor Description 
Coreference Type 
Weight 
Average of weights of coreference relations for which this mention 
is antecedent:  0.8 for name-name coreference; 0.5 for apposition;  
0.3 for other name-nominal coreference 
First 
Mention 
Is first name mention in the entity 
Head Includes head word of name 
Idiom Name is part of an idiom 
PER context For PER Name, has context word in text 
PER title For PER Name, includes title word 
 
 
Mention 
Weight 
ORG suffix For ORG Name, includes suffix word 
Entity Weight Number of mentions in entity / total number of mentions in all 
entities in document which include a name mention  
 
Table 3 Coreference factors for name recognition 
6.2 Name Type Errors 
Some names are mistakenly recognized as other 
name types.  For example, the name tagger has 
difficulty in distinguishing transliterated PER 
name and transliterated GPE names. 
To solve this problem we designed the 
following rules based on the relation among 
entities. 
Rule 3: If namei is recognized as type1, the 
entity it belongs to has only one mention; and 
namej is recognized as type2, the entity it belongs 
to has at least two mentions; and namei is identical 
with namej or namei is a substring of namej, then 
correct type1 to type2. 
 
For example, if ????? / Kremlin? is 
mistakenly identified as PER, while ?????? 
/ Kremlin Palace? is correctly identified as ORG, 
and in coreference results, ????? / Kremlin? 
belongs to a singleton entity, while ?????? / 
Kremlin Palace? has coref-ed mentions, then we 
correct the type of ????? / Kremlin? to ORG.  
 
Another common mistake gives rise to the 
sequence ?PER+title+PER?, because our name 
tagger uses the title word as an important context 
feature for a person name (either preceding or 
following the title).  But this is an impossible 
structure in Chinese.  We can also use coreference 
information to fix it. 
Rule 4: If ?PER+title+PER? appears in the 
name tagger?s output,  then we discard the PER 
name with lower coref certainty; and check 
whether it is coref-ed to other mentions in a GPE 
entity or ORG entity; if it is, correct the type. 
Using this rule we can correctly identify ?[??
?? / Sri Lanka GPE] ?? / Premier [????
? / Bandaranaike PER]?, instead of ?[???? / 
Sri Lanka PER] ?? / Premier [????? / 
Bandaranaike PER]?. 
 
6.3 Name Abbreviation Errors 
Name abbreviations are difficult to recognize 
correctly due to a lack of training data.  Usually 
people adopt a separate list of abbreviations or 
design separate rules (Sun et al 2002) to identify 
them.  But many wrong abbreviation names might 
be produced.  We find that coreference 
information helps to select abbreviations. 
Rule 5: If an abbreviation name has no coref-ed 
mentions and it is not adjacent to another 
abbreviation (ex. ??/China ? /America?), then 
we discard it. 
 
7 System Flow 
Combining all the methods presented above, the 
flow of our final system is shown in Figure 2: 
 
 
Figure 2  System Flow 
 
8 Experiments 
8.1 Training and Test Data 
For our experiments, we used the Beijing 
University Insititute of Computational Linguistics 
corpus ? 2978 documents from the People?s Daily 
in 1998, one million words with name tags ? and 
the training corpus for  the 2003 ACE evaluation, 
223 documents.  153 of our ACE documents were 
used as our test set. 4   The 153 documents 
contained 1614 names.  Of the system-tagged 
names, 959 were considered ?obscure?:  were not 
on a name list and had a margin below the 
threshold.  These were the names to which the 
rules and classifier were applied.  We ran all the 
following experiments using the MUC scorer.  
                                                   
4
 The test set was divided into two parts, of 95 
documents and 58 documents.  We trained two name 
tagger and classifier models, each time using one part 
of the test set alng with all the other documents, and 
evaluated on the other part of the test set.  The results 
reported here are the combined results for the entire 
test set. 
Input 
 
Name 
tagger 
Nominal 
mention 
tagger 
Coreference 
Resolver 
Coreference 
Rules to fix  name 
errors 
SVM classifier to select 
correct names using 
coreference features 
Output 
8.2 Overall Performance Comparison 
Table 4 shows the performance of the baseline 
system; Table 5 the system with rule-based 
corrections; and Table 6 the system with both 
rules and the SVM classifier. 
 
Name Precision Recall F 
PER 90.9 88.2 89.5 
GPE 82.3 90.8 86.3 
ORG 92.1 91.8 91.9 
ALL 87.8 90.5 89.1 
 
Table 4 Baseline Name Tagger 
 
Name Precision Recall F 
PER 93.3 87.5 90.3 
GPE 83.5 90.4 86.8 
ORG 90.9 92.1 91.5 
ALL 88.5 90.3 89.4 
 
Table 5 Results with Coref Rules Alone 
 
Name Precision Recall F 
PER 95.7 84.4 89.7 
GPE 88.0 91.7 89.8 
ORG 94.5 91.2 92.8 
ALL 92.2 89.6 90.9 
 
Table 6 Results for Single Document System 
 
The gains we observed from coreference within 
single documents suggested that further 
improvement might be possible by gathering 
evidence from several related documents. 5  We 
did this in two stages.  First, we clustered the 153 
documents in the test set into 38 topical clusters.  
Most (29) of the clusters had only two documents;  
the largest had 28 documents.  We then applied 
the same procedures, treating the entire cluster as 
a single document.  This yielded another 1.0% 
improvement in overall F score (Table 7). 
The improvement in F score was consistent for 
the larger clusters (3 or more documents):  the F 
score improved for 8 of those clusters and 
remained the same for the 9th.  To heighten the 
multi-document benefit, we took 11 of the small 
                                                   
5
 Borthwick (1999) did use some cross-document 
information across the entire test corpus, maintaining 
in effect a name cache for the corpus, in addition to one 
for the document.  No attempt was made to select or 
cluster documents. 
(2 document clusters) and enlarged them by 
retrieving related documents from 
sina.com.cn.  In total, we added 52 texts to 
these 11 clusters.  The net result was a further 
improvement of 0.3% in F score (Table 8).6 
 
 
Name Precision Recall F 
PER 93.3 86.8 90.5 
GPE 95.2 90.0 92.5 
ORG 92.9 91.7 92.3 
ALL 93.8 90.1 91.9 
 
Table 7 Results for Mutiple Document System 
 
Name Precision Recall F 
PER 94.7 87.1 90.7 
GPE 95.6 89.6 92.5 
ORG 95.8 90.3 93.0 
ALL 95.4 89.2 92.2 
 
Table 8 Results for Mutiple Document System 
               with additional retrieved texts 
 
8.3 Contribution of Coreference Features 
Since feature selection is crucial to SVMs, we 
did experiments to determine how precision 
increased as each feature was added.  The results 
are shown in Figure 3.  We can see that each 
feature in the SVM helps to select correct names 
from the output of the baseline name tagger, 
although some (like FirstMention) are more 
crucial than others. 
 
87
88
89
90
91
92
93
B as
elin
e
Edg
eT
yp e
Fir
stM
en
tion He
ad
Id io
m
Per
C o
nte
x t
Per
Tit
le
Org
Suf
fix
Ent
ityW
eig
ht
Feature
Pr
ec
is
io
n
(%
)
 
Figure 3  Contributions of features 
 
                                                   
6
 Scores are still computed on the 153 test 
documents ;  the retrieved documents are excluded 
from the scoring. 
8.4 Comparison to Cache Model 
Some named entity systems use a name cache, 
in which tokens or complete names which have 
been previously assigned a tag are available as 
features in tagging the remainder of a document.  
Other systems have made a second tagging pass 
which uses information on token sequences 
tagged in the first pass (Borthwick 1999), or have 
used as features information about features 
assigned to other instances of the same token 
(Chieu and Ng 2002).  Our system, while more 
complex, makes use of a richer set of global 
features, involving the detailed structure of 
individual mentions, and in particular makes use 
of both name ? name and name ? nominal 
relations. 
 
We have compared the performance of our 
method (applied to single documents) with a 
voted cache model, which takes into account the 
number of times a particular name has been 
previously assigned each type of tag: 
 
System Precision Recall F 
baseline 88.8 90.5 89.1 
 voted cache 87.6 92.8 90.1 
current 92.2 89.6 90.9 
 
Table 9.  Comparison with voted cache 
 
Compared to a simple voted cache model, our 
model provides a greater improvement in name 
recognition F score; in particular, it can 
substantially increase the precision of name 
recognition.  The voted cache model can recover 
some missed names, but at some loss in precision. 
9 Conclusions and Future Work 
In this paper, we presented a novel idea of 
applying coreference information to improve 
name recognition.  We used both a statistical filter 
based on a set of coreference features and rules 
for correcting specific errors in name recognition.  
Overall, we obtained an absolute improvement of 
3.1% in F score.  Put another way, we were able 
to eliminate about 60% of erroneous name tags 
with only a small loss in recall. 
The methods were tested on a Chinese name 
tagger, but most of the techniques should be 
applicable to other languages.  More generally, it 
offers an example of using global and cross-
document information to improve local decisions 
for information extraction.  Such methods will be 
important for breaking the ?performance ceiling? 
in many areas of information extraction. 
In the future, we plan to experiment with 
improvements in coreference resolution (in 
particular, adding pronoun resolution) to see if we 
can obtain further gains in name recognition.  We 
also intend to explore the production of multiple 
tagging hypotheses by our statistical name tagger, 
with the alternative hypotheses then reranked 
using global information.  This may allow us to 
replace some of our hand-coded error-correction 
rules with corpus-trained methods. 
10 Acknowledgements 
This research was supported by the Defense 
Advanced Research Projects Agency as part of the 
Translingual Information Detection, Extraction 
and Summarization (TIDES) program, under 
Grant N66001-001-1-8917 from the Space and 
Naval Warfare Systems Center San Diego, and by 
the National Science Foundation under Grants 
IIS-0081962 and 0325657.  This paper does not 
necessarily reflect the position or the policy of the 
U.S. Government. 
References  
Daniel M. Bikel, Scott Miller, Richard Schuartz, 
and Ralph Weischedel. 1999. Nymble: a high-
performance Learning Name-finder.  Proc. Fifth 
Conf. On Applied Natural Language Processing, 
Washington, D.C. 
Andrew Borthwick.  1999. A Maximum Entropy 
Approach to Named Entity Recognition.  Ph.D. 
Dissertation, Dept. of Computer Science, New 
York University. 
Andrew Borthwick, John Sterling, Eugene 
Agichtein, and Ralph Grishman. 1998. 
Exploiting Diverse Knowledge Sources via 
Maximum Entropy in Named Entity 
Recognition. Proc. Sixth Workshop on Very 
Large Corpora, Montreal, Canada. 
Hai Leong Chieu and Hwee Tou Ng. 2002.  
Named Entity Recognition: A Maximum 
Entropy Approach Using Global Information.  
Proc.: 17th Int?l Conf. on Computational 
Linguistics (COLING 2002), Taipei, Taiwan. 
Ralph Grishman and Beth Sundheim.  1996. 
Message understanding conference - 6: A brief 
history. Proc. 16th Int?l Conference on 
Computational Linguistics (COLING 96), 
Copenhagen. 
Heng Ji, Zhensheng Luo, 2001. A Chinese Name 
Identifying System Based on Inverse Name 
Frequency Model and Rules. Natural Language 
Processing and Knowledge Engineering 
(NLPKE) Mini Symposium of 2001 IEEE 
International Conference on Systems, Man, and 
Cybernetics (SMC2001) 
Andrew McCallum and Wei Li.  2003.  Early 
results for Named Entity Recognition With 
Conditional Random Fields, Feature Induction, 
and Web-Enhanced Lexicons. Proc. Seventh 
Conf. on Computational Natural Language 
Learning (CONLL-2003), Edmonton, Canada. 
Tobias Scheffer, Christian Decomain, and Stefan 
Wrobel. 2001. Active Hidden Markov Models 
for Information Extraction. Proc. Int?l 
Symposium on Intelligent Data Analysis (IDA-
2001). 
Satoshi Sekine, Ralph Grishman and Hiroyuki 
Shinnou. 1998. A Decision Tree Method for 
Finding and Classifying Names in Japanese 
Texts.  Proc. Sixth Workshop on Very Large 
Corpora; Montreal, Canada. 
Jian Sun, Jianfeng Gao, Lei Zhang, Ming Zhou 
and Changning Huang. 2002. Chinese Named 
Entity Identification Using Class-based 
Language Model.  Coling 2002. 
Marc Vilain, John Burger, John Aberdeen, Dennis 
Connelly, Lynette Hirschman. 1995. A model --
Theoretic Coreference Scoring Scheme. MUC-6 
Proceedings, Nov. 1995. 
Shiren Ye, Tat-Seng Chua, Liu Jimin. 2002. An 
Agent-based Approach to Chinese Named 
Entity Recognition. Coling 2002. 
The NomBank Project: An Interim Report
Adam Meyers, Ruth Reeves, Catherine Macleod, Rachel Szekely,
Veronika Zielinska, Brian Young and Ralph Grishman
New York University
meyers/reevesr/macleod/szekely/zielinsk/byoung/grishman@cs.nyu.edu
Abstract
This paper describes NomBank, a project that
will provide argument structure for instances of
common nouns in the Penn Treebank II corpus.
NomBank is part of a larger effort to add ad-
ditional layers of annotation to the Penn Tree-
bank II corpus. The University of Pennsylva-
nia?s PropBank, NomBank and other annota-
tion projects taken together should lead to the
creation of better tools for the automatic analy-
sis of text. This paper describes the NomBank
project in detail including its specifications and
the process involved in creating the resource.
1 Introduction
This paper introduces the NomBank project. When com-
plete, NomBank will provide argument structure for in-
stances of about 5000 common nouns in the Penn Tree-
bank II corpus. NomBank is part of a larger effort to
add layers of annotation to the Penn Treebank II cor-
pus. PropBank (Kingsbury et al, 2002; Kingsbury and
Palmer, 2002; University of Pennsylvania, 2002), Nom-
Bank and other annotation projects taken together should
lead to the creation of better tools for the automatic anal-
ysis of text. These annotation projects may be viewed
as part of what we think of as an a la carte strategy for
corpus-based natural language processing. The fragile
and inaccurate multistage parsers of a few decades were
replaced by treebank-based parsers, which had better per-
formance, but typically provided more shallow analyses.1
As the same set of data is annotated with more and more
levels of annotation, a new type of multistage processing
becomes possible that could reintroduce this information,
1A treebank-based parser output is defined by the treebank
on which it is based. As these treebanks tend to be of a fairly
shallow syntactic nature, the resulting parsers tend to be so also.
but in a more robust fashion. Each stage of processing
is defined by a body of annotated data which provides a
symbolic framework for that level of representation. Re-
searchers are free to create and use programs that map
between any two levels of representation, or which map
from bare sentences to any level of representation.2 Fur-
thermore, users are free to shop around among the avail-
able programs to map from one stage to another. The
hope is that the standardization imposed by the anno-
tated data will insure that many researchers will be work-
ing within the same set of frameworks, so that one re-
searcher?s success will have a greater chance of benefit-
ing the whole community.
Whether or not one adapts an a la carte approach,
NomBank and PropBank projects provide users with data
to recognize regularizations of lexically and syntactically
related sentence structures. For example, suppose one has
an Information Extraction System tuned to a hiring/firing
scenario (MUC, 1995). One could use NomBank and
PropBank to generalize patterns so that one pattern would
do the work of several. Given a pattern stating that the ob-
ject (ARG1) of appoint is John and the subject (ARG0)
is IBM, a PropBank/NomBank enlightened system could
detect that IBM hired John from the following strings:
IBM appointed John, John was appointed by IBM, IBM?s
appointment of John, the appointment of John by IBM
and John is the current IBM appointee. Systems that do
not regularize across predicates would require separate
patterns for each of these environments.
The NomBank project went through several stages be-
fore annotation could begin. We had to create specifica-
tions and various lexical resources to delineate the task.
Once the task was set, we identified classes of words. We
used these classes to approximate lexical entries, make
time estimates and create automatic procedures to aid in
2Here, we use the term ?level of representation? quite
loosely to include individual components of what might con-
ventionally be considered a single level.
1. Her gift of a book to John [NOM]
REL = gift, ARG0 = her, ARG1 = a book, ARG2 =
to John
2. his promise to make the trains run on time [NOM]
REL = promise, ARG0 = his, ARG2-PRD = to make
the trains run on time
3. her husband [DEFREL RELATIONAL NOUN]
REL = husband, ARG0 = husband, ARG1 = her
4. a set of tasks [PARTITIVE NOUN]
REL = set, ARG1 = of tasks
5. The judge made demands on his staff [NOM
w/SUPPORT]
REL = demands, SUPPORT = made, ARG0 = The
judge, ARG2 = on his staff
6. A savings institution needs your help [NOM
w/SUPPORT]
REL = help, SUPPORT = needs, ARG0 = your,
ARG2 = A savings institution
7. 12% growth in dividends next year [NOM
W/ARGMs]
REL = growth, ARG1 = in dividends, ARG2-EXT
= 12%, ARGM-TMP = next year
8. a possible U.S. troop reduction in South Ko-
rea[NOM W/ARGMs]
REL = reduction, ARG1 = U.S. troop, ARGM-LOC
= in South Korea, ARGM-ADV = possible
Figure 1: Sample NomBank Propositions
annotation. For the first nine months of the project, the
NomBank staff consisted of one supervisor and one anno-
tator. Once the specifications were nailed down, we hired
additional annotators to complete the project. This pa-
per provides an overview of the project including an ab-
breviated version of the specifications (the full version is
obtainable upon request) and a chronicle of our progress.
2 The Specifications
Figure 1 lists some sample NomBank propositions along
with the class of the noun predicate (NOM stands for
nominalization, DEFREL is a type of relational noun).
For each ?markable? instance of a common noun in the
Penn Treebank, annotators create a ?proposition?, a sub-
set of the features   REL, SUPPORT, ARG0, ARG1,
ARG2, ARG3, ARG4, ARGM  paired with pointers to
phrases in Penn Treebank II trees. A noun instance is
markable if it is accompanied by one of its arguments
(ARG0, ARG1, ARG2, ARG3, ARG4) or if it is a nomi-
nalization (or similar word) and it is accompanied by one
of the allowable types of adjuncts (ARGM-TMP, ARGM-
LOC, ARGM-ADV, ARGM-EXT, etc.) ? the same set of
adjuncts used in PropBank.3
The basic idea is that each triple   REL, SENSE,
ARGNUM  uniquely defines an argument, given a par-
ticular sense of a particular REL (or predicate), where
ARGNUM is one of the numbered arguments (ARG0,
ARG1, ARG2, ARG3, ARG4) and SENSE is one of the
senses of that REL. The arguments are essentially the
same as the initial relations of Relational Grammar (Perl-
mutter and Postal, 1984; Rosen, 1984). For example,
agents tend to be classified as ARG0 (RG?s initial sub-
ject), patients and themes tend to be classified as ARG1
(RG?s initial object) and indirect objects of all kinds tend
to be classified as ARG2.
The lexical entry or frame for each noun provides
one inventory of argument labels for each sense of that
word.4 Each proposition (cf. figure 1) consists of an in-
stance of an argument-taking noun (REL) plus arguments
(ARG0, ARG1, ARG2,  ), SUPPORT items and/or ad-
juncts (ARGM). SUPPORT items are words that link ar-
guments that occur outside an NP to the nominal predi-
cate that heads that NP, e.g., ?made? SUPPORTS ?We?
as the ARG0 of decision in We made a decision. ARGMs
are adjuncts of the noun. However, we only mark the
sort of adjuncts that also occur in sentences: locations
(ARGM-LOC), temporal (ARGM-TMP), sentence ad-
verbial (ARGM-ADV) and various others.
3 Lexical Entries and Noun Classes
Before we could begin annotation, we needed to classify
all the common nouns in the corpus. We needed to know
which nouns were markable and make initial approxima-
tions of the inventories of senses and arguments for each
noun. Toward this end, we pooled a number of resources:
COMLEX Syntax (Macleod et al, 1998a), NOMLEX
(Macleod et al, 1998b) and the verb classes from (Levin,
1993). We also used string matching techniques and hand
classification in combination with programs that automat-
ically merge crucial features of these resources. The re-
sult was NOMLEX-PLUS, a NOMLEX-style dictionary,
which includes the original 1000 entries in NOMLEX
plus 6000 additional entries (Meyers et al, 2004). The re-
sulting noun classes include verbal nominalizations (e.g.,
destruction, knowledge, believer, recipient), adjectival
nominalizations (ability, bitterness), and 16 other classes
such as relational (father, president) and partitive nouns
(set, variety). NOMLEX-PLUS helped us break down
3To make our examples more readable, we have replaced
pointers to the corpus with the corresponding strings of words.
4For a particular noun instance, only a subset of these argu-
ments may appear, e.g., the ARG2 (indirect object) to Dorothy
can be left out of the phrase Glinda?s gift of the slippers.
the nouns into classes, which in turn helped us gain an
understanding of the difficulty of the task and the man-
power needed to complete the task.
We used a combination of NOMLEX-PLUS and Prop-
Bank?s lexical entries (or frames) to produce automatic
approximations of noun frames for NomBank. These en-
tries specify the inventory of argument roles for the an-
notators. For nominalizations of verbs that were covered
in PropBank, we used straightforward procedures to con-
vert existing PropBank lexical entries to nominal ones.
However, other entries needed to be created by automatic
means, by hand or by a combination of the two. Figure 2
compares the PropBank lexical entry for the verb claim
with the NomBank entry for the noun claim. The noun
claim and the verb claim share both the ASSERT sense
and the SEIZE sense, permitting the same set of argu-
ment roles for those senses. However, only the ASSERT
sense is actually attested in the sample PropBank corpus
that was available when we began working on NomBank.
Thus we added the SEIZE sense to both the noun and
verb entries. The noun claim also has a LAWSUIT sense
which bears an entry similar to the verb sue. Thus our
initial entry for the noun claim was a copy of the verb en-
try at that time. An annotator edited the frames to reflect
noun usage ? she added the second and third senses to
the noun frame and updated the verb frame to include the
second sense.
In NOMLEX-PLUS, we marked anniversary and ad-
vantage as ?cousins? of nominalizations indicating that
their lexical entries should be modeled respectively on
the verbs commemorate and exploit, although both en-
tries needed to be modified in some respect. We use the
term ?cousins? of nominalizations to refer to those nouns
which take argument structure similar to some verb (or
adjective), but which are not morphologically related to
that word. Examples are provided in Figure 3 and 4. For
adjective nominalizations, we began with simple proce-
dures which created frames based on NOMLEX-PLUS
entries (which include whether the subject is +/-sentient).
The entry for ?accuracy? (the nominalization of the ad-
jective accurate) plus a simple example is provided in fig-
ure 5 ? the ATTRIBUTE-LIKE frame is one of the most
common frames for adjective nominalizations. To cover
the remaining nouns in the corpus, we created classes
of lexical items and manually constructed one frame for
each class. Each member of a class was was given the
corresponding frame. Figure 6 provides a sample of these
classes, along with descriptions of their frames. As with
the nominalization cousins, annotators sometimes had to
adjust these frames for particular words.
4 A Merged Representation
Beginning with the PropBank and NomBank propo-
sitions in Figure 7, it is straight-forward to derive the
1. ASSERT Sense
Roles: ARG0 = AGENT, ARG1 = TOPIC
Noun Example: Her claim that Fred can y
REL = claim, ARG0 = her, ARG1 = that Fred
can fly
Verb Example: She claimed that Fred can y
REL = claimed, ARG0 = She, ARG1 = that
Fred can fly
2. SEIZE Sense
Roles: ARG0 = CLAIMER, ARG1 = PROPERTY,
ARG2 = BENEFICIARY
Noun Example: He laid claim to Mexico for Spain
REL = claim, SUPPORT = laid, ARG0 = He,
ARG1 = to Mexico, ARG2 = for Spain
Verb Example: He claimed Mexico for Spain
REL = claim, ARG0 = He, ARG1 = Mexico,
ARG2 = for Spain
3. SUE Sense
Roles: ARG0 = CLAIMANT, ARG1 = PURPOSE,
ARG2 = DEFENDANT, ARG3 = AWARD
Noun Example: His $1M abuse claim against Dan
ARG0 = His, ARG1 = abuse, ARG2 = against
Dan, ARG3 = $1M
Verb Example: NOT A VERB SENSE
Figure 2: Verb and Noun Senses of claim
1. HONOR (based on a sense of commemorate)
Roles: ARG0 = agent, ARG1 = thing remembered,
ARG2 = times celebrated
Noun Example: Investors celebrated the second
anniversary of Black Monday.
REL = anniversary, SUPPORT = celebrated,
ARG0 = Investors, ARG1 = of Black Monday,
ARG2 = second
Figure 3: One sense for anniversary
1. EXPLOIT
Roles: ARG0 = exploiter, ARG1 = entity exploited
Noun Example: Investors took advantage of Tues-
day ?s stock rally.
REL = advantage, SUPPORT = took, ARG0 =
Investors, ARG1 = of Tuesday?s stock rally
Figure 4: One sense for advantage
1. ATTRIBUTE-LIKE
Roles: ARG1 = theme
Noun Example: the accuracy of seasonal adjust-
ments built into the employment data
REL = accuracy, ARG1 = of seasonal adjust-
ments built into 
Figure 5: One Sense for accuracy
ACTREL Relational Nouns with beneficiaries
Roles: ARG0 = JOB HOLDER, ARG1 = THEME,
ARG2 = BENEFICIARY
Example: ACME will gain printing customers
REL = customers, SUPPORT = gain, ARG0 =
customers, ARG1 = printing, ARG2 = ACME
DEFREL Relational Nouns for personal relationships
Roles: ARG0 = RELATION HOLDER, ARG1 =
RELATION RECEPTOR
Example: public enemies REL = enemies, ARG0
= enemies, ARG1 = public
ATTRIBUTE Nouns representing attribute relations
Roles: ARG1 = THEME, ARG2 = VALUE
Example: a lower grade of gold
REL = grade, ARG1 = of gold, ARG2 = lower
ABILITY-WITH-AGENT Ability-like nouns
Roles: ARG0 = agent, ARG1 = action
Example: the electrical current-carrying capacity
of new superconductor crystals
REL = capacity, ARG0 = of new superconduc-
tor crystals, ARG1 = electrical current-carrying
ENVIRONMENT Roles: ARG1 = THEME
Example: the circumstances of his departure
REL = circumstances, ARG1 = of his departure
Figure 6: Frames for Classes of Nouns
PropBank: REL = gave, ARG0 = they, ARG1 = a
standing ovation, ARG2 = the chefs
NomBank: REL = ovation, ARG0 = they, ARG1 = the
chefs, SUPPORT = gave
Figure 7: They gave the chefs a standing ovation
gave
chefsthe
a ovationstanding
They
S
REL
NP
NP
SUPPORT
NP
ARG1
ARG1
ARG2
ARG0
ARG0
REL
Figure 8: They gave the chefs a standing ovation
combined PropBank/NomBank graphical representation
in Figure 8 in which each role corresponds to an arc la-
bel. For this example, think of the argument structure of
the noun ovation as analogous to the verb applaud. Ac-
cording to our analysis, they are both the givers and the
applauders and the chefs are both the recipients of some-
thing given and the ones who are applauded. Gave and
ovation have two distinct directional relations: a stand-
ing ovation is something that is given and gave serves as
a link between ovation and its two arguments. This dia-
gram demonstrates how NomBank is being designed for
easy integration with PropBank. We believe that this is
the sort of predicate argument representation that will be
needed to easily merge this work with other annotation
efforts.
5 Analysis of the Task
As of this writing we have created the various lexicons
associated with NomBank. This has allowed us to break
down the task as follows:
 There are approximately 240,000 instances of com-
mon nouns in the PTB (approximately one out of
every 5 words).
 At least 36,000 of these are nouns that cannot take
arguments and therefore need not be looked at by an
annotator.
 There are approximately 99,000 instances of verbal
nominalizations or related items (e.g., cousins)
 There are approximately 34,000 partitives (includ-
ing 6,000 instances of the percent sign), 18,000 sub-
ject nominalizations, 14,000 environmental nouns,
14,000 relational nouns and fewer instances of the
various other classes.
 Approximately 1/6 of the cases are instances of
nouns which occur in multiple classes.5
The difficulty of the annotation runs the gamut from
nominalization instances which include the most argu-
ments, the most adjuncts and the most instances of sup-
port to the partitives, which have the simplest and most
predictable structure.
6 Error Analysis and Error Detection
We have conducted some preliminary consistency tests
for about 500 instances of verbal nominalizations dur-
ing the training phases of NomBank. These tests yielded
inter-annotator agreement rates of about 85% for argu-
ment roles and lower for adjunct roles. We are currently
engaging in an effort to improve these results.6
We have identified certain main areas of disagreement
including: disagreements concerning SUPPORT verbs
and the shared arguments that go with them; disagree-
ments about role assignment to prenominals; and differ-
ences between annotators caused by errors (typos, slips
of the mouse, ill-formed output, etc.) In addition to im-
proving our specifications and annotator help texts, we
are beginning to employ some automatic means for error
detection.
6.1 Support
For inconsistencies with SUPPORT, our main line of at-
tack has been to outline problems and solutions in our
specifications. We do not have any automatic system in
effect yet, although we may in the near future.
SUPPORT verbs (Gross, 1981; Gross, 1982; Mel?c?uk,
1988; Mel?c?uk, 1996; Fontenelle, 1997) are verbs which
5When a noun fits into multiple categories, those categories
may predict multiple senses, but not necessarily. For example,
drive has a nominalization sense (He went for a drive) and an
attribute sense (She has a lot of drive). Thus the lexical entry
for drive includes both senses. In constrast, teacher in the math
teacher has the same analysis regardless of whether one thinks
of it as the nominalization of teach or as a relational (ACTREL)
noun.
6Consistency is the average precision and recall against a
gold standard. The preliminary tests were conducted during
training, and only on verbal nominalizations.
connect nouns to one (or more) of their arguments via ar-
gument sharing. For example, in John took a walk, the
verb took ?shares? its subject with the noun walk. SUP-
PORT verbs can be problematic for a number of reasons.
First of all the concept of argument sharing is not black
and white. To illustrate these shades of gray, compare
the relation of Mary to attack in: Mary?s attack against
the alligator, Mary launched an attack against the alliga-
tor, Mary participated in an attack against the alligator,
Mary planned an attack against the alligator and Mary
considered an attack against the alligator. In each subse-
quent example, Mary?s ?level of agency? decreases with
respect to the noun attack. However, in each case Mary
may still be viewed as some sort of potential attacker. It
turned out that the most consistent position for us to take
was to assume all degrees of argument-hood (in this case
subject-hood) were valid. So, we would mark Mary as the
ARG0 of attack in all these instances. This is consistent
with the way control and raising structures are marked
for verbs, e.g., John is the subject of leave and do in John
did not seem to leave and John helped do the project un-
der most accounts of verbal argument structure that take
argument sharing (control, raising, etc.) into account.
Of course a liberal view of SUPPORT has the danger
of overgeneration. Consider for example, Market con-
ditions led to the cancellation of the planned exchange.
The unwary annotator might assume that market condi-
tions is the ARG0 (or subject) of cancellation. In fact,
the combination lead to and cancellation do not have any
of the typical features of SUPPORT described in figure 9.
However, the final piece of evidence is that market con-
ditions violate the selection restrictions of cancellation.
Thus the following paraphrase is ill-formed *Market con-
ditions canceled the planned exchange. This suggests
that market conditions is the subject of lead and not the
subject of cancellation. Therefore, this is not an instance
of support in spite of the apparent similarity.
We require that the SUPPORT relation be lexical. In
other words, there must be something special about a
SUPPORT verb or the combination of the SUPPORT
verb and the noun to license the argument sharing rela-
tion. In addition to SUPPORT, we have cataloged several
argument sharing phenomena which are markable. For
example, consider the sentence, President Bush arrived
for a celebration. Clearly, President Bush is the ARG0
of celebration (one of the people celebrating). However,
arrive is not a SUPPORT verb. The phrase for a cele-
bration is a subject-oriented adverbial, similar to adverbs
like willingly, which takes the subject of the sentence as
an argument. Thus President Bush could also be the sub-
ject of celebration in President Bush waddled into town
for the celebration and many similar sentences that con-
tain this PP.
Finally, there are cases where argument sharing may
 Support verb/noun pairs can be idiosyncratically
connected to the point that some researchers would
call them idioms or phrasal verbs, e.g., take a walk,
keep tabs on.
 The verb can be essentially ?empty?, e.g., make an
attack, have a visit.
 The ?verb/noun? combination may take a different
set of arguments than either does alone, e.g., take
advantage of.
 Some support verbs share the subject of almost any
nominalization in a particular argument slot. For ex-
ample attempt shares its subject with most follow-
ing nominalizations, e.g., He attempted an attack.
These are the a lot like raising/control predicates.
 In some cases, the support verb and noun are from
similar semantic classes, making argument sharing
very likely, e.g., fight a battle.
Figure 9: Possible Features of Support
be implied by discourse processes, but which we do
not mark (as we are only handling sentence-level phe-
nomena). For example, the words proponent and rival
strongly imply that certain arguments appear in the dis-
course, but not necessarily in the same sentence. For ex-
ample in They didn?t want the company to fall into the
hands of a rival, there is an implication that the company
is an ARG1 of rival, i.e., a rival should be interpreted as
a rival of the company.7 The connection between a rival
and the company is called a ?bridging? relation (a pro-
cess akin to coreference, cf. (Poesio and Vieira, 1998))
In other words, fall into the hands of does not link ?ri-
val? with the company by means of SUPPORT. The fact
that a discourse relation is responsible for this connection
becomes evident when you see that the link between ri-
val and company can cross sentence boundaries, e.g., The
company was losing money. This was because a rival had
come up with a really clever marketing strategy.
6.2 Prenominal Adjectives and Error Detection
ARGM is the annotation tag used for nonarguments, also
known as adjuncts. For nouns, it was decided to only tag
such types of adjuncts as are also found with verbs, e.g.,
temporal, locative, manner, etc. The rationale for this in-
cluded: (1) only the argument-taking common nouns are
being annotated and other sorts of adjuncts occur with
common nouns in general; (2) narrowing the list of po-
tential labels helped keep the labeling consistent; and (3)
this was the minimum set of adjuncts that would keep the
7The noun rival is a subject nominalization of the verb rival.
noun annotation consistent with the verb annotation.
Unfortunately, it was not always clear whether a
prenominal modifier (particularly an adjective) fell into
one of our classes or not. If an annotator felt that a modi-
fier was somehow ?important?, there was a temptation to
push it into one of the modifier classes even if it was not
a perfect fit. Furthermore, some annotators had a broader
view than others as to the sorts of semantic relationships
that fell within particular classes of adjuncts, particularly
locative (LOC), manner (MNR) and extent (EXT). Un-
like the SUPPORT verbs, which are often idiosyncratic to
particular nominal predicates, adjunct prenominal modi-
fiers usually behave the same way regardless of the noun
with which they occur.
In order to identify these lexical properties of prenom-
inals, we created a list of all time nouns from COMLEX
Syntax (ntime1 and ntime2) and we created a specialized
dictionary of adjectives with adverbial properties which
we call ADJADV. The list of adjective/adverb pairs in
ADJADV came from two sources: (1) a list of adjec-
tives that are morphologically linked to -ly adverbs cre-
ated using some string matching techniques; and (2) ad-
jective/adverb pairs from CATVAR (Habash and Dorr,
2003). We pruned this list to only include adjectives
found in the Penn Treebank and then edited out inappro-
priate word pairs. We completed the dictionary by trans-
ferring portions of the COMLEX Syntax adverb entries
to the corresponding adjectives.
We now use ADJADV and our list of temporal nouns
to evaluate NOMBANK annotation of modifiers. Each
annotated left modifier is compared against our dictio-
naries. If a modifier is a temporal noun, it can bear the
ARGM-TMP role (temporal adjunct role), e.g., the tem-
poral noun morning can fill the ARGM-TMP slot in the
morning broadcast. Most other common nouns are com-
patible with argument role slots (ARG0, ARG1, etc.),
e.g., the noun news can fill the ARG1 slot in the news
broadcast. Finally, roles associated with adjectives de-
pend on their ADJADV entry, e.g., possible can be an
ARGM-ADV in possible broadcasts due to the epistemic
feature encoded in the lexical entry for possible (derived
from the corresponding adjverb possibly). Discrepancies
between these procedures and the annotator are resolved
on a case by case basis. If the dictionary is wrong, the
dictionary should be changed, e.g., root, as in root cause
was added to the dictionary as a potential MNR adjective
with a meaning like the adverb basically. However, if
the annotator is wrong, the annotation should be changed,
e.g., if an annotator marked ?slow? as a ARGM-TMP, the
program would let them know that it should be a ARGM-
MNR. This process both helps with annotation accuracy
and enriches our lexical database.
6.3 Other Automatically Detected Errors
We used other procedures to detect errors including:
Nom-type Argument nominalizations are nominaliza-
tions that play the role of one of the arguments in
the ROLESET. Thus the word acquirer should be
assigned the ARG0 role in the following example
because acquirer is a subject nominalization:
a possible acquirer of Manville
REL = acquirer, ARG0 = acquirer, ARG1 = of
Manville, ARGM-ADV = possible
A procedure can compare the NOMLEX-PLUS en-
try for each noun to each annotated instance of that
noun to check for incompatibilities.
Illformedness Impossible instances are ruled out.
Checks are made to make sure obligatory labels
(REL) are present and illegal labels are not. Simi-
larly, procedures make sure that infinitive arguments
are marked with the -PRD function tag (a PropBank
convention).
Probable Illformedness Certain configurations of role
labels are possible, but very unlikely. For example,
the same argument role should not appear more than
once (the stratal uniqueness condition in Relational
Grammar or the theta criterion in Principles and pa-
rameters, etc.). Furthermore, it is unlikely for the
first word of a sentence to be an argument unless
the main predicate is nearby (within three words) or
unless there is a nearby support verb. Finally, it is
unlikely that there is an empty category that is an
argument of a predicate noun unless the empty cate-
gory is linked to some real NP.8
WRONG-POS We use procedures that are part of our
systems for generating GLARF, a predicate argu-
ment framework discussed in (Meyers et al, 2001a;
Meyers et al, 2001b), to detect incorrect parts of
speech in the Penn Treebank. If an instance is pre-
dicted to be a part of speech other than a common
noun, but it is still tagged, that instance is flagged.
For example, if a word tagged as a singular common
noun is the first word in a VP, it is probably tagged
with the wrong part of speech.
6.4 The Results of Error Detection
The processes described in the previous subsections are
used to create a list of annotation instances to check along
with short standardized descriptions of what was wrong,
e.g., wrong-pos, non-functional (if there were two iden-
tical argument roles), etc. Annotators do a second pass
8Empty categories mark ?invisible? constituents in the Tree-
bank, e.g., the subject of want in John  wanted e  to leave.
PARTITIVE-QUANT
Roles: ARG1 = QUANTIFIED
Example: lots of internal debate
REL = lots, ARG1 = of internal debate
Figure 10: The entry for lot
on just these instances (currently about 5 to 10% of the
total). We will conduct a formal evaluation of this proce-
dure over the next month.
7 Future Research: Automatic Annotation
We are just starting a new phase in this project: the cre-
ation of an automatic annotator. Using techniques similar
to those described in (Meyers et al, 1998) in combina-
tion with our work on GLARF (Meyers et al, 2001a;
Meyers et al, 2001b), we expect to build a hand-coded
PROPBANKER a program designed to produce a Prop-
Bank/NomBank style analysis from Penn Treebank style
input. Although the PropBanker should work with in-
put in the form of either treebank annotation or treebank-
based parser output, this project only requires applica-
tion to the Penn Treebank itself. While previous pro-
grams with similar goals (Gildea and Jurafsky, 2002)
were statistics-based, this tool will be based completely
on hand-coded rules and lexical resources.
Depending on its accuracy, automatically produced an-
notation should be useful as either a preprocessor or as
an error detector. We expect high precision for very sim-
ple frames, e.g., nouns like lot as in figure 10. Annota-
tors will have the opportunity to judge whether particu-
lar automatic annotation is ?good enough? to serve as a
preprocessor. We hypothesize that a comparison of auto-
matic annotation that fails this level of accuracy against
the hand annotation will still be useful for detecting er-
rors. Comparisons between the hand annotated data and
the automatically annotated data will yield a set of in-
stances that warrant further checking along the same lines
as our previously described error checking mechanisms.
8 Summary
This paper outlines our current efforts to produce Nom-
Bank, annotation of the argument structure for most com-
mon nouns in the Penn Treebank II corpus. This is part of
a larger effort to produce more detailed annotation of the
Penn Treebank. Annotation for NomBank is progress-
ing quickly. We began with a single annotator while we
worked on setting the task and have ramped up to four an-
notators. We continue to work on various quality control
procedures which we outline above. In the near future,
we intend to create an automatic annotation program to
be used both as a preprocessor for manual annotation and
as a supplement to error detection.
The argument structure of NPs has been less studied
both in theoretical and computational linguistics, than
the argument structure of verbs. As with our work on
NOMLEX, we are hoping that NomBank will substan-
tially contribute to improving the NLP community?s abil-
ity to understand and process noun argument structure.
Acknowledgments
Nombank is supported under Grant N66001-001-1-8917
from the Space and Naval Warfare Systems Center San
Diego. This paper does not necessarily reflect the posi-
tion or the policy of the U.S. Government.
We would also like to acknowledge the people at the
University of Pennsylvania who helped make NomBank
possible, including, Martha Palmer, Scott Cotton, Paul
Kingsbury and Olga Babko-Malaya. In particular, the use
of PropBank?s annotation tool and frame files proved in-
valuable to our effort.
References
T. Fontenelle. 1997. Turning a bilingual dictionary into
a lexical-semantic database. Lexicographica Series
Maior 79. Max Niemeyer Verlag, Tu?bingen.
D. Gildea and D. Jurafsky. 2002. Automatic Labeling of
Semantic Roles. Computational Linguistics, 28:245?
288.
M. Gross. 1981. Les bases empiriques de la notion de
pre?dicat se?mantique. In A. Guillet and C. Lecl?ere,
editors, Formes Syntaxiques et Pr?edicat S?emantiques,
volume 63 of Langages, pages 7?52. Larousse, Paris.
M. Gross. 1982. Simple Sentences: Discussion of Fred
W. Householder?s Paper ?Analysis, Synthesis and Im-
provisation?. In Text Processing. Text Analysis and
Generation. Text Typology and Attribution. Proceed-
ings of Nobel Symposium 51.
N. Habash and B. Dorr. 2003. CatVar: A Database of
Categorial Variations for English. In Proceedings of
the MT Summit, pages 471?474, New Orleans.
P. Kingsbury and M. Palmer. 2002. From treebank to
propbank. In Proceedings of the 3rd International
Conference on Language Resources and Evaluation
(LREC-2002), Las Palmas, Spain.
P. Kingsbury, M. Palmer, and Mitch Marcus. 2002.
Adding semantic annotation to the penn treebank. In
Proceedings of the Human Language Technology Con-
ference, San Diego, California.
B. Levin. 1993. English Verb Classes and Alterna-
tions: A Preliminary Investigation. The University of
Chicago Press, Chicago.
C. Macleod, R. Grishman, and A. Meyers. 1998a.
COMLEX Syntax. Computers and the Humanities,
31(6):459?481.
C. Macleod, R. Grishman, A. Meyers, L. Barrett, and
R. Reeves. 1998b. Nomlex: A lexicon of nominal-
izations. In Proceedings of Euralex98.
I. A. Mel?c?uk. 1988. Dependency Syntax: Theory and
Practice. State University Press of New York, Albany.
I. A. Mel?c?uk. 1996. Lexical Functions: A Tool for
the Description of Lexical Relations in a Lexicon. In
Lexical Functions in Lexicography and Natural Lan-
guage Processing. John Benjamins Publishing Com-
pany, Amsterdam.
A. Meyers, C. Macleod, R. Yangarber, R. Grishman,
Leslie Barrett, and Ruth Reeves. 1998. Using NOM-
LEX to Produce Nominalization Patterns for Informa-
tion Extraction. In Coling-ACL98 workshop Proceed-
ings: the Computational Treatment of Nominals.
A. Meyers, R. Grishman, M. Kosaka, and S. Zhao.
2001a. Covering Treebanks with GLARF. In
ACL/EACL Workshop on Sharing Tools and Resources
for Research and Education.
A. Meyers, M. Kosaka, S. Sekine, R. Grishman, and
S. Zhao. 2001b. Parsing and GLARFing. In Proceed-
ings of RANLP-2001, Tzigov Chark, Bulgaria.
A. Meyers, R. Reeves, Catherine Macleod, Rachel Szeke-
ley, Veronkia Zielinska, Brian Young, and R. Grish-
man. 2004. The Cross-Breeding of Dictionaries. In
Proceedings of LREC-2004, Lisbon, Portugal. To ap-
pear.
MUC-6. 1995. Proceedings of the Sixth Message Under-
standing Conference. Morgan Kaufman. (MUC-6).
D. M. Perlmutter and P. M. Postal. 1984. The 1-
Advancement Exclusiveness Law. In D. M. Perlmutter
and C. G. Rosen, editors, Studies in Relational Gram-
mar 2. The University of Chicago Press, Chicago.
M. Poesio and R. Vieira. 1998. A Corpus-based Inves-
tigation of Definite Description Use. Computational
Linguistics, 24(2):183?216.
C. G. Rosen. 1984. The Interface between Semantic
Roles and Initial Grammatical Relations. In D.. M.
Perlmutter and C. G. Rosen, editors, Studies in Rela-
tional Grammar 2. The University of Chicago Press,
Chicago.
University of Pennsylvania. 2002. Annotation guidelines
for PropBank. http://www.cis.upenn.edu/
?ace/propbank-guidelines-feb02.pdf.
Proceedings of the Workshop on Information Extraction Beyond The Document, pages 48?55,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Data Selection in Semi-supervised Learning for Name Tagging 
Heng Ji Ralph Grishman 
Department of Computer Science 
New York University 
New York, NY, 10003, USA 
hengji@cs.nyu.edu grishman@cs.nyu.edu 
 
 
 
 
Abstract 
We present two semi-supervised learning 
techniques to improve a state-of-the-art 
multi-lingual name tagger. For English 
and Chinese, the overall system obtains 
1.7% - 2.1% improvement in F-measure, 
representing a 13.5% - 17.4% relative re-
duction in the spurious, missing, and in-
correct tags. We also conclude that 
simply relying upon large corpora is not 
in itself sufficient: we must pay attention 
to unlabeled data selection too. We de-
scribe effective measures to automatically 
select documents and sentences. 
1 Introduction 
When applying machine learning approaches to 
natural language processing tasks, it is time-
consuming and expensive to hand-label the large 
amounts of training data necessary for good per-
formance. Unlabeled data can be collected in 
much larger quantities. Therefore, a natural ques-
tion is whether we can use unlabeled data to 
build a more accurate learner, given the same 
amount of labeled data. This problem is often 
referred to as semi-supervised learning. It signifi-
cantly reduces the effort needed to develop a 
training set. It has shown promise in improving 
the performance of many tasks such as name tag-
ging (Miller et al, 2004), semantic class extrac-
tion (Lin et al, 2003), chunking (Ando and 
Zhang, 2005), coreference resolution (Bean and 
Riloff, 2004) and text classification (Blum and 
Mitchell, 1998).  
However, it is not clear, when semi-supervised 
learning is applied to improve a learner, how the 
system should effectively select unlabeled data, 
and how the size and relevance of data impact the 
performance. 
In this paper we apply two semi-supervised 
learning algorithms to improve a state-of-the-art 
name tagger. We run the baseline name tagger on 
a large unlabeled corpus (bootstrapping) and the 
test set (self-training), and automatically generate 
high-confidence machine-labeled sentences as 
additional ?training data?. We then iteratively re-
train the model on the increased ?training data?. 
We first investigated whether we can improve 
the system by simply using a lot of unlabeled 
data. By dramatically increasing the size of the 
corpus with unlabeled data, we did get a signifi-
cant improvement compared to the baseline sys-
tem. But we found that adding off-topic 
unlabeled data sometimes makes the performance 
worse. Then we tried to select relevant docu-
ments from the unlabeled data in advance, and 
got clear further improvements. We also obtained 
significant improvement by self-training (boot-
strapping on the test data) without any additional 
unlabeled data.  
Therefore, in contrast to the claim in (Banko 
and Brill, 2001), we concluded that, for some 
applications, effective use of large unlabeled cor-
pora demands good data selection measures. We 
propose and quantify some effective measures to 
select documents and sentences in this paper. 
The rest of this paper is structured as follows. 
Section 2 briefly describes the efforts made by 
previous researchers to use semi-supervised 
learning as well as the work of (Banko and Brill, 
2001).  Section 3 presents our baseline name tag-
ger. Section 4 describes the motivation for our 
approach while Section 5 presents the details of 
two semi-supervised learning methods. Section 6 
presents and discusses the experimental results 
on both English and Chinese. Section 7 presents 
our conclusions and directions for future work. 
2 Prior Work 
This work presented here extends a substantial 
body of previous work (Blum and Mitchell, 1998; 
Riloff and Jones, 1999; Ando and Zhang, 2005) 
48
that all focus on reducing annotation require-
ments. For the specific task of named entity an-
notation, some researchers have emphasized the 
creation of taggers from minimal seed sets 
(Strzalkowski and Wang, 1996; Collins and 
Singer, 1999; Lin et al, 2003) while another line 
of inquiry (which we are pursuing) has sought to 
improve on high-performance baseline taggers 
(Miller et al, 2004). 
Banko and Brill (2001) suggested that the de-
velopment of very large training corpora may be 
most effective for progress in empirical natural 
language processing. Their experiments show a 
logarithmic trend in performance as corpus size 
increases without performance reaching an upper 
bound. Recent work has replicated their work on 
thesaurus extraction (Curran and Moens, 2002) 
and is-a relation extraction (Ravichandran et al, 
2004), showing that collecting data over a very 
large corpus significantly improves system per-
formance. However, (Curran, 2002) and (Curran 
and Osborne, 2002) claimed that the choice of 
statistical model is more important than relying 
upon large corpora. 
3 Motivation 
The performance of name taggers has been lim-
ited in part by the amount of labeled training data 
available.  How can an unlabeled corpus help to 
address this problem?  Based on its original train-
ing (on the labeled corpus), there will be some 
tags (in the unlabeled corpus) that the tagger will 
be very sure about.  For example, there will be 
contexts that were always followed by a person 
name (e.g., "Capt.") in the training corpus.  If we 
find a new token T in this context in the unla-
beled corpus, we can be quite certain it is a per-
son name.  If the tagger can learn this fact about 
T, it can successfully tag T when it appears in the 
test corpus without any indicative context.  In the 
same way, if a previously-unseen context appears 
consistently in the unlabeled corpus before 
known person names, the tagger should learn that 
this is a predictive context. 
We have adopted a simple learning approach:  
we take the unlabeled text about which the tagger 
has greatest confidence in its decisions, tag it, 
add it to the training set, and retrain the tagger.  
This process is performed repeatedly to bootstrap 
ourselves to higher performance.  This approach 
can be used with any supervised-learning tagger 
that can produce some reliable measure of confi-
dence in its decisions. 
4 Baseline Multi-lingual Name Tagger 
Our baseline name tagger is based on an HMM 
that generally follows the Nymble model (Bikel 
et al 1997). Then it uses best-first search to gen-
erate NBest hypotheses, and also computes the 
margin ? the difference between the log prob-
abilities of the top two hypotheses.  This is used 
as a rough measure of confidence in our name 
tagging.1 
In processing Chinese, to take advantage of 
name structures, we do name structure parsing 
using an extended HMM which includes a larger 
number of states (14). This new HMM can han-
dle name prefixes and suffixes, and transliterated 
foreign names separately. We also augmented the 
HMM model with a set of post-processing rules 
to correct some omissions and systematic errors. 
The name tagger identifies three name types: 
Person (PER), Organization (ORG) and Geo-
political (GPE) entities (locations which are also 
political units, such as countries, counties, and 
cities).  
5 Two Semi-Supervised Learning Meth-
ods for Name Tagging 
We have applied this bootstrapping approach to 
two sources of data: first, to a large corpus of 
unlabeled data and second, to the test set.  To 
distinguish the two, we shall label the first "boot-
strapping" and the second "self-training". 
We begin (Sections 5.1 and 5.2) by describing 
the basic algorithms used for these two processes.  
We expected that these basic methods would 
provide a substantial performance boost, but our 
experiments showed that, for best gain, the addi-
tional training data should be related to the target 
problem, namely, our test set.  We present meas-
ures to select documents (Section 5.3) and sen-
tences (Section 5.4), and show (in Section 6) the 
effectiveness of these measures. 
5.1 Bootstrapping 
We divided the large unlabeled corpus into seg-
ments based on news sources and dates in order 
to: 1) create segments of manageable size; 2) 
separately evaluate the contribution of each seg-
ment (using a labeled development test set) and 
reject those which do not help; and 3) apply the 
latest updated best model to each subsequent 
                                                          
1 We have also used this metric in the context of rescoring of 
name hypotheses (Ji and Grishman, 2005); Scheffer et al 
(2001) used a similar metric for active learning of name tags. 
49
segment. The procedure can be formalized as 
follows. 
 
1. Select a related set RelatedC from a large cor-
pus of unlabeled data with respect to the test set 
TestT, using the document selection method de-
scribed in section 5.3. 
 
2. Split RelatedC into n subsets and mark them 
C1, C2?Cn. Call the updated HMM name tagger 
NameM (initially the baseline tagger), and a de-
velopment test set DevT.  
 
3. For i=1 to n 
(1)  Run NameM on Ci;  
 
    (2) For each tagged sentence S in Ci, if S is 
tagged with high confidence, then keep S; 
otherwise remove S; 
 
    (3) Relabel the current name tagger (NameM) 
as OldNameM, add Ci to the training data, 
and retrain the name tagger, producing an 
updated model NameM; 
 
    (4) Run NameM on DevT; if the performance 
gets worse, don?t use Ci and reset NameM 
= OldNameM; 
5.2 Self-training 
An analogous approach can be used to tag the 
test set. The basic intuition is that the sentences 
in which the learner has low confidence may get 
support from those sentences previously labeled 
with high confidence. 
Initially, we build the baseline name tagger 
from the labeled examples, then gradually add 
the most confidently tagged test sentences into 
the training corpus, and reuse them for the next 
iteration, until all sentences are labeled. The pro-
cedure can be formalized as follows. 
 
1. Cluster the test set TestT into n clusters T1, 
T2, ?,Tn, by collecting document pairs with low 
cross entropy (described in section 5.3.2) into the 
same cluster.  
 
2. For i=1 to n 
(1) NameM = baseline HMM name tagger; 
 
(2) While (there are new sentences tagged with 
confidence higher than a threshold) 
          a. Run NameM on Ti;  
          b. Set an appropriate threshold for margin; 
c. For each tagged sentence S in Ti, if S is 
tagged with high confidence, add S to the 
training data; 
d. Retrain the name tagger NameM with 
augmented training data. 
 
At each iteration, we lower the threshold so 
that about 5% of the sentences (with the largest 
margin) are added to the training corpus.2  As an 
example, this yielded the following gradually 
improving performance for one English cluster 
including 7 documents and 190 sentences. 
 
No. of  
iterations
No. of  
sentences 
added 
No. of 
tags 
changed 
F-Measure
0 0 0 91.4 
1 37 28 91.9 
2 69 22 92.1 
3 107 21 92.4 
4 128 11 92.6 
5 146 9 92.7 
6 163 8 92.8 
7 178 6 92.8 
8 190 0 92.8 
 
Table 1. Incremental Improvement from  
Self-training (English) 
 
Self-training can be considered a cache model 
variant, operating across the entire test collection.  
But it uses confidence measures as weights for 
each name candidate, and relies on names tagged 
with high confidence to re-adjust the prediction 
of the remaining names, while in a cache model, 
all name candidates are equally weighted for vot-
ing (independent of the learner?s confidence). 
5.3 Unlabeled Document Selection 
To further investigate the benefits of using very 
large corpora in bootstrapping, and also inspired 
by the gain from the ?essence? of self-training, 
which aims to gradually emphasize the predic-
tions from related sentences within the test set, 
we reconsidered the assumptions of our approach. 
The bootstrapping method implicitly assumes 
that the unlabeled data is reliable (not noisy) and 
uniformly useful, namely: 
 
                                                          
2 To be precise, we repeatedly reduce the threshold by 0.1 
until an additional 5% or more of the sentences are included;  
however, if more than an additional 20% of the sentences 
are captured because many sentences have the same margin, 
we add back 0.1 to the threshold. 
50
? The unlabeled data supports the acquisition 
of new names and contexts, to provide new 
evidence to be incorporated in HMM and re-
duce the sparse data problem; 
? The unlabeled data won?t make the old esti-
mates worse by adding too many names 
whose tags are incorrect, or at least are incor-
rect in the context of the labeled training data 
and the test data. 
 
If the unlabeled data is noisy or unrelated to 
the test data, it can hurt rather than improve the 
learner?s performance on the test set.  So it is 
necessary to coarsely measure the relevance of 
the unlabeled data to our target test set. We de-
fine an IR (information retrieval) - style rele-
vance measure between the test set TestT and an 
unlabeled document d as follows. 
5.3.1 ?Query set? construction 
We model the information expected from the 
unlabeled data by a 'bag of words' technique.  We 
construct a query term set from the test corpus 
TestT to check whether each unlabeled document 
d is useful or not. 
 
? We prefer not to use all the words in TestT  
as key words, since we are only concerned 
about the distribution of name candidates. 
(Adding off-topic documents may in fact in-
troduce noise into the model). For example, 
if one document in TestT talks about the 
presidential election in France while d talks 
about the presidential election in the US, they 
may share many common words such as 
'election', ?voting?, 'poll', and ?camp?, but we 
would expect more gain from other unlabeled 
documents talking about the French election, 
since they may share many name candidates. 
 
? On the other hand it is insufficient to only 
take the name candidates in the top one hy-
pothesis for each sentence (since we are par-
ticularly concerned with tokens which might 
be names but are not so labeled in the top 
hypothesis).  
 
So our solution is to take all the name candi-
dates in the top N best hypotheses for each sen-
tence to construct a query set Q. 
5.3.2 Cross-entropy Measure 
Using Q, we compute the cross entropy H(TestT, 
d) between TestT and d by: 
?
?
??=
Qx
dxprobTestTxprobdTestTH )|(log)|(),( 2
 
 where x is a name candidate in Q, and 
prob(x|TestT) is the probability (frequency) of x 
appearing in TestT while prob(x|d) is the prob-
ability of x in d. If H(T, d) is smaller than a 
threshold then we consider d a useful unlabeled 
document3. 
5.4 Sentence Selection 
We don?t want to add all the tagged sentences in 
a relevant document to the training corpus be-
cause incorrectly tagged or irrelevant sentences 
can lead to degradation in model performance. 
The value of larger corpora is partly dependent 
on how much new information is extracted from 
each sentence of the unlabeled data compared to 
the training corpus that we already have.  
The following confidence measures were ap-
plied to assist the semi-supervised learning algo-
rithm in selecting useful sentences for re-training 
the model.  
5.4.1 Margin to find reliable sentences  
For each sentence, we compute the HMM hy-
pothesis margin (the difference in log probabili-
ties) between the first hypothesis and the second 
hypothesis. We select the sentences with margins 
larger than a threshold4 to be added to the train-
ing data. 
Unfortunately, the margin often comes down 
to whether a specific word has previously been 
observed in training; if the system has seen the 
word, it is certain, if not, it is uncertain. There-
fore the sentences with high margins are a mix of 
interesting and uninteresting samples. We need to 
apply additional measures to remove the uninter-
esting ones.  On the other hand, we may have 
confidence in a tagging due to evidence external 
to the HMM, so we explored measures beyond 
the HMM margin in order to recover additional 
sentences. 
 
 
                                                          
3 We also tried a single match method, using the query set to 
find all the relevant documents that include any names be-
longing to Q, and got approximately the same result as 
cross-entropy. In addition to this relevance selection, we 
used one other simple filter: we removed a document if it 
includes fewer than five names, because it is unlikely to be 
news. 
4 In bootstrapping, this margin threshold is selected by test-
ing on the development set, to achieve more than 93% F-
Measure. 
51
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 1. Bootstrapping for Name Tagging 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
  
 
 
 
 
 
 
 
 
 
Figure 2.  Self-Training for Name Tagging 
 
 
Data English Chinese 
Baseline 
Training data 
ACE02,03,04 989,003 words Beijing Corpus +ACE03,04,05 
1,460,648 words 
Total 196,494 docs in Mar-Jun of 2003  
(69M words) from ACE05 unlabeled data
41061 docs in Nov,Dec of 2000, and Jan 
of 2001 (25M words) from ACE05 and 
TDT4 transcripts 
Selected 
Docs 
62584 docs (1,314,148 Sentences) 14,537 docs (222,359 sentences) 
Unlabeled 
Data 
Selected 
Sentences 
290,973 sentences (6,049,378 words) 55,385 sentences (1,128,505 words) 
Dev Set 20 ACE04 texts in Oct of 2000 90 ACE05 texts in Oct of 2000 
Test Set 20 ACE04 texts in Oct of 2000 
and 80 ACE05 texts in Mar-May of 2003 
(3093 names, 1205 PERs, 1021GPEs, 867 
ORGs) 
90 ACE05 texts in Oct of 2000 
(3093 names, 1013 PERs, 695 GPEs, 769 
ORGs) 
 
Table 2. Data Description 
 
C1 Ci ? ? 
Unlabeled Data 
Cross-entropy based Document Selection
i=i+1
Save Ti? as  
system output 
T1 Ti ? ? 
Test Set 
Cross-entropy based Document Clustering
Ti?? Ti tagged 
with NameM
Yes 
Add Ti? to training corpus 
Retrain NameM 
Ti? Empty?
Ti?? sentences 
selected from Ti?
No 
NameM ? baseline tagger 
i?1
i < n?Yes 
i?1 
NameM performs 
better on dev set?
Yes 
No 
OldNameM ? NameM 
Ci??Ci tagged with NameM 
Add Ci? to training corpus 
Retrain NameM
i=i+1
i < n?
Ci?? sentences selected from Ci? 
Yes 
NameM ? baseline tagger 
Cn Tn
NameM ? OldNameM 
Set margin threshold 
52
5.4.2 Name coreference to find more reliable 
sentences 
Names introduced in an article are likely to be 
referred to again, so a name coreferred to by 
more other names is more likely to have been 
correctly tagged. In this paper, we use simple 
coreference resolution between names such as 
substring matching and name abbreviation reso-
lution.  
In the bootstrapping method we apply single-
document coreference for each individual unla-
beled text. In self-training, in order to further 
benefit from global contexts, we consider each 
cluster of relevant texts as one single big docu-
ment, and then apply cross-document coreference. 
Assume S is one sentence in the document, and 
there are k names tagged in S: {N1, N2 .?.. Nk}, 
which are coreferred to by {CorefNum1, Coref-
Num2, ?CorefNumk} other names separately. 
Then we use the following average name 
coreference count AveCoref as a confidence 
measure for tagging S:5 
?
=
=
k
i
i kCorefNumAveCoref
1
/)(  
5.4.3 Name count and sentence length to re-
move uninteresting sentences 
In bootstrapping on unlabeled data, the margin 
criterion often selects some sentences which are 
too short or don?t include any names. Although 
they are tagged with high confidence, they may 
make the model worse if added into the training 
data (for example, by artificially increasing the 
probability of non-names). In our experiments we 
don?t use a sentence if it includes fewer than six 
words, or doesn?t include any names. 
5.5 Data Flow 
We depict the above two semi-supervised learn-
ing methods in Figure 1 and Figure 2. 
6 Evaluation Results and Discussions 
6.1 Data 
We evaluated our system on two languages: En- 
glish and Chinese. Table 2 shows the data used in 
our experiments. 
                                                          
5 For the experiments reported here, sentences were selected 
if AveCoref > 3.1 (or 3.1?number of documents for cross-
document coreference) or the sentence margin exceeded the 
margin threshold. 
We present in section 6.2 ? 6.4 the overall per-
formance of precision (P), recall (R) and F-
measure (F) for both languages, and also some 
diagnostic experiment results. For significance 
testing (using the sign test), we split the test set 
into 5 folders, 20 texts in each folder of English, 
and 18 texts in each folder of Chinese. 
6.2 Overall Performance 
Table 3 and Table 4 present the overall perform-
ance6 by applying the two semi-supervised learn-
ing methods, separately and in combination, to 
our baseline name tagger. 
 
Learner P R F 
Baseline 87.3 87.6 87.4
Bootstrapping  
with data selection 
88.2 88.6 88.4
Self-training 88.1 88.4 88.2
Bootstrapping with data 
selection + Self-training 
 
89.0 
 
89.2 
 
89.1
 
Table 3. English Name Tagger 
 
Learner P R F 
Baseline 88.2 87.6 87.9
Bootstrapping  
with data selection 
89.8 89.5 89.6
Self-training 89.5 88.3 88.9
Bootstrapping with data 
selection + Self-training 
 
90.2 
 
89.7 
 
90.0
 
Table 4. Chinese Name Tagger 
 
For English, the overall system achieves a 
13.4% relative reduction on the spurious and in-
correct tags, and 12.9% reduction in the missing 
rate. For Chinese, it achieves a 16.9% relative 
reduction on the spurious and incorrect tags, and 
16.9% reduction in the missing rate.7 For each of 
the five folders, we found that both bootstrapping 
and self-training produced an improvement in F 
score for each folder, and the combination of two 
methods is always better than each method alone. 
This allows us to reject the hypothesis that these 
                                                          
6 Only names which exactly match the key in both extent 
and type are counted as correct; unlike MUC scoring, no 
partial credit is given. 
7 The performance achieved should be considered in light of 
human performance on this task.  The ACE keys used for 
the evaluations were obtained by dual annotation and adju-
dication.  A single annotator, evaluated against the key, 
scored F=93.6% to 94.1% for English and 92.5% to 92.7% 
for Chinese.  A second key, created independently by dual 
annotation and adjudication for a small amount of the Eng-
lish data, scored F=96.5% against the original key. 
53
improvements were random at a 95% confidence 
level. 
6.3 Analysis of Bootstrapping 
6.3.1 Impact of Data Size 
Figure 3 and 4 below show the results as each 
segment of the unlabeled data is added to the 
training corpus.  
 
 
Figure 3. Impact of Data Size (English) 
 
 
 
Figure 4. Impact of Data Size (Chinese) 
 
We can see some flattening of the gain at the 
end, particularly for the larger English corpus, 
and that some segments do not help to boost the 
performance (reflected as dips in the Dev Set 
curve and gaps in the Test Set curve). 
6.3.2 Impact of Data Selection 
In order to investigate the contribution of docu-
ment selection in bootstrapping, we performed 
diagnostic experiments for Chinese, whose re-
sults are shown in Table 5. All the bootstrapping 
tests (rows 2 - 4) use margin for sentence selec-
tion; row 4 augments this with the selection 
methods described in sections 5.4.2 and 5.4.3. 
Learner P R F 
(1) Baseline 88.2 87.6 87.9
(2) (1) + Bootstrapping 88.9 88.7 88.8
(3) (2) + Document  
Selection 
89.3 88.9 89.1
(4) (3) + Sentence  
Selection 
89.8  89.5 89.6
 
Table 5. Impact of Data Selection (Chinese) 
 
Comparing row 2 with row 3, we find that not 
using document selection, even though it multi-
plies the size of the corpus, results in 0.3% lower 
performance (0.3-0.4% loss for each folder). This 
leads us to conclude that simply relying upon 
large corpora is not in itself sufficient. Effective 
use of large corpora demands good confidence 
measures for document selection to remove off-
topic material. By adding sentence selection (re-
sults in row 4) the system obtained 0.5% further 
improvement in F-Measure (0.4-0.7% for each 
folder). All improvements are statistically sig-
nificant at the 95% confidence level. 
6.4 Analysis of Self-training  
We have applied and evaluated different meas-
ures to extract high-confidence sentences in self-
training. The contributions of these confidence 
measures to F-Measure are presented in Table 6. 
 
Confidence Measure English Chinese
Baseline 87.4 87.9 
Margin 87.8 88.3 
Margin + single-doc  
name coreference 
88.0 88.7 
Margin + cross-doc  
name coreference 
88.2 88.9 
 
Table 6. Impact of Confidence Measures 
 
It shows that Chinese benefits more from add-
ing name coreference, mainly because there are 
more coreference links between name abbrevia-
tions and full names. And we also can see that 
the margin is an important measure for both lan-
guages. All differences are statistically signifi-
cant at the 95% confidence level except for the 
gain using cross-document information for the 
Chinese name tagging. 
7 Conclusions and Future Work 
This paper demonstrates the effectiveness of two 
straightforward semi-supervised learning meth-
ods for improving a state-of-art name tagger, and 
54
investigates the importance of data selection for 
this application. 
Banko and Brill (2001) suggested that the de-
velopment of very large training corpora may be 
central to progress in empirical natural language 
processing. When using large amounts of unla-
beled data, as expected, we did get improvement 
by using unsupervised bootstrapping. However, 
exploiting a very large corpus did not by itself 
produce the greatest performance gain. Rather, 
we observed that good measures to select rele-
vant unlabeled documents and useful labeled sen-
tences are important.  
The work described here complements the ac-
tive learning research described by (Scheffer et 
al., 2001). They presented an effective active 
learning approach that selects ?difficult? (small 
margin) sentences to label by hand and then add 
to the training set. Our approach selects ?easy? 
sentences ? those with large margins ? to add 
automatically to the training set. Combining 
these methods can magnify the gains possible 
with active learning. 
In the future we plan to try topic identification 
techniques to select relevant unlabeled docu-
ments, and use the downstream information ex-
traction components such as coreference 
resolution and relation detection to measure the 
confidence of the tagging for sentences. We are 
also interested in applying clustering as a pre-
processing step for bootstrapping. 
Acknowledgment 
This material is based upon work supported by 
the Defense Advanced Research Projects Agency 
under Contract No. HR0011-06-C-0023, and the 
National Science Foundation under Grant IIS-
00325657.  Any opinions, findings and conclu-
sions expressed in this material are those of the 
authors and do not necessarily reflect the views 
of the U. S. Government. 
References  
Rie Ando and Tong Zhang. 2005. A High-
Performance Semi-Supervised Learning Methods 
for Text Chunking. Proc.  ACL2005. pp. 1-8. Ann 
Arbor, USA 
Michele Banko and Eric Brill. 2001. Scaling to very 
very large corpora for natural language disam-
biguation. Proc.  ACL2001. pp. 26-33. Toulouse, 
France 
David Bean and Ellen Riloff. 2004. Unsupervised 
Learning of Contextual Role Knowledge for 
Coreference Resolution. Proc.  HLT-NAACL2004. 
pp. 297-304. Boston, USA 
Daniel M. Bikel, Scott Miller, Richard Schwartz, and 
Ralph Weischedel. 1997. Nymble: a high-
performance Learning Name-finder. Proc. Fifth 
Conf. on Applied Natural Language Processing. 
pp.194-201. Washington D.C., USA 
Avrim Blum and Tom Mitchell. 1998. Combining 
Labeled and Unlabeled Data with Co-training. Proc. 
of the Workshop on Computational Learning The-
ory. Morgan Kaufmann Publishers 
Michael Collins and Yoram Singer. 1999. Unsuper-
vised Models for Named Entity Classification. Proc. 
of EMNLP/VLC-99. 
James R. Curran and Marc Moens. 2002. Scaling con-
text space. Proc. ACL 2002. Philadelphia, USA 
James R. Curran. 2002. Ensemble Methods for Auto-
matic Thesaurus Extraction. Proc. EMNLP 2002. 
Philadelphia, USA 
James R. Curran and Miles Osborne. 2002. A very 
very large corpus doesn?t always yield reliable es-
timates. Proc. ACL 2002 Workshop on Effective 
Tools and Methodologies for Teaching Natural 
Language Processing and Computational Linguis-
tics. Philadelphia, USA 
Heng Ji and Ralph Grishman. 2005. Improving Name 
Tagging by Reference Resolution and Relation De-
tection. Proc. ACL2005. pp. 411-418. Ann Arbor, 
USA. 
Winston Lin, Roman Yangarber and Ralph Grishman. 
2003. Bootstrapping Learning of Semantic Classes 
from Positive and Negative Examples. Proc.  
ICML-2003 Workshop on The Continuum from La-
beled to Unlabeled Data. Washington, D.C. 
Scott Miller, Jethran Guinness and Alex Zamanian. 
2004. Name Tagging with Word Clusters and Dis-
criminative Training. Proc. HLT-NAACL2004. pp. 
337-342. Boston, USA 
Deepak Ravichandran, Patrick Pantel, and Eduard 
Hovy. 2004. The Terascale Challenge. Proc. KDD 
Workshop on Mining for and from the Semantic 
Web (MSW-04). pp. 1-11. Seattle, WA, USA 
Ellen Riloff and Rosie Jones. 1999. Learning Diction-
aries for Information Extraction by Multi-Level 
Bootstrapping. Proc. AAAI/IAAI 
Tobias Scheffer, Christian Decomain, and Stefan 
Wrobel. 2001. Active Hidden Markov Models for 
Information Extraction. Proc. Int?l Symposium on 
Intelligent Data Analysis (IDA-2001). 
Tomek Strzalkowski and Jin Wang. 1996. A Self-
Learning Universal Concept Spotter. Proc. 
COLING. 
55
Workshop on Computationally Hard Problemsand Joint Inference in Speech and Language Processing, pages 49?56,
New York City, New York, June 2006. c?2006 Association for Computational Linguistics
Re-Ranking Algorithms for Name Tagging 
 
Heng Ji Cynthia Rudin Ralph Grishman 
Dept. of Computer Science Center for Neural Science and Courant 
Institute of Mathematical Sciences 
Dept. of Computer Science 
New York University 
New York, N.Y. 10003 
hengji@cs.nyu.edu rudin@nyu.edu grishman@cs.nyu.edu 
 
 
 
 
Abstract 
Integrating information from different 
stages of an NLP processing pipeline can 
yield significant error reduction. We dem-
onstrate how re-ranking can improve name 
tagging in a Chinese information extrac-
tion system by incorporating information 
from relation extraction, event extraction, 
and coreference. We evaluate three state-
of-the-art re-ranking algorithms (MaxEnt-
Rank, SVMRank, and p-Norm Push Rank-
ing), and show the benefit of multi-stage 
re-ranking for cross-sentence and cross-
document inference. 
1 Introduction 
In recent years, re-ranking techniques have been 
successfully applied to enhance the performance 
of NLP analysis components based on generative 
models. A baseline generative model produces N-
best candidates, which are then re-ranked using a 
rich set of local and global features in order to 
select the best analysis. Various supervised learn-
ing algorithms have been adapted to the task of re-
ranking for NLP systems, such as MaxEnt-Rank 
(Charniak and Johnson, 2005; Ji and Grishman, 
2005), SVMRank (Shen and Joshi, 2003), Voted 
Perceptron (Collins, 2002; Collins and Duffy, 
2002; Shen and Joshi, 2004), Kernel Based Meth-
ods (Henderson and Titov, 2005), and RankBoost 
(Collins, 2002; Collins and Koo, 2003; Kudo et al, 
2005). 
These algorithms have been used primarily 
within the context of a single NLP analysis com-
ponent, with the most intensive study devoted to 
improving parsing performance. The re-ranking 
models for parsing, for example, normally rely on 
structures generated within the baseline parser 
itself. Achieving really high performance for some 
analysis components, however, requires that we 
take a broader view, one that looks outside a sin-
gle component in order to bring to bear knowl-
edge from the entire NL analysis process.  In this 
paper we will demonstrate the potential of this 
approach in enhancing the performance of Chi-
nese name tagging within an information extrac-
tion application.  
Combining information from other stages in the 
analysis pipeline allows us to incorporate informa-
tion from a much wider context, spanning the en-
tire document and even going across documents.  
This will give rise to new design issues; we will 
examine and compare different re-ranking algo-
rithms when applied to this task.  
We shall first describe the general setting and 
the special characteristics of re-ranking for name 
tagging. Then we present and evaluate three re-
ranking algorithms ? MaxEnt-Rank, SVMRank 
and a new algorithm, p-Norm Push Ranking ? for 
this problem, and show how an approach based on 
multi-stage re-ranking can effectively handle fea-
tures across sentence and document boundaries. 
2 Prior Work 
2.1 Ranking 
We will describe the three state-of-the-art super-
vised ranking techniques considered in this work. 
Later we shall apply and evaluate these algorithms 
for re-ranking in the context of name tagging. 
Maximum Entropy modeling (MaxEnt) has 
been extremely successful for many NLP classifi-
49
cation tasks, so it is natural to apply it to re-
ranking problems. (Charniak and Johnson, 2005) 
applied MaxEnt to improve the performance of a 
state-of-art parser; also in (Ji and Grishman, 2005) 
we used it to improve a Chinese name tagger.  
Using SVMRank, (Shen and Joshi, 2003) 
achieved significant improvement on parse re-
ranking. They compared two different sample 
creation methods, and presented an efficient train-
ing method by separating the training samples into 
subsets.  
The last approach we consider is a boosting-
style approach. We implement a new algorithm 
called p-Norm Push Ranking (Rudin, 2006). This 
algorithm is a generalization of RankBoost 
(Freund et al 1998) which concentrates specifi-
cally on the top portion of a ranked list. The pa-
rameter ?p? determines how much the algorithm 
concentrates at the top.  
2.2 Enhancing Named Entity Taggers 
There have been a very large number of NE tagger 
implementations since this task was introduced at 
MUC-6 (Grishman and Sundheim, 1996).  Most 
implementations use local features and a unifying 
learning algorithm based on, e.g., an HMM, Max-
Ent, or SVM. Collins (2002) augmented a baseline 
NE tagger with a re-ranker that used only local, 
NE-oriented features.  Roth and Yih (2002) com-
bined NE and semantic relation tagging, but 
within a quite different framework (using a linear 
programming model for joint inference). 
3 A Framework for Name Re-Ranking 
3.1 The Information Extraction Pipeline 
The extraction task we are addressing is that of the 
Automatic Content Extraction (ACE)1 evaluations. 
The 2005 ACE evaluation had 7 types of entities, 
of which the most common were PER (persons), 
ORG (organizations), LOC (natural locations) and 
GPE (?geo-political entities? ? locations which are 
also political units, such as countries, counties, 
and cities).  There were 6 types of semantic rela-
tions, with 18 subtypes.  Examples of these rela-
tions are ?the CEO of Microsoft? (an 
organization-affiliation relation), ?Fred?s wife? (a 
                                                          
1 The ACE task description can be found at 
http://www.itl.nist.gov/iad/894.01/tests/ace/ 
personal-social relation), and ?a military base in 
Germany? (a located relation). And there were 8 
types of events, with 33 subtypes, such as ?Kurt 
Schork died in Sierra Leone yesterday? (a Die 
event), and ?Schweitzer founded a hospital in 
1913? (a Start-Org event). 
  To extract these elements we have developed a 
Chinese information extraction pipeline that con-
sists of the following stages: 
? Name tagging and name structure parsing 
(which identifies the internal structure of some 
names); 
? Coreference resolution, which links "men-
tions" (referring phrases of selected semantic 
types) into "entities": this stage is a combina-
tion of high-precision heuristic rules and 
maximum entropy models; 
? Relation tagging, using a K-nearest-neighbor 
algorithm to identify relation types and sub-
types; 
? Event patterns, semi-automatically extracted 
from ACE training corpora. 
3.2 Hypothesis Representation and Genera-
tion 
Again, the central idea is to apply the baseline 
name tagger to generate N-Best multiple hypothe-
ses for each sentence; the results from subsequent 
components are then exploited to re-rank these 
hypotheses and the new top hypothesis is output 
as the final result. 
In our name re-ranking model, each hypothesis 
is an NE tagging of the entire sentence. For ex-
ample, ?<PER>John</PER> was born in 
<GPE>New York</GPE>.? is one hypothesis 
for the sentence ?John was born in New York?. 
We apply a HMM tagger to identify four named 
entity types: Person, GPE, Organization and Loca-
tion. The HMM tagger generally follows the 
Nymble model (Bikel et al 1997), and uses best-
first search to generate N-Best hypotheses. It also 
computes the ?margin?, which is the difference 
between the log probabilities of the top two hy-
potheses.  This is used as a rough measure of con-
fidence in the top hypothesis. A large margin 
indicates greater confidence that the first hypothe-
sis is correct. The margin also determines the 
number of hypotheses (N) that we will store. Us-
ing cross-validation on the training data, we de-
termine the value of N required to include the best 
50
hypothesis, as a function of the margin.  We then 
divide the margin into ranges of values, and set a 
value of N for each range, with a maximum of 30. 
To obtain the training data for the re-ranking 
algorithm, we separate the name tagging training 
corpus into k folders, and train the HMM name 
tagger on k-1 folders. We then use the HMM to 
generate N-Best hypotheses H = {h1, h2,?,hN} for 
each sentence in the remaining folder.  Each hi in 
H is then paired with its NE F-measure, measured 
against the key in the annotated corpus. 
We define a ?crucial pair? as a pair of hypothe-
ses such that, according to F-Measure, the first 
hypothesis in the pair should be more highly 
ranked than the second. That is, if for a sentence, 
the F-Measure of hypothesis hi is larger than that 
of hj, then (hi, hj) is a crucial pair. 
3.3 Re-Ranking Functions 
We investigated the following three different for-
mulations of the re-ranking problem: 
? Direct Re-Ranking by Score 
For each hypothesis hi, we attempt to learn a scor-
ing function f : H ? R, such that f(hi) > f(hj) if the 
F-Measure of hi is higher than the F-measure of hj. 
? Direct Re-Ranking by Classification 
For each hypothesis hi, we attempt to learn f : H 
? {-1, 1}, such that f(hi) = 1 if hi has the top F-
Measure among H; otherwise f(hi) = -1. This can 
be considered a special case of re-ranking by 
score. 
? Indirect Re-Ranking Function 
For each ?crucial? pair of hypotheses (hi, hj), we 
learn f : H ? H ? {-1, 1}, such that f(hi, hj) = 1 if 
hi is better than hj; f (hi, hj) = -1 if hi is worse than 
hj. We call this ?indirect? ranking because we 
need to apply an additional decoding step to pick 
the best hypothesis from these pair-wise compari-
son results. 
4 Features for Re-Ranking 
4.1 Inferences From Subsequent Stages 
Information extraction is a potentially symbiotic 
pipeline with strong dependencies between stages 
(Roth and Yih, 2002&2004; Ji and Grishman, 
2005). Thus, we use features based on the output 
of four subsequent stages ? name structure parsing, 
relation extraction, event patterns, and coreference 
analysis ? to seek the best hypothesis.  
We included ten features based on name struc-
ture parsing to capture the local information 
missed by the baseline name tagger such as details 
of the structure of Chinese person names. 
The relation and event re-ranking features are 
based on matching patterns of words or constitu-
ents.  They serve to correct name boundary errors 
(because such errors would prevent some patterns 
from matching).  They also exert selectional pref-
erences on their arguments, and so serve to correct 
name type errors.  For each relation argument, we 
included a feature whose value is the likelihood 
that relation appears with an argument of that se-
mantic type (these probabilities are obtained from 
the training corpus and binned).  For each event 
pattern, a feature records whether the types of the 
arguments match those required by the pattern. 
Coreference can link multiple mentions of 
names provided they have the same spelling 
(though if a name has several parts, some may be 
dropped) and same semantic type. So if the 
boundary or type of one mention can be deter-
mined with some confidence, coreference can be 
used to disambiguate other mentions, by favoring 
hypotheses which support more coreference. To 
this end, we incorporate several features based on 
coreference, such as the number of mentions re-
ferring to a name candidate.  
Each of these features is defined for individual 
name candidates; the value of the feature for a 
hypothesis is the sum of its values over all names 
in the hypothesis. The complete set of detailed 
features is listed in (Ji and Grishman, 2006). 
4.2 Handling Cross-Sentence Features by 
Multi-Stage Re-Ranking 
Coreference is potentially a powerful contributor 
for enhancing NE recognition, because it provides 
information from other sentences and even docu-
ments, and it applies to all sentences that include 
names. For a name candidate, 62% of its corefer-
ence relations span sentence boundaries.  How-
ever, this breadth poses a problem because it 
means that the score of a hypothesis for a given 
51
sentence may depend on the tags assigned to the 
same names in other sentences.2 
Ideally, when we re-rank the hypotheses for one 
sentence S, the other sentences that include men-
tions of the same name should already have been 
re-ranked, but this is not possible because of the 
mutual dependence. Repeated re-ranking of a sen-
tence would be time-consuming, so we have 
adopted an alternative approach. Instead of incor-
porating coreference evidence with all other in-
formation in one re-ranker, we apply two re-
rankers in succession.  
   In the first re-ranking step, we generate new 
rankings for all sentences based on name structure, 
relation and event features, which are all sentence-
internal evidence.  Then in a second pass, we ap-
ply a re-ranker based on coreference between the 
names in each hypothesis of sentence S and the 
mentions in the top-ranking hypothesis (from the 
first re-ranker) of all other sentences.3  In this way, 
the coreference re-ranker can propagate globally 
(across sentences and documents) high-confidence 
decisions based on the other evidence. In our final 
MaxEnt Ranker we obtained a small additional 
gain by further splitting the first re-ranker into 
three separate steps: a name structure based re-
ranker, a relation based re-ranker and an event 
based re-ranker; these were incorporated in an 
incremental structure.   
4.3 Adding Cross-Document Information 
The idea in coreference is to link a name mention 
whose tag is locally ambiguous to another men-
tion that is unambiguously tagged based on local 
evidence.  The wider a net we can cast, the greater 
the chance of success.  To cast the widest net pos-
sible, we have used cross-document coreference 
for the test set. We cluster the documents using a 
cross-entropy metric and then treat the entire clus-
ter as a single document.      
We take all the name candidates in the top N 
hypotheses for each sentence in each cluster T to 
construct a ?query set? Q. The metric used for the 
clustering is the cross entropy H(T, d) between the 
distribution of the name candidates in T and 
                                                          
2 For in-document coreference, this problem could be avoided if the tagging of 
an entire document constituted a hypothesis, but that would be impractical ? a 
very large N would be required to capture sufficient alternative taggings in an 
N-best framework. 
3 This second pass is skipped for sentences for which the confidence in the top 
hypothesis produced by the first re-ranker is above a threshold. 
document d. If H(T, d) is smaller than a threshold 
then we add d to T. H(T, d) is defined by: 
?
?
??=
Qx
xdprobxTprobdTH ),(log),(),( . 
We built these clusters two ways: first, just 
clustering the test documents; second, by aug-
menting these clusters with related documents 
retrieved from a large unlabeled corpus (with 
document relevance measured using cross-
entropy). 
5 Re-Ranking Algorithms 
We have been focusing on selecting appropriate 
ranking algorithms to fit our application. We 
choose three state-of-the-art ranking algorithms 
that have good generalization ability. We now 
describe these algorithms. 
5.1 MaxEnt-Rank 
5.1.1  Sampling and Pruning 
 
Maximum Entropy models are useful for the task 
of ranking because they compute a reliable rank-
ing probability for each hypothesis.  We have tried 
two different sampling methods ? single sampling 
and pairwise sampling.  
The first approach is to use each single hy-
pothesis hi as a sample. Only the best hypothesis 
of each sentence is regarded as a positive sample; 
all the rest are regarded as negative samples. In 
general, absolute values of features are not good 
indicators of whether a hypothesis will be the best 
hypothesis for a sentence; for example, a co-
referring mention count of 7 may be excellent for 
one sentence and poor for another.  Consequently, 
in this single-hypothesis-sampling approach, we 
convert each feature to a Boolean value, which is 
true if the original feature takes on its maximum 
value (among all hypotheses) for this hypothesis.  
This does, however, lose some of the detail about 
the differences between hypotheses. 
In pairwise sampling we used each pair of hy-
potheses (hi, hj) as a sample. The value of a fea-
ture for a sample is the difference between its 
values for the two hypotheses.  However, consid-
ering all pairs causes the number of samples to 
grow quadratically (O(N2)) with the number of 
hypotheses, compared to the linear growth with 
best/non-best sampling. To make the training and 
52
test procedures more efficient, we prune the data 
in several ways.  
We perform pruning by beam setting, removing 
candidate hypotheses that possess very low prob-
abilities from the HMM, and during training we 
discard the hypotheses with very low F-measure 
scores. Additionally, we incorporate the pruning 
techniques used in (Chiang 2005), by which any 
hypothesis with a probability lower than?times 
the highest probability for one sentence is dis-
carded. We also discard the pairs very close in 
performance or probability. 
 
5.1.2 Decoding 
 
If f is the ranking function, the MaxEnt model 
produces a probability for each un-pruned ?cru-
cial? pair: prob(f(hi, hj) = 1), i.e., the probability 
that for the given sentence, hi is a better hypothe-
sis than hj. We need an additional decoding step to 
select the best hypothesis. Inspired by the caching 
idea and the multi-class solution proposed by 
(Platt et al 2000), we use a dynamic decoding 
algorithm with complexity O(n) as follows. 
We scale the probability values into three types: 
CompareResult (hi, hj) = ?better? if prob(f(hi, hj) = 
1) >?1, ?worse? if prob(f(hi, hj) = 1) <?2, and 
?unsure? otherwise, where ?1??2. 4  
 
Prune 
for i = 1 to n 
Num = 0; 
for j = 1 to n and j?i 
If CompareResult(hi, hj) = ?worse? 
Num++; 
    if Num>?then discard hi from H 
 
Select 
Initialize: i = 1, j = n 
while (i<j) 
if CompareResult(hi, hj) = ?better? 
discard hj from H; 
j--; 
else if CompareResult(hi, hj) = ?worse? 
discard hi from H; 
i++; 
else break; 
 
                                                          
4 In the final stage re-ranker we use?1=?2 so that we don?t generate the 
output of ?unsure?, and one hypothesis is finally selected. 
Output 
If the number of remaining hypotheses in H is 1, 
then output it as the best hypothesis; else propa-
gate all hypothesis pairs into the next re-ranker. 
5.2 SVMRank 
We implemented an SVM-based model, which 
can theoretically achieve very low generalization 
error. We use the SVMLight package (Joachims, 
1998), with the pairwise sampling scheme as for 
MaxEnt-Rank. In addition we made the following 
adaptations: we calibrated the SVM outputs, and 
separated the data into subsets. 
To speed up training, we divided our training 
samples into k subsets. Each subset contains N(N-
1)/k pairs of hypotheses of each sentence.  
In order to combine the results from these dif-
ferent SVMs, we must calibrate the function val-
ues; the output of an SVM yields a distance to the 
separating hyperplane, but not a probability. We 
have applied the method described in (Shen and 
Joshi, 2003), to map SVM?s results to probabili-
ties via a sigmoid. Thus from the kth SVM, we get 
the probability for each pair of hypotheses: 
)1),(( =jik hhfprob , 
namely the probability of hi being better than hj. 
Then combining all k SVMs? results we get: 
       ? ==
k
jikji hhfprobhhZ )1),((),( . 
So the hypothesis hi with maximal value is cho-
sen as the top hypothesis:  
?
j
ji
h
hhZ
i
)),((maxarg . 
5.3 P-Norm Push Ranking 
The third algorithm we have tried is a general 
boosting-style supervised ranking algorithm called 
p-Norm Push Ranking (Rudin, 2006). We de-
scribe this algorithm in more detail since it is quite 
new and we do not expect many readers to be fa-
miliar with it.  
The parameter ?p? determines how much em-
phasis (or ?push?) is placed closer to the top of the 
ranked list, where p?1. The p-Norm Push Ranking 
algorithm generalizes RankBoost (take p=1 for 
RankBoost). When p is set at a large value, the 
rankings at the top of the list are given higher pri-
ority (a large ?push?), at the expense of possibly 
making misranks towards the bottom of the list. 
53
Since for our application, we do not care about the 
rankings at the bottom of the list (i.e., we do not 
care about the exact rank ordering of the bad hy-
potheses), this algorithm is suitable for our prob-
lem. There is a tradeoff for the choice of p; larger 
p yields more accurate results at the very top of 
the list for the training data. If we want to consider 
more than simply the very top of the list, we may 
desire a smaller value of p. Note that larger values 
of p also require more training data in order to 
maintain generalization ability (as shown both by 
theoretical generalization bounds and experi-
ments). If we want large p, we must aim to choose 
the largest value of p that allows generalization, 
given our amount of training data. When we are 
working on the first stage of re-ranking, we con-
sider the whole top portion of the ranked list, be-
cause we use the rank in the list as a feature for 
the next stage. Thus, we have chosen the value 
p1=4 (a small ?push?) for the first re-ranker. For 
the second re-ranker we choose p2=16 (a large 
?push?). 
The objective of the p-Norm Push Ranking al-
gorithm is to create a scoring function f: H?R 
such that for each crucial pair (hi, hj), we shall 
have f(hi) > f(hj). The form of the scoring function 
is f(hi) = ??kgk(hi), where gk is called a weak 
ranker: gk : H ? [0,1]. The values of ?k are de-
termined by the p-Norm Push algorithm in an it-
erative way.  
The weak rankers gk are the features described 
in Section 4. Note that we sometimes allow the 
algorithm to use both gk and g?k(hi)=1-gk(hi) as 
weak rankers, namely when gk has low accuracy 
on the training set; this way the algorithm itself 
can decide which to use.  
As in the style of boosting algorithms, real-
valued weights are placed on each of the training 
crucial pairs, and these weights are successively 
updated by the algorithm. Higher weights are 
given to those crucial pairs that were misranked at 
the previous iteration, especially taking into ac-
count the pairs near the top of the list. At each 
iteration, one weak ranker gk is chosen by the al-
gorithm, based on the weights. The coefficient ?k 
is then updated accordingly.  
6 Experiment Results 
6.1 Data and Resources 
We use 100 texts from the ACE 04 training corpus 
for a blind test. The test set included 2813 names: 
1126 persons, 712 GPEs, 785 organizations and 
190 locations. The performance is measured via 
Precision (P), Recall (R) and F-Measure (F). 
The baseline name tagger is trained from 2978 
texts from the People?s Daily news in 1998 and 
also 1300 texts from ACE training data.  
The 1,071,285 training samples (pairs of hy-
potheses) for the re-rankers are obtained from the 
name tagger applied on the ACE training data, in 
the manner described in Section 3.2. 
We use OpenNLP5 for the MaxEnt-Rank ex-
periments. We use SVMlight (Joachims, 1998) for 
SVMRank, with a linear kernel and the soft mar-
gin parameter set to the default value. For the p-
Norm Push Ranking, we apply 33 weak rankers, 
i.e., features described in Section 4. The number 
of iterations was fixed at 110, this number was 
chosen by optimizing the performance on a devel-
opment set of 100 documents. 
6.2 Effect of Pairwise Sampling 
We have tried both single-hypothesis and pairwise 
sampling (described in section 5.1.1) in MaxEnt-
Rank and p-Norm Push Ranking. Table 1 shows 
that pairwise sampling helps both algorithms. 
MaxEnt-Rank benefited more from it, with preci-
sion and recall increased 2.2% and 0.4% respec-
tively. 
 
Model P R F 
Single Sampling 89.6 90.2 89.9MaxEnt-
Rank Pairwise Sampling 91.8 90.6 91.2
Single Sampling 91.4 89.6 90.5p-Norm 
Push Pairwise Sampling 91.2 90.8 91.0
 
Table 1. Effect of Pairwise Sampling 
6.3 Overall Performance 
In Table 2 we report the overall performance for 
these three algorithms. All of them achieved im-
provements on the baseline name tagger. MaxEnt 
yields the highest precision, while p-Norm Push 
Ranking with p2 = 16 yields the highest recall. 
A larger value of ?p? encourages the p-Norm 
Push Ranking algorithm to perform better near the 
top of the ranked list. As we discussed in section 
                                                          
5 http://maxent.sourceforge.net/index.html 
54
5.3, we use p1 = 4 (a small ?push?) for the first re-
ranker and p2 = 16 (a big ?push?) for the second 
re-ranker. From Table 2 we can see that p2 = 16 
obviously performed better than p2 = 1. In general, 
we have observed that for p2 ?16, larger p2 corre-
lates with better results. 
 
Model P R F 
Baseline  87.4 87.6 87.5
MaxEnt-Rank 91.8 90.6 91.2
SVMRank 89.5 90.1 89.8
p-Norm Push Ranking (p2 =16) 91.2 90.8 91.0
p-Norm Push Ranking  
(p2 =1, RankBoost) 
89.3 89.7 89.5
 
Table 2. Overall Performance  
The improved NE results brought better per-
formance for the subsequent stages of information 
extraction too. We use the NE outputs from Max-
Ent-Ranker as inputs for coreference resolver and 
relation tagger. The ACE value6 of entity detec-
tion (mention detection + coreference resolution) 
is increased from 73.2 to 76.5; the ACE value of 
relation detection is increased from 34.2 to 34.8. 
6.4 Effect of Cross-document Information 
As described in Section 4.3, our algorithm incor-
porates cross-document coreference information. 
The 100 texts in the test set were first clustered 
into 28 topics (clusters). We then apply cross-
document coreference on each cluster. Compared 
to single document coreference, cross-document 
coreference obtained 0.5% higher F-Measure, us-
ing MaxEnt-Ranker, improving performance for 
15 of these 28 clusters. 
These clusters were then extended by selecting 
84 additional related texts from a corpus of 15,000 
unlabeled Chinese news articles (using a cross-
entropy metric to select texts). 24 clusters gave 
further improvement, and an overall 0.2% further 
improvement on F-Measure was obtained.  
6.5 Efficiency 
Model Training Test 
MaxEnt-Rank 7 hours 55 minutes
SVMRank 48 hours 2 hours 
p-Norm Push Ranking 3.2 hours 10 minutes
 
Table 3. Efficiency Comparison 
                                                          
6 The ACE04 value scoring metric can be found at: 
http://www.nist.gov/speech/tests/ace/ace04/doc/ace04-evalplan-v7.pdf 
In Table 3 we summarize the running time of 
these three algorithms in our application. 
7 Discussion 
We have shown that the other components of an 
IE pipeline can provide information which can 
substantially improve the performance of an NE 
tagger, and that these improvements can be real-
ized through a variety of re-ranking algorithms.  
MaxEnt re-ranking using binary sampling and p-
Norm Push Ranking proved about equally effec-
tive.7  p-Norm Push Ranking was particularly ef-
ficient for decoding (about 10 documents / 
minute), although no great effort was invested in 
tuning these procedures for speed. 
We presented methods to handle cross-sentence 
inference using staged re-ranking and to incorpo-
rate additional evidence through document clus-
tering. 
An N-best / re-ranking strategy has proven ef-
fective for this task because with relatively small 
values of N we are already able to include highly-
rated hypotheses for most sentences.  Using the 
values of N we have used throughout (dependent 
on the margin of the baseline HMM, but never 
above 30), the upper bound of N-best performance 
(if we always picked the top-scoring hypothesis) 
is 97.4% recall, 96.2% precision, F=96.8%. 
Collins (2002) also applied re-ranking to im-
prove name tagging. Our work has addressed both 
name identification and classification, while his 
only evaluated name identification.  Our re-ranker 
used features from other pipeline stages, while his 
were limited to local features involving lexical 
information and 'word-shape' in a 5-token window.  
Since these feature sets are essentially disjoint, it 
is quite possible that a combination of the two 
could yield even further improvements. His boost-
ing algorithm is a modification of the method in 
(Freund et al, 1998), an adaptation of AdaBoost, 
whereas our p-Norm Push Ranking algorithm can 
emphasize the hypotheses near the top, matching 
our objective. 
Roth and Yih (2004) combined information 
from named entities and semantic relation tagging, 
adopting a similar overall goal but using a quite 
different approach based on linear programming.  
                                                          
7 The features were initially developed and tested using the MaxEnt re-ranker, 
so it is encouraging that they worked equally well with the p-Norm Push 
Ranker without further tuning. 
55
They limited themselves to name classification, 
assuming the identification given.  This may be a 
natural subtask for English, where capitalization is 
a strong indicator of a name, but is much less use-
ful for Chinese, where there is no capitalization or 
word segmentation, and boundary errors on name 
identification are frequent. Expanding their ap-
proach to cover identification would have greatly 
increased the number of hypotheses and made 
their approach slower.  In contrast, we adjust the 
number of hypotheses based on the margin in or-
der to maintain efficiency while minimizing the 
chance of losing a high-quality hypothesis. 
In addition we were able to capture selectional 
preferences (probabilities of semantic types as 
arguments of particular semantic relations as 
computed from the corpus), whereas Roth and Yih 
limited themselves to hard (boolean) type con-
straints. 
Acknowledgment 
This material is based upon work supported by the 
Defense Advanced Research Projects Agency un-
der Contract No. HR0011-06-C-0023, and the Na-
tional Science Foundation under Grant IIS-
00325657 and a postdoctoral research fellowship.  
Any opinions, findings and conclusions expressed 
in this material are those of the authors and do not 
necessarily reflect the views of the U. S. Govern-
ment. 
References  
Daniel M. Bikel, Scott Miller, Richard Schwartz, and 
Ralph Weischedel. 1997. Nymble: a high-
performance Learning Name-finder.  Proc. 
ANLP1997. pp. 194-201. Washington, D.C.  
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
Fine N-Best Parsing and MaxEnt Discriminative 
Reranking. Proc. ACL2005. pp. 173-180. Ann Arbor, 
USA 
David Chiang. 2005. A Hierarchical Phrase-Based 
Model for Statistical Machine Translation. Proc. 
ACL2005. pp. 263-270. Ann Arbor, USA 
Michael Collins. 2002. Ranking Algorithms for 
Named-Entity Extraction: Boosting and the Voted 
Perceptron. Proc. ACL 2002. pp. 489-496 
Michael Collins and Nigel Duffy. 2002. New Ranking 
Algorithms for Parsing and Tagging: Kernels over 
Discrete Structures, and the Voted Perceptron. Proc. 
ACL2002. pp. 263-270. Philadelphia, USA 
Michael Collins and Terry Koo. 2003. Discriminative 
Reranking for Natural Language Parsing. Journal of 
Association for Computational Linguistics. pp. 175-
182. 
Yoav Freund, Raj Iyer, Robert E. Schapire and Yoram 
Singer. 1998. An efficient boosting algorithm for 
combining  preferences. Machine Learning: Pro-
ceedings of the Fifteenth International Conference. 
pp. 170-178 
Ralph Grishman and Beth Sundheim.  1996. Message 
understanding conference - 6: A brief history. Proc. 
COLING1996,. pp. 466-471. Copenhagen. 
James Henderson and Ivan Titov. 2005. Data-Defined 
Kernels for Parse Reranking Derived from Probabil-
istic Models. Proc. ACL2005. pp. 181-188. Ann Ar-
bor, USA. 
Heng Ji and Ralph Grishman. 2005. Improving Name 
Tagging by Reference Resolution and Relation De-
tection. Proc. ACL2005. pp. 411-418. Ann Arbor, 
USA. 
Heng Ji and Ralph Grishman. 2006. Analysis and Re-
pair of Name Tagger Errors. Proc. ACL2006 
(POSTER). Sydney, Australia. 
Thorsten Joachims. 1998. Making large-scale support 
vector machine learning practical. Advances in Ker-
nel Methods: Support Vector Machine. MIT Press. 
Taku Kudo, Jun Suzuki and Hideki Isozaki. 2005. 
Boosting-based Parse Reranking Derived from 
Probabilistic Models. Proc. ACL2005. pp. 189-196. 
Ann Arbor, USA. 
John Platt, Nello Cristianini, and John Shawe-Taylor. 
2000.  Large margin dags for multiclass classifica-
tion. Advances in Neural Information Processing 
Systems 12. pp. 547-553 
Dan Roth and Wen-tau Yih. 2004. A Linear Program-
ming Formulation for Global Inference in Natural 
Language Tasks. Proc. CONLL2004. pp. 1-8 
Dan Roth and Wen-tau Yih. 2002. Probabilistic Rea-
soning for Entity & Relation Recognition. Proc. 
COLING2002. pp. 835-841 
Cynthia Rudin. 2006. Ranking with a p-Norm Push. 
Proc. Nineteenth Annual  Conference on Computa-
tional Learning Theory (CoLT 2006), Pittsburgh, 
Pennsylvania. 
Libin Shen and Aravind K. Joshi. 2003. An SVM 
Based Voting Algorithm with Application to Parse 
ReRanking. Proc. HLT-NAACL 2003 workshop on 
Analysis of Geographic References. pp. 9-16 
Libin Shen and Aravind K. Joshi. 2004. Flexible Mar-
gin Selection for Reranking with Full Pairwise Sam-
ples. Proc.IJCNLP2004. pp. 446-455. Hainan Island, 
China. 
56
Proceedings of the 2009 Workshop on Language Generation and Summarisation, ACL-IJCNLP 2009, pages 48?55,
Suntec, Singapore, 6 August 2009. c?2009 ACL and AFNLP
A Parse-and-Trim Approach with Information Significance               
for Chinese Sentence Compression 
 
 
 
Wei Xu           Ralph Grishman 
Computer Science Department 
New York University 
New York, NY, 10003, USA 
{xuwei,grishman}@cs.nyu.edu 
 
 
 
  
 
Abstract 
In this paper, we propose an event-based ap-
proach for Chinese sentence compression 
without using any training corpus. We en-
hance the linguistically-motivated heuristics 
by exploiting event word significance and 
event information density. This is shown to 
improve the preservation of important infor-
mation and the tolerance of POS and parsing 
errors, which are more common in Chinese 
than English. The heuristics are only required 
to determine possibly removable constituents 
instead of selecting specific constituents for 
removal, and thus are easier to develop and 
port to other languages and domains. The ex-
perimental results show that around 72% of 
our automatic compressions are grammatically 
and semantically correct, preserving around 
69% of the most important information on av-
erage.  
1 Introduction 
The goal of sentence compression is to shorten 
sentences while preserving their grammaticality 
and important information. It has recently at-
tracted much attention because of its wide range 
of applications, especially in summarization 
(Jing, 2000) and headline generation (which can 
be viewed as summarization with very short 
length requirement). Sentence compression can 
improve extractive summarization in coherence 
and amount of information expressed within a 
fixed length. 
An ideal sentence compression will include 
complex paraphrasing operations, such as word 
deletion, substitution, insertion, and reordering. 
In this paper, we focus on the simpler instantia-
tion of sentence simplification, namely word de-
letion, which has been proved a success in the 
literature (Knight and Marcu, 2002; Dorr et al 
2003; Clarke and Lapata, 2006).  
In this paper, we present our technique for 
Chinese sentence compression without the need 
for a sentence/compression parallel corpus. We 
combine linguistically-motivated heuristics and 
word significance scoring together to trim the 
parse tree, and rank candidate compressions ac-
cording to event information density. In contrast 
to probabilistic methods, the heuristics are more 
likely to produce grammatical and fluent com-
pressed sentences. We reduce the difficulty and 
linguistic skills required for composing heuristics 
by only requiring these heuristics to identify pos-
sibly removable constituents instead of selecting 
specific constituents for removal. The word sig-
nificance helps to preserve informative constitu-
ents and overcome some POS and parsing errors. 
In particular, we seek to assess the event infor-
mation during the compression process, accord-
ing to the previous successes in event-based 
summarization (Li et al 2006) and a new event-
oriented 5W summarization task (Parton et al 
2009). 
The next section presents previous approaches 
to sentence compression. In section 3, we de-
scribe our system with three modules, viz. lin-
guistically-motivated heuristics, word signific-
ance scoring and candidate compression selec-
tion. We also develop a heuristics-only approach 
for comparison. In section 4, we evaluate the 
compressions in terms of grammaticality, infor-
48
mativeness and compression rate. Finally, Sec-
tion 5 concludes this paper and discusses direc-
tions of future work. 
2 Previous Work 
Most previous studies relied on a parallel cor-
pus to learn the correspondences between origi-
nal and compressed sentences. Typically sen-
tences are represented by features derived from 
parsing results, and used to learn the transforma-
tion rules or estimate the parameters in the score 
function of a possible compression. A variety of 
models have been developed, including but not 
limited to the noisy-channel model (Knight and 
Marcu, 2002; Galley and McKeown, 2007), the 
decision-tree model (Knight and Marcu, 2002), 
support vector machines (Nguyen et al 2004) 
and large-margin learning (McDonald, 2006; 
Cohn and Lapata 2007).  
Approaches which do not employ parallel cor-
pora are less popular, even though the parallel 
sentence/compression corpora are not as easy to 
obtain as multilingual corpora for machine trans-
lation. Only a few studies have been done requir-
ing no or minimal training corpora (Dorr et al 
2003; Hori and Furui, 2004; Turner and Char-
niak, 2005). The scarcity of parallel corpora also 
constrains the development in languages other 
than English. To the best of our knowledge, no 
study has been done on Chinese sentence com-
pression.  
An algorithm making limited use of training 
corpora was proposed originally by Hori and Fu-
rui (2004) for spoken text in Japanese, and later 
modified by Clarke and Lapata (2006) for Eng-
lish text. Their model searches for the compres-
sion with highest score according to the signific-
ance of each word, the existence of Subject-
Verb-Object structures and the language model 
probability of the resulting word combination. 
The weight factors to balance the three mea-
surements are experimentally optimized by a 
parallel corpus or estimated by experience.  
Turner and Charniak (2005) present semi-
supervised and unsupervised variants of the noi-
sy channel model. They approximate the rules of 
compression from a non-parallel corpus (e.g. the 
Penn Treebank) based on probabilistic context 
free grammar derivation. 
Our approach is most similar to the Hedge 
Trimmer for English headline generation (Dorr et 
al, 2003), in which linguistically-motivated heu-
ristics are used to trim the parse tree. This me-
thod removes low content components in a preset 
order until the desired length requirement is 
reached. It reduces the risk of deleting subordi-
nate clauses and prepositional phrases by delay-
ing these operations until no other rules can be 
applied. This fixed order of applying rules limits 
the flexibility and capability for preserving in-
formative constituents during deletions. It is like-
ly to fail by producing a grammatical but seman-
tically useless compressed sentence. Another 
major drawback is that it requires considerable 
linguistic skill to produce proper rules in a proper 
order.   
3 Algorithms for Sentence Compression 
Our system takes the output of a Chinese Tree-
bank-style syntactic parser (Huang and Harper, 
2009) as input and performs tree trimming opera-
tions to obtain compression. We propose and 
compare two approaches. One uses only linguis-
tically-motivated heuristics to delete words and 
gets the compression result directly. The other 
one uses heuristics to determine which nodes in 
the parse tree are potentially removable. Then all 
removable nodes are deleted one by one accord-
ing to their significance weights to generate a 
series of candidate compressions. Finally, the 
best compression is selected based on sentence 
length and informativeness criteria.  
3.1 Linguistically-motivated Heuristics 
This module aims to identify the nodes in the 
parse tree which may be removed without severe 
loss in grammaticality and information. Based on 
an analysis of the Penn Treebank corpus and 
human-produced compression, we decided that 
the following parse constituents are potential low 
content units.  
 
Set 0 ? basic: 
? Parenthetical elements  
? Adverbs except negative, some temporal 
and degree adverbs 
? Adjectives except when the modified noun 
consists of only one character 
? DNPs (which are formed by various 
phrasal categories plus ??? and appear as 
modifiers of NP in Chinese) 
? DVPs (which are formed by various 
phrasal categories plus ??? in Chinese, 
and appear as modifiers of VP in Chinese) 
? All nodes in noun coordination phrases 
except the first noun 
 
 
49
Set 1 ? fixed: 
? All children of NP nodes except temporal 
nouns and proper nouns and the last noun 
word 
? All simple clauses (IP) except the first one, 
if the sentence consists of more than one 
IP 
? Prepositional phrases except those that 
may contain location or date information, 
according to a hand-made list of preposi-
tions  
 
Set 2 ? flexible: 
? All nodes in verb coordination phrases ex-
cept the first one. 
? Relative clauses 
? Appositive clauses 
? All prepositional phrases 
? All children of NP nodes except the last 
noun word 
? All simple clauses, if the sentence consists 
of more than one IP (at least one clause is 
required to be preserved in later trimming) 
 
Set 0 lists all the fundamental constituents that 
may be removed and is used in both approaches. 
Set 1 and Set 2 are designed to handle more 
complex constituents for the two approaches re-
spectively.  
The heuristics-only approach exploits Set 0 
and Set 1. It can be viewed as the Chinese ver-
sion of Hedge Trimmer (Dorr et al 2003), but 
differs in the following ways: 
1) Chinese has different language construc-
tions and grammar from English. 
2) We eliminate the strict compression 
length constraint in order to yield more 
natural compressions with varying length. 
3) We do not remove time expressions on 
purpose to benefit further applications, 
such as event extraction.  
 
The heuristics-only approach deletes low con-
tent units mechanically while preserving syntac-
tic correctness, as long as parsing is accurate. 
Our preliminary experiments showed that the 
heuristics in Set 0 and Set 1 can generate a com-
paratively satisfying compression, but is sensi-
tive to part-of-speech and parsing errors, e.g. the 
proper noun ??? (Hyundai)? as motor compa-
ny is tagged as an adjective (shown in Figure 1) 
and thus removed since its literal meaning is ??
?(modern)?. Moreover, the rules in Set 1 reduce 
the sentence length in a gross manner, risking 
serious information or grammaticality loss. For 
example, the first clause may not be a complete 
grammatical sentence, and is not always the most 
important clause in the sentence though that is 
usually the case. We also want to point out that 
the heuristics tend to reduce the sentence length 
and preserve the grammar by removing most of 
the modifiers, even though modifiers may con-
tain a lot of important information. 
To address the above problems of heuristics, 
we exploit word significance to measure the im-
portance of each constituent. Set 2 was created to 
work with Set 0 to identify removable low con-
tent units. The heuristics in this approach are 
used only to detect all possible candidates for 
deletion and thus are more general and easier to 
create than Set 1. For instance, we do not need to 
carefully determine which kinds of prepositional 
phrases are safe or dangerous to delete but in-
stead mark all of them as potentially removable.  
The actual word deletion is performed later by 
a compression generation and selection module, 
taking word significance and compression rate 
into consideration. The heuristics in Set 2 are 
able to cover more risky constituents than Set 1, 
e.g. clauses and parallel structures, since the risk 
will be controlled by the later processes. 
 
 ( (IP  
        (NP  
              (*NP (NR ??))                                South Korean  
              (#*ADJP (JJ ??))                            Hyundai 
              (NP     
                       (#*NN ??)                              motor 
                       (NN ??)))                               company 
        (VP (VC ?)                                             is 
              (NP (#*DNP (NP (NR ???))        Volvo 
                                    (DEG ?))                     ?s 
                     (#*ADJP (JJ ??))                     potential 
                     (NP (NN ??))))                        buyer 
        (PU .))) 
 
Figure 1. Parse tree trimming by heuristics 
(#: nodes trimmed out by Set0 & Set1;  
*: nodes labeled as removable by Set0 & Set2.) 
 
Figure 1 shows an example of applying heuris-
tics to the parse tree of the sentence ?????
?????????????? (The South 
Korean Hyundai Motor Company is a potential 
buyer of Volvo.). The heuristics-only approach 
produces ????????? (The South Korean 
company is a buyer.), which is grammatical but 
semantically meaningless. We will see how word 
significance and information density scoring 
produce a better compression in section 3.3. 
50
3.2 Event-based Word Significance 
Based on our observations, a human-compressed 
sentence primarily describes an event or a set of 
relevant events and contains a large proportion of 
named entities, especially in the news article 
domain. Similar to event-based summarization 
(Li et al 2006), we consider only the event 
terms, namely verbs and nouns, with a prefe-
rence for proper nouns. 
The word significance score Ij(wi) indicates 
how important a word wi is to a document j. It is 
a tf-idf weighting scheme with additional weight 
for proper nouns:  
 
?
?
?
?
?
??
?
?
otherwise
nounproperiswifidftf
nouncommonorverbiswifidftf
wI iiij
iiij
ij
,0
,
,
)( ?
 (1) 
where 
wi : a word in the sentence of document j 
tfij :  term frequency of wi in document j 
idfi : inverse document frequency of wi  
? : additional weight for proper noun. 
 
The nodes in the parse tree are then weighted 
by the word significance for leaves or the sum of 
the children?s weights for internal nodes. The 
weighting depends on the word itself regardless 
of its part-of-speech tags in order to overcome 
some part-of-speech errors. 
3.3 Compression Generation and Selection 
In this module, we first apply a greedy algorithm 
to trim the weighted parse tree to obtain a series 
of candidate compressions. Recall that the heu-
ristics Set 0 and 2 have provided the removabili-
ty judgment for each node in the tree. The parse 
tree trimming algorithm is as follows:  
1) remove one node with the lowest weight 
and get a candidate compressed sentence 
2) update  the weights of all ancestors of 
the removed node  
3) repeat until no node is removable  
 
The selection among candidate compressions is a 
tradeoff between sentence length and amount of 
information. Inspired by headlines in news ar-
ticles, most of which contain a large proportion 
of named entities, we create an information den-
sity measurement D(sk) for sentence sk to select 
the best compression: 
                 
)(
)(
)(
k
Pw
i
k sL
wI
sD i
?
??                     (2) 
where 
P : the set of words whose significance scores 
are larger  than ? in (1) 
I(wi) : the significance score of word wi 
L(sk) : the length of sentence in characters  
 
Table 1 shows the effectiveness of information 
density to select a proper compression with a 
balance between length and meaningfulness. Ta-
ble 1 lists all candidate compressions in sequence 
generated from the parse tree in Figure 1. The 
words in bold are considered in information den-
sity. The underlined compression is picked as 
final output as ?????????????
?? (The South Korean Hyundai company is a 
buyer of Volvo.), which makes more sense than 
the one produced by heuristics-only approach as 
????????? (The South Korean company 
is a buyer.). In our approach, ???(Hyundai)? 
tagged as adjective and ?????(Volvo?s)? as 
a modifier to buyer are preserved successfully. 
 
D(s) Sentence  
0.254 ?????????????????. 
The South Korean Hyundai Motor Company 
is a potential buyer of Volvo. 
0.288 ???????????????. 
The South Korean Hyundai Motor Company is 
a buyer of Volvo. 
0.332 ?????????????. 
The South Korean Hyundai Company is a buy-
er of Volvo. 
0.282 ???????????. 
The South Korean company is a buyer of Vol-
vo. 
0.209 ?????????. 
The company is a buyer of Volvo. 
0.0 ?????. 
The company is a buyer. 
 
Table 1. Compression generation and selection 
for the sentence in Figure 1 
 
The compression with highest information 
density is chosen as system output. To achieve a 
better compression rate and avoids overly con-
densed sentences (i.e. very short sentences with 
only a proper noun), we further constrain the 
compression to a limited but varying length 
range [min_length, max_length] according to the 
length of the original sentence: 
 
?
?
? ???
?
?
otherwiselengthoriginal
lengthoriginaliflengthorig
max_length
lengthoriginalmin_length
,_
_,_
},_min{
???
? (3) 
 
51
where 
orig_length : the length of original sentence in 
characters 
?,? : fixed lengths in characters 
 
In contrast to a fixed limitation of length, this 
varying length simulates human behavior in 
creating compression and avoid the overcom-
pression caused by the density selection schema. 
4 Experiments 
4.1 Experiment Setup 
Our experiments were designed to evaluate the 
quality of automatic compression. The evaluation 
corpus is 79 documents from Chinese newswires, 
and the first sentence of each news article is 
compressed.  
The compression of the first sentences in the 
Chinese news articles is a comparatively chal-
lenging task. Unlike English, Chinese often con-
nects two or more self-complete sentences to-
gether without any indicating word or punctua-
tion; this is extremely frequent for the first sen-
tence of news text. The average length of the first 
sentences in the 79 documents is 61.5 characters, 
compared to 46.8 characters for the sentences in 
the body of these news articles.  
We compare the compressions generated by 
four different methods: 
? Human [H]: A native Chinese speaker is 
asked to generate a headline-like compres-
sion (must be a complete sentence, not a 
fragment, and need not preserve original 
SVO structure) based on the first sentence 
of each news article. Only word deletion 
operations are allowed. 
? Heuristics [R]: The heuristics-only ap-
proach mentioned in section 2.1.  
? Heuristics + Word Significance [W]: The 
approach combines heuristics and word 
significance. The parameter ? in (1) is set 
to be 1, which is an upper bound of word?s 
tf-idf value throughout the corpus. 
? Heuristics + Word Significance + Length 
Constraints [L]: Compression is con-
strained to a limited but varying length, as 
mentioned in section 2.3. The length pa-
rameters ? and ? in (3) are set roughly to 
be 10 and 20 characters based on our ex-
perience. 
 
4.2 Human Evaluation 
Sentence compression is commonly evaluated 
by human judgment. Following the literature 
(Knight and Marcu, 2002; Dorr et al 2003; 
Clarke and Lapata, 2006; Cohn and Lapata 
2007), we asked three native Chinese speakers to 
rate the grammaticality of compressions using 
the 1 to 5 scale. We find that all three non-
linguist human judges tend to take semantic cor-
rectness into consideration when scoring gram-
maticality.  
We also asked these three judges to give a list 
of keywords from the original sentence before 
seeing compressions, which they would preserve 
if asked to create a headline based on the sen-
tence. Instead of a subjective score, the informa-
tiveness is evaluated by measuring the keyword 
coverage of the target compression on a percen-
tage scale. The three judges give different num-
bers of keywords varying from 3.33 to 6.51 on 
average over the 79 sentences. 
The compression rate is the ratio of the num-
ber of Chinese characters in a compressed sen-
tence to that in its original sentence. 
The experimental results in Table 2 show that 
our automatically generated compressions pre-
serve grammaticality, with an average score of 
about 4 out of 5, because of the use of linguisti-
cally-motivated heuristics.  
 
 Compres-
sion Rate 
Grammat-
icality 
(1 ~ 5) 
Informa-
tiveness 
(0~100%) 
Human 38.5% 4.962 90.7% 
Heuristics 54.1% 4.114 64.9% 
Heu+Sig 52.8% 3.854 68.8% 
Heu+Sig+L 34.3% 3.664 56.1% 
  
Table 2. Mean rating from human evaluation on 
first sentence compression 
 
Event-based word significance and informa-
tion density increase the amount of important 
information by 6% with similar sentence length, 
but decreases the average grammaticality score 
by 6.5%. This is because the method using word 
significance sacrifices grammaticality to reduce 
the linguistic complexity of the heuristics. None-
theless, this method does improve grammaticali-
ty for 16 of the 79 compressed sentences, typi-
cally for those with POS or parsing errors.  
The compression rates of the two basic auto-
matic approaches are around 53%, while it is 
38.5% for manual compression. This is partially 
because our heuristics only trim the parse tree 
52
but do not transform the structure of it, while a 
human may change the grammatical structure, 
remove more linking words and even abbreviate 
some words. The length constraint boosts the 
compression rate of our combined approach by 
35% with a loss of 18.5% in informativeness and 
5% in grammaticality.  
 
Grammaticali-
ty 
(1 ~ 5) 
Number of 
Sentence 
Compres-
sion Rate 
Informa-
tiveness 
(0~100%) 
Heuristics > 4.5 45 64.1% 75.9% 
Heuristics >= 4 62 54.5% 70.6% 
Heu+Sig  > 4.5 35 59.8% 81.8% 
Heu+Sig  >= 4 57 56.7% 75.8% 
 
Table 3. Compressions with good grammar 
 
We further investigate the performance of our 
automatic system by considering only relatively 
grammatical compressions, as shown in Table 3. 
The compressions which receive an average 
score of more than 4.5 are comparatively reada-
ble.  The combined approach generates 35 such 
compressions among a total of 79 sentences, pre-
serving 81.8% important information on average, 
which is quite satisfying since human-generated 
compression only achieves 90.7%.  
The infomativeness score of human-generated 
compression also demonstrates the difficulty of 
this task. We compare our automatically generat-
ed event words list with the keywords picked by 
human judges. 61.8% of human-selected key-
words are included in the event words list, thus 
considered when calculating information signi-
ficance. This fact demonstrates some success but 
also potential room for improving keyword se-
lection. 
4.3 Some Examples 
We illustrate several representative samples of 
our system output in Table 4. In the first example, 
all three automatic compressions are acceptable, 
though different in preserving important infor-
mation. [W] and [L] concisely contain the WHO, 
WHAT, WHOM information of the event, while 
[R] further preserves the WHY and WHEN in-
formation.  
In the second example, the heuristics-only ap-
proach produced a decent compression by keep-
ing only the first self-complete sub-sentence. The 
weight of word ???(White House)? is some-
what overwhelming and resulted in dense com-
pressions in [W] and [L], which are too short to 
be good. Besides, [W] and [L] in this example 
show that not all the prepositional phrases, noun 
modifiers etc. can be removed in Chinese with-
out affecting grammaticality, though in most 
cases the removals are safe. This is one of the 
main reasons for grammar errors in the compres-
sion results except POS and parsing errors.  
The third example shows how the combined 
approach overcomes POS errors and how length 
constraints avoid overcompression. In [R], ???
?(Nadal)? is deleted because it is mistakenly 
tagged as an adverb modifying the action ?claim 
the victory and progress through?. Since Nadal is 
tagged as proper noun somewhere else in the 
document, its significance makes it survive the 
compression process. [L] produces a perfect 
compression with proper length, information and 
grammar, just as human-made compression. [W] 
selects a very condensed version of compression 
but loses some information.  
 
1.  
[O] ?????????????,???????????
???????????. 
Because both sides were immovable on the drawing of maritime 
borders, a three-day high-level military meeting between North 
and South Korea broke up in discord today. 
[H]??????????????. 
A high-level military meeting between two Koreas broke up in 
discord today. 
[R]??????,??????????????????. 
Because both sides were immovable, a three-day high-level 
meeting between two Koreas broke up in discord today. 
[L]??????????. 
A high-level meeting between two Koreas broke up in discord. 
[W]??????????. 
A high-level meeting between two Koreas broke up in discord. 
2.  
[O]??????????????,??????????
???;??????????????????????
??,??????. 
The White House today called for nuclear inspectors to be sent 
as soon as possible to monitor North Korea?s closure of its nuc-
lear reactors. The White House made this call after US President 
Bush had telephone conversations with South Korean President 
Roh Moo-hyun. 
[H] ????????????????????. 
The White House today called for inspectors to be sent to moni-
tor North Korea?s closure of its nuclear reactors. 
[R]??????????,??????????. 
 The White House today called for inspectors to be sent to moni-
tor North Korea?s closure of its reactors. 
[L]??????????, ???, ????. 
The White House today called for inspectors to be sent. The 
White House is,  made this call. 
[W]???,????. 
The White House is, made this call. 
3.  
[O]??????????,??????,???,????
?????????????. 
Fourth seed Djokovic withdrew from the game, and allowed 
second seed Nadal , who was leading 3-6 , 6-1 , 4-1 , to claim the 
victory and progress through. 
[H]??????????????. 
Djokovic withdrew from the game, and allowed Nadal to claim 
the victory and progress through. 
53
[R]??????,?????,???,?????????
?????. 
Djokovic withdrew from the game, and allowed second seed, 
who was leading 3-6 , 6-1 , 4-1 , to claim the victory and 
progress through. 
 [L]????????????????. 
Djokovic withdrew from the game, and allowed seed Nadal to 
claim the victory and progress through. 
[W]??????. 
Djokovic withdrew from the game. 
4.  
[O]??? 7 ? 31 ????? 30 ???????????
??????????. 
Chinanews.com , July 31 On the 30th Chen Shui-bian questioned 
that members of the judiciary on the island may have tried to get 
involved in elections for leaders in the Taiwan region.  
[H]???????????????????. 
Chen Shui-bian questioned that members of the judiciary may 
get involved in elections for leaders in the Taiwan region. 
[R]??? 7 ? 31 ????? 30 ???????????
????????. 
Chinanews.com , July 31 On the 30th Chen Shui-bian questioned 
that members on the island may have tried to get involved in 
elections for leaders in the Taiwan region.  
[L]??? 30?????????????????. 
On the 30th Chen Shui-bian questioned that members may have 
tried to get involved in elections for leaders in the Taiwan re-
gion. 
[W]??? 30 ?????????????????. 
On the 30th Chen Shui-bian questioned that members may have 
tried to get involved in elections for leaders in the Taiwan re-
gion. 
5.  
[O]??????????????????,?????,
????????????????. 
Patil is India?s first woman presidential candidate, if she is 
elected, she will become India?s first woman president in history. 
[H] ??????????????????. 
Patil is India?s first woman presidential candidate. 
[R]??????????????. 
Patil is the first candidate in the history of India. 
[L]???????,?????????????. 
Patil is the candidate, she will become president of Indian histo-
ry. 
[W]???????. 
Patil is the candidate. 
 
Table 4. Compression examples including human 
and system results, with reference translation 
 (O: Original sentence) 
 
The fourth sample indicates an interesting lin-
guistic phenomenon. The head of the noun 
phrase ???????(members of the judiciary 
on the island)?, ???(members)? cannot stand 
alone making a fluent and valid sentence, though 
all the compressions are grammatically correct. 
Our human assessors also show a preference of 
[R] to [L, W] in grammaticality evaluation, tak-
ing semantic correctness into consideration as 
well. This is probably a reason that our combined 
approach performs worse than heuristic-only ap-
proach in grammaticality. The combined ap-
proach tends to remove risky constituents, but it 
is hard for word significance to control this risk 
properly in every case. This is another of the 
main reasons for bad compression.  
In the fifth sample, all the automatic compres-
sions are grammatically correct preserving well 
the heads of subject and object, but are semanti-
cally incorrect. This case should be hard to han-
dle by any compression approach. 
5 Conclusions and Future Work 
In this paper, we propose a novel approach to 
combine linguistically-motivated heuristics and 
word significance scoring for Chinese sentence 
compression. We take advantage of heuristics to 
preserve grammaticality and not rely on a paral-
lel corpus. We reduce the complexity involved in  
preparing complicated deterministic rules for 
constituent deletion, requiring people only to 
determine potentially removable constituents. 
Therefore, this approach can be easily extended 
to languages or domains for which parallel com-
pression corpora are scarce. The word signific-
ance scoring is used to control the word deletion 
process, pursuing a balance between sentence 
length and information loss. The exploitation of 
event information improves the mechanical rule-
based approach in preserving event-related 
words and overcomes some POS and parsing 
errors.  
The experimental results prove that this com-
bined approach is competitive with a finely-
tuned heuristics-only approach to grammaticality, 
and includes more important information in the 
compressions of the same length.  
In the future, we plan to apply the compres-
sion to Chinese summarization and headline gen-
eration tasks. A careful study on keyword selec-
tion and word weighting may further improve the 
performance of the current system. We also con-
sider incorporating language models to produce 
fluent and natural compression and reduce se-
mantically invalid cases.   
Another important future direction lies in 
creating a parallel compression corpus in Chi-
nese and exploiting statistical and machine learn-
ing techniques. We also expect that an abstrac-
tive approach involving paraphrasing operations 
besides word deletion will create more natural 
compression than an extractive approach.  
 
Acknowledgments 
This work was supported in part by the Defense 
Advanced Research Projects Agency (DARPA) 
under Contract HR0011-06-C-0023.  Any opi-
nions, findings, conclusions, or recommenda-
54
tions expressed in this material are the authors' 
and do not necessarily reflect those of the U.S. 
Government. 
References  
J. Clarke and M. Lapata, 2006. Models for Sentence 
Compression: A Comparison across Domains, 
Training Requirements and Evaluation Measures. 
In Proceedings of the COLING/ACL 2006, Syd-
ney, Australia, pp. 377-384. 
T. Cohn and M. Lapata. 2007. Large Margin Syn-
chronous generation and its application to sentence 
compression. In the Proceedings of the EMNLP/ 
CoNLL 2007, Pragure, Czech Republic, pp. 73-82. 
B. Dorr, D. Zajic and R. Schwartz. 2003. Hedge 
Trimmer: A Parse-and-Trim Approach to Headline 
Generation. In the Proceedings of the 
NAACL/HLT text summarization workshop, Ed-
monton, Canada, pp. 1-8.  
M. Galley and K. McKeown, 2007. Lexicalized Mar-
kov Grammars for Sentence Compression. In the 
Proceedings of NAACL/HLT 2007, Rochester, 
NY, pp. 180-187. 
C. Hori and S. Furui. 2004. Speech Summarization:  
An Approach through Word Extraction and a Me-
thod for Evaluation. IEICE Transactions on Infor-
mation and Systems, E87-D(1): 15-25. 
Z. Huang and M. Harper, 2009. Self-training PCFG 
Grammars with Latent Annotations Across Lan-
guages. In the proceedings of EMNLP 2009, Sin-
gapore.  
H. Jing. 2000. Sentence Reduction for Automatic 
Text Summarization. In Proceedings of the 6th 
ANLP, Seattle, WA, pp. 310-315.  
K. Knight and D. Marcu, 2002. Summarization 
beyond Sentence Extraction: a Probabilistic Ap-
proach to Sentence Compression. Artificial Intelli-
gence, 139(1): 91-107. 
W. Li, W. Xu, M. Wu, C. Yuan and Q. Lu. 2006. Ex-
tractive Summarization using Inter- and Intra- 
Event Relevance. In the Proceedings of COL-
ING/ACL 2006, Sydney, Australia, pp 369-376. 
R. McDonald. 2006. Discriminative Sentence Com-
pression with Soft Syntactic Constraints. In the 
Proceedings of 11th EACL, Trento, Italy, pp. 297-
304. 
M. L. Nguyen, A. Shimazu, S. Horiguchi, T. B. Ho 
and M. Fukushi. 2004. Probabilistic Sentence Re-
duction using Support Vector Machines. In Pro-
ceedings of the 20th COLING, Geneva, Switzer-
land, pp. 743-749.  
K. McKeown, R. Barzilay, S. Blair-Goldensohn, D. 
Evans, V. Hatzivassiloglou, J. Klavans, A. Nenko-
va, B. Schiffman and S. Sigelman. 2002. The Co-
lumbia Multi-Document Summarizer for DUC 
2002. In the Proceedings of the ACL workshop on 
Document Understanding Conference (DUC) 
workshop, Philadelphia, PA, pp. 1-8.  
K. Parton, K. McKeown, R. Coyne, M. Diab, R. 
Grishman, D. Hakkani-T?r, M. Harper, H. Ji, W. 
Ma, A. Meyers, S. Stolbach, A. Sun, G. Tur, W. 
Xu and S. Yaman. 2009. Who, What, When, 
Where, Why? Comparing Multiple Approaches to 
the Cross-Lingual 5W Task. In the Proceedings of 
ACL-IJCNLP, Singapore. 
J. Turner and E. Charniak. 2005. Supervised and Un-
supervised Learning for Sentence Compression. In 
the Proceedings of 43rd ACL, Ann Arbor, MI, pp. 
290-297. 
 
 
55
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 680?688,
Beijing, August 2010
Filtered Ranking for Bootstrapping in Event Extraction 
Shasha Liao Dept. of Computer Science New York University liaoss@cs.nyu.edu 
Ralph Grishman Dept. of Computer Science New York University  grishman@cs.nyu.edu 
 Abstract Several researchers have proposed semi-supervised learning methods for adapting event extraction systems to new event types. This paper investigates two kinds of bootstrapping methods used for event extraction: the document-centric and similarity-centric approaches, and proposes a filtered ranking method that combines the advantages of the two. We use a range of extraction tasks to compare the generality of this method to previous work. We analyze the results using two evaluation metrics and observe the effect of different training corpora. Experiments show that our new ranking method not only achieves higher performance on different evaluation metrics, but also is more stable across different bootstrapping corpora. 1 Introduction The goal of event extraction is to identify instances of a class of events in text, along with the arguments of the event (the participants, place, and time). In this paper we shall focus on the sub-problem of identifying the events themselves. Event extraction systems from the early and mid 90s relied primarily on hand-coded rules, which must be written anew for every task. Since then, supervised and semi-supervised methods have been developed in order to build systems for new scenarios more easily. Supervised methods can perform quite well with enough training data, but annotating sufficient data may require months of labor. 
Semi-supervised methods aim to reduce the annotated data required, ideally to a small set of seeds. Most semi-supervised event extractors seek to learn sets of patterns consisting of a predicate and some lexical or semantic constraints on its arguments. The semi-supervised learning was based primarily on one of two assumptions: the document-centric approach, which assumes that relevant patterns should appear more frequently in relevant documents (Riloff 1996; Yangarber et al 2000; Yangarber 2003; Surdeanu et al2006); and the similarity-centric approach, which assumes that relevant patterns should have lexically related terms (Stevenson and Greenwood 2005, Greenwood and Stevenson 2006). An effective semi-supervised extractor will have good performance over a range of extraction tasks and corpora. However, many of the learning procedures just cited have been tested on only one or two extraction tasks, so their generality is uncertain. To remedy this, we have tested learners based on both assumptions, targeting both a MUC (Message Understanding Conference) scenario and several ACE (Automatic Content Extraction) event types. We identify shortcomings of the prior bootstrapping methods, propose a more effective and stable ranking method, and consider the effect of different corpora and evaluation metrics. 2 Related Work The basic assumption of the document-centric approach is that documents containing a large number of patterns already identified as relevant to a particular IE scenario are likely to contain further relevant patterns. Riloff (1996) initiated 
680
this approach and claimed that if a corpus can be divided into documents involving a certain event type and those not involving that type, patterns can be evaluated based on their frequency in relevant and irrelevant documents. Yangarber et al (2000) incorporated Riloff?s metric into a bootstrapping procedure, which started with several seed patterns but required no manual document classification or corpus annotation.  The seed patterns were used to identify some relevant documents, and the top-ranked patterns (based on their distribution in relevant and irrelevant documents) were added to the seed set. This process was repeated, assigning a relevance score to each document based on the relevance of the patterns it contains and gradually growing the set of relevant patterns. This approach was further refined by Surdeanu et al (2006), who used a co-training strategy in which two classifiers seek to classify documents as relevant to a particular scenario. Patwardhan and Riloff (2007) presented an information extraction system that find relevant regions of text and applies extraction patterns within those regions. They created a self-trained relevant sentence classifier to identify relevant regions, and use a semantic affinity measure to automatically learn domain-relevant extraction patterns. They also distinguish primary patterns from secondary patterns and apply the patterns selectively in the relevant regions. Stevenson and Greenwood (2005) (henceforth ?S&G?) suggested an alternative method for ranking the candidate patterns. Their approach relied on the assumption that useful patterns will have similar lexical items to the patterns that have already been accepted. They used WordNet to calculate word similarity. They chose to represent each pattern as a vector consisting of the lexical items and used a version of the cosine metric to determine the similarity between pairs of patterns. Later, Greenwood and Stevenson (2006) introduced a structural similarity measure that could be applied to extraction patterns consisting of linked dependency chains. 3 Ranking Methods in Bootstrapping Most semi-supervised event extraction systems are based on patterns with variables which have semantic type constraints. A simple example is ?organization appoints person as position?; if 
this pattern matches a passage in a test document, a hiring event will be instantiated with the items matching the variables being the arguments of the event. So training an event extractor becomes primarily a task of acquiring these patterns. In a semi-supervised setting, this involves ranking candidate patterns and accepting the top-ranked patterns at each iteration.  Our goal was to create a more robust learner through improved pattern ranking. 3.1 Problems of Document-centric Bootstrapping Document-centric bootstrapping tries to find patterns with high frequency in relevant documents and low frequency in irrelevant documents. The assumption is that descriptions of the same event or the same type of event may occur multiple times in a document, and so a document containing a relevant pattern is more likely to contain more such patterns. This approach may end up extracting patterns for related events; for example, start-position often comes with end-position events. This effect may be salutary if the extraction scenario includes these related events (as in MUC-6), but will pose a problem if the goal is to extract individual event types. Also, because an extra corpus for bootstrapping is needed, different corpora might perform quite differently (see Figure 2). 3.2 Problems of Similarity-centric Bootstrapping Similarity-centric bootstrapping tries to find patterns with high lexical similarities. The most crucial issue is how to evaluate the similarity of two patterns, which is based on the similarity of two words. In this strategy, no extra corpus is needed, which eliminates the effort to find a good bootstrapping corpus, but a semantic dictionary that can provide word similarity is required. S&G used WordNet1 to provide word similarity information. However, in the similarity-centric approach, lexical polysemy can lead the bootstrapping down false paths. For example, for start-position (hire) events, ?name? and ?charge? are in the same Synset as appoint, but including these words is quite dangerous because they contain other common senses                                                            1http://wordnet.princeton.edu/ 
681
unrelated to start-position events. For die events, we might have words like ?go? and ?pass?, which are also used in very specific contexts when they refer to ?die?. If similarity-centric ranking extracts patterns including these words, performance will deteriorate very quickly, because most of the time, these words do not predicate the proper event, and more and more wrong patterns will be extracted. 3.3 Our Approach We propose a new ranking method, which constrains the document-centric and similarity-centric assumptions, and makes a more restricted assumption: patterns that appear in relevant documents and are lexically similar are most likely to be relevant. This method limits the effect of ambiguous patterns by narrowing the search to relevant documents, and limits irrelevant patterns in relevant documents by word similarity restriction.  For example, although ?charge? has high word similarity to ?appoint?, its document relevance score is very low, and we will not include this word in bootstrapping starting from ?appoint?. Many different combinations are possible; we propose one that uses the word similarity as a filter. The document relevance score is first applied to rank the patterns in relevant documents, then the patterns with lexical similarity scores below a similarity threshold will be removed from the ranking; only patterns above threshold will be added to the seeds. However, if in the current iteration, no pattern meets the threshold, the threshold will be lowered until new patterns can be found. We call this ranking method filtered ranking2: 
? 
Filter(p) =
Yangarber(p) Stevenson(p) >= t
0 otherwise
?? 
?? 
??  where t is the threshold, which is initialized to 0.9 in our experiments. 4 System Description Our approach is similar to that for document-centric bootstrapping, but the ranking                                                            2 We also tried using the product of the document relevance score and word similarity score, and found the results to be quite similar. Due to space limitations, we do not report these results here.  
function is changed to incorporate lexical similarity information. For our experiments bootstrapping was terminated after a fixed number of iterations; in practice, we would monitor performance on a held-out (dev-test) sample and stop when it declines for k iterations. 4.1 Pre-processing Instead of limiting ourselves to surface syntactic relations, we want to get more general and meaningful patterns. To this end, we used semantic role labeling (Gildea and Jurafsky, 2002) to generate the logical grammatical and predicate-argument representation automatically from a parse tree (Meyers et al 2009). The output of the semantic labeling is the dependency representation of the text, where each sentence is a graph consisting of nodes (corresponding to words) and arcs. Each arc captures up to three relations between two words: (1) a SURFACE relation, the relation between a predicate and an argument in the parse tree of a sentence; (2) a LOGIC1 (grammatical logical) relation which regularizes for lexical and syntactic phenomena like passive, relative clauses, and deleted subjects; and (3) a LOGIC2 (predicate-argument) relation corresponding to relations in PropBank (Palmer et al 2005) and NomBank  In constructing extraction patterns from this graph, we take each dependency link along with its predicate-argument role; if that role is null, we use its logical grammatical role, and finally, its surface role. For example, for the sentence: John is hit by Tom?s brother. we generate the patterns: <Arg1 hit John> <Arg0 hit brother> <T-pos brother Tom> where the first two represent LOGIC2 relations and the third a SURFACE relation.  To reduce data sparseness, all inflected words are changed to their root form (e.g. ?attackers???attacker?), and all names are replaced by their ACE type (person, organization, location, etc.), so the first pattern would become <Arg1 hit PERSON> 4.2 Document-based Ranking The document-centric method employs a 
682
re-implementation of the procedure described in (Yangarber et al 2000), using the disjunctive voting scheme for document relevance.  At each iteration i we compute a precision score Preci(p) for each pattern p and a relevance score Reli(d) for each document d.  Initially the seed patterns have precision 1 and all other patterns precision 0.  These are updated by 
? 
Re l
i
(d) =1? (1?Prec
i
(p))
p?K (d )
?
 
where K(d) is the set of accepted patterns  that match document d, and 
? 
Prec
i+1
(p) =
1
| H(p) |
? Re l
i
(d)
d ?H (p )
?
 
where H(p) is the set of documents matching pattern p.  Patterns are then ranked by 
? 
RankFun
Yangarber
(p) =
Sup(p)
H(p)
* logSup(p)  
where  (a generalization of Yangarber?s metric), and the top-ranked candidates are added to the set of accepted patterns. 4.3 Pattern Similarity  For two words, there are several ways to measure their similarity using WordNet, which can be roughly divided into two categories: distance-based, including Leacock and Chodorow (1998), Wu and Palmer (1994); and information content based, including Resnik (1995), Lin (1998), and Jiang and Conrath (1997). We follow S&G (2005)?s method and use the semantic similarity of concepts based on Information Content (IC). Every pattern consists of a predicate and a constraint (?argument?) on its local syntactic context, and so the similarity of two patterns depends on the similarity of the predicates and the similarity of the arguments.  We modified S&G?s structural similarity measure to reflect some differences in pattern structure: first, S&G only focus on patterns headed by verbs, while we include verbs, nouns and adjectives; second, they only record the subject and object to a verb, while we record all argument relations; third, 
our patterns only contain a predicate and a single constraint (argument), while their pattern might contain two arguments, subject and object. With two arguments, many more patterns are possible and the vector similarity calculation over all patterns in a large corpus becomes very time consuming. We do not limit ourselves to verb patterns because nouns and (occasionally) adjectives can also represent an event. For example, ?Stevenson?s promotion is a signal ?? expresses a start-position event. Moreover, in our pattern, we assume that the predicate is more important than constraint, because it is the root (head) of the pattern in the semantic graph structure, and place different weights on predicate and constraint. Finally, the similarity of two patterns p1 and p2 is computed as follows: 
 where ?+?=1, f represents a predicate, r represent a role, and a represent an argument. In our experiment, ? is set to 0.6 and ? is set to 0.4. The role similarity is 1 for identical roles and for roles which generally correspond at the syntactic and predicate-argument level (arg0 ? subj; arg1 ? obj); selected other role pairs are assigned a small positive similarity (0.1 or 0.2), and others 0. As with the document-centric method, bootstrapping begins by accepting a set of seed patterns. At each iteration, the procedure computes the similarity between all patterns in the training corpus and the currently accepted patterns and accepts the most similar pattern(s). In S&G?s experiments the evaluation corpus also served as the training corpus. 5 Experiments There have been two types of event extraction tasks. One involved several ?elementary? event types, such as ?attack?, ?die?, ?injure? etc.; for example, the ACE 2005 evaluation3 used a set of 33 event types and subtypes. The other type involved a scenario ? a set of related events, like ?attacks and the damage, injury, and death they cause?, or ?arrest, trial, sentencing etc.?. The                                                            3See http://projects.ldc.upenn.edu/ace/docs/English-Events- Guidelines_v5.4.3.pdf for a description of this task. 
683
MUC evaluations included two scenarios that have been the subject of considerable research on learning methods: terrorist incidents (MUC-3/4) and executive succession (MUC-6). We conducted experiments on the MUC-6 task to make a comparison to previous work. We also did experiments on ACE 2005 data, because it provides many distinct event types; we conducted experiments on three disparate event types: attack, die, and start-position. Note that MUC-6 identifies a scenario while ACE identifies specific event types, and types which are in the same MUC scenario might represent different ACE events. For example, the executive succession scenario (MUC-6) includes the start-position and end-position events in ACE.  5.1 Data Description There are four corpora used in the experiments: MUC-6 corpora ? Bootstrapping: pre-selected data from the Reuters corpus (Rose et al 2002) from 1996 and 1997, including 3000 related documents and 3000 randomly chosen unrelated documents ? Evaluation: MUC-6 annotated data, including 200 documents (official training and test). We were guided by the MUC-6 key file in annotating every document and sentence as relevant or irrelevant. ACE corpora ? Bootstrapping: untagged data from the Gigaword corpus from January 2006, including 14,171 English newswire articles from Agence France-Presse (AFP). ? Evaluation: ACE 2005 annotated (training) data, including 589 documents 5.2 Parameters used in Experiments In our bootstrapping process, we only extract patterns appearing more than 2 times in the corpus, and the similarity filter threshold is originally set to 0.9. If no patterns are found, it is reduced by 0.1 until new patterns are found.  In each iteration, the top 3 patterns in the ranking function will be added to the seeds. For the similarity-centric method, only patterns appearing more than 2 times and in less than 30% of the documents will be extracted, which is the same as S&G?s approach. 
5.3 MUC-6 Experiments Our overall goal was to demonstrate that filtered ranking was in all cases competitive with and in at least some cases clearly superior to the earlier methods, over a range of extraction tasks and bootstrapping corpora. We began with the MUC-6 task, where the efficacy of the earlier methods had already been demonstrated.  < Arg0 resign Person > < Arg1 appoint Person > < Arg0 appoint Org_commercial> <Arg1 succeed Person > Table 1. Seeds for MUC-6 evaluation  For MUC-6 evaluation, we follow S&G?s approach and assess extraction patterns by their ability to identify event-relevant sentences.4 The system treats a sentence as relevant if it matches an extraction pattern. Bootstrapping starts from four seeds which yield 80% precision and 24% recall for sentence filtering.  To compare with previous work, we tested the filtered ranking method on two corpora: the first is the Reuters corpus used in S&G?s recreation of Yangarber?s experiment (Filter1), to compare with their results for the document-centric method; the second uses the test corpus as S&G did (Filter2), to compare with their results for the similarity-centric method. We compare methods based on peak F score; in practice, this would mean controlling the bootstrapping using a held-out test sample.   
 Figure 1. F score for different ranking methods on MUC-6 evaluation  Figure 1 showed that the filtered ranking                                                            4 We also tried the document filtering evaluation introduced by Yangarber but, as S&G observed, this metric is too insensitive because over 50% of the documents in the MUC-6 test set are relevant. 
684
methods edge out both document and similarity-centric methods.  Our scores are comparable to S&G?s, although they report somewhat better performance for similarity-centric than for document-centric (55 vs. 51) whereas document-centric did better for us. This difference may reflect differences in pattern generation (discussed above) and possibly differences in the specific corpora used. However, document-centric bootstrapping needs an extra corpus for bootstrapping; S&G used a pre-selected corpus that contains approximately same number of relevant and irrelevant documents5. We wanted to check if such a corpus is essential for the document-centric method, and if the need for pre-selection can be reduced through filtered ranking. Thus, we set up another experiment to see if the document-centric method is stable or sensitive to different corpora. We used two additional corpora for MUC-6 evaluation: one is a subset of the Wall Street Journal (WSJ) 1991 corpus, which contains 18,734 untagged documents; the other is the Gigaword AFP corpus described in section 5.1. Both corpora are much larger than the Reuters corpus, and while we do not have precise information about relevant document density, the WSJ contains quite a few start-position events because it is primarily business news; the Gigaword corpus (AFP newswire) has fewer start-position events because it contains a wider variety of news.   
 Figure 2. Document-centric and Filtered ranking results on different corpora for MUC-6   Figure 2 showed that the document-centric method performs quite differently on different corpora, which indicates that a pre-selected corpus plays an important role in                                                            5 The pre-selection of relevant and irrelevant documents is based on document meta-data provided as part of the Reuters Corpus Volume I (Rose et al, 2002). 
document-centric ranking. It suggests that the percentage of relevant documents may be more important than the overall corpus size. The figure also shows that filtered ranking is much more stable across different corpora. Richer corpora still have better peak performance, but the difference is not quite as great; also, peak performance on a given corpus is consistently better than the document-centric method. From the above experiments, we conclude that our filtering method is better in two aspects: first, bootstrapping on the same corpus performs better than either document or similarity-centric methods; second, if we can not get a corpus with an assured high density of relevant documents, it is safer to use filtered ranking because it is more stable across different corpora. 5.4 ACE2005 Experiments The ACE2005 corpus includes annotations for 33 different event types and subtypes, offering us an opportunity to assess the generality of our methods across disparate event types. We selected 3 event types to report on here: ? Die: ?occurs whenever the life of a PERSON Entity ends. It can be accidental, intentional or self-inflicted.? This event appears 535 times in the corpus. ? Attack: ?is defined as a violent physical act causing harm or damage.? Attack events include a variety of sub-events like ?person attack person?, ?country invade country?, and ?weapons attack locations?. This event type appears 1120 times. ? Start-Position: ?occurs whenever a PERSON Entity begins working for (or changes offices within) an ORGANIZATION or GPE. This includes government officials starting their terms, whether elected or appointed?. It appears 116 times in the corpus. We choose these three event types because they reflect the diversity of events ACE annotated: die events appear frequently in the ACE corpus and its definition is very clear; attack events also appear frequently, but its definition is rather complicated and contains several different sub-events; start-position?s definition is clear, but it is relatively infrequent in the corpus. Based on the observations from the MUC-6 corpus, we eschewed corpus pre-selection for 
685
two reasons: first, building a different corpus for training each event type is an extra burden in developing a system for handling multiple events; second, we want to demonstrate that filtered ranking would work without pre-selection, while the document-centric method does not. As a result, we used the Gigaword AFP corpus for all event types. In the ACE 2005 corpus, for every event, the annotators recorded a trigger, which is the main word that most clearly expresses an event occurrence. This added information allowed us to conduct dual evaluations: one based on sentence relevance - following S&G - presented in section 5.4.2, and one based on trigger identification, presented in section 5.4.3. 5.4.1 ACE2005 Supervised Model To provide a benchmark for our semi-supervised learners, we built a very simple pattern-based supervised learning model. For training, for every pattern, we count how many times it contains an event trigger and how many times it does not. If more than 50% of the time it contains an event trigger, we treat it as a positive pattern.  For sentence level evaluation, if there is a positive pattern in a sentence, we tag this sentence as relevant; otherwise not. For word level evaluation, if the word is the predicator of a positive pattern, we tag it as a trigger; otherwise not6.  We did a 5-fold cross-validation on the ACE 2005 data, report the average results and compare it to the semi-supervised learning method (see figure 3 & 4). 5.4.2 Sentence level ACE Event Evaluation7 Different event types have quite different performance (see figure 3): for the die event, the peak performance of all methods is quite good, and quite close to the supervised result; for the attack event, filtered ranking performs much better than both document and similarity-centric                                                            6For word-level evaluation, we only consider trigger words with at least one semantic argument such as subject, object or a preposition; for that reason the performance is quite different from sentence level evaluation. We did the same for the word-level evaluation of semi-supervised learning.  7 We do not list Attack seed patterns here as there are 34 patterns used. 
methods, but still worse than the supervised method; for start-position events, the semi-supervised method beats the supervised method. The reason might be as follows: Die events appear frequently in ACE 2005, and most instances correspond to a small number of forms, so it is easy to find the correct patterns both from WordNet or related documents. As a result, filtered ranking provides no apparent benefit.  Attack is a more complicated event including several sub-events, which also have a lot of related events like die and injure. As a result, the document-centric method?s performance goes down much faster, because patterns for related event types get drawn in; while the similarity-centric method performs worse than filtered ranking because some ambiguous words are introduced. For example, ?hit? is an attack trigger, but words in the same Synset, such as ?reach?, ?make?, ?attain?, ?gain? are quite dangerous because most of the time, these words do not refer to an attack event. Start-position events do not appear frequently in ACE 2005, and supervised learning cannot achieve good performance because it can?t collect enough training samples. The similarity-centric and Filter2 methods, which also depend on the ACE 2005 corpus, do not perform well either. Filter1 performs quite well because the Gigaword AFP corpus is quite large and contains more relevant documents, although the percentage is very small. This confirms our assumption that filtered ranking can achieve reasonable performance on a large unselected corpus, which is especially useful when the event is rare in the evaluation corpus.  <Arg1 kill Person> <Arg1 slay Person> <Arg1 death Person> Table 2. Seeds for Ace 2005 Die evaluation  <Arg0 hire ORG> <Arg1 hire Person> <Arg1 appoint Person> <Arg0 appoint ORG> Table 3. Seeds for Ace 2005 Start-Position evaluation  
686
 Figure 3. Performance on different ranking methods on ACE2005 sentence level evaluation  
 Figure 4. Performance on different ranking methods on ACE2005 word level evaluation  5.4.3 Word-level ACE Event Evaluation Word-level evaluation is different from sentence-level evaluation because patterns which appear around an event but do not predicate an event are penalized in this evaluation. For example, the pattern <Sbj chairman PERSON>, which arises from a phrase like ?PERSON was the chairman of COMPANY?, appears much more in relevant start-position sentences than irrelevant sentences, and adding this pattern to the seeds will improve performance using the relevant-sentence metric. We would prefer a metric which discounted such patterns. As noted above, ACE event annotations contain triggers, which are more specific event locators than a sentence, and we use this as the basis for a more specific evaluation. Extracted patterns are used to identify event triggers instead of identifying relevant sentences. For every word w in the ACE corpus, we extract all the patterns whose predicate is w. If the event extraction patterns include one of these patterns, we tag w as a trigger.  In word level evaluation, document-centric performs worse than the other methods. The reason is that some patterns appear often in the 
context of an event and are positive patterns for sentence level evaluation, but they do not actually predicate an event and are negative patterns in word level evaluation. In this situation, the document-centric method performs worse than the similarity-centric method, because it extracts many such patterns. For example, of the sentences which contain die events, 29% also contain attack events.  Thus in word level evaluation, filtered ranking continues to outperform either document- or similarity-centric methods, and its advantage over document-centric methods is accentuated. 6 Conclusions In this paper, we propose a new ranking method in bootstrapping for event extraction and investigate the performance on different bootstrapping corpora with different ranking methods. This new method can block some irrelevant patterns coming from relevant documents, and, by preferring patterns from relevant documents, can eliminate some lexical ambiguity. Experiments show that this new ranking method performs better than previous ranking methods and is more stable across different corpora.  
687
References  D. Gildea and D. Jurafsky. 2002. Automatic Labeling of Semantic Roles. Computational Linguistics, 28:245?288. MA Greenwood, M. Stevenson. 2006. Improving semi-supervised acquisition of relation extraction patterns. Proceedings of the Workshop on Information Extraction Beyond the Document, pages 29?35. Jay J. Jiang and David W. Conrath. 1997. Semantic Similarity Based on Corpus Statistics and Lexical Taxonomy. In Proceedings of International Conference Research on Computational Linguistics (ROCLING X), Taiwan C. Leacock and M. Chodorow. 1998. Combining local context and WordNet similarity for word sense identification. In C. Fellbaum, editor, WordNet: An electronic lexical database, pages 265?283. MIT Press. D. Lin. 1998. An information-theoretic definition of similarity. In Proceedings of the International Conference on Machine Learning, Madison, August. A. Meyers, M. Kosaka, N. Xue, H. Ji, A. Sun, S.  Liao and W. Xu. 2009. Automatic Recognition of Logical Relations for English, Chinese and Japanese in the GLARF Framework. In SEW-2009 (Semantic Evaluations Workshop) at NAACL HLT-2009 MUC. 1995. Proceedings of the Sixth Message Understanding Conference (MUC-6), San Mateo, CA. Morgan Kaufmann. Martha Palmer, Dan Gildea, and Paul Kingsbury, The Proposition Bank: A Corpus Annotated with Semantic Roles, Computational Linguistics, 31:1, 2005. Siddharth Patwardhan, Satanjeev Banerjee, and Ted Pedersen. 2003. Using measures of semantic relatedness for word sense disambiguation. In Proceedings of the Fourth International Conference on Intelligent Text Processing and Computational Linguistics, Mexico City, Mexico.  Patwardhan, S. and Riloff, E. 2007. Effective Information Extraction with Semantic Affinity Patterns and Relevant Regions. Proceedings of the 2007 Conference on Empirical Methods in Natural Language Processing (EMNLP-07) T. Pedersen, S. Patwardhan, and J. Michelizzi. 2004. WordNet::Similarity - Measuring the Relatedness of Concepts. In Proceedings of the Nineteenth 
National Conference on Artificial Intelligence (Intelligent Systems Demonstrations), pages 1024-1025, San Jose, CA, July 2004. P. Resnik. 1995. Using information content to evaluate semantic similarity in a taxonomy. In Proceedings of the 14th International Joint Conference on Artificial Intelligence, pages 448?453, Montreal, August. Ellen Riloff. 1996. Automatically Generating Extraction Patterns from Untagged Text. In Proc. Thirteenth National Conference on Artificial Intelligence (AAAI-96), 1996, pp. 1044-1049. T. Rose, M. Stevenson, and M. Whitehead. 2002. The Reuters Corpus Volume 1 - from Yesterday?s news to tomorrow?s language resources. In LREC-02, pages 827?832, La Palmas, Spain. M. Stevenson and M. Greenwood. 2005. A Semantic Approach to IE Pattern Induction. Proceedings of ACL 2005. Mihai Surdeanu, Jordi Turmo, and Alicia Ageno. 2006. A Hybrid Approach for the Acquisition of Information Extraction Patterns. Proceedings of the EACL 2006 Workshop on Adaptive Text Extraction and Mining (ATEM 2006) Z. Wu and M. Palmer. 1994. Verb semantics and lexical selection. In 32nd Annual Meeting of the Association for Computational Linguistics, pages 133?138, Las Cruces, New Mexico. Roman Yangarber; Ralph Grishman; Pasi Tapanainen; Silja Huttunen. 2000. Automatic Acquisition of Domain Knowledge for Information Extraction. Proc. COLING 2000. Roman Yangarber. 2003. Counter-Training in Discovery of Semantic Patterns. Proceedings of ACL2003  
688
Coling 2010: Poster Volume, pages 1194?1202,
Beijing, August 2010
Semi-supervised Semantic Pattern Discovery with Guidance 
from Unsupervised Pattern Clusters 
Ang Sun 
Computer Science Department 
New York University 
asun@cs.nyu.edu 
Ralph Grishman 
Computer Science Department 
New York University 
grishman@cs.nyu.edu 
 
Abstract 
We present a simple algorithm for 
clustering semantic patterns based on 
distributional similarity and use cluster 
memberships to guide semi-supervised 
pattern discovery. We apply this 
approach to the task of relation 
extraction. The evaluation results 
demonstrate that our novel 
bootstrapping procedure significantly 
outperforms a standard bootstrapping. 
Most importantly, our algorithm can 
effectively prevent semantic drift and 
provide semi-supervised learning with a 
natural stopping criterion. 
1 Introduction 
The Natural Language Processing (NLP) 
community faces new tasks and new domains 
all the time. Without enough labeled data of a 
new task or a new domain to conduct supervised 
learning, semi-supervised learning (SSL) is 
particularly attractive to NLP researchers since 
it only requires a handful of labeled examples, 
known as seeds.  SSL starts with these seeds to 
train an initial model; it then applies this model 
to a large volume of unlabeled data to get more 
labeled examples and adds the most confident 
ones as new seeds to re-train the model. This 
iterative procedure has been successfully 
applied to a variety of NLP tasks, such as 
hypernym/hyponym extraction (Hearst, 1992), 
word sense disambiguation (Yarowsky, 1995), 
question answering (Ravichandran and Hovy, 
2002), and information extraction (Brin, 1998; 
Collins and Singer, 1999; Riloff and Jones, 
1999; Agichtein and Gravano, 2000; Yangarber 
et al, 2000; Chen and Ji, 2009).  
While SSL can give good performance for 
many tasks, it is a procedure born with two 
defects. One is semantic drift.  When SSL is 
under-constrained, the semantics of newly 
promoted examples might stray away from the 
original meaning of seed examples as discussed 
in (Brin, 1998; Curran et al, 2007; Carlson et 
al., 2010). For example, a SSL procedure to 
learn semantic patterns for the LocatedIn 
relation (PERSON in LOCATION/GPE1) might 
accept patterns for the Employment relation 
(employee of GPE / ORGANIZATION) 
because many unlabeled pairs of names are 
connected by patterns belonging to multiple 
relations. Patterns connecting <Bill Clinton, 
Arkansas> include LocatedIn patterns such as 
?visit?, ?arrive in? and ?fly to?, but also patterns 
indicating other relations such as ?governor of?, 
?born in?, and ?campaign in?. Similar analyses 
can be applied to many other examples such as 
<Bush, Texas> and <Schwarzenegger, 
California>. Without careful design, SSL 
procedures usually accept bogus examples 
during certain iterations and hence the learning 
quality degrades.  
The other shortcoming of SSL is its lack of 
natural stopping criteria. Most SSL algorithms 
either run a fixed number of iterations 
(Agichtein and Gravano, 2000) or run against a 
separate labeled test set to find the best stopping 
criterion (Abney, 2008). The former solution 
needs a human to keep eyeballing the learning 
quality of different iterations and set ad-hoc 
thresholds accordingly. The latter requires a 
                                                 
1 These are the types of relations and names used in 
the NIST-sponsored ACE evaluation. 
http://www.itl.nist.gov/iad/mig//tests/ace/. GPE 
represents a Geo-Political Entity ? an entity with 
land and a government. 
1194
separate labeled test set for each new task or 
domain. They make SSL less appealing than it 
could be since the intention of using SSL is to 
minimize supervision.  
In this paper, we propose a novel learning 
framework which can automatically monitor the 
semantic drift and find a natural stopping 
criterion for SSL. Central to our idea is that 
instead of using unlabeled data directly in SSL, 
we first cluster the seeds and unlabeled data in 
an unsupervised way before conducting SSL. 
The semantics of unsupervised clusters are 
usually unknown. However, the cluster to which 
the seeds belong can serve as the target cluster. 
Then we guide the SSL procedure using the 
target cluster. Under such learning settings, 
semantic drift can be automatically detected and 
a stopping criterion can be found:  stopping the 
SSL procedure when it tends to accept examples 
belonging to clusters other than the target 
cluster.  
We demonstrate in this paper the above 
general idea by considering a bootstrapping 
procedure to discover semantic patterns for 
extracting relations between named entities 
(NE). Standard bootstrapping usually starts with 
some high-precision and high frequency seed 
patterns for a specific relation to match named 
entities, then it uses newly promoted entities to 
search for additional confident patterns 
connecting them. It is a procedure driven by the 
duality between patterns and entities: a good 
pattern can connect more than one pair of 
named entities and a pair of named entities is 
usually connected by more than one good 
pattern.  
We present a new bootstrapping procedure in 
which we first cluster the seed and other 
patterns in a large corpus based on distributional 
similarity. We then guide the bootstrapping 
using the target cluster.  
The next section describes our unsupervised 
pattern clusters. Section 3 presents the details of 
our novel bootstrapping procedure with 
guidance from pattern clusters. We evaluate our 
algorithms in Section 4 and present related work 
in Section 5. We draw conclusions and point to 
future work in Section 6. 
2 Pattern Clusters 
2.1 Distributional Hypothesis 
The Distributional Hypothesis (Harris, 1954) 
states that words that tend to occur in similar 
contexts tend to have similar meanings. Lin and 
Pantel (2001) extended this hypothesis to cover 
patterns (dependency paths in their case). The 
idea of the extension is that if two patterns tend 
to occur in similar contexts then the meanings 
of the patterns tend to be similar. For example, 
in ?X solves Y? and ?X finds a solution to Y?, 
?solves? and ?finds a solution to? share many 
common Xs and Ys and hence are similar to 
each other. This extended distributional 
hypothesis serves as the basis on which we 
compute similarities for each pair of patterns. 
2.2 Pattern Representation ? Shortest 
Dependency Path 
We adopt a shortest dependency path (SDP) 
representation of relation patterns. SDP has 
demonstrated its power in kernel methods for 
relation extraction (Bunescu and Mooney, 2005). 
Its capability in capturing most of the 
information of interest is also evidenced by a 
systematic comparison of effectiveness of 
different information extraction (IE) patterns in 
(Stevenson and Greenwood, 2006) 2 . For 
example, ?nsubj ? met ? prep_in? is able to 
represent LocatedIn between ?Gates? and 
?Seattle? while a token-based pattern would be 
much less general because it would have to 
specify all the intervening tokens. 
 
Figure 1.  Stanford dependency tree for sentence 
?Gates, Microsoft?s chairman, met with President 
Clinton in Seattle?.  
 
                                                 
2 SDP is equivalent to the linked chains described in 
Stevenson and Greenwood (2006) when the 
dependency of a sentence is represented as a tree not 
a graph. 
1195
2.3 Pre-processing 
We tag and parse each sentence in our corpus 
with the NYU named entity tagger 3  and the 
Stanford dependency parser. Then for each pair 
of names in the dependency tree, we extract the 
SDP connecting them. Names in the path are 
replaced by their types. We require SDP to 
contain at least one verb or noun. We use the 
base form of words in SDP. We also require the 
length of the path (defined as the number of 
dependency relations and words in it) to be 
between 3 and 7. Short paths are more likely to 
be generic patterns such as ?of? and can be 
handled separately as in (Pantel and 
Pennacchiotti, 2006). Very long paths are more 
likely to be non-relation patterns and too sparse 
to be useful even if they are relation patterns. 
2.4 Clustering Algorithm 
The basic idea of our clustering algorithm is to 
group all the paths (including the seed paths 
used later for SSL) in our corpus into different 
clusters based on distributional similarities. We 
first extract a variety of features from the named 
entities X and Y connected by a path P as shown 
in Table 1. We then compute an analogue of tf-
idf for each feature f of P as follows: tf as the 
number of corpus instances of P having feature f 
divided by the number of instances of P; idf as 
the total number of paths in the corpus divided 
by the number of paths with at least one 
instance with feature f. Then we adopt a vector 
space model, i.e., we construct a tf-idf feature 
vector for each P.  Now we compute the 
similarity between two vectors/paths using 
Cosine similarity and cluster all the paths using 
Complete Linkage. 
Some technical details deserve more attention 
here.  
Feature extraction: We extract more types 
of features than the DIRT paraphrase discovery 
procedure used in (Lin and Pantel, 2001). Lin 
and Pantel (2001) considered X and Y separately 
while we also use the conjunction of X and Y. 
We also extract named entity types as features 
since we are interested in discovering relations 
among different types of names. Some names 
are ambiguous such as Jordan. We hope 
                                                 
3 Please refer to Grishman et al (2005) and 
http://cs.nyu.edu/grishman/jet/license.html 
coupling the type with the string of the name 
may alleviate the ambiguity. 
 
Table 1. Sample features for ?X visited Y? as in ?Jordan 
visited China? 
Feature Type Example 
Name Type of X LEFT_PERSON 
Name Type of Y RIGHT_GPE 
Combination of 
Types of X and Y 
PERSON_GPE 
Conjunction of String 
and Type of X 
LEFT_Jordan_PERSON 
Conjunction of String 
and Type of Y 
RIGHT_China_GPE 
Conjunction of 
Strings and Types of 
X and Y 
Jordan_PERSON_China_GPE 
 
Similarity measure and clustering method: 
There are many ways to compute the 
similarity/distance between two feature vectors, 
such as Cosine, Euclidean, Hamming, and 
Jaccard coefficient. There are also many 
standard clustering algorithms. A systematic 
comparison of the performance of different 
distance measures and clustering algorithms is 
beyond the scope of this paper.  
3 Semi-supervised Relation Pattern 
Discovery 
We first present a standard bootstrapping 
algorithm coupled with analyses of some of its 
shortcomings. Then we describe our new 
bootstrapping procedure which is guided by 
pattern clusters.   
3.1 Bootstrapping without Guidance  
The procedure associates a precision between 0 
and 1 with each pattern, and a confidence 
between 0 and 1 with each name pair. Initially 
the seed patterns for a specific relation R have 
precision 1 and all other patterns 0. It consists of 
the following steps: 
Step1: Use seed patterns to match new NE 
pairs and evaluate NE pairs. 
Intuitively, for a newly matched NE pair iN , 
if many of the k patterns connecting the two 
names are high-precision patterns then the name 
pair has a high confidence. The confidence is 
computed by the following formula. 
1
( ) 1 (1 Pr ( ))
k
i j
j
Conf N ec p
=
= ? ??  (1) 
1196
Problem: While the intuition is correct, in 
practice this will over-rank NE pairs which are 
not only matched by patterns belonging to the 
target relation R but are also connected by 
patterns of many other relations. This is because 
of the initial settings used in many SSL systems: 
seeds are assigned high confidence. Thus all NE 
pairs matched by initial seed patterns will have 
very high confidence.  
Suppose the target relation is LocatedIn, and 
?visited? is a seed pattern; then the <Clinton, 
Arkansas> example will be over-rated because 
we cannot take into account that it would also 
match patterns of other relations such as 
PersonGovernorOfLocation and 
PersonBornInLocation in a real corpus. This 
will cause a vicious circle, i.e., bogus NE pairs 
extract more bogus patterns which further 
extract more bogus NE pairs. We believe this 
flaw of the initial settings partially results in the 
semantic drift problem.  
One can imagine that this is not a problem 
that can be solved by using a different formula 
to replace the one presented here. A possible 
solution is to study the structure of unlabeled 
data (NE pairs in our case) and integrate this 
structure information into the initial settings. 
Indeed, this is where pattern clusters come into 
play. We will demonstrate this in Section 3.2. 
Step 2: Use NE pairs to search for new 
patterns and rank patterns. 
Similar to the intuition in Step 1, for a pattern 
p, if many of the NE pairs it matches are very 
confident then p has many supporters and 
should have a high ranking. We can use formula 
(2) to estimate the confidence of patterns and 
rank them. 
( )
( ) log ( )
| |
Sup p
Conf p Sup p
H
= ?   (2) 
Here |H| is the number of unique NE pairs 
matched by p and Sup(p) is the sum of the 
support it can get from the |H| pairs: 
| |
1
( ) ( )
H
j
j
Sup p Conf N
=
= ?   (3) 
The precision of p is given by the average 
confidence of the NE pairs matched by p. 
( )
Pr ( )
| |
Sup p
ec p
H
=     (4) 
Formula (4) normalizes the precision to range 
from 0 to 1. As a result the confidence of each 
NE pair is also normalized to between 0 and 1. 
Step 3: Accept patterns 
Most systems accept the K top ranked 
patterns in Step 2 as new seeds, subject to some 
restrictions such as requiring the differences of 
confidence of the K patterns to be within a small 
range. 
 Step 4: Loop or stop 
The procedure now decides whether to repeat 
from Step 1 or to terminate. 
Most systems simply do not know when to 
stop. They either run a fixed number of 
iterations or use some held-out data to find one 
criterion that works the best for the held-out 
data. 
3.2 Bootstrapping Guided by Clusters 
Recall that our clustering algorithm in Section 2 
provides us with K clusters, each of which 
contains n (n differs in different clusters) 
patterns. Every pattern in our corpus now has a 
cluster membership (the seed patterns have the 
same membership).  
The most important benefit from our pattern 
clusters is that now we can measure how 
strongly a NE pair iN  is associated with our 
target cluster tC  (the one to which the seed 
patterns belong).  
( , )
Pr ( ) t
i
p C
i t
freq N p
ob N C
m
?? =
?
      (5) 
Here ( , )ifreq N p  is the number of times p 
matches iN  and m is the total number of pattern 
instances matching iN . 
We integrate this prior cluster distribution of 
each NE pair into the initial settings of our new 
bootstrapping procedure.  
Step1: Use seed patterns to match new NE 
pairs and evaluate NE pairs. 
 Assumption: A good NE pair must be 
strongly associated with the target cluster and 
can be matched by multiple high-precision 
patterns.  
So we evaluate a NE pair by the harmonic 
mean of two confidence scores, namely the 
confidence as its association with the target 
cluster and the confidence given by the patterns 
matching it. 
1197
_ ( ) _ ( )
( ) 2
_ ( ) _ ( )
i i
i
i i
Semi Conf N Cluster Conf N
Conf N
Semi Conf N Cluster Conf N
?= ? +
     (6) 
1
_ ( ) 1 (1 ( ))
k
i j
j
Semi Conf N Prec p
=
= ? ??  (7) 
_ ( ) Pr ( )i i tCluster Conf N ob N C= ?      (8) 
Under such settings, <Clinton, Arkansas> 
will be assigned a lower confidence score for 
the LocatedIn relation than it is in the standard 
bootstrapping. Even if we assign high precision 
to our seed patterns such as ?visited? and 
consequently the Semi_Conf is very high, it can 
still be discounted by the Cluster_Conf4.   
Step 2: Use NE pairs to search for new 
patterns and rank patterns. 
All the measurement functions are the same 
as those used in the standard bootstrapping. 
However, with better ranking of NE pairs in 
Step 1, the patterns are also ranked better than 
they are in the standard bootstrapping. 
Step 3: Accept patterns 
We also accept the K top ranked patterns.  
Step 4: Loop or stop 
Since each pattern in our corpus has a cluster 
membership, we can monitor the semantic drift 
easily and naturally stop: it drifts when the 
procedure tries to accept patterns which do not 
belong to the target cluster; we can stop when 
the procedure tends to accept more patterns 
outside of the target cluster. 
If our clustering algorithm can give us perfect 
pattern clusters, we can stop bootstrapping 
immediately after it accepts the first pattern not 
belonging to the target cluster. Then the 
bootstrapping becomes redundant since all it 
does is to consume the patterns of the target 
cluster.  
Facing the reality of the behavior of many 
clustering algorithms, we allow the procedure to 
occasionally accept patterns outside of the target 
cluster but we are not tolerant when it tries to 
accept more patterns outside of the target cluster 
than patterns in it. Note that when such patterns 
are accepted they will be moved to the target 
cluster and invoke the recomputation of 
Cluster_Conf of NE pairs connected by these 
patterns. The ranking functions in step 1 and 2 
                                                 
4 The Cluster_Conf of <Clinton, Arkansas> related 
to the LocatedIn relation is indeed very low (less 
than 0.1) in our experiments. 
insure that the procedure will only accept 
patterns which can gain strong support from NE 
pairs that are strongly associated with the target 
cluster and are connected by many confident 
patterns.  
4 Experiments 
4.1 Corpus 
Our corpora contain 37 years of news articles: 
TDT5, NYT(94-00), APW(98-00), 
XINHUA(96-00), WSJ(94-96), LATWP(94-97), 
REUFF(94-96), REUTE(94-96), and 
WSJSF(87-94). It contains roughly 65 million 
sentences and 1.3 billion tokens.  
4.2 Seeds 
Seeds of the 3 relations we are going to test are 
given in table 2. LocatedIn detects relation 
between PERSON and LOCATION/GPE; 
Social (SOC) detects social relations (either 
business or family) between PERSON and 
PERSON; Employment (EMP) detects 
employment relations between PERSON and 
ORGANIZATION.  
 
Table 2.  Seed Patterns 
Relation Seeds 
Located-
in 
nsubj' visit dobj 
nsubj' travel prep_to 
poss' trip prep_to 
SOC appos friend/lawyer poss 
appos son/spokesman prep_of/prep_for 
nsubj' fire dobj 
nsubjpass' fire agent 
 EMP5 appos chairman/executive/founder prep_of 
appos editor prep_of  
appos director/head/officer/analyst prep_at 
appos manager prep_with 
 
(nsubj, dobj, prep, appos, poss, nsubjpass, agent 
stand for subject, direct object, preposition, 
apposition, possessive, passive nominal subject 
and complement of passive verb. The quote 
marks in Table 2 and Table 3 denote inverse 
dependencies in the dependency path.) 
We work on these three relations mainly 
because of the availability of benchmark 
evaluation data. These are the most frequent 
relations in our evaluation data.  
                                                 
5 We provide more seeds (executives and staff) for 
EMP because it has been pointed out in (Sun, 2009) 
that EMP contains a lot of job titles.  
1198
4.3 Unsupervised Experiments 
We run the clustering algorithm described in 
Section 2 using all the 37 years? data. We 
require that a pattern match at least 7 distinct 
NE pairs and that an NE pair must be connected 
by at least 7 unique patterns. As a result, there 
are 635,128 patterns (22,225 unique ones) used 
in experiments. We use 0.005 as the cutoff 
threshold of complete linkage. The threshold is 
decided by trying a series of thresholds and 
searching for the maximal6 one that is capable 
of placing the seed patterns for each relation 
into a single cluster. Table 3 shows the top 15 
patterns (ranked by their corpus frequency) of 
the cluster into which our LocatedIn seeds fall.  
 
Table 3.  Top 15 patterns in the LocatedIn Cluster 
Index Pattern Frequency 
1 nsubj' said prep_in 2203 
2 nsubj' visit dobj 1831 
3 poss' visit prep_to 1522 
4 nsubj' return prep_to 1394 
5 nsubj' tell prep_in 1363 
6 nsubj' be prep_in 1283 
7 nsubj' arrive prep_in 1113 
8 nsubj' leave dobj 1106 
9 nsubj' go prep_to 926 
10 nsubj' fly prep_to 700 
11 nsubj' come prep_to 658 
12 appos leader poss 454 
13 poss' trip prep_to 442 
14 rcmod be prep_in 419 
15 nsubj' make prep_in 418 
4.4 Semi-supervised Experiments 
To provide strong statistical evidence, we divide 
our data into 10 folds (combinations of news 
articles from different years and different news 
resources). We then run both the standard and 
our new bootstrapping on the 10 folds. For both 
procedures, we accept n patterns in a single 
iteration (n is initialized to 2 and set to n + 1 
after each iteration). We run 50 iterations in the 
standard bootstrapping and 1,325 patterns are 
accepted for each fold and each relation. Our 
new bootstrapping procedure stops when there 
are two consecutive iterations in which more 
than half of the newly accepted patterns do not 
belong to the target cluster. Thus the number of 
                                                 
6  We choose the maximal value because many 
clusters will be merged to a single one when the 
threshold is close to 0, making the clusters too 
general to be useful. 
patterns accepted for each fold and each relation 
differs as the last iteration differs. 
4.5 Evaluation 
The output of our bootstrapping procedures is 
60 sets of patterns (3 relations ?  2 methods ?  
10 folds). We need a data set and evaluation 
method which can compare their effectiveness 
equally and consistently.  
Evaluation data: ACE 2004 training data. 
ACE does not provide relation annotation 
between each pair of names. For example, in 
?US President Clinton said that the United 
States ?? ACE annotates an EMP relation 
between the name ?US? and nominal 
?President?. There is no annotation between 
?US? and ?Clinton?. However, it provides entity 
co-reference information which connects 
?President? to ?Clinton?. So we take advantage 
of this entity co-reference information to 
automatically re-annotate the relations where 
possible to link a pair of names within a single 
sentence. The re-annotation yields an EMP 
relation between ?US? and ?Clinton?. The re-
annotation is reviewed by hand to avoid adding 
a relation linking ?Clinton? and the more distant 
co-referent ?United States?, even though ?US? 
and ?the United States? refer to the same entity. 
This data set provides us with 412/3492 
positive/negative relation instances between 
names. Among the 412 positive instances, there 
are 188/117/35 instances for 
EMP/LocatedIn/SOC relations.  
Evaluation method: We adopt a direct 
evaluation method, i.e., use our sets of patterns 
to extract relations between names on ACE data. 
Applying patterns to a benchmark data set can 
provide us with better precision/recall analyses. 
We use a strict pattern match strategy. We can 
certainly take advantage of loose match or add 
patterns as additional features to feature-based 
relation extraction systems to boost our 
performance but we do not want these to 
complicate the comparison of the standard and 
our new bootstrapping procedures.  
4.6 Results and Analyses  
We average our results on the 10 folds. We plot 
precision against recall and semantic drift rate 
against iterations (Drift). We compute the 
semantic drift rate as the percentage of false 
1199
Figure 2.  Performance for EMP/LocatedIn/SOC 
EMP  Precision vs. Recall
Recall
0.0 .1 .2 .3 .4 .5 .6
P
re
ci
si
on
.5
.6
.7
.8
.9
1.0
LocatedIn  Precision vs. Recall
Recall
0.00 .05 .10 .15 .20 .25 .30 .35
P
re
ci
si
on
.3
.4
.5
.6
.7
.8
.9
1.0
1.1
 
SOC  Precision vs. Recall
Recall
.05 .10 .15 .20 .25 .30 .35 .40 .45
P
re
ci
si
on
0.0
.2
.4
.6
.8
1.0
1.2
 
positive instances belonging to ACE relations 
other than the target relation. Take EMP for 
example, we compute how many of the false 
positive instances belonging to other relations 
such as LocatedIn, SOC and other ACE 
relations. In all plots, red solid lines represent 
bootstrapping with guidance from clusters and  
blue dotted lines standard bootstrapping. 
  There are a number of conclusions that can be  
Figure 3.  Drift for EMP/LocatedIn/SOC 
EMP  Drift
Iteration
0 10 20 30 40 50 60
D
rif
t
0.00
.05
.10
.15
.20
.25
.30
 
LocatedIn  Drift
Iteration
0 10 20 30 40 50 60
D
rif
t
0.00
.05
.10
.15
.20
.25
.30
.35
SOC  Drift
Iteration
0 10 20 30 40 50 60
D
rif
t
0.00
.01
.02
.03
.04
.05
.06
.07
   
 
drawn from these results. We are particularly 
interested in the following two questions: To 
what extent did we prevent semantic drift by the 
guidance of pattern clusters? Did we stop at the 
right point, i.e., can we keep high precision 
while maintaining near maximal recall? 
1) It is obvious from the drift curves that our 
bootstrapping effectively prevents semantic drift. 
Indeed, there is no drift at all when LocatedIn 
1200
and SOC learners terminate. Although drift 
indeed occurs in the EMP relation, its curve is 
much lower than that of the standard 
bootstrapping.  
2) Our new procedure terminates when the 
precision is still high while maintaining a 
reasonable recall. Our bootstrapping for 
EMP/SOC/LocatedIn terminates at F-measures 
of 60/37/28 (in percentage). We conducted the 
Wilcoxon Matched-Pairs Signed-Ranks Test on 
the 10 folds, comparing the F-measures of the 
last iteration of our bootstrapping guided by 
clusters and the iteration which provides the 
best average F-measure over the 3 relations of 
the standard bootstrapping. The results show 
that the improvement of using clusters to guide 
bootstrapping is significant at a 97% confidence 
level. 
We hypothesize that when working on 
dozens or hundreds of relations the gain of our 
procedure will be even bigger since we can 
effectively prevent inter-class errors.  
5 Related Work 
Recent research starts exploring unlabeled data 
for discriminative learning. Miller et al, (2004) 
augmented name tagging training data with 
hierarchical word clusters and encoded cluster 
membership in features for improving name 
tagging. Lin and Wu (2009) further explored a 
two-stage cluster-based approach: first 
clustering phrases and then relying on a 
supervised learner to identify useful clusters and 
assign proper weights to cluster features. Other 
similar work includes (Wong and Ng, 2007) for 
name tagging, and (Koo et. al., 2008) for 
dependency parsing.  
While similar in spirit, our supervision is 
minimal, i.e., we only use a few seeds while the 
above approaches rely on a large amount of 
labeled data. To the best of our knowledge, the 
theme explored in this paper is the first study of 
using pattern clusters for preventing semantic 
drift in semi-supervised pattern discovery.  
Recent research also explored the idea of 
driving SSL with explicit constraints 
constructed by hand such as identifying mutual 
exclusion of different categories (i.e., people 
and sport are mutually exclusive). This is 
termed constraint-driven learning in (Chang et 
al., 2007), coupled learning in (Carlson et al, 
2010) and counter-training in (Yangarber, 2003). 
The learning quality largely depends on the 
completeness of explicit constraints. While we 
share the same goal, i.e., to prevent semantic 
drift, we rely on unsupervised clusters to 
discover implicit constraints for us instead of 
generating constraints by hand. 
Our research is also close to semi-supervised 
IE pattern learners including (Riloff and Jones, 
1999), (Agichtein and Gravano, 2000), 
(Yangarber et al, 2000), and many others. 
While they conduct bootstrapping on unlabeled 
data directly, we first cluster unlabeled data and 
then bootstrap with help from clusters. 
There are also clear connections to work on 
unsupervised relation discovery (Hasegawa et 
al., 2004; Zhang et al, 2005; Rosenfeld and 
Feldman, 2007). They group pairs of names into 
relation clusters based on the contexts between 
names while we group the contexts/patterns into 
clusters based on features extracted from names. 
6 Conclusions and Future Work 
We presented a simple algorithm for clustering 
patterns and used pattern clusters to guide semi-
supervised semantic pattern discovery. The 
novel bootstrapping procedure can achieve the 
best F-1 score while maintaining a good trade-
off between precision and recall. We also 
demonstrated that it can effectively prevent 
semantic drift and naturally terminate.  
We plan to extend this idea to improve 
relation extraction performance with a richer 
model as used in (Zhang et al, 2004; Zhou et al, 
2008) than a simple pattern learner. The feature 
space will be much larger than the one adopted 
in this paper. We will investigate how to 
overcome the memory bottleneck when we 
apply rich models to millions of instances.  
7 Acknowledgements 
We would like to thank Prof. Satoshi Sekine for 
his useful suggestions. 
References 
Steven Abney. 2008. Semisupervised Learning for 
Computational Linguistics, Chapman and Hall. 
Eugene Agichtein and Luis Gravano. 2000. Snowball: 
Extracting relations from large plain-text 
1201
collections. In Proc. of the Fifth ACM 
International Conference on Digital Libraries. 
Sergey Brin. Extracting patterns and relations from 
the World-Wide Web. 1998. In Proc. of the 1998 
Intl. Workshop on the Web and Databases. 
Razvan C. Bunescu and Raymond J. Mooney. 2005. 
A Shortest Path Dependency Kernel for Relation 
Extraction. In Proc. of HLT/EMNLP. 
Andrew Carlson, Justin Betteridge, Richard C. Wang, 
Estevam Rafael Hruschka Junior and Tom M. 
Mitchell. 2010. Coupled Semi-Supervised 
Learning for Information Extraction. In WSDM. 
Ming-Wei Chang, Lev Ratinov, and Dan Roth. 2007. 
Guiding semisupervision with constraint-driven 
learning. In Proc. of ACL-2007, Prague.  
Zheng Chen and Heng Ji. 2009. Can One Language 
Bootstrap the Other: A Case Study on Event 
Extraction. In NAACL HLT Workshop on Semi-
supervised Learning for NLP. 
Michael Collins and Yoram Singer. 1999. 
Unsupervised models for named entity 
classication. In Proc. of EMNLP-99. 
James R. Curran, Tara Murphy, and Bernhard Scholz. 
2007. Minimising semantic drift with Mutual 
Exclusion Bootstrapping. In Proc. of PACLING. 
Ralph Grishman, David Westbrook and Adam 
Meyers. 2005. NYU?s English ACE 2005 System 
Description. ACE 2005 Evaluation Workshop.  
Zellig S. Harris. 1954. Distributional Structure. Word. 
Vol 10,1954, 146-162. 
Takaaki Hasegawa, Satoshi Sekine, Ralph Grishman. 
2004. Discovering Relations among Named 
Entities from Large Corpora. In Proc. of ACL-04.  
Marti Hearst. 1992. Automatic acquisition of 
hyponyms from large text corpora. In Proc. of the 
14th Intl. Conf. on Computational Linguistics. 
Terry Koo, Xavier Carreras, and Michael Collins. 
2008. Simple Semi-supervised Dependency 
Parsing. In Proceedings of ACL-08: HLT. 
Dekang Lin and Patrick Pantel. 2001. Discovery of 
inference rules for question-answering. Natural 
Language Engineering, 7(4):343?360. 
Dekang Lin and Xiaoyun Wu. 2009. Phrase 
Clustering for Discriminative Learning. In 
Proceedings of the ACL and IJCNLP 2009. 
Marie-Catherine de Marneffe and Christopher D. 
Manning. 2008. The Stanford typed dependencies 
representation. In COLING Workshop on Cross-
framework and Cross-domain Parser Evaluation. 
Scott Miller, Jethran Guinness and Alex Zamanian. 
2004. Name Tagging with Word Clusters and 
Discriminative Training. In Proc. of HLT-NAACL. 
Patrick Pantel and Marco Pennacchiotti. 2006. 
Espresso: Leveraging Generic Patterns for 
Automatically Harvesting Semantic Relations. In 
Proc. of COLING-06 and ACL-06. 
Deepak Ravichandran and Eduard Hovy. 2002. 
Learning Surface Text Patterns for a Question 
Answering System. In Proc. of ACL-2002. 
Ellen Riloff and Rosie Jones. 1999. Learning 
dictionaries for information extraction by multi-
level bootstrapping. In Proc. of AAAI-99. 
Benjamin Rosenfeld, Ronen Feldman. 2007. 
Clustering for Unsupervised Relation 
Identification. In Proc. of CIKM ?07. 
Mark Stevenson and Mark A. Greenwood. 2006. 
Comparing Information Extraction Pattern 
Models. In Proceedings of the Workshop on 
Information Extraction Beyond The Document. 
Mark Stevenson and Mark A. Greenwood. 2005. A 
Semantic Approach to IE Pattern Induction. In 
Proc. of the 43rd Annual Meeting of the ACL. 
Ang Sun. 2009. A Two-stage Bootstrapping 
Algorithm for Relation Extraction. In RANLP-09. 
Yingchuan Wong and Hwee Tou Ng. 2007. One 
Class per Named Entity: Exploiting Unlabeled 
Text for Named Entity Recognition. In Proc. of 
IJCAI-07. 
Roman Yangarber. 2003. Counter-training in the 
discovery of semantic patterns. In Proc. of ACL.  
Roman Yangarber, Ralph Grishman, Pasi 
Tapanainen and Silja Huttunen. 2000. Automatic 
acquisition of domain knowledge for information 
extraction. In Proc. of COLING-2000. 
David Yarowsky. 1995. Unsupervised word sense 
disambiguation rivaling supervised methods. In 
Proc. of ACL-95. 
Min Zhang, Jian Su, Danmei Wang, Guodong Zhou, 
and Chew Lim Tan. 2005. Discovering Relations 
Between Named Entities from a Large Raw 
Corpus Using Tree Similarity-Based Clustering. 
In IJCNLP 2005, LNAI 3651, pp. 378 ? 389. 
Zhu Zhang. (2004). Weakly supervised relation 
classification for information extraction. In Proc. 
of CIKM?2004.  
GuoDong Zhou, JunHui Li, LongHua Qian and 
QiaoMing Zhu. 2008.  Semi-supervised learning 
for relation extraction.  IJCNLP?2008:32-39. 
1202
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1291?1300,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Exploiting Syntactic and Distributional Information
for Spelling Correction with Web-Scale N-gram Models
Wei Xuc,?Joel Tetreaulta Martin Chodorowb Ralph Grishmanc Le Zhaod
aEducational Testing Service, Princeton, NJ, USA
jtetreault@ets.org
bHunter College of CUNY, New York, NY, USA
martin.chodorow@hunter.cuny.edu
cNew York University, NY, USA
{xuwei,grishman}@cs.nyu.edu
dCarnegie Mellon University, Pittsburgh, PA, USA
lezhao@cs.cmu.edu
Abstract
We propose a novel way of incorporating de-
pendency parse and word co-occurrence in-
formation into a state-of-the-art web-scale n-
gram model for spelling correction. The syn-
tactic and distributional information provides
extra evidence in addition to that provided by a
web-scale n-gram corpus and especially helps
with data sparsity problems. Experimental
results show that introducing syntactic fea-
tures into n-gram based models significantly
reduces errors by up to 12.4% over the current
state-of-the-art. The word co-occurrence in-
formation shows potential but only improves
overall accuracy slightly.
1 Introduction
The function of context-sensitive text correction is
to identify word-choice errors in text (Bergsma et
al., 2009). It can be viewed as a lexical disambigua-
tion task (Lapata and Keller, 2005), where a system
selects from a predefined confusion word set, such
as {affect, effect} or {complement, compliment},
and provides the most appropriate word choice given
the context. Typically, one determines if a word has
been used correctly based on lexical, syntactic and
semantic information from the context of the word.
One of the top performing models of spelling cor-
rection (Bergsma et al, 2010) is based on web-scale
n-gram counts, which reflect both syntax and mean-
ing. However, even with a large-scale n-gram cor-
pus, data sparsity can hurt performance in two ways.
?This work was done when the first author was an intern
for Educational Testing Service.
First, n-gram based methods require exact word and
order matches. If there is a low frequency word in
the context, such as a person?s name, there will be
little, if any, evidence in the n-gram data to sup-
port the usage. Second, if the target confusable word
is rare, there will not be enough n-gram support or
training data to render a confident decision. Because
of the data sparsity problem, language modeling is
not always sufficient to capture the meaning of the
sentence and the correct usage of the word.
Take a sentence from The New York Times
(NYT) for example: ??This fellow?s won a war,? the
dean of the capital?s press corps, David Broder, an-
nounced on ?Meet the Press? after complimenting
the president on the ?great sense of authority and
command? he exhibited in a flight suit.? Unfortu-
nately, neither the phrase ?complementing the pres-
ident? nor ?complimenting the president? exists in
the web-scale Google N-gram corpus (Brants and
Franz, 2006). The n-gram models decide solely
based on the frequency of the bi-grams ?after com-
ple(i)menting? and ?comple(i)menting the?, which
are common usages for both words. The real ques-
tion is whether we are more likely to ?compliment?
or ?complement? a person, the ?president?. Several
clues could help us answer that question. A de-
pendency parser can identify the word ?president?
as the subject of ?compliment? or ?complement?
which also may be the case in some of the train-
ing data. Lexical co-occurrence (Edmonds, 1997)
and semantic word relatedness measurements, such
as Random Indexing (Sahlgren, 2006), could pro-
vide evidence that ?compliment? is more likely to
co-occur with ?president? than ?complement?. Fur-
1291
thermore, some important clues can be quite distant
from the target word, e.g. outside the 9-word context
window Bergsma et al (2010) and Carlson (2007)
used. Consider another sentence in the NYT corpus,
?GM says the addition of OnStar, which includes a
system that automatically notifies an OnStar opera-
tor if the vehicle is involved in a collision, comple-
ments the Vue?s top five-star safety rating for the
driver and front passenger in both front- and side-
impact crash tests.? The dependency parser finds the
object of ?complement? is ?rating?, which is outside
the 9-word window.
We propose enhancing state-of-the-art web-scale
n-gram models for spelling correction with syntac-
tic structures and distributional information. For our
work, we build on a baseline system that combines
n-gram and lexical features (Bergsma et al, 2010).
Specifically, this paper makes the following contri-
butions:
1. We show that the baseline system can be
improved by augmenting it with dependency
parse features.
2. We show that the impact of parse features can
be further augmented when combined with dis-
tributional information, specifically word co-
occurrence information.
In the following section, we describe related
work and how our approach differs from these ap-
proaches. In Sections 3 and 4, we discuss our meth-
ods for using parse features and word co-occurrence
information. In Section 5, we present experimental
results and analysis.
2 Related Work
A variety of approaches have been proposed for
context-sensitive spelling correction ranging from
semantic methods to machine learning classifiers to
large-scale n-gram models.
Some semantics-based systems have been devel-
oped based on an intuitive assumption that the in-
tended word is more likely to be semantically coher-
ent with the context than is a spelling error. Jones
and Martin (1997) made use of the semantic simi-
larity produced by Latent Semantic Analysis. Bu-
danitsky and Hirst (2001) investigated the effective-
ness of predicting words based on different semantic
similarity/distance measures in WordNet. Both sys-
tems report performance that is lower than systems
developed more recently.
A variety of machine-learning methods have been
proposed in spelling correction and preposition and
article error correction fields, such as Bayesian clas-
sifiers (Golding, 1995; Golding and Roth, 1996),
Winnow-based learning (Golding and Roth, 1999),
decision lists (Golding, 1995), transformation-based
learning (Mangu and Brill, 1997), augmented mix-
ture models (Cucerzan and Yarowsky, 2002) and
maximum entropy classifiers (Izumi et al, 2003;
Han et al, 2006; Chodorow et al, 2007; Tetreault
and Chodorow, 2008; Felice and Pulman, 2008).
Despite their differences, these approaches mainly
use contextual features to capture the lexical, seman-
tic and/or syntactic environment of the target word.
The use of distributional similarity measures for
spelling correction has been previously explored in
(Mohammad and Hist, 2006). In our work, distribu-
tional similarity is not the primary contribution but
we show the impact it can have when used in con-
junction with a large scale n-gram model and with
parse features, which allows the system to select
words outside the local window for distributional
similarity. In the prior work, the words for distri-
butional similarity are constrained to the local win-
dow, and positional information of the words is not
encoded.
Recent work (Carlson and Fette, 2007; Gamon
et al, 2008; Bergsma et al, 2009) has demon-
strated that large-scale language modeling is ex-
tremely helpful for contextual spelling correction
and other lexical disambiguation tasks. These sys-
tems make the word choice depending on how fre-
quently each candidate word has been seen in the
given context in web-scale data. As n-gram data has
become more readily available, such as the Google
N-gram Corpus, the likelihood of a word being used
in a certain context can be better estimated.
Bergsma et al (2009; 2010) presented a series
of simple but powerful models which relied heavily
on web-scale n-gram counts. From the Google Web
N-gram Corpus, they retrieve counts of n-grams of
different sizes (2-5) and positions that span the tar-
get word w0 within a window of 9 words. For
example, for the following sentence: ?The system
tried to decide {among, between} the two confus-
1292
able words.?, the method would extract the five 5-
gram patterns, shown below in Figure 2, where w0
can be either word in the confusion set {among, be-
tween} in this particular example. Similarly, there
are four 4-grams, three 3-grams, and two 2-grams,
in total, 14 n-grams for each of the words in the con-
fusion set.
system tried to decide w0
tried to decide w0 the
to decide w0 the two
decide w0 the two confusable
w0 the two confusable words
We briefly describe three of Bergsma et al?s
(2009; 2010) best systems below, which are reported
to achieve state-of-the-art accuracy (NG = n-gram;
LEX = lexical).
1. sumLM: For each candidate word, (Bergsma
et al, 2009) sum the log-counts of all 14 pat-
terns filled with the candidate, and choose the
candidate with the highest total.
2. NG: Bergsma et al (2009) exploit each can-
didate?s 14 log-counts of n-gram patterns as
features in a Support Vector Machine (SVM)
model.
3. NG+LEX: Bergsma et al (2010) augment the
NG model with lexical features (described in
detail in Section 3.1).
Bergsma et al (2009; 2010) restricted their exper-
iments to only five confusion sets where the reported
performance in (Golding and Roth, 1999) was below
90%: {among, between}, {amount, number}, {cite,
sight, site}, {peace, piece} and {raise, rise}. They
reported that the SVM model with NG features out-
performed its unsupervised version, sumLM. How-
ever, the limited confusion word sets they evaluated
may not comprehensively represent the word usage
errors that writers typically make. In this paper, we
test nine additional commonly confused word pairs
to expand the scope of the evaluation. These words
were selected based on their lower frequencies com-
pared to the five pairs in the above work (as shown
later in Table 2).
3 Enhanced N-gram Models with Parse
Features
To our knowledge, only (Elmi and Evans, 1998)
have used parsing for spell correction. They focus
on using a parser as a filter to discriminate between
possible real-world corrections where the part-of-
speech differs. In our work, we show that parse fea-
tures are effective when used directly in the classifi-
cation mode (as opposed to as a final filter) to select
the best correction regardless of whether or not the
part-of-speech of the choices differ.
Statistical parsers have also seen limited use in
the sister tasks of preposition and article error detec-
tion (Hermet et al, 2008; Lee and Knutsson, 2008;
Felice and Pulman, 2009; Tetreault et al, 2010)
and verb sense disambiguation (Dligach and Palmer,
2008). In those instances where parsers have been
used, they have mainly provided shallow analyses
or relations involving specific target words, such as
a preposition or verb. Unlike preposition errors,
spelling errors can occur in any word.
In this paper, we propose a novel way to incor-
porate the parse into spelling correction, applying
the parser to sentences filled by each candidate word
equivalently and extracting salient features. This
overcomes two problem in the existing methods: 1)
the parse trees of the same sentence filled by differ-
ent confusion words can be different. However, in
the test phase, we do not know which word should
be put in the sentences to create parse features for
test examples. Previous studies (Tetreault et al,
2010) failed to discuss this issue. 2) Some existing
work (Whitelaw et al, 2009; Rozovskaya and Roth,
2010) in the text correction field introduced artificial
errors into training data to adapt the system to bet-
ter handle ill-formed text. But this method will en-
counter serious data sparsity problems when facing
rare words.
3.1 Baseline System
We chose one of the leading spelling correction sys-
tems, (Bergsma et al, 2010), as our primary base-
line. As noted earlier, it is an SVM-based system
combining web-scale n-gram counts (NG) and con-
textual words (LEX) as features. To simplify the ex-
planation, throughout the paper, we will only con-
sider the situation with two confusion words. The
1293
problem with more than two words in pre-defined
confusion sets can be solved similarly by using a
one-vs.-all strategy. As we mentioned in Section 2,
NG features include log-counts of 3-to-5-gram pat-
terns for each candidate word with the given context.
LEX features can be broken down into three sub-
categories: 1) bag-of-words (words at all positions
in a 9-word window around the target word), 2) in-
dicators for the words preceding or following the tar-
get word, and 3) indicators for all n-grams and their
positions. For the sentence ?The system tried to de-
cide {among, between} the two confusable words.?,
examples of bag-of-word features would be ?tried?,
?two?, etc., the two positional bigrams would be
?decide? and ?the?, and examples of the n-gram fea-
tures would be right-trigram = ?among the two? and
left-4-gram = ?tried to decide between?.
3.2 Parse Features
The benefit of introducing dependency parse fea-
tures is that 1) parse features capture contextual in-
formation in a larger context window; 2) parse fea-
tures specify which words in the context are salient
to the usage of the target word while purely lexi-
cally based approaches treat all words in the context
equally. We use the Stanford dependency parser (de
Marneffe et al, 2006) to extract six relevant feature
classes.
Parse Features (PAR):
1. relation names (target word as head)
2. complement of the target word
3. combination of 1 and 2
4. relation names (target word as complement)
5. head of the target word
6. combination of 4 and 5
Each of these six classes of PAR features can
contain zero to many values, since the target word
can be involved in none to multiple grammatical
relations and features of different filler words are
merged together. The PAR features, like the LEX
features, are binary. In Table 1, we present the parse
features for an example sentence. The parse fea-
tures here are listed as string values, but are later
converted into binary numbers in the vectors for the
SVM model.
4 Distributional Word Co-occurrence
Though lexical and parse features are complemen-
tary to n-gram models, they are learned from a nor-
mal training corpus and may not have enough cov-
erage due to data sparsity. Take a sentence from the
NYT for example: ?An economist, he began his ca-
reer as a professor ? he is still called ?the professor,?
by friends as a compliment and by foes as an insult ?
and taught at Harvard and Stanford .? If the most in-
dicative word ?friends? does not appear or does not
appear enough times in the local context or depen-
dencies with ?compliment? as compared to ?com-
plement? in the training corpus, then the classifier
may be unable to make the correct selection.
It is impractical and computationally costly to en-
large the training corpus without limit to include
all possible language phenomena. A good compro-
mise is to use word co-occurrence information from
web-scale data. The other option is to make use of
high-order word co-occurrence, which is included in
many semantic word relatedness measures, such as
Latent Semantic Analysis (LSA) (Landauer et al,
1998; Deerwester et al, 1990) or Random Indexing,
both of which can be estimated from a moderate-size
corpus.
Our intuition is to choose the confusion word
which is most relevant to a given context. We define
the salient words in context as a set M=m1, m2, m3,
..., and the relevance between two words as a func-
tion Relevance(w1, w2), which can either be calcu-
lated fromword co-occurrence or Random Indexing.
The score of each candidate word c in the confusion
set given a context with meaningful words M is cal-
culated by the following formula:
Score(c) =
?
m?M
Relevance(c,m)
In this paper, we experiment with first-order word
co-occurrence and Random Indexing as relevance
measures. And we define salient contextual words
as heads or complements in the dependency rela-
tions with the target word. In this way, we use the
parse information to constrain the two distribution
models. Thus the word co-occurrence information
1294
Feature Name PAR Features (compliment) PAR Features (complement)
1. Head Relation Name ccomp appos
2. Head of Relation says collisions
3. Head Combination ccomp says appos collisions
4. Comp Relation Name nsubj dep
5. Comp of Relation addition rating
6. Comp Combination nsub addition dep rating
Table 1: Parse Feature Example for the sentence: ?GM says the addition of OnStar, which includes a system that
automatically notifies an OnStar operator if the vehicle is involved in a collision, complements the Vue?s top five-star
safety rating for the driver and front passenger in both front- and side-impact crash tests.?
considerably overlaps with some values of the PAR
features, but provides extra evidence from web-scale
data rather than a limited amount of training data.
4.1 First-order Word Co-occurrence
The relevance based on first-order word co-
occurrence is calculated from the Google Web 5-
gram Corpus in a fashion similar to how we dealt
with n-gram counts in the previous section. Given
two words, w1 and w2, we consider all 8 possible
patterns that appear in a local context (5-word win-
dow), where we use wildcard (*) to indicate any to-
ken:
w1 w2
w1 * w2
w1 * * w2
w1 * * * w2
w2 w1
w2 * w1
w2 * * w1
w2 * * * w1
The relevance is then calculated by summing the
logarithm of each of the 8 different counts. Finally,
we compare the score of each candidate word and
output the one with higher score.
4.2 Random Indexing
The relevance scores based on Random Indexing
are provided by a tool FRanI (Higgins, 2004) and
a model trained on the Touchstone Applied Science
Associates (TASA) corpus which contains 750k sen-
tences and covers diverse topics (from a diversity of
textbooks up to the college level). Take the sentence
at the beginning of this section for example, where
only the words ?a? and ?friends? are related to the
target word (either ?complement? or ?compliment?)
by either relevance measure. The relevance based
on Random Indexing for (complement, friends) is
0.08, (compliment, friends) is 0.19 and both (com-
pliment, a) and (complement, a) are 0 because ?a?
is in the stop word list. Meanwhile, the relevance
based on first order word co-occurrence for (com-
pliment, friends) is 7.39, (complement, friends) is
5.38, (compliment, a) is 13.25, and (complement, a)
is 13.42. The system with either kind of relevance
outputs ?compliment?.
4.3 System Combination
Since the numeric measurement of word co-
occurrence is not as specific as the PAR features and
less trustworthy, adding word co-occurrence infor-
mation as features into the classifier along with n-
gram counts, lexical and parse features will hurt the
overall performance. It is more practical to combine
the two approaches in the following fashion:
1. When the SVM classifier (using NG, LEX and
PAR features) has high confidence (over a cer-
tain threshold) in the output label, output that
label;
2. Otherwise, output the results of the word
relatedness/co-occurrence-based system.
5 Evaluation
We evaluate the effectiveness of syntactic and dis-
tributional information on spelling correction. The
performance of the system is measured by accu-
racy: the percentage of sentences in the test data
for which the system chooses the correct word. We
compare our results against two baselines: 1) MA-
JOR chooses the most frequent candidate from the
1295
confusion set in the training corpus, and 2) Bergsma
et al?s (2010) best systems, NG+LEX. We include
inflectional variants (?-ing?, ?-ed?, ?-s?, ?-ly?) of
confusion words in the evaluation, such as comple-
menting, complimenting in addition to complement,
compliment, because this better corresponds to the
range of errors that may be encountered in actual
use and thus increases the scope of the system as a
real world application. Also following Bergsma et
al. (2010), we use a linear SVM, more exactly, the
L2-regularized L2-loss dual SVM in LIBLINEAR
(Fan et al, 2008). Unlike Bergsma et al, who used
development data to optimize parameters, we always
use default parameters, since training data is limited
for many of the words we are dealing with.
5.1 Data
Following Bergsma et al (2009; 2010), the test
examples are extracted from The New York Times
(NYT) portion of Gigaword1, but constrained to a
9-month publication time frame from October 2005
to July 2006. Unlike Bergsma et al who use the
same source as training data for the lexical features,
our training data (for both lexical and parse features)
comes from larger and more diverse news sources.
We use the very large database from Sekine?s n-gram
search engine (Sekine, 2008) as training data, which
consists of 1.9B words of newspaper text spanning
89 years from NYT, BBC, WSJ, Xinhua, etc.
We evaluate our systems on 5 confusion sets from
Bergsma et al (2009; 2010) and 9 commonly con-
fused word pairs with moderate frequency in daily
usage (randomly selected from those listed in En-
glish educational resources2). Shown in Table 2,
these 9 sets of words appear much less frequently
than the words selected by Bergsma et al, even
given the fact that we are using a considerably large
training corpus.
For each confusable word pair, sentences that
contain either of the words are extracted to form
training and test data. The word that appears in the
original sentences of the news article is treated as
the gold standard. For frequently occurring confu-
sion word sets used by Bergsma et al, we extract
up to 10k examples for testing, and up to 100k ex-
1Available from the LDC as LDC2003T05
2Such as an English learning blog post at
http://elisaenglish.pixnet.net/blog/post/1335194
Word Confusion Set # in Training Corpus
adverse / averse 13.5k / 1.8k
advice / advise 62.k / 12.9k
allusion / illusion 1.0k / 5.4k
complement / compliment 6.8k / 3.1k
confidant / confident 2.4k / 63.6k
desert / dessert 24.7k / 3.7k
discreet / discrete 0.7k / 2.4k
elicit / illicit 1.9k / 10.0k
stationary / stationery 2.5k/2.3k
wander / wonder 3.3k / 39.5k
Table 2: Training Data Sizes for Common ESL Confused
Words
amples for training. For the 9 less frequent confu-
sion word sets, we extract all the unique examples
for training and testing from the above sources. The
spelling correction system is evaluated by measur-
ing its accuracy in comparison to the gold standard
in test data. The error rate is the complement of ac-
curacy.
Following Carlson et al (2007) and Bergsma
et al (2009; 2010), we obtain the n-gram counts
from the GoogleWeb 1T 5-gram Corpus (Brants and
Franz, 2006).
5.2 Experimental Results
We present the results for each set separately be-
cause each set may behave very differently, depend-
ing upon its frequency, part-of-speech, number of
senses and other differences between the words in
each confusion set. The overall accuracy across con-
fusion sets is also presented to show the effective-
ness of different approaches. The results are tested
for statistical significance using McNemar?s test of
correlated proportions. The performance differences
are marked as significant when p < 0.05.
5.2.1 Effectiveness of Parse Features
We exploit the n-gram counts (NG), lexical fea-
tures (LEX) of Bergsma et al (2010) and our own
parse features (PAR) in linear SVM models.
The first comparison is between the supervised
learning systems with LEX and LEX+PAR. As
shown in Table 3, by exploiting our unique parse
features, for the total 14 confusion sets, the accuracy
increases on 12 sets and decreases on 2 sets. Over-
all, the spelling correction accuracy improves an ab-
1296
solute 1.35% for our 9 confusion sets and 0.60% for
Bergsma et al?s 5 confusion sets.
The second comparison is to see how parse fea-
tures interact with n-gram count features in a su-
pervised classifier. The best system from (Bergsma
et al, 2010) is listed in the table as ?NG+LEX?.
As shown in Table 3, the parse features proved to
be beneficial when augmenting this baseline, except
for the decrease in accuracy on adverse, averse by
only 2 cases out of 368, and among, between by
2 cases out of 10227. For all other confusion sets,
parse features decrease the error rate by as much as
2.74% (absolute) and as much as 38.5% (relative).
Improvements are statistically significant on all con-
fusion sets together, although for each separate set,
improvements are significant on only 5 sets, in part
due to an insufficient number of test cases.
The reason that parse features are occasionally not
helpful is because they sometimes include an un-
common word in dependencies, which happens to
appear once with the wrong word but not with the
correct word in the training data; or they sometimes
include too common words, which bias the classifier
in favor of the more frequent word in the confusion
set. We also noticed that lexical features are not al-
ways helpful when added to n-gram count features,
even for in-domain applications (i.e., with training
data and test data coming from the same domain or
corpus), as marked by underlines. However, lexical
and parse features together show more significant
and constant improvement over n-gram count-based
models, as marked by ?.
Of the six systems, every system that uses parse
features gets the example correct in Section 1, ?com-
plementing the president?; LEX by itself also gets
the example correct, but NG and NG+LEX fail.
In summary, our system NG+LEX+PAR outper-
forms the state-of-the-art system NG+LEX. It re-
duces the error rate by 12.4% across our 9 confusion
sets and by 8.4% across Bergsma et al?s 5 confusion
sets. Both improvements are significant (p < 0.05)
by the McNemar test. In addition, while NG+LEX
is not always better than NG, NG+LEX+PAR is con-
sistently better than NG.
5.2.2 Impact of Word Co-occurrence
The LIBLINEAR tool does not provide probabil-
ity estimates for SVM models but Logistic Regres-
sion can. In this set of experiments, we train a Logis-
tic Regression model with NG+LEX+PAR features
and empirically set the confidence threshold at 0.6,
as described in Section 4, based on the performance
on two word pairs. In the combined system, when
the Logistic Regression model estimates a probabil-
ity higher than the threshold we output its results,
otherwise we output the result of the system based
on word co-occurrence.
Surprisingly, although Random Indexing takes
into account more information than first-order word
co-occurrence, it lowered overall performance sub-
stantially. Thus in Table 4, we only present results
of using first-order word co-occurrence rather than
Random Indexing. For all 12 confusion sets, distri-
butional word co-occurrence information improves
9 sets and hurts 5 sets. Overall, it reduces the er-
ror rate slightly by 0.2% for our 9 sets and 1.5% for
Bergsma et al?s sets.
We believe there are two reasons why Ran-
dom Indexing fared worse than first-order word
co-occurrence: 1) Random Indexing considers co-
occurrence on a document level, while our first-
order word co-occurrence is limited to a 5-word win-
dow context. The latter is more suitable to context-
sensitive spelling correction. 2) The model for Ran-
dom Indexing is trained on a relatively small size
corpus compared to the web-scale data we used to
get n-gram count features for the classifier and thus
is not able to introduce much new evidence besides
the information carried by NG+LEX+PAR features.
Reason 2) also suggests why first-order co-
occurrence helps on some occasions while not on
other occasions. Its impact is limited because the
word co-occurrence information overlaps with some
of the PAR feature values as mentioned earlier. It
improves some cases because it provides some new
evidence from web-scale data to the system based on
NG+LEX+PAR features. It introduces new errors
because it simply favors the word that co-occurred
more often regardless of other factors. Its impact is
also limited because it is only considered when clas-
sifiers with NG+LEX+PAR features are not confi-
dent.
1297
CONFUSION SET # TEST MAJOR LEX LEX+PAR NG NG+LEX NG+LEX+PAR (&)
9 commonly cited ESL confusion pairs
adverse / averse 368 85.87 97.01 96.74 91.03 97.55 97.01 (+22.2%) ?
allusion / illusion 535 76.64 91.22 91.40 91.40 92.52 93.08 (-7.5%) ?
complement / compliment 860 51.51 83.84 85.12 88.49 88.37 89.53 (-10.0%)
confidant / confident 2416 94.41 97.97 98.30 98.51 99.05 99.09 (-4.3%) ?
desert / dessert 2357 70.81 90.71 91.56 87.31 93.68 94.57 (-14.1%) ?*
discreet / discrete 219 79.45 84.48 85.84 85.84 90.41 91.32 (-9.5%) ?
elicit / illicit 563 53.46 82.77 95.56 97.51 97.51 98.22 (-28.6%)
stationary / stationery 182 62.64 87.36 92.31* 93.96 92.86 95.60 (-38.5%)
wander / wonder 6506 86.37 96.42 97.42* 97.56 98.23 98.48 (-13.9%) ?*
Total 13972 81.08 93.94 95.29* 94.82 96.56 96.99 (-12.4%) ?*
5 Original Bergsma pairs
# among / between 10227 57.46 91.89 91.86 88.34 93.60 93.58 (+3.1%) ?
# amount / number 7398 76.44 92.34 93.16* 93.03 93.42 94.08 (-10.1%) ?*
# cite / site 10185 95.71 99.42 99.53 99.16 99.52 99.63 (-22.4%)?
# peace / piece 7330 56.81 95.01 97.01* 95.55 96.74 97.46 (-22.2%)? *
# raise / rise 9464 55.98 96.12 96.64* 94.45 96.68 97.05 (-11.5%) ?
Total 44604 68.92 95.09 95.69* 94.07 96.09 96.42 (-8.4%) ?
Table 3: Spelling correction precision (%), impact of adding parse features
SVM trained on 1G words of news text, tested on 9-months of NYT data.
*: Improvement of (NG+)LEX+PAR vs. (NG+)LEX is statistically significant.
?: Improvement of NG+LEX+PAR vs. NG is statistically significant.
&: Relative increase or decrease of error rate compared to ?NG+LEX?
#: As in Bergsma et al (2009; 2010) no morphological variants of the words are used in evaluation
CONFUSION SET # TEST MAJOR CLASSIFIER COMBINED SYSTEM (&)
9 commonly cited ESL confusion pairs
adverse / averse 368 85.87 97.55 96.74 (+33.3%)
allusion / illusion 535 76.64 92.34 92.34 (- 0.0%)
complement / compliment 860 51.51 89.88 90.81 (-9.2%)
confidant / confident 2416 94.41 99.13 99.05 (+9.5%)
desert / dessert 2357 70.81 93.98 94.23 (-3.7%)
discreet / discrete 219 79.45 90.41 91.78 (-14.3%)
elicit / illicit 563 53.46 98.40 98.76 (-22.2%)
stationary / stationery 182 62.64 93.41 93.96 (-9.1%)
wander / wonder 6506 86.37 98.49 98.36 (+9.2%)
5 Original Bergsma pairs
# among / between 10227 57.46 92.73 92.73 (-0.1%)
# amount / number 7398 76.44 93.44 93.76 (-4.74%)
# cite / site 10185 95.71 99.49 99.47 (+3.8%)
# peace / piece 7330 56.81 96.19 96.38 (-5.0%)
# raise / rise 9464 55.98 96.66 96.59 (+2.2%)
Table 4: Spelling correction accuracy (%), impact of combining word co-occurrence
CLASSIFIER: Logistic Regression trained on 1G words of news text, tested on 9-months NYT data.
COMBINED SYSTEM: CLASSIFER plus system based on first-order word co-occurrence.
&: Relative increase or decrease in error rate compared to CLASSIFIER
#: As in Bergsma et al (2009; 2010), no morphological variants of the words are used in evaluation
1298
6 Conclusions
We propose a novel approach that uses parse
features and lexical features together to improve
the performance of web-scale n-gram models for
spelling correction. This method is especially adap-
tive when less training data are available, which is
the case for confusable words that are not very fre-
quently used. We also investigate the effectiveness
of incorporating web-scale word co-occurrence and
corpus-based semantic word relatedness (Random
Indexing).
For future work, we will investigate using seman-
tic information (e.g. WordNet) to extend n-gram
models. It will be interesting to see if the usage of
the word ?compliment? in ?complimenting the pres-
ident? can be estimated by considering similar us-
ages in the corpus, such as ?complimenting the stu-
dent? or by creating an n-gram database of synset
patterns. We will investigate extending, to other ap-
plications, this general methodology combining dis-
tributional, semantic and syntactic information with
language models.
Acknowledgments
We wish to thank Michael Flor of Educational
Testing Service for his TrendStream tool, which
provides fast access and easy manipulation of the
Google N-gram Corpus. We also thank Derrick Hig-
gins of Educational Testing Service for his Random
Indexing support. We also thank Satoshi Sekine of
New York University, Matthew Snover of City Uni-
versity of New York, and Jing Jiang of Singapore
Management University for their advice.
References
Shane Bergsma, Dekang Lin, and Randy Goebel. 2009.
Web-scale n-gram models for lexical disambiguation.
In IJCAI.
Shane Bergsma, Emily Pitler, and Dekang Lin. 2010.
Creating robust supervised classifiers via web-scale n-
gram data. In ACL.
Thorsten Brants and Alex Franz. 2006. Web 1T 5-gram
version 1. Available at http://www.ldc.upenn.edu.
Alexander Budanitsky and Graeme Hirst. 2001. Seman-
tic distance in wordnet: An experimental, application-
oriented evaluation of five measures. In ACL Work-
shop on WordNet and Other Lexical Resources.
Andrew Carlson and Ian Fette. 2007. Memory-based
context sensitive spelling correction at web scale. In
ICMLA.
Martin Chodorow, Joel Tetreault, and Na-Rae Han. 2007.
Detection of grammatical errors involving preposi-
tions. In Proceedings of the Fourth ACL-SIGSEM
Workshop on Prepositions, pages 25?30.
Silviu Cucerzan and David Yarowsky. 2002. Aug-
mented mixture models for lexical disambigua-tion. In
EMNLP.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed de-
pendency parses from phrase structure parses. In Pro-
ceedings of LREC, Genoa, Italy.
Scott Deerwester, Susan Dumais, George Furmas,
Thomas Landauer, and Richar Harshman. 1990. In-
dexing by latent semantic analysis. The American So-
ciety for Information Science.
Dmitriy Dligach and Martha Palmer. 2008. Novel se-
mantic features for verb sense disambiguation. In
ACL.
Philip Edmonds. 1997. Choosing the word most typical
in context using a lexical co-occurrence network. In
EACL.
Mohammed Ali Elmi and Martha Evans. 1998. Spelling
correction using context. In COLING.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui
Wang, and Chih-Jen Lin. 2008. Liblinear: A library
for large linear classification. Machine Learning Re-
search, 9(1871-1874).
Rachele De Felice and Stephen G. Pulman. 2008.
A classifier-based approach to preposition and deter-
miner error correction in L2 English. In Proceedings
of COLING, Manchester, UK.
Rachele De Felice and Stephen G. Pulman. 2009. Auto-
matic detection of preposition errors in learner writing.
CALICO Journal, 26(3).
Michael Gamon, Jianfeng Gao, Chris Brockett, Alex
Klementiev, William B. Dolan, Dmitriy Belenko, and
Lucy Vanderwende. 2008. Using contextual speller
techniques and language modeling for ESL error cor-
rection. In Proceedings of the International Joint Con-
ference on Natural Language Processing (IJCNLP),
pages 449?456, Hyderabad, India.
Andrew Golding and Dan Roth. 1996. Applying Win-
now to context-sensitive spelling correction. In Pro-
ceedings of the International Conference on Machine
Learning (ICML), pages 182?190.
Andrew Golding and Dan Roth. 1999. A winnow-based
approach to context-sensitive spelling correction. Ma-
chine Learning, 34(1-3):107?130.
Andrew Golding. 1995. A Bayesian hybrid method for
context sensitive spelling correction. In Proceedings
1299
of the Third Workshop on Very Large Corpora (WVLC-
3), pages 39?53.
Na-Rae Han, Martin Chodorow, and Claudia Leacock.
2006. Detecting errors in English article usage by
non-native speakers. Natural Language Engineering,
12(2):115?129.
Matthieu Hermet, Alain De?silets, and Stan Szpakowicz.
2008. Using the web as a linguistic resource to au-
tomatically correct lexico-syntactic errors. In Pro-
ceedings of the Sixth International Conference on Lan-
guage Resources and Evaluation (LREC), pages 390?
396, Marrekech, Morocco.
Emi Izumi, Kiyotaka Uchimoto, Toyomi Saiga, Thepchai
Supnithi, and Hitoshi Isahara. 2003. Automatic er-
ror detection in the Japanese learners? English spoken
data. In Companion Volume to the Proceedings of the
41st Annual Meeting of the Association for Computa-
tional Linguistics (ACL), pages 145?148.
Michael Jones and James Martin. 1997. Contextual
spelling correction using latent semantic analysis. In
ANLC.
Thomas Landauer, Darrell Laham, and Peter Foltz. 1998.
Learning human-like knowledge by singular value de-
composition: A progress report. Advances in Neural
Information Processing Systems, 10:45?51.
Mirella Lapata and Frank Keller. 2005. Web-based mod-
els for natural language processing. ACM Transac-
tions on Speech and Language Processing, 21:1?31.
John Lee and Ola Knutsson. 2008. The role of pp attach-
ment in preposition generation. In CICLING.
Lidia Mangu and Eric Brill. 1997. Automatic rule acqui-
sition for spelling correction. In ICML.
Saif Mohammad and Graeme Hist. 2006. Distributional
measures of concept distance: A task-oriented evalua-
tion. In EMNLP.
Alla Rozovskaya and Dan Roth. 2010. Training
paradigms for correcting errors in grammar and usage.
In ACL.
Magnus Sahlgren. 2006. The Word-Space Model: us-
ing distributional analysis to represent syntagmatic
and paradigmatic relations between words in high-
dimensional vector spaces. Ph.D. thesis.
Joel Tetreault and Martin Chodorow. 2008. The ups and
downs of prepostion error detection in esl writing. In
COLING.
Joel Tetreault, Jennifer Foster, and Martin Chodorow.
2010. Using parse features for preposition selection
and error detection. In ACL.
Casey Whitelaw, Ben Hutchinson, Grace Y. Chung, and
Gerard Ellis. 2009. Using the web for language inde-
pendent spellchecking and autocorrection. In ACL.
1300
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 1027?1037, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Ensemble Semantics for Large-scale Unsupervised Relation Extraction 
 
 
Bonan Min1* Shuming Shi2 Ralph Grishman1 Chin-Yew Lin2 
  
1New York University 2Microsoft Research Asia 
New York, NY, USA Beijing, China 
{min,grishman}@cs.nyu.edu {shumings,cyl}@microsoft.com 
  
Abstract 
Discovering significant types of relations 
from the web is challenging because of its 
open nature. Unsupervised algorithms are 
developed to extract relations from a cor-
pus without knowing the relations in ad-
vance, but most of them rely on tagging 
arguments of predefined types. Recently, 
a new algorithm was proposed to jointly 
extract relations and their argument se-
mantic classes, taking a set of relation in-
stances extracted by an open IE algorithm 
as input. However, it cannot handle poly-
semy of relation phrases and fails to 
group many similar (?synonymous?) rela-
tion instances because of the sparseness of 
features. In this paper, we present a novel 
unsupervised algorithm that provides a 
more general treatment of the polysemy 
and synonymy problems. The algorithm 
incorporates various knowledge sources 
which we will show to be very effective 
for unsupervised extraction. Moreover, it 
explicitly disambiguates polysemous rela-
tion phrases and groups synonymous 
ones. While maintaining approximately 
the same precision, the algorithm achieves 
significant improvement on recall com-
pared to the previous method. It is also 
very efficient. Experiments on a real-
world dataset show that it can handle 14.7 
million relation instances and extract a 
very large set of relations from the web.  
1 Introduction 
Relation extraction aims at discovering semantic 
relations between entities. It is an important task 
that has many applications in answering factoid 
questions, building knowledge bases and improv-
ing search engine relevance. The web has become 
a massive potential source of such relations. How-
ever, its open nature brings an open-ended set of 
relation types. To extract these relations, a system 
should not assume a fixed set of relation types, nor 
rely on a fixed set of relation argument types.  
The past decade has seen some promising solu-
tions, unsupervised relation extraction (URE) algo-
rithms that extract relations from a corpus without 
knowing the relations in advance. However, most 
algorithms (Hasegawa et al 2004, Shinyama and 
Sekine, 2006, Chen et. al, 2005) rely on tagging 
predefined types of entities as relation arguments, 
and thus are not well-suited for the open domain.  
Recently, Kok and Domingos (2008) proposed 
Semantic Network Extractor (SNE), which gener-
ates argument semantic classes and sets of synon-
ymous relation phrases at the same time, thus 
avoiding the requirement of tagging relation argu-
ments of predefined types. However, SNE has 2 
limitations: 1) Following previous URE algo-
rithms, it only uses features from the set of input 
relation instances for clustering.  Empirically we 
found that it fails to group many relevant relation 
instances. These features, such as the surface forms 
of arguments and lexical sequences in between, are 
very sparse in practice. In contrast, there exist sev-
eral well-known corpus-level semantic resources 
that can be automatically derived from a source 
corpus and are shown to be useful for generating 
the key elements of a relation: its 2 argument se-
mantic classes and a set of synonymous phrases. 
For example, semantic classes can be derived from 
a source corpus with contextual distributional simi-
larity and web table co-occurrences. The ?synony-
my? 1  problem for clustering relation instances 
                                                          
* Work done during an internship at Microsoft Research Asia 
1027
could potentially be better solved by adding these 
resources. 2) SNE assumes that each entity or rela-
tion phrase belongs to exactly one cluster, thus is 
not able to effectively handle polysemy of relation 
phrases2. An example of a polysemous phrase is be 
the currency of  as in 2 triples <Euro, be the cur-
rency of, Germany> and <authorship, be the cur-
rency of, science>. As the target corpus expands 
from mostly news to the open web, polysemy be-
comes more important as input covers a wider 
range of domains. In practice, around 22% (section 
3) of relation phrases are polysemous. Failure to 
handle these cases significantly limits its effective-
ness. 
    To move towards a more general treatment of 
the polysemy and synonymy problems, we present a 
novel algorithm WEBRE for open-domain large-
scale unsupervised relation extraction without pre-
defined relation or argument types. The contribu-
tions are: 
? WEBRE incorporates a wide range of cor-
pus-level semantic resources for improving rela-
tion extraction. The effectiveness of each 
knowledge source and their combination are stud-
ied and compared. To the best of our knowledge, it 
is the first to combine and compare them for unsu-
pervised relation extraction. 
? WEBRE explicitly disambiguates polyse-
mous relation phrases and groups synonymous 
phrases, thus fundamentally it avoids the limitation 
of previous methods. 
? Experiments on the Clueweb09 dataset 
(lemurproject.org/clueweb09.php) show that 
WEBRE is effective and efficient. We present a 
large-scale evaluation and show that WEBRE can 
extract a very large set of high-quality relations. 
Compared to the closest prior work, WEBRE sig-
nificantly improves recall while maintaining the 
same level of precision. WEBRE is efficient. To 
the best of our knowledge, it handles the largest 
triple set to date (7-fold larger than largest previous 
effort). Taking 14.7 million triples as input, a com-
plete run with one CPU core takes about a day.  
 
 
 
                                                                                           
1 We use the term synonymy broadly as defined in Section 3. 
2 A cluster of relation phrases can, however, act as a whole as 
the phrase cluster for 2 different relations in SNE. However, 
this only accounts for 4.8% of the polysemous cases. 
2 Related Work 
Unsupervised relation extraction (URE) algorithms 
(Hasegawa et al 2004; Chen et al 2005; Shinya-
ma and Sekine, 2006) collect pairs of co-occurring 
entities as relation instances, extract features for 
instances and then apply unsupervised clustering 
techniques to find the major relations of a corpus. 
These UREs rely on tagging a predefined set of 
argument types, such as Person, Organization, and 
Location, in advance. Yao et al2011 learns fine-
grained argument classes with generative models, 
but they share the similar requirement of tagging 
coarse-grained argument types. Most UREs use a 
quadratic clustering algorithm such as Hierarchical 
Agglomerate Clustering (Hasegawa et al 2004, 
Shinyama and Sekine, 2006), K-Means (Chen et 
al., 2005), or both (Rosenfeld and Feldman, 2007); 
thus they are not scalable to very large corpora.  
As the target domain shifts to the web, new 
methods are proposed without requiring predefined 
entity types. Resolver (Yates and Etzioni, 2007) 
resolves objects and relation synonyms. Kok and 
Domingos (2008) proposed Semantic Network Ex-
tractor (SNE) to extract concepts and relations. 
Based on second-order Markov logic, SNE used a 
bottom-up agglomerative clustering algorithm to 
jointly cluster relation phrases and argument enti-
ties. However, both Resolver and SNE require 
each entity and relation phrase to belong to exactly 
one cluster. This limits their ability to handle poly-
semous relation phrases. Moreover, SNE only uses 
features in the input set of relation instances for 
clustering, thus it fails to group many relevant in-
stances. Resolver has the same sparseness problem 
but it is not affected as much as SNE because of its 
different goal (synonym resolution).  
As the preprocessing instance-detection step for 
the problem studied in this paper, open IE extracts 
relation instances (in the form of triples) from the 
open domain (Etzioni et al 2004; Banko et al 
2007; Fader et al 2011; Wang et al2011). For 
efficiency, they only use shallow features. Reverb 
(Fader et al 2011) is a state-of-the-art open do-
main extractor that targets verb-centric relations, 
which have been shown in Banko and Etzioni 
(2008) to cover over 70% of open domain rela-
tions. Taking their output as input, algorithms have 
been proposed to resolve objects and relation syn-
onyms (Resolver),  extract semantic networks 
1028
(SNE), and map extracted relations into an existing 
ontology (Soderland and Mandhani, 2007).  
Recent work shows that it is possible to con-
struct semantic classes and sets of similar phrases 
automatically with data-driven approaches. For 
generating semantic classes, previous work applies 
distributional similarity (Pasca, 2007; Pantel et al 
2009), uses a few linguistic patterns (Pasca 2004; 
Sarmento et al 2007), makes use of structure in 
webpages (Wang and Cohen 2007, 2009), or com-
bines all of them (Shi et al 2010). Pennacchiotti 
and Pantel (2009) combines several sources and 
features. To find similar phrases, there are 2 close-
ly related tasks: paraphrase discovery and recog-
nizing textual entailment. Data-driven paraphrase 
discovery methods (Lin and Pantel, 2001; Pasca 
and Dienes, 2005; Wu and Zhou, 2003; Sekine, 
2005) extends the idea of distributional similarity 
to phrases. The Recognizing Textual Entailment 
algorithms (Berant et al2011) can also be used to 
find related phrases since they find pairs of phrases 
in which one entails the other.  
To efficiently cluster high-dimensional datasets, 
canopy clustering (McCallum et al 2000) uses a 
cheap, approximate distance measure to divide da-
ta into smaller subsets, and then cluster each subset 
using an exact distance measure. It has been ap-
plied to reference matching. The second phase of 
WEBRE applies the similar high-level idea of par-
tition-then-cluster for speeding up relation cluster-
ing. We design a graph-based partitioning 
subroutine that uses various types of evidence, 
such as shared hypernyms.  
3 Problem Analysis 
The basic input is a collection of relation instances 
(triples) of the form <ent1, ctx, ent2>. For each tri-
ple, ctx is a relational phrase expressing the rela-
tion between the first argument ent1 and the second 
argument ent2. An example triple is <Obama, win 
in, NY>. The triples can be generated by an open 
IE extractor such as TextRunner or Reverb. Our 
goal is to automatically build a list of relations 
? = {< ent1, ???, ent2 >} ? 3 < ?1,?,?2 >  where P 
is the set of relation phrases, and ?1  and  ?2  are 
two argument classes. Examples of triples and rela-
tions R (as Type B) are shown in Figure 1. 
                                                          
3 This approximately equal sign connects 2 possible represen-
tations of a relation: as a set of triple instances or a triple with 
2 entity classes and a relation phrase class. 
The first problem is the polysemy of relation 
phrases, which means that a relation phrase ctx can 
express different relations in different triples. For 
example, the meaning of be the currency of in the 
following two triples is quite different: <Euro, be 
the currency of, Germany> and <authorship, be 
the currency of, science>. It is more appropriate to 
assign these 2 triples to 2 relations ?a currency is 
the currency of a country? and ?a factor is im-
portant in an area? than to merge them into one. 
Formally, a relation phrase ctx is polysemous if 
there exist 2 different relations < ?1,?,?2 >  and 
< ?1
?
,??,?2
?
> where ??? ? ? ? ??. In the previ-
ous example, be the currency of  is polysemous 
because it appears in 2 different relations.  
Polysemy of relation phrases is not uncommon. 
We generate clusters from a large sample of triples 
with the assistance of a soft clustering algorithm, 
and found that around 22% of relation phrases can 
be put into at least 2 disjoint clusters that represent 
different relations. More importantly, manual in-
spection reveals that some common phrases are 
polysemous. For example, be part of can be put 
into a relation ?a city is located in a country? when 
connecting Cities to Countries, and another rela-
tion ?a company is a subsidiary of a parent com-
pany? when connecting Companies to Companies. 
Failure to handle polysemous relation phrases fun-
damentally limits the effectiveness of an algorithm. 
The WEBRE algorithm described later explicitly 
handles polysemy and synonymy of relation 
phrases in its first and second phase respectively. 
The second problem is the ?synonymy? of rela-
tion instances. We use the term synonymy broadly 
and we say 2 relation instances are synonymous if 
they express the same semantic relation between 
the same pair of semantic classes. For example, 
both <Euro, be the currency used in, Germany> 
and <Dinar, be legal tender in, Iraq> express the 
relation <Currencies, be currency of, Countries>. 
Solving this problem requires grouping synony-
mous relation phrases and identifying argument 
semantic classes for the relation.  
Various knowledge sources can be derived from 
the source corpus for this purpose. In this paper we 
pay special attention to incorporating various se-
mantic resources for relation extraction. We will 
show that these semantic sources can significantly 
improve the coverage of extracted relations and the  
 
1029
Figure 1. Overview of the WEBRE algorithm (Illustrated with examples sampled from experiment results). The tables and rec-
tangles with a database sign show knowledge sources, shaded rectangles show the 2 phases, and the dotted shapes show the sys-
tem output, a set of Type A relations and a set of Type B relations. The orange arrows denote resources used in phase 1 and the 
green arrows show the resources used in phase 2. 
 
best performance is achieved when various re-
sources are combined together.  
4 Mining Relations from the Web 
We first describe relevant knowledge sources, and 
then introduce the WEBRE algorithm, followed by 
a briefly analysis on its computational complexity.  
4.1 Knowledge Sources 
Entity similarity graph We build two similarity 
graphs for entities: a distributional similarity (DS) 
graph and a pattern-similarity (PS) graph. The DS 
graph is based on the distributional hypothesis 
(Harris, 1985), saying that terms sharing similar 
contexts tend to be similar. We use a text window 
of size 4 as the context of a term, use Pointwise 
Mutual Information (PMI) to weight context fea-
tures, and use Jaccard similarity to measure the 
similarity of term vectors. The PS graph is gener-
ated by adopting both sentence lexical patterns and 
HTML tag patterns (Hearst, 1992; Kozareva et al 
2008; Zhang et al 2009; Shi et al 2010). Two 
terms (T) tend to be semantically similar if they co-
occur in multiple patterns. One example of sen-
tence lexical patterns is (such as | including) 
T{,T}* (and|,|.). HTML tag patterns include tables, 
dropdown boxes, etc. In these two graphs, nodes 
are entities and the edge weights indicate entity 
similarity. In all there are about 29.6 million nodes 
and 1.16 billion edges. 
Hypernymy graph Hypernymy relations are 
very useful for finding semantically similar term 
pairs. For example, we observed that a small city 
in UK and another small city in Germany share 
common hypernyms such as city, location, and 
place. Therefore the similarity between the two 
cities is large according to the hypernymy graph, 
while their similarity in the DS graph and the PS 
graph may be very small. Following existing work 
(Hearst, 1992, Pantel & Ravichandran 2004; Snow 
et al 2005; Talukdar et al 2008; Zhang et al 
2011), we adopt a list of lexical patterns to extract 
hypernyms. The patterns include NP {,} (such as) 
{NP,}* {and|or} NP, NP (is|are|was|were|being) 
(a|an|the) NP, etc. The hypernymy graph is a bi-
partite graph with two types of nodes: entity nodes 
and label (hypernym) nodes. There is an edge (T, 
L) with weight w if L is a hypernym of entity T 
with probability w. There are about 8.2 million 
nodes and 42.4 million edges in the hypernymy 
graph. In this paper, we use the terms hypernym 
and label interchangeably. 
Relation phrase similarity: To generate the pair-
wise similarity graph for relation phrases with re-
gard to the probability of expressing the same 
relation, we apply a variant of the DIRT algorithm 
(Lin and Pantel, 2001). Like DIRT, the paraphrase 
discovery relies on the distributional hypothesis, 
but there are a few differences: 1) we use stemmed 
lexical sequences (relation phrases) instead of de-
pendency paths as phrase candidates because of the 
very large scale of the corpus. 2) We used ordered 
1030
pairs of arguments as features of phrases while 
DIRT uses them as independent features. We em-
pirically tested both feature schemes and found 
that using ordered pairs results in likely para-
phrases but using independent features the result 
contains general inference rules4. 
4.2 WEBRE for Relation Extraction 
WEBRE consists of two phases. In the first 
phase, a set of semantic classes are discovered and 
used as argument classes for each relation phrase. 
This results in a large collection of relations whose 
arguments are pairs of semantic classes and which 
have exactly one relation phrase. We call these 
relations the Type A relations. An example Type A 
relation is <{New York, London?}, be locate in, 
{USA, England, ?}>. During this phase, polyse-
mous relation phrases are disambiguated and 
placed into multiple Type A relations. The second 
phase is an efficient algorithm which groups simi-
lar Type A relations together. This step enriches 
the argument semantic classes and groups synon-
ymous relation phrases to form relations with mul-
tiple expressions, which we called Type B 
relations. Both Type A and Type B relations are 
system outputs since both are valuable resources 
for downstream applications such as QA and Web 
Search. An overview of the algorithm is shown in 
Figure 1. Here we first briefly describe a clustering 
subroutine that is used in both phases, and then 
describe the algorithm in detail. 
To handle polysemy of objects (e.g., entities or 
relations) during the clustering procedure, a key 
building block is an effective Multi-Membership 
Clustering algorithm (MMClustering). For simplic-
ity and effectiveness, we use a variant of Hierar-
chical Agglomerative Clustering (HAC), in which 
we first cluster objects with HAC, and then reas-
sign each object to additional clusters when its 
similarities with these clusters exceed a certain 
threshold5. In the remainder of this paper, we use 
{C} = MMClustering({object}, SimFunc, ?) to rep-
resent running MMClustering over a set of objects, 
                                                          
4 For example, be part of  has ordered argument pairs <A, B> 
and <C, D>, and be not part of has ordered argument pairs 
<A, D> and <B, C>. If arguments are used as independent 
features, these two phrases shared the same set of features {A, 
B, C, D}. However, they are inferential (complement relation-
ship) rather than being similar phrases. 
5 This threshold should be slightly greater than the clustering 
threshold for HAC to avoid generating duplicated clusters. 
with threshold ? to generate a list of clusters {C} of 
the objects, given the pairwise object similarity 
function SimFunc. Our implementation uses HAC 
with average linkage since empirically it performs 
well. 
Discovering Type A Relations The first phase 
of the relation extraction algorithm generates Type 
A relations, which have exactly one relation phrase 
and two argument entity semantic classes. For each 
relation phrase, we apply a clustering algorithm on 
each of its two argument sets to generate argument 
semantic classes. The Phase 1 algorithm processes 
relation phrases one by one. For each relation 
phrase ctx, step 4 clusters the set {ent1} using 
MMClustering to find left-hand-side argument se-
mantic classes {C1}. Then for each cluster C in 
{C1}, it gathers the right-hand-side arguments 
which appeared in some triple whose left hand-
side-side argument is in C, and puts them into 
{ent2?}. Following this, it clusters {ent2?} to find 
right-hand-side argument semantic classes. This 
results in pairs of semantic classes which are ar-
guments of ctx. Each relation phrase can appear in 
multiple non-overlapping Type A relations. For 
example, <Cities, be part of, Countries> and 
<Companies, be part of, Companies> are different 
Type A relations which share the same relation 
phrase be part of. In the pseudo code, SimEntFunc 
is encoded in the entity similarity graphs.  
 
Algorithm Phase 1: Discovering Type A relations 
Input:  set of triples T={<ent1, ctx, ent2>} 
 entity similarity function SimEntFunc 
 Similarity threshold ? 
Output:  list of Type A relations {<C1, ctx, C2>} 
Steps:  
01. For each relation phrase ctx 
02.     {ent1, ctx, ent2} = set of triples sharing ctx 
03.     {ent1} = set of ent1 in {ent1, ctx, ent2} 
04.     {C1} = MMClustering({ent1}, SimEntFunc, ?) 
05.     For each C in { C1} 
06.         {ent2?} = set of ???2 ?. ?.?< ???1, ???, ???2 > ?
 ? ? ???1 ? ?1 
07.         {C2} = MMClustering({ent2?}, SimEntFunc, ?) 
08.         For each C2 in {C2} 
09.             Add <C1, ctx, C2> into {<C1, ctx, C2>} 
10. Return {<C1, ctx, C2>} 
 
    Discovering Type B Relations  The goal of 
phase 2 is to merge similar Type A relations, such 
as <Cities, be locate in, Countries> and <Cities, 
be city of, Countries>, to produce Type B relations, 
which have a set of synonymous relation phrases 
and more complete argument entity classes. The 
challenge for this phase is to cluster a very large 
1031
set of Type A relations, on which it is infeasible to 
run a clustering algorithm that does pairwise all 
pair comparison. Therefore, we designed an evi-
dence-based partition-then-cluster algorithm. 
The basic idea is to heuristically partition the 
large set of Type A relations into small subsets, 
and run clustering algorithms on each subset. It is 
based on the observation that most pairs of Type A 
relations are not similar because of the sparseness 
in the entity class and the relation semantic space. 
If there is little or no evidence showing that two 
Type A relations are similar, they can be put into 
different partitions. Once partitioned, the clustering 
algorithm only has to be run on each much smaller 
subset, thus computation complexity is reduced.  
The 2 types of evidence we used are shared 
members and shared hypernyms of relation argu-
ments. For example, 2 Type A relations 
r1=<Cities, be city of, Countries> and r2=<Cities, 
be locate in, Countries> share a pair of arguments 
<Tokyo, Japan>, and a pair of hypernyms <?city?, 
?country?>. These pieces of evidence give us hints 
that they are likely to be similar. As shown in the 
pseudo code, shared arguments and hypernyms are 
used as independent evidence to reduce sparseness. 
 
Algorithm Phase 2: Discovering Type B relations 
Input:  Set of Type A relations {r}={<C1, ctx, C2>} 
 Relation similarity function SimRelationFunc 
 Map from entities to their hypernyms: Mentity2label 
 Similarity threshold ? 
Edge weight threshold ? 
Variables G(V, E) = weighted graph in which V={r} 
Output:  Set of Type B relations {<C1, P, C2>} 
Steps:  
01. {<ent, {r?}>} = build  inverted index from argument 
ent to set of Type A relations {r?} on {<C1, ctx, C2>}  
02 {<l, {r?}>} = build  inverted index from hypernym l 
of arguments to set of Type A relations {r?} on {<C1, 
ctx, C2>} with map Mentity2label  
03. For each ent in {<ent, {r?}>} 
04.     For each pair of r1 and r2  s.t. ?1 ? {?
?} ? ?2 ? {??}    
05.        weight_edge(<r1, r2>) += weight (ent) 
06. For each l in {<l, {r?}>} 
07.     For each pair of r1 and r2  s.t. ?1 ? {?
?} ? ?2 ? {??}    
08.        weight_edge(<r1, r2>) += weight (l) 
09. For each edge <r1, r2> in G 
10.     If weight_edge(<r1, r2>) < ? 
11.         Remove edge <r1, r2> from G 
12. {CC}= DFS(G) 
13. For each connected component CC in {CC} 
14.     {<C1, ctx, C2>} = vertices in CC 
15. {<C1?, P?, C2?>} = MMClustering({<C1, ctx, C2>},  
  SimRelationFunc, ?) 
16.     Add {<C1?, P?, C2?>} into {<C1, P, C2>} 
17. Return {<C1, P, C2>} 
 
Steps 1 and 2 build an inverted index from evi-
dence to sets of Type A relations. On the graph G 
whose vertices are Type A relations, steps 3 to 8 
set the value of edge weights based on the strength 
of evidence that shows the end-points are related. 
The weight of evidence E is calculated as follows: 
 
??????(?) =
# ?????? ?????? ?? ????? ? ??????? ?? 
max(# ??????? ? ??????? ??)
 
 
The idea behind this weighting scheme is similar 
to that of TF-IDF in that the weight of evidence is 
higher if it appears more frequently and is less am-
biguous (appeared in fewer semantic classes during 
clustering of phase 1). The weighting scheme is 
applied to both shared arguments and labels. 
After collecting evidence, we prune (steps 9 to 
11) the edges with a weight less than a threshold ? 
to remove noise. Then a Depth-First Search (DFS) 
is called on G to find all Connected Components 
CC of the graph. These CCs are the partitions of 
likely-similar Type A relations. We run MMClus-
tering on each CC in {CC} and generate Type B  
relations (step 13 to step 16).  The similarity of 2 
relations (SimRelationFunc) is defined as follows: 
???(< ?1,?,?2 >, < ?1
?,??,?2
? >) 
 
= ?
0,     ?? ???(?,??) <  ?
min????(?1,?1
?), ???(?2,?2
?)? ,   ???? 
  
4.3 Computational Complexity 
WEBRE is very efficient since both phases de-
compose the large-clustering task into much small-
er clustering tasks over partitions. Given n objects 
for clustering, a hierarchical agglomerative cluster-
ing algorithm requires ?(?2)  pairwise compari-
sons. Assuming the clustering task is split into 
subtasks of size ?1, ?2, ?, ??, thus the computa-
tional complexity is reduced to ?(? ??
2?
1 ). Ideally 
each subtask has an equal size of ?/?, so the com-
putational complexity is reduced to O(?2/?) , a 
factor of ? speed up. In practice, the sizes of parti-
tions are not equal. Taking the partition sizes ob-
served in the experiment with 0.2 million Type A 
relations as input, the phase 2 algorithm achieves 
around a 100-fold reduction in pairwise compari-
sons compared to the agglomerative clustering al-
gorithm. The combination of phase 1 and phase 2 
achieves more than a 1000-fold reduction in pair-
wise comparison, compared to running an agglom-
erative clustering algorithm directly on 14.7 
million triples. This reduction of computational 
1032
complexity makes the unsupervised extraction of 
relations on a large dataset a reality. In the experi-
ments with 14.7 million triples as input, phase 1 
finished in 22 hours, and the phase 2 algorithm 
finished in 4 hours with one CPU core. 
Furthermore, both phases can be run in parallel 
in a distributed computing environment because 
data is partitioned. Therefore it is scalable and effi-
cient for clustering a very large number of relation 
instances from a large-scale corpus like the web.  
5 Experiment 
Data preparation We tested WEBRE on re-
sources extracted from the English subset of the 
Clueweb09 Dataset, which contains 503 million 
webpages. For building knowledge resources, all 
webpages are cleaned and then POS tagged and 
chunked with in-house tools. We implemented the 
algorithms described in section 4.1 to generate the 
knowledge sources, including a hypernym graph, 
two entity similarity graphs and a relation phrase 
similarity graph. 
We used Reverb Clueweb09 Extractions 1.1 
(downloaded from reverb.cs.washington.edu) as 
the triple store (relation instances). It is the com-
plete extraction of Reverb over Clueweb09 after 
filtering low confidence and low frequency triples. 
It contains 14.7 million distinct triples with 3.3 
million entities and 1.3 million relation phrases. 
We choose it because 1) it is extracted by a state-
of-the-art open IE extractor from the open-domain, 
and 2) to the best of our knowledge, it contains the 
largest number of distinct triples extracted from the 
open-domain and which is publicly available. 
 
Evaluation setup The evaluations are organized as 
follows: we evaluate Type A relation extraction 
and Type B relation extraction separately, and then 
we compare WEBRE to its closest prior work 
SNE.  Since both phases are essentially clustering 
algorithms, we compare the output clusters with 
human labeled gold standards and report perfor-
mance measures, following most previous work 
such as Kok and Domingos (2008) and Hasegawa 
et al(2004). Three gold standards are created for 
evaluating Type A relations, Type B relations and 
the comparison to SNE, respectively. In the exper-
iments, we set ?=0.6, ?=0.1 and ?=0.02 based on 
trial runs on a small development set of 10k rela-
tion instances. We filtered out the Type A relations 
and Type B relations which only contain 1 or 2 
triples since most of these relations are not differ-
ent from a single relation instance and are not very 
interesting. Overall, 0.2 million Type A relations 
and 84,000 Type B relations are extracted. 
 
Evaluating Type A relations To understand the 
effectiveness of knowledge sources, we run Phase 
1 multiple times taking entity similarity graphs 
(matrices) constructed with resources listed below: 
? TS: Distributional similarity based on the triple 
store. For each triple <ent1, ctx, ent2>, features 
of ent1 are {ctx} and {ctx ent2}; features of ent2 
are {ctx} and {ent1 ctx}. Features are weighted 
with PMI. Cosine is used as similarity measure.  
? LABEL: The similarity between two entities is 
computed according to the percentage of top 
hypernyms they share. 
? SIM: The similarity between two entities is the 
linear combination of their similarity scores in 
the distributional similarity graph and in the 
pattern similarity graph. 
? SIM+LABEL SIM and LABEL are combined. 
Observing that SIM generates high quality but 
overly fine-grained semantic classes, we modify 
the entity clustering procedure to cluster argu-
ment entities based on SIM first, and then fur-
ther clustering the results based on LABEL. 
The outputs of these runs are pooled and mixed 
for labeling. We randomly sampled 60 relation 
phrases. For each phrase, we select the 5 most fre-
quent Type A relations from each run (4?5=206 
Type A relations in all). For each relation phrase, 
we ask a human labeler to label the mixed pool of 
Type A relations that share the phrase: 1) The la-
belers7 are asked to first determine the major se-
mantic relation of each Type A relation, and then 
label the triples as good, fair or bad based on 
whether they express the major relation. 2) The 
labeler also reads all Type A relations and manual-
ly merges the ones that express the same relation. 
These 2 steps are repeated for each phrase. After 
labeling, we create a gold standard GS1, which 
contains roughly 10,000 triples for 60 relation 
phrases. On average, close to 200 triples are manu-
                                                          
6  Here 4 means the 4 methods (the bullet items above) of 
computing similarity. 
7 4 human labelers perform the task. A portion of the judg-
ments were independently dual annotated; inter-annotator 
agreement is 79%. Moreover, each judgment is cross-checked 
by at least one more annotator, further improving quality. 
1033
ally labeled and clustered for each phrase. This 
creates a large data set for evaluation.  
We report micro-average of precision, recall and 
F1 on the 60 relation phrases for each method. Pre-
cision (P) and Recall (R) of a given relation phrase 
is defined as follows. Here ?? and ??
?  represents a 
Type A relation in the algorithm output and GS1, 
respectively. We use t for triples and s(t) to repre-
sent the score of the labeled triple t. s(t) is set to 
1.0, 0.5 or 0 for t labeled as good, fair and bad, 
respectively. 
 
? =
? ? ?(?) ????  ??
? |??|??
, ? =
? ? ?(?) ????  ??
? ? ?(??) ?????
???
?
 
 
The results are in table 1. Overall, LABEL per-
forms 53% better than TS in F-measure, and 
SIM+LABEL performs the best, 8% better than 
LABEL. Applying a simple sign test shows both 
differences are clearly significant (p<0.001). Sur-
prisingly, SIM, which uses the similarity matrix 
extracted from full text, has a F1 of 0.277, which is 
lower than TS. We also tried combining TS and 
LABEL but did not find encouraging performance 
compared to SIM+LABEL. 
 
Algorithm Precision Recall F1 
TS 0.842 (0.886) 0.266 0.388 
LABEL 0.855 (0.870) 0.481 0.596 
SIM 0.755 (0.964) 0.178 0.277 
SIM+LABEL 0.843 (0.872) 0.540 0.643 
 
Table 1. Phase 1 performance (averaged on multiple runs) of 
the 4 methods. The highest performance numbers are in bold. 
(The number in parenthesis is the micro-average when empty-
result relation phrases are not considered for the method). 
 
Among the 4 methods, SIM has the highest preci-
sion (0.964) when relation phrases for which it 
fails to generate any Type A relations are exclud-
ed, but its recall is low. Manual checking shows 
that SIM tends to generate overly fine-grained ar-
gument classes. If fine-grained argument classes or 
extremely high-precision Type A relations are pre-
ferred, SIM is a good choice. LABEL performs 
significantly better than TS, which shows that hy-
pernymy information is very useful for finding ar-
gument semantic classes. However, it has coverage 
problems in that the hypernym finding algorithm 
failed to find any hypernym from the corpus for 
some entities. Following up, we found that 
SIM+LABEL has similar precision and the highest 
recall. This shows that the combination of semantic 
spaces is very helpful. The significant recall im-
provement from TS to SIM+LABEL shows that 
the corpus-based knowledge resources significant-
ly reduce the data sparseness, compared to using 
features extracted from the triple store only. The 
result of the phase 1 algorithm with SIM+LABEL 
is used as input for phase 2. 
 
Evaluating Type B relations The goal is 2-fold: 
1) to evaluate the phase 2 algorithm. This involves 
comparing system output to a gold standard con-
structed by hand, and reporting performance; 2) to 
evaluate the quality of Type B relations. For this, 
we will also report triple-level precision. 
    We construct a gold standard GS28 for evaluat-
ing Type B relations as follows: We randomly 
sampled 178 Type B relations, which contain 1547 
Type A relations and more than 100,000 triples. 
Since the number of triples is very large, it is in-
feasible for labelers to manually cluster triples to 
construct a gold standard. To report precision, we 
asked the labelers to label each Type A relation 
contained in this Type B relation as good, fair or 
bad based on whether it expresses the same rela-
tion. For recall evaluation, we need to know how 
many Type A relations are missing from each Type 
B relation. We provide the full data set of Type A 
relations along with three additional resources: 1) a 
tool which, given a Type A relation, returns a 
ranked list of similar Type A relations based on the 
pairwise relation similarity metric in section 4, 2) 
DIRT paraphrase collection, 3) WordNet (Fell-
baum, 1998) synsets. The labelers are asked to find 
similar phrases by checking phrases which contain 
synonyms of the tokens in the query phrase. Given 
a Type B relation, ideally we expect the labelers to 
find all missing Type A relations using these re-
sources. We report precision (P) and recall (R) as 
follows. Here ??  and ??
?  represent Type B rela-
tions in the algorithm output and GS2, respective-
ly. ??  and ??
?  represent Type A relations. ?(??) 
denotes the score of ??. It is set to 1.0, 0.5 and 0 
for good, fair or bad respectively.  
 
? =
? ? |??|??(??) ???????
? ? |??|  ???????
, ? =
? ? |??|??(??)  ???????
? ? ???
? ???
? ???
???
?
 
 
We also ask the labeler to label at most 50 ran-
domly sampled triples from each Type B relation, 
and calculate triple-level precision as the ratio of 
the sum of scores of triples over the number of  
                                                          
8 3 human labelers performed the task. A portion of the judg-
ments were independently dual annotated; inter-annotator 
agreement is 73%. Similar to labeling Type A relations, each 
judgment is cross-checked by at least one more annotator, 
further improving quality. 
1034
Argument 1 Relation phrase Argument 2 
marijuana, caffeine, nicotine? result in, be risk factor for, be major cause of? insomnia, emphysema, breast cancer,? 
C# 2.0, php5, java, c++, ? allow the use of, also use, introduce the concept of? destructors, interfaces, template,? 
clinton, obama, mccain, ? win, win in, take, be lead in,? ca, dc, fl, nh, pa, va, ga, il, nc,? 
Table 3. Sample Type B relations extracted. 
 
sampled triples. We use ???? to represent the preci-
sion calculated based on labeled triples. Moreover, 
as we are interested in how many phrases are 
found by our algorithm, we also include ???????, 
which is the recall of synonymous phrases. Results 
are shown in Table 2.  
 
Interval P R (???????) F1 ???? count 
[3, 5) 0.913 0.426 (0.026) 0.581 0.872 52149 
[5, 10) 0.834 0.514 (0.074) 0.636 0.863 21981 
[10, 20) 0.854 0.569 (0.066) 0.683 0.883 6277 
[20, 50) 0.899 0.675 (0.406) 0.771 0.894 2630 
[50, +?) 0.922 0.825 (0.594) 0.871 0.929 1089 
Overall 0.897 0.684 (0.324) 0.776 0.898 84126 
Table 2. Performance for Type B relation extraction. The first 
column shows the range of the maximum sizes of Type A 
relations in the Type B relation. The last column shows the 
number of Type B relations that are in this range. The number 
in parenthesis in the third column is the recall of phrases.  
 
The result shows that WEBRE can extract Type B 
relations at high precision (both P and ????). The 
overall recall is 0.684. Table 2 also shows a trend 
that if the maximum number of Type A relation in 
the target Type B relation is larger, the recall is 
better. This shows that the recall of Type B rela-
tions depends on the amount of data available for 
that relation. Some examples of Type B relations 
extracted are shown in Table 3. 
  
Comparison with SNE We compare WEBRE?s 
extracted Type B relations to the relations extract-
ed by its closest prior work SNE9. We found SNE 
is not able to handle the 14.7 million triples in a 
foreseeable amount of time, so we randomly sam-
pled 1 million (1M) triples 10 and test both algo-
rithms on this set. We also filtered out result 
clusters which have only 1 or 2 triples from both 
system outputs. For comparison purposes, we con-
structed a gold standard GS3 as follows: randomly 
select 30 clusters from both system outputs, and 
then find similar clusters from the other system 
output, followed by manually refining the clusters 
                                                          
9 Obtained from alchemy.cs.washington.edu/papers/kok08 
10 We found that SNE?s runtime on 1M triples varies from 
several hours to over a week, depending on the parameters. 
The best performance is achieved with runtime of approxi-
mately 3 days. We also tried SNE with 2M triples, on which 
many runs take several days and show no sign of convergence. 
For fairness, the comparison was done on 1M triples. 
by merging similar ones and splitting non-coherent 
clusters. GS3 contains 742 triples and 135 clusters. 
We report triple-level pairwise precision, recall 
and F1 for both algorithms against GS3, and report 
results in Table 4. We fine-tuned SNE (using grid 
search, internal cross-validation, and coarse-to-fine 
parameter tuning), and report its best performance. 
 
Algorithm Precision Recall F1 
WEBRE 0.848 0.734 0.787 
SNE 0.850 0.080 0.146 
 
Table 4. Pairwise precision/recall/F1 of WEBRE and SNE.  
 
Table 4 shows that WEBRE outperforms SNE 
significantly in pairwise recall while having similar 
precision. There are two reasons. First, WEBRE 
makes use of several corpus-level semantic sources 
extracted from the corpus for clustering entities 
and phrases while SNE uses only features in the 
triple store. These semantic resources significantly 
reduced data sparseness. Examination of the output 
shows that SNE is unable to group many triples 
from the same generally-recognized fine-grained 
relations. For example, SNE placed relation in-
stances <Barbara, grow up in, Santa Fe> and 
<John, be raised mostly in, Santa Barbara> into 2 
different clusters because the arguments and 
phrases do not share features nor could be grouped 
by SNE?s mutual clustering. In contrast, WEBRE 
groups them together. Second, SNE assumes a re-
lation phrase to be in exactly one cluster. For ex-
ample, SNE placed be part of in the phrase cluster 
be city of and failed to place it in another cluster be 
subsidiary of. This limits SNE?s ability to placing 
relation instances with polysemous phrases into 
correct relation clusters. 
It should be emphasized that we use pairwise 
precision and recall in table 4 to be consistent with 
the original SNE paper. Pairwise metrics are much 
more sensitive than instance-level metrics, and pe-
nalize recall exponentially in the worst case11 if an 
algorithm incorrectly splits a coherent cluster; 
therefore the absolute pairwise recall difference 
                                                          
11 Pairwise precision and recall are calculated on all pairs that 
are in the same cluster, thus are very sensitive. For example, if 
an algorithm incorrectly split a cluster of size N to a smaller 
main cluster of size N/2 and some constant-size clusters, pair-
wise recall could drop to as much as ? of its original value. 
1035
should not be interpreted as the same as the in-
stance-level recall reported in previous experi-
ments. On 1 million triples, WEBRE generates 
12179 triple clusters with an average size12 of 13 
while SNE generate 53270 clusters with an aver-
age size 5.1. In consequence, pairwise recall drops 
significantly. Nonetheless, at above 80% pairwise 
precision, it demonstrates that WEBRE can group 
more related triples by adding rich semantics har-
vested from the web and employing a more general 
treatment of polysemous relation phrases.  On 1M 
triples, WEBRE finished in 40 minutes, while the 
run time of SNE varies from 3 hours to a few days. 
6 Conclusion 
We present a fully unsupervised algorithm 
WEBRE for large-scale open-domain relation ex-
traction. WEBRE explicitly handles polysemy rela-
tions and achieves a significant improvement on 
recall by incorporating rich corpus-based semantic 
resources. Experiments on a large data set show 
that it can extract a very large set of high-quality 
relations. 
 
Acknowledgements 
Supported in part by the Intelligence Advanced 
Research Projects Activity (IARPA) via Air Force 
Research Laboratory (AFRL) contract number 
FA8650-10-C-7058. The U.S. Government is au-
thorized to reproduce and distribute reprints for 
Governmental purposes notwithstanding any copy-
right annotation thereon. The views and conclu-
sions contained herein are those of the authors and 
should not be interpreted as necessarily represent-
ing the official policies or endorsements, either 
expressed or implied, of IARPA, AFRL, or the 
U.S. Government. 
 
References 
Michele Banko, Michael J. Cafarella, Stephen Soder-
land, Matt Broadhead, and Oren Etzioni. 2007. Open 
Information Extraction from the Web. In Proceedings 
of IJCAI 2007. 
                                                          
12 The clusters which have only 1 or 2 triples are removed and 
not counted here for both algorithms. 
Michele Banko and Oren Etzioni. 2008. The Tradeoffs 
Between Open and Traditional Relation Extraction. 
In Proceedings of ACL 2008. 
Jonathan Berant, Ido Dagan and Jacob Goldberger. 
2011. Global Learning of Typed Entailment Rules. In 
Proceedings of ACL 2011. 
Razvan Bunescu and Raymond J. Mooney. 2004. Col-
lective Information Extraction with Relational Mar-
kov Networks. In Proceedings of ACL 2004. 
Jinxiu Chen, Donghong Ji, Chew Lim Tan, Zhengyu 
Niu. 2005. Unsupervised Feature Selection for Rela-
tion Extraction. In Proceedings of IJCNLP 2005. 
Oren Etzioni, Michael Cafarella, Doug Downey, Stan-
ley Kok, Ana-Maria Popescu, Tal Shaked, Stephen 
Soderland, Daniel S. Weld, and Alexander Yates. 
2004. Web-scale information extraction in 
KnowItAll (preliminary results). In Proceedings of 
WWW 2004. 
Oren Etzioni, Michael Cafarella, Doug Downey, 
AnaMaria Popescu, Tal Shaked, Stephen Soderland, 
Daniel S. Weld and Alexander Yates. 2005. Unsu-
pervised named-entity extraction from the Web: An 
Experimental Study. In Artificial Intelligence, 
165(1):91-134. 
Anthony Fader, Stephen Soderland, and Oren Etzioni. 
2011. Identifying Relations for Open Information Ex-
traction. In Proceedings of EMNLP 2011. 
Christiane Fellbaum (Ed.). 1998. WordNet: An Elec-
tronic Lexical Database. Cambridge, MA: MIT Press. 
Zelig S. Harris. 1985. Distributional Structure. The Phi-
losophy of Linguistics. New York: Oxford Uni-
versity Press. 
Takaaki Hasegawa, Satoshi Sekine, Ralph Grishman . 
2004.Discovering Relations among Named Entities 
from Large Corpora. In Proceedings of ACL 2004. 
Marti A. Hearst. 1992. Automatic  Acquisition of  Hy-
ponyms from Large Text Corpora. In Proceedings of 
COLING 1992. 
Stanley Kok and Pedro Domingos. 2008. Extracting 
Semantic Networks from Text via Relational Cluster-
ing. In Proceedings of ECML 2008. 
Zornitsa Kozareva, Ellen Riloff, Eduard Hovy. 2008. 
Semantic Class Learning from the Web with Hypo-
nym Pattern Linkage Graphs. In Proceedings of ACL 
2008. 
Dekang Lin and Patrick Pantel. 2001. DIRT ? Discov-
ery of Inference Rules from Text. In Proceedings of 
KDD 2001. 
Andrew McCallum, Kamal Nigam and Lyle Ungar. 
2000. Efficient Clustering of High-Dimensional Data 
Sets with Application to Reference Matching. In Pro-
ceedings of KDD 2000. 
Patrick Pantel, Eric Crestan, Arkady Borkovsky, Ana-
Maria Popescu and Vishnu Vyas. 2009. Web-Scale 
Distributional Similarity and Entity Set Expansion. In 
Proceedings of EMNLP 2009. 
1036
Patrick Pantel and Dekang Lin. 2002. Discovering word 
senses from text. In Proceedings of KDD2002. 
Patrick Pantel and Deepak Ravichandran. 2004. Auto-
matically Labeling Semantic Classes. In Proceedings 
of HLT/NAACL-2004. 
Marius Pasca. 2004. Acquisition of Categorized Named 
Entities for Web Search, In Proceedings of CIKM 
2004. 
Marius Pasca. 2007. Weakly-supervised discovery of 
named entities using web search queries. In Proceed-
ings of CIKM 2007. 
Marius Pasca and Peter Dienes. 2005. Aligning needles 
in a haystack: Paraphrase acquisition across the Web. 
In Proceedings of IJCNLP 2005. 
Marco Pennacchiotti and Patrick Pantel. 2009. Entity 
Extraction via Ensemble Semantics. In Proceedings 
of EMNLP 2009. 
Benjamin Rosenfeld and Ronen Feldman. 2007. Clus-
tering for Unsupervised Relation Identification. In 
Proceedings of CIKM 2007. 
Luis Sarmento, Valentin Jijkoun, Maarten de Rijke and 
Eugenio Oliveira. 2007. ?More like these?: growing 
entity classes from seeds. In Proceedings of CIKM 
2007. 
Satoshi Sekine. 2005. Automatic paraphrase discovery 
based on context and keywords between NE pairs. In 
Proceedings of the International Workshop on Para-
phrasing, 2005. 
Shuming Shi, Huibin Zhang, Xiaojie Yuan, Ji-Rong 
Wen. 2010. Corpus-based Semantic Class Mining: 
Distributional vs. Pattern-Based Approaches. In Pro-
ceedings of COLING 2010. 
Yusuke Shinyama, Satoshi Sekine. 2006. Preemptive 
Information Extraction using Unrestricted Relation 
Discovery, In Proceedings of NAACL 2006. 
Rion Snow, Daniel Jurafsky, and Andrew Y. Ng. 2005. 
Learning Syntactic Patterns for Automatic Hypernym 
Discovery. In Proceedings of  In NIPS 17, 2005. 
Stephen Soderland and Bhushan Mandhani. 2007. Mov-
ing from Textual Relations to Ontologized Relations. 
In Proceedings of the 2007 AAAI Spring Symposium 
on Machine Reading. 
Partha Pratim Talukdar, Joseph Reisinger, Marius Pasca, 
Deepak Ravichandran, Rahul Bhagat and Fernando 
Pereira. 2008. Weakly-Supervised Acquisition of La-
beled Class Instances using Graph Random Walks. In 
Proceedings of EMNLP 2008. 
David Vickrey, Oscar Kipersztok and Daphne Koller. 
2010. An Active Learning Approach to Finding Re-
lated Terms. In Proceedings of ACL 2010. 
Vishnu Vyas and Patrick Pantel. 2009. SemiAutomatic 
Entity Set Refinement. In Proceedings of 
NAACL/HLT 2009. 
Vishnu Vyas, Patrick Pantel and Eric Crestan. 2009, 
Helping Editors Choose Better Seed Sets for Entity 
Set Expansion, In Proceedings of CIKM 2009. 
Richard C. Wang and William W. Cohen. 2007. Lan-
guage- Independent Set Expansion of Named Entities 
Using the Web. In Proceedings of ICDM 2007. 
Richard C. Wang and William W. Cohen. 
2009. Automatic Set Instance Extraction using the 
Web. In Proceedings of ACL-IJCNLP 2009. 
Wei Wang, Romaric Besan?on and Olivier Ferret. 2011. 
Filtering and Clustering Relations for Unsupervised 
Information Extraction in Open Domain. In Proceed-
ings of CIKM 2011. 
Fei Wu and Daniel S. Weld. 2010. Open information 
extraction using Wikipedia. In Proceedings of ACL 
2010. 
Hua Wu and Ming Zhou. 2003. Synonymous colloca-
tion extraction using translation information. In Pro-
ceedings of the ACL Workshop on Multiword 
Expressions: Integrating Processing 2003. 
Limin Yao, Aria Haghighi, Sebastian Riedel, Andrew 
McCallum. 2011. Structured Relation Discovery Us-
ing Generative Models. In Proceedings of EMNLP 
2011.  
Alexander Yates and Oren Etzioni. 2007. Unsupervised 
Resolution of Objects and Relations on the Web.  In 
Proceedings of HLT-NAACL 2007.  
Fan Zhang, Shuming Shi, Jing Liu, Shuqi Sun, Chin-
Yew Lin. 2011. Nonlinear Evidence Fusion and 
Propagation for Hyponymy Relation Mining. In Pro-
ceedings of ACL 2011. 
Huibin Zhang, Mingjie Zhu, Shuming Shi, and Ji-Rong 
Wen. 2009. Employing Topic Models for Pattern-
based Semantic Class Discovery. In Proceedings of 
ACL 2009. 
1037
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 194?203,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
Compensating for Annotation Errors in Training a Relation Extractor 
Bonan Min Ralph Grishman 
New York University New York University 
715 Broadway, 7th floor 715 Broadway, 7th floor 
New York, NY 10003 USA New York, NY 10003 USA 
min@cs.nyu.edu grishman@cs.nyu.edu 
  
Abstract 
The well-studied supervised Relation 
Extraction algorithms require training 
data that is accurate and has good 
coverage. To obtain such a gold standard, 
the common practice is to do independent 
double annotation followed by 
adjudication. This takes significantly 
more human effort than annotation done 
by a single annotator. We do a detailed 
analysis on a snapshot of the ACE 2005 
annotation files to understand the 
differences between single-pass 
annotation and the more expensive nearly 
three-pass process, and then propose an 
algorithm that learns from the much 
cheaper single-pass annotation and 
achieves a performance on a par with the 
extractor trained on multi-pass annotated 
data. Furthermore, we show that given 
the same amount of human labor, the 
better way to do relation annotation is not 
to annotate with high-cost quality 
assurance, but to annotate more.  
1. Introduction 
Relation Extraction aims at detecting and 
categorizing semantic relations between pairs of 
entities in text. It is an important NLP task that 
has many practical applications such as 
answering factoid questions, building knowledge 
bases and improving web search.  
    Supervised methods for relation extraction 
have been studied extensively since rich 
annotated linguistic resources, e.g. the Automatic 
Content Extraction1 (ACE) training corpus, were 
released. We will give a summary of related 
methods in section 2. Those methods rely on 
accurate and complete annotation. To obtain high 
quality annotation, the common wisdom is to let 
                                                 
1 http://www.itl.nist.gov/iad/mig/tests/ace/ 
two annotators independently annotate a corpus, 
and then asking a senior annotator to adjudicate 
the disagreements 2 . This annotation procedure 
roughly requires 3 passes3 over the same corpus. 
Therefore it is very expensive. The ACE 2005 
annotation on relations is conducted in this way. 
    In this paper, we analyzed a snapshot of ACE 
training data and found that each annotator 
missed a significant fraction of relation mentions 
and annotated some spurious ones. We found 
that it is possible to separate most missing 
examples from the vast majority of true-negative 
unlabeled examples, and in contrast, most of the 
relation mentions that are adjudicated as 
incorrect contain useful expressions for learning 
a relation extractor. Based on this observation, 
we propose an algorithm that purifies negative 
examples and applies transductive inference to 
utilize missing examples during the training 
process on the single-pass annotation. Results 
show that the extractor trained on single-pass 
annotation with the proposed algorithm has a 
performance that is close to an extractor trained 
on the 3-pass annotation. We further show that 
the proposed algorithm trained on a single-pass 
annotation on the complete set of documents has 
a higher performance than an extractor trained on 
3-pass annotation on 90% of the documents in 
the same corpus, although the effort of doing a 
single-pass annotation over the entire set costs 
less than half that of doing 3 passes over 90% of 
the documents. From the perspective of learning 
a high-performance relation extractor, it suggests 
that a better way to do relation annotation is not 
to annotate with a high-cost quality assurance, 
but to annotate more. 
                                                 
2 The senior annotator also found some missing examples as 
shown in figure 1. 
3 In this paper, we will assume that the adjudication pass has 
a similar cost compared to each of the two first-passes. The 
adjudicator may not have to look at as many sentences as an 
annotator, but he is required to review all instances found by 
both annotators. Moreover, he has to be more skilled and 
may have to spend more time on each instance to be able to 
resolve disagreements.  
194
2. Background 
2.1 Supervised Relation Extraction 
One of the most studied relation extraction tasks 
is the ACE relation extraction evaluation 
sponsored by the U.S. government. ACE 2005 
defined 7 major entity types, such as PER 
(Person), LOC (Location), ORG (Organization). 
A relation in ACE is defined as an ordered pair 
of entities appearing in the same sentence which 
expresses one of the predefined relations. ACE 
2005 defines 7 major relation types and more 
than 20 subtypes. Following previous work, we 
ignore sub-types in this paper and only evaluate 
on types when reporting relation classification 
performance. Types include General-affiliation 
(GEN-AFF), Part-whole (PART-WHOLE), 
Person-social (PER-SOC), etc. ACE provides a 
large corpus which is manually annotated with 
entities (with coreference chains between entity 
mentions annotated), relations, events and 
values. Each mention of a relation is tagged with 
a pair of entity mentions appearing in the same 
sentence as its arguments. More details about the 
ACE evaluation are on the ACE official website. 
    Given a sentence s and two entity mentions 
arg1 and arg2 contained in s, a candidate relation 
mention r with argument arg1 preceding arg2 is 
defined as r=(s, arg1, arg2). The goal of Relation 
Detection and Classification (RDC) is to 
determine whether r expresses one of the types 
defined. If so, classify it into one of the types. 
Supervised learning treats RDC as a 
classification problem and solves it with 
supervised Machine Learning algorithms such as 
MaxEnt and SVM. There are two commonly 
used learning strategies (Sun et al 2011). Given 
an annotated corpus, one could apply a flat 
learning strategy, which trains a single multi-
class classifier on training examples labeled as 
one of the relation types or not-a-relation, and 
apply it to determine its type or output not-a 
relation for each candidate relation mention 
during testing. The examples of each type are the 
relation mentions that are tagged as instances of 
that type, and the not-a-relation examples are 
constructed from pairs of entities that appear in 
the same sentence but are not tagged as any of 
the types. Alternatively, one could apply a 
hierarchical learning strategy, which trains two 
classifiers, a binary classifier RD for relation 
detection and the other a multi-class classifier RC 
for relation classification. RD is trained by 
grouping tagged relation mentions of all types as 
positive instances and using all the not-a-relation 
cases (same as described above) as negative 
examples. RC is trained on the annotated 
examples with their tagged types. During testing, 
RD is applied first to identify whether an 
example expresses some relation, then RC is 
applied to determine the most likely type only if 
it is detected as correct by RD. 
    State-of-the-art supervised methods for 
relation extraction also differ from each other on 
data representation. Given a relation mention, 
feature-based methods (Miller et al 2000;  
Kambhatla, 2004; Boschee et al 2005; 
Grishman et al 2005; Zhou et al 2005; Jiang 
and Zhai, 2007; Sun et al 2011) extract a rich 
list of structural, lexical, syntactic and semantic 
features to represent it; in contrast, the kernel 
based methods (Zelenko et al 2003; Bunescu 
and Mooney, 2005a; Bunescu and Mooney, 
2005b; Zhao and Grishman, 2005; Zhang et al 
2006a; Zhang et al 2006b; Zhou et al 2007; 
Qian et al 2008) represent each instance with an 
object such as augmented token sequences or a 
parse tree, and used a carefully designed kernel 
function, e.g. subsequence kernel (Bunescu and 
Mooney, 2005b) or convolution tree kernel 
(Collins and Duffy, 2001),  to calculate their 
similarity. These objects are usually augmented 
with features such as semantic features. 
In this paper, we use the hierarchical learning 
strategy since it simplifies the problem by letting 
us focus on relation detection only. The relation 
classification stage remains unchanged and we 
will show that it benefits from improved 
detection. For experiments on both relation 
detection and relation classification, we use 
SVM4 (Vapnik 1998) as the learning algorithm 
since it can be extended to support transductive 
inference as discussed in section 4.3. However, 
for the analysis in section 3.2 and the purification 
preprocess steps in section 4.2, we use a 
MaxEnt5 model since it outputs probabilities6 for 
its predictions.  For the choice of features, we use 
the full set of features from Zhou et al(2005) 
since it is reported to have a state-of-the-art 
performance (Sun et al 2011).  
2.2 ACE 2005 annotation 
The ACE 2005 training data contains 599 articles 
                                                 
4 SVM-Light is used. http://svmlight.joachims.org/ 
5 OpenNLP MaxEnt package is used. 
http://maxent.sourceforge.net/about.html 
6 SVM also outputs a value associated with each prediction. 
However, this value cannot be interpreted as probability.  
195
from newswire, broadcast news, weblogs, usenet 
newsgroups/discussion forum, conversational 
telephone speech and broadcast conversations. 
The annotation process is conducted as follows: 
two annotators working independently annotate 
each article and complete all annotation tasks 
(entities, values, relations and events). After two 
annotators both finished annotating a file, all 
discrepancies are then adjudicated by a senior 
annotator. This results in a high-quality 
annotation file. More details can be found in the 
documentation of ACE 2005 Multilingual 
Training Data V3.0. 
    Since the final release of the ACE training 
corpus only contains the final adjudicated 
annotations, in which all the traces of the two 
first-pass annotations are removed, we use a 
snapshot of almost-finished annotation, ACE 
2005 Multilingual Training Data V3.0, for our 
analysis. In the remainder of this paper, we will 
call the two independent first-passes of 
annotation fp1 and fp2. The higher-quality data 
done by merging fp1 and fp2 and then having 
disagreements adjudicated by the senior 
annotator is called adj. From this corpus, we 
removed the files that have not been completed 
for all three passes. On the final corpus 
consisting of 511 files, we can differentiate the 
annotations on which the three annotators have 
agreed and disagreed.  
    A notable fact of ACE relation annotation is 
that it is done with arguments from the list of 
annotated entity mentions. For example, in a 
relation mention tyco's ceo and president dennis 
kozlowski which expresses an EMP-ORG 
relation, the two arguments tyco and dennis 
kozlowski must have been tagged as entity 
mentions previously by the annotator. Since fp1 
and fp2 are done on all tasks independently, their 
disagreement on entity annotation will be 
propagated to relation annotation; thus we need 
to deal with these cases specifically.  
3. Analysis of data annotation 
3.1 General statistics 
As discussed in section 2, relation mentions are 
annotated with entity mentions as arguments, and 
the lists of annotated entity mentions vary in fp1, 
fp2 and adj. To estimate the impact propagated 
from entity annotation, we first calculate the ratio 
of overlapping entity mentions between entities 
annotated in fp1/fp2 with adj. We found that 
fp1/fp2 each agrees with adj on around 89% of 
the entity mentions. Following up, we checked 
the relation mentions7 from fp1 and fp2 against 
the adjudicated list of entity mentions from adj 
and found that 682 and 665 relation mentions 
respectively have at least one argument which 
doesn?t appear in the list of adjudicated entity 
mentions. 
    Given the list of relation mentions with both 
arguments appearing in the list of adjudicated 
entity mentions, figure 1 shows the inter-
annotator agreement of the ACE 2005 relation 
annotation. In this figure, the three circles 
represent the list of relation mentions in fp1, fp2 
and adj, respectively. 
3065
1486 1525
645 538
47
383
fp1 fp2
adj  
Figure 1. Inter-annotator agreement of ACE 2005 relation 
annotation. Numbers are the distinct relation mentions 
whose both arguments are in the list of adjudicated entity 
mentions. 
 
    It shows that each annotator missed a 
significant number of relation mentions 
annotated by the other. Considering that we 
removed 682/665 relation mentions from fp1/fp2 
because we generate this figure based on the list 
of adjudicated entity mentions, we estimate that 
fp1 and fp2 both missed around 18.3-28.5%8 of 
the relation mentions. This clearly shows that 
both of the annotators missed a significant 
fraction of the relation mentions. They also 
annotated some spurious relation mentions (as 
adjudicated in adj), although the fraction is 
smaller (close to 10% of all relation mentions in 
adj). 
    ACE 2005 relation annotation guidelines 
(ACE English Annotation Guidelines for 
Relations, version 5.8.3) defined 7 syntactic 
classes and the other class. We plot the 
distribution of syntactic classes of the annotated 
                                                 
7 This is done by selecting the relation mentions whose both 
arguments are in the list of adjudicated entity mentions. 
8 We calculate the lower bound by assuming that the 682 
relation mentions removed from fp1 are found in fp2, 
although with different argument boundary and headword 
tagged. The upper bound is calculated by assuming that they 
are all irrelevant and erroneous relation mentions. 
196
relations in figure 2 (3 of the classes, accounting 
together for less than 10% of the cases, are 
omitted) and the other class. It seems that it is 
generally easier for the annotators to find and 
agree on relation mentions of the type 
Preposition/PreMod/Possessives but harder to 
find and agree on the ones belonging to Verbal 
and Other. The definition and examples of these 
syntactic classes can be found in the annotation 
guidelines.  
    In the following sections, we will show the 
analysis on fp1 and adj since the result is similar 
for fp2. 
 
Figure 2. Percentage of examples of major syntactic classes. 
3.2 Why the differences? 
To understand what causes the missing 
annotations and the spurious ones, we need 
methods to find how similar/different the false 
positives are to true positives and also how 
similar/different the false negatives (missing 
annotations) are to true negatives. If we adopt a 
good similarity metric, which captures the 
structural, lexical and semantic similarity 
between relation mentions, this analysis will help 
us to understand the similarity/difference from an 
extraction perspective. 
    We use a state-of-the-art feature space (Zhou 
et al 2005) to represent examples (including all 
correct examples, erroneous ones and untagged 
examples) and use MaxEnt as the weight 
learning model since it shows competitive 
performance in relation extraction (Jiang and 
Zhai, 2007) and outputs probabilities associated 
with each prediction. We train a MaxEnt model 
for relation detection on true positives and true 
negatives, which respectively are the subset of 
correct examples annotated by fp1 (and 
adjudicated as correct ones) and negative 
examples that are not annotated in adj, and use it 
to make predictions on the mixed pool of correct 
examples, missing examples and spurious ones. 
To illustrate how distinguishable the missing 
examples (false negatives) are from the true 
negative ones, 1) we apply the MaxEnt model on 
both false negatives and true negatives, 2) put 
them together and rank them by the model-
predicted probabilities of being positive, 3) 
calculate their relative rank in this pool. We plot 
the Cumulative distribution of frequency (CDF) 
of the ranks (as percentages in the mixed pools) 
of false negatives in figure 3. We took similar 
steps for the spurious ones (false positives) and 
plot them in figure 3 as well (However, they are 
ranked by model-predicted probabilities of being 
negative). 
 
 
Figure 3: cumulative distribution of frequency (CDF) of the 
relative ranking of model-predicted probability of being 
positive for false negatives in a pool mixed of false 
negatives and true negatives; and the CDF of the relative 
ranking of model-predicted probability of being negative for 
false positives in a pool mixed of false positives and true 
positives. 
 
    For false negatives, it shows a highly skewed 
distribution in which around 75% of the false 
negatives are ranked within the top 10%. That 
means the missing examples are lexically, 
structurally or semantically similar to correct 
examples, and are distinguishable from the true 
negative examples. However, the distribution of 
false positives (spurious examples) is close to 
uniform (flat curve), which means they are 
generally indistinguishable from the correct 
examples. 
3.3 Categorize annotation errors 
The automatic method shows that the errors 
(spurious annotations) are very similar to the 
correct examples but provides little clue as to 
why that is the case. To understand their causes, 
we sampled 65 examples from fp1 (10% of the 
645 errors), read the sentences containing these 
197
Category Percentage 
Example 
Relation 
Type 
Sampled text of spurious examples in fp1 
Notes (examples are similar 
ones in adj for comparison) 
Duplicate 
relation 
mention for 
coreferential 
entity mentions 
49.2% ORG-AFF 
? his budding friendship with US      President 
George W. Bush in the face of ? 
? his budding friendship 
with US      President George 
W. Bush in the face of ? 
Correct 20% 
PHYS 
Hundreds of thousands of demonstrators took to 
the streets in Britain? 
 
PER-SOC 
The dead included the quack doctor, 55-year-old 
Nityalila Naotia, his teenaged son and? 
(Symmetric relation)  
The dead included the quack 
doctor, 55-year-old Nityalila 
Naotia, his teenaged son 
Argument not 
in list 
15.4% 
 
PER-SOC 
Putin had even secretly invited British Prime 
Minister Tony Blair, Bush's staunchest backer      
in the war on Iraq? 
 
Violate 
reasonable 
reader rule 
6.2% PHYS 
"The      amazing thing is they are going to turn 
San Francisco into ground zero for every criminal 
who wants to profit at their chosen profession", 
Paredes said. 
 
Errors 6.1% 
PART-
WHOLE 
?a likely candidate to run Vivendi Universal's 
entertainment unit in the United States? 
Arguments are tagged 
reversed 
PART-
WHOLE 
 
Khakamada argued that the United 
States would also need Russia's help "to make the 
new Iraqi government seem legitimate. 
Relation type error 
illegal 
promotion 
through 
?blocked? 
categories 
3% 
PHYS 
 
Up to 20,000 protesters thronged the plazas and 
streets of San Francisco, where? 
Up to 20,000 protesters 
thronged the plazas and 
streets of San Francisco, 
where? 
Table 1. Categories of spurious relation mentions in fp1 (on a sample of 10% of relation mentions), ranked by the percentage 
of the examples in each category. In the sample text, red text (also marked with dotted underlines) shows head words of the 
first arguments and the underlined text shows head words of the second arguments. 
 
erroneous relation mentions and compared them 
to the correct relation mentions in the same 
sentence; we categorized these examples and 
show them in table 1. The most common type of 
error is duplicate relation mention for 
coreferential entity mentions. The first row in 
table 1 shows an example, in which there is a 
relation ORG-AFF tagged between US and 
George W. Bush in adj. Because President and 
George W. Bush are coreferential, the example 
<US, President > from fp1 is adjudicated as 
incorrect. This shows that if a relation is 
expressed repeatedly across relation mentions 
whose arguments are coreferential, the 
adjudicator only tags one of the relation mentions 
as correct, although the other is correct too. This 
shared the same principle with another type of 
error illegal promotion through ?blocked? 
categories 9  as defined in the annotation 
guideline. The second largest category is correct, 
by which we mean the example is a correct 
relation mention and the adjudicator made a 
                                                 
9 For example, in sentence Smith went to a hotel in Brazil, 
(Smith, hotel) is a taggable PHYS Relation but (Smith, 
Brazil) is not, because to get the second relationship, one 
would have to ?promote? Brazil through hotel. For the 
precise definition of annotation rules, please refer to ACE 
(Automatic Content Extraction) English Annotation 
Guidelines for Relations, version 5.8.3. 
mistake. The third largest category is argument 
not in list, by which we mean that at least one of 
the arguments is not in the list of adjudicated 
entity mentions. 
    Based on Table 1, we can see that as many as 
72%-88% of the examples which are adjudicated 
as incorrect are actually correct if viewed from a 
relation learning perspective, since most of them 
contain informative expressions for tagging 
relations.  The annotation guideline is designed 
to ensure high quality while not imposing too 
much burden on human annotators. To reduce 
annotation effort, it defined rules such as illegal 
promotion through ?blocked? categories. The 
annotators? practice suggests that they are 
following another rule not to annotate duplicate 
relation mention for coreferential entity 
mentions. This follows the similar principle of 
reducing annotation effort but is not explicitly 
stated in the guideline: to avoid propagation of a 
relation through a coreference chain. However, 
these examples are useful for learning more ways 
to express a relation. Moreover, even for the 
erroneous examples (as shown in table 1 as 
violate reasonable reader rule and errors), most 
of them have some level of similar structures or 
semantics to the targeted relation. Therefore, it is 
very hard to distinguish them without human 
proofreading. 
198
Exp # Training 
data 
Testing 
data 
Detection (%) Classification (%) 
Precision Recall F1 Precision Recall F1 
1 fp1 adj 83.4 60.4 70.0 75.7 54.8 63.6 
2 fp2 adj 83.5 60.5 70.2 76.0 55.1 63.9 
3 adj adj 80.4 69.7 74.6 73.4 63.6 68.2 
Table 2. Performance of RDC trained on fp1/fp2/adj, and tested on adj. 
 
3.4 Why missing annotations and how 
many examples are missing? 
For the large number of missing annotations, 
there are a couple of possible reasons. One 
reason is that it is generally easier for a human 
annotator to annotate correctly given a well-
defined guideline, but it is hard to ensure 
completeness, especially for a task like relation 
extraction.  Furthermore, the ACE 2005 
annotation guideline defines more than 20 
relation subtypes. These many subtypes make it 
hard for an annotator to keep all of them in mind 
while doing the annotation, and thus it is 
inevitable that some examples are missed. 
    Here we proceed to approximate the number 
of missing examples given limited knowledge. 
Let each annotator annotate n examples and 
assume that each pair of annotators agrees on a 
certain fraction p of the examples. Assuming the 
examples are equally likely to be found by an 
annotator, therefore the total number of unique 
examples found by ?  annotators is ? (1???=0
?)??. If we had an infinite number of annotators 
(? ? ?), the total number of unique examples 
will be 
?
?
, which is the upper bound of the total 
number of examples. In the case of the ACE 
2005 relation mention annotation, since the two 
annotators annotate around 4500 examples and 
they agree on 2/3 of them, the total number of all 
positive examples is around 6750. This is close 
to the number of relation mentions in the 
adjudicated list: 6459. Here we assume the 
adjudicator is doing a more complex task than an 
annotator, resolving the disagreements and 
completing the annotation (as shown in figure 1).  
    The assumption of the calculation is a little 
crude but reasonable given the limited number of 
passes of annotation we have. Recent research (Ji 
et al2010) shows that, by adding annotators for 
IE tasks, the merged annotation tends to 
converge after having 5 annotators. To 
understand the annotation behavior better, in 
particular whether annotation will converge after 
adding a few annotators, more passes of 
annotation need to be collected. We leave this as 
future work. 
 
4. Relation extraction with low-cost 
annotation 
4.1 Baseline algorithm 
To see whether a single-pass annotation is useful 
for relation detection and classification, we did 
5-fold cross validation (5-fold CV) with each of 
fp1, fp2 and adj as the training set, and tested on 
adj. The experiments are done with the same 511 
documents we used for the analysis. As shown in 
table 2, we did 5-fold CV on adj for experiment 
3. For fairness, we use settings similar to 5-fold 
CV for experiment 1 and 2. Take experiment 1 as 
an example: we split both of fp1 and adj into 5 
folds, use 4 folds from fp1 as training data, and 1 
fold from adj as testing data and does one train-
test cycle. We rotate the folds (both training and 
testing) and repeat 5 times. The final results are 
averaged over the 5 runs. Experiment 2 was 
conducted similarly. In the reminder of the paper, 
5-fold CV experiments are all conducted in this 
way. 
    Table 2 shows that a relation tagger trained on 
the single-pass annotated data fp1 performs 
worse than the one trained on merged and 
adjudicated data adj, with 4.6 points lower F 
measure in relation detection, and 4.6 points 
lower relation classification. For detection, 
precision on fp1 is 3 points higher than on adj 
but recall is much lower (close to 10 points). The 
recall difference shows that the missing 
annotations contain expressions that can help to 
find more correct examples during testing. The 
small precision difference indirectly shows that 
the spurious ones in fp1 (as adjudicated) do not 
hurt precision. Performance on classification 
shows a similar trend because the relation 
classifier takes the examples predicted by the 
detector as correct as its input. Therefore, if there 
is an error, it gets propagated to this stage. Table 
2 also shows similar performance differences 
between fp2 and adj.  
    In the remainder of this paper, we will discuss 
a few algorithms to improve a relation tagger 
trained on single-pass annotated data10. Since we 
                                                 
10 We only use fp1 and adj in the following experiments 
because we observed that fp1 and fp2 are similar in general 
in the analysis, though a fraction of the annotation in fp1 
199
already showed that most of the spurious 
annotations are not actually errors from an 
extraction perspective and table 2 shows that 
they do not hurt precision, we will only focus on 
utilizing the missing examples, in other words, 
training with an incomplete annotation. 
4.2 Purify the set of negative examples 
As discussed in section 2, traditional supervised 
methods find all pairs of entity mentions that 
appear within a sentence, and then use the pairs 
that are not annotated as relation mentions as the 
negative examples for the purpose of training a 
relation detector. It relies on the assumption that 
the annotators annotated all relation mentions 
and missed no (or very few) examples. However, 
this is not true for training on a single-pass 
annotation, in which a significant portion of 
relation mentions are left not annotated. If this 
scheme is applied, all of the correct pairs which 
the annotators missed belong to this ?negative? 
category. Therefore, we need a way to purify the 
?negative? set of examples obtained by this 
conventional approach. 
    Li and Liu (2003) focuses on classifying 
documents with only positive examples. Their 
algorithm initially sets all unlabeled data to be 
negative and trains a Rocchio classifier, selects 
negative examples which are closer to the 
negative centroid than positive centroid as the 
purified negative examples, and then retrains the 
model. Their algorithm performs well for text 
classification. It is based on the assumption that 
there are fewer unannotated positive examples 
than negative ones in the   unlabeled set, so true 
negative examples still dominate the set of noisy 
?negative? examples in the purification step. 
Based on the same assumption, our purification 
process consists of the following steps: 
1) Use annotated relation mentions as 
positive examples; construct all possible 
relation mentions that are not annotated, and 
initially set them to be negative. We call this 
noisy data set D. 
2) Train a MaxEnt relation detection model 
Mdet on D. 
3) Apply Mdet  on all unannotated 
examples, and rank them by the model-
predicted probabilities of being positive, 
4) Remove the top N examples from D. 
These preprocessing steps result in a purified 
data set  ?????. We can use ????? for the normal 
                                                                          
and fp2 is different. Moreover, algorithms trained on them 
show similar performance. 
training process of a supervised relation 
extraction algorithm. 
    The algorithm is similar to Li and Liu 2003. 
However, we drop a few noisy examples instead 
of choosing a small purified subset since we have 
relatively few false negatives compared to the 
entire set of unannotated examples. Moreover, 
after step 3, most false negatives are clustered 
within the small region of top ranked examples 
which has a high model-predicated probability of 
being positive. The intuition is similar to what 
we observed from figure 3 for false negatives 
since we also observed very similar distribution 
using the model trained with noisy data. 
Therefore, we can purify negatives by removing 
examples in this noisy subset.  
    However, the false negatives are still mixed 
with true negatives. For example, still slightly 
more than half of the top 2000 examples are true 
negatives. Thus we cannot simply flip their 
labels and use them as positive examples. In the 
following section, we will use them in the form 
of unlabeled examples to help train a better 
model. 
4.3 Transductive inference on unlabeled 
examples 
Transductive SVM (Vapnik, 1998; Joachims, 
1999) is a semi-supervised learning method 
which learns a model from a data set consisting 
of both labeled and unlabeled examples. 
Compared to its popular antecedent SVM, it also 
learns a maximum margin classification 
hyperplane, but additionally forces it to separate 
a set of unlabeled data with large margin. The 
optimization function of Transductive  SVM 
(TSVM) is the following: 
 
 
Figure 4. TSVM optimization function for non-separable 
case (Joachims, 1999) 
 
    TSVM can leverage an unlabeled set of 
examples to improve supervised learning. As 
shown in section 3, a significant number of 
relation mentions are missing from the single-
pass annotation data. Although it is not possible 
to find all missing annotations without human 
effort, we can improve the model by further 
200
utilizing the fact that some unannotated examples 
should have been annotated.  
    The purification process discussed in the 
previous section removes N examples which 
have a high density of false negatives. We further 
utilize the N examples as follows: 
1) Construct a training corpus ??????? from 
?????  by taking a random sample
11 of N*(1-
p)/p (p is the ratio of annotated examples to 
all examples; p=0.05 in fp1) negatively 
labeled examples in ????? and setting them to 
be unlabeled. In addition, the N examples 
removed by the purification process are added 
back as unlabeled examples.  
2) Train TSVM on ???????.  
    The second step trained a model which 
replaced the detection model in the hierarchical 
detection-classification learning scheme we used. 
We will show in the next section that this 
improves the model. 
 
5. Experiments 
 
Experiments were conducted over the same set of 
documents on which we did analysis: the 511 
documents which have completed annotation in 
all of the fp1, fp2 and adj from the ACE 2005 
Multilingual Training Data V3.0. To 
reemphasize, we apply the hierarchical learning 
scheme and we focus on improving relation 
detection while keeping relation classification 
unchanged (results show that its performance is 
improved because of the improved detection). 
We use SVM as our learning algorithm with the 
full feature set from Zhou et al(2005).  
    Baseline algorithm: The relation detector is 
unchanged. We follow the common practice, 
which is to use annotated examples as positive 
ones and all possible untagged relation mentions 
as negative ones. We sub-sampled the negative 
data by ? since that shows better performance. 
    +purify:  This algorithm adds an additional 
purification preprocessing step (section 4.2) 
before the hierarchical learning RDC algorithm. 
After purification, the RDC algorithm is trained 
on the positive examples and purified negative 
examples. We set N=200012 in all experiments. 
                                                 
11 We included this large random sample so that the balance 
of positive to negative examples in the unlabeled set would 
be similar to that of the labeled data. The test data is not 
included in the unlabeled set. 
12 We choose 2000 because it is close to the number of 
relations missed from each single-pass annotation. In 
practice, it contains more than 70% of the false negatives, 
and it is less than 10% of the unannotated examples. To 
estimate how many examples are missing (section 3.4), one 
    +tSVM: First, the same purification process of 
+purify is applied. Then we follow the steps 
described in section 4.3 to construct the set of 
unlabeled examples, and set althe rest of 
purified negative examples to be negative. 
Finally, we train TSVM on both labeled and 
unlabeled data and replace the relation detection 
in the RDC algorithm. The relation classification 
is unchanged. 
    Table 3 shows the results. All experiments are 
done with 5-fold cross validation13 using testing 
data from adj. The first three rows show 
experiments trained on fp1, and the last row 
(ADJ) shows the unmodified RDC algorithm 
trained on adj for comparison. The purification 
of negative examples shows significant 
performance gain, 3.7% F1 on relation detection 
and 3.4% on relation classification. The precision 
decreases but recall increases substantially since 
the missing examples are not treated as 
negatives. Experiment shows that the purification 
process removes more than 60% of the false 
negatives. Transductive SVM further improved 
performance by a relatively small margin. This 
shows that the latent positive examples can help 
refine the model. Results also show that 
transductive inference can find around 17% of 
missing relation mentions. We notice that the 
performance of relation classification is 
improved since by improving relation detection, 
some examples that do not express a relation are 
removed. The classification performance on 
single-pass annotation is close to the one trained 
on adj due to the help from a better relation 
detector trained with our algorithm.  
    We also did 5-fold cross validation with a 
model trained on a fraction of the 4/5 (4 folds) of 
adj data (each experiment shown in table 4 uses 
4 folds of adj documents for training since one 
fold is left for cross validation). The documents 
are sampled randomly. Table 4 shows results for 
varying training data size. Compared to the 
results shown in the ?+tSVM? row of table 3, we 
can see that our best model trained on single-pass 
annotation outperforms SVM trained on 90% of 
the dual-pass, adjudicated data in both relation 
detection and classification, although it costs less 
than half the 3-pass annotation. This suggests 
that given the same amount of human effort for 
                                                                          
should perform multiple passes of independent annotation 
on a small dataset and measure inter-annotator agreements. 
13 Details about the settings for 5-fold cross validation are in 
section 4.1. 
201
Algorithm 
Detection (%) Classification (%) 
Precision Recall F1 Precision Recall F1 
Baseline 83.4 60.4 70.0 75.7 54.8 63.6 
+purify 76.8 70.9 73.7 69.8 64.5 67.0 
+tSVM 76.4 72.1 74.2 69.4 65.2 67.2 
ADJ (on adj) 80.4 69.7 74.6 73.4 63.6 68.2 
Table 3. 5-fold cross-validation results. All are trained on fp1 (except the last row showing the unchanged algorithm trained 
on adj for comparison), and tested on adj. McNemar's test show that the improvement from +purify to +tSVM, and from 
+tSVM to ADJ are statistically significant (with p<0.05). 
 
Percentage of 
adj used 
Detection (%) Classification (%) 
Precision Recall F1 Precision Recall F1 
60% ? 4/5 86.9 41.2 55.8 78.6 37.2 50.5 
70% ? 4/5 85.5 51.3 64.1 77.7 46.6 58.2 
80% ? 4/5 83.3 58.1 68.4 75.8 52.9 62.3 
90% ? 4/5 82.0 64.9 72.5 74.9 59.4 66.2 
Table 4. Performance with SVM trained on a fraction of adj. It shows 5 fold cross validation results. 
 
relation annotation, annotating more documents 
with single-pass offers advantages over 
annotating less data with high quality assurance 
(dual passes and adjudication). 
6. Related work 
Dligach et al(2010) studied WSD annotation 
from a cost-effectiveness viewpoint. They 
showed empirically that, with same amount of 
annotation dollars spent, single-annotation is 
better than dual-annotation and adjudication. The 
common practice for quality control of WSD 
annotation is similar to Relation annotation. 
However, the task of WSD annotation is very 
different from relation annotation. WSD requires 
that every example must be assigned some tag, 
whereas that is not required for relation tagging. 
Moreover, relation tagging requires identifying 
two arguments and correctly categorizing their 
types.  
The purified approach applied in this paper is 
related to the general framework of learning from 
positive and unlabeled examples. Li and Liu 
(2003) initially set alunlabeled data to be 
negative and train a Rocchio classifier, then 
select negative examples which are closer to the 
negative centroid than positive centroid as the 
purified negative examples.  We share a similar 
assumption with Li and Liu (2003) but we use a 
different method to select negative examples 
since the false negative examples show a very 
skewed distribution, as described in section 5.2.  
Transductive SVM was introduced by Vapnik 
(1998) and later refined in Joachims (1999). A 
few related methods were studied on the subtask 
of relation classification (the second stage of the 
hierarchical learning scheme) in Zhang (2005).   
Chan and Roth (2011) observed the similar 
phenomenon that ACE annotators rarely 
duplicate a relation link for coreferential 
mentions. They use an evaluation scheme to 
avoid being penalized by the relation mentions 
which are not annotated because of this behavior. 
 
7. Conclusion 
 
We analyzed a snapshot of the ACE 2005 
relation annotation and found that each single-
pass annotation missed around 18-28% of 
relation mentions and contains around 10% 
spurious mentions. A detailed analysis showed 
that it is possible to find some of the false 
negatives, and that most spurious cases are 
actually correct examples from a system 
builder?s perspective. By automatically purifying 
negative examples and applying transductive 
inference on suspicious examples, we can train a 
relation classifier whose performance is 
comparable to a classifier trained on the dual-
annotated and adjudicated data. Furthermore, we 
show that single-pass annotation is more cost-
effective than annotation with high quality 
assurance. 
Acknowledgments 
Supported by the Intelligence Advanced 
Research Projects Activity (IARPA) via Air 
Force Research Laboratory (AFRL) contract 
number FA8650-10-C-7058. The U.S. 
Government is authorized to reproduce and 
distribute reprints for Governmental purposes 
notwithstanding any copyright annotation 
thereon. The views and conclusions contained 
herein are those of the authors and should not be 
interpreted as necessarily representing the 
official policies or endorsements, either 
expressed or implied, of IARPA, AFRL, or the 
U.S. Government. 
202
References 
ACE. http://www.itl.nist.gov/iad/mig/tests/ace/ 
ACE (Automatic Content Extraction) English 
Annotation Guidelines for Relations, version 5.8.3. 
2005.  http://projects.ldc.upenn.edu/ace/. 
ACE 2005 Multilingual Training Data V3.0. 2005. 
LDC2005E18. LDC Catalog. 
Elizabeth Boschee, Ralph Weischedel, and Alex 
Zamanian. 2005. Automatic information extraction. 
In Proceedings of the International Conference on 
Intelligence Analysis. 
Razvan C. Bunescu and Raymond J. Mooney. 2005a. 
A shortest path dependency kenrel for relation 
extraction. In Proceedings of HLT/EMNLP-2005. 
Razvan C. Bunescu and Raymond J. Mooney. 2005b. 
Subsequence kernels for relation extraction. In 
Proceedings of NIPS-2005. 
Yee Seng Chan and Dan Roth. 2011. Exploiting 
Syntactico-Semantic Structures for Relation 
Extraction. In Proceedings of ACL-2011. 
Michael Collins and Nigel Duffy. Convolution 
Kernels for Natural Language. In Proceedings of 
NIPS-2001. 
Dmitriy Dligach, Rodney D. Nielsen and Martha 
Palmer. 2010. To annotate more accurately or to 
annotate more. In Proceedings of Fourth Linguistic 
Annotation Workshop at ACL 2010 
Ralph Grishman, David Westbrook and Adam 
Meyers. 2005. NYU?s English ACE 2005 System 
Description. In Proceedings of ACE 2005 
Evaluation Workshop 
Scott Miller, Heidi Fox, Lance Ramshaw, and Ralph 
Weischedel. 2000. A novel use of statistical 
parsing to extract information from text In 
Proceedings of NAACL-2010. 
Heng Ji, Ralph Grishman, Hoa Trang Dang and Kira 
Griffitt. 2010. An Overview of the TAC2010 
Knowledge Base Population Track. In Proceedings 
of TAC-2010 
Jing  Jiang  and ChengXiang Zhai. 2007. A systematic 
exploration of the feature space for relation 
extraction. In Proceedings of HLT-NAACL-2007. 
Thorsten Joachims. 1999. Transductive Inference for 
Text Classification using Support Vector 
Machines. In Proceedings of ICML-1999. 
Nanda Kambhatla. 2004. Combining lexical, 
syntactic, and semantic features with maximum 
entropy models for information extraction. In 
Proceedings of ACL-2004 
Xiao-Li Li and Bing Liu. 2003. Learning to classify 
text using positive and unlabeled data. In 
Proceedings of IJCAI-2003. 
Longhua Qian,  Guodong Zhou,  Qiaoming Zhu and 
Peide Qian. 2008. Exploiting constituent 
dependencies for tree kernel-based semantic 
relation extraction . In Proc. of COLING-2008. 
Ang Sun, Ralph Grishman and Satoshi Sekine. 2011. 
Semi-supervised Relation Extraction with Large-
scale Word Clustering. In Proceedings of ACL-
2011. 
Vladimir N. Vapnik. 1998. Statistical Learning 
Theory. John Wiley. 
Dmitry Zelenko, Chinatsu Aone, and Anthony 
Richardella. 2003. Kernel methods for relation 
extraction. Journal of Machine Learning Research. 
Min Zhang, Jie Zhang and Jian Su. 2006a. Exploring 
syntactic features for relation extraction using a 
convolution tree kernel, In Proceedings of HLT-
NAACL-2006. 
Min Zhang, Jie Zhang, Jian Su, and GuoDong Zhou. 
2006b. A composite kernel to extract relations 
between entities with both flat and structured 
features. In Proceedings of COLING-ACL-2006. 
Zhu Zhang. 2005. Mining Inter-Entity Semantic 
Relations Using Improved Transductive Learning. 
In Proceedings of ICJNLP-2005. 
Shubin Zhao and Ralph Grishman, 2005. Extracting 
Relations with Integrated Information Using Kern 
el Methods. In Proceedings of ACL-2005. 
Guodong Zhou, Jian Su, Jie Zhang and Min Zhang. 
2005. Exploring various knowledge in relation 
extraction. In Proceedings of ACL-2005. 
Guodong Zhou, Min Zhang, DongHong Ji, and 
QiaoMing Zhu. 2007. Tree kernel-based relation 
extraction with context-sensitive structured parse 
tree information. In Proceedings of 
EMNLP/CoNLL-2007. 
203
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 285?288,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Utility Evaluation of Cross-document Information Extraction 

Heng Ji
a
, Zheng Chen
a
, Jonathan Feldman
a
, Antonio Gonzalez
a
, Ralph Grishman
b
, Vivek Upadhyay
a 
a
Computer Science Department, Queens College and the Graduate Center, City University of New York 
New York, NY 11367, USA 
b
Computer Science Department, New York University, New York, NY 10003, USA 
hengji@cs.qc.cuny.edu, zchen1@gc.cuny.edu, agonzalez117@qc.cuny.edu, grishman@cs.nyu.edu, 
vivekqc@gmail.com 
 
  
 
Abstract 
We describe a utility evaluation to determine 
whether cross-document information extrac-
tion (IE) techniques measurably improve user 
performance in news summary writing. Two 
groups of subjects were asked to perform the 
same time-restricted summary writing tasks, 
reading news under different conditions: with 
no IE results at all, with traditional single-
document IE results, and with cross-document 
IE results. Our results show that, in compari-
son to using source documents only, the qual-
ity of summary reports assembled using IE 
results, especially from cross-document IE, 
was significantly better and user satisfaction 
was higher. We also compare the impact of 
different user groups on the results.  
1 Introduction 
Information Extraction (IE) is a task of identifying 
??facts?? (entities, relations and events) within un-
structured documents, and converting them into 
structured representations (e.g., databases). IE 
techniques have been effectively applied to differ-
ent domains (e.g. daily news, Wikipedia, biomedi-
cal reports, financial analysis and legal 
documentations) and different languages. Recently 
we described a new cross-document IE task (Ji et 
al., 2009) to extract events across-documents and 
track them on a time line.  Compared to traditional 
single-document IE, this new task can extract more 
salient, accurate and concise event information. 
However, a significant question remains: will 
the events extracted by IE, especially this new 
cross-document IE task, actually help end-users to 
make better use of the large volumes of news? In 
order to investigate whether we have reached this 
goal, we performed an extrinsic utility (i.e., use-
fulness) and usability evaluation on IE results. 
Two groups of subjects were asked to perform the 
same time-restricted summary writing tasks, read-
ing news under different conditions: with no IE 
results at all, with traditional single-document IE 
results, and with cross-document IE results. Our 
results show that, in comparison to using source 
documents only, the quality of summary reports 
assembled using IE techniques, especially from 
cross-document IE, was significantly better. Also, 
as extraction quality increases from no IE at all to 
single-document IE and then to cross-document IE, 
user satisfaction increases. We also compare the 
impact of different user groups on the results. To 
the best of our knowledge, this is the first system-
atic evaluation of cross-document IE from a us-
ability perspective.  
2 Overview of IE Systems 
We applied the English single-document IE system 
(Ji and Grishman, 2008) and cross-document IE 
system presented in (Ji et al, 2009). Both systems 
were developed for the ACE program1.  
The single-document IE system can extract 
events from individual documents. The core stages 
include entity extraction, time expression extrac-
tion and normalization, relation extraction and 
event extraction. Events include the 33 distinct 
types defined in ACE05. The extraction results are 
presented in tabular form.  
The cross-document IE system can identify im-
portant person entities which are frequently in-
                                                          
1 http://www.itl.nist.gov/iad/mig/tests/ace/2005/ 
285
volved in events as ??centroid entities??; and then for 
each centroid entity, link and order the events cen-
tered around it on a time line and associate them to 
a geographical map. The event chains are pre-
sented in a user-friendly graphical interface (Ji and 
Chen, 2009). Both systems link the events back to 
their context documents.  
3 Evaluation Methods 
3.1 Study Execution 
Our measurement challenge is to assess how IE 
techniques affect users?? abilities to perform real-
world tasks. We followed the summary writing 
task described in the Integrated Feasibility Ex-
periment of the DARPA TIDES program (Colbath 
and Kubala, 2003) and the daily task conducted by 
intelligence analysts (Bodnar, 2003). Each task in 
our evaluation is based on writing a summary of 
ACE-type events involving a specific centroid en-
tity, using one of three levels of support: 
? Level (I): Read the news articles, with assistance 
of keyword based sentence search; 
? Level (II): (I) + with assistance from single-
document IE results; 
? Level (III): (I) + with assistance from cross-
document IE results. 
The summary writing task for each entity using 
any level should be finished in 10 minutes. The 
users can choose to trust the IE results to create 
new sentences or select relevant sentences from 
the source documents. The IE systems were ap-
plied to a corpus of 106 articles from ACE 2005 
training data. 
3.2 Summary Scoring 
We measure user responses in three aspects:  
? Observer-based Quantity -- How many sen-
tences are extracted in each summary? How 
many of them are uniquely correct? 
? Observer-based Quality-- How fluent and coher-
ent are the sentences in each summary?  
? User-based Usability -- How does the user feel 
about the system?  
3.3 User Group Selection 
We selected user groups based on the principles 
that we should run as many tests as we can afford 
(Nielsen, 1994), and at least 5 to insure that we 
detect any major usability problems (Faulkner, 
2003). Two different groups of users were asked to 
conduct the evaluation: 
(1) Hallway Evaluation 
We chose the first group of users with a ??Hallway 
Testing?? user-study method described in (Nielsen, 
1994). We randomly asked 11 PhD students in the 
field of natural language processing to conduct the 
evaluation. In order to evaluate these three levels 
independently, each student was asked to write at 
most one summary, using one of the three levels, 
for any single centroid entity. To avoid the impact 
of diverse text comprehension abilities, each stu-
dent was involved in all of these three levels for 
different centroid entities. 
(2) Remote Evaluation 
An effective utility evaluation will require users 
with a diversity of prior knowledge and computer 
experience. Therefore we asked the second group 
of 11 users in a remote usability testing mode 
(Hammontree et al, 1994). We sent out the request 
to university-wide undergraduate student mailing 
lists and found 11 users to work on the evaluation. 
The evaluation procedure follows the Hallway 
Testing method, except that the tests are carried 
out in the user??s own environment (rather than labs) 
helping further simulate real-life scenario testing. 
Also the users didn??t meet with the observers and 
thus they were not aware of any expectations for 
results. 
4 Evaluation Results 
In this section we will focus on reporting the re-
sults from Hallway Evaluation, while providing 
comparisons with Remote Evaluation. 
4.1 Observer-based Quantity 
The summaries were judged by two annotators and 
the judgements reconciled. A summary sentence is 
judged as uniquely correct if it: (1) includes rele-
vant events involving the centroid entity; and (2) 
the same information was not included in previous 
sentences in the current summary. This metric can 
be considered as an approximate com bination of 
the ??content responsiveness??, ??non-
redundancy??and ??focus?? criteria in the NIST TAC 
summarization track2.  Table 1 presents the  
                                                          
2http://www.nist.gov/tac/2009/Summarization/update.su
mm.09.guidelines.html 
286
Cen-
troid 
(I) (II) (III) Cen-
troid 
(I) (II) (III) Cen-
troid 
(I) (II) (III) 
Bush 3/1/0 5/1/2 6/0/0 Al-douri 4/3/3 4/2/0 6/0/1 Ba??asyir 3/1/0 3/0/0 5/0/0
Ibrahim 4/0/1 5/0/0 8/0/0 Giuliani 2/0/0 3/2/0 5/0/0 Erdogan 1/0/1 4/0/0 4/0/0
Toefting 0/0/0 7/1/0 4/0/0 Blair 2/0/1 3/0/0 5/0/0 Diller 3/0/0 4/1/0 3/0/0
Putin 2/1/0 4/3/2 7/1/1 Pasko 3/0/0 3/0/0 2/0/0 Overall 27/6/6 45/10/5 55/1/2
 
Table 1. # (uniquely correct sentences)/ #(redundant correct sentences)/ 
#(spurious sentences) in a summary in Hallway Evaluation 
 
quantified Hallway Testing results for each cen-
troid separately and the overall score. It shows that 
overall Level (II) contained 18 more correct sen-
tences than the baseline (I), while (III) achieved 11 
further correct sentences. (I) obtained significantly 
fewer sentences without assistance from IE tools. 
We conducted the Wilcoxon Matched-Pairs 
Signed-Ranks Test on a query entity basis for ac-
curacy - number of (uniquely correct sen-
tences)/number of (total extracted sentences in a 
summary). The results show that (III) is signifi-
cantly better than (I) at a 99.2% confidence level, 
and better than (II) at a 96.9% confidence level. (II) 
is not significantly better than (I). 
We can also see that for some centroid entities 
such as ??Putin??, ??Al-douri?? and ??Giuliani??, (II) 
generated more sentences but also introduced more 
redundant information. The user feedback has in-
dicated that they did not have enough time to re-
move redundancy. In contrast, (III) yielded much 
less redundant information. In fact, the average 
time the users spent using (III) was only about 7.2 
minutes. Therefore we can conclude that cross-
document IE can produce more informative sum-
maries in a more efficient way. 
Error analysis showed that the major error types 
propagated from IE to summaries are as follows. 
1. Event time errors. For example, the summary 
sentence ??Toefting was convicted in September 
2001 of assaulting a pair of restaurant workers in 
the capital?? was judged as incorrect because the 
time argument should be ??October 2002??. 
2. Pronoun resolution errors. When a pronoun is 
mistakenly linked to an entity, incorrect event ar-
guments will be included in the summaries.  
3. Event type errors. When an event is mis-
classified, the users tend to use incorrect templates 
and thus generate wrong summaries. 
4. Negative events. Sometimes the event attrib-
ute classifier makes mistakes and the users include 
negative events in the summaries. 
4.2 Impact of User Groups 
In the Remote Testing, the accuracy results from 
the three levels are as follows: 21/37, 28/37 and 
31/36. Thus both user groups benefited from using 
IE techniques, but the enhancements vary a lot. In 
the Hallway Testing, the users were better trained 
and more familiar with IE tools (including the 
graphical interface of cross-document IE); and thus 
they can benefit more from the IE techniques. In 
contrast, in the Remote Evaluation, the users had 
quite diverse knowledge backgrounds. For exam-
ple, one remote user was only able to find 1-2 sen-
tences using any of the three levels; while another, 
more skilled remote user found more than 5 sen-
tences with any level. However the Remote 
Evaluation is important to gather the feedback of 
the more subjective usability evaluation in section 
4.4. Because the users in Hallway Testing may be 
aware of the observations that the observer is hop-
ing to achieve, they may provide potentially biased 
feedback. 
4.3 Observer-based Quality 
The evaluation also showed that (III) produced 
summaries with better quality. We asked the ob-
servers to give a score between [1, 10] to each 
summary according to the following TAC summa-
rization quality criteria: Readability/Fluency, Ref-
erential Clarity and Structure/Coherence. Table 2 
shows the evaluation results for the three different 
methods. 
 
Criteria (I) (II) (III) 
Readability/Fluency 9.4 8.5 8.2 
Referential Clarity 6.1 8.3 8.7 
Structure/Coherence 7.1 7.6 8.5 
 
Table 2. Observer-based Average Quality 
 
In their detailed feedback, the users indicated 
that (III) has the following advantages: (1) Better 
287
Cen-nt-r eoindt (n-Ir )Bur sneor hn3Cdo or /-1r /hht0
e/ or  o3Cne/dr ne1oer 5oh/tior )222ur6/-r eohnAoer t-0
l-n4-r  (3or /eat3o- ir ti(-ar henii01nht3o- r
(-??oeo-hoyr)bur6/-rao-oe/ or/5i e/h (Aorit33/e(oiyr
mner  8or 5(nae/C8(h/dr oAo- ir )oyayr o3CdnG3o- uEr
in3or tioeir 4oeor /5dor  nr tior iCoh(??(hr  o3Cd/ oir
ith8r/irgTf7r4/ir8(eo1r5GrD7Pr/ rk2sf.r nr4e( or
it33/e(oiyr mner o#/3CdoEr /r io- o-hor gqti8r /-1r
qd/(er3o r/ r6/3Crc/A(1r/-1r 8orpHr 8eoor (3oir(-r
s/eh8r Bwwb.r 4/ir 1oe(Ao1r ??en3r  8eoor 1(????oeo- r
g6n- /h 0soo (-a.r oAo- ir (-r  8oroAo- r h8/(-iyr )vur
6/-r hn--oh r eod/ o1r oAo- ir (- nr 3neor hn-h(ior
it33/e(oiyrmnero#/3CdoErioAoe/droAo- ir4oeorhn-0
-oh o1r nrao-oe/ or  8or??nddn4(-ario- o-hoirgT/ilnr
4/ir /CCo/do1r??ner  eo/in-rhe(3orn-rLCe(dr ,WErBwwbr
and thenreodo/io1rn-rxt-or,MErBwwb.yrk8oreo/1/5(d0
( Gr ihneoir (-rk/5dorBr /dinr (-1(h/ or  8/ r /r3neoro??0
??oh (Aor  o3Cd/ or ao-oe/ (n-r 3o 8n1r i8ntd1r 5or
1oAodnCo1r nrCen1thor3neor??dto- rit33/e(oir5/io1r
n-r2freoitd iyr
4.4 User-based Usability 
k8or tioer ??oo15/hlr ??en3r 5n 8r oA/dt/ (n-ir /dinr
i8n4o1r  8/ r )22ur /-1r )222ur eoitd ir 4oeor  eti o1r /d0
3ni roSt/ddGEr/-1r)222ur4/irhd/(3o1r nrCenA(1or 8or
3ni r tio??tdr ??t-h (n-iyr k8or Cni( (Aor hn33o- ir
/5nt r)222ur(-hdt1orgko3Cne/drR(-l(-ar/ddn4irdna(0
h/dreo/in-(-ar/-1rao-oe/d(9/ (n-.Erg6o- en(1rio/eh8r
8odCir  nr ??nhtir (33o1(/ odG.Er g%C/ (/drR(-l(-ar /d0
dn4ir  nr 5en4ior /ddr  8orCd/hoir 48(h8r /rCoein-r 8/ir
A(i( o1.Er g??/3or 1(i/35(at/ (n-r 8odCir  nr ??(d oer (e0
eodoA/- r (-??ne3/ (n-.Er g6/-r ??(-1r loGr (-??ne3/ (n-r
??en3r oAo- r h8/(-i.Er gk(3od(-or 8odCir hneeod/ or
oAo- i.Ir /-1r  8or -oa/ (Aor hn33o- ir (-hdt1or
g%n3o (3oir 2fr oeeneir 3(ido/1r dnh/ (-ar  8or io-0
 o-hoi.Erg??nritCCne rn??r-/3orC/(erio/eh8r??ner3oo 0
(-ar oAo- i.Erg??nrhndnero3C8/i(ir n??r oAo- ir n-r  8or
ne(a(-/dr 1nht3o- i.r /-1r g??nr itaaoi (n-ir n??r  o30
Cd/ oir nrhn3Cniorit33/eGrio- o-hoi.yrr
5 Conclusion and Future Work 
k8enta8r /r t (d( Gr oA/dt/ (n-r n-r it33/eGr 4e( (-ar
4or 8/Aor CenAo1r  8/ r 2fr  oh8-(StoiEr oiCoh(/ddGr
henii01nht3o- r2fErh/-r/(1r-o4ir5en4i(-aErio/eh8r
/-1r/-/dGi(iyr2-rC/e (htd/eEr o3Cne/droAo- r e/hl(-ar
/heniir 1nht3o- ir 8odCir tioeir Coe??ne3r 5o  oer / r
??/h 0a/ 8oe(-ar  8/-r  8oGr1nr4( 8nt r2fyrpioeir /dinr
Cen1tho1r3neor(-??ne3/ (Aorit33/e(oir4( 8rhenii0
1nht3o- r2fr 8/-r4( 8r e/1( (n-/dri(-ado01nht3o- r
2fyr??or/dinrhn3C/eo1r/-1r/-/dG9o1r 8or1(????oeo-hoir
5o 4oo-r  4nr tioer aentCiyr %th8r 3o/iteoir n??r  8or
5o-o??( ir  nr  8or oAo- t/dr o-1r tioeir /dinr CenA(1o1r
??oo15/hlrn-r48/ r4nelir4oddr /-1r (1o- (??(o1r /11(0
 (n-/dr eoio/eh8r Cen5do3iEr ith8r /ir  nr o#C/-1r  8or
ho- en(1r  nr /rC/(ern??ro- ( (oir /-1r  nrCenA(1orhn-??(0
1o-hor3o e(hir(-r 8or(- oe??/hoyr2-r 8or??t teor4or/(3r
 nrio rtCr/-rn-d(-or-o4ir/e (hdor/-/dGi(iriGi o3r/-1r
Coe??ne3rd/eaoer/-1reoatd/ert (d( GroA/dt/ (n-iyrr
Acknowledgement 
k8(ir 4nelr 4/ir itCCne o1r 5Gr  8or py%yr ??%mr
6L7ff7r L4/e1r t-1oer Pe/- r 22%0wjMb,vjEr  8or
py%yr Le3Gr 7oio/eh8r R/5ne/ neGr t-1oer 6nnCoe/0
 (Aor Laeoo3o- r ??t35oer ??j,,??m0wj0B0wwMbEr
PnnadoEr 2-hyEr 6p??Fr 7oio/eh8r f-8/-ho3o- r Ten0
ae/3Erm/htd GrTt5d(h/ (n-rTenae/3r/-1rP7k2rTen0
ae/3yrk8orA(o4ir/-1rhn-hdti(n-irhn- /(-o1r(-r 8(ir
1nht3o- r /eor  8niorn??r  8or /t 8neir /-1r i8ntd1r -n r
5or(- oeCeo o1r/ireoCeoio- (-ar  8orn????(h(/drCnd(h(oiEr
o( 8oero#Ceoiio1rner(3Cd(o1Ern??r 8orLe3Gr7oio/eh8r
R/5ne/ neGr ner  8or py%yr PnAoe-3o- yr k8or py%yr
PnAoe-3o- r (ir /t 8ne(9o1r  nr eoCen1thor /-1r 1(i0
 e(5t oreoCe(- ir ??nerPnAoe-3o- rCteCnioir -n 4( 80
i /-1(-ar/-GrhnCGe(a8 r-n / (n-r8oeorn-yr
References  
xn8-r ??yr qn1-/eyr Bwwbyr ??/e-(-ar L-/dGi(ir ??ner  8or 2-0
??ne3/ (n-rLaoOr7o 8(-l(-ar  8or 2- odd(ao-horTenhoiiyr
Center for Strategic Intelligence Research, Joint 
Military Intelligence CollegeEr??/i8(-a n-Ercy6yr
%o/-r 6nd5/ 8r /-1rme/-h(ir Ht5/d/yrBwwbyr kLT0NROrL-r
Lt n3/ o1r L-/dGi :ir Lii(i /- yr Proc. HLT-NAACL 
2003 (demonstrations)yr
R/te/rm/tdl-oeyrBwwbyrqoGn-1r 8or??(Ao0tioer/iit3C (n-Or
qo-o??( irn??r(-heo/io1ri/3Cdori(9oir(-rti/5(d( Gr oi (-ayr
Behavior Research Methods Instruments and Com-
putersrbM)buErb;j0b[byr
sn- Gr ]/33n- eooEr T/tdr ??o(doer /-1r ??/-1(-(r ??/G/lyr
,jjvyr 7o3n or pi/5(d( Gr koi (-ayr Interactionsyr znd0
t3or,Er2iitorbyrT/aoiOrB,0BMyr
]o-arx(r /-1rQ8o-ar68o-yrBwwjyr6enii01nht3o- rko30
Cne/dr /-1r %C/ (/dr Toein-r ke/hl(-ar %Gi o3r co3n-0
i e/ (n-yrProc. HLT-NAACL 2009yr
]o-ar x(Er 7/dC8r Pe(i83/-Er Q8o-ar 68o-r /-1r Te/i8/- r
PtC /yr Bwwjyr 6enii01nht3o- r fAo- r f# e/h (n-Er
7/-l(-ar /-1r ke/hl(-ayr Proc. Recent Advances in 
Natural Language Processing 2009.r
x/ln5r ??(odio-yr ,jjvyr pi/5(d( Gr f-a(-ooe(-ayr snea/-r
H/t??3/--rTt5d(i8oeiyr
288
Proceedings of NAACL-HLT 2013, pages 777?782,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Distant Supervision for Relation Extraction
with an Incomplete Knowledge Base
Bonan Min, Ralph Grishman, Li Wan
New York University
New York, NY 10003
{min,grishman,wanli}
@cs.nyu.edu
Chang Wang, David Gondek
IBM T. J. Watson Research Center
Yorktown Heights, NY 10598
{wangchan,dgondek}
@us.ibm.com
Abstract
Distant supervision, heuristically labeling a
corpus using a knowledge base, has emerged
as a popular choice for training relation ex-
tractors. In this paper, we show that a sig-
nificant number of ?negative? examples gen-
erated by the labeling process are false neg-
atives because the knowledge base is incom-
plete. Therefore the heuristic for generating
negative examples has a serious flaw. Building
on a state-of-the-art distantly-supervised ex-
traction algorithm, we proposed an algorithm
that learns from only positive and unlabeled
labels at the pair-of-entity level. Experimental
results demonstrate its advantage over existing
algorithms.
1 Introduction
Relation Extraction is a well-studied problem
(Miller et al, 2000; Zhou et al, 2005; Kambhatla,
2004; Min et al, 2012a). Recently, Distant Super-
vision (DS) (Craven and Kumlien, 1999; Mintz et
al., 2009) has emerged to be a popular choice for
training relation extractors without using manually
labeled data. It automatically generates training ex-
amples by labeling relation mentions1 in the source
corpus according to whether the argument pair is
listed in the target relational tables in a knowledge
base (KB). This method significantly reduces human
efforts for relation extraction.
The labeling heuristic has a serious flaw. Knowl-
edge bases are usually highly incomplete. For exam-
1An occurrence of a pair of entities with the source sentence.
ple, 93.8% of persons from Freebase2 have no place
of birth, and 78.5% have no nationality (section 3).
Previous work typically assumes that if the argument
entity pair is not listed in the KB as having a re-
lation, all the corresponding relation mentions are
considered negative examples.3 This crude assump-
tion labeled many entity pairs as negative when in
fact some of their mentions express a relation. The
number of such false negative matches even exceeds
the number of positive pairs, by 3 to 10 times, lead-
ing to a significant problem for training. Previous
approaches (Riedel et al, 2010; Hoffmann et al,
2011; Surdeanu et al, 2012) bypassed this problem
by heavily under-sampling the ?negative? class.
We instead deal with a learning scenario where we
only have entity-pair level labels that are either posi-
tive or unlabeled. We proposed an extension to Sur-
deanu et al (2012) that can train on this dataset. Our
contribution also includes an analysis on the incom-
pleteness of Freebase and the false negative match
rate in two datasets of labeled examples generated
by DS. Experimental results on a realistic and chal-
lenging dataset demonstrate the advantage of the al-
gorithm over existing solutions.
2 Related Work
Distant supervision was first proposed by Craven
and Kumlien (1999) in the biomedical domain.
2Freebase is a large collaboratively-edited KB. It is available
at http://www.freebase.com.
3There are variants of labeling heuristics. For example, Sur-
deanu et al (2011) and Sun et al (2011) use a pair < e, v >
as a negative example, when it is not listed in Freebase, but e is
listed with a different v?. These assumptions are also problem-
atic in cases where the relation is not functional.
777
Since then, it has gain popularity (Mintz et al, 2009;
Bunescu and Mooney, 2007; Wu and Weld, 2007;
Riedel et al, 2010; Hoffmann et al, 2011; Sur-
deanu et al, 2012; Nguyen and Moschitti, 2011).
To tolerate noisy labels in positive examples, Riedel
et al (2010) use Multiple Instance Learning (MIL),
which assumes only at-least-one of the relation men-
tions in each ?bag? of mentions sharing a pair of ar-
gument entities which bears a relation, indeed ex-
presses the target relation. MultiR (Hoffmann et
al., 2011) and Multi-Instance Multi-Label (MIML)
learning (Surdeanu et al, 2012) further improve it
to support multiple relations expressed by different
sentences in a bag. Takamatsu et al (2012) mod-
els the probabilities of a pattern showing relations,
estimated from the heuristically labeled dataset.
Their algorithm removes mentions that match low-
probability patterns. Sun et al (2011) and Min et
al. (2012b) also estimate the probablities of patterns
showing relations, but instead use them to relabel ex-
amples to their most likely classes. Their approach
can correct highly-confident false negative matches.
3 Problem Definition
Distant Supervision: Given a KB D (a collection
of relational tables r(e1, e2), in which r?R (R is the
set of relation labels), and < e1, e2 > is a pair of
entities that is known to have relation r) and a cor-
pus C, the key idea of distant supervision is that we
align D to C, label each bag4 of relation mentions
that share argument pair < e1, e2 > with r, other-
wise OTHER. This generates a dataset that has labels
on entity-pair (bag) level. Then a relation extractor
is trained with single-instance learning (by assum-
ing all mentions have the same label as the bag), or
Multiple-Instance Learning (by assuming at-least-
one of the mentions expresses the bag-level label),
or Multi-Instance Multi-Label learning (further as-
suming a bag can have multiple labels) algorithms.
All of these works treat the OTHER class as exam-
ples that are labeled as negative.
The incomplete KB problem: KBs are usually
incomplete because they are manually constructed,
and it is not possible to cover all human knowledge
4A bag is defined as a set of relation mentions sharing the
same entity pair as relation arguments. We will use the terms
bag and entity pair interchangeably in this paper.
nor stay current. We took frequent relations, which
involve an entity of type PERSON, from Freebase
for analysis. We define the incompleteness ?(r) of a
relation r as follows:
?(r) = |{e}|?|{e|?e?,s.t.r(e,e?)?D}||{e}|
?(r) is the percentage of all persons {e} that do
not have an attribute e? (with which r(e, e?) holds).
Table 1 shows that 93.8% of persons have no place
of birth, and 78.5% of them have no nationality.
These are must-have attributes for a person. This
shows that Freebase is highly incomplete.
Freebase relation types Incompleteness
/people/person/education 0.792
/people/person/employment history 0.923
/people/person/nationality* 0.785
/people/person/parents* 0.988
/people/person/place of birth* 0.938
/people/person/places lived* 0.966
Table 1: The incompleteness of Freebase (* are must-
have attributes for a person).
We further investigate the rate of false negative
matches, as the percentage of entity-pairs that are
not listed in Freebase but one of its mentions gen-
erated by DS does express a relation in the tar-
get set of types. We randomly picked 200 unla-
beled bags5 from each of the two datasets (Riedel
et al, 2010; Surdeanu et al, 2012) generated by DS,
and we manually annotate all relation mentions in
these bags. The result is shown in Table 2, along
with a few examples that indicate a relation holds in
the set of false negative matches (bag-level). Both
datasets have around 10% false negative matches in
the unlabeled set of bags. Taking into considera-
tion that the number of positive bags and unlabeled
bags are highly imbalanced (1:134 and 1:37 in the
Riedel and KBP dataset respectively, before under-
sampling the unlabeled class), the number of false
negative matches are 11 and 4 times the number
of positive bags in Reidel and KBP dataset, respec-
tively. Such a large ratio shows false negatives do
have a significant impact on the learning process.
4 A semi-supervised MIML algorithm
Our goal is to model the bag-level label noise,
caused by the incomplete KB problem, in addition
585% and 95.7% of the bags in the Riedel and KBP datasets
have only one relation mention.
778
Dataset
(train-
ing)
# pos-
itive
bags
# positive :
# unlabeled
% are
false
negatives
# positive
: # false
negative
has human
assessment
Examples of false negative mentions
Riedel 4,700 1:134(BD*) 8.5% 1:11.4 no
(/location/location/contains)... in Brooklyn ?s Williamsburg.
(/people/person/place lived) Cheryl Rogowski , a farmer from
Orange County ...
KBP 183,062 1:37(BD*) 11.5% 1:4 yes
(per:city of birth) Juan Martn Maldacena (born September
10, 1968) is a theoretical physicist born in Buenos Aires
(per:employee of)Dave Matthews, from the ABC News, ...
Table 2: False negative matches on the Riedel (Riedel et al, 2010) and KBP dataset (Surdeanu et al, 2012). All
numbers are on bag (pairs of entities) level. BD* are the numbers before downsampling the negative set to 10% and
5% in Riedel and KBP dataset, respectively.
to modeling the instance-level noise using a 3-layer
MIL or MIML model (e.g., Surdeanu et al (2012)).
We propose a 4-layer model as shown in Figure 1.
The input to the model is a list of n bags with a
vector of binary labels, either Positive (P), or Un-
labled (U) for each relation r. Our model can be
viewed as a semi-supervised6 framework that ex-
tends a state-of-the-art Multi-Instance Multi-Label
(MIML) model (Surdeanu et al, 2012). Since the
input to previous MIML models are bags with per-
relation binary labels of either Positive (P) or Neg-
ative (N), we add a set of latent variables ? which
models the true bag-level labels, to bridge the ob-
served bag labels y and the MIML layers. We con-
sider this as our main contribution to the model. Our
hierarchical model is shown in Figure 1.
Figure 1: Plate diagram of our model.
Let i, j be the index in the bag and mention level,
respectively. Following Surdeanu et al (2012), we
model mention-level extraction p(zrij |xij ;wz) and
multi-instance multi-label aggregation p(?ri |zi;wr?)
in the bottom 3 layers. We define:
? r is a relation label. r?R ? {OTHER}, in
which OTHER denotes no relation expressed.
? yri ?{P,U}: r holds for ith bag or the bag is
unlabeled.
6We use the term semi-supervised because the algorithm
uses unlabeled bags but existing solutions requires bags to be
labeled either positive or negative.
? ?ri ?{P,N}: a hidden variable that denotes
whether r holds for the ith bag.
? ? is an observed constant controlling the total
number of bags whose latent label is positive.
We define the following conditional probabilities:
? p(yri |?ri ) =
?
?
?
?
?
?
?
1/2 if yri = P ? ?ri = P ;
1/2 if yri = U ? ?ri = P ;
1 if yri = U ? ?ri = N ;
0 otherwise ;
It encodes the constraints between true bag-
level labels and the entity pair labels in the KB.
? p(?|?) ? N (
?n
i=1
?
r?R ?(?ri ,P )
n , 1k ) where
?(x, y) = 1 if x = y, 0 otherwise. k is a large
number. ? is the fraction of the bags that are
positive. It is an observed parameter that de-
pends on both the source corpus and the KB
used.
Similar to Surdeanu et al (2012), we also define
the following parameters and conditional probabili-
ties (details are in Surdeanu et al (2012)):
? zij?R ? {OTHER}: a latent variable that de-
notes the relation type of the jth mention in the
ith bag.
? xij is the feature representation of the jth rela-
tion mention in the ith bag. We use the set of
features in Surdeanu et al (2012).
? wz is the weight vector for the multi-class rela-
tion mention-level classifier.
? wr? is the weight vector for the rth binary top-
level aggregation classifier (from mention la-
bels to bag-level prediction). We usew? to rep-
resent w1? ,w2? , ...w
|R|
? .
? p(?ri |zi;wr?) ? Bern(f?(wr? , zi)) where f? is
probability produced by the rth top-level clas-
sifier, from the mention-label level to the bag-
label level.
? p(zrij |xij ;wz) ? Multi(fz(wz,xij)) where fz
779
is probability produced by the mention-level
classifier, from the mentions to the mention-
label level.7
4.1 Training
We use hard Expectation-Maximization (EM) algo-
rithm for training the model. Our objective function
is to maximize log-likelihood:
L(wz,w?) = logp(y, ?|x;wz,w?)
= log
?
?
p(y, ?, ?|x;wz,w?)
Since solving it exactly involves exploring an expo-
nential assignment space for ?, we approximate and
iteratively set ?? = arg? max p(?|y, ?,x;wz,w?)
p(?|y, ?,x;wz,w?) ? p(y, ?, ?|x;wz,w?)
= p(y, ?|?,x)p(?|x;wz,w?)
= p(y|?)p(?|?)p(?|x;wz,w?)
Rewriting in log form:
logp(?|y, ?,x;wz,w?)
= logp(y|?) + logp(?|?) + logp(?|x;wz,w?)
=
n
?
i=1
?
r?R
logp(yri |?ri )? k(
n
?
i=1
?
r?R
?(?ri , P )
n ? ?)
2
+
n
?
i=1
?
r?R
logp(?ri |xi;wz,w?) + const
Algorithm 1 Training (E-step:2-11; M-step:12-15)
1: for i = 1, 2 to T do
2: ?ri ? N for all yri = U and r?R
3: ?ri ? P for all yri = P and r?R
4: I = {< i, r > |?ri = N}; I ? = {< i, r > |?ri = P}
5: for k = 0, 1 to ?n? |I ?| do
6: < i?, r? >= argmax<i,r>?I p(?ri |xi;wz,w?)
7: ?r?i? ? P ; I = I\{< i?, r? >}
8: end for
9: for i = 1, 2 to n do
10: z?i = argmaxzi p(zi|?i,xi;wz,w?)
11: end for
12: w?z = argmaxwz
?n
i=1
?|xi|
j=1 logp(zij |xij ,wz)
13: for all r?R do
14: w
r(?)
? = argmaxwr?
?n
i=1 p(?ri |zi,wr?)
15: end for
16: end for
17: return wz,w?
7All classifiers are implemented with L2-regularized logistic
regression with Stanford CoreNLP package.
In the E-step, we do a greedy search (steps 5-8
in algorithm 1) in all p(?ri |xi;wz,w?) and update ?ri
until the second term is maximized. wz , w? are the
model weights learned from the previous iteration.
After fixed ?, we seek to maximize:
logp(?|xi;wz,w?) =
n
?
i=1
logp(?i|xi;wz,w?)
=
n
?
i=1
log
?
zi
p(?i, zi|xi;wz,w?)
which can be solved with an approxi-
mate solution in Surdeanu et al (2012)
(step 9-11): update zi independently with:
z?i = argmaxzi p(zi|?i,xi;wz,w?). More details
can be found in Surdeanu et al (2012).
In the M-step, we retrain both of the mention-
level and the aggregation level classifiers.
The full EM algorithm is shown in algorithm 1.
4.2 Inference
Inference on a bag xi is trivial. For each mention:
z?ij = argzij?R?{OTHER} max p(zij |xij ,wz)
Followed by the aggregation (directly with w?):
yr(?)i = argyri ?{P,N} max p(y
r
i |zi;wr?)
4.3 Implementation details
We implement our model on top of the
MIML(Surdeanu et al, 2012) code base.8 We
use the same mention-level and aggregate-level
feature sets as Surdeanu et al (2012). We adopt
the same idea of using cross validation for the E
and M steps to avoid overfitting. We initialize our
algorithm by sampling 5% unlabeled examples as
negative, in essence using 1 epoch of MIML to
initialize. Empirically it performs well.
5 Experiments
Data set: We use the KBP (Ji et al, 2011)
dataset9 prepared and publicly released by Surdeanu
et al (2012) for our experiment since it is 1) large
and realistic, 2) publicly available, 3) most im-
portantly, it is the only dataset that has associated
human-labeled ground truth. Any KB held-out eval-
uation without manual assessment will be signif-
icantly affected by KB incompleteness. In KBP
8Available at http://nlp.stanford.edu/software/mimlre.shtml
9Available from Linguistic Data Consortium (LDC).
http://projects.ldc.upenn.edu/kbp/data/
780
Figure 2: Performance on the KBP dataset. The figures on the left, middle and right show MIML, Hoffmann, and
Mintz++ compared to the same MIML-Semi curve, respectively. MIML-Semi is shown in red curves (lighter curves in
black and white) while other algorithms are shown in black curves (darker curves in black and white).
dataset, the training bags are generated by mapping
Wikipedia (http://en.wikipedia.org) infoboxes (after
merging similar types following the KBP 2011 task
definition) into a large unlabeled corpus (consisting
of 1.5M documents from the KBP source corpus and
a complete snapshot of Wikipedia). The KBP shared
task provided 200 query named entities with their as-
sociated slot values (in total several thousand pairs).
We use 40 queries as development dataset (dev), and
the rest (160 queries) as evaluation dataset. We set
? = 0.25 by tuning on the dev set and use it in the
experiments. For a fair comparison, we follow Sur-
deanu et al (2012) and begin by downsampling the
?negative? class to 5%. We also set T=8 and use
the following noisy-or (for ith bag) of mention-level
probability to rank predicted types (r) of pairs and
plot the precision-recall curves for all experiments.
Probi(r) = 1?
?
j
(1? p(zij = r|xij ;wz))
Evaluation: We compare our algorithm (MIML-
semi) to three algorithms: 1) MIML (Surdeanu et
al., 2012), the Multiple-Instance Multiple Label al-
gorithm which labels the bags directly with the KB
(y = ?). 2) MultiR (denoted as Hoffmann) (Hoff-
mann et al, 2011), a Multiple-Instance algorithm
that supports overlapping relations. It also imposes
y = ?. 3) Mintz++ (Surdeanu et al, 2012), a vari-
ant of the single-instance learning algorithm (section
3). The first two are stat-of-the-art Multi-Instance
Multi-Label algorithms. Mintz++ is a strong base-
line (Surdeanu et al, 2012) and an improved ver-
sion of Mintz et al (2009). Figure 2 shows that
our algorithm consistently outperforms all three al-
gorithms at almost all recall levels (with the excep-
tion of a very small region in the PR-curve). This
demonstrates that by treating unla-beled data set dif-
ferently and leveraging the missing positive bags,
MIML-semi is able to learn a more accurate model
for extraction. Although the proposed solution is a
specific algorithm, we believe the idea of treating
unlabeled data differently can be incorporated into
any of these algorithms that only use unlabeled data
as negative examples.
6 Conclusion
We show that the distant-supervision labeling pro-
cess generates a significant number of false nega-
tives because the knowledge base is incomplete. We
proposed an algorithm that learns from only positive
and unlabeled bags. Experimental results demon-
strate its advantage over existing algorithms.
Acknowledgments
Supported in part by the Intelligence Advanced Re-
search Projects Activity (IARPA) via Department of
Interior National Business Center contract number
D11PC20154. The U.S. Government is authorized
to reproduce and distribute reprints for Governmen-
tal purposes notwithstanding any copyright annota-
tion thereon. The views and conclusions contained
herein are those of the authors and should not be
interpreted as necessarily representing the official
policies or endorsements, either expressed or im-
plied, of IARPA, DoI/NBC, or the U.S. Government.
781
References
Razvan Bunescu and Raymond Mooney. 2007. Learning
to extract relations from the web using minimal super-
vision. In Proceedings of the 45th Annual Meeting of
the Association for Computational Linguistics.
Mark Craven and Johan Kumlien. 1999. Constructing bi-
ological knowledge bases by extracting information
from text sources. In Proceedings of the Seventh Inter-
national Conference on Intelligent Systems for Molec-
ular Biology.
Raphael Hoffmann, Congle Zhang, Xiao Ling, Luke
Zettlemoyer, and Daniel S. Weld. 2011. Knowledge-
based weak supervision for information extraction of
overlapping relations. In Proceedings of the Annual
Meeting of the Association for Computational Linguis-
tics.
Heng Ji, Ralph Grishman, and Hoa T. Dang. 2011.
Overview of the TAC 2011 knowledge base popula-
tion track. In Proceedings of the Text Analytics Con-
ference.
Jing Jiang and ChengXiang Zhai. 2007. A systematic ex-
ploration of the feature space for relation extraction. In
Proceedings of HLT-NAACL-2007.
Nanda Kambhatla. 2004. Combining lexical, syntactic,
and semantic features with maximum entropy mod-
els for information extraction. In Proceedings of ACL-
2004.
Scott Miller, Heidi Fox, Lance Ramshaw, and Ralph
Weischedel. 2000. A novel use of statistical parsing
to extract information from text. In Proceedings of
NAACL-2000.
Bonan Min, Shuming Shi, Ralph Grishman and Chin-
Yew Lin. 2012a. Ensemble Semantics for Large-scale
Unsupervised Relation Extraction. In Proceedings of
EMNLP-CoNLL 2012.
Bonan Min, Xiang Li, Ralph Grishman and Ang Sun.
2012b. New York University 2012 System for KBP
Slot Filling. In Proceedings of the Text Analysis Con-
ference (TAC) 2012.
Mike Mintz, Steven Bills, Rion Snow, and Daniel Juraf-
sky. 2009. Distant supervision for relation extraction
without labeled data. In Proceedings of the 47th An-
nual Meeting of the Association for Computational
Linguistics.
Truc Vien T. Nguyen and Alessandro Moschitti. 2011.
End-to-end relation extraction using distant supervi-
sion from external semantic repositories. In Proceed-
ings of the 49th Annual Meeting of the Association for
Computational Linguistics: Human Language Tech-
nologies.
Sebastian Riedel, Limin Yao, and Andrew McCallum.
2010. Modeling relations and their mentions without
labeled text. In Proceedings of the European Confer-
ence on Machine Learning and Knowledge Discovery
in Databases (ECML PKDD 10).
Ang Sun, Ralph Grishman, Wei Xu, and Bonan Min.
2011. New York University 2011 system for KBP slot
filling. In Proceedings of the Text Analytics Confer-
ence.
Mihai Surdeanu, Sonal Gupta, John Bauer, David Mc-
Closky, Angel X. Chang, Valentin I. Spitkovsky, and
Christopher D. Manning. 2011. Stanfords distantly-
supervised slot-filling system. In Proceedings of the
Text Analytics Conference.
Mihai Surdeanu, Julie Tibshirani, Ramesh Nallapati,
Christopher D. Manning. 2012. Multi-instance Multi-
label Learning for Relation Extraction. In Proceed-
ings of the 2012 Conference on Empirical Methods in
Natural Language Processing and Natural Language
Learning.
TAC KBP 2011 task definition. 2011. http://nlp
.cs.qc.cuny.edu/kbp/2011/KBP2011 TaskDefinition.pdf
Shingo Takamatsu, Issei Sato, Hiroshi Nakagawa. 2012.
ReducingWrong Labels in Distant Supervision for Re-
lation Extraction. In Proceedings of 50th Annual Meet-
ing of the Association for Computational Linguistics.
Fei Wu and Daniel S. Weld. 2007. Autonomously seman-
tifying wikipedia. In Proceedings of the International
Conference on Information and Knowledge Manage-
ment (CIKM-2007).
Guodong Zhou, Jian Su, Jie Zhang and Min Zhang. 2005.
Exploring various knowledge in relation extraction. In
Proceedings of ACL-2005.
782
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 789?797,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Using Document Level Cross-Event Inference  to Improve Event Extraction 
  Shasha Liao New York University 715 Broadway, 7th floor New York, NY 10003 USA liaoss@cs.nyu.edu  
Ralph Grishman New York University 715 Broadway, 7th floor New York, NY 10003 USA grishman@cs.nyu.edu      Abstract 
Event extraction is a particularly challenging type of information extraction (IE). Most current event extraction systems rely on local information at the phrase or sentence level. However, this local context may be insufficient to resolve ambiguities in identifying particular types of events; information from a wider scope can serve to resolve some of these ambiguities. In this paper, we use document level information to improve the performance of ACE event extraction. In contrast to previous work, we do not limit ourselves to information about events of the same type, but rather use information about other types of events to make predictions or resolve ambiguities regarding a given event. We learn such relationships from the training corpus and use them to help predict the occurrence of events and event arguments in a text. Experiments show that we can get 9.0% (absolute) gain in trigger (event) classification, and more than 8% gain for argument (role) classification in ACE event extraction. 1 Introduction The goal of event extraction is to identify instances of a class of events in text. The ACE 2005 event extraction task involved a set of 33 generic event types and subtypes appearing frequently in the news. In addition to identifying the event itself, it also identifies all of the participants and attributes of each event; these are the entities that are involved in that event.  Identifying an event and its participants and attributes is quite difficult because a larger field of view is often needed to understand how facts 
tie together. Sometimes it is difficult even for people to classify events from isolated sentences. From the sentence: (1) He left the company. it is hard to tell whether it is a Transport event in ACE, which means that he left the place; or an End-Position event, which means that he retired from the company. However, if we read the whole document, a clue like ?he planned to go shopping before he went home? would give us confidence to tag it as a Transport event, while a clue like ?They held a party for his retirement? would lead us to tag it as an End-Position event. Such clues are evidence from the same event type. However, sometimes another event type is also a good predictor. For example, if we find a Start-Position event like ?he was named president three years ago?, we are also confident to tag (1) as End-Position event. Event argument identification also shares this benefit. Consider the following two sentences: (2) A bomb exploded in Bagdad; seven people died while 11 were injured.  (3) A bomb exploded in Bagdad; the suspect got caught when he tried to escape.  If we only consider the local context of the trigger ?exploded?, it is hard to determine that ?seven people? is a likely Target of the Attack event in (2), or that the ?suspect? is the Attacker of the Attack event, because the structures of (2) and (3) are quite similar. The only clue is from the semantic inference that a person who died may well have been a Target of the Attack event, and the person arrested is probably the Attacker of the Attack event. These may be seen as 
789
examples of a broader textual inference problem, and in general such knowledge is quite difficult to acquire and apply. However, in the present case we can take advantage of event extraction to learn these rules in a simpler fashion, which we present below. Most current event extraction systems are based on phrase or sentence level extraction.  Several recent studies use high-level information to aid local event extraction systems. For example, Finkel et al (2005), Maslennikov and Chua (2007), Ji and Grishman (2008), and Patwardhan and Riloff (2007, 2009) tried to use discourse, document, or cross-document information to improve information extraction.  However, most of this research focuses on single event extraction, or focuses on high-level information within a single event type, and does not consider information acquired from other event types. We extend these approaches by introducing cross-event information to enhance the performance of multi-event-type extraction systems. Cross-event information is quite useful: first, some events co-occur frequently, while other events do not. For example, Attack, Die, and Injure events very frequently occur together, while Attack and Marry are less likely to co-occur. Also, typical relations among the arguments of different types of events can be helpful in predicting information to be extracted. For example, the Victim of a Die event is probably the Target of the Attack event. As a result, we extend the observation that ?a document containing a certain event is likely to contain more events of the same type?, and base our approach on the idea that ?a document containing a certain type of event is likely to contain instances of related events?. In this paper, automatically extracted within-event and cross-event information is used to aid traditional sentence level event extraction. 2 Task Description Automatic Content Extraction (ACE) defines an event as a specific occurrence involving participants1, and it annotates 8 types and 33 subtypes of events. We first present some ACE terminology to understand this task more easily: ? Entity: an object or a set of objects in one of the semantic categories of interest, referred to in the document by one or more                                                            1 See http://projects.ldc.upenn.edu/ace/docs/English-Events- Guidelines_v5.4.3.pdf for a description of this task. 
(coreferential) entity mentions. ? Entity mention: a reference to an entity (typically, a noun phrase) ? Timex: a time expression including date, time of the day, season, year, etc. ? Event mention: a phrase or sentence within which an event is described, including trigger and arguments. An event mention must have one and only one trigger, and can have an arbitrary number of arguments. ? Event trigger: the main word that most clearly expresses an event occurrence. An ACE event trigger is generally a verb or a noun. ? Event mention arguments (roles)2 : the entity mentions that are involved in an event mention, and their relation to the event. For example, event Attack might include participants like Attacker, Target, or attributes like Time_within and Place. Arguments will be taggable only when they occur within the scope of the corresponding event, typically the same sentence. Consider the sentence: (4) Three murders occurred in France today, including the senseless slaying of Bob Cole and the assassination of Joe Westbrook. Bob was on his way home when he was attacked?    Event extraction depends on previous phases like name identification, entity mention classification and coreference. Table 1 shows the results of this preprocessing. Note that entity mentions that share the same EntityID are coreferential and treated as the same object.  Entity(Timex) mention head word Entity ID Entity type 0001-1-1 France 0001-1 GPE 0001-T1-1 Today 0001-T1 Timex 0001-2-1 Bob Cole 0001-2 PER 0001-3-1 Joe Westbrook 0001-3 PER 0001-2-2 Bob 0001-2 PER 0001-2-3 He 0001-2 PER Table 1. An example of entities and entity mentions and their types                                                            2 Note that we do not deal with event mention coreference in this paper, so each event mention is treated as a separate event. 
790
There are three Die events, which share the same Place and Time roles, with different Victim roles. And there is one Attack event sharing the same Place and Time roles with the Die events.  Role Event type Trigger Place Victim Time Die murder 0001-1-1  0001-T1-1 Die death 0001-1-1 0001-2-1 0001-T1-1 Die killing 0001-1-1 0001-3-1 0001-T1-1 Role Event type Trigger Place Target Time Attack attack 0001-1-1 0001-2-3 0001-T1-1 Table2. An example of event trigger and roles  In this paper, we treat the 33 event subtypes as separate event types and do not consider the hierarchical structure among them. 3 Related Work Almost all the current ACE event extraction systems focus on processing one sentence at a time (Grishman et al, 2005; Ahn, 2006; Hardy et al 2006). However, there have been several studies using high-level information from a wider scope: Maslennikov and Chua (2007) use discourse trees and local syntactic dependencies in a pattern-based framework to incorporate wider context to refine the performance of relation extraction. They claimed that discourse information could filter noisy dependency paths as well as increasing the reliability of dependency path extraction. Finkel et al (2005) used Gibbs sampling, a simple Monte Carlo method used to perform approximate inference in factored probabilistic models. By using simulated annealing in place of Viterbi decoding in sequence models such as HMMs, CMMs, and CRFs, it is possible to incorporate non-local structure while preserving tractable inference. They used this technique to augment an information extraction system with long-distance dependency models, enforcing label consistency and extraction template consistency constraints. Ji and Grishman (2008) were inspired from the hypothesis of ?One Sense Per Discourse? (Yarowsky, 1995); they extended the scope from a single document to a cluster of topic-related documents and employed a rule-based approach 
to propagate consistent trigger classification and event arguments across sentences and documents. Combining global evidence from related documents with local decisions, they obtained an appreciable improvement in both event and event argument identification. Patwardhan and Riloff (2009) proposed an event extraction model which consists of two components: a model for sentential event recognition, which offers a probabilistic assessment of whether a sentence is discussing a domain-relevant event; and a model for recognizing plausible role fillers, which identifies phrases as role fillers based upon the assumption that the surrounding context is discussing a relevant event. This unified probabilistic model allows the two components to jointly make decisions based upon both the local evidence surrounding each phrase and the ?peripheral vision?. Gupta and Ji (2009) used cross-event information within ACE extraction, but only for recovering implicit time information for events. 4 Motivation We analyzed the sentence-level baseline event extraction, and found that many events are missing or spuriously tagged because the local information is not sufficient to make a confident decision. In some local contexts, it is easy to identify an event; in others, it is hard to do so. Thus, if we first tag the easier cases, and use such knowledge to help tag the harder cases, we might get better overall performance. In addition, global information can make the event tagging more consistent at the document level. Here are some examples. For trigger classification: The pro-reform director of Iran's biggest-selling daily newspaper and official organ of Tehran's municipality has stepped down following the appointment of a conservative ?it was founded a decade ago ? but a conservative city council was elected in the February 28 municipal polls ? Mahmud Ahmadi-Nejad, reported to be a hardliner among conservatives, was appointed mayor on Saturday ?Founded by former mayor Gholamhossein Karbaschi, Hamshahri?    
791
  Figure 1. Conditional probability of the other 32 event types in documents where a Die event appears  
 Figure 2. Conditional probability of the other 32 event types in documents where a Start-Org event appears   The sentence level baseline system finds event triggers like ?founded? (trigger of Start-Org), ?elected? (trigger of Elect), and ?appointment? (trigger of Start-Position), which are easier to identify because these triggers have more specific meanings. However, it does not recognize the trigger ?stepped? (trigger of End-Position) because in the training corpus ?stepped? does not always appear as an End-Position event, and local context does not provide enough information for the MaxEnt model to tag it as a trigger. However, in the document that contains related events like Start-Position, ?stepped? is more likely to be tagged as an End-Position event. For argument classification, the cross-event evidence from the document level is also useful: British officials say they believe Hassan was a blindfolded woman seen being shot in the head by a hooded militant on a video obtained but not aired by the Arab television station Al-Jazeera. She would be the first foreign woman to die in the wave of kidnappings in Iraq?she's been killed by 
(men in pajamas), turn Iraq upside down and find them. From this document, the local information is not enough for our system to tag ?Hassan? as the target of an Attack event, because it is quite far from the trigger ?shot? and the syntax is somewhat complex. However, it is easy to tag ?she? as the Victim of a Die event, because it is the object of the trigger ?killed?. As ?she? and ?Hassan? are co-referred, we can use this easily tagged argument to help identify the harder one. 4.1 Trigger Consistency and Distribution Within a document, there is a strong trigger consistency: if one instance of a word triggers an event, other instances of the same word will trigger events of the same type3.  There are also strong correlations among event types in a document. To see this we calculated the conditional probability (in the ACE corpus) of a certain event type appearing in a document when another event type appears in the same document.                                                            3 This is true over 99.4% of the time in the ACE corpus. 
792
    
 Figure 3. Conditional probability of all possible roles in other event types for entities that are the Targets of Attack events (roles with conditional probability below 0.002 are omitted)   Event Cond. Prob. Attack 0.714 Transport 0.507 Injure 0.306 Meet 0.164 Arrest-Jail 0.153 Sentence 0.126 Phone-Write 0.111 End-Position 0.116 Trial-Hearing 0.105 Convict 0.100 Table 3. Events co-occurring with die events with conditional probability > 10%  As there are 33 subtypes, there are potentially 33?32/2=528 event pairs. However, only a few of these appear with substantial frequency. For example, there are only 10 other event types that occur in more than 10% of the documents in which a die event appears. From Table 3, we can see that Attack, Transport and Injure events appear frequently with Die. We call these the related event types for Die (see Figure 1 and Table 3).  The same thing happens for Start-Org events, although its distribution is quite different from Die events. For Start-Org, there are more related events like End-Org, Start-Position, and End-Position (Figure 2). But there are 12 other event types which never appear in documents containing Start-Org events.  From the above, we can see that the distributions of different event types are quite different, and these distributions might be good predictors for event extraction. 
4.2 Role Consistency and Distribution Normally one entity, if it appears as an argument of multiple events of the same type in a single document, is assigned the same role each time.4 There is also a strong relationship between the roles when an entity participates in different types of events in a single document. For example, we checked all the entities in the ACE corpus that appear as the Target role for an Attack event, and recorded the roles they were assigned for other event types. Only 31 other event-role combinations appeared in total (out of 237 possible with ACE annotation), and 3 clearly dominated. In Figure 3, we can see that the most likely roles for the Target role of the Attack event are the Victim role of the Die or Injure event and the Artifact role of the Transport event. The last of these corresponds to troop movements prior to or in response to attacks. 5 Cross-event Approach In this section we present our approach to using document-level event and role information to improve sentence-level ACE event extraction.  Our event extraction system is a two-pass system where the sentence-level system is first applied to make decisions based on local information. Then the confident local information is collected and gives an approximate view of the content of the document. The document level system is finally applied to deal with the cases which the local 
                                                           4 This is true over 97% of the time in the ACE corpus. 
793
system can?t handle, and achieve document consistency. 5.1 Sentence-level Baseline System We use a state-of-the-art English IE system as our baseline (Grishman et al 2005). This system extracts events independently for each sentence, because the definition of event mention argument constrains them to appear in the same sentence. The system combines pattern matching with statistical models. In the training process, for every event mention in the ACE training corpus, patterns are constructed based on the sequences of constituent heads separating the trigger and arguments. A set of Maximum Entropy based classifiers are also trained: ? Argument Classifier: to distinguish arguments of a potential trigger from non-arguments; ? Role Classifier: to classify arguments by argument role.  ? Reportable-Event Classifier (Trigger Classifier): Given a potential trigger, an event type, and a set of arguments, to determine whether there is a reportable event mention. In the test procedure, each document is scanned for instances of triggers from the training corpus. When an instance is found, the system tries to match the environment of the trigger against the set of patterns associated with that trigger. This pattern-matching process, if successful, will assign some of the mentions in the sentence as arguments of a potential event mention. The argument classifier is applied to the remaining mentions in the sentence; for any argument passing that classifier, the role classifier is used to assign a role to it. Finally, once all arguments have been assigned, the reportable-event classifier is applied to the potential event mention; if the result is successful, this event mention is reported.5 5.2 Document-level Confident Information Collector To use document-level information, we need to collect information based on the sentence-level baseline system. As it is a statistically-based model, it can provide a value that indicates how likely it is that this word is a trigger, or that the mention is an argument and has a particular role.                                                            5 If the event arguments include some assigned by the pattern-matching process, the event mention is accepted unconditionally, bypassing the reportable- event classifier. 
We want to see if this value can be trusted as a confidence score. To this end, we set different thresholds from 0.1 to 1.0 in the baseline system output, and only evaluate triggers, arguments or roles whose confidence score is above the threshold. Results show that as the threshold is raised, the precision generally increases and the recall falls. This indicates that the value is consistent and a useful indicator of event/argument confidence (see Figure 4).6  
 Figure 4. The performance of different confidence thresholds in the baseline system  on the development set  To acquire confident document-level information, we only collect triggers and roles tagged with high confidence. Thus, a trigger threshold t_threshold and role threshold r_threshold are set to remove low confidence triggers and arguments. Finally, a table with confident event information is built. For every event, we collect its trigger and event type; for every argument, we use co-reference information and record every entity and its role(s) in events of a certain type.  To achieve document consistency, in cases where the baseline system assigns a word to triggers for more than one event type, if the margin between the probability of the highest and the second highest scores is above a threshold m_threshold, we only keep the event type with highest score and record this in the confident-event table. Otherwise (if the margin is smaller) the event type assignments will be recorded in a separate conflict table. The same strategy is applied to argument/role conflicts. We will not use information in the conflict table to infer the event type or argument/roles for other event mentions, because we cannot                                                            6 The trigger classification curve doesn?t follow the expected recall/precision trade-off, particularly at high thresholds.  This is due, at least in part, to the fact that some events bypass the reportable-event classifier (trigger classifier) (see footnote 5). At high thresholds this is true of the bulk of the events. 
794
confidently resolve the conflict. However, the event type and argument/role assignments in the conflict table will be included in the final output because the local confidence for the individual assignments is high.  As a result, we finally build two document-level confident-event tables: the event type table and the argument (role) table. A conflict table is also built but not used for further predictions (see Table 4).  Confident table Event type table Trigger Event Type Met Meet Exploded Attack Went Transport   Injured Injure Attacked Attack Died Die Argument role table Entity ID Event type Role 0004-T2 Die Time Within 0004-6 Die Place 0004-4 Die Victim 0004-7 Die Agent 0004-11 Attack Target 0004-T3 Attack Time Within 0004-12 Attack Place 0004-10 Attack Attacker Conflict table Entity ID Event type Roles 0004-8 Attack Victim, Agent Table 4. Example of document-level confident-event table (event type and argument role entries) and conflict table  5.3 Statistical Cross-event Classifiers To take advantage of cross-event relationships, we train two additional MaxEnt classifiers ? a document-level trigger and argument classifier ? and then use these classifiers to infer additional events and event arguments. In analyzing new text, the trigger classifier is first applied to tag an event, and then the argument (role) classifier is applied to tag possible arguments and roles of this event.  5.3.1 Document Level Trigger Classifier From the document-level confident-event table, we have a rough view of what kinds of events 
are reported in this document. The trigger classifier predicts whether a word is the trigger of an event, and if so of what type, given the information (from the confident-event table) about other types of events in the document. Each feature of this classifier is the conjunction of: ? The base form of the word ? An event type ? A binary indicator of whether this event type is present elsewhere in the document (There are 33 event types and so 33 features for each word).  5.3.2 Document Level Argument (Role) Classifier The role classifier predicts whether a given mention is an argument of a given event and, if so, what role it takes on, again using information from the confident-event table about other events. As noted above, we assume that the role of an entity is unique for a specific event type, although an entity can take on different roles for different event types. Thus, if there is a conflict in the document level table, the collector will only keep the one with highest confidence, or discard them all. As a result, every entity is assigned a unique role with respect to a particular event type, or null if it is not an argument of a certain event type. Each feature is the conjunction of: ? The event type we are trying to assign an argument/role to. ? One of the 32 other event types ? The role of this entity with respect to the other event type elsewhere in the document, or null if this entity is not an argument of that type of event  5.4 Document Level Event Tagging At this point, the low-confidence triggers and arguments (roles) have been removed and the document-level confident-event table has been built; the new classifiers are now used to augment the confident tags that were previously assigned based on local information. For trigger tagging, we only apply the classifier to the words that do not have a confident local labeling; if the trigger is already in the document level confident-event table, we will not re-tag it.   
795
           performance system/human Trigger classification Argument classification Role classification  P R F P R F P R F Sentence-level baseline system 67.56 53.54 59.74 46.45 37.15 41.29 41.02 32.81 36.46 Within-event-type rules 63.03 59.90 61.43 48.59 46.16 47.35 43.33 41.16 42.21 Cross-event statistical model 68.71 68.87 68.79 50.85 49.72 50.28 45.06 44.05 44.55 Human annotation1 59.2 59.4 59.3 60.0 69.4 64.4 51.6 59.5 55.3 Human annotation2 69.2 75.0 72.0 62.7 85.4 72.3 54.1 73.7 62.4 Table 5. Overall performance on blind test data  The argument/role tagger is then applied to all events?those in the confident-event table and those newly tagged. For argument tagging, we only consider the entity mentions in the same sentence as the trigger word, because by the ACE event guidelines, the arguments of an event should appear within the same sentence as the trigger. For a given event, we re-tag the entity mentions that have not already been assigned as arguments of that event by the confident-event or conflict table. 6 Experiments We followed Ji and Grishman (2008)?s evaluation and randomly select 10 newswire texts from the ACE 2005 training corpora as our development set, which is used for parameter tuning, and then conduct a blind test on a separate set of 40 ACE 2005 newswire texts. We use the rest of the ACE training corpus (549 documents) as training data for both the sentence-level baseline event tagger and document-level event tagger.  To compare with previous work on within-event propagation, we reproduced Ji and Grishman (2008)?s approach for cross-sentence, within-event-type inference (see ?within-event-type rules? in Table 5). We applied their within-document inference rules using the cross-sentence confident-event information. These rules basically serve to adjust trigger and argument classification to achieve document-wide consistency. This process treats each event type separately: information about events of a given type is used to infer information about other events of the same type. We report the overall Precision (P), Recall (R), and F-Measure (F) on blind test data. In addition, we also report the performance of two human 
annotators on 28 ACE newswire texts (a subset of the blind test set).7 From the results presented in Table 5, we can see that using the document level cross-event information, we can improve the F score for trigger classification by 9.0%, argument classification by 9.0%, and role classification by 8.1%. Recall improved sharply, demonstrating that cross-event information could recover information that is difficult for the sentence-level baseline to extract; precision also improved over the baseline, although not as markedly. Compared to the within-event-type rules, the cross-event model yields much more improvement for trigger classification: rule-based propagation gains 1.7% improvement while the cross-event model achieves a further 7.3% improvement. For argument and role classification, the cross-event model also gains 3% and 2.3% above that obtained by the rule-based propagation process. 7 Conclusion and Future Work We propose a document-level statistical model for event trigger and argument (role) classification to achieve document level within-event and cross-event consistency. Experiments show that document-level information can improve the performance of a sentence-level baseline event extraction system.  The model presented here is a simple two-stage recognition process; nonetheless, it has proven sufficient to yield substantial improvements in event recognition and event                                                            7 The final key was produced by review and adjudication of the two annotations by a third annotator, which indicates that the event extraction task is quite difficult and human agreement is not very high. 
796
argument recognition. Richer models, such as those based on joint inference, may produce even greater gains. In addition, extending the approach to cross-document information, following (Ji and Grishman 2008), may be able to further improve performance. References  David Ahn. 2006. The stages of event extraction. In Proc. COLING/ACL 2006 Workshop on Annotating and Reasoning about Time and Events. Sydney, Australia.  J. Finkel, T. Grenager, and C. Manning. 2005. Incorporating Non-local Information into Information Extraction Systems by Gibbs Sampling. In Proc. 43rd Annual Meeting of the Association for Computational Linguistics, pages 363?370, Ann Arbor, MI, June. Ralph Grishman, David Westbrook and Adam Meyers. 2005. NYU?s English ACE 2005 System Description. In Proc. ACE 2005 Evaluation Workshop, Gaithersburg, MD. Prashant Gupta, Heng Ji. 2009. Predicting Unknown Time Arguments based on Cross-Event Propagation. In Proc. ACL-IJCNLP 2009. Hilda Hardy, Vika Kanchakouskaya and Tomek Strzalkowski. 2006. Automatic Event Classification Using Surface Text Features. In Proc. AAAI06 Workshop on Event Extraction and Synthesis. Boston, MA. H. Ji and R. Grishman. 2008. Refining Event Extraction through Cross-Document Inference. In Proc. ACL-08: HLT, pages 254?262, Columbus, OH, June. M. Maslennikov and T. Chua. 2007. A Multi resolution Framework for Information Extraction from Free Text. In Proc. 45th Annual Meeting of the Association of Computational Linguistics, pages 592?599, Prague, Czech Republic, June.  S. Patwardhan and E. Riloff. 2007. Effective Information Extraction with Semantic Affinity Patterns and Relevant Regions. In Proc. Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, 2007, pages 717?727, Prague, Czech Republic, June. Patwardhan, S. and Riloff, E. 2009. A Unified Model of Phrasal and Sentential Evidence for Information Extraction. In Proc. Conference on Empirical Methods in Natural Language Processing 2009, (EMNLP-09). David Yarowsky. 1995. Unsupervised Word Sense Disambiguation Rivaling Supervised Methods. In Proc. ACL 1995. Cambridge, MA.  
797
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 521?529,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Semi-supervised Relation Extraction with Large-scale Word Clustering 
 
 
Ang Sun             Ralph Grishman             Satoshi Sekine 
  
Computer Science Department 
New York University 
{asun,grishman,sekine}@cs.nyu.edu 
 
 
 
 
 
 
Abstract 
We present a simple semi-supervised 
relation extraction system with large-scale 
word clustering. We focus on 
systematically exploring the effectiveness 
of different cluster-based features. We also 
propose several statistical methods for 
selecting clusters at an appropriate level of 
granularity. When training on different 
sizes of data, our semi-supervised approach 
consistently outperformed a state-of-the-art 
supervised baseline system. 
1 Introduction 
Relation extraction is an important information 
extraction task in natural language processing 
(NLP), with many practical applications. The goal 
of relation extraction is to detect and characterize 
semantic relations between pairs of entities in text. 
For example, a relation extraction system needs to 
be able to extract an Employment relation between 
the entities US soldier and US in the phrase US 
soldier.  
Current supervised approaches for tackling this 
problem, in general, fall into two categories: 
feature based and kernel based. Given an entity 
pair and a sentence containing the pair, both 
approaches usually start with multiple level 
analyses of the sentence such as tokenization, 
partial or full syntactic parsing, and dependency 
parsing. Then the feature based method explicitly 
extracts a variety of lexical, syntactic and semantic 
features for statistical learning, either generative or 
discriminative (Miller et al, 2000; Kambhatla, 
2004; Boschee et al, 2005; Grishman et al, 2005; 
Zhou et al, 2005; Jiang and Zhai, 2007). In 
contrast, the kernel based method does not 
explicitly extract features; it designs kernel 
functions over the structured sentence 
representations (sequence, dependency or parse 
tree) to capture the similarities between different 
relation instances (Zelenko et al, 2003; Bunescu 
and Mooney, 2005a; Bunescu and Mooney, 2005b; 
Zhao and Grishman, 2005; Zhang et al, 2006; 
Zhou et al, 2007; Qian et al, 2008). Both lines of 
work depend on effective features, either explicitly 
or implicitly.  
The performance of a supervised relation 
extraction system is usually degraded by the 
sparsity of lexical features. For example, unless the 
example US soldier has previously been seen in the 
training data, it would be difficult for both the 
feature based and the kernel based systems to 
detect whether there is an Employment relation or 
not. Because the syntactic feature of the phrase US 
soldier is simply a noun-noun compound which is 
quite general, the words in it are crucial for 
extracting the relation. 
This motivates our work to use word clusters as 
additional features for relation extraction. The 
assumption is that even if the word soldier may 
never have been seen in the annotated Employment 
relation instances, other words which share the 
same cluster membership with soldier such as 
president and ambassador may have been 
observed in the Employment instances. The 
absence of lexical features can be compensated by 
521
the cluster features. Moreover, word clusters may 
implicitly correspond to different relation classes. 
For example, the cluster of president may be 
related to the Employment relation as in US 
president while the cluster of businessman may be 
related to the Affiliation relation as in US 
businessman.   
The main contributions of this paper are: we 
explore the cluster-based features in a systematic 
way and propose several statistical methods for 
selecting effective clusters.  We study the impact 
of the size of training data on cluster features and 
analyze the performance improvements through an 
extensive experimental study. 
The rest of this paper is organized as follows: 
Section 2 presents related work and Section 3 
provides the background of the relation extraction 
task and the word clustering algorithm. Section 4 
describes in detail a state-of-the-art supervised 
baseline system. Section 5 describes the cluster-
based features and the cluster selection methods. 
We present experimental results in Section 6 and 
conclude in Section 7.  
2 Related Work 
The idea of using word clusters as features in 
discriminative learning was pioneered by Miller et 
al. (2004), who augmented name tagging training 
data with hierarchical word clusters generated by 
the Brown clustering algorithm (Brown et al, 1992) 
from a large unlabeled corpus. They used different 
thresholds to cut the word hierarchy to obtain 
clusters of various granularities for feature 
decoding. Ratinov and Roth (2009) and Turian et 
al. (2010) also explored this approach for name 
tagging. Though all of them used the same 
hierarchical word clustering algorithm for the task 
of name tagging and reported improvements, we 
noticed that the clusters used by Miller et al (2004) 
were quite different from that of Ratinov and Roth 
(2009) and Turian et al (2010). To our knowledge, 
there has not been work on selecting clusters in a 
principled way. We move a step further to explore 
several methods in choosing effective clusters. A 
second difference between this work and the above 
ones is that we utilize word clusters in the task of 
relation extraction which is very different from 
sequence labeling tasks such as name tagging and 
chunking. 
Though Boschee et al (2005) and Chan and 
Roth (2010) used word clusters in relation 
extraction, they shared the same limitation as the 
above approaches in choosing clusters. For 
example, Boschee et al (2005) chose clusters of 
different granularities and Chan and Roth (2010) 
simply used a single threshold for cutting the word 
hierarchy.  Moreover, Boschee et al (2005) only 
augmented the predicate (typically a verb or a 
noun of the most importance in a relation in their 
definition) with word clusters while Chan and Roth 
(2010) performed this for any lexical feature 
consisting of a single word. In this paper, we 
systematically explore the effectiveness of adding 
word clusters to different lexical features.  
3 Background  
3.1 Relation Extraction 
One of the well defined relation extraction tasks is 
the Automatic Content Extraction1 (ACE) program 
sponsored by the U.S. government. ACE 2004 
defined 7 major entity types: PER (Person), ORG 
(Organization), FAC (Facility), GPE (Geo-Political 
Entity: countries, cities, etc.), LOC (Location), 
WEA (Weapon) and VEH (Vehicle). An entity has 
three types of mention: NAM (proper name), NOM 
(nominal) or PRO (pronoun). A relation was 
defined over a pair of entity mentions within a 
single sentence. The 7 major relation types with 
examples are shown in Table 1. ACE 2004 also 
defined 23 relation subtypes. Following most of 
the previous work, this paper only focuses on 
relation extraction of major types. 
Given a relation instance ( , , )i jx s m m?
, where 
im  and jm
 are a pair of mentions and s  is the 
sentence containing the pair, the goal is to learn a 
function which maps the instance x to a type c, 
where c is one of the 7 defined relation types or the 
type Nil (no relation exists). There are two 
commonly used learning paradigms for relation 
extraction: 
Flat: This strategy performs relation detection 
and classification at the same time. One multi-class 
classifier is trained to discriminate among the 7 
relation types plus the Nil type. 
Hierarchical: This one separates relation 
detection from relation classification. One binary 
                                                        
1 Task definition: http://www.itl.nist.gov/iad/894.01/tests/ace/ 
ACE guidelines: http://projects.ldc.upenn.edu/ace/ 
522
classifier is trained first to distinguish between 
relation instances and non-relation instances. This 
can be done by grouping all the instances of the 7 
relation types into a positive class and the instances 
of Nil into a negative class. Then the thresholded 
output of this binary classifier is used as training 
data for learning a multi-class classifier for the 7 
relation types (Bunescu and Mooney, 2005b). 
 
Type Example 
EMP-ORG US president 
PHYS a military base in Germany 
GPE-AFF U.S. businessman 
PER-SOC a spokesman for the senator 
DISC each of whom 
ART US helicopters 
OTHER-AFF Cuban-American people 
 
Table 1:  ACE relation types and examples from the 
annotation guideline 2 . The heads of the two entity 
mentions are marked. Types are listed in decreasing 
order of frequency of occurrence in the ACE corpus. 
3.2 Brown Word Clustering 
The Brown algorithm is a hierarchical clustering 
algorithm which initially assigns each word to its 
own cluster and then repeatedly merges the two 
clusters which cause the least loss in average 
mutual information between adjacent clusters 
based on bigram statistics.  By tracing the pairwise 
merging steps, one can obtain a word hierarchy 
which can be represented as a binary tree. A word 
can be compactly represented as a bit string by 
following the path from the root to itself in the tree, 
assigning a 0 for each left branch, and a 1 for each 
right branch. A cluster is just a branch of that tree. 
A high branch may correspond to more general 
concepts while the lower branches it includes 
might correspond to more specific ones.  
Brown et al (1992) described an efficient 
implementation based on a greedy algorithm which 
initially assigned only the most frequent words into 
distinct clusters. It is worth pointing out that in this 
implementation each word occupies a leaf in the 
hierarchy, but each leaf might contain more than 
one word as can be seen from Table 2. The lengths 
of the bit strings also vary among different words. 
 
 
                                                        
2 http://projects.ldc.upenn.edu/ace/docs/EnglishRDCV4-3-
2.PDF 
Bit string Examples 
111011011100 US ? 
1110110111011 U.S. ? 
1110110110000 American ? 
1110110111110110 Cuban, Pakistani, Russian ?  
11111110010111 Germany, Poland, Greece ?  
110111110100 businessman, journalist, reporter 
1101111101111 president, governor, premier?  
1101111101100    senator, soldier, ambassador ? 
11011101110 spokesman, spokeswoman, ? 
11001100 people, persons, miners, Haitians 
110110111011111 base, compound, camps, camp ? 
110010111 helicopters, tanks, Marines ? 
 
Table 2: An example of words and their bit string 
representations obtained in this paper. Words in bold are 
head words that appeared in Table 1. 
4 Feature Based Relation Extraction 
Given a pair of entity mentions ,i jm m? ?
and the 
sentence containing the pair, a feature based 
system extracts a feature vector v  which contains 
diverse lexical, syntactic and semantic features. 
The goal is to learn a function which can estimate 
the conditional probability ( | )p c v , the probability 
of a relation type c given the feature vector v . The 
type with the highest probability will be output as 
the class label for the mention pair.  
We now describe a supervised baseline system 
with a very large set of features and its learning 
strategy.  
4.1 Baseline Feature Set 
We first adopted the full feature set from Zhou et 
al. (2005), a state-of-the-art feature based relation 
extraction system. For space reasons, we only 
show the lexical features as in Table 3 and refer the 
reader to the paper for the rest of the features.  
At the lexical level, a relation instance can be 
seen as a sequence of tokens which form a five 
tuple <Before, M1, Between, M2, After>. Tokens 
of the five members and the interaction between 
the heads of the two mentions can be extracted as 
features as shown in Table 3. 
In addition, we cherry-picked the following 
features which were not included in Zhou et al 
(2005) but were shown to be quite effective for 
relation extraction. 
Bigram of the words between the two mentions: 
This was extracted by both Zhao and Grishman 
(2005) and Jiang and Zhai (2007), aiming to 
523
provide more order information of the tokens 
between the two mentions. 
Patterns:  There are three types of patterns: 1) 
the sequence of the tokens between the two 
mentions as used in Boschee et al (2005); 2) the 
sequence of the heads of the constituents between 
the two mentions as used by Grishman et al (2005); 
3) the shortest dependency path between the two 
mentions in a dependency tree as adopted by 
Bunescu and Mooney (2005a). These patterns can 
provide more structured information of how the 
two mentions are connected.  
Title list: This is tailored for the EMP-ORG type 
of relations as the head of one of the mentions is 
usually a title. The features are decoded in a way 
similar to that of Sun (2009).  
 
Position Feature Description 
Before BM1F first word before M1 
BM1L second word before M1 
M1 WM1 bag-of-words in M1 
HM1 head3 word of M1 
Between WBNULL when no word in between 
WBFL the only word in between when 
only one word in between 
WBF first word in between when at 
least two words in between 
WBL last word in between when at 
least two words in between 
WBO other words in between except 
first and last words when at 
least three words in between 
M2 WM2 bag-of-words in M2 
HM2 head word of M2 
M12 HM12 combination of HM1 and HM2 
After AM2F  first word after M2 
AM2L  second word after M2 
 
Table 3: Lexical features for relation extraction. 
4.2 Baseline Learning Strategy 
We employ a simple learning framework that is 
similar to the hierarchical learning strategy as 
described in Section 3.1. Specifically, we first train 
a binary classifier to distinguish between relation 
instances and non-relation instances. Then rather 
than using the thresholded output of this binary 
classifier as training data, we use only the 
annotated relation instances to train a multi-class 
classifier for the 7 relation types. In the test phase, 
                                                        
3 The head word of a mention is normally set as the last word 
of the mention as in Zhou et al (2005). 
given a test instance x , we first apply the binary 
classifier to it for relation detection; if it is detected 
as a relation instance we then apply the multi-class 
relation classifier to classify it4. 
5 Cluster Feature Selection 
The selection of cluster features aims to answer the 
following two questions: which lexical features 
should be augmented with word clusters to 
improve generalization accuracy? How to select 
clusters at an appropriate level of granularity? We 
will describe our solutions in Section 5.1 and 5.2. 
5.1 Cluster Feature Decoding 
While each one of the lexical features in Table 3 
used by the baseline can potentially be augmented 
with word clusters, we believe the effectiveness of 
a lexical feature with augmentation of word 
clusters should be tested either individually or 
incrementally according to a rank of its importance 
as shown in Table 4. We will show the 
effectiveness of each cluster feature in the 
experiment section. 
 
Impor- 
tance 
Lexical 
Feature 
Description of 
lexical feature 
Cluster Feature 
1 HM HM1, HM2 and 
HM12 
HM1_WC, 
HM2_WC, 
HM12_WC 
2 BagWM WM1 and WM2 BagWM_WC 
3 HC a head5 of a chunk 
in context 
HC_WC 
4 BagWC word of context BagWC_WC 
 
Table 4: Cluster features ordered by importance. 
 
The importance is based on linguistic intuitions 
and observations of the contributions of different 
lexical features from various feature based systems. 
Table 4 simplifies a relation instance as a three 
tuple <Context, M1, M2> where the Context 
includes the Before, Between and After from the 
                                                        
4 Both the binary and multi-class classifiers output normalized 
probabilities in the range [0,1]. When the binary classifier?s 
prediction probability is greater than 0.5, we take the 
prediction with the highest probability of the multi-class 
classifier as the final class label. When it is in the range 
[0.3,0.5], we only consider as the final class label the 
prediction of the multi-class classifier with a probability which 
is greater than 0.9. All other cases are taken as non-relation 
instances. 
5 The head of a chunk is defined as the last word in the chunk. 
524
five tuple representation. As a relation in ACE is 
usually short, the words of the two entity mentions 
can provide more critical indications for relation 
classification than the words from the context. 
Within the two entity mentions, the head word of 
each mention is usually more important than other 
words of the mention; the conjunction of the two 
heads can provide an additional clue. And in 
general words other than the chunk head in the 
context do not contribute to establishing a 
relationship between the two entity mentions. 
The cluster based semi-supervised system works 
by adding an additional layer of lexical features 
that incorporate word clusters as shown in column 
4 of Table 4. Take the US soldier as an example, if 
we decide to use a length of 10 as a threshold to 
cut the Brown word hierarchy to generate word 
clusters, we will extract a cluster feature 
HM1_WC10=1101111101 in addition to the 
lexical feature HM1=soldier given that the full bit 
string of soldier is  1101111101100 in Table 2. 
(Note that the cluster feature is a nominal feature, 
not to be confused with an integer feature.) 
5.2 Selection of Clusters 
Given the bit string representations of all the words 
in a vocabulary, researchers usually use prefixes of 
different lengths of the bit strings to produce word 
clusters of various granularities. However, how to 
choose the set of prefix lengths in a principled way? 
This has not been answered by prior work. 
Our main idea is to learn the best set of prefix 
lengths, perhaps through the validation of their 
effectiveness on a development set of data. To our 
knowledge, previous research simply uses ad-hoc 
prefix lengths and lacks this training procedure. 
The training procedure can be extremely slow for 
reasons to be explained below. 
Formally, let l  be the set of available prefix 
lengths ranging from 1 bit to the length of the 
longest bit string in the Brown word hierarchy and 
let m  be the set of prefix lengths we want to use in 
decoding cluster features, then the problem of 
selecting effective clusters transforms to finding a 
| |m -combination of the set l which maximizes 
system performance. The training procedure can be 
extremely time consuming if we enumerate every 
possible | |m -combination of l , given that | |m  
can range from 1 to the size of l and the size of 
l equals the length of the longest bit string which is 
usually 20 when inducing 1,000 clusters using the 
Brown algorithm.                                  
One way to achieve better efficiency is to 
consider only a subset of l instead of the full set. In 
addition, we limit ourselves to use sizes 3 and 4 for 
m  for matching prior work. This keeps the cluster 
features to a manageable size considering that 
every word in your vocabulary could contribute to 
a lexical feature. For picking a subset of l , we 
propose below two statistical measures for 
computing the importance of a certain prefix 
length. 
Information Gain (IG): IG measures the 
quality or importance of a feature f by computing 
the difference between the prior entropy of classes 
C and the posterior entropy, given values V of the 
feature f (Hunt et al, 1966; Quinlan, 1986). For 
our purpose, C is the set of relation types, f is a 
cluster-based feature with a certain prefix length 
such as HM1_WC* where * means the prefix 
length and a value v is the prefix of the bit string 
representation of HM1. More formally, the IG of f 
is computed as follows: 
( ) ( ) log ( )
( ( ) ( | ) log ( | ))
c C
v V c C
IG f p c p c
p v p c v p c v
?
? ?
? ? ?
? ?
?
? ?
        (1) 
where the first and second terms refer to the prior 
and posterior entropies respectively. 
For each prefix length in the set l , we can 
compute its IG for a type of cluster feature and 
then rank the prefix lengths based on their IGs for 
that cluster feature. For simplicity, we rank the 
prefix lengths for a group of cluster features (a 
group is a row from column 4 in Table 4) by 
collapsing the individual cluster features into a 
single cluster feature. For example, we collapse the 
3 types: HM1_WC, HM2_WC and HM12_WC into 
a single type HM_WC for computing the IG.  
Prefix Coverage (PC): If we use a short prefix 
then the clusters produced correspond to the high 
branches in the word hierarchy and would be very 
general. The cluster features may not provide more 
informative information than the words themselves. 
Similarly, if we use a long prefix such as the length 
of the longest bit string, then maybe only a few of 
the lexical features can be covered by clusters. To 
capture this intuition, we define the PC of a prefix 
length i as below: 
525
( )( ) ( )
ic
l
count fPC i count f?
                        (2) 
where 
lf  stands for a lexical feature such as HM1 
and
icf
 a cluster feature with prefix length i such as 
HM1_WCi, (*)count  is the number of 
occurrences of that feature in training data. 
Similar to IG, we compute PC for a group of 
cluster features, not for each individual feature. 
In our experiments, the top 10 ranked prefix 
lengths based on IG and prefix lengths with PC 
values in the range [0.4, 0.9] were used. 
In addition to the above two statistical measures, 
for comparison, we introduce another two simple 
but extreme measures for the selection of clusters. 
Use All Prefixes (UA): UA produces a cluster 
feature at every available bit length with the hope 
that the underlying supervised system can learn 
proper weights of different cluster features during 
training. For example, if the full bit representation 
of ?Apple? is ?000?, UA would produce three 
cluster features: prefix1=0, prefix2=00 and 
prefix3=000. Because this method does not need 
validation on the development set, it is the laziest 
but the fastest method for selecting clusters.  
Exhaustive Search (ES): ES works by trying 
every possible combination of the set l and picking 
the one that works the best for the development set. 
This is the most cautious and the slowest method 
for selecting clusters. 
6 Experiments 
In this section, we first present details of our 
unsupervised word clusters, the relation extraction 
data set and its preprocessing. We then present a 
series of experiments coupled with result analyses. 
We used the English portion of the TDT5 
corpora (LDC2006T18) as our unlabeled data for 
inducing word clusters. It contains roughly 83 
million words in 3.4 million sentences with a 
vocabulary size of 450K. We left case intact in the 
corpora. Following previous work, we used 
Liang?s implementation of the Brown clustering 
algorithm (Liang, 2005).  We induced 1,000 word 
clusters for words that appeared at least twice in 
the corpora. The reduced vocabulary contains 
255K unique words. The clusters are available at 
http://www.cs.nyu.edu/~asun/data/TDT5_BrownW
C.tar.gz. 
For relation extraction, we used the benchmark 
ACE 2004 training data. Following most of the 
previous research, we used in experiments the 
nwire (newswire) and bnews (broadcast news) 
genres of the data containing 348 documents and 
4374 relation instances. We extracted an instance 
for every pair of mentions in the same sentence 
which were separated by no more than two other 
mentions. The non-relation instances generated 
were about 8 times more than the relation instances.  
Preprocessing of the ACE documents: We used 
the Stanford parser6 for syntactic and dependency 
parsing. We used chunklink7  to derive chunking 
information from the Stanford parsing. Because 
some bnews documents are in lower case, we 
recover the case for the head of a mention if its 
type is NAM by making the first character into its 
upper case. This is for better matching between the 
words in ACE and the words in the unsupervised 
word clusters. 
We used the OpenNLP 8  maximum entropy 
(maxent) package as our machine learning tool. 
We choose to work with maxent because the 
training is fast and it has a good support for multi-
class classification. 
6.1 Baseline Performance 
Following previous work, we did 5-fold cross-
validation on the 348 documents with hand-
annotated entity mentions. Our results are shown in 
Table 5 which also lists the results of another three 
state-of-the-art feature based systems. For this and 
the following experiments, all the results were 
computed at the relation mention level. 
 
System P(%) R(%) F(%) 
Zhou et al (2007)9 78.2 63.4 70.1 
Zhao and Grishman (2005)10 69.2 71.5 70.4 
Our Baseline 73.4 67.7 70.4 
Jiang and Zhai (2007) 11 72.4 70.2 71.3 
 
Table 5: Performance comparison on the ACE 2004 
data over the 7 relation types. 
                                                        
6 http://nlp.stanford.edu/software/lex-parser.shtml 
7 http://ilk.uvt.nl/team/sabine/chunklink/README.html 
8 http://opennlp.sourceforge.net/ 
9 Zhou et al (2005) tested their system on the ACE 2003 data; 
Zhou et al (2007) tested their system on the ACE 2004 data. 
10  The paper gives a recall value of 70.5, which is not 
consistent with the given values of P and F. An examination of 
the correspondence in preparing this paper indicates that the 
correct recall value is 71.5. 
11 The result is from using the All features in Jiang and Zhai 
(2007). It is not quite clear from the paper that whether they 
used the 348 documents or the whole 2004 training data. 
526
Note that although all the 4 systems did 5-fold 
cross-validation on the ACE 2004 data, the 
detailed data partition might be different. Also, we 
were doing cross-validation at the document level 
which we believe was more natural than the 
instance level. Nonetheless, we believe our 
baseline system has achieved very competitive 
performance. 
6.2 The Effectiveness of Cluster Selection 
Methods 
We investigated the tradeoff between performance 
and training time of each proposed method in 
selecting clusters. In this experiment, we randomly 
selected 70 documents from the 348 documents as 
test data which roughly equaled the size of 1 fold 
in the baseline in Section 6.1. For the baseline in 
this section, all the rest of the documents were used 
as training data. For the semi-supervised system, 
70 percent of the rest of the documents were 
randomly selected as training data and 30 percent 
as development data. The set of prefix lengths that 
worked the best for the development set was 
chosen to select clusters. We only used the cluster 
feature HM_WC in this experiment.  
 
System F ? Training  Time (in minute) 
Baseline 70.70  1 
UA 71.19 +0.49 1.5 
PC3 71.65 +0.95 30 
PC4 71.72 +1.02 46 
IG3 71.65 +0.95 45 
IG4 71.68 +0.98 78 
ES3 71.66 +0.96 465 
ES4 71.60 +0.90 1678 
 
Table 6: The tradeoff between performance and training 
time of each method in selecting clusters. PC3 means 
using 3 prefixes with the PC method. ? in this paper 
means the difference between a system and the baseline. 
 
Table 6 shows that all the 4 proposed methods 
improved baseline performance, with UA as the 
fastest and ES as the slowest. It was interesting that 
ES did not always outperform the two statistical 
methods which might be because of its overfitting 
to the development set. In general, both PC and IG 
had good balances between performance and 
training time. There was no dramatic difference in 
performance between using 3 and 4 prefix lengths.  
For the rest of this paper, we will only use PC4 
as our method in selecting clusters. 
6.3 The Effectiveness of Cluster Features 
The baseline here is the same one used in Section 
6.1. For the semi-supervised system, each test fold 
was the same one used in the baseline and the other 
4 folds were further split into a training set and a 
development set in a ratio of 7:3 for selecting 
clusters. We first added the cluster features 
individually into the baseline and then added them 
incrementally according to the order specified in 
Table 4. 
 
System F ? 
1 Baseline 70.4  
2 1 + HM_WC 71.5 + 1.1 
3 1 + BagWM_WC 71.0 + 0.6 
4 1 + HC_WC 69.6 - 0.8 
5 1 + BagWC_WC 46.1 - 24.3 
6 2 + BagWM_WC 71.0 + 0.6 
7 6 + HC_WC 70.6 + 0.2 
8 7+ BagWC_WC 50.3 - 20.1 
 
Table 7: Performance 12  of the baseline and using 
different cluster features with PC4 over the 7 types.  
 
We found that adding clusters to the heads of the 
two mentions was the most effective way of 
introducing cluster features. Adding clusters to the 
words of the mentions can also help, though not as 
good as the heads. We were surprised that the 
heads of chunks in context did not help. This might 
be because ACE relations are usually short and the 
limited number of long relations is not sufficient in 
generalizing cluster features. Adding clusters to 
every word in context hurt the performance a lot. 
Because of the behavior of each individual feature, 
it was not surprising that adding them 
incrementally did not give more performance gain.  
For the rest of this paper, we will only use 
HM_WC as cluster features. 
6.4 The Impact of Training Size 
We studied the impact of training data size on 
cluster features as shown in Table 8. The test data 
was always the same as the 5-fold used in the 
baseline in Section 6.1. no matter the size of the 
training data. The training documents for the  
                                                        
12  All the improvements of F in Table 7, 8 and 9 were 
significant at confidence levels >= 95%. 
527
# docs F of Relation Classification F of Relation Detection 
Baseline PC4 (?) Prefix10(?) Baseline PC4(?) Prefix10(?) 
50 62.9 63.8(+ 0.9) 63.7(+0.8) 71.4 71.9(+ 0.5) 71.6(+0.2) 
75 62.8 64.6(+ 1.8) 63.9(+1.1) 71.5 72.3(+ 0.8) 72.5(+1.0) 
125 66.1 68.1(+ 2.0) 67.5(+1.4) 74.5 74.8(+ 0.3) 74.3(-0.2) 
175 67.8 69.7(+ 1.9) 69.5(+1.7) 75.2 75.5(+ 0.3) 75.2(0.0) 
225 68.9 70.1(+ 1.2) 69.6(+0.7) 75.6 75.9(+ 0.3) 75.3(-0.3) 
?280 70.4 71.5(+ 1.1) 70.7(+0.3) 76.4 76.9(+ 0.5) 76.3(-0.1) 
 
Table 8: Performance over the 7 relation types with different sizes of training data. Prefix10 uses the single prefix 
length 10 to generate word clusters as used by Chan and Roth (2010). 
 
Type P R F 
Baseline PC4 (?) Baseline PC4 (?) Baseline PC4 (?) 
EMP-ORG 75.4 77.2(+1.8) 79.8 81.5(+1.7) 77.6 79.3(+1.7) 
PHYS 73.2 71.2(-2.0) 61.6 60.2(-1.4) 66.9 65.3(-1.7) 
GPE-AFF 67.1 69.0(+1.9) 60.0 63.2(+3.2) 63.3 65.9(+2.6) 
PER-SOC 88.2 83.9(-4.3) 58.4 61.0(+2.6) 70.3 70.7(+0.4) 
DISC 79.4 80.6(+1.2) 42.9 46.0(+3.2) 55.7 58.6(+2.9) 
ART 87.9 96.9(+9.0) 63.0 67.4(+4.4) 73.4 79.3(+5.9) 
OTHER-AFF 70.6 80.0(+9.4) 41.4 41.4(0.0) 52.2 54.6(+2.4) 
 
Table 9: Performance of each individual relation type based on 5-fold cross-validation. 
 
current size setup were randomly selected and 
added to the previous size setup (if applicable). For 
example, we randomly selected another 25 
documents and added them to the previous 50 
documents to get 75 documents. We made sure 
that every document participated in this experiment. 
The training documents for each size setup were 
split into a real training set and a development set 
in a ratio of 7:3 for selecting clusters.  
There are some clear trends in Table 8. Under 
each training size, PC4 consistently outperformed 
the baseline and the system Prefix10 for relation 
classification. For PC4, the gain for classification 
was more pronounced than detection. The mixed 
detection results of Prefix10 indicated that only 
using a single prefix may not be stable.   
We did not observe the same trend in the 
reduction of annotation need with cluster-based 
features as in Koo et al (2008) for dependency 
parsing. PC4 with sizes 50, 125, 175 outperformed 
the baseline with sizes 75, 175, 225 respectively. 
But this was not the case when PC4 was tested 
with sizes 75 and 225.  This might due to the 
complexity of the relation extraction task. 
6.5 Analysis 
There were on average 69 cross-type errors in the 
baseline in Section 6.1 which were reduced to 56 
by using PC4. Table 9 showed that most of the 
improvements involved EMP-ORG, GPE-AFF, 
DISC, ART and OTHER-AFF. The performance 
gain for PER-SOC was not as pronounced as the 
other five types. The five types of relations are 
ambiguous as they share the same entity type GPE 
while the PER-SOC relation only holds between 
PER and PER. This reflects that word clusters can 
help to distinguish between ambiguous relation 
types. 
As mentioned earlier the gain of relation 
detection was not as pronounced as classification 
as shown in Table 8. The unbalanced distribution 
of relation instances and non-relation instances 
remains as an obstacle for pushing the performance 
of relation extraction to the next level. 
7 Conclusion and Future Work 
We have described a semi-supervised relation 
extraction system with large-scale word clustering. 
We have systematically explored the effectiveness 
of different cluster-based features. We have also 
demonstrated that the two proposed statistical 
methods are both effective and efficient in 
selecting clusters at an appropriate level of 
granularity through an extensive experimental 
study. 
528
Based on the experimental results, we plan to 
investigate additional ways to improve the 
performance of relation detection. Moreover, 
extending word clustering to phrase clustering (Lin 
and Wu, 2009) and pattern clustering (Sun and 
Grishman, 2010) is worth future investigation for 
relation extraction. 
References  
Rie K. Ando and Tong Zhang. 2005 A Framework for 
Learning Predictive Structures from Multiple Tasks 
and Unlabeled Data. Journal of Machine Learning 
Research, Vol 6:1817-1853. 
Elizabeth Boschee, Ralph Weischedel, and Alex 
Zamanian. 2005. Automatic information extraction. 
In Proceedings of the International Conference on 
Intelligence Analysis. 
Peter F. Brown, Vincent J. Della Pietra, Peter V. 
deSouza,  Jenifer C. Lai, and Robert L. Mercer. 1992. 
Class-based n-gram models of natural language. 
Computational Linguistics, 18(4):467?479. 
Razvan C. Bunescu and Raymond J. Mooney. 2005a. A 
shortest path dependency kenrel for relation 
extraction. In Proceedings of HLT/EMNLP. 
Razvan C. Bunescu and Raymond J. Mooney. 2005b. 
Subsequence kernels for relation extraction. In 
Proceedings of NIPS. 
Yee Seng Chan and Dan Roth. 2010. Exploiting 
background knowledge for relation extraction. In 
Proc. of COLING. 
Ralph Grishman, David Westbrook and Adam Meyers. 
2005. NYU?s English ACE 2005 System Description. 
ACE 2005 Evaluation Workshop.  
Earl B. Hunt, Philip J. Stone and Janet Marin. 1966. 
Experiments in Induction. New York: Academic 
Press, 1966. 
Jing Jiang and ChengXiang Zhai. 2007. A systematic 
exploration of the feature space for relation 
extraction. In Proceedings of HLT-NAACL-07.  
Nanda Kambhatla. 2004. Combining lexical, syntactic, 
and semantic features with maximum entropy models 
for information extraction. In Proceedings of ACL-04. 
Terry Koo, Xavier Carreras, and Michael Collins. 2008. 
Simple Semi-supervised Dependency Parsing. In 
Proceedings of ACL-08: HLT. 
Percy Liang. 2005. Semi-Supervised Learning for 
Natural Language. Master?s thesis, Massachusetts 
Institute of Technology. 
Dekang Lin and Xiaoyun Wu. 2009. Phrase Clustering 
for Discriminative Learning. In Proc. of ACL-09. 
Scott Miller, Heidi Fox, Lance Ramshaw, and Ralph 
Weischedel. 2000. A novel use of statistical parsing 
to extract information from text. In Proc. of NAACL. 
Scott Miller, Jethran Guinness and Alex Zamanian. 
2004. Name Tagging with Word Clusters and 
Discriminative Training. In Proc. of HLT-NAACL. 
Longhua Qian, Guodong Zhou, Qiaoming Zhu and 
Peide Qian. 2008. Exploiting constituent 
dependencies for tree kernel-based semantic relation 
extraction . In Proc. of COLING. 
John Ross Quinlan. 1986. Induction of decision trees. 
Machine Learning, 1(1), 81-106. 
Lev Ratinov and Dan Roth. 2009. Design challenges 
and misconceptions in named entity recognition. In 
Proceedings of CoNLL-09. 
Ang Sun. 2009. A Two-stage Bootstrapping Algorithm 
for Relation Extraction. In RANLP-09. 
Ang Sun and Ralph Grishman. 2010. Semi-supervised 
Semantic Pattern Discovery with Guidance from 
Unsupervised Pattern Clusters. In Proc. of COLING. 
Joseph Turian, Lev-Arie Ratinov, and Yoshua Bengio. 
2010. Word representations: A simple and general 
method for semi-supervised learning. In Proceedings 
of ACL. 
Dmitry Zelenko, Chinatsu Aone, and Anthony 
Richardella. 2003. Kernel methods for relation 
extraction. Journal of Machine Learning Research, 
3:1083?1106. 
Zhu Zhang. 2004. Weakly supervised relation 
classification for information extraction. In Proc. of 
CIKM?2004. 
Min Zhang, Jie Zhang, Jian Su, and GuoDong Zhou. 
2006. A composite kernel to extract relations 
between entities with both flat and structured features. 
In Proceedings of COLING-ACL-06. 
Shubin Zhao and Ralph Grishman. 2005. Extracting 
relations with integrated information using kernel 
methods. In Proceedings of ACL. 
Guodong Zhou, Jian Su, Jie Zhang, and Min Zhang. 
2005. Exploring various knowledge in relation 
extraction. In Proceedings of ACL-05. 
Guodong Zhou, Min Zhang, DongHong Ji, and 
QiaoMing Zhu. 2007. Tree kernel-based relation 
extraction with context-sensitive structured parse tree 
information. In Proceedings of EMNLPCoNLL-07. 
529
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1148?1158,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Knowledge Base Population:  Successful Approaches and Challenges 
 
Heng Ji Ralph Grishman 
Computer Science Department Computer Science Department 
Queens College and Graduate Center  
City University of New York 
New York University 
New York, NY 11367, USA New York, NY 10003, USA 
hengji@cs.qc.cuny.edu grishman@cs.nyu.edu 
 
 
 
  
 
Abstract 
In this paper we give an overview of the 
Knowledge Base Population (KBP) track at 
the 2010 Text Analysis Conference. The main 
goal of KBP is to promote research in discov-
ering facts about entities and augmenting a 
knowledge base (KB) with these facts. This is 
done through two tasks, Entity Linking ? link-
ing names in context to entities in the KB ? 
and Slot Filling ? adding information about an 
entity to the KB.  A large source collection of 
newswire and web documents is provided 
from which systems are to discover informa-
tion. Attributes (?slots?) derived from 
Wikipedia infoboxes are used to create the 
reference KB. In this paper we provide an 
overview of the techniques which can serve as 
a basis for a good KBP system, lay out the 
remaining challenges by comparison with tra-
ditional Information Extraction (IE) and Ques-
tion Answering (QA) tasks, and provide some 
suggestions to address these challenges. 
1 Introduction 
Traditional information extraction (IE) evaluations, 
such as the Message Understanding Conferences 
(MUC) and Automatic Content Extraction (ACE), 
assess the ability to extract information from indi-
vidual documents in isolation. In practice, how-
ever, we may need to gather information about a 
person or organization that is scattered among the 
documents of a large collection.  This requires the 
ability to identify the relevant documents and to 
integrate facts, possibly redundant, possibly com-
plementary, possibly in conflict, coming from 
these documents. Furthermore, we may want to use 
the extracted information to augment an existing 
data base.  This requires the ability to link indi-
viduals mentioned in a document, and information 
about these individuals, to entries in the data base. 
On the other hand, traditional Question Answering 
(QA) evaluations made limited efforts at disam-
biguating entities in queries (e.g. Pizzato et al, 
2006), and limited use of relation/event extraction 
in answer search (e.g. McNamee et al, 2008). 
  The Knowledge Base Population (KBP) shared 
task, conducted as part of the NIST Text Analysis 
Conference, aims to address and evaluate these 
capabilities, and bridge the IE and QA communi-
ties to promote research in discovering facts about 
entities and expanding a knowledge base with 
these facts. KBP is done through two separate sub-
tasks, Entity Linking and Slot Filling; in 2010, 23 
teams submitted results for one or both sub-tasks. 
A variety of approaches have been proposed to 
address both tasks with considerable success; nev-
ertheless, there are many aspects of the task that 
remain unclear. What are the fundamental tech-
niques used to achieve reasonable performance? 
What is the impact of each novel method? What 
types of problems are represented in the current 
KBP paradigm compared to traditional IE and QA? 
In which way have the current testbeds and evalua-
tion methodology affected our perception of the 
task difficulty? Have we reached a performance 
ceiling with current state of the art techniques?  
What are the remaining challenges and what are 
the possible ways to address these challenges? In 
this paper we aim to answer some of these ques-
tions based on our detailed analysis of evaluation 
results. 
1148
2 Task Definition and Evaluation Metrics 
This section will summarize the tasks conducted at 
KBP 2010. The overall goal of KBP is to auto-
matically identify salient and novel entities, link 
them to corresponding Knowledge Base (KB) en-
tries (if the linkage exists), then discover attributes 
about the entities, and finally expand the KB with 
any new attributes.  
  In the Entity Linking task, given a person (PER), 
organization (ORG) or geo-political entity (GPE, a 
location with a government) query that consists of 
a name string and a background document contain-
ing that name string, the system is required to pro-
vide the ID of the KB entry to which the name 
refers; or NIL if there is no such KB entry. The 
background document, drawn from the KBP cor-
pus, serves to disambiguate ambiguous name 
strings. 
In selecting among the KB entries, a system 
could make use of the Wikipedia text associated 
with each entry as well as the structured fields of 
each entry.  In addition, there was an optional task 
where the system could only make use of the struc-
tured fields; this was intended to be representative 
of applications where no backing text was avail-
able.  Each site could submit up to three runs with 
different parameters. 
  The goal of Slot Filling is to collect from the cor-
pus information regarding certain attributes of an 
entity, which may be a person or some type of or-
ganization. Each query in the Slot Filling task con-
sists of the name of the entity, its type (person or 
organization), a background document containing 
the name (again, to disambiguate the query in case 
there are multiple entities with the same name), its 
node ID (if the entity appears in the knowledge 
base), and the attributes which need not be filled.  
Attributes are excluded if they are already filled in 
the reference data base and can only take on a sin-
gle value. Along with each slot fill, the system 
must provide the ID of a document which supports 
the correctness of this fill.  If the corpus does not 
provide any information for a given attribute, the 
system should generate a NIL response (and no 
document ID). KBP2010 defined 26 types of at-
tributes for persons (such as the age, birthplace, 
spouse, children, job title, and employing organiza-
tion) and 16 types of attributes for organizations 
(such as the top employees, the founder, the year 
founded, the headquarters location, and subsidiar-
ies).  Some of these attributes are specified as only 
taking a single value (e.g., birthplace), while some 
can take multiple values (e.g., top employees). 
  The reference KB includes hundreds of thousands 
of entities based on articles from an October 2008 
dump of English Wikipedia which includes 
818,741 nodes. The source collection includes 
1,286,609 newswire documents, 490,596 web 
documents and hundreds of transcribed spoken 
documents. 
  To score Entity Linking, we take each query and 
check whether the KB node ID (or NIL) returned 
by a system is correct or not.  Then we compute 
the Micro-averaged Accuracy, computed across all 
queries. 
  To score Slot Filling, we first pool all the system 
responses (as is done for information retrieval 
evaluations) together with a set of manually-
prepared slot fills.  These responses are then as-
sessed by hand.  Equivalent answers (such as ?Bill 
Clinton? and ?William Jefferson Clinton?) are 
grouped into equivalence classes.  Each system 
response is rated as correct, wrong, or redundant (a 
response which is equivalent to another response 
for the same slot or an entry already in the knowl-
edge base). Given these judgments, we count 
Correct = total number of non-NIL system output 
slots judged correct 
System = total number of non-NIL system output 
slots 
Reference = number of single-valued slots with a 
correct non-NIL response + 
 number of equivalence classes for all list-
valued slots 
Recall = Correct / Reference 
Precision = Correct / System 
F-Measure = (2 ? Recall ? Precision) / (Recall + 
Precision) 
3 Entity Linking: What Works 
In Entity Linking, we saw a general improvement 
in performance over last year?s results ? the top 
system achieved 85.78% micro-averaged accuracy. 
When measured against a benchmark based on in-
ter-annotator agreement, two systems? perform-
ance approached and one system exceeded the 
benchmark on person entities.  
3.1 A General Architecture 
A typical entity linking system architecture is de-
picted in Figure 1. 
 
1149
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 1. General Entity Linking  
System Architecture 
It includes three steps: (1) query expansion ? ex-
pand the query into a richer set of forms using 
Wikipedia structure mining or coreference resolu-
tion in the background document. (2) candidate 
generation ? finding all possible KB entries that a 
query might link to; (3) candidate ranking ? rank 
the probabilities of all candidates and NIL answer.  
Table 1 summarizes the systems which ex-
ploited different approaches at each step. In the 
following subsections we will highlight the new 
and effective techniques used in entity linking. 
3.2 Wikipedia Structure Mining 
Wikipedia articles are peppered with structured 
information and hyperlinks to other (on average 
25) articles (Medelyan et al, 2009). Such informa-
tion provides additional sources for entity linking: 
(1). Query Expansion: For example, WebTLab 
(Fernandez et al, 2010) used Wikipedia link struc-
ture (source, anchors, redirects and disambigua-
tion) to extend the KB and compute entity co-
occurrence estimates. Many other teams including 
CUNY and Siel used redirect pages and disam-
biguation pages for query expansion. The Siel team 
also exploited bold texts from first paragraphs be-
cause they often contain nicknames, alias names 
and full names.  
 
Methods  System Examples System 
Ranking 
Range 
Wikipedia Hyperlink Mining  CUNY (Chen et al, 2010), NUSchime (Zhang et al, 
2010), Siel (Bysani et al, 2010), SMU-SIS (Gottipati et 
al., 2010), USFD (Yu et al, 2010), WebTLab team (Fer-
nandez et al, 2010) 
[2, 15]  
Query 
Expansion 
 
 Source document coreference 
resolution 
CUNY (Chen et al, 2010) 9 
 
Document semantic analysis 
and context modeling 
ARPANI (Thomas et al, 2010), CUNY (Chen et al, 
2010), LCC (Lehmann et al, 2010) 
[1,14] Candidate 
Generation 
 IR CUNY (Chen et al, 2010), Budapestacad (Nemeskey et 
al., 2010), USFD (Yu et al, 2010) 
[9, 16] 
Unsupervised Similarity 
Computation (e.g. VSM) 
CUNY (Chen et al, 2010), SMU-SIS (Gottipati et al, 
2010), USFD (Yu et al, 2010) 
[9, 14] 
Supervised  
Classification 
LCC (Lehmann et al, 2010), NUSchime (Zhang et al, 
2010), Stanford-UBC (Chang et al, 2010),  HLTCOE 
(McNamee, 2010), UC3M (Pablo-Sanchez et al, 2010) 
[1, 10] 
Rule-based LCC (Lehmann et al, 2010), BuptPris (Gao et al, 2010) [1, 8] 
Global Graph-based Ranking CMCRC (Radford et al, 2010) 3 
Candidate 
Ranking 
IR Budapestacad (Nemeskey et al, 2010) 16 
 
Table 1. Entity Linking Method Comparison
Query 
Query Expansion 
Wiki 
hyperlink 
mining
Source doc 
Coreference 
Resolution
KB Node Candidate Generation 
KB Node Candidate Ranking 
Wiki KB 
+Texts 
unsupervised 
similarity  
computation 
supervised 
classifica-
tion 
IR 
Answer 
IR
Document semantic analysis
Graph
-based 
1150
(2). Candidate Ranking: Stanford-UBC used 
Wikipedia hyperlinks (clarification, disambigua-
tion, title) for query re-mapping, and encoded lexi-
cal and part-of-speech features  from Wikipedia 
articles containing hyperlinks to the queries to train 
a supervised classifier; they reported a significant 
improvement on micro-averaged accuracy, from 
74.85% to 82.15%.  In fact, when the mined attrib-
utes become rich enough, they can be used as an 
expanded query and sent into an information re-
trieval engine in order to obtain the relevant source 
documents. Budapestacad team (Nemeskey et al, 
2010) adopted this strategy. 
3.3 Ranking Approach Comparison 
The ranking approaches exploited in the KBP2010 
entity linking systems can be generally categorized 
into four types:  
(1). Unsupervised or weakly-supervised learning, 
in which annotated data is minimally used to tune 
thresholds and parameters. The similarity measure 
is largely based on the unlabeled contexts. 
(2). Supervised learning, in which a pair of entity 
and KB node is modeled as an instance for classi-
fication. Such a classifier can be learned from the 
annotated training data based on many different 
features. 
(3). Graph-based ranking, in which context entities 
are taken into account in order to reach a global 
optimized solution together with the query entity. 
(4). IR (Information Retrieval) approach, in which 
the entire background source document is consid-
ered as a single query to retrieve the most relevant 
Wikipedia article. 
The first question we will investigate is how 
much higher performance can be achieved by us-
ing supervised learning? Among the 16 entity link-
ing systems which participated in the regular 
evaluation, LCC (Lehmann et al, 2010), HLTCOE 
(McNamee, 2010), Stanford-UBC (Chang et al, 
2010), NUSchime (Zhang et al, 2010) and UC3M 
(Pablo-Sanchez et al, 2010) have explicitly used 
supervised classification based on many lexical 
and name tagging features, and most of them are 
ranked in top 6 in the evaluation. Therefore we can 
conclude that supervised learning normally leads to 
a reasonably good performance. However, a high-
performing entity linking system can also be im-
plemented in an unsupervised fashion by exploit-
ing effective characteristics and algorithms, as we 
will discuss in the next sections. 
3.4 Semantic Relation Features 
Almost all entity linking systems have used seman-
tic relations as features (e.g. BuptPris (Gao et al, 
2010), CUNY (Chen et al, 2010) and HLTCOE).  
The semantic features used in the BuptPris system 
include name tagging, infoboxes, synonyms, vari-
ants and abbreviations. In the CUNY system, the 
semantic features are automatically extracted from 
their slot filling system. The results are summa-
rized in Table 2, showing the gains over a baseline 
system (using only Wikipedia title features in the 
case of BuptPris, using tf-idf weighted word fea-
tures for CUNY). As we can see, except for person 
entities in the BuptPris system, all types of entities 
have obtained significant improvement by using 
semantic features in entity linking. 
 
System Using Se-
mantic 
Features 
PER ORG GPE Overall 
No 83.89 59.47 33.38 58.93 BuptPris 
 Yes 79.09 74.13 66.62 73.29 
No 84.55 63.07 57.54 59.91 CUNY 
 
 Yes 92.81 65.73 84.10 69.29 
 
Table 2. Impact of Semantic Features on Entity 
Linking (Micro-Averaged Accuracy %)  
3.5 Context Inference 
In the current setting of KBP, a set of target enti-
ties is provided to each system in order to simplify 
the task and its evaluation, because it?s not feasible 
to require a system to generate answers for all pos-
sible entities in the entire source collection. How-
ever, ideally a fully-automatic KBP system should 
be able to automatically discover novel entities 
(?queries?) which have no KB entry or few slot 
fills in the KB, extract their attributes, and conduct 
global reasoning over these attributes in order to 
generate the final output. At the very least, due to 
the semantic coherence principle (McNamara, 
2001), the information of an entity depends on the 
information of other entities. For example, the 
WebTLab team and the CMCRC team extracted all 
entities in the context of a given query, and disam-
biguated all entities at the same time using a Pag-
eRank-like algorithm (Page et al, 1998) or a 
Graph-based Re-ranking algorithm. The SMU-SIS 
team (Gottipati and Jiang, 2010) re-formulated 
queries using contexts. The LCC team modeled 
1151
contexts using Wikipedia page concepts, and com-
puted linkability scores iteratively. Consistent im-
provements were reported by the WebTLab system 
(from 63.64% to 66.58%). 
4 Entity Linking: Remaining Challenges 
4.1 Comparison with Traditional Cross-
document Coreference Resolution 
Part of the entity linking task can be modeled as a 
cross-document entity resolution problem which 
includes two principal challenges: the same entity 
can be referred to by more than one name string 
and the same name string can refer to more than 
one entity. The research on cross-document entity 
coreference resolution can be traced back to the 
Web People Search task (Artiles et al, 2007) and 
ACE2008 (e.g. Baron and Freedman, 2008).   
Compared to WePS and ACE, KBP requires link-
ing an entity mention in a source document to a 
knowledge base with or without Wikipedia arti-
cles. Therefore sometimes the linking decisions 
heavily rely on entity profile comparison with 
Wikipedia infoboxes. In addition, KBP introduced 
GPE entity disambiguation. In source documents, 
especially in web data, usually few explicit attrib-
utes about GPE entities are provided, so an entity 
linking system also needs to conduct external 
knowledge discovery from background related 
documents or hyperlink mining. 
4.2 Analysis of Difficult Queries 
There are 2250 queries in the Entity Linking 
evaluation; for 58 of them at most 5 (out of the 46) 
system runs produced correct answers. Most of 
these queries have corresponding KB entries. For 
19 queries all 46 systems produced different results 
from the answer key. Interestingly, the systems 
which perform well on the difficult queries are not 
necessarily those achieved top overall performance 
? they were ranked 13rd, 6th, 5th, 12nd, 10th, and 16th 
respectively for overall queries. 11 queries are 
highly ambiguous city names which can exist in 
many states or countries (e.g. ?Chester?), or refer 
to person or organization entities. From these most 
difficult queries we observed the following chal-
lenges and possible solutions. 
 
 
 
? Require deep understanding of context enti-
ties for GPE queries 
 
In a document where the query entity is not a cen-
tral topic, the author often assumes that the readers 
have enough background knowledge (?anchor? lo-
cation from the news release information, world 
knowledge or related documents) about these enti-
ties.  For 6 queries, a system would need to inter-
pret or extract attributes for their context entities. 
For example, in the following passage: 
 
?There are also photos of Jake on IHJ in 
Brentwood, still looking somber? 
 
in order to identify that the query ?Brentwood? is 
located in California, a system will need to under-
stand that ?IHJ? is ?I heart Jake community? and 
that the ?Jake? referred to lives in Los Angeles, of 
which Brentwood is a part. 
In the following example, a system is required to 
capture the knowledge that ?Chinese Christian 
man? normally appears in ?China? or there is a 
?Mission School? in ?Canton, China? in order to 
link the query ?Canton? to the correct KB entry. 
This is a very difficult query also because the more 
common way of spelling ?Canton? in China is 
?Guangdong?. 
 
?and was from a Mission School in Canton, ? 
but for the energetic efforts of this Chinese Chris-
tian man and the Refuge Matron? 
 
? Require external hyperlink analysis 
 
Some queries require a system to conduct detailed 
analysis on the hyperlinks in the source document 
or the Wikipedia document. For example, in the 
source document ??Filed under: Falcons 
<http://sports.aol.com/fanhouse/category/atlanta-
falcons/>?, a system will need to analyze the 
document which this hyperlink refers to. Such 
cases might require new query reformulation and 
cross-document aggregation techniques, which are 
both beyond traditional entity disambiguation 
paradigms. 
 
1152
? Require Entity Salience Ranking 
 
Some of these queries represent salient entities and 
so using web popularity rank (e.g. ranking/hit 
counts of Wikipedia pages from search engine) can 
yield correct answers in most cases (Bysani et al, 
2010; Dredze et al, 2010). In fact we found that a 
na?ve candidate ranking approach based on web 
popularity alone can achieve 71% micro-averaged 
accuracy, which is better than 24 system runs in 
KBP2010.   
Since the web information is used as a black box 
(including query expansion and query log analysis) 
which changes over time, it?s more difficult to du-
plicate research results. However, gazetteers with 
entities ranked by salience or major entities 
marked are worth encoding as additional features.   
For example, in the following passages: 
 
... Tritschler brothers competed in gymnastics at the 
1904 Games in St Louis 104 years ago? and ?A char-
tered airliner carrying Democratic White House hope-
ful Barack Obama was forced to make an unscheduled 
landing on Monday in St. Louis after its flight crew 
detected mechanical problems? 
 
although there is little background information to 
decide where the query ?St Louis? is located, a sys-
tem can rely on such a major city list to generate 
the correct linking. Similarly, if a system knows 
that ?Georgia Institute of Technology? has higher 
salience than ?Georgian Technical University?, it 
can correctly link a query ?Georgia Tech? in most 
cases. 
5 Slot Filling: What Works 
5.1 A General Architecture 
The slot-filling task is a hybrid of traditional IE (a 
fixed set of relations) and QA (responding to a 
query, generating a unified response from a large 
collection).  Most participants met this challenge 
through a hybrid system which combined aspects 
of QA (passage retrieval) and IE (answer extrac-
tion).  A few used off-the-shelf QA, either bypass-
ing question analysis or (if QA was used as a 
?black box?) creating a set of questions corre-
sponding to each slot. 
The basic system structure (Figure 2) involved 
three phases:  document/passage retrieval (retriev-
ing passages involving the queried entity), answer 
extraction (getting specific answers from the re-
trieved passages), and answer combination (merg-
ing and selecting among the answers extracted). 
   The solutions adopted for answer extraction re-
flected the range of current IE methods as well as 
QA answer extraction techniques (see Table 3). 
Most systems used one main pipeline, while 
CUNY and BuptPris adopted a hybrid approach of 
combining multiple approaches. 
One particular challenge for KBP, in compari-
son with earlier IE tasks, was the paucity of train-
ing data. The official training data, linked to 
specific text from specific documents, consisted of 
responses to 100 queries; the participants jointly 
prepared responses to another 50.  So traditional 
supervised learning, based directly on the training 
data, would provide limited coverage.  Coverage 
could be improved by using the training data as 
seeds for a bootstrapping procedure.     
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 2. General Slot Filling System Architecture 
 
IE  
(Distant Learning/ 
Bootstrapping) 
Query 
Source  
Collection 
IR Document  Level 
IR, QA Sentence/Passage Level 
Pattern
Answer  
Level Classifier
QA 
Training 
Data/ 
External 
KB
Rules 
Answers 
Query  
Expansion
Knowledge 
Base
Redundancy 
Removal 
1153
Methods System Examples 
Distant Learning (large 
seed, one iteration) 
CUNY (Chen et al, 2010)  
Pattern 
Learning Bootstrapping (small 
seed, multiple iterations) 
NYU (Grishman and Min, 2010) 
Distant Supervision Budapestacad (Nemeskey et al, 2010), lsv (Chrupala et al, 
2010), Stanford (Surdeanu et al, 2010), UBC (Intxaurrondo 
et al, 2010) 
 
 
Trained 
IE 
 
Supervised  
Classifier 
Trained from KBP train-
ing data and other re-
lated tasks 
BuptPris (Gao et al, 2010), CUNY (Chen et al, 2010), IBM 
(Castelli et al, 2010), ICL (Song et al, 2010),  LCC 
(Lehmann et al, 2010), lsv (Chrupala et al, 2010), Siel 
(Bysani et al, 2010) 
QA CUNY (Chen et al, 2010), iirg (Byrne and Dunnion, 2010) 
Hand-coded Heuristic Rules BuptPris (Gao et al, 2010), USFD (Yu et al, 2010) 
 
  Table 3. Slot Filling Answer Extraction Method Comparison 
 
  On the other hand, there were a lot of 'facts' avail-
able ? pairs of entities bearing a relationship corre-
sponding closely to the KBP relations ? in the form 
of filled Wikipedia infoboxes.  These could be 
used for various forms of indirect or distant learn-
ing, where instances in a large corpus of such pairs 
are taken as (positive) training instances.  How-
ever, such instances are noisy ? if a pair of entities 
participates in more than one relation, the found 
instance may not be an example of the intended 
relation ? and so some filtering of the instances or 
resulting patterns may be needed.  Several sites 
used such distant supervision to acquire patterns or 
train classifiers, in some cases combined with di-
rect supervision using the training data (Chrupala 
et al, 2010). 
  Several groups used and extended existing rela-
tion extraction systems, and then mapped the re-
sults into KBP slots.  Mapping the ACE relations 
and events by themselves provided limited cover-
age  (34% of slot fills in the training data), but was 
helpful when combined with other sources (e.g. 
CUNY).  Groups with more extensive existing ex-
traction systems could primarily build on these 
(e.g. LCC, IBM). 
   For example, IBM (Castelli et al, 2010) ex-
tended their mention detection component to cover 
36 entity types which include many non-ACE 
types; and added new relation types between enti-
ties and event anchors. LCC and CUNY applied 
active learning techniques to cover non-ACE types 
of entities, such as ?origin?, ?religion?, ?title?, 
?charge?, ?web-site? and ?cause-of-death?, and 
effectively develop lexicons to filter spurious an-
swers.  
  Top systems also benefited from customizing and 
tightly integrating their recently enhanced extrac-
tion techniques into KBP.  For example, IBM, 
NYU (Grishman and Min, 2010) and CUNY ex-
ploited entity coreference in pattern learning and 
reasoning. It is also notable that traditional extrac-
tion components trained from newswire data suffer 
from noise in web data. In order to address this 
problem, IBM applied their new robust mention 
detection techniques for noisy inputs (Florian et al, 
2010); CUNY developed a component to recover 
structured forms such as tables in web data auto-
matically and filter spurious answers. 
5.2 Use of External Knowledge Base 
Many instance-centered knowledge bases that have 
harvested Wikipedia are proliferating on the se-
mantic web.  The most well known are probably 
the Wikipedia derived resources, including DBpe-
dia (Auer 2007), Freebase (Bollacker 2008) and 
YAGO (Suchanek et al, 2007) and Linked Open 
Data (http://data.nytimes.com/). The main motiva-
tion of the KBP program is to automatically distill 
information from news and web unstructured data 
instead of manually constructed knowledge bases, 
but these existing knowledge bases can provide a 
large number of seed tuples to bootstrap slot filling 
or guide distant learning.  
Such resources can also be used in a more direct 
way. For example, CUNY exploited Freebase and 
LCC exploited DBpedia as fact validation in slot 
filling. However, most of these resources are 
manually created from single data modalities and 
only cover well-known entities. For example, 
while Freebase contains 116 million instances of 
1154
7,300 relations for 9 million entities, it only covers 
48% of the slot types and 5% of the slot answers in 
KBP2010 evaluation data. Therefore, both CUNY 
and LCC observed limited gains from the answer 
validation approach from Freebase. Both systems 
gained about 1% improvement in recall with a 
slight loss in precision. 
5.3 Cross-Slot and Cross-Query Reasoning 
Slot Filling can also benefit from extracting re-
vertible queries from the context of any target 
query, and conducting global ranking or reasoning 
to refine the results. CUNY and IBM developed 
recursive reasoning components to refine extrac-
tion results. For a given query, if there are no other 
related answer candidates available, they built "re-
vertible? queries in the contexts, similar to (Prager 
et al, 2006), to enrich the inference process itera-
tively. For example, if a is extracted as the answer 
for org:subsidiaries of the query q,  we can con-
sider a as a new revertible query and verify that a 
org:parents answer of a is q. Both systems signifi-
cantly benefited from recursive reasoning (CUNY 
F-measure on training data was enhanced from 
33.57% to 35.29% and IBM F-measure was en-
hanced from 26% to 34.83%). 
6 Slot Filling: Remaining Challenges 
Slot filling remains a very challenging task; only 
one system exceeded 30% F-measure on the 2010 
evaluation.  During the 2010 evaluation data anno-
tation/adjudication process, an initial answer key 
annotation was created by a manual search of the 
corpus (resulting in 797 instances), and then an 
independent adjudication pass was applied to as-
sess these annotations together with pooled system 
responses. The Precision, Recall and F-measure for 
the initial human annotation are only about 70%, 
54% and 61% respectively. While we believe the 
annotation consistency can be improved, in part by 
refinement of the annotation guidelines, this does 
place a limit on system performance. 
   Most of the shortfall in system performance re-
flects inadequacies in the answer extraction stage, 
reflecting limitations in the current state-of-the-art 
in information extraction.  An analysis of the 2010 
training data shows that cross-sentence coreference 
and some types of inference are critical to slot fill-
ing.  In only 60.4% of the cases do the entity name 
and slot fill appear together in the same sentence, 
so a system which processes sentences in isolation 
is severely limited in its performance.  22.8% of 
the cases require cross-sentence (identity) corefer-
ence; 15% require some cross-sentence inference 
and 1.8% require cross-slot inference. The infer-
ences include: 
 
? Non-identity coreference: in the following pas-
sage: ?Lahoud is married to an Armenian and the 
couple have three children. Eldest son Emile Emile 
Lahoud was a member of parliament between 2000 
and 2005.? the semantic relation between ?chil-
dren? and ?son? needs to be exploited in order 
to generate ?Emile Emile Lahoud? as the 
per:children of the query entity ?Lahoud?; 
 
? Cross-slot inference based on revertible que-
ries, propagation links or even world knowl-
edge to capture some of the most challenging 
cases. In the KBP slot filling task, slots are of-
ten dependent on each other, so we can im-
prove the results by improving the ?coherence? 
of the story (i.e. consistency among all gener-
ated answers (query profiles)). In the following 
example: 
?People Magazine has confirmed that actress Julia 
Roberts has given birth to her third child a boy 
named Henry Daniel Moder. Henry was born 
Monday in Los Angeles and weighed 8? lbs. Rob-
erts, 39, and husband Danny Moder, 38, are al-
ready parents to twins Hazel and Phinnaeus who 
were born in November 2006.?  
 
the following reasoning rules are needed to 
generate the answer ?Henry Daniel Moder? as 
per:children of ?Danny Moder?: 
 ChildOf (?Henry Daniel Moder?, ?Julia Roberts?) 
    ?  Coreferential (?Julia Roberts?, ?Roberts?)  
   ?  SpouseOf (?Roberts?, ?Danny Moder?) ?  
ChildOf (?Henry Daniel Moder?, ?Danny Moder?) 
 
    KBP Slot Filling is similar to ACE Relation Ex-
traction, which has been extensively studied for the 
past 7 years. However, the amount of training data 
is much smaller, forcing sites to adjust their train-
ing strategies. Also, some of the constraints of 
ACE relation mention extraction ? notably, that 
both arguments are present in the same sentence ? 
are not present, making the role of coreference and 
cross-sentence inference more critical. 
The role of coreference and inference as limiting 
factors, while generally recognized, is emphasized 
1155
by examining the 163 slot values that the human 
annotators filled but that none of the systems were 
able to get correct.  Many of these difficult cases 
involve a combination of problems, but we esti-
mate that at least 25% of the examples involve 
coreference which is beyond current system capa-
bilities, such as nominal anaphors: 
?Alexandra Burke is out with the video for her second 
single ? taken from the British artist?s debut album? 
?a woman charged with running a prostitution ring ? 
her business, Pamela Martin and Associates? 
  (underlined phrases are coreferential).    
While the types of inferences which may be re-
quired is open-ended, certain types come up re-
peatedly, reflecting the types of slots to be filled:  
systems would benefit from specialists which are 
able to reason about times, locations, family rela-
tionships, and employment relationships. 
7 Toward System Combination 
The increasing number of diverse approaches 
based on different resources provide new opportu-
nities for both entity linking and slot filling tasks to 
benefit from system combination.  
  The NUSchime entity linking system trained a 
SVM based re-scoring model to combine two indi-
vidual pipelines. Only one feature based on confi-
dence values from the pipelines was used for re-
scoring. The micro-averaged accuracy was en-
hanced from 79.29%/79.07% to 79.38% after 
combination. We also applied a voting approach on 
the top 9 entity linking systems and found that all 
combination orders achieved significant gains, 
with the highest absolute improvement of 4.7% in 
micro-averaged accuracy over the top entity link-
ing system. 
The CUNY slot filling system trained a maxi-
mum-entropy-based re-ranking model to combine 
three individual pipelines, based on various global 
features including voting and dependency rela-
tions. Significant gain in F-measure was achieved:  
from 17.9%, 27.7% and 21.0% (on training data) to 
34.3% after combination. When we applied the 
same re-ranking approach to the slot filling sys-
tems which were ranked from the 2nd to 14th, we 
achieved 4.3% higher F-score than the best of 
these systems. 
8 Conclusion 
Compared to traditional IE and QA tasks, KBP has 
raised some interesting and important research is-
sues: It places more emphasis on cross-document 
entity resolution which received limited effort in 
ACE; it forces systems to deal with redundant and 
conflicting answers across large corpora; it links 
the facts in text to a knowledge base so that NLP 
and data mining/database communities have a bet-
ter chance to collaborate; it provides opportunities 
to develop novel training methods such as distant 
(and noisy) supervision through Infoboxes (Sur-
deanu et al, 2010; Chen et al, 2010).  
  In this paper, we provided detailed analysis of the 
reasons which have made KBP a more challenging 
task, shared our observations and lessons learned 
from the evaluation, and suggested some possible 
research directions to address these challenges 
which may be helpful for current and new partici-
pants, or IE and QA researchers in general. 
Acknowledgements 
The first author was supported by the U.S. Army Re-
search Laboratory under Cooperative Agreement Num-
ber W911NF-09-2-0053, the U.S. NSF CAREER 
Award under Grant IIS-0953149 and PSC-CUNY Re-
search Program. The views and conclusions contained 
in this document are those of the authors and should not 
be interpreted as representing the official policies, either 
expressed or implied, of the Army Research Laboratory 
or the U.S. Government. The U.S. Government is au-
thorized to reproduce and distribute reprints for Gov-
ernment purposes notwithstanding any copyright 
notation hereon. 
References  
Javier Artiles, Julio Gonzalo and Satoshi Sekine. 2007. 
The SemEval-2007 WePS Evaluation: Establishing a 
benchmark for the Web People Search Task. Proc. 
the 4th International Workshop on Semantic Evalua-
tions (Semeval-2007). 
S. Auer, C. Bizer, G. Kobilarov, J. Lehmann and Z. Ives. 
2007. DBpedia: A nucleus for a web of open data. 
Proc. 6th International Semantic Web Conference. 
K. Balog, L. Azzopardi, M. de Rijke. 2008. Personal 
Name Resolution of Web People Search. Proc. 
WWW2008 Workshop: NLP Challenges in the Infor-
mation Explosion Era (NLPIX 2008).  
 
1156
Alex Baron and Marjorie Freedman. 2008. Who is Who 
and What is What: Experiments in Cross-Document 
Co-Reference. Proc. EMNLP 2008. 
K. Bollacker, R. Cook, and P. Tufts. 2007. Freebase: A 
Shared Database of Structured General Human 
Knowledge. Proc. National Conference on Artificial 
Intelligence (Volume 2). 
Lorna Byrne and John Dunnion. 2010. UCD IIRG at 
TAC 2010. Proc. TAC 2010 Workshop. 
Praveen Bysani, Kranthi Reddy, Vijay Bharath Reddy, 
Sudheer Kovelamudi, Prasad Pingali and Vasudeva 
Varma. 2010. IIIT Hyderabad in Guided Summariza-
tion and Knowledge Base Population. Proc. TAC 
2010 Workshop. 
Vittorio Castelli, Radu Florian and Ding-jung Han. 
2010. Slot Filling through Statistical Processing and 
Inference Rules. Proc. TAC 2010 Workshop. 
Angel X. Chang, Valentin I. Spitkovsky, Eric Yeh, 
Eneko Agirre and Christopher D. Manning. 2010. 
Stanford-UBC Entity Linking at TAC-KBP. Proc. 
TAC 2010 Workshop. 
Zheng Chen, Suzanne Tamang, Adam Lee, Xiang Li, 
Wen-Pin Lin, Matthew Snover, Javier Artiles, 
Marissa Passantino and Heng Ji. 2010. CUNY-
BLENDER TAC-KBP2010 Entity Linking and Slot 
Filling System Description. Proc. TAC 2010 Work-
shop. 
Grzegorz Chrupala, Saeedeh Momtazi, Michael Wie-
gand, Stefan Kazalski, Fang Xu, Benjamin Roth, Al-
exandra Balahur, Dietrick Klakow. Saarland 
University Spoken Language Systems at the Slot Fill-
ing Task of TAC KBP 2010. Proc. TAC 2010 Work-
shop. 
Mark Dredze, Paul McNamee, Delip Rao, Adam Gerber 
and Tim Finin. 2010. Entity Disambiguation for 
Knowledge Base Population. Proc. COLING 2010. 
Norberto Fernandez, Jesus A. Fisteus, Luis Sanchez and 
Eduardo Martin. 2010. WebTLab: A Cooccurence-
based Approach to KBP 2010 Entity-Linking Task. 
Proc. TAC 2010 Workshop. 
Radu Florian, John F. Pitrelli, Salim Roukos and Imed 
Zitouni. 2010. Improving Mention Detection Robust-
ness to Noisy Input. Proc. EMNLP2010. 
Sanyuan Gao, Yichao Cai, Si Li, Zongyu Zhang, Jingyi 
Guan, Yan Li, Hao Zhang, Weiran Xu and Jun Guo. 
2010. PRIS at TAC2010 KBP Track. Proc. TAC 
2010 Workshop. 
Swapna Gottipati and Jing Jiang. 2010. SMU-SIS at 
TAC 2010 ? KBP Track Entity Linking. Proc. TAC 
2010 Workshop. 
Ralph Grishman and Bonan Min. 2010. New York Uni-
versity KBP 2010 Slot-Filling System. Proc. TAC 
2010 Workshop. 
Ander Intxaurrondo, Oier Lopez de Lacalle and Eneko 
Agirre. 2010. UBC at Slot Filling TAC-KBP2010. 
Proc. TAC 2010 Workshop. 
John Lehmann, Sean Monahan, Luke Nezda, Arnold 
Jung and Ying Shi. 2010. LCC Approaches to 
Knowledge Base Population at TAC 2010. Proc. 
TAC 2010 Workshop. 
Paul McNamee and Hoa Dang. 2009. Overview of the 
TAC 2009 Knowledge Base Population Track. Proc. 
TAC 2009 Workshop. 
Paul McNamee, Hoa Trang Dang, Heather Simpson, 
Patrick Schone and Stephanie M. Strassel. 2010. An 
Evaluation of Technologies for Knowledge Base 
Population. Proc. LREC2010. 
Paul McNamee, Rion Snow, Patrick Schone and James 
Mayfield. 2008. Learning Named Entity Hyponyms 
for Question Answering. Proc. IJCNLP2008. 
Paul McNamee. 2010. HLTCOE Efforts in Entity Link-
ing at TAC KBP 2010. Proc. TAC 2010 Workshop. 
Danielle S McNamara. 2001. Reading both High-
coherence and Low-coherence Texts: Effects of Text 
Sequence and Prior Knowledge. Canadian Journal of 
Experimental Psychology. 
Olena Medelyan, Catherine Legg, David Milne and Ian 
H. Witten. 2009. Mining Meaning from Wikipedia. 
International Journal of Human-Computer Studies 
archive. Volume 67 , Issue 9. 
David Nemeskey, Gabor Recski, Attila Zseder and An-
dras Kornai. 2010. BUDAPESTACAD at TAC 2010. 
Proc. TAC 2010 Workshop. 
Cesar de Pablo-Sanchez, Juan Perea and Paloma Marti-
nez. 2010. Combining Similarities with Regression 
based Classifiers for Entity Linking at TAC 2010. 
Proc. TAC 2010 Workshop. 
Lawrence Page, Sergey Brin, Rajeev Motwani and 
Terry Winograd. 1998. The PageRank Citation Rank-
ing: Bringing Order to the Web. Proc. the 7th Interna-
tional World Wide Web Conference. 
Luiz Augusto Pizzato, Diego Molla and Cecile Paris. 
2006. Pseudo Relevance Feedback Using Named En-
tities for Question Answering. Proc. the Australasian 
Language Technology Workshop 2006. 
J. Prager, P. Duboue, and J. Chu-Carroll. 2006. Improv-
ing QA Accuracy by Question Inversion. Proc. ACL-
COLING 2006. 
1157
Will Radford, Ben Hachey, Joel Nothman, Matthew 
Honnibal and James R. Curran. 2010. CMCRC at 
TAC10: Document-level Entity Linking with Graph-
based Re-ranking. Proc. TAC 2010 Workshop. 
Yang Song, Zhengyan He and Houfeng Wang. 2010. 
ICL_KBP Approaches to Knowledge Base Popula-
tion at TAC2010. Proc. TAC 2010 Workshop. 
F. M. Suchanek, G. Kasneci, and G. Weikum. 2007. 
Yago: A Core of Semantic Knowledge. Proc. 16th 
International World Wide Web Conference. 
Mihai Surdeanu, David McClosky, Julie Tibshirani, 
John Bauer, Angel X. Chang, Valentin I. Spitkovsky, 
Christopher D. Manning. 2010. A Simple Distant 
Supervision Approach for the TAC-KBP Slot Filling 
Task. Proc. TAC 2010 Workshop. 
Ani Thomas, Arpana Rawai, M K Kowar, Sanjay 
Sharma, Sarang Pitale and Neeraj Kharya. 2010. 
Bhilai Institute of Technology Durg at TAC 2010: 
Knowledge Base Population Task Challenge. Proc. 
TAC 2010 Workshop. 
Jingtao Yu, Omkar Mujgond and Rob Gaizauskas. 
2010. The University of Sheffield System at TAC 
KBP 2010. Proc. TAC 2010 Workshop. 
Wei Zhang, Yan Chuan Sim, Jian Su and Chew Lim 
Tan. 2010. NUS-I2R: Learning a Combined System 
for Entity Linking. Proc. TAC 2010 Workshop. 
 
1158
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 260?265,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Can Document Selection Help Semi-supervised Learning?  
A Case Study On Event Extraction 
 
 
Shasha Liao Ralph Grishman 
Computer Science Department 
New York University 
liaoss@cs.nyu.edu grishman@cs.nyu.edu 
 
 
 
 
 
 
Abstract 
Annotating training data for event 
extraction is tedious and labor-intensive. 
Most current event extraction tasks rely 
on hundreds of annotated documents, but 
this is often not enough. In this paper, we 
present a novel self-training strategy, 
which uses Information Retrieval (IR) to 
collect a cluster of related documents as 
the resource for bootstrapping. Also, 
based on the particular characteristics of 
this corpus, global inference is applied to 
provide more confident and informative 
data selection. We compare this approach 
to self-training on a normal newswire 
corpus and show that IR can provide a 
better corpus for bootstrapping and that 
global inference can further improve 
instance selection. We obtain gains of 
1.7% in trigger labeling and 2.3% in role 
labeling through IR and an additional 
1.1% in trigger labeling and 1.3% in role 
labeling by applying global inference. 
1 Introduction 
The goal of event extraction is to identify 
instances of a class of events in text. In addition 
to identifying the event itself, it also identifies 
all of the participants and attributes of each 
event; these are the entities that are involved in 
that event. The same event might be presented 
in various expressions, and an expression might 
represent different events in different contexts. 
Moreover, for each event type, the event 
participants and attributes may also appear in 
multiple forms and exemplars of the different 
forms may be required. Thus, event extraction is 
a difficult task and requires substantial training 
data. However, annotating events for training is 
a tedious task. Annotators need to read the 
whole sentence, possibly several sentences, to 
decide whether there is a specific event or not, 
and then need to identify the event participants 
(like Agent and Patient), and attributes (like 
place and time) to complete an event annotation. 
As a result, for event extraction tasks like 
MUC4, MUC6 (MUC 1995) and ACE2005, 
from one to several hundred annotated 
documents were needed. 
In this paper, we apply a novel self-training 
process on an existing state-of-the-art baseline 
system. Although traditional self-training on 
normal newswire does not work well for this 
specific task, we managed to use information 
retrieval (IR) to select a better corpus for 
bootstrapping. Also, taking advantage of 
properties of this corpus, cross-document 
inference is applied to obtain more 
?informative? probabilities. To the best of our 
knowledge, we are the first to apply information 
retrieval and global inference to semi-supervised 
learning for event extraction. 
2 Task Description 
Automatic Content Extraction (ACE) defines an 
event as a specific occurrence involving 
260
participants 1 ; it annotates 8 types and 33 
subtypes of events.2 We first present some ACE 
terminology to understand this task more easily: 
? Event mention3: a phrase or sentence within 
which an event is described, including one 
trigger and an arbitrary number of arguments.  
? Event trigger: the main word that most 
clearly expresses an event occurrence. 
? Event mention arguments (roles): the entity 
mentions that are involved in an event 
mention, and their relation to the event.  
Here is an example: 
(1) Bob Cole was killed in France today; 
he was attacked?    
Table 1 shows the results of the preprocessing, 
including name identification, entity mention 
classification and coreference, and time 
stamping. Table 2 shows the results for event 
extraction. 
 
Mention 
ID 
Head  Ent.ID Type 
E1-1 France E-1 GPE 
T1-1 today T1 Timex 
E2-1 Bob Cole E-2 PER 
E2-2 He E-2 PER 
 
Table 1. An example of entities and entity 
mentions and their types 
 
Event 
type 
Trigger Role 
Place Victim Time 
Die killed E1-1 E2-1 T1-1 
  Place Target Time 
Attack attacked E1-1 E2-2 T1-1 
 
Table 2. An example of event triggers and roles 
                                                          
1http://projects.ldc.upenn.edu/ace/docs/English-Event
s-Guidelines_v5.4.3.pdf 
2  In this paper, we treat the event subtypes 
separately, and no type hierarchy is considered. 
3  Note that we do not deal with event mention 
coreference in this paper, so each event mention is 
treated separately.  
3 Related Work 
Self-training has been applied to several natural 
language processing tasks. For event extraction, 
there are several studies on bootstrapping from a 
seed pattern set. Riloff (1996) initiated the idea of 
using document relevance for extracting new 
patterns, and Yangarber et al (2000, 2003) 
incorporated this into a bootstrapping approach, 
extended by Surdeanu et al (2006) to co-training. 
Stevenson and Greenwood (2005) suggested an 
alternative method for ranking the candidate 
patterns by lexical similarities. Liao and 
Grishman (2010b) combined these two 
approaches to build a filtered ranking algorithm. 
However, these approaches were focused on 
finding instances of a scenario/event type rather 
than on argument role labeling. Starting from a 
set of documents classified for relevance, 
Patwardhan and Riloff (2007) created a 
self-trained relevant sentence classifier and 
automatically learned domain-relevant extraction 
patterns. Liu (2009) proposed the BEAR system, 
which tagged both the events and their roles. 
However, the new patterns were boostrapped 
based on the frequencies of sub-pattern mutations 
or on rules from linguistic contexts, and not on 
statistical models. 
The idea of sense consistency was first 
introduced and extended to operate across related 
documents by (Yarowsky, 1995). Yangarber et 
al. (Yangarber and Jokipii, 2005; Yangarber, 
2006; Yangarber et al, 2007) applied 
cross-document inference to correct local 
extraction results for disease name, location and 
start/end time. Mann (2007) encoded specific 
inference rules to improve extraction of 
information about CEOs (name, start year, end 
year). Later, Ji and Grishman (2008) employed a 
rule-based approach to propagate consistent 
triggers and arguments across topic-related 
documents. Gupta and Ji (2009) used a similar 
approach to recover implicit time information for 
events. Liao and Grishman (2010a) use a 
statistical model to infer the cross-event 
information within a document to improve event 
extraction.  
261
4 Event Extraction Baseline System 
We use a state-of-the-art English IE system as 
our baseline (Grishman et al 2005). This system 
extracts events independently for each sentence, 
because the definition of event mention 
arguments in ACE constrains them to appear in 
the same sentence. The system combines pattern 
matching with statistical models. In the training 
process, for every event mention in the ACE 
training corpus, patterns are constructed based on 
the sequences of constituent heads separating the 
trigger and arguments. A set of Maximum 
Entropy based classifiers are also trained: 
? Argument Classifier: to distinguish 
arguments of a potential trigger from 
non-arguments. 
? Role Classifier: to classify arguments by 
argument role. We use the same features as 
the argument classifier. 
? Reportable-Event Classifier (Trigger 
Classifier): Given a potential trigger, an 
event type, and a set of arguments, to 
determine whether there is a reportable 
event mention. 
In the test procedure, each document is 
scanned for instances of triggers from the 
training corpus. When an instance is found, the 
system tries to match the environment of the 
trigger against the set of patterns associated with 
that trigger. If this pattern-matching process 
succeeds, the argument classifier is applied to the 
entity mentions in the sentence to assign the 
possible arguments; for any argument passing 
that classifier, the role classifier is used to assign 
a role to it. Finally, once all arguments have been 
assigned, the reportable-event classifier is 
applied to the potential event mention; if the 
result is successful, this event mention is 
reported. 
5 Our Approach 
In self-training, a classifier is first trained with a 
small amount of labeled data. The classifier is 
then used to classify the unlabeled data. 
Typically the most confident unlabeled points, 
together with their predicted labels, are added to 
the training set. The classifier is re-trained and 
the procedure repeated. As a result, the criterion 
for selecting the most confident examples is 
critical to the effectiveness of self-training. 
To acquire confident samples, we need to first 
decide how to evaluate the confidence for each 
event. However, as an event contains one trigger 
and an arbitrary number of roles, a confident 
event might contain unconfident arguments. 
Thus, instead of taking the whole event, we select 
a partial event, containing one confident trigger 
and its most confident argument, to feed back to 
the training system.  
For each mention mi, its probability of filling a 
role r in a reportable event whose trigger is t is 
computed by: 
? 
PRoleOfTrigger(mi,r,t) = PArg(mi) ? PRole(mi,r) ? PEvent (t) 
 where PArg(mi) is the probability from the 
argument classifier, PRole(mi,r) is that from the 
role classifier, and PEvent(t) is that from the 
trigger classifier. In each iteration, we added the 
most confident <role, trigger> pairs to the 
training data, and re-trained the system. 
5.1 Problems of Traditional Self-training 
(ST) 
However, traditional self-training does not 
perform very well (see our results in Table 3). 
The newly added samples do not improve the 
system performance; instead, its performance 
stays stable, and even gets worse after several 
iterations.  
We analyzed the data, and found that this is 
caused by two common problems of traditional 
self-training. First, the classifier uses its own 
predictions to train itself, and so a classification 
mistake can reinforce itself. This is particularly 
true for event extraction, due to its relatively poor 
performance, compared to other NLP tasks, like 
Named Entity Recognition, parsing, or 
part-of-speech tagging, where self-training has 
been more successful. Figure 1 shows that the 
precision using the original training data is not 
very good: while precision improves with 
increasing classifier threshold, about 1/3 of the 
roles are still incorrectly tagged at a threshold of 
0.90. 
 
262
0.35 
0.4 
0.45 
0.5 
0.55 
0.6 
0.65 
0.7 
0.75 
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 
Trigger Labeling 
Argument Labeling 
Role Labeling 
 
 
Figure 1. Precision on the original training data 
with different thresholds (from 0.0 to 0.9) 
 
Another problem of self-training is that 
nothing ?novel? is added because the most 
confident examples are those frequently seen in 
the training data and might not provide ?new? 
information. Co-training is a form of 
self-training which can address this problem to 
some extent. However, it requires two views of 
the data, where each example is described using 
two different feature sets that provide different, 
complementary information. Ideally, the two 
views are conditionally independent  and each 
view is sufficient (Zhu, 2008). Co-training has 
had some success in training (binary) semantic 
relation extractors for some relations, where the 
two views correspond to the arguments of the 
relation and the context of these arguments 
(Agichtein and Gravano 2000).  However, it has 
had less success for event extraction because 
event arguments may participate in multiple 
events in a corpus and individual event instances 
may omit some arguments. 
5.2 Self-training on Information Retrieval 
Selected Corpus (ST_IR) 
To address the first problem (low precision of 
extracted events), we tried to select a corpus 
where the baseline system can tag the instances 
with greater confidence. (Ji and Grishman 2008) 
have observed that the events in a cluster of 
documents on the same topics as documents in 
the training corpus can be tagged more 
confidently. Thus, we believe that bootstrapping 
on a corpus of topic-related documents should 
perform better than a regular newswire corpus. 
We followed Ji and Grishman (2008)?s 
approach and used the INDRI retrieval system4 
(Strohman et al, 2005) to obtain the top N  
                                                          
4 http://www.lemurproject.org/indri/ 
related documents for each annotated document 
in the training corpus. The query is event-based 
to insure that related documents contain the same 
events. For each training document, we construct 
an INDRI query from the triggers and arguments. 
For example, for sentence (1) in section 2, we use 
the keywords ?killed?, ?attacked?, ?France?, 
?Bob Cole?, and ?today? to extract related 
documents. Only names and nominal arguments 
will be used; pronouns appearing as arguments 
are not included. For each argument we also add 
other names coreferential with the argument. 
5.3 Self-training using Global Inference 
(ST_GI) 
Although bootstrapping on related documents 
can solve the problem of ?confidence? to some 
extent, the ?novelty? problem still remains:  the 
top-ranked extracted events will be too similar to 
those in the training corpus. To address this 
problem, we propose to use a simple form of 
global inference based on the special 
characteristics of related-topic documents. 
Previous studies pointed out that information 
from wider scope, at the document or 
cross-document level, could provide non-local 
information to aid event extraction (Ji and 
Grishman 2008, Liao and Grishman 2010a). 
There are two common assumptions within a 
cluster of related documents (Ji and Grishman 
2008): 
? Trigger Consistency Per Cluster: if one 
instance of a word triggers an event, other 
instances of the same word will trigger events 
of the same type. 
? Role Consistency Per Cluster: if one entity 
appears as an argument of multiple events of 
the same type in a cluster of related 
documents, it should be assigned the same 
role each time. 
Based on these assumptions, if a trigger/role 
has a low probability from the baseline system, 
but a high one from global inference, it means 
that the local context of this trigger/role tag is not 
frequently seen in the training data, but the tag is 
still confident. Thus, we can confidently add it to 
the training data and it can provide novel 
information which the samples confidently 
tagged by the baseline system cannot provide. 
263
To start, the baseline system extracts a set of 
events and estimates the probability that a 
particular instance of a word triggers an event of 
that type, and the probability that it takes a 
particular argument. The global inference 
process then begins by collecting all the 
confident triggers and arguments from a cluster 
of related documents.5 For each trigger word and 
event type, it records the highest probability 
(over all instances of that word in the cluster) that 
the word triggers an event of that type.  For each 
argument, within-document and cross-document 
coreference6 are used to collect all instances of 
that entity; we then compute the maximum 
probability (over all instances) of that argument 
playing a particular role in a particular event 
type. These maxima will then be used in place of 
the locally-computed probabilities in computing 
the probability of each trigger-argument pair in 
the formula for PRoleOfTrigger given above.
7  For 
example, if the entity ?Iraq? is tagged confidently 
(probability > 0.9) as the ?Attacker? role 
somewhere in a cluster, and there is another 
instance where from local information it is only 
tagged with 0.1 probability to be an ?Attacker? 
role, we use probability of 0.9 for both instances. 
In this way, a trigger pair containing this 
argument is more likely to be added into the 
training data through bootstrapping, because we 
have global evidence that this role probability is 
high, although its local confidence is low. In this 
way, some novel trigger-argument pairs will be 
chosen, thus improving the baseline system. 
6 Results 
We randomly chose 20 newswire texts from the 
ACE 2005 training corpora (from March to May 
of 2003) as our evaluation set, and used the 
                                                          
5 In our experiment, only triggers and roles with 
probability higher than 0.9 will be extracted. 
6 We use a statistical within-document coreference 
system (Grishman et al 2005), and a simple 
rule-based cross-document coreference system, 
where entities sharing the same names will be treated 
as coreferential across documents. 
7 If a word or argument has multiple tags (different 
event types or roles) in a cluster, and the difference 
in the probabilities of the two tags is less than some 
threshold, we treat this as a ?conflict? and do not use 
the conflicting information for global inference. 
remaining newswire texts as the original training 
data (83 documents). For self-training, we picked 
10,000 consecutive newswire texts from the 
TDT5 corpus from 20038 for the ST experiment. 
For ST_IR and ST_GI, we retrieved the best N 
(using N = 25, which (Ji and Grishman 2008) 
found to work best) related texts for each training 
document from the English TDT5 corpus 
consisting of 278,108 news texts (from April to 
September of 2003). In total we retrieved 1650 
texts; the IR system returned no texts or fewer 
than 25 texts for some training documents. In 
each iteration, we extract 500 trigger and 
argument pairs to add to the training data. 
Results (Table 3) show that bootstrapping on 
an event-based IR corpus can produce 
improvements on all three evaluations, while 
global inference can yield further gains.  
 
 Trigger 
labeling 
Argument 
labeling 
Role 
labeling 
Baseline 54.1 39.2 35.4 
ST 54.2 40.0 34.6 
ST_IR 55.8 42.1 37.7 
ST_GI 56.9 43.8 39.0 
 
Table 3. Performance (F score) with different 
self-training strategies after 10 iterations 
7 Conclusions and Future Work 
We proposed a novel self-training process for 
event extraction that involves information 
retrieval (IR) and global inference to provide 
more accurate and informative instances. 
Experiments show that using an IR-selected 
corpus improves trigger labeling F score 1.7%, 
and role labeling 2.3%. Global inference can 
achieve further improvement of 1.1% for trigger 
labeling, and 1.3% for role labeling. Also, this 
bootstrapping involves processing a much 
                                                          
8  We selected all bootstrapping data from 2003 
newswire, with the same genre and time period as 
ACE 2005 data to avoid possible influences of 
variations in the genre or time period on the 
bootstrapping. Also, we selected 10,000 documents 
because this size of corpus yielded a set of 
confidently-extracted events (probability > 0.9) 
roughly comparable in size to those extracted from 
the IR-selected corpus; a larger corpus would have 
slowed the bootstrapping. 
264
smaller but more closely related corpus, which is 
more efficient. Such pre-selection of documents 
may benefit bootstrapping for other NLP tasks as 
well, such as name and relation extraction. 
Acknowledgments 
We would like to thank Prof. Heng Ji for her kind 
help in providing IR data and useful suggestions. 
References  
Eugene Agichtein and Luis Gravano.  2000.  
Snowball:  Extracting relations from large 
plain-text collections. In Proceedings of 5th ACM 
International Conference on Digital Libraries. 
Ralph Grishman, David Westbrook and Adam 
Meyers. 2005. NYU?s English ACE 2005 System 
Description. In Proc. ACE 2005 Evaluation 
Workshop, Gaithersburg, MD. 
Prashant Gupta and Heng Ji. 2009. Predicting 
Unknown Time Arguments based on Cross-Event 
Propagation. In Proceedings of ACL-IJCNLP 
2009. 
Heng Ji and Ralph Grishman. 2008. Refining Event 
Extraction through Cross-Document Inference. In 
Proceedings of ACL-08: HLT, pages 254?262, 
Columbus, OH, June. 
Shasha Liao and Ralph Grishman. 2010a. Using 
Document Level Cross-Event Inference to 
Improve Event Extraction. In Proceedings of ACL 
2010. 
Shasha Liao and Ralph Grishman. 2010b. Filtered 
Ranking for Bootstrapping in Event Extraction. In 
Proceedings of COLING 2010. 
Ting Liu. 2009. Bootstrapping events and relations 
from text. Ph.D. thesis, State University of New 
York at Albany. 
Gideon Mann. 2007. Multi-document Relationship 
Fusion via Constraints on Probabilistic Databases. 
In Proceedings of HLT/NAACL 2007. Rochester, 
NY, US. 
MUC. 1995. Proceedings of the Sixth Message 
Understanding Conference (MUC-6), San Mateo, 
CA. Morgan Kaufmann. 
S. Patwardhan and E. Riloff. 2007. Effective 
Information Extraction with Semantic Affinity 
Patterns and Relevant Regions. In Proceedings of 
the 2007 Conference on Empirical Methods in 
Natural Language Processing (EMNLP-07). 
Ellen Riloff. 1996. Automatically Generating 
Extraction Patterns from Untagged Text. In 
Proceedings of Thirteenth National Conference on 
Artificial Intelligence (AAAI-96), pp. 1044-1049. 
M. Stevenson and M. Greenwood. 2005. A Semantic 
Approach to IE Pattern Induction. In Proceedings 
of ACL 2005. 
Trevor Strohman, Donald Metzler, Howard Turtle 
and W. Bruce Croft. 2005. Indri: A 
Language-model based Search Engine for 
Complex Queries (extended version). Technical 
Report IR-407, CIIR, UMass Amherst, US. 
Mihai Surdeanu, Jordi Turmo, and Alicia Ageno. 
2006. A Hybrid Approach for the Acquisition of 
Information Extraction Patterns. In Proceedings of 
the EACL 2006 Workshop on Adaptive Text 
Extraction and Mining (ATEM 2006). 
Roman Yangarber, Ralph Grishman, Pasi 
Tapanainen, and Silja Huttunen. 2000. Automatic 
Acquisition of Domain Knowledge for 
Information Extraction. In Proceedings of 
COLING 2000. 
Roman Yangarber. 2003. Counter-Training in 
Discovery of Semantic Patterns. In Proceedings of 
ACL2003. 
Roman Yangarber and Lauri Jokipii. 2005. 
Redundancy-based Correction of Automatically 
Extracted Facts. In Proceedings of HLT/EMNLP 
2005. Vancouver, Canada. 
Roman Yangarber. 2006. Verification of Facts across 
Document Boundaries.  In Proceedings of 
International Workshop on Intelligent Information 
Access. Helsinki, Finland. 
Roman Yangarber, Clive Best, Peter von Etter, Flavio 
Fuart, David Horby and Ralf Steinberger. 2007. 
Combining Information about Epidemic Threats 
from Multiple Sources. In Proceedings of RANLP 
2007 workshop on Multi-source, Multilingual 
Information Extraction and Summarization. 
Borovets, Bulgaria. 
David Yarowsky. 1995. Unsupervised Word Sense 
Disambiguation Rivaling Supervised Methods. In 
Proceedings of ACL 1995. Cambridge, MA.  
Xiaojin Zhu. 2008 Semi-Supervised Learning 
Literature Survey. http:// pages.cs.wisc.edu/ 
~jerryzhu/research/ssl/semireview.html 
 
265
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 665?670,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Filling Knowledge Base Gaps for Distant Supervision
of Relation Extraction
Wei Xu+ Raphael Hoffmann? Le Zhao#,* Ralph Grishman+
+New York University, New York, NY, USA
{xuwei, grishman}@cs.nyu.edu
?University of Washington, Seattle, WA, USA
raphaelh@cs.washington.edu
#Google Inc., Mountain View, CA, USA
lezhao@google.com
Abstract
Distant supervision has attracted recent in-
terest for training information extraction
systems because it does not require any
human annotation but rather employs ex-
isting knowledge bases to heuristically la-
bel a training corpus. However, previous
work has failed to address the problem
of false negative training examples misla-
beled due to the incompleteness of knowl-
edge bases. To tackle this problem, we
propose a simple yet novel framework that
combines a passage retrieval model using
coarse features into a state-of-the-art rela-
tion extractor using multi-instance learn-
ing with fine features. We adapt the in-
formation retrieval technique of pseudo-
relevance feedback to expand knowledge
bases, assuming entity pairs in top-ranked
passages are more likely to express a rela-
tion. Our proposed technique significantly
improves the quality of distantly super-
vised relation extraction, boosting recall
from 47.7% to 61.2% with a consistently
high level of precision of around 93% in
the experiments.
1 Introduction
A recent approach for training information ex-
traction systems is distant supervision, which ex-
ploits existing knowledge bases instead of anno-
tated texts as the source of supervision (Craven
and Kumlien, 1999; Mintz et al, 2009; Nguyen
and Moschitti, 2011). To combat the noisy train-
ing data produced by heuristic labeling in distant
supervision, researchers (Bunescu and Mooney,
2007; Riedel et al, 2010; Hoffmann et al, 2011;
Surdeanu et al, 2012) exploited multi-instance
*This work was done while Le Zhao was at Carnegie
Mellon University.
learning models. Only a few studies have directly
examined the influence of the quality of the train-
ing data and attempted to enhance it (Sun et al,
2011; Wang et al, 2011; Takamatsu et al, 2012).
However, their methods are handicapped by the
built-in assumption that a sentence does not ex-
press a relation unless it mentions two entities
which participate in the relation in the knowledge
base, leading to false negatives.
aligned 
mentions 
true 
 mentions 5.5% 2.7% 1.7% false  negatives false  
positives 
Figure 1: Noisy training data in distant supervi-
sion
In reality, knowledge bases are often incom-
plete, giving rise to numerous false negatives in
the training data. We sampled 1834 sentences that
contain two entities in the New York Times 2006
corpus and manually evaluated whether they ex-
press any of a set of 50 common Freebase1 rela-
tions. As shown in Figure 1, of the 133 (7.3%)
sentences that truly express one of these relations,
only 32 (1.7%) are covered by Freebase, leaving
101 (5.5%) false negatives. Even for one of the
most complete relations in Freebase, Employee-of
(with more than 100,000 entity pairs), 6 out of 27
sentences with the pattern ?PERSON executive of
ORGANIZATION? contain a fact that is not in-
cluded in Freebase and are thus mislabeled as neg-
ative. These mislabelings dilute the discriminative
capability of useful features and confuse the mod-
els. In this paper, we will show how reducing this
source of noise can significantly improve the per-
formance of distant supervision. In fact, our sys-
tem corrects the relation labels of the above 6 sen-
tences before training the relation extractor.
1http://www.freebase.com
665
 
D ocuments  Knowledge 
Base  
Relation 
Extractor  
Passage  
Retriever  
? 
? 
? 
Pseudo - relevant 
Relation Instances  
? 
? 
? 
Figure 2: Overall system architecture: The system
(1) matches relation instances to sentences and (2)
learns a passage retrieval model to (3) provide rel-
evance feedback on sentences; Relevant sentences
(4) yield new relation instances which are added
to the knowledge base; Finally, instances are again
(5) matched to sentences to (6) create training data
for relation extraction.
Encouraged by the recent success of simple
methods for coreference resolution (Raghunathan
et al, 2010) and inspired by pseudo-relevance
feedback (Xu and Croft, 1996; Lavrenko and
Croft, 2001; Matveeva et al, 2006; Cao et al,
2008) in the field of information retrieval, which
expands or reformulates query terms based on
the highest ranked documents of an initial query,
we propose to increase the quality and quantity
of training data generated by distant supervision
for information extraction task using pseudo feed-
back. As shown in Figure 2, we expand an orig-
inal knowledge base with possibly missing rela-
tion instances with information from the highest
ranked sentences returned by a passage retrieval
model (Xu et al, 2011) trained on the same data.
We use coarse features for our passage retrieval
model to aggressively expand the knowledge base
for maximum recall; at the same time, we exploit
a multi-instance learning model with fine features
for relation extraction to handle the newly intro-
duced false positives and maintain high precision.
Similar to iterative bootstrapping tech-
niques (Yangarber, 2001), this mechanism uses
the outputs of the first trained model to expand
training data for the second model, but unlike
bootstrapping it does not require iteration and
avoids the problem of semantic drift. We further
note that iterative bootstrapping over a single
distant supervision system is difficult, because
state-of-the-art systems (Surdeanu et al, 2012;
Hoffmann et al, 2011; Riedel et al, 2010; Mintz
et al, 2009), detect only few false negatives in the
training data due to their high-precision low-recall
features, which were originally proposed by Mintz
et al (2009). We present a reliable and novel way
to address these issues and achieve significant
improvement over the MULTIR system (Hoff-
mann et al, 2011), increasing recall from 47.7%
to 61.2% at comparable precision. The key to this
success is the combination of two different views
as in co-training (Blum and Mitchell, 1998):
an information extraction technique with fine
features for high precision and an information
retrieval technique with coarse features for high
recall. Our work is developed in parallel with
Min et al (2013), who take a very different
approach by adding additional latent variables to
a multi-instance multi-label model (Surdeanu et
al., 2012) to solve this same problem.
2 System Details
In this section, we first introduce some formal no-
tations then describe in detail each component of
the proposed system in Figure 2.
2.1 Definitions
A relation instance is an expression r(e1, e2)
where r is a binary relation, and e1 and e2 are
two entities having such a relation, for example
CEO-of(Tim Cook, Apple). The knowledge-based
distant supervised learning problem takes as input
(1) ?, a training corpus, (2) E, a set of entities
mentioned in that corpus, (3) R, a set of relation
names, and (4) ?, a set of ground facts of relations
in R. To generate our training data, we further as-
sume (5) T , a set of entity types, as well as type
signature r(E1, E2) for relations.
We define the positive data set POS(r) to be
the set of sentences in which any related pair
of entities of relation r (according to the knowl-
edge base) is mentioned. The negative data set
RAW (r) is the rest of the training data, which
contain two entities of the required types in the
knowledge base, e.g. one person and one or-
ganization for the CEO-of relation in Freebase.
Another negative data set with more conservative
sense NEG(r) is defined as the set of sentences
which contain the primary entity e1 (e.g. person
in any CEO-of relation in the knowledge base) and
any secondary entity e2 of required type (e.g. or-
ganization for the CEO-of relation) but the relation
does not hold for this pair of entities in the knowl-
edge base.
666
2.2 Distantly Supervised Passage Retrieval
We extend the learning-to-rank techniques (Liu,
2011) to distant supervision setting (Xu et al,
2011) to create a robust passage retrieval system.
While relation extraction systems exploit rich and
complex features that are necessary to extract the
exact relation (Mintz et al, 2009; Riedel et al,
2010; Hoffmann et al, 2011), passage retrieval
components use coarse features in order to provide
different and complementary feedback to informa-
tion extraction models.
We exploit two types of lexical features: Bag-
Of-Words and Word-Position. The two types of
simple binary features are shown in the following
example:
Sentence: Apple founder Steve Jobs died.
Target (Primary) entity: Steve Jobs
Bag-Of-Word features: ?apple? ?founder? ?died? ?.?
Word-Position features: ?apple:-2? ?founder:-1?
?died:+1? ?.:+2?
For each relation r, we assume each sentence
has a binary relevance label to form distantly su-
pervised training data: sentences in POS(r) are
relevant and sentences in NEG(r) are irrelevant.
As a pointwise learning-to-rank approach (Nallap-
ati, 2004), the probabilities of relevance estimated
by SVMs (Platt and others, 1999) are used for
ranking all the sentences in the original training
corpus for each relation respectively. We use Lib-
SVM 2 (Chang and Lin, 2011) in our implementa-
tion.
2.3 Psuedo-relevance Relation Feedback
In the field of information retrieval, pseudo-
relevance feedback assumes that the top-ranked
documents from an initial retrieval are likely rel-
evant, and extracts relevant terms to expand the
original query (Xu and Croft, 1996; Lavrenko and
Croft, 2001; Cao et al, 2008). Analogously, our
assumption is that entity pairs that appear in more
relevant and more sentences are more likely to
express the relation, and can be used to expand
knowledge base and reduce false negative noise in
the training data for information extraction. We
identify the most likely relevant entity pairs as fol-
lows:
2http://www.csie.ntu.edu.tw/?cjlin/
libsvm
initialize ?? ?? ?
for each relation type r ? R do
learn a passage (sentence) retrieval model L(r)
using coarse features and POS(r)?NEG(r)
as training data
score the sentences in the RAW (r) by L(r)
score the entity pairs according to the scores
of sentences they are involved in
select the top ranked pairs of entities, then add
the relation r to their label in ??
end for
We select the entity pairs whose average score
of the sentences they are involved in is greater
than p, where p is a parameter tuned on develop-
ment data.3 The relation extraction model is then
trained using (?, E,R,??) with a more complete
database than the original knowledge base ?.
2.4 Distantly Supervised Relation Extraction
We use a state-of-the-art open-source system,
MULTIR (Hoffmann et al, 2011), as the rela-
tion extraction component. MULTIR is based
on multi-instance learning, which assumes that
at least one sentence of those matching a given
entity-pair contains the relation of interest (Riedel
et al, 2010) in the given knowledge base to tol-
erate false positive noise in the training data and
superior than previous models (Riedel et al, 2010;
Mintz et al, 2009) by allowing overlapping rela-
tions. MULTIR uses features which are based on
Mintz et al (2009) and consist of conjunctions of
named entity tags, syntactic dependency paths be-
tween arguments, and lexical information.
3 Experiments
For evaluating extraction accuracy, we follow the
experimental setup of Hoffmann et al (2011), and
use their implementation of MULTIR4 with 50
training iterations as our baseline. Our complete
system, which we call IRMIE, combines our pas-
sage retrieval component with MULTIR. We use
the same datasets as in Hoffmann et al (2011) and
Riedel et al (2010), which include 3-years of New
York Times articles aligned with Freebase. The
sentential extraction evaluation is performed on
a small amount of manually annotated sentences,
sampled from the union of matched sentences and
3We found p = 0.5 to work well in practice.
4http://homes.cs.washington.edu/
?raphaelh/mr/
667
Test Data Set Original Test Set Corrected Test Set
P? R? F? ?F? P? R? F? ?F?
MULTIR 80.0 44.6 62.3 92.7 47.7 70.2
IRMIE 84.6 56.1 70.3 +8.0 92.6 61.2 76.9 +6.7
MULTIRLEX 91.8 43.0 67.4 79.6 57.0 68.3
IRMIELEX 89.2 52.5 70.9 +3.5 78.0 69.2 73.6 +5.3
Table 1: Overall sentential extraction performance evaluated on the original test set of Hoffmann et
al. (2011) and our corrected test set: Our proposed relevance feedback technique yields a substantial
increase in recall.
system predictions. We define Se as the sentences
where some system extracted a relation and SF
as the sentences that match the arguments of a
fact in ?. The sentential precision and recall is
computed on a randomly sampled set of sentences
from Se?SF , in which each sentence is manually
labeled whether it expresses any relation in R.
Figure 3 shows the precision/recall curves for
MULTIR with and without pseudo-relevance feed-
back computed on the test dataset of 1000 sen-
tence used by Hoffmann et al (2011). With the
pseudo-relevance feedback from passage retrieval,
IRMIE achieves significantly higher recall at a
consistently high level of precision. At the highest
recall point, IRMIE reaches 78.5% precision and
59.2% recall, for an F1 score of 68.9%.
Because the two types of lexical features used in
our passage retrieval models are not used in MUL-
TIR, we created another baseline MULTIRLEX
by adding these features into MULTIR in order
to rule out the improvement from additional infor-
mation. Note that the sentences are sampled from
the union of Freebase matches and sentences from
which some systems in Hoffmann et al (2011) ex-
tracted a relation. It underestimates the improve-
ments of the newly developed systems in this pa-
per. We therefore also created a new test set of
1000 sentences by sampling from the union of
Freebase matches and sentences where MULTIR-
LEX or IRMIELEX extracted a relation. Table 1
shows the overall precision and recall computed
against these two test datasets, with and without
adding lexical features into multi-instance learn-
ing models. The performance improvement by us-
ing pseudo-feedback is significant (p < 0.05) in
McNemar?s test for both datasets.
4 Conclusion and Perspectives
This paper proposes a novel approach to address
an overlooked problem in distant supervision: the
knowledge base is often incomplete causing nu-
Recall
Precision
0.0 0.1 0.2 0.3 0.4 0.5 0.6
0.5
0.6
0.7
0.8
0.9
1.0
IRMIE
MULTIR
Figure 3: Sentential extraction: precision/recall
curves using exact same training and test data,
features and system settings as in Hoffmann et
al. (2011).
merous false negatives in the training data. It
greatly improves a state-of-the-art multi-instance
learning model by correcting the most likely false
negatives in the training data based on the ranking
of a passage retrieval model.
In the future, we would like to more tightly inte-
grate a coarser featured estimator of sentential rel-
evance and a finer featured relation extractor, such
that a single joint-model can be learned.
Acknowledgments
Supported in part by NSF grant IIS-1018317,
the Air Force Research Laboratory (AFRL)
under prime contract number FA8750-09-C-
0181 and the Intelligence Advanced Research
Projects Activity (IARPA) via Department of In-
terior National Business Center contract number
D11PC20154. The U.S. Government is authorized
to reproduce and distribute reprints for Govern-
mental purposes notwithstanding any copyright
annotation thereon. Disclaimer: The views and
conclusions contained herein are those of the au-
thors and should not be interpreted as necessarily
representing the official policies or endorsements,
either expressed or implied, of AFRL, IARPA,
DoI/NBC, or the U.S. Government.
668
References
Avrim Blum and Tom M. Mitchell. 1998. Combin-
ing labeled and unlabeled sata with co-training. In
Proceedings of the 11th Annual Conference on Com-
putational Learning Theory (COLT), pages 92?100.
Razvan C. Bunescu and Raymond J. Mooney. 2007.
Learning to extract relations from the web using
minimal supervision. In Proceedings of the 45th An-
nual Meeting of the Association for Computational
Linguistics (ACL).
Guihong Cao, Jian-Yun Nie, Jianfeng Gao, and
Stephen Robertson. 2008. Selecting good expan-
sion terms for pseudo-relevance feedback. In Pro-
ceedigns of the 31st Annual International ACM SI-
GIR Conference on Research and Development in
Information Retrieval (SIGIR), pages 243?250.
Chih-Chung Chang and Chih-Jen Lin. 2011. Lib-
svm: A library for support vector machines. ACM
Transactions on Intelligent Systems and Technology,
2(3):27.
Mark Craven and Johan Kumlien. 1999. Constructing
biological knowledge bases by extracting informa-
tion from text sources. In Proceedings of the Sev-
enth International Conference on Intelligent Systems
for Molecular Biology (ISMB), pages 77?86.
Raphael Hoffmann, Congle Zhang, Xiao Ling,
Luke S. Zettlemoyer, and Daniel S. Weld. 2011.
Knowledge-based weak supervision for information
extraction of overlapping relations. In Proceedings
of the 49th Annual Meeting of the Association for
Computational Linguistics (ACL), pages 541?550.
Victor Lavrenko and W. Bruce Croft. 2001.
Relevance-based language models. In Proceedings
of the 24th Annual International ACM SIGIR Con-
ference on Research and Development in Informa-
tion Retrieval (SIGIR), pages 120?127.
Tie-Yan Liu. 2011. Learning to Rank for Information
Retrieval. Springer-Verlag Berlin Heidelberg.
Irina Matveeva, Chris Burges, Timo Burkard, Andy
Laucius, and Leon Wong. 2006. High accuracy re-
trieval with multiple nested ranker. In Proceedings
of the 29th Annual International ACM SIGIR Con-
ference on Research and Development in Informa-
tion Retrieval (SIGIR), pages 437?444.
Bonan Min, Ralph Grishman, Li Wan, Chang Wang,
and David Gondek. 2013. Distant supervision for
relation extraction with an incomplete knowledge
base. In Proceedings of the Conference of the North
American Chapter of the Association for Computa-
tional Linguistics (NAACL 2013).
Mike Mintz, Steven Bills, Rion Snow, and Daniel Ju-
rafsky. 2009. Distant supervision for relation ex-
traction without labeled data. In Proceedigns of the
47th Annual Meeting of the Association for Compu-
tational Linguistics and the 4th International Joint
Conference on Natural Language Processing (ACL),
pages 1003?1011.
Ramesh Nallapati. 2004. Discriminative models for
information retrieval. In Proceedigns of the 27th An-
nual International ACM SIGIR Conference on Re-
search and Development in Information Retrieval
(SIGIR), pages 64?71.
Truc Vien T Nguyen and Alessandro Moschitti. 2011.
End-to-end relation extraction using distant super-
vision from external semantic repositories. In Pro-
ceedings of the 49th Annual Meeting of the Associ-
ation for Computational Linguistics: Human Lan-
guage Technologies, pages 277?282.
John Platt et al 1999. Probabilistic outputs for sup-
port vector machines and comparisons to regular-
ized likelihood methods. Advances in Large Margin
Classifiers, 10(3):61?74.
Karthik Raghunathan, Heeyoung Lee, Sudarshan Ran-
garajan, Nathanael Chambers, Mihai Surdeanu, Dan
Jurafsky, and Christopher Manning. 2010. A multi-
pass sieve for coreference resolution. In Proceed-
ings of the 2010 Conference on Empirical Methods
in Natural Language Processing, pages 492?501.
Association for Computational Linguistics.
Sebastian Riedel, Limin Yao, and Andrew McCallum.
2010. Modeling relations and their mentions with-
out labeled text. In Proceedigns of the European
Conference on Machine Learning and Principles
and Practice of Knowledge Discovery in Databases
(ECML/PKDD), pages 148?163.
Ang Sun, Ralph Grishman, Wei Xu, and Bonan Min.
2011. New york university 2011 system for kbp slot
filling. In Text Analysis Conference 2011 Workshop.
Mihai Surdeanu, Julie Tibshirani, Ramesh Nallapati,
and Christopher D. Manning. 2012. Multi-instance
multi-label learning for relation extraction. In Pro-
ceedings of the 50th Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL), pages
455?465.
Shingo Takamatsu, Issei Sato, and Hiroshi Nakagawa.
2012. Reducing wrong labels in distant supervi-
sion for relation extraction. In Proceedings of the
50th Annual Meeting of the Association for Compu-
tational Linguistics (ACL), pages 721?729.
Chang Wang, James Fan, Aditya Kalyanpur, and David
Gondek. 2011. Relation extraction with relation
topics. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing
(EMNLP), pages 1426?1436.
Jinxi Xu and W. Bruce Croft. 1996. Query expansion
using local and global document analysis. In Hans-
Peter Frei, Donna Harman, Peter Scha?uble, and Ross
Wilkinson, editors, Proceedings of the 19th Annual
International ACM SIGIR Conference on Research
and Development in Information Retrieval (SIGIR),
pages 4?11. ACM.
669
Wei Xu, Ralph Grishman, and Le Zhao. 2011. Passage
retrieval for information extraction using distant su-
pervision. In Proceedings of the International Joint
Conference on Natural Language Processing (IJC-
NLP), pages 1046?1054.
Roman Yangarber. 2001. Scenario customization for
information extraction. Ph.D. thesis, Department of
Computer Science, Graduate School of Arts and Sci-
ence, New York University.
670
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 68?74,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Employing Word Representations and Regularization for
Domain Adaptation of Relation Extraction
Thien Huu Nguyen
Computer Science Department
New York University
New York, NY 10003 USA
thien@cs.nyu.edu
Ralph Grishman
Computer Science Department
New York University
New York, NY 10003 USA
grishman@cs.nyu.edu
Abstract
Relation extraction suffers from a perfor-
mance loss when a model is applied to
out-of-domain data. This has fostered the
development of domain adaptation tech-
niques for relation extraction. This paper
evaluates word embeddings and clustering
on adapting feature-based relation extrac-
tion systems. We systematically explore
various ways to apply word embeddings
and show the best adaptation improvement
by combining word cluster and word em-
bedding information. Finally, we demon-
strate the effectiveness of regularization
for the adaptability of relation extractors.
1 Introduction
The goal of Relation Extraction (RE) is to detect
and classify relation mentions between entity pairs
into predefined relation types such as Employ-
ment or Citizenship relationships. Recent research
in this area, whether feature-based (Kambhatla,
2004; Boschee et al, 2005; Zhou et al, 2005;
Grishman et al, 2005; Jiang and Zhai, 2007a;
Chan and Roth, 2010; Sun et al, 2011) or kernel-
based (Zelenko et al, 2003; Bunescu and Mooney,
2005a; Bunescu and Mooney, 2005b; Zhang et al,
2006; Qian et al, 2008; Nguyen et al, 2009), at-
tempts to improve the RE performance by enrich-
ing the feature sets from multiple sentence anal-
yses and knowledge resources. The fundamental
assumption of these supervised systems is that the
training data and the data to which the systems are
applied are sampled independently and identically
from the same distribution. When there is a mis-
match between data distributions, the RE perfor-
mance of these systems tends to degrade dramat-
ically (Plank and Moschitti, 2013). This is where
we need to resort to domain adaptation techniques
(DA) to adapt a model trained on one domain (the
source domain) into a new model which can per-
form well on new domains (the target domains).
The consequences of linguistic variation be-
tween training and testing data on NLP tools have
been studied extensively in the last couple of years
for various NLP tasks such as Part-of-Speech tag-
ging (Blitzer et al, 2006; Huang and Yates, 2010;
Schnabel and Sch?utze, 2014), named entity recog-
nition (Daum?e III, 2007) and sentiment analysis
(Blitzer et al, 2007; Daum?e III, 2007; Daum?e
III et al, 2010; Blitzer et al, 2011), etc. Un-
fortunately, there is very little work on domain
adaptation for RE. The only study explicitly tar-
geting this problem so far is by Plank and Mos-
chitti (2013) who find that the out-of-domain per-
formance of kernel-based relation extractors can
be improved by embedding semantic similarity in-
formation generated from word clustering and la-
tent semantic analysis (LSA) into syntactic tree
kernels. Although this idea is interesting, it suf-
fers from two major limitations:
+ It does not incorporate word cluster informa-
tion at different levels of granularity. In fact, Plank
and Moschitti (2013) only use the 10-bit cluster
prefix in their study. We will demonstrate later
that the adaptability of relation extractors can ben-
efit significantly from the addition of word cluster
features at various granularities.
+ It is unclear if this approach can encode real-
valued features of words (such as word embed-
dings (Mnih and Hinton, 2007; Collobert and We-
ston, 2008)) effectively. As the real-valued fea-
tures are able to capture latent yet useful proper-
ties of words, the augmentation of lexical terms
with these features is desirable to provide a more
general representation, potentially helping relation
extractors perform more robustly across domains.
In this work, we propose to avoid these limita-
tions by applying a feature-based approach for RE
which allows us to integrate various word features
of generalization into a single system more natu-
68
rally and effectively.
The application of word representations such
as word clusters in domain adaptation of RE
(Plank and Moschitti, 2013) is motivated by its
successes in semi-supervised methods (Chan and
Roth, 2010; Sun et al, 2011) where word repre-
sentations help to reduce data-sparseness of lexi-
cal information in the training data. In DA terms,
since the vocabularies of the source and target do-
mains are usually different, word representations
would mitigate the lexical sparsity by providing
general features of words that are shared across
domains, hence bridge the gap between domains.
The underlying hypothesis here is that the absence
of lexical target-domain features in the source do-
main can be compensated by these general fea-
tures to improve RE performance on the target do-
mains.
We extend this motivation by further evaluat-
ing word embeddings (Bengio et al, 2001; Ben-
gio et al, 2003; Mnih and Hinton, 2007; Col-
lobert and Weston, 2008; Turian et al, 2010) on
feature-based methods to adapt RE systems to new
domains. We explore the embedding-based fea-
tures in a principled way and demonstrate that
word embedding itself is also an effective repre-
sentation for domain adaptation of RE. More im-
portantly, we show empirically that word embed-
dings and word clusters capture different informa-
tion and their combination would further improve
the adaptability of relation extractors.
2 Regularization
Given the more general representations provided
by word representations above, how can we learn a
relation extractor from the labeled source domain
data that generalizes well to new domains? In tra-
ditional machine learning where the challenge is
to utilize the training data to make predictions on
unseen data points (generated from the same dis-
tribution as the training data), the classifier with
a good generalization performance is the one that
not only fits the training data, but also avoids ov-
efitting over it. This is often obtained via regular-
ization methods to penalize complexity of classi-
fiers. Exploiting the shared interest in generaliza-
tion performance with traditional machine learn-
ing, in domain adaptation for RE, we would prefer
the relation extractor that fits the source domain
data, but also circumvents the overfitting problem
over this source domain
1
so that it could general-
ize well on new domains. Eventually, regulariza-
tion methods can be considered naturally as a sim-
ple yet general technique to cope with DA prob-
lems.
Following Plank and Moschitti (2013), we as-
sume that we only have labeled data in a single
source domain but no labeled as well as unlabeled
target data. Moreover, we consider the single-
system DA setting where we construct a single
system able to work robustly with different but
related domains (multiple target domains). This
setting differs from most previous studies (Blitzer
et al, 2006) on DA which have attempted to de-
sign a specialized system for every specific tar-
get domain. In our view, although this setting is
more challenging, it is more practical for RE. In
fact, this setting can benefit considerably from our
general approach of applying word representations
and regularization. Finally, due to this setting, the
best way to set up the regularization parameter is
to impose the same regularization parameter on
every feature rather than a skewed regularization
(Jiang and Zhai, 2007b).
3 Related Work
Although word embeddings have been success-
fully employed in many NLP tasks (Collobert and
Weston, 2008; Turian et al, 2010; Maas and
Ng, 2010), the application of word embeddings
in RE is very recent. Kuksa et al (2010) pro-
pose an abstraction-augmented string kernel for
bio-relation extraction via word embeddings. In
the surge of deep learning, Socher et al (2012)
and Khashabi (2013) use pre-trained word embed-
dings as input for Matrix-Vector Recursive Neu-
ral Networks (MV-RNN) to learn compositional
structures for RE. However, none of these works
evaluate word embeddings for domain adaptation
of RE which is our main focus in this paper.
Regarding domain adaptation, in representation
learning, Blitzer et al (2006) propose structural
correspondence learning (SCL) while Huang and
Yates (2010) attempt to learn a multi-dimensional
feature representation. Unfortunately, these meth-
ods require unlabeled target domain data which
are unavailable in our single-system setting of DA.
Daum?e III (2007) proposes an easy adaptation
framework (EA) which is later extended to a semi-
supervised version (EA++) to incorporate unla-
1
domain overfitting (Jiang and Zhai, 2007b)
69
beled data (Daum?e III et al, 2010). In terms of
word embeddings for DA, recently, Xiao and Guo
(2013) present a log-bilinear language adaptation
framework for sequential labeling tasks. However,
these methods assume some labeled data in target
domains and are thus not applicable in our setting
of unsupervised DA. Above all, we move one step
further by evaluating the effectiveness of word em-
beddings on domain adaptation for RE which is
very different from the principal topic of sequence
labeling in the previous research.
4 Word Representations
We consider two types of word representations and
use them as additional features in our DA sys-
tem, namely Brown word clustering (Brown et
al., 1992) and word embeddings (Bengio et al,
2001). While word clusters can be recognized
as an one-hot vector representation over a small
vocabulary, word embeddings are dense, low-
dimensional, and real-valued vectors (distributed
representations). Each dimension of the word em-
beddings expresses a latent feature of the words,
hopefully reflecting useful semantic and syntactic
regularities (Turian et al, 2010). We investigate
word embeddings induced by two typical language
models: Collobert and Weston (2008) embeddings
(C&W) (Collobert and Weston, 2008; Turian et
al., 2010) and Hierarchical log-bilinear embed-
dings (HLBL) (Mnih and Hinton, 2007; Mnih and
Hinton, 2009; Turian et al, 2010).
5 Feature Set
5.1 Baseline Feature Set
Sun et al (2011) utilize the full feature set from
(Zhou et al, 2005) plus some additional features
and achieve the state-of-the-art feature-based RE
system. Unfortunately, this feature set includes
the human-annotated (gold-standard) information
on entity and mention types which is often miss-
ing or noisy in reality (Plank and Moschitti, 2013).
This issue becomes more serious in our setting of
single-system DA where we have a single source
domain with multiple dissimilar target domains
and an automatic system able to recognize entity
and mention types very well in different domains
may not be available. Therefore, following the set-
tings of Plank and Moschitti (2013), we will only
assume entity boundaries and not rely on the gold
standard information in the experiments. We ap-
ply the same feature set as Sun et al (2011) but
remove the entity and mention type information
2
.
5.2 Lexical Feature Augmentation
While Sun et al (2011) show that adding word
clusters to the heads of the two mentions is the
most effective way to improve the generaliza-
tion accuracy, the right lexical features into which
word embeddings should be introduced to obtain
the best adaptability improvement are unexplored.
Also, which dimensionality of which word embed-
ding should we use with which lexical features?
In order to answer these questions, following Sun
et al (2011), we first group lexical features into 4
groups and rank their importance based on linguis-
tic intuition and illustrations of the contributions
of different lexical features from various feature-
based RE systems. After that, we evaluate the ef-
fectiveness of these lexical feature groups for word
embedding augmentation individually and incre-
mentally according to the rank of importance. For
each of these group combinations, we assess the
system performance with different numbers of di-
mensions for both C&W and HLBL word embed-
dings. Let M1 and M2 be the first and second men-
tions in the relation. Table 1 describes the lexical
feature groups.
Rank Group Lexical Features
1 HM HM1 (head of M1)
HM2 (head of M2)
2 BagWM WM1 (words in M1)
WM2 (words in M2)
3 HC heads of chunks in context
4 BagWC words of context
Table 1: Lexical feature groups ordered by importance.
6 Experiments
6.1 Tools and Data
Our relation extraction system is hierarchical
(Bunescu and Mooney, 2005b; Sun et al, 2011)
and apply maximum entropy (MaxEnt) in the
MALLET
3
toolkit as the machine learning tool.
For Brown word clusters, we directly apply the
clustering trained by Plank and Moschitti (2013)
2
We have the same observation as Plank and Moschitti
(2013) that when the gold-standard labels are used, the
impact of word representations is limited since the gold-
standard information seems to dominate. However, whenever
the gold labels are not available or inaccurate, the word rep-
resentations would be useful for improving adaptability per-
formance. Moreover, in all the cases, regularization methods
are still effective for domain adaptation of RE.
3
http://mallet.cs.umass.edu/
70
In-domain (bn+nw) Out-of-domain (bc development set)
System C&W,25 C&W,50 C&W,100 HLBL,50 HLBL,100 C&W,25 C&W,50 C&W,100 HLBL,50 HLBL,100
1
Baseline 51.4 51.4 51.4 51.4 51.4 49.0 49.0 49.0 49.0 49.0
2
1+HM ED 54.0(+2.6) 54.1(+2.7) 55.7(+4.3) 53.7(+2.3) 55.2(+3.8) 51.5(+2.5) 52.7(+3.7) 52.5(+3.5) 50.2(+1.2) 50.6(+1.6)
3
1+BagWM ED 52.3(+0.9) 50.9(-0.5) 51.5(+0.1) 51.8(+0.4) 52.5(+1.1) 48.5(-0.5) 48.9(-0.1) 48.6(-0.4) 48.7(-0.3) 49.0(+0.0)
4
1+HC ED 51.3(-0.1) 50.9(-0.5) 48.3(-3.1) 50.8(-0.6) 49.8(-1.6) 44.9(-4.1) 45.8(-3.2) 45.8(-3.2) 48.7(-0.3) 47.3(-1.7)
5
1+BagWC ED 51.5(+0.1) 50.8(-0.6) 49.5(-1.9) 51.4(+0.0) 50.3(-1.1) 48.3(-0.7) 46.3(-2.7) 44.0(-5.0) 46.6(-2.4) 44.8(-4.2)
6
2+BagWM ED 54.3(+2.9) 53.2(+1.8) 53.2(+1.8) 54.0(+2.6) 53.8(+2.4) 52.5(+3.5) 51.4(+2.4) 50.6(+1.6) 50.0(+1.0) 48.6(-0.4)
7
6+HC ED 53.4(+2.0) 52.3(+0.9) 52.7(+1.3) 54.2(+2.8) 53.1(+1.7) 50.5(+1.5) 50.9(+1.9) 48.4(-0.6) 50.0(+1.0) 48.9(-0.1)
8
7+BagWC ED 53.4(+2.0) 52.2(+0.8) 50.8(-0.6) 53.5(+2.1) 53.6(+2.2) 49.2(+0.2) 50.7(+1.7) 49.2(+0.2) 47.9(-1.1) 49.5(+0.5)
Table 2: In-domain and Out-of-domain performance for different embedding features. The cells in bold are the best results.
to facilitate system comparison later. We evalu-
ate C&W word embeddings with 25, 50 and 100
dimensions as well as HLBL word embeddings
with 50 and 100 dimensions that are introduced
in Turian et al (2010) and can be downloaded
here
4
. The fact that we utilize the large, general
and unbiased resources generated from the previ-
ous works for evaluation not only helps to verify
the effectiveness of the resources across different
tasks and settings but also supports our setting of
single-system DA.
We use the ACE 2005 corpus for DA experi-
ments (as in Plank and Moschitti (2013)). It in-
volves 6 relation types and 6 domains: broadcast
news (bn), newswire (nw), broadcast conversation
(bc), telephone conversation (cts), weblogs (wl)
and usenet (un). We follow the standard prac-
tices on ACE (Plank and Moschitti, 2013) and use
news (the union of bn and nw) as the source do-
main and bc, cts and wl as our target domains. We
take half of bc as the only target development set,
and use the remaining data and domains for testing
purposes (as they are small already). As noted in
Plank and Moschitti (2013), the distributions of re-
lations as well as the vocabularies of the domains
are quite different.
6.2 Evaluation of Word Embedding Features
We investigate the effectiveness of word embed-
dings on lexical features by following the proce-
dure described in Section 5.2. We test our system
on two scenarios: In-domain: the system is trained
and evaluated on the source domain (bn+nw, 5-
fold cross validation); Out-of-domain: the system
is trained on the source domain and evaluated on
the target development set of bc (bc dev). Table
2 presents the F measures of this experiment
5
(the
4
http://metaoptimize.com/projects/
wordreprs/
5
All the in-domain improvement in rows 2, 6, 7 of Table
2 are significant at confidence levels ? 95%.
suffix ED in lexical group names is to indicate the
embedding features).
From the tables, we find that for C&W and
HLBL embeddings of 50 and 100 dimensions, the
most effective way to introduce word embeddings
is to add embeddings to the heads of the two men-
tions (row 2; both in-domain and out-of-domain)
although it is less pronounced for HLBL embed-
ding with 50 dimensions. Interestingly, for C&W
embedding with 25 dimensions, adding the em-
bedding to both heads and words of the two men-
tions (row 6) performs the best for both in-domain
and out-of-domain scenarios. This is new com-
pared to the word cluster features where the heads
of the two mentions are always the best places for
augmentation (Sun et al, 2011). It suggests that
a suitable amount of embeddings for words in the
mentions might be useful for the augmentation of
the heads and inspires further exploration. Intro-
ducing embeddings to words of mentions alone
has mild impact while it is generally a bad idea to
augment chunk heads and words in the contexts.
Comparing C&W and HLBL embeddings is
somehow more complicated. For both in-domain
and out-of-domain settings with different num-
bers of dimensions, C&W embedding outperforms
HLBL embedding when only the heads of the
mentions are augmented while the degree of neg-
ative impact of HLBL embedding on chunk heads
as well as context words seems less serious than
C&W?s. Regarding the incremental addition of
features (rows 6, 7, 8), C&W is better for the out-
of-domain performance when 50 dimensions are
used, whereas HLBL (with both 50 and 100 di-
mensions) is more effective for the in-domain set-
ting. For the next experiments, we will apply the
C&W embedding of 50 dimensions to the heads
of the mentions for its best out-of-domain perfor-
mance.
71
6.3 Domain Adaptation with Word
Embeddings
This section examines the effectiveness of word
representations for RE across domains. We evalu-
ate word cluster and embedding (denoted by ED)
features by adding them individually as well as
simultaneously into the baseline feature set. For
word clusters, we experiment with two possibil-
ities: (i) only using a single prefix length of 10
(as Plank and Moschitti (2013) did) (denoted by
WC10) and (ii) applying multiple prefix lengths of
4, 6, 8, 10 together with the full string (denoted by
WC). Table 3 presents the system performance (F
measures) for both in-domain and out-of-domain
settings.
System
In-domain bc cts wl
Baseline(B) 51.4 49.7 41.5 36.6
B+WC10 52.3(+0.9) 50.8(+1.1) 45.7(+4.2) 39.6(+3)
B+WC 53.7(+2.3) 52.8(+3.1) 46.8(+5.3) 41.7(+5.1)
B+ED 54.1(+2.7) 52.4(+2.7) 46.2(+4.7) 42.5(+5.9)
B+WC+ED 55.5(+4.1) 53.8(+4.1) 47.4(+5.9) 44.7(+8.1)
Table 3: Domain Adaptation Results with Word Represen-
tations. All the improvements over the baseline in Table 3 are
significant at confidence level ? 95%.
The key observations from the table are:
(i): The baseline system achieves a performance
of 51.4% within its own domain while the per-
formance on target domains bc, cts, wl drops to
49.7%, 41.5% and 36.6% respectively. Our base-
line performance is worse than that of Plank and
Moschitti (2013) only on the target domain cts and
better in the other cases. This might be explained
by the difference between our baseline feature set
and the feature set underlying their kernel-based
system. However, the performance order across
domains of the two baselines are the same. Be-
sides, the baseline performance is improved over
all target domains when the system is enriched
with word cluster features of the 10 prefix length
only (row 2).
(ii): Over all the target domains, the perfor-
mance of the system augmented with word cluster
features of various granularities (row 3) is supe-
rior to that when only cluster features for the pre-
fix length 10 are added (row 2). This is significant
(at confidence level ? 95%) for domains bc and
wl and verifies our assumption that various granu-
larities for word cluster features are more effective
than a single granularity for domain adaptation of
RE.
(iii): Row 4 shows that word embedding itself is
also very useful for domain adaptation in RE since
it improves the baseline system for all the target
domains.
(iv): In row 5, we see that the addition of both
word cluster and word embedding features im-
proves the system further and results in the best
performance over all target domains (this is sig-
nificant with confidence level ? 95% in domains
bc and wl). The result suggests that word embed-
dings seem to capture different information from
word clusters and their combination would be ef-
fective to generalize relation extractors across do-
mains. However, in domain cts, the improvement
that word embeddings provide for word clusters is
modest. This is because the RCV1 corpus used to
induce the word embeddings (Turian et al, 2010)
does not cover spoken language words in cts very
well.
(v): Finally, the in-domain performance is also
improved consistently demonstrating the robust-
ness of word representations (Plank and Moschitti,
2013).
6.4 Domain Adaptation with Regularization
All the experiments we have conducted so far do
not apply regularization for training. In this sec-
tion, in order to evaluate the effect of regulariza-
tion on the generalization capacity of relation ex-
tractors across domains, we replicate all the ex-
periments in Section 6.3 but apply regularization
when relation extractors are trained
6
. Table 4
presents the results.
System
In-domain bc cts wl
Baseline(B) 56.2 55.5 48.7 42.2
B+WC10 57.5(+1.3) 57.3(+1.8) 52.3(+3.6) 45.0(+2.8)
B+WC 58.9(+2.7) 58.4(+2.9) 52.8(+4.1) 47.3(+5.1)
B+ED 58.9(+2.7) 59.5(+4.0) 52.6(+3.9) 48.6(+6.4)
B+WC+ED 59.4(+3.2) 59.8(+4.3) 52.9(+4.2) 49.7(+7.5)
Table 4: Domain Adaptation Results with Regularization.
All the improvements over the baseline in Table 4 are signif-
icant at confidence level ? 95%.
For this experiment, every statement in (ii), (iii),
(iv) and (v) of Section 6.3 also holds. More impor-
tantly, the performance in every cell of Table 4 is
significantly better than the corresponding cell in
Table 3 (5% or better gain in F measure, a sig-
nificant improvement at confidence level ? 95%).
This demonstrates the effectiveness of regulariza-
tion for RE in general and for domain adaptation
of RE specifically.
6
We use a L2 regularizer with the regularization parame-
ter of 0.5 for its best experimental results.
72
References
Yoshua Bengio, R?ejean Ducharme, and Pascal Vincent.
2001. A Neural Probabilistic Language Model. In
Advances in Neural Information Processing Systems
(NIPS?13), pages 932-938, MIT Press, 2001.
Yoshua Bengio, R?ejean Ducharme, Pascal Vincent, and
Christian Jauvin. 2003. A Neural Probabilistic Lan-
guage Model. In Journal of Machine Learning Re-
search (JMLR), 3, pages 1137-1155, 2003.
John Blitzer, Ryan McDonald, and Fernando Pereira.
2006. Domain Adaptation with Structural Corre-
spondence Learning. In Proceedings of the 2006
Conference on Empirical Methods in Natural Lan-
guage Processing, Sydney, Australia.
John Blitzer, Mark Dredze, and Fernando Pereira.
2007. Biographies, Bollywood, Boom-boxes, and
Blenders: Domain Adaptation for Sentiment Classi-
fication. In Proceedings of the ACL, pages 440-447,
Prague, Czech Republic, June 2007.
John Blitzer, Dean Foster, and Sham Kakade. 2011.
Domain Adaptation with Coupled Subspaces. In
Proceedings of the 14th International Conference on
Artificial Intelligence and Statistics, pages 173-181,
Fort Lauderdale, FL, USA.
Elizabeth Boschee, Ralph Weischedel, and Alex Zama-
nian. 2005. Automatic Information Extraction. In
Proceedings of the International Conference on In-
telligence Analysis.
Peter F. Brown, Peter V. deSouza, Robert L. Mercer,
Vincent J. Della Pietra, and Jenifer C. Lai. 1992.
Class-Based n-gram Models of Natural Language.
In Journal of Computational Linguistics, Volume 18,
Issue 4, pages 467-479, December 1992.
Razvan C. Bunescu and Raymond J. Mooney. 2005a.
A Shortest Path Dependency Kenrel for Relation Ex-
traction. In Proceedings of HLT/EMNLP.
Razvan C. Bunescu and Raymond J. Mooney. 2005b.
Subsequence Kernels for Relation Extraction. In
Proceedings of NIPS.
Yee S. Chan and Dan Roth. 2010. Exploiting Back-
ground Knowledge for Relation Extraction. In
Proceedings of the 23rd International Conference
on Computational Linguistics (Coling 2010), pages
152-160, Beijing, China, August.
Ronan Collobert and Jason Weston. 2008. A Unied Ar-
chitecture for Natural Language Processing: Deep
Neural Networks with Multitask Learning. In Inter-
national Conference on Machine Learning, ICML,
2008.
Hal Daum?e III. 2007. Frustratingly Easy Domain
Adaptation. In Proceedings of the ACL, pages 256-
263, Prague, Czech Republic, June 2007.
Hal Daum?e III, Abhishek Kumar and Avishek Saha.
2010. Co-regularization Based Semi-supervised
Domain Adaptation. In Advances in Neural Infor-
mation Processing Systems 23 (2010).
Ralph Grishman, David Westbrook and Adam Meyers.
2005. NYU?s English ACE 2005 System Descrip-
tion. ACE 2005 Evaluation Workshop.
Fei Huang and Alexander Yates. 2010. Explor-
ing Representation-Learning Approaches to Domain
Adaptation. In Proceedings of the 2010 Workshop
on Domain Adaptation for Natural Language Pro-
cessing, pages 23-30, Uppsala, Sweden, July 2010.
Jing Jiang and ChengXiang Zhai. 2007a. A Sys-
tematic Exploration of the Feature Space for Re-
lation Extraction. In Proceedings of the Hu-
man Language Technology Conference of the North
American Chapter of the Association for Compu-
tational Linguistics (NAACL-HLT?07), pages 113-
120, 2007.
Jing Jiang and ChengXiang Zhai. 2007b. A Two-stage
Approach to Domain Adaptation for Statistical Clas-
sifiers. In Proceedings of the ACM 16th Confer-
ence on Information and Knowledge Management
(CIKM?07), pages 401-410, 2007.
Nanda Kambhatla. 2004. Combining Lexical, Syntac-
tic, and Semantic Features with Maximum Entropy
Models for Information Extraction. In Proceedings
of ACL-04.
Daniel Khashabi. 2013. On the Recursive Neural Net-
works for Relation Extraction and Entity Recogni-
tion. Technical Report (May, 2013), UIUC.
Pavel Kuksa, Yanjun Qi, Bing Bai, Ronan Collobert,
Jason Weston, Vladimir Pavlovic, and Xia Ning.
2010. Semi-Supervised Abstraction-Augmented
String Kernel for Multi-Level Bio-Relation Extrac-
tion. In Proceedings of the 2010 European Confer-
ence on Machine Learning and Knowledge Discov-
ery in Databases, Part II (ECML PKDD?10), pages
128-144, 2010.
Andrew L. Maas and Andrew Y. Ng. 2010. A Proba-
bilistic Model for Semantic Word Vectors. In NIPS
Workshop on Deep Learning and Unsupervised Fea-
ture Learning, 2010.
Andriy Mnih and Geoffrey Hinton. 2007. Three new
Graphical Models for Statistical Language Mod-
elling. In Proceedings of ICML?07, pages 641-648,
Corvallis, OR, 2007.
Andriy Mnih and Geoffrey Hinton. 2009. A Scalable
Hierarchical Distributed Language Model. In NIPS,
page 1081-1088.
Truc-Vien T. Nguyen, Alessandro Moschitti, and
Giuseppe Riccardi. 2009. Convolution Kernels on
Constituent, Dependency and Sequential Structures
for Relation Extraction. In Proceedings of EMNLP
09, pages 1378-1387, Stroudsburg, PA, USA.
73
Barbara Plank and Alessandro Moschitti. 2013. Em-
bedding Semantic Similarity in Tree Kernels for Do-
main Adaptation of Relation Extraction. In Proceed-
ings of the ACL 2013, pages 1498-1507, Sofia, Bul-
garia.
Longhua Qian, Guodong Zhou, Qiaoming Zhu and
Peide Qian. 2008. Exploiting Constituent Dde-
pendencies for Tree Kernel-based Semantic Relation
Extraction. In Proceedings of COLING, pages 697-
704, Manchester.
Tobias Schnabel and Hinrich Sch?utze. 2014. FLORS:
Fast and Simple Domain Adaptation for Part-of-
Speech Tagging. In Transactions of the Associa-
tion for Computational Linguistics, 2 (2014), pages
1526.
Richard Socher, Brody Huval, Christopher D. Man-
ning, and Andrew Y. Ng. 2012. Semantic Compo-
sitionality through Recursive Matrix-Vector Spaces.
In Proceedings EMNLP-CoNLL?12, pages 1201-
1211, Jeju Island, Korea, July 2012.
Ang Sun, Ralph Grishman, and Satoshi Sekine. 2011.
Semi-supervised Relation Extraction with Large-
scale Word Clustering. In Proceedings of ACL-
HLT, pages 521-529, Portland, Oregon, USA.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: A simple and general method
for semi-supervised learning. In Proceedings of the
48th Annual Meeting of the Association for Compu-
tational Linguistics (ACL?10), pages 384-394, Upp-
sala, Sweden, July, 2010.
Min Xiao and Yuhong Guo. 2013. Domain Adaptation
for Sequence Labeling Tasks with a Probabilistic
Language Adaptation Model. In Proceedings of the
30th International Conference on Machine Learning
(ICML-13), pages 293-301, 2013.
Dmitry Zelenko, Chinatsu Aone, and Anthony
Richardella. 2003. Kernel Methods for Relation
Extraction. Journal of Machine Learning Research,
3:10831106.
Min Zhang, Jie Zhang, Jian Su, and GuoDong Zhou.
2006. A Composite Kernel to Extract Relations be-
tween Entities with both Flat and Structured Fea-
tures. In Proceedings of COLING-ACL-06, pages
825-832, Sydney.
Guodong Zhou, Jian Su, Jie Zhang, and Min Zhang.
2005. Exploring various Knowledge in Relation Ex-
traction. In Proceedings of ACL?05, pages 427-434,
Ann Arbor, USA, 2005.
74
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 732?738,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
Infusion of Labeled Data into Distant Supervision for Relation Extraction
Maria Pershina
+
Bonan Min
? ?
Wei Xu
#
Ralph Grishman
+
+
New York University, New York, NY
{pershina, grishman}@cs.nyu.edu
?
Raytheon BBN Technologies, Cambridge, MA
bmin@bbn.com
#
University of Pennsylvania, Philadelphia, PA
xwe@cis.upenn.edu
Abstract
Distant supervision usually utilizes only
unlabeled data and existing knowledge
bases to learn relation extraction models.
However, in some cases a small amount
of human labeled data is available. In this
paper, we demonstrate how a state-of-the-
art multi-instance multi-label model can
be modified to make use of these reli-
able sentence-level labels in addition to
the relation-level distant supervision from
a database. Experiments show that our ap-
proach achieves a statistically significant
increase of 13.5% in F-score and 37% in
area under the precision recall curve.
1 Introduction
Relation extraction is the task of tagging semantic
relations between pairs of entities from free text.
Recently, distant supervision has emerged as an
important technique for relation extraction and has
attracted increasing attention because of its effec-
tive use of readily available databases (Mintz et
al., 2009; Bunescu and Mooney, 2007; Snyder and
Barzilay, 2007; Wu and Weld, 2007). It automat-
ically labels its own training data by heuristically
aligning a knowledge base of facts with an unla-
beled corpus. The intuition is that any sentence
which mentions a pair of entities (e
1
and e
2
) that
participate in a relation, r, is likely to express the
fact r(e
1
,e
2
) and thus forms a positive training ex-
ample of r.
One of most crucial problems in distant super-
vision is the inherent errors in the automatically
generated training data (Roth et al, 2013). Ta-
ble 1 illustrates this problem with a toy exam-
ple. Sophisticated multi-instance learning algo-
rithms (Riedel et al, 2010; Hoffmann et al, 2011;
?
Most of the work was done when this author was at
New York University
Surdeanu et al, 2012) have been proposed to ad-
dress the issue by loosening the distant supervision
assumption. These approaches consider all men-
tions of the same pair (e
1
,e
2
) and assume that at-
least-one mention actually expresses the relation.
On top of that, researchers further improved per-
formance by explicitly adding preprocessing steps
(Takamatsu et al, 2012; Xu et al, 2013) or addi-
tional layers inside the model (Ritter et al, 2013;
Min et al, 2013) to reduce the effect of training
noise.
True Positive ... to get information out of captured
al-Qaida leader Abu Zubaydah.
False Positive ...Abu Zubaydah and former Taliban
leader Jalaluddin Haqqani ...
False Negative ...Abu Zubaydah is one of Osama bin
Laden?s senior operational planners...
Table 1: Classic errors in the training data gener-
ated by a toy knowledge base of only one entry
personTitle(Abu Zubaydah, leader).
However, the potential of these previously pro-
posed approaches is limited by the inevitable
gap between the relation-level knowledge and the
instance-level extraction task. In this paper, we
present the first effective approach, Guided DS
(distant supervision), to incorporate labeled data
into distant supervision for extracting relations
from sentences. In contrast to simply taking the
union of the hand-labeled data and the corpus la-
beled by distant supervision as in the previous
work by Zhang et al (2012), we generalize the
labeled data through feature selection and model
this additional information directly in the latent
variable approaches. Aside from previous semi-
supervised work that employs labeled and unla-
beled data (Yarowsky, 2013; Blum and Mitchell,
1998; Collins and Singer, 2011; Nigam, 2001, and
others), this is a learning scheme that combines
unlabeled text and two training sources whose
quantity and quality are radically different (Liang
et al, 2009).
To demonstrate the effectiveness of our pro-
732
Guideline g = {g
i
|i = 1, 2, 3}: Relation r(g)
types of entities, dependency path, span word (optional)
person person, nsubj ?? dobj, married personSpouse
person organization, nsubj ?? prep of , became personMemberOf
organization organization, nsubj ?? prep of , company organizationSubsidiaries
person person, poss?? appos, sister personSiblings
person person, poss?? appos, father personParents
person title,? nn personTitle
organization person, prep of ? appos? organizationTopMembersEmployees
person cause, nsubj ?? prep of personCauseOfDeath
person number,? appos personAge
person date, nsubjpass?? prep on? num personDateOfBirth
Table 2: Some examples from the final set G of extracted guidelines.
posed approach, we extend MIML (Surdeanu et
al., 2012), a state-of-the-art distant supervision
model and show a significant improvement of
13.5% in F-score on the relation extraction bench-
mark TAC-KBP (Ji and Grishman, 2011) dataset.
While prior work employed tens of thousands of
human labeled examples (Zhang et al, 2012) and
only got a 6.5% increase in F-score over a logistic
regression baseline, our approach uses much less
labeled data (about 1/8) but achieves much higher
improvement on performance over stronger base-
lines.
2 The Challenge
Simply taking the union of the hand-labeled data
and the corpus labeled by distant supervision is not
effective since hand-labeled data will be swamped
by a larger amount of distantly labeled data. An
effective approach must recognize that the hand-
labeled data is more reliable than the automatically
labeled data and so must take precedence in cases
of conflict. Conflicts cannot be limited to those
cases where all the features in two examples are
the same; this would almost never occur, because
of the dozens of features used by a typical relation
extractor (Zhou et al, 2005). Instead we propose
to perform feature selection to generalize human
labeled data into training guidelines, and integrate
them into latent variable model.
2.1 Guidelines
The sparse nature of feature space dilutes the dis-
criminative capability of useful features. Given
the small amount of hand-labeled data, it is im-
portant to identify a small set of features that are
general enough while being capable of predicting
quite accurately the type of relation that may hold
between two entities.
We experimentally tested alternative feature
sets by building supervised Maximum Entropy
(MaxEnt) models using the hand-labeled data (Ta-
ble 3), and selected an effective combination of
three features from the full feature set used by Sur-
deanu et al, (2011):
? the semantic types of the two arguments (e.g.
person, organization, location, date, title, ...)
? the sequence of dependency relations along the
path connecting the heads of the two arguments
in the dependency tree.
? a word in the sentence between the two argu-
ments
These three features are strong indicators of the
type of relation between two entities. In some
cases the semantic types of the arguments alone
narrows the possibilities to one or two relation
types. For example, entity types such as person
and title often implies the relation personTitle.
Some lexical items are clear indicators of partic-
ular relations, such as ?brother? and ?sister? for a
sibling relationship
We extract guidelines from hand-labeled data.
Each guideline g={g
i
|i=1,2,3} consists of a pair
of semantic types, a dependency path, and option-
ally a span word and is associated with a partic-
ular relation r(g). We keep only those guidelines
Model Precision Recall F-score
MaxEnt
all
18.6 6.3 9.4
MaxEnt
two
24.13 10.75 14.87
MaxEnt
three
40.27 12.40 18.97
Table 3: Performance of a MaxEnt, trained on
hand-labeled data using all features (Surdeanu et
al., 2011) vs using a subset of two (types of en-
tities, dependency path), or three (adding a span
word) features, and evaluated on the test set.
733
which make the correct prediction for all and at
least k=3 examples in the training corpus (thresh-
old 3 was obtained by running experiments on the
development dataset). Table 2 shows some exam-
ples in the final set G of extracted guidelines.
3 Guided DS
Our goal is to jointly model human-labeled ground
truth and structured data from a knowledge base
in distant supervision. To do this, we extend the
MIML model (Surdeanu et al, 2012) by adding a
new layer as shown in Figure 1.
The input to the model consists of (1) distantly
supervised data, represented as a list of n bags
1
with a vector y
i
of binary gold-standard labels, ei-
ther Positive(P ) or Negative(N) for each rela-
tion r?R; (2) generalized human-labeled ground
truth, represented as a set G of feature conjunc-
tions g={g
i
|i=1,2,3} associated with a unique re-
lation r(g). Given a bag of sentences, x
i
, which
mention an ith entity pair (e
1
, e
2
), our goal is to
correctly predict which relation is mentioned in
each sentence, or NR if none of the relations under
consideration are mentioned. The vector z
i
con-
tains the latent mention-level classifications for the
ith entity pair. We introduce a set of latent vari-
ables h
i
which model human ground truth for each
mention in the ith bag and take precedence over
the current model assignment z
i
.
G
|R|
|xi|
n
zi
hi
yi
xi
9
>
=
>
;
{
relation
level
mention
level
Figure 1: Plate diagram of Guided DS
Let i, j be the index in the bag and the men-
tion level, respectively. We model mention-
level extraction p(z
ij
|x
ij
;w
z
), human relabel-
ing h
ij
(x
ij
, z
ij
) and multi-label aggregation
p(y
r
i
|h
i
;w
y
). We define:
? y
r
i
?{P,N} : r holds for the ith bag or not.
? x
ij
is the feature representation of the jth rela-
tion mention in the ith bag. We use the same set
of features as in Surdeanu et al (2012).
1
A bag is a set of mentions sharing same entity pair.
? z
ij
?R ? NR: a latent variable that denotes the
relation of the jth mention in the ith bag
? h
ij
?R ?NR: a latent variable that denotes the
refined relation of the mention x
ij
We define relabeled relations h
ij
as following:
h
ij
(x
ij
, z
ij
)=
{
r(g), if ?!g?G s.t.g={g
k
}?{x
ij
}
z
ij
, otherwise
Thus, relation r(g) is assigned to h
ij
iff there
exists a unique guideline g ? G, such that the
feature vector x
ij
contains all constituents of g,
i.e. entity types, a dependency path and maybe a
span word, if g has one. We use mention relation
z
ij
inferred by the model only in case no such a
guideline exists or there is more than one match-
ing guideline. We also define:
? w
z
is the weight vector for the multi-class rela-
tion mention-level classifier
2
? w
r
y
is the weight vector for the rth binary top-
level aggregation classifier (from mention labels
to bag-level prediction). We use w
y
to represent
w
1
y
,w
2
y
, . . . ,w
|R|
y
.
Our approach is aimed at improving the mention-
level classifier, while keeping the multi-instance
multi-label framework to allow for joint modeling.
4 Training
We use a hard expectation maximization algorithm
to train the model. Our objective function is to
maximize log-likelihood of the data:
LL(w
y
,w
z
) =
n
?
i=1
log p(y
i
|x
i
,w
y
,w
z
,G)
=
n
?
i=1
log
?
h
i
p(y
i
,h
i
|x
i
,w
y
,w
z
,G)
=
n
?
i=1
log
?
h
i
|h
i
|
?
j=1
p(h
ij
|x
ij
,w
z
,G)
?
r?P
i
?N
i
p(y
r
i
|h
i
,w
r
y
)
where the last equality is due to conditional
independence. Because of the non-convexity
of LL(w
y
,w
z
) we approximate and maximize
the joint log-probability p(y
i
,h
i
|x
i
,w
y
,w
z
,G) for
each entity pair in the database:
log p(y
i
,h
i
|x
i
,w
y
,w
z
,G)
=
|h
i
|
?
j=1
log p(h
ij
|x
ij
,w
z
,G)+
?
r?P
i
?N
i
log p(y
r
i
|h
i
,w
r
y
).
2
All classifiers are implemented using L2-regularized lo-
gistic regression with Stanford CoreNLP package.
734
Iteration 1 2 3 4 5 6 7 8
(a) Corrected relations: 2052 718 648 596 505 545 557 535
(b) Retrieved relations: 10219 860 676 670 621 599 594 592
Total relabelings 12271 1578 1324 1264 1226 1144 1153 1127
Table 4: Number of relabelings for each training iteration of Guided DS: (a) relabelings due to cor-
rected relations, e.g. personChildren? personSiblings (b) relabelings due to retrieved relations, e.g.
notRelated(NR)?personTitle
Algorithm 1 : Guided DS training
1: Phase 1: build set G of guidelines
2: Phase 2: EM training
3: for iteration = 1, . . . , T do
4: for i = 1, . . . , n do
5: for j = 1, . . . , |x
i
| do
6: z
?
ij
= argmax
z
ij
p(z
ij
|x
i
,y
i
,w
z
,w
y
)
7: h
?
ij
=
{
r(g), if ?!g?G :{g
k
}?{x
ij
}
z
ij
?
, otherwise
8: update h
i
with h
?
ij
9: end for
10: end for
11: w
?
z
=argmax
w
?
n
i=1
?
|x
i
|
j=1
log p(h
ij
|x
ij
,w)
12: for r ? R do
13: w
r?
y
=argmax
w
?
1?i?n s.t. r?P
i
?N
i
log p(y
r
i
|h
i
,w)
14: end for
15: end for
16: return w
z
,w
y
The pseudocode is presented as algorithm 1.
The following approximation is used for infer-
ence at step 6:
p(z
ij
|x
i
,y
i
,w
z
,w
y
) ? p(y
i
, z
ij
|x
i
,w
y
,w
z
)
? p(z
ij
|x
ij
,w
z
)p(y
i
|h
?
i
,w
y
)
= p(z
ij
|x
ij
,w
z
)
?
r?P
i
?N
i
p(y
r
i
|h
?
i
,w
r
y
),
where h
?
i
contains previously inferred and
maybe further relabeled mention labels for group
i (steps 5-10), with the exception of component j
whose label is replaced by z
ij
. In the M-step (lines
12-15) we optimize model parameters w
z
,w
y
,
given the current assignment of mention-level la-
bels h
i
.
Experiments show that Guided DS efficiently
learns new model, resulting in a drastically de-
creasing number of needed relabelings for further
iterations (Table 4). At the inference step we first
classify all mentions:
z
?
ij
= argmax
z?R?NR
p(z|x
ij
,w
z
)
Then final relation labels for ith entity tuple are
obtained via the top-level classifiers:
y
r?
i
= argmax
y?{P,N}
p(y|z
?
i
,w
r
y
)
5 Experiments
5.1 Data
We use the KBP (Ji and Grishman, 2011) dataset
3
which is preprocessed by Surdeanu et al (2011)
using the Stanford parser
4
(Klein and Manning,
2003). This dataset is generated by mapping
Wikipedia infoboxes into a large unlabeled corpus
that consists of 1.5M documents from KBP source
corpus and a complete snapshot of Wikipedia.
The KBP 2010 and 2011 data includes 200
query named entities with the relations they are
involved in. We used 40 queries as development
set and the rest 160 queries (3334 entity pairs that
express a relation) as the test set. The official KBP
evaluation is performed by pooling the system re-
sponses and manually reviewing each response,
producing a hand-checked assessment data. We
used KBP 2012 assessment data to generate guide-
lines since queries from different years do not
overlap. It contains about 2500 labeled sentences
of 41 relations, which is less than 0.09% of the
size of the distantly labeled dataset of 2M sen-
tences. The final set G consists of 99 guidelines
(section 2.1).
5.2 Models
We implement Guided DS on top of the MIML
(Surdeanu et al, 2012) code base
5
. Training
MIML on a simple fusion of distantly-labeled
and human-labeled datasets does not improve the
maximum F-score since this hand-labeled data is
swamped by a much larger amount of distant-
supervised data of much lower quality. Upsam-
pling the labeled data did not improve the perfor-
mance either. We experimented with different up-
sampling ratios and report best results using ratio
1:1 in Figure 2.
3
Available from Linguistic Data Consortium (LDC) at
http://projects.ldc.upenn.edu/kbp/data.
4
http://nlp.stanford.edu/software/lex-parser.shtml
5
Available at http://nlp.stanford.edu/software/mimlre.shtml.
735
a)
b) 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
Recall
Prec
ision
Student Version of MATLAB
0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
Recall
Pre
cis
ion
 
 
Guided DS
MIML
Mintz++
MultiR
Student Version of MATLAB
0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 0.50.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
Recall
Prec
ision
Student Version of MATLAB
0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 0.5
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
Recall
Pr
ec
isio
n
 
 
Guided DS
Semi?MIML
DS+upsampling
MaxEnt
Student Version of MATLAB
Model P R F1 AUC Model P R F1 AUC
MaxEnt 40.27 12.40 18.97 1.97 MultiR 30.64 19.79 24.05 6.4
DS+upsampling 32.26 24.31 27.72 12.00 Mintz++ 25.17 25.87 25.51 10.94
Semi MIML 30.02 26.21 27.98 12.31 MIML 28.06 28.64 28.35 11.74
Guided DS 31.9 32.46 32.19 16.1
Model P R F1 AUC
s
t
a
t
e
-
o
f
-
a
r
t
Model P R F1 AUC
MaxEnt 40.27 12.40 18.97 1.97 MultiR 30.64 19.79 24.05 6.4
DS+upsampling 32.26 24.31 27.72 12.00 Mintz++ 25.17 25.87 25.51 10.94
Semi MIML 30.02 26.21 27.98 12.31 MIML 28.06 28.64 28.35 11.74
Guided DS 31.9 32.46 32.19 16.1
b
a
s
e
l
i
n
e
Model P R F1 AUC
s
t
a
t
e
-
o
f
-
a
r
t
Model P R F1 AUC
MaxEnt 40.27 12.40 18.97 1.97 MultiR 30.64 19.79 24.05 6.4
DS+upsampling 32.26 24.31 27.72 12.00 Mintz++ 25.17 25.87 25.51 10.94
Semi MIML 30.02 26.21 27.98 12.31 MIML 28.06 28.64 28.35 11.74
Guided DS 31.9 32.46 32.19 16.1
b
a
s
e
l
i
n
e
Model P R F1 AUC
s
t
a
t
e
-
o
f
-
a
r
t
Model P R F1 AUC
MaxEnt 40.27 12.40 18.97 1.97 MultiR 30.64 19.79 24.05 6.4
DS+upsampling 32.26 24.31 27.72 12.00 Mintz++ 25.17 25.87 25.51 10.94
Semi MIML 30.02 26.21 27.98 12.31 MIML 28.06 28.64 28.35 11.74
Guided DS 31.9 32.46 32.19 16.1
1
Figure 2: Performance of Guided DS on KBP task compared to a) baselines: MaxEnt, DS+upsampling,
Semi-MIML (Min et al, 2013) b) state-of-art models: Mintz++ (Mintz et al, 2009), MultiR (Hoffmann
et al, 2011), MIML (Surdeanu et al, 2012)
Our baselines: 1) MaxEnt is a supervised maxi-
mum entropy baseline trained on a human-labeled
data; 2) DS+upsampling is an upsampling ex-
periment, where MIML was trained on a mix of
a distantly-labeled and human-labeled data; 3)
Semi-MIML is a recent semi-supervised exten-
sion. We also compare Guided DS with three
state-of-the-art models: 1) MultiR and 2) MIML
are two distant supervision models that support
multi-instance learning and overlapping relations;
3) Mintz++ is a single-instance learning algorithm
for distant supervision. The difference between
Guided DS and all other systems is significant
with p-value less than 0.05 according to a paired
t-test assuming a normal distribution.
5.3 Results
We scored our model against all 41 relations and
thus replicated the actual KBP evaluation. Figure
2 shows that our model consistently outperforms
all six algorithms at almost all recall levels and im-
proves the aximum F -score by more than 13.5%
relative to MIML (from 28.35% to 32.19%) as well
as increases the area under precision-recall curve
by more than 37% (from 11.74 to 16.1). Also,
Guided DS improves the overall recall by more
than 9% absolute (from 30.9% to 39.93%) at a
comparable level of precision (24.35% for MIML
vs 23.64% for Guided DS), while increases the
running time of MIML by only 3%. Thus, our
approach outperforms state-of-the-art model for
relation extraction using much less labeled data
that was used by Zhang et al, (2012) to outper-
form logistic regression baseline. Performance
of Guided DS also compares favorably with best
scored hand-coded systems for a similar task such
as Sun et al, (2011) system for KBP 2011, which
reports an F-score of 25.7%.
6 Conclusions and Future Work
We show that relation extractors trained with dis-
tant supervision can benefit significantly from a
small number of human labeled examples. We
propose a strategy to generate and select guide-
lines so that they are more generalized forms of
labeled instances. We show how to incorporate
these guidelines into an existing state-of-art model
for relation extraction. Our approach significantly
improves performance in practice and thus opens
up many opportunities for further research in RE
where only a very limited amount of labeled train-
ing data is available.
Acknowledgmen s
Supported by the Intelligence Advanced Research
Projects Activity ( IARPA) via Air Force Research
Laboratory (AFRL) contract number FA8650-10-
C-7058. The U.S. Government is authorized to
reproduce and distribute reprints for Governmen-
tal purposes notwithstanding any copyright anno-
tation thereon. The views and conclusions con-
tained herein are those of the authors and should
not be interpreted as necessarily representing the
official policies or endorsements, either expressed
or implied, of IARPA, AFRL, or the U.S. Govern-
ment.
736
References
Avrim Blum and Tom M. Mitchell. 1998. Combin-
ing labeled and unlabeled sata with co-training. In
Proceedings of the 11th Annual Conference on Com-
putational Learning Theory (COLT), pages 92?100.
Razvan C. Bunescu and Raymond J. Mooney. 2007.
Learning to extract relations from the web using
minimal supervision. In Proceedings of the 45th An-
nual Meeting of the Association for Computational
Linguistics (ACL).
Michael Collins and Yorav Singer. 1999. Unsuper-
vised models for named entity classification. Pro-
ceedings of the Conference on Empirical Methods
in Natural Language Processing (EMNLP-VLC). ,
Mark Craven and Johan Kumlien. 1999. Constructing
biological knowledge bases by extracting informa-
tion from text sources. In Proceedings of the Sev-
enth International Conference on Intelligent Systems
for Molecular Biology (ISMB), pages 77?86.
Oren Etzioni, Michele Banko, Stephen Soderland, and
Daniel S. Weld. 2008. Open information extrac-
tion from the web. Communications of the ACM,
51(12):68?74.
Raphael Hoffmann, Congle Zhang, and Daniel S.
Weld. 2010. Learning 5000 relational extractors.
In Proceedings of the 49th Annual Meetings of the
Association for Computational Linguistics (ACL),
pages 286?295.
Raphael Hoffmann, Congle Zhang, Xiao Ling,
Luke S. Zettlemoyer, and Daniel S. Weld. 2011.
Knowledge-based weak supervision for information
extraction of overlapping relations. In Proceedings
of the 49th Annual Meeting of the Association for
Computational Linguistics (ACL), pages 541?550.
Heng Ji and Ralph Grishman. 2011. Knowledge base
population: Successful approaches and challenges.
In Proceedings of the 49th Annual Meeting of the
Association for Computational Linguistics (ACL),
pages 1148?1158.
Heng Ji, Ralph Grishman, and Hoa Trang Dang. 2011.
Overview of the TAC-2011 knowledge base popula-
tion track. In Text Analysis Conference Workshop.
Dan Klein and Christopher D. Manning. 2003. Ac-
curate unlexicalized parsing. In Proceedings of the
41th Annual Meetings of the Association for Com-
putational Linguistics (ACL).
Percy Liang, Michael I.Jordan and Dan Klein. 2009.
Learning From Measurements in Exponential Fami-
lies. In Proceedings of the 26th Annual International
Conference on Machine Learning (ICML), pages =
641?648
Bonan Min, Ralph Grishman, Li Wan, Chang Wang,
and David Gondek. 2013. Distant supervision for
relation extraction with an incomplete knowledge
base. In Proceedings of the Conference of the North
American Chapter of the Association for Computa-
tional Linguistics (NAACL).
Mike Mintz, Steven Bills, Rion Snow, and Daniel Ju-
rafsky. 2009. Distant supervision for relation ex-
traction without labeled data. In Proceedigns of the
47th Annual Meeting of the Association for Compu-
tational Linguistics and the 4th International Joint
Conference on Natural Language Processing (ACL),
pages 1003?1011.
Ramesh Nallapati. 2004. Discriminative models for
information retrieval. In Proceedigns of the 27th An-
nual International ACM SIGIR Conference on Re-
search and Development in Information Retrieval
(SIGIR), pages 64?71.
Truc-Vien T. Nguyen and Alessandro Moschitti. 2011.
End-to-end relation extraction using distant super-
vision from external semantic repositories. In Pro-
ceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL), pages
277?282.
Kamal Paul Nigam. 2001. Using Unlabeled Data to
Improve Text Classification. Ph.D. thesis, School of
Computer Science, Carnegie Mellon University.
Sebastian Riedel, Limin Yao, and Andrew McCallum.
2010. Modeling relations and their mentions with-
out labeled text. In Proceedigns of the European
Conference on Machine Learning and Principles
and Practice of Knowledge Discovery in Databases
(ECML/PKDD), pages 148?163.
Alan Ritter, Luke Zettlemoyer, Mausam, and Oren Et-
zioni. 2013. Modeling missing data in distant su-
pervision for information extraction. Transactions
of the Association for Computational Linguistics.
Benjamin Roth, Tassilo Barth, Michael Wiegand, and
Dietrich Klakow 2013. A Survey of Noise Reduc-
tion Methods for Distant Supervision. In Proceed-
ings of Conference on Information and Knowledge
Management (CIKM-AKBC).
Benjamin Snyder and Regina Barzilay 2007.
Database-text alignment via structured multilabel
classification. In Proceedings of IJCAI.
Ang Sun, Ralph Grishman, Wei Xu, and Bonan Min.
2011. New york university 2011 system for kbp slot
filling. In Text Analysis Conference (TAC-KBP).
Mihai Surdeanu, J. Turmo, and A. Ageno. 2006. A
hybrid approach for the acquisition of information
extraction patterns. In Proceedings of the 11th Con-
ference of the European Chapter of the Associate
for Computational Linguistics Workshop on Adap-
tive Text Extraction and Mining (EACL).
Mihai Surdeanu, Sonal Gupta, John Bauer, David Mc-
Closky, Angel X. Chang, Valentin I. Spitkovsky,
and Christopher D.Manning. 2011. Stanford?s
737
Distantly-Supervised Slot-Filling System. In Pro-
ceedings of the Text Analysis Conference (TAC-
KBP).
Mihai Surdeanu, Julie Tibshirani, Ramesh Nallapati,
and Christopher D. Manning. 2012. Multi-instance
multi-label learning for relation extraction. In Pro-
ceedings of the 50th Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL), pages
455?465.
Shingo Takamatsu, Issei Sato, and Hiroshi Nakagawa.
2012. Reducing wrong labels in distant supervi-
sion for relation extraction. In Proceedings of the
50th Annual Meeting of the Association for Compu-
tational Linguistics (ACL), pages 721?729.
Fei Wu and Daniel S. Weld. 2007. Autonomously se-
mantifying wikipedia. In Proceedings of the Inter-
national Conference on Information and Knowledge
Management (CIKM), pages 41?50.
Wei Xu, Raphael Hoffmann, Zhao Le, and Ralph Gr-
ishman. 2013. Filling knowledge base gaps for dis-
tant supervision of relation extraction. In Proceed-
ings of the 51th Annual Meeting of the Association
for Computational Linguistics (ACL).
David Yarowsky. 1995. Unsupervised word sense dis-
ambiguation rivaling supervised methods. In Pro-
ceedings of the 33th Annual Meeting of the Associa-
tion for Computational Linguistics (ACL).
Ce Zhang, Feng Niu, Christopher R?e, and Jude Shav-
lik. 2012. Big data versus the crowd: Looking for
relationships in all the right places. In Proceedings
of the 50th Annual Meeting of the Association for
Computational Linguistics, pages 825?834. Associ-
ation for Computational Linguistics.
Guodong Zhou, Jian Su, Jie Zhang, and Min Zhang.
2005. Exploring various knowledge in relation ex-
traction. In Proceedings of the Annual Meeting
of the Association for Computational Linguistics
(ACL).
738
Proceedings of the Second Workshop on NLP Challenges in the Information Explosion Era (NLPIX 2010), pages 60?68,
Beijing, August 2010
Large Corpus-based Semantic Feature Extraction  for Pronoun Coreference  
Shasha Liao Dept. of Computer Science New York University liaoss@cs.nyu.edu 
Ralph Grishman Dept. of Computer Science New York University  grishman@cs.nyu.edu 
 Abstract 
Semantic information is a very important factor in coreference resolution. The combination of large corpora and ?deep? analysis procedures has made it possible to acquire a range of semantic informa-tion and apply it to this task. In this pa-per, we generate two statistically-based semantic features from a large corpus and measure their influence on pronoun coreference. One is contextual compati-bility, which decides if the antecedent can be used in the anaphor?s context; the other is role pair, which decides if the ac-tions asserted of the antecedent and the anaphor are likely to apply to the same entity. We apply a semantic labeling sys-tem and a baseline coreference system to a large corpus to generate semantic pat-terns and convert them into features in a MaxEnt model. These features produce an absolute gain of 1.5% to 1.7% in reso-lution accuracy (a 6% reduction in er-rors). To understand the limitations of these features, we also extract patterns from the test corpus, use these patterns to train a coreference model, and examine some of the cases where coreference still fails. We also compare the performance of patterns extracted from semantic role labeling and syntax. 1 Introduction Coreference resolution is the task of determining whether two phrases refer to the same entity. 
Coreference is critical to most NLP tasks, yet even the sub-problem of pronoun coreference remains very challenging. In principle, we need several types of information to identify the right antecedent. First, number and gender agreement constraints can narrow the candidate set.  If mul-tiple candidates remain, we would next use some sequence or syntactic features, like position, word, word salience and discourse focus. For example, whether an antecedent is in subject po-sition might be helpful because the subject is more likely to be referred to; or an entity that has been referred to repeatedly is more likely to be referred to again. However, these features do not suffice to pick the correct antecedent, and some-times similar syntactic structures might have quite different coreference solutions. For exam-ple, for the following two sentences: (1) The terrorist shot a 13-year-old boy; he was arrested after the attack. (2) The terrorist shot a 13-year-old boy; he was fatally wounded in the attack. it is likely that ?he? refers to ?terrorist? in (1) and ?boy? in (2). However, we cannot get the right antecedent using the features we mentioned above because the examples share the same ante-cedent words and syntactic structure.  People can still resolve these correctly because ?terrorist? is more likely to be arrested than ?boy?, and be-cause the one shooting is more likely to be ar-rested than the one being shot. In such cases, semantic constraints and prefer-ences are required for correct coreference resolu-tion. Methods for acquiring and using such knowledge are receiving increasing attention in 
60
recent work on anaphora resolution. Dagan and Itai (1990), Bean and Riloff (2004), Yang and Su (2007), and Ponzetto and Strube (2006) all ex-plored this task.  However, this task is difficult because it re-quires the acquisition of a large amount of se-mantic information. Furthermore, there is not universal agreement on the value of these seman-tic preferences for pronoun coreference. Kehler et al (2004) reported that such information did not produce apparent improvement in overall pronoun resolution.  In this paper, we will extract semantic features from a semantic role labeling system instead of a parse tree, and explore whether pronoun corefer-ence resolution can benefit from such knowledge, which is automatically extracted from a large corpus. We studied two features: the contextual compatibility feature which has been demon-strated to work at the syntactic level by previous work; and the role pair feature, which has not previously been applied to general domain pro-noun co-reference. In addition, to obtain a rough upper bound on the benefits of our approach and understand its limitations, we conducted a second experiment in which the semantic knowledge is extracted from the evaluation corpus.  We will use the term mention to describe an individual referring phrase. For most studies of coreference, mentions are noun phrases and may be headed by a name, a common noun, or a pro-noun.  We will use the term entity to refer to a set of coreferential mentions. 2 Related Work Contextual compatibility features have long been studied for pronoun coreference: Dagan and Itai (1990) proposed a heuristics-based approach to pronoun resolution. It determined the preference of candidates based on predicate-argument fre-quencies. Bean and Riloff (2004) present a system, which uses contextual role knowledge to aid coreference resolution. They used lexical and syntactic heuristics to identify high-confidence coreference relations and used them as training data for learning contextual role knowledge. They got substantial gains on articles in two spe-cific domains, terrorism and natural disasters.  Yang et al (2005) use statistically-based se-mantic compatibility information to improve 
pronoun resolution. They use corpus-based and web-based extraction strategies, and their work shows that statistically-based semantic compati-bility information can improve coreference reso-lution. In contrast, Kehler et al (2004) claimed that the contextual compatibility feature does not help much for pronoun coreference: existing learning-based approaches already performed well; such statistics are simply not good predictors for pro-noun interpretation; data is sparse in the collected predicate-argument statistics. The role pair feature has not been studied for general, broad-domain pronoun co-reference, but it has been used for other tasks: Pekar (2006) built pairs of 'templates' which share an 'anchor' argument; these correspond closely to our role pairs.  Association statistics of the template pairs were used to acquire verb entailments. Abe et al (2008) looked for pairs appearing in specific syn-tactic patterns in order to acquire finer-grained event relations.  Chambers and Jurafsky (2008) built narrative event chains, which are partially ordered sets of events related by a common pro-tagonist. They use high-precision hand-coded rules to get coreference information, extract predicate arguments that link the mentions to verbs, and link the arguments of the coreferred mentions to build a verb entailment model.  Bean and Riloff (2004) used high-precision hand-coded rules to identify coreferent mention pairs, which are then used to acquire role pairs that they refer to as Caseframe Network features.  They use these features to improve coreference resolution for two domain-specific corpora in-volving terrorism and natural disasters. Their result raises the natural question as to whether the approach (which may capture domain-specific pairs such as ?kidnap?release? in the terrorism domain) can be successfully extended to a general news corpus.  We address this ques-tion in the experiments reported here. 3 Corpus Analysis In order to extract semantic features from our large training corpus, we apply a sequence of analyzers. These include name tagging, parsing, a baseline coreference analyzer, and, most im-portant, a semantic labeling system that can gen-erate the logical grammatical and predicate-argument representation automatically from a 
61
parse tree (Meyers et al 2009). We use semantic labeling because it provides more general and meaningful patterns, with a ?deeper? analysis than parsed text. The output of the semantic la-beling is the dependency representation of the text, where each sentence is a graph consisting of nodes (corresponding to words) and arcs. Each arc captures up to three relations between two words: (1) a SURFACE relation, the relation be-tween a predicate and an argument in the parse of a sentence; (2) a LOGIC1 (grammatical logical) relation which regularizes for lexical and syntac-tic phenomena like passive, relative clauses, and deleted subjects; and (3) a LOGIC2 (predicate-argument) relation corresponding to relations in PropBank and NomBank. It is designed to be compatible with the Penn TreeBank (Marcus et al, 1994) framework and therefore, Penn Tree-Bank-based parsers, while incorporating Named Entities, PropBank, and NomBank.  Because nouns and verbs provide the most relevant contexts and capture the events in which the entities participate, we generate semantic pat-terns (triples) only for those arcs with verb or noun heads.  We use the following relations: ? Logic2 relations:  We use in particular the Arg0 relation (which corresponds roughly to agent) and Arg1 relation (which corresponds roughly to patient).  ? Logic1 relations: We use in particular the Sbj and Obj relations, representing the logical subject and object of a verb (regularizing passive, relative clauses, deleted subjects) ? Surface relations: T-pos relation is particu-larly used, which captures the head noun ? determiner relation for possessive constructs such as ?bomber?s attack? and ?his responsi-bility?. For example, for the sentence: John is hit by Tom?s brother. we generate the semantic patterns  <Arg1 hit John> <Arg0 hit brother> <T-pos brother Tom>  We apply this labeling system to all the data we use, and to generate the semantic pattern, we take first its predicate-argument role; if that is 
null, we take its logical grammatical role; if both are null, we take its surface role.    To reduce data sparseness, all inflected words are changed to their base form (e.g. ?attack-ers???attacker?). All names are replaced by their ACE types (person, organization, location, etc.). Only patterns with noun arguments are ex-tracted because we only consider noun phrases as possible antecedents. 4 Semantic Features 4.1 Contextual Compatibility Patterns Pronouns, especially neutral pronouns (?it?, ?they?), carry little semantics of their own, so examining the compatibility of the context of a pronoun and its candidate antecedents is a good way to improve antecedent selection. Specifi-cally, we want to determine whether the predi-cate, which is applied to the anaphor, can be ap-plied to the antecedents.  We take the semantic pattern with the anaphor in third position. Then, each candidate antecedent is substituted for the anaphor to see if it is suitable for the context. For example, consider the sentence The company issued a statement that it  bought G.M. which would generate the semantic patterns  <Arg0 issue company> <Arg1 issue statement> <Arg0 buy it> <Arg1 buy Organization>  (here ?G.M? is a name of type organization and so is replaced by the token Organization).  The relevant context of the anaphor is the semantic pattern <Arg0 buy it>.  Suppose there are two candidate antecedents for ?it?: ?company? and ?statement?. We would generate the two seman-tic patterns <Arg0 buy company> and <Arg0 buy statement>. Assuming <Arg0 buy company> is more highly ranked than <Arg0 buy statement>, we can infer that the anaphor is more likely to refer to ?company?. (We describe the specific metric we use for ranking below, in section 4.3.)  As further examples consider:  (3) The suspect's lawyer, Chifumu Banda, told the court he had advised Chiluba not to ap-pear in court Friday. 
62
(4) Foreign military analysts said it would be highly unusual for an accident to kill a whole submarine crew and they suggested possible causes to a disaster? For (3), if we know that a lawyer is more likely to give advice than a suspect, we could link ?he? to ?lawyer? instead of ?suspect? in the first sentence. For (4), if we know that analysts are more likely to ?suggest? than crew, we can link ?they? to ?analysts? in the second sentence. 4.2 Role Pair Patterns The role pair pattern is a new feature in general pronoun co-reference.  The original intuition for introducing it into coreference is that there are pairs of actions involving the same entity that are much more likely to occur together than would be true if one assumed statistical independence.  The second action may be a rephrasing or elabo-ration of the first, or the two might be actions that are part of a common ?script?.  For example: (5) Prime Minister Mahathir Mohamad sacked the former deputy premier in 1998, who was sentenced to a total of 15 years in jail after being convicted of corruption and sodomy.   He was released after four years because?.  (6) The robber attacked the boy with a knife; he was bleeding heavily and died in the hospital the next day. For (5), if we know that the person who was sentenced is more likely to be released than the person who sacked others, we would know ?he? refers to ?deputy premier? instead of ?prime min-ister?. And in (6), because someone being at-tacked is more likely to die than the attacker, we can infer that ?he? refers to ?boy?. To acquire such information, we need to iden-tify those pairs of predicates which are likely to apply to the same entity.  We collect this data from a large corpus. The basic process is: apply a baseline coreference system to produce mentions and entities for a large corpus. For every entity, record the predicates for every mention, and then the pairs of predicates for successive mentions within each entity.  Although the performance of the baseline coreference is not very high, and individual documents may yield many idiosyncratic pairs, we can gather many significant role pairs by col-
lecting statistics from a large corpus and filtering out the low frequency patterns; this process can eliminate much of the noise due to coreference errors.  Here is an example of the extracted role pairs involving ?attack?:   Obj volley  x Arg0 bombard  x Obj barrage  x Arg0 snatch  x Sbj attack  x Arg0 pound  x Obj reoccupy x Arg1 halt  x Arg0 assault  x 
Arg0 attack x  ? 
Arg1 bombard  x Table1. Top 10 role pairs associated with  ?Arg0 attack x? 4.3 Contextual Compatibility Scores To properly compare the patterns involving al-ternative candidate antecedents, we need to nor-malize the raw frequencies first.  We followed Yang et al (2005)?s idea, which normalizes the pattern frequency by the frequency of the candi-dates, and use a relative score that is normalized by the maximum score of all its candidates: 
        
? 
CompScore(P
context,Cand
)
=
CompFreq(Pcontext,Cand)
Max
Ci?Set(cands)
CompFreq(Pcontext,Ci)
 
and        
? 
CompFreq(P
context,Cand
) =
freq(P
context ,Cand
)
freq(Cand)
 
where 
? 
P
context,Cand
 is the contextual compatibility pattern built from the context of the pronoun and the base form of the candidate.  In contrast to Yang?s work, which used con-textual compatibility on the mention level, we consider the contextual compatibility of an entity to an anaphor:  we calculate the contextual in-formation of all the mentions and choose the one with highest score as the contextual compatibility score for this entity1: 
                                                1 Note that all the mentions in the entity are generated by the overall coreference system. Also, the ACE entity type of names is determined by the system.  No key annotations are considered in the entire coreference phase. 
63
? 
freq(context,entity)
= Max
mention
i
?entity
freq(P
context,mention
i
)
 
4.4 Role Pair Scores Unlike the contextual compatibility feature, we only take the role pair of the successive mentions in the candidate entity and the anaphor, because they are more reliably coreferential than arbitrary pairs of mentions within an entity:  
where  and  are the contextual pat-terns of the anaphor and the last mention in the candidate entity.  For a set of possible candidates, we compute a relative score: 
? 
PairScore(p
ana
, p
cand
)
=
PairFreq(p
ana
, p
cand
)
Max
pi?Set(cands)
PairFreq(p
ana
, pi)  Both scores are quantized (binned) in intervals of 0.1 for use as MaxEnt features.  5 Experiment Our coreference solution system uses ACE anno-tated data and follows the ACE 2005 English entity guidelines.2 The baseline coreference sys-tem to compare with is the same one used for extracting semantic features from the large cor-pus. It employs an entity-mention (rather than a mention-pair) model.  Besides entity and mention information, which (as mentioned above) is system output, the se-mantic information is also automatically ex-tracted by a semantic labeling system. As a result, we report results in section 5.4 which involve no information from the reference (key) annotation. 5.1 Baseline System Description The baseline system first applies processes like parsing, semantic labeling, name tagging, and entity mention tagging, producing a set of men-tions to which coreference analysis is then ap-plied. The coreference phase deals with corefer-ence among mentions that might be pronouns,                                                 2 Automatic Content Extraction evaluation, http://projects.ldc.upenn.edu/ace/ 
names or proper nouns, and generates entities when it is finished. The whole is a one-pass process, resolving coreference in the order in which mentions appear in the document. In the pronoun coreference process, every pronoun mention is assigned to one of the candidate enti-ties.   Features Description Hobbs_    Distance Hobbs distance between the last mention in the entity and the anaphor Head_Pro Combined word features of the head of the last mention in the entity and anaphor Is_Subject True if the last mention in the entity is a subject of the sentence Last_Cat Whether the last mention in the entity is a noun phrase, a pronoun or a name Co_Prior Number of prior references to this entity Table 2. Features used in baseline system  The baseline co-reference system has separate, quite elaborate, primarily rule-based systems to handle names, nominals, headless NP's, and ad-verbs ("here", "there") which may be anaphoric, as well as first- and second-person pronouns. The MaxEnt model under study in this paper is only responsible for third-person pronouns.  Also, gender, number, and human/non-human are han-dled separately outside of the MaxEnt model, and the model only resolves mentions that satisfy these constraints.3 In the MaxEnt model, 5 basic features (described in table 2) are used. Thus, while the set of features used in the model is relatively small in comparison to many current statistically based reference resolvers, these are the primary features relevant to the limited task 
                                                3 Gender information is obtained from a dictionary of gen-der-specific nouns and from first-name lists from the US Census.  Number information comes from large syntactic dictionaries, corpus annotation of collective nouns (syntac-tically singular nouns which may take plural anaphors), and name tagger information (some organizations and political entities may take plural anaphors). 
64
of the MaxEnt model, and its performance is still competitive4.  5.2 Corpus Description There are two kinds of corpora used in our ex-periment, a small coreference-annotated corpus used for training and evaluating (in cross-validation) the pronoun coreference model, and a large raw-text corpus for extracting semantic in-formation. For model training and evaluation, we assem-bled two small corpora from the available ACE data. One consists of news articles (460 docu-ments) from ACE 2005 (330 documents) and ACE 2003 (130 documents), which together con-tain 3934 pronouns. The other is the full ACE 2005 training set (592 documents), which in-cludes newswire, broadcast news, broadcast con-versations (interviews and discussions), web logs, web forums, and Fisher telephone transcripts, and contains 5659 pronouns. In evaluation, we consider a pronoun to be correctly resolved if its antecedent in the system output (the most recent prior mention in the en-tity to which the pronoun is assigned) matches the antecedent in the key. We report accuracy (percentage of pronouns which are correctly re-solved). We used a large corpus to extract semantic in-formation, consisting of five years of AFP newswire from the LDC English Gigaword cor-pus (1996, 2002, 2004, 2005 and 2006), a total of 907,368 documents. We omit news articles writ-ten in 1998, 2000 and 2003 to insure there is no overlap between the ACE data and Gigaword data. We pre-processed each document (parsing, name identification, and semantic labeling) and ran the baseline coreference system, which automatically identified all the mentions (includ-ing name mentions and nominal mentions) and built a set of entities for each document.  
                                                4For example, among papers reporting a pronoun accuracy metric, Kehler et al (2004), testing on a 2002 ACE news corpus, get a pronoun accuracy (without semantic features) of 75.7%; (Yang et  al. 2005), testing on the MUC corefer-ence corpora (also news) get for  their single-candidate baseline (without semantic features) 75.1%  pronoun accu-racy. Although the testing conditions in each case are  dif-ferent, these are comparable to our baseline performance. 
5.3 Semantic Information Extraction from Large Corpus In order to remove noise, we only keep contex-tual compatibility patterns that appear more than 5 times; and only keep role pair patterns which appear more than 15 times, and appear in more than three different years to avoid random pairs extracted from repeated stories. We automati-cally extracted 626,008 contextual compatibility patterns and 4,736,359 role pairs.  Note that we extract fewer patterns than Yang (2005), who extracted in total 2,203,203 contextual compati-bility patterns, from a much smaller corpus (173,252 Wall Street Journal articles). This might be for two reasons: first, we pruned low frequency patterns; second, we used a semantic labeling system instead of shallow parsing. Sec-tion 5.6 gives a comparison of pattern extraction based on different levels of analysis.  5.4 Results  News Corpus 2005 Corpus  Accu SignTest (p <=) Accu SignTest (p <=) baseline 75.54  72.04  context 76.59 0.025 73.35 0.002 role pair 76.28 0.031 73.03 0.003 combine 77.02 0.0005 73.72 0.0015 Table 3. Accuracy of 5-fold cross-validation with sta-tistics-based semantic features  We did a 5-fold cross validation to test the con-tribution from statistically-based semantic fea-tures, and report an average accuracy. All the mentions and their features are obtained from system output; as a result, if the correct antece-dent is not correctly discovered and analyzed from the previous phases, we will not be able to co-refer the pronoun correctly.  Experiments on the news articles show that each feature provides approximately 1% gain by itself, and contributes to a substantial overall gain of 1.45%. For the 2005 corpus, the baseline is lower because the documents come from different genres, and we get more gain from each semantic feature. We also computed the significance over the baseline using the sign test5.                                                  5 In applying the sign test, we treated each pronoun as an independent sample, which is either correctly resolved or incorrectly resolved. Where the individual observations are 
65
5.5 Self-Extracted Bound To better understand the potential maximum con-tribution of our semantic features, we constructed an approximation to the most favorable possible semantic features for each test set. We did this by using perfect coreference knowledge and by col-lecting patterns for each test set from the test set itself. For each corpus used for cross-validation, we first collect all the contextual compatibility and role pair patterns corresponding to the cor-rect antecedents (we ignore the patterns corre-sponding to the wrong antecedents, because we can not get this negative information when we extract them from a large corpus), and score these patterns to produce semantic features for the MaxEnt Model, both training and testing.  We then use these features in the model and do a cross-validation as before.  Also, as before, we rely on system output to identify and analyze potential antecedents; if the prior phases do not do so correctly, coreference analysis may well fail.  This experiment shows that we can get about 3 to 4% gain from each feature type sepa-rately; 4.5 to 5.5% gain is achieved from the two features together.   News Corpus 2005 Corpus  Accu SignTest (p <=) Accu SignTest (p <=) baseline 75.54  72.04  context 79.23 7e-14 76.04 9e-27 role pair 78.85 6e-13 75.95 1e-26 combine 79.97 4e-16 77.50 2e-38 Table 4. Accuracy of 5-fold cross-validation with self-extracted semantic features  5.6 Comparison between Semantic and Syntax Patterns To better understand the difference between se-mantic role labeling and syntactic relations, we did a comparison between patterns extracted from the syntax level and those extracted from semantic role labeling: Experiments show that using semantic roles (such as Arg0 and Arg1) works better. This may                                                                        (changes in) binary outcomes, the sign test provides a suita-bly sensitive significance test. (In particular, it is compara-ble to performing a paired t-test over counts of correct reso-lutions, aggregated over documents.) 
be because the "deeper" representation provides more generalization of relations. For example, the phrases ?weapon?s use? and ?use weapon? share the same semantic relation <Arg1 use weapon>, while they yield different grammatical relations: <T-pos use weapon> and <Obj use weapon>.   News Corpus 2005 Corpus  semantic syntax semantic syntax baseline 75.54  72.04  context 79.23 77.73 76.04 75.83 role pair 78.85 76.87 75.95 74.17 combine 79.97 78.42 77.50 76.76 Table 5. Accuracy of 5-fold cross-validation with self-extracted semantic features based on different levels of syntactic/semantic relations 5.7 Error Analysis We analyzed the errors in the self-extracted re-sults, to see why such corpus-specific semantic features do not produce an even greater reduction in errors. For the contextual compatibility feature, we find cases where an incorrect candidate is equally compatible with the context of the ana-phor; for example, if all the candidates are person names, they will share the same context feature because they generate the same ACE type. In other cases, the context does not provide enough information. For example, in a context tuple <Arg0 get x>, x can be almost any noun, because ?get? is too vague to predicate the compatible subjects. There are similar limitations with the role pair feature; for example, <Arg0 get they> can be associated with a lot of other actions. To quantify this problem, we counted the pat-terns that appear in both positive examples (cor-rect antecedents) and negative examples (incor-rect antecedents). For contextual compatibility patterns, 39.5% of the patterns which appear with positive examples also appear in the negative sample, while for role pair patterns, 19% of the patterns which appear with positive examples also appear in the negative sample.   So we see that, even with a pattern set highly tuned to the test set, many patterns do not by themselves serve to distinguish correct from incorrect coreference. We analyzed some of the cases where the se-mantic information does not help, or even harms the analysis.   In some cases all the antecedent 
66
scores are very low, either because the patterns are very rare or the antecedent is a common word that appears in a lot of patterns.  In other cases, several antecedents have a high compatibility score but the correct one does not have the top score. In these cases, the contextual compatibility is not reliable, as was pointed out by Kehler et al (2004): (7) The model for a republic, adopted over bitter objections from those advocating direct elec-tion of a president, is for presidential nomi-nations to be made with public input and the winning candidate decided by a two-thirds majority of Parliament. Former prime minis-ter Paul Keating, who put the republic issue in the spotlight in his unsuccessful 1996 campaign for re-election, welcomed the re-sult. Here adding semantic features leads ?his? to be incorrectly resolved to ?president? rather than the entity with mentions ?prime minister? and ?Paul Keating?; all the relevant patterns are common, but the score for <Arg0 campaign president> is higher (around 0.0012) than for <Arg0 campaign minister> (0.0004) or <Arg0 campaign Person> (0.0006). Another problem is that the patterns do not capture enough context information, for example: (8) The U.S. administration has been pressing the Security Council to adopt a statement condemning Pyongyang for failing to meet its obligations. If we can get the semantic context of ?fail to meet its obligations? instead of ?its obligations?, we might get better solutions for (8).  The role pair information raises similar prob-lems. Some verbs are very vague, like ?get?, ?take?, ?have?, and role pairs with these verbs might not be very useful. Here is an example: (9) The retired Greek officer tried to get Ocalan to the Netherlands, home to a large Kurdish community. He claimed he had been ma-nipulated by the government. In this sentence, the role pair information is very vague and it is hard to select a proper ante-cedent by connecting the subject of ?try? or ?get? or the object of ?get? to the subject of ?claim?. 
5.8 Limitations of Semantic Features The availability of very large corpora coupled with improved pre-processing (e.g., faster pars-ers, accurate semantic labelers) is making it eas-ier to extract large sets of semantic patterns. However, results on ?perfect? semantic informa-tion show that even if we can get very good se-mantic features, there are at least two concerns to address: ? How to best capture the context information: larger context patterns may suffer from data sparseness; simple patterns may be insuffi-ciently selective, appearing in both positive and negative samples.  ? In some cases, the baseline features are suffi-cient to select the antecedent and the semantic features only do harm.  If we are able to better gauge our confidence in the decisions based on the baseline features and on the semantic features, we may be able to combine these two sources more effectively. 6 Conclusions and Future Work We have presented two ways to incorporate se-mantic features into a MaxEnt model-based pro-noun coreference system, where these features have been extracted from a large corpus using a baseline IE (Information Extraction) system and a semantic labeling system, with no specific do-main information.  We also estimated the maximal benefit of these features and did some error analysis to identify cases where this semantic knowledge did not suffice. Our experiments show the value of these semantic features for pronoun coreference, but also the limitations of our current context representation and reference resolution models.  Last, we compared the features extracted from different levels of analysis, and showed that 'deeper' representations worked better. References Shuya Abe, Kentaro Inui, and Yuji Matsumoto. 2008. Two-phased event relation acquisition:  coupling the relation-oriented and argument-oriented ap-proaches.  Proc. 22nd Int'l Conf. on Computational Linguistics (COLING 2008). David Bean and Ellen Riloff. 2004. Unsupervised Learning of Contextual Role Knowledge for 
67
Coreference Resolution. Proc. HLT-NAACL 2004. Nathanael Chambers and Dan Jurafsky. 2008. Unsupervised Learning of Narrative Event Chains. Proc. ACL-08:  HLT. I. Dagan and A. Itai. 1990. Automatic processing of large corpora for the resolution of anaphora references. Proc. 13th International Conference on Computational Linguistics (COLING  1990). J. Hobbs. 1978. Resolving pronoun references. Lingua, 44:339?352. A. Kehler, D. Appelt, L. Taylor, and A. Simma. 2004. The (non)utility of predicate-argument fre-quencies for pronoun interpretation. Proc. HLT-NAACL 2004. A. Meyers, M. Kosaka, N. Xue, H. Ji, A. Sun, S.  Liao and W. Xu. 2009. Automatic Recognition of Logi-cal Relations for English, Chinese and Japanese in the GLARF Framework. In SEW-2009 (Semantic Evaluations Workshop) at NAACL HLT-2009 Viktor Pekar.  2006.  Acquisition of verb entailment from text.  Proc. HLT-NAACL 2006. Simone Paolo Ponzetto and Michael Strube. 2006 Exploiting semantic Role Labeling, WordNet and Wikipedia for Coreference Resolution. Proc. HLT-NAACL 2006. X. Yang, J. Su, G. Zhou, and C. Tan. 2004. An NP-cluster approach to coreference resolution. Proc. 20th International Conference on Computa-tional Linguistics (COLING 2004). Xiaofeng Yang, Jian Su, Chew Lim Tan.2005.  Im-proving Pronoun Resolution Using Statistics-Based Semantic Compatibility Information. Proc. 43rd Annual Meeting of the Assn. for Com-putational Linguistics. Xiaofeng Yang and Jian Su. 2007. Coreference Resolution Using Semantic Relatedness In-formation from Automatically Discovered Pat-terns. Proc. 45th Annual Meeting of the Assn. for Computational Linguistics.  
68
Proceedings of the Workshop on Language in Social Media (LASM 2013), pages 20?29,
Atlanta, Georgia, June 13 2013. c?2013 Association for Computational Linguistics
A Preliminary Study of Tweet Summarization using Information Extraction
Wei Xu, Ralph Grishman, Adam Meyers
Computer Science Department
New York University
New York, NY 10003, USA
{xuwei,grishman,meyers}@cs.nyu.edu
Alan Ritter
Computer Science and Engineering
University of Washington
Seattle, WA 98125, USA
aritter@cs.washington.edu
Abstract
Although the ideal length of summaries dif-
fers greatly from topic to topic on Twitter, pre-
vious work has only generated summaries of
a pre-fixed length. In this paper, we propose
an event-graph based method using informa-
tion extraction techniques that is able to cre-
ate summaries of variable length for different
topics. In particular, we extend the Pagerank-
like ranking algorithm from previous work to
partition event graphs and thereby detect fine-
grained aspects of the event to be summarized.
Our preliminary results show that summaries
created by our method are more concise and
news-worthy than SumBasic according to hu-
man judges. We also provide a brief survey of
datasets and evaluation design used in previ-
ous work to highlight the need of developing a
standard evaluation for automatic tweet sum-
marization task.
1 Introduction
Tweets contain a wide variety of useful information
from many perspectives about important events tak-
ing place in the world. The huge number of mes-
sages, many containing irrelevant and redundant in-
formation, quickly leads to a situation of informa-
tion overload. This motivates the need for automatic
summarization systems which can select a few mes-
sages for presentation to a user which cover the most
important information relating to the event without
redundancy and filter out irrelevant and personal in-
formation that is not of interest beyond the user?s
immediate social network.
Although there is much recent work focusing on
the task of multi-tweet summarization (Becker et al,
2011; Inouye and Kalita, 2011; Zubiaga et al, 2012;
Liu et al, 2011a; Takamura et al, 2011; Harabagiu
and Hickl, 2011; Wei et al, 2012), most previous
work relies only on surface lexical clues, redun-
dancy and social network specific signals (e.g. user
relationship), and little work has considered taking
limited advantage of information extraction tech-
niques (Harabagiu and Hickl, 2011) in generative
models. Because of the noise and redundancy in
social media posts, the performance of off-the-shelf
news-trained natural language process systems is de-
graded while simple term frequency is proven pow-
erful for summarizing tweets (Inouye and Kalita,
2011). A natural and interesting research question
is whether it is beneficial to extract named entities
and events in the tweets as has been shown for clas-
sic multi-document summarization (Li et al, 2006).
Recent progress on building NLP tools for Twitter
(Ritter et al, 2011; Gimpel et al, 2011; Liu et al,
2011b; Ritter et al, 2012; Liu et al, 2012) makes
it possible to investigate an approach to summariz-
ing Twitter events which is based on Information Ex-
traction techniques.
We investigate a graph-based approach which
leverages named entities, event phrases and their
connections across tweets. A similar idea has been
studied by Li et al (2006) to rank the salience
of event concepts in summarizing news articles.
However, the extreme redundancy and simplicity of
tweets allows us to explicitly split the event graph
into subcomponents that cover various aspects of the
initial event to be summarized to create comprehen-
20
Work Dataset (size of each clus-
ter)
System Output Evaluation Metrics
Inouye and
Kalita (2011)
trending topics (approxi-
mately 1500 tweets)
4 tweets ROUGE and human (over-
all quality comparing to
human summary)
Sharifi et al
(2010)
same as above 1 tweet same as above
Rosa et al
(2011)
segmented hashtag top-
ics by LDA and k-means
clustering (average 410
tweets)
1, 5, 10 tweets Precision@k (relevance to
topic)
Harabagiu and
Hickl (2011)
real-word event topics (a
minimum of 2500 tweets)
top tweets until a limit of
250 words was reached
human (coverage and co-
herence)
Liu et al
(2011a)
general topics and hash-
tag topics (average 1.7k
tweets)
same lengths as of the
human summary, vary
for each topic (about 2 or
3 tweets)
ROUGE and human (con-
tent coverage, grammat-
icality, non-redundancy,
referential clarity, focus)
Wei et al
(2012)
segmented hashtag top-
ics according to burstiness
(average 10k tweets)
10 tweets ROUGE, Precison/Recall
(good readability and rich
content)
Takamura et al
(2011)
specific soccer games
(2.8k - 5.2k tweets)
same lengths as the hu-
man summary, vary for
each topic (26 - 41
tweets)
ROUGE (considering
only content words)
Chakrabarti and
Punera (2011)
specific football games
(1.8k tweets)
10 - 70 tweets Precision@k (relevance to
topic)
Table 1: Summary of datasets and evaluation metrics used in several previous work on tweet summarization
sive and non-redundant summaries. Our work is the
first to use a Pagerank-like algorithm for graph parti-
tioning and ranking in the context of summarization,
and the first to generate tweet summaries of variable
length which is particularly important for tweet sum-
marization. Unlike news articles, the amount of in-
formation in a set of topically clustered tweets varies
greatly, from very repetitive to very discrete. For ex-
ample, the tweets about one album release can be
more or less paraphrases, while those about another
album by a popular singer may involve rumors and
release events etc. In the human study conducted by
Inouye and Kalita (2011), annotators strongly prefer
different numbers of tweets in a summary for dif-
ferent topics. However, most of the previous work
produced summaries of a pre-fixed length and has
no evaluation on conciseness. Liu et al (2011a)
and Takamura et al (2011) also noticed the ideal
length of summaries can be very different from topic
to topic, and had to use the length of human refer-
ence summaries to decide the length of system out-
puts, information which is not available in practice.
In contrast, we developed a system that is capable
of detecting fine-grained sub-events and generating
summaries with the proper number of representative
tweets accordingly for different topics.
Our experimental results show that with informa-
tion extraction it is possible to create more mean-
ingful and concise summaries. Tweets that contain
real-world events are usually more informative and
readable. Event-based summarization is especially
beneficial in this situation due to the fact that tweets
are short and self-contained with simple discourse
structure. The boundary of 140 characters makes it
efficient to extract semi-structured events with shal-
low natural language processing techniques and re-
21
Tweets (Date Created) Named Entity Event Phrases Date Mentioned
Nooooo.. Season premiere of Doctor Who is on
Sept 1 world wide and we?ll be at World Con
(8/22/2012)
doctor who,
world con
season, is on,
premiere
sept 1
(9/1/2012)
guess what I DON?T get to do tomorrow!
WATCH DOCTOR WHO (8/31/2012)
doctor who watch tomorrow
(9/1/2012)
As I missed it on Saturday, I?m now catching up
on Doctor Who (9/4/2012)
doctor who missed,
catching up
saturday
(9/1/2012)
Rumour: Nokia could announce two WP8 de-
vices on September 5 http://t.co/yZUwDFLV (via
@mobigyaan)
nokia, wp8 announce september 5
(9/5/2012)
Verizon and Motorola won?t let Nokia have all
the fun ; scheduling September 5th in New York
http://t.co/qbBlYnSl (8/19/2012)
nokia, verizon,
motorola,
new york
scheduling september 5th
(9/5/2012)
Don?t know if it?s excitement or rooting for the
underdog, but I am genuinely excited for Nokia
come Sept 5: http://t.co/UhV5SUMP (8/7/2012)
nokia rooting,
excited
sept 5
(9/5/2012)
Table 2: Event-related information extracted from tweets
duces the complexity of the relationship (or no re-
lationship) between events according to their co-
occurrence, resulting in differences in constructing
event graphs from previous work in news domain
(Li et al, 2006).
2 Issues in Current Research on Tweet
Summarization
The most serious problem in tweet summarization
is that there is no standard dataset, and conse-
quently no standard evaluation methodology. Al-
though there are more than a dozen recent works on
social media summarization, astonishingly, almost
each research group used a different dataset and a
different experiment setup. This is largely attributed
to the difficulty of defining the right granularity of a
topic in Twitter. In Table 1, we summarize the exper-
iment designs of several selective works. Regardless
of the differences, researchers generally agreed on :
? clustering tweets topically and temporally
? generating either a very short summary for a
focused topic or a long summary for large-size
clusters
? difficulty and necessity to generate summaries
of variable length for different topics
Although the need of variable-length summaries
have been raised in previous work, none has pro-
vide a good solution (Liu et al, 2011a; Takamura
et al, 2011; Inouye and Kalita, 2011). In this pa-
per, our focus is study the feasibility of generating
concise summaries of variable length and improv-
ing meaningfulness by using information extraction
techniques. We hope this study can provide new in-
sights on the task and help in developing a standard
evaluation in the future.
3 Approach
We first extract event information including named
entities and event phrases from tweets and construct
event graphs that represent the relationship between
them. We then rank and partition the events using
PageRank-like algorithms, and create summaries of
variable length for different topics.
3.1 Event Extraction from Tweets
As a first step towards summarizing popular events
discussed on Twitter, we need a way to identify
events from Tweets. We utilize several natural lan-
guage processing tools that specially developed for
noisy text to extract text phrases that bear essential
event information, including named entities (Ritter
et al, 2011), event-referring phrases (Ritter et al,
22
2012) and temporal expressions (Mani and Wilson,
2000). Both the named entity and event taggers uti-
lize Conditional Random Fields models (Lafferty,
2001) trained on annotated data, while the temporal
expression resolver uses a mix of hand-crafted and
machine-learned rules. Example event information
extracted from Tweets are presented in Table 2.
The self-contained nature of tweets allows effi-
cient extraction of event information without deep
analysis (e.g. co-reference resolution). On the other
hand, individual tweets are also very terse, often
lacking sufficient context to access the importance
of events. It is crucial to exploit the highly redun-
dancy in Twitter. Closely following previous work
by Ritter et al (2012), we group together sets of
topically and temporally related tweets, which men-
tion the same named entity and a temporal refer-
ence resolved to the same unique calendar date. We
also employ a statistical significance test to measure
strength of association between each named entity
and date, and thereby identify important events dis-
cussed widely among users with a specific focus,
such as the release of a new iPhone as opposed to in-
dividual users discussing everyday events involving
their phones. By discarding frequent but insignifi-
cant events, we can produce more meaningful sum-
maries about popular real-world events.
3.2 Event Graphs
Since tweets have simple discourse and are self-
contained, it is a reasonable assumption that named
entities and event phrases that co-occurred together
in a single tweet are very likely related. Given a col-
lection of tweets, we represent such connections by
a weighted undirected graph :
? Nodes: named entities and event phrases are
represented by nodes and treated indifferently.
? Edges: two nodes are connected by an undi-
rected edge if they co-occurred in k tweets, and
the weight of edge is k.
We find it helpful to merge named entities and
event phrases that have lexical overlap if they are fre-
quent but not the topic of the tweet cluster. For ex-
ample, ?bbc?, ?radio 1?, ?bbc radio 1? are combined
together in a set of tweets about a band. Figure 1
shows a very small toy example of event graph. In
the experiments of this paper, we also exclude the
edges with k < 2 to reduce noise in the data and
calculation cost.
Figure 1: A toy event graph example built from the three
sentences of the event ?Nokia - 9/5/2012? in Table 2
3.3 Event Ranking and Partitioning
Graph-based ranking algorithms are widely used in
automatic summarization to decide salience of con-
cepts or sentences based on global information re-
cursively drawn from the entire graph. We adapt the
PageRank-like algorithm used in TextRank (Mihal-
cea and Tarau, 2004) that takes into account edge
weights when computing the score associated with a
vertex in the graph.
Formally, let G = (V,E) be a undirected graph
with the set of vertices V and set of edges E, whereE is a subset of V ? V . For a given vertex Vi, letAd(Vi) be the set of vertices that adjacent to it. The
weight of the edge between Vi and Vj is denoted aswij , and wij = wji. The score of a vertex Vi is
defined as follows:S(Vi) = (1  d) + d? X
Vj2Ad(Vi)
wij ? S(Vj)P
Vk2Ad(Vj) wjk
where d is a damping factor that is usually set to 0.85
(Brin and Page, 1998), and this is the value we are
also using in our implementation.
23
Starting from arbitrary values assigned to each
node in the graph, the computation iterates until con-
vergence. Note that the final salience score of each
node is not affected by the choice of the initial val-
ues assigned to each node in the graph, but rather the
weights of edges.
In previous work computed scores are then used
directly to select text fractions for summaries (Li et
al., 2006). However, the redundancy and simplic-
ity of tweets allow further exploration into sub-event
detection by graph partitioning. The intuition is that
the correlations between named entities and event
phrases within same sub-events are much stronger
than between sub-events. This phenomena is more
obvious and clear in tweet than in news articles,
where events are more diverse and complicated re-
lated to each other given lengthy context.
As theoretically studied in local partitioning prob-
lem (Andersen et al, 2006), a good partition of the
graph can be obtained by separating high ranked ver-
tices from low ranked vertices, if the nodes in the
graph have ranks that are distinguishable. Utilizing
a similar idea, we show that a simple greedy algo-
rithm is efficient to find important sub-events and
generate useful summaries in our tasks. As shown
in Figure 2 and 3, the high ranked nodes (whose
scores are greater than 1, the average score of all
nodes in the graph) in tweet event graphs show the
divisions within a topic. We search for strongly con-
nected sub-graphs, as gauged by parameter ?, from
the highest ranked node to lower ranked ones.The
proportion of tweets in a set that are related to a
sub-event is then estimated according to the ratio be-
tween the sum of node scores in the sub-graph ver-
sus the entire graph. We select one tweet for each
sub-event that best covers the related nodes with the
highest sum of node scores normalized by length as
summaries. By adding a cutoff (parameter  ) on
proportion of sub-event required to be included into
summaries, we can produce summaries with the ap-
propriate length according to the diversity of infor-
mation in a set of tweets.
In Figure 2, 3 and 4, the named entity which is
also the topic of tweet cluster is omitted since it is
connected with every node in the event graph. The
size of node represents the salience score, while the
shorter, straighter and more vertical the edge is, the
higher its weight. The nodes with rectangle shapes
Algorithm 1 Find important sub-events
Require: Ranked event graph G = (V,E), the
named entity V0 which is the topic of event
cluster, parameters ? and   that can be set
towards user preference over development data
1: Initialize the pool of high ranked nodesV?  {Vi|8Vi 2 V, S(Vi) > 1}   V0 and the
total weight W  PVi2V? S(Vi)
2: while V? 6= ; do
3: Pop the highest ranked node Vm from V?
4: Put Vm to a temporary sub-event e  {Vm}
5: for all Vn in V? do
6: if wmn/w0m > ? and w0n/w0m > ?
then
7: e  e [ {Vn}
8: end if
9: end for
10: We  PVi2e S(Vi)
11: if We/W >   then
12: Successfully find a sub-event e
13: Remove all nodes in e from V?
14: end if
15: end while
are named entities, while round shaped ones are
event phrases. Note that in most cases, sub-events
correspond to connected components in the event
graph of high ranked nodes as in Figure 2 and 3.
However, our simple greedy algorithm also allows
multiple sub-events for a single connected compo-
nent that can not be covered by one tweet in the
summary. For example, in Figure 4, two sub-eventse1 = {sell, delete, start, payment} and e2 =
{facebook, share user data, privacy policy, debut}
are chosen to accommodate the complex event.
4 Experiments
4.1 Data
We gathered tweets over a 4-month period spanning
November 2012 to February 2013 using the Twitter
Streaming API. As described in more details in pre-
vious work on Twitter event extraction by Ritter et
al. (2012), we grouped together all tweets which
mention the same named entity (recognized using
24
Figure 2: Event graph of ?Google - 1/16/2013?, an example of event cluster with multiple focuses
Figure 3: Event graph of ?Instagram - 1/16/2013?, an example of event cluster with a single but complex focus
25
Figure 4: Event graph of ?West Ham - 1/16/2013?, an
example of event cluster with a single focus
a Twitter specific name entity tagger1) and a refer-
ence to the same unique calendar date (resolved us-
ing a temporal expression processor (Mani and Wil-
son, 2000)). Tweets published during the whole pe-
riod are aggregated together to find top events that
happen on each calendar day. We applied the G2
test for statistical significance (Dunning, 1993) to
rank the event clusters, considering the corpus fre-
quency of the named entity, the number of times the
date has been mentioned, and the number of tweets
which mention both together. We randomly picked
the events of one day for human evaluation, that is
the day of January 16, 2013 with 38 events and an
average of 465 tweets per event cluster.
For each cluster, our systems produce two ver-
sions of summaries, one with a fixed number (set
to 3) of tweets and another one with a flexible num-
ber (vary from 1 to 4) of tweets. Both ? and   are
set to 0.1 in our implementation. All parameters are
set experimentally over a small development dataset
consisting of 10 events in Twitter data of September
2012.
1
https://github.com/aritter/twitter_nlp
4.2 Baseline
SumBasic (Vanderwende et al, 2007) is a simple
and effective summarization approach based on term
frequency, which we use as our baseline. It uses
word probabilities with an update function to avoid
redundancy to select sentences or posts in a social
media setting. It is shown to outperform three other
well-known multi-document summarization meth-
ods, namely LexRank (Erkan and Radev, 2004),
TextRank (Mihalcea and Tarau, 2004) and MEAD
(Radev et al, 2004) on tweets in (Inouye and Kalita,
2011), possibly because that the relationship be-
tween tweets is much simpler than between sen-
tences in news articles and can be well captured by
simple frequency methods. The improvement over
the LexRank model on tweets is gained by consid-
ering the number of retweets and influential users is
another side-proof (Wei et al, 2012) of the effective-
ness of frequency.
EventRank?Flexible EventRank?Fixed SumBasic
Annotator 1
0
1
2
3
4
5 compactnesscompletenessoverall
EventRank?Flexible EventRank?Fixed SumBasic
Annotator 2
0
1
2
3
4
5 compactnesscompletenessoverall
Figure 5: human judgments evaluating tweet summariza-
tion systems
26
Event System Summary
- Google ?s home page is a Zamboni game in celebration of Frank Zam-
boni ?s birthday January 16 #GameOn
EventRank
(Flexible)
- Today social , Tomorrow Google ! Facebook Has Publicly Redefined
Itself As A Search Company http://t.co/dAevB2V0 via @sai
Google
1/16/2013
- Orange says has it has forced Google to pay for traffic . The Head of
the Orange said on Wednesday it had ... http://t.co/dOqAHhWi
- Tomorrow?s Google doodle is going to be a Zamboni! I may have to
take a vacation day.
SumBasic - the game on google today reminds me of hockey #tooexcited #saturday
- The fact that I was soooo involved in that google doodle game says
something about this Wednesday #TGIW You should try it!
EventRank
(Flexible)
- So Instagram can sell your pictures to advertisers without u knowing
starting January 16th I?m bout to delete my instagram !
- Instagram debuts new privacy policy , set to share user data with Face-
book beginning January 16
Instagram
1/16/2013
- Instagram will have the rights to sell your photos to Advertisers as of
jan 16
SumBasic - Over for Instagram on January 16th
- Instagram says it now has the right to sell your photos unless you delete
your account by January 16th http://t.co/tsjic6yA
EventRank
(Flexible)
- RT @Bassa_Mufc : Wayne Rooney and Nani will feature in the FA Cup
replay with West Ham on Wednesday - Sir Alex Ferguson
West Ham
1/16/2013
- Wayne Rooney could be back to face West Ham in next Wednesday?s
FA Cup replay at Old Trafford. #BPL
SumBasic - Tomorrow night come on West Ham lol
- Nani?s fit abd WILL play tomorrow against West Ham! Sir Alex con-
firmed :)
Table 3: Event-related information extracted from tweets
4.3 Preliminary Results
We performed a human evaluation in which two an-
notators were asked to rate the system on a five-
point scale (1=very poor, 5=very good) for com-
pleteness and compactness. Completeness refers to
how well the summary cover the important content
in the tweets. Compactness refers to how much
meaningful and non-redundant information is in the
summary. Because the tweets were collected ac-
cording to information extraction results and ranked
by salience, the readability of summaries generated
by different systems are generally very good. The
top 38 events of January 16, 2013 are used as test
set. The aggregate results of the human evaluation
are displayed in Figure 5. Agreement between an-
notators measured using Pearson?s Correlation Co-
efficient is 0.59, 0.62, 0.62 respectively for compact-
ness, completeness and overall judgements.
Results suggest that the models described in this
paper produce more satisfactory results as the base-
line approaches. The improvement of EventRank-
Flexible over SumBasic is significant (two-tailedp < 0.05) for all three metrics according to stu-
dent?s t test. Example summaries of the events in
Figure 2, 3 and 4 are presented respectively in Table
3. The advantages of our method are the follow-
ing: 1) it finds important facts of real-world events
2) it prefers tweets with good readability 3) it in-
cludes the right amount of information with diversity
and without redundancy. For example, our system
picked only one tweet about ?West Ham -1/16/2013?
that convey the same message as the three tweets to-
27
gether of the baseline system. For another example,
among the tweets about Google around 1/16/2013,
users intensively talk about the Google doodle game
with a very wide range of words creatively, giving
word-based methods a hard time to pick up the di-
verse and essential event information that is less fre-
quent.
5 Conclusions and Future Work
We present an initial study of feasibility to gen-
erate compact summaries of variable lengths for
tweet summarization by extending a Pagerank-like
algorithm to partition event graphs. The evalua-
tion shows that information extraction techniques
are helpful to generate news-worthy summaries of
good readability from tweets.
In the future, we are interested in improving the
approach and evaluation, studying automatic met-
rics to evaluate summarization of variable length
and getting involved in developing a standard eval-
uation for tweet summarization tasks. We wonder
whether other graph partitioning algorithms may im-
prove the performance. We also consider extending
this graph-based approach to disambiguate named
entities or resolve event coreference in Twitter data.
Another direction of future work is to extend the
proposed approach to different data, for example,
temporal-aware clustered tweets etc.
Acknowledgments
This research was supported in part by NSF grant
IIS-0803481, ONR grant N00014-08-1-0431, and
DARPA contract FA8750- 09-C-0179, and carried
out at the University of Washington?s Turing Center.
We thank Mausam and Oren Etzioni of University
of Washington, Maria Pershina of New York Univer-
sity for their advice.
References
Reid Andersen, Fan Chung, and Kevin Lang. 2006.
Local graph partitioning using pagerank vectors. In
Foundations of Computer Science, 2006. FOCS?06.
47th Annual IEEE Symposium on, pages 475?486.
IEEE.
Hila Becker, Mor Naaman, and Luis Gravano. 2011. Se-
lecting quality twitter content for events. In Proceed-
ings of the Fifth International AAAI Conference onWe-
blogs and Social Media (ICWSM?11).
Sergey Brin and Lawrence Page. 1998. The anatomy of a
large-scale hypertextual web search engine. Computer
networks and ISDN systems, 30(1):107?117.
Deepayan Chakrabarti and Kunal Punera. 2011. Event
summarization using tweets. In Proceedings of the
Fifth International AAAI Conference on Weblogs and
Social Media, pages 66?73.
Ted Dunning. 1993. Accurate methods for the statistics
of surprise and coincidence. Computational linguis-
tics, 19(1):61?74.
G?nes Erkan and Dragomir R. Radev. 2004. Lexrank:
Graph-based lexical centrality as salience in text sum-
marization. J. Artif. Intell. Res. (JAIR), 22:457?479.
Kevin Gimpel, Nathan Schneider, Brendan O?Connor,
Dipanjan Das, Daniel Mills, Jacob Eisenstein, Michael
Heilman, Dani Yogatama, Jeffrey Flanigan, and
Noah A. Smith. 2011. Part-of-speech tagging for twit-
ter: Annotation, features, and experiments. In ACL.
Sanda Harabagiu and Andrew Hickl. 2011. Relevance
modeling for microblog summarization. In Fifth In-
ternational AAAI Conference on Weblogs and Social
Media.
David Inouye and Jugal K Kalita. 2011. Comparing twit-
ter summarization algorithms for multiple post sum-
maries. In Privacy, security, risk and trust (passat),
2011 ieee third international conference on and 2011
ieee third international conference on social comput-
ing (socialcom), pages 298?306. IEEE.
John Lafferty. 2001. Conditional random fields: Proba-
bilistic models for segmenting and labeling sequence
data. pages 282?289. Morgan Kaufmann.
Wenjie Li, Wei Xu, Chunfa Yuan, Mingli Wu, and Qin
Lu. 2006. Extractive summarization using inter- and
intra- event relevance. In Proceedings of the 21st In-
ternational Conference on Computational Linguistics
and the 44th annual meeting of the Association for
Computational Linguistics, ACL-44, pages 369?376,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Fei Liu, Yang Liu, and Fuliang Weng. 2011a. Why is
?sxsw? trending? exploring multiple text sources for
twitter topic summarization. ACL HLT 2011, page 66.
Xiaohua Liu, Shaodian Zhang, Furu Wei, and Ming
Zhou. 2011b. Recognizing named entities in tweets.
In ACL.
Xiaohua Liu, Furu Wei, Ming Zhou, et al 2012. Quick-
view: Nlp-based tweet search. In Proceedings of the
ACL 2012 System Demonstrations, pages 13?18. As-
sociation for Computational Linguistics.
Inderjeet Mani and GeorgeWilson. 2000. Robust tempo-
ral processing of news. In Proceedings of the 38th An-
28
nual Meeting on Association for Computational Lin-
guistics, ACL ?00, pages 69?76, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Rada Mihalcea and Paul Tarau. 2004. Textrank: Bring-
ing order into texts. In Proceedings of EMNLP, vol-
ume 4, pages 404?411. Barcelona, Spain.
Dragomir Radev, Timothy Allison, Sasha Blair-
Goldensohn, John Blitzer, Arda Celebi, Stanko
Dimitrov, Elliott Drabek, Ali Hakim, Wai Lam, Danyu
Liu, et al 2004. Mead-a platform for multidocument
multilingual text summarization. In Proceedings of
LREC, volume 2004.
Alan Ritter, Sam Clark, Mausam, and Oren Etzioni.
2011. Named entity recognition in tweets: An experi-
mental study.
Alan Ritter, Mausam, Oren Etzioni, and Sam Clark.
2012. Open domain event extraction from twitter. In
KDD, pages 1104?1112. ACM.
Kevin Dela Rosa, Rushin Shah, Bo Lin, Anatole Gersh-
man, and Robert Frederking. 2011. Topical clustering
of tweets. Proceedings of the ACM SIGIR: SWSM.
Beaux Sharifi, Mark-Anthony Hutton, and Jugal K
Kalita. 2010. Experiments in microblog summariza-
tion. In Proc. of IEEE Second International Confer-
ence on Social Computing.
Hiroya Takamura, Hikaru Yokono, and Manabu Oku-
mura. 2011. Summarizing a document stream. Ad-
vances in Information Retrieval, pages 177?188.
Lucy Vanderwende, Hisami Suzuki, Chris Brockett, and
Ani Nenkova. 2007. Beyond sumbasic: Task-focused
summarization with sentence simplification and lex-
ical expansion. Information Processing & Manage-
ment, 43(6):1606?1618.
Furu Wei, Ming Zhou, and Heung-Yeung Shum. 2012.
Twitter topic summarization by ranking tweets using
social influence and content quality. In COLING.
Arkaitz Zubiaga, Damiano Spina, Enrique Amig?, and
Julio Gonzalo. 2012. Towards real-time summariza-
tion of scheduled events from twitter streams. In Pro-
ceedings of the 23rd ACM conference on Hypertext
and social media, pages 319?320. ACM.
29
Proceedings of the 6th Workshop on Building and Using Comparable Corpora, pages 121?128,
Sofia, Bulgaria, August 8, 2013. c?2013 Association for Computational Linguistics
Gathering and Generating Paraphrases from Twitter
with Application to Normalization
Wei Xu+ Alan Ritter? Ralph Grishman+
+New York University, New York, NY, USA
{xuwei, grishman}@cs.nyu.edu
?University of Washington, Seattle, WA, USA
aritter@cs.washington.edu
Abstract
We present a new and unique para-
phrase resource, which contains meaning-
preserving transformations between infor-
mal user-generated text. Sentential para-
phrases are extracted from a compara-
ble corpus of temporally and topically
related messages on Twitter which of-
ten express semantically identical infor-
mation through distinct surface forms. We
demonstrate the utility of this new re-
source on the task of paraphrasing and
normalizing noisy text, showing improve-
ment over several state-of-the-art para-
phrase and normalization systems 1.
1 Introduction
Social media services provide a massive amount
of valuable information and demand NLP tools
specifically developed to accommodate their noisy
style. So far not much success has been reported
on a key NLP technology on social media data:
paraphrasing. Paraphrases are alternative ways to
express the same meaning in the same language
and commonly employed to improve the perfor-
mance of many other NLP applications (Madnani
and Dorr, 2010). In the case of Twitter, Petrovic? et
al. (2012) showed improvements on first story de-
tection by using paraphrases extracted from Word-
Net.
Learning paraphrases from tweets could be es-
pecially beneficial. First, the high level of in-
formation redundancy in Twitter provides a good
opportunity to collect many different expressions.
Second, tweets contain many kinds of paraphrases
not available elsewhere including typos, abbre-
viations, ungrammatical expressions and slang,
1Our Twitter paraphrase models are available
online at https://github.com/cocoxu/
twitterparaphrase/
which can be particularly valuable for many appli-
cations, such as phrase-based text normalization
(Kaufmann and Kalita, 2010) and correction of
writing mistakes (Gamon et al, 2008), given the
difficulty of acquiring annotated data. Paraphrase
models that are derived from microblog data could
be useful to improve other NLP tasks on noisy
user-generated text and help users to interpret a
large range of up-to-date abbreviations (e.g. dlt ?
Doritos Locos Taco) and native expressions (e.g.
oh my god ? {oh my goodness | oh my gosh | oh
my gawd | oh my jesus}) etc.
This paper presents the first investigation into
automatically collecting a large paraphrase cor-
pus of tweets, which can be used for building
paraphrase systems adapted to Twitter using tech-
niques from statistical machine translation (SMT).
We show experimental results demonstrating the
benefits of an in-domain parallel corpus when
paraphrasing tweets. In addition, our paraphrase
models can be applied to the task of normalizing
noisy text where we show improvements over the
state-of-the-art.
Relevant previous work has extracted sentence-
level paraphrases from news corpora (Dolan et
al., 2004; Barzilay and Lee, 2003; Quirk et al,
2004). Paraphrases gathered from noisy user-
generated text on Twitter have unique character-
istics which make this comparable corpus a valu-
able new resource for mining sentence-level para-
phrases. Twitter also has much less context than
news articles and much more diverse content, thus
posing new challenges to control the noise in min-
ing paraphrases while retaining the desired super-
ficial dissimilarity.
2 Related Work
There are several key strands of related work, in-
cluding previous work on gathering parallel mono-
lingual text from topically clustered news articles,
normalizing noisy Twitter text using word-based
121
models, and applying out-of-domain paraphrase
systems to improve NLP tasks in Twitter.
On the observation of the lack of a large para-
phrase corpus, Chen and Dolan (2011) have re-
sorted to crowdsourcing to collect paraphrases by
asking multiple independent users for descriptions
of the same short video. As we show in ?5, how-
ever, this data is very different from Twitter, so
paraphrase systems trained on in-domain Twitter
paraphrases tend to perform much better.
The task of paraphrasing tweets is also related
to previous work on normalizing noisy Twitter text
(Han and Baldwin, 2011; Han et al, 2012; Liu
et al, 2012). Most previous work on normaliza-
tion has applied word-based models. While there
are challenges in applying Twitter paraphrase sys-
tems to the task of normalization, access to paral-
lel text allows us to make phrase-based transfor-
mations to the input string rather than relying on
word-to-word mappings (for more details see ?4).
Also relevant is recent work on collecting bilin-
gual parallel data from Twitter (Jehl et al, 2012;
Ling et al, 2013). In contrast, we focus on mono-
lingual paraphrases rather than multilingual trans-
lations.
Finally we highlight recent work on apply-
ing out-of-domain paraphrase systems to improve
performance at first story detection in Twitter
(Petrovic? et al, 2012). By building better para-
phrase models adapted to Twitter, it should be pos-
sible to improve performance at such tasks, which
benefit from paraphrasing Tweets.
3 Gathering A Parallel Tweet Corpus
There is a huge amount of redundant information
on Twitter. When significant events take place in
the world, many people go to Twitter to share,
comment and discuss them. Among tweets on
the same topic, many will convey similar mean-
ing using widely divergent expressions. Whereas
researchers have exploited multiple news reports
about the same event for paraphrase acquisition
(Dolan et al, 2004), Twitter contains more vari-
ety in terms of both language forms and types of
events, and requires different treatment due to its
unique characteristics.
As described in ?3.1, our approach first identi-
fies tweets which refer to the same popular event
as those which mention a unique named entity and
date, then aligns tweets within each event to con-
struct a parallel corpus. To generate paraphrases,
we apply a typical phrase-based statistical MT
pipeline, performing word alignment on the paral-
lel data using GIZA++ (Och and Ney, 2003), then
extracting phrase pairs and performing decoding
uses Moses (Koehn et al, 2007).
3.1 Extracting Events from Tweets
As a first step towards extracting paraphrases from
popular events discussed on Twitter, we need a
way to identify Tweets which mention the same
event. To do this we follow previous work by Rit-
ter et al (2012), extracting named entities and
resolving temporal expressions (for example ?to-
morrow? or ?on Wednesday?). Because tweets are
compact and self-contained, those which mention
the same named entity and date are likely to refer-
ence the same event. We also employ a statistical
significance test to measure strength of association
between each named entity and date, and thereby
identify important events discussed widely among
users with a specific focus, such as the release of
a new iPhone as opposed to individual users dis-
cussing everyday events involving their phones.
By gathering tweets based on popular real-world
events, we can efficiently extract pairwise para-
phrases within a small group of closely related
tweets, rather than exploring every pair of tweets
in a large corpus. By discarding frequent but in-
significant events, such as ?I like my iPhone? and
?I like broke my iPhone?, we can reduce noise
and encourage diversity of paraphrases by requir-
ing less lexical overlap. Example events identified
using this procedure are presented in Table 1.
3.2 Extracting Paraphrases Within Events
Twitter users are likely to express the same mean-
ing in relation to an important event, however not
every pair of tweets mentioning the same event
will have the same meaning. People may have
opposite opinions and complicated events such as
presidential elections can have many aspects. To
build a useful monolingual paraphrase corpus, we
need some additional filtering to prevent unrelated
sentence pairs.
If two tweets mention the same event and also
share many words in common, they are very likely
to be paraphrases. We use the Jaccard distance
metric (Jaccard, 1912) to identify pairs of sen-
tences within an event that are similar at the lexical
level. Since tweets are extremely short with little
context and include a broad range of topics, using
only surface similarity is prone to unrelated sen-
122
Entity/Date Example Tweets
Vote for Obama on November
6th!
Obama
11/6/2012
OBAMA is #winning his 2nd
term on November 6th 2012.
November 6th we will re-elect
Obama!!
Bought movie tickets to see
James Bond tomorrow. I?m a
big #007 fan!
James Bond
11/9/2012
Who wants to go with me and
see that new James Bond movie
tomorrow?
I wanna go see James Bond to-
morrow
North Korea Announces De-
cember 29 Launch Date for
Rocket
North Korea
12/29/2012
Pyongyang reschedules launch
to December 29 due to ?techni-
cal deficiency?
North Korea to extend rocket
launch period to December 29
Table 1: Example sentences taken from automat-
ically identified significant events extracted from
Twitter. Because many users express similar in-
formation when mentioning these events, there are
many opportunities for paraphrase.
tence pairs. The average sentence length is only
11.9 words in our Twitter corpus, compared to
18.6 words in newswire (Dolan et al, 2004) which
also contains additional document-level informa-
tion. Even after filtering tweets with both their
event cluster and lexical overlap, some unrelated
sentence pairs remain in the parallel corpus. For
example, names of two separate music venues in
the same city might be mismatched together if they
happen to have concerts on the same night that
people tweeted using a canonical phrasing like ?I
am going to a concert at in Austin tonight?.
4 Paraphrasing Tweets for
Normalization
Paraphrase models built from grammatical text are
not appropriate for the task of normalizing noisy
text. However, the unique characteristics of the
Twitter data allow our paraphrase models to in-
clude both normal and noisy language and conse-
quently translate between them. Our models have
a tendency to normalize because correct spellings
and grammar are most frequently used,2 but there
is still danger of introducing noise. For the pur-
poses of normalization, we therefore biased our
models using a language model built using text
taken from the New York Times which is used to
represent grammatical English.
Previous work on microblog normalization is
mostly limited to word-level adaptation or out-of-
domain annotated data. Our phrase-based mod-
els fill the gap left by previous studies by exploit-
ing a large, automatically curated, in-domain para-
phrase corpus.
Lexical normalization (Han and Baldwin, 2011)
only considers transforming an out-of-vocabulary
(OOV) word to its standard form, i.e. in-
vocabulary (IV) word. Beyond word-to-word con-
versions, our phrase-based model is also able to
handle the following types of errors without re-
quiring any annotated data:
Error type Ill form Standard
form
1-to-many everytime every time
incorrect IVs can?t want
for
can?t wait for
grammar I?m going a
movie
I?m going to
a movie
ambiguities 4 4 / 4th / for /
four
Kaufmann and Kalita (2010) explored machine
translation techniques for the normalization task
using an SMS corpus which was manually anno-
tated with grammatical paraphrases. Microblogs,
however, contain a much broader range of content
than SMS and have no in-domain annotated data
available. In addition, the ability to gather para-
phrases automatically opens up the possibility to
build normalization models from orders of mag-
nitude more data, and also to produce up-to-date
normalization models which capture new abbrevi-
ations and slang as they are invented.
5 Experiments
We evaluate our system and several baselines
at the task of paraphrasing Tweets using pre-
viously developed automatic evaluation metrics
which have been shown to have high correlation
with human judgments (Chen and Dolan, 2011).
2Even though misspellings and grammatical errors are
quite common, there is much more variety and less agree-
ment.
123
In addition, because no previous work has evalu-
ated these metrics in the context of noisy Twitter
data, we perform a human evaluation in which an-
notators are asked to choose which system gen-
erates the best paraphrase. Finally we evaluate
our phrase-based normalization system against a
state-of-the-art word-based normalizer developed
for Twitter (Han et al, 2012).
5.1 Paraphrasing Tweets
5.1.1 Data
Our paraphrase dataset is distilled from a large
corpus of tweets gathered over a one-year period
spanning November 2011 to October 2012 using
the Twitter Streaming API. Following Ritter et
al. (2012), we grouped together all tweets which
mention the same named entity (recognized using
a Twitter specific name entity tagger3) and a ref-
erence to the same unique calendar date (resolved
using a temporal expression processor (Mani and
Wilson, 2000)). Then we applied a statistical sig-
nificance test (the G test) to rank the events, which
considers the corpus frequency of the named en-
tity, the number of times the date has been men-
tioned, and the number of tweets which mention
both together. Altogether we collected more than
3 million tweets from the 50 top events of each day
according to the p-value from the statistical test,
with an average of 229 tweets per event cluster.
Each of these tweets was passed through a Twit-
ter tokenizer4 and a simple sentence splitter, which
also removes emoticons, URLs and most of the
hashtags and usernames. Hashtags and usernames
that were in the middle of sentences and might
be part of the text were kept. Within each event
cluster, redundant and short sentences (less than 3
words) were filtered out, and the remaining sen-
tences were paired together if their Jaccard simi-
larity was no less than 0.5. This resulted in a par-
allel corpus consisting of 4,008,946 sentence pairs
with 800,728 unique sentences.
We then trained paraphrase models by applying
a typical phrase-based statistical MT pipeline on
the parallel data, which uses GIZA++ for word
alignment and Moses for extracting phrase pairs,
training and decoding. We use a language model
trained on the 3 million collected tweets in the de-
coding process. The parameters are tuned over de-
3https://github.com/aritter/twitter_
nlp
4https://github.com/brendano/
tweetmotif
velopment data and the exact configuration are re-
leased together with the phrase table for system
replication.
Sentence alignment in comparable corpora is
more difficult than between direct translations
(Moore, 2002), and Twitter?s noisy style, short
context and broad range of content present ad-
ditional complications. Our automatically con-
structed parallel corpus contains some proportion
of unrelated sentence pairs and therefore does re-
sult in some unreasonable paraphrases. We prune
out unlikely phrase pairs using a technique pro-
posed by Johnson et al (2007) with their recom-
mended setting, which is based on the significance
testing of phrase pair co-occurrence in the parallel
corpus (Moore, 2004). We further prevent unrea-
sonable translations by adding additional entries
to the phrase table to ensure every phrase has an
option to remain unchanged during paraphrasing
and normalization. Without these noise reduction
steps, our system will produce paraphrases with
serious errors (e.g. change a person?s last name)
for 100 out of 200 test tweets in the evaluation in
?5.1.5.
At the same time, it is also important to promote
lexical dissimilarity in the paraphrase task. Fol-
lowing Ritter et. al. (2011) we add a lexical sim-
ilarity penalty to each phrase pair in our system,
in addition to the four basic components (transla-
tion model, distortion model, language model and
word penalty) in SMT.
5.1.2 Evaluation Details
The beauty of lexical similarity penalty is that it
gives control over the degree of paraphrasing by
adjusting its weight versus the other components.
Thus we can plot a BLEU-PINC curve to express
the tradeoff between semantic adequacy and lexi-
cal dissimilarity with the input, where BLUE (Pa-
pineni et al, 2002) and PINC (Chen and Dolan,
2011) are previously proposed automatic evalua-
tion metrics to measure respectively the two crite-
ria of paraphrase quality.
To compute these automatic evaluation met-
rics, we manually prepared a dataset of gold para-
phrases by tracking the trending topics on Twitter5
and gathering groups of paraphrases in November
2012. In total 20 sets of sentences were collected
and each set contains 5 different sentences that ex-
press the same meaning. Each sentence is used
5https://support.twitter.com/articles/
101125-faqs-about-twitter-s-trends
124
Input Output
Hostess is going outta biz hostess is going out of business
REPUBLICAN IMMIGRATION REFORM IS A
THING NOW
gop imigration law is a thing now
Freedom Writers will always be one of my fav
movies
freedom writers will forever be one of my favorite
movies
sources confirm that Phil Jackson has cancelled
all weekend plans and upcoming guest appear-
ances, will meet with LAL front office
source confirms that phil jackson has canceled all
weekend plans , upcomin guest appearances and
will meet with lakers front office
Table 2: Example paraphrases generated by our system on the test data.
once as input while other 4 sentences in the same
set serve as reference translation for automatic
evaluation of semantic adequacy using BLEU.
5.1.3 Baselines
We consider two state-of-the-art paraphrase sys-
tems as baselines, both of which are trained on
parallel corpora of aligned sentences. The first one
is trained on a large-scale corpus gathered by ask-
ing users of Amazon?s Mechanical Turk Service
(Snow et al, 2008) to write a one-sentence de-
scription of a short video clip (Chen and Dolan,
2011). We combined a phrase table and distor-
tion table extracted from this parallel corpus with
the same Twitter language model, applying the
Moses decoder to generate paraphrases. The ad-
ditional noise removal steps described in ?5.1.1
were found helpful for this model during devel-
opment and were therefore applied. The second
baseline uses the Microsoft Research paraphrase
tables that are automatically extracted from news
articles in combination with the Twitter language
model.6
5.1.4 Results
Figure 1 compares our system against both base-
lines, varying the lexical similarity penalty for
each system to generate BLEU-PINC curves.
Our system trained on automatically gathered
in-domain Twitter paraphrases achieves higher
BLEU at equivalent PINC for the entire length of
the curves. Table 2 shows some sample outputs of
our system on real Twitter data.
One novel feature of our approach, compared
to previous work on paraphrasing, is that it cap-
tures many slang terms, acronyms, abbreviations
and misspellings that are otherwise hard to learn.
6No distortion table or noisy removal process is applied
because the parallel corpus is not available.
lll ll l l l l
l l
l
l
l
l
l
l l
l
0 20 40 60
0
5
10
15
20
PINC
BLE
U
l OursVideoMSR
Figure 1: Results from automatic paraphrase eval-
uation. PINC measures n-gram dissimilarity from
the source sentence, whereas BLEU roughly mea-
sures n-gram similarity to the reference para-
phrases.
Several examples are shown in table 3. The rich
semantic redundancy in Twitter helps generate a
large variety of typical paraphrases as well (see an
example in table 4).
5.1.5 Human Evaluation
In addition to automatic evaluation, we also per-
formed a human evaluation in which annotators
were asked to pick which system generated the
best paraphrase. We used the same dataset of
200 tweets gathered for the automatic evaluation
and generated paraphrases using the 3 systems in
Figure 1 with the highest BLEU which achieve a
PINC of at least 40. The human annotators were
then asked to pick which of the 3 systems gener-
ated the best paraphrase using the criteria that it
should be both different from the original and also
125
Input Top-ranked Outputs
amped pumped
lemme kno let me know
bb bigbang, big brother
snl nbcsnl, saturday night live
apply 4 tix apply for tickets, ask for tickets,
applying for tickets
the boys one direction (a band, whose
members are often referred as
?the boys?), they, the boy, the
gys, the lads, my boys, the direc-
tion (can be used to refer to the
band ?one direction?), the onedi-
rection, our boys, our guys
oh my god oh my gosh, omfg, thank the
lord, omg, oh my lord, thank you
god, oh my jesus, oh god
can?t wait cant wait, cant wait, cannot wait,
i cannot wait, so excited, cnt
wait, i have to wait, i can?wait,
ready, so ready, so pumped, seri-
ously can?wait, really can?t wait
Table 3: Example paraphrases of noisy phrases
and slang commonly found on Twitter
Input Top-ranked Outputs
who want
to get a
beer
wants to get a beer, so who wants
to get a beer, who wants to go
get a beer, who wants to get beer,
who want to get a beer, trying to
get a beer, who wants to buy a
beer, who wants to get a drink,
who wants to get a rootbeer, who
trying to get a beer, who wants to
have a beer, who wants to order
a beer, i want to get a beer, who
wants to get me a beer, who else
wants to get a beer, who wants to
win a beer, anyone wants to get
a beer, who wanted to get a beer,
who wants to a beer, someone to
get a beer, who wants to receive a
beer, someone wants to get a beer
Table 4: Example paraphrases of a given sentence
?who want to get a beer?
Ours Video MSR
0
20
40
60
80
100
120
annotator 1annotator 2
Figure 2: Number of paraphrases (200 in total)
preferred by the annotators for each system
capture as much of the original meaning as pos-
sible. The annotators were asked to abstain from
picking one as the best in cases where there were
no changes to the input, or where the resulting
paraphrases totally lost the meaning.
Figure 2 displays the number of times each an-
notator picked each system?s output as the best.
Annotator 2 was somewhat more conservative
than annotator 1, choosing to abstain more fre-
quently and leading to lower overall frequencies,
however in both cases we see a clear advantage
from paraphrasing using in-domain models. As
a measure of inter-rater agreement, we computed
Cohen?s Kappa between the annotators judgment
as to whether the Twitter-trained system?s output
best. The value of Cohen?s Kappa in this case was
0.525.
5.2 Phrase-Based Normalization
Because Twitter contains both normal and noisy
language, with appropriate tuning, our models
have the capability to translate between these two
styles, e.g. paraphrasing into noisy style or nor-
malizing into standard language. Here we demon-
strate its capability to normalize tweets at the
sentence-level.
5.2.1 Baselines
Much effort has been devoted recently for devel-
oping normalization dictionaries for Microblogs.
One of the most competitive dictionaries avail-
able today is HB-dict+GHM-dict+S-dict used by
Han et al (2012), which combines a manually-
constructed Internet slang dictionary , a small
(Gouws et al, 2011) and a large automatically-
126
derived dictionary based on distributional and
string similarity. We evaluate two baselines using
this large dictionary consisting of 41181 words;
following Han et. al. (2012), one is a simple dic-
tionary look up. The other baseline uses the ma-
chinery of statistical machine translation using this
dictionary as a phrase table in combination with
Twitter and NYT language models.
5.2.2 System Details
Our base normalization system is the same as
the paraphrase model described in ?5.1.1, except
that the distortion model is turned off to exclude
reordering. We tuned the system towards cor-
rect spelling and grammar by adding a language
model built from all New York Times articles
written in 2008. We also filtered out the phrase
pairs which map from in-vocabulary to out-of-
vocabulary words. In addition, we integrated the
dictionaries by linear combination to increase the
coverage of phrase-based SMT model (Bisazza et
al., 2011).
5.2.3 Evaluation Details
We adopt the normalization dataset of Han and
Baldwin (2011), which was initially annotated
for the token-level normalization task, and which
we augmented with sentence-level annotations.
It contains 549 English messages sampled from
Twitter API from August to October, 2010.
5.2.4 Results
Normalization results are presented in figure 5.
Using only our phrase table extracted from Twit-
ter events we achieve poorer performance than the
state-of-the-art dictionary baseline, however we
find that by combining the normalization dictio-
nary of Han et. al. (2012) with our automatically
constructed phrase-table we are able to combine
the high coverage of the normalization dictionary
with the ability to perform phrase-level normaliza-
tions (e.g. ?outta? ? ?out of? and examples in
?4) achieving both higher PINC and BLEU than
the systems which rely exclusively on word-level
mappings. Our phrase table also contains many
words that are not covered by the dictionary (e.g.
?pts? ? ?points?, ?noms? ? ?nominations?).
6 Conclusions
We have presented the first approach to gather-
ing parallel monolingual text from Twitter, and
built the first in-domain models for paraphrasing
BLEU PINC
No-Change 60.00 0.0
SMT+TwitterLM 62.54 5.78
SMT+TwitterNYTLM 65.72 9.23
Dictionary 75.07 22.10
Dicionary+TwitterNYTLM 75.12 20.26
SMT+Dictionary+TwitterNYTLM 77.44 25.33
Table 5: Normalization performance
tweets. By paraphrasing using models trained
on in-domain data we showed significant per-
formance improvements over state-of-the-art out-
of-domain paraphrase systems as demonstrated
through automatic and human evaluations. We
showed that because tweets include both normal
and noisy language, paraphrase systems built from
Twitter can be fruitfully applied to the task of nor-
malizing noisy text, covering phrase-based nor-
malizations not handled by previous dictionary-
based normalization systems. We also make our
Twitter-tuned paraphrase models publicly avail-
able. For future work, we consider developing ad-
ditional methods to improve the accuracy of tweet
clustering and paraphrase pair selection.
Acknowledgments
This research was supported in part by NSF grant
IIS-0803481, ONR grant N00014-08-1-0431, and
DARPA contract FA8750- 09-C-0179.
References
Regina Barzilay and Lillian Lee. 2003. Learn-
ing to paraphrase: an unsupervised approach using
multiple-sequence alignment. In Proceedings of the
2003 Conference of the North American Chapter
of the Association for Computational Linguistics on
Human Language Technology - Volume 1, NAACL
?03.
Arianna Bisazza, Nick Ruiz, and Marcello Federico.
2011. Fill-up versus interpolation methods for
phrase-based smt adaptation. In International Work-
shop on Spoken Language Translation (IWSLT), San
Francisco, CA.
David L. Chen and William B. Dolan. 2011. Collect-
ing highly parallel data for paraphrase evaluation. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL-2011),
Portland, OR, June.
Bill Dolan, Chris Quirk, and Chris Brockett. 2004.
Unsupervised construction of large paraphrase cor-
pora: Exploiting massively parallel news sources. In
Proceedings of Coling 2004.
127
Michael Gamon, Jianfeng Gao, Chris Brockett,
Alexander Klementiev, William B. Dolan, Dmitriy
Belenko, and Lucy Vanderwende. 2008. Using con-
textual speller techniques and language modeling for
esl error correction. IJCNLP.
S. Gouws, D. Hovy, and D. Metzler. 2011. Unsu-
pervised mining of lexical variants from noisy text.
In Proceedings of the First workshop on Unsuper-
vised Learning in NLP, pages 82?90. Association
for Computational Linguistics.
Bo Han and Timothy Baldwin. 2011. Lexical normali-
sation of short text messages: Makn sens a# twitter.
In Proceedings of the 49th Annual Meeting of the
Association for Computational Linguistics: Human
Language Technologies, volume 1, pages 368?378.
Bo Han, Paul Cook, and Timothy Baldwin. 2012. Au-
tomatically constructing a normalisation dictionary
for microblogs. In Proceedings of the 2012 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning, pages 421?432, Stroudsburg, PA,
USA.
P. Jaccard. 1912. The distribution of the flora in the
alpine zone. New Phytologist, 11(2):37?50.
Laura Jehl, Felix Hieber, and Stefan Riezler. 2012.
Twitter translation using translation-based cross-
lingual retrieval. In Proceedings of the Seventh
Workshop on Statistical Machine Translation, pages
410?421. Association for Computational Linguis-
tics.
J.H. Johnson, J. Martin, G. Foster, and R. Kuhn. 2007.
Improving translation quality by discarding most of
the phrasetable.
Max Kaufmann and Jugal Kalita. 2010. Syntac-
tic normalization of twitter messages. In Interna-
tional Conference on Natural Language Processing,
Kharagpur, India.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondr?ej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: open
source toolkit for statistical machine translation. In
Proceedings of the 45th Annual Meeting of the ACL
on Interactive Poster and Demonstration Sessions.
Wang Ling, Guang Xiang, Chris Dyer, Alan Black, and
Isabel Trancoso. 2013. Microblogs as parallel cor-
pora. In Proceedings of the 51st Annual Meeting of
the Association for Computational Linguistics.
Fei Liu, Fuliang Weng, and Xiao Jiang. 2012. A
broadcoverage normalization system for social me-
dia language. In Proceedings of the 50th Annual
Meeting of the Association for Computational Lin-
guistics (ACL 2012), Jeju, Republic of Korea.
Nitin Madnani and Bonnie J. Dorr. 2010. Generating
phrasal and sentential paraphrases: A survey of data-
driven methods. Comput. Linguist.
Inderjeet Mani and George Wilson. 2000. Robust tem-
poral processing of news. In Proceedings of the
38th Annual Meeting on Association for Computa-
tional Linguistics, ACL ?00, pages 69?76, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Robert C. Moore. 2002. Fast and accurate sentence
alignment of bilingual corpora. In Proceedings of
the 5th Conference of the Association for Machine
Translation in the Americas on Machine Transla-
tion: From Research to Real Users, AMTA ?02.
Robert C. Moore. 2004. On log-likelihood-ratios and
the significance of rare events. In Proceedings of the
2004 Conference on Empirical Methods in Natural
Language Processing, pages 333?340.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Comput. Linguist.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of the
40th Annual Meeting on Association for Computa-
tional Linguistics.
Sas?a Petrovic?, Miles Osborne, and Victor Lavrenko.
2012. Using paraphrases for improving first story
detection in news and twitter.
Chris Quirk, Chris Brockett, and William Dolan.
2004. Monolingual machine translation for para-
phrase generation. In Proceedings of EMNLP 2004.
Alan Ritter, Colin Cherry, and William B. Dolan. 2011.
Data-driven response generation in social media. In
Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing.
Alan Ritter, Mausam, Oren Etzioni, and Sam Clark.
2012. Open domain event extraction from twitter.
In KDD, pages 1104?1112. ACM.
Rion Snow, Brendan O?Connor, Daniel Jurafsky, and
Andrew Y. Ng. 2008. Cheap and fast?but is it
good?: evaluating non-expert annotations for natural
language tasks. In Proceedings of the Conference on
Empirical Methods in Natural Language Process-
ing, EMNLP ?08.
128
Proceedings of SADAATL 2014, pages 11?20,
Dublin, Ireland, August 24, 2014.
Jargon-Term Extraction by Chunking
Adam Meyers
?
, Zachary Glass
?
, Angus Grieve-Smith
?
, Yifan He
?
,
Shasha Liao
?
and Ralph Grishman
?
New York University
?
, Google
?
meyers/angus/yhe/grishman@cs.nyu.edu, zglass@alumni.princeton.edu
Abstract
NLP definitions of Terminology are usually application-dependent. IR terms are noun sequences
that characterize topics. Terms can also be arguments for relations like abbreviation, definition or
IS-A. In contrast, this paper explores techniques for extracting terms fitting a broader definition:
noun sequences specific to topics and not well-known to naive adults. We describe a chunking-
based approach, an evaluation, and applications to non-topic-specific relation extraction.
1 Introduction
Webster?s II New College Dictionary (Houghton Mifflin Company, 2001, p.1138) defines terminology
as: The vocabulary of technical terms and usages appropriate to a particular field, subject, science,
or art. Systems for automatically extracting instances of terminology (terms) usually assume narrow
operational definitions that are compatible with particular tasks. Terminology, in the context of Infor-
mation Retrieval (IR) (Jacquemin and Bourigault, 2003) refers to keyword search terms (microarray,
potato, genetic algorithm), single or multi-word (mostly nominal) expressions collectively representing
topics of documents that contain them. These same terms are also used for creating domain-specific
thesauri and ontologies (Velardi et al., 2001). We will refer to these types of terms as topic-terms and
this type of terminology topic-terminology. In other work, types of terminology (genes, chemical names,
biological processes, etc.) are defined relative to a specific field like Chemistry or Biology (Kim et al.,
2003; Corbett et al., 2007; Bada et al., 2010). These classes are used for narrow tasks, e.g., Information
Extraction (IE) slot filling tasks within a particular genre of interest (Giuliano et al., 2006; Bundschus
et al., 2008; BioCreAtIvE, 2006). Other projects are limited to Information Extraction tasks that may
not be terminology-specific, but have terms as arguments, e.g., (Schwartz and Hearst, 2003; Jin et al.,
2013) detect abbreviation and definition relations respectively and the arguments are terms. In contrast
to this previous work, we have built a system that extracts a larger set of terminology, which we call
jargon-terminology. Jargon-terms may include ultracentrifuge, which is unlikely to be a topic-term of a
current biology article, but will not include potato, a non-technical word that could be a valid topic-term.
We aim to find all the jargon-terms found in a text, not just the ones that fill slots for specific relations.
As we show, jargon-terminology closely matches the notional (e.g., Webster?s) definition of terminol-
ogy. Furthermore, the important nominals in technical documents tend to be jargon-terms, making them
likely arguments of a wide variety of possible IE relations (concepts or objects that are invented, two
nominals that are in contrast, one object that is ?better than? another, etc.). Specifically, the identification
of jargon-terms lays the ground for IE tasks that are not genre or task dependent. Our approach which
finds all instances of terms (tokens) in text is conducive to these tasks. In contrast, topic-term detection
techniques find smaller sets of terms (types), each term occurring multiple times and the set of terms
collectively represents a topic, in a similar way that a set of documents can represent a topic.
This work is licensed under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organisers. License details: http://creativecommons.org/licenses/by/4.0/
11
This paper describes a system for extracting jargon-terms in technical documents (patents and journal
articles); the evaluation of this system using manually annotated documents; and a set of information
extraction (IE) relations which take jargon-terms as arguments. We incorporate previous work in termi-
nology extraction, assuming that terminology is restricted to noun groups (minus some left modifiers)
(Justeson and Katz, 1995);
1
and we use both topic-term extraction techniques (Navigli and Velardi,
2004) and relation-based extraction techniques (Jin et al., 2013) in components of our system. Rather
than looking at the distribution of noun groups as a whole for determining term-hood, we refine the
classes used by the noun group chunker itself, placing limitations on the candidate noun groups pro-
posed and then filtering the output by setting thresholds on the number and quality of the ?jargon-like?
components of the phrase. The resulting system admits not only topic-terms, but also other non-topic
instances of terminology. Using the more inclusive set of jargon-terms (rather than just topic-terms) as
arguments of the IE relations in section 6, we are able to detect a larger and more informative set of rela-
tion. Furthermore, these relations are salient for a wide variety of genres (unlike those in (BioCreAtIvE,
2006)) ? a genre-neutral definition of terminology makes this possible. For example, the CONTRAST
relation between the two bold face terms in necrotrophic effector system
A1
that is an exciting contrast
to the biotrophic effector models
A2
. would be applicable in most academic genres. Our jargon-terms
also contrast with the tactic of filling terminology slots in relations with any noun-group (Justeson and
Katz, 1995), as such a strategy overgenerates, lowering precision.
2 Topic-term Extraction
Topic-term extractors (Velardi et al., 2001; Tomokiyo and Hurst, 2003) collect candidate terms (N-grams,
noun groups, words) that are more representative of a foreground corpus (documents about a specific
topic) than they are of a background corpus (documents about a wide range of topics), using statistical
measures such as
Term Frequency
Inverse Document Frequency
(TFIDF), or a variation thereof. Due to the metrics used
and cutoffs assumed, the list of terms selected is usually no more than a few hundred distinct terms,
even for a large set of foreground documents and tend to be especially salient to that topic. The terms
can be phrases that lay people would not know (e.g., microarray, genetic algorithm) or common topics
for that document set (e.g., potato, computer). Such systems rank all candidate terms, using cutoffs
(minimum scores or percentages of the list) to separate out the highest-ranked terms as output. Thus
sets of topic-terms, derived this way, are dependent on the foreground and background assumed, and the
publication dates. So a precise definition would include such information, e.g., topic-terms(biomedical-
patents, random-patents, 1990?1999) would refer to those topic-terms that differentiate a foreground of
biomedical patents from the 1990s from a background of diverse patents from the same epoch. Narrower
topics are possible (e.g., comparing DNA-microarray patents to the same background); or broader ones
(e.g., if a diverse corpus including news articles, fiction and travel writing are the background set instead
of patents, then patent terms such as national stage application may be highly ranked in the output).
Thus topic-terms generated by these methods model a relationally based definition and are relative to the
chosen foregrounds, backgrounds and dates.
Topic-terms can include words/phrases like potato, wheat, rat, monkey, which may be common sub-
jects of some set of biomedical documents, but are not specific to a technical field. In contrast, jargon-
terms would include words (like ultracentrifuge, theorem, graduated cylinder) that are specific to tech-
nical language, but don?t tend to be topics of any current document of interest. Jargon-terms, like topic-
terms, can be defined relative to a particular foreground (which can also be represented as a set of
documents), but there is the implicit assumption that they all share the same background set: non-genre-
specific language (or simply a very diverse set of documents). It is also possible to refer to terminology in
general as the union of jargon-terms with respect to the set of specialized knowledge areas as foregrounds
and all sharing the same background of non-genre-specific language. Jargon-terms, like topic-terms, are
also time dependent, since some terms will eventually be absorbed into the common lexicon, e.g., com-
puter. However, we can make the simplifying assumption that we are talking about jargon in the present
1
We restrict our scope to nominal terminology, but acknowledge the importance of non-nominal terminology, e.g., event
verb terms (calcify, coactivate) which are crucial to IE.
12
time. Furthermore, jargon-term status is somewhat less time sensitive than topic-term status because ter-
minology is absorbed very sparingly (and very slowly) into the popular lexicon, whereas topics go in and
out of fashion quickly within a literature that is meant for an expert audience. Ignoring the potato type
cases, topic-terms are a proper subset of jargon-terms and, thus, the set of jargon-terms is larger than the
set of topic-terms. Finally, topic terms are ranked with respect to how well they can serve as keywords,
i.e., how specific they are to a particular document set, whereas +/-jargon-term is a binary distinction.
We built a topic term extractor that combines several metrics together in an ensemble including:
TFIDF, KL Divergence (Cover and Thomas, 1991; Hisamitsu et al., 1999) and a combination of Do-
main Relevance and Document Consensus (DRDC) based on (Navigli and Velardi, 2004). Furthermore,
we filtered the output by requiring that each term would be recognized as a term by the jargon-term chun-
ker described below in section 3. We manually scored the top 100 terms generated for two classes of
biology patents (US patent classes 435 and 436) and achieved accuracies of 85% and 76% respectively.
We also manually evaluated the top 100 terms taken from biology articles, yielding an accuracy of about
88%. As discussed, we use the output of this system for our jargon-term extraction system.
3 Jargon-term Extraction by Chunking
(Justeson and Katz, 1995) uses manual rules to detect noun groups (sequences of nouns and adjectives
ending in a noun) with the goal of detecting instances of topic-terms. They filter out those noun groups
that occur only once in the document on the theory that the multiply used noun groups are more likely to
be topics. They manually score their output from two computer science articles and one biotechnology
article, with 146, 350 and 834 instances of terms and achieve accuracies of 96%, 86% and 77%. (Frantzi
et al., 2000) uses linguistic rules similar to noun chunking to detect candidate terms; filters the results
using a stop list and other linguistic constraints; uses statistical filters to determine whether substrings
are likely to be terms as well; and uses statistical filters based on neighboring words (context). (Frantzi et
al., 2000) ranks their terms by scores and achieve about 75% accuracy for the top 40 terms ? their system
is tested on medical records (quite a different corpus form ours). Our system identifies all instances
of terminology (not just topic terms) and identifies many more instances per document (919, 1131 and
2166) than (Justeson and Katz, 1995) or (Frank, 2000). As we aim to find all instances of jargon-terms,
we evaluate for both precision and recall rather than just accuracy (section 5). Two of the documents
that we test on are patents, which have a very different word distribution than articles. In fact, due to
both the amount of repetition in patents and the presence of multiple types of terminology (legal terms
as well as topic-related terms), it is hard to imagine that eliminating terms occurring below a frequency
threshold (as with (Justeson and Katz, 1995)) would be an effective method of filtering. Furthermore,
(Frank, 2000) used a very different corpus than we did and they focused on a slightly different problem
(e.g., we did not attempt to find the highest-ranked terms and we did not attempt to find both long terms
and substrings which were terms). Thus while it is appropriate to compare our methodology, it is difficult
to compare our results.
We have implemented a hand-crafted term extractor, which we will call a jargon-term chunker because
it functions in much the same way as a noun group chunker. It uses a deterministic finite state machine,
based on parts of speech (POS) and a fine-tuned set of lexical categories. We observed that jargon-terms
are typically noun groups, minus some left modifiers, and normally include words that are not in standard
vocabulary or belong to certain other classes of words (e.g., nominalizations). While topic-term tech-
niques factor the distribution of whole term sequences into the choice of topic-terms, our method focuses
on the distribution of words within topic-term sequences. The primary function of POS classification is
to cluster words distributionally in a language. A POS tag reflects the syntactic distribution of the word
in the sense that words with the same POS should be able to replace each other in sentences. Morpholog-
ically, POSs are subject to the same morphological variation (prefixes, suffixes, tense, gender, number,
etc.). For example, the English word duck belongs to the POS noun because it tends to occur: after a
determiner, after an adjective, and ending a unit that can be the subject of a verb: nouns are substitutable
for each other. Furthermore, it has a plural form resulting from an -s or -es suffix, etc. Similarly, we
hold that the presence of particular classes of words within a noun group affects its potential to function
13
as a jargon-term. As will become evident, we can use topic-term-like metrics to identify some of these
word classes. Furthermore, given our previous assertion that topic-terms are a subset of jargon-terms,
we assume that the most saliently ranked topic-terms are also jargon-terms and words that are commonly
parts of topic-terms tend to be parts of jargon-terms. There are also ?morphological properties? that are
indicative of subsets of jargon-terms: allCap acronyms, chemical formulas, etc.
Our system classifies each word using POS tags, manually created dictionaries and the output of our
own topic-term system. These classifications are achieved in four stages. In the first stage we divide
the text into smaller segments using coordinate conjunctions (and, or, as well as, . . .) and punctuation
(periods, left/right parentheses and brackets, quotation marks, commas, colons, semi-colons). These
segments are typically smaller than the level of the sentence, but larger than most noun groups. These
segments are good units to process because they are larger than jargon-terms (substrings of noun groups)
and smaller than sentences (and thus provide a smaller search space). In the second stage, potential
jargon-term (PJs) are generated by processing tokens from left to right and classifying them using a
finite state machine (FSM). The third stage filters the PJs generated with a set of manually constructed
constraints, yielding a set of jargon-terms. A final filter (stage 4) identifies named entities and separates
them out from the true jargon-terms: it turns out that many named entities have similar phrase-internal
properties as jargon-terms.
The FSM (that generates PJs) in the second stage includes the following states (Ramshaw and Marcus,
1995): START (S) (marking the beginning of a segment), Begin Term (B-T), Inside Term (I-T), End
Term (E-T), and Other (O). A PJ is a sequence consisting of: (a) a single E-T; or (b) exactly one B-T,
followed by zero or more instances of I-T, followed by zero or one instances of E-T. Each transition to
a new state is conditioned on: (a) the (extended) POS tag of the current word; (b) the extended POS
tag of the previous word; and (c) the previous state. The extended POSs are derived from the output of
a Penn-Treebank-based POS tagger and refinements based on machine readable dictionaries, including
COMLEX Syntax (Macleod et al., 1997), NOMLEX (Macleod et al., 1998), and some manually encoded
dictionaries created for this project. Table 1 describes the transitions in the FSM (unspecified entries
mean no restriction). ELSE indicates that in all cases other than those listed, the FST goes to state O.
Extended POS tags are classified as follows.
Adjectives, words with POS tags JJ, JJR or JJS, are subdivided into:
STAT-ADJ: Words in this class are marked adjective in our POS dictionaries and found as the first word
in one of the top ranked topic-terms (for the topic associated with the input document).
TECH-ADJ: If an adjective ends in a suffix indicating (-ic, -cous, -xous, and several others) it is a
technical word, but it is not found in our list of exceptions, it is marked TECH-ADJ.
NAT-ADJ: An adjective, usually capitalized, that is the adjectival form of a country, state, city or conti-
nent, e.g., European, Indian, Peruvian, . . .
CAP-ADJ: An adjective such that the first letter is capitalized (but is not marked NAT-ADJ).
ADJ: Other adjectives
Nouns are marked NN or NNS by the POS tagger and are the default POS for out of vocabulary (OOV)
words. POS tags like NNP, NNPS and FW (proper nouns and foreign nouns) are not reliable for our POS
tagger (trained on news) when applied to patents and technical articles. So NOUN is also assumed for
these. Subclasses include:
O-NOUN: (Singular or plural) nouns not found in any of our dictionaries (COMLEX plus some person
names) or nouns found in lists of specialized vocabulary which currently include chemical names.
PER-NOUN: Nouns beginning with a capital that are in our dictionary of first and last names.
PLUR-NOUN: Nouns with POS NNS nouns that are not marked O-NOUN or PER-NOUN.
C-NOUN: Nouns with POS NN that are not marked O-NOUN or PER-NOUN.
Verbs Only ING-VERBs (VBG) and ED-VERBs (VBN and VBD) are needed for this task (other
verbs trigger state O). Finally, we use the following additional POS tags:
POSS: POS for ?s, split off from a possessive noun.
PREP: All prepositions (POS IN and TO)
ROM-NUM: Roman numerals (I, II, . . ., MMM)
14
Previous Current Previous New
POS POS State State
DET, PREP, POSS, VERB O
O-NOUN, C-NOUN, PLUR-NOUN ROM-NUM B-T, I-T E-T
PLUR-NOUN B-T,I-T I-T
ADJ, CAP-ADJ I-T I-T
C-NOUN, PER-NOUN, O-NOUN B-T, I-T I-T
O-NOUN CAP-ADJ, TECH-ADJ, B-T, I-T I-T
STAT-ADJ, NAT-ADJ
CAP-ADJ, TECH-ADJ, NAT-ADJ, E-T, O, S B-T
ING-VERB, ED-VERB, STAT-ADJ
C-NOUN, O-NOUN, PER-NOUN
TECH-ADJ, NAT-ADJ TECH-ADJ, NAT-ADJ B-T, I-T I-T
ADJ, CAP-ADJ ADJ, CAP-ADJ
ELSE O
Table 1: Transition Table
A potential jargon-term (PJ) is an actual jargon-term unless it is filtered out as follows. First, a jargon
term J must meet all of these conditions:
1. J must contain at least one noun.
2. J must be more than one character long, not counting a final period.
3. J must contain at least one word consisting completely of alphabetic characters.
4. J must not end in a common abbreviation from a list (e.g., cf., etc.)
5. J must not contain a word that violates a morphological filter, designed to rule out numeric identi-
fiers (patent numbers), mathematical formulas and other non-words. This rules out tokens beginning
with numbers that include letters; tokens including plus signs, ampersands, subscripts, superscripts;
tokens containing no alphanumeric characters at all, etc.
6. J must not contain a word that is a member of a list of common patent section headings.
Secondly, a jargon-term J must satisfy at least one of the following additional conditions:
1. J = highly ranked topic-term or a substring of J is a highly ranked topic-term.
2. J contains at least one O-NOUN.
3. J consists of at least 4 words, at least 3 of which are either nominalizations (C-NOUNs found in
NOMLEX-PLUS (Meyers et al., 2004; Meyers, 2007)) or TECH-ADJs.
4. J = nominalization at least 11 characters long.
5. J = multi-word ending in a common noun and containing a nominalization.
A final stage aims to distinguish named entities from jargon-terms. It turns out that named entities, like
jargon terms, include many out of vocabulary words. Thus we look for NEs among those PJs that remain
after stage 3 and contain capitalized words (a single capital letter followed by lowercase letters). These
NE filters are based on manually collected lists of named entities and nationality adjectives, as well as
common NE endings. Dictionary lookup is used to assign GPE (ACE?s Geopolitical Entity) to New York
or American; LOC(ation) to Aegean Sea and Ural Mountains; and FAC(ility) to Panama Canal and Suez
Canal. Plurals of nationality words, e.g., Americans are filtered out as non-terms. PJs are filtered by
endings typically associated with non-terms, e.g., et al signals PJs as citations to articles and honorifics
(Esq, PhD, Jr, Snr) signal PER(son) named entities. Finally, if at least one of the words in a multi-word
term is a first or last person name, we can further filter them by endings, where ORGanization endings
15
include Agency, Association, College and more than 65 others; GPE endings include Heights, Township,
Park; LOC(ation) endings include Street, Avenue and Boulevard. It turns out that 2 word capitalized
structures including at least one person name are usually either ORG or GPE in our patent corpus, and
we maintain this ambiguity, but mark them as non-terms.
We have described a first implementation of a jargon-term chunker based on a combination of prin-
ciples previously implemented in noun group chunking and topic-term extraction systems. The chunker
can use essentially the same algorithms as previous noun group chunkers, though in this case we used
a manual-rule based FSM. The extended POSs are defined according to conventional POS (represent-
ing substitutability, morphology, etc.), statistical topic-term extraction, OOV status (absence from our
dictionary) or presence in specialized dictionaries (NOMLEX, dictionary of chemicals, etc.). We use
topic-term extraction to identify both particular noun sequences (high-ranked topic-terms) and some of
their components (STAT-ADJ), and could extend this strategy to other components, e.g., common head
nouns. We approximated the concept of ?rare word? by noting which words were not found in our
standard dictionary (O-NOUN). As is well-known, ?noun? and ?adjective? are the first and second most
frequent POS for OOV words and both POSs are typically found as part of noun groups. Furthermore,
rare instances of O-NOUN (and OOV adjectives) are typically parts of jargon-terms. This approximation
is fine-tuned by the addition of word lists (e.g., chemicals). In future work, we can use more distribu-
tional information to fine-tune these categories, e.g., we can use topic-term techniques to identify single
topic words (nouns and adjectives) and experiment with these additional POS (instead of or in addition
to the current POS classes).
4 The Annotator Definition of Jargon-Term
For purposes of annotation, we defined jargon-term as a word or multi-word nominal expression that is
specific to some technical sublanguage. It need not be a proper noun, but it should be conventionalized
in one of the following two ways:
1. The term is defined early (possibly by being abbreviated) in the document and used repeatedly
(possibly only in its abbreviated form).
2. The term is special to a particular field or subfield (not necessarily the field of the document being
annotated). It is not enough if the document contains a useful description of an object of interest
? there must be some conventional, definable term that can be used and reused. Thus multi-word
expressions that are defined as jargon terms must be somewhat word-like ? mere descriptions that
are never reused verbatim are not jargon terms. (Justeson and Katz, 1995) goes further than we do:
they require that terms be reused within the document being annotated, whereas we only require
that they be reused (e.g., frequent hits in a web search).
Criterion 2 leaves open the question of how specific to a genre an expression must be to be considered a
jargon-term. At an intuitive level, we would like to exclude words like patient, which occur frequently
in medical texts, but are also commonly found in non-expert, everyday language. By contrast, we would
like to include words like tumor and chromosome, which are more intrinsic to technical language insofar
as they have specialized definitions and subtypes within medical language. To clarify, we posited that a
jargon-term must be sufficiently specialized so that a typical naive adult should not be expected to know
the meaning of the term. We developed 2 alternative models of a naive adult:
1. Homer Simpson, an animated TV character who caricatures the typical naive adult?the annotators
invoke the question: Would Homer Simpson know what this means?
2. The Juvenile Fiction sub-corpus of the COCA: The annotators go to http://corpus.byu.
edu/coca/ and search under FIC:Juvenile ? a single occurrence of an expression in this corpus
suggests that it is probably not a jargon-term.
In addition, several rules limited the span of terms to include the head and left modifiers that collocate
with the heads. Decisions about which modifiers to include in a term were difficult. However, as this
16
Strict Sloppy
Doc Terms Matches Pre Rec F Matches Pre Rec F
Annot 1
SRP 1131 798 70.8% 70.6% 70.7% 1041 92.5% 92.0% 92.2%
SUP 2166 1809 87.5% 83.5% 85.5% 1992 96.3% 92.0% 94.1%
VVA 919 713 90.9% 77.6% 83.7% 762 97.2% 82.9% 89.5%
Annot 2
SRP 1131 960 98.4% 84.9% 91.1% 968 99.2% 85.6% 91.9%
SUP 2166 1999 95.5% 92.3% 93.8% 2062 98.5% 95.2% 96.8%
VVA 919 838 97.4% 91.2% 94.2% 855 99.4% 93.0% 96.1%
Base 1
SRP 1131 602 24.3% 53.2% 33.4% 968 44.2% 96.8% 60.7%
SUP 2166 1367 36.5% 63.1% 46.2% 1897 50.6% 87.6% 64.2%
VVA 919 576 28.5% 62.7% 39.2% 887 44.0% 96.5% 60.4%
Base 2:
SRP 1131 66 24.9% 5.8% 9.5% 151 57.0% 13.4% 21.6%
SUP 2166 771 52.3% 35.6% 42.4% 1007 68.4% 46.5% 55.3%
VVA 919 270 45.8% 29.4% 35.8% 392 66.5% 42.6% 51.9%
System SRP 1131 932 39.0% 82.4% 53.0% 1121 46.9% 99.1% 63.7%
Without SUP 2166 1475 39.7% 68.1% 50.2% 1962 52.8% 90.6% 66.7%
Filter VVA 919 629 27.8% 68.4% 39.5% 900 39.8% 97.9% 56.6%
System
SRP 1131 669 69.0% 59.2% 63.7% 802 82.8% 70.9% 76.4%
SUP 2166 1193 64.7% 55.1% 59.5% 1526 82.8% 70.5% 76.1%
VVA 919 581 62.1% 63.2% 62.7% 722 77.2% 78.6% 77.9%
Table 2: Evaluation of Annotation, Baseline and Complete System Against Adjudicated Data
evaluation task came on the heels of the relation extraction task described in section 6, we based our
extent rules on the definitions and the set of problematic examples that were discussed and cataloged
during that project. This essentially formed the annotation equivalent of case-law for extents. We will
make our annotation specifications available on-line, along with discussions of these cases.
5 Evaluation
For evaluation purposes, we annotated all the instances of jargon-terms in a speech recognition patent
(SRP), a sunscreen patent (SUP) and an article about a virus vaccine (VVA). Each document was an-
notated by 2 people and then adjudicated by Annotator 2 after discussing controversial cases. Table 2
scores the system, annotator 1 and annotator 2, by comparing each against the answer key providing:
number of terms in the answer key, number of matches, precision, recall and F-measure. The ?strict?
scores are based on exact matches between system terms and answer key terms, whereas the ?sloppy?
scores count as correct instances where part of a system term matches part of an answer key term (span
errors). As the SRP document was annotated first, some of specification agreement process took place
after annotation and the scores for annotators are somewhat lower than for the other documents. How-
ever, Annotator 1?s scores for SUP and VVA are good approximations of how well a human being should
be expected to perform and the system?s scores should be compared to Annotator 1 (i.e., accounting for
the adjudicator?s bias).
There are 4 system results: two baseline systems and two stages of the system described in section 3.
Baseline 1 assumes terms derived by removing determiners from noun groups ? we used an MEMM
chunker using features from the GENIA corpus (Kim et al., 2003). That system has relatively high recall,
but overgenerates, yielding a lower precision and F-measure than our full system ? it is also inaccurate
at determining the extent of terms. Baseline 2 restricts the noun groups from this same chunker to those
with O-NOUN heads. This improves the precision at a high cost to recall. Similarly, we first ran our
system without filtering the potential jargon-terms, and then we ran the full system. Clearly our more
complex strategy performs better than these baselines and the linguistic filters increase precision more
than they reduce recall, resulting in higher F-measures (though low-precision high-recall output may be
better for some applications).
17
6 Relations with Jargon-Terms
(Meyers et al., 2014) describes the annotation of 200 PubMed articles from and 26 patents with several
relations, as well as a system for automatically extracting relations. It turned out that the automatic
system depended on the creation of a jargon-term extraction system and thus that work was the major
motivating factor for the research described here. Choosing topic-terms as potential arguments would
have resulted in low recall. In contrast, allowing any noun-group to be an argument would have lowered
precision, e.g., diagram, large number, accordance and first step are unlikely to be valid arguments of
relations. In the example: The resequencing pathogen microarray
A2
in the diagram is a promising new
technology., we can detect that the authors of the articles view pathogen microarray as significant, and
not the NG diagram. By selecting jargon-terms as potential arguments we are selecting the most probable
noun group arguments for our relations. For the current system (which does not use a parser), the system
performs best if non-jargon-terms are not considered as potential relation arguments at all. However, one
could imagine a wider coverage (and slower) system incorporating a preference for jargon-terms (like a
selection restriction) with dependency-based constraints.
We will only describe a few of these relations due to space considerations. Our relations include:
(1) ABBREVIATE, a relation between two terms that are equivalent. In the normal case, one term
is clearly a shorthand version of the other, e.g., ?The D. melanogaster gene Muscle LIM protein at
84B
A1
(abbreviated as Mlp84B
A2
)?. However, in the special case (ABBREVIATE:ALIAS) neither
term is a shorthand for the other. For example in ?Silver behenate
A1
, also known as CH3-(CH2)20-
COOAg
A2
?, the chemical name establishes that this substance is a salt, whereas the formula provides
the proportions of all its constituent elements; (2) ORIGINATE, the relation between an ARG1 (person,
organization or document) and an ARG2 (a term), such that the ARG1 is an inventor, discoverer, manu-
facturer, or distributor of the ARG2 and some of these roles are differentiated as subtypes of the relation.
Examples include the following: ?Eagle
A1
?s minimum essential media
A2
and DOPG
A2
was obtained
from Avanti Polar Lipids
A1
?. (3) EXEMPLIFY, an IS-A relation (Hearst, 1992) between terms so
that ARG1 is an instance of ARG2, e.g., ?Cytokines
A2
, for instance interferon
A1
?; and ?proteins
A2
such as insulin
A1
?; (4) CONTRAST relations, e.g., ?necrotrophic effector system
A1
that is an ex-
citing contrast to the biotrophic effector models
A2
?; (5) BETTER THAN relations, e.g., ?Bayesian
networks
A1
hold a considerable advantage over pairwise association tests
A2
?; and (6) SIGNIFICANT
relations, e.g., ?Anaerobic SBs
A2
are an emerging area of research and development? (ARG1, the author
of the article, is implicit). These relations are applicable to most technical genres.
7 Concluding Remarks
We have described a method for extracting instances of jargon-terms with an F-measure of between
62% and 77% (strict vs sloppy), about 73% to 84% of human performance. We expect this work to
facilitate the extraction of a wide reange of relations from technical documents. Previous work has
focused on generating topic-terminology or term types, extracted over sets of documents. In contrast, we
describe an effective method of extracting term tokens, which represent a larger percent of the instances
of terminology in documents and constitute arguments of many more potential relations. Our work on
relation extraction yielded very low recalls until we adopted this methodology. Consequently, we have
obtained recall of over 50% for many relations (with precision ranging from 70% for OPINION relations
like Significant to 96% for Originate.).
Acknowledgments
Supported by the Intelligence Advanced Research Projects Activity (IARPA) via Department of Inte-
rior National Business Center contract number D11PC20154. The U.S. Government is authorized to
reproduce and distribute reprints for Governmental purposes notwithstanding any copyright annotation
thereon. Disclaimer: The views and conclusions contained herein are those of the authors and should
not be interpreted as necessarily representing the official policies or endorsements, either expressed or
implied, of IARPA, DoI/NBC, or the U.S. Government.
18
References
M. Bada, L. E. Hunter, M. Eckert, and M. Palmer. 2010. An overview of the craft concept annotation guidelines.
In The Linguistic Annotation Workshop, ACL 2010, pages 207?211.
BioCreAtIvE. 2006. Biocreative ii.
M. Bundschus, M. Dejori, M. Stetter, V Tresp, and H. Kriegel. 2008. Extraction of semantic biomedical relations
from text using conditional random fields. BMC Bioinformatics, 9.
P. Corbett, C. Batchelor, and S. Teufel. 2007. Annotation of chemical named entities. In BioNLP 2007, pages
57?64.
Thomas M. Cover and Joy A. Thomas. 1991. Elements of Information Theory. Wiley-Interscience, New York.
A. Frank. 2000. Automatic F-Structure Annotation of Treebank Trees. In Proceedings of The LFG00 Conference,
Berkeley.
K. Frantzi, S. Ananiadou, and H. Mima. 2000. Automatic recognition of multi-word terms:. the C-value/NC-value
method. International Journal on Digital Libraries, 3(2):115?130.
C. Giuliano, A. Lavelli, and L. Romano. 2006. Exploiting shallow linguistic information for relation extraction
from biomedical literature. In EACL 2006, pages 401?408, Trento.
M. Hearst. 1992. Automatic Acquisition of Hyponyms from Large Text Corpora. In ACL 1992, pages 539?545.
T. Hisamitsu, Y. Niwa, S. Nishioka, H. Sakurai, O. Imaichi, M. Iwayama, and A. Takano. 1999. Term extraction
using a new measure of term representativeness. In Proceedings of the First NTCIR Workshop on Research in
Japanese Text Retrieval and Term Recognition.
Houghton Mifflin Company. 2001. Webster?s II New College Dictionary. Houghton Mifflin Company.
C. Jacquemin and D. Bourigault. 2003. Term Extraction and Automatic Indexing. In R. Mitkov, editor, Handbook
of Computational Linguistics. Oxford University Press, Oxford.
Y. Jin, M. Kan, J. Ng, and X. He. 2013. Mining scientific terms and their definitions: A study of the acl anthology.
In EMNLP-2013.
J. S. Justeson and S. M. Katz. 1995. Technical terminology: some linguistic properties and an algorithm for
identification in text. Natural Language Engineering, 1(1):9?27.
J. D. Kim, T. Ohta, Y. Tateisi, and J. I. Tsujii. 2003. Genia corpus?a semantically annotated corpus for bio-
textmining. Bioinformatics, 19 (suppl 1):i180?i182.
C. Macleod, R. Grishman, and A. Meyers. 1997. COMLEX Syntax. Computers and the Humanities, 31:459?481.
C. Macleod, R. Grishman, A. Meyers, L. Barrett, and R. Reeves. 1998. Nomlex: A lexicon of nominalizations. In
Proceedings of Euralex98.
A. Meyers, R. Reeves, C. Macleod, R. Szekeley, V. Zielinska, and B. Young. 2004. The Cross-Breeding of
Dictionaries. In Proceedings of LREC-2004, Lisbon, Portugal.
A. Meyers, G. Lee, A. Grieve-Smith, Y. He, and H. Taber. 2014. Annotating Relations in Scientific Articles. In
LREC-2014.
A. Meyers. 2007. Those Other NomBank Dictionaries ? Manual for Dictionaries that Come with NomBank.
http:nlp.cs.nyu.edu/meyers/nombank/nomdicts.pdf.
R. Navigli and P. Velardi. 2004. Learning Domain Ontologies from Document Warehouses and Dedicated Web
Sites. Computational Linguistics, 30.
L. A. Ramshaw and M. P. Marcus. 1995. Text Chunking using Transformation-Based Learning. In ACL Third
Workshop on Very Large Corpora, pages 82?94.
A. Schwartz and M. Hearst. 2003. A simple algorithm for identifying abbreviation definitions in biomedical text.
In Pacific Composium on Biocomputing.
T. Tomokiyo and M. Hurst. 2003. A language model approach to keyphrase extraction. In ACL 2003 Workshop
on Multiword Expressions: Analysis, Acquisition and Treatment.
19
P. Velardi, M. Missikoff, and R. Basili. 2001. Identification of relevant terms to support the construction of domain
ontologies. In Workshop on Human Language Technology and Knowledge Management - Volume 2001, pages
5:1?5:8.
20
