Balto-Slavonic Natural Language Processing 2007, June 29, 2007, pages 82?87,
Prague, June 2007. c?2007 Association for Computational Linguistics
Multilingual Word Sense Discrimination: A Comparative Cross-Linguistic
Study
Alla Rozovskaya
Department of Linguistics
Univ. of Illinois at Urbana-Champaign
Urbana, IL 61801
rozovska@uiuc.edu
Richard Sproat
Department of Linguistics
Univ. of Illinois at Urbana-Champaign
Urbana, IL 61801
rws@uiuc.edu
Abstract
We describe a study that evaluates an ap-
proach to Word Sense Discrimination on
three languages with different linguistic
structures, English, Hebrew, and Russian.
The goal of the study is to determine
whether there are significant performance
differences for the languages and to iden-
tify language-specific problems. The algo-
rithm is tested on semantically ambiguous
words using data from Wikipedia, an online
encyclopedia. We evaluate the induced clus-
ters against sense clusters created manually.
The results suggest a correlation between the
algorithm?s performance and morphological
complexity of the language. In particular,
we obtain FScores of 0.68 , 0.66 and 0.61 for
English, Hebrew, and Russian, respectively.
Moreover, we perform an experiment on
Russian, in which the context terms are lem-
matized. The lemma-based approach signif-
icantly improves the results over the word-
based approach, by increasing the FScore by
16%. This result demonstrates the impor-
tance of morphological analysis for the task
for morphologically rich languages like Rus-
sian.
1 Introduction
Ambiguity is pervasive in natural languages and cre-
ates an additional challenge for Natural Language
applications. Determining the sense of an ambigu-
ous word in a given context may benefit many NLP
tasks, such as Machine Translation, Question An-
swering, or Text-to-Speech synthesis.
The Word Sense Discrimination (WSD) or Word
Sense Induction task consists of grouping together
the occurrences of a semantically ambiguous term
according to its senses. Word Sense Discrimination
is similar to Word Sense Disambiguation, but allows
for a more unsupervised approach to the problem,
since it does not require a pre-defined set of senses.
This is important, given the number of potentially
ambiguous words in a language. Moreover, labeling
an occurrence with its sense is not always necessary.
For example, in Information Retrieval WSD would
be useful for the identification of documents relevant
to a query containing an ambiguous term.
Different approaches to WSD have been pro-
posed, but the evaluation is often conducted using
a single language, so it is difficult to predict per-
formance on another language. To the best of our
knowledge, there has not been a systematic com-
parative analysis of WSD systems on different lan-
guages. Yet, it is interesting to see whether there
are significant differences in performance when a
method is applied to several languages that have dif-
ferent linguistic structures. Identifying the reasons
for performance differences might suggest what fea-
tures are useful for the task.
The present project adopts an approach to WSD
that is based on similarity measure between context
terms of an ambiguous word. We compare the per-
formance of an algorithm for WSD on English, He-
brew, and Russian, using lexically ambiguous words
and corpora of similar sizes.
We believe that testing on the above languages
82
might give an idea about how accuracy of an algo-
rithm for WSD is affected by language choice. Rus-
sian is a member of the Slavic language group and is
morphologically rich. Verbs, nouns, and adjectives
are characterized by a developed inflectional system,
which results in a large number of wordforms. He-
brew is a Semitic language, and is complex in a dif-
ferent way. In addition to the root-pattern morphol-
ogy that affects the word stem, it also has a com-
plex verb declination system. Moreover, function
words, such as prepositions and determiners, cliti-
cize, thereby increasing the number of wordforms.
Lastly, cliticization, coupled with the absence of
short vowels in text, introduces an additional level
of ambiguity for Hebrew.
There are two main findings to this study. First,
we show that the morphological complexity of the
language affects the performance of the algorithm
for WSD. Second, the lemma-based approach to
RussianWSD significantly improves the results over
the word-based approach.
The rest of the paper is structured as follows:
first, we describe previous work that is related to the
project. Section 3 provides details about the algo-
rithm for WSD that we use. We then describe the
experiments and the evaluation methodology in Sec-
tions 4 and 5, respectively. We conclude with a dis-
cussion of the results and directions for future work.
2 Related Work
First, we describe several approaches to WSD that
are most relevant to the present project: Since we
are dealing with languages that do not have many
linguistic resources available, we chose a most unsu-
pervised, knowledge-poor approach to the task that
relies on words occurring in the context of an am-
biguous word. Next, we consider two papers on
WSD that provide evaluation for two languages. Fi-
nally, we describe work that is concerned with the
role of morphology for the task.
2.1 Approaches to Word Sense Discrimination
Pantel and Lin (2002) learn word sense induction
from an untagged corpus by finding the set of the
most similar words to the target and by clustering
the words. Each word cluster corresponds to a sense.
Thus, senses are viewed as clusters of words.
Another approach is based on clustering the oc-
currences of an ambiguous word in a corpus into
clusters that correspond to distinct senses of the
word. Based on this approach, a sense is defined
as a cluster of contexts of an ambiguous word. Each
occurrence of an ambiguous word is represented as a
vector of features, where features are based on terms
occurring in the context of the target word. For ex-
ample, Pedersen and Bruce (1997) cluster the oc-
currences of an ambiguous word by constructing a
vector of terms occurring in the context of the tar-
get. Schu?tze (1992) presents a method that explores
the similarity between the context terms occurring
around the target. This is accomplished by consider-
ing feature vectors of context terms of the ambigu-
ous word. The algorithm is evaluated on natural and
artificially-constructed ambiguous English words.
Sproat and van Santen (1998) introduce a tech-
nique for automatic detection of ambiguous words
in a corpus and measuring their degree of polysemy.
This technique employs a similarity measure be-
tween the context terms similar in spirit to the one
in (Schu?tze, 1992) and singular value decomposi-
tion in order to detect context terms that are impor-
tant for disambiguating the target. They show that
the method is capable of identifying polysemous En-
glish words.
2.2 Cross-Linguistic Study of WSD
Levinson (1999) presents an approach to WSD that
is evaluated on English and Hebrew. He finds 50
most similar words to the target and clusters them
into groups, the number of groups being the num-
ber of senses. He reports comparable results for
the two languages, but he uses both morphologi-
cally and lexically ambiguous words. Moreover, the
evaluation methodology focuses on the success of
disambiguation for an ambiguous word, and reports
the number of ambiguous words that were disam-
biguated successfully.
Davidov and Rappoport (2006) describe an al-
gorithm for unsupervised discovery of word cate-
gories and evaluate it on Russian and English cor-
pora. However, the focus of their work is on the dis-
covery of semantic categories and from the results
they report for the two languages it is difficult to in-
fer how the languages compare against each other.
We conduct a more thorough evaluation. We also
83
control cross-linguistically for number of training
examples and level of ambiguity of selected words,
as described in Section 4.
2.3 Morphology and WSD
McRoy (1992) describes a study of different sources
useful for word sense disambiguation, including
morphological information. She reports that mor-
phology is useful, but the focus is on derivational
morphology of the English language. In the present
context, we are interested in the effect of inflectional
morphology onWSD, especially for languages, such
as Russian and Hebrew.
Gaustad (2004) proposes a lemma-based ap-
proach to a Maximum Entropy Word Sense Disam-
biguation System for Dutch. She shows that collaps-
ing wordforms of an ambiguous word yields a more
robust classifier due to the availability of more train-
ing data. The results indicate an improvement of this
approach over classification based on wordforms.
3 Approach
Our algorithm relies on the method for selection of
relevant contextual terms and on distance measure
between them introduced in (Sproat and van Santen,
1998) and on the approach described in (Schu?tze,
1998), though the details of clustering differ slightly.
The intuition behind the algorithm can be summa-
rized as follows: (1) words that occur in the context
of the ambiguous word are useful for determining
its sense; and (2) contextual terms of an ambiguous
word belong to topics corresponding to the senses
of the ambiguous word. Before describing the algo-
rithm in detail, we give an overview of the system.
The algorithm starts by collecting all the occur-
rences of an ambiguous word in the corpus together
with the surrounding context. Next, we build a sym-
metric distance matrix D, where rows and columns
correspond to context terms, and D[i][j] is the dis-
tance value of term i and term j. The distance mea-
sure is supposed to reflect how the two terms are
close semantically (whether they are related to the
same topic). For example, we would expect the dis-
tance between the words financial and money to be
smaller than the distance between the words finan-
cial and river: The first pair is more likely to occur
in the same context, than the second one. Using the
distance measure, the context terms are partitioned
into sense clusters. Finally, we group the sentences
containing the ambiguous word into sentence clus-
ters using the context term clusters.
We now describe each step in detail:
1. We collect contextual terms of an ambigu-
ous word w in a context window of 50 words
around the target. Each context term t is as-
signed a weight (Sproat and J. van Santen,
1998):
wt =
CO(t|w)
FREQ(t)
(1)
CO(t|w) is the frequency of the term in the
context of w, and FREQ(t) is the frequency
of the term in the corpus. Term weights are
used to select context terms that will be help-
ful in determining the sense of the ambiguous
word in a particular context. Furthermore, term
weights are employed in (4) in sentence clus-
tering.
2. For each pair ti and tj of context terms, we
compute the distance between them (Sproat
and J. van Santen,1998):
Dw[i][j] = 1 ?
[
COw(ti|tj)
FREG(ti)
+
COw(tj |ti)
FREQ(tj)
]
2
(2)
COw(ti|tj) is the frequency of ti in the con-
text of tj , and FREQ(ti) is the frequency of
ti in the training corpus. We assume that the
distance between ti and tj is inversely propor-
tional to the semantic similarity between ti and
tj .
3. Using the distance matrix from (2), the con-
text terms are clustered using an agglomerative
clustering technique:
? Start by assigning each context term to a separate
cluster
? While stopping criterion is false: merge two clus-
ters whose distance 1 is the smallest.2
1There are several ways to define the distance between clus-
ters. Having experimented with three - Single Link, Complete
Link and Group Average, it was found that Complete Link def-
inition works best for the present task. (Complete Link distance
between clusters i and j is defined as the maximum distance be-
tween a term from cluster i and a term from cluster j).
2In the present study, the clusters are merged as long as the
84
The output of step (3) is a set of context term
clusters for the target word. Below are shown
select members for term clusters for the English
word bass:
Cluster 1: songwriter singer joined keyboardist
Cluster 2: waters fishing trout feet largemouth
4. Finally, the sentences containing the ambigu-
ous word are grouped using the context term
clusters from (3). Specifically, given a sen-
tence with the ambiguous word, we compute
the score of the sentence with respect to each
context word cluster in (3) and assign the sen-
tence to the cluster with the highest score. The
score of the sentence with respect to cluster c
is the sum of weights of sentence context terms
that are in c.
4 Experiments
The algorithm is evaluated on 9 ambiguous words
with two-sense distinctions. We select words that
(i) have the same two-sense distinction in all three
languages or (ii) are ambiguous in one of the lan-
guages, but each of their senses corresponds to an
unambiguous translation in the other two languages.
In the latter case, the translations are merged to-
gether to create an artificially ambiguous word. We
believe that this selection approach allows for a col-
lection of a comparable set of ambiguous words for
the three languages. An example of an ambiguous
word is the English word table, that corresponds to
two gross sense distinctions (tabular array, and a
piece of furniture). This word has two translations
into Russian and Hebrew, that correspond to the two
senses. The selected words are presented in Table 1.
The words display different types of ambigu-
ity. In particular, disambiguating the Hebrew word
gishah (access; approach) or the Russian word mir
(peace; world) would be useful in Machine Transla-
tion, while determining the sense of a word like lan-
guage would benefit an Information Retrieval sys-
tem. It should also be noted that several words pos-
sess additional senses, which were ignored because
they rarely occurred in the corpus. For example, the
Russian word yazyk (language) also has the meaning
of tongue (body part).
number of clusters exceeds the number of senses of the ambigu-
ous word in the test data.
The corpus for each language consists of 15M
word tokens, and for the same ambiguous word the
same number of training examples is selected from
each language. For each ambiguous word, a set of
100-150 examples together with 50 words of con-
text is selected from the section of the corpus not
used for training. These examples are manually an-
notated for senses and used as the test set for each
language.
5 Evaluation Methodology
The evaluation is conducted by comparing the in-
duced sentence clusters to clusters created manually.
We use three evaluation measures : cluster purity,
entropy, and FScore. 3
For a cluster Cr of size qr, where the size is the
number of examples in that cluster, the dominating
sense Si in that cluster is selected and cluster purity
is computed as follows:
P (Cr) =
nir
qr
, (3)
where nir is the number of examples in cluster Cr
with sense Si.
For an ambiguous word w, cluster purity P(w) is
the weighted average of purities of the clusters for
that word. 4. Higher cluster purity score corresponds
to a better clustering outcome.
Entropy and FScore measures are described in de-
tail in Zhao and Karypis (2005). Entropy indicates
how distinct senses are distributed between the two
clusters. The perfect distribution is the assignment
of all examples with sense 1 to one cluster and all
examples with sense 2 to the other cluster. In such
case, the entropy is 0. In general, a lower value in-
dicates a better cluster quality. Entropy is computed
for each cluster. Entropy for word w is the weighted
average of the entropies of the clusters for that word.
Finally, FScore considers both the coverage of the
algorithm and its ability to discriminate between the
two senses. FScore is computed as the harmonic
3Examples whose scores with respect to all clusters are zero
(examples that do not contain any terms found in the distance
matrix) are not assigned to any cluster, and thus do not affect
cluster purity and cluster entropy. This is captured by the FS-
core measure described below.
4In the present study, the number of clusters and the number
of senses for a word is always 2
85
Senses English Hebrew Russian
access;approach access;approach gishah dostup;podxod
actor;player actor;player saxqan akter;igrok
evidence; quarrel argument vikuax;nimuq argument
body part; chief head rosh golova;glava
world;peace world; peace shalom;olam mir
furniture; tabular array table shulxan;tavlah stol;tablitza
allow;resolve allow;resolve hershah;patar razreshat?
ambiance; air atmosphere avira;atmosfera atmosfera
human lang.;program. lang. language safah yazyk
Table 1: Ambiguous words for testing: The first column indicates the senses; unambiguous translations that
were merged to create an ambiguous word are indicated by a semicolon
mean of Precision and Recall, where recall and pre-
cision for sense Si with respect to cluster Cr are
computed by treating cluster Cr as the output of a
retrieval system for sense Si .
6 Results and Discussion
We show results for two experiments. Experi-
ment 1 compares the algorithm?s performance cross-
linguistically without morphological analysis ap-
plied to any of the languages. Experiment 2 com-
pares the performance for Russian in two settings:
with and without morphological processing per-
formed on the context terms.
Table 2 presents experimental results. Baseline is
computed by assigning the most common sense to
all occurrences of the ambiguous word. We observe
that English achieves the highest performance both
in terms of cluster purity and FScore, while Russian
performs most poorly among the three languages.
This behavior may be correlated with the average
frequency of the context terms that are used to con-
struct the distance matrix in the corpus (cf. 7 for
English and 4.2 for Russian). In particular, the dif-
ference in the frequencies can be attributed to the
morphological complexity of Russian, as compared
to English and Hebrew. Hebrew is more complex
than English morphologically, which would account
for a drop in performance for the Hebrew words vs.
the English words. Furthermore, one would expect
a higher degree of ambiguity for Hebrew due to the
absence of short vowels in text.
It is worth noting that while both Hebrew and
Russian possess features that might negatively af-
fect the performance, Hebrew performs better than
Russian. We hypothesize that cliticization and the
lack of vowels in text are not as significant factors
for the performance as the high inflectional nature
of a language, such as Russian. We observe that the
majotity of the context terms selected by the algo-
rithm for disambiguation belong to the noun cate-
gory. This seems intuitive, since nouns generally
provide more information content than other parts
of speech and thus should be useful for resolving
lexical ambiguity. While an English or a Hebrew
noun only has several wordforms, a Russian noun
may have up to 12 different forms due to various in-
flections.
The morphological complexity of Russian affects
the performance in two ways. First, cluster purity
is affected, since the counts of context terms are not
sufficiently reliable to accurately estimate term dis-
tances. Incorrect term distances subsequently affect
the quality of the term clusters. Second, the percent-
age of default occurrences (examples that have no
context terms occurring in the distance matrix) is the
least for English (0.22) and the highest for Russian
(0.27). The default occurrences affect the recall.
The results of experiment 2 support the fact that
morphological complexity of a language negatively
affects the performance. In that experiment, the in-
flections are removed from all the context terms. We
apply a morphological analyzer 5 to the corpus and
replace each word with its lemma. In 10% of the
word tokens, the analyzer gives more than one pos-
sible analysis, in which case the first analysis is se-
lected. As can be seen in Table 2 (last row), remov-
ing inflections produces a significant improvement
both in recall and precision, while preserving the
cluster purity and slightly reducing cluster entropy.
Moreover, the performance in terms of recall, pre-
cision, and coverage is better than for English and
5Available at http://www.aot.ru/
86
Language Baseline Coverage Precision Recall FScore Purity Entropy
English 0.73 0.78 0.77 0.61 0.68 0.79 0.61
Hebrew 0.72 0.79 0.76 0.58 0.66 0.82 0.59
Russian 0.72 0.73 0.70 0.54 0.61 0.81 0.62
Russian(lemma) 0.72 0.80 0.77 0.66 0.71 0.82 0.61
Table 2: Results: Baseline is the most frequent sense; coverage is the number of occurrences on which the
decision was made by the algorithm
Hebrew.
7 Conclusions and Future Work
We have described a cross-linguistic study of a
Word Sense Discrimination technique. An algo-
rithm based on context term clustering was applied
to ambiguous words from English, Hebrew, and
Russian, and a comparative analysis of the results
was presented. Several observations can be made.
First, the results suggest that the performance can
be affected by morphological complexity in the case
of a language, such as Russian, specifically, both in
terms of precision and recall. Second, removing in-
flectional morphology not only boosts the recall, but
significantly improves the precision. These results
support the view that morphological processing is
beneficial for WSD.
For future work, we plan to investigate more
thoroughly the role of morphological analysis
for WSD in Russian and Hebrew. In particular,
we will focus on the inflectional morphology of
Russian in order to determine whether removing
inflections consistently improves results for Russian
ambiguous words across different parts of speech.
Further, considering the complex structure of the
Hebrew language, we would like to determine what
kind of linguistic processing is useful for Hebrew in
the WSD context.
Acknowledgments
We are grateful to Roxana Girju and the anony-
mous reviewers for very useful suggestions and
comments. This work is funded in part by grants
from the National Security Agency and the National
Science Foundation.
References
Dmitry Davidov and Ari Rappoport 2006. Efficient Un-
supervised Discovery of Word Categories Using Sym-
metric Patterns and High Frequency Words. In Pro-
ceedings of the 21st International Conference on Com-
putational Linguistics and 44th Annual Meeting of the
Association for Computational Linguistics, 297?304.
Sydney, Australia.
Richard O. Duda and Peter E. Hart. 1973. Pattern Classi-
fication and Scene Analysis. John Wiley & Sons, New
York.
Tanja Gaustad. 2004. A Lemma-Based Approach to a
Maximum Entropy Word Sense Disambiguation Sys-
tem for Dutch. In Proceedings of the 20th Interna-
tional Conference on Computational Linguistics (Col-
ing 2004), 778-784. Geneva.
Dmitry Levinson. 1999. Corpus-Based Method
for Unsupervised Word Sense Disambiguation.
www.stanford.edu/ dmitryle/acai99w1.ps.
Susan Weber McRoy. 1992. Using Multiple Knowledge
Sources for Word Sense Discrimination. Computa-
tional Linguistics, 18(1): 1?30.
Patrick Pantel and Dekang Lin. 2002. Discovering
Word Senses from Text In In Proceedings of ACM
SIGKDD, pages 613-619. Edmonton.
Ted Pedersen and Rebecca Bruce. 1997. Distinguishing
word senses in untagged text. In Proceedings of the
Second Conference on Empirical Methods in Natural
Language Processing, 197-207. Providence, RI, Au-
gust.
Hinrich Schu?tze. 1998. Automatic word sense discrimi-
nation. Computational Linguistics, 24(1):97?123.
Richard Sproat and Jan van Santen. 1998. Automatic
ambiguity detection. In Proceedings of International
Conference on Spoken Language Processing . Sydney,
Australia, 1998.
Ying Zhao and George Karypis. 2005. Hierarchical
Clustering Algorithms for Document Datasets Data
Mining and Knowledge Discovery, 10(2):141?168.
87
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 386?389,
Prague, June 2007. c?2007 Association for Computational Linguistics
UIUC: A Knowledge-rich Approach to Identifying Semantic Relations
between Nominals
Brandon Beamer,1,4 Suma Bhat,2,4 Brant Chee,3,4 Andrew Fister,1,4 Alla Rozovskaya,1,4
Roxana Girju1,4
Department of Linguistics1,
Department of Electrical and Computer Engineering2,
Department of Library and Information Science3,
Beckman Institute4,
University of Illinois at Urbana-Champaign
{bbeamer, spbhat2, chee, afister2, rozovska, girju}@uiuc.edu
Abstract
This paper describes a supervised,
knowledge-intensive approach to the auto-
matic identification of semantic relations
between nominals in English sentences.
The system employs different sets of new
and previously used lexical, syntactic, and
semantic features extracted from various
knowledge sources. At SemEval 2007 the
system achieved an F-measure of 72.4% and
an accuracy of 76.3%.
1 Introduction
The SemEval 2007 task on Semantic Relations be-
tween Nominals is to identify the underlying se-
mantic relation between two nouns in the context
of a sentence. The dataset provided consists of a
definition file and 140 training and about 70 test
sentences for each of the seven relations consid-
ered: Cause-Effect, Instrument-Agency, Product-
Producer, Origin-Entity, Theme-Tool, Part-Whole,
and Content-Container. The task is defined as a
binary classification problem. Thus, given a pair
of nouns and their sentential context, the classifier
decides whether the nouns are linked by the target
semantic relation. In each training and test exam-
ple sentence, the nouns are identified and manu-
ally labeled with their corresponding WordNet 3.0
senses. Moreover, each example is accompanied by
the heuristic pattern (query) the annotators used to
extract the sentence from the web and the position
of the arguments in the relation.
(1) 041 ?He derives great joy and <e1>happiness</e1>
from <e2>cycling</e2>.? WordNet(e1) =
?happiness%1:12:00::?, WordNet(e2) = ?cy-
cling%1:04:00::?, Cause-Effect(e2,e1) = ?true?,
Query = ?happiness from *?
Based on the information employed, systems can
be classified in four types of classes: (A) systems
that use neither the given WordNet synsets nor the
queries, (B) systems that use only WordNet senses,
(C) systems that use only the queries, and (D) sys-
tems that use both.
In this paper we present a type-B system that re-
lies on various sets of new and previously used lin-
guistic features employed in a supervised learning
model.
2 Classification of Semantic Relations
Semantic relations between nominals can be en-
coded by different syntactic constructions. We
extend here over previous work that has focused
mainly on noun compounds and other noun phrases,
and noun?verb?noun constructions.
We selected a list of 18 lexico-syntactic and se-
mantic features split here into three sets: feature set
#1 (core features), feature set #2 (context features),
and the feature set #3 (special features). Table 1
shows all three sets of features along with their defi-
nitions; a detailed description is presented next. For
some features, we list previous works where they
proved useful. While features F1 ? F4 were selected
from our previous experiments, all the other features
are entirely the contribution of this research.
Feature set #1: Core features
This set contains six features that were employed
in all seven relation classifiers. The features take
into consideration only lexico-semantic information
386
No. Feature Definition
Feature Set #1: Core features
F1 Argument position indicates the position of the arguments in the semantic relation
(Girju et al, 2005; Girju et al, 2006) (e.g., Part-Whole(e1, e2), where e1 is the part and e2 is the whole).
F2 Semantic specialization this is the prediction returned by the automatic WordNet IS-A semantic
(Girju et al, 2005; Girju et al, 2006) specialization procedure.
F3, F4 Nominalization indicates whether the nouns e1 (F3) and e2 (F4) are nominalizations
(Girju et al, 2004) or not. Specifically, we distinguish here between agential nouns,
other nominalizations, and neither.
F5, F6 Spatio-Temporal features indicate if e1 (F5) or e2 (F6) encode time or location.
Feature Set #2: Context features
F7, F8 Grammatical role describes the grammatical role of e1 (F7) and e2 (F8). There are three
possible values: subject, direct object, or neither.
F9 PP Attachment applies to NP PP constructions and indicates if the prepositional phrase
containing e2 attaches to the NP containing e1.
F10, F11 Semantic Role is concerned with the semantic role of the phrase containing
either e1 (F10) or e2 (F11). In particular, we focused on three semantic
roles: Time, Location, Manner. The feature is set to 1 if the target noun
is part of a phrase of that type and to 0 otherwise.
F12, F13, Inter-noun context sequence is a set of three features. F12 captures the sequence of stemmed
F14 words between e1 and e2, while F13 lists the part of speech sequence in
between the target nouns. F14 is a scoring weight (with possible values
1, 0.5, 0.25, and 0.125) which measures the similarity of an unseen
sequence to the set of sequence patterns associated with a relation.
Feature Set #3: Special features
F15, F16 Psychological feature is used in the Theme-Tool classifier; indicates if e1 (F15) or e2 (F16)
belong or not to a predefined set of psychological features.
F17 Instrument semantic role is used for the Instrument-Agency relation and indicates whether
the phrase containing e1 is labeled as em Instrument or not.
F18 Syntactic attachment is used for the Instrument-Agent relation and indicates whether the phrase
containing the Instrument role attaches to a noun or a verb
Table 1: The three sets of features used for the automatic semantic relation classification.
about the two target nouns.
Argument position (F1) indicates the position of
the semantic arguments in the relation. This infor-
mation is very valuable, since some relations have a
particular argument arrangement depending on the
lexico-syntactic construction in which they occur.
For example, most of the noun compounds encod-
ing Stuff-Object / Part-Whole relations have e1 as
the part and e2 as the whole (e.g., silk dress).
Semantic specialization (F2) is a binary feature
representing the prediction of a semantic specializa-
tion learning model. The method consists of a set
of iterative procedures of specialization of the train-
ing examples on the WordNet IS-A hierarchy. Thus,
after all the initial noun?noun pairs are mapped
through generalization to entity ? entity pairs in
WordNet, a set of necessary specialization iterations
is applied until it finds a boundary that separates pos-
itive and negative examples. This boundary is tested
on new examples for relation prediction.
The nominalization features (F3, F4) indicate if
the target noun is a nominalization and, if yes, of
what type. We distinguish here between agential
nouns, other nominalizations, and neither. The
features were identified based on WordNet and
NomLex-Plus1 and were introduced to filter some
of negative examples, such as car owner/THEME.
Spatio?Temporal features (F5, F6) were also in-
troduced to recognize some near miss examples,
such as Temporal and Location relations. For in-
stance, activation by summer (near-miss for Cause-
Effect) and mouse in the field (near-miss for Content-
Container). Similarly, for Theme-Tool, a word act-
ing as a Theme should not indicate a period of time,
as in <e1>the appointment</e1> was for more
than one <e2>year</e2>. For this we used the in-
formation provided by WordNet and special classes
generated from the works of (Herskovits, 1987),
(Linstromberg, 1997), and (Tyler and Evans, 2003).
1NomLex-Plus is a hand-coded database of 5,000 verb nom-
inalizations, de-adjectival, and de-adverbial nouns.
http://nlp.cs.nyu.edu/nomlex/index.html
387
Feature set #2: Context features
This set takes advantage of the sentence context to
identify features at different linguistic levels.
The grammatical role features (F7, F8) determine
if e1 or e2 is the subject, direct object, or neither.
This feature helps filter out some instances with poor
context, such as noun compounds and identify some
near-miss examples. For example, a restriction im-
posed by the definition of Theme-Tool indicates that
in constructions such as Y/Tool is used for V-ing
X/Theme, neither X nor Y can be the subject of
the sentence, and hence Theme-Tool(X, Y) would be
false. This restriction is also captured by the nomi-
nalization feature in case X or Y is an agential noun.
PP attachment (F9) is defined for NP PP construc-
tions, where the prepositional phrase containing the
noun e2 attaches or not to the NP (containing e1).
The rationale is to identify negative instances where
the PP attaches to any other word before NP in the
sentence. For example, eat <e1>pizza</e1> with
<e2>a fork</e2>, where with a fork attaches to
the verb to eat (cf. (Charniak, 2000)).
Furthermore, we implemented and used two se-
mantic role features which identify the semantic role
of the phrase in a verb?argument structure, phrase
containing either e1 (F10) or e2 (F11). In particular,
we focus on three semantic roles: Time, Location,
Manner. The feature is set to 1 if the target noun
is part of a semantic role phrase and to 0 otherwise.
The idea is to filter out near-miss examples, expe-
cially for the Instrument-Agency relation. For this,
we used ASSERT, a semantic role labeler developed
at the University of Colorado at Boulder2 which was
queried through a web interface.
Inter-noun context sequence features (F12, F13)
encode the sequence of lexical and part of speech
information between the two target nouns. Feature
F14 is a weight feature on the values of F12 and
F13 and indicates how similar a new sequence is to
the already observed inter-noun context associated
with the relation. If there is a direct match, then the
weight is set to 1. If the part-of-speech pattern of the
new substring matches that of an already seen sub-
string, then the weight is set to 0.5. Weights 0.25
and 0.125 are given to those sequences that overlap
entirely or partially with patterns encoding other se-
2http://oak.colorado.edu/assert/
mantic relations in the same contingency set (e.g.,
semantic relations that share syntactic pattern se-
quences). The value of the feature is the summation
of the weights thus obtained. The rationale is that
the greater the weight, the more representative is the
context sequence for that relation.
Feature set #3: Special features
This set includes features that help identify specific
information about some semantic relations.
Psychological feature was defined for the Theme-
Tool relation and indicates if the target noun (F15,
F16) belongs to a list of special concepts. This fea-
ture was obtained from the restrictions listed in the
definition of Theme-Tool. In the example need for
money, the noun need is a psychological feature, and
thus the instance cannot encode a Theme-Tool rela-
tion. A list of synsets from WordNet subhierarchy
of motivation and cognition constituted the psycho-
logical factors. This was augmented with precondi-
tions such as foundation and requirement since they
would not be allowed as tools for the theme.
The Instrument semantic role is used for the
Instrument-Agency relation as a boolean feature
(F17) indicating whether the argument identified as
Instrument in the relation (e.g., e1 if Instrument-
Agency(e1, e2)) belongs to an instrument phrase as
identified by a semantic role tool, such as ASSERT.
The syntactic attachment feature (F18) is a fea-
ture that indicates whether the argument identified
as Instrument in the relation attaches to a verb or to
a noun in the syntactically parsed sentence.
3 Learning Model and Experimental
Setting
For our experiments we chose libSVM, an open
source SVM package3. Since some of our features
are nominal, we followed the standard practice of
representing a nominal feature with n discrete val-
ues as n binary features. We used the RBF kernel.
We built a binary classifier for each of the seven
relations. Since the size of the task training data per
relation is small, we expanded it with new examples
from various sources. We added a new corpus of
3,000 sentences of news articles from the TREC-9
text collection (Girju, 2003) encoding Cause-Effect
(1,320) and Product-Producer (721). Another col-
3http://www.csie.ntu.edu.tw/?cjlin/libsvm/
388
Relation P R F Acc Total Base-F Base-Acc Best features
Cause-Effect 69.5 100.0 82.0 77.5 80 67.8 51.2 F1, F2, F5, F6, F12?F14
Instrument-Agency 68.2 78.9 73.2 71.8 78 65.5 51.3 F7, F8, F10, F11, F15?F18
Product-Producer 84.5 79.0 81.7 76.3 93 80.0 66.7 F1?F4, F12?F14
Origin-Entity 86.4 52.8 65.5 75.3 81 61.5 55.6 F1, F2, F5, F6, F12?F14
Theme-Tool 85.7 41.4 55.8 73.2 71 58.0 59.2 F1?F6, F15, F16
Part-Whole 70.8 65.4 68.0 77.8 72 53.1 63.9 F1?F4
Content-Container 93.1 71.1 80.6 82.4 74 67.9 51.4 F1?F6, F12?F14
Average 79.7 69.8 72.4 76.3 78.4
Table 2: Performance obtained per relation. Precision, Recall, F-measure, Accuracy, and Total (number of examples) are macro-
averaged for system?s performance on all 7 relations. Base-F shows the baseline F measure (all true), while Base-Acc shows the
baseline accuracy score (majority).
lection of 3,129 sentences from Wall Street Journal
(Moldovan et al, 2004; Girju et al, 2004) was con-
sidered for Part-Whole (1,003), Origin-Entity (167),
Product-Producer (112), and Theme-Tool (91). We
also extracted 552 Product-Producer instances from
eXtended WordNet4 (noun entries and their gloss
definition). Moreover, for Theme-Tool and Content-
Container we used special lists of constraints5. Be-
sides the selectional restrictions imposed on the
nouns by special features such as F15 and F16 (psy-
chological feature), we created lists of containers
from various thesauri6 and identified selectional re-
strictions that differentiate between containers and
locations relying on taxonomies of spatial entities
discussed in detail in (Herskovits, 1987) and (Tyler
and Evans, 2003).
Each instance in this text collection had the tar-
get nouns identified and annotated with WordNet
senses. Since the annotations used different Word-
Net versions, senses were mapped to sense keys.
4 Experimental Results
Table 2 shows the performance of our system for
each semantic relation. Base-F indicates the base-
line F-measure (all true), while Base-Acc shows the
baseline accuracy score (majority). The Average
score of precision, recall, F-measure, and accuracy
is macroaveraged over all seven relations. Overall,
all features contributed to the performance, with a
different contribution per relation (cf. Table 2).
5 Conclusions
This paper describes a method for the automatic
identification of a set of seven semantic relations
4http://xwn.hlt.utdallas.edu/
5The Instrument-Agency classifier was trained only on the
task dataset.
6Thesauri such as TheFreeDictionary.com.
based on support vector machines (SVMs). The ap-
proach benefits from an extended dataset on which
binary classifiers were trained for each relation. The
feature sets fed into the SVMs produced very good
results.
Acknowledgments
We would like to thank Brian Drexler for his valu-
able suggestions on the set of semantic relations.
References
E. Charniak. 2000. A Maximum-entropy-inspired Parser. In
the Proceedings of the 1st NAACL Conference.
R. Girju, A. Giuglea, M. Olteanu, O. Fortu, O. Bolohan, and
D. Moldovan. 2004. Support vector machines applied to
the classification of semantic relations in nominalized noun
phrases. In the Proceedings of the HLT/NAACL Workshop
on Computational Lexical Semantics.
R. Girju, D. Moldovan, M. Tatu, and D. Antohe. 2005. On
the semantics of noun compounds. Computer Speech and
Language, 19(4):479?496.
R. Girju, A. Badulescu, and D. Moldovan. 2006. Automatic
discovery of part-whole relations. Computational Linguis-
tics, 32(1).
R. Girju. 2003. Automatic detection of causal relations for
question answering. In the Proceedings of the ACL Work-
shop on ?Multilingual Summarization and Question Answer-
ing - Machine Learning and Beyond?.
A. Herskovits. 1987. Language and spatial cognition: An in-
terdisciplinary study of the prepositions in English. Cam-
bridge University Press.
S. Linstromberg. 1997. English Prepositions Explained. John
Benjamins Publishing Co., Amsterdam/Philaderphia.
D. Moldovan, A. Badulescu, M. Tatu, D. Antohe, and R. Girju.
2004. Models for the semantic classification of noun
phrases. In the Proceedings of the HLT/NAACL Workshop
on Computational Lexical Semantics.
A. Tyler and V. Evans. 2003. The Semantics of English Prepo-
sitions: Spatial Sciences, Embodied Meaning, and Cogni-
tion. Cambridge University Press.
389
Proceedings of the NAACL HLT Workshop on Unsupervised and Minimally Supervised Learning of Lexical Semantics, pages 54?62,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Using DEDICOM for  
Completely Unsupervised Part-of-Speech Tagging 
Peter A. Chew, Brett W. Bader 
Sandia National Laboratories 
P. O. Box 5800, MS 1012 
Albuquerque, NM 87185-1012, USA 
{pchew,bwbader}@sandia.gov 
Alla Rozovskaya 
Department of Computer Science 
University of Illinois 
Urbana, IL 61801, USA 
rozovska@illinois.edu 
 
Abstract 
A standard and widespread approach to 
part-of-speech tagging is based on Hidden 
Markov Models (HMMs). An alternative 
approach, pioneered by Sch?tze (1993), 
induces parts of speech from scratch using 
singular value decomposition (SVD). We 
introduce DEDICOM as an alternative to 
SVD for part-of-speech induction. 
DEDICOM retains the advantages of 
SVD in that it is completely unsupervised: 
no prior knowledge is required to induce 
either the tagset or the associations of 
types with tags. However, unlike SVD, it 
is also fully compatible with the HMM 
framework, in that it can be used to esti-
mate emission- and transition-probability 
matrices which can then be used as the 
input for an HMM. We apply the 
DEDICOM method to the CONLL corpus 
(CONLL 2000) and compare the output of 
DEDICOM to the part-of-speech tags 
given in the corpus, and find that the cor-
relation (almost 0.5) is quite high. Using 
DEDICOM, we also estimate part-of-
speech ambiguity for each type, and find 
that these estimates correlate highly with 
part-of-speech ambiguity as measured in 
the original corpus (around 0.88). Finally, 
we show how the output of DEDICOM 
can be evaluated and compared against 
the more familiar output of supervised 
HMM-based tagging. 
1 Introduction 
Traditionally, part-of-speech tagging has been ap-
proached either in a rule-based fashion, or stochas-
tically. Harris (1962) was among the first to 
develop algorithms of the former type. The rule-
based approach relies on two elements: a dictio-
nary to assign possible parts of speech to each 
word, and a list of hand-written rules ? which must 
be painstakingly developed for each new language 
or domain ? to disambiguate tokens in context. 
Stochastic taggers, on the other hand, avoid the 
need for hand-written rules by tabulating probabili-
ties of types and part-of-speech tags (which must 
be gathered from a tagged training corpus), and 
applying a special case of Bayesian inference 
(usually, Hidden Markov Models [HMMs]) to dis-
ambiguate tokens in context. The latter approach 
was pioneered by Stolz et al (1965) and Bahl and 
Mercer (1976), and became widely known through 
the work of e.g. Church (1988) and DeRose 
(1988). 
A third and more recent approach, known as 
?distributional tagging? and exemplified by 
Sch?tze (1993, 1995) and Biemann (2006), aims to 
eliminate the need for both hand-written rules and 
a tagged training corpus, since the latter may not 
be available for every language or domain. Distri-
butional tagging is fully-unsupervised, unlike the 
two traditional approaches described above. 
Sch?tze suggests analyzing the distributional pat-
terns of words by forming a term adjacency matrix, 
then subjecting that matrix to Singular Value De-
composition (SVD) to reveal latent dimensions. He 
shows that in the reduced-dimensional space im-
plied by SVD, tokens do indeed cluster intuitively 
by part-of-speech; and that if context is taken into 
account, something akin to part-of-speech tagging 
54
can be achieved. Whereas the performance of sto-
chastic taggers is generally sub-optimal when the 
domain of the training data differs from that of the 
test data, distributional tagging sidesteps this prob-
lem, since each corpus can be considered in its 
own right. Sch?tze (1995) notes two general draw-
backs of distributional tagging methods: the per-
formance is relatively modest compared to that of 
supervised methods; and languages with rich mor-
phology may pose a challenge.1
In this paper, we present an alternative unsuper-
vised approach to distributional tagging. Instead of 
SVD, we use a dimensionality reduction technique 
known as DEDICOM, which has various advan-
tages over the SVD-based approach. Principal 
among these is that, even though no pre-tagged 
corpus is required, DEDICOM can easily be used 
as input to a HMM-based approach (and the two 
share linear-algebraic similarities, as we will make 
clear in section 4). Although our empirical results, 
like those of Sch?tze (1995), are perhaps still rela-
tively modest, the fact that a clearer connection 
exists between DEDICOM and HMMs than be-
tween SVD and HMMs gives us good reason to 
believe that with further refinements, DEDICOM 
may be able to give us ?the best of both worlds? in 
many respects: the benefits of avoiding the need 
for a pre-tagged corpus, with empirical results ap-
proaching those of HMM-based tagging. 
In the following sections, we introduce 
DEDICOM, describe its applicability to the part-
of-speech tagging problem, and outline its connec-
tions to the standard HMM-based approach to tag-
ging. We evaluate the use of DEDICOM on the 
CONLL 2000 shared task data, discuss the results 
and suggest avenues for improvement. 
2 DEDICOM 
DEDICOM, which stands for ?DEcomposition into 
DIrectional COMponents?, is a linear-algebraic 
decomposition method attributable to Harshman 
(1978) which has been used to analyze matrices of 
 
1 We note the latter is also true for languages in which word 
order is relatively free ? usually the same languages as those 
with rich morphology. While English word order is signifi-
cantly constrained by part-of-speech categorizations, this is 
not as true of, say, Russian. Thus, an adjacency matrix formed 
from a Russian corpus is likely to be less informative about 
part-of-speech classifications as one formed from an English 
corpus. Quite possibly, this is as much of a limitation for 
DEDICOM as it is for SVD. 
asymmetrical directional relationships between 
objects or persons. Early on, the technique was 
applied by Harshman et al (1982) to the analysis 
of two types of marketing data: ?free associations? 
? how often one phrase (describing hair shampoo) 
evokes another in the minds of survey respondents, 
and ?car switching data? ? how often people switch 
from one to another of 16 car types. Both datasets 
are asymmetric and directional: in the first dataset, 
for example, the phrase ?body? (referring to sham-
poo) evoked the phrase ?fullness? twice as often in 
the minds of respondents as ?fullness? evoked 
?body?. Likewise, the data from Harshman et al 
(1982) show that in the given period, 3,820 people 
switched from ?midsize import? cars to ?midsize 
domestic? cars, but only 2,140 switches were made 
in the reverse direction. Another characteristic of 
these ?asymmetric directional? datasets is that they 
can be represented in square matrices. For exam-
ple, the raw car switching data can be represented 
in a 16 ? 16 matrix, since there are 16 car types. 
The objective of DEDICOM, which can be 
compared to that of SVD, is to factorize the raw 
data matrices into a lower-dimensional space iden-
tifying underlying, idealized directional patterns in 
the data. For example, while there are 16 car types 
in the raw car switching data, Harshman shows 
that under a 4-dimensional DEDICOM analysis, 
these can be ?boiled down? to the basic types ?plain 
large-midsize?, ?specialty?, ?fancy large?, and 
?small? ? and that patterns of switching among 
these more basic types can then be identified. 
If X represents the original n ? n matrix of 
asymmetric relationships, and a general entry xij in 
X represents the strength of the directed relation-
ship of object i to object j, then the single-domain 
DEDICOM model2 can be written as follows: 
 
X = ARAT + E (1) 
 
where A denotes an n ? q matrix of weights of the 
n observed objects in q dimensions (where q < n), 
and R is a dense q ? q asymmetric matrix express-
ing the directional relationships between the q di-
mensions or basic types. AT is simply the transpose 
 
2 There is a dual-domain DEDICOM model, which is also 
described in Harshman (1978). The dual-domain DEDICOM 
model is not relevant to our discussion, and thus it will not be 
mentioned further. References in this paper to ?DEDICOM? 
are to be understood as references in shorthand to ?single-
domain DEDICOM?. 
55
of A, and E is a matrix of error terms. Our objec-
tive is to minimize E, so we can also write: 
 
X  ARAT (2) 
 
As noted by Harshman (1978: 209), the fact that 
A appears on both the left and right of R means 
that the data is described ?in terms of asymmetric 
relations among a single set of things? ? in other 
words, when objects are on the receiving end of the 
directional relationships, they are still of the same 
type as those on the initiating end. 
One difference between DEDICOM and SVD is 
that there is no unique solution: either A or R can 
be scaled or rotated without changing the goodness 
of fit, so long as the inverse operation is applied to 
the other. For example, if we let ? = AD, where D 
is any diagonal scaling matrix (or, more generally, 
any nonsingular matrix), then we can write 
 
X  ARAT = ?D-1RD-1?T (3) 
since ?T = (AD) T = DAT
(In our application, we constrain A and R to be 
nonnegative as noted below.) 
To our knowledge, there have been no applica-
tions of DEDICOM to date in computational lin-
guistics. This is in contrast to SVD, which has 
been extensively used for text analysis (for appli-
cations other than unsupervised part-of-speech 
tagging, see Baeza-Yates and Ribeiro-Neto 1999). 
3 Applicability of DEDICOM to part-of-
speech tagging 
Sch?tze?s (1993) key insight is that ? at least in 
English ? adjacencies between types are a good 
guide to their grammatical functions. That insight 
can be leveraged by applying either SVD or 
DEDICOM to a type-by-type adjacency matrix. 
With DEDICOM, however, we add the constraint 
(already stated) that the types are a ?single set of 
things?: whether a type ?precedes? or ?follows? ? 
i.e., whether it is in a row or a column of the ma-
trix ? does not affect its grammatical function. This 
constraint is as it should be, and, to our knowledge, 
sets DEDICOM apart from all previous unsuper-
vised approaches including those of Sch?tze (1993, 
1995) and Biemann (2006). 
Given any corpus containing n types and k to-
kens, we can let X be an n ? n token-adjacency 
matrix. Let each entry xij in X denote the number 
of times in the corpus that type i immediately pre-
cedes type j. X is thus a matrix of bigram frequen-
cies. It follows that the sum of the elements of X 
equals k ? 1 (because the first token in the corpus 
is preceded by nothing, and the last token is fol-
lowed by nothing). Any given row sum of X (the 
type frequency corresponding to the particular 
row) will equal the corresponding column sum, 
except if the type happens to occur in the first or 
last position in the corpus. X will be asymmetric, 
since the frequency of bigram ij is clearly not the 
same as that of bigram ji for all i and j.
It can be seen, therefore, that our X represents 
asymmetric directional data, very similar to the 
data analyzed in Harshman (1978) and Harshman 
et al (1982). If we fit the DEDICOM model to our 
X matrix, then we obtain an A matrix which 
represents types by latent classes, and an R matrix 
which represents directional relationships between 
latent classes. We can think of the latent classes as 
induced parts of speech. 
With SVD, we believe that the orthogonality of 
the reduced-dimensional features militates against 
any attempt to correlate these features with parts of 
speech. From a linguistic point of view, there is no 
reason to believe that parts of speech are orthogon-
al to one another in any sense. For example, nouns 
and adjectives (traditionally classified together as 
?nominals?) seem to share more in common with 
one another than nouns and verbs. With 
DEDICOM, this is not an issue, because the col-
umns of A are not required to be mutually ortho-
gonal to one another, unlike the left and right 
singular vectors from SVD. 
Thus, the A matrix from DEDICOM shows how 
strongly associated each type is with the different 
induced parts of speech; we would expect types 
which are ambiguous (such as ?claims?, which can 
be either a noun or a verb) to have high loadings 
on more than one column in A. Again, if the 
classes correlate with parts of speech, the R matrix 
will show the latent patterns of adjacency between 
different parts of speech. 
4 Connections between DEDICOM and 
HMM-based tagging 
For any HMM, two components are necessary: a 
set of emission probabilities and a set of transition 
probabilities. Applying this framework to part-of-
56
speech tagging, the tags are conceived of as the 
hidden layer of the HMM and the tokens (each of 
which is associated with a type) as the visible 
layer. The emission probabilities are then the prob-
abilities of types given the tags, and the transition 
probabilities are the probabilities of the tags given 
the preceding tags. If these probabilities are 
known, then there are algorithms (such as the Vi-
terbi algorithm) to determine the most likely se-
quence of tags given the visible sequence of types. 
In the case of supervised learning, we obtain the 
emission and transition probabilities by observing 
actual frequencies in a tagged corpus. Suppose our 
corpus, as previously discussed, consists of n types 
and k tokens. Since we are dealing with supervised 
learning, the number of the tags in the tagset is also 
known: we denote this number q. Now, the ob-
served frequencies can be represented, respective-
ly, as n ? q and q ? q matrices: we denote these A* 
and R*. Each entry aij in A* denotes the number of 
times type i is associated with tag j, and each entry 
rij in R* denotes the number of times tag j imme-
diately follows tag i. Moreover, we know some 
other properties of A* and R*: 
 
 the respective sums of the elements of A* and 
R* are equal to k ? 1; 
 each row sum of A* (
=
q
x
ixa
1
) corresponds to 
the frequency in the corpus of type i;
 each column sum of A*, as well as the corres-
ponding row and column sums of R*, are the 
frequencies of the given tags in the corpus (for 
all j, 
===
==
q
x
jx
q
x
xj
q
x
xj rra
111
). 
 
If A* and R* contain frequencies, however, we 
must perform a matrix operation to obtain transi-
tion and emission probabilities for use in an 
HMM-based tagger. In effect, A* must be made 
column-stochastic, and R* must be made row-
stochastic. Since the column sums of A* equal the 
respective row sums of R*, this can be achieved by 
post-multiplying both A* and R* by DA, where DA
is a diagonal scaling matrix containing the inverses 
of the column sums of A (or equivalently, the row 
sums of R). Then the matrix of emission probabili-
ties is given by A*DA, and the matrix of transition 
probabilities by R*DA.
We can now make the connection to DEDICOM 
explicit. Let A = A*DA and R = R*, then we can 
rewrite (2) as follows: 
 
X  ARAT = (A*DA) R* (A*DA)T (4) 
X  A*DA R*DA A*T (5) 
 
In other words, for any corpus we may compute 
a probabilistic representation of the type adjacency 
matrix X (which will contain expected frequencies 
comparable to the actual frequencies) by multiply-
ing the emission probability matrix A*DA, the 
transition probability matrix R*DA, and the type-
by-tag frequency matrix A*. (Presumably, the 
closer the approximation, the better the tagging in 
the training set actually factorizes the true direc-
tional relationships.) 
Conversely, for fully unsupervised tagging, we 
can fit the DEDICOM model to the type adjacency 
matrix X. The resulting A matrix contains esti-
mates of what the tags should be (if a tagged train-
ing corpus is unavailable), as well as the emission 
probability of each type given each tag, and the 
resulting R matrix is the corresponding transition 
probability matrix given those tags. In this case, a 
column-stochastic A can be used directly as the 
emission probability matrix, and we simply make 
R* row-stochastic to obtain the matrix of transition 
probabilities. The only difference then between the 
output of the fully-unsupervised DEDICOM/HMM 
tagger and that of a supervised HMM tagger is that 
in the first case, the ?tags? are numeric indices 
representing the corresponding column of A, and 
in the second case, they are the members of the 
tagset used in the training data. 
The fact that emission and transition probabili-
ties (or at least something very like them) are a 
natural by-product of DEDICOM sets DEDICOM 
apart from Sch?tze?s SVD-based approach, and is 
for us a significant reason which recommends the 
use of DEDICOM. 
5 Evaluation 
For all evaluation described here, we used the 
CONLL 2000 shared task data (CONLL 2000). 
This English-language newswire corpus consists of 
19,440 types and 259,104 tokens (including punc-
tuation marks as separate types/tokens). Each to-
ken is associated with a part-of-speech tag and a 
chunk tag, although we did not use the chunk tags 
57
in the work described here. The tags are from a 44-
item tagset. The CONLL 2000 tags against which 
we measure our own results are in fact assigned by 
the Brill tagger (Brill 1992), and while these may 
not correlate perfectly with those that would have 
been assigned by a human linguist, we believe that 
the correlation is likely to be good enough to allow 
for an informative evaluation of our method. 
Before discussing the evaluation of unsuper-
vised DEDICOM, let us briefly reconsider the si-
milarities of DEDICOM to the supervised HMM 
model in the light of actual data in the CONLL 
corpus. We stated in (5) that X  A*DAR*DAA*T.
For the CONLL 2000 tagged data, A* is a 19,440 
? 44 matrix and R* is a 44 ? 44 matrix. Using 
A*DA and R*DA as emission- and transition-
probability matrices within a standard HMM 
(where the entire CONLL 2000 corpus is treated as 
both training and test data), we obtained a tagging 
accuracy of 95.6%. By multiplying 
A*DAR*DAA*T, we expect to obtain a matrix ap-
proximating X, the table of bigram frequencies. 
This is indeed what we found: it will be apparent 
from Table 1 that the top 10 expected bigram fre-
quencies based on this matrix multiplication are 
generally quite close to actual frequencies. Moreo-
ver, the sum of the elements in A*DAR*DAA*T is 
equal to the sum of the elements in X, and if we let 
E be the matrix of error terms (X - 
A*DAR*DAA*T), then we find that ||E|| (the Frobe-
nius norm of E) is 38.764% of ||X|| - in other 
words, A*DAR*DAA*T accounts for just over 60% 
of the data in X. 
 
Type 1 Type 2 Actual 
frequency 
Expected 
frequency 
of the 1,421.000 1,202.606 
in the 1,213.000 875.822 
for the 553.000 457.067 
to the 445.000 415.524 
on the 439.000 271.528 
the company 383.000 105.794 
a share 371.000 32.447 
that the 315.000 258.679 
and the 302.000 296.737 
to be 285.000 499.315 
Table 1. Actual versus expected frequencies for 10 most 
common bigrams in CONLL 2000 corpus 
 
Having confirmed that there exists an A 
(=A*DA) and R (=R*) which both satisfies the 
DEDICOM model and can be used directly within 
a HMM-based tagger to achieve satisfactory re-
sults, we now consider whether A and R can be 
estimated if no tagged training set is available. 
We start, therefore, from X, the square 19,440 ?
19,440 (sparse) matrix of raw bigram frequencies 
from the CONLL 2000 data. Using Matlab and the 
Tensor Toolbox (Bader and Kolda 2006, 2007), we 
computed the best rank-44 non-negative 
DEDICOM3 decomposition of this matrix using 
the 2-way version of the ASALSAN algorithm 
presented in Bader et al (2007), which is based on 
iteratively improving random initial guesses for A 
and R. As with SVD, the rank of the decomposi-
tion can be selected by the user; we chose 44 since 
that was known to be the number of items in the 
CONLL 2000 tagset, but a lower number could be 
selected for a coarser-grained part-of-speech anal-
ysis. Ultimately, perhaps the best way to determine 
the optimal rank would be to evaluate different 
options within a larger end-to-end system, for ex-
ample an information retrieval system; this, how-
ever, was beyond our scope in this study. 
As already mentioned, there are indeterminacies 
of rotation and scale in DEDICOM. As Harshman 
et al (1982: 211) point out, ?when the columns of 
A are standardized? the R matrix can then be in-
terpreted as expressing relationships among the 
dimensions in the same units as the original data. 
That is, the R matrix can be interpreted as a ma-
trix of the same kind as the original data matrix X, 
but describing the relations among the latent as-
pects of the phrases, rather than the phrases them-
selves?. Thus, if DEDICOM is constrained so that 
A is column-stochastic (which is required in any 
case of the matrix of emission probabilities), then 
the sum of the elements in R should approximate 
the sum of the elements in X. R is therefore com-
parable to R* (with some provisos which shall be 
enumerated below), and to obtain the row-
stochastic transition-probability matrix, we simply 
multiply R by a diagonal matrix DR whose ele-
ments are the inverses of R?s row sums. 
 
3 Non-negative DEDICOM imposes the constraint not present 
in Harshman (1978, 1982) that all entries in A and R must be 
non-negative. This constraint is appropriate in the present 
case, since the entries in A* and R* (and of course the proba-
bilities in A*D and R*D) are by definition non-negative. 
58
Table 2. Partial confusion matrix of gold-standard tags against DEDICOM-induced tags for CONLL 2000 dataset 
With A as an emission-probability matrix and 
RDR as a transition-probability matrix, we now 
have all that is needed for an HMM-based tagger 
to estimate the most likely sequence of ?tags? given 
the corpus. However, since the ?tags? here are nu-
merical indices, as mentioned, to evaluate the out-
put we must look at the correlation between these 
?tags? and the gold-standard tags given in the 
CONLL 2000 data. One way this can be done is by 
presenting a 44 ? 44 confusion matrix (of gold-
standard tags against induced tags), and then mea-
suring the correlation coefficient (Pearson?s R) 
between that matrix and the ?idealized? confusion 
matrix in which each induced tag corresponds to 
one and only one ?gold standard? tag. Using A and 
RDR as the input to a HMM-based tagger, we 
tagged the CONLL 2000 dataset with induced tags 
and obtained the confusion matrix shown in Table 
2 (owing to space constraints, only the first 20 col-
umns are shown). The correlation between this 
matrix and the equivalent diagonalized ?ideal? ma-
trix is in fact 0.4942, which is significantly higher 
than could have occurred by chance. 
It should be noted that a lack of correlation be-
tween the induced tags and the gold standard tags 
can be attributed to at least two independent fac-
tors. The first, of course, is any inability of the 
DEDICOM model to fit the particular problem and 
data. Clearly, this is undesirable. The other factor 
to be borne in mind, which works to DEDICOM?s 
favor, is that the DEDICOM model could yield an 
A and R which factorize the data more optimally 
than the A*D and R* implied by the gold-standard 
tags. There are three methods we can use to try and 
tease apart these competing explanations of the 
results, two quantitative and the other subjective. 
Quantitatively, we can compare the respective er-
ror matrices E. We have already mentioned that 
38764.0||X||
||ADRDAX|| T*A*A*  (6) 
Similarly, using the A and R from DEDICOM we 
can compute 
24078.0||X||
||ARAX|| T  (7) 
59
The fact that the error is lower in the second case 
implies that DEDICOM allows us to find a part-of-
speech ?factorization? of the data which fits better 
even than the gold standard, although again there 
are some caveats to this; we will return to these in 
the discussion. 
Another way to evaluate the output of 
DEDICOM is by comparing the number of part-of-
speech tags for a type in the gold standard to the 
number of classes in the A matrix with which the 
type is strongly associated. We test this by measur-
ing the Pearson correlation between the two va-
riables. First, we compute the average number of 
part-of-speech tags per type using the gold stan-
dard. We refer to this value as ambiguity coeffi-
cient; for the CONLL dataset, this is 1.05. Because 
A is dense, if we count all non-zero columns for a 
type in the A matrix as possible classes, we obtain 
a much higher ambiguity coefficient. We therefore 
set a threshold and consider only those columns 
whose values exceed a certain threshold. The thre-
shold is selected so that the ambiguity coefficient 
of the A matrix is the same as that of the gold stan-
dard. For a given type, every column with a value 
exceeding the threshold is counted as a possible 
class for that type. We then compute the Pearson 
correlation coefficient between the number of 
classes for a type in the A matrix and the number 
of part-of speech tags for that type in the CONLL 
dataset as provided by the Brill tagger. We ob-
tained a correlation coefficient of 0.88, which 
shows that there is indeed a high correlation be-
tween the induced tags and the gold standard tags 
obtained with DEDICOM. 
Finally, we can evaluate the output subjectively 
by looking at the content of the A matrix. For each 
?tag? (column) in A, the ?types? (rows) can be 
listed in decreasing order of their weighting in A. 
This gives us an idea of which types are most cha-
racteristic of which tags, and whether the grouping 
into tags makes any intuitive sense. These results 
(for selected tags only, owing to limitations of 
space) are given in Table 3. 
Many groupings in Table 3 do make sense: for 
example, the fourth tag is clearly associated with 
verbs, while the two types with significant weight-
ings for tag 2 are both determiners. By referring 
back to Table 2, we can see that many tokens in the 
CONLL 2000 dataset tagged as verbs are indeed 
tagged by the DEDICOM tagger as ?tag 4?, while 
many determiners are tagged as ?tag 3?. To under-
stand where a lack of correlation may arise, how-
ever, it is informative to look at apparent 
anomalies in the A matrix. For example, it can be 
seen from Table 3 that ?new?, an adjective, is 
grouped in the third tag with ?a? and ?the? (and 
ranking above ?an?). Although not in agreement 
with the CONLL 2000 ?gold standard? tagging, the 
idea that determiners are a type of adjective is in 
fact in accordance with traditional English gram-
mar. Here, the grouping of ?new?, ?a? and ?the? can 
be explained by the distributional similarities (all 
precede nouns). It should also be emphasized that 
the A matrix is essentially a ?soft clustering? of 
types (meaning that types can belong to more than 
one cluster). Thus, for example, ?u.s.? (the abbrevi-
ation for United States) appears under both tag 2 
(which appears to have high loadings for nouns) 
and tag 8 (with high loadings for adjectives). 
We have alluded above in passing to possible 
methods for improving the results of the 
DEDICOM analysis. One would be to pre-process 
the data differently. Here, a variety of options are 
available which maintain a generally unsupervised 
approach (one example is to avoid treating punctu-
ation as tokens). However, variations in pre-
processing are beyond the scope of this paper. 
Tag Top 10 types (by weight) with weightings 
1 million share said . year billion inc. corp. years quarter 
0.0246 0.0146 0.0129 0.0098 0.0088 0.0069 0.0064 0.0061 0.0058 0.0054 
2 company u.s. new first market share year stock . government 
0.0264 0.0136 0.0113 0.0095 0.0086 0.0086 0.0079 0.0077 0.0065 0.006 
3 the a new an other its any addition their 1988 
0.2889 0.1194 0.0121 0.0094 0.0092 0.0085 0.0067 0.0062 0.0062 0.0057 
?
8 the its his about those their all u.s. . this 
0.0935 0.0462 0.0208 0.0160 0.0096 0.0095 0.0088 0.0077 0.0074 0.0071 
?
Table 3. Type weightings in A matrix, by tag 
60
Another method would be to constrain 
DEDICOM so that the output more closely models 
the characteristics of A* and R*, the emission- and 
transition-probability matrices obtained from a 
tagged training set. In particular, there is one im-
portant constraint on R* which is not replicated in 
R: the constraint mentioned above that for all j,

==
=
q
x
jx
q
x
xj rr
11
. We note that this constraint can be 
satisfied by Sinkhorn balancing (Sinkhorn 1964)4,
although it remains to be seen how the constraint 
on R can best be incorporated into the DEDICOM 
architecture. Assuming that A is column-
stochastic, another desirable constraint is that the 
rows of A(DR)-1 should sum to the same as the 
rows of X (the respective type frequencies). With 
the implementation of these (and any other) con-
straints, one would expect the fit of DEDICOM to 
the data to worsen (cf. (6) and (7) above), but in-
curring this cost could be worthwhile if the payoff 
were somehow linguistically interesting (for ex-
ample, if it turned out we could achieve a much 
higher correlation to gold-standard tagging). 
6 Conclusion 
In this paper, we have introduced DEDICOM, an 
analytical technique which to our knowledge has 
not previously been used in computational linguis-
tics, and applied it to the problem of completely 
unsupervised part-of-speech tagging. Theoretical-
ly, the model has features which recommend it 
over other previous approaches to unsupervised 
tagging, specifically SVD. Principal among the 
advantages is the compatibility of DEDICOM with 
the standard HMM-based approach to part-of-
speech tagging, but another significant advantage 
is the fact that types are treated as ?a single set of 
objects? regardless of whether they occupy the first 
or second position in a bigram. 
By applying DEDICOM to a tagged dataset, we 
have shown that there is a significant correlation 
between the tags induced by unsupervised, 
DEDICOM-based tagging, and the pre-existing 
gold-standard tags. This points both to an inherent 
validity in the gold-standard tags (as a reasonable 
 
4 It is also worth noting that Sinkhorn was motivated by the 
same problem which concerns us, that of estimating a transi-
tion-probability matrix for a Markov model. 
factorization of the data) and to the fact that 
DEDICOM appears promising as a method of in-
ducing tags in cases where no gold standard is 
available. 
We have also shown that the factors of 
DEDICOM are interesting in their own right: our 
tests show that the A matrix (similar to an emis-
sion-probability matrix) models type part-of-
speech ambiguity well. Using insights from 
DEDICOM, we have also shown how linear alge-
braic techniques may be used to estimate the fit of 
a given part-of-speech factorization (whether in-
duced or manually created) to a given dataset, by 
comparing actual versus expected bigram frequen-
cies. 
In summary, it appears that DEDICOM is a 
promising way forward for bridging the gap be-
tween unsupervised and supervised approaches to 
part-of-speech tagging, and we are optimistic that 
with further refinements to DEDICOM (such as 
the addition of appropriate constraints), more in-
sight will be gained on how DEDICOM may most 
profitably be used to improve part-of-speech tag-
ging when few pre-existing resources (such as 
tagged corpora) are available. 
Acknowledgements 
We are grateful to Danny Dunlavy for contributing 
his thoughts to this work. 
Sandia is a multiprogram laboratory operated by 
Sandia Corporation, a Lockheed Martin Company, 
for the United States Department of Energy?s Na-
tional Nuclear Security Administration under con-
tract DE-AC04-94AL85000. 
61
References  
Brett W. Bader, Richard A. Harshman, and Tamara G. 
Kolda. 2007. Temporal analysis of semantic graphs 
using ASALSAN. In Proceedings of the 7th IEEE In-
ternational Conference on Data Mining, 33-42. 
Brett W. Bader and Tamara G. Kolda. 2006.  Efficient 
MATLAB Computations with Sparse and Factored 
Tensors.  Technical Report SAND2006-7592, Sandia 
National Laboratories, Albuquerque, NM and Liver-
more, CA. 
Brett W. Bader and Tamara G. Kolda. 2007.  The 
MATLAB Tensor Toolbox, version 2.2.  
http://csmr.ca.sandia.gov/~tgkolda/TensorToolbox/.  
Ricardo Baeza-Yates and Berthier Ribeiro-Neto. 1999. 
Modern Information Retrieval. New York: ACM 
Press. 
L. R. Bahl and R. L. Mercer. 1976. Part of speech as-
signment by a statistical decision algorithm. In Pro-
ceedings of the IEEE International Symposium on 
Information Theory, 88-89. 
C. Biemann. 2006. Unsupervised part-of-speech tagging 
employing efficient graph clustering. In Proceedings 
of the COLING/ACL 2006 Student Research Work-
shop, 7-12. 
E. Brill. 1992. A simple rule-based part of speech tag-
ger. In Proceedings of the Third Conference on Ap-
plied Natural Language Processing, 152-155. 
K. W. Church. 1988. A stochastic parts program and 
noun phrase parser for unrestricted text. In ANLP 
1988, 136-143. 
CONLL 2000. Shared task data. Retrieved Dec. 1, 2008 
from http://www.cnts.ua.ac.be/conll2000/chunking/. 
S. J. DeRose. 1988. Grammatical category disambigua-
tion by statistical optimization. Computational Lin-
guistics 14, 31-39. 
Harris, Z. S. 1962. String Analysis of Sentence Struc-
ture. Mouton: The Hague. 
Richard Harshman. 1978. Models for Analysis of 
Asymmetrical Relationships Among N Objects or 
Stimuli. Paper presented at the First Joint Meeting of 
the Psychometric Society and The Society for Ma-
thematical Psychology. Hamilton, Canada. 
Richard Harshman, Paul Green, Yoram Wind, and Mar-
garet Lundy. 1982. A Model for the Analysis of 
Asymmetric Data in Marketing Research. Marketing 
Science 1(2), 205-242. 
Hinrich Sch?tze. 1993. Part-of-Speech Induction from 
Scratch. In Proceedings of the 31st Annual Meeting of 
the Association for Computational Linguistics, 251-
258. 
Hinrich Sch?tze. 1995. Distributional Part-of-Speech 
Tagging. In Proceedings of the 7th Conference of the 
European Chapter of the Association for Computa-
tional Linguistics, 141-148. 
Richard Sinkhorn. 1964. A Relationship Between Arbi-
trary Positive Matrices and Doubly Stochastic Ma-
trices. The Annals of Mathematical Statistics 35(2), 
876-879. 
W. S. Stolz, P. H. Tannenbaum, and F. V. Carstensen. 
1965. A stochastic approach to the grammatical cod-
ing of English. Communications of the ACM 8(6), 
399-405. 
62
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 961?970,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Generating Confusion Sets for Context-Sensitive Error Correction
Alla Rozovskaya and Dan Roth
University of Illinois at Urbana-Champaign
Urbana, IL 61801
{rozovska,danr}@illinois.edu
Abstract
In this paper, we consider the problem of gen-
erating candidate corrections for the task of
correcting errors in text. We focus on the
task of correcting errors in preposition usage
made by non-native English speakers, using
discriminative classifiers. The standard ap-
proach to the problem assumes that the set of
candidate corrections for a preposition con-
sists of all preposition choices participating
in the task. We determine likely preposition
confusions using an annotated corpus of non-
native text and use this knowledge to produce
smaller sets of candidates.
We propose several methods of restricting
candidate sets. These methods exclude candi-
date prepositions that are not observed as valid
corrections in the annotated corpus and take
into account the likelihood of each preposi-
tion confusion in the non-native text. We find
that restricting candidates to those that are ob-
served in the non-native data improves both
the precision and the recall compared to the
approach that views all prepositions as pos-
sible candidates. Furthermore, the approach
that takes into account the likelihood of each
preposition confusion is shown to be the most
effective.
1 Introduction
We address the problem of generating candidate cor-
rections for the task of correcting context-dependent
mistakes in text, mistakes that involve confusing
valid words in a language. A well-studied instance
of this problem ? context-sensitive spelling errors ?
has received a lot of attention in natural language
research (Golding and Roth, 1999; Carlson et al,
2001; Carlson and Fette, 2007; Banko and Brill,
2001). The context-sensitive spelling correction task
addresses the problem of correcting spelling mis-
takes that result in legitimate words, such as confus-
ing their and there or your and you?re. In this task, a
candidate set or a confusion set is defined that spec-
ifies a list of confusable words, e.g., {their, there}
or {cite, site, sight}. Each occurrence of a confus-
able word in text is represented as a vector of fea-
tures derived from a small context window around
the target. A classifier is trained on text assumed
to be error-free, replacing each target word occur-
rence (e.g. their) with a confusion set consisting of
{their, there}, thus generating both positive and neg-
ative examples, respectively, from the same context.
Given a text to correct, for each word in text that be-
longs to the confusion set the classifier predicts the
most likely candidate in the confusion set.
More recently, work in error correction has taken
an interesting turn and focused on correcting errors
made by English as a Second Language (ESL) learn-
ers, with a special interest given to errors in article
and preposition usage. These mistakes are some of
the most common mistakes for non-native English
speakers of all proficiency levels (Dalgish, 1985;
Bitchener et al, 2005; Leacock et al, 2010). Ap-
proaches to correcting these mistakes have adopted
the methods of the context-sensitive spelling cor-
rection task. A system is usually trained on well-
formed native English text (Izumi et al, 2003; Eeg-
Olofsson and Knuttson, 2003; Han et al, 2006; Fe-
lice and Pulman, 2008; Gamon et al, 2008; Tetreault
961
and Chodorow, 2008; Elghaari et al, 2010; Tetreault
et al, 2010), but several works incorporate into
training error-tagged data (Gamon, 2010; Han et
al., 2010) or error statistics (Rozovskaya and Roth,
2010b). The classifier is then applied to non-native
text to predict the correct article/preposition in con-
text. The possible candidate selections include the
set of all articles or all prepositions.
While in the article correction task the candidate
set is small (a, the, no article), systems for correct-
ing preposition errors, even when they consider the
most common prepositions, may include between 9
to 34 preposition classes. For each preposition in
the non-native text, every other candidate in the con-
fusion set is viewed as a potential correction. This
approach, however, does not take into account that
writers do not make mistakes randomly: Not all can-
didates are equally likely given the preposition cho-
sen by the author and errors may depend on the first
language (L1) of the writer. In this paper, we de-
fine L1-dependent candidate sets for the preposition
correction task (Section 4.1). L1-dependent can-
didate sets reflect preposition confusions observed
with the speakers of the first language L1. We pro-
pose methods of enforcing L1-dependent candidate
sets in training and testing.
We consider mistakes involving the top ten En-
glish prepositions. As our baseline system, we train
a multi-class classifier in one-vs-all approach, which
is a standard approach to multi-class classification.
In this approach, a separate binary classifier for each
preposition pi, 1 ? i ? 10, is trained, s.t. all pi
examples are positive examples for the classifier and
all other nine classes act as negative examples. Thus,
for each preposition pi in non-native text there are
ten1 possible prepositions that the classifier can pro-
pose as corrections for pi.
We contrast this baseline method to two methods
that enforce L1-dependent candidate sets in train-
ing. First, we train a separate classifier for each
preposition pi on the prepositions that belong to L1-
dependent candidate set of pi. In this setting, the
negative examples for pi are those that belong to L1-
dependent candidate set of pi.
The second method of enforcing L1-dependent
1This includes the preposition pi itself. If proposed by the
classifier, it would not be flagged as an error.
candidate sets in training is to train on native data
with artificial preposition errors in the spirit of Ro-
zovskaya and Roth (2010b), where the errors mimic
the error rates and error patterns of the non-native
text. This method requires more knowledge, since
it uses a distribution of errors from an error-tagged
corpus.
We also propose a method of enforcing L1-
dependent candidate sets in testing, through the use
of a confidence threshold. We consider two ways of
applying a threshold: (1) the standard way, when a
correction is proposed only if the classifier?s con-
fidence is sufficiently high and (2) L1-dependent
threshold, when a correction is proposed only if it
belongs to L1-dependent candidate set.
We show that the methods of restricting candidate
sets to L1-dependent confusions improve the prepo-
sition correction system. We demonstrate that re-
stricting candidate sets to those prepositions that are
confusable in the data by L1 writers is beneficial,
when compared to a system that assumes an unre-
stricted candidate set by considering as valid correc-
tions all prepositions participating in the task. Fur-
thermore, we find that the most effective method is
the one that uses knowledge about the likelihoods of
preposition confusions in the non-native text intro-
duced through artificial errors in training.
The rest of the paper is organized as follows.
First, we describe related work on error correction.
Section 3 presents the ESL data and statistics on
preposition errors. Section 4 describes the meth-
ods of restricting candidate sets in training and test-
ing. Section 5 describes the experimental setup. We
present and discuss the results in Section 6. The key
findings are summarized in Table 5 and Fig. 1 in
Section 6. We conclude with a brief discussion of
directions for future work.
2 Related Work
Work in text correction has focused primarily on
correcting context-sensitive spelling errors (Golding
and Roth, 1999; Banko and Brill, 2001; Carlson et
al., 2001; Carlson and Fette, 2007) and mistakes
made by ESL learners, especially errors in article
and preposition usage.
Roth (1998) takes a unified approach to resolving
semantic and syntactic ambiguities in natural lan-
962
guage by treating several related problems, includ-
ing word sense disambiguation, word selection, and
context-sensitive spelling correction as instances of
the disambiguation task. Given a candidate set or a
confusion set of confusable words, the task is to se-
lect the most likely candidate in context. Examples
of confusion sets are {sight, site, cite} for context-
sensitive spelling correction, {among, between} for
word selection, or a set of prepositions for the prepo-
sition correction problem.
Each occurrence of a candidate word in text is
represented as a vector of features. A classifier is
trained on a large corpus of error-free text. Given
text to correct, for each word in text that belongs to
the confusion set the classifier is used to predict the
most likely candidate in the confusion set given the
word?s context.
In the same spirit, models for correcting ESL er-
rors are generally trained on well-formed native text.
Han et al (2006) train a maximum entropy model to
correct article mistakes. Chodorow et. al (2007),
Tetreault and Chodorow (2008), and De Felice and
Pulman (2008) train a maximum entropy model and
De Felice and Pulman (2007) train a voted percep-
tron algorithm to correct preposition errors. Gamon
et al (2008) train a decision tree model and a lan-
guage model to correct errors in article and preposi-
tion usage. Bergsma et al (2009) propose a Na??ve
Bayes algorithm with web-scale N-grams as fea-
tures, for preposition selection and context-sensitive
spelling correction.
The set of valid candidate corrections for a target
word includes all words in the confusion set. For the
preposition correction task, the entire set of prepo-
sitions considered for the task is viewed as the set
of possible corrections for each preposition in non-
native text. Given a preposition with its surround-
ing context, the model selects the most likely prepo-
sition from the set of all candidates, where the set
of candidates consists of nine (Felice and Pulman,
2008), 12 (Gamon, 2010), or 34 (Tetreault et al,
2010; Tetreault and Chodorow, 2008) prepositions.
2.1 Using Error-tagged Data in Training
Several recent works explore ways of using anno-
tated non-native text when training error correction
models.
One way to incorporate knowledge about which
confusions are likely with ESL learners into the error
correction system is to train a model on error-tagged
data. Preposition confusions observed in the non-
native text can then be included in training, by us-
ing the preposition chosen by the author (the source
preposition) as a feature. This is not possible with a
system trained on native data, because each source
preposition is always the correct preposition.
Han et al (2010) train a model on partially anno-
tated Korean learner data. The error-tagged model
trained on one million prepositions obtains a slightly
higher recall and a significant improvement in preci-
sion (from 0.484 to 0.817) over a model fives times
larger trained on well-formed text.
Gamon (2010) proposes a hybrid system for
preposition and article correction, by incorporating
the scores of a language model and class probabil-
ities of a maximum entropy model, both trained on
native data, into a meta-classifier that is trained on
a smaller amount of annotated ESL data. The meta-
classifier outperforms by a large margin both of the
native models, but it requires large amounts of ex-
pensive annotated data, especially in order to correct
preposition errors, where the problem complexity is
much larger.
Rozovskaya and Roth (2010b) show that by intro-
ducing into native training data artificial article er-
rors it is possible to improve the performance of the
article correction system, when compared to a clas-
sifier trained on native data. In contrast to Gamon
(2010) and Han et al (2010) that use annotated data
for training, the system is trained on native data, but
the native data are transformed to be more like L1
data through artificial article errors that mimic the
error rates and error patterns of non-native writers.
This method is cheaper, since obtaining error statis-
tics requires much less annotated data than training.
Moreover, the training data size is not restricted by
the amount of the error-tagged data available. Fi-
nally, the source article of the writer can be used in
training as a feature, in the exact same way as with
the models trained on error-tagged data, providing
knowledge about which confusions are likely. Un-
like article errors, preposition errors lend themselves
very well to a study of confusion sets because the set
of prepositions participating in the task is a lot big-
ger than the set of article choices.
963
3 ESL Data
3.1 Preposition Errors in Learner Data
Preposition errors are one of the most common mis-
takes that non-native speakers make. In the Cam-
bridge Learner Corpus2 (CLC), which contains data
by learners of different first language backgrounds
and different proficiency levels, preposition errors
account for about 13.5% of all errors and occur on
average in 10% of all sentences (Leacock et al,
2010). Similar error rates have been reported for
other annotated ESL corpora, e.g. (Izumi et al,
2003; Rozovskaya and Roth, 2010a; Tetreault et al,
2010). Learning correct preposition usage in En-
glish is challenging for learners of all first language
backgrounds (Dalgish, 1985; Bitchener et al, 2005;
Gamon, 2010; Leacock et al, 2010).
3.2 The Annotated Corpus
We use data from an annotated corpus of essays
written by ESL students. The essays were fully cor-
rected and error-tagged by native English speakers.
For each preposition used incorrectly by the author,
the annotator also indicated the correct preposition
choice. Rozovskaya and Roth (2010a) provide a de-
tailed description of the annotation of the data.
The annotated data include sentences by speakers
of five first language backgrounds: Chinese, Czech,
Italian, Russian, and Spanish. The Czech, Italian,
Russian and Spanish data come from the Interna-
tional Corpus of Learner English (ICLE, (Granger
et al, 2002)), which is a collection of essays writ-
ten by advanced learners of English. The Chinese
data is a part of the Chinese Learners of English cor-
pus (CLEC, (Gui and Yang, 2003)) that contains es-
says by students of all levels of proficiency. Table 1
shows preposition statistics based on the annotated
data.
The combined data include 4185 prepositions,
8.4% of which were judged to be incorrect by the
annotators. Table 1 demonstrates that the error rates
in the Chinese speaker data, for which different pro-
ficiency levels are available, are 2 or 3 times higher
than the error rates in other language groups. The
data for other languages come from very advanced
learners and, while there are also proficiency differ-
2http://www.cambridge.org/elt
Source Total Incorrect Error
language preps. preps. rate
Chinese 953 144 15.1%
Czech 627 28 4.5%
Italian 687 43 6.3%
Russian 1210 85 7.0%
Spanish 708 52 7.3%
All 4185 352 8.4%
Table 1: Statistics on prepositions in the ESL data.
Column Incorrect denotes the number of prepositions
judged to be incorrect by the native annotators. Column
Error rate denotes the proportion of prepositions used in-
correctly.
ences among advanced speakers, their error rates are
much lower.
We would also like to point out that we take as
the baseline3 for the task the accuracy of the non-
native data, or the proportion of prepositions used
correctly. Using the error rate numbers shown in
Table 1, the baseline for Chinese speakers is thus
84.9%, and for all the data combined it is 91.6%.
3.3 Preposition Errors and L1
We focus on preposition confusion errors, mistakes
that involve an incorrectly selected preposition4. We
consider ten most frequent prepositions in English:
on, from, for, of, about, to, at, in, with, and by5.
We mentioned in Section 2 that not all preposition
confusions are equally likely to occur and preposi-
tion errors may depend on the first language of the
writer. Han et al (2010) show that preposition er-
rors in the annotated corpus by Korean learners are
not evenly distributed, some confusions occurring
more often than others. We also observe that con-
fusion frequencies differ by L1. This is consistent
with other studies, which show that learners? errors
are influenced by their first language (Lee and Sen-
eff, 2008; Leacock et al, 2010).
3It is argued in Rozovskaya and Roth (2010b) that the most
frequent class baselines are not relevant for error correction
tasks. Instead, the error rate in the data need to be considered,
when determining the baseline.
4We do not address errors of missing or extraneous preposi-
tions.
5It is common to restrict the systems that detect errors in
preposition usage to the top prepositions. In the CLC corpus,
the usage of the ten most frequent prepositions accounts for
82% of all preposition errors (Leacock et al, 2010).
964
4 Methods of Improving Candidate Sets
In this section, we describe methods of restricting
candidate sets according to the first language of the
writer. For the preposition correction task, the stan-
dard approach considers all prepositions participat-
ing in the task as valid corrections for every prepo-
sition in the non-native data.
In Section 3.3, we pointed out that (1) not all
preposition confusions are equally likely to occur
and (2) preposition errors may depend on the first
language of the writer. The methods of restricting
confusion sets proposed in this work use knowledge
about which prepositions are confusable based on
the data by speakers of language L1.
We refer to the preposition originally chosen by
the author in the non-native text as the source prepo-
sition, and label denotes the correct preposition
choice, as chosen by the annotator. Consider, for ex-
ample, the following sentences from the annotated
corpus.
1. We ate by*/with our hands .
2. To tell the truth , time spent in jail often changes prisoners to*/for
the worse.
3. And the problem that immediately appeared was that men were
unable to cope with the new woman image .
In example 1, the annotator replaced by with with;
by is the source preposition and with is the label. In
example 2, to is the source and for is the label. In
example 3, the preposition with is judged as correct.
Thus, with is both the source and the label.
4.1 L1-Dependent Confusion Sets
Let source preposition pi denote a preposition that
appears in the data by speakers of L1. Let Conf-
Set denote the set of all prepositions that the sys-
tem can propose as a correction for source preposi-
tion pi. We define two types of confusion sets Con-
fSet. An unrestricted confusion set AllConfSet in-
cludes all ten prepositions. L1-dependent confusion
set L1ConfSet(pi) is defined as follows:
Definition L1ConfSet(pi) = {pj |? a sentence in
which an L1 writer replaced preposition pj with pi }
For example, in the Spanish speaker data, from
is used incorrectly in place of of and for. Then for
Spanish speakers, L1ConfSet(from)={from, of, for}.
Source L1ConfSet(pi)
prep. pi
on {on, about, of, to, at, in, with, by}
by {with, by, in}
from {of, from, for}
Table 2: L1-dependent confusion sets for three preposi-
tions based on data by Chinese speakers.
Table 2 shows for Chinese speakers three preposi-
tions and their L1-dependent confusion sets.
We now describe methods of enforcing L1-
dependent confusion sets in training and testing.
4.2 Enforcing L1-dependent Confusion Sets in
Training
We propose two methods of enforcing L1-dependent
confusion sets in training. They are contrasted to
the typical method of training a multi-class 10-way
classifier, where each class corresponds to one of the
ten participating prepositions.
First, we describe the typical training setting.
NegAll Training proceeds in a standard way of
training a multi-class classifier (one-vs-all ap-
proach) on all ten prepositions using well-
formed native English data. For each prepo-
sition pi, pi examples are positive and the other
nine prepositions are negative examples.
We now describe two methods of enforcing L1-
dependent confusion sets in training.
NegL1 This method explores the difference be-
tween training with nine types as negative ex-
amples and (fewer than nine) L1-dependent
negative examples.
For every preposition pi, we train a classifier
using only examples that are in L1ConfSet(pi).
In contrast to NegAll, for each source prepo-
sition, the negative examples are not all other
nine types, but only those that belong in
L1ConfSet(pi). For each language L1, we train
ten classifiers, one for each source preposition.
For source preposition pi in test, we consult
the classifier for pi. In this model, the con-
fusion set for source pi is restricted through
training, since for source pi, the possible can-
didate replacements are only those that the
classifier sees in training, and they are all in
L1ConfSet(pi).
965
Training Negative examples
data NegAll NegL1
Clean NegAll-Clean NegL1-Clean
ErrorL1 NegAll-ErrorL1 -
Table 3: Training conditions that result in unrestricted
(All) and L1-dependent training paradigms.
ErrorL1 This method restricts the candidate set to
L1ConfSet(pi) by generating artificial preposi-
tion errors in the spirit of Rozovskaya and Roth
(2010b). The training data are thus no longer
well-formed or clean, but augmented with L1
error statistics. Specifically, each preposition
pi in training is replaced with a different prepo-
sition pj with probability probConf, s.t.
probConf = prob(pi|pj) (1)
Suppose 10% of all source prepositions to in
the Russian speaker data correspond to label
for. Then for is replaced with to with proba-
bility 0.1.
The classifier uses in training the source prepo-
sition as a feature, which cannot be done when
training on well-formed text, as discussed in
Section 2.1. By providing the source prepo-
sition as a feature, we enforce L1-dependent
confusion sets in training, because the system
learns which candidate corrections occur with
source preposition pi. An important distinction
of this approach is that it does not simply pro-
vide L1-dependent confusion sets in training:
Because errors are generated using L1 writers?
error statistics, the likelihood of each candidate
correction is also provided. This approach is
also more knowledge-intensive, as it requires
annotated data to obtain error statistics.
It should be noted that this method is orthogo-
nal to the NegAll and NegL1 methods of train-
ing described above and can be used in con-
junction with each of them, only that it trans-
forms the training data to account in a more
natural way for ESL writing.
We combine the proposed methods NegAll,
NegL1 with the Clean or ErrorL1 methods and cre-
ate three training approaches shown in Table 3.
4.3 Restricting Confusion Sets in Testing
To reduce the number of false alarms, correction
systems generally use a threshold on the confidence
of the classifier, following (Carlson et al, 2001), and
propose a correction only when the confidence of the
classifier is above the threshold. We show in Section
5 that the system trained on data with artificial er-
rors performs competitively even without a thresh-
old. The other systems use a threshold. We consider
two ways of applying a threshold6:
1. ThreshAll A correction for source preposition
pi is proposed only when the confidence of
the classifier exceeds the threshold. For each
preposition in the non-native data, this method
considers all candidates as valid corrections.
2. ThreshL1Conf A correction for source prepo-
sition pi is proposed only when the confi-
dence of the classifier exceeds the empirically
found threshold and the preposition proposed
as a correction for pi is in the confusion set
L1ConfSet(pi).
5 Experimental Setup
In this section, we describe experiments with L1-
dependent confusion sets. Combining the three
training conditions shown in Table 3 with the two
ways of thresholding described in Section 4.3, we
build four systems7:
1. NegAll-Clean-ThreshAll This system assumes
both in training and in testing stages that all
preposition confusions are possible. The sys-
tem is trained as a multi-class 10-way classifier,
where for each preposition pi, all other nine
prepositions are negative examples. In testing,
when applying the threshold, all prepositions
are considered as valid corrections.
2. NegAll-Clean-ThreshL1 This system is
trained exactly as NegAll-Clean-ThreshAll
but in testing only corrections that belong
6Thresholds are found empirically: We divide the evaluation
data into three equal parts and to each part apply the threshold,
which is optimized on the other two parts of the data.
7In testing, it is not possible to consider a confusion set
larger than the one used in training. Therefore, ThreshAll is
only possible with NegAll training condition.
966
to L1ConfSet(pi) are considered as valid
corrections for pi.
3. NegL1-Clean-ThreshL1 For each preposition
pi, a separate classifier is trained on the prepo-
sitions that are in L1ConfSet(pi), where pi ex-
amples are positive and a set of (fewer than
nine) pi-dependent prepositions are negative.
Only corrections that belong to L1ConfSet(pi)
are considered as valid corrections for pi.8 Ten
pi-dependent classifiers for each L1 are trained.
4. NegAll-ErrorL1-NoThresh A system is trained
as a multi-class 10-way classifier with artifi-
cial preposition errors that mimic the errors
rates and confusion patterns of the non-native
text. For each L1, an L1-dependent system is
trained. This system does not use a threshold.
We discuss this in more detail below.
The system NegAll-Clean-ThreshAll is our base-
line system. It assumes both in training and in test-
ing that all preposition confusions are possible.
All of the systems are trained on the same set of
word and part-of-speech features using the same set
of training examples. Features are extracted from a
window of eight words around the preposition and
include words, part-of-speech tags and conjunctions
of words and tags of lengths two, three, and four.
Training data are extracted from English Wikipedia
and the New York Times section of the Gigaword
corpus (Linguistic Data Consortium, 2003).
In each training paradigm, we follow a discrimi-
native approach, using an online learning paradigm
and making use of the Averaged Perceptron Algo-
rithm (Freund and Schapire, 1999) ? we use the
regularized version in Learning Based Java9 (LBJ,
(Rizzolo and Roth, 2007)). While classical Per-
ceptron comes with generalization bound related to
the margin of the data, Averaged Perceptron also
comes with a PAC-like generalization bound (Fre-
und and Schapire, 1999). This linear learning al-
gorithm is known, both theoretically and experi-
mentally, to be among the best linear learning ap-
proaches and is competitive with SVM and Logistic
8ThreshAll is not possible with this training option, as the
system never proposes a correction that is not in L1ConfSet(pi).
9LBJ code is available at http://cogcomp.cs.
illinois.edu/page/software
Regression, while being more efficient in training.
It also has been shown to produce state-of-the-art
results on many natural language applications (Pun-
yakanok et al, 2008).
6 Results and Discussion
Table 4 shows performance of the four systems
by the source language. For each source lan-
guage, the methods that restrict candidate sets in
training or testing outperform the baseline system
NegAll-Clean-ThreshAll that does not restrict can-
didate sets. The NegAll-ErrorL1-NoThresh system
performs better than the other three systems for all
languages, except for Italian. In fact, for the Czech
speaker data, all systems other than NegAll-ErrorL1-
NoThresh, have a precision and a recall of 0, since
no errors are detected10.
Source System Acc. P R
lang.
CH
NegAll-Clean-ThreshAll 84.78 47.58 11.46
NegAll-Clean-ThreshL1 84.84 48.05 15.28
NegL1-Clean-ThreshL1 84.94 50.87 11.46
NegAll-ErrorL1-NoThresh 86.36 55.27 27.43
Baseline 84.89
CZ
NegAll-Clean-ThreshAll 94.74 0.00 0.00
NegAll-Clean-ThreshL1 94.98 0.00 0.00
NegL1-Clean-ThreshL1 94.66 0.00 0.00
NegAll-ErrorL1-NoThresh 95.85 75.00 10.71
Baseline 95.53
IT
NegAll-Clean-ThreshAll 93.23 26.14 8.14
NegAll-Clean-ThreshL1 94.03 51.59 18.60
NegL1-Clean-ThreshL1 93.16 35.00 16.28
NegAll-ErrorL1-NoThresh 93.60 44.95 10.47
Baseline 93.74
RU
NegAll-Clean-ThreshAll 92.73 31.11 3.53
NegAll-Clean-ThreshL1 93.02 48.81 8.24
NegL1-Clean-ThreshL1 92.44 34.42 8.82
NegAll-ErrorL1-NoThresh 93.14 52.38 12.94
Baseline 92.98
SP
NegAll-Clean-ThreshAll 91.95 26.14 5.77
NegAll-Clean-ThreshL1 92.02 28.64 5.77
NegL1-Clean-ThreshL1 92.44 40.00 7.69
NegAll-ErrorL1-NoThresh 93.71 77.50 19.23
Baseline 92.66
Table 4: Performance results for the 4 systems. All sys-
tems, except for NegAll-ErrorL1-NoThresh, use a thresh-
old, which is optimized for accuracy on the development
set. Baseline denotes the percentage of prepositions used
correctly in the data. The baseline allows us to evaluate
the systems with respect to accuracy, the percentage of
prepositions, on which the prediction of the system is the
same as the label. Averaged results over 2 runs.
10The Czech data set is the smallest and contains a total of
627 prepositions and only 28 errors.
967
The NegAll-ErrorL1-NoThresh system does not
use a threshold. However, as shown in Fig. 1, it
is possible to increase the precision of the NegAll-
ErrorL1-NoThresh system by applying a threshold,
at the expense of a lower recall.
While the ordering of the systems with respect to
quality is not consistent from Table 4, due to modest
test data sizes, Table 5 and Fig. 1 show results for the
models on all data combined and thus give a better
idea of how the systems compare against each other.
Table 5 shows performance results for all
data combined. Both NegAll-Clean-ThreshL1 and
NegL1-Clean-ThreshL1 achieve a better precision
and recall over the system with an unrestricted can-
didate set NegAll-Clean-ThreshAll. Recall that both
of the systems restrict candidate sets, the former at
testing stage, the latter by training a separate clas-
sifier for each source preposition. NegAll-Clean-
ThreshL1 performs slightly better than NegL1-
Clean-ThreshL1. We hypothesize that the NegAll-
Clean-ThreshAll performance may be affected be-
cause the classifiers for different source preposi-
tions contain different number of classes, depend-
ing on the size of L1ConfSet confusion sets, which
makes it more difficult to find a unified thresh-
old. The best performing system overall is NegAll-
ErrorL1-NoThresh. While NegAll-Clean-ThreshL1
and NegL1-Clean-ThreshL1 restrict candidate sets,
NegAll-ErrorL1-NoThresh also provides informa-
tion about the likelihood of each confusion, which
benefits the classifier. The differences between
NegAll-ErrorL1-ThreshL1 and each of the other
three systems are statistically significant11 (McNe-
mar?s test, p < 0.01). The table also demon-
strates that the results on the correction task may
vary widely. For example, the recall varies by lan-
guage between 10.47% and 27.43% for the NegAll-
ErrorL1-NoThresh system. The highest recall num-
bers are obtained for Chinese speakers. These
speakers also have the highest error rate, as we noted
in Section 3.
11Tests of statistical significance compare the combined re-
sults from all language groups for each model. For example, to
compare the model NegAll-Clean-ThreshAll to NegAll-ErrorL1-
NoThresh, we combine the results from the five language-
specific models NegAll-ErrorL1-NoThresh and compare them
to the results on the combined data from the five language
groups achieved by the model NegAll-Clean-ThreshAll.
System Acc. P R
NegAll-Clean-ThreshAll 90.90 31.11 7.95
NegAll-Clean-ThreshL1 91.11 37.82 12.78
NegL1-Clean-ThreshL1 90.97 34.34 9.66
NegAll-ErrorL1-NoThresh 92.23 58.47 19.60
Table 5: Comparison of the performance of the 4 sys-
tems on all data combined. All systems, except for
NegAll-ErrorL1-NoThresh, use a threshold, which is op-
timized for accuracy on the development set. The dif-
ferences between NegAll-ErrorL1-ThreshL1 and each of
the other three systems are statistically significant (Mc-
Nemar?s test, p < 0.01).
Finally, Fig. 1 shows precision/recall curves for
the systems12. The curves are obtained by varying
a decision threshold for each system. Before we ex-
amine the differences between the models, it should
be noted that in error correction tasks precision is
favored over recall due to the low level of error.
0
20
40
60
80
100
0 10 20 30 40 50 60
P
R
NegAll-Clean-ThreshAll
NegAll-Clean-ThreshL1
NegAll-ErrorL1-ThreshL1
?
?
????????????????????
?
Figure 1: Precision and recall (%) for three mod-
els: NegAll-Clean-ThreshAll, NegAll-Clean-ThreshL1,
and NegAll-ErrorL1-ThreshL1.
The curves demonstrate that NegAll-Clean-
ThreshL1 and NegAll-ErrorL1-ThreshL1 are supe-
rior to the baseline system NegAll-Clean-ThreshAll:
on the same recall points, the precision for both
systems is consistently better than for the base-
12NegL1-Clean-ThreshL1 is not shown, since it is similar in
its behavior to NegAll-Clean-ThreshL1.
968
line model13. Moreover, while restricting candi-
date sets improves the results, providing informa-
tion to the classifier about the likelihoods of differ-
ent confusions is more helpful, which is reflected
in the precision differences between NegAll-Clean-
ThreshL1 and NegAll-ErrorL1-ThreshL1. In fact,
NegAll-ErrorL1-ThreshL1 achieves a higher preci-
sion compared to the other systems, even when no
threshold is used (Tables 4 and 5). This is because,
unlike the other models, this system does not tend to
propose too many false alarms.
6.1 Comparison to Other Systems
It is difficult to compare performance to other sys-
tems, since training and evaluation are not per-
formed on the same data, and results may vary
widely depending on the first language and profi-
ciency level of the writer. However, in Table 6 we
list several systems and their performance on the
task. Tetreault et al (2010) train on native data and
obtain a precision of 48.6% and a recall of 22.5%
with top 34 prepositions on essays from the Test
of English as a Foreign Language exams. Han et
al. (2010) obtain a precision of 81.7% and a recall
of 13.2% using a model trained on partially error-
tagged data by Korean speakers on top ten preposi-
tions. A model trained on 2 million examples from
clean text achieved on the same data set a precision
of 46.3% and a recall of 11.6%.
Gamon (2010) shows precision/recall curves on
the combined task of detecting missing, extrane-
ous and confused prepositions. For recall points
10% and 20%, precisions of 55% and 40%, respec-
tively, are obtained. For our data, a recall of 10%
corresponds to a precision of 46% for the worst-
performing model and 78% for the best-performing
model. For 20% recall, we obtain a precision of
33% for the worst-performing model and 58% for
the best-performing model. We would like to em-
phasize that these comparisons should be interpreted
with caution.
13While significance tests did not show differences between
NegAll-Clean-ThreshAll and NegAll-Clean-ThreshL1, perhaps
due to a modest test set size, the curves demonstrate that the lat-
ter system indeed provides a stable advantage over the baseline
unrestricted approach.
7 Conclusion and Future Work
In this paper, we proposed methods for improving
candidate sets for the task of detecting and correct-
ing errors in text. To correct errors in preposition
usage made by non-native speakers of English, we
proposed L1-dependent confusion sets that deter-
mine valid candidate corrections using knowledge
about preposition confusions observed in the non-
native text. We found that restricting candidates to
System Training Data P R
Tetreault et al, 2010 native; 34 preps. 48.6 22.5
Han et al, 2010 partially error-tagged; 81.7 13.2
10 preps.
Han et al, 2010 native; 10 preps. 46.3 11.6
Gamon, 2010 native; 12 preps.+ 33.0 10.0
extraneous+missing
Gamon, 2010 native+error-tagged; 55.0 10.0
12 preps.+
extraneous+missing
NegAll-Clean-ThreshAll native; 10 preps. 46.0 10.0
NegAll-ErrorL1-ThreshL1 native with 78.0 10.0
L1 error statistics;
10 preps.
Table 6: Comparison to other systems. Please note
that a direct comparison is not possible, since the systems
are trained and evaluated on different data sets. Gamon
(2010) also considers missing and extraneous preposition
errors.
those that are observed in the non-native data im-
proves both the precision and the recall compared to
a classifier that considers as possible candidates the
set of all prepositions. Furthermore, the approach
that takes into account the likelihood of each prepo-
sition confusion is shown to be the most effective.
The methods proposed in this paper make use of
select characteristics that the error-tagged data can
provide. We would also like to compare the pro-
posed methods to the quality of a model trained on
error-tagged data. Improving the system is also in
our future work, but orthogonal to the current con-
tribution.
Acknowledgments
We thank Nick Rizzolo for helpful discussions on
LBJ. We also thank Peter Chew and the anonymous
reviewers for their insightful comments. This re-
search is partly supported by a grant from the U.S.
Department of Education.
969
References
M. Banko and E. Brill. 2001. Scaling to very very large
corpora for natural language disambiguation. In Pro-
ceedings of 39th Annual Meeting of the Association for
Computational Linguistics, pages 26?33, Toulouse,
France, July.
J. Bitchener, S. Young, and D. Cameron. 2005. The ef-
fect of different types of corrective feedback on ESL
student writing. Journal of Second Language Writing.
A. Carlson and I. Fette. 2007. Memory-based context-
sensitive spelling correction at web scale. In Proceed-
ings of the IEEE International Conference on Machine
Learning and Applications (ICMLA).
A. J. Carlson, J. Rosen, and D. Roth. 2001. Scaling
up context sensitive text correction. In Proceedings of
the National Conference on Innovative Applications of
Artificial Intelligence (IAAI), pages 45?50.
M. Chodorow, J. Tetreault, and N.-R. Han. 2007. Detec-
tion of grammatical errors involving prepositions. In
Proceedings of the Fourth ACL-SIGSEM Workshop on
Prepositions, pages 25?30, Prague, Czech Republic,
June. Association for Computational Linguistics.
G. Dalgish. 1985. Computer-assisted ESL research.
CALICO Journal, 2(2).
J. Eeg-Olofsson and O. Knuttson. 2003. Automatic
grammar checking for second language learners - the
use of prepositions. Nodalida.
A. Elghaari, D. Meurers, and H. Wunsch. 2010. Ex-
ploring the data-driven prediction of prepositions in
english. In Proceedings of COLING 2010, Beijing,
China.
R. De Felice and S. Pulman. 2007. Automatically ac-
quiring models of preposition use. In Proceedings of
the Fourth ACL-SIGSEM Workshop on Prepositions,
pages 45?50, Prague, Czech Republic, June.
R. De Felice and S. Pulman. 2008. A classifier-based ap-
proach to preposition and determiner error correction
in L2 English. In Proceedings of the 22nd Interna-
tional Conference on Computational Linguistics (Col-
ing 2008), pages 169?176, Manchester, UK, August.
Y. Freund and R. E. Schapire. 1999. Large margin clas-
sification using the perceptron algorithm. Machine
Learning, 37(3):277?296.
M. Gamon, J. Gao, C. Brockett, A. Klementiev,
W. Dolan, D. Belenko, and L. Vanderwende. 2008.
Using contextual speller techniques and language
modeling for ESL error correction. In Proceedings of
IJCNLP.
M. Gamon. 2010. Using mostly native data to correct
errors in learners? writing. In NAACL, pages 163?171,
Los Angeles, California, June.
A. R. Golding and D. Roth. 1999. A Winnow based
approach to context-sensitive spelling correction. Ma-
chine Learning, 34(1-3):107?130.
S. Granger, E. Dagneaux, and F. Meunier. 2002. Inter-
national Corpus of Learner English. Presses universi-
taires de Louvain.
S. Gui and H. Yang. 2003. Zhongguo Xuexizhe Yingyu
Yuliaohu. (Chinese Learner English Corpus). Shang-
hai Waiyu Jiaoyu Chubanshe. (In Chinese).
N. Han, M. Chodorow, and C. Leacock. 2006. Detecting
errors in English article usage by non-native speakers.
Journal of Natural Language Engineering, 12(2):115?
129.
N. Han, J. Tetreault, S. Lee, and J. Ha. 2010. Us-
ing an error-annotated learner corpus to develop and
ESL/EFL error correction system. In LREC, Malta,
May.
E. Izumi, K. Uchimoto, T. Saiga, T. Supnithi, and H. Isa-
hara. 2003. Automatic error detection in the Japanese
learners? English spoken data. In The Companion Vol-
ume to the Proceedings of 41st Annual Meeting of
the Association for Computational Linguistics, pages
145?148, Sapporo, Japan, July.
C. Leacock, M. Chodorow, M. Gamon, and J. Tetreault.
2010. Morgan and Claypool Publishers.
J. Lee and S. Seneff. 2008. An analysis of grammatical
errors in non-native speech in English. In Proceedings
of the 2008 Spoken Language Technology Workshop.
V. Punyakanok, D. Roth, and W. Yih. 2008. The impor-
tance of syntactic parsing and inference in semantic
role labeling. Computational Linguistics, 34(2).
N. Rizzolo and D. Roth. 2007. Modeling Discriminative
Global Inference. In Proceedings of the First Inter-
national Conference on Semantic Computing (ICSC),
pages 597?604, Irvine, California, September. IEEE.
D. Roth. 1998. Learning to resolve natural language am-
biguities: A unified approach. In Proceedings of the
National Conference on Artificial Intelligence (AAAI),
pages 806?813.
A. Rozovskaya and D. Roth. 2010a. Annotating ESL
errors: Challenges and rewards. In Proceedings of the
NAACL Workshop on Innovative Use of NLP for Build-
ing Educational Applications.
A. Rozovskaya and D. Roth. 2010b. Training paradigms
for correcting errors in grammar and usage. In Pro-
ceedings of the NAACL-HLT.
J. Tetreault and M. Chodorow. 2008. The ups and
downs of preposition error detection in ESL writing.
In Proceedings of the 22nd International Conference
on Computational Linguistics (Coling 2008), pages
865?872, Manchester, UK, August.
J. Tetreault, J. Foster, and M. Chodorow. 2010. Using
parse features for preposition selection and error de-
tection. In ACL.
970
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 791?802,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Joint Learning and Inference for Grammatical Error Correction
Alla Rozovskaya and Dan Roth
Cognitive Computation Group
University of Illinois at Urbana-Champaign
201 N. Goodwin Avenue
Urbana, IL 61801
{rozovska,danr}@illinois.edu
Abstract
State-of-the-art systems for grammatical er-
ror correction are based on a collection of
independently-trained models for specific er-
rors. Such models ignore linguistic interac-
tions at the sentence level and thus do poorly
on mistakes that involve grammatical depen-
dencies among several words. In this paper,
we identify linguistic structures with interact-
ing grammatical properties and propose to ad-
dress such dependencies via joint inference
and joint learning.
We show that it is possible to identify interac-
tions well enough to facilitate a joint approach
and, consequently, that joint methods correct
incoherent predictions that independently-
trained classifiers tend to produce. Further-
more, because the joint learning model con-
siders interacting phenomena during training,
it is able to identify mistakes that require mak-
ing multiple changes simultaneously and that
standard approaches miss. Overall, our model
significantly outperforms the Illinois system
that placed first in the CoNLL-2013 shared
task on grammatical error correction.
1 Introduction
There has recently been a lot of work addressing er-
rors made by English as a Second Language (ESL)
learners. In the past two years, three competitions
devoted to grammatical error correction for non-
native writers took place: HOO-2011 (Dale and Kil-
garriff, 2011), HOO-2012 (Dale et al, 2012), and
the CoNLL-2013 shared task (Ng et al, 2013).
Nowadays *phone/phones *has/have many
functionalities, *included/including *?/a
camera and *?/a Wi-Fi receiver.
Figure 1: Examples of representative ESL errors.
Most of the work in the area of ESL error cor-
rection has addressed the task by building statistical
models that specialize in correcting a specific type
of a mistake. Figure 1 illustrates several types of
errors common among non-native speakers of En-
glish: article, subject-verb agreement, noun num-
ber, and verb form. A significant proportion of re-
search has focused on correcting mistakes in article
and preposition usage (Izumi et al, 2003; Han et
al., 2006; Felice and Pulman, 2008; Gamon et al,
2008; Tetreault and Chodorow, 2008; Gamon, 2010;
Rozovskaya and Roth, 2010b). Several studies also
consider verb-related and noun-related errors (Lee
and Seneff, 2008; Gamon et al, 2008; Dahlmeier
and Ng, 2012). The predictions made by individual
models are then applied independently (Rozovskaya
et al, 2011) or pipelined (Dahlmeier and Ng, 2012).
The standard approach of training individual clas-
sifiers considers each word independently and thus
assumes that there are no interactions between er-
rors and between grammatical phenomena. But an
ESL writer may make multiple mistakes in a single
sentence and these result in misleading local cues
given to individual classifiers. In the example shown
in Figure 1, the agreement error on the verb ?have?
interacts with the noun number error: a correction
system that takes into account the context may in-
fer, because of the word ?phone?, that the verb num-
ber is correct. For this reason, a system that consid-
791
ers noun and agreement errors separately will fail to
identify and correct the interacting errors shown in
Fig. 1. Furthermore, it may also produce inconsis-
tent predictions.
Even though it is quite clear that grammatical er-
rors interact, for various conceptual and technical
reasons, this issue has not been addressed in a sig-
nificant way in the literature. We believe that the
reasons for that are three-fold: (1) Data: until very
recently we did not have data that jointly annotates
sufficiently many errors of interacting phenomena
(see Sec. 2). (2) Conceptual: Correcting errors in
interacting linguistic phenomena requires that one
identifies those phenomena and, more importantly,
can recognize reliably the interacting components
(e.g., given a verb, identify the subject to enable en-
forcing agreement). The perception has been that
this cannot be done reliably (Sec. 4). (3) Technical:
The NLP community has started to better understand
joint learning and inference and apply it to various
phenomena (Roth and Yih, 2004; Punyakanok et al,
2008; Martins et al, 2011; Clarke and Lapata, 2007;
Sutton and McCallum, 2007) (Sec. 5).
In this paper we present, for the first time, a suc-
cessful approach to jointly resolving grammatical er-
rors. Specifically:
? We identify two pairs of interacting phenomena,
subject-verb and article-NPhead agreements; we
show how to reliably identify these pairs in noisy
ESL data, thereby facilitating the joint correction of
these phenomena.
?We propose two joint approaches: (1) a joint infer-
ence approach implemented on top of individually
learned models using an integer linear programming
formulation (ILP, (Roth and Yih, 2004)), and (2) a
model that jointly learns each pair of these phenom-
ena. We show that each of these methods has its ad-
vantages, and that both solve the two challenges out-
lined above: the joint models exclude inconsistent
predictions that violate linguistic constraints. The
joint learning model exhibits superior performance,
as it is also able to overcome the problem of the
noisy context encountered by the individual mod-
els and to identify errors in contexts, where multiple
changes need to be applied at the same time.
We show that our joint models produce state-of-
the-art performance and, in particular, significantly
outperform the University of Illinois system that
placed first in the CoNLL-2013 shared task, increas-
ing the F1 score by 2 and 4 points in different evalu-
ation settings.
2 Task Description and Motivation
To illustrate the utility of jointly addressing interact-
ing grammatical phenomena, we consider the cor-
pus of the CoNLL-2013 shared task on grammatical
error correction (Ng et al, 2013), which we found
to be particularly well-suited for addressing interac-
tions between grammatical phenomena. The task fo-
cuses on the following five common mistakes made
by ESL writers: article, preposition, noun number,
subject-verb agreement, and verb form, and we ad-
dress two interactions: article-NPhead and subject-
verb.
The training data for the task is from the NUCLE
corpus (Dahlmeier et al, 2013), an error-tagged col-
lection of essays written by non-native learners of
English. The test data is an additional set of essays
by learners from the same linguistic background.
The training and the test data contain 1.2M and 29K
words, respectively. Although the corpus contains
errors of other types, the task focuses on five types
of errors. Table 1 shows the number of mistakes1 of
each type and the error rates, i.e. the percentage of
erroneous words by error type.
Error Number of errors and error rate
Training Test
Article 6658 (2.4%) 690 (10.0%)
Prep. 2404 (2.0%) 311 (10.7%)
Noun 3779 (1.6%) 396 (6.0%)
Verb Agr. 1527(2.0%) 124 (5.2%)
Verb Form 1453 (0.8%) 122 (2.5%)
Table 1: Number of annotated errors in the CoNLL-
2013 shared task. Percentage denotes the error rates, i.e.
the number of erroneous instances with respect to the to-
tal number of relevant instances in the data. For example,
10.7% of prepositions in the test data are used incorrectly.
The numbers in the revised data set are slightly higher.
We note that the CoNLL-2013 data set is the first
annotated collection that makes a study like ours
feasible. The presence of a common test set that
1System performance in the shared task is evaluated on data
with and without additional revisions added based on the input
from participants. The number of mistakes in the revised test
data is slightly higher.
792
contains a good number of interacting errors ? ar-
ticle, noun, and verb agreement mistakes ? makes
the data set well-suited for studying which approach
works best for addressing interacting phenomena.
The HOO-2011 shared task collection (Dale and
Kilgarriff, 2011) contains a very small number of
noun and agreement errors (41 and 11 in test, re-
spectively), while the HOO-2012 competition (Dale
et al, 2012) only addresses article and preposition
mistakes. Indeed, in parallel to the work presented
here, Wu and Ng (2013) attempted the ILP-based
approach of Roth and Yih (2004) in this domain.
They were not able to show any improvement, for
two reasons. First, the HOO-2011 data set which
they used does not contain a good number of errors
in interacting structures. Second, and most impor-
tantly, they applied constraints in an indiscriminate
manner. In contrast, we show how to identify the
interacting structures? components in a reliable way,
and this plays a key role in the joint modeling im-
provements.
Lack of data hindered other earlier efforts for
error correction beyond individual language phe-
nomena. Brockett et al (2006) applied machine-
translation techniques to correct noun number errors
on mass nouns and article usage but their application
was restricted to a small set of constructions. Park
and Levy (2011) proposed a language-modeling ap-
proach to whole sentence error correction but their
model is not competitive with individually trained
models. Finally, Dahlmeier and Ng (2012) proposed
a decoder model, focusing on four types of errors
in the data set of the HOO-2011 competition (Dale
and Kilgarriff, 2011). The decoder optimized the se-
quence in which individual classifiers were to be ap-
plied to the sentence. However, because the decoder
still corrected mistakes in a pipeline fashion, one at
a time, it is unlikely that it could deal with cases that
require simultaneous changes.
3 The University of Illinois System
Below, we briefly describe the University of Illinois
system (henceforth Illinois; in the overview paper of
the shared task the system is referred to as UI) that
achieved the best result in the CoNLL-2013 shared
task and which we use as our baseline model. For
a complete description, we refer the reader to Ro-
zovskaya et al (2013).
The Illinois system implements five machine-
learning independently-trained classifiers that fol-
low the popular approach to ESL error correction
borrowed from the context-sensitive spelling correc-
tion task (Golding and Roth, 1999; Carlson et al,
2001). A confusion set is defined that specifies a
list of confusable words. Each occurrence of a con-
fusable word in text is represented as a vector of
features derived from a context window around the
target. The problem is cast as a multi-class classi-
fication task and a classifier is trained on native or
learner data. At prediction time, the model selects
the most likely candidate from the confusion set.
The confusion set for prepositions includes the
top 12 most frequent English prepositions. The arti-
cle confusion set is as follows: {a,the,?}2. The con-
fusion sets for noun, agreement, and form modules
depend on the target word and include its morpho-
logical variants (Table 2).
?Hence, the environmental *factor/factors also
*contributes/contribute to various difficulties,
*included/including problems in nuclear technol-
ogy.?
Error type Confusion set
Noun {factor, factors}
Verb Agr. {contribute, contributes}
Verb Form {included, including, includes, include}
Table 2: Confusion sets for noun number, agreement,
and form classifiers.
The article classifier is a discriminative model
that draws on the state-of-the-art approach described
in Rozovskaya et al (2012). The model makes use
of the Averaged Perceptron algorithm (Freund and
Schapire, 1996) and is trained on the training data of
the shared task with rich features.
The other models are trained on native English
data, the Google Web 1T 5-gram corpus (henceforth,
Google, (Brants and Franz, 2006)) with the Na??ve
Bayes (NB) algorithm. All models use word n-gram
features derived from the 4-word window around the
target word. In the preposition model, priors for
preposition preferences are learned from the shared
task training data (Rozovskaya and Roth, 2011).
2? denotes noun-phrase-initial contexts where an article is
likely to have been omitted. The variants ?a? and ?an? are con-
flated and are restored later.
793
Example Predictions made by the Illinois system
?They believe that such situation must be avoided.? such situation? such a situations
?Nevertheless , electric cars is still regarded as a great trial innovation.? cars is? car are
?Every students have appointments with the head of the department.? No change
Table 3: Examples of predictions of the Illinois system that combines independently-trained models.
The words that are selected as input to classifiers
are called candidates. Article and preposition can-
didates are identified with a closed list of words;
noun-phrase-initial contexts for the article classifier
are determined using a shallow parser3 (Punyakanok
and Roth, 2001). Candidates for the noun, agree-
ment, and form classifiers are identified with a part-
of-speech tagger4, e.g. noun candidates are words
that are tagged as NN or NNS. Table 4 shows the
total number of candidates for each classifier.
Classifier
Art. P N Agr. F
Train 254K 103K 240K 75K 175K
Test 6K 2.5K 2.6K 2.4K 4.8K
Table 4: Number of candidate words by classifier type
in training and test data.
4 Interacting Mistakes
The approach of addressing each type of mistake in-
dividually is problematic when multiple phenomena
interact. Consider the examples in Table 3 and the
predictions made by the Illinois system. In the first
and second sentences, there are two possible ways
to correct the structures ?such situation? and ?cars
is?. In the former, either the article or the noun num-
ber should be changed; in the latter, either the noun
number or the verb agreement marker5. In these ex-
amples, each of the independently-trained classifiers
identifies the problem because each system makes a
decision using the second error as part of its contex-
tual cues, and thus the individual systems produce
inconsistent predictions.
3http://cogcomp.cs.illinois.edu/page/
software view/Chunker
4http://cogcomp.cs.illinois.edu/page/
software view/POS
5Both of these solutions will result in grammatical output
and the specific choice between the two depends on the wider
essay context.
The second type of interaction concerns cases that
require correcting more than one word at a time:
the last example in Table 3 requires making changes
both to the verb and the subject. Since each of the in-
dependent classifiers (for nouns and for verb agree-
ment) takes into account the other word as part of
its features, they both infer that the verb number is
correct and that the grammatical subject ?student?
should be plural.
We refer to the words whose grammatical prop-
erties interact as structures. The independently-
trained classifiers tend to fail to provide valid cor-
rections in contexts where it is important to consider
both words of the structure.
4.1 Structures for Joint Modeling
We address two linguistic structures that are relevant
for the grammatical phenomena considered: article-
NPhead and subject-verb. In the article-NPhead
structures, the interaction is between the head of
the noun phrase (NP) and the article that refers to
the NP (first example in Table 3). In particular,
the model should take into account that the article
?a? cannot be combined with a noun in plural form.
For subject-verb agreement, the subject and the verb
should agree in number.
We now need to identify all pairs of candidates
that form the relevant structures. Article-NPhead
structures are pairs of words, such that the first word
is a candidate of type article, while the second word
is a noun candidate. Given an article candidate, the
head of its NP is determined using the POS infor-
mation (this information is obtained from the article
feature vector because the NP head is a feature used
by the article system)6. Subject-verb structures are
pairs of noun-agreement candidates. Given a verb,
its subject is identified with a dependency parser
(Marneffe et al, 2006).
To evaluate the accuracy of subject and NP head
6Some heads are not identified or belong to a different part
of speech.
794
predictions, a random sample of 500 structures of
each type from the training data was examined by
a human annotator with formal training in Linguis-
tics. The human annotations were then compared
against the automatic predictions. The results of
the evaluation for subject-verb and article-NPhead
structures are shown in Tables 5 and 6, respectively.
Although the overall accuracy is above 90% for both
structures, the accuracy varies by the distance be-
tween the structure components and drops signifi-
cantly as the distance increases. For article-NPhead
structures, distance indicates the position of the NP
head with respect to the article, e.g. distance of 1
means that the head immediately follows the arti-
cle. For subject-verb structures, distance is shown
with respect to the verb: a distance of -1 means that
the subject immediately precedes the verb. Although
in most cases the subject is located to the left of
the verb, in some constructions, such as existential
clauses and inversions, it occurs after the verb.
Based on the accuracy results for identifying the
structure components, we select those structures
where the components are reliably identified. For
article-NPhead, valid structures are those where the
distance is at most three words. For subject-verb, we
consider as valid those structures where the identi-
fied subject is located within two words to the left or
three words to the right of the verb.
The valid structures are selected as input to the
joint model (Sec. 5). The joint learning model con-
siders only those valid structures whose components
are adjacent. In adjacent structures the NP head im-
mediately follows the article, and the verb immedi-
ately follows the subject. Joint inference is not re-
stricted to adjacent structures.
The last column of Table 5 shows that valid
subject-verb structures account for 67.5% of all
verbs whose subjects are common nouns (51.7% are
cases where the words are adjacent). Verbs whose
subjects are common nouns account for 57.8% of all
verbs that have subjects (verbs with different types
of subjects, most of which are personal pronouns,
are not considered here, since these subjects are not
part of the noun classifier).
Valid article-NPhead structures account for
98.0% of all articles whose NP heads are common
nouns (47.5% of those are adjacent structures), as
shown in the last column of Table 6. 71.0% of arti-
cles in the training data belong to an NP whose head
is a common noun; NPs whose heads belong to dif-
ferent parts of speech are not considered.
Note also that because a noun may belong both to
an article-NPhead and a subject-verb structure, the
structures contain an overlap.
Distance Accuracy % of all subj. Cumul.
predictions
-1 97.6% 51.7% 51.7%
1,2,3 100.0% 8.9% 60.6%
-2 88.2% 6.9% 67.5%
Other 80.8% 32.5% 100.0%
Table 5: Accuracy of subject identification on a random
sample of subject-verb structures from the training data.
The overall accuracy is 91.52%. For each distance, the follow-
ing are shown: accuracy based on comparison with human eval-
uation; the percentage of all predictions that have this distance;
the cumulative percentage.
Distance Accuracy % of all head Cumul.
predictions
1 94.8% 47.5% 47.5%
2 94.4% 44.0% 91.5%
3 92.3% 6.5% 98.0%
Other 89.1% 2.0% 100%
Table 6: Accuracy of NP head identification on a random
sample of article-NPhead structures from training data. The
overall accuracy is 94.45%. For each distance, the following are
shown: accuracy based on comparison with human evaluation;
the percentage of all predictions that have this distance; the cu-
mulative percentage.
5 The Joint Model
In this section, we present the joint inference and
the joint learning approaches. In the joint inference
approach, we use the independently-learned models
from the Illinois system, and the interacting target
words identified earlier are considered only at infer-
ence stage. In the joint learning method, we jointly
learn a model for the interacting phenomena.
The label space in the joint models corresponds
to sequences of labels from the confusion sets of
the individual classifiers: {a ? sing, a ? pl, the ?
sing, the ? pl,? ? sing,? ? pl} and {sing ?
sing, sing?pl, pl?sing, pl?pl} for article-NPhead
and subject-verb structures, respectively7. Invalid
7?sing? and ?pl? refer to the grammatical number of noun
795
structures, such as pl-sing are excluded via hard con-
straints (when we run joint inference) or via implicit
soft constraints (when we use joint learning).
5.1 Joint Inference
In the individual model approach, decisions are
made for each word independently, ignoring the in-
teractions among linguistic phenomena. The pur-
pose of joint inference is to include linguistic (i.e.
structural) knowledge, such as ?plural nouns do not
take an indefinite article?, and ?agreement consis-
tency between the verb and the subject that controls
it?. This knowledge should be useful for resolving
inconsistencies produced by individual classifiers.
The inference approach we develop in this paper
follows the one proposed by Roth and Yih (2004)
of training individual models and combining them
at decision time via joint inference. The advantage
of this method is that it allows us to build upon
any existing independently-learned models that pro-
vide a distribution over their outcome, and produce
a coherent global output that respects our declarative
constraints. We formulate our component inference
problems as integer linear program (ILP) instances
as in Roth and Yih (2004).
The inference takes as input the individual clas-
sifiers? confidence scores for each prediction, along
with a list of constraints. The output is the optimal
solution that maximizes the linear sum of the confi-
dence scores, subject to the constraints that encode
the interactions. The joint model thus selects a hy-
pothesis that both obtains the best score according
to the individual models and satisfies the constraints
that reflect the interactions among the grammatical
phenomena at the level of linguistic structures, as
defined in Sec. 4.
Inference The joint inference is enforced at the
level of structures, and each structure corresponds
to one ILP instance. All structures consist of two or
three words: when an article-NPhead structure and
a subject-verb structure include the same noun, the
structure input to the ILP consists of an article-noun-
and verb agreement candidates. The candidates themselves are
the surface forms of specific words that realize these grammat-
ical properties. Note that a subject in subject-verb structures is
always third person, since all subjects in subject-verb structures
are common nouns; other subjects, including pronouns, are ex-
cluded. Thus the agreement distinction is singular vs. plural.
verb triple. We formulate the inference problem as
follows: Given a structure s that consists of n words,
let wi correspond to the ith word in the structure. Let
h denote a hypothesis from the hypothesis space H
for s, and score(wi, h, li) denote the score assigned
by the appropriate error-specific model to wi under
h for label l from the confusion set of word wi. We
denote by ew,l the Boolean variable that indicates
whether the prediction on word w is assigned the
value l (ew,l = 1) or not (ew,l = 0).
We assume that each independent classifier re-
turns a score that corresponds to the likelihood of
word wi under h being labeled li. The softmax func-
tion (Bishop, 1995) is used to convert raw activation
scores to conditional probabilities for the discrimi-
native article model. The NB scores are also normal-
ized and correspond to probabilities. Then the infer-
ence task is solved by maximizing the overall score
of a candidate assignment of labels l to words w (this
set of feasible assignments is denoted H here) sub-
ject to the constraints C for the structure s:
h? = argmax
h?H
score(h) =
= argmax
h?H
n?
i=1
score(wi, h, l
i)ewi,li
subject to C(s)
Constraints In the {0, 1} linear programming for-
mulation described above, we can encode linguis-
tic constraints that reflect the interactions among the
linguistic phenomena. The inference enforces the
following structural and linguistic constraints:
1. The indefinite article ?a? cannot refer to an NP headed by
a plural noun.
2. Subject and verb must agree in number.
In addition, we encode ?legitimacy? constraints, that
make sure that each w is assigned a single label. All
constraints are encoded as hard constraints.
5.2 Joint Learning
We now describe how we learn the subject-verb and
article-NPhead structures jointly. The joint model is
implemented as a NB classifier and is trained in the
same way as the independent models on the Google
corpus with word n-gram features. Unlike the inde-
pendent models, where the target corresponds to one
796
System Adjacent structures All distances
F1 (Orig.) F1 (Revised) F1 (Orig.) F1 (Revised)
Illinois 31.20 42.14 31.20 42.14
Na??veVerb 31.19 42.20 31.13 42.16
Na??veNoun 31.03 41.87 30.91 41.70
This paper joint systems Joint Inference (adjacent) Joint Inference (all distances)
F1 (Orig.) F1 (Revised) F1 (Orig.) F1 (Revised)
Subject-verb 31.90 42.94 31.97 42.86
Article-NPhead 31.63 42.48 31.79 42.59
Subject-verb + article-NPhead 32.35 43.16 32.51 43.19
Table 7: Joint Inference Results. All results are on the CoNLL-2013 test data using the original and revised gold annotations.
Adjacent denotes a setting, where the joint inference is applied to structures with consecutive components (article-NPhead or
subject-verb). All distances denotes a setting, where the constraints are applied to all valid structures, as described in Sec. 4.1.
Illinois denotes the result obtained by the top CoNLL-2013 shared task system. In all cases, the candidates that are not part of the
structures are handled by the respective components of the Illinois system. Na??veVerb and Na??veNoun denote heuristics, where a
verb or subject are changed to ensure agreement. All improvements over the Illinois system are statistically significant (McNemar?s
test, p < 0.01).
word, here the target corresponds to two words that
are part of the structure and the label space of the
model is modified accordingly. Since we use fea-
tures that can be computed from the small windows
in the Google corpus, the joint learning model han-
dles only adjacent structures (Sec. 4.1). Because the
target consists of two words and the Google corpus
contains counts for n-grams of length at most five,
the features are collected in the three word window
around the target.8
Unlike with the joint inference, here we do not
explicitly encode linguistic constraints. One reason
for this is that the NP head and subject predictions
are not 100% accurate, so input structures will have
noise. However, the joint model learns these con-
straints through the evidence seen in training.
6 Experiments
In this section, we describe our experimental setup
and evaluate the performance of the joint approach.
In the joint approach, the joint components pre-
sented in Sec. 5 handle the interacting structures de-
scribed in Sec. 4. The individual classifiers of the
Illinois system make predictions for the remaining
words. The research question addressed by the ex-
periments is the following: Given independently-
trained systems for different types of errors, can we
improve the performance by considering the phe-
8Also note that when the article is ?, the surface form of
the structure corresponds to the NP head alone; this does not
present a problem because in the NB model the context counts
are normalized with the prior counts.
nomena that interact jointly? To address this, we
report the results in the following settings:
1. Joint Inference: we compare the Illinois sys-
tem that is a collection of individually-trained mod-
els that are applied independently with a model
that uses joint inference encoded as declarative con-
straints in the ILP formulation and show that using
joint inference results in a strong performance gain.
2. Joint Learning: we compare the Illinois system
with a model that incorporates jointly-trained com-
ponents for the two linguistic structures that we de-
scribed in Sec. 4. We show that joint training pro-
duces an even stronger gain in performance com-
pared to the Illinois model.
2. Joint Learning and Inference: we apply joint in-
ference to the output of the joint learning system to
account for dependencies not covered by the joint
learning model.
We report F1 performance scored using the offi-
cial scorer from the shared task (Dahlmeier and Ng,
2012). The task reports two types of evaluation: on
the original gold data and on gold data with addi-
tional corrections. We refer to the results as Origi-
nal and Revised.
6.1 Joint Inference Results
Table 7 shows the results of applying joint infer-
ence to the Illinois system. Both the article-NPhead
and the subject-verb constraints improve the perfor-
mance. The results for the joint inference are shown
in two settings, adjacent and all structures, so that
later we can compare joint inference with the joint
learning model that handles only adjacent structures.
797
Illinois system Illinois-NBArticle
F1 (Orig.) F1 (Revised) F1 (Orig.) F1 (Revised)
Illinois 31.20 42.14 31.71 41.38
This paper joint systems Joint Learning (adjacent) Joint Learning (adjacent)
F1 (Orig.) F1 (Revised) F1 (Orig.) F1 (Revised)
Subject-verb 32.64* 43.37* 33.09* 42.78*
Article-NPhead 33.89* 42.57* 33.16* 41.51
Subject-verb + article-NPhead 35.12* 43.73* 34.41* 42.76*
Table 8: Joint Learning Results. All results are on the CoNLL-2013 test data using the original and revised gold annotations.
Illinois-NBArticle denotes the Illinois system, where the discriminative article model is replaced with a NB classifier. Adjacent
denotes a setting, where the structure components are consecutive (article-NPhead or subject-verb), as described in Sec. 4.1.
Illinois denotes the result obtained by the top CoNLL-2013 shared task system. In all cases, the candidates that are not part of the
structures are handled by the respective components of the Illinois system. Statistically significant improvements (McNemar?s test,
p < 0.01) over the Illinois system are marked with an asterisk (*).
It is also interesting to note that the key improvement
comes from considering structures whose compo-
nents are adjacent. This is not surprising given that
the accuracy for subject and NP head identification
drops as the distance increases.
For subject-verb constraints, we also implement
a na??ve approach that looks for contradictions and
changes either the verb or the subject if they do not
satisfy the number agreement. These two heuris-
tics are denoted as Na??veVerb and Na??veNoun. The
heuristics differ from the joint inference in that they
enforce agreement by always changing either the
noun (Na??veNoun) or the verb (Na??veVerb), while
the joint inference does this using the scores pro-
duced by the independent models. In other words,
the key is the objective function, while the compo-
nents of the objective function are the same in the
heuristics and the joint inference. The results in Ta-
ble 7 show that simply enforcing agreement does not
work well and that the ILP formulation is indeed ef-
fective and improves over the independently-trained
models in all cases.
Recall that valid structures include only those
whose components can be identified in a reliable
way (Sec. 4.1). To evaluate the impact of that filter-
ing, we perform two experiments with subject-verb
structures (long-distance dependencies are more
common in those constructions than in the article-
NPhead structures): first, we apply joint inference
to all subject-verb structures. We obtain F1 scores of
31.61 and 42.28, on original and revised gold data,
respectively, which is significantly worse than the
results on subject-verb structures in Table 7 (31.97
and 42.86, respectively) and only slightly better than
the baseline performance of the Illinois system. Fur-
thermore, when we apply joint inference to those
structures which were excluded by filtering in Sec.
4.1, we find that the performance degrades com-
pared to the Illinois system (30.85 and 41.58). These
results demonstrate that the joint inference improve-
ments are due to structures whose components can
be identified with high accuracy and that it is essen-
tial to identify these structures; bad structures, on the
other hand, hurt performance.
6.2 Joint Learning Results
Now we show experimental results of the joint learn-
ing (Table 8). Note that the joint learning component
considers only those structures where the words are
adjacent. Because the Illinois system presented in
Sec. 3 makes use of a discriminative article model,
while the joint model uses NB, we also show results,
where the article model is replaced by a NB classi-
fier trained on the Google corpus. In all cases, joint
learning demonstrates a strong performance gain.
6.3 Joint Learning and Inference Results
Finally, we apply joint inference to the output of the
joint learning system in Sec. 6.2. Table 9 shows
the results of the Illinois model, the model that ap-
plies joint inference and joint learning separately,
and both. Even though the joint learning performs
better than the joint inference, the joint learning
covers only adjacent structures. Furthermore, joint
learning does not address overlapping structures of
triples that consist of article, subject, and verb (6%
of all structures). Joint inference allows us to ensure
consistent predictions in cases not addressed by the
798
Example Illinois system JL and JI
?Moreover, the increased technologies help people to overcome
different natural disasters.
No change technology helps
?At that time,... there are surveillances in everyone?s heart and
criminals are more difficult to hide.?
there are* surveillance* there is surveillance
?In such situation, individuals will lose their basic privacy.? such a* situations* such a situation
?In supermarket monitor is needed because we have to track
thieves.?
No change monitors are
Table 10: Examples of mistakes that are corrected by the joint model but not by the Illinois model. Illinois denotes the result
obtained by the top CoNLL-2013 shared task system from the University of Illinois. JL and JI stand for joint learning and joint
inference, respectively. Inconsistent predictions are starred.
F1 (Orig.) F1 (Revised)
Illinois 31.20 42.14
Joint Inference 32.51 43.19
Joint Learning 35.12 43.73
Joint Learn. + Inf. 35.21 43.74
Table 9: Joint Learning and Inference. All results are on the
CoNLL-2013 test data using the original and revised gold anno-
tations. Results of the joint models that include the joint infer-
ence component are shown for structures of all distances. Illi-
nois denotes the result obtained by the top CoNLL-2013 shared
task system. All joint systems demonstrate a statistically sig-
nificant improvement over the Illinois system; joint learning
improvements are also statistically significant compared to the
joint inference results (McNemar?s test, p < 0.01).
joint learning model. Indeed, we can get a small im-
provement by adding joint inference on top of the
joint learning on original annotations. Since the re-
vised corrections are based on the participants? input
and are most likely biased towards system predic-
tions for corrections missed by the original annota-
tors (Ng et al, 2013), it is more difficult to show
improvement on revised data.
7 Discussion and Error Analysis
In the previous section, we evaluated the proposed
joint inference and joint learning models that han-
dle interacting grammatical phenomena. We showed
that the joint models produce significant improve-
ments over the highest-scoring CoNLL-2013 shared
task system that consists of independently-trained
classifiers: the joint approaches increase the F1
score by 4 F1 points on the original gold data and
almost 2 points on the revised data (Table 9).
These results are interesting from the point of
view of developing a practical error correction sys-
tem. However, recall that the errors in the interact-
ing structures are only a subset of mistakes in the
CoNLL-2013 data set but the evaluation in Sec. 6 is
performed with respect to all of these errors. From
a scientific point of view, it is interesting to evalu-
ate the impact of the joint models more precisely by
considering the improvements on the relevant struc-
tures only. Table 11 shows how much the joint learn-
ing approach improves on the subset of relevant mis-
takes.
Structure
Performance (F1)
Illinois Joint Learning
Subject-verb 39.64 52.25
Article-NPhead 30.65 35.90
Table 11: Evaluation of the joint learning performance on
the subset of the data containing interacting errors. All re-
sults are on the CoNLL-2013 test data using the original anno-
tations. Illinois denotes the result obtained by the top CoNLL-
2013 shared task system. All improvements are statistically sig-
nificant over the Illinois system (McNemar?s test, p < 0.01).
Error Analysis To better understand where the joint
models have an advantage over the independently-
trained classifiers, we analyze the output produced
by each of the approaches. In Table 10 we show
examples of mistakes that the model that uses joint
learning and inference is able to identify correctly,
along with the original predictions made by the Illi-
nois system.
Joint Inference vs. Joint Learning We wish
to stress that the joint approaches do not simply
perform better but also make coherent decisions
by disallowing illegitimate outputs. The joint in-
ference approach does this by enforcing linguis-
tic constraints on the output. The joint learning
model, while not explicitly encoding these con-
straints, learns them from the distribution of the
training data.
799
Joint inference is a less expensive model, since it
uses the scores produced by the individual classifiers
and thus does not require additional training. Joint
learning, on the other hand, is superior to joint infer-
ence, since it is better at modeling interactions where
multiple errors occur simultaneously ? it eliminates
the noisy context present when learning the inde-
pendent classifiers. Consider the first example from
Table 10, where both the noun and the agreement
classifiers receive noisy input: the verb ?help? and
the noun ?technologies? act as part of input features
for the noun and agreement classifiers, respectively.
The noisy features prevent both modules from iden-
tifying the two errors.
Finally, an important distinction of the joint learn-
ing method is that it considers all possible output se-
quences in training, and thus it is able to better iden-
tify errors that require multiple changes, such as the
last example in Table 10, where the Illinois system
proposes no changes.
7.1 Error Correction: Challenges
We finalize our discussion with a few comments on
the challenges of the error correction task.
Task Difficulty As shown in Table 1 in Sec. 2, only
a small percentage of words have mistakes, while
over 90% (about 98% in training) are used correctly.
The low error rates are the key reason the error cor-
rection task is so difficult: it is quite challenging for
a system to improve over a writer that already per-
forms at the level of over 90%. Indeed, very few
NLP tasks already have systems that perform at that
level, even when the data is not as noisy as the ESL
data.
Evaluation Metrics In the CoNLL-2013 competi-
tion, as well as the competitions alluded to earlier,
systems were compared on F1 performance, and,
consequently, this is the metric we optimize in this
paper. Practical error correction systems, however,
should be tuned to minimize recall to guarantee that
the overall quality of the text does not go down. In-
deed, the error sparsity makes it very challenging to
identify mistakes accurately, and no system in the
shared task achieves a precision over 50%. How-
ever, once the precision drops below 50%, the sys-
tem introduces more mistakes than it identifies.
Clearly, optimizing the F1 measure does not en-
sure that the quality of the text improves as a re-
sult of running the system. Thus, it can be argued
that the F1 measure is not the right measure for er-
ror correction. A different evaluation metric based
on the accuracy of the data before and after running
the system was proposed in Rozovskaya and Roth
(2010c). When optimizing for this metric, the noun
module, for instance, at recall point 20%, achieves
a precision of 63.93%. This translates into accuracy
of 94.46%, while the baseline on noun errors in the
test data (i.e. the accuracy of the data before running
the system) is 94.0% (Table 1). This means that the
system improves the quality of the data.
Annotation Lastly, we believe that it is important
to provide alternative corrections, as the agreement
on what constitutes a mistake even among native
English speakers can be quite low (Madnani et al,
2011).
8 Conclusion
This work presented the first successful study that
jointly corrects grammatical mistakes. We ad-
dressed two pairs of interacting phenomena and
showed that it is possible to reliably identify their
components, thereby facilitating the joint approach.
We described two joint methods: a joint in-
ference approach implemented via ILP and a
joint learning model. The joint inference en-
forces constraints using the scores produced by the
independently-trained models. The joint learning
model learns the interacting phenomena as struc-
tures. The joint methods produce a significant im-
provement over a state-of-the-art system that com-
bines independently-trained models and, impor-
tantly, produce linguistically legitimate output.
Acknowledgments
The authors thank Peter Chew, Jennifer Cole, Mark Sam-
mons, and the anonymous reviewers for their helpful
feedback. The authors thank Josh Gioja for the code
that performs phonetic disambiguation of the indefinite
article. This material is based on research sponsored
by DARPA under agreement number FA8750-13-2-0008.
The U.S. Government is authorized to reproduce and dis-
tribute reprints for Governmental purposes notwithstand-
ing any copyright notation thereon. The views and con-
clusions contained herein are those of the authors and
should not be interpreted as necessarily representing the
official policies or endorsements, either expressed or im-
plied, of DARPA or the U.S. Government.
800
References
C. Bishop. 1995. Neural Networks for Pattern Recogni-
tion, chapter 6.4: Modelling conditional distributions.
Oxford University Press.
T. Brants and A. Franz. 2006. Web 1T 5-gram Version 1.
Linguistic Data Consortium, Philadelphia, PA.
C. Brockett, D. B. William, and M. Gamon. 2006.
Correcting ESL errors using phrasal SMT techniques.
In Proceedings of the 21st International Conference
on Computational Linguistics and 44th Annual Meet-
ing of the Association for Computational Linguistics,
pages 249?256, Sydney, Australia, July. Association
for Computational Linguistics.
A. J. Carlson, J. Rosen, and D. Roth. 2001. Scaling up
context sensitive text correction. In IAAI.
J. Clarke and M. Lapata. 2007. Modelling compression
with discourse constraints. In Proceedings of the 2007
Joint Conference of EMNLP-CoNLL.
D. Dahlmeier and H.T Ng. 2012. A beam-search de-
coder for grammatical error correction. In EMNLP-
CoNLL, Jeju Island, Korea, July. Association for Com-
putational Linguistics.
D. Dahlmeier, H.T. Ng, and S.M. Wu. 2013. Building
a large annotated corpus of learner english: The nus
corpus of learner english. In Proc. of the NAACL HLT
2013 Eighth Workshop on Innovative Use of NLP for
Building Educational Applications, Atlanta, Georgia,
June. Association for Computational Linguistics.
R. Dale and A. Kilgarriff. 2011. Helping Our Own: The
HOO 2011 pilot shared task. In Proceedings of the
13th European Workshop on Natural Language Gen-
eration.
R. Dale, I. Anisimoff, and G. Narroway. 2012. A re-
port on the preposition and determiner error correction
shared task. In Proc. of the NAACL HLT 2012 Seventh
Workshop on Innovative Use of NLP for Building Edu-
cational Applications, Montreal, Canada, June. Asso-
ciation for Computational Linguistics.
R. De Felice and S. Pulman. 2008. A classifier-based ap-
proach to preposition and determiner error correction
in L2 English. In Proceedings of the 22nd Interna-
tional Conference on Computational Linguistics (Col-
ing 2008), pages 169?176, Manchester, UK, August.
Y. Freund and R. E. Schapire. 1996. Experiments with
a new boosting algorithm. In Proc. 13th International
Conference on Machine Learning.
M. Gamon, J. Gao, C. Brockett, A. Klementiev,
W. Dolan, D. Belenko, and L. Vanderwende. 2008.
Using contextual speller techniques and language
modeling for ESL error correction. In Proceedings of
IJCNLP.
M. Gamon. 2010. Using mostly native data to correct
errors in learners? writing. In NAACL, pages 163?171,
Los Angeles, California, June.
A. R. Golding and D. Roth. 1999. A Winnow based
approach to context-sensitive spelling correction. Ma-
chine Learning.
N. Han, M. Chodorow, and C. Leacock. 2006. Detecting
errors in English article usage by non-native speakers.
Journal of Natural Language Engineering, 12(2):115?
129.
E. Izumi, K. Uchimoto, T. Saiga, T. Supnithi, and H. Isa-
hara. 2003. Automatic error detection in the Japanese
learners? English spoken data. In The Companion Vol-
ume to the Proceedings of 41st Annual Meeting of
the Association for Computational Linguistics, pages
145?148, Sapporo, Japan, July.
J. Lee and S. Seneff. 2008. Correcting misuse of verb
forms. In ACL, pages 174?182, Columbus, Ohio,
June. Association for Computational Linguistics.
N. Madnani, M. Chodorow, J. Tetreault, and A. Ro-
zovskaya. 2011. They can help: Using crowdsourcing
to improve the evaluation of grammatical error detec-
tion systems. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguistics:
Human Language Technologies, pages 508?513, Port-
land, Oregon, USA, June. Association for Computa-
tional Linguistics.
M. Marneffe, B. MacCartney, and Ch. Manning. 2006.
Generating typed dependency parses from phrase
structure parses. In LREC.
A. Martins, Noah N. Smith, M. Figueiredo, and P. Aguiar.
2011. Dual decomposition with many overlapping
components. In Proceedings of the 2011 Conference
on Empirical Methods in Natural Language Process-
ing, pages 238?249, Edinburgh, Scotland, UK., July.
Association for Computational Linguistics.
H. T. Ng, S. M. Wu, Y. Wu, Ch. Hadiwinoto, and
J. Tetreault. 2013. The conll-2013 shared task on
grammatical error correction. In Proc. of the Sev-
enteenth Conference on Computational Natural Lan-
guage Learning. Association for Computational Lin-
guistics.
A. Park and R. Levy. 2011. Automated whole sentence
grammar correction using a noisy channel model. In
ACL, Portland, Oregon, USA, June. Association for
Computational Linguistics.
V. Punyakanok and D. Roth. 2001. The use of classifiers
in sequential inference. In NIPS.
V. Punyakanok, D. Roth, and W. Yih. 2008. The impor-
tance of syntactic parsing and inference in semantic
role labeling. Computational Linguistics, 34(2).
D. Roth and W. Yih. 2004. A linear programming formu-
lation for global inference in natural language tasks. In
Hwee Tou Ng and Ellen Riloff, editors, CoNLL.
A. Rozovskaya and D. Roth. 2010a. Annotating ESL
errors: Challenges and rewards. In Proceedings of the
801
NAACL Workshop on Innovative Use of NLP for Build-
ing Educational Applications.
A. Rozovskaya and D. Roth. 2010b. Generating con-
fusion sets for context-sensitive error correction. In
Proceedings of the Conference on Empirical Methods
in Natural Language Processing (EMNLP).
A. Rozovskaya and D. Roth. 2010c. Training paradigms
for correcting errors in grammar and usage. In
NAACL.
A. Rozovskaya and D. Roth. 2011. Algorithm selec-
tion and model adaptation for ESL correction tasks. In
ACL.
A. Rozovskaya, M. Sammons, J. Gioja, and D. Roth.
2011. University of Illinois system in HOO text cor-
rection shared task.
A. Rozovskaya, M. Sammons, and D. Roth. 2012. The
UI system in the HOO 2012 shared task on error cor-
rection.
A. Rozovskaya, K.-W. Chang, M. Sammons, and
D. Roth. 2013. The University of Illinois system in
the CoNLL-2013 shared task. In CoNLL Shared Task.
C. Sutton and A. McCallum. 2007. Piecewise pseudo-
likelihood for efficient training of conditional random
fields. In Zoubin Ghahramani, editor, ICML.
J. Tetreault and M. Chodorow. 2008. The ups and
downs of preposition error detection in ESL writing.
In Proceedings of the 22nd International Conference
on Computational Linguistics (Coling 2008), pages
865?872, Manchester, UK, August.
Y. Wu and H.T. Ng. 2013. Grammatical error correction
using integer linear programming. In Proceedings of
the 51st Annual Meeting of the Association for Com-
putational Linguistics (Volume 1: Long Papers), pages
1456?1465, Sofia, Bulgaria, August. Association for
Computational Linguistics.
802
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 358?367,
Gothenburg, Sweden, April 26-30 2014.
c
?2014 Association for Computational Linguistics
Correcting Grammatical Verb Errors
Alla Rozovskaya
Columbia University
New York, NY 10115
ar3366@columbia.edu
Dan Roth
University of Illinois
Urbana, IL 61801
danr@illinois.edu
Vivek Srikumar
Stanford University
Stanford, CA 94305
svivek@cs.stanford.edu
Abstract
Verb errors are some of the most com-
mon mistakes made by non-native writers
of English but some of the least studied.
The reason is that dealing with verb er-
rors requires a new paradigm; essentially
all research done on correcting grammat-
ical errors assumes a closed set of trig-
gers ? e.g., correcting the use of prepo-
sitions or articles ? but identifying mis-
takes in verbs necessitates identifying po-
tentially ambiguous triggers first, and then
determining the type of mistake made and
correcting it. Moreover, once the verb is
identified, modeling verb errors is chal-
lenging because verbs fulfill many gram-
matical functions, resulting in a variety of
mistakes. Consequently, the little earlier
work done on verb errors assumed that the
error type is known in advance.
We propose a linguistically-motivated ap-
proach to verb error correction that makes
use of the notion of verb finiteness to iden-
tify triggers and types of mistakes, before
using a statistical machine learning ap-
proach to correct these mistakes. We show
that the linguistically-informed model sig-
nificantly improves the accuracy of the
verb correction approach.
1 Introduction
We address the problem of correcting grammati-
cal verb mistakes made by English as a Second
Language (ESL) learners. Recent work in ESL er-
ror correction has focused on errors in article and
preposition usage (Han et al., 2006; Felice and
Pulman, 2008; Gamon et al., 2008; Tetreault et
al., 2010; Gamon, 2010; Rozovskaya and Roth,
2010b; Dahlmeier and Ng, 2011).
While verb errors occur as often as article and
preposition mistakes, with a few exceptions (Lee
and Seneff, 2008; Gamon et al., 2009; Tajiri et al.,
2012), there has been little work on verbs. There
are two reasons for why it is difficult to deal with
verb mistakes. First, in contrast to articles and
prepositions, verbs are more difficult to identify
in text, as they can often be confused with other
parts of speech, and processing tools are known to
make more errors on noisy ESL data (Nagata et al.,
2011). Second, verbs are more complex linguisti-
cally: they fulfill several grammatical functions,
and these different roles imply different types of
errors.
These difficulties have led all previous work
on verb mistakes to assume prior knowledge of
the mistake type; however, identifying the specific
category of a verb error is nontrivial, since the sur-
face form of the verb may be ambiguous, espe-
cially when that verb is used incorrectly. Consider
the following examples of verb mistakes:
1. ?We discusses*/discuss this every time.?
2. ?I will be lucky if I {will find}*/find something that
fits.?
3. ?They wanted to visit many places without
spend*/spending a lot of money.?
4. ?They arrived early to organized*/organize every-
thing?.
These examples illustrate three grammatical
verb properties: Agreement, Tense, and non-finite
Form choice that encompass the most common
grammatical verb problems for ESL learners. The
first two examples show mistakes on verbs that
function as main verbs in a clause: sentence (1)
shows an example of subject-verb Agreement er-
ror; (2) is an example of a Tense mistake where
the ambiguity is between {will find} (Future tense)
358
and find (Present tense). Examples (3) and (4) dis-
play Form mistakes: confusing the infinitive and
gerund forms in (3) and including an inflection on
an infinitive verb in (4).
This paper addresses the specific challenges of
verb error correction that have not been addressed
previously ? identifying candidates for mistakes
and determining which class of errors is present,
before proceeding to correct the error. The ex-
perimental results show that our linguistically-
motivated approach benefits verb error correction.
In particular, in order to determine the error type,
we build on the notion of verb finiteness to distin-
guish between finite and non-finite verbs (Quirk et
al., 1985), that correspond to Agreement and Tense
mistakes (examples (1) and (2) above) and Form
mistakes (examples (3) and (4) above), respec-
tively (see Sec. 3). The approach presented in this
work was evaluated empirically and competitively
in the context of the CoNLL shared task on error
correction (Ng et al., 2013) where it was imple-
mented as part of the highest-scoring University
of Illinois system (Rozovskaya et al., 2013) and
demonstrated superior performance on the verb er-
ror correction sub-task.
This paper makes the following contributions:
?We present a holistic, linguistically-motivated
framework for correcting grammatical verb mis-
takes; our approach ?starts from scratch? with-
out any knowledge of which mistakes should be
corrected or of the mistake type; in doing that
we show that the specific challenges of verb error
correction are better addressed by first identifying
the finiteness of the verb in the error identification
stage.
? Within the proposed model, we describe and
evaluate several methods of selecting verb candi-
dates, an algorithm for determining the verb type,
and a type-driven verb error correction system.
?We annotate a subset of the FCE data set with
gold verb candidates and gold verb type.
1
2 Related Work
Earlier work in ESL error correction follows the
methodology of the context-sensitive spelling cor-
rection task (Golding and Roth, 1996; Golding
and Roth, 1999; Banko and Brill, 2001; Carlson
et al., 2001; Carlson and Fette, 2007). Most of
the effort in ESL error correction so far has been
1
The annotation is available at http://cogcomp.cs.illinois.
edu/page/publication view/743
on article and preposition usage errors, as these
are some of the most common mistakes among
non-native English speakers (Dalgish, 1985; Lea-
cock et al., 2010). These phenomena are generally
modeled as multiclass classification problems: a
single classifier is trained for a given error type
where the set of classes includes all articles or the
top n most frequent English prepositions (Izumi
et al., 2003; Han et al., 2006; Felice and Pul-
man, 2008; Gamon et al., 2008; Tetreault et al.,
2010; Rozovskaya and Roth, 2010b; Rozovskaya
and Roth, 2011; Dahlmeier and Ng, 2011).
Mistakes on verbs have attracted significantly
less attention in the error correction literature.
Moreover, the little earlier work done on verb er-
rors only considered subsets of these errors and
assumed the error sub-type is known in advance.
Gamon et al. (2009) mentioned a model for learn-
ing gerund/infinitive confusions and auxiliary verb
presence/choice. Lee and Seneff (2008) proposed
an approach based on pattern matching on trees
combined with word n-gram counts for correcting
agreement misuse and some types of verb form
errors. However, they excluded tense mistakes,
which is the most common error category for ESL
learners (40% of all verb errors, Sec. 3). Tajiri
et al. (2012) considered only tense mistakes. In
the above studies, it was assumed that the type of
mistake that needs to be corrected is known, and
irrelevant verb errors were excluded (e.g., Tajiri
et al. (2012) addressed only tense mistakes and
excluded from the evaluation other kinds of verb
errors). In other words, it was assumed that part
of the task was solved. But, unlike in article and
preposition error correction where the type of mis-
take is known based on the surface form of the
word, in verb error correction, it is not obvious.
The key distinction of our work is that we pro-
pose a holistic approach that starts from ?scratch?
and, given an instance, first detects a mistake and
identifies its type, and then proceeds to correct
it. We also evaluate several methods for select-
ing verb candidates and show the significance of
this step for improving verb error correction per-
formance, while earlier studies do not discuss this
aspect of the problem. In the CoNLL shared task
(Ng et al., 2013) that included verb errors in agree-
ment and form, the participating teams did not pro-
vide details on how specific challenges were han-
dled, but the University of Illinois system obtained
the highest score on the verb sub-task, even though
359
Tag Error type Rel. freq. (%)
TV Tense 40.0
FV Form 22.3
AGV Verb-subject agreement 11.5
MV Missing verb 11.7
UV Unneccesary verb 7.3
IV Inflection 5.4
DV Derivation 1.8
Total 6640
Table 1: Grammatical verb errors in FCE.
all teams used similar resources (Ng et al., 2013).
3 Verb Errors in ESL Writing
Verb-related errors are very prominent among
non-native English speakers: grammatical mis-
use of verbs constitutes one of the most com-
mon errors in several learner corpora, including
those previously used (Izumi et al., 2003; Lee
and Seneff, 2008) and the one employed in this
work. We study verb errors using the FCE cor-
pus (Yannakoudakis et al., 2011). The corpus
possesses several desirable characteristics: it is
large (500,000 words), has been annotated by na-
tive English speakers, and contains data by learn-
ers of multiple first-language backgrounds. The
FCE corpus contains 5056 determiner errors, 5347
preposition errors, and 6640 grammatical verb
mistakes (Table 1).
3.1 Verb Finiteness
There are many grammatical categories for which
English verbs can be marked. The linguistic no-
tion of verb finiteness or verb type (Radford, 1988;
Quirk et al., 1985) distinguishes between verbs
that function on their own in a clause as main verbs
(finite) and those that do not (non-finite). Gram-
matical properties associated with each group are
mutually exclusive: tense and agreement markers,
for example, do not apply to non-finite verbs; non-
finite verbs are not marked for many grammatical
functions but may appear in several forms.
The most common verb problems for ESL
learners ? Tense, Agreement, non-finite Form ?
involve verbs both in finite and non-finite roles.
Table 2 illustrates contexts that license finite and
non-finite verbs.
Our intuition is that, because properties associ-
ated with each verb type are mutually exclusive,
verb finiteness should benefit verb error correc-
tion models: an observed verb error may be due
to several grammatical phenomena, and knowing
which phenomena are active depends on the func-
tion of the verb in the current context. Note that
Agreement, Tense, and Form errors account for
Category Agreement Kappa Random
Correct verbs 0.97 0.95 0.51
Erroneous verbs 0.88 0.81 0.41
Table 3: Inter-annotator agreement based on 250 verb
errors and 250 correct verbs, randomly selected.
about 74% of all grammatical verb errors in Ta-
ble 1 but the finiteness distinction applies to all
English verbs ? every verb is either finite or non-
finite in a specific syntactic context ? and is also
relevant for the remaining mistakes not addressed
here.
2
4 Annotation for Verb Finiteness
In order to evaluate the quality of the algorithm
for verb finiteness and of the candidate selection
methods, we annotated all verbs ? correct and er-
roneous ? in a random set of 124 documents from
our corpus with the information about verb finite-
ness. We refer to these 124 documents as gold sub-
set. We also annotated erroneous verbs in the re-
maining 1120 documents of the corpus. The anno-
tation was performed by two students with back-
ground in Linguistics. The inter-annotator agree-
ment is shown in Table 3 and is high.
Annotating Verb Errors For each verb error that
was tagged as Tense (TV), Agreement (AGV), and
Form (FV), the annotators marked verb finiteness.
Additionally, the annotators also specified the type
of error (Tense, Agreement, or Form) (Table 4),
since the FCE tags do not always correspond to
the three error types we study here. For exam-
ple, the FV tag may mark errors on finite verbs.
Overall, about 7% of verb errors have to do with
phenomena different from the three verb proper-
ties considered in this work and thus are excluded
from the present study.
Annotating Correct Verbs Correct verbs were
identified in text using an automated proce-
dure that relies on part-of-speech information
(Sec. 5.1). Valid candidates were specified for
verb finiteness. The candidates that were iden-
tified incorrectly due to mistakes by the part-of-
speech tagger were marked as invalid.
5 The Computational Model
The verb error correction problem is formulated
as a classification task in the spirit of the learn-
2
For instance, the missing verb errors (MV, 11.7%) re-
quire an additional step to identify contexts for missing verbs,
and then appropriate verb properties need to be determined
based on verb finiteness.
360
Verb type Example Verb properties
Agreement Tense Form
Finite
?He discussed this with me last week? - Past Simple -
?He discusses this with me every week.? 3rd person,Sing. Present Simple -
Non-finite
?He left without discussing it with me.? - - Gerund
?They let him discuss this with me.? - - Infinitive
?To discuss this now would be ill-advised.? - - to-Infinitive
Table 2: Contexts that license finite and non-finite verbs and the corresponding active properties.
Error on Verb Type Subcategory Example
Finite (67.7%)
Agreement (20%) ?We discusses*/discuss this every time.?
Tense (80%) ?If you buy something, you {would be}*/{will be} happy.?
Non-finite (25.3%)
?If one is famous he has to accept the disadvantages of be*/being famous.? ?I am very
glad {for receiving}*/{to receive} it.?
?They arrived early to organized*/organize everything.?
Other errors (7.0%)
Passive/Active(42.3%) ?Our end-of-conference party {is included}*/includes dinner and dancing.?
Compound (40.7%) ?You ask me for some informations*/information- here they*/it are*/is.?
Other (16.8%) ?Nobody {has to be}*/{should be} late.?
Table 4: Verb error classification based on 4864 mistakes marked as TV, AGV, and FV errors in the FCE corpus.
ing paradigm commonly used for correcting other
ESL errors (Sec. 2), with the exception that the
verb model includes additional components. All
of the components are listed below:
1. Candidate selection (5.1)
2. Verb finiteness prediction (5.2)
3. Feature generation (5.3)
4. Error identification (5.4)
5. Error correction (5.5)
After verb candidates are selected, verb finite-
ness is determined and features are generated for
each candidate. The finiteness prediction is used
in the error identification component. Given the
output of the error identification stage, the corre-
sponding classifiers for each error type are invoked
to propose an appropriate correction.
We split the corpus documents into two equal
parts ? training and test. We chose a train-test split
and not cross-validation, since the FCE data set is
quite large to allow for such a split. The training
data is also used to develop the components for
candidate selection and verb finiteness prediction.
5.1 Candidate Selection
This stage selects the set of verb instances that
are presented as input to the classifier. A verb in-
stance refers to the verb, including its auxiliaries
or the infinitive marker (e.g. ?found?, ?will find?,
?to find?). Candidate selection is a crucial step for
models that correct mistakes on open-class words
because those errors that are missed at this stage
have no chance of being detected. We implement
four candidate selection methods. Method (1) ex-
tracts all verbs heading a verb phrase, as identi-
fied by a shallow parser (Punyakanok and Roth,
2001).
3
Method (2) also includes words tagged
with one of the verb tags: {VB, VBN, VBG,
VBD, VBP, VBZ} predicted by the POS tagger.
4
However, relying on the POS information is not
good enough, since the POS tagger performance
on ESL data is known to be suboptimal (Nagata et
al., 2011). For example, verbs lacking agreement
markers are likely to be mistagged as nouns (Lee
and Seneff, 2008). Methods (3) and (4) address
the problem of pre-processing errors. Method (3)
adds words that are on the list of valid English
verb lemmas; the lemma list is constructed us-
ing a POS-tagged version of the NYT section of
the Gigaword corpus and contains about 2,600 of
frequently-occurring words tagged as VB; for ex-
ample, (3) will add shop but not shopping, but (4)
will add both.
For methods (3) and (4), we developed verb-
Morph,
5
a tool that performs morphological anal-
ysis on verbs and is used to lemmatize verbs and
to generate morphological variants. The module
makes uses of (1) the verb lemma list and (2) a list
of irregular English verbs.
The quality of the candidate selection methods
is evaluated in Table 5 on the gold subset by com-
puting the recall, i.e. the percentage of erroneous
verbs that have been selected as candidates. Meth-
ods that address pre-processing mistakes are able
to recover more erroneous verb candidates in text.
It is also interesting to note that across all methods,
the highest recall is obtained for tense errors. This
suggests that the POS tagger is more prone to fail-
3
http://cogcomp.cs.illinois.edu/demo/shallowparse
4
http://cogcomp.cs.illinois.edu/page/software view/POS
5
The tool and more detail about it can be found at
http://cogcomp.cs.illinois.edu/page/publication view/743
361
Method Recall Recall by error group (%)
(%) Agr. Tense Form
(1) All verb phrases 83.00 86.62 93.55 59.08
(2) + tokens tagged as verbs 91.96 90.30 94.33 87.79
(3) + tokens that are valid
verb lemmas
95.50 95.99 96.46 93.23
(4) + tokens with inflections
that are valid verb lemmas
96.09 96.32 96.62 94.84
Table 5: Candidate selection methods performance.
ure due to errors in agreement and form. The eval-
uation in Table 5 uses recall, as the goal is to assess
the ability of the methods to select erroneous verbs
as candidates. In Sec. 6.1, the contribution of each
method to error identification is evaluated.
5.2 Predicting Verb Finiteness
Predicting verb finiteness is not trivial, as almost
all English verbs can occur in both finite and non-
finite form and the surface forms of a verb in finite
and non-finite form may be the same (see Table 2).
While we cannot learn verb type automatically
due to lack of annotation, we show, however, that,
for the majority of verbs, finiteness can be reliably
predicted using linguistic knowledge. We imple-
ment a decision-list classifier that makes use of
linguistically-motivated rules (Table 6). The algo-
rithm covers about 92% of all verb candidates, ab-
staining on the remaining highly-ambiguous 8%.
The evaluation of the method on the gold sub-
set (last column in Table 6) shows that despite its
simplicity, this method is highly effective: 98% on
correct verbs and over 89% on errors.
5.3 Features
The baseline features are word n-grams in the 4-
word window around the verb instance. Addi-
tional features are intended to characterize a given
error type and are selected based on previous stud-
ies: for Agreement and Form errors, we use a
parser (Klein and Manning, 2003) and define fea-
tures that reflect dependency relations between the
verb and its neighbors. We denote these features
by syntax. Syntactic knowledge via tree patterns
has been shown useful for Agreement mistakes
(Lee and Seneff, 2008). Features for Tense in-
clude temporal adverbs in the sentence and tenses
of other verbs in the sentence and are similar to
the features used in other verb classification tasks
(Reichart and Rappoport, 2010; Lee, 2011; Tajiri
et al., 2012). The features are shown in Table 7.
5.4 Error Identification
The goal of this stage is to identify errors and to
predict their type. We define a linear model where,
given a verb, a weight vector w assigns a score
to each label in the label space {Correct, Form,
Agreement, Tense}. The prediction of the classi-
fier is the label with the highest score.
The baseline error identification model, called
combined, is agnostic to the type of the verb. In
the combined model, for each verb v and label l,
we generate a feature vector, ?(v, l) and the best
label is predicted as
argmax
l
w
T
?(v, l).
The combined model makes use of all the fea-
tures we have defined earlier for each verb.
The type-based model uses the verb finiteness
prediction made by the verb finiteness classifier.
A soft way to use the finiteness prediction is to
add the predicted finiteness value as a feature. The
other ? hard-decision approach ? is to use only
a subset of the features depending on the pre-
dicted finiteness: Agreement and Tense for the fi-
nite verbs, and Form features for non-finite. The
hard-decision type-driven approach defines a fea-
ture vector for a verb based on its type. Thus,
given the verb v and its type t, we define fea-
tures ?(v, t, l) for each label l. Thus, the label is
predicted as
argmax
l
w
T
?(v, t, l).
5.5 Error Correction
The correction module consists of three compo-
nents, one for each type of mistake. Given the
output of the error identification model, the ap-
propriate correction component is run for each in-
stance predicted to be a mistake.
6
The verb finite-
ness prediction is used to select finite instances for
training the Agreement and Tense components and
non-finite ? for the Form component. The label
space for Tense specifies tense and aspect prop-
erties of the English verbs (see Tajiri et al., 2012
for more detail), the Agreement component spec-
ifies the person and number properties, while the
Form component includes the commonly confus-
able non-finite English forms (see Table 2). These
components are trained as multiclass classifiers.
6
We assume that each verb contains at most one mistake.
Less than 1% of all erroneous verbs have more than one error
present.
362
A verb is Non-Finite if any of the following hold: A verb is Finite if any of the following hold Accuracy on
Correct Erroneous
verbs verbs
(1) All verbs identified by shallow parser
98.01 89.4
(1) [numTokens = 2] ? [firstToken = to] (2) can; could
(2) firstToken = be (3) [numTokens = 1] ? [pos ? {V BD, V BP, V BZ}]
(3) [numTokens = 1] ? [pos = V BG] (4) [numTokens = 2] ? [firstToken! = to]
(5) numTokens > 2
Table 6: Algorithm for determining verb type. numTokens denotes the number of tokens in the verb instance, e.g., for the
verb instance ?to go?, numTokens = 2. Verbs not covered by the rules, e.g. those that are not tagged with a verb-related POS
in methods (3) and (4), are not assigned any verb type. The last column shows algorithm accuracy on the gold subset separately
for correct and incorrect verbs.
Agreement Description
(1) subjHead, subjPOS The surface form and the POS tag of the subject head
(2) subjDet {those,this,..} Determiner of the subject phrase
(3) subjDistance Distance between the verb and the subject head
(4) subjNumber {Sing, Pl} Sing ? singular pronouns and nouns; Pl ? plural pronouns and nouns
(5) subjPerson {3rdSing, Not3rdSing, 1stSing} 3rdSing ? she,he,it,singular nouns; Not3rdSing ? we,you,they, plural nouns; 1stSing ? ?I?
(6) conjunctions (1)&(3);(4)&(5)
Tense Description
(1) verb phrase (VP) verb lemma, negation, surface forms and POS tags of all words in the verb phrase
(2) verbs in sentence(4 features) tenses and lemmas of the finite verbs preceding and following the verb instance
(3) time adverbs (2 features) temporal adverb before and after the verb instance
(4) bag-of-words (BOW) (8 features) Includes the following words in the sentence: {if, when, since, then, wish, hope, when, since,
after}
Form Description
(1) closest word surface form, lemma, POS tag, and distance of the closest open-class word to the left of the
verb
(2) governor surface form, POS tag and dependency type of the target
(3) preposition if the verb is preceded by a preposition: preposition itself and the surface form, POS tag and
dependency of the governor of the preposition
(4) pos and lemma POS tag and lemma of the verb and their conjunctions with features in (2) and (3) and word
ngrams
Table 7: Features used, grouped by error type.
6 Experiments
The main goal of this work is to propose a uni-
fied framework for correcting verb mistakes and
to address the specific challenges of the problem.
We thus do not focus on features or on the spe-
cific learning algorithm. Our experimental study
addresses the following research questions:
I. Linguistic questions: (i) candidate selection
methods; (ii) verb finiteness contribution to
error identification
II. Computational Framework: error identifi-
cation vs. correction
III. Gold annotation: (i) using gold candidates
and verb type vs. automatic; (ii) performance
comparison by error type
Learning Framework There is a lot of under-
standing for which algorithmic methods work
best for ESL correction tasks, how they compare
among themselves, and how they compare to n-
gram based methods. Specifically, despite their in-
tuitive appeal, language models were shown to not
work well on these tasks, while the discriminative
learning framework has been shown to be superior
to other approaches and thus is commonly used
for error correction tasks (see Sec. 2). Since we
do not address the algorithmic aspect of the prob-
lem, we refer the reader to Rozovskaya and Roth
(2011) for a discussion of these issues. We train
all our models with the SVM learning algorithm
implemented in JLIS (Chang et al., 2010).
Evaluation We report both Precision/Recall
curves and AAUC (as a summary). Error cor-
rection is generally evaluated using F1 (Dale et
al., 2012); Precision and Recall (Gamon, 2010;
Tajiri et al., 2012); or Average Area Under Curve
(AAUC) (Rozovskaya and Roth, 2011). For a dis-
cussion on these metrics with respect to error cor-
rection tasks, we refer the reader to Rozovskaya
(2013). AAUC (Hanley and McNeil, 1983)) is a
measure commonly used to generate a summary
statistic, computed as an average precision value
over a range of recall points. In this paper, AAUC
is computed over the first 15 recall points:
AAUC =
1
15
?
15
?
i=1
Precision(i).
6.1 Linguistic Questions
Candidate Selection Methods The contribution
of the candidate selection component with respect
to error identification is evaluated in Table 8, us-
ing the methods presented in Sec. 5.1. Overall,
363
Recall of candidate AAUC
selection method (%) Combined Type-based
(1) (83.00) 73.38 79.49
(2) (91.96) 80.36 86.48
(3) (95.50) 81.39 87.05
(4) (96.09) 81.27 86.81
Table 8: Impact of candidate selection methods on error
identification performance. The first column shows the per-
centage of erroneous verbs selected by each method. Type-
based models are discussed in Sec. 6.1.
Correct verbs Erroneous verbs Error rate
Training 41721 1981 4.75%
Test 41836 2014 4.81%
Table 9: Training and test data statistics. Candidates are
selected using method (3).
better performance is achieved by methods with
higher recall, with the exception of method (4); its
performance on error identification is behind that
of method (3), perhaps due to the amount of noise
that is also added. While the difference is small,
method (3) is also simpler than method (4). We
thus use method (3) in the rest of the paper. Table
9 shows the number of verb instances in training
and test selected with this method.
Verb Finiteness Sec. 5.4 presented two ways of
adding verb finiteness: (1) adding the predicted
verb type as a feature and (2) selecting only the
relevant features depending on the finiteness of the
verb. Table 10 shows the results of using verb type
in the error identification stage. While the first
approach does not provide improvement over the
combined model, the second method is very ef-
fective. We conjecture that because verb type pre-
diction is quite accurate, the second, hard-decision
approach is preferred, as it provides knowledge in
a direct way. Henceforth, we will use the second
method in the type-based model.
Fig. 1 compares the performance of the com-
bined and the hard-decision type-based models
shown in Table 10. Precision/Recall curves are
generated by varying the threshold on the confi-
dence of the classifier. This graph reveals the be-
havior of the systems at multiple recall points: we
observe that at every recall point the type-based
classifier has higher precision.
So far, the models used all features defined in
Sec. 5.3. Table 11 reveals that the type-driven
Model AAUC
Combined 81.39
Type-based I (soft) 81.11
Type-based II (hard) 87.05
Table 10: Verb finiteness contribution to error identifi-
cation.
 
70
 
75
 
80
 
85
 
90
 
95  0
 
2
 
4
 
6
 
8
 
10
 
12
 
14
PRECISION
RECAL
L
Comb
ined
Type-
based
Figure 1: Verb finiteness contribution to error identifi-
cation: key result. AAUC shown in Table 10. The combined
model uses no verb type information. In the hard-decision
type-based model, each verb uses the features according to
its finiteness. The differences are statistically significant (Mc-
Nemar?s test, p < 0.0001).
Feature set AAUC
Combined Type-based
Baseline 46.62 49.72
All?Syntax 79.47 84.88
Full feature set 81.39 87.05
Table 11: Verb finiteness contribution to error identifi-
cation for different features.
approach is superior to the combined approach
across different feature sets, and the performance
gap increases with more sophisticated feature sets,
which is to be expected, since more complex fea-
tures are tailored toward relevant verb errors. Fur-
thermore, adding features specific to each error
type significantly improves the performance over
the word n-gram features. The rest of the experi-
ments use all features (denoted Full feature set).
6.2 Identification vs. Correction
After running the error identification component,
we apply the appropriate correction models to
those instances identified as errors. The results
for identification and correction are shown in Ta-
ble 12. The correction models are also finiteness-
aware models trained on the relevant verb in-
stances (finite or non-finite), as predicted by the
verb finiteness classifier.
We evaluate the correction components by fix-
ing a recall point in the error identification stage.
7
We observe the relatively low recall obtained by
the models. Error correction models tend to have
low recall (see, for example, the recent shared
tasks on ESL error correction (Dale and Kilgar-
riff, 2011; Dale et al., 2012; Ng et al., 2013)). The
key reason for the low recall is the error sparsity:
over 95% of verbs are correct, as shown in Table 9.
7
We can increase recall using a different threshold but
higher precision is preferred in error correction tasks.
364
Error type Correction Identification
P R F1 P R F1
Agreement 90.62 9.70 17.52 90.62 9.70 17.52
Tense 60.51 7.47 13.31 86.62 10.70 19.05
Form 81.82 16.34 27.24 83.47 16.67 27.79
Total 71.94 10.24 17.94 85.81 12.22 21.20
Table 12: Performance of the complete model after the
correction stage. The results on Agreement mistakes are the
same, since Agreement errors are always binary decisions,
unlike Tense and Form mistakes.
The only way to improve over this 95% baseline is
by forcing the system to have very good precision
(at the expense of recall). The performance shown
in Table 12 corresponds to an accuracy of 95.60%
in identification (error reduction of 8.7%) and
95.40% in correction (error reduction of 4.5%)
over the baseline of 95.19%.
6.3 Analysis on Gold Data
To further study the impact of each step of the sys-
tem, we analyze our model on the gold subset of
the data. The gold subset contains two additional
pieces of information not available for the rest of
the corpus: gold verb candidates and gold verb
finiteness (Sec. 4). The set contains 7784 gold
verbs, including 464 errors. Experiments are run
in 10-fold cross-validation where on each run 90%
of the documents are used for training and the re-
maining 10% are used for evaluation. The gold
annotation can be used instead of automatic pre-
dictions in two system components: (1) candidate
selection and (2) verb finiteness.
Table 13 shows the performance on error identi-
fication when gold vs. automatic settings are used.
As expected, using the gold verb type is more ef-
fective than using the automatic one, both with au-
tomatic and gold candidates. The same is true for
candidate selection. For instance, the combined
model improves by 14 AAUC points (from 55.90
to 69.86) with gold candidates. These results indi-
cate that candidate selection is an important com-
ponent of the verb error correction system.
Note that compared to the performance on the
entire data set (Table 10), the performance of the
models shown here that use automatic components
is lower, since the training size is smaller. On the
other hand, because of the smaller training size,
the gain due to the type-based approach is larger
on the gold subset (19 vs. 6 AAUC points).
Finally, in Table 14, we evaluate the contribu-
tion of verb finiteness to error identification by er-
ror type. While performance varies by error, it is
clear that all errors benefit from verb typing.
Candidate selection Verb type prediction AAUC
Automatic
None 55.90
Automatic 74.72
Gold 89.45
Gold
None 69.86
Automatic 90.89
Gold 96.42
Table 13: Gold subset: error identification with gold vs.
automatic candidates and finiteness information. Value
None for verb type prediction denotes the combined model.
Error type AAUC
Combined Type-based Type-based
Automatic Gold
Agreement 86.80 88.43 89.21
Tense 18.07 25.62 26.87
Form 97.08 98.23 98.36
Table 14: Gold subset: gold vs. automatic finiteness con-
tribution to error identification by error type.
7 Conclusion
Verb errors are commonly made by ESL writers
but difficult to address due to to their diversity
and the fact that identifying verbs in (noisy) text
may itself be difficult. We develop a linguistically-
inspired approach that first identifies verb candi-
dates in noisy learner text and then makes use
of verb finiteness to identify errors and character-
ize the type of mistake. This is important, since
most errors made by non-native speakers cannot
be identified by considering only closed classes
(e.g., prepositions and articles). Our model inte-
grates a statistical machine learning approach with
a rule-based system that encodes linguistic knowl-
edge to yield the first general correction approach
to verb errors (that is, one that does not assume
prior knowledge of which mistake was made).
This work thus provides a first step in consider-
ing more general algorithmic paradigms for cor-
recting grammatical errors and paves the way for
developing models to address other ?open-class?
mistakes.
Acknowledgments
The authors thank Graeme Hirst, Julia Hockenmaier, Mark
Sammons, and the anonymous reviewers for their helpful
feedback. This work was done while the first and the third
authors were at the University of Illinois. This material is
based on research sponsored by DARPA under agreement
number FA8750-13-2-0008 and by the Army Research Lab-
oratory (ARL) under agreement W911NF-09-2-0053. Any
opinions, findings, conclusions or recommendations are those
of the authors and do not necessarily reflect the view of the
agencies.
365
References
M. Banko and E. Brill. 2001. Scaling to very very
large corpora for natural language disambiguation.
In Proceedings of 39th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 26?33,
Toulouse, France, July.
A. Carlson and I. Fette. 2007. Memory-based context-
sensitive spelling correction at web scale. In Pro-
ceedings of the IEEE International Conference on
Machine Learning and Applications (ICMLA).
A. Carlson, J. Rosen, and D. Roth. 2001. Scaling up
context sensitive text correction. In Proceedings of
the National Conference on Innovative Applications
of Artificial Intelligence (IAAI), pages 45?50.
M. Chang, V. Srikumar, D. Goldwasser, and D. Roth.
2010. Structured output learning with indirect su-
pervision. In Proc. of the International Conference
on Machine Learning (ICML).
D. Dahlmeier and H. T. Ng. 2011. Grammatical er-
ror correction with alternating structure optimiza-
tion. In Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics: Hu-
man Language Technologies, pages 915?923, Port-
land, Oregon, USA, June. Association for Computa-
tional Linguistics.
R. Dale and A. Kilgarriff. 2011. Helping Our Own:
The HOO 2011 pilot shared task. In Proceedings of
the 13th European Workshop on Natural Language
Generation.
R. Dale, I. Anisimoff, and G. Narroway. 2012. A
report on the preposition and determiner error cor-
rection shared task. In Proc. of the NAACL HLT
2012 Seventh Workshop on Innovative Use of NLP
for Building Educational Applications, Montreal,
Canada, June. Association for Computational Lin-
guistics.
G. Dalgish. 1985. Computer-assisted ESL research.
CALICO Journal, 2(2).
R. De Felice and S. Pulman. 2008. A classifier-based
approach to preposition and determiner error correc-
tion in L2 English. In Proceedings of the 22nd In-
ternational Conference on Computational Linguis-
tics (Coling 2008), pages 169?176, Manchester, UK,
August.
M. Gamon, J. Gao, C. Brockett, A. Klementiev,
W. Dolan, D. Belenko, and L. Vanderwende. 2008.
Using contextual speller techniques and language
modeling for ESL error correction. In Proceedings
of IJCNLP.
M. Gamon, C. Leacock, C. Brockett, W. B. Dolan,
J. Gao, D. Belenko, and A. Klementiev. 2009. Us-
ing statistical techniques and web search to correct
ESL errors. CALICO Journal, Special Issue on Au-
tomatic Analysis of Learner Language, 26(3):491?
511.
M. Gamon. 2010. Using mostly native data to correct
errors in learners? writing. In NAACL, pages 163?
171, Los Angeles, California, June.
A. R. Golding and D. Roth. 1996. Applying Winnow
to context-sensitive spelling correction. In Proc. of
the International Conference on Machine Learning
(ICML), pages 182?190.
A. R. Golding and D. Roth. 1999. A Winnow
based approach to context-sensitive spelling correc-
tion. Machine Learning, 34(1-3):107?130.
N. Han, M. Chodorow, and C. Leacock. 2006. De-
tecting errors in English article usage by non-native
speakers. Journal of Natural Language Engineer-
ing, 12(2):115?129.
J. Hanley and B. McNeil. 1983. A method of com-
paring the areas under receiver operating character-
istic curves derived from the same cases. Radiology,
148(3):839?843.
E. Izumi, K. Uchimoto, T. Saiga, T. Supnithi, and
H. Isahara. 2003. Automatic error detection in
the Japanese learners? English spoken data. In The
Companion Volume to the Proceedings of 41st An-
nual Meeting of the Association for Computational
Linguistics, pages 145?148, Sapporo, Japan, July.
T.-H. Kao, Y.-W. Chang, H. w. Chiu, T-.H. Yen, J. Bois-
son, J. c. Wu, and J.S. Chang. 2013. Conll-2013
shared task: Grammatical error correction nthu sys-
tem description. In Proceedings of the Seventeenth
Conference on Computational Natural Language
Learning: Shared Task, pages 20?25, Sofia, Bul-
garia, August. Association for Computational Lin-
guistics.
D. Klein and C. D. Manning. 2003. Fast exact in-
ference with a factored model for natural language
parsing. In Advances in Neural Information Pro-
cessing Systems 15 NIPS, pages 3?10. MIT Press.
C. Leacock, M. Chodorow, M. Gamon, and J. Tetreault.
2010. Automated Grammatical Error Detection for
Language Learners. Morgan and Claypool Publish-
ers.
J. Lee and S. Seneff. 2008. Correcting misuse of verb
forms. In ACL, pages 174?182, Columbus, Ohio,
June. Association for Computational Linguistics.
J. Lee. 2011. Verb tense generation. Social and Be-
havioral Sciences, 27:122?130.
R. Nagata, E. Whittaker, and V. Sheinman. 2011. Cre-
ating a manually error-tagged and shallow-parsed
learner corpus. In ACL, pages 1210?1219, Portland,
Oregon, USA, June. Association for Computational
Linguistics.
H. T. Ng, S. M. Wu, Y. Wu, Ch. Hadiwinoto, and
J. Tetreault. 2013. The CoNLL-2013 shared task
on grammatical error correction. In Proc. of the
Seventeenth Conference on Computational Natural
366
Language Learning. Association for Computational
Linguistics.
V. Punyakanok and D. Roth. 2001. The use of classi-
fiers in sequential inference. In The Conference on
Advances in Neural Information Processing Systems
(NIPS), pages 995?1001. MIT Press.
R. Quirk, S. Greenbaum, G. Leech, and J. Svartvik.
1985. A Comprehensive Grammar of the English
Language. Longman, New York.
A. Radford. 1988. Transformational Grammar. Cam-
bridge University Press.
R. Reichart and A. Rappoport. 2010. Tense sense
disambiguation: A new syntactic polysemy task.
In Proceedings of the 2010 Conference on Empiri-
cal Methods in Natural Language Processing, pages
325?334, Cambridge, MA, October. Association for
Computational Linguistics.
A. Rozovskaya and D. Roth. 2010a. Annotating ESL
errors: Challenges and rewards. In Proceedings of
the NAACL Workshop on Innovative Use of NLP for
Building Educational Applications.
A. Rozovskaya and D. Roth. 2010b. Training
paradigms for correcting errors in grammar and us-
age. In Proceedings of the NAACL-HLT.
A. Rozovskaya and D. Roth. 2011. Algorithm selec-
tion and model adaptation for esl correction tasks.
In ACL, Portland, Oregon, 6. Association for Com-
putational Linguistics.
A. Rozovskaya, K.-W. Chang, M. Sammons, and
D. Roth. 2013. The University of Illinois system
in the CoNLL-2013 shared task. In CoNLL Shared
Task.
A. Rozovskaya. 2013. Automated Methods for Text
Correction. Ph.D. thesis.
T. Tajiri, M. Komachi, and Y. Matsumoto. 2012. Tense
and aspect error correction for esl learners using
global context. In Proceedings of the 50th Annual
Meeting of the Association for Computational Lin-
guistics (Volume 2: Short Papers), pages 198?202,
Jeju Island, Korea, July. Association for Computa-
tional Linguistics.
J. Tetreault, J. Foster, and M. Chodorow. 2010. Us-
ing parse features for preposition selection and error
detection. In ACL.
H. Yannakoudakis, T. Briscoe, and B. Medlock. 2011.
A new dataset and method for automatically grading
esol texts. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 180?189, Portland, Oregon, USA, June.
Association for Computational Linguistics.
367
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 154?162,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Training Paradigms for Correcting Errors in Grammar and Usage
Alla Rozovskaya and Dan Roth
University of Illinois at Urbana-Champaign
Urbana, IL 61801
{rozovska,danr}@illinois.edu
Abstract
This paper proposes a novel approach to the
problem of training classifiers to detect and
correct grammar and usage errors in text by
selectively introducing mistakes into the train-
ing data. When training a classifier, we would
like the distribution of examples seen in train-
ing to be as similar as possible to the one seen
in testing. In error correction problems, such
as correcting mistakes made by second lan-
guage learners, a system is generally trained
on correct data, since annotating data for train-
ing is expensive. Error generation methods
avoid expensive data annotation and create
training data that resemble non-native data
with errors.
We apply error generation methods and train
classifiers for detecting and correcting arti-
cle errors in essays written by non-native En-
glish speakers; we show that training on data
that contain errors produces higher accuracy
when compared to a system that is trained on
clean native data. We propose several train-
ing paradigms with error generation and show
that each such paradigm is superior to training
a classifier on native data. We also show that
the most successful error generation methods
are those that use knowledge about the arti-
cle distribution and error patterns observed in
non-native text.
1 Introduction
This paper considers the problem of training clas-
sifiers to detect and correct errors in grammar and
word usage in text. Both native and non-native
speakers make a variety of errors that are not always
easy to detect. Consider, for example, the problem
of context-sensitive spelling correction (e.g., (Gold-
ing and Roth, 1996; Golding and Roth, 1999; Carl-
son et al, 2001)). Unlike spelling errors that result in
non-words and are easy to detect, context-sensitive
spelling correction task involves correcting spelling
errors that result in legitimate words, such as confus-
ing peace and piece or your and you?re. The typical
training paradigm for these context-sensitive ambi-
guities is to use text assumed to be error free, replac-
ing each target word occurrence (e.g. peace) with a
confusion set consisting of, say {peace, piece}, thus
generating both positive and negative examples, re-
spectively, from the same context.
This paper proposes a novel error generation ap-
proach to the problem of training classifiers for the
purpose of detecting and correcting grammar and
usage errors in text. Unlike previous work (e.g.,
(Sjo?bergh and Knutsson, 2005; Brockett et al, 2006;
Foster and Andersen, 2009)), we selectively intro-
duce mistakes in an appropriate proportion. In par-
ticular, to create training data that closely resemble
text with naturally occurring errors, we use error fre-
quency information and error distribution statistics
obtained from corrected non-native text. We apply
the method to the problem of detecting and correct-
ing article mistakes made by learners of English as
a Second Language (ESL).
The problem of correcting article errors is gener-
ally viewed as that of article selection, cast as a clas-
sification problem and is trained as described above:
a machine learning algorithm is used to train a clas-
sifier on native English data, where the possible se-
lections are used to generate positive and negative
154
examples (e.g., (Izumi et al, 2003; Han et al, 2006;
De Felice and Pulman, 2008; Gamon et al, 2008)).
The classifier is then applied to non-native text to
predict the correct article in context. But the article
correction problem differs from the problem of ar-
ticle selection in that we know the original (source)
article that the writer used. When proposing a cor-
rection, we would like to use information about the
original article. One reason for this is that about 90%
of articles are used correctly by ESL learners; this is
higher than the performance of state-of-the-art clas-
sifiers for article selection. Consequently, not us-
ing the writer?s article, when making a prediction,
may result in making more mistakes than there are
in the data. Another reason is that statistics on ar-
ticle errors (e.g., (Han et al, 2006; Lee and Sen-
eff, 2008)) and in the annotation performed for the
present study reveal that non-native English speak-
ers make article mistakes in a consistent manner.
The system can consider the article used by the
writer at evaluation time, by proposing a correction
only when the confidence of the classifier is high
enough, but the article cannot be used in training
if the classifier is trained on clean native data that
do not have errors. Learning Theory says that the
distribution of examples seen in testing should be
as similar as possible to the one seen in training, so
one would like to train on errors similar to those ob-
served in testing. Ideally, we would like to train us-
ing corrected non-native text. In that case, the orig-
inal article of the writer can be used as a feature for
the classifier and the correct article, as judged by
a native English speaker, will be viewed as the la-
bel. However, obtaining annotated data for training
is expensive and, since the native training data do
not contain errors, we cannot use the writer?s article
as a feature for the classifier.
This paper compares the traditional training
paradigm that uses native data to training paradigms
that use data with artificial mistakes. We propose
several methods of generating mistakes in native
training data and demonstrate that they outperform
the traditional training paradigm. We also show that
the most successful error generation methods use
knowledge about the article distribution and error
patterns observed in the ESL data.
The rest of the paper is organized as follows.
First, we discuss the baseline on the error correc-
tion task and show why the baselines used in selec-
tion tasks are not relevant for the error correction
task. Next, we describe prior work in error genera-
tion and show the key difference of our approach.
Section 4 presents the ESL data that we use and
statistics on article errors. Section 5 describes train-
ing paradigms that employ error generation. In Sec-
tions 6 and 7 we present the results and discuss the
results. The key findings are summarized in Table 7
in Section 6. We conclude with a brief discussion of
directions for future work.
2 Measuring Success in Error Correction
Tasks
The distinction between the selection and the error
correction tasks alluded to earlier is important not
only for training but also in determining an appro-
priate evaluation method.
The standard baseline used in selection tasks is
the relative frequency of the most common class.
For example, in word sense disambiguation, the
baseline is the most frequent sense. In the task
of article selection, the standard baseline used is
to predict the article that occurs most frequently in
the data (usually, it is the zero article, whose fre-
quency is 60-70%). In this context, the performance
of a state-of-the-art classifier (Knight and Chander,
1994; Minnen et al, 2000; Turner and Charniak,
2007; Gamon et al, 2008) whose accuracy is 85-
87% is a significant improvement over the base-
line. The majority has been used as the baseline also
in the context-sensitive spelling task (e.g., (Golding
and Roth, 1999)).
However, in article correction, spelling correc-
tion, and other text correction applications the split
of the classes is not an appropriate baseline since the
majority of the words in the confusion set are used
correctly in the text. Han et al (2006) report an av-
erage error rate of 13% on article data from TOEFL
essays, which gives a baseline of 87%, versus the
baseline of 60-70% used in the article selection task.
Statistics on article mistakes in our data suggest a
baseline of about 90%, depending on the source lan-
guage of the writer. So the real baseline on the task
is ?do nothing?. Therefore, to determine the base-
line for a correction task, one needs to consider the
error rate in the data.
155
Using the definitions of precision and recall and
the ?real? baseline, we can also relate the resulting
accuracy of the classifier to the precision and recall
on an error correction task as follows: Let P and R
denote the precision and recall, respectively, of the
system on an error correction task, and Base denote
the error rate in the data. Then the task baseline (i.e.,
accuracy of the data before running the system) is:
Baseline = 1?Base
It can be shown that the error rate after running the
classifier is:
Error =
Base ? (P + R? 2RP )
P
It follows that the accuracy of the system on the task
is 1? Error.
For example, we can obtain a rough estimate on
the accuracy of the system in Han et al (2006), us-
ing precision and recall numbers by error type. Ex-
cluding the error type of category other, we can esti-
mate that Base = 0.1, so the baseline is 0.9, average
precision and recall are 0.85 and 0.25, respectively,
and the resulting overall accuracy of the system is
92.2%.
3 Related Work
3.1 Generating Errors in Text
In text correction, adding mistakes in training has
been explored before. Although the general ap-
proach has been to produce errors similar to those
observed in the data to be corrected, mistakes were
added in an ad-hoc way, without respecting the er-
ror frequencies and error patterns observed in non-
native text. Izumi et al (2003) train a maxi-
mum entropy model on error-tagged data from the
Japanese Learners of English corpus (JLE, (Izumi et
al., 2004)) to detect 8 error types in the same cor-
pus. They show improvement when the training set
is enhanced with sentences from the same corpus
to which artificial article mistakes have been added.
Though it is not clear in what proportion mistakes
were added, it is also possible that the improvement
was due to a larger training set. Foster and Ander-
sen (2009) attempt to replicate naturally occurring
learner mistakes in the Cambridge Learner Corpus
(CLC)1, but show a drop in accuracy when the orig-
inal error-tagged data in training are replaced with
corrected CLC sentences containing artificial errors.
Brockett et al (2006) generate mass noun er-
rors in native English data using relevant exam-
ples found in the Chinese Learners English Cor-
pus (CLEC, (Gui and Yang, 2003)). Training data
consist of an equal number of correct and incor-
rect sentences. Sjo?bergh and Knutsson (2005) in-
troduce split compound and agreement errors into
native Swedish text: agreement errors are added in
every sentence and for compound errors, the train-
ing set consists of an equal number of negative and
positive examples. Their method gives higher recall
at the expense of lower precision compared to rule-
based grammar checkers.
To sum up, although the idea of using data with ar-
tificial mistakes is not new, the advantage of training
on such data has not been investigated. Moreover,
training on error-tagged data is currently unrealistic
in the majority of error correction scenarios, which
suggests that using text with artificial mistakes is the
only alternative to using clean data. However, it has
not been shown whether training on data with artifi-
cial errors is beneficial when compared to utilizing
clean data. More importantly, error statistics have
not been considered for error correction tasks. Lee
and Seneff (2008) examine statistics on article and
preposition mistakes in the JLE corpus. While they
do not suggest a specific approach, they hypothesize
that it might be helpful to incorporate this knowl-
edge into a correction system that targets these two
language phenomena.
3.2 Approaches to Detecting Article Mistakes
Automated methods for detecting article mistakes
generally use a machine learning algorithm. Ga-
mon et al (2008) use a decision tree model and a
5-gram language model trained on the English Giga-
word corpus (LDC2005T12) to correct errors in En-
glish article and preposition usage. Han et al (2006)
and De Felice and Pulman (2008) train a maximum
entropy classifier. Yi et al (2008) propose a web
count-based system to correct determiner errors. In
the above approaches, the classifiers are trained on
native data. Therefore the classifiers cannot use the
1http://www.cambridge.org/elt
156
original article that the writer used as a feature. Han
et al (2006) use the source article at evaluation time
and propose a correction only when the score of the
classifier is high enough, but the source article is not
used in training.
4 Article Errors in ESL Data
Article errors are one of the most common mistakes
that non-native speakers make, especially those
whose native language does not have an article sys-
tem. For example, Han et al (2006) report that in
the annotated TOEFL data by Russian, Chinese, and
Japanese speakers 13% of all noun phrases have an
incorrect article. It is interesting to note that article
errors are present even with very advanced speakers.
While the TOEFL data include essays by students of
different proficiency levels, we use data from very
advanced learners and find that error rates on articles
are similar to those reported by Han et al (2006).
We use data from speakers of three first language
backgrounds: Chinese, Czech, and Russian. None
of these languages has an article system. The Czech
and the Russian data come from the ICLE corpus
(Granger et al, 2002), which is a collection of es-
says written by advanced learners of English. The
Chinese data is a part of the CLEC corpus that con-
tains essays by students of all levels of proficiency.
4.1 Data Annotation
A portion of data for each source language was cor-
rected and error-tagged by native speakers. The an-
notation was performed at the sentence level: a sen-
tence was presented to the annotator in the context
of the entire essay. Essay context can become nec-
essary, when an article is acceptable in the context
of a sentence, but is incorrect in the context of the
essay. Our goal was to correct all article errors, in-
cluding those that, while acceptable in the context of
the sentence, were not correct in the context of the
essay. The annotators were also encouraged to pro-
pose more than one correction, as long as all of their
suggestions were consistent with the essay context.
The annotators were asked to correct all mistakes
in the sentence. The annotation schema included
the following error types: mistakes in article and
preposition usage, errors in noun number, spelling,
verb form, and word form2. All other corrections
were marked as word replacement, word deletion,
and word insertion. For details about annotation and
data selection, please refer to the companion paper
(Rozovskaya and Roth, 2010).
4.2 Statistics on Article Errors
Traditionally, three article classes are distinguished:
the, a(an)3 and None (no article). The training and
the test data are thus composed of two types of
events:
1. All articles in the data
2. Spaces in front of a noun phrase if that noun
phrase does not start with an article. To identify
the beginning of a noun phrase, we ran a part-
of-speech tagger and a phrase chunker4 and ex-
cluded all noun phrases not headed5 by a per-
sonal or demonstrative pronoun.
Table 1 shows the size of the test data by source
language, proportion of errors and distribution of ar-
ticle classes before and after annotation and com-
pares these distributions to the distribution of articles
in English Wikipedia. The distribution before anno-
tation shows statistics on article usage by the writers
and the distribution after annotation shows statistics
after the corrections made by the annotators were
applied. As the table shows, the distribution of arti-
cles is quite different for native data (Wikipedia) and
non-native text. In particular, non-native data have a
lower proportion of the.
The annotation statistics also reveal that learn-
ers do not confuse articles randomly. From Table
2, which shows the distribution of article errors by
type, we observe that the majority of mistakes are
omissions and extraneous articles. Table 3 shows
statistics on corrections by source and label, where
source refers to the article used by the writer, and
label refers to the article chosen by the annotator.
Each entry in the table indicates Prob(source =
2Our classification, was inspired by the classification pre-
sented in Tetreault and Chodorow (2008)
3Henceforth, we will use a to refer to both a and an
4The tagger and the chunker are available at http://
L2R.cs.uiuc.edu/?cogcomp/software.php
5We assume that the last word of the noun phrase is its head.
157
Source Number of Proportion of Errors Article Classes
language test examples errors total distribution a the None
Chinese 1713 9.2% 158
Before annotation 8.5 28.2 63.3
After annotation 9.9 24.9 65.2
Czech 1061 9.6% 102
Before annotation 9.1 22.9 68.0
After annotation 9.9 22.3 67.8
Russian 2146 10.4% 224
Before annotation 10.5 21.7 67.9
After annotation 12.5 20.1 67.4
English Wikipedia 9.6 29.1 61.4
Table 1: Statistics on articles in the annotated data before and after annotation.
Source Proportion of Errors total Errors by Type
language errors in the data Extraneous Missing a Missing the Confusion
Chinese 9.2% 158 57.0% 13.3% 22.8% 7.0%
Czech 9.6% 102 45.1% 14.7% 33.3% 6.9%
Russian 10.4% 224 41.5% 20.1% 25.5% 12.3%
Table 2: Distribution of article errors in the annotated data by error type. Extraneous refers to using a or the where
None (no article) is correct. Confusion is using a instead of the or vice versa.
Label Source Source
language a the None
a
Chinese 81.7% 5.9% 12.4%
Czech 81.0% 4.8% 14.3%
Russian 75.3% 7.9% 16.9%
the
Chinese 0.2% 91.3% 8.5%
Czech 0.9% 84.7% 14.4%
Russian 1.9% 84.9% 13.2%
None
Chinese 0.6% 7.4%% 92.0%
Czech 1.3% 5.2% 93.6%
Russian 1.0% 5.4%% 93.6%
Table 3: Statistics on article corrections by the original
article (source) and the annotator?s choice (label). Each
entry in the table indicates Prob(source = s|label = l)
for each article pair.
s|label = l) for each article pair. We can also ob-
serve specific error patterns. For example, the is
more likely than a to be used superfluously.
5 Introducing Article Errors into Training
Data
This section describes experiments with error gener-
ation methods. We conduct four sets of experiments.
Each set differs in how article errors are generated in
the training data. We now give a description of error
generation paradigms in each experimental set.
5.1 Methods of error generation
We refer to the article that the writer used in the ESL
data as source, and label refers to the article that
the annotator chose. Similarly, when we introduce
errors into the training data, we refer to the original
article as label and to the replacement as source.
This is because the original article is the correct
article choice, and the replacement that the classifier
will see as a feature can be an error. We call this
feature source feature. In other words, both for
training (native data) and test (ESL data), source
denotes the form that the classifier sees as a feature
(which could be an error) and label denotes the
correct article. Below we describe how errors are
generated in each set of experiments.
Method 1: General With probability x each ar-
ticle in the training data is replaced with
a different article uniformly at random, and
with probability (1 ? x) it remains un-
changed. We build six classifiers, where x
? {5%, 10%, 12%, 14%, 16%, 18%}. We call
this method general since it uses no informa-
tion about article distribution in the ESL data.
Method 2: ArticleDistrBeforeAnnot We use the
distribution of articles in the ESL data before
the annotation to change the distribution of ar-
ticles in the training. Specifically, we change
the articles so that their distribution approxi-
mates the distribution of articles in the ESL
data. For example, the relative frequency of
the in English Wikipedia data is 29.1%, while
in the writing by Czech speakers it is 22.3%.
It should be noted that this method changes
the distribution only of source articles, but the
158
distribution of labels is not affected. An ad-
ditional constraint that we impose is the mini-
mum error rate r for each article class, so that
Prob(s|l) ? r ?l ? labels. In this fashion, for
each source language we train four classifiers,
where we use article distribution from Chinese,
Czech, and Russian, and where we set the min-
imum error rate r to be ? {2%, 3%, 4%, 5%}.
Method 3: ArticleDistrAfterAnnot This method
is similar to the one above but we use the dis-
tribution of articles in the ESL data after the
corrections have been made by the annotators.
Method 4: ErrorDistr This method uses informa-
tion about error patterns in the annotated ESL
data. For example, in the Czech annotated sub-
corpus, label the corresponds to source the in
85% of the cases and corresponds to source
None in 14% of the cases. In other words, in
14% of the cases where the article the should
have been used, the writer used no article at all.
Thus, with probability 14% we change the in
the training data to None.
6 Experimental Results
In this section, we compare the quality of the sys-
tem trained on clean native English data to the qual-
ity of the systems trained on data with errors. The
errors were introduced into the training data using
error generation methods presented in Section 5.
In each training paradigm, we follow a discrimi-
native approach, using an online learning paradigm
and making use of the Averaged Perceptron Al-
gorithm (Freund and Schapire, 1999) implemented
within the Sparse Network of Winnow framework
(Carlson et al, 1999) ? we use the regularized
version in Learning Based Java6 (LBJ, (Rizzolo
and Roth, 2007)). While classical Perceptron
comes with generalization bound related to the mar-
gin of the data, Averaged Perceptron also comes
with a PAC-like generalization bound (Freund and
Schapire, 1999). This linear learning algorithm is
known, both theoretically and experimentally, to
be among the best linear learning approaches and
is competitive with SVM and Logistic Regression,
6LBJ code is available at http://L2R.cs.uiuc.edu/
?cogcomp/asoftware.php?skey=LBJ
while being more efficient in training. It also has
been shown to produce state-of-the-art results on
many natural language applications (Punyakanok et
al., 2008).
Since the methods of error generation described in
Section 5 rely on the distribution of articles and ar-
ticle mistakes and these statistics are specific to the
first language of the writer, we conduct evaluation
separately for each source language. Thus, for each
language group, we train five system types: one sys-
tem is trained on clean English data without errors
(the same classifier for the three language groups)
and four systems are trained on data with errors,
where errors are produced using the four methods
described in Section 5. Training data are extracted
from English Wikipedia.
All of the five systems employ the same set of fea-
tures based on three tokens to the right and to the left
of the target article. For each context word, we use
its relative position, its part-of-speech tag and the
word token itself. We also use the head of the noun
phrase and the conjunctions of the pairs and triples
of the six tokens and their part-of-speech tags7. In
addition to these features, the classifiers trained on
data with errors also use the source article as a fea-
ture. The classifier that is trained on clean English
data cannot use the source feature, since in training
the source always corresponds to the label. By con-
trast, when the training data contain mistakes, the
source is not always the same as the label, the situa-
tion that we also have with the test (ESL) data.
We refer to the classifier trained on clean data
as TrainClean. We refer to the classifiers trained
on data with mistakes as TWE (TrainWithErrors).
There are four types of TWE systems for each lan-
guage group, one for each of the methods of error
generation described in Section 5. All results are the
averaged results of training on three random sam-
ples from Wikipedia with two million training ex-
amples on each round. All five classifiers are trained
on exactly the same set of Wikipedia examples, ex-
cept that we add article mistakes to the data used
by the TWE systems. The TrainClean system
achieves an accuracy of 87.10% on data from En-
glish Wikipedia. This performance is state-of-the-
7Details about the features are given in the paper?s web page,
accessible from http://L2R.cs.uiuc.edu/?cogcomp/
159
art compared to other systems reported in the lit-
erature (Knight and Chander, 1994; Minnen et al,
2000; Turner and Charniak, 2007; Han et al, 2006;
De Felice and Pulman, 2008). The best results
of 92.15% are reported by De Felice and Pulman
(2008). But their system uses sophisticated syntac-
tic features and they observe that the parser does not
perform well on non-native data.
As mentioned in Section 4, the annotation of the
ESL data consisted of correcting all errors in the sen-
tence. We exclude from evaluation examples that
have spelling errors in the 3-word window around
the target article and errors on words that immedi-
ately precede or immediately follow the article, as
such examples would obscure the evaluation of the
training paradigms.
Tables 4, 5 and 6 show performance by language
group. The tables show the accuracy and the er-
ror reduction on the test set. The results of systems
TWE (methods 2 and 3) that use the distribution of
articles before and after annotation are merged and
appear as ArtDistr in the tables, since, as shown
in Table 1, these distributions are very similar and
thus produce similar results. Each table compares
the performance of the TrainClean system to the
performance of the four systems trained on data with
errors.
For all language groups, all classifiers of type
TWE outperform the TrainClean system. The
reduction in error rate is consistent when the TWE
classifiers are compared to the TrainClean system.
Table 7 shows results for all three languages, com-
paring for each language group the TrainClean
classifier to the best performing system of type
TWE.
Training Errors in Accuracy Error
paradigm training reduction
TrainClean 0.0% 91.85% -2.26%
TWE(General) 10.0% 92.57% 6.78%
TWE(ArtDistr) 13.2% 92.67% 8.33%
TWE(ErrorDistr) 9.2% 92.31% 3.51%
Baseline 92.03%
Table 4: Chinese speakers: Performance of the
TrainClean system (without errors in training) and of
the best classifiers of type TWE. Rows 2-4 show the
performance of the systems trained with error generation
methods described in 5. Error reduction denotes the per-
centage reduction in the number of errors when compared
to the number of errors in the ESL data.
Training Errors in Accuracy Error
paradigm training reduction
TrainClean 0.0% 91.82% 10.31%
TWE(General) 18.0% 92.22% 14.69%
TWE(ArtDistr) 21.6% 92.00% 12.28%
TWE(ErrorDistr) 10.2% 92.15% 13.93%
Baseline 90.88%
Table 5: Czech speakers: Performance of the
TrainClean system (without errors in training) and of
the best classifiers of type TWE. Rows 2-4 show the
performance of the systems trained with error generation
methods described in 5. Error reduction denotes the per-
centage reduction in the number of errors when compared
to the number of errors in the ESL data.
Training Errors in Accuracy Error
paradigm training reduction
TrainClean 0.0% 90.62% 5.92%
TWE(General) 14.0% 91.25% 12.24%
TWE(ArtDistr) 18.8% 91.52% 14.94%
TWE(ErrorDistr) 10.7% 91.63% 16.05%
Baseline 90.03%
Table 6: Russian speakers: Performance of the
TrainClean system (without errors in training) and of
the best classifiers of type TWE. Rows 2-4 show the
performance of the systems trained with error generation
methods described in 5. Error reduction denotes the per-
centage reduction in the number of errors when compared
to the number of errors in the ESL data.
7 Discussion
As shown in Section 6, training a classifier on
data that contain errors produces better results when
compared to the TrainClean classifier trained on
clean native data. The key results for all language
groups are summarized in Table 7. It should be
noted that the TrainClean system also makes use
of the article chosen by the author through a confi-
dence threshold8; it prefers to keep the article chosen
by the user. The difference is that the TrainClean
system does not consider the author?s article in train-
ing. The results of training with error generation
are better, which shows that training on automati-
cally corrupted data indeed helps. While the per-
formance is different by language group, there is an
observable reduction in error rate for each language
group when TWE systems are used compared to
TrainClean approach. The reduction in error rate
8The decision threshold is found empirically on a subset of
the ESL data set aside for development.
160
achieved by the best performing TWE system when
compared to the error rate of the TrainClean sys-
tem is 10.06% for Chinese, 4.89% for Czech and
10.77% for Russian, as shown in Table 7. We also
note that the best performing TWE systems for Chi-
nese and Russian speakers are those that rely on the
distribution of articles (Chinese) and the distribution
of errors (Russian), but for Czech it is the General
TWE system that performs the best, maybe because
we had less data for Czech speakers, so their statis-
tics are less reliable.
There are several additional observations to be
made. First, training paradigms that use error gen-
eration methods work better than the training ap-
proach of using clean data. Every system of type
TWE outperforms the TrainClean system, as ev-
idenced by Tables 4, 5, and 6. Second, the propor-
tion of errors in the training data should be similar
to the error rate in the test data. The proportion of
errors in training is shown in Tables 4, 5 and 6 in col-
umn 2. Furthermore, TWE systems ArtDistr and
ErrorDistr that use specific knowledge about arti-
cle and error distributions, respectively, work better
for Russian and Chinese groups than the General
method that adds errors to the data uniformly at ran-
dom. Since ArtDistr and ErrorDistr depend on
the statistics of learner mistakes, the success of the
systems that use these methods for error generation
depends on the accuracy of these statistics, and we
only have between 100 and 250 errors for each lan-
guage group. It would be interesting to see whether
better results can be achieved with these methods if
more annotated data are available. Finally, for the
same reason, there is no significant difference in the
performance of methods ArtDistrBeforeAnnot
and ArtDistrAfterAnnot: With small sizes of an-
notated data there is no difference in article distribu-
tions before and after annotation.
8 Conclusion and Future Work
We have shown that error correction training
paradigms that introduce artificial errors are supe-
rior to training classifiers on clean data. We pro-
posed several methods of error generation that ac-
count for error frequency information and error dis-
tribution statistics from non-native text and demon-
strated that the methods that work best are those that
Source Accuracy Error
language Train TWE reduction
Clean
Chinese 91.85% 92.67% 10.06%
Czech 91.82% 92.22% 4.89%
Russian 90.62% 91.63% 10.77%
Table 7: Improvement due to training with errors. For
each source language, the last column of the table shows
the reduction in error rate achieved by the best perform-
ing TWE system when compared to the error rate of the
TrainClean system. The error rate for each system is
computed by subtracting the accuracy achieved by the
system, as shown in columns 2 and 3.
result in a training corpus that statistically resembles
the non-native text. Adding information about arti-
cle distribution in non-native data and statistics on
specific error types is even more helpful.
We have also argued that the baselines used ear-
lier in the relevant literature ? all based on the major-
ity of the most commonly used class ? suit selection
tasks, but are inappropriate for error correction. In-
stead, the error rate in the data should be taken into
account when determining the baseline.
The focus of the present study was on training
paradigms. While it is quite possible that the article
correction system presented here can be improved
? we would like to explore improving the system
by using a more sophisticated feature set ? we be-
lieve that the performance gap due to the error driven
training paradigms shown here will remain. The rea-
son is that even with better features, some of the fea-
tures that hold in the native data will not be active in
in the ESL writing.
Finally, while this study focused on the problem
of correcting article mistakes, we plan to apply the
proposed training paradigms to similar text correc-
tion problems.
Acknowledgments
We thank Nick Rizzolo for helpful discussions on
LBJ. We also thank Peter Chew and the anonymous
reviewers for their insightful comments. This re-
search is partly supported by a grant from the U.S.
Department of Education.
161
References
C. Brockett, W. B. Dolan, and M. Gamon. 2006. Cor-
recting ESL errors using phrasal SMT techniques. In
Proceedings of the 21st COLING and the 44th ACL,
Sydney.
A. Carlson, C. Cumby, J. Rosen, and D. Roth. The
SNoW learning architecture. Technical report.
A. J. Carlson and J. Rosen and D. Roth. 2001. Scaling
Up Context Sensitive Text Correction. IAAI, 45?50.
R. De Felice and S. Pulman. 2008. A Classifier-Based
Approach to Preposition and Determiner Error Correc-
tion in L2 English. In Proceedings of COLING-08.
J. Foster and ?. Andersen. 2009. GenERRate: Gener-
ating Errors for Use in Grammatical Error Detection.
In Proceedings of the NAACL Workshop on Innovative
Use of NLP for Building Educational Applications.
Y. Freund and R. E. Schapire. 1999. Large margin clas-
sification using the perceptron algorithm. Machine
Learning, 37(3):277-296.
M. Gamon, J. Gao, C. Brockett, A. Klementiev, W.
Dolan, D. Belenko and L. Vanderwende. 2008. Using
Contextual Speller Techniques and Language Model-
ing for ESL Error Correction. Proceedings of IJCNLP.
A. R. Golding and D. Roth. 1996. Applying Winnow
to Context-Sensitive Spelling Correction. ICML, 182?
190.
A. R. Golding and D. Roth. 1999. A Winnow based ap-
proach to Context-Sensitive Spelling Correction. Ma-
chine Learning, 34(1-3):107?130.
S. Granger, E. Dagneaux and F. Meunier 2002. Interna-
tional Corpus of Learner English.
S. Gui and H. Yang. 2003. Zhongguo Xuexizhe Yingyu
Yuliaohu. (Chinese Learner English Corpus). Shang-
hai Waiyu Jiaoyu Chubanshe. (In Chinese).
N. Han, M. Chodorow and C. Leacock. 2006. De-
tecting Errors in English Article Usage by Non-native
Speakers. Journal of Natural Language Engineering,
12(2):115?129.
E. Izumi, K. Uchimoto, T. Saiga and H. Isahara. 2003.
Automatic Error Detection in the Japanese Leaners
English Spoken Data. ACL.
E. Izumi, K. Uchimoto and H. Isahara. 2004. The
NICT JLE Corpus: Exploiting the Language Learner?s
Speech Database for Research and Education. Inter-
national Journal of the Computer, the Internet and
Management, 12(2):119?125.
K. Knight and I. Chander. 1994. Automatic Postediting
of Documents. In Proceedings of the American Asso-
ciation of Artificial Intelligence, pp 779?784.
J. Lee and S. Seneff. 2008. An analysis of grammatical
errors in non-native speech in English. In Proceedings
of the 2008 Spoken Language Technology Workshop,
Goa.
G. Minnen, F. Bond and A. Copestake 2000. Memory-
Based Learning for Article Generation. In Proceed-
ings of the Fourth Conference on Computational Nat-
ural Language Learning and of the Second Learning
Language in Logic Workshop, pp 43?48.
V. Punyakanok, D. Roth, and W. Yih. The importance of
syntactic parsing and inference in semantic role label-
ing. Computational Linguistics, 34(2).
N. Rizzolo and D. Roth 2007. Modeling Discriminative
Global Inference. In Proceedings of the First Interna-
tional Conference on Semantic Computing (ICSC), pp
597?604.
A. Rozovskaya and D. Roth 2010. Annotating ESL Er-
rors: Challenges and Rewards. In Proceedings of the
NAACL Workshop on Innovative Use of NLP for Build-
ing Educational Applications.
J. Sjo?bergh and O. Knutsson. 2005. Faking errors to
avoid making errors. In Proceedings of RANLP 2005,
Borovets.
J. Tetreault and M. Chodorow. 2008. Native Judgments
of Non-Native Usage: Experiments in Preposition Er-
ror Detection. COLING Workshop on Human Judg-
ments in Computational Linguistics, Manchester, UK.
J. Turner and E. Charniak. 2007. Language Modeling
for Determiner Selection. In Human Language Tech-
nologies 2007: The Conference of the North American
Chapter of the Association for Computational Linguis-
tics; Companion Volume, Short Papers, pp 177?180.
J. Wagner, J. Foster, and J. van Genabith. 2009. Judg-
ing grammaticality: Experiments in sentence classifi-
cation. CALICO Journal. Special Issue on the 2008
Automatic Analysis of Learner Language CALICO
Workshop.
Y. Xing, J. Gao, and W. Dolan. 2009. A web-based En-
glish proofing system for ESL users. In Proceedings
of IJCNLP.
162
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 924?933,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Algorithm Selection and Model Adaptation for ESL Correction Tasks
Alla Rozovskaya and Dan Roth
University of Illinois at Urbana-Champaign
Urbana, IL 61801
{rozovska,danr}@illinois.edu
Abstract
We consider the problem of correcting errors
made by English as a Second Language (ESL)
writers and address two issues that are essen-
tial to making progress in ESL error correction
- algorithm selection and model adaptation to
the first language of the ESL learner.
A variety of learning algorithms have been
applied to correct ESL mistakes, but often
comparisons were made between incompara-
ble data sets. We conduct an extensive, fair
comparison of four popular learning methods
for the task, reversing conclusions from ear-
lier evaluations. Our results hold for different
training sets, genres, and feature sets.
A second key issue in ESL error correction
is the adaptation of a model to the first lan-
guage of the writer. Errors made by non-native
speakers exhibit certain regularities and, as we
show, models perform much better when they
use knowledge about error patterns of the non-
native writers. We propose a novel way to
adapt a learned algorithm to the first language
of the writer that is both cheaper to imple-
ment and performs better than other adapta-
tion methods.
1 Introduction
There has been a lot of recent work on correct-
ing writing mistakes made by English as a Second
Language (ESL) learners (Izumi et al, 2003; Eeg-
Olofsson and Knuttson, 2003; Han et al, 2006; Fe-
lice and Pulman, 2008; Gamon et al, 2008; Tetreault
and Chodorow, 2008; Elghaari et al, 2010; Tetreault
et al, 2010; Gamon, 2010; Rozovskaya and Roth,
2010c). Most of this work has focused on correcting
mistakes in article and preposition usage, which are
some of the most common error types among non-
native writers of English (Dalgish, 1985; Bitchener
et al, 2005; Leacock et al, 2010). Examples below
illustrate some of these errors:
1. ?They listen to None*/the lecture carefully.?
2. ?He is an engineer with a passion to*/for what he
does.?
In (1) the definite article is incorrectly omitted. In
(2), the writer uses an incorrect preposition.
Approaches to correcting preposition and article
mistakes have adopted the methods of the context-
sensitive spelling correction task, which addresses
the problem of correcting spelling mistakes that re-
sult in legitimate words, such as confusing their
and there (Carlson et al, 2001; Golding and Roth,
1999). A candidate set or a confusion set is defined
that specifies a list of confusable words, e.g., {their,
there}. Each occurrence of a confusable word in text
is represented as a vector of features derived from a
context window around the target, e.g., words and
part-of-speech tags. A classifier is trained on text
assumed to be error-free. At decision time, for each
word in text, e.g. there, the classifier predicts the
most likely candidate from the corresponding con-
fusion set {their, there}.
Models for correcting article and preposition er-
rors are similarly trained on error-free native English
text, where the confusion set includes all articles
or prepositions (Izumi et al, 2003; Eeg-Olofsson
and Knuttson, 2003; Han et al, 2006; Felice and
Pulman, 2008; Gamon et al, 2008; Tetreault and
Chodorow, 2008; Tetreault et al, 2010).
924
Although the choice of a particular learning al-
gorithm differs, with the exception of decision trees
(Gamon et al, 2008), all algorithms used are lin-
ear learning algorithms, some discriminative (Han
et al, 2006; Felice and Pulman, 2008; Tetreault
and Chodorow, 2008; Rozovskaya and Roth, 2010c;
Rozovskaya and Roth, 2010b), some probabilistic
(Gamon et al, 2008; Gamon, 2010), or ?counting?
(Bergsma et al, 2009; Elghaari et al, 2010).
While model comparison has not been the goal
of the earlier studies, it is quite common to com-
pare systems, even when they are trained on dif-
ferent data sets and use different features. Further-
more, since there is no shared ESL data set, sys-
tems are also evaluated on data from different ESL
sources or even on native data. Several conclusions
have been made when comparing systems devel-
oped for ESL correction tasks. A language model
was found to outperform a maximum entropy classi-
fier (Gamon, 2010). However, the language model
was trained on the Gigaword corpus, 17 ? 109 words
(Linguistic Data Consortium, 2003), a corpus sev-
eral orders of magnitude larger than the corpus used
to train the classifier. Similarly, web-based models
built on Google Web1T 5-gram Corpus (Bergsma et
al., 2009) achieve better results when compared to a
maximum entropy model that uses a corpus 10, 000
times smaller (Chodorow et al, 2007)1.
In this work, we compare four popular learning
methods applied to the problem of correcting prepo-
sition and article errors and evaluate on a common
ESL data set. We compare two probabilistic ap-
proaches ? Na??ve Bayes and language modeling; a
discriminative algorithm Averaged Perceptron; and a
count-based method SumLM (Bergsma et al, 2009),
which, as we show, is very similar to Na??ve Bayes,
but with a different free coefficient. We train our
models on data from several sources, varying train-
ing sizes and feature sets, and show that there are
significant differences in the performance of these
algorithms. Contrary to previous results (Bergsma et
al., 2009; Gamon, 2010), we find that when trained
on the same data with the same features, Averaged
Perceptron achieves the best performance, followed
by Na??ve Bayes, then the language model, and fi-
nally the count-based approach. Our results hold for
1These two models also use different features.
training sets of different sizes, genres, and feature
sets. We also explain the performance differences
from the perspective of each algorithm.
The second important question that we address is
that of adapting the decision to the source language
of the writer. Errors made by non-native speakers
exhibit certain regularities. Adapting a model so
that it takes into consideration the specific error pat-
terns of the non-native writers was shown to be ex-
tremely helpful in the context of discriminative clas-
sifiers (Rozovskaya and Roth, 2010c; Rozovskaya
and Roth, 2010b). However, this method requires
generating new training data and training a separate
classifier for each source language. Our key contri-
bution here is a novel, simple, and elegant adaptation
method within the framework of the Na??ve Bayes
algorithm, which yields even greater performance
gains. Specifically, we show how the error patterns
of the non-native writers can be viewed as a different
distribution on candidate priors in the confusion set.
Following this observation, we train Na??ve Bayes in
a traditional way, regardless of the source language
of the writer, and then, only at decision time, change
the prior probabilities of the model from the ones
observed in the native training data to the ones corre-
sponding to error patterns in the non-native writer?s
source language (Section 4). A related idea has been
applied in Word Sense Disambiguation to adjust the
model priors to a new domain with different sense
distributions (Chan and Ng, 2005).
The paper has two main contributions. First, we
conduct a fair comparison of four learning algo-
rithms and show that the discriminative approach
Averaged Perceptron is the best performing model
(Sec. 3). Our results do not support earlier conclu-
sions with respect to the performance of count-based
models (Bergsma et al, 2009) and language mod-
els (Gamon, 2010). In fact, we show that SumLM
is comparable to Averaged Perceptron trained with
a 10 times smaller corpus, and language model is
comparable to Averaged Perceptron trained with a 2
times smaller corpus.
The second, and most significant, of our contribu-
tions is a novel way to adapt a model to the source
language of the writer, without re-training the model
(Sec. 4). As we show, adapting to the source lan-
guage of the writer provides significant performance
improvement, and our new method also performs
925
better than previous, more complicated methods.
Section 2 presents the theoretical component of
the linear learning framework. In Section 3, we
describe the experiments, which compare the four
learning models. Section 4 presents the key result of
this work, a novel method of adapting the model to
the source language of the learner.
2 The Models
The standard approach to preposition correction
is to cast the problem as a multi-class classifica-
tion task and train a classifier on features defined
on the surrounding context2. The model selects
the most likely candidate from the confusion set,
where the set of candidates includes the top n most
frequent English prepositions. Our confusion set
includes the top ten prepositions3: ConfSet =
{on, from, for, of, about, to, at, in, with, by}. We
use p to refer to a candidate preposition from
ConfSet.
Let preposition context denote the preposition and
the window around it. For instance, ?a passion to
what he? is a context for window size 2. We use
three feature sets, varying window size from 2 to 4
words on each side (see Table 1). All feature sets
consist of word n-grams of various lengths span-
ning p and all the features are of the form s?kps+m,
where s?k and s+m denote k words before and m
words after p; we show two 3-gram features for il-
lustration:
1. a passion p
2. passion p what
We implement four linear learning models: the
discriminative method Averaged Perceptron (AP);
two probabilistic methods ? a language model (LM)
and Na??ve Bayes (NB); and a ?counting? method
SumLM (Bergsma et al, 2009).
Each model produces a score for a candidate in
the confusion set. Since all of the models are lin-
ear, the hypotheses generated by the algorithms dif-
fer only in the weights they assign to the features
2We also report one experiment on the article correction
task. We take the preposition correction task as an example;
the article case is treated in the same way.
3This set of prepositions is also considered in other works,
e.g. (Rozovskaya and Roth, 2010b). The usage of the ten most
frequent prepositions accounts for 82% of all preposition errors
(Leacock et al, 2010).
Feature Preposition context N-gram
set lengths
Win2 a passion [to] what he 2,3,4
Win3 with a passion [to] what he does 2,3,4
Win4 engineer with a passion [to] what he does . 2,3,4,5
Table 1: Description of the three feature sets used in
the experiments. All feature sets consist of word n-grams
of various lengths spanning the preposition and vary by
n-gram length and window size.
Method Free Coefficient Feature weights
AP bias parameter mistake-driven
LM ? ? prior(p)
?
vl?vr
?vr ? log(P (u|vr))
NB log(prior(p)) log(P (f |p))
SumLM |F (S, p)| ? log(C(p)) log(P (f |p))
Table 2: Summary of the learning methods. C(p) de-
notes the number of times preposition p occurred in train-
ing. ? is a smoothing parameter, u is the rightmost word
in f , vl ? vr denotes all concatenations of substrings vl
and vr of feature f without u.
(Roth, 1998; Roth, 1999). Thus a score computed
by each of the models for a preposition p in the con-
text S can be expressed as follows:
g(S, p) = C(p) +
?
f?F (S,p)
wa(f), (1)
where F (S, p) is the set of features active in con-
text S relative to preposition p, wa(f) is the weight
algorithm a assigns to feature f ? F , and C(p) is
a free coefficient. Predictions are made using the
winner-take-all approach: argmaxpg(S, p). The al-
gorithms make use of the same feature set F and
differ only by how the weights wa(f) and C(p) are
computed. Below we explain how the weights are
determined in each method. Table 2 summarizes the
four approaches.
2.1 Averaged Perceptron
Discriminative classifiers represent the most com-
mon learning paradigm in error correction. AP (Fre-
und and Schapire, 1999) is a discriminative mistake-
driven online learning algorithm. It maintains a vec-
tor of feature weights w and processes one training
example at a time, updating w if the current weight
assignment makes a mistake on the training exam-
ple. In the case of AP, the C(p) coefficient refers to
the bias parameter (see Table 2).
926
We use the regularized version of AP in Learn-
ing Based Java4 (LBJ, (Rizzolo and Roth, 2007)).
While classical Perceptron comes with a generaliza-
tion bound related to the margin of the data, Aver-
aged Perceptron also comes with a PAC-like gener-
alization bound (Freund and Schapire, 1999). This
linear learning algorithm is known, both theoreti-
cally and experimentally, to be among the best linear
learning approaches and is competitive with SVM
and Logistic Regression, while being more efficient
in training. It also has been shown to produce state-
of-the-art results on many natural language applica-
tions (Punyakanok et al, 2008).
2.2 Language Modeling
Given a feature f = s?kps+m, let u denote the
rightmost word in f and vl ? vr denote all concate-
nations of substrings vl and vr of feature f without
u. The language model computes several probabil-
ities of the form P (u|vr). If f =?with a passion
p what?, then u =?what?, and vr ? {?with a pas-
sion p?, ?a passion p?, ?passion p?, ?p? }. In prac-
tice, these probabilities are smoothed and replaced
with their corresponding log values, and the total
weight contribution of f to the scoring function of
p is
?
vl?vr
?vr ? log(P (u|vr)). In addition, this
scoring function has a coefficient that only depends
on p: C(p) = ? ? prior(p) (see Table 2). The prior
probability of a candidate p is:
prior(p) =
C(p)
?
q?ConfSetC(q)
, (2)
where C(p) and C(q) denote the number of
times preposition p and q, respectively, occurred in
the training data. We implement a count-based
LM with Jelinek-Mercer linear interpolation as a
smoothing method5 (Chen and Goodman, 1996),
where each n-gram length, from 1 to n, is associated
with an interpolation smoothing weight ?. Weights
are optimized on a held-out set of ESL sentences.
Win2 and Win3 features correspond to 4-gram
LMs and Win4 to 5-gram LMs. Language models
are trained with SRILM (Stolcke, 2002).
4LBJ can be downloaded from http://cogcomp.cs.
illinois.edu.
5Unlike other LM methods, this approach allows us to train
LMs on very large data sets. Although we found that backoff
LMs may perform slightly better, they still maintain the same
hierarchy in the order of algorithm performance.
2.3 Na??ve Bayes
NB is another linear model, which is often hard to
beat using more sophisticated approaches. NB ar-
chitecture is also particularly well-suited for adapt-
ing the model to the first language of the writer (Sec-
tion 4). Weights in NB are determined, similarly to
LM, by the feature counts and the prior probability
of each candidate p (Eq. (2)). For each candidate
p, NB computes the joint probability of p and the
feature space F , assuming that the features are con-
ditionally independent given p:
g(S, p) = log{prior(p) ?
?
f?F (S,p)
P (f |p)}
= log(prior(p)) +
+
?
f?F (S,p)
log(P (f |p)) (3)
NB weights and its free coefficient are also summa-
rized in Table 2.
2.4 SumLM
For candidate p, SumLM (Bergsma et al, 2009)6
produces a score by summing over the logs of all
feature counts:
g(S, p) =
?
f?F (S,p)
log(C(f))
=
?
f?F (S,p)
log(P (f |p)C(p))
= |F (S, p)|C(p) +
?
f?F (S,p)
log(P (f |p))
where C(f) denotes the number of times n-gram
feature f was observed with p in training. It should
be clear from equation 3 that SumLM is very similar
to NB, with a different free coefficient (Table 2).
3 Comparison of Algorithms
3.1 Evaluation Data
We evaluate the models using a corpus of ESL es-
says, annotated7 by native English speakers (Ro-
zovskaya and Roth, 2010a). For each preposition
6SumLM is one of several related methods proposed in this
work; its accuracy on the preposition selection task on native
English data nearly matches the best model, SuperLM (73.7%
vs. 75.4%), while being much simpler to implement.
7The annotation of the ESL corpus can be downloaded from
http://cogcomp.cs.illinois.edu.
927
Source Prepositions Articles
language Total Incorrect Total Incorrect
Chinese 953 144 1864 150
Czech 627 28 575 55
Italian 687 43 - -
Russian 1210 85 2292 213
Spanish 708 52 - -
All 4185 352 4731 418
Table 3: Statistics on prepositions and articles in the
ESL data. Column Incorrect denotes the number of
cases judged to be incorrect by the annotator.
(article) used incorrectly, the annotator indicated the
correct choice. The data include sentences by speak-
ers of five first languages. Table 3 shows statistics by
the source language of the writer.
3.2 Training Corpora
We use two training corpora. The first corpus,
WikiNYT, is a selection of texts from English
Wikipedia and the New York Times section of the
Gigaword corpus and contains 107 preposition con-
texts. We build models of 3 sizes8: 106, 5 ? 106, and
107.
To experiment with larger data sets, we use the
Google Web1T 5-gram Corpus, which is a collec-
tion of n-gram counts of length one to five over a
corpus of 1012 words. The corpus contains 2.6 ?1010
prepositions. We refer to this corpus as GoogleWeb.
We stress that GoogleWeb does not contain com-
plete sentences, but only n-gram counts. Thus, we
cannot generate training data for AP for feature sets
Win3 and Win4: Since the algorithm does not as-
sume feature independence, we need to have 7 and
9-word sequences, respectively, with a preposition
in the middle (as shown in Table 1) and their corpus
frequencies. The other three models can be eval-
uated with the n-gram counts available. For exam-
ple, we compute NB scores by obtaining the count
of each feature independently, e.g. the count for left
context 5-gram ?engineer with a passion p? and right
context 5-gram ?p what he does .?, due to the con-
ditional independence assumption that NB makes.
On GoogleWeb, we train NB, SumLM, and LM with
three feature sets: Win2, Win3, and Win4.
From GoogleWeb, we also generate a smaller
training set of size 108: We use 5-grams with
a preposition in the middle and generate a new
8Training size refers to the number of preposition contexts.
count, proportional to the size of the smaller cor-
pus9. For instance, a preposition 5-gram with a
count of 2600 in GoogleWeb, will have a count of
10 in GoogleWeb-108.
3.3 Results
Our key results of the fair comparison of the four
algorithms are shown in Fig. 1 and summarized in
Table 4. The table shows that AP trained on 5 ? 106
preposition contexts performs as well as NB trained
on 107 (i.e., with twice as much data; the perfor-
mance of LM trained on 107 contexts is better than
that of AP trained with 10 times less data (106), but
not as good as that of AP trained with half as much
data (5?106); AP outperforms SumLM, when the lat-
ter uses 10 times more data. Fig. 1 demonstrates the
performance results reported in Table 4; it shows the
behavior of different systems with respect to preci-
sion and recall on the error correction task. We gen-
erate the curves by varying the decision threshold on
the confidence of the classifier (Carlson et al, 2001)
and propose a correction only when the confidence
of the classifier is above the threshold. A higher pre-
cision and a lower recall are obtained when the de-
cision threshold is high, and vice versa.
Key results
AP > NB > LM > SumLM
AP ? 2 ?NB
5 ?AP > 10 ? LM > AP
AP > 10 ? SumLM
Table 4: Key results on the comparison of algorithms.
2 ?NB refers to NB trained with twice as much data as
AP ; 10 ? LM refers to LM trained with 10 times more
data asAP ; 10?SumLM refers to SumLM trained with
10 times more data as AP . These results are also shown
in Fig. 1.
We now show a fair comparison of the four algo-
rithms for different window sizes, training data and
training sizes. Figure 2 compares the models trained
on WikiNY T -107 corpus for Win4. AP is the su-
perior model, followed by NB, then LM, and finally
SumLM.
Results for other training sizes and feature10 set
9Scaling down GoogleWeb introduces some bias but we be-
lieve that it should not have an effect on our experiments.
10We have also experimented with additional POS-based fea-
tures that are commonly used in these tasks and observed simi-
lar behavior.
928
 
0
 
10
 
20
 
30
 
40
 
50
 
60  0
 
10
 
20
 
30
 
40
 
50
 
60
PRECISION
RECA
LL
SumL
M-10
7
LM-1
07
NB-1
07
AP-1
06
AP-5
*106 AP-1
07
Figure 1: Algorithm comparison across different
training sizes. (WikiNYT, Win3). AP (106 preposition
contexts) performs as well as SumLM with 10 times more
data, and LM requires at least twice as much data to
achieve the performance of AP.
configurations show similar behavior and are re-
ported in Table 5, which provides model compari-
son in terms of Average Area Under Curve (AAUC,
(Hanley and McNeil, 1983)). AAUC is a measure
commonly used to generate a summary statistic and
is computed here as an average precision value over
12 recall points (from 5 to 60):
AAUC =
1
12
?
12?
i=1
Precision(i ? 5)
The Table also shows results on the article correc-
tion task11.
Training data Feature Performance (AAUC)
set AP NB LM SumLM
WikiNY T -5 ? 106 Win3 26 22 20 13
WikiNY T -107 Win4 33 28 24 16
GoogleWeb-108 Win2 30 29 28 15
GoogleWeb Win4 - 44 41 32
Article
WikiNY T -5 ? 106 Win3 40 39 - 30
Table 5: Performance Comparison of the four algo-
rithms for different training data, training sizes, and win-
dow sizes. Each row shows results for training data of the
same size. The last row shows performance on the article
correction task. All other results are for prepositions.
11We do not evaluate the LM approach on the article correc-
tion task, since with LM it is difficult to handle missing article
errors, one of the most common error types for articles, but the
expectation is that it will behave as it does for prepositions.
 
0
 
10
 
20
 
30
 
40
 
50
 
60  0
 
10
 
20
 
30
 
40
 
50
 
60
PRECISION
RECA
LL
SumL
M LM NB AP
Figure 2: Model Comparison for training data of the
same size: Performance of models for feature set Win4
trained on WikiNY T -107.
3.3.1 Effects of Window Size
We found that expanding window size from 2 to 3
is helpful for all of the models, but expanding win-
dow to 4 is only helpful for the models trained on
GoogleWeb (Table 6). Compared to Win3, Win4 has
five additional 5-gram features. We look at the pro-
portion of features in the ESL data that occurred in
two corpora: WikiNY T -107 and GoogleWeb (Ta-
ble 7). We observe that only 4% of test 5-grams oc-
cur inWikiNY T -107. This number goes up 7 times
to 28% for GoogleWeb, which explains why increas-
ing the window size is helpful for this model. By
comparison, a set of native English sentences (dif-
ferent from the training data) has 50% more 4-grams
and about 3 times more 5-grams, because ESL sen-
tences often contain expressions not common for na-
tive speakers.
Training data Performance (AAUC)
Win2 Win3 Win4
GoogleWeb 35 39 44
Table 6: Effect of Window Size in terms ofAAUC. Per-
formance improves, as the window increases.
4 Adapting to Writer?s Source Language
In this section, we discuss adapting error correction
systems to the first language of the writer. Non-
native speakers make mistakes in a systematic man-
ner, and errors often depend on the first language of
the writer (Lee and Seneff, 2008; Rozovskaya and
929
Test Train N-gram length
2 3 4 5
ESL WikiNY T -107 98% 66% 22% 4%
Native WikiNY T -107 98% 67% 32% 13%
ESL GoogleWeb 99% 92% 64% 28%
Native-B09 GoogleWeb - 99% 93% 70%
Table 7: Feature coverage for ESL and native data.
Percentage of test n-gram features that occurred in train-
ing. Native refers to data from Wikipedia and NYT. B09
refers to statistics from Bergsma et al (2009).
Roth, 2010a). For instance, a Chinese learner of
English might say ?congratulations to this achieve-
ment? instead of ?congratulations on this achieve-
ment?, while a Russian speaker might say ?congrat-
ulations with this achievement?.
A system performs much better when it makes use
of knowledge about typical errors. When trained
on annotated ESL data instead of native data, sys-
tems improve both precision and recall (Han et al,
2010; Gamon, 2010). Annotated data include both
the writer?s preposition and the intended (correct)
one, and thus the knowledge about typical errors is
made available to the system.
Another way to adapt a model to the first language
is to generate in native training data artificial errors
mimicking the typical errors of the non-native writ-
ers (Rozovskaya and Roth, 2010c; Rozovskaya and
Roth, 2010b). Henceforth, we refer to this method,
proposed within the discriminative framework AP,
as AP-adapted. To determine typical mistakes, error
statistics are collected on a small set of annotated
ESL sentences. However, for the model to use these
language-specific error statistics, a separate classi-
fier for each source language needs to be trained.
We propose a novel adaptation method, which
shows performance improvement over AP-adapted.
Moreover, this method is much simpler to imple-
ment, since there is no need to train per source lan-
guage; only one classifier is trained. The method
relies on the observation that error regularities can
be viewed as a distribution on priors over the cor-
rection candidates. Given a preposition s in text, the
prior for candidate p is the probability that p is the
correct preposition for s. If a model is trained on na-
tive data without adaptation to the source language,
candidate priors correspond to the relative frequen-
cies of the candidates in the native training data.
More importantly, these priors remain the same re-
gardless of the source language of the writer or of
the preposition used in text. From the model?s per-
spective, it means that a correction candidate, for
example to, is equally likely given that the author?s
preposition is for or from, which is clearly incorrect
and disagrees with the notion that errors are regular
and language-dependent.
We use the annotated ESL data and define
adapted candidate priors that are dependent on the
author?s preposition and the author?s source lan-
guage. Let s be a preposition appearing in text by
a writer of source language L1, and p a correction
candidate. Then the adapted prior of p given s is:
prior(p, s, L1) =
CL1(s, p)
CL1(s)
,
where CL1(s) denotes the number of times s ap-
peared in the ESL data by L1 writers, and CL1(s, p)
denotes the number of times p was the correct prepo-
sition when s was used by an L1 writer.
Table 8 shows adapted candidate priors for two
author?s choices ? when an ESL writer used on and
at ? based on the data from Chinese learners. One
key distinction of the adapted priors is the high prob-
ability assigned to the author?s preposition: the new
prior for on given that it is also the preposition found
in text is 0.70, vs. the 0.07 prior based on the native
data. The adapted prior of preposition p, when p is
used, is always high, because the majority of prepo-
sitions are used correctly. Higher probabilities are
also assigned to those candidates that are most often
observed as corrections for the author?s preposition.
For example, the adapted prior for at when the writer
chose on is 0.10, since on is frequently incorrectly
chosen instead of at.
To determine a mechanism to inject the adapted
priors into a model, we note that while all of our
models use priors in some way, NB architecture di-
rectly specifies the prior probability as one of its pa-
rameters (Sec. 2.3). We thus train NB in a traditional
way, on native data, and then replace the prior com-
ponent in Eq. (3) with the adapted prior, language
and preposition dependent, to get the score for p of
the NB-adapted model:
g(S, p) = log{prior(p, s, L1) ?
?
f?F (S,p)
P (f |p)}
930
Candidate Global Adapted prior
prior author?s prior author?s prior
choice choice
of 0.25 on 0.03 at 0.02
to 0.22 on 0.06 at 0.00
in 0.15 on 0.04 at 0.16
for 0.10 on 0.00 at 0.03
on 0.07 on 0.70 at 0.09
by 0.06 on 0.00 at 0.02
with 0.06 on 0.04 at 0.00
at 0.04 on 0.10 at 0.75
from 0.04 on 0.00 at 0.02
about 0.01 on 0.03 at 0.00
Table 8: Examples of adapted candidate priors for
two author?s choices ? on and at ? based on the er-
rors made by Chinese learners. Global prior denotes
the probability of the candidate in the standard model
and is based on the relative frequency of the candidate
in native training data. Adapted priors are dependent on
the author?s preposition and the author?s first language.
Adapted priors for the author?s choice are very high.
Other candidates are given higher priors if they often ap-
pear as corrections for the author?s choice.
We stress that in the new method there is no need
to train per source language, as with previous adap-
tion methods. Only one model is trained, and only
at decision time, we change the prior probabilities of
the model. Also, while we need a lot of data to train
the model, only one parameter depends on annotated
data. Therefore, with rather small amounts of data, it
is possible to get reasonably good estimates of these
prior parameters.
In the experiments below, we compare four mod-
els: AP, NB AP-adapted and NB-adapted. AP-
adapted is the adaptation through artificial errors
and NB-adapted is the method proposed here. Both
of the adapted models use the same error statistics in
k-fold cross-validation (CV): We randomly partition
the ESL data into k parts, with each part tested on
the model that uses error statistics estimated on the
remaining k ? 1 parts. We also remove all prepo-
sition errors that occurred only once (23% of all er-
rors) to allow for a better evaluation of the adapted
models. Although we observe similar behavior on
all the data, the models especially benefit from the
adapted priors when a particular error occurred more
than once. Since the majority of errors are not due
to chance, we focus on those errors that the writers
will make repeatedly.
Fig. 3 shows the four models trained on
WikiNY T -107. First, we note that the adapted
 
0
 
10
 
20
 
30
 
40
 
50
 
60
 
70
 
80  0
 
10
 
20
 
30
 
40
 
50
PRECISION
RECA
LL
NB-a
dapte
d
AP-a
dapte
d AP NB
Figure 3: Adapting to Writer?s Source Language. NB-
adapted is the method proposed here. AP-adapted and
NB-adapted results are obtained using 2-fold CV, with
50% of the ESL data used for estimating the new priors.
All models are trained on WikiNY T -107.
models outperform their non-adapted counterparts
with respect to precision. Second, for the recall
points less than 20%, the adapted models obtain very
similar precision values. This is interesting, espe-
cially because NB does not perform as well as AP, as
we also showed in Sec. 3.3. Thus, NB-adapted not
only improves over NB, but its gap compared to the
latter is much wider than the gap between the AP-
based systems. Finally, an important performance
distinction between the two adapted models is the
loss in recall exhibited by AP-adapted ? its curve is
shorter because AP-adapted is very conservative and
does not propose many corrections. In contrast, NB-
adapted succeeds in improving its precision over NB
with almost no recall loss.
To evaluate the effect of the size of the data used
to estimate the new priors, we compare the perfor-
mance of NB-adapted models in three settings: 2-
fold CV, 10-fold CV, and Leave-One-Out (Figure 4).
In 2-fold CV, priors are estimated on 50% of the ESL
data, in 10-fold on 90%, and in Leave-One-Out on
all data but the testing example. Figure 4 shows the
averaged results over 5 runs of CV for each setting.
The model converges very quickly: there is almost
no difference between 10-fold CV and Leave-One-
Out, which suggests that we can get a good estimate
of the priors using just a little annotated data.
Table 9 compares NB and NB-adapted for two
corpora: WikiNY T -107 and GoogleWeb. Since
931
 
0
 
10
 
20
 
30
 
40
 
50
 
60
 
70
 
80
 
90  0
 
10
 
20
 
30
 
40
 
50
 
60
PRECISION
RECA
LL
NB-a
dapte
d-Lea
veOn
eOut
NB-a
dapte
d-10-
fold
NB-a
dapte
d-2-fo
ld NB
Figure 4: How much data are needed to estimate
adapted priors. Comparison of NB-adapted models
trained on GoogleWeb that use different amounts of data
to estimate the new priors. In 2-fold CV, priors are es-
timated on 50% of the data; in 10-fold on 90% of the
data; in Leave-One-Out, the new priors are based on all
the data but the testing example.
GoogleWeb is several orders of magnitude larger,
the adapted model behaves better for this corpus.
So far, we have discussed performance in terms
of precision and recall, but we can also discuss it
in terms of accuracy, to see how well the algorithm
is performing compared to the baseline on the task.
Following Rozovskaya and Roth (2010c), we con-
sider as the baseline the accuracy of the ESL data
before applying the model12, or the percentage of
prepositions used correctly in the test data. From
Table 3, the baseline is 93.44%13. Compared to
this high baseline, NB trained on WikiNY T -107
achieves an accuracy of 93.54, and NB-adapted
achieves an accuracy of 93.9314.
Training data Algorithms
NB NB-adapted
WikiNY T -107 29 53
GoogleWeb 38 62
Table 9: Adapting to writer?s source language. Re-
sults are reported in terms of AAUC. NB-adapted is the
model with adapted priors. Results for NB-adapted are
based on 10-fold CV.
12Note that this baseline is different from the majority base-
line used in the preposition selection task, since here we have
the author?s preposition in text.
13This is the baseline after removing the singleton errors.
14We select the best accuracy among different values that can
be achieved by varying the decision threshold.
5 Conclusion
We have addressed two important issues in ESL
error correction, which are essential to making
progress in this task. First, we presented an exten-
sive, fair comparison of four popular linear learning
models for the task and demonstrated that there are
significant performance differences between the ap-
proaches. Since all of the algorithms presented here
are linear, the only difference is in how they learn
the weights. Our experiments demonstrated that the
discriminative approach (AP) is able to generalize
better than any of the other models. These results
correct earlier conclusions, made with incompara-
ble data sets. The model comparison was performed
using two popular tasks ? correcting errors in article
and preposition usage ? and we expect that our re-
sults will generalize to other ESL correction tasks.
The second, and most important, contribution of
the paper is a novel method that allows one to
adapt the learned model to the source language of
the writer. We showed that error patterns can be
viewed as a distribution on priors over the correc-
tion candidates and proposed a method of injecting
the adapted priors into the learned model. In ad-
dition to performing much better than the previous
approaches, this method is also very cheap to im-
plement, since it does not require training a separate
model for each source language, but adapts the sys-
tem to the writer?s language at decision time.
Acknowledgments
The authors thank Nick Rizzolo for many helpful
discussions. The authors also thank Josh Gioja, Nick
Rizzolo, Mark Sammons, Joel Tetreault, Yuancheng
Tu, and the anonymous reviewers for their insight-
ful comments. This research is partly supported by
a grant from the U.S. Department of Education.
References
S. Bergsma, D. Lin, and R. Goebel. 2009. Web-scale
n-gram models for lexical disambiguation. In 21st In-
ternational Joint Conference on Artificial Intelligence,
pages 1507?1512.
J. Bitchener, S. Young, and D. Cameron. 2005. The ef-
fect of different types of corrective feedback on ESL
student writing. Journal of Second Language Writing.
A. Carlson, J. Rosen, and D. Roth. 2001. Scaling up
context sensitive text correction. In Proceedings of the
932
National Conference on Innovative Applications of Ar-
tificial Intelligence (IAAI), pages 45?50.
Y. S. Chan and H. T. Ng. 2005. Word sense disambigua-
tion with distribution estimation. In Proceedings of
IJCAI 2005.
S. Chen and J. Goodman. 1996. An empirical study of
smoothing techniques for language modeling. In Pro-
ceedings of ACL 1996.
M. Chodorow, J. Tetreault, and N.-R. Han. 2007. Detec-
tion of grammatical errors involving prepositions. In
Proceedings of the Fourth ACL-SIGSEM Workshop on
Prepositions, pages 25?30, Prague, Czech Republic,
June. Association for Computational Linguistics.
G. Dalgish. 1985. Computer-assisted ESL research.
CALICO Journal, 2(2).
J. Eeg-Olofsson and O. Knuttson. 2003. Automatic
grammar checking for second language learners - the
use of prepositions. Nodalida.
A. Elghaari, D. Meurers, and H. Wunsch. 2010. Ex-
ploring the data-driven prediction of prepositions in
english. In Proceedings of COLING 2010, Beijing,
China.
R. De Felice and S. Pulman. 2008. A classifier-based ap-
proach to preposition and determiner error correction
in L2 English. In Proceedings of the 22nd Interna-
tional Conference on Computational Linguistics (Col-
ing 2008), pages 169?176, Manchester, UK, August.
Y. Freund and R. E. Schapire. 1999. Large margin clas-
sification using the perceptron algorithm. Machine
Learning, 37(3):277?296.
M. Gamon, J. Gao, C. Brockett, A. Klementiev,
W. Dolan, D. Belenko, and L. Vanderwende. 2008.
Using contextual speller techniques and language
modeling for ESL error correction. In Proceedings of
IJCNLP.
M. Gamon. 2010. Using mostly native data to correct
errors in learners? writing. In NAACL, pages 163?171,
Los Angeles, California, June.
A. R. Golding and D. Roth. 1999. A Winnow based
approach to context-sensitive spelling correction. Ma-
chine Learning, 34(1-3):107?130.
N. Han, M. Chodorow, and C. Leacock. 2006. Detecting
errors in English article usage by non-native speakers.
Journal of Natural Language Engineering, 12(2):115?
129.
N. Han, J. Tetreault, S. Lee, and J. Ha. 2010. Us-
ing an error-annotated learner corpus to develop and
ESL/EFL error correction system. In LREC, Malta,
May.
J. Hanley and B. McNeil. 1983. A method of comparing
the areas under receiver operating characteristic curves
derived from the same cases. Radiology, 148(3):839?
843.
E. Izumi, K. Uchimoto, T. Saiga, T. Supnithi, and H. Isa-
hara. 2003. Automatic error detection in the Japanese
learners? English spoken data. In The Companion Vol-
ume to the Proceedings of 41st Annual Meeting of
the Association for Computational Linguistics, pages
145?148, Sapporo, Japan, July.
C. Leacock, M. Chodorow, M. Gamon, and J. Tetreault.
2010. Morgan and Claypool Publishers.
J. Lee and S. Seneff. 2008. An analysis of grammatical
errors in non-native speech in English. In Proceedings
of the 2008 Spoken Language Technology Workshop.
V. Punyakanok, D. Roth, and W. Yih. 2008. The impor-
tance of syntactic parsing and inference in semantic
role labeling. Computational Linguistics, 34(2).
N. Rizzolo and D. Roth. 2007. Modeling Discriminative
Global Inference. In Proceedings of the First Inter-
national Conference on Semantic Computing (ICSC),
pages 597?604, Irvine, California, September. IEEE.
D. Roth. 1998. Learning to resolve natural language am-
biguities: A unified approach. In Proceedings of the
National Conference on Artificial Intelligence (AAAI),
pages 806?813.
D. Roth. 1999. Learning in natural language. In Proc. of
the International Joint Conference on Artificial Intelli-
gence (IJCAI), pages 898?904.
A. Rozovskaya and D. Roth. 2010a. Annotating ESL
errors: Challenges and rewards. In Proceedings of the
NAACL Workshop on Innovative Use of NLP for Build-
ing Educational Applications.
A. Rozovskaya and D. Roth. 2010b. Generating con-
fusion sets for context-sensitive error correction. In
Proceedings of the Conference on Empirical Methods
in Natural Language Processing (EMNLP).
A. Rozovskaya and D. Roth. 2010c. Training paradigms
for correcting errors in grammar and usage. In Pro-
ceedings of the NAACL-HLT.
A. Stolcke. 2002. Srilm-an extensible language mod-
eling toolkit. In Proceedings International Confer-
ence on Spoken Language Processing, pages 257?286,
November.
J. Tetreault and M. Chodorow. 2008. The ups and
downs of preposition error detection in ESL writing.
In Proceedings of the 22nd International Conference
on Computational Linguistics (Coling 2008), pages
865?872, Manchester, UK, August.
J. Tetreault, J. Foster, and M. Chodorow. 2010. Using
parse features for preposition selection and error de-
tection. In ACL.
933
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 508?513,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
They Can Help: Using Crowdsourcing to Improve the Evaluation of
Grammatical Error Detection Systems
Nitin Madnania Joel Tetreaulta Martin Chodorowb Alla Rozovskayac
aEducational Testing Service
Princeton, NJ
{nmadnani,jtetreault}@ets.org
bHunter College of CUNY
martin.chodorow@hunter.cuny.edu
cUniversity of Illinois at Urbana-Champaign
rozovska@illinois.edu
Abstract
Despite the rising interest in developing gram-
matical error detection systems for non-native
speakers of English, progress in the field has
been hampered by a lack of informative met-
rics and an inability to directly compare the
performance of systems developed by differ-
ent researchers. In this paper we address
these problems by presenting two evaluation
methodologies, both based on a novel use of
crowdsourcing.
1 Motivation and Contributions
One of the fastest growing areas in need of NLP
tools is the field of grammatical error detection for
learners of English as a Second Language (ESL).
According to Guo and Beckett (2007), ?over a bil-
lion people speak English as their second or for-
eign language.? This high demand has resulted in
many NLP research papers on the topic, a Synthesis
Series book (Leacock et al, 2010) and a recurring
workshop (Tetreault et al, 2010a), all in the last five
years. In this year?s ACL conference, there are four
long papers devoted to this topic.
Despite the growing interest, two major factors
encumber the growth of this subfield. First, the lack
of consistent and appropriate score reporting is an
issue. Most work reports results in the form of pre-
cision and recall as measured against the judgment
of a single human rater. This is problematic because
most usage errors (such as those in article and prepo-
sition usage) are a matter of degree rather than sim-
ple rule violations such as number agreement. As a
consequence, it is common for two native speakers
to have different judgments of usage. Therefore, an
appropriate evaluation should take this into account
by not only enlisting multiple human judges but also
aggregating these judgments in a graded manner.
Second, systems are hardly ever compared to each
other. In fact, to our knowledge, no two systems
developed by different groups have been compared
directly within the field primarily because there is
no common corpus or shared task?both commonly
found in other NLP areas such as machine transla-
tion.1 For example, Tetreault and Chodorow (2008),
Gamon et al (2008) and Felice and Pulman (2008)
developed preposition error detection systems, but
evaluated on three different corpora using different
evaluation measures.
The goal of this paper is to address the above
issues by using crowdsourcing, which has been
proven effective for collecting multiple, reliable
judgments in other NLP tasks: machine transla-
tion (Callison-Burch, 2009; Zaidan and Callison-
Burch, 2010), speech recognition (Evanini et al,
2010; Novotney and Callison-Burch, 2010), au-
tomated paraphrase generation (Madnani, 2010),
anaphora resolution (Chamberlain et al, 2009),
word sense disambiguation (Akkaya et al, 2010),
lexicon construction for less commonly taught lan-
guages (Irvine and Klementiev, 2010), fact min-
ing (Wang and Callison-Burch, 2010) and named
entity recognition (Finin et al, 2010) among several
others.
In particular, we make a significant contribution
to the field by showing how to leverage crowdsourc-
1There has been a recent proposal for a related shared
task (Dale and Kilgarriff, 2010) that shows promise.
508
ing to both address the lack of appropriate evaluation
metrics and to make system comparison easier. Our
solution is general enough for, in the simplest case,
intrinsically evaluating a single system on a single
dataset and, more realistically, comparing two dif-
ferent systems (from same or different groups).
2 A Case Study: Extraneous Prepositions
We consider the problem of detecting an extraneous
preposition error, i.e., incorrectly using a preposi-
tion where none is licensed. In the sentence ?They
came to outside?, the preposition to is an extrane-
ous error whereas in the sentence ?They arrived
to the town? the preposition to is a confusion er-
ror (cf. arrived in the town). Most work on au-
tomated correction of preposition errors, with the
exception of Gamon (2010), addresses preposition
confusion errors e.g., (Felice and Pulman, 2008;
Tetreault and Chodorow, 2008; Rozovskaya and
Roth, 2010b). One reason is that in addition to the
standard context-based features used to detect con-
fusion errors, identifying extraneous prepositions
also requires actual knowledge of when a preposi-
tion can and cannot be used. Despite this lack of
attention, extraneous prepositions account for a sig-
nificant proportion?as much as 18% in essays by
advanced English learners (Rozovskaya and Roth,
2010a)?of all preposition usage errors.
2.1 Data and Systems
For the experiments in this paper, we chose a propri-
etary corpus of about 500,000 essays written by ESL
students for Test of English as a Foreign Language
(TOEFL R?). Despite being common ESL errors,
preposition errors are still infrequent overall, with
over 90% of prepositions being used correctly (Lea-
cock et al, 2010; Rozovskaya and Roth, 2010a).
Given this fact about error sparsity, we needed an ef-
ficient method to extract a good number of error in-
stances (for statistical reliability) from the large es-
say corpus. We found all trigrams in our essays con-
taining prepositions as the middle word (e.g., marry
with her) and then looked up the counts of each tri-
gram and the corresponding bigram with the prepo-
sition removed (marry her) in the Google Web1T
5-gram Corpus. If the trigram was unattested or had
a count much lower than expected based on the bi-
gram count, then we manually inspected the trigram
to see whether it was actually an error. If it was,
we extracted a sentence from the large essay corpus
containing this erroneous trigram. Once we had ex-
tracted 500 sentences containing extraneous prepo-
sition error instances, we added 500 sentences con-
taining correct instances of preposition usage. This
yielded a corpus of 1000 sentences with a 50% error
rate.
These sentences, with the target preposition high-
lighted, were presented to 3 expert annotators who
are native English speakers. They were asked to
annotate the preposition usage instance as one of
the following: extraneous (Error), not extraneous
(OK) or too hard to decide (Unknown); the last cat-
egory was needed for cases where the context was
too messy to make a decision about the highlighted
preposition. On average, the three experts had an
agreement of 0.87 and a kappa of 0.75. For subse-
quent analysis, we only use the classes Error and
OK since Unknown was used extremely rarely and
never by all 3 experts for the same sentence.
We used two different error detection systems to
illustrate our evaluation methodology:2
? LM: A 4-gram language model trained on
the Google Web1T 5-gram Corpus with
SRILM (Stolcke, 2002).
? PERC: An averaged Perceptron (Freund and
Schapire, 1999) classifier? as implemented in
the Learning by Java toolkit (Rizzolo and Roth,
2007)?trained on 7 million examples and us-
ing the same features employed by Tetreault
and Chodorow (2008).
3 Crowdsourcing
Recently,we showed that Amazon Mechanical Turk
(AMT) is a cheap and effective alternative to expert
raters for annotating preposition errors (Tetreault et
al., 2010b). In other current work, we have extended
this pilot study to show that CrowdFlower, a crowd-
sourcing service that allows for stronger quality con-
trol on untrained human raters (henceforth, Turkers),
is more reliable than AMT on three different error
detection tasks (article errors, confused prepositions
2Any conclusions drawn in this paper pertain only to these
specific instantiations of the two systems.
509
& extraneous prepositions). To impose such quality
control, one has to provide ?gold? instances, i.e., ex-
amples with known correct judgments that are then
used to root out any Turkers with low performance
on these instances. For all three tasks, we obtained
20 Turkers? judgments via CrowdFlower for each in-
stance and found that, on average, only 3 Turkers
were required to match the experts.
More specifically, for the extraneous preposition
error task, we used 75 sentences as gold and ob-
tained judgments for the remaining 923 non-gold
sentences.3 We found that if we used 3 Turker judg-
ments in a majority vote, the agreement with any one
of the three expert raters is, on average, 0.87 with a
kappa of 0.76. This is on par with the inter-expert
agreement and kappa found earlier (0.87 and 0.75
respectively).
The extraneous preposition annotation cost only
$325 (923 judgments ? 20 Turkers) and was com-
pleted in a single day. The only restriction on the
Turkers was that they be physically located in the
USA. For the analysis in subsequent sections, we
use these 923 sentences and the respective 20 judg-
ments obtained via CrowdFlower. The 3 expert
judgments are not used any further in this analysis.
4 Revamping System Evaluation
In this section, we provide details on how crowd-
sourcing can help revamp the evaluation of error de-
tection systems: (a) by providing more informative
measures for the intrinsic evaluation of a single sys-
tem (? 4.1), and (b) by easily enabling system com-
parison (? 4.2).
4.1 Crowd-informed Evaluation Measures
When evaluating the performance of grammatical
error detection systems against human judgments,
the judgments for each instance are generally re-
duced to the single most frequent category: Error
or OK. This reduction is not an accurate reflection
of a complex phenomenon. It discards valuable in-
formation about the acceptability of usage because
it treats all ?bad? uses as equal (and all good ones
as equal), when they are not. Arguably, it would
be fairer to use a continuous scale, such as the pro-
portion of raters who judge an instance as correct or
3We found 2 duplicate sentences and removed them.
incorrect. For example, if 90% of raters agree on a
rating of Error for an instance of preposition usage,
then that is stronger evidence that the usage is an er-
ror than if 56% of Turkers classified it as Error and
44% classified it as OK (the sentence ?In addition
classmates play with some game and enjoy? is an ex-
ample). The regular measures of precision and recall
would be fairer if they reflected this reality. Besides
fairness, another reason to use a continuous scale is
that of stability, particularly with a small number of
instances in the evaluation set (quite common in the
field). By relying on majority judgments, precision
and recall measures tend to be unstable (see below).
We modify the measures of precision and re-
call to incorporate distributions of correctness, ob-
tained via crowdsourcing, in order to make them
fairer and more stable indicators of system perfor-
mance. Given an error detection system that classi-
fies a sentence containing a specific preposition as
Error (class 1) if the preposition is extraneous and
OK (class 0) otherwise, we propose the following
weighted versions of hits (Hw), misses (Mw) and
false positives (FPw):
Hw =
N?
i
(cisys ? p
i
crowd) (1)
Mw =
N?
i
((1? cisys) ? p
i
crowd) (2)
FPw =
N?
i
(cisys ? (1? p
i
crowd)) (3)
In the above equations, N is the total number of
instances, cisys is the class (1 or 0) , and p
i
crowd
indicates the proportion of the crowd that classi-
fied instance i as Error. Note that if we were to
revert to the majority crowd judgment as the sole
judgment for each instance, instead of proportions,
picrowd would always be either 1 or 0 and the above
formulae would simply compute the normal hits,
misses and false positives. Given these definitions,
weighted precision can be defined as Precisionw =
Hw/(Hw + FPw) and weighted recall as Recallw =
Hw/(Hw + Mw).
510
agreement
co
un
t
0
100
200
300
400
500
50 60 70 80 90 100
Figure 1: Histogram of Turker agreements for all 923 in-
stances on whether a preposition is extraneous.
Precision Recall
Unweighted 0.957 0.384
Weighted 0.900 0.371
Table 1: Comparing commonly used (unweighted) and
proposed (weighted) precision/recall measures for LM.
To illustrate the utility of these weighted mea-
sures, we evaluated the LM and PERC systems
on the dataset containing 923 preposition instances,
against all 20 Turker judgments. Figure 1 shows a
histogram of the Turker agreement for the major-
ity rating over the set. Table 1 shows both the un-
weighted (discrete majority judgment) and weighted
(continuous Turker proportion) versions of precision
and recall for this system.
The numbers clearly show that in the unweighted
case, the performance of the system is overesti-
mated simply because the system is getting as much
credit for each contentious case (low agreement)
as for each clear one (high agreement). In the
weighted measure we propose, the contentious cases
are weighted lower and therefore their contribution
to the overall performance is reduced. This is a
fairer representation since the system should not be
expected to perform as well on the less reliable in-
stances as it does on the clear-cut instances. Essen-
tially, if humans cannot consistently decide whether
0.0
0.2
0.4
0.6
0.8
1.0
Pre
cisio
n/Re
call
50?75%[n=93] 75?90%[n=114] 90?100%[n=716]Agreement Bin
LM PrecisionPERC PrecisionLM RecallPERC Recall
Figure 2: Unweighted precision/recall by agreement bins
for LM & PERC.
a case is an error then a system?s output cannot be
considered entirely right or entirely wrong.4
As an added advantage, the weighted measures
are more stable. Consider a contentious instance in
a small dataset where 7 out of 15 Turkers (a minor-
ity) classified it as Error. However, it might easily
have happened that 8 Turkers (a majority) classified
it as Error instead of 7. In that case, the change in
unweighted precision would have been much larger
than is warranted by such a small change in the
data. However, weighted precision is guaranteed to
be more stable. Note that the instability decreases
as the size of the dataset increases but still remains a
problem.
4.2 Enabling System Comparison
In this section, we show how to easily compare dif-
ferent systems both on the same data (in the ideal
case of a shared dataset being available) and, more
realistically, on different datasets. Figure 2 shows
(unweighted) precision and recall of LM and PERC
(computed against the majority Turker judgment)
for three agreement bins, where each bin is defined
as containing only the instances with Turker agree-
ment in a specific range. We chose the bins shown
4The difference between unweighted and weighted mea-
sures can vary depending on the distribution of agreement.
511
since they are sufficiently large and represent a rea-
sonable stratification of the agreement space. Note
that we are not weighting the precision and recall in
this case since we have already used the agreement
proportions to create the bins.
This curve enables us to compare the two sys-
tems easily on different levels of item contentious-
ness and, therefore, conveys much more information
than what is usually reported (a single number for
unweighted precision/recall over the whole corpus).
For example, from this graph, PERC is seen to have
similar performance as LM for the 75-90% agree-
ment bin. In addition, even though LM precision is
perfect (1.0) for the most contentious instances (the
50-75% bin), this turns out to be an artifact of the
LM classifier?s decision process. When it must de-
cide between what it views as two equally likely pos-
sibilities, it defaults to OK. Therefore, even though
LM has higher unweighted precision (0.957) than
PERC (0.813), it is only really better on the most
clear-cut cases (the 90-100% bin). If one were to re-
port unweighted precision and recall without using
any bins?as is the norm?this important qualifica-
tion would have been harder to discover.
While this example uses the same dataset for eval-
uating two systems, the procedure is general enough
to allow two systems to be compared on two dif-
ferent datasets by simply examining the two plots.
However, two potential issues arise in that case. The
first is that the bin sizes will likely vary across the
two plots. However, this should not be a significant
problem as long as the bins are sufficiently large. A
second, more serious, issue is that the error rates (the
proportion of instances that are actually erroneous)
in each bin may be different across the two plots. To
handle this, we recommend that a kappa-agreement
plot be used instead of the precision-agreement plot
shown here.
5 Conclusions
Our goal is to propose best practices to address the
two primary problems in evaluating grammatical er-
ror detection systems and we do so by leveraging
crowdsourcing. For system development, we rec-
ommend that rather than compressing multiple judg-
ments down to the majority, it is better to use agree-
ment proportions to weight precision and recall to
yield fairer and more stable indicators of perfor-
mance.
For system comparison, we argue that the best
solution is to use a shared dataset and present the
precision-agreement plot using a set of agreed-upon
bins (possibly in conjunction with the weighted pre-
cision and recall measures) for a more informative
comparison. However, we recognize that shared
datasets are harder to create in this field (as most of
the data is proprietary). Therefore, we also provide
a way to compare multiple systems across differ-
ent datasets by using kappa-agreement plots. As for
agreement bins, we posit that the agreement values
used to define them depend on the task and, there-
fore, should be determined by the community.
Note that both of these practices can also be im-
plemented by using 20 experts instead of 20 Turkers.
However, we show that crowdsourcing yields judg-
ments that are as good but without the cost. To fa-
cilitate the adoption of these practices, we make all
our evaluation code and data available to the com-
munity.5
Acknowledgments
We would first like to thank our expert annotators
Sarah Ohls and Waverely VanWinkle for their hours
of hard work. We would also like to acknowledge
Lei Chen, Keelan Evanini, Jennifer Foster, Derrick
Higgins and the three anonymous reviewers for their
helpful comments and feedback.
References
Cem Akkaya, Alexander Conrad, Janyce Wiebe, and
Rada Mihalcea. 2010. Amazon Mechanical Turk
for Subjectivity Word Sense Disambiguation. In Pro-
ceedings of the NAACL Workshop on Creating Speech
and Language Data with Amazon?s Mechanical Turk,
pages 195?203.
Chris Callison-Burch. 2009. Fast, Cheap, and Creative:
Evaluating Translation Quality Using Amazon?s Me-
chanical Turk. In Proceedings of EMNLP, pages 286?
295.
Jon Chamberlain, Massimo Poesio, and Udo Kruschwitz.
2009. A Demonstration of Human Computation Us-
ing the Phrase Detectives Annotation Game. In ACM
SIGKDD Workshop on Human Computation, pages
23?24.
5http://bit.ly/crowdgrammar
512
Robert Dale and Adam Kilgarriff. 2010. Helping Our
Own: Text Massaging for Computational Linguistics
as a New Shared Task. In Proceedings of INLG.
Keelan Evanini, Derrick Higgins, and Klaus Zechner.
2010. Using Amazon Mechanical Turk for Transcrip-
tion of Non-Native Speech. In Proceedings of the
NAACL Workshop on Creating Speech and Language
Data with Amazon?s Mechanical Turk, pages 53?56.
Rachele De Felice and Stephen Pulman. 2008. A
Classifier-Based Approach to Preposition and Deter-
miner Error Correction in L2 English. In Proceedings
of COLING, pages 169?176.
Tim Finin, William Murnane, Anand Karandikar,
Nicholas Keller, Justin Martineau, and Mark Dredze.
2010. Annotating Named Entities in Twitter Data with
Crowdsourcing. In Proceedings of the NAACL Work-
shop on Creating Speech and Language Data with
Amazon?s Mechanical Turk, pages 80?88.
Yoav Freund and Robert E. Schapire. 1999. Large Mar-
gin Classification Using the Perceptron Algorithm.
Machine Learning, 37(3):277?296.
Michael Gamon, Jianfeng Gao, Chris Brockett, Alexan-
der Klementiev, William Dolan, Dmitriy Belenko, and
Lucy Vanderwende. 2008. Using Contextual Speller
Techniques and Language Modeling for ESL Error
Correction. In Proceedings of IJCNLP.
Michael Gamon. 2010. Using Mostly Native Data to
Correct Errors in Learners? Writing. In Proceedings
of NAACL, pages 163?171.
Y. Guo and Gulbahar Beckett. 2007. The Hegemony
of English as a Global Language: Reclaiming Local
Knowledge and Culture in China. Convergence: In-
ternational Journal of Adult Education, 1.
Ann Irvine and Alexandre Klementiev. 2010. Using
Mechanical Turk to Annotate Lexicons for Less Com-
monly Used Languages. In Proceedings of the NAACL
Workshop on Creating Speech and Language Data
with Amazon?s Mechanical Turk, pages 108?113.
Claudia Leacock, Martin Chodorow, Michael Gamon,
and Joel Tetreault. 2010. Automated Grammatical
Error Detection for Language Learners. Synthesis
Lectures on Human Language Technologies. Morgan
Claypool.
Nitin Madnani. 2010. The Circle of Meaning: From
Translation to Paraphrasing and Back. Ph.D. thesis,
Department of Computer Science, University of Mary-
land College Park.
Scott Novotney and Chris Callison-Burch. 2010. Cheap,
Fast and Good Enough: Automatic Speech Recogni-
tion with Non-Expert Transcription. In Proceedings
of NAACL, pages 207?215.
Nicholas Rizzolo and Dan Roth. 2007. Modeling
Discriminative Global Inference. In Proceedings of
the First IEEE International Conference on Semantic
Computing (ICSC), pages 597?604, Irvine, California,
September.
Alla Rozovskaya and D. Roth. 2010a. Annotating ESL
errors: Challenges and rewards. In Proceedings of the
NAACLWorkshop on Innovative Use of NLP for Build-
ing Educational Applications.
Alla Rozovskaya and D. Roth. 2010b. Generating Con-
fusion Sets for Context-Sensitive Error Correction. In
Proceedings of EMNLP.
Andreas Stolcke. 2002. SRILM: An Extensible Lan-
guage Modeling Toolkit. In Proceedings of the Inter-
national Conference on Spoken Language Processing,
pages 257?286.
Joel Tetreault and Martin Chodorow. 2008. The Ups and
Downs of Preposition Error Detection in ESL Writing.
In Proceedings of COLING, pages 865?872.
Joel Tetreault, Jill Burstein, and Claudia Leacock, edi-
tors. 2010a. Proceedings of the NAACL Workshop on
Innovative Use of NLP for Building Educational Ap-
plications.
Joel Tetreault, Elena Filatova, and Martin Chodorow.
2010b. Rethinking Grammatical Error Annotation and
Evaluation with the Amazon Mechanical Turk. In Pro-
ceedings of the NAACL Workshop on Innovative Use
of NLP for Building Educational Applications, pages
45?48.
Rui Wang and Chris Callison-Burch. 2010. Cheap Facts
and Counter-Facts. In Proceedings of the NAACL
Workshop on Creating Speech and Language Data
with Amazon?s Mechanical Turk, pages 163?167.
Omar F. Zaidan and Chris Callison-Burch. 2010. Pre-
dicting Human-Targeted Translation Edit Rate via Un-
trained Human Annotators. In Proceedings of NAACL,
pages 369?372.
513
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 161?167,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Generalized Character-Level Spelling Error Correction
Noura Farra, Nadi Tomeh?, Alla Rozovskaya, Nizar Habash
Center for Computational Learning Systems, Columbia University
{noura,alla,habash}@ccls.columbia.edu
?LIPN, Universit? Paris 13, Sorbonne Paris Cit?
nadi.tomeh@lipn.univ-paris13.fr
Abstract
We present a generalized discrimina-
tive model for spelling error correction
which targets character-level transforma-
tions. While operating at the charac-
ter level, the model makes use of word-
level and contextual information. In con-
trast to previous work, the proposed ap-
proach learns to correct a variety of er-
ror types without guidance of manually-
selected constraints or language-specific
features. We apply the model to cor-
rect errors in Egyptian Arabic dialect text,
achieving 65% reduction in word error
rate over the input baseline, and improv-
ing over the earlier state-of-the-art system.
1 Introduction
Spelling error correction is a longstanding Natural
Language Processing (NLP) problem, and it has
recently become especially relevant because of the
many potential applications to the large amount
of informal and unedited text generated online,
including web forums, tweets, blogs, and email.
Misspellings in such text can lead to increased
sparsity and errors, posing a challenge for many
NLP applications such as text summarization, sen-
timent analysis and machine translation.
In this work, we present GSEC, a Generalized
character-level Spelling Error Correction model,
which uses supervised learning to map input char-
acters into output characters in context. The ap-
proach has the following characteristics:
Character-level Corrections are learned at the
character-level
1
using a supervised sequence la-
beling approach.
Generalized The input space consists of all
characters, and a single classifier is used to learn
1
We use the term ?character? strictly in the alphabetic
sense, not the logographic sense (as in the Chinese script).
common error patterns over all the training data,
without guidance of specific rules.
Context-sensitive The model looks beyond the
context of the current word, when making a deci-
sion at the character-level.
Discriminative The model provides the free-
dom of adding a number of different features,
which may or may not be language-specific.
Language-Independent In this work, we in-
tegrate only language-independent features, and
therefore do not consider morphological or lin-
guistic features. However, we apply the model
to correct errors in Egyptian Arabic dialect text,
following a conventional orthography standard,
CODA (Habash et al, 2012).
Using the described approach, we demonstrate
a word-error-rate (WER) reduction of 65% over a
do-nothing input baseline, and we improve over
a state-of-the-art system (Eskander et al, 2013)
which relies heavily on language-specific and
manually-selected constraints. We present a de-
tailed analysis of mistakes and demonstrate that
the proposed model indeed learns to correct a
wider variety of errors.
2 Related Work
Most earlier work on automatic error correction
addressed spelling errors in English and built mod-
els of correct usage on native English data (Ku-
kich, 1992; Golding and Roth, 1999; Carlson
and Fette, 2007; Banko and Brill, 2001). Ara-
bic spelling correction has also received consider-
able interest (Ben Othmane Zribi and Ben Ahmed,
2003; Haddad and Yaseen, 2007; Hassan et al,
2008; Shaalan et al, 2010; Alkanhal et al, 2012;
Eskander et al, 2013; Zaghouani et al, 2014).
Supervised spelling correction approaches
trained on paired examples of errors and their cor-
rections have recently been applied for non-native
English correction (van Delden et al, 2004; Li et
al., 2012; Gamon, 2010; Dahlmeier and Ng, 2012;
161
Rozovskaya and Roth, 2011). Discriminative
models have been proposed at the word-level for
error correction (Duan et al, 2012) and for error
detection (Habash and Roth, 2011).
In addition, there has been growing work on lex-
ical normalization of social media data, a some-
what related problem to that considered in this pa-
per (Han and Baldwin, 2011; Han et al, 2013;
Subramaniam et al, 2009; Ling et al, 2013).
The work of Eskander et al (2013) is the
most relevant to the present study: it presents
a character-edit classification model (CEC) using
the same dataset we use in this paper.
2
Eskan-
der et al (2013) analyzed the data to identify the
seven most common types of errors. They devel-
oped seven classifiers and applied them to the data
in succession. This makes the approach tailored to
the specific data set in use and limited to a specific
set of errors. In this work, a single model is con-
sidered for all types of errors. The model consid-
ers every character in the input text for a possible
spelling error, as opposed to looking only at cer-
tain input characters and contexts in which they
appear. Moreover, in contrast to Eskander et al
(2013), it looks beyond the boundary of the cur-
rent word.
3 The GSEC Approach
3.1 Modeling Spelling Correction at the
Character Level
We recast the problem of spelling correction into
a sequence labeling problem, where for each input
character, we predict an action label describing
how to transform it to obtain the correct charac-
ter. The proposed model therefore transforms a
given input sentence e = e
1
, . . . , e
n
of n char-
acters that possibly include errors, to a corrected
sentence c of m characters, where corrected char-
acters are produced by one of the following four
actions applied to each input character e
i
:
? ok: e
i
is passed without transformation.
? substitute ? with(c): e
i
is substituted with
a character c where c could be any character
encountered in the training data.
? delete: e
i
is deleted.
? insert(c): A character c is inserted before
e
i
. To address errors occurring at the end
2
Eskander et al (2013) also considered a slower, more
expensive, and more language-specific method using a mor-
phological tagger (Habash et al, 2013) that outperformed the
CEC model; however, we do not compare to it in this paper.
Input Action Label
k substitute-with(c)
o ok
r insert(r)
e ok
c ok
t ok
d delete
Table 1: Character-level spelling error correction process
on the input word korectd, with the reference word correct
Train Dev Test
Sentences 10.3K 1.67K 1.73K
Characters 675K 106K 103K
Words 134K 21.1K 20.6K
Table 2: ARZ Egyptian dialect corpus statistics
of the sentence, we assume the presence of a
dummy sentence-final stop character.
We use a multi-class SVM classifier to predict the
action labels for each input character e
i
? e. A
decoding process is then applied to transform the
input characters accordingly to produce the cor-
rected sentence. Note that we consider the space
character as a character like any other, which gives
us the ability to correct word merge errors with
space character insertion actions and word split er-
rors with space character deletion actions. Table 1
shows an example of the spelling correction pro-
cess.
In this paper, we only model single-edit actions
and ignore cases where a character requires mul-
tiple edits (henceforth, complex actions), such as
multiple insertions or a combination of insertions
and substitutions. This choice was motivated by
the need to reduce the number of output labels, as
many infrequent labels are generated by complex
actions. An error analysis of the training data, de-
scribed in detail in section 3.2, showed that com-
plex errors are relatively infrequent (4% of data).
We plan to address these errors in future work.
Finally, in order to generate the training data
in the described form, we require a parallel cor-
pus of erroneous and corrected reference text (de-
scribed below), which we align at the character
level. We use the alignment tool Sclite (Fiscus,
1998), which is part of the SCTK Toolkit.
3.2 Description of Data
We apply our model to correcting Egyptian Ara-
bic dialect text. Since there is no standard dialect
orthography adopted by native speakers of Ara-
bic dialects, it is common to encounter multiple
162
Action % Errors Example Error? Reference
Substitute 80.9
E
Alif A @ forms ( @/

@/ @

/

@A?/
?
A/
?
A) 33.3 AHdhm? ?Hdhm ??Yg@? ??Yg

@
E
Ya ?


/? forms ( y/?) 26.7 ?ly? ?l?
?


?
??
??
?
E
h/~ ?/

? , h/w ?/? forms 14.9 kfrh? kfr~ ?Q
	
???

?Q
	
??
E
h/H ?/h forms 2.2 ht?mlhA? Ht?mlhA A????

J?? A????

Jk
Other substitutions 3.8 AltAny~? Al?Any~

?J


	
K A

J? @?

?J


	
K A

J? @ ; dA? dh @X? ?X
Insert 10.5
EP
Insert {A} 3.0 ktbw? ktbwA ?J
.

J?? @?J
.

J?
EP
Insert {space} 2.9 mAtz?l?? mA tz?l?

???
	
Q

KA??

???
	
Q

K A?
Other insertion actions 4.4 Aly? Ally ?


?@?
?


?
? @
Delete 4.7
E
Del{A} 2.4 whmA? whm A???? ???
Other deletion actions 2.3 wfyh? wfy ?J


	
??? ?


	
??
Complex 4.0 mykwn?? mA ykwn?

?
	
??J


??

?
	
??K


A?
Table 3: Character-level distribution of correction labels. We model all types of transformations except complex actions, and
rare Insert labels with counts below a tuned threshold. The Delete label is a single label that comprises all deletion actions.
Labels modeled by Eskander et al (2013) are marked with
E
, and
EP
for cases modeled partially, for example, the Insert{A}
would only be applied at certain positions such as the end of the word.
spellings of the same word. The CODA orthogra-
phy was proposed by Habash et al (2012) in an
attempt to standardize dialectal writing, and we
use it as a reference of correct text for spelling
correction following the previous work by Eskan-
der et al (2013). We use the same corpus (la-
beled "ARZ") and experimental setup splits used
by them. The ARZ corpus was developed by
the Linguistic Data Consortium (Maamouri et al,
2012a-e). See Table 2 for corpus statistics.
Error Distribution Table 3 presents the distri-
bution of correction action labels that correspond
to spelling errors in the training data together with
examples of these errors.
3
We group the ac-
tions into: Substitute, Insert, Delete, and Complex,
and also list common transformations within each
group. We further distinguish between the phe-
nomena modeled by our system and by Eskander
et al (2013). At least 10% of all generated action
labels are not handled by Eskander et al (2013).
3.3 Features
Each input character is represented by a feature
vector. We include a set of basic features inspired
by Eskander et al (2013) in their CEC system and
additional features for further improvement.
Basic features We use a set of nine basic fea-
tures: the given character, the preceding and fol-
lowing two characters, and the first two and last
3
Arabic transliteration is presented in the Habash-Soudi-
Buckwalter scheme (Habash et al, 2007). For more informa-
tion on Arabic orthography in NLP, see (Habash, 2010).
two characters in the word. These are the same
features used by CEC, except that CEC does
not include characters beyond the word boundary,
while we consider space characters as well as char-
acters from the previous and next words.
Ngram features We extract sequences of char-
acters corresponding to the current character and
the following and previous two, three, or four
characters. We refer to these sequences as bi-
grams, trigrams, or 4-grams, respectively. These
are an extension of the basic features and allow
the model to look beyond the context of the cur-
rent word.
3.4 Maximum Likelihood Estimate (MLE)
We implemented another approach for error cor-
rection based on a word-level maximum likeli-
hood model. The MLE method uses a unigram
model which replaces each input word with its
most likely correct word based on counts from the
training data. The intuition behind MLE is that it
can easily correct frequent errors; however, it is
quite dependent on the training data.
4 Experiments
4.1 Model Evaluation
Setup The training data was extracted to gener-
ate the form described in Section 3.1, using the
Sclite tool (Fiscus, 1998) to align the input and
reference sentences. A speech effect handling step
was applied as a preprocessing step to all models.
163
This step removes redundant repetitions of charac-
ters in sequence, e.g.,
Q



J





J






J? ktyyyyyr ?veeeeery?.
The same speech effect handling was applied by
Eskander et al (2013).
For classification, we used the SVM implemen-
tation in YamCha (Kudo and Matsumoto, 2001),
and trained with different variations of the fea-
tures described above. Default parameters were
selected for training (c=1, quadratic kernel, and
context window of +/- 2).
In all results listed below, the baseline corre-
sponds to the do-nothing baseline of the input text.
Metrics Three evaluation metrics are used. The
word-error-rate WER metric is computed by sum-
ming the total number of word-level substitution
errors, insertion errors, and deletion errors in the
output, and dividing by the number of words in the
reference. The correct-rate Corr metric is com-
puted by dividing the number of correct output
words by the total number of words in the refer-
ence. These two metrics are produced by Sclite
(Fiscus, 1998), using automatic alignment. Fi-
nally, the accuracy Acc metric, used by Eskander
et al (2013), is a simple string matching metric
which enforces a word alignment that pairs words
in the reference to those of the output. It is cal-
culated by dividing the number of correct output
words by the number of words in the input. This
metric assumes no split errors in the data (a word
incorrectly split into two words), which is the case
in the data we are working with.
Character-level Model Evaluation The per-
formance of the generalized spelling correction
model (GSEC) on the dev data is presented in the
first half of Table 4. The results of the Eskan-
der et al (2013) CEC system are also presented
for the purpose of comparison. We can see that
using a single classifier, the generalized model is
able to outperform CEC, which relies on a cascade
of classifiers (p = 0.03 for the basic model and
p < 0.0001 for the best model, GSEC+4grams).
4
Model Combination Evaluation Here we
present results on combining GSEC with the
MLE component (GSEC+MLE). We combine the
two models in cascade: the MLE component is
applied to the output of GSEC. To train the MLE
model, we use the word pairs obtained from the
original training data, rather than from the output
of GSEC. We found that this configuration allows
4
Significance results are obtained using McNemar?s test.
Approach Corr%/WER Acc%
Baseline 75.9/24.2 76.8
CEC 88.7/11.4 90.0
GSEC 89.7/10.4* 90.3*
GSEC+2grams 90.6/9.5* 91.2*
GSEC+4grams 91.0/9.2* 91.6*
MLE 89.7/10.4 90.5
CEC + MLE 90.8/9.4 91.5
GSEC+MLE 91.0/9.2 91.3
GSEC+4grams+ MLE 91.7/8.3* 92.2*
Table 4: Model Evaluation. GSEC represents the gener-
alized character-level model. CEC represents the character-
level-edit classification model of Eskander et al (2013).
Rows marked with an asterisk (*) are statistically signifi-
cant compared to CEC (for the first half of the table) or
CEC+MLE (for the second half of the table), with p < 0.05.
us to include a larger sample of word pair errors
for learning, because our model corrects many
errors, leaving fewer example pairs to train an
MLE post-processor. The results are shown in the
second half of Table 4.
We first observe that MLE improves the per-
formance of both CEC and GSEC. In fact,
CEC+MLE and GSEC+MLE perform similarly
(p = 0.36, not statistically significant). When
adding features that go beyond the word bound-
ary, we achieve an improvement over MLE,
GSEC+MLE, and CEC+MLE, all of which are
mostly restricted within the boundary of the word.
The best GSEC model outperforms CEC+MLE
(p < 0.0001), achieving a WER of 8.3%, corre-
sponding to 65% reduction compared to the base-
line. It is worth noting that adding the MLE com-
ponent allows Eskander?s CEC to recover various
types of errors that were not modeled previously.
However, the contribution of MLE is limited to
words that are in the training data. On the other
hand, because GSEC is trained on character trans-
formations, it is likely to generalize better to words
unseen in the training data.
Results on Test Data Table 5 presents the re-
sults of our best model (GSEC+4grams), and best
model+MLE. The latter achieves a 92.1% Acc
score. The Acc score reported by Eskander et al
(2013) for CEC+MLE is 91.3% . The two results
are statistically significant (p < 0.0001) with re-
spect to CEC and CEC+MLE respectively.
Approach Corr%/WER Acc%
Baseline 74.5/25.5 75.5
GSEC+4grams 90.9/9.1 91.5
GSEC+4grams+ MLE 91.8/8.3 92.1
Table 5: Evaluation on test data.
164
4.2 Error Analysis
To gain a better understanding of the performance
of the models on different types of errors and their
interaction with the MLE component, we separate
the words in the dev data into: (1) words seen in
the training data, or in-vocabulary words (IV), and
(2) out-of-vocabulary (OOV) words not seen in
the training data. Because the MLE model maps
every input word to its most likely gold word seen
in the training data, we expect the MLE compo-
nent to recover a large portion of errors in the IV
category (but not all, since an input word can have
multiple correct readings depending on the con-
text). On the other hand, the recovery of errors in
OOV words indicates how well the character-level
model is doing independently of the MLE compo-
nent. Table 6 presents the performance, using the
Acc metric, on each of these types of words. Here
our best model (GSEC+4grams) is considered.
#Inp Words Baseline CEC+MLE GSEC+MLE
OOV 3,289 (17.2%) 70.7 76.5 80.5
IV 15,832 (82.8%) 78.6 94.6 94.6
Total 19,121 (100%) 77.2 91.5 92.2
Table 6: Accuracy of character-level models shown sepa-
rately on out-of-vocabulary and in-vocabulary words.
When considering words seen in the training
data, CEC and GSEC have the same performance.
However, when considering OOV words, GSEC
performs significantly better (p < 0.0001), veri-
fying our hypothesis that a generalized model re-
duces dependency on training data. The data is
heavily skewed towards IV words (83%), which
explains the generally high performance of MLE.
We performed a manual error analysis on a sam-
ple of 50 word errors from the IV set and found
that all of the errors came from gold annotation er-
rors and inconsistencies, either in the dev or train.
We then divided the character transformations in
the OOV words into four groups: (1) characters
that were unchanged by the gold (X-X transforma-
tions), (2) character transformations modeled by
CEC (X-Y CEC), (3) character transformations not
modeled by CEC, and which include all phenom-
ena that were only partially modeled by CEC (X-Y
not CEC), and (4) complex errors. The character-
level accuracy on each of these groups is shown in
Table 7.
Both CEC and GSEC do much better on the
second group of character transformations (that
is, X-Y CEC) than on the third group (X-Y not
CEC). This is not surprising because the former
Type #Chars Example CEC GSEC
X-X 16502 m-m, space-space 99.25 99.33
X-Y 609 ~-h, h-~,
?
A-A 80.62 83.09
(CEC) A-
?
A, y-?
X-Y 161 t-? , del{w} 31.68 43.48
(not CEC) n-ins{space}
Complex 32 n-ins{A}{m} 37.5 15.63
Table 7: Character-level accuracy on different transforma-
tion types for out-of-vocabulary words. For complex trans-
formations, the accuracy represents the complex category
recognition rate, and not the actual correction accuracy.
transformations correspond to phenomena that are
most common in the training data. For GSEC,
they are learned automatically, while for CEC they
are selected and modeled explicitly. Despite this
fact, GSEC generalizes better to OOV words. As
for the third group, both CEC and GSEC per-
form more poorly, but GSEC corrects more errors
(43.48% vs. 31.68% accuracy). Finally, CEC is
better at recognizing complex errors, which, al-
though are not modeled explicitly by CEC, can
sometimes be corrected as a result of applying
multiple classifiers in cascade. Dealing with com-
plex errors, though there are few of them in this
dataset, is an important direction for future work,
and for generalizing to other datasets, e.g., (Za-
ghouani et al, 2014).
5 Conclusions
We showed that a generalized character-level
spelling error correction model can improve
spelling error correction on Egyptian Arabic data.
This model learns common spelling error patterns
automatically, without guidance of manually se-
lected or language-specific constraints. We also
demonstrate that the model outperforms existing
methods, especially on out-of-vocabulary words.
In the future, we plan to extend the model to use
word-level language models to select between top
character predictions in the output. We also plan
to apply the model to different datasets and differ-
ent languages. Finally, we plan to experiment with
more features that can also be tailored to specific
languages by using morphological and linguistic
information, which was not explored in this paper.
Acknowledgments
This publication was made possible by grant
NPRP-4-1058-1-168 from the Qatar National Re-
search Fund (a member of the Qatar Foundation).
The statements made herein are solely the respon-
sibility of the authors.
165
References
Mohamed I. Alkanhal, Mohammed A. Al-Badrashiny,
Mansour M. Alghamdi, and Abdulaziz O. Al-
Qabbany. 2012. Automatic Stochastic Arabic
Spelling Correction With Emphasis on Space Inser-
tions and Deletions. IEEE Transactions on Audio,
Speech & Language Processing, 20:2111?2122.
Michele Banko and Eric Brill. 2001. Scaling to very
very large corpora for natural language disambigua-
tion. In Proceedings of 39th Annual Meeting of the
Association for Computational Linguistics, pages
26?33, Toulouse, France, July.
Chiraz Ben Othmane Zribi and Mohammed Ben
Ahmed. 2003. Efficient Automatic Correction
of Misspelled Arabic Words Based on Contextual
Information. In Proceedings of the Knowledge-
Based Intelligent Information and Engineering Sys-
tems Conference, Oxford, UK.
Andrew Carlson and Ian Fette. 2007. Memory-based
context-sensitive spelling correction at web scale. In
Proceedings of the IEEE International Conference
on Machine Learning and Applications (ICMLA).
Daniel Dahlmeier and Hwee Tou Ng. 2012. A beam-
search decoder for grammatical error correction. In
Proceedings of the 2012 Joint Conference on Empir-
ical Methods in Natural Language Processing and
Computational Natural Language Learning, pages
568?578.
Huizhong Duan, Yanen Li, ChengXiang Zhai, and
Dan Roth. 2012. A discriminative model for
query spelling correction with latent structural svm.
In Proceedings of the 2012 Joint Conference on
Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning,
EMNLP-CoNLL ?12, pages 1511?1521, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Ramy Eskander, Nizar Habash, Owen Rambow, and
Nadi Tomeh. 2013. Processing spontaneous orthog-
raphy. In The Conference of the North American
Chapter of the Association for Computational Lin-
guistics: Human Language Technologies, NAACL
HLT ?13.
Jon Fiscus. 1998. Sclite scoring package ver-
sion 1.5. US National Institute of Standard
Technology (NIST), URL http://www. itl. nist.
gov/iaui/894.01/tools.
Michael Gamon. 2010. Using mostly native data to
correct errors in learners? writing. In Human Lan-
guage Technologies: The 2010 Annual Conference
of the North American Chapter of the Association
for Computational Linguistics, pages 163?171, Los
Angeles, California, June.
Andrew R. Golding and Dan Roth. 1999. A Winnow
based approach to context-sensitive spelling correc-
tion. Machine Learning, 34(1-3):107?130.
Nizar Habash and Ryan M. Roth. 2011. Using deep
morphology to improve automatic error detection in
arabic handwriting recognition. In Proceedings of
the 49th Annual Meeting of the Association for Com-
putational Linguistics: Human Language Technolo-
gies - Volume 1, HLT ?11, pages 875?884, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Nizar Habash, Abdelhadi Soudi, and Tim Buckwalter.
2007. On Arabic Transliteration. In A. van den
Bosch and A. Soudi, editors, Arabic Computa-
tional Morphology: Knowledge-based and Empiri-
cal Methods. Springer.
Nizar Habash, Mona Diab, and Owen Rambow.
2012. Conventional orthography for dialectal Ara-
bic. In Nicoletta Calzolari (Conference Chair),
Khalid Choukri, Thierry Declerck, Mehmet U?gur
Do?gan, Bente Maegaard, Joseph Mariani, Jan
Odijk, and Stelios Piperidis, editors, Proceedings
of the Eight International Conference on Language
Resources and Evaluation (LREC?12), Istanbul,
Turkey, may. European Language Resources Asso-
ciation (ELRA).
Nizar Habash, Ryan Roth, Owen Rambow, Ramy Es-
kander, and Nadi Tomeh. 2013. Morphological
Analysis and Disambiguation for Dialectal Arabic.
In Proceedings of the 2013 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies
(NAACL-HLT), Atlanta, GA.
Nizar Habash. 2010. Introduction to Arabic Natural
Language Processing. Morgan & Claypool Publish-
ers.
Bassam Haddad and Mustafa Yaseen. 2007. Detection
and Correction of Non-Words in Arabic: A Hybrid
Approach. International Journal of Computer Pro-
cessing Of Languages (IJCPOL).
Bo Han and Timothy Baldwin. 2011. Lexical normali-
sation of short text messages: Makn sens a# twitter.
In Proceedings of the 49th Annual Meeting of the
Association for Computational Linguistics: Human
Language Technologies-Volume 1, pages 368?378.
Association for Computational Linguistics.
Bo Han, Paul Cook, and Timothy Baldwin. 2013.
Lexical normalization for social media text. ACM
Transactions on Intelligent Systems and Technology
(TIST), 4(1):5.
Ahmed Hassan, Sara Noeman, and Hany Hassan.
2008. Language Independent Text Correction us-
ing Finite State Automata. In Proceedings of the In-
ternational Joint Conference on Natural Language
Processing (IJCNLP 2008).
Taku Kudo and Yuji Matsumoto. 2001. Chunking
with support vector machines. In Proceedings of
the second meeting of the North American Chap-
ter of the Association for Computational Linguistics
on Language technologies, NAACL ?01, pages 1?
8, Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Karen Kukich. 1992. Techniques for Automatically
Correcting Words in Text. ACM Computing Sur-
veys, 24(4).
166
Yanen Li, Huizhong Duan, and ChengXiang Zhai.
2012. A generalized hidden markov model with dis-
criminative training for query spelling correction. In
Proceedings of the 35th international ACM SIGIR
conference on Research and development in infor-
mation retrieval, SIGIR ?12, pages 611?620, New
York, NY, USA. ACM.
Wang Ling, Chris Dyer, Alan W Black, and Isabel
Trancoso. 2013. Paraphrasing 4 microblog normal-
ization. In Proceedings of the 2013 Conference on
Empirical Methods in Natural Language Process-
ing, pages 73?84, Seattle, Washington, USA, Octo-
ber. Association for Computational Linguistics.
Mohamed Maamouri, Ann Bies, Seth Kulick, Sondos
Krouna, Dalila Tabassi, and Michael Ciul. 2012a.
Egyptian Arabic Treebank DF Part 1 V2.0. LDC
catalog number LDC2012E93.
Mohamed Maamouri, Ann Bies, Seth Kulick, Sondos
Krouna, Dalila Tabassi, and Michael Ciul. 2012b.
Egyptian Arabic Treebank DF Part 2 V2.0. LDC
catalog number LDC2012E98.
Mohamed Maamouri, Ann Bies, Seth Kulick, Sondos
Krouna, Dalila Tabassi, and Michael Ciul. 2012c.
Egyptian Arabic Treebank DF Part 3 V2.0. LDC
catalog number LDC2012E89.
Mohamed Maamouri, Ann Bies, Seth Kulick, Sondos
Krouna, Dalila Tabassi, and Michael Ciul. 2012d.
Egyptian Arabic Treebank DF Part 4 V2.0. LDC
catalog number LDC2012E99.
Mohamed Maamouri, Ann Bies, Seth Kulick, Sondos
Krouna, Dalila Tabassi, and Michael Ciul. 2012e.
Egyptian Arabic Treebank DF Part 5 V2.0. LDC
catalog number LDC2012E107.
Alla Rozovskaya and Dan Roth. 2011. Algorithm se-
lection and model adaptation for esl correction tasks.
In Proc. of the Annual Meeting of the Association of
Computational Linguistics (ACL), Portland, Oregon,
6. Association for Computational Linguistics.
Khaled Shaalan, Rana Aref, and Aly Fahmy. 2010. An
approach for analyzing and correcting spelling er-
rors for non-native Arabic learners. Proceedings of
Informatics and Systems (INFOS).
L Venkata Subramaniam, Shourya Roy, Tanveer A
Faruquie, and Sumit Negi. 2009. A survey of types
of text noise and techniques to handle noisy text.
In Proceedings of The Third Workshop on Analytics
for Noisy Unstructured Text Data, pages 115?122.
ACM.
Sebastian van Delden, David B. Bracewell, and Fer-
nando Gomez. 2004. Supervised and unsupervised
automatic spelling correction algorithms. In Infor-
mation Reuse and Integration, 2004. Proceedings of
the 2004 IEEE International Conference on, pages
530?535.
Wajdi Zaghouani, Behrang Mohit, Nizar Habash, Os-
sama Obeid, Nadi Tomeh, Alla Rozovskaya, Noura
Farra, Sarah Alkuhlani, and Kemal Oflazer. 2014.
Large scale Arabic error annotation: Guidelines and
framework. In Proceedings of the 9th edition of the
Language Resources and Evaluation Conference.
167
Proceedings of the NAACL HLT 2010 Fifth Workshop on Innovative Use of NLP for Building Educational Applications, pages 28?36,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Annotating ESL Errors: Challenges and Rewards
Alla Rozovskaya and Dan Roth
University of Illinois at Urbana-Champaign
Urbana, IL 61801
{rozovska,danr}@illinois.edu
Abstract
In this paper, we present a corrected and error-
tagged corpus of essays written by non-native
speakers of English. The corpus contains
63000 words and includes data by learners of
English of nine first language backgrounds.
The annotation was performed at the sentence
level and involved correcting all errors in the
sentence. Error classification includes mis-
takes in preposition and article usage, errors
in grammar, word order, and word choice. We
show an analysis of errors in the annotated
corpus by error categories and first language
backgrounds, as well as inter-annotator agree-
ment on the task.
We also describe a computer program that was
developed to facilitate and standardize the an-
notation procedure for the task. The program
allows for the annotation of various types of
mistakes and was used in the annotation of the
corpus.
1 Introduction
Work on automated methods for detecting and cor-
recting context dependent mistakes (e.g., (Golding
and Roth, 1996; Golding and Roth, 1999; Carlson
et al, 2001)) has taken an interesting turn over the
last few years, and has focused on correcting mis-
takes made by non-native speakers of English. Non-
native writers make a variety of errors in grammar
and word usage. Recently, there has been a lot of
effort on building systems for detecting mistakes in
article and preposition usage (DeFelice, 2008; Eeg-
Olofsson, 2003; Gamon et al, 2008; Han et al,
2006; Tetreault and Chodorow, 2008b). Izumi et al
(2003) consider several error types, including article
and preposition mistakes, made by Japanese learn-
ers of English, and Nagata et al (2006) focus on the
errors in mass/count noun distinctions with an ap-
plication to detecting article mistakes also made by
Japanese speakers. Article and preposition mistakes
have been shown to be very common mistakes for
learners of different first language (L1) backgrounds
(Dagneaux et al, 1998; Gamon et al, 2008; Izumi
et al, 2004; Tetreault and Chodorow, 2008a), but
there is no systematic study of a whole range of er-
rors non-native writers produce, nor is it clear what
the distribution of different types of mistakes is in
learner language.
In this paper, we describe a corpus of sentences
written by English as a Second Language (ESL)
speakers, annotated for the purposes of developing
an automated system for correcting mistakes in text.
Although the focus of the annotation were errors
in article and preposition usage, all mistakes in the
sentence have been corrected. The data for anno-
tation were taken from two sources: The Interna-
tional Corpus of Learner English (ICLE, (Granger
et al, 2002a)) and Chinese Learners of English Cor-
pus (CLEC, (Gui and Yang, 2003)). The annotated
corpus includes data from speakers of nine first lan-
guage backgrounds. To our knowledge, this is the
first corpus of non-native English text (learner cor-
pus) of fully-corrected sentences from such a diverse
group of learners1. The size of the annotated corpus
is 63000 words, or 2645 sentences. While a corpus
1Possibly, except for the Cambridge Learner Corpus
http://www.cambridge.org/elt
28
of this size may not seem significant in many natu-
ral language applications, this is in fact a large cor-
pus for this field, especially considering the effort to
correct all mistakes, as opposed to focusing on one
language phenomenon. This corpus was used in the
experiments described in the companion paper (Ro-
zovskaya and Roth, 2010).
The annotation schema that we developed was
motivated by our special interest in errors in arti-
cle and preposition usage, but also includes errors
in verbs, morphology, and noun number. The cor-
pus contains 907 article corrections and 1309 prepo-
sition corrections, in addition to annotated mistakes
of other types.
While the focus of the present paper is on anno-
tating ESL mistakes, we have several goals in mind.
First, we present the annotation procedure for the
task, including an error classification schema, anno-
tation speed, and inter-annotator agreement. Sec-
ond, we describe a computer program that we de-
veloped to facilitate the annotation of mistakes in
text. Third, having such a diverse corpus allows
us to analyze the annotated data with respect to the
source language of the learner. We show the anal-
ysis of the annotated data through an overall break-
down of error types by the writer?s first language.
We also present a detailed analysis of errors in arti-
cle and preposition usage. Finally, it should be noted
that there are currently very few annotated learner
corpora available. Consequently, systems are eval-
uated on different data sets, which makes perfor-
mance comparison impossible. The annotation of
the data presented here is available2 and, thus, can
be used by researchers who obtain access to these
respective corpora3.
The rest of the paper is organized as follows.
First, we describe previous work on the annotation
of learner corpora and statistics on ESL mistakes.
Section 3 gives a description of the annotation pro-
cedure, Section 4 presents the annotation tool that
was developed for the purpose of this project and
used in the annotation. We then present error statis-
tics based on the annotated corpus across all error
types and separately for errors in article and preposi-
tion usage. Finally, in Section 6 we describe how we
2Details about the annotation are accessible from
http://L2R.cs.uiuc.edu/?cogcomp/
3The ICLE and CLEC corpora are commercially available.
evaluate inter-annotator agreement and show agree-
ment results for the task.
2 Learner Corpora and Error Tagging
In this section, we review research in the annota-
tion and error analysis of learner corpora. For a
review of learner corpus research see, for exam-
ple, (D??az-Negrillo, 2006; Granger, 2002b; Pravec,
2002). Comparative error analysis is difficult, as
there are no standardized error-tagging schemas, but
we can get a general idea about the types of errors
prevalent with such speakers. Izumi et al (2004a)
describe a speech corpus of Japanese learners of En-
glish (NICT JLE). The corpus is corrected and anno-
tated and consists of the transcripts (2 million words)
of the audio-recordings of the English oral profi-
ciency interview test. In the NICT corpus, whose
error tag set consists of 45 tags, about 26.6% of er-
rors are determiner related, and 10% are preposition
related, which makes these two error types the most
common in the corpus (Gamon et al, 2008). The
Chinese Learners of English corpus (CLEC, (Gui
and Yang, 2003)) is a collection of essays written
by Chinese learners of beginning, intermediate, and
advanced levels. This corpus is also corrected and
error-tagged, but the tagging schema does not allow
for an easy isolation of article and preposition errors.
The International Corpus of Learner English (ICLE,
(Granger et al, 2002a)) is a corpus of argumenta-
tive essays by advanced English learners. The cor-
pus contains 2 million words of writing by European
learners from 14 mother tongue backgrounds. While
the entire corpus is not error-tagged, the French sub-
part of the corpus along with other data by French
speakers of a lower level of proficiency has been an-
notated (Dagneaux et al, 1998). The most com-
mon errors for the advanced level of proficiency
were found to be lexical errors (words) (15%), regis-
ter (10%), articles (10%), pronouns (10%), spelling
(8%) , verbs (8%).
In a study of 53 post-intermediate ESOL (mi-
grant) learners in New Zealand (Bitchener et al,
2005), the most common errors were found to be
prepositions (29%), articles (20%), and verb tense
(22%). Dalgish (1985) conducted a study of er-
rors produced by ESL students enrolled at CUNY.
It was found that across students of different first
29
languages, the most common error types among
24 different error types were errors in article us-
age (28%), vocabulary error (20-25%) (word choice
and idioms), prepositions (18%), and verb-subject
agreement (15%). He also noted that the speakers of
languages without article system made considerably
more article errors, but the breakdown of other error
types across languages was surprisingly similar.
3 Annotation
3.1 Data Selection
Data for annotation were extracted from the ICLE
corpus (Granger et al, 2002a) and CLEC (Gui and
Yang, 2003). As stated in Section 2, the ICLE con-
tains data by European speakers of advanced level
of proficiency, and the CLEC corpus contains es-
says by Chinese learners of different levels of pro-
ficiency. The annotated corpus includes sentences
written by speakers of nine languages: Bulgarian,
Chinese, Czech, French, German, Italian, Polish,
Russian, and Spanish. About half of the sentences
for annotation were selected based on their scores
with respect to a 4-gram language model built using
the English Gigaword corpus (LDC2005T12). This
was done in order to exclude sentences that would
require heavy editing and sentences with near-native
fluency, sentences with scores too high or too low.
Such sentences would be less likely to benefit from
a system on preposition/article correction. The sen-
tences for annotation were a random sample out of
the remaining 80% of the data.
To collect more data for errors in preposition us-
age, we also manually selected sentences that con-
tained such errors. This might explain why the pro-
portion of preposition errors is so high in our data.
3.2 Annotation Procedure
The annotation was performed by three native
speakers of North American English, one under-
graduate and two graduate students, specializing in
foreign languages and Linguistics, with previous ex-
perience in natural language annotation. A sentence
was presented to the annotator in the context of the
essay from which it was extracted. Essay context
can become necessary, especially for the correction
of article errors, when an article is acceptable in the
context of a sentence, but is incorrect in the context
of the essay. The annotators were also encouraged
to propose more than one correction, as long as all
of their suggestions were consistent with the essay
context.
3.3 Annotation Schema
While we were primarily interested in article and
preposition errors, the goal of the annotation was to
correct all mistakes in the sentence. Thus, our er-
ror classification schema4, though motivated by our
interest in errors in article and preposition usage,
was also intended to give us a general idea about
the types of mistakes ESL students make. A better
understanding of the nature of learners? mistakes is
important for the development of a robust automated
system that detects errors and proposes corrections.
Even when the focus of a correction system is on
one language phenomenon, we would like to have
information about all mistakes in the context: Error
information around the target article or preposition
could help us understand how noisy data affect the
performance.
But more importantly, a learner corpus with er-
ror information could demonstrate how mistakes in-
teract in a sentence. A common approach to de-
tecting and correcting context-sensitive mistakes is
to deal with each phenomenon independently, but
sometimes errors cannot be corrected in isolation.
Consider, for example, the following sentences that
are a part of the corpus that we annotated.
1. ?I should know all important aspects of English.? ? ?I should
know all of the important aspects of English.?
2. ?But some of the people thought about him as a parodist of a
rhythm-n-blues singer.? ? ?But some people considered him to
be a parodist of a rhythm-n-blues singer.?
3. ?...to be a competent avionics engineer...? ? ...?to become com-
petent avionics engineers...?
4. ?...which reflect a traditional female role and a traditional attitude
to a woman...? ? ?...which reflect a traditional female role and
a traditional attitude towards women...?
5. ?Marx lived in the epoch when there were no entertainments.?
? ?Marx lived in an era when there was no entertainment.?
In the examples above, errors interact with one an-
other. In example 1, the context requires a definite
article, and the definite article, in turn, calls for the
4Our error classification was inspired by the classification
developed for the annotation of preposition errors (Tetreault and
Chodorow, 2008a).
30
preposition ?of?. In example 2, the definite article
after ?some of? is used extraneously, and deleting it
also requires deleting preposition ?of?. Another case
of interaction is caused by a word choice error: The
writer used the verb ?thought? instead of ?consid-
ered?; replacing the verb requires also changing the
syntactic construction of the verb complement. In
examples 3 and 4, the article choice before the words
?engineer? and ?woman? depends on the number
value of those nouns. To correctly determine which
article should be used, one needs to determine first
whether the context requires a singular noun ?engi-
neer? or plural ?engineers?. Finally, in example 5,
the form of the predicate in the relative clause de-
pends on the number value of the noun ?entertain-
ment?.
For the reasons mentioned above, the annotation
involved correcting all mistakes in a sentence. The
errors that we distinguish are noun number, spelling,
verb form, and word form, in addition to article and
preposition errors . All other corrections, the major-
ity of which are lexical errors, were marked as word
replacement, word deletion, and word insertion. Ta-
ble 1 gives a description of each error type.
4 Annotation Tool
In this section, we describe a computer program that
was developed to facilitate the annotation process.
The main purpose of the program is to allow an an-
notator to easily mark the type of mistake, when cor-
recting it. In addition, the tool allows us to provide
the annotator with sufficient essay context. As de-
scribed in Section 3, sentences for annotation came
from different essays, so each new sentence was usu-
ally extracted from a new context. To ensure that
the annotators preserved the meaning of the sentence
being corrected, we needed to provide them with the
essay context. A wider context could affect the an-
notator?s decision, especially when determining the
correct article choice. The tool allowed us to effi-
ciently present to the annotator the essay context for
each target sentence.
Fig. 1 shows the program interface. The sentence
for annotation appears in the white text box and the
annotator can type corrections in the box, as if work-
ing in a word processor environment. Above and be-
low the text box we can see the context boxes, where
the rest of the essay is shown. Below the lower con-
text box, there is a list of buttons. The pink buttons
and the dark green buttons correspond to different
error types, the pink buttons are for correcting arti-
cle and preposition errors, and the dark green but-
tons ? for correcting other errors. The annotator can
indicate the type of mistake being corrected by plac-
ing the cursor after the word that contains an error
and pressing the button that corresponds to this er-
ror type. Pressing on an error button inserts a pair of
delimiters after the word. The correction can then be
entered between the delimiters. The yellow buttons
and the three buttons next to the pink ones are the
shortcuts that can be used instead of typing in arti-
cles and common preposition corrections. The but-
ton None located next to the article buttons is used
for correcting cases of articles and prepositions used
superfluously. To correct other errors, the annotator
needs to determine the type of error, insert the corre-
sponding delimiters after the word by pressing one
of the error buttons and enter the correction between
the delimiters.
The annotation rate for the three annotators varied
between 30 and 40 sentences per hour.
Table 2 shows sample sentences annotated with
the tool. The proposed corrections are located inside
the delimiters and follow the word to which the cor-
rection refers. When replacing a sequence of words,
the sequence was surrounded with curly braces. This
is useful if a sequence is a multi-word expression,
such as at last.
5 Annotation Statistics
In this section, we present the results of the anno-
tation by error type and the source language of the
writer.
Table 3 shows statistics for the annotated sen-
tences by language group and error type. Because
the sub-corpora differ in size, we show the number
of errors per hundred words. In total, the annotated
corpus contains 63000 words or 2645 sentences of
learner writing. Category punctuation was not spec-
ified in the annotation, but can be easily identified
and includes insertion, deletion, and replacement of
punctuation marks. The largest error category is
word replacement, which combines deleted, inserted
words and word substitutions. This is followed by
31
Error type Description Examples
Article error Any error involving an article ?Women were indignant at [None/the] inequality
from men.?
Preposition error Any error involving a preposition ?...to change their views [to/for] the better.?
Noun number Errors involving plural/singular
confusion of a noun
?Science is surviving by overcoming the mistakes not
by uttering the [truths/truth] .?
Verb form Errors in verb tense and verb inflec-
tions
?He [write/writes] poetry.?
Word form Correct lexeme, but wrong suffix ?It is not [simply/simple] to make professional army
.?
Spelling Error in spelling ?...if a person [commited/committed] a crime...?
Word insertion, deletion,
or replacement
Other corrections that do not fall
into any of the above categories
?There is a [probability/possibility] that today?s fan-
tasies will not be fantasies tomorrow.?
Table 1: Error classification used in annotation
Figure 1: Example of a sentence for annotation as it appears in the annotation tool window. The target sentence is
shown in the white box. The surrounding essay context is shown in the brown boxes. The buttons appear below the
boxes with text: pink buttons (for marking article and preposition errors), dark green (for marking other errors), ligh t
green (article buttons) and yellow (preposition buttons).
Annotated sentence Corrected errors
1. Television becomes their life , and in many cases it replaces their real life /lives/ noun number (life ? lives)
2. Here I ca n?t $help$ but mention that all these people were either bankers or the
Heads of companies or something of that kind @nature, kind@.
word insertion (help); word replacement (kind ? kind,
nature)
3. We exterminated *have exterminated* different kinds of animals verb form (exterminated ? have exterminated)
4. ... nearly 30000 species of plants are under the <a> serious threat of disappear-
ance |disappearing|
article replacement (the ? a); word form (disappear-
ance ? disappearing)
5. There is &a& saying that laziness is the engine of the <None> progress article insertion (a); article deletion (the)
6. ...experience teaches people to strive to <for> the <None> possible things preposition replacement (to ? for); article deletion
(the)
Table 2: Examples of sentences annotated using the annotation tool. Each type of mistake is marked using a different
set of delimiters. The corrected words are enclosed in the delimiters and follow the word to which the correction
refers. In example 2, the annotator preserved the author?s choice kind and added a better choice nature.
32
Source Total Total Errors per Corrections by Error Type
language sent. words 100 words Articles Prepo- Verb Word Noun Word Spell. Word Punc.
sitions form form number order repl.
Bulgarian 244 6197 11.9 10.3% 12.1% 3.5% 3.1% 3.0% 2.0% 5.0% 46.7% 14.2%
Chinese 468 9327 15.1 12.7% 27.2% 7.9% 3.1% 4.6% 1.4% 5.4% 26.2% 11.3%
Czech 296 6570 12.9 16.3% 10.8% 5.2% 3.4% 2.7% 3.2% 8.3% 32.5% 17.5%
French 238 5656 5.8 6.7% 17.4% 2.1% 4.0% 4.6% 3.1% 9.8% 12.5% 39.8%
German 198 5086 11.4 4.0% 13.0% 4.3% 2.8% 1.9% 2.9% 4.7% 15.4% 51.0%
Italian 243 6843 10.6 5.9% 16.6% 6.4% 1.4% 3.0% 2.4% 4.6% 20.5% 39.3%
Polish 198 4642 10.1 15.1% 16.3% 4.0% 1.3% 1.3% 2.3% 2.1% 12.3% 45.2%
Russian 464 10844 13.0 19.2% 17.8% 3.7% 2.5% 2.5% 2.1% 5.0% 28.3% 18.8%
Spanish 296 7760 15.0 11.5% 14.2% 6.0% 3.8% 2.6% 1.6% 11.9% 37.7% 10.7%
All 2645 62925 12.2 12.5% 17.1% 5.2% 2.9% 3.0% 2.2% 6.5% 28.2% 22.5%
Table 3: Error statistics on the annotated data by source language and error type
the punctuation category, which comprises 22% of
all corrections. About 12% of all errors involve ar-
ticles, and prepositions comprise 17% of all errors.
We would expect the preposition category to be less
significant if we did not specifically look for such er-
rors, when selecting sentences for annotation. Two
other common categories are spelling and verb form.
Verb form combines errors in verb conjugation and
errors in verb tense. It can be observed from the
table that there is a significantly smaller proportion
of article errors for the speakers of languages that
have articles, such as French or German. Lexical
errors (word replacement) are more common in lan-
guage groups that have a higher rate of errors per
100 words. In contrast, the proportion of punctua-
tion mistakes is higher for those learners that make
fewer errors overall (cf. French, German, Italian,
and Polish). This suggests that punctuation errors
are difficult to master, maybe because rules of punc-
tuation are not generally taught in foreign language
classes. Besides, there is a high degree of variation
in the use of punctuation even among native speak-
ers.
5.1 Statistics on Article Corrections
As stated in Section 2, article errors are one of the
most common mistakes made by non-native speak-
ers of English. This is especially true for the speak-
ers of languages that do not have articles, but for ad-
vanced French speakers this is also a very common
mistake (Dagneaux et al, 1998), suggesting that ar-
ticle usage in English is a very difficult language fea-
ture to master.
Han et al (2006) show that about 13% of noun
phrases in TOEFL essays by Chinese, Japanese, and
Russian speakers have article mistakes. They also
show that learners do not confuse articles randomly
and the most common article mistakes are omissions
and superfluous article usage. Our findings are sum-
marized in Table 4 and are very similar. We also
distinguish between the superfluous use of a and
the, we allows us to observe that most of the cases
of extraneously used articles involve article the for
all language groups. In fact, extraneous the is the
most common article mistake for the majority of
our speakers. Superfluous the is usually followed
by the omission of the and the omission of a. An-
other statistic that our table demonstrates and that
was shown previously (e.g. (Dalgish, 1985)) is that
learners whose first language does not have articles
make more article mistakes: We can see from col-
umn 3 of the table that the speakers of German,
French and Italian are three to four times less likely
to make an article mistake than the speakers of Chi-
nese and all of the Slavic languages. The only ex-
ception are Spanish speakers. It is not clear whether
the higher error rate is only due to a difference in
overall language proficiency (as is apparent from the
average number of mistakes by these speakers in Ta-
ble 3) or to other factors. Finally, the last column in
the table indicates that confusing articles with pro-
nouns is a relatively common error and on average
accounts for 10% of all article mistakes5. Current
article correction systems do not address this error
type.
5An example of such confusion is ? To pay for the crimes,
criminals are put in prison?, where the is used instead of their.
33
Source Errors Errors Article mistakes by error type
language total per 100 Miss. Miss. Extr. Extr. Confu- Mult. Other
words the a the a sion labels
Bulgarian 76 1.2 9% 25% 41% 3% 8% 1% 13%
Chinese 179 1.9 20% 12% 48% 4% 7% 2% 7%
Czech 138 2.1 29% 13% 29% 9% 7% 4% 9%
French 22 0.4 9% 14% 36% 14% 0% 23% 5%
German 23 0.5 22% 9% 22% 4% 8% 9% 26%
Italian 43 0.6 16% 40% 26% 2% 9% 0% 7%
Polish 71 1.5 37% 18% 17% 8% 11% 4% 4%
Russian 271 2.5 24% 18% 31% 6% 11% 1% 9%
Spanish 134 1.7 16% 10% 51% 7% 3% 1% 10%
All 957 1.5 22% 16% 36% 6% 8% 3% 9%
Table 4: Distribution of article mistakes by error type and source language of the writer. Confusion error type refers to
confusing articles a and the. Multiple labels denotes cases where the annotator specified more than one article choice,
one of which was used by the learner. Other refers to confusing articles with possessive and demonstrative pronouns.
5.2 Statistics on Preposition Corrections
Table 5 shows statistics on errors in preposition us-
age. Preposition mistakes are classified into three
categories: replacements, insertions, and deletions.
Unlike with article errors, the most common type
of preposition errors is confusing two prepositions.
This category accounts for more than half of all er-
rors, and the breakdown is very similar for all lan-
guage groups. The fourth category in the table, with
original, refers to the preposition usages that were
found acceptable by the annotators, but with a bet-
ter suggestion provided. We distinguish this case
as a separate category because preposition usage is
highly variable, unlike, for example, article usage.
Tetreault and Chodorow (Tetreault and Chodorow,
2008a) show that agreement between two native
speakers on a cloze test targeting prepositions is
about 76%, which demonstrates that there are many
contexts that license multiple prepositions.
6 Inter-annotator Agreement
Correcting non-native text for a variety of mistakes
is challenging and requires a number of decisions on
the part of the annotator. Human language allows for
many ways to express the same idea. Furthermore, it
is possible that the corrected sentence, even when it
does not contain clear mistakes, does not sound like
a sentence produced by a native speaker. The latter
is complicated by the fact that native speakers differ
widely with respect to what constitutes acceptable
usage (Tetreault and Chodorow, 2008a).
To date, a common approach to annotating non-
native text has been to use one rater (Gamon et al,
Source Errors Errors Mistakes by error type
language total per 100 Repl. Ins. Del. With
words orig.
Bulgarian 89 1.4 58% 22% 11% 8%
Chinese 384 4.1 52% 24% 22% 2%
Czech 91 1.4 51% 21% 24% 4%
French 57 1.0 61% 9% 12% 18%
German 75 1.5 61% 8% 16% 15%
Italian 120 1.8 57% 22% 12% 8%
Polish 77 1.7 49% 18% 16% 17%
Russian 251 2.3 53% 21% 17% 9%
Spanish 165 2.1 55% 20% 19% 6%
All 1309 2.1 54% 21% 18% 7%
Table 5: Distribution of preposition mistakes by error
type and source language of the writer. With orig refers to
prepositions judged as acceptable by the annotators, but
with a better suggestion provided.
2008; Han et al, 2006; Izumi et al, 2004; Na-
gata et al, 2006). The output of human annota-
tion is viewed as the gold standard when evaluating
an error detection system. The question of reliabil-
ity of using one rater has been raised in (Tetreault
and Chodorow, 2008a), where an extensive reliabil-
ity study of human judgments in rating preposition
usage is described. In particular, it is shown that
inter-annotator agreement on preposition correction
is low (kappa value of 0.63) and that native speakers
do not always agree on whether a specific preposi-
tion constitutes acceptable usage.
We measure agreement by asking an annotator
whether a sentence corrected by another person is
correct. After all, our goal was to make the sentence
sound native-like, without enforcing that errors are
corrected in the same way. One hundred sentences
annotated by each person were selected and the cor-
34
Agreement set Rater Judged Judged
correct incorrect
Agreement set 1 Rater #2 37 63Rater #3 59 41
Agreement set 2 Rater #1 79 21Rater #3 73 27
Agreement set 3 Rater #1 83 17Rater #2 47 53
Table 6: Annotator agreement at the sentence level. The
number next to the agreement set denotes the annotator
who corrected the sentences on the first pass. Judged cor-
rect denotes the proportion of sentences in the agreement
set that the second rater did not change. Judged incorrect
denotes the proportion of sentences, in which the second
rater made corrections.
rections were applied. This corrected set was mixed
with new sentences and given to the other two anno-
tators. In this manner, each annotator received two
hundred sentences corrected by the other two anno-
tators. For each pair of the annotators, we compute
agreement based on the 100 sentences on which they
did a second pass after the initial corrections by the
third rater. To compute agreement at the sentence
level, we assign the annotated sentences to one of
the two categories: ?correct? and ?incorrect?: A sen-
tence is considered ?correct? if a rater did not make
any corrections in it on the second pass 6. Table 6
shows for each agreement set the number of sen-
tences that were corrected on the second pass. On
average, 40.8% of the agreement set sentences be-
long to the ?incorrect? category, but the proportion
of ?incorrect? sentences varies across annotators.
We also compute agreement on the two cate-
gories, ?correct? and ?incorrect?. The agreement
and the kappa values are shown in Table 7. Agree-
ment on the sentences corrected on the second pass
varies between 56% to 78% with kappa values rang-
ing from 0.16 to 0.40. The low numbers reflect the
difficulty of the task and the variability of the na-
tive speakers? judgments about acceptable usage. In
fact, since the annotation requires looking at sev-
eral phenomena, we can expect a lower agreement,
when compared to agreement rate on one language
phenomenon. Suppose rater A disagrees with rater
B on a given phenomenon with probability 1/4,
then, when there are two phenomena, the probabil-
ity that he will disagree with at least on of them is
6We ignore punctuation corrections.
Agreement set Agreement kappa
Agreement set 1 56% 0.16
Agreement set 2 78% 0.40
Agreement set 3 60% 0.23
Table 7: Agreement at the sentence level. Agreement
shows how many sentences in each agreement set were
assigned to the same category (?correct?, ?incorrect?) for
each of the two raters.
1 ? 9/16 = 7/16. And the probability goes down
with the number of phenomena.
7 Conclusion
In this paper, we presented a corpus of essays by stu-
dents of English of nine first language backgrounds,
corrected and annotated for errors. To our knowl-
edge, this is the first fully-corrected corpus that con-
tains such diverse data. We have described an anno-
tation schema, have shown statistics on the error dis-
tribution for writers of different first language back-
grounds and inter-annotator agreement on the task.
We have also described a program that was devel-
oped to facilitate the annotation process.
While natural language annotation, especially in
the context of error correction, is a challenging and
time-consuming task, research in learner corpora
and annotation is important for the development of
robust systems for correcting and detecting errors.
Acknowledgments
We thank the anonymous reviewers for their helpful
comments. This research is partly supported by a
grant from the U.S. Department of Education.
References
J. Bitchener, S. Young and D. Cameron. 2005. The Ef-
fect of Different Types of Corrective Feedback on ESL
Student Writing. Journal of Second Language Writ-
ing.
A. J. Carlson and J. Rosen and D. Roth. 2001. Scaling
Up Context Sensitive Text Correction. IAAI, 45?50.
M. Chodorow, J. Tetreault and N-R. Han. 2007. De-
tection of Grammatical Errors Involving Prepositions.
Proceedings of the Fourth ACL-SIGSEM Workshop on
Prepositions.
E. Dagneaux, S. Denness and S. Granger. 1998.
Computer-aided Error Analysis. System, 26:163?174.
35
G. Dalgish. 1985. Computer-assisted ESL Research.
CALICO Journal, 2(2).
G. Dalgish. 1991. Computer-Assisted Error Analysis
and Courseware Design: Applications for ESL in the
Swedish Context. CALICO Journal, 9.
R. De Felice and S. Pulman. 2008. A Classifier-Based
Approach to Preposition and Determiner Error Correc-
tion in L2 English. In Proceedings of COLING-08.
A. D??az-Negrillo and J. Ferna?ndez-Dom??nguez. 2006.
Error Tagging Systems for Learner Corpora. RESLA,
19:83-102.
J. Eeg-Olofsson and O. Knuttson. 2003. Automatic
Grammar Checking for Second Language Learners -
the Use of Prepositions. In Nodalida.
M. Gamon, J. Gao, C. Brockett, A. Klementiev, W.
Dolan, D. Belenko and L. Vanderwende. 2008. Using
Contextual Speller Techniques and Language Model-
ing for ESL Error Correction. Proceedings of IJCNLP.
A. R. Golding and D. Roth. 1996. Applying Winnow
to Context-Sensitive Spelling Correction. ICML, 182?
190.
A. R. Golding and D. Roth. 1999. A Winnow based ap-
proach to Context-Sensitive Spelling Correction. Ma-
chine Learning, 34(1-3):107?130.
S. Granger, E. Dagneaux and F. Meunier. 2002. Interna-
tional Corpus of Learner English
S. Granger. 2002. A Bird?s-eye View of Learner Cor-
pus Research. Computer Learner Corpora, Second
Language Acquisition and Foreign Language Teach-
ing, Eds. S. Granger, J. Hung and S. Petch-Tyson,
Amsterdam: John Benjamins. 3?33.
S. Gui and H. Yang. 2003. Zhongguo Xuexizhe Yingyu
Yuliaohu. (Chinese Learner English Corpus). Shang-
hai Waiyu Jiaoyu Chubanshe. (In Chinese).
N. Han, M. Chodorow and C. Leacock. 2006. Detect-
ing Errors in English Article Usage by Non-native
Speakers. Journal of Natural Language Engineering,
12(2):115?129.
E. Izumi, K. Uchimoto, T. Saiga and H. Isahara. 2003.
Automatic Error Detection in the Japanese Leaners
English Spoken Data. ACL.
E. Izumi, K. Uchimoto and H. Isahara. 2004. The
Overview of the SST Speech Corpus of Japanese
Learner English and Evaluation through the Exper-
iment on Automatic Detection of Learners? Errors.
LREC.
E. Izumi, K. Uchimoto and H. Isahara. 2004. The
NICT JLE Corpus: Exploiting the Language Learner?s
Speech Database for Research and Education. Inter-
national Journal of the Computer, the Internet and
Management, 12(2):119?125.
R. Nagata, A. Kawai, K. Morihiro, and N. Isu. 2006. A
Feedback-Augmented Method for Detecting Errors in
the Writing of Learners of English. ACL/COLING.
N. Pravec. 2002. Survey of learner corpora. ICAME
Journal, 26:81?114.
A. Rozovskaya and D. Roth 2010. Training Paradigms
for Correcting Errors in Grammar and Usage. In Pro-
ceedings of the NAACL-HLT, Los-Angeles, CA.
J. Tetreault and M. Chodorow. 2008. Native Judgments
of Non-Native Usage: Experiments in Preposition Er-
ror Detection. COLING Workshop on Human Judg-
ments in Computational Linguistics, Manchester, UK.
J. Tetreault and M. Chodorow. 2008. The Ups and
Downs of Preposition Error Detection in ESL Writing.
COLING, Manchester, UK.
36
Proceedings of the 15th Conference on Computational Natural Language Learning: Shared Task, pages 40?44,
Portland, Oregon, 23-24 June 2011. c?2011 Association for Computational Linguistics
Inference Protocols for Coreference Resolution
Kai-Wei Chang Rajhans Samdani
Alla Rozovskaya Nick Rizzolo
Mark Sammons Dan Roth
University of Illinois at Urbana-Champaign
{kchang10|rsamdan2|rozovska|rizzolo|mssammon|danr}@illinois.edu
Abstract
This paper presents Illinois-Coref, a system
for coreference resolution that participated
in the CoNLL-2011 shared task. We in-
vestigate two inference methods, Best-Link
and All-Link, along with their corresponding,
pairwise and structured, learning protocols.
Within these, we provide a flexible architec-
ture for incorporating linguistically-motivated
constraints, several of which we developed
and integrated. We compare and evaluate the
inference approaches and the contribution of
constraints, analyze the mistakes of the sys-
tem, and discuss the challenges of resolving
coreference for the OntoNotes-4.0 data set.
1 Introduction
The coreference resolution task is challenging, re-
quiring a human or automated reader to identify
denotative phrases (?mentions?) and link them to
an underlying set of referents. Human readers use
syntactic and semantic cues to identify and dis-
ambiguate the referring phrases; a successful auto-
mated system must replicate this behavior by linking
mentions that refer to the same underlying entity.
This paper describes Illinois-Coref, a corefer-
ence resolution system built on Learning Based
Java (Rizzolo and Roth, 2010), that participated
in the ?closed? track of the CoNLL-2011 shared
task (Pradhan et al, 2011). Building on elements
of the coreference system described in Bengtson
and Roth (2008), we design an end-to-end system
(Sec. 2) that identifies candidate mentions and then
applies one of two inference protocols, Best-Link
and All-Link (Sec. 2.3), to disambiguate and clus-
ter them. These protocols were designed to easily
incorporate domain knowledge in the form of con-
straints. In Sec. 2.4, we describe the constraints that
we develop and incorporate into the system. The
different strategies for mention detection and infer-
ence, and the integration of constraints are evaluated
in Sections 3 and 4.
2 Architecture
Illinois-Coref follows the architecture used in
Bengtson and Roth (2008). First, candidate men-
tions are detected (Sec. 2.1). Next, a pairwise
classifier is applied to each pair of mentions, gen-
erating a score that indicates their compatibility
(Sec. 2.2). Next, at inference stage, a coreference
decoder (Sec. 2.3) aggregates these scores into men-
tion clusters. The original system uses the Best-Link
approach; we also experiment with All-Link decod-
ing. This flexible decoder architecture allows lin-
guistic or knowledge-based constraints to be easily
added to the system: constraints may force mentions
to be coreferent or non-coreferent and can be option-
ally used in either of the inference protocols. We
designed and implemented several such constraints
(Sec. 2.4). Finally, since mentions that are in single-
ton clusters are not annotated in the OntoNotes-4.0
data set, we remove those as a post-processing step.
2.1 Mention Detection
Given a document, a mention detector generates a
set of mention candidates that are used by the subse-
quent components of the system. A robust mention
detector is crucial, as detection errors will propagate
to the coreference stage. As we show in Sec. 3, the
system that uses gold mentions outperforms the sys-
tem that uses predicted mentions by a large margin,
from 15% to 18% absolute difference.
40
For the ACE 2004 coreference task, a good per-
formance in mention detection is typically achieved
by training a classifier e.g., (Bengtson and Roth,
2008). However, this model is not appropriate for
the OntoNotes-4.0 data set, in which (in contrast to
the ACE 2004 corpus) singleton mentions are not
annotated: a specific noun phrase (NP) may corre-
spond to a mention in one document but will not
be a mention in another document. Therefore, we
designed a high recall (? 90%) and low precision
(? 35%) rule-based mention detection system that
includes all phrases recognized as Named Entities
(NE?s) and all phrases tagged as NPs in the syntac-
tic parse of the text. As a post-processing step, we
remove all predicted mentions that remain in single-
ton clusters after the inference stage.
The best mention detection result on the DEV set1
is 64.93% in F1 score (after coreference resolution)
and is achieved by our best inference protocol, Best-
Link with constraints.
2.2 Pairwise Mention Scoring
The basic input to our inference algorithm is a pair-
wise mention score, which indicates the compatibil-
ity score of a pair of mentions. For any two mentions
u and v, the compatibility score wuv is produced
by a pairwise scoring component that uses extracted
features ?(u, v) and linguistic constraints c:
wuv = w ? ?(u, v) + c(u, v) + t, (1)
where w is a weight vector learned from training
data, c(u, v) is a compatibility score given by the
constraints, and t is a threshold parameter (to be
tuned). We use the same features as Bengtson and
Roth (2008), with the knowledge extracted from the
OntoNotes-4.0 annotation. The exact use of the
scores and the procedure for learning weights w are
specific to the inference algorithm and are described
next.
2.3 Inference
In this section, we present our inference techniques
for coreference resolution. These clustering tech-
niques take as input a set of pairwise mention scores
over a document and aggregate them into globally
1In the shared task, the data set is split into three sets:
TRAIN, DEV, and TEST.
consistent cliques representing entities. We investi-
gate the traditional Best-Link approach and a more
intuitively appealing All-Link algorithm.
2.3.1 Best-Link
Best-Link is a popular approach to coreference
resolution. For each mention, it considers the best
mention on its left to connect to (best according
the pairwise score wuv) and creates a link between
them if the pairwise score is above some thresh-
old. Although its strategy is simple, Bengtson and
Roth (2008) show that with a careful design, it can
achieve highly competitive performance.
Inference: We give an integer linear programming
(ILP) formulation of Best-Link inference in order to
present both of our inference algorithms within the
same framework. Given a pairwise scorer w, we
can compute the compatibility scores ? wuv from
Eq. (1) ? for all mention pairs u and v. Let yuv be
a binary variable, such that yuv = 1 only if u and v
are in the same cluster. For a document d, Best-Link
solves the following ILP formulation:
argmaxy
?
u,v
wuvyuv
s.t
?
u<v
yuv ? 1 ?v,
yuw ? {0, 1}.
(2)
Eq. (2) generates a set of connected components and
all the mentions in each connected component con-
stitute an entity.
Learning: We follow the strategy in (Bengtson
and Roth, 2008, Section 2.2) to learn the pairwise
scoring function w. The scoring function is trained
on:
? Positive examples: for each mention u, we con-
struct a positive example (u, v), where v is the
closest preceding mention in u?s equivalence
class.
? Negative examples: all mention pairs (u, v),
where v is a preceding mention of u and u, v
are not in the same class.
As a result of the singleton mentions not being anno-
tated, there is an inconsistency in the sample distri-
butions in the training and inference phases. There-
fore, we apply the mention detector to the training
set, and train the classifier using the union set of gold
and predicted mentions.
41
2.3.2 All-Link
The All-Link inference approach scores a cluster-
ing of mentions by including all possible pairwise
links in the score. It is also known as correlational
clustering (Bansal et al, 2002) and has been applied
to coreference resolution in the form of supervised
clustering (Mccallum and Wellner, 2003; Finley and
Joachims, 2005).
Inference: Similar to Best-Link, for a document d,
All-Link inference finds a clustering All-Link(d;w)
by solving the following ILP problem:
argmaxy
?
u,v
wuvyuv
s.t yuw ? yuv + yvw ? 1 ?u,w, v,
yuw ? {0, 1}.
(3)
The inequality constraints in Eq. (3) enforce the
transitive closure of the clustering. The solution of
Eq. (3) is a set of cliques, and the mentions in the
same cliques corefer.
Learning: We present a structured perceptron al-
gorithm, which is similar to supervised clustering
algorithm (Finley and Joachims, 2005) to learn w.
Note that as an approximation, it is certainly pos-
sible to use the weight parameter learned by using,
say, averaged perceptron over positive and negative
links. The pseudocode is presented in Algorithm 1.
Algorithm 1 Structured Perceptron like learning al-
gorithm for All-Link inference
Given: Annotated documents D and initial
weight winit
Initialize w ? winit
for Document d in D do
Clustering y ? All-Link(d;w)
for all pairs of mentions u and v do
I1(u, v) = [u, v coreferent in D]
I2(u, v) = [y(u) = y(v)]
w ? w +
(
I1(u, v)? I2(u, v)
)
?(u, v)
end for
end for
return w
For the All-Link clustering, we drop one of the
three transitivity constraints for each triple of men-
tion variables. Similar to Pascal and Baldridge
(2009), we observe that this improves accuracy ?
the reader is referred to Pascal and Baldridge (2009)
for more details.
2.4 Constraints
The constraints in our inference algorithm are based
on the analysis of mistakes on the DEV set2. Since
the majority of errors are mistakes in recall, where
the system fails to link mentions that refer to the
same entity, we define three high precision con-
straints that improve recall on NPs with definite de-
terminers and mentions whose heads are NE?s.
The patterns used by constraints to match mention
pairs have some overlap with those used by the pair-
wise mention scorer, but their formulation as con-
straints allow us to focus on a subset of mentions
to which a certain pattern applies with high preci-
sion. For example, the constraints use a rule-based
string similarity measure that accounts for the in-
ferred semantic type of the mentions compared. Ex-
amples of mention pairs that are correctly linked by
the constraints are: Governor Bush? Bush; a cru-
cial swing state , Florida? Florida; Sony itself ?
Sony; Farmers? Los Angeles - based Farmers.
3 Experiments and Results
In this section, we present the performance of the
system on the OntoNotes-4.0 data set. A previous
experiment using an earlier version of this data can
be found in (Pradhan et al, 2007). Table 1 shows the
performance for the two inference protocols, with
and without constraints. Best-Link outperforms All-
Link for both predicted and gold mentions. Adding
constraints improves the performance slightly for
Best-Link on predicted mentions. In the other con-
figurations, the constraints either do not affect the
performance or slightly degrade it.
Table 2 shows the results obtained on TEST, using
the best system configurations found on DEV. We
report results on predicted mentions with predicted
boundaries, predicted mentions with gold bound-
aries, and when using gold mentions3.
2We provide a more detailed analysis of the errors in Sec. 4.
3Note that the gold boundaries results are different from the
gold mention results. Specifying gold mentions requires coref-
erence resolution to exclude singleton mentions. Gold bound-
aries are provided by the task organizers and also include sin-
gleton mentions.
42
Method
Pred. Mentions w/Pred. Boundaries Gold Mentions
MD MUC BCUB CEAF AVG MUC BCUB CEAF AVG
Best-Link 64.70 55.67 69.21 43.78 56.22 80.58 75.68 64.69 73.65
Best-Link W/ Const. 64.69 55.8 69.29 43.96 56.35 80.56 75.02 64.24 73.27
All-Link 63.30 54.56 68.50 42.15 55.07 77.72 73.65 59.17 70.18
All-Link W/ Const. 63.39 54.56 68.46 42.20 55.07 77.94 73.43 59.47 70.28
Table 1: The performance of the two inference protocols on both gold and predicted mentions. The systems are
trained on the TRAIN set and evaluated on the DEV set. We report the F1 scores (%) on mention detection (MD)
and coreference metrics (MUC, BCUB, CEAF). The column AVG shows the averaged scores of the three coreference
metrics.
Task MD MUC BCUB CEAF AVG
Pred. Mentions w/ Pred. Boundaries 64.88 57.15 67.14 41.94 55.96
Pred. Mentions w/ Gold Boundaries 67.92 59.79 68.65 41.42 56.62
Gold Mentions - 82.55 73.70 65.24 73.83
Table 2: The results of our submitted system on the TEST set. The system uses Best-Link decoding with constraints
on predicted mentions and Best-Link decoding without constraints on gold mentions. The systems are trained on a
collection of TRAIN and DEV sets.
4 Discussion
Most of the mistakes made by the system are due to
not linking co-referring mentions. The constraints
improve slightly the recall on a subset of mentions,
and here we show other common errors for the sys-
tem. For instance, the system fails to link the two
mentions, the Emory University hospital in Atlanta
and the hospital behind me, since each of the men-
tions has a modifier that is not part of the other men-
tion. Another common error is related to pronoun
resolution, especially when a pronoun has several
antecedents in the immediate context, appropriate in
gender, number, and animacy, as in ? E. Robert Wal-
lach was sentenced by a U.S. judge in New York to
six years in prison and fined $ 250,000 for his rack-
eteering conviction in the Wedtech scandal .?: both
E. Robert Wallach and a U.S. judge are appropri-
ate antecedents for the pronoun his. Pronoun errors
are especially important to address since 35% of the
mentions are pronouns.
The system also incorrectly links some mentions,
such as: ?The suspect said it took months to repack-
age...? (?it? cannot refer to a human); ?They see
them.? (subject and object in the same sentence are
linked); and ?Many freeway accidents occur simply
because people stay inside the car and sort out...?
(the NP the car should not be linked to any other
mention, since it does not refer to a specific entity).
5 Conclusions
We have investigated a coreference resolution sys-
tem that uses a rich set of features and two popular
types of clustering algorithm.
While the All-Link clustering seems to be capable
of taking more information into account for making
clustering decisions, as it requires each mention in
a cluster to be compatible with all other mentions in
that cluster, the Best-Link approach still outperforms
it. This raises a natural algorithmic question regard-
ing the inherent nature of clustering style most suit-
able for coreference and regarding possible ways of
infusing more knowledge into different coreference
clustering styles. Our approach accommodates in-
fusion of knowledge via constraints, and we have
demonstrated its utility in an end-to-end coreference
system.
Acknowledgments This research is supported by the Defense
Advanced Research Projects Agency (DARPA) Machine Reading Pro-
gram under Air Force Research Laboratory (AFRL) prime contract no.
FA8750-09-C-0181 and the Army Research Laboratory (ARL) under
agreement W911NF-09-2-0053. Any opinions, findings, and conclu-
sion or recommendations expressed in this material are those of the au-
thor(s) and do not necessarily reflect the view of the DARPA, AFRL,
ARL or the US government.
43
References
N. Bansal, A. Blum, and S. Chawla. 2002. Correlation
clustering. In Proceedings of the 43rd Symposium on
Foundations of Computer Science.
E. Bengtson and D. Roth. 2008. Understanding the value
of features for coreference resolution. In EMNLP, 10.
T. Finley and T. Joachims. 2005. Supervised cluster-
ing with support vector machines. In Proceedings
of the International Conference on Machine Learning
(ICML).
A. Mccallum and B. Wellner. 2003. Toward condi-
tional models of identity uncertainty with application
to proper noun coreference. In The Conference on
Advances in Neural Information Processing Systems
(NIPS).
D. Pascal and J. Baldridge. 2009. Global joint models for
coreference resolution and named entity classification.
In Procesamiento del Lenguaje Natural.
S. Pradhan, L. Ramshaw, R. Weischedel, J. MacBride,
and L. Micciulla. 2007. Unrestricted Coreference:
Identifying Entities and Events in OntoNotes. In in
Proceedings of the IEEE International Conference on
Semantic Computing (ICSC), September 17-19.
S. Pradhan, L. Ramshaw, M. Marcus, M. Palmer,
R. Weischedel, and N. Xue. 2011. Conll-2011 shared
task: Modeling unrestricted coreference in ontonotes.
In Proceedings of the Annual Conference on Compu-
tational Natural Language Learning (CoNLL).
N. Rizzolo and D. Roth. 2010. Learning Based Java
for Rapid Development of NLP Systems. In Proceed-
ings of the International Conference on Language Re-
sources and Evaluation (LREC), Valletta, Malta, 5.
44
The 7th Workshop on the Innovative Use of NLP for Building Educational Applications, pages 272?280,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
The UI System in the HOO 2012 Shared Task on Error Correction
Alla Rozovskaya Mark Sammons Dan Roth
Cognitive Computation Group
University of Illinois at Urbana-Champaign
Urbana, IL 61801
{rozovska,mssammon,danr}@illinois.edu
Abstract
We describe the University of Illinois (UI) sys-
tem that participated in the Helping Our Own
(HOO) 2012 shared task, which focuses on
correcting preposition and determiner errors
made by non-native English speakers. The
task consisted of three metrics: Detection,
Recognition, and Correction, and measured
performance before and after additional revi-
sions to the test data were made. Out of 14
teams that participated, our system scored first
in Detection and Recognition and second in
Correction before the revisions; and first in
Detection and second in the other metrics af-
ter revisions. We describe our underlying ap-
proach, which relates to our previous work in
this area, and propose an improvement to the
earlier method, error inflation, which results
in significant gains in performance.
1 Introduction
The task of correcting grammar and usage mistakes
made by English as a Second Language (ESL) writ-
ers is difficult: many of these errors are context-
sensitive mistakes that confuse valid English words
and thus cannot be detected without considering the
context around the word.
Below we show examples of two common ESL
mistakes considered in this paper:
1. ?Nowadays ?*/the Internet makes us closer and closer.?
2. ?I can see at*/on the list a lot of interesting sports.?
In (1), the definite article is incorrectly omitted.
In (2), the writer uses an incorrect preposition.
This paper describes the University of Illinois sys-
tem that participated in the HOO 2012 shared task
on error detection and correction in the use of prepo-
sitions and determiners (Dale et al, 2012). Fourteen
teams took part in the the competition. The scoring
included three metrics: Detection, Recognition, and
Correction, and our team scored first or second in
each metric (see Dale et al (2012) for details).
The UI system consists of two components, a de-
terminer classifier and a preposition classifier, with
a common pre-processing step that corrects spelling
mistakes. The determiner system builds on the ideas
described in Rozovskaya and Roth (2010c). The
preposition classifier uses a combined system, build-
ing on work described in Rozovskaya and Roth
(2011) and Rozovskaya and Roth (2010b).
Both the determiner and the preposition systems
apply the method proposed in our earlier work,
which uses the error distribution of the learner data
to generate artificial errors in training data. The orig-
inal method was proposed for adding artificial er-
rors when training on native English data. In this
task, however, we apply this method when training
on annotated ESL data. Furthermore, we introduce
an improvement that is conceptually simple but very
effective and which also proved to be successful in
an earlier error correction shared task (Dale and Kil-
garriff, 2011; Rozovskaya et al, 2011). We identify
the unique characteristics of the error correction task
and analyze the limitations of existing approaches to
error correction that are due to these characteristics.
Based on this analysis, we propose the error infla-
tion method (Sect. 6.2).
In this paper, we first briefly discuss the task (Sec-
272
tion 2) and present our overall approach (Section
3. Next, we describe the spelling correction mod-
ule (Section 4). Section 5 provides an overview of
the training approaches for error correction tasks.
We present the inflation method in Section 6. Next,
we describe the determiner error correction system
(Section 7), and the preposition error correction
module (Section 8). In Section 9, we present the
performance results of our system in the competi-
tion. We conclude with a brief discussion (Section
10).
2 Task Description
The HOO 2012 shared task focuses on correcting
determiner and preposition errors made by non-
native speakers of English. These errors are some of
the most common and also some of the most difficult
for ESL learners (Leacock et al, 2010); even very
advanced learners make these mistakes (Rozovskaya
and Roth, 2010a).
The training data released by the task organizers
comes from the publicly available FCE corpus (Yan-
nakoudakis et al, 2011). The original FCE data set
contains 1244 essays written by non-native English
speakers and is corrected and error-tagged using a
detailed error classification schema. The HOO train-
ing data contains 1000 of those files.1 The test data
for the task consists of an additional set of 100 stu-
dent essays, different from the 1244 above.
Since the HOO task focuses on determiner and
preposition mistakes, only annotations marking
preposition and determiner mistakes were kept.
Note that while the other error annotations were
removed, the errors still remain in the HOO data.
More details can be found in Dale et al (2012).
3 System Overview
Our system consists of two components that address
individually article2 and preposition errors and use
the same pre-processing.
1In addition, the participating teams were allowed to use for
training the remaining 244 files of this corpus, as well as any
other data. We also use a publicly available data set of native
English, Google Web 1T corpus (Brants and Franz, 2006), in
one of our models.
2We will use the terms ?article-? and ?determiner errors? in-
terchangeably: article errors constitute the majority of deter-
miner errors, and we address only article mistakes.
The first pre-processing step is correcting spelling
errors. Since the essays were written by students of
English as a Second language, and these essays were
composed on-the-fly, they contain a large number of
spelling errors. These errors add noise to the context
around the target word (article or preposition). Good
context is crucial for robust detection and correction
of article and preposition mistakes.
After spelling errors are corrected, we run a sen-
tence splitter, part-of-speech tagger3 and shallow
parser4 (Punyakanok and Roth, 2001) on the data.
Both the article and the preposition systems use fea-
tures based on the output of these tools.
We made a 244-document subset of the FCE data
a held-out set for development. The results in Sec-
tions 7 and 8 give performance on this held-out set,
where we use the HOO data (1000 files) for train-
ing. The actual performance in the task (Section 9)
reflects the system trained on the whole set of 1244
documents.
Our article and preposition modules build on the
elements of the systems described in Rozovskaya
and Roth (2010b), Rozovskaya and Roth (2010c)
and Rozovskaya and Roth (2011). All article sys-
tems are trained using the Averaged Perceptron
(AP) algorithm (Freund and Schapire, 1999), im-
plemented within Learning Based Java (Rizzolo and
Roth, 2010). Our preposition systems combine the
AP algorithm with the Na??ve Bayes (NB) classifier
with prior parameters adapted to the learner data
(see Section 5). The AP systems are trained using
the inflation method (see Section 6.2).
We submitted 10 runs. All of our runs achieved
comparable performance. Sections 7 and 8 describe
our modules.
4 Correcting Spelling Errors
Analysis of the HOO data made clear the need for
a variety of corrections beyond the immediate scope
of the current evaluation. When a mistake occurs in
the vicinity of a target (i.e. preposition or article) er-
ror, it may result in local cues that obscure the nature
of the desired correction.
3http://cogcomp.cs.illinois.edu/page/
software view/POS
4http://cogcomp.cs.illinois.edu/page/
software view/Chunker
273
The following example illustrates such a problem:
?In my opinion your parents should be arrive in the
first party of the month becouse we could be go in
meeting with famous writer, travelled and journalist
who wrote book about Ethiopia.?
In this sample sentence, there are multiple errors
in close proximity: the misspelled word becouse; the
verb form should be arrive; the use of the word party
instead of part; the verb travelled instead of a noun
form; an incorrect preposition in (in meeting).
The context thus contains a considerable amount
of noise that is likely to negatively affect system per-
formance. To address some of these errors, we run a
standard spell-checker over the data.
We use Jazzy5, an open-source Java spell-checker.
The distribution, however, comes only with a US
English dictionary, which also has gaps in its cov-
erage of the language. The FCE corpus prefers UK
English spelling, so we use a mapping from US to
UK English6 to automatically correct the original
dictionary. We also keep the converted US spelling,
since our preposition module makes use of native
English data, where the US spelling is prevalent.
The Jazzy API allows the client to query a word,
and get a list of candidate corrections sorted in or-
der of edit distance from the original term. We
take the first suggestion and replace the original
word. The resulting substitution may be incorrect,
which may in turn mislead the downstream correc-
tion components. However, manual evaluation of
the spelling corrections suggested about 80% were
appropriate, and experimental evaluation on the cor-
pus development set indicated a modest overall im-
provement when the spell-checked documents were
used in place of the originals.
5 Training for Correction Tasks
The standard approach to correcting context-
sensitive ESL mistakes follows the methodology of
the context-sensitive spelling correction task that ad-
dresses such misspellings as their and there (Carl-
son et al, 2001; Golding and Roth, 1999; Golding
and Roth, 1996; Carlson and Fette, 2007; Banko and
Brill, 2001).
Following Rozovskaya and Roth (2010c), we dis-
5http://jazzy.sourceforge.net/
6http://www.tysto.com/articles05/q1/20050324uk-us.shtml
tinguish between two training paradigms in ESL er-
ror correction, depending on whether the author?s
original word choice is used in training as a feature.
In the standard context-sensitive spelling correction
paradigm, the decision of the classifier depends only
on the context around the author?s word, e.g. arti-
cle or preposition, and the author?s word itself is not
taken into consideration in training.
Mistakes made by non-native speakers obey cer-
tain regularities (Lee and Seneff, 2008; Rozovskaya
and Roth, 2010a). Adding knowledge about typ-
ical errors to a model significantly improves its
performance (Gamon, 2010; Rozovskaya and Roth,
2010c; Dahlmeier and Ng, 2011). Typical errors
may refer both to speakers whose first language is
L1 and to specific authors. For example, non-native
speakers whose first language does not have articles
tend to make more articles errors in English (Ro-
zovskaya and Roth, 2010a).
Since non-native speakers? mistakes are system-
atic, the author?s word choice (the source word)
carries a lot of information. Models that use the
source word in training (Han et al, 2010; Gamon,
2010; Dahlmeier and Ng, 2011) learn which errors
are typical for the learner and thus significantly out-
perform systems that only look at context. We call
these models adapted. Training adapted models re-
quires annotated data, since in native English data
the source word is always correct and thus cannot be
used by the classifier.
In this work, we use two methods of adapting a
model to typical errors that have been proposed ear-
lier. Both methods were originally developed for
models trained on native English data: they use a
small amount of annotated ESL data to generate er-
ror statistics. The artificial errors method is based
on generating artificial errors7 in correct native En-
glish training data. The method was implemented
within the Averaged Perceptron (AP) algorithm (Ro-
zovskaya and Roth, 2010c; Rozovskaya and Roth,
2010b), a discriminative learning algorithm, and this
is the algorithm that we use in this work. The NB-
priors method is a special adaptation technique for
the Na??ve Bayes algorithm (Rozovskaya and Roth,
2011). While NB-priors improves both precision
7For each task, only relevant errors are generated ? for ex-
ample, article mistakes for the article correction task.
274
and recall, the artificial errors approach suffers
from low recall due to error sparsity (Sec. 6.1).
In this work, in the preposition correction task,
we use the NB-priors method without modifications
(as described in the original paper). We use the ar-
tificial errors approach both for article and prepo-
sition error correction but with two important mod-
ifications: we train on annotated ESL data instead
of native data, and use the proposed error inflation
method (described in Section 6) to increase the error
rate in training.
6 Error Inflation
In this section, we show why AP (Freund and
Schapire, 1999), a discriminative classifier, is sen-
sitive to the error sparsity of the data, and propose
a method that addresses the problems raised by this
sensitivity.
6.1 Error Sparsity and Low Recall
The low recall of the AP algorithm is related to the
nature of the error correction tasks, which exhibit
low error rates. Even for ESL writers, over 90% of
their preposition and article usage is correct, which
makes the errors very sparse (Rozovskaya and Roth,
2010c). The low recall problem is, in fact, a special
case of a more general problem where there is one
or a small group of dominant features that are very
strongly correlated with the label. In this case, the
system tends to predict the label that matches this
feature, and tends to not predict it when that fea-
ture is absent. In error correction, which tends to
have a very skewed label distribution, this results in
very few errors being detected by the system: when
training on annotated data with naturally occurring
errors and using the source word as a feature, the
system will learn that in the majority of cases the
source word corresponds to the label, and will tend
to over-predict it, which will result in low recall.
In the artificial errors approach, errors are sim-
ulated according to real observed mistakes. Ta-
ble 1 shows a sample confusion matrix based on
preposition mistakes in the FCE corpus; we show
four rows, but the entire table contains 17 rows and
columns, one for each preposition, and each entry
shows Prob(pi|pj), the probability that the author?s
preposition is pi given that the correct preposition
is pj . The matrix also shows the preposition count
for each source and label in the data set. Given the
entire matrix and the counts, it is also possible to
generate the matrix in the other direction and obtain
Prob(pj |pi), the probability that the correct prepo-
sition is pj given that the author?s preposition is pi.
This other matrix is used for adapting NB with the
priors method.
The confusion matrix is sparse and shows that the
distribution of alternatives for each source preposi-
tion is very different from that of the others. This
strongly suggests that these errors are systematic.
Additionally, most prepositions are used correctly,
so the error rate is very low (the error rate can be
estimated by looking at the matrix diagonal in the
table; for example, the error rate for the preposition
about is lower than for into, since 94.4% of the oc-
currences of label about are correct, but only 76.8%
of label into are correct).
The artificial errors thus model the two proper-
ties that we mentioned: the confusability of differ-
ent preposition pairs and the low error rate, and the
artificial errors are similarly sparse.
6.2 The Error Inflation Method
Two extreme choices for solving the low recall prob-
lem due to error sparsity are: (1) training without the
source word feature or (2) training with this feature,
where the classifier relies on it too much. Models
trained without the source feature have very poor
precision. While the NB-priors method does have
good recall, our expectation is that with the right ap-
proach, a discriminative classifier will also improve
recall, but maintain higher precision as well.
We wish to reduce the confidence that the system
has in the source word, while preserving the knowl-
edge the model has about likely confusions and con-
texts of confused words. To accomplish this, we re-
duce the proportion of correct examples, i.e. exam-
ples where the source and the label are the same,
by some positive constant < 1.0 and distribute the
extra probability mass among the typical errors in
an appropriate proportion by generating additional
error examples. This inflates the proportion of ar-
tificial errors in the training data, and hence the er-
ror rate, while keeping the probability distribution
among likely corrections the same. Increasing the
error rate improves the recall, while the typical er-
275
Label Sources
on about into with as at by for from in of over to
(648) (700) (54) (733) (410) (880) (243) (1394) (515) (2213) (1954) (98) (1418)
on (598) 0.846 0.003 0.003 0.008 0.013 - 0.003 0.022 - 0.076 0.013 0.001 0.009
about (686) 0.004 0.944 - 0.007 - - - 0.022 0.005 0.002 0.016 0.001 -
into (55) 0.001 - 0.768 - - - 0.011 0.011 - 0.147 - - 0.053
with (710) 0.001 0.006 - 0.934 - 0.001 0.007 0.004 0.001 0.027 0.003 - 0.015
Table 1: Confusion matrix for preposition errors. Based on data from the FCE corpus for top 17 most frequent English
prepositions. The left column shows the correct preposition. Each row shows the author?s preposition choices for that label and
Prob(source|label). The sources among, between, under and within are not shown for lack of space; they all have 0 probabilities
in the matrix. The numbers next to the targets show the count of the label (or source) in the data set.
ror knowledge ensures that high precision is main-
tained. This method causes the classifier to rely on
the source feature less and increases the contribu-
tion of the features based on context. The learning
algorithm therefore finds a more optimal balance be-
tween the source feature and the context features.
Algorithm 1 shows the pseudo-code for generat-
ing training data; it takes as input training examples,
the confusion matrix CM as shown in Table 1, and
the inflation constant, and generates artificial source
features for correct training examples.8 An infla-
tion constant value of 1.0 simulates learner mistakes
without inflation. Table 2 shows the proportion of
artificial errors created in training using the inflation
method for different inflation rates.
Algorithm 1 Data Generation with Inflation
Input: Training examples E with correct sources, confusion matrix
CM , inflation constant C
Output: Training examples E with artificial errors
for Example e in E do
Initialize lab? e.label, e.source? e.label
Randomize targets ? CM [lab]
Initialize flag? False
for target t in targets do
if flag equals True then
Break
end if
if t equals lab then
Prob(t) = CM [lab][t] ? C
else
Prob(t) = 1.0?CM [lab][lab]?C1.0?CM [lab][lab] ? CM [lab][t]
end if
x? Random[0, 1]
if x < Prob(t) then
e.source? t
flag? True
end if
end for
end for
return E
8When training on native English data, all examples are cor-
rect. When training on annotated learner data, some examples
will contain naturally occurring mistakes.
Inflation rate
1.0 (Regular) 0.9 0.8 0.7 0.6 0.5
7.7% 15.1% 22.6% 30.1% 37.5% 45.0%
Table 2: Artificial errors. Proportion of generated artificial
preposition errors in training using the inflation method (based
on the FCE corpus).
7 Determiners
Table 4 shows the distribution of determiner errors
in the HOO training set. Even though the majority
of determiner errors involve article mistakes, 14% of
errors are personal and possessive pronouns.9 Most
of the determiner errors involve omitting an article.
Similar error patterns have been observed in other
ESL corpora (Rozovskaya and Roth, 2010a).
Our system focuses on article errors. Because
the majority of determiner errors are omissions, it is
very important to target this subset of mistakes. One
approach would be to consider every space as a pos-
sible article insertion point. However, this method
will likely produce a lot of noise. The standard
approach is to consider noun-phrase-initial contexts
(Han et al, 2006; Rozovskaya and Roth, 2010c).
Error type Example
Repl. 15.7% ?Can you send me the*/a letter back writing
what happened to you recently.?
Omis. 57.5% ?Nowadays ?*/the Internet makes us closer and
closer.?
Unnec. 26.8% ?One of my hobbies is the*/? photography.?
Table 4: Distribution of determiner errors in the HOO
training data.
9e.g. ?Pat apologized to me for not keeping the*/my secrets.?
276
Feature Type Description
Word n-grams wB, w2B, w3B, wA, w2A, w3A, wBwA, w2BwB, wAw2A, w3Bw2BwB, w2BwBwA, wBwAw2A,
wAw2Aw3A, w4Bw3Bw2BwB, w3w2BwBwA, w2BwBwAw2A, wBwAw2Aw3A, wAw2Aw3w4A
POS features pB, p2B, p3B , pA, p2A, p3A, pBpA, p2BpB, pAp2A, pBwB, pAwA, p2Bw2B, p2Aw2A, p2BpBpA, pBpAp2A,
pAp2Ap3A
NP1 headWord, npWords, NC, adj&headWord, adjTag&headWord, adj&NC, adjTag&NC, npTags&headWord, npTags&NC
NP2 headWord&headPOS, headNumber
wordsAfterNP headWord&wordAfterNP, npWords&wordAfterNP, headWord&2wordsAfterNP, npWords&2wordsAfterNP,
headWord&3wordsAfterNP, npWords&3wordsAfterNP
wordBeforeNP wB&fi ?i ? NP1
Verb verb, verb&fi ?i ? NP1
Preposition prep&fi ?i ? NP1
Table 3: Features used in the article error correction system. wB and wA denote the word immediately before and after
the target, respectively; and pB and pA denote the POS tag before and after the target. headWord denotes the head of the NP
complement. NC stands for noun compound and is active if second to last word in the NP is tagged as a noun. Verb features are
active if the NP is the direct object of a verb. Preposition features are active if the NP is immediately preceded by a preposition. adj
feature is active if the first word (or the second word preceded by an adverb) in the NP is an adjective. npWords and npTags denote
all words (POS tags) in the NP.
7.1 Determiner Features
The features are presented in Table 3. The model
also uses the source article as a feature.
7.2 Training the Determiner System
Model Detection Correction
AP (natural errors) 30.75 28.97
AP (inflation) 34.62 32.02
Table 5: Article development results: AP with inflation. The
performance shows the F-Score for the 244 held-out documents
of the original FCE data set. AP with inflation uses the constant
value of 0.8.
The article classifier is based on the artificial er-
rors approach (Rozovskaya and Roth, 2010c). The
original method trains a system on native English
data. The current setting is different, since the FCE
corpus contains annotated learner errors. Since the
errors are sparse, we use the error inflation method
(Section 6.2) to boost the proportion of errors in
training using the error distribution obtained from
the same training set. The effectiveness of this
method is demonstrated by the system performance:
we obtain the top or second result in every metric.
Note also that the article system does not use addi-
tional data for training.
Table 5 compares the performance of the system
trained on natural errors with the performance of the
system trained with the inflation method. We found
that any value of the inflation constant between 0.9
and 0.5 will give a boost in performance. We use
several values; the top determiner model uses the in-
flation constant of 0.8.
8 Prepositions
Table 6 shows the distribution of the three types of
preposition errors in the HOO training data. The
FCE annotation distinguishes between preposition
mistakes and errors involving the infinitive marker
to, e.g. ?He wants ?*/to go there.?, which are anno-
tated as verb errors. Since in the competition only
article and preposition annotations are kept, these
errors are not annotated, and thus we do not target
these mistakes.
Error type Example
Repl. 57.9% ?I can see at*/on the list a lot of interesting
sports.?
Omis. 24.0% ?I will be waiting ?*/for your call.?
Unnec. 18.1% ?Despite of */? being tiring , it was rewarding?
Table 6: Distribution of preposition errors in the HOO
training data.
To detect missing preposition errors, we use a set
of rules, mined from the training data, to identify
possible locations where a preposition might have
been incorrectly omitted. Below we show examples
of such contexts.
? ?I will be waiting ?*/for your call.?
? ?But now we use planes to go ?*/to far places.?
8.1 Preposition Features
All features used in the preposition module are lex-
ical: word n-grams in the 4-word window around
277
Feature Type Description
Word n-ngram features in the 4-word window
around the target
wB, w2B, w3B , wA, w2A, w3A, wBwA, w2BwB, wAw2A, w3Bw2BwB,
w2BwBwA, wBwAw2A, wAw2Aw3A, w4Bw3Bw2BwB, w3w2BwBwA,
w2BwBwAw2A, wBwAw2Aw3A, wAw2Aw3w4A
Preposition complement features compHead, wB&compHead, w2BwB&compHead
Table 7: Features used in the preposition error correction system. wB and wA denote the word immediately before and
after the target, respectively; the other features are defined similarly. compHead denotes the head of the preposition complement.
wB&compHead, w2BwB&compHead are feature conjunctions of compHead with wB and w2BwB, respectively.
the target preposition, and three features that use the
head of the preposition complement (see Table 7).
The NB-priors classifier, which is part of our model,
can only make use of the word n-gram features; it
uses n-gram features of lengths 3, 4, and 5. AP is
trained on the HOO data and uses n-grams of lengths
2, 3, and 4, the head complement features, and the
author?s preposition as a feature.
Model Detection Correction
AP (inflation) 34.64 27.51
NB-priors 38.76 26.57
Combined 41.27 29.35
Table 8: Preposition development results: performance of
individual and combined systems. The performance shows
the F-Score for the 244 held-out documents of the original FCE
data set.
8.2 Training the Preposition System
We train two systems. The first one is an AP model
trained on the FCE data with inflation (similar to
the article system). Correcting preposition errors re-
quires more data to achieve performance compara-
ble to article error correction, due to the task com-
plexity (Gamon, 2010). Moreover, given that the
development and test data are quite different,10 it
makes sense to use a model that is independent of
those, to avoid overfitting. We combine the AP
model with a model trained on native English data.
Our second system is an NB-priors classifier trained
on the the Google Web 1T 5-gram corpus (Brants
and Franz, 2006). We use training data to replace the
prior parameters of the model (see Rozovskaya and
Roth, 2011 for more detail). The NB-priors model
does not target preposition omissions.
10The data contains essays written on prompts, so that the
training data may contain several essays written on the same
prompt and thus will be very similar in content. In contrast,
we expected that the test data will likely contain essays on a
different set of prompts.
The NB-priors model outperforms the AP classi-
fier. The two models are also very different due to
the different learning algorithms and the type of the
data used in training. Our final preposition model
is thus a combination of these two, where we take
as the base the decisions of the NB-priors classifier
and add the AP model predictions for cases when
the base model does not flag a mistake. Table 8
shows the results. The combined model improves
both the detection and correction scores. Our prepo-
sition system ranked first in detection and recogni-
tion and second in correction.
Model Detection Correction
AP (natural errors) 13.50 12.73
AP (inflation) 21.31 32.02
Table 9: Preposition development results: AP with infla-
tion. The performance shows the F-Score for the 244 held-out
documents of the original FCE data set. AP with inflation uses
the constant value of 0.7.
9 Test Performance
A number of revisions were made to the test data
based on the input from the participating teams af-
ter the initial results were obtained, where each team
submitted proposed edits to correct annotation mis-
takes. We show both results.
Table 10 shows results before the revisions were
made. Row 1 shows the performance of the de-
terminer system for the three metrics. This system
achieved the best score in correction, and the second
best scores in detection and recognition. The system
is described in Section 7.2, with the exception that
the final system for the article correction is trained
on the entire FCE data set.
Table 10 (row 2) presents the results on prepo-
sition error correction. The system is described in
Section 8.2 and is a combined model of AP trained
with inflation on the FCE data set and NB-priors
model trained on the Google Web 1T corpus. The
278
Model Detection Recognition Correction
Precision Recall F-Score Precision Recall F-Score Precision Recall F-Score
Articles 40.00 37.79 38.862 38.05 35.94 36.972 35.61 33.64 34.601
Prepositions 38.21 45.34 41.471 31.05 40.25 35.061 20.36 24.15 22.092
Combined 37.22 43.71 40.201 34.23 36.64 35.391 26.39 28.26 27.292
Table 10: Performance on test before revisions. Results are shown before revisions were made to the data. The rank of the
system is shown as a superscript.
Model Detection Recognition Correction
Precision Recall F-Score Precision Recall F-Score Precision Recall F-Score
Articles 43.90 39.30 41.472 45.98 34.93 39.702 41.46 37.12 39.172
Prepositions 41.43 47.54 44.271 37.14 42.62 39.691 26.79 30.74 28.632
Combined 43.56 42.92 43.241 38.97 39.96 39.462 32.58 33.40 32.992
Table 11: Performance on test after revisions. Results are shown after revisions were made to the data. The rank of the system
is shown as a superscript.
preposition system achieved the best scores in detec-
tion and recognition, scoring second in correction.
Row 3 shows the performance of the combined
system. This system was ranked first in detection
and recognition, and second in correction.
Table 11 shows our performance after the revi-
sions were applied.
10 Discussion
The HOO 2012 shared task follows the HOO 2011
pilot shared task (Dale and Kilgarriff, 2011), where
the data was fully corrected and error-tagged and
the participants could address any types of mistakes.
The current task allows for comparison of individ-
ual systems for each error type considered. This is
important, since to date it has been difficult to com-
pare different systems due to the lack of a bench-
mark data set.
The data used for the shared task has many errors
besides the preposition and determiner errors; the
annotations for these have been removed. One un-
desirable consequence of this approach is that some
complex errors that involve either an article or a
preposition mistake but depend on other corrections
on neighboring words, e.g. a noun of a verb, may
result in ungrammatical sequences.
Clearly, the task of annotating all requisite correc-
tions is a daunting task, and it is preferable to iden-
tify subsets of these corrections that can be tackled
somewhat independently of the rest, and these more
complex cases present a problem.
To address these conflicting needs, we propose
that the scope of all ?final? corrections be marked,
without necessarily specifying all individual correc-
tions necessary to transform the original text into
correct English. Edits that plausibly require correc-
tions to their context to resolve correctly could then
be treated as out of scope, and ignored by spelling
correction systems even though in other contexts,
those same edits would be in scope.
11 Conclusion
We have demonstrated how a competitive system for
preposition and determiner error correction can be
built using techniques that address the error sparsity
of the data and the overfitting problem. We built on
our previous work and presented the error inflation
method that can be applied to the earlier proposed
artificial errors approach to boost recall. Our de-
terminer system used error inflation and trained a
model using only the annotated FCE corpus. Our
preposition system combined the FCE-trained sys-
tem with a native-data model that was adapted to
learner errors, using the NB-priors approach pro-
posed earlier. Both of the systems showed compet-
itive performance, scoring first or second in every
task ranking.
Acknowledgments
The authors thank Jeff Pasternack for his assistance and Vivek
Srikumar for helpful feedback. This research is supported by
a grant from the U.S. Department of Education and is partly
supported by the Defense Advanced Research Projects Agency
(DARPA) Machine Reading Program under Air Force Research
Laboratory (AFRL) prime contract no. FA8750-09-C-018.
References
M. Banko and E. Brill. 2001. Scaling to very very large
corpora for natural language disambiguation. In Proc.
279
of 39th Annual Meeting of the Association for Com-
putational Linguistics (ACL), pages 26?33, Toulouse,
France, July.
T. Brants and A. Franz. 2006. Web 1T 5-gram Version 1.
Linguistic Data Consortium, Philadelphia, PA.
A. Carlson and I. Fette. 2007. Memory-based context-
sensitive spelling correction at web scale. In Proc. of
the IEEE International Conference on Machine Learn-
ing and Applications (ICMLA).
A. Carlson, J. Rosen, and D. Roth. 2001. Scaling up
context sensitive text correction. In Proceedings of the
National Conference on Innovative Applications of Ar-
tificial Intelligence (IAAI), pages 45?50.
D. Dahlmeier and H. T. Ng. 2011. Grammatical er-
ror correction with alternating structure optimization.
In Proc. of the 49th Annual Meeting of the Associa-
tion for Computational Linguistics: Human Language
Technologies (ACL), pages 915?923, Portland, Ore-
gon, USA, June. Association for Computational Lin-
guistics.
R. Dale and A. Kilgarriff. 2011. Helping Our Own:
The HOO 2011 pilot shared task. In Proc. of the 13th
European Workshop on Natural Language Generation
(ENLG), pages 242?249, Nancy, France.
R. Dale, I. Anisimoff, and G. Narroway. 2012. A re-
port on the preposition and determiner error correction
shared task. In Proc. of the NAACL HLT 2012 Sev-
enth WorkshopWorkshop on Innovative Use of NLP for
Building Educational Applications, Montreal, Canada,
June. Association for Computational Linguistics.
Y. Freund and R. E. Schapire. 1999. Large margin clas-
sification using the perceptron algorithm. Machine
Learning, 37(3):277?296.
M. Gamon. 2010. Using mostly native data to correct
errors in learners? writing. In Proc. of the 2010 An-
nual Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies (NAACL), pages 163?171, Los
Angeles, California, June.
A. R. Golding and D. Roth. 1996. Applying Win-
now to context-sensitive spelling correction. In Proc.
of the International Conference on Machine Learning
(ICML), pages 182?190.
A. R. Golding and D. Roth. 1999. A Winnow based
approach to context-sensitive spelling correction. Ma-
chine Learning, 34(1-3):107?130.
N. Han, M. Chodorow, and C. Leacock. 2006. Detecting
errors in English article usage by non-native speakers.
Journal of Natural Language Engineering, 12(2):115?
129.
N. Han, J. Tetreault, S. Lee, and J. Ha. 2010. Us-
ing an error-annotated learner corpus to develop and
ESL/EFL error correction system. In Proc. of the Sev-
enth conference on International Language Resources
and Evaluation (LREC), Valletta, Malta, May. Euro-
pean Language Resources Association (ELRA).
C. Leacock, M. Chodorow, M. Gamon, and J. Tetreault.
2010. Automated Grammatical Error Detection for
Language Learners. Morgan and Claypool Publish-
ers.
J. Lee and S. Seneff. 2008. An analysis of grammatical
errors in non-native speech in English. In Proc. of the
2008 Spoken Language Technology Workshop, Goa.
V. Punyakanok and D. Roth. 2001. The use of classi-
fiers in sequential inference. In The Conference on
Advances in Neural Information Processing Systems
(NIPS), pages 995?1001. MIT Press.
N. Rizzolo and D. Roth. 2010. Learning based java for
rapid development of nlp systems. In Proceedings of
the International Conference on Language Resources
and Evaluation (LREC), Valletta, Malta, 5.
A. Rozovskaya and D. Roth. 2010a. Annotating ESL er-
rors: Challenges and rewards. In Proc. of the NAACL
HLT 2010 Fifth Workshop on Innovative Use of NLP
for Building Educational Applications, pages 28?36,
Los Angeles, California, June. Association for Com-
putational Linguistics.
A. Rozovskaya and D. Roth. 2010b. Generating confu-
sion sets for context-sensitive error correction. In Pro-
ceedings of the 2010 Conference on Empirical Meth-
ods in Natural Language Processing (EMNLP), pages
961?970, Cambridge, MA, October. Association for
Computational Linguistics.
A. Rozovskaya and D. Roth. 2010c. Training paradigms
for correcting errors in grammar and usage. In Proc. of
the Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics:
Human Language Technologies (NAACL), pages 154?
162, Los Angeles, California, June. Association for
Computational Linguistics.
A. Rozovskaya and D. Roth. 2011. Algorithm selec-
tion and model adaptation for ESL correction tasks.
In Proc. of the 49th Annual Meeting of the Associa-
tion for Computational Linguistics: Human Language
Technologies (ACL), pages 924?933, Portland, Ore-
gon, USA, June. Association for Computational Lin-
guistics.
A. Rozovskaya, M. Sammons, J. Gioja, and D. Roth.
2011. University of Illinois system in HOO text cor-
rection shared task. In Proc. of the 13th European
Workshop on Natural Language Generation (ENLG).
H. Yannakoudakis, T. Briscoe, and B. Medlock. 2011. A
new dataset and method for automatically grading esol
texts. In Proc. of the 49th Annual Meeting of the As-
sociation for Computational Linguistics (ACL), pages
180?189, Portland, Oregon, USA, June. Association
for Computational Linguistics.
280
Proceedings of the Joint Conference on EMNLP and CoNLL: Shared Task, pages 113?117,
Jeju Island, Korea, July 13, 2012. c?2012 Association for Computational Linguistics
Illinois-Coref: The UI System in the CoNLL-2012 Shared Task
Kai-Wei Chang Rajhans Samdani Alla Rozovskaya Mark Sammons Dan Roth
University of Illinois at Urbana-Champaign
{kchang10|rsamdan2|rozovska|mssammon|danr}@illinois.edu
Abstract
The CoNLL-2012 shared task is an extension
of the last year?s coreference task. We partici-
pated in the closed track of the shared tasks in
both years. In this paper, we present the im-
provements of Illinois-Coref system from last
year. We focus on improving mention detec-
tion and pronoun coreference resolution, and
present a new learning protocol. These new
strategies boost the performance of the system
by 5% MUC F1, 0.8% BCUB F1, and 1.7%
CEAF F1 on the OntoNotes-5.0 development
set.
1 Introduction
Coreference resolution has been a popular topic of
study in recent years. In the task, a system requires
to identify denotative phrases (?mentions?) and to
cluster the mentions into equivalence classes, so that
the mentions in the same class refer to the same en-
tity in the real world.
Coreference resolution is a central task in the
Natural Language Processing research. Both the
CoNLL-2011 (Pradhan et al, 2011) and CoNLL-
2012 (Pradhan et al, 2012) shared tasks focus on
resolving coreference on the OntoNotes corpus. We
also participated in the CoNLL-2011 shared task.
Our system (Chang et al, 2011) ranked first in two
out of four scoring metrics (BCUB and BLANC),
and ranked third in the average score. This year,
we further improve the system in several respects.
In Sec. 2, we describe the Illinois-Coref system
for the CoNLL-2011 shared task, which we take as
the baseline. Then, we discuss the improvements
on mention detection (Sec. 3.1), pronoun resolu-
tion (Sec. 3.2), and learning algorithm (Sec. 3.3).
Section 4 shows experimental results and Section 5
offers a brief discussion.
2 Baseline System
We use the Illinois-Coref system from CoNLL-2011
as the basis for our current system and refer to it as
the baseline. We give a brief outline here, but fo-
cus on the innovations that we developed; a detailed
description of the last year?s system can be found in
(Chang et al, 2011).
The Illinois-Coref system uses a machine learn-
ing approach to coreference, with an inference pro-
cedure that supports straightforward inclusion of do-
main knowledge via constraints.
The system first uses heuristics based on Named
Entity recognition, syntactic parsing, and shallow
parsing to identify candidate mentions. A pair-
wise scorer w generates compatibility scores wuv
for pairs of candidate mentions u and v using ex-
tracted features ?(u, v) and linguistic constraints c.
wuv = w ? ?(u, v) + c(u, v) + t, (1)
where t is a threshold parameter (to be tuned). An
inference procedure then determines the optimal set
of links to retain, incorporating constraints that may
override the classifier prediction for a given mention
pair. A post-processing step removes mentions in
singleton clusters.
Last year, we found that a Best-Link decoding
strategy outperformed an All-Link strategy. The
Best-Link approach scans candidate mentions in a
document from left to right. At each mention, if cer-
tain conditions are satisfied, the pairwise scores of
all previous mentions are considered, together with
any constraints that apply. If one or more viable
113
links is available, the highest-scoring link is selected
and added to the set of coreference links. After the
scan is complete, the transitive closure of edges is
taken to generate the coreference clusters, each clus-
ter corresponding to a single predicted entity in the
document.
The formulation of this best-link solution is as fol-
lows. For two mentions u and v, u < v indicates
that the mention u precedes v in the document. Let
yuv be a binary variable, such that yuv = 1 only if
u and v are in the same cluster. For a document d,
Best-Link solves the following formulation:
argmaxy
?
u,v:u<v
wuvyuv
s.t
?
u<v
yuv ? 1 ?v,
yuw ? {0, 1}.
(2)
Eq. (2) generates a set of connected components
and the set of mentions in each connected compo-
nent constitute an entity. Note that we solve the
above Best-Link inference using an efficient algo-
rithm (Bengtson and Roth, 2008) which runs in time
quadratic in the number of mentions.
3 Improvements over the Baseline System
Below, we describe improvements introduced to the
baseline Illinois-Coref system.
3.1 Mention Detection
Mention detection is a crucial component of an end-
to-end coreference system, as mention detection er-
rors will propagate to the final coreference chain.
Illinois-Coref implements a high recall and low
precision rule-based system that includes all noun
phrases, pronouns and named entities as candidate
mentions. The error analysis shows that there are
two main types of errors.
Non-referential Noun Phrases. Non-referential
noun phrases are candidate noun phrases, identified
through a syntactic parser, that are unlikely to re-
fer to any entity in the real world (e.g., ?the same
time?). Note that because singleton mentions are not
annotated in the OntoNotes corpus, such phrases are
not considered as mentions. Non-referential noun
phrases are a problem, since during the coreference
stage they may be incorrectly linked to a valid men-
tion, thereby decreasing the precision of the system.
To deal with this problem, we use the training data
to count the number of times that a candidate noun
phrase happens to be a gold mention. Then, we re-
move candidate mentions that frequently appear in
the training data but never appear as gold mentions.
Relaxing this approach, we also take the predicted
head word and the words before and after the men-
tion into account. This helps remove noun phrases
headed by a preposition (e.g., the noun ?fact? in the
phrase ?in fact?). This strategy will slightly degrade
the recall of mention detection, so we tune a thresh-
old learned on the training data for the mention re-
moval.
Incorrect Mention Boundary. A lot of errors in
mention detection happen when predicting mention
boundaries. There are two main reasons for bound-
ary errors: parser mistakes and annotation incon-
sistencies. A mistake made by the parser may be
due to a wrong attachment or adding extra words
to a mention. For example, if the parser attaches
the relative clause inside of the noun phrase ?Pres-
ident Bush, who traveled to China yesterday? to a
different noun, the algorithm will predict ?President
Bush? as a mention instead of ?President Bush, who
traveled to China yesterday?; thus it will make an er-
ror, since the gold mention also includes the relative
clause. In this case, we prefer to keep the candi-
date with a larger span. On the other hand, we may
predict ?President Bush at Dayton? instead of ?Pres-
ident Bush?, if the parser incorrectly attaches the
prepositional phrase. Another example is when ex-
tra words are added, as in ?Today President Bush?.
A correct detection of mention boundaries is cru-
cial to the end-to-end coreference system. The re-
sults in (Chang et al, 2011, Section 3) show that the
baseline system can be improved from 55.96 avg F1
to 56.62 in avg F1 by using gold mention boundaries
generated from a gold annotation of the parsing tree
and the name entity tagging. However, fixing men-
tion boundaries in an end-to-end system is difficult
and requires additional knowledge. In the current
implementation, we focus on a subset of mentions
to further improve the mention detection stage of the
baseline system. Specifically, we fix mentions start-
ing with a stop word and mentions ending with a
punctuation mark. We also use training data to learn
patterns of inappropriate mention boundaries. The
mention candidates that match the patterns are re-
114
moved. This strategy is similar to the method used
to remove non-referential noun phrases.
As for annotation inconsistency, we find that in a
few documents, a punctuation mark or an apostrophe
used to mark the possessive form are inconsistently
added to the end of a mention. The problem results
in an incorrect matching between the gold and pre-
dicted mentions and downgrades the performance of
the learned model. Moreover, the incorrect mention
boundary problem also affects the training phase be-
cause our system is trained on a union set of the pre-
dicted and gold mentions. To fix this problem, in
the training phase, we perform a relaxed matching
between predicted mentions and gold mentions and
ignore the punctuation marks and mentions that start
with one of the following: adverb, verb, determiner,
and cardinal number. For example, we successfully
match the predicted mention ?now the army? to the
gold mention ?the army? and match the predicted
mention ?Sony ?s? to the gold mention ?Sony.? Note
that we cannot fix the inconsistency problem in the
test data.
3.2 Pronoun Resolution
The baseline system uses an identical model for
coreference resolution on both pronouns and non-
pronominal mentions. However, in the litera-
ture (Bengtson and Roth, 2008; Rahman and Ng,
2011; Denis and Baldridge, 2007) the features
for coreference resolution on pronouns and non-
pronouns are usually different. For example, lexi-
cal features play an important role in non-pronoun
coreference resolution, but are less important for
pronoun anaphora resolution. On the other hand,
gender features are not as important in non-pronoun
coreference resolution.
We consider training two separate classifiers with
different sets of features for pronoun and non-
pronoun coreference resolution. Then, in the decod-
ing stage, pronoun and non-pronominal mentions
use different classifiers to find the best antecedent
mention to link to. We use the same features for
non-pronoun coreference resolution, as the baseline
system. For the pronoun anaphora classifier, we use
a set of features described in (Denis and Baldridge,
2007), with some additional features. The aug-
mented feature set includes features to identify if a
pronoun or an antecedent is a speaker in the sen-
Algorithm 1 Online Latent Structured Learning for
Coreference Resolution
Loop until convergence:
For each document Dt and each v ? Dt
1. Let u? = max
u?y(v)
wT?(u, v), and
2. u? = max
u?{u<v}?{?}
wT?(u, v) + ?(u, v, y(v))
3. Let w? w + ?wT (?(u?, v)? ?(u?, v)).
tence. It also includes features to reflect the docu-
ment type. In Section 4, we will demonstrate the im-
provement of using separate classifiers for pronoun
and non-pronoun coreference resolution.
3.3 Learning Protocol for Best-Link Inference
The baseline system applies the strategy in (Bengt-
son and Roth, 2008, Section 2.2) to learn the pair-
wise scoring functionw using the Averaged Percep-
tron algorithm. The algorithm is trained on mention
pairs generated on a per-mention basis. The exam-
ples are generated for a mention v as
? Positive examples: (u, v) is used as a positive
example where u < v is the closest mention to
v in v?s cluster
? Negative examples: for all w with u < w < v,
(w, v) forms a negative example.
Although this approach is simple, it suffers from
a severe label imbalance problem. Moreover, it does
not relate well to the best-link inference, as the deci-
sion of picking the closest preceding mention seems
rather ad-hoc. For example, consider three men-
tions belonging to the same cluster: {m1: ?Presi-
dent Bush?, m2: ?he?, m3:?George Bush?}. The
baseline system always chooses the pair (m2,m3)
as a positive example because m2 is the closet men-
tion of m3. However, it is more proper to learn the
model on the positive pair (m1,m3), as it provides
more information. Since the best links are not given
but are latent in our learning problem, we use an on-
line latent structured learning algorithm (Connor et
al., 2011) to address this problem.
We consider a structured problem that takes men-
tion v and its preceding mentions {u | u < v} as
inputs. The output variables y(v) is the set of an-
tecedent mentions that co-refer with v. We define
a latent structure h(v) to be the bestlink decision
of v. It takes the value ? if v is the first mention
115
Method
Without Separating Pronouns With Separating Pronouns
MD MUC BCUB CEAF AVG MD MUC BCUB CEAF AVG
Binary Classifier (baseline) 70.53 61.63 69.26 43.03 57.97 73.24 64.57 69.78 44.95 59.76
Latent-Structured Learning 73.02 64.98 70.00 44.48 59.82 73.95 65.75 70.25 45.30 60.43
Table 1: The performance of different learning strategies for best-link decoding algorithm. We show the results
with/without using separate pronoun anaphora resolver. The systems are trained on the TRAIN set and evaluated on
the CoNLL-2012 DEV set. We report the F1 scores (%) on mention detection (MD) and coreference metrics (MUC,
BCUB, CEAF). The column AVG shows the averaged scores of the three coreference metrics.
System MD MUC BCUB CEAF AVG
Baseline 64.58 55.49 69.15 43.72 56.12
New Sys. 70.03 60.65 69.95 45.39 58.66
Table 2: The improvement of Illinois-Coref. We report
the F1 scores (%) on the DEV set from CoNLL-2011
shared task. Note that the CoNLL-2011 data set does not
include corpora of bible and of telephone conversation.
in the equivalence class, otherwise it takes values
from {u | u < v}. We define a loss function
?(h(v), v, y(v)) as
?(h(v), v, y(v)) =
{
0 h(v) ? y(v),
1 h(v) /? y(v).
We further define the feature vector ?(?, v) to be a
zero vector and ? to be the learning rate in Percep-
tron algorithm. Then, the weight vectorw in (1) can
be learned from Algorithm 1. At each step, Alg. 1
picks a mention v and finds the Best-Link decision
u? that is consistent with the gold cluster. Then, it
solves a loss-augmented inference problem to find
the best link decision u? with current model (u? = ?
if the classifier decides that v does not have coref-
erent antecedent mention). Finally, the model w is
updated by the difference between the feature vec-
tors ?(u?, v) and ?(u?, v).
Alg. 1 makes learning more coherent with infer-
ence. Furthermore, it naturally solves the data im-
balance problem. Lastly, this algorithm is fast and
converges very quickly.
4 Experiments and Results
In this section, we demonstrate the performance of
Illinois-Coref on the OntoNotes-5.0 data set. A pre-
vious experiment using an earlier version of this data
can be found in (Pradhan et al, 2007). We first show
the improvement of the mention detection system.
Then, we compare different learning protocols for
coreference resolution. Finally, we show the overall
performance improvement of Illinois-Coref system.
First, we analyze the performance of mention de-
tection before the coreference stage. Note that sin-
gleton mentions are included since it is not possible
to identify singleton mentions before running coref-
erence. They are removed in the post-processing
stage. The mention detection performance of the
end-to-end system will be discussed later in this sec-
tion. With the strategy described in Section 3.1, we
improve the F1 score for mention detection from
55.92% to 57.89%. Moreover, we improve the de-
tection performance on short named entity mentions
(name entity with less than 5 words) from 61.36 to
64.00 in F1 scores. Such mentions are more impor-
tant because they are easier to resolve in the corefer-
ence layer.
Regarding the learning algorithm, Table 1 shows
the performance of the two learning protocols
with/without separating pronoun anaphora resolver.
The results show that both strategies of using a pro-
noun classifier and training a latent structured model
with a online algorithm improve the system perfor-
mance. Combining the two strategies, the avg F1
score is improved by 2.45%.
Finally, we compare the final system with the
baseline system. We evaluate both systems on the
CoNLL-11 DEV data set, as the baseline system
is tuned on it. The results show that Illinois-Coref
achieves better scores on all the metrics. The men-
tion detection performance after coreference resolu-
tion is also significantly improved.
116
Task MD MUC BCUB CEAF AVG
English (Pred. Mentions) 74.32 66.38 69.34 44.81 60.18
English (Gold Mention Boundaries) 75.72 67.80 69.75 45.12 60.89
English (Gold Mentions) 100.00 85.74 77.46 68.46 77.22
Chinese (Pred Mentions) 47.58 37.93 63.23 35.97 45.71
Table 3: The results of our submitted system on the TEST set. The systems are trained on a collection of TRAIN and
DEV sets.
4.1 Chinese Coreference Resolution
We apply the same system to Chinese coreference
resolution. However, because the pronoun proper-
ties in Chinese are different from those in English,
we do not train separate classifiers for pronoun and
non-pronoun coreference resolution. Our Chinese
coreference resolution on Dev set achieves 37.88%
MUC, 63.37% BCUB, and 35.78% CEAF in F1
score. The performance for Chinese coreference is
not as good as the performance of the coreference
system for English. One reason for that is that we
use the same feature set for both Chinese and En-
glish systems, and the feature set is developed for
the English corpus. Studying the value of strong fea-
tures for Chinese coreference resolution system is a
potential topic for future research.
4.2 Test Results
Table 3 shows the results obtained on TEST, using
the best system configurations found on DEV. We
report results on both English and Chinese coref-
erence resolution on predicted mentions with pre-
dicted boundaries. For English coreference resolu-
tion, we also report the results when using gold men-
tions and when using gold mention boundaries1.
5 Conclusion
We described strategies for improving mention de-
tection and proposed an online latent structure al-
gorithm for coreference resolution. We also pro-
posed using separate classifiers for making Best-
Link decisions on pronoun and non-pronoun men-
tions. These strategies significantly improve the
Illinois-Coref system.
1Note that, in Ontonotes annotation, specifying gold men-
tions requires coreference resolution to exclude singleton men-
tions. Gold mention boundaries are provided by the task orga-
nizers and include singleton mentions.
Acknowledgments This research is supported by the
Defense Advanced Research Projects Agency (DARPA)
Machine Reading Program under Air Force Research
Laboratory (AFRL) prime contract no. FA8750-09-C-
0181 and the Army Research Laboratory (ARL) under
agreement W911NF-09-2-0053. Any opinions, findings,
and conclusion or recommendations expressed in this ma-
terial are those of the author(s) and do not necessarily
reflect the view of the DARPA, AFRL, ARL or the US
government.
References
E. Bengtson and D. Roth. 2008. Understanding the value
of features for coreference resolution. In EMNLP.
K. Chang, R. Samdani, A. Rozovskaya, N. Rizzolo,
M. Sammons, and D. Roth. 2011. Inference proto-
cols for coreference resolution. In CoNLL.
M. Connor, C. Fisher, and D. Roth. 2011. Online latent
structure training for language acquisition. In IJCAI.
P. Denis and J. Baldridge. 2007. A ranking approach to
pronoun resolution. In IJCAI.
S. Pradhan, L. Ramshaw, R. Weischedel, J. MacBride,
and L. Micciulla. 2007. Unrestricted Coreference:
Identifying Entities and Events in OntoNotes. In
ICSC.
S. Pradhan, L. Ramshaw, M. Marcus, M. Palmer,
R. Weischedel, and N. Xue. 2011. CoNLL-2011
shared task: Modeling unrestricted coreference in
OntoNotes. In CoNLL.
S. Pradhan, A. Moschitti, N. Xue, O. Uryupina, and
Y. Zhang. 2012. CoNLL-2012 shared task: Modeling
multilingual unrestricted coreference in OntoNotes. In
CoNLL.
A. Rahman and V. Ng. 2011. Narrowing the modeling
gap: a cluster-ranking approach to coreference resolu-
tion. Journal of AI Research, 40(1):469?521.
117
Proceedings of the Seventeenth Conference on Computational Natural Language Learning: Shared Task, pages 13?19,
Sofia, Bulgaria, August 8-9 2013. c?2013 Association for Computational Linguistics
The University of Illinois System in the CoNLL-2013 Shared Task
Alla Rozovskaya Kai-Wei Chang Mark Sammons Dan Roth
Cognitive Computation Group
University of Illinois at Urbana-Champaign
Urbana, IL 61801
{rozovska,kchang10,mssammon,danr}@illinois.edu
Abstract
The CoNLL-2013 shared task focuses on
correcting grammatical errors in essays
written by non-native learners of English.
In this paper, we describe the University
of Illinois system that participated in the
shared task. The system consists of five
components and targets five types of com-
mon grammatical mistakes made by En-
glish as Second Language writers. We de-
scribe our underlying approach, which re-
lates to our previous work, and describe
the novel aspects of the system in more de-
tail. Out of 17 participating teams, our sys-
tem is ranked first based on both the orig-
inal annotation and on the revised annota-
tion.
1 Introduction
The task of correcting grammar and usage mis-
takes made by English as a Second Language
(ESL) writers is difficult for several reasons. First,
many of these errors are context-sensitive mistakes
that confuse valid English words and thus can-
not be detected without considering the context
around the word. Second, the relative frequency
of mistakes is quite low: for a given type of mis-
take, an ESL writer will typically make mistakes
in only a small proportion of relevant structures.
For example, determiner mistakes usually occur
in 5% to 10% of noun phrases in various anno-
tated ESL corpora (Rozovskaya and Roth, 2010a).
Third, an ESL writer may make multiple mistakes
in a single sentence, which may give misleading
local cues for individual classifiers. In the exam-
ple shown in Figure 1, the agreement error on the
verb ?tend? interacts with the noun number error
on the word ?equipments?.
Therefore , the *equipments/equipment of bio-
metric identification *tend/tends to be in-
expensive .
Figure 1: Representative ESL errors in a sample
sentence from the training data.
The CoNLL-2013 shared task (Ng et al, 2013)
focuses on the following five common mistakes
made by ESL writers:
? article/determiner
? preposition
? noun number
? subject-verb agreement
? verb form
Errors outside this target group are present in the
task corpora, but are not evaluated.
In this paper, we present a system that combines
a set of statistical models, where each model spe-
cializes in correcting one of the errors described
above. Because the individual error types have
different characteristics, we use several different
approaches. The article system builds on the el-
ements of the system described in (Rozovskaya
and Roth, 2010c). The preposition classifier uses
a combined system, building on work described
in (Rozovskaya and Roth, 2011) and (Rozovskaya
and Roth, 2010b). The remaining three models are
all Na??ve Bayes classifiers trained on the Google
Web 1T 5-gram corpus (henceforth, Google cor-
pus, (Brants and Franz, 2006)).
We first briefly discuss the task (Section 2) and
give the overview of our system (Section 3). We
then describe the error-specific components (Sec-
tions 3.1, 3.2 and 3.3). The sections describ-
ing individual components quantify their perfor-
mance on splits of the training data. In Section 4,
13
we evaluate the complete system on the training
data using 5-fold cross-validation (hereafter, ?5-
fold CV?) and in Section 5 we show the results we
obtained on test.
We close with a discussion focused on error
analysis (Section 6) and our conclusions (Sec-
tion 7).
2 Task Description
The CoNLL-2013 shared task focuses on correct-
ing five types of mistakes that are commonly made
by non-native speakers of English. The train-
ing data released by the task organizers comes
from the NUCLE corpus (Dahlmeier et al, 2013),
which contains essays written by learners of En-
glish as a foreign language and is corrected by
English teachers. The test data for the task con-
sists of an additional set of 50 student essays. Ta-
ble 1 illustrates the mistakes considered in the task
and Table 2 illustrates the distribution of these er-
rors in the released training data and the test data.
We note that the test data contains a much larger
proportion of annotated mistakes. For example,
while only 2.4% of noun phrases in the training
data have determiner errors, in the test data 10%
of noun phrases have mistakes.
Error type Percentage of errors
Training Test
Articles 2.4% 10.0%
Prepositions 2.0% 10.7%
Noun number 1.6% 6.0%
Subject-verb agreement 2.0% 5.2%
Verb form 0.8% 2.5%
Table 2: Statistics on error distribution in train-
ing and test data. Percentage denotes the erro-
neous instances with respect to the total number of
relevant instances in the data. For example, 10%
of noun phrases in the test data have determiner
errors.
Since the task focuses on five error types, only
annotations marking these mistakes were kept.
Note that while the other error annotations were
removed, the errors still remain in the data.
3 System Components
Our system consists of five components that ad-
dress individually article1, preposition, noun verb
1We will use the terms ?article-? and ?determiner errors?
interchangeably: article errors constitute the majority of de-
form and subject-verb agreement errors.
Our article and preposition modules build on the
elements of the systems described in Rozovskaya
and Roth (2010b), Rozovskaya and Roth (2010c)
and Rozovskaya and Roth (2011). The article sys-
tem is trained using the Averaged Perceptron (AP)
algorithm (Freund and Schapire, 1999), imple-
mented within Learning Based Java (Rizzolo and
Roth, 2010). The AP system is trained using the
inflation method (Rozovskaya et al, 2012). Our
preposition system is a Na??ve Bayes (NB) classi-
fier trained on the Google corpus and with prior
parameters adapted to the learner data.
The other modules ? those that correct noun and
verb errors ? are all NB models trained on the
Google corpus.
All components take as input the corpus doc-
uments preprocessed with a part-of-speech tag-
ger2 and shallow parser3 (Punyakanok and Roth,
2001). Note that the shared task data already
contains comparable pre-processing information,
in addition to other information, including depen-
dency parse and constituency parse, but we chose
to run our own pre-processing tools. The article
module uses the POS and chunker output to gen-
erate some of its features and to generate candi-
dates (likely contexts for missing articles). The
other system components use the pre-processing
tools only as part of candidate generation (e.g., to
identify all nouns in the data for the noun classi-
fier) because these components are trained on the
Google corpus and thus only employ word n-gram
features.
During development, we split the released train-
ing data into five parts. The results in Sections 3.1,
3.2, and 3.3 give performance of 5-fold CV on the
training data. In Section 4 we report the develop-
ment 5-fold CV results of the complete model and
the performance on the test data. Note that the per-
formance reported for the overall task on the test
data in Section 4 reflects the system that makes use
of the entire training corpus. It is also important to
remark that only the determiner system is trained
on the ESL data. The other models are trained on
native data, and the ESL training data is only used
to optimize the decision thresholds of the models.
terminer errors, and we address only article mistakes.
2http://cogcomp.cs.illinois.edu/page/
software view/POS
3http://cogcomp.cs.illinois.edu/page/
software view/Chunker
14
Error type Examples
Article ?It is also important to create *a/? better material that can support
*the/? buildings despite any natural disaster like earthquakes.?
Preposition ?As the number of people grows, the need *of /for habitable environ-
ment is unquestionably essential.
Noun number Some countries are having difficulties in managing a place to live for
their *citizen/citizens as they tend to get overpopulated.?
Subject-verb agreement ?Therefore , the equipments of biometric identification *tend/tends
to be inexpensive.
Verb form
?...countries with a lot of deserts can terraform their desert to increase
their habitable land and *using/use irrigation..?
?it was not *surprised/surprising to observe an increasing need for a
convenient and cost effective platform.?
Table 1: Example errors. Note that only the errors exemplifying the relevant phenomena are marked
in the table; the sentences may contain other mistakes. Errors marked as verb form include multiple
grammatical phenomena that may characterize verbs.
3.1 Determiners
There are three types of determiner error: omitting
a determiner; choosing an incorrect determiner;
and adding a spurious determiner. Even though
the majority of determiner errors involve article
mistakes, some of these errors involve personal
and possessive pronouns.4 Most of the determiner
errors, however, involve omitting an article (these
make up over 60% in the training data). Similar er-
ror patterns have been observed in other ESL cor-
pora (Rozovskaya and Roth, 2010a).
Our system focuses on article errors. The sys-
tem first extracts from the data all articles, and all
spaces at the beginning of a noun phrase where an
article is likely to be omitted (Han et al, 2006; Ro-
zovskaya and Roth, 2010c). Then we train a multi-
class classifier with features described in Table 3.
These features were used successfully in previous
tasks in error correction (Rozovskaya et al, 2012;
Rozovskaya et al, 2011).
The original word choice (the source article)
used by the writer is also used as a feature. Since
the errors are sparse, this feature causes the model
to abstain from flagging a mistake, which results
in low recall. To avoid this problem, we adopt the
approach proposed in (Rozovskaya et al, 2012),
the error inflation method, and add artificial arti-
cle errors in the training data based on the error
distribution on the training set. This method pre-
vents the source feature from dominating the con-
text features, and improves the recall of the sys-
4e.g. ?Pat apologized to me for not keeping the*/my se-
crets.?
tem.
We experimented with two types of classifiers:
Averaged Perceptron (AP) and an L1-generalized
logistic regression classifier (LR). Since the arti-
cle system is trained on the ESL data, of which
we have a limited amount, we also experimented
with adding a language model (LM) feature to the
LR learner. This feature indicates if the correc-
tion is accepted by a language model trained on
the Google corpus. The performance of each clas-
sifier on 5-fold CV on the training data is shown in
Table 4. The results show that AP performs better
than LR. We observed that adding the LM feature
improves precision but results in lower F1, so we
chose the AP classifier without the LM feature for
our final system.
Model Precision Recall F1
AP (inflation) 0.17 0.31 0.22
AP (inflation+LM) 0.26 0.15 0.19
LR (inflation) 0.17 0.29 0.22
LR (inflation+LM) 0.24 0.21 0.22
Table 4: Article development results Results on 5-fold
CV. AP With Inflation achieves the best development using an
inflation constant of 0.85. AP achieves higher performance
without using the language model feature.
3.2 Prepositions
The most common preposition errors are replace-
ments, i.e., where the author correctly recognized
the need for a preposition, but chose the wrong one
to use.
15
Feature Type Description
Word n-grams wB, w2B, w3B, wA, w2A, w3A, wBwA, w2BwB, wAw2A, w3Bw2BwB, w2BwBwA, wBwAw2A, wAw2Aw3A,
w4Bw3Bw2BwB, w3w2BwBwA, w2BwBwAw2A, wBwAw2Aw3A, wAw2Aw3w4A
POS features pB, p2B, p3B , pA, p2A, p3A, pBpA, p2BpB, pAp2A, pBwB, pAwA, p2Bw2B, p2Aw2A, p2BpBpA, pBpAp2A,
pAp2Ap3A
NP1 headWord, npWords, NC, adj&headWord, adjTag&headWord, adj&NC, adjTag&NC, npTags&headWord, npTags&NC
NP2 headWord&headPOS, headNumber
wordsAfterNP headWord&wordAfterNP, npWords&wordAfterNP, headWord&2wordsAfterNP, npWords&2wordsAfterNP, headWord&3wordsAfterNP,
npWords&3wordsAfterNP
wordBeforeNP wB&fi ?i ? NP1
Verb verb, verb&fi ?i ? NP1
Preposition prep&fi ?i ? NP1
Source the word used by the original writer
LM a binary feature assigned by a language model
Table 3: Features used in the article error correction system. wB and wA denote the word immediately before and after
the target, respectively; and pB and pA denote the POS tag before and after the target. headWord denotes the head of the NP
complement. NC stands for noun compound and is active if second to last word in the NP is tagged as a noun. Verb features are
active if the NP is the direct object of a verb. Preposition features are active if the NP is immediately preceded by a preposition.
adj feature is active if the first word (or the second word preceded by an adverb) in the NP is an adjective. npWords and npTags
denote all words (POS tags) in the NP.
3.2.1 Preposition Features
All features used in the preposition module are
lexical: word n-grams in the 4-word window
around the target preposition. The NB-priors clas-
sifier, which is part of our model, can only make
use of the word n-gram features; it uses n-gram
features of lengths 3, 4, and 5. Note that since the
NB model is trained on the Google corpus, the an-
notated ESL training data is used only to replace
the prior parameters of the model (see Rozovskaya
and Roth, 2011 for more details).
3.2.2 Training the Preposition System
Correcting preposition errors requires more data
to achieve performance comparable to article er-
ror correction due to the task complexity (Gamon,
2010). We found that training an AP model on
the ESL training data with more sophisticated fea-
tures is not as effective as training on a native En-
glish dataset of larger size. The ESL training data
contains slightly over 100K preposition examples,
which is several orders of magnitude smaller than
the Google n-gram corpus. We use the shared
task training data to replace the prior parameters
of the model (see Rozovskaya and Roth, 2011 for
more details). The NB-priors model does not tar-
get preposition omissions and insertions: it cor-
rects only preposition replacements that involve
the 12 most common English prepositions. The
task includes mistakes that cover 36 prepositions
but we found that the model performance drops
once the confusion set becomes too large. Table
5 shows the performance of the system on the 5-
fold CV on the training data, where each time the
classifier was trained on 80% of the documents.
Model Precision Recall F1
NB-priors 0.14 0.14 0.14
Table 5: Preposition results: NB with priors. Results on
5-fold CV. The model is trained on the Google corpus.
3.3 Correcting Nouns and Verbs
The three remaining types of errors ? noun num-
ber errors, subject-verb agreement, and the various
verb form mistakes ? are corrected using separate
NB models also trained on the Google corpus. We
focus here on the selection of candidates for cor-
rection, as this strongly affects performance.
3.3.1 Candidate Selection
This stage selects the set of words that are pre-
sented as input to the classifier. This is a crucial
step because it limits the performance of any sys-
tem: those errors that are missed at this stage have
no chance of being detected by the later stages.
This is also a challenging step as the class of
verbs and nouns is open, with many English verbs
and nouns being compatible with multiple parts of
speech. This problem does not arise in preposi-
tion and article error correction, where candidates
are determined by surface form (i.e. can be deter-
mined using a closed list of prepositions or arti-
cles).
We use the POS tag and the shallow parser out-
put to identify the set of candidates that are input
to the classifiers. In particular, for nouns, we col-
lect all words tagged as NN or NNS. Since pre-
processing tools are known to make more mis-
takes on ESL data than on native data, this pro-
cedure does not have a perfect result on the iden-
tification of all noun mistakes. For example, we
16
miss about 10% of noun errors due to POS/shallow
parser errors. For verbs, we compared several
candidate selection methods. Method (1) ex-
tracts all verbs heading a verb phrase, as iden-
tified by the shallow parser. Method (2) ex-
pands this set to words tagged with one of the
verb POS tags {VB,VBN,VBG,VBD,VBP,VBZ}.
However, generating candidates by selecting only
those tagged as verbs is not good enough, since the
POS tagger performance on ESL data is known to
be suboptimal (Nagata et al, 2011), especially for
verbs containing errors. For example, verbs lack-
ing agreement markers are likely to be mistagged
as nouns (Lee and Seneff, 2008). Erroneous verbs
are exactly the cases that we wish to include.
Method (3) adds words that are in the lemma list of
common English verbs compiled using the Giga-
word corpus. The last method has the highest re-
call on the candidate identification; it misses only
5% of verb errors, and also has better performance
in the complete model. We thus use this method.
3.3.2 Noun-Verb Correction Performance
Table 6 shows the performance of the systems
based on 5-fold CV on the training data. Each
model is trained individually on the Google cor-
pus, and is individually processed to optimize the
respective thresholds.
Model Precision Recall F1
Noun number 0.17 0.38 0.23
Subject-verb agr. 0.19 0.24 0.21
Verb form 0.07 0.20 0.10
Table 6: Noun, subject-verb agreement and
verb form results. Results on 5-fold CV. The
models are trained on the Google corpus.
4 Combined Model
In the previous sections, we described the individ-
ual components of the system developed to target
specific error types. The combined model includes
all of these modules, which are each applied to
examples individually: there is no pipeline, and
the individual predictions of the modules are then
pooled.
The combined system also includes a post-
processing step where we remove certain correc-
tions of noun and verb forms that we found oc-
cur quite often but are never correct. This hap-
pens when both choices ? the writer?s selection
and the correction ? are valid but the latter is ob-
served more frequently in the native training data.
For example, the phrase ?developing country? is
changed to ?developed country? even though both
are legitimate English expressions. If a correction
is frequently proposed but always results in a false
alarm, we add it to a list of changes that is ignored
when we generate the system output. When we
generate the output on Test set, 8 unique pairs of
such changes are ignored (36 pairs of changes in
total).
We now show the combined results on the train-
ing data by conducting 5-fold CV, where we add
one component at a time. Table 8 shows that the
recall and the F1 scores improve when each com-
ponent is added to the system. The final system
achieves an F1 score of 0.21 on the training data
in 5-fold CV.
Model Precision Recall F1
Articles 0.16 0.12 0.14
+Prepositions 0.16 0.14 0.15
+Noun number 0.17 0.23 0.20
+Subject-verb agr. 0.18 0.25 0.21
+Verb form (All) 0.18 0.27 0.21
Table 7: Results on 5-fold CV on the training
data. The article model is trained on the ESL
data using AP. The other models are trained on the
Google corpus. The last line shows the results,
when all of the five modules are included.
5 Test Results
The previous section showed the performance of
the system on the training data. In this section,
we show the results on the test set. As previously,
the performance improves when each component
is added into the final system. However, we also
note that the precision is much higher while the
recall is only slightly lower. We attribute this in-
creased precision to the observed differences in
the percentage of annotated errors in training vs.
test (see Section 3) and hypothesize that the train-
ing data may contain additional relevant errors that
were not included in the annotation.
Besides the original official annotations an-
nounced by the organizers, another set of anno-
tations is offered based on the combination of re-
vised official annotations and accepted alternative
annotations proposed by participants. We show in
Table 8 when our system is scored based on the
17
revised annotations, both the precision and the re-
call are higher. Our system achieves the highest
scores out of 17 participating teams based on both
the original and revised annotations.
Model Precision Recall F1
Scores based on the original annotations
Articles 0.48 0.11 0.18
+Prepositions 0.45 0.12 0.19
+Noun number 0.48 0.21 0.29
+Subject-verb agr. 0.48 0.22 0.30
+Verb form (All) 0.46 0.23 0.31
Scores based on the revised annotations
All 0.62 0.32 0.42
Table 8: Results on Test. The article model is
trained on the ESL data using AP. The other mod-
els are trained on the Google corpus. All denotes
the results of the complete model that includes all
of the five modules.
6 Discussion and Error Analysis
Here, we present some interesting errors that our
system makes.
6.1 Error Analysis
Incorrect verb form correction: Safety is one of
the crucial problems that many countries and com-
panies *concerned/concerns.
Here, the phrasing requires multiple changes;
to maintain the same word order, this correction
would be needed in tandem with the insertion of
the auxiliary ?have? to create a passive construc-
tion.
Incorrect determiner insertion: In this era,
Engineering designs can help to provide more
habitable accommodation by designing a stronger
material so it?s possible to create a taller and safer
building, a better and efficient sanitation system
to prevent *?/ the disease, and also by designing
a way to change the condition of the inhabitable
environment.
This example requires a model of discourse at
the level of recognizing when a specific disease
is a focus of the text, rather than disease in gen-
eral. The use of a singular construction ?a taller
and safer building? in this context is somewhat un-
conventional and potentially makes this distinction
even harder to detect.
Incorrect verb number correction:
One current human *need/needs that should
be given priority is the search for renewable re-
sources.
This appears to be the result of the system
heuristics intended to mitigate POS tagging errors
on ESL text, where the word ?need? is considered
as a candidate verb rather thana noun; this results
in an incorrect change to make the ?verb? agree in
number with the phrase ?one human?.
Incorrect determiner deletion: This had
shown that the engineering design process is es-
sential in solving problems and it ensures that the
problem is thoroughly looked into and ensure that
the engineers are generating ideas that target the
main problem, *the/? depletion and harmful fuel.
In this example, local context may suggest a list
structure, but the wider context indicates that the
comma represents an appositive structure.
6.2 Discussion
Note that the presence of multiple errors can have
very negative effects on preprocessing. For exam-
ple, when an incorrect verb form is used that re-
sults in a word form commonly used as a noun,
the outputs of the parsers tend to be incorrect. This
limits the potential of rule-based approaches.
Machine learning approaches, on the other
hand, require sufficient examples of each error
type to allow robust statistical modeling of contex-
tual features. Given the general sparsity of ESL
errors, together with the additional noise intro-
duced into more sophisticated preprocessing com-
ponents by errors with overlapping contexts, it ap-
pears hard to leverage these more sophisticated
tools to generate features for machine learning ap-
proaches. This motivates our use of just POS and
shallow parse analysis, together with language-
modeling approaches that can use counts derived
from very large native corpora, to provide robust
inputs for machine learning algorithms.
The interaction between errors suggests that
constraints could be used to improve results by en-
suring, for example, that verb number, noun num-
ber, and noun phrase determiner are consistent.
This is more difficult than it may first appear for
two reasons. First, the noun that is the subject
of the verb under consideration may be relatively
distant in the sentence (due to the presence of in-
tervening relative clauses, for example). Second,
the constraint only limits the possible correction
options: the correct number for the noun in fo-
18
cus may depend on the form used in the preceding
sentences ? for example, to distinguish between a
general statement about some type of entity, and a
statement about a specific entity.
These observations suggest that achieving very
high performance in the task of grammar correc-
tion requires sophisticated modeling of deep struc-
ture in natural language documents.
7 Conclusion
We have described our system that participated in
the shared task on grammatical error correction
and ranked first out of 17 participating teams. We
built specialized models for the five types of mis-
takes that are the focus of the competition. We
have also presented error analysis of the system
output and discussed possible directions for future
work.
Acknowledgments
This material is based on research sponsored by DARPA under agreement num-
ber FA8750-13-2-0008. The U.S. Government is authorized to reproduce and
distribute reprints for Governmental purposes notwithstanding any copyright
notation thereon. The views and conclusions contained herein are those of the
authors and should not be interpreted as necessarily representing the official
policies or endorsements, either expressed or implied, of DARPA or the U.S.
Government. This research is also supported by a grant from the U.S. Depart-
ment of Education and by the DARPA Machine Reading Program under Air
Force Research Laboratory (AFRL) prime contract no. FA8750-09-C-018.
References
T. Brants and A. Franz. 2006. Web 1T 5-gram Version
1. Linguistic Data Consortium, Philadelphia, PA.
D. Dahlmeier, H.T. Ng, and S.M. Wu. 2013. Building
a large annotated corpus of learner english: The nus
corpus of learner english. In Proc. of the NAACL
HLT 2013 Eighth Workshop on Innovative Use of
NLP for Building Educational Applications, Atlanta,
Georgia, June. Association for Computational Lin-
guistics.
Yoav Freund and Robert E. Schapire. 1999. Large
margin classification using the perceptron algorithm.
Machine Learning.
M. Gamon. 2010. Using mostly native data to correct
errors in learners? writing. In NAACL, pages 163?
171, Los Angeles, California, June.
N. Han, M. Chodorow, and C. Leacock. 2006. De-
tecting errors in English article usage by non-native
speakers. Journal of Natural Language Engineer-
ing, 12(2):115?129.
J. Lee and S. Seneff. 2008. Correcting misuse of verb
forms. In ACL, pages 174?182, Columbus, Ohio,
June. Association for Computational Linguistics.
R. Nagata, E. Whittaker, and V. Sheinman. 2011. Cre-
ating a manually error-tagged and shallow-parsed
learner corpus. In ACL, pages 1210?1219, Portland,
Oregon, USA, June. Association for Computational
Linguistics.
H. T. Ng, S. M. Wu, Y. Wu, Ch. Hadiwinoto, and
J. Tetreault. 2013. The conll-2013 shared task
on grammatical error correction. In Proc. of the
Seventeenth Conference on Computational Natural
Language Learning. Association for Computational
Linguistics.
V. Punyakanok and D. Roth. 2001. The use of classi-
fiers in sequential inference. In NIPS.
N. Rizzolo and D. Roth. 2010. Learning Based Java
for Rapid Development of NLP Systems. In LREC.
A. Rozovskaya and D. Roth. 2010a. Annotating ESL
errors: Challenges and rewards. In Proceedings of
the NAACL Workshop on Innovative Use of NLP for
Building Educational Applications.
A. Rozovskaya and D. Roth. 2010b. Generating con-
fusion sets for context-sensitive error correction. In
Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing (EMNLP).
A. Rozovskaya and D. Roth. 2010c. Training
paradigms for correcting errors in grammar and us-
age. In NAACL.
A. Rozovskaya and D. Roth. 2011. Algorithm selec-
tion and model adaptation for esl correction tasks.
In ACL.
A. Rozovskaya, M. Sammons, J. Gioja, and D. Roth.
2011. University of Illinois system in HOO text cor-
rection shared task.
A. Rozovskaya, M. Sammons, and D. Roth. 2012. The
UI system in the hoo 2012 shared task on error cor-
rection.
19
Proceedings of the Eighteenth Conference on Computational Natural Language Learning: Shared Task, pages 34?42,
Baltimore, Maryland, 26-27 July 2014.
c?2014 Association for Computational Linguistics
The Illinois-Columbia System in the CoNLL-2014 Shared Task
Alla Rozovskaya
1
Kai-Wei Chang
2
Mark Sammons
2
Dan Roth
2
Nizar Habash
1
1
Center for Computational Learning Systems, Columbia University
{alla,habash}@ccls.columbia.edu
2
Cognitive Computation Group, University of Illinois at Urbana-Champaign
{kchang10,mssammon,danr}@illinois.edu
Abstract
The CoNLL-2014 shared task is an ex-
tension of last year?s shared task and fo-
cuses on correcting grammatical errors in
essays written by non-native learners of
English. In this paper, we describe the
Illinois-Columbia system that participated
in the shared task. Our system ranked sec-
ond on the original annotations and first on
the revised annotations.
The core of the system is based on the
University of Illinois model that placed
first in the CoNLL-2013 shared task. This
baseline model has been improved and ex-
panded for this year?s competition in sev-
eral respects. We describe our underly-
ing approach, which relates to our previ-
ous work, and describe the novel aspects
of the system in more detail.
1 Introduction
The topic of text correction has seen a lot of inter-
est in the past several years, with a focus on cor-
recting grammatical errors made by English as a
Second Language (ESL) learners. ESL error cor-
rection is an important problem since most writers
of English are not native English speakers. The in-
creased interest in this topic can be seen not only
from the number of papers published on the topic
but also from the three competitions devoted to
grammatical error correction for non-native writ-
ers that have recently taken place: HOO-2011
(Dale and Kilgarriff, 2011), HOO-2012 (Dale et
al., 2012), and the CoNLL-2013 shared task (Ng
et al., 2013).
In all three shared tasks, the participating sys-
tems performed at a level that is considered ex-
tremely low compared to performance obtained in
other areas of NLP: even the best systems attained
F1 scores in the range of 20-30 points.
The key reason that text correction is a diffi-
cult task is that even for non-native English speak-
ers, writing accuracy is very high, as errors are
very sparse. Even for some of the most com-
mon types of errors, such as article and preposi-
tion usage, the majority of the words in these cate-
gories (over 90%) are used correctly. For instance,
in the CoNLL training data, only 2% of preposi-
tions are incorrectly used. Because errors are so
sparse, it is more difficult for a system to identify a
mistake accurately and without introducing many
false alarms.
The CoNLL-2014 shared task (Ng et al., 2014)
is an extension of the CoNLL-2013 shared task
(Ng et al., 2013). Both competitions make use
of essays written by ESL learners at the National
University of Singapore. However, while the first
one focused on five kinds of mistakes that are com-
monly made by ESL writers ? article, preposition,
noun number, verb agreement, and verb form ?
this year?s competition covers all errors occurring
in the data. Errors outside the target group were
present in the task corpora last year as well, but
were not evaluated.
Our system extends the one developed by the
University of Illinois (Rozovskaya et al., 2013)
that placed first in the CoNLL-2013 competition.
For this year?s shared task, the system has been
extended and improved in several respects: we ex-
tended the set of errors addressed by the system,
developed a general approach for improving the
error-specific models, and added a joint inference
component to address interaction among errors.
See Rozovskaya and Roth (2013) for more detail.
We briefly discuss the task (Section 2) and give
an overview of the baseline Illinois system (Sec-
tion 3). Section 4 presents the novel aspects of the
system. In Section 5, we evaluate the complete
system on the development data and show the re-
sults obtained on test. We offer error analysis and a
brief discussion in Section 6. Section 7 concludes.
34
Error type Rel. freq. Examples
Article (ArtOrDet) 14.98% *?/The government should help encourage *the/?
breakthroughs as well as *a/? complete medication
system .
Wrong collocation (Wci) 11.94% Some people started to *think/wonder if electronic
products can replace human beings for better perfor-
mances .
Local redundancy (Rloc-) 10.52% Some solutions *{as examples}/? would be to design
plants/fertilizers that give higher yield ...
Noun number (Nn) 8.49% There are many reports around the internet and on
newspaper stating that some users ? *iPhone/iPhones
exploded .
Verb tense (Vt) 7.21% Through the thousands of years , most Chinese scholars
*are/{have been} greatly affected by Confucianism .
Orthography/punctuation (Mec) 6.88% Even British Prime Minister , Gordon Brown *?/, has
urged that all cars in *britain/Britain to be green by
2020 .
Preposition (Prep) 5.43% I do not agree *on/with this argument that surveillance
technology should not be used to track people .
Word form (Wform) 4.87% On the other hand , the application of surveillance tech-
nology serves as a warning to the *murders/murderers
and they might not commit more murder .
Subject-verb agreement (SVA) 3.44% However , tracking people *are/is difficult and different
from tracking goods .
Verb form (Vform) 3.25% Travelers survive in desert thanks to GPS
*guide/guiding them .
Tone (Wtone) 1.29% Hence , as technology especially in the medical field
continues to get developed and updated , people {do
n?t}/{do not} risk their lives anymore .
Table 1: Example errors. In the parentheses, the error codes used in the shared task are shown. Note
that only the errors exemplifying the relevant phenomena are marked in the table; the sentences may
contain other mistakes. Errors marked as verb form include multiple grammatical phenomena that may
characterize verbs. Our system addresses all of the error types except ?Wrong Collocation? and ?Local
Redundancy?.
2 Task Description
Both the training and the test data of the CoNLL-
2014 shared task consist of essays written by stu-
dents at the National University of Singapore. The
training data contains 1.2 million words from the
NUCLE corpus (Dahlmeier et al., 2013) corrected
by English teachers, and an additional set of about
30,000 words that was released last year as a test
set for the CoNLL-2013 shared task. We use last
year?s test data as a development set; the results in
the subsequent sections are reported on this subset.
The CoNLL corpus error tagset includes 28 er-
ror categories. Table 1 illustrates the most com-
mon error categories in the training data; errors are
marked with an asterisk, and ? denotes a missing
word. Our system targets all of these, with the ex-
ception of collocation and local redundancy errors.
Among the less commonly occurring error types,
our system addresses tone (style) errors; these are
illustrated in the table.
It should be noted that the proportion of erro-
neous instances is several times higher in the de-
velopment data than in the training data for all of
the error categories. For example, while only 2.4%
of noun phrases in the training data have deter-
miner errors, in the development data 10% of noun
phrases have determiner errors.
35
?Hence, the environmental *factor/factors also
*contributes/contribute to various difficulties,
*included/including problems in nuclear tech-
nology.?
Error type Confusion set
Noun number {factor, factors}
Verb Agreement {contribute, contributes}
Verb Form
{included, including,
includes, include}
Table 2: Sample confusion sets for noun num-
ber, verb agreement, and verb form.
3 The Baseline System
In this section, we briefly describe the Univer-
sity of Illinois system (henceforth Illinois; in the
overview paper of the shared task the system is re-
ferred to as UI) that achieved the best result in the
CoNLL-2013 shared task and which we use as our
baseline model. For a complete description, we
refer the reader to Rozovskaya et al. (2013).
The Illinois system implements five
independently-trained machine-learning clas-
sifiers that follow the popular approach to ESL
error correction borrowed from the context-
sensitive spelling correction task (Golding and
Roth, 1999; Carlson et al., 2001). A confusion
set is defined as a list of confusable words.
Each occurrence of a confusable word in text is
represented as a vector of features derived from a
context window around the target. The problem
is cast as a multi-class classification task and a
classifier is trained on native or learner data. At
prediction time, the model selects the most likely
candidate from the confusion set.
The confusion set for prepositions includes the
top 12 most frequent English prepositions (this
year, we extend the confusion set and also target
extraneous preposition usage). The article confu-
sion set is as follows: {a, the, ?}.
1
The confu-
sion sets for noun, agreement, and form modules
depend on the target word and include its morpho-
logical variants. Table 2 shows sample confusion
sets for noun, agreement, and form errors.
Each classifier takes as input the corpus doc-
uments preprocessed with a part-of-speech tag-
1
? denotes noun-phrase-initial contexts where an article
is likely to have been omitted. The variants ?a? and ?an? are
conflated and are restored later.
ger
2
and shallow parser
3
(Punyakanok and Roth,
2001). The other system components use the pre-
processing tools only as part of candidate genera-
tion (e.g., to identify all nouns in the data for the
noun classifier).
The choice of learning algorithm for each clas-
sifier is motivated by earlier findings showing
that discriminative classifiers outperform other
machine-learning methods on error correction
tasks (Rozovskaya and Roth, 2011). Thus, the
classifiers trained on the learner data make use of
a discriminative model. Because the Google cor-
pus does not contain complete sentences but only
n-gram counts of length up to five, training a dis-
criminative model is not desirable, and we thus use
NB (details in Rozovskaya and Roth (2011)).
The article classifier is a discriminative model
that draws on the state-of-the-art approach de-
scribed in Rozovskaya et al. (2012). The model
makes use of the Averaged Perceptron (AP) algo-
rithm (Freund and Schapire, 1996) and is trained
on the training data of the shared task with rich
features. The article module uses the POS and
chunker output to generate some of its features and
candidates (likely contexts for missing articles).
The original word choice (the source article)
used by the writer is also used as a feature. Since
the errors are sparse, this feature causes the model
to abstain from flagging mistakes, resulting in low
recall. To avoid this problem, we adopt the ap-
proach proposed in Rozovskaya et al. (2012), the
error inflation method, and add artificial article er-
rors to the training data based on the error distribu-
tion on the training set. This method prevents the
source feature from dominating the context fea-
tures, and improves the recall of the system.
The other classifiers in the baseline system ?
noun number, verb agreement, verb form, and
preposition ? are trained on native English data,
the Google Web 1T 5-gram corpus (henceforth,
Google, (Brants and Franz, 2006)) with the Na??ve
Bayes (NB) algorithm. All models use word n-
gram features derived from the 4-word window
around the target word. In the preposition model,
priors for preposition preferences are learned from
the shared task training data (Rozovskaya and
Roth, 2011).
The modules targeting verb agreement and
2
http://cogcomp.cs.illinois.edu/page/
software view/POS
3
http://cogcomp.cs.illinois.edu/page/
software view/Chunker
36
verb form mistakes draw on the linguistically-
motivated approach to correcting verb errors pro-
posed in Rozovskaya et. al (2014).
4 The CoNLL-2014 System
The system in the CoNLL-2014 shared task is im-
proved in three ways: 1) Additional error-specific
classifiers: word form, orthography/punctuation,
and style; 2) Model combination; and 3) Joint in-
ference to address interacting errors. Table 3 sum-
marizes the Illinois and the Illinois-Columbia sys-
tems.
4.1 Targeting Additional Errors
The Illinois-Columbia system implements several
new classifiers to address word form, orthography
and punctuation, and style errors (Table 1).
4.1.1 Word Form Errors
Word form (Wform) errors are grammatical er-
rors that involve confusing words that share a
base form but differ in derivational morphology,
e.g. ?use? and ?usage? (see also Table 1). Con-
fusion sets for word form errors thus should in-
clude words that differ derivationally but share the
same base form. In contrast to verb form errors
where confusion sets specify all possible inflec-
tional forms for a given verb, here, the associated
parts-of-speech may vary more widely. An ex-
ample of a confusion set is {technique, technical,
technology, technological}.
Because word form errors encompass a wide
range of misuse, one approach is to consider ev-
ery word as an error candidate. We follow a more
conservative method and only attempt to correct
those words that occurred in the training data and
were tagged as word form errors (we cleaned up
that list by removing noisy annotations).
A further challenge in addressing word form er-
rors is generating confusion sets. We found that
about 45% of corrections for word form errors in
the development data are covered by the confusion
sets from the training data for the same word. We
thus derive the confusion sets using the training
data. Specifically, for every source word that is
tagged as a word form error in the training data,
the confusion set includes all labels to which that
word is mapped in the training data. In addition,
plural and singular forms are added for all words
tagged as nouns, and inflectional forms are added
for words tagged as verbs. For more detail on
correcting verb errors, we refer the reader to Ro-
zovskaya et al. (2014).
4.1.2 Orthography and Punctuation Errors
The Mec error category includes errors in
spelling, context-sensitive spelling, capitalization,
and punctuation. Our system addresses punctua-
tion errors and capitalization errors.
To correct capitalization errors, we collected
words that are always capitalized in the train-
ing and development data when not occurring
sentence-initially.
The punctuation classifier includes two mod-
ules: a learned component targets missing and
extraneous comma usage and is an AP classifier
trained on the learner data with error inflation.
A second, pattern-based component, complements
the AP model: it inserts missing commas by using
a set of patterns that overwhelmingly prefer the us-
age of a comma, e.g. when a sentence starts with
the word ?hence?. The patterns are learned auto-
matically over the training data: specifically, us-
ing a sliding window of three words on each side,
we compiled a list of word n-gram contexts that
are strongly associated with the usage of a comma.
This list is then used to insert missing commas in
the test data.
4.1.3 Style Errors
The style (Wtone) errors marked in the corpus are
diverse, and the annotations are often not consis-
tent. We constructed a pattern-based system to
deal with two types of style errors that are com-
monly annotated. The first type of style edit avoids
using contractions of negated auxiliary verbs. For
example, it changes ?do n?t? to ?do not?. We use a
pattern-based classifier to identify such errors and
replace the contractions. The second type of style
edit encourages the use of a semi-colon to join
two independent clauses when a conjunctive ad-
verb is used. For example, it edits ?[clause], how-
ever, [clause]? to ?[clause]; however, [clause]?. To
identify such errors, we use a part-of-speech tag-
ger to recognize conjunctive adverbs signifying in-
dependent clauses: if two clauses are joined by the
pattern ?, [conjunctive adverb],?, we will replace it
with ?; [conjunctive adverb],?.
4.2 Modules not Included in the Final System
In addition to the modules described above, we at-
tempted to address two other common error cate-
gories: spelling errors and collocation errors. We
37
Illinois
Classifiers Training data Algorithm
Article Learner AP with inflation
Preposition Native NB-priors
Noun number Native NB
Verb agreement Native NB
Verb form Native NB
Illinois-Columbia
Classifiers Training data Algorithm
Article Learner and native AP with infl. (learner) and NB-priors (native)
Preposition Learner and native AP with infl. (learner) and NB-priors (native)
Noun number Learner and native AP with infl. (learner) and NB (native)
Verb agreement Native AP with infl. (learner) and NB (native)
Verb form Native NB-priors
Word form Native NB-priors
Orthography/punctuation Learner AP and pattern-based
Style Learner Pattern-based
Model combination Section 4.3
Global inference Section 4.4
Table 3: The baseline (Illinois) system vs. the Illinois-Columbia system. AP stands for Averaged
Perceptron, and NB stands for the Na??ve Bayes algorithm.
describe these below even though they were not
included in the final system.
Regular spelling errors are noticeable but not
very frequent, and a number are not marked in
the corpus (for example, the word ?dictronary? in-
stead of ?dictionary? is not tagged as an error). We
used an open source package ? ?Jazzy?
4
? to at-
tempt to automatically correct these errors to im-
prove context signals for other modules. However,
there are often multiple similar words that can be
proposed as corrections, and Jazzy uses phonetic
guidelines that sometimes lead to unintuitive pro-
posals (such as ?doctrinaire? for ?dictronary?). It
would be possible to extend the system with a filter
on candidate answers that uses n-grams or some
other context model to choose better candidates,
but the relatively small number of such errors lim-
its the potential impact of such a system.
Collocation errors are the second most common
error category accounting for 11.94% of all errors
in the training data (Table 1). We tried using the
Illinois context-sensitive spelling system
5
to de-
tect these errors, but this system requires prede-
fined confusion sets to detect possible errors and
to propose valid corrections. The coverage of the
pre-existing confusion sets was poor ? the system
4
http://jazzy.sourceforge.net
5
http://cogcomp.cs.illinois.edu/cssc/
could potentially correct only 2.5% of collocation
errors ? and it is difficult to generate new con-
fusion sets that generalize well, which requires a
great deal of annotated training data. The sys-
tem performance was relatively poor because it
proposed many spurious corrections: we believe
this is due to the relatively limited context it uses,
which makes it particularly susceptible to making
mistakes when there are multiple errors in close
proximity.
4.3 Model Combination
Model combination is another key extension of the
Illinois system.
In the Illinois-Columbia system, article, prepo-
sition, noun, and verb agreement errors are each
addressed via a model that combines error predic-
tions made by a classifier trained on the learner
data with the AP algorithm and those made by
the NB model trained on the Google corpus. The
AP classifiers all make use of richer sets of fea-
tures than the native-trained classifiers: the article,
noun number, and preposition classifiers employ
features that use POS information, while the verb
agreement classifier also makes use of dependency
features extracted using a parser (de Marneffe et
al., 2008). For more detail on the features used
in the agreement module, we refer the reader to
38
Rozovskaya et al. (2014). Finally, all of the AP
models use the source word of the author as a fea-
ture and, similar to the article AP classifier (Sec-
tion 3), implement the error inflation method. The
combined model generates a union of corrections
produced by the components.
We found that for every error type, the com-
bined model is superior to each of the single classi-
fiers, as it combines the advantages of both of the
classifiers so that they complement one another.
In particular, while each of the learner and native
components have similar precision, since the pre-
dictions made differ, the recall of the combined
model improves.
4.4 Joint Inference
One of the mistakes typical for Illinois system
were inconsistent predictions. Inconsistent predic-
tions occur when the classifiers address grammat-
ical phenomena that interact at the sentence level,
e.g. noun number and verb agreement. To ad-
dress this problem, the Illinois-Columbia system
makes use of global inference via an Integer Lin-
ear Programming formulation (Rozovskaya and
Roth, 2013). Note that Rozovskaya and Roth
(2013) also describe a joint learning model that
performs better than the joint inference approach.
However, the joint learning model is based on
training a joint model on the Google corpus, and
is not as strong as the individually-trained classi-
fiers of the Illinois-Columbia system that combine
predictions from two components ? NB classifiers
trained on the native data from the Google corpus
and AP models trained on the learner data (Sec-
tion 4.3).
5 Experimental Results
In Sections 3 and 4, we described the individual
system components that address different types of
errors. In this section, we show how the system
improves when each component is added into the
system. In this year?s competition, systems are
compared using F0.5 measure instead of F1. This
is because in error correction good precision is
more important than having a high recall, and the
F0.5 reflects that by weighing precision twice as
much as recall. System output is scored with the
M2 scorer (Dahlmeier and Ng, 2012).
Table 4 reports performance results of each in-
dividual classifier. In the final system, the arti-
cle, preposition, noun number, and verb agree-
Model P R F0.5
Articles (AP) 38.97 8.85 23.19
Articles (NB-priors) 47.34 6.01 19.93
Articles (Comb.) 38.73 10.93 25.67
Prep. (AP) 34.00 0.5 2.35
Prep. (NB-priors) 33.33 0.79 3.61
Prep. (Comb.) 30.06 1.17 5.13
Noun number (NB) 44.74 5.48 18.39
Noun number (AP) 82.35 0.41 2.01
Noun number (Comb.) 45.02 5.57 18.63
Verb agr. (AP) 38.56 1.23 5.46
Verb agr. (NB) 63.41 0.76 3.64
Verb agr. (Comb.) 41.09 1.55 6.75
Verb form (NB-priors) 59.26 1.41 6.42
Word form (NB-priors) 57.54 3.02 12.48
Mec (AP; patterns) 48.48 0.47 2.26
Style (patterns) 84.62 0.64 3.13
Table 4: Performance of classifiers targeting
specific errors.
Model P R F0.5
The baseline (Illinois) system
Articles 38.97 8.85 23.19
+Prepositions 39.24 9.35 23.93
+Noun number 42.13 14.83 30.79
+Subject-verb agr. 42.25 16.06 31.86
+Verb form 43.19 17.20 33.17
Model Combination
+Model combination 42.72 20.19 34.92
Additional Classifiers
+Word form 43.39 21.54 36.07
+Mec 43.70 22.04 36.52
+Style 44.22 21.54 37.09
Joint Inference
+Joint Inference 44.28 22.57 37.13
Table 5: Results on the development data. The
top part of the table shows the performance of the
baseline (Illinois) system from last year.
P R F0.5
Scores based on the original annotations
41.78 24.88 36.79
Scores based on the revised annotations
52.44 29.89 45.57
Table 6: Results on Test.
39
ment classifiers use combined models, each con-
sisting of a classifier trained on the learner data
and a classifier trained on native data. We report
performance of each such component separately
and when they are combined. The results show
that combining models boosts the performance of
each classifier: for example, the performance of
the article classifier improves by more than 2 F0.5
points. It should be noted that results are com-
puted with respect to all errors present in the data.
For this reason, recall is low.
Next, in Table 5, we show the contribution of
the novel components over the baseline system on
the development set. As described in Section 3,
the baseline Illinois system consists of five indi-
vidual components; their performance is shown in
the top part of the table. Note that although for the
development set we make use of last year?s test
set, these results are not comparable to the perfor-
mance results reported in last year?s competition
that used the F1 measure. Overall, the baseline
system achieves an F0.5 score of 33.17 on the de-
velopment set.
Then, by applying the model combination tech-
nique introduced in Section 4.3, the performance
is improved to 34.92. By adding modules to tar-
get three additional error types, the overall perfor-
mance becomes 37.09. Finally, the joint inference
technique (see Section 4.4) slightly improves the
performance further. The final system achieves an
F0.5 score of 37.13.
Table 6 shows the results on the test set provided
by the organizers. As was done previously, the
organizers also offered another set of annotations
based on the combination of revised official anno-
tations and accepted alternative annotations pro-
posed by participants. Performance results on this
set are also shown in Table 6.
6 Discussion and Error Analysis
Here, we present some interesting errors that our
system makes on the development set and discuss
our observations on the competition. We analyze
both the false positive errors and those cases that
are missed by our system.
6.1 Error Analysis
Stylistic preference Surveillance technology
such as RFID (radio-frequency identification) is
one type of examples that has currently been im-
plemented.
Here, our system proposes a change to plural
for the noun ?technology?. The gold standard
solution instead proposes a large number of cor-
rections throughout that work with the choice of
the singular ?technology?. However, using the
plural ?technologies? as proposed by the Illinois-
Columbia system is quite acceptable, and a com-
parable number of corrections would make the rest
of the sentence compatible. Note also that the
gold standard proposes the use of commas around
the phrase ?such as RFID (radio-frequency iden-
tification)?, which could also be omitted based on
stylistic considerations alone.
Word choice The high accuracy in utiliz-
ing surveillance technology eliminates the
*amount/number of disagreements among people.
The use of ?amount? versus ?number? depends
on the noun to which the term attaches. This could
conceivably be achieved by using a rule and word
list, but many such rules would be needed and each
would have relatively low coverage. Our system
does not detect this error.
Presence of multiple errors Not only the details
of location will be provided, but also may lead to
find out the root of this kind of children trading
agency and it helps to prevent more this kind of
tragedy to happen on any family.
The writer has made numerous errors in this
sentence. To determine the correct preposition in
the marked location requires at least the preced-
ing verb phrase to be corrected to ?from happen-
ing?; the extraneous ?more? after ?prevent? in turn
makes the verb phrase correction more unlikely as
it perturbs the contextual clues that a system might
learn to make that correction. Our system pro-
poses a different preposition ? ?in? ? that is better
than the original in the local context, but which is
not correct in the wider context.
Locally coherent, globally incorrect People?s
lives become from increasingly convenient to al-
most luxury, thanks to the implementation of in-
creasingly technology available for the Man?s life.
In this example, the system proposes to delete
the preposition ?from?. This correctiom improves
the local coherency of the sentence. However, the
resulting construction is not consistent with ?to al-
most luxury?, suggesting a more complex correc-
tion (changing the word ?become? to ?are going?).
40
Cascading NLP errors In this, I mean that we
can input this device implant into an animal or
birds species, for us to track their movements and
actions relating to our human research that can
bring us to a new regime.
The word ?implant? in the example sentence
has been identified as a verb by the system and
not a noun due to the unusual use as part of the
phrase ?device implant?. As a result, the system
incorrectly proposes the verb form correction ?im-
planted?.
6.2 Discussion
The error analysis suggests that there are three sig-
nificant challenges to developing a better gram-
mar correction system for the CoNLL-2014 shared
task: identifying candidate errors; modeling the
context of possible errors widely enough to cap-
ture long-distance cues where necessary; and
modeling stylistic preferences involving word
choice, selection of plural or singular, standards
for punctuation, use of a definite or indefinite arti-
cle (or no article at all), and so on. For ESL writ-
ers, the tendency for multiple errors to be made in
close proximity means that global decisions must
be made about sets of possible mistakes, and a sys-
tem must therefore have a quite sophisticated ab-
stract model to generate the basis for consistent
sets of corrections to be proposed.
7 Conclusion
We have described our system that participated in
the shared task on grammatical error correction.
The system builds on the elements of the Illinois
system that participated in last year?s shared task.
We extended and improved the Illinois system in
three key dimensions, which we presented and
evaluated in this paper. We have also presented
error analysis of the system output and discussed
possible directions for future work.
Acknowledgments
This material is based on research sponsored by DARPA un-
der agreement number FA8750-13-2-0008. The U.S. Gov-
ernment is authorized to reproduce and distribute reprints for
Governmental purposes notwithstanding any copyright nota-
tion thereon. The views and conclusions contained herein are
those of the authors and should not be interpreted as necessar-
ily representing the official policies or endorsements, either
expressed or implied, of DARPA or the U.S. Government.
This research is also supported by a grant from the U.S. De-
partment of Education and by the DARPA Machine Reading
Program under Air Force Research Laboratory (AFRL) prime
contract no. FA8750-09-C-018. The first and last authors
were partially funded by grant NPRP-4-1058-1-168 from the
Qatar National Research Fund (a member of the Qatar Foun-
dation). The statements made herein are solely the responsi-
bility of the authors.
References
T. Brants and A. Franz. 2006. Web 1T 5-gram Version
1. Linguistic Data Consortium, Philadelphia, PA.
A. J. Carlson, J. Rosen, and D. Roth. 2001. Scaling up
context sensitive text correction. In IAAI.
D. Dahlmeier and H.T. Ng. 2012. Better evaluation
for grammatical error correction. In NAACL, pages
568?572, Montr?eal, Canada, June. Association for
Computational Linguistics.
D. Dahlmeier, H.T. Ng, and S.M. Wu. 2013. Building
a large annotated corpus of learner english: The nus
corpus of learner english. In Proc. of the NAACL
HLT 2013 Eighth Workshop on Innovative Use of
NLP for Building Educational Applications, Atlanta,
Georgia, June. Association for Computational Lin-
guistics.
R. Dale and A. Kilgarriff. 2011. Helping Our Own:
The HOO 2011 pilot shared task. In Proceedings of
the 13th European Workshop on Natural Language
Generation.
R. Dale, I. Anisimoff, and G. Narroway. 2012. A
report on the preposition and determiner error cor-
rection shared task. In Proc. of the NAACL HLT
2012 Seventh Workshop on Innovative Use of NLP
for Building Educational Applications, Montreal,
Canada, June. Association for Computational Lin-
guistics.
Marie-Catherine de Marneffe, Anna N. Rafferty, and
Christopher D. Manning. 2008. Finding contradic-
tions in text. In ACL.
Yoav Freund and Robert E. Schapire. 1996. Experi-
ments with a new boosting algorithm. In Proc. 13th
International Conference on Machine Learning.
A. R. Golding and D. Roth. 1999. A Winnow
based approach to context-sensitive spelling correc-
tion. Machine Learning.
H.T. Ng, S.M. Wu, Y. Wu, C. Hadiwinoto, and
J. Tetreault. 2013. The conll-2013 shared task
on grammatical error correction. In Proceedings of
the Seventeenth Conference on Computational Nat-
ural Language Learning: Shared Task, pages 1?12,
Sofia, Bulgaria, August. Association for Computa-
tional Linguistics.
41
H. T. Ng, S. M. Wu, T. Briscoe, C. Hadiwinoto, R. H.
Susanto, and C. Bryant. 2014. The CoNLL-2014
shared task on grammatical error correction. In Pro-
ceedings of the Eighteenth Conference on Compu-
tational Natural Language Learning: Shared Task,
Baltimore, Maryland, USA, June. Association for
Computational Linguistics.
V. Punyakanok and D. Roth. 2001. The use of classi-
fiers in sequential inference. In NIPS.
A. Rozovskaya and D. Roth. 2011. Algorithm selec-
tion and model adaptation for esl correction tasks.
In ACL.
A. Rozovskaya and D. Roth. 2013. Joint learning
and inference for grammatical error correction. In
EMNLP, 10.
A. Rozovskaya, M. Sammons, and D. Roth. 2012.
The UI system in the HOO 2012 shared task on er-
ror correction. In Proc. of the Annual Conference
of the North American Chapter of the Association
for Computational Linguistics (NAACL) Workshop
on Innovative Use of NLP for Building Educational
Applications.
A. Rozovskaya, K.-W. Chang, M. Sammons, and
D. Roth. 2013. The University of Illinois system
in the CoNLL-2013 shared task. In CoNLL Shared
Task.
A. Rozovskaya, D. Roth, and V. Srikumar. 2014. Cor-
recting grammatical verb errors. In EACL.
42
Proceedings of the EMNLP 2014 Workshop on Arabic Natural Langauge Processing (ANLP), pages 39?47,
October 25, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
The First QALB Shared Task on Automatic Text Correction for Arabic
Behrang Mohit
1?
, Alla Rozovskaya
2?
, Nizar Habash
3
, Wajdi Zaghouani
1
, Ossama Obeid
1
1
Carnegie Mellon University in Qatar
2
Center for Computational Learning Systems, Columbia University
3
New York University Abu Dhabi
behrang@cmu.edu, alla@ccls.columbia.edu, nizar.habash@nyu.edu
wajdiz@qatar.cmu.edu,owo@qatar.cmu.edu
Abstract
We present a summary of the first shared
task on automatic text correction for Ara-
bic text. The shared task received 18 sys-
tems submissions from nine teams in six
countries and represented a diversity of ap-
proaches. Our report includes an overview
of the QALB corpus which was the source
of the datasets used for training and eval-
uation, an overview of participating sys-
tems, results of the competition and an
analysis of the results and systems.
1 Introduction
The task of text correction has recently gained a
lot of attention in the Natural Language Process-
ing (NLP) community. Most of the effort in this
area concentrated on English, especially on errors
made by learners of English as a Second Lan-
guage. Four competitions devoted to error cor-
rection for non-native English writers took place
recently: HOO (Dale and Kilgarriff, 2011; Dale
et al., 2012) and CoNLL (Ng et al., 2013; Ng et
al., 2014). Shared tasks of this kind are extremely
important, as they bring together researchers who
focus on this problem and promote development
and dissemination of key resources, such as bench-
mark datasets.
Recently, there have been several efforts aimed
at creating data resources related to the correc-
tion of Arabic text. Those include human anno-
tated corpora (Zaghouani et al., 2014; Alfaifi and
Atwell, 2012), spell-checking lexicon (Attia et al.,
2012) and unannotated language learner corpora
(Farwaneh and Tamimi, 2012). A natural exten-
sion to these resource production efforts is the cre-
ation of robust automatic systems for error correc-
tion.
* These authors contributed equally to this work.
In this paper, we present a summary of the
QALB shared task on automatic text correction
for Arabic. The Qatar Arabic Language Bank
(QALB) project
1
is one of the first large scale data
and system development efforts for automatic cor-
rection of Arabic which has resulted in annota-
tion of the QALB corpus. In conjunction with the
EMNLP Arabic NLP workshop, the QALB shared
task is the first community effort for construction
and evaluation of automatic correction systems for
Arabic.
The results of the competition indicate that the
shared task attracted a lot of interest and generated
a diverse set of approaches from the participating
teams.
In the next section, we present the shared task
framework. This is followed by an overview of
the QALB corpus (Section 3). Section 4 describes
the shared task data, and Section 5 presents the ap-
proaches adopted by the participating teams. Sec-
tion 6 discusses the results of the competition. Fi-
nally, in Section 7, we offer a brief analysis and
present preliminary experiments on system com-
bination.
2 Task Description
The QALB shared task was created as a forum for
competition and collaboration on automatic error
correction in Modern Standard Arabic. The shared
task makes use of the QALB corpus (Zaghouani et
al., 2014), which is a manually-corrected collec-
tion of Arabic texts. The shared task participants
were provided with training and development data
to build their systems, but were also free to make
use of additional resources, including corpora, lin-
guistic resources, and software, as long as these
were publicly available.
For evaluation, a standard framework devel-
1
http://nlp.qatar.cmu.edu/qalb/
39
Original Corrected

??

K @Q? @

HCJ


?j

J? @

?
	
Y?

?

K @Q

? Y
	
J? ?



GXA?? ?


Y? @?P??

J

K B
?


X

?

@
	
?@ ?

<? @
	
?? ?


	
??

JK
.

I
	
J? ? H
.
A

? ?


	
G @

B

??
Q

j?
?
@ ?
YJ


?K
.
@
	
Y?
	
?@ @?YJ
.
K


	
?A? ? ?


??

?B@ Yj
.
??
?
AK
.
@P?Q?

?Q???@
	
???
?
?
	
K @ ??

?J


K
.
	
?A?

?J


	
J?B@ ???


Yg ?


	
? A? ??
	
? ?A
	
J?
?
@
.

??J


j

?? ?

J


	
J?@
	
?

BA??

?

?m
Proceedings of the EMNLP 2014 Workshop on Arabic Natural Langauge Processing (ANLP), pages 160?164,
October 25, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
The Columbia System in the QALB-2014 Shared Task
on Arabic Error Correction
Alla Rozovskaya Nizar Habash
?
Ramy Eskander Noura Farra Wael Salloum
Center for Computational Learning Systems, Columbia University
?
New York University Abu Dhabi
{alla,ramy,noura,wael}@ccls.columbia.edu
?
nizar.habash@nyu.edu
Abstract
The QALB-2014 shared task focuses on
correcting errors in texts written in Mod-
ern Standard Arabic. In this paper, we
describe the Columbia University entry in
the shared task. Our system consists of
several components that rely on machine-
learning techniques and linguistic knowl-
edge. We submitted three versions of the
system: these share several core elements
but each version also includes additional
components. We describe our underlying
approach and the special aspects of the dif-
ferent versions of our submission. Our
system ranked first out of nine participat-
ing teams.
1 Introduction
The topic of text correction has seen a lot of in-
terest in the past several years, with a focus on
correcting grammatical errors made by learners of
English as a Second Language (ESL). The two
most recent CoNLL shared tasks were devoted to
grammatical error correction for non-native writ-
ers (Ng et al., 2013; Ng et al., 2014).
The QALB-2014 shared task (Mohit et al.,
2014) is the first competition that addresses the
problem of text correction in Modern Standard
Arabic (MSA) texts. The competition makes
use of the recently developed QALB corpus (Za-
ghouani et al., 2014). The shared task covers all
types of mistakes that occur in the data.
Our system consists of statistical models, lin-
guistic resources, and rule-based modules that ad-
dress different types of errors.
We briefly discuss the task in Section 2. Sec-
tion 3 gives an overview of the Columbia system
and describes the system components. In Sec-
tion 4, we evaluate the complete system on the de-
velopment data and show the results obtained on
test. Section 5 concludes.
2 Task Description
The QALB-2014 shared task addresses the prob-
lem of correcting errors in texts written in Modern
Standard Arabic (MSA). The task organizers re-
leased training, development, and test data. All
of the data comes from online commentaries writ-
ten to Aljazeera articles.
1
The training data con-
tains 1.2 million words; the development and the
test data contain about 50,000 words each. The
data was annotated and corrected by native Arabic
speakers. For more detail on the QALB corpus, we
refer the reader to Zaghouani et al. (2014). The re-
sults in the subsequent sections are reported on the
development set.
It should be noted that in the annotation process,
the annotators did not assign error categories but
only specified an appropriate correction. In spite
of this, it is possible, to isolate certain error types
automatically, by using the corrections in coordi-
nation with the input words. The first type con-
cerns punctuation errors. Errors involving punc-
tuation account for about 39% of all errors in the
data. In addition to punctuation mistakes, another
very common source of errors refers to subopti-
mal spelling for two groups of letters ? Alif (and
its Hamzated versions) and Ya (and its undotted or
Alif Maqsura versions). For more detail on this
and other Arabic phenomena, we refer the reader
to Habash (2010; Buckwalter (2007; El Kholy and
Habash (2012). Mistakes associated with Alif and
1
http://www.aljazeera.net/
160
Component System
CLMB-1 CLMB-2 CLMB-3
MADAMIRA
MLE
Na??ve Bayes
GSEC
MLE-unigram
Punctuation
Dialectal
Patterns
Table 1: The three versions of the Columbia sys-
tem and their components.
Ya spelling constitute almost 30% of all errors.
3 System Overview
The Columbia University system consists of sev-
eral components designed to address different
types of errors. We submitted three versions of the
system. We refer to these as CLMB-1, CLMB-2,
and CLMB-3. Table 1 lists all of the components
and indicates which components are included in
each version. The components are applied in the
order shown in the table. Below we describe each
component in more detail.
3.1 MADAMIRA Corrector
MADAMIRA (Pasha et al., 2014) is a tool
designed for morphological analysis and dis-
ambiguation of Modern Standard Arabic.
MADAMIRA performs morphological analysis
in context. This is a knowledge-rich resource
that requires a morphological analyzer and a
large corpus where every word is marked with
its morphological features. The task organizers
provided the shared task data pre-processed
with MADAMIRA, including all of the features
generated by the tool for every word. In addition
to the morphological analysis and contextual
morphological disambiguation, MADAMIRA
also performs Alif and Ya spelling correction
for the phenomena associated with these letters
discussed in Section 2. The corrected form was
included among the features and can be used
for correcting the input. We use the corrections
proposed by MADAMIRA and apply them to the
data. As we show in Section 4, while the form
proposed by MADAMIRA may not necessarily
be correct, MADAMIRA performs at a very high
precision. MADAMIRA corrector is used in the
CLMB-1 and CLMB-2 systems.
3.2 Maximum Likelihood Model
The Maximum Likelihood Estimator (MLE) is a
supervised component that is trained on the train-
ing data of the shared task. Given the annotated
training data, a map is defined that specifies for ev-
ery word n-gram in the source text the most likely
n-gram corresponding to it in the target text. The
MLE model considers source n-grams of lengths
between 1 to 3; the MLE-unigram model that is
part of the CLMB-3 version only considers n-
grams of length 1.
The MLE approach performs well on errors that
have been observed in the training data and can
be unambiguously corrected without using the sur-
rounding context, i.e. do not have many alternative
corrections. Consequently, MLE fails on words
that have many possible corrections, as well as
words not seen in training.
3.3 Na??ve Bayes for Unseen Words
The Na??ve Bayes component addresses errors for
words that were not seen in training. The system
uses the approach proposed in Rozovskaya and
Roth (2011) that proved to be successful for cor-
recting errors made by English as a Second Lan-
guage learners. The model operates at the word
level and targets word replacement errors that in-
volve single tokens. Candidate corrections are
generated using a character confusion table that is
based on the training data. The model is a Na??ve
Bayes classifier trained on the Arabic Gigaword
corpus (Parker et al., 2011) with word n-gram fea-
tures in the 4-word window around the word to be
corrected. The Na??ve Bayes component is used in
the CLMB-1 system.
3.4 The GSEC Model
The CLMB-3 system implements a Generalized
Character-Level Error Correction model (GSEC)
proposed in Farra et al. (2014). GSEC is a super-
vised model that operates at the character level.
Because of this, the source and the target side of
the training data need to be aligned at the charac-
ter level. We use the alignment tool Sclite (Fiscus,
1998). The alignment maps each source charac-
ter to itself, a different character, a pair of char-
acters, or an empty string. For the shared task,
punctuation corrections are ignored since punctu-
ation errors are handled by the punctuation correc-
tor described in the following section. It should
161
also be noted that the model was not trained to
insert missing characters. The model is a multi-
class SVM classifier (Kudo, 2005) that makes use
of character-level features using a window of four
characters that may occur within the word bound-
aries as well as in the surrounding context. Due
to a long training time, GSEC was trained on a
quarter of the training data. The system is post-
processed with a unigram word-level maximum-
likelihood model described in Section 3.2. For
more detail on the GSEC approach, we refer the
reader to Farra et al. (2014).
3.5 Punctuation Corrector
The shared task data contains a large number of
punctuation mistakes. Punctuation errors, such as
missing periods and commas, account for about
30% of all errors in the data. Most of these errors
involve incorrectly omitting a punctuation symbol.
Our punctuation corrector is a statistical model
that inserts periods and commas. The system is
a decision tree model trained on the shared task
training data using WEKA (Hall et al., 2009). For
punctuation insertion, every space that is not fol-
lowed or preceded by a punctuation mark is con-
sidered.
To generate features, we use a window of size
three around the target space. The features are de-
fined as follows:
? The part-of-speech of the previous word
? The existence of a conjunctive or connective
proclitic in the following word; that is a ?w?
or ?f? proclitic that is either a conjunction, a
sub-conjunction or a connective particle
The part-of-speech and proclitic information is
obtained by running MADAMIRA on the text.
We also ran experiments where the model is
trained with a complete list of features produced
by MADAMIRA; that is part-of-speech, gender,
number, person, aspect, voice, case, mood, state,
proclitics and enclitics. This was done for two pre-
ceding words and two following words. However,
this model did not perform as well as the one de-
scribed above, which we used in the final system.
Note that the punctuation model predicts pres-
ence or absence of a punctuation mark in a spe-
cific location and is applied to the source data
from which all punctuation marks have been re-
moved. However, when we apply our punctuation
model in the correction pipeline, we find that it
is always better to keep the already existing peri-
ods and commas in the input text instead of over-
writing them with the model prediction. In other
words, we only attempt to add missing punctua-
tion.
3.6 Dialectal Usage Corrector
Even though the shared task data is written in
MSA, MSA is not a native language for Arabic
speakers. Typically, an Arabic speaker has a native
proficiency in one of the many Arabic dialects and
learns to write and read MSA in a formal setting.
For this reason, even in MSA texts produced by
native Arabic speakers, one typically finds words
and linguistic features specific to the writer?s na-
tive dialect that are not found in the standard lan-
guage.
To address such errors, we use Elissa (Salloum
and Habash, 2012), which is Dialectal to Standard
Arabic Machine Translation System. Elissa uses
a rule-based approach that relies on the existence
of a dialectal morphological analyzer (Salloum
and Habash, 2011), a list of hand-written trans-
fer rules, and dialectal-to-standard Arabic lexi-
cons. Elissa uses different dialect identification
techniques to select dialectal words and phrases
(dialectal multi-word expressions) that need to be
handled. Then equivalent MSA paraphrases of the
selected words/phrases are generated and an MSA
lattice for each input sentence is constructed. The
paraphrases within the lattice are then ranked us-
ing language models and the n-best sentences are
extracted from lattice. We use 5-gram language
models trained using SRILM (Stolcke, 2002) on
about 200 million untokenized, Alif /Ya normal-
ized words extracted from Arabic GigaWord. This
component is employed in the CLMB-2 system.
3.7 Pattern-Based Corrector
We created a set of rules that account for very
common phenomena involving incorrectly split or
merged tokens. The MADAMIRA corrector de-
scribed above does not handle splits and merges;
however, some of the cases are handled in the
MLE method. Note that the MLE method is re-
strictive since it does not correct words not seen
in training, while the pattern-based corrector is
more general. The rules were created through
analysis of samples of the QALB Shared Task
162
training data. Some of the rules use regular ex-
pressions, while others make use of the rule-
based Standard Arabic Morphological Analyzer
(SAMA) (Maamouri et al., 2010), the same out-
of-context analyzer used inside of MADAMIRA.
Rules for splitting words
? All digits are separated from words.
? A space is added after all word medial Ta-
Marbuta characters.
? A space is added after the very common
?ElY? ?at/about/on? preposition if it is at-
tached to the following word.
? If a word has a morphological analysis that
includes ?lmA? (as negation particle, relative
pronoun or pseudo verb), ?hA? (a demonstra-
tive pronoun), or ?Ebd? and ?>bw? in proper
nouns, a space is inserted after those parts of
the analysis.
? If a word has no morphological analysis, but
starts with a set of commonly mis-attached
words, and the rest of the word has an anal-
ysis, the word is split after the mis-attached
word sequence.
Rules for merging words
? All lone occurrences of the conjunction w
?and? are attached to the following word.
? All sequences of the punctuation marks (., ?,
!) that occur between two and six times are
merged: e.g ! ! ! ? !!!.
4 Experimental Results
In Section 3, we described the individual sys-
tem components that address different types of
errors. In this section, we show how the sys-
tem improves when each component is added into
the system. System output is scored with the
M2 scorer (Dahlmeier and Ng, 2012), the official
scorer of the shared task.
Table 2 reports performance results of each ver-
sion of the Columbia system on the development
data. Table 3 shows the performance results for the
best-performing system, CLMB-1, as each system
component is added.
System P R F1
CLMB-1 72.22 62.79 67.18
CLMB-2 69.49 61.72 65.38
CLMB-3 69.71 59.42 64.15
Table 2: Performance of the Columbia systems
on the development data.
System P R F1
MADAMIRA 83.33 32.94 47.21
+ MLE 86.52 42.52 57.02
+ NB 85.80 43.27 57.53
+ Punc. 73.66 59.51 65.83
+ Patterns 72.22 62.79 67.18
Table 3: Performance of the CLMB-1 system on
the development data and the contribution of
its components.
System P R F1
CLMB-1 73.34 63.23 67.91
CLMB-2 70.86 62.21 66.25
CLMB-3 71.45 60.00 65.22
Table 4: Performance of the Columbia systems
on the test data.
Finally, Table 4 reports results obtained on the
test data. These results are comparable to the per-
formance observed on the development data. In
particular, CLMB-1 achieves the highest score.
5 Conclusion
We have described the Columbia University sys-
tem that participated in the first shared task
on grammatical error correction for Arabic and
ranked first out of nine participating teams. We
have presented three versions of the system; all of
these incorporate several components that target
different types of mistakes, which we presented
and evaluated in this paper.
Acknowledgments
This material is based on research funded by grant
NPRP-4-1058-1-168 from the Qatar National Re-
search Fund (a member of the Qatar Foundation).
The statements made herein are solely the respon-
sibility of the authors. Nizar Habash performed
most of his contribution to this paper while he was
at the Center for Computational Learning Systems
at Columbia University.
163
References
T. Buckwalter. 2007. Issues in Arabic Morphological
Analysis. In A. van den Bosch and A. Soudi, editors,
Arabic Computational Morphology: Knowledge-
based and Empirical Methods. Springer.
D. Dahlmeier and H. T. Ng. 2012. Better evaluation
for grammatical error correction. In Proceedings of
NAACL.
A. El Kholy and N. Habash. 2012. Orthographic and
morphological processing for English?Arabic sta-
tistical machine translation. Machine Translation,
26(1-2).
N. Farra, N. Tomeh, A. Rozovskaya, and N. Habash.
2014. Generalized character-level spelling error cor-
rection. In Proceedings of ACL.
J. Fiscus. 1998. Sclite scoring package ver-
sion 1.5. US National Institute of Standard
Technology (NIST), URL http://www. itl. nist.
gov/iaui/894.01/tools.
N. Y. Habash. 2010. Introduction to Arabic natural
language processing. Synthesis Lectures on Human
Language Technologies 3.1.
M. Hall, F. Eibe, G. Holmes, B. Pfahringer, P. Reute-
mann, and I. H. Witten. 2009. The WEKA data
mining software: an update. SIGKDD Explorations,
11(1):10?18.
T. Kudo. 2005. YamCha: Yet another multipurpose
chunk annotator. http://chasen.org/ taku/software/.
M. Maamouri, D. Graff, B. Bouziri, S. Krouna, A. Bies,
and S. Kulick. 2010. LDC Standard Arabic Mor-
phological Analyzer (SAMA) Version 3.1. Linguistic
Data Consortium.
B. Mohit, A. Rozovskaya, N. Habash, W. Zaghouani,
and O. Obeid. 2014. The first QALB shared task on
automatic text correction for Arabic. In Proceedings
of EMNLP Workshop on Arabic Natural Language
Processing.
H. T. Ng, S. M. Wu, Y. Wu, Ch. Hadiwinoto, and
J. Tetreault. 2013. The CoNLL-2013 shared task
on grammatical error correction. In Proceedings of
CoNLL: Shared Task.
H. T. Ng, S. M. Wu, T. Briscoe, C. Hadiwinoto, R. H.
Susanto, and C. Bryant. 2014. The CoNLL-2014
shared task on grammatical error correction. In Pro-
ceedings of CoNLL: Shared Task.
R. Parker, D. Graff, K. Chen, J. Kong, and K. Maeda.
2011. Arabic Gigaword Fifth Edition. Linguistic
Data Consortium.
A. Pasha, M. Al-Badrashiny, A. E. Kholy, R. Eskan-
der, M. Diab, N. Habash, M. Pooleery, O. Rambow,
and R. Roth. 2014. MADAMIRA: A fast, compre-
hensive tool for morphological analysis and disam-
biguation of arabic. In Proceedings of LREC.
A. Rozovskaya and D. Roth. 2011. Algorithm selec-
tion and model adaptation for ESL correction tasks.
In Proceedings of ACL.
W. Salloum and N. Habash. 2011. Dialectal to stan-
dard arabic paraphrasing to improve arabic-english
statistical machine translation. In Proceedings of
the First Workshop on Algorithms and Resources for
Modelling of Dialects and Language Varieties.
W. Salloum and N. Habash. 2012. Elissa: A dialectal
to standard arabic machine translation system. In
Proceedings of COLING (Demos).
A. Stolcke. 2002. Srilm-an extensible language mod-
eling toolkit. In Proceedings of International Con-
ference on Spoken Language Processing.
W. Zaghouani, B. Mohit, N. Habash, O. Obeid,
N. Tomeh, A. Rozovskaya, N. Farra, S. Alkuhlani,
and K. Oflazer. 2014. Large scale arabic error anno-
tation: Guidelines and framework. In Proceedings
of the Ninth International Conference on Language
Resources and Evaluation (LREC?14).
164
