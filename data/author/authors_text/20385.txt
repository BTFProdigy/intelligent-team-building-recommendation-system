Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 624?627,
Dublin, Ireland, August 23-24, 2014.
Team Z: Wiktionary as a L2 Writing Assistant
Anubhav Gupta
UFR SLHS
Universit?e de Franche-Comt?e
anubhav.gupta@edu.univ-fcomte.fr
Abstract
This paper presents a word-for-word trans-
lation approach using Wiktionary for
SemEval-2014 Task 5. The language
pairs attempted for this task were English-
Spanish and English-German. Since this
approach did not take context into account,
it performed poorly.
1 Introduction
The objective of SemEval-2014 Task 5 is to trans-
late a few words or a phrase from one language
(L1) into another (L2). More specifically, a sen-
tence containing primarily L2 and a few L1 words
is provided, and the task is to translate the L1
words into the L2. This task is similar to the previ-
ous cross-linguistic SemEval tasks involving lexi-
cal substitution (Mihalcea et al., 2010) and word-
sense disambiguation (Lefever and Hoste, 2013).
For example, consider the following sentence,
written entirely in German except for one English
word: Aber auf diesem Schiff wollen wir auch
Ruderer sein, wir sitzen im selben Boot und wollen
mit Ihnen row. Here, the word row is polysemous
and can be translated as the verb rudern or as the
noun Reihe depending on context. The words to
be translated can also form an idiomatic expres-
sion, such as in exchange in die 1967 eroberten
arabischen Gebiete in exchange gegen Frieden.
These examples reveal that this is not a straightfor-
ward task, as word-for-word translation may give
inaccurate results.
Wiktionary is a multilingual dictionary con-
taining word-sense, examples, sample quotations,
collocations, usage notes, proverbs and transla-
tions (Torsten et al., 2008; Meyer and Gurevych,
2012). Since Wiktionary data have previously
This work is licensed under a Creative Commons Attribution
4.0 International Licence. Page numbers and proceedings
footer are added by the organisers. Licence details: http:
//creativecommons.org/licenses/by/4.0/
been used for translations (Orlandi and Passant,
2010), it was chosen for looking up the transla-
tion of source language (L1) words. However, the
translation approach was word-for-word and ig-
nored the target language (L2) context, i.e., the
context in which the text fragment to be trans-
lated is found. The Wiktionary-based solution
is for English-to-Spanish and English-to-German
language translation though four language pairs
were provided in this shared task.
2 Wiktionary
For a given word, the English version of Wik-
tionary gives not only its definition but also possi-
ble translations. The translations are divided based
on part of speech (PoS) and word sense and at
times also encode gender and number informa-
tion. For example, the German and Spanish trans-
lations for the English word book are stored in
Wiktionary as follows:
====Noun====
{{en-noun}}
=====Translations=====
{{trans-top|collection of sheets
of paper bound together
containing printed or written
material}}
*
German: {{t+|de|Buch|n}}
*
Spanish: {{t+|es|libro|m}}
{{trans-top|record of betting}}
*
German: {{t|de|Wettliste|f}}
{{trans-top|convenient collection
of small paper items, such as
stamps}}
*
German: {{t+|de|Album|n}}
*
Spanish: {{t+|es|?album|m}}
{{trans-top|major division of
a published work, larger than
624
a chapter}}
{{trans-top|script of a musical}}
*
Spanish: {{t+|es|libreto|m}}
{{trans-top|usually in plural:
records of the accounts of
a business}}
*
German: {{t+|de|B?ucher|n-p}}
{{trans-top|ebook}}
*
German: {{t+|de|E-Book|n}}
====Verb====
{{en-verb}}
=====Translations=====
{{trans-top|to reserve}}
*
German: {{t+|de|buchen}},
{{t+|de|reservieren}}
*
Spanish: {{t|es|reservar}}
{{trans-top|to write down,
register, record}}
*
German: {{t+|de|notieren}},
{{t+|de|schreiben}}
*
Spanish: {{t+|es|anotar}}
{{trans-top|to record the details
of}}
*
{{ttbc|de}}: {{t+|de|bestrafen}}
{{trans-top|sports: to issue with
a caution}}
{{trans-top|slang: to travel
very fast}}
*
German: {{t+|de|rasen}}
*
{{ttbc|es}}: {{t|es|multar}}
The Wiktionary dump
1
is an XML file contain-
ing the word in the <title> tag and its descrip-
tion under the <text> tag. The translation of the
word is indicated by {{t| or {{t+| followed by
two letters to denote the target language (es for
Spanish and de for German). This is followed by
the translation and gender information in the case
of nouns.
The information in Wiktionary was converted
into a multidimensional hash table consisting of
English words as key and PoS and translations in
1
For this task the 17 Dec 2013 version was used.
Spanish and German as the values. This table was
used to look up the translations for the task.
Wiktionary also contains lists of the 10000 most
frequent words in Spanish and of the 2000 most
frequent words in German. This information was
used to sort the target language words in the hash
table in decreasing order of frequency. The trans-
lations absent from these frequency lists were kept
in the hash table in the order that they were ex-
tracted from Wiktionary.
3 Translation
TreeTagger PoS Wiktionary PoS
DT Determiner, Article
NC, NN, NNS Noun
IN, TO Preposition
VB, VBG,VBZ, MD Verb
RB, RBR, RP, WRB Adverb
CD Numeral
CC Conjunction
PP, PRP, WP Pronoun
JJ, JJS Adjective
Table 1: PoS Mapping
The TreeTagger (Schmid, 1994) was used to
parse the English (L1) phrases to obtain the PoS of
each word along with the lemma. The PoS tags re-
turned by the TreeTagger were mapped to the PoS
used in Wiktionary as shown in Table 1. The word
and its PoS were searched for in the hash table. If
the translation was not found, then the lemma and
its PoS were looked up. If the lemma lookup also
failed then the phrase was not translated.
Once the L2 words were obtained for all the L1
words in the phrase, the L2 words were matched
based on the gender and number information pro-
vided. For example, for the phrase this ques-
tion, Wiktionary offered este|m and esta|f as
Spanish translations of this, and interrogante|m
pregunta|f duda|f cuesti
?
on|f inc
?
ognita|f for
question. The translations were paired based on
gender agreement rules (e.g. este interrogante,
where both are masculine, and esta pregunta,
where both are feminine) and provided as solu-
tions.
3.1 Rules for English-to-Spanish Translation
Wiktionary only provides translations for the cita-
tion form of a word (even though other forms ex-
ist in WIktionary as valid entries), which is prob-
625
Language Pair Dataset Approach Evaluation Accuracy Word Accuracy Recall
en-es
Trial
Word-by-Word
Best 0.278 0.372 0.876
Oof 0.382 0.471 0.876
Word-by-Word Best 0.340 0.434 0.844
+ Rules Oof 0.444 0.535 0.844
Test
Word-by-Word
Best 0.200 0.308 0.785
Oof 0.246 0.356 0.785
Word-by-Word Best 0.223 0.333 0.751
+ Rules Oof 0.277 0.386 0.751
en-de
Trial Word-by-Word
Best 0.210 0.306 0.900
Oof 0.316 0.422 0.900
Test Word-by-Word
Best 0.218 0.293 0.851
Oof 0.307 0.385 0.851
Table 2: Performance of the System.
lematic when translating plural nouns or conju-
gated (finite) verbs. Lack of this inflectional infor-
mation degraded the overall performance of both
English-to-Spanish and English-to-German trans-
lations. Two rules were included in an attempt to
improve the English-to-Spanish translations: (1)
plural nouns and adjectives were formed by adding
-s or -es, and (2) where a noun was preceded by
an adjective in a L1 phrase, after the translation,
the positions of the noun and the adjective were
switched to respect the noun-adjective word order
that is more commonly found in Spanish.
4 Results and Conclusions
Table 2 shows the performance of the system
for the English-to-Spanish and English-to-German
translations. The approach in bold was submit-
ted for evaluation. The accuracy refers to the per-
centage of the fragments that were predicted accu-
rately, whereas word accuracy measures the par-
tially correct solutions. For each fragment, up to
5 translations could be submitted with one consid-
ered as the best answer and the rest regarded as
alternatives. The best evaluation considered only
the best answers. On the other hand, oof (out-of-
five) evaluation considered the alternative answers
to calculate the scores if the best answer was in-
correct.
A context-independent, word-for-word transla-
tion approach to L2 Writing Assistant was pro-
posed. The mediocre performance was due to
the fact the approach was very basic. The sys-
tem can be significantly improved by using the
Spanish and German versions of Wiktionary to
make up for the translations missing from the
English version and by considering the L2 con-
text provided. One such example in the German
Wiktionary is the {{Charakteristische
Wortkombinationen}} tag, which refers to
the possible collocations. For example, one of
the translations of the English word exchange in
German is Austausch, which is most often col-
located with im or als. Also, using a tool like
JWKTL
2
would improve the quality of informa-
tion extracted from Wiktionary.
References
Els Lefever and V?eronique Hoste. 2013. SemEval-2013
Task 10: Cross-lingual Word Sense Disambiguation.
In Proceedings of the Seventh International Work-
shop on Semantic Evaluation (SemEval 2013). At-
lanta, Georgia, USA.
Christian M. Meyer and Iryna Gurevych. 2012. Wik-
tionary: A New Rival for Expert-Built Lexicons?
Exploring the Possibilities of Collaborative Lex-
icography. In Electronic Lexicography, edited by
Sylviane Granger and Magali Paquot, 259?91. Ox-
ford: Oxford University Press.
Rada Mihalcea, Ravi Sinha, and Diana McCarthy.
2010. Semeval 2010 Task 2: Cross-lingual Lex-
ical Substitution. In Proceedings of the 5th In-
ternational Workshop on Semantic Evaluations
(SemEval-2010). Uppsala, Sweden.
Fabrizio Orlandi and Alexandre Passant. 2010. Se-
mantic Search on Heterogeneous Wiki Systems. In
Proceedings of the 6th International Symposium on
Wikis and Open Collaboration, 4:1?4:10. WikiSym
?10. New York, NY, USA: ACM.
Helmut Schmid. 1994. Probabilistic Part-of-Speech
Tagging Using Decision Trees. In Proceedings of
2
https://code.google.com/p/jwktl/
626
International Conference on New Methods in Lan-
guage Processing. Manchester, UK.
Torsten Zesch, Christof M?uller, and Iryna Gurevych.
20008. Extracting Lexical Semantic Knowledge
from Wikipedia and Wiktionary. In Proceedings of
the 6th International Conference on Language Re-
sources and Evaluation. Marrakech, Morocco.
627
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 633?635,
Dublin, Ireland, August 23-24, 2014.
TeamZ: Measuring Semantic Textual Similarity for Spanish Using an
Overlap-Based Approach
Anubhav Gupta
UFR SLHS
Universit?e de Franche-Comt?e
anubhav.gupta@edu.univ-fcomte.fr
Abstract
This paper presents an overlap-based ap-
proach using bag of words and the Spanish
WordNet to solve the STS-Spanish sub-
task (STS-Es) of SemEval-2014 Task 10.
Since bag of words is the most commonly
used method to ascertain similarity, the
performance is modest.
1 Introduction
The objective of STS-Es is to score a pair of sen-
tences in Spanish on the scale of 0 (the two sen-
tences are on different topics) to 4 (the two sen-
tences are completely equivalent, as they mean the
same thing) (Agirre et al., 2014). The textual sim-
ilarity finds its utility in various NLP applications
such as information retrieval, text categorisation,
word sense disambiguation, text summarisation,
topic detection, etc. (Besanc?on et al., 1999; Mi-
halcea et al., 2006; Islam and Inkpen, 2008).
The method presented in this paper calculates
the similarity based on the number of words that
are common in two given sentences. This ap-
proach, being simplistic, suffers from various
drawbacks. Firstly, the semantically similar sen-
tences need not have many words in common (Li
et al., 2006). Secondly, even if the sentences have
many words in common, the context in which they
are used can be different (Sahami and Heilman,
2006). For example, based on the bag of words ap-
proach, the sentences in Table 1 would be scored
the same:
However, only sentences [2] and [3] mean the
same.
Despite the flaws, this approach was used be-
cause of the Basic Principle of Compositional-
ity (Zimmermann, 2011), which states that the
This work is licensed under a Creative Commons At-
tribution 4.0 International Licence. Page numbers and pro-
ceedings footer are added by the organisers. Licence details:
http://creativecommons.org/licenses/by/4.0/
No. Spanish English
1
?
El es listo. He is clever.
2
?
El est?a listo. He is ready.
3
?
El est?a preparado. He is prepared.
Table 1: Examples.
meaning of a complex expression depends upon
the meaning of its components and the man-
ner in which they are composed. Furthermore,
mainly nouns were considered in the bag of words
because Spanish is an exocentric language, and
nouns contain more specific, concrete semantic
information than verbs (Michael Herslund, 2010;
Michael Herslund, 2012).
2 Methodology
The training dataset provided for the task con-
sisted of 65 pairs of sentences along with their cor-
responding similarity scores. There were two test
sets: one consisted of 480 sentence pairs from a
news corpus, and the other had 324 sentence pairs
taken from Wikipedia.
The approach consisted of learning the scoring
with the help of linear regression. Two runs were
submitted as solutions. The first run used three-
feature vectors, whereas the second one used four-
feature vectors. The features are the Jaccard in-
dices for the lemmas, noun lemmas, synsets, and
noun subjects in each sentence pair. For both runs,
the sentence pairs were parsed using the TreeTag-
ger (Schmid, 1994). The TreeTagger was used be-
cause it provides the part-of-speech tag and lemma
for each word of a sentence.
Run 1 used these features:
? The fraction of lemmas that were common
between the two sentences. In other words,
the number of unique lemmas common be-
tween the sentences divided by the total num-
ber of unique lemmas of the two sentences.
633
? The fraction of noun lemmas common be-
tween the two sentences.
? The fraction of synsets common between the
two sentences. For each noun, its correspond-
ing synset
1
was extracted from the Span-
ish WordNet (spaWN) of the Multilingual
Central Repository
2
(MCR 3.0) (Gonzalez-
Agirre et al., 2012).
Run 2 employed one more feature in addition
to the aforementioned, which was the fraction of
synsets of noun subjects that were common for
each sentence pair. The subject nouns were ex-
tracted from the sentences after parsing them with
the MaltParser (Nivre et al., 2007). Since the Tree-
Tagger PoS tagset
3
differed from the EAGLES
(Expert Advisory Group on Language Engineer-
ing Standards) tagset
4
required by the MaltParser,
rules were written to best translate the TreeTag-
ger tags into EAGLES tags. However, one-to-
one mapping was not possible: EAGLES tags are
seven characters long and encode number and gen-
der, whereas TreeTagger tags do not. For example,
using the EAGLES tagset, the masculine singular
common noun ?arbol ?tree? is tagged as NCMS000,
whereas the feminine singular common noun hoja
?leaf? is tagged as NCFS000; TreeTagger, on the
other hand, tags both as NC.
3 Results and Conclusions
Table 2 presents the performance, measured us-
ing the Pearson correlation, of the approach. Run
1 achieved a weighted correlation of 0.66723 and
ranked 15th among 22 submissions to the task.
Dataset Run 1 Run 2
Training 0.83693 0.83773
Wikipedia (Test) 0.61020 0.60425
News (Test) 0.71654 0.70974
Table 2: Performance of the Approach.
Given that the approach relied mostly on bag
of words, a modest performance was expected.
The performance was also affected by the fact
that the spaWN did not have synsets for most of
1
stored as synset offset in wei spa-30 variant.tsv
2
The resource can be obtained from
http://grial.uab.es/descarregues.php
3
http://www.cis.uni-muenchen.de/?schmid/tools/
TreeTagger/data/spanish-tagset.txt
4
http://nlp.lsi.upc.edu/freeling/doc/tagsets/tagset-es.html
the words. Finally, converting TreeTagger tags to
those required by the MaltParser instead of using
a parser which annotates with EAGLES tags may
also have contributed to the relatively low Run 2
score. However, the confidence intervals of the
two runs obtained after bootstrapping overlapped.
Thus, the difference between the two runs for both
the datasets is not statistically significant.
Acknowledgements
I would like to thank Vlad Niculae,
`
Angels Catena
and Calvin Cheng for their inputs and feedback.
References
Eneko Agirre, Carmen Banea, Claire Cardie, Daniel
Cer, Mona Diab, Aitor Gonzalez-Agirre, Weiwei
Guo, Rada Mihalcea, German Rigau, and Janyce
Wiebe. 2014. ?SemEval-2014 Task 10: Multilingual
Semantic Textual Similarity.? In Proceedings of the
8th International Workshop on Semantic Evaluation
(SemEval-2014). Dublin, Ireland.
Romaric Besanc?on, Martin Rajman, and Jean-C?edric
Chappelier. 1999. Textual Similarities Based on a
Distributional Approach. In Proceedings of the 10th
International Workshop on Database & Expert Sys-
tems Applications. 180?184. DEXA ?99. Washing-
ton, DC, USA: IEEE Computer Society.
Aitor Gonzalez-Agirre, Egoitz Laparra, and German
Rigau. 2012. Multilingual Central Repository ver-
sion 3.0: upgrading a very large lexical knowledge
base. In Proceedings of the Sixth International
Global WordNet Conference (GWC ?12).
Aminul Islam and Diana Inkpen. 2008. Semantic Text
Similarity Using Corpus-Based Word Similarity and
String Similarity. ACM Transactions on Knowledge
Discovery from Data 2 (2): 1?25.
Michael Herslund. 2010. Predicati e sostantivi comp-
lessi. In Language, Cognition and Identity, eds. Irn
Korzen and Emanuela Cresti. 1?9. Strumenti per La
Didattica E La Ricerca. Firenze University Press.
Michael Herslund. 2012. Structures lexicales et ty-
pologie. In S?emantique et lexicologie des langues
d?Europe, eds. Louis Begioni and Christine Brac-
quenier. 35?52. Rivages Linguistiques. Presses Uni-
versitaires de Rennes.
Rada Mihalcea, Courtney Corley, and Carlo Strap-
parava. 2006. Corpus-Based and Knowledge-Based
Measures of Text Semantic Similarity. In Proceed-
ings of the 21st National Conference on Artifi-
cial Intelligence. 775?80. AAAI?06. Boston, Mas-
sachusetts: AAAI Press.
Yuhua Li, David McLean, Zuhair A. Bandar, James D.
O?Shea, and Keeley Crockett. 2006. Sentence Simi-
larity Based on Semantic Nets and Corpus Statistics.
634
IEEE Transactions on Knowledge and Data Engi-
neering, 18 (8): 1138?50.
Joakim Nivre, Johan Hall, Jens Nilsson, Atanas
Chanev, G?uls?en Eryi?git, Sandra K?ubler, Svetovlas
Marinov, and Erwin Marsi. 2007. MaltParser: A
language-independent system for data-driven depen-
dency parsing. Natural Language Engineering, 13
(2): 95?135.
Mehran Sahami and Timothy D. Heilman. 2006. A
Web-Based Kernel Function for Measuring the Sim-
ilarity of Short Text Snippets. In Proceedings of the
15th International Conference on World Wide Web,
377?86. WWW ?06. New York, NY, USA: ACM.
Helmut Schmid. 1994. Probabilistic Part-of-Speech
Tagging Using Decision Trees. Proceedings of Inter-
national Conference on New Methods in Language
Processing. Manchester, UK.
Thomas Ede Zimmermann. 2011. Model-theoretic
semantics. Semantics. An International Handbook
of Natural Language Meaning. edited by Claudia
Maienborn, Klaus von Heusinger, and Paul Portner.
Vol. 1. Berlin, Boston: De Gruyter Mouton.
635
Proceedings of the Seventeenth Conference on Computational Natural Language Learning: Shared Task, pages 96?101,
Sofia, Bulgaria, August 8-9 2013. c?2013 Association for Computational Linguistics
Rule-based System for Automatic Grammar Correction
Using Syntactic N-grams for English Language Learning (L2)
Grigori Sidorov?, Anubhav Gupta?, Martin Tozer?, Dolors Catala?, Angels Catena? and Sandrine Fuentes?
?Centro de Investigacio?n en Computacio?n, Instituto Polite?cnico Nacional (IPN), Mexico
? Departament de Filologia Francesa i Roma`nica, Universitat Auto`noma de Barcelona, Spain
www.cic.ipn.mx\?sidorov,
{anubhav.gupta, tozer.martin}@e-campus.uab.cat,
{dolors.catala, angels.catena, sandrine.fuentes}@uab.cat
Abstract
We describe the system developed for the
CoNLL-2013 shared task?automatic En-
glish L2 grammar error correction. The
system is based on the rule-based ap-
proach. It uses very few additional re-
sources: a morphological analyzer and a
list of 250 common uncountable nouns,
along with the training data provided by
the organizers. The system uses the syn-
tactic information available in the train-
ing data: this information is represented
as syntactic n-grams, i.e. n-grams ex-
tracted by following the paths in depen-
dency trees. The system is simple and
was developed in a short period of time
(1 month). Since it does not employ
any additional resources or any sophisti-
cated machine learning methods, it does
not achieve high scores (specifically, it has
low recall) but could be considered as a
baseline system for the task. On the other
hand, it shows what can be obtained using
a simple rule-based approach and presents
a few situations where the rule-based ap-
proach can perform better than ML ap-
proach.
1 Introduction
There are two main approaches in the design of the
modern linguistic experiments and the develop-
ment of the natural language processing applica-
tions: rule-based and machine learning-based. In
practical applications of machine learning (ML),
the best results are achieved by the methods that
use supervised learning, i.e., that are based on
manually prepared training data for learning. It
is also worth mentioning what can be considered
a general rule for the combination of these two
approaches: a system based on the mixed ap-
proach should obtain better results if each part
of the system is applied according to its ?com-
petence?. Specifically, some problems are better
solved by the application of the rules?like the
rules for choosing the correct allomorph of the ar-
ticle ?a? vs. ?an?, while other problems are better
solved by the usage of ML methods?such as de-
ciding the presence or absence of a definite or an
indefinite determiner.
This paper describes the system developed for
the CoNLL-2013 shared task. The task consists
of grammar correction in texts written by people
learning English as a second language (L2). There
are five types of errors considered in the task: noun
number, subject-verb agreement, verb form, ar-
ticle/determiner and choice of preposition. The
training data processed by the Stanford parser (de
Marneffe et al, 2006) is provided. This data is part
of the NUCLE corpus (Dahlmeier et al, 2013).
The data also contains the error types and the cor-
rected version.
Development of the systemwas started only two
months before the deadline, so it is also an inter-
esting example of what can be done in a rather
short period of time and with relatively little ef-
fort: only one person-month joint effort in total.
In our system, we considered mainly the rule-
based approach. Note that we used the ConLL
data to extract preposition patterns, which can be
considered as a very reduced form of machine
learning with yes/no classifier, as well as to con-
struct rules directly from the data.
Another feature of our system is the widespread
use of the syntactic information present in the pro-
vided data. In our previous works, we general-
ized the use of syntactic information in NLP by
introducing the concept of syntactic n-grams, i.e.
n-grams constructed by following the dependency
paths in a syntactic tree (Sidorov et al, 2012;
Sidorov et al, 2013). Note that they are not n-
grams of POS tags, as could be assumed from the
name; the name refers to the manner in which they
96
Figure 1: Example of syntactic tree (for extraction
of syntactic n-grams).
are constructed. That is to say, in a dependency
relation, there is always a head word and a depen-
dent word. In the syntactic tree, this relation is
graphically represented by an arrow: head? de-
pendent. As it can be observed in Fig. 1, we can
also use the tree hierarchy?the head word is al-
ways ?higher? in the syntactic tree.
The algorithm for the construction of syntactic
n-grams is as follows: we start from the root word
and move to each dependent word following the
dependency relations. At each step, the sequence
of previous elements in the route taken are taken
into account. The last n words in the sequence
correspond to the syntactic n-gram. This could be
reformulated as: we should take the last n words
of the (unique) path from the root to the current
word.
In other words, we start from the root and reach
one of the dependent words. If we want to con-
struct bigrams, then we have a bigram already. If
we need other elements of the n-gram, then we
move to the word that is dependent and continue to
the words that are dependent on it. If a word has
several dependent words, we consider them one
after another and thus, obtain several syntactic n-
grams. Note that the head word always appears
before the dependent word in the syntactic n-gram
during the construction process.
For example, from the tree presented in Fig. 1,
the following syntactic bigrams can be extracted:
likes-also, likes-dog, dog-my, likes-eating, eating-
sausage. Note that only two syntactic 3-grams
can be constructed: likes-dog-my, likes-eating-
sausage. The construction process is the follow-
ing: we start with the root word like. It has several
dependent words: dog, also, eating. Considering
them one after another, we obtain three syntactic
bigrams. Then we move on to the word dog. It
has only one dependent word: my. This is another
bigram dog-my. However, the path from like also
goes through it, so this is also the 3-gram like-dog-
my, etc.
The reader can compare these syntactic n-grams
with traditional n-grams and consider their advan-
tages: there are a lot less syntactic n-grams, they
are less arbitrary, they have linguistic interpreta-
tion, etc.
Note that syntactic n-grams can be formed by
words (lemmas, stems), POS tags, names of de-
pendency relations, or they can be mixed, i.e., a
combination of the mentioned types. Being n-
grams, they can be applied in any machine learn-
ing task where traditional n-grams are applied.
However, unlike traditional n-grams, they have a
clear linguistic interpretation and can be consid-
ered as an introduction of linguistic (syntactic) in-
formation into machine learning methods. Previ-
ously, we obtained better results by applying the
syntactic n-grams to opinion mining and author-
ship attribution tasks compared to the traditional
n-grams. Further in this paper, it is described how
we use syntactic n-grams for the formulation of
rules in our system and for the extraction of pat-
terns.
The system described in this paper does not ob-
tain high scores. In our opinion, it could be con-
sidered a baseline system for the grammar correc-
tion task due to its simplicity, its use of very few
additional resources and the speed of its develop-
ment. Concretely, if a more sophisticated system
outperforms ours, it reflects well upon that system.
If it performs more poorly, its design should be
revised. On the other hand, this paper also dis-
cusses the few situations where the rule-based sys-
tem can outperform an ML approach. As we men-
tioned earlier, the ideal system would combine
both these approaches. To quote Tapanainen and
Voutilainen (1994), ?don?t guess if you know?.
Further below, we describe the lexical resources
that we used, the processing of each type of error
and the evaluation of the system.
2 The System?s Linguistic Resources
The system consists of several program modules
written in the Python programming language. We
used only three types of linguistic resources:
? The provided corpus NUCLE data was pro-
cessed with the Stanford parser. It was
used for the extraction of patterns to identify
97
preposition errors and for the formulation of
rules.
? A list of the 250 most common uncountable
nouns1. This list was used for processing the
possibility of using the nouns in plural form.
? A morphological analysis system for English
that in our case was based on the FreeL-
ing morphological dictionary (Padro? et al,
2010).
The FreeLing dictionary is a freely available
text file which contains more than 71,000 word
forms with standard POS tags. It has the follow-
ing data: for each word form, it contains a list of
lemmas and POS tags. An example of the entries:
...abandon abandon VB abandon VBP
abandoned abandon VBD abandon VBN
abandoning abandon VBG
abandonment abandonment NN
abandons abandon VBZ...
This list can also be easily reordered by lemmas.
It is therefore very easy to apply this word list to
both morphological analysis and generation. The
morphological analysis simply consists of search-
ing for a word form in the list, while the mor-
phological generation involves searching the list
of lemmas and then finding the word form with
the necessary POS tag, i.e., for the generation, the
input consists in the lemma and the POS tag. For
example, if we want to generate the VBZ form of
the verb take, then we search in the list ordered ac-
cording to the lemma take; there are several forms:
take took VBP, take taken VBN, take takes VBZ and
choose the form that has the POS tag VBZ.
3 Error Processing
In accordance with the rules of the ConLL shared
task, only five types of errors were considered:
noun number, incorrect preposition, choice of de-
terminer or article, subject-verb agreement and
verb form. More error types are marked in the
corpus, but they are much more complex, being
related to the meaning and content.
Let us see examples of the errors:
? Preposition error: ?...the need of habitable
environment...?, where ?for? should be used.
1List of 250 most common uncountable nouns.
www.englishclub.com>Learn English>Vocabulary>Nouns.
? Nn error: ?...people are getting more con-
scious of the damages...?, the word ?damage?
in singular should be used.
? SVA error: ?...relevant information are read-
ily available...?, where ?is? should be used in-
stead.
? Vform error: ?The solution can be obtain
by using technology...?, where ?obtained?
should appear.
? ArtOrDet error: ?...It is also important to cre-
ate a better material...?, where ?a? should not
be used.
The total number of errors marked in the train-
ing and the test data for ConLL 2013 are presented
by type in Table 1.
Table 1: Numbers of errors in training and test data
listed by type.
Error type Training Test
Vform (Verb form) 1,451 122
SVA (Subject-verb agreement) 1,529 124
ArtOrDet 6,654 690
Nn (Noun number) 3,773 396
Prep (Preposition) 2,402 311
Note that the errors related to the noun num-
ber should be processed first since later, an agree-
ment error could be produced if the noun number
is changed. If the agreement error is introduced by
the modification of the noun number, it is not the
error committed by the student, however it is con-
sidered as such in the current version of the task.
Probably, it can be considered as some sort of sec-
ondary error. The order in which other errors are
processed is irrelevant.
3.1 Noun Number Error Processing
The only rule we implemented in this case was that
uncountable nouns do not have a plural. We used
a list of the 250 most common uncountable nouns
(as mentioned in the Section 2) to determine the
possibility of a plural form for a noun. For ex-
ample: ...ethics, evidence, evolution, failure, faith,
fame, fiction, flour, flu, food, freedom...
We made an exception for the noun ?time? and
do not consider it as uncountable, because its use
in the common expressions such as ?many times?
98
is much more frequent than its use as an uncount-
able noun as in ?theory of time? or ?what time is
it now??. More sophisticated systems should ana-
lyze the contexts obtained from vast data sets (cor-
pora), i.e. consider n-grams or syntactic n-grams.
Note that word sense disambiguation would be
helpful in the resolution of the mentioned ambigu-
ities. Also, the rule that considers the presence of
the dependent words like ?many, a lot of, amount
of? could be added.
3.2 Subject-Verb Agreement and Verb Form
Error Processing
We consider these two types of errors together be-
cause they are related to a similar and a rather sim-
ple grammatical phenomenon. To correct these
errors we used syntactic information to formulate
the rules. This is logical because we cannot rely
on the context words (neighbours) as they appear
in texts (traditional n-grams). Note that the rules
are also related to the modal verbs and the passive
constructions.
The rules for the agreement are very simple: 1)
if the noun is in plural and the VBZ tag is present,
then change the tag to VB, 2) if the noun is in
singular and the VB tag is present, then change
the tag to VBZ. The corresponding morphological
generation is also performed.
The rules for verb form correction are as fol-
lows: 1) if we have a modal verb, then the depend-
ing verb should have a VB tag, 2) if we have an
auxiliary verb ?have?, then the main verb should
have a VB tag (perfect tense), etc. Moreover, the
FreeLing morphological dictionary is utilized to
identify the correct verb form. Note that there are
some assumptions here about what drives the verb
form, e.g., that a noun or a modal verb are correct
and the verb needs to change. This appears to be
a reasonable assumption, but may not always be
correct.
3.3 Preposition Error Processing
It is well-known that prepositions depend on lex-
ical units that are their heads, see (Eeg-Olofsson
and Knutsson, 2003). But what should be done
if we want to consider the dependent word? Say,
that in the PP attachment task, the lexical unit is
the preferred solution as well. In general, it would
be an ideal solution in grammar correction, but in
the case of our system, very little training data was
used. If we consider that the dependent word is a
lexical unit, we will have less recall. We are there-
fore practically obliged to consider that it is a POS
tag.
To process the prepositions, we used the train-
ing data provided by the organizers. Specifically,
we extracted preposition patterns. We apply the
concept of syntactic n-grams to include both the
head word of the preposition and the dependent
word into the pattern. The pattern data corre-
sponds to syntactic n-grams because they are con-
structed using syntactic dependencies. As we
mentioned previously, syntactic n-grams can con-
sist of words, POS tags or a combination. In our
case, we used mixed syntactic n-grams: the head
word is the lexical unit, while the dependent word
is the POS tag, as shown in Table 2.
For example, the first line corresponds to the er-
roneous phrase ?...unwelcomed among public...?,
where ?among? should be substituted by ?by?.
Note that there can be other words between these
three words in the surface representation of the
sentence, but the parser allows the extraction of
the syntactic n-gram, which represents the ?pure?
pattern.
In order to choose the syntactic n-gram type, our
first consideration was that the head word should
be a lexical unit (word), because this determines
the choice of the preposition. We used a POS
tag for the dependent element, because we consid-
ered that using a word there would be too specific.
Thus, our final syntactic n-gram for the first line
was ?...unwelcomed among NN...?, which should
be changed to ?...unwelcomed by NN...?. The syn-
tactic n-gram for the second line was ?...trouble for
NN...?, which should be changed to ?...trouble in
NN...?, etc. Note that insertion of prepositions is
not considered, but deletion can be performed, i.e.,
changing the preposition to nothing.
The rule for the system is formulated in the fol-
lowing way: if we find a relation ?preposition? in
the dependency tree, then for the preposition that
corresponds to this relation, we search the list of
the extracted patterns. If we find the pattern, then
we change the preposition. It is quite clear that
the training data is too limited to obtain patterns
for a great majority of words. Our list contained
only 1,896 elements. These patterns should be ex-
tracted from a very large corpus or a dictionary.
3.4 Article or Determiner Error Processing
In this case, we found only two clear rules, both
related to the article ?a?: 1) choice of the allo-
99
Table 2: Examples of patterns for prepositions.
Preposition Preposition Head word Head word Dependent word Dependent word
(error) (correction) (lemma) (POS) (lemma) (POS)
among by unwelcomed VBN public NN
for in trouble NN development NN
on in practice NN October NNP
on in face VBG field NN
morph ?a/an?, and 2) the fact that the article ?a?
cannot be used with nouns in plural. Other rules
would be too complex for a manually created rule-
based system. The first rule takes into the account
the immediate neighbor: the choice depends on its
phonetic properties. The second rule considers the
syntactically related head word, which cannot be
in plural if we use the indefinite article.
4 Evaluation of the System
For the evaluation, the organizers provided data
similar to the training data from the same NU-
CLE corpus, which also contained syntactic in-
formation. The evaluation results were provided
by the organizers using their evaluation script in
Python (Dahlmeier and Ng, 2012). The results ob-
tained with this script for our system are: precision
17.4 %, recall 1.8%, and F1 measure 3.3% (the
preliminary scores were: 12.4%, 1.2% and 2.2%
correspondingly). See the final remarks in this
section, where we argue that the real values should
be: precision 25%, recall 2.6%, and F1 measure
4.7%.
The results are low, but as we mentioned previ-
ously, our system uses a rule-based approach with
very few additional resources, so it cannot com-
pete with ML based approaches that additionally
rely on vast lexical resources and the Internet. Due
to its simplicity, low use of additional resources,
and very short development time, we consider our
system a possible baseline system for the task. On
the other hand, we showed that in some cases the
rules should be used as a complementary tech-
nique for ML learning methods: don?t guess if you
know.
The low recall of the system is to be expected
as we process only clearly defined errors, ignoring
more complex cases.
It is always interesting to perform an analysis of
the errors committed by a system. Let us analyze
the supposed errors committed by our system
for the noun number error type. It performed 18
corrections, 3 of which coincide with the marks
in the corpus data. Two of them are clear errors
of the system: ?traffic jam?, where the word
?jam? is used in a sense other than that of the
?substance?, and ?many respects?, where again
the word ?respect? has a different meaning to that
of the uncountable noun. There are 13 cases listed
below, that our system marked as errors, because
they are uncountable nouns in plural, but they
are not marked in the corpus. Let us consider the
nouns in capital letters:
...peaceful(JJ) LIVINGS(NNS)
2
...,
...life(NN) QUALITIES(NNS)...,
...Many(JJ) science(NN) FICTIONS(NNS)...,
...does(VBZ) not(RB) have(VB) enough(JJ)
LANDS(NNS)...,
...indicates(VBZ) that(IN) the(DT) FOODS(NNS)
the(DT) people(NNS) eat(VBP)...,
...problem(NN) of(IN) public(JJ) TRANSPORTA-
TIONS(NNS)...,
...healthcare(NN) consume(VBP) large(JJ)
QUANTITIES(NNS) of(IN) energy...,
...this(DT) society(NN) may(MD) lack(VB) of(IN)
LABOURS(NNS)...
Note that the words ?equipment? and ?usage?
in plural were marked as errors in the corpus. In
our opinion, it is inconsistent to mark these two as
errors, and not to mark the words from this list as
such. While it is true that their use in plural is pos-
sible, it is clearly forced and is much less probable.
At least, students of English should learn to use
these words in singular only. Some of these mis-
takes (but not all) were corrected by the organizers
for the final scoring data. If we consider all these
cases as correctly marked errors, then the preci-
sion of our system is around 25%, recall 2.6%, and
F1 measure 4.7%.
2?LIVINGS? is encountered 5 times and ?QUANTITIES?
is encountered 2 times
100
5 Conclusions
In this paper we have described the system pre-
sented for the CoNLL-2013 shared task for gram-
mar correction in English (L2). The system uses
a rule-based approach and relies on very few addi-
tional resources: a list of 250 uncountable nouns, a
morphological analyzer and the training data from
the NUCLE corpus provided by the organizers.
The system uses syntactic n-grams for rule formu-
lation, i.e., n-grams that are constructed by follow-
ing the dependency paths in a parsed tree.
We analyzed various situations in which a rule
based technique can give better results than ML
techniques: don?t guess if you know. These cases
are: 1) two rules for the article ?a?, and 2) the
rules for uncountable nouns (in this case, word
sense disambiguation would help to determine if
the sense in the text is an uncountable noun or
has some other use), and 3) the subject-verb agree-
ment rule. In the case of prepositions, ML learn-
ing is definitely better. Otherwise, vast resources
would need to be used, which in any case, would
resemble machine learning. We are not sure about
verb form errors: the rules which we formulated
are rather simple, but the performance of various
ML methods should be analysed in order to decide
which technique is better.
The system is simple and was developed in a
very short time. It does not obtain high scores and
could be considered as a baseline system for the
task.
Acknowledgements
This work was done under partial support of the
Mexican Government (CONACYT, SNI, COFAA-
IPN, SIP-IPN 20120418, 20121823), CONACYT-
DST India (?Answer Validation through Textual
Entailment?), Mexico City Government (ICYT
PICCO10-120), and FP7-PEOPLE-2010- IRSES:
Web Information Quality - Evaluation Initiative
(WIQ-EI) European Commission project 269180.
References
Daniel Dahlmeier and Hwee Tou Ng. 2012. Better
evaluation for grammatical error correction. In Pro-
ceedings of the 2012 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics (NAACL 2012), pages 568?572.
Daniel Dahlmeier, Hwee Tou Ng, and Siew Mei Wu.
2013. Building a large annotated corpus of learner
English: The NUS corpus of learner English.
M.C. de Marneffe, B. MacCartney, and C.D. Manning.
2006. Generating typed dependency parses from
phrase structure parses. In LREC 2006.
Jens Eeg-Olofsson and Ola Knutsson. 2003. Auto-
matic grammar checking for second language learn-
ers ? the use of prepositions. In Proceedings of
Nodalida?03.
Llus Padro?, Miquel Collado, Samuel Reese, Marina
Lloberes, and Irene Castello?n. 2010. Freeling
2.1: Five years of open-source language processing
tools. In Proceedings of 7th Language Resources
and Evaluation Conference (LREC 2010), ELRA.
G. Sidorov, F. Velasquez, E. Stamatatos, A. Gel-
bukh, and L. Chanona-Hernandez. 2012. Syntac-
tic dependency-based n-grams as classification fea-
tures. LNAI 7630, pages 1?11.
G. Sidorov, F. Velasquez, E. Stamatatos, A. Gel-
bukh, and L. Chanona-Hernandez. 2013. Syntactic
dependency-based n-grams: More evidence of use-
fulness in classification. LNCS 7816 (Proc. of CI-
CLing), pages 13?24.
Pasi Tapanainen and Atro Voutilainen. 1994. Tagging
accurately - don?t guess if you know. In Proceedings
of ANLP ?94.
101
Proceedings of the Eighteenth Conference on Computational Natural Language Learning: Shared Task, pages 49?52,
Baltimore, Maryland, 26-27 July 2014.
c
?2014 Association for Computational Linguistics
Grammatical Error Detection and Correction Using Tagger Disagreement
Anubhav Gupta
Universit?e de Franche-Comt?e
anubhav.gupta@edu.univ-fcompte.fr
Abstract
This paper presents a rule-based approach
for correcting grammatical errors made
by non-native speakers of English. The
approach relies on the differences in the
outputs of two POS taggers. This paper
is submitted in response to CoNLL-2014
Shared Task.
1 Introduction
A part-of-speech (POS) tagger, like any other soft-
ware, has a set of inputs and outputs. The input
for a POS tagger is a group of words and a tagset,
and the output is a POS tag for a word (Jurafsky
and Martin, 2009). Given that a software is bound
to provide incorrect output for an incorrect input
(garbage in, garbage out), it is quite likely that tag-
gers trained to tag grammatically correct sentences
(the expected input) would not tag grammatically
incorrect sentences properly. Furthermore, it is
possible that the output of two different taggers for
a given incorrect input would be different.
For this shared task, the POS taggers used were
the Stanford Parser, which was used to preprocess
the training and test data (Ng et al., 2014) and the
TreeTagger (Schmid, 1994). The Stanford Parser
employs unlexicalized PCFG
1
(Klein and Man-
ning, 2003), whereas the TreeTagger uses decision
trees. The TreeTagger is freely available
2
, and its
performance is comparable to that of the Stanford
Log-Linear Part-of-Speech Tagger (Toutanova et
al., 2003). Since the preprocessed dataset was al-
ready annotated with POS tags, the Stanford Log-
Linear POS Tagger was not used.
If the annotation of preprocessed data differed
from that of the TreeTagger, it was assumed that
the sentence might have grammatical errors. Once
an error was detected it was corrected using the
1
Probabilistic Context-Free Grammar
2
http://www.cis.uni-muenchen.de/?schmid/tools/TreeTagger/
Nodebox English Linguistics library
3
(De Bleser
et al., 2002).
2 Error Detection
The POS tag for each token in the data was com-
pared with the tag given by the TreeTagger. Sen-
tences were considered grammatically incorrect
upon meeting the following conditions:
? The number of tags in the preprocessed
dataset for a given sentence should be equal
to the number of tags returned by the Tree-
Tagger for the same sentence.
? There should be at least one token with dif-
ferent POS tags.
As an exception, if the taggers differed only on the
first token, such that the Stanford Parser tagged it
as NNP or NNPS, then the sentence was not con-
sidered for correction, as this difference can be
attributed to the capitalisation of the first token,
which the Stanford Parser interprets as a proper
noun.
Table 1 shows the precision (P) and the recall
(R) scores of this method for detecting erroneous
sentences in the training and test data. The low
recall score indicates that for most of the incorrect
sentences, the output of the taggers was identical.
2.1 Preprocessing
The output of the TreeTagger was modified so that
it had the same tag set as that used by the Stan-
ford Parser. The differences in the output tagset is
displayed in the Table 2.
2.2 Errors
Where the mismatch of tags is indicative of error,
it does not offer insight into the nature of the er-
ror and thus does not aid in error correction per se.
For example, the identification of a token as VBD
3
http://nodebox.net/code/index.php/Linguistics
49
Dataset Total Erroneous Sentences with Erroneous Sentences P R
Sentences Tag Mismatch Identified Correctly
Training 21860 26282 11769 44.77 53.83
Test 1176 642 391 60.90 33.24
Test (Alternative)
?
1195 642 398 61.99 33.30
?
consists of additional error annotations provided by the participating teams.
Table 1: Performance of Error Detection.
TreeTagger Stanford Parser
Tagset Tagset
( -LRB-
) -RRB-
NP NNP
NPS NNPS
PP PRP
SENT .
Table 2: Comparison of Tagsets.
(past tense) by one tagger and as VBN (past par-
ticiple) another does not imply that the mistake is
necessarily a verb tense (Vt) error. Table 4 lists
some of the errors detected by this approach.
3 Error Correction
Since mismatched tag pairs did not consistently
correspond to a particular error type, not all er-
rors detected were corrected. Certain errors were
detected using hand-crafted rules.
3.1 Subject-Verb Agreement (SVA) Errors
SVA errors were corrected with aid of dependency
relationships provided in the preprocessed data. If
a singular verb (VBZ) referred to a plural noun
(NNS) appearing before it, then the verb was made
plural. Similarly, if the singular verb (VBZ) was
the root of the dependency tree and was referred
to by a plural noun (NNS), then it was changed to
the plural.
3.2 Verb Form (Vform) Errors
If a modal verb (MD) preceded a singular verb
(VBZ), then the second verb was changed to the
bare infinitive form. Also, if the preposition
to preceded a singular verb, then the verb was
changed to its bare infinitive form.
3.3 Errors Detected by POS Tag Mismatch
If a token followed by a noun is tagged as an ad-
jective (JJ) in the preprocessed data and as an ad-
Dataset P R F
 =0.5
Training 23.89 0.31 1.49
Test 70.00 1.72 7.84
Test (Alternative) 72.00 1.90 8.60
Table 3: Performance of the Approach.
verb (RB) by the TreeTagger, then the adverbial
morpheme -ly was removed, resulting in the ad-
jective. For example, completely is changed to
complete in the second sentence of the fifth para-
graph of the essay 837 (Dahlmeier et al., 2013).
On the other hand, adverbs (RB) in the prepro-
cessed dataset that were labelled as adjectives (JJ)
by the TreeTagger were changed into their corre-
sponding adverbs.
A token preceded by the verb to be, tagged as
JJ by the Stanford Parser and identified by the
TreeTagger as a verb is assumed to be a verb
and accordingly converted into its past partici-
ple. Finally, the tokens labelled NNS and VBZ
by the Stanford Parser and the TreeTagger respec-
tively are likely to be Mec
4
or Wform
5
errors.
These tokens are replaced by plural nouns hav-
ing same initial substring (this is achieved using
the get close matches API of the difflib Python
library).
The performance of this approach, as measured
by the M2 scorer (Dahlmeier and Ng, 2012), is
presented in Table 3.
4 Conclusion
The approach used in this paper is useful in de-
tecting mainly verb form, word form and spelling
errors. These errors result in ambiguous or incor-
rect input to the POS tagger, thus forcing it to pro-
duce incorrect output. However, it is quite likely
that with a different pair of taggers, different rules
4
Punctuation, capitalisation, spelling, typographical er-
rors
5
Word form
50
nid 829
Sentence This caused problem like the appearance
Stanford Parser DT VBD NN IN DT NN
TreeTagger DT VBN NN IN DT NN
Error Type Vt
nid 829
Sentence but also to reforms the land
Stanford Parser CC RB TO VB DT NN
TreeTagger CC RB TO NNS DT NN
Error Type Wci
nid 840
Sentence India , their population amount to
Stanford Parser NNP , PRP$ NN VB TO
TreeTagger NNP , PRP$ NN NN TO
Error Type Vform (This was not an error in the training corpus.)
nid 1051
Sentence Singapore is currently a develop country
Stanford Parser NNP VBZ RB DT JJ NN
TreeTagger NNP VBZ RB DT VB NN
Error Type Vform
nid 858
Sentence Therefore most of China enterprisers focus
Stanford Parser RB JJS IN NNP VBZ NN
TreeTagger RB RBS IN NNP NNS VBP
Error Type Wform
nid 847
Sentence and social constrains faced by engineers
Stanford Parser CC JJ NNS VBN IN NNS
TreeTagger CC JJ VBZ VBN IN NNS
Error Type Mec
Table 4: Errors Detected.
would be required to correct these errors. Errors
concerning noun number, determiners and prepo-
sitions, which constitute a large portion of errors
committed by L2 learners (Chodorow et al., 2010;
De Felice and Pulman, 2009; Gamon et al., 2009),
were not addressed in this paper. This is the main
reason for low recall.
Acknowledgments
I would like to thank Calvin Cheng for proofread-
ing the paper and providing valuable feedback.
References
Martin Chodorow, Michael Gamon, and Joel Tetreault.
2010. The Utility of Article and Preposition Error
Correction Systems for English Language Learners:
Feedback and Assessment. Language Testing 27 (3):
419?436. doi:10.1177/0265532210364391.
Daniel Dahlmeier, Hwee Tou Ng, and Siew Mei
Wu. 2013. Building a Large Annotated Corpus of
Learner English: The NUS Corpus of Learner En-
glish. Proceedings of the Eighth Workshop on Inno-
vative Use of NLP for Building Educational Appli-
cations (BEA 2013). 22 ?31. Atlanta, Georgia, USA.
Daniel Dahlmeier and Hwee Tou Ng. 2012. Bet-
ter Evaluation for Grammatical Error Correction.
Proceedings of the 2012 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies
(NAACL 2012). 568 ? 572. Montreal, Canada.
Frederik De Bleser, Tom De Smedt, and Lucas Nijs
2002. NodeBox version 1.9.6 for Mac OS X.
Rachele De Felice and Stephen G Pulman. 2009. Au-
tomatic Detection of Preposition Errors in Learner
Writing. CALICO Journal 26 (3): 512?528.
Michael Gamon, Claudia Leacock, Chris Brockett,
William B. Dolan, Jianfeng Gao, Dmitriy Belenko,
51
and Alexandre Klementiev. 2009. Using Statistical
Techniques and Web Search to Correct ESL Errors.
CALICO Journal 26 (3): 491?511.
Daniel Jurafsky and James H. Martin. 2009. Part-of-
Speech Tagging. Speech and Language Processing:
An Introduction to Natural Language Processing,
Speech Recognition, and Computational Linguistics.
Prentice-Hall.
Dan Klein and Christopher D. Manning. 2003. Accu-
rate Unlexicalized Parsing. Proceedings of the 41st
Meeting of the Association for Computational Lin-
guistics. 423?430.
Hwee Tou Ng, Siew Mei Wu, Ted Briscoe, Christian
Hadiwinoto, Raymond Hendy Susanto, and Christo-
pher Bryant. 2014. The CoNLL-2014 Shared Task
on Grammatical Error Correction. Proceedings of
the Eighteenth Conference on Computational Natu-
ral Language Learning: Shared Task (CoNLL-2014
Shared Task). Baltimore, Maryland, USA.
Helmut Schmid. 1994. Probabilistic Part-of-Speech
Tagging Using Decision Trees. Proceedings of Inter-
national Conference on New Methods in Language
Processing. Manchester, UK.
Kristina Toutanova, Dan Klein, Christopher Man-
ning, and Yoram Singer. 2003. Feature-Rich Part-
of-Speech Tagging with a Cyclic Dependency Net-
work. Proceedings of HLT-NAACL 2003. 252?259.
52
