Assigning Time-Stamps to Event-Clauses
Elena Filatova
Information Sciences Institute
University of Southern
California
elena@isi.edu
Eduard Hovy
Information Sciences Institute
University of Southern
California
hovy@isi.edu
Abstract
We describe a procedure for arranging into a
time-line the contents of news stories
describing the development of some situation.
We describe the parts of the system that deal
with 1. breaking sentences into event-clauses
and 2. resolving both explicit and implicit
temporal references. Evaluations show a
performance of  52%, compared to humans.
1 Introduction
Linguists who have analyzed news stories
(Schokkenbroek,1999; Bell,1997; Ohtsuka and
Brewer,1992, etc.) noticed that ?narratives1 are
about more than one event and these events are
temporally ordered. Though it seems most
logical to recapitulate events in the order in
which they happened, i.e. in chronological order,
the events are often presented in a different
sequence?.  The  same  paper  states that ?it is
important to reconstruct the underlying event
order2 for narrative analysis to assign meaning to
the sequence in which the events are narrated at
the level of discourse structure?.If the
underlying event structure cannot be
reconstructed, it may well be impossible to
understand the narrative at all, let alne assign
meaning to its structure?.
Several psycholinguistic experiments show
the influence of event-arrangement in news
stories on the ease of comprehension by readers.
Duszak (1991) had readers reconstruct a news
story from the randomized sentences. According
to his experiments readers have a default strategy
by which?in the absence of cues to the
contrary?they re-impose chronological order on
events in the discourse.
                                                
1 Schokkenbroek (1999) uses the term narrative for news
stories that relate more than one event.
2 i.e., chronological order.
The problem of reconstructing the
chronological order of events becomes more
complicated if we have to deal with separate
news stories, written at different times and
describing the development of some situation, as
is the case for multidocument summarization.
By judicious definition, one can make this
problem easy or hard.  Selecting only specific
items to assign time-points to, and then
measuring correctness on them alone, may give
high performance but leave much of the text
unassigned.  We address the problem of
assigning a time-point to every clause in the text.
Our approach is to break the news stories into
their constituent events and to assign time-
stamps?either time-points or time-intervals?to
these events. When assigning time-stamps we
analyze both implicit time references (mainly
through the tense system) and explicit ones
(temporal adverbials) such as ?on Monday?, ?in
1998?, etc. The result of the work is a prototype
program which takes as input set of news stories
broken into separate sentences and produces as
output a text that combines all the events from all
the articles, organized in chronological order.
2 Data
As data we used a set of news stories about an
earthquake in Afghanistan that occurred at the
end of May in 1998. These news stories were
taken from CNN, ABC, and APW websites for
the DUC-2000 meeting. The stories were all
written within one week. Some of the texts were
written on the same day. In addition to a
description of the May earthquake, these texts
contain references to another earthquake that
occurred in the same region in February 1998.
3 Identifying Events
To divide sentences into event-clauses we use
CONTEX (Hermjakob, 1997), a parser that
produces a syntactic parse tree augmented with
semantic labels.  CONTEX uses machine
learning techniques to induce a grammar from a
given treebanks.  A sample output of CONTEX
is given in Appendix 1.
To divide a sentence into event-clauses the
parse tree output by CONTEX is analyzed from
left to right (root to leaf). The ::CAT field for
each node provides the necessary information
about whether the node under consideration
forms a part of its upper level event or whether it
introduces a new event.  ::CAT features that
indicate new events are: S-CLAUSE, S-SNT, S-
SUB-CLAUSE, S-PART-CLAUSE, S-REL-
CLAUSE.  These features mark clauses which
contain both subject (one or several NPs) and
predicate (VP containing one or several verbs).
The above procedure classifies a clause
containing more than one verb as a simple
clause.  Such clauses are treated as one event and
only one time-point will be assigned to them.
This is fine when the second verb is used in the
same tense as the first, but may be wrong in
some cases, as in He lives in this house now and
will stay here for one more year. There are no
such clauses in the analyzed data, so we ignore
this complication for the present.
The parse tree also gives information about
the tense of verbs, used later for time assignment.
In order to facilitate subsequent processing,
we wish to rephrase relative clauses as full
independent sentences.  We therefore have to
replace pronouns where it is possible by their
antecedents. Very often the parser gives
information about the referential antecedents (in
the example below, Russia). Therefore we
introduced the rule: if it is possible to identify
the referent, put it into the event-clause:
1.Russia <..> said;
2.which <Russia> has loaned helicopters in
previous disasters;
3.it <Russia> would consider sending aid.
But sometimes the antecedent is identified
incorrectly.
Qulle charged that the United Nations and
non-governmental organizations involved
in the relief were poorly coordinated,
which was costing lives.
Here the antecedent for which is identified as the
relief, and gives which <the relief> was costing
lives instead of which <poor coordination> was
costing lives. Fortunately, in most cases our rule
works correctly.
Although the event-identifier works
reasonably well, breaking text into event-clauses
needs further investigation. Table 1 shows the
performance of the system. Two kinds of
mistakes are made by the event identifier: those
caused by CONTEX (it does not identify clauses
with omitted predicate, etc.) and those caused by
the fact that our clause identifier does too
shallow analysis of the parse tree.
4 Time-stamper
According to (Bell, 1997) ?time is expressed at
different levels?in the morphology and syntax of
the verb phrase, in time adverbials whether lexical
or phrasal, and in the discourse structure of the
stories above the sentence?.
4.1 Representation of Time-points and -intervals
For the present work we use slightly modified
time representations suggested in (Allen, 1991).
Formats used for time representation:
? {YYYY:DDD:W}3 Used when it is possible to
point out the particular day the event occurred.
{YYYY1:DDD1:W1},{YYYY2:DDD2:W2}...
Used when it is possible to point out several
concrete days when the events occurred.
? {YYYY1:DDD1:W1}---{YYYY2:DDD2:W2}
Used when it is possible to point out a range of days
when the event occurred.
? <<<{YYYY:DDD:W} Used when it is possible to
say the event occurred {YYYY:DDD:W} or earlier.
? >>>{YYYY:DDD:W} Used when it is possible to
say the event occurred {YYYY:DDD:W} or later.
4.2 Time-points Used for the Time-stamper
We use two anchoring time points:
1. Time of the article
We require that the first sentence for each article
contains time information. For example:
T1 (05/30/1998:Saturday 18:35:42.49)
PAKINSTAN MAY BE PREPARING FOR
ANOTHER TEST.
The date information is in bold. We denote by Ti
the reference time-point for the article, where i
                                                
3 YYYY?year number, DDD?absolute number of the day
within the year (1?366), W-?umber of the day in a week
(1- Monday, ? 7- Saturday). If it is impossible to point out
the day of the week then W is assigned 0.
 Recall = (# of event-clauses correctly identified by system) / ( # of event-clauses identified manually)
 Precision = (# of event-clauses correctly identified by system) / ( # of event-clauses identified by system)
Text
number
# of clauses
by human
# of clauses
by system
#
correct
recall precision
Text 1 7 6 5 5/7 = 71.42% 5/6 = 83.33%
Text 2 27 31 15 15/27 = 55.55% 15/31 = 48.38%
Text 3 5 8 3 3/5 = 60% 3/8 = 37.5%
Text 4 28 28 18 18/28 = 64.28% 18/28 = 64.28%
Text 5 33 36 19 19/33 = 57.57% 19/36= 52.77%
Text 6 58 63 36 36/58=62.07% 36/63 = 57.14%
total 158 172 96 96/158 = 60.76% 96/172 = 55.81%
Table 1. Recall and precision scores for event identifier.
means that it is the time point of article i.  The
symbol Ti is used as a comparative time-point if the
time the article was written is unknown. The
information in brackets gives the exact date the
article was written, which is the main anchor point
for the time-stamper. The information about hours,
minutes and seconds is ignored for the present.
2. Last time point assigned in the same sentence
While analyzing different event-clauses within the
same sentence we keep track of what time-point was
most recently assigned within this sentence. If
needed, we can refer to this time-point. In case the
most recent time information assigned is not a date
but an interval we record information about both
time boundaries. When the program proceeds to the
next sentence, the variable for the most recently
assigned date becomes undefined. In most cases this
assumption works correctly (example 5.2?5.3):
5.2.1 In the village of Kol, hundreds of people
swarmed a United Nations helicopter
5.2.2 that <a United Nations helicopter>
touched down three days after Saturday?s
earthquake
5.2.3 <after Saturday?s earthquake> struck a
remote mountainous area rocked three months
earlier by another massive quake
5.2.4 that <another massive quake> claimed
some 2,300 victims.
5.3.1 On Monday and Tuesday, U.N. helicopters
evacuated 50 of the most seriously injured to
emergency medical centers.
The last time interval assigned for sentence 5.2 is
{1998:53:0}---{1998:71:0}, which gives an
approximate range of days when the previous
earthquake happened. But the information in
sentence 5.3 is about the recent earthquake and not
about the previous one of 3 months earlier, which is
why it would be a mistake to point Monday and
Tuesday within that range.
Mani and Wilson (2000) point out ?over half of
the errors [made by his time-stamper] were due to
propagation of spreading of an incorrect event time
to neighboring events?. The rule of dropping the
most recently assigned date as an anchor point when
proceeding to the next sentence very often helps us
to avoid this problem.
There are however cases where dropping the
most recent time as an anchor when proceeding to
the next sentence causes errors:
4.8.1 But in February a devastating earthquake
in the same region killed 2,300 people and left
thousands of people homeless.
4.9.1 At the time international aid workers
suffered through a logistical nightmare to reach
the snow-bound region with assistance.
It is clear that sentence 4.9 is the continuation of
sentence 4.8 and refers to the same time point
(February earthquake). In this case our rule assigns
the wrong time to 4.9.1. Still we retain this rule
because it is more frequently correct than incorrect.
4.3 Preprocessing
First, the text divided into event-clauses is run
through a program that extracts all the date-stamps
(made available by Kevin Knight, ISI). In most
cases this program does not miss any date-stamps
and extracts only the correct ones. The only cases in
which it did not work properly for the texts were:
? sentence 1.h1
PAKISTAN MAY BE PREPARING FOR
ANOTHER TEST.
Here the modal verb MAY was assumed to be the
month, given that it started with a capital letter.
? sentence 6.24
Tuberculosis is already common in the area
where people live in close quarters and have
poor hygiene
here the noun quarters, which in this case is used in
the sense immediate contact or close range
(Merriam-Webster dictionary), was assumed to be
used in the sense the fourth part of a measure of
time (Merriam-Webster dictionary).
After extracting all the date-phrases we proceed
to time assignment.
4.4 Rules of Time Assignment
When assigning a time to an event, we select the
time to be either the most recently assigned date or,
if the value of the most recently assigned date is
undefined, to the date of the article.  We use a set of
rules to perform this selection. These rules can be
divided into two main categories: those that work
for sentences containing explicit date information,
and those that work for sentences that do not.
4.4.1 Assigning Time-Stamps to the Clauses
with Explicit Date Information
? Day of the Week
If the day-of-the-week used in the event-
clause is the same as that of the article (or the
most recently assigned date, if it is defined), and
there no words before it could signal that the
described event happened earlier or will happen
later, then the time-point of the article (or the
most recently assigned date, if it is defined) is
assigned to this event. If before or after a day-of-
the-week there is a word/words signaling that the
event happened earlier of will happen later then
the time-point is assigned in accordance with this
signal-word and the most recently assigned date,
if it is defined.
If the day-of-the-week used in the event-
clause is not the same as that of the article (or the
most recently assigned date, if it is defined), then
if there are words pointing out that the event
happened before the article was written or the
tense used in the clause is past, then the time for
the event-clause is assigned in accordance with
this word (such words we call signal-words), or
the most recent day corresponding to the current
day-of-the-week is chosen. If the signal-word
points out that the event will happen after the
article was written or the tense used in the clause
is future, then the time for the event-clause is
assigned in accordance with the signal word or
the closest subsequent day corresponding to the
current day-of-the-week.
5.3.1 On Monday and Tuesday, U.N.
helicopters evacuated 50 of the most
seriously injured to emergency medical
centers.
The time for article 5 is (06/06/1998:Tuesday
15:17:00). So, the time assigned to this event-
clause is: 5.3.1 {1998:151:1}, {1998:152:2}.
? Name of Month
The rules are the same as for a day-of-the-
week, but in this case a time-range is assigned to
the event-clause. The left boundary of the range
is the first day of the month, the right boundary
is the last day of the month, and though it is
possible to figure out the days of weeks for these
boundaries, this aspect is ignored for the present.
4.8.1 But in February a devastating
earthquake in the same region killed 2,300
people and left thousands of people
homeless.
The time for article 4 is (05/30/1998:Saturday
14:41:00). So, the time assigned to this event-
clause is 4.8.1 {1998:32:0}---{1998:60:0}.
In the analyzed corpus there is a case where
the presence of a name of month leads to a
wrong time-stamping:
6.3.1 Estimates say
6.3.2 up to 5,000 people died from the May
30 quake,
6.3.3 more than twice as many fatalities as in
the February disaster.
Because of February, a wrong time-interval is
assigned to clause 6.3.3, namely {1998:32:0}---
{1998:60:0}. As this event-clause is a description
of the latest news as compared to some figures it
should have the time-point of the article. Such
cases present a good possibility for the use of
machine learning techniques to disambiguate
between the cases where we should take into
account date-phrase information and where not.
? Weeks, Days, Months, Years
We might have date-stamps where the words
weeks, days, months, years are used with
modifiers. For example
5.2.1 In the village of Kol, hundreds of
people swarmed a United Nations helicopter
5.2.2 that <a United Nations helicopter>
touched down three days after Saturday?s
earthquake
5.2.3 after Saturday?s earthquake struck a
remote mountainous area rocked three
months earlier by another massive quake
5.2.4 that <another massive quake> claimed
some 2,300 victims.
In event-clause 5.2.3 the expression three months
earlier is used. It is clear that to get the time for
the event it is not enough to subtract 3 months
from the time of the article because the above
expression gives an approximate range within
which this event could happen and not a
particular date. For such cases we invented the
following rule:
Time=multiplier*length4; (in this case 3*30);
Day=DDD-Time; (for years Year=YYYY-Time)
Left boundary of the range=
Day-round (10%(Day));
(for years = Year - round(10%(Year)))
Right boundary of the range =
       Day + round (10%(Day));
(for years = Year + round (10%(Year)))
For event 5.2.3 the time range will be
{1998:53:0}---{1998:71:0} (the exact date of the
article is {1998:152:2}).
If the modifier used with weeks, days, months
or years is several, then the multiplier used in (1)
is equal to 2.
? When, Since, After, Before, etc.
If an event-clause does not contain any date-
phrase but contains one of the words ?when?,
?since?, ?after?, ?before?, etc., it might mean that
this clause refers to an event, the time of which
can be used as a reference point for the event
under analysis. In this case we ask the user to
insert the time for this reference event manually.
This rule can cause problems in cases where
?after? or ?before? are used not as temporal
connectors but as spatial ones, though in the
analyzed texts we did not face this problem.
4.4.2 Assigning Time-Stamps to the Clauses
without Explicit Date Information
                                                
4 For days, length is equal to 1, weeks?7, months?30.
? Present/Past Perfect
If the current event-clause refers to a time-
point in Present/Past Perfect tense, then an open-
ended time-interval is assigned to this event. The
starting point is unknown; the end-point is either
the most recently assigned date or the time-point
of the article.
? Future Tense
      If the current event-clause contains a verb in
future tense (one of the verbs ?shall?, ?will?,
?should?, ?would?, ?might? is present in the
clause) then the open-ended time-interval
assigned to this event-clause has the starting
point at either the most recently assigned date or
the date of the article.
? Other Tenses
Other tenses that can be identified with the
help of CONTEX are Present and Past
Indefinite. In the analyzed data all the verbs in
Present Indefinite are given the most recently
assigned date (or the date of the article). The
situation with Past Indefinite is much more
complicated and requires further investigation of
more data. News stories usually describe the
events that already took place at some time in the
past, which is why even if the day when the
event happened is not over, past tense is very
often used for the description (this is especially
noticeable for US news of European, Asian,
African and Australian events). This means that
very often an event-clause containing a verb in
Past Indefinite Tense can be assigned the most
recently assigned date (or the date of the article).
It might prove useful to use machine learned
rules for such cases.
? No verb in the event-clause
If there is no verb in the event-clause then the
most recently assigned date (or the date of the
article) is assigned to the event-clause.
4.5 Sources of Errors for Time-stamper
We ran the time-stamper program on two
types of data: list of event-clauses extracted by
the event identifier and list of event-clauses
created manually. Tables 2 and 3 show the
results.  In the former case we analyzed only the
correctly identified clauses. One can see that
even on manually created data the performance
of the time-stamper is not 100%. Why?
Some errors are caused by assigning the time
based on the date-phrase present in the event-
clause, when this date-phrase is not an adverbial
time modifier but an attribute. For example,
 1. Estimates say
2. up to 5,000 people died from the May 30
earthquake,
3. more than twice as many fatalities as in
the February disaster.
The third event describes the May 30 earthquake
but the time interval given for this event is
{1998:32:0}---{1998:60:0} (i.e., the event
happened in February). It might be possible to
use machine learned rules to correct such cases.
One more significant source of errors is the
writing style:
1. ?When I left early this morning,
2. everything was fine.
3. After the earthquake, I came back,
4. and the house had collapsed.
5. I looked for two days and gave up.
6. Everybody gave up
When the reader sees early this morning he or
she tends to assign to this clause the time of the
article, but later as seeing looked for two days,
realizes that the time of the clause containing
early this morning is two days earlier than the
time of the article. It seems that the errors caused
by the writing style can hardly be avoided.
If an event happened at some time-point but
according to the information in the sentence we
can assign only a time-interval to this event (for
example, February Earthquake) then we say that
the time-interval is assigned correctly if the
necessary time-point is within this time-interval
5 Time-line for Several News Stories and its
Applications
After stamping all the news stories from the
analyzed set, we arrange the event-clauses from
all the articles into a chronological order. After
doing that we obtain a new set of event-clauses
which can easily be divided into two subsets?the
first one containing all the references to the
February earthquake, the second one containing
the list of event-clauses in chronological order,
describing what happened in May.
Such a text where all the events are organized
in a chronological order might be very helpful in
multidocument summarization, where it is
important to include into the final summary not
only the most important information but also the
most recent one. The output of the presented
system gives the information about the time-
order of the events described in several
documents.
6 Related work
Several linguistic and psycholinguistic studies
deal with the problem of time-arrangement of
different texts. The research presented in these
studies highlights many problems but does not
solve them.
As for computational applications of time
theories, most work was done on temporal
expressions that appear in scheduling dialogues
(Busemann et al, 1997; Alexandresson et al,
1997). There are many constraints on temporal
expressions in this domain. The most relevant
prior work is (Mani and Wilson, 2000), who
implemented their system on news stories,
introduced rules spreading time-stamps obtained
with the help of explicit temporal expressions
throughout the whole article, and invented
machine learning rules for disambiguating
between specific and generic use of temporal
expressions (for example, whether Christmas is
used to denote the 25th of December or to denote
some period of time around the 25th of
December). They also mention a problem of
disambiguating between temporal expression and
proper name, as in ?USA Today?.
7 Conclusion
Bell (1997) notices ?more research is needed
on the effects of time structure on news
comprehension. The hypothesis that the non-
canonical news format does adversely affect
understanding is a reasonable one on the basis of
comprehension research into other narrative
genres, but the degree to which familiarity with
news models may mitigate these problems is
unclear?. This research can greatly improve the
performance of time-stamper and might lead to a
list of machine learning rules for time detection.
In this paper we made an attempt to not just
analyze and decode temporal expressions but to
apply this analysis throughout the whole text and
assign time-stamps to such type of clauses,
which later could be used as separate sentences
in various natural language applications, for
example in multidocument summarization.
Text number Number of event-
clauses identified
correctly
Number of time point
correctly assigned to
correctly identified clauses
percentage of
correct
assignment
text 1 5 4 80.00
text 2 15 15 100
text 3 3 2 66.67
text 4 18 17 94.44
text 5 19 17 89.47
text 6 36 24 66.66
Total 96 79 82.29
Table 2. Time-stamper performance on automatically claused texts
(only correctly identified clauses are analyzed).
text
number
number of manually
created event-clauses
number of time point
correctly assigned to
manually created clauses
percentage of
correct
assignment
target 1 7 6 85.71
target 2 27 20 74.07
target 3 5 4 80.00
target 4 28 26 92.85
target 5 33 30 90.91
target 6 58 37 63.79
Total 158 123 77.85
Table 3. Time-stamper performance on manually (correct) claused texts.
References:
J. Alexandresson, N. Reithinger, and E. Maier,
1997. Insights into the Dialogue Processing of
VERBMOBIL. Proceedings of the Fifth
Conference on Applied Natural Language
Processing, 33?40
J. Allen, 1991. Time and Time Again: The
Many Ways to Represent Time. Int'l. Journal of
Intelligent Systems 4, 341?356.
J. Allen and G. Ferguson, 1994. Actions and
Events in Interval Temporal Logic. The University
of Rochester, Technical Report 521
A. Bell, 1997. The Discourse Structure of News
Structure. Approaches to Media Discourse, ed. A.
Bell, 64?104.
S. Busemann, T. Decleck, A.K. Diagne, L. Dini,
J. Klein, and S. Schmeier, 1997. Natural Language
Dialogue Service for Appointment Scheduling
Agents. Proceedings of the Fifth Conference on
Applied Natural Language Processing, 25?32.
I. Mani and G. Wilson, 2000. Robust Temporal
Processing of News. Proceedings of the ACL-2000
Conference.
C. Schokkenbroek, 1999. News Stories:
Structure, Time and Evaluation. Time & Society, vol.
8(1), 59?98.
Appendix 1
CONTEX output for the sentence ?Russia, which has loaned helicopters in previous disasters, said it would
consider sending aid? (sentence 5.11) . The surface nodes (lexemes) are underlined.
::NODE 1 ::SURF "Russia , which has loaned helicopters in previous disasters , said it would consider sending aid ." ::CAT S-SNT ::SUBS (2
20 21 29) ::LEX "say" ::FORMS (((MODE F-DECL) (PERSON F-THIRD-P) (NUMBER F-SING) (CASE F-NOM) (TENSE F-PAST-TENSE)))
::NODE 2 ::SURF "Russia , which has loaned helicopters in previous disasters ," ::CAT S-NP ::SUBS (3 7) ::PARENTS (1) ::LEX "Russia"
::ROLES (SUBJ) ::FORMS (((NUMBER F-SING) (PERSON F-THIRD-P))) ::INDEX 2
::NODE 3 ::SURF "Russia ," ::CAT S-NP ::SUBS (4 6) ::PARENTS (2)::LEX "Russia" ::ROLES (PRED) ::FORMS (((NUMBER F-SING)
(PERSON F-THIRD-P))) ::INDEX 1
::NODE 4 ::SURF "Russia" ::CAT S-NP ::SUBS (5) ::PARENTS (3)::LEX "Russia" ::ROLES (PRED) ::FORMS (((NUMBER F-SING)
(PERSON F-THIRD-P)))
::NODE 5 ::SURF "Russia" ::CAT S-PROPER-NAME ::PARENTS (4) ::LEX "Russia" ::ROLES (PRED) ::FORMS (((PERSON F-THIRD-P)
(NUMBER F-SING)))
::NODE 6 ::SURF "," ::CAT D-COMMA ::PARENTS (3) ::LEX "," ::ROLES (DUMMY)
::NODE 7 ::SURF "which has loaned helicopters in previous disasters ," ::CAT S-REL-CLAUSE ::SUBS (8 10 11 13 19) ::PARENTS (2) ::LEX
"loan" ::ROLES (MOD) ::FORMS (((MODE F-DECL) (TENSE F-PERF-TENSE) (PERSON F-THIRD-P) (NUMBER F-SING)))
::NODE 8 ::SURF "which" ::CAT S-INTERR-NP ::SUBS (9) ::PARENTS (7) ::LEX "which" ::ROLES (SUBJ) ::FORMS (((NUMBER F-
SING) (PERSON F-THIRD-P))) ::PRON-REF 1
::NODE 9 ::SURF "which" ::CAT S-INTERR-PRON ::PARENTS (8)::LEX "which" ::ROLES (PRED) ::FORMS (((PERSON F-THIRD-P)
(NUMBER F-SING)))
::NODE 10 ::SURF "has loaned" ::CAT S-VERB ::PARENTS (7) ::LEX "loan" ::ROLES (PRED) ::FORMS (((NUMBER F-SING) (PERSON
F-THIRD-P) (TENSE F-PERF-TENSE)))
::NODE 11 ::SURF "helicopters" ::CAT S-NP ::SUBS (12) ::PARENTS (7)::LEX "helicopter" ::ROLES (OBJ) ::FORMS (((PERSON F-
THIRD-P) (NUMBER F-PLURAL)))
::NODE 12 ::SURF "helicopters" ::CAT S-NOUN ::PARENTS (11)::LEX "helicopter" ::ROLES (PRED) ::FORMS (((PERSON F-THIRD-P)
(NUMBER F-PLURAL)))
::NODE 13 ::SURF "in previous disasters" ::CAT S-PP ::SUBS (14 15) ::PARENTS (7) ::LEX "disaster" ::ROLES (LOCATION) ::FORMS
(((NUMBER F-PLURAL) (PERSON F-THIRD-P)))
::NODE 14 ::SURF "in" ::CAT S-PREP ::PARENTS (13)::LEX "in" ::ROLES (P)
::NODE 15 ::SURF "previous disasters" ::CAT S-NP ::SUBS (16 18) ::PARENTS (13) ::LEX "disaster" ::ROLES (PRED) ::FORMS
(((PERSON F-THIRD-P) (NUMBER F-PLURAL)))
::NODE 16 ::SURF "previous" ::CAT S-ADJP ::SUBS (17) ::PARENTS (15) ::LEX "previous" ::ROLES (MOD)
::NODE 17 ::SURF "previous" ::CAT S-ADJ ::PARENTS (16)::LEX "previous" ::ROLES (PRED)
::NODE 18 ::SURF "disasters" ::CAT S-NOUN ::PARENTS (15) ::LEX "disaster" ::ROLES (PRED) ::FORMS (((PERSON F-THIRD-P)
(NUMBER F-PLURAL)))
::NODE 19 ::SURF "," ::CAT D-COMMA ::PARENTS (7) ::LEX "," ::ROLES (DUMMY)
::NODE 20 ::SURF "said" ::CAT S-VERB ::PARENTS (1)::LEX "say" ::ROLES (PRED) ::FORMS (((PERSON F-THIRD-P) (NUMBER F-
SING) (TENSE F-PAST-TENSE)))
::NODE 21 ::SURF "it would consider sending aid" ::CAT S-SNT ::SUBS (22 24 25) ::PARENTS (1)::LEX "consider" ::ROLES (COMPL)
::FORMS (((MODE F-DECL) (GENDER F-NEUT) (PERSON F-THIRD-P) (NUMBER F-SING) (CASE F-NOM) (TENSE F-PRES-TENSE)
(MODALS F-WOULD)))
::NODE 22 ::SURF "it" ::CAT S-NP ::SUBS (23) ::PARENTS (21)::LEX "PRON" ::ROLES (SUBJ) ::FORMS (((NUMBER F-SING)
(PERSON F-THIRD-P) (GENDER F-NEUT)))::PRON-REF 2
::NODE 23 ::SURF "it" ::CAT S-REG-PRON ::PARENTS (22)::LEX "PRON" ::ROLES (PRED) ::FORMS (((GENDER F-NEUT) (PERSON
F-THIRD-P) (NUMBER F-SING)))
::NODE 24 ::SURF "would consider" ::CAT S-TR-VERB ::PARENTS (21)::LEX "consider" ::ROLES (PRED) ::FORMS (((GENDER F-
NEUT) (PERSON F-THIRD-P) (NUMBER F-SING) (TENSE F-PRES-TENSE) (MODALS F-WOULD)))
::NODE 25 ::SURF "sending aid" ::CAT S-INF-CLAUSE ::SUBS (26 27) ::PARENTS (21) ::LEX "send" ::ROLES (COMPL) ::FORMS
(((TENSE F-PRES-PART) (MODE F-DECL)))
::NODE 26 ::SURF "sending" ::CAT S-DITR-VERB ::PARENTS (25)::LEX "send" ::ROLES (PRED) ::FORMS (((TENSE F-PRES-PART)))
::NODE 27 ::SURF "aid" ::CAT S-NP ::SUBS (28) ::PARENTS (25)::LEX "aid" ::ROLES (OBJ) ::FORMS (((PERSON F-THIRD-P)
(NUMBER F-SING)))
::NODE 28 ::SURF "aid" ::CAT S-COUNT-NOUN ::PARENTS (27)::LEX "aid" ::ROLES (PRED) ::FORMS (((PERSON F-THIRD-P)
(NUMBER F-SING)))
::NODE 29 ::SURF "." ::CAT D-PERIOD ::PARENTS
A Formal Model for Information Selection in Multi-Sentence Text
Extraction
Elena Filatova
Department of Computer Science
Columbia University
New York, NY 10027, USA
filatova@cs.columbia.edu
Vasileios Hatzivassiloglou
Center for Computational Learning Systems
Columbia University
New York, NY 10027, USA
vh@cs.columbia.edu
Abstract
Selecting important information while account-
ing for repetitions is a hard task for both sum-
marization and question answering. We pro-
pose a formal model that represents a collec-
tion of documents in a two-dimensional space
of textual and conceptual units with an asso-
ciated mapping between these two dimensions.
This representation is then used to describe the
task of selecting textual units for a summary or
answer as a formal optimization task. We pro-
vide approximation algorithms and empirically
validate the performance of the proposed model
when used with two very different sets of fea-
tures, words and atomic events.
1 Introduction
Many natural language processing tasks involve the
collection and assembling of pieces of informa-
tion from multiple sources, such as different doc-
uments or different parts of a document. Text sum-
marization clearly entails selecting the most salient
information (whether generically or for a specific
task) and putting it together in a coherent sum-
mary. Question answering research has recently
started examining the production of multi-sentence
answers, where multiple pieces of information are
included in the final output.
When the answer or summary consists of mul-
tiple separately extracted (or constructed) phrases,
sentences, or paragraphs, additional factors influ-
ence the selection process. Obviously, each of the
selected text snippets should individually be impor-
tant. However, when many of the competing pas-
sages are included in the final output, the issue of
information overlap between the parts of the output
comes up, and a mechanism for addressing redun-
dancy is needed. Current approaches in both sum-
marization and long answer generation are primar-
ily oriented towards making good decisions for each
potential part of the output, rather than examining
whether these parts overlap. Most current methods
adopt a statistical framework, without full semantic
analysis of the selected content passages; this makes
the comparison of content across multiple selected
text passages hard, and necessarily approximated by
the textual similarity of those passages.
Thus, most current summarization or long-
answer question-answering systems employ two
levels of analysis: a content level, where every tex-
tual unit is scored according to the concepts or fea-
tures it covers, and a textual level, when, before
being added to the final output, the textual units
deemed to be important are compared to each other
and only those that are not too similar to other can-
didates are included in the final answer or summary.
This comparison can be performed purely on the ba-
sis of text similarity, or on the basis of shared fea-
tures that may be the same as the features used to
select the candidate text units in the first place.
In this paper, we propose a formal model for in-
tegrating these two tasks, simultaneously perform-
ing the selection of important text passages and the
minimization of information overlap between them.
We formalize the problem by positing a textual unit
space, from which all potential parts of the summary
or answer are drawn, a conceptual unit space, which
represents the distinct conceptual pieces of informa-
tion that should be maximally included in the final
output, and a mapping between conceptual and tex-
tual units. All three components of the model are
application- and task-dependent, allowing for dif-
ferent applications to operate on text pieces of dif-
ferent granularity and aim to cover different concep-
tual features, as appropriate for the task at hand. We
cast the problem of selecting the best textual units
as an optimization problem over a general scoring
function that measures the total coverage of concep-
tual units by any given set of textual units, and pro-
vide general algorithms for obtaining a solution.
By integrating redundancy checking into the se-
lection of the textual units we provide a unified
framework for addressing content overlap that does
not require external measures of similarity between
textual units. We also account for the partial overlap
of information between textual units (e.g., a single
shared clause), a situation which is common in nat-
ural language but not handled by current methods
for reducing redundancy.
2 Formal Model for Information Selection
and Packing
Our model for selecting and packing information
across multiple text units relies on three compo-
nents that are specified by each application. First,
we assume that there is a finite set T of textual units
t1, t2, . . . , tn, a subset of which will form the an-
swer or summary. For most approaches to sum-
marization and question answering, which follow
the extraction paradigm, the textual units ti will
be obtained by segmenting the input text(s) at an
application-specified granularity level, so each ti
would typically be a sentence or paragraph.
Second, we posit the existence of a finite set C
of conceptual units c1, c2, . . . , cm. The conceptual
units encode the information that should be present
in the output, and they can be defined in different
ways according to the task at hand and the prior-
ities of each system. Obviously, defining the ap-
propriate conceptual units is a core problem, akin
to feature selection in machine learning: There is
no exact definition of what an important concept is
that would apply to all tasks. Current summariza-
tion systems often represent concepts indirectly via
textual features that give high scores to the textual
units that contain important information and should
be used in the summary and low scores to those tex-
tual units which are not likely to contain informa-
tion worth to be included in the final output. Thus,
many summarization approaches use as conceptual
units lexical features like tf*idf weighing of words
in the input text(s), words used in the titles and sec-
tion headings of the source documents (Luhn, 1959;
H.P.Edmundson, 1968), or certain cue phrases like
significant, important and in conclusion (Kupiec et
al., 1995; Teufel and Moens, 1997). Conceptual
units can also be defined out of more basic concep-
tual units, based on the co-occurrence of important
concepts (Barzilay and Elhadad, 1997) or syntac-
tic constraints between representations of concepts
(Hatzivassiloglou et al, 2001). Conceptual units do
not have to be directly observable as text snippets;
they can represent abstract properties that particular
text units may or may not satisfy, for example, status
as a first sentence in a paragraph or generally posi-
tion in the source text (Lin and Hovy, 1997). Some
summarization systems assume that the importance
of a sentence is derivable from a rhetorical repre-
sentation of the source text (Marcu, 1997), while
others leverage information from multiple texts to
re-score the importance of conceptual units across
all the sources (Hatzivassiloglou et al, 2001).
No matter how these important concepts are de-
fined, different systems use text-observable features
that either correspond to the concepts of interest
(e.g., words and their frequencies) or point out those
text units that potentially contain important con-
cepts (e.g., position or discourse properties of the
text unit in the source document). The former class
of features can be directly converted to concep-
tual units in our representation, while the latter can
be accounted for by postulating abstract conceptual
units associated with a particular status (e.g., first
sentence) for a particular textual unit. We assume
that each conceptual unit has an associated impor-
tance weight wi that indicates how important unit ci
is to the overall summary or answer.
2.1 A first model: Full correspondence
Having formally defined the sets T and C of tex-
tual and conceptual units, the part that remains in
order to have the complete picture of the constraints
given by the data and summarization approach is the
mapping between textual units and conceptual units.
This mapping, a function f : T?C ? [0, 1], tells us
how well each conceptual unit is covered by a given
textual unit. Presumably, different approaches will
assign different coverage scores for even the same
sentences and conceptual units, and the consistency
and quality of these scores would be one way to de-
termine the success of each competing approach.
We first examine the case where the function f is
limited to zero or one values, i.e., each textual unit
either contains/matches a given conceptual feature
or not. This is the case with many simple features,
such as words and sentence position. Then, we de-
fine the total information covered by any given sub-
set S of T (a proposed summary or answer) as
I(S) =
?
i=1,...,m
wi ? ?i (1)
where wi is the weight of the concept ci and
?i =
{ 1, if ?j ? {1, . . . ,m} such that f(tj , ci) = 1
0, otherwise
In other words, the information contained in a
summary is the sum of the weights of the concep-
tual units covered by at least one of the textual units
included in the summary.
2.2 Partial correspondence between textual
and conceptual units
Depending on the nature of the conceptual units, the
assumption of a 0-1 mapping between textual and
conceptual units may or may not be practical or even
feasible. For many relatively simple representations
of concepts, this restriction poses no difficulties: the
concept is uniquely identified and can be recognized
as present or absent in a text passage. However, it is
possible that the concepts have some structure and
can be decomposed to more elementary conceptual
units, or that partial matches between concepts and
text are natural. For example, if the conceptual units
represent named entities (a common occurrence in
list-type long answers), a partial match between a
name found in a text and another name is possi-
ble; handling these two names as distinct concepts
would be inaccurate. Similarly, an event can be rep-
resented as a concept with components correspond-
ing to participants, time, location, and action, with
only some of these components found in a particular
piece of text.
Partial matches between textual and conceptual
units introduce a new problem, however: if two tex-
tual units partially cover the same concept, it is
not apparent to what extent the coverage overlaps.
Thus, there are multiple ways to revise equation (1)
in order to account for partial matches, depending
on how conservative we are on the expected over-
lap. One such way is to assume minimum overlap
(the most conservative assumption) and define the
total information in the summary as
I(S) =
?
i=1,...,m
wi ?maxj f(tj , ci) (2)
An alternative is to consider that f(tj , ci) repre-
sents the extent of the [0, 1] interval corresponding
to concept ci that tj covers, and assume that the
coverage is spread over that interval uniformly and
independently across textual units. Then the com-
bined coverage of two textual units tj and tk is
f(tj , ci) + f(tk, ci)? f(tj , ci) ? f(tk, ci)
This operator can be naturally extended to more
than two textual units and plugged into equation (2)
in the place of the max operator, resulting into an
equation we will refer to as equation (3). Note that
both of these equations reduce to our original for-
mula for information content (equation (1)) if the
mapping function f only produces 0 and 1 values.
2.3 Length and textual constraints
We have provided formulae that measure the infor-
mation covered by a collection of textual units un-
der different mapping constraints. Obviously, we
want to maximize this information content. How-
ever, this can only sensibly happen when additional
constraints on the number or length of the selected
textual units are introduced; otherwise, the full set
of available textual units would be a solution that
proffers a maximal value for equations (1)?(3), i.e.,
?S ? T, I(S) ? I(T ). We achieve this by assign-
ing a cost pi to each textual unit ti, i = 1, . . . , n,
and defining a function P over a set of textual units
that provides the total penalty associated with se-
lecting those textual units as the output. In our ab-
straction, replacing a textual unit with one or more
textual units that provide the same content should
only affect the penalty, and it makes sense to assign
the same cost to a long sentence as to two sentences
produced by splitting the original sentence. Also,
a shorter sentence should be preferable to a longer
sentence with the same information content. Hence,
our operational definitions for pi and P are
pi = length(ti), P (S) =
?
ti?S
pi
i.e., the total penalty is equal to the total length of
the answer in some basic unit (e.g., words).
Note however, than in the general case the pi?s
need not depend solely on the length, and the to-
tal penalty does not need to be a linear combina-
tion of them. The cost function can depend on
features other then length, for example, number of
pronouns?the more pronouns used in a textual unit,
the higher the risk of dangling references and the
higher the price should be. Finding the best cost
function is an interesting research problem by itself.
With the introduction of the cost function P (S)
our model has two generally competing compo-
nents. One approach is to set a limit on P (S) and
optimize I(S) while keeping P (S) under that limit.
This approach is similar to that taken in evaluations
that keep the length of the output summary within
certain bounds, such as the recent major summa-
rization evaluations in the Document Understand-
ing Conferences from 2001 to the present (Harman
and Voorhees, 2001). Another approach would be
to combine the two components and assign a com-
posite score to each summary, essentially mandat-
ing a specific tradeoff between recall and precision;
for example, the total score can be defined as a lin-
ear combination of I(S) and P (S), in which case
the weights specify the relative importance of cov-
erage and precision/brevity, as well as accounting
for scale differences between the two metrics. This
approach is similar to the calculation of recall, pre-
cision, and F-measure adopted in the recent NIST
evaluation of long answers for definitional questions
(Voorhees, 2003). In this paper, we will follow the
first tactic of maximizing I(S) with a limit on P (S)
rather than attempting to solve the thorny issues of
weighing the two components appropriately.
3 Handling Redundancy in
Summarization
Redundancy of information has been found useful
in determining what text pieces should be included
during summarization, on the basis that information
that is repeated is likely to be central to the topic or
event being discussed. Earlier work has also recog-
nized that, while it is a good idea to select among
the passages repeating information, it is also impor-
tant to avoid repetition of the same information in
the final output.
Two main approaches have been proposed for
avoiding redundancy in the output. One approach
relies on grouping together potential output text
units on the basis of their similarity, and outputting
only a representative from each group (Hatzivas-
siloglou et al, 2001). Sentences can be clustered
in this manner according to word overlap, or by us-
ing additional content similarity features. This ap-
proach has been recently applied to the construction
of paragraph-long answers (e.g., (Blair-Goldensohn
et al, 2003; Yu and Hatzivassiloglou, 2003)).
An alternative approach, proposed for the synthe-
sis of information during query-based passage re-
trieval is the maximum marginal relevance (MMR)
method (Goldstein et al, 2000). This approach as-
signs to each potential new sentence in the output a
similarity score with the sentences already included
in the summary. Only those sentences that contain a
substantial amount of new information can get into
the summary. MMR bases this similarity score on
word overlap and additional information about the
time when each document was released, and thus
can fail to identify repeated information when para-
phrasing is used to convey the same meaning.
In contrast to these approaches, our model han-
dles redundancy in the output at the same time it
selects the output sentences. It is clear from equa-
tions (1)?(3) that each conceptual unit is counted
only once whether it appears in one or multiple tex-
tual units. Thus, when we find the subset of textual
units that maximizes overall information coverage
with a constraint on the total number or length of
textual units, the model will prefer the collection of
textual units that have minimal overlap of covered
conceptual units. Our approach offers three advan-
tages versus both clustering and MMR: First, it in-
tegrates redundancy elimination into the selection
process, requiring no additional features for defin-
ing a text-level similarity between selected textual
units. Second, decisions are based on the same fea-
tures that drive the summarization itself, not on ad-
ditional surface properties of similarity. Finally, be-
cause all decisions are informed by the overlap of
conceptual units, our approach accounts for partial
overlap of information across textual units. To illus-
trate this last point, consider a case where three fea-
tures A, B, and C should be covered in the output,
and where three textual units are available, cover-
ing A and B, A and C, and B and C, respectively.
Then our model will determine that selecting any
two of the textual units is fully sufficient, while this
may not be apparent on the basis of text similarity
between the three text units; a clustering algorithm
may form three singleton clusters, and MMR may
determine that each textual unit is sufficiently dif-
ferent from each other, especially if A, B, and C
are realized with nearly the same number of words.
4 Applying the Model
Having presented a formal metric for the informa-
tion content (and optionally the cost) of any poten-
tial summary or answer, the task that remains is to
optimize this metric and select the corresponding
set of textual units for the final output. As stated
in Section 2.3, one possible way to do this is to fo-
cus on the information content metric and introduce
an additional constraint, limiting the total cost to a
constant. An alternative is to optimize directly the
composite function that combines cost and informa-
tion content into a single number.
We examine the case of zero-one mappings be-
tween textual and conceptual units, where the to-
tal information content is specified by equation (1).
The complexity of the problem depends on the
cost function, and whether we optimize I(S) while
keeping P (S) fixed or whether we optimize a com-
bined function of both of those quantities. We will
only consider the former case in the present paper.
We start by examining an artificially simple case,
where the cost assigned to each textual unit is 1, and
the function P for combining costs is their sum. In
this case, the total cost is equal to the number of
textual units used in a summary.
This problem, as we have formalized it above,
is identical to the Maximum Set Coverage problem
studied in theoretical computer science: given C, a
finite set of weighted elements, a collection T of
subsets of C, and an integer k, find those k sets that
maximize the total number of elements in the union
of T ?s members (Hochbaum, 1997). In our case,
the zero-one mapping allows us to view each textual
unit as a subset of the conceptual units space, con-
taining those conceptual units covered by the tex-
tual unit, and k is the total target cost. Unfortu-
nately, maximum set coverage is NP-hard, as it is
reducible to the classic set cover problem (given a
finite set and a collection of subsets of that set, find
the smallest subset of that collection whose mem-
bers? union is equal to the original set) (Hochbaum,
1997). It follows that more general formulations of
the cost function that actually are more realistic for
our problem (such as defining the total cost as the
sum of the lengths of the selected textual units and
allowing the textual units to have different lengths)
will also result in an NP-hard problem, as we can re-
duce these versions to the special case of maximum
set coverage.
Nevertheless, the correspondence with maximum
set coverage provides a silver lining. Since the
problem is known to be NP-hard, properties of
simple greedy algorithms have been explored, and
a straightforward local maximization method has
been proved to give solutions within a known bound
of the optimal solution. The greedy algorithm for
maximum set coverage has as follows: Start with an
empty solution S, and iteratively add to the S the
set Ti that maximizes I(S ? Ti). It is provable that
this algorithm is the best polynomial approximation
algorithm for the problem (Hochbaum, 1997), and
that it achieves a solution bounded as follows
I(OPT) ? I(GREEDY) ?
[
1?
(
1? 1k
)k]
I(OPT)
>
(
1? 1e
)
I(OPT) ? 0.6321? I(OPT)
where I(OPT) is the information content of the op-
timal summary and I(GREEDY) is the information
content of the summary produced by this greedy al-
gorithm.
For the more realistic case where cost is speci-
fied as the total length of the summary, and where
we try to optimize I(S) with a limit on P (S) (see
Section 2.3), we propose two greedy algorithms in-
spired by the algorithm above. Both our algorithms
operate by first calculating a ranking of the textual
units in decreasing order. This ranking is for the
first algorithm, which we call adaptive greedy algo-
rithm, identical to the ranking provided by the ba-
sic greedy algorithm, i.e., each textual unit receives
as score the increase in I(S) that it generates when
added to the output, in the order specified by the ba-
sic greedy algorithm. Our second greedy algorithm
(dubbed modified greedy algorithm below) modifies
this ranking by prioritizing the conceptual units with
highest individual weight wi; it ranks first the tex-
tual unit that has the highest contribution to I(S)
while covering this conceptual unit with the high-
est individual weight, and then iteratively proceeds
with the textual unit that has the highest contribu-
tion to I(S) while covering the next most important
unaccounted for conceptual unit.
Given the rankings of textual units, we can then
produce an output of a given length by adopting ap-
propriate stopping criteria for when to stop adding
textual units (in order according to their ranking)
to the output. There is no clear rule for conform-
ing to a specific length (for example, DUC 2001 al-
lowed submitted summaries to go over ?a reason-
able percentage? of the target length, while DUC
2004 cuts summaries mid-sentence at exactly the
target length). As the summary length in DUC is
measured in words, in our experiments we extracted
the specified number of words out of the top sen-
tences (truncating the last sentence if necessary).
5 Experiments
To empirically establish the effectiveness of the pre-
sented model we ran experiments comparing evalu-
ation scores on summaries obtained with a baseline
algorithm that does not account for redundancy of
information and with the two variants of greedy al-
gorithms described in Section 4. We chose summa-
rization as the evaluation task because ?ideal? out-
put (prepared by humans) and methods for scoring
arbitrary system output were available for this task,
but not for evaluating long answers to questions.
Data We chose as our input data the document
sets used in the evaluation of multidocument sum-
marization during the Document Understanding
Conference (DUC), organized by NIST in 2001
(Harman and Voorhees, 2001). This collection con-
tains 30 test document sets, each containing approx-
imately 10 news stories on different events; docu-
ment sets vary significantly in their internal cohere-
ness. For each document set 12 human-constructed
summaries are provided, 3 for each of the target
lengths of 50, 100, 200, and 400 words. We se-
lected DUC 2001 because unlike later DUCs, ideal
summaries are available for multiple lengths. We
consider sentences as our textual units.
Features In our experiments we used two sets of
features (i.e., conceptual units). First, we chose
a fairly basic and widely used set of lexical fea-
tures, namely the list of words present in each input
text. We set the weight of each feature to its tf*idf
value, taking idf values from http://elib.cs.
berkeley.edu/docfreq/.
Our alternative set of conceptual units was the list
of weighted atomic events extracted from the input
texts. An atomic event is a triplet consisting of two
named entities extracted from a sentence and a con-
nector expressed by a verb or an event-related noun
that appears in-between these two named entities.
The score of the atomic event depends on the fre-
quency of the named entities pair for the input text
and the frequency of the connector for that named
entities pair. Filatova and Hatzivassiloglou (2003)
define the procedure for extracting atomic events in
detail, and show that these triplets capture the most
important relations connecting the major constituent
parts of events, such as location, dates and partici-
pants. Our hypothesis is that using these events as
conceptual units would provide a reasonable basis
for summarizing texts that are supposed to describe
one or more events.
Evaluation Metric Given the difficulties in com-
ing up with a universally accepted evaluation mea-
sure for summarization, and the fact that judgments
by humans are time-consuming and labor-intensive,
we adopted an automated process for comparing
system-produced summaries to the ideal summaries
written by humans. The ROUGE method (Lin and
Hovy, 2003) is based on n-gram overlap between
the system-produced and ideal summaries. As such,
it is a recall-based measure, and it requires that
the length of the summaries be controlled in or-
der to allow for meaningful comparisons. Although
ROUGE is only a proxy measure of summary qual-
ity, it offers the advantage that it can be readily ap-
plied to compare the performance of different sys-
tems on the same set of documents, assuming that
ideal summaries are available for those documents.
Baseline Our baseline method does not consider
the overlap in information content between selected
textual units. Instead, we fix the score of each sen-
tence as the sum of tf*idf values or atomic event
scores. At every step we choose the remaining sen-
tence with the largest score, until the stopping crite-
rion for summary length is satisfied.
Results For every version of our baseline and
approximation algorithms, and separately for the
tf*idf -weighted words and event features, we get a
sorted list of sentences extracted according to a par-
ticular algorithm. Then, for each DUC document set
we create four summaries of each suggested length
(50, 100, 200, and 400 words) by extracting accord-
ingly the first 50, 100, 200, and 400 words from the
top sentences.
To evaluate the performance of our summarizers
we compare their outputs against the human models
of the corresponding length provided by DUC, us-
ing the ROUGE-created scores for unigrams. Since
scores are not comparable across different docu-
ment sets, instead of average scores we report the
number of document sets for which one algorithm
outperforms another. We compare each of our
Length Events tf*idf
50 +3 0
100 +4 ?4
200 +2 ?4
400 +5 0
Table 1: Adaptive greedy algorithm versus baseline.
Length Events tf*idf
50 0 + 7
100 +4 + 4
200 +8 + 6
400 +2 +14
Table 2: Modified greedy algorithm versus baseline.
approximation algorithms (adaptive and modified
greedy) to the baseline.
Table 1 shows the number of data sets for
which the adaptive greedy algorithm outperforms
our baseline. This implementation of our informa-
tion packing model improves the ROUGE scores in
most cases when events are used as features, while
the opposite is true when tf*idf provides the con-
ceptual units. This may be partly explained because
of the nature of the tf*idf -weighted word features:
it is possible that important words cannot be con-
sidered independently, and that the repetition of im-
portant words in later sentence does not necessarily
mean that the sentence offers no new information.
Thus words may not provide independent enough
features for our approach to work.
Table 2 compares our modified greedy algorithm
to the baseline. In that case, the model offers gains
in performance when both events and words are
used as features, and in fact the gains are most pro-
nounced with the word features. For both algo-
rithms, the gains are generally minimal for 50 word
summaries and most pronounced for the longest,
400 word summaries. This validates our approach,
as the information packing model has a limited op-
portunity to alter the set of selected sentences when
those sentences are very few (often one or two for
the shortest summaries).
It is worth noting that in direct comparisons be-
tween the adaptive and modified greedy algorithm
we found the latter to outperform the former. We
found also events to lead to better performance than
tf*idf -weighted words with statistically significant
differences. Events tend to be a particularly good
representation for document sets with well-defined
constituent parts (such as specific participants) that
cluster around a narrow event. Events not only give
us a higher absolute performance when compared
to just words but also lead to more pronounced im-
provement when our model is employed. A more
detailed analysis of the above experiments together
with the discussion of advantages and disadvantages
of our evaluation schema can be found in (Filatova
and Hatzivassiloglou, 2004).
6 Conclusion
In this paper we proposed a formal model for in-
formation selection and redundancy avoidance in
summarization and question-answering. Within
this two-dimensional model, summarization and
question-answering entail mapping textual units
onto conceptual units, and optimizing the selection
of a subset of textual units that maximizes the in-
formation content of the covered conceptual units.
The formalization of the process allows us to benefit
from theoretical results, including suitable approx-
imation algorithms. Experiments using DUC data
showed that this approach does indeed lead to im-
provements due to better information packing over
a straightforward content selection method.
7 Acknowledgements
We wish to thank Rocco Servedio and Mihalis
Yannakakis for valuable discussions of theoreti-
cal foundations of the set cover problem. This
work was supported by ARDA under Advanced
Question Answering for Intelligence (AQUAINT)
project MDA908-02-C-0008.
References
Regina Barzilay and Michael Elhadad. 1997. Us-
ing lexical chains for text summarization. In Pro-
ceedings of the ACL/EACL 1997 Workshop on In-
telligent Scalable Text Summarization, Spain.
Sasha Blair-Goldensohn, Kathleen R. McKeown,
and Andrew Hazen Schlaikjer. 2003. Defscriber:
A hybrid system for definitional qa. In Proceed-
ings of 26th Annual International ACM SIGIR
Conference, Toronoto, Canada, July.
Elena Filatova and Vasileios Hatzivassiloglou.
2003. Domain-independent detection, extraction,
and labeling of atomic events. In Proceedings of
Recent Advances in Natural Language Process-
ing Conference, RANLP, Bulgaria.
Elena Filatova and Vasileios Hatzivassiloglou.
2004. Event-based extractive summarization. In
Proceedings of ACL Workshop on Summariza-
tion, Barcelona, Spain, July.
Jade Goldstein, Vibhu Mittal, Jaime Carbonell,
and Jamie Callan. 2000. Creating and evaluat-
ing multi-document sentence extract summaries.
In Proceedings of the ninth international con-
ference on Information and knowledge manage-
ment, pages 165?172.
Donna Harman and Ellen Voorhees, editors. 2001.
Proceedings of the Document Understanding
Conference (DUC). NIST, New Orleans, USA.
Vasileios Hatzivassiloglou, Judith L. Klavans,
Melissa L. Holcombe, Regina Barzilay, Min-
Yen Kan, and Kathleen R. McKeown. 2001.
Simfinder: A flexible clustering tool for summa-
rization. In Proceedings of workshop on Auto-
matic Summarization, NAACL, Pittsburg, USA.
Dorit S. Hochbaum. 1997. Approximating cov-
ering and packing problems: Set cover, vertex
cover, independent set, and related problems. In
Dorit S. Hochbaum, editor, Approximation Al-
gorithms for NP-hard Problems, pages 94?143.
PWS Publishing Company, Boston, MA.
H.P.Edmundson. 1968. New methods in automatic
extracting. Journal of the Association for Com-
puting Machinery, 23(1):264?285, April.
Julian Kupiec, Jan Pedersen, and Francine Chen.
1995. A trainable document summarizer. In Pro-
ceedings of 18th Annual International ACM SI-
GIR Conference, pages 68?73, Seattle, USA.
Chin-Yew Lin and Eduard Hovy. 1997. Identify-
ing topic by position. In Proceedings of the 5th
Conference on Applied Natural Language Pro-
cessing, ANLP, Washington, DC.
Chin-Yew Lin and Eduard Hovy. 2003. Auto-
matic evaluation of summaries using n-gram co-
occurrence statistics. In Proceedings of 2003
Language Technology Conference (HLT-NAACL
2003), Edmonton, Canada, May.
H.P. Luhn. 1959. The automatic creation of litera-
ture abstracts. IBM Journal of Research and De-
velopment, 2(2):159?165, April.
Daniel Marcu. 1997. From discourse struc-
tures to text summaries. In Proceedings of the
ACL/EACL 1997 Workshop on Intelligent Scal-
able Text Summarization, pages 82?88, Spain.
Simone Teufel and Marc Moens. 1997. Sentence
extraction as a classification task. In Proceedings
of the ACL/EACL 1997 Workshop on Intelligent
Scalable Text Summarizaion, Spain.
Ellen M. Voorhees. 2003. Evaluating answers to
definition questions. In Proceedings of HLT-
NAACL, Edmonton, Canada, May.
Hong Yu and Vasileios Hatzivassiloglou. 2003. To-
wards answering opinion questions: Separating
facts from opinions and identifying the polarity of
opinion sentences. In Proceedings of the Confer-
ence on Empirical Methods in Natural Language
Processing (EMNLP), Sapporo, Japan, July.
Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 113?120, Vancouver, October 2005. c?2005 Association for Computational Linguistics
113
114
115
116
117
118
119
120
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 207?214,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Automatic Creation of Domain Templates
Elena Filatova*, Vasileios Hatzivassiloglou? and Kathleen McKeown*
*Department of Computer Science
Columbia University
{filatova,kathy}@cs.columbia.edu
?Department of Computer Science
The University of Texas at Dallas
vh@hlt.utdallas.edu
Abstract
Recently, many Natural Language Processing
(NLP) applications have improved the quality of
their output by using various machine learning tech-
niques to mine Information Extraction (IE) patterns
for capturing information from the input text. Cur-
rently, to mine IE patterns one should know in ad-
vance the type of the information that should be
captured by these patterns. In this work we pro-
pose a novel methodology for corpus analysis based
on cross-examination of several document collec-
tions representing different instances of the same
domain. We show that this methodology can be
used for automatic domain template creation. As the
problem of automatic domain template creation is
rather new, there is no well-defined procedure for
the evaluation of the domain template quality. Thus,
we propose a methodology for identifying what in-
formation should be present in the template. Using
this information we evaluate the automatically cre-
ated domain templates through the text snippets re-
trieved according to the created templates.
1 Introduction
Open-ended question-answering (QA) systems
typically produce a response containing a vari-
ety of specific facts proscribed by the question
type. A biography, for example, might contain the
date of birth, occupation, or nationality of the per-
son in question (Duboue and McKeown, 2003;
Zhou et al, 2004; Weischedel et al, 2004; Fila-
tova and Prager, 2005). A definition may contain
the genus of the term and characteristic attributes
(Blair-Goldensohn et al, 2004). A response to a
question about a terrorist attack might include the
event, victims, perpetrator and date as the tem-
plates designed for the Message Understanding
Conferences (Radev and McKeown, 1998; White
et al, 2001) predicted. Furthermore, the type of in-
formation included varies depending on context. A
biography of an actor would include movie names,
while a biography of an inventor would include the
names of inventions. A description of a terrorist
event in Latin America in the eighties is different
from the description of today?s terrorist events.
How does one determine what facts are im-
portant for different kinds of responses? Often
the types of facts that are important are hand en-
coded ahead of time by a human expert (e.g., as
in the case of MUC templates). In this paper, we
present an approach that allows a system to learn
the types of facts that are appropriate for a par-
ticular response. We focus on acquiring fact-types
for events, automatically producing a template that
can guide the creation of responses to questions
requiring a description of an event. The template
can be tailored to a specific time period or coun-
try simply by changing the document collections
from which learning takes place.
In this work, a domain is a set of events of a par-
ticular type; earthquakes and presidential elections
are two such domains. Domains can be instanti-
ated by several instances of events of that type
(e.g., the earthquake in Japan in October 2004, the
earthquake in Afghanistan in March 2002, etc.).1
The granularity of domains and instances can be
altered by examining data at different levels of de-
tail, and domains can be hierarchically structured.
An ideal template is a set of attribute-value pairs,
with the attributes specifying particular functional
roles important for the domain events.
In this paper we present a method of domain-
independent on-the-fly template creation. Our
method is completely automatic. As input it re-
quires several document collections describing do-
main instances. We cross-examine the input in-
stances, we identify verbs important for the major-
ity of instances and relationships containing these
verbs. We generalize across multiple domain in-
stances to automatically determine which of these
relations should be used in the template. We re-
port on data collection efforts and results from four
domains. We assess how well the automatically
produced templates satisfy users? needs, as man-
ifested by questions collected for these domains.
1Unfortunately, NLP terminology is not standardized
across different tasks. Two NLP tasks most close to our
research are Topic Detection and Tracking (TDT) (Fiscus
et al, 1999) and Information Extraction (IE) (Marsh and
Perzanowski, 1997). In TDT terminology, our domains are
topics and our instances are events. In IE terminology, our
domains are scenarios and our domain templates are scenario
templates.
207
2 Related Work
Our system automatically generates a template
that captures the generally most important infor-
mation for a particular domain and is reusable
across multiple instances of that domain. Decid-
ing what slots to include in the template, and what
restrictions to place on their potential fillers, is
a knowledge representation problem (Hobbs and
Israel, 1994). Templates were used in the main
IE competitions, the Message Understanding Con-
ferences (Hobbs and Israel, 1994; Onyshkevych,
1994; Marsh and Perzanowski, 1997). One of the
recent evaluations, ACE,2 uses pre-defined frames
connecting event types (e.g., arrest, release) to a
set of attributes. The template construction task
was not addressed by the participating systems.
The domain templates were created manually by
experts to capture the structure of the facts sought.
Although templates have been extensively used
in information extraction, there has been little
work on their automatic design. In the Concep-
tual Case Frame Acquisition project (Riloff and
Schmelzenbach, 1998), extraction patterns, a do-
main semantic lexicon, and a list of conceptual
roles and associated semantic categories for the
domain are used to produce multiple-slot case
frames with selectional restrictions. The system
requires two sets of documents: those relevant to
the domain and those irrelevant. Our approach
does not require any domain-specific knowledge
and uses only corpus-based statistics.
The GISTexter summarization sys-
tem (Harabagiu and Maiorano, 2002) used
statistics over an arbitrary document collection
together with semantic relations from WordNet.
The created templates heavily depend on the top-
ical relations encoded in WordNet. The template
models an input collection of documents. If there
is only one domain instance described in the input
than the template is created for this particular
instance rather than for a domain. In our work,
we learn domain templates by cross-examining
several collections of documents on the same
topic, aiming for a general domain template. We
rely on relations cross-mentioned in different
instances of the domain to automatically prioritize
roles and relationships for selection.
Topic Themes (Harabagiu and La?ca?tus?u, 2005)
used for multi-document summarization merge
various arguments corresponding to the same se-
2http://www.nist.gov/speech/tests/ace/index.htm
mantic roles for the semantically identical verb
phrases (e.g., arrests and placed under arrest).
Atomic events also model an input document
collection (Filatova and Hatzivassiloglou, 2003)
and are created according to the statistics col-
lected for co-occurrences of named entity pairs
linked through actions. GISTexter, atomic events,
and Topic Themes were used for modeling a col-
lection of documents rather than a domain.
In other closely related work, Sudo et al (2003)
use frequent dependency subtrees as measured by
TF*IDF to identify named entities and IE patterns
important for a given domain. The goal of their
work is to show how the techniques improve IE
pattern acquisition. To do this, Sudo et al con-
strain the retrieval of relevant documents for a
MUC scenario and then use unsupervised learn-
ing over descriptions within these documents that
match specific types of named entities (e.g., Ar-
resting Agency, Charge), thus enabling learning
of patterns for specific templates (e.g., the Ar-
rest scenario). In contrast, the goal of our work
is to show how similar techniques can be used to
learn what information is important for a given
domain or event and thus, should be included
into the domain template. Our approach allows,
for example, learning that an arrest along with
other events (e.g., attack) is often part of a ter-
rorist event. We do not assume any prior knowl-
edge about domains. We demonstrate that frequent
subtrees can be used not only to extract specific
named entities for a given scenario but also to
learn domain-important relations. These relations
link domain actions and named entities as well as
general nouns and words belonging to other syn-
tactic categories.
Collier (1998) proposed a fully automatic
method for creating templates for information ex-
traction. The method relies on Luhn?s (1957) idea
of locating statistically significant words in a cor-
pus and uses those to locate the sentences in which
they occur. Then it extracts Subject-Verb-Object
patterns in those sentences to identify the most
important interactions in the input data. The sys-
tem was constructed to create MUC templates for
terrorist attacks. Our work also relies on corpus
statistics, but we utilize arbitrary syntactic pat-
terns and explicitly use multiple domain instances.
Keeping domain instances separated, we cross-
examine them and estimate the importance of a
particular information type in the domain.
208
3 Our Approach to Template Creation
After reading about presidential elections in dif-
ferent countries on different years, a reader has a
general picture of this process. Later, when read-
ing about a new presidential election, the reader al-
ready has in her mind a set of questions for which
she expects answers. This process can be called
domain modeling. The more instances of a partic-
ular domain a person has seen, the better under-
standing she has about what type of information
should be expected in an unseen collection of doc-
uments discussing a new instance of this domain.
Thus, we propose to use a set of document col-
lections describing different instances within one
domain to learn the general characteristics of this
domain. These characteristics can be then used to
create a domain template. We test our system on
four domains: airplane crashes, earthquakes, pres-
idential elections, terrorist attacks.
4 Data Description
4.1 Training Data
To create training document collections we used
BBC Advanced Search3 and submitted queries of
the type ?domain title + country?. For example,
??presidential election? USA?.
In addition, we used BBC?s Advanced Search
date filter to constrain the results to different date
periods of interest. For example, we used known
dates of elections and allowed a search for articles
published up to five days before or after each such
date. At the same time for the terrorist attacks or
earthquakes domain the time constraints we sub-
mitted were the day of the event plus ten days.
Thus, we identify several instances for each of
our four domains, obtaining a document collec-
tion for each instance. E.g., for the earthquake do-
main we collected documents on the earthquakes
in Afghanistan (March 25, 2002), India (January
26, 2001), Iran (December 26, 2003), Japan (Oc-
tober 26, 2004), and Peru (June 23, 2001). Using
this procedure we retrieve training document col-
lections for 9 instances of airplane crashes, 5 in-
stances of earthquakes, 13 instances of presiden-
tial elections, and 6 instances of terrorist attacks.
4.2 Test Data
To test our system, we used document clusters
from the Topic Detection and Tracking (TDT) cor-
3http://news.bbc.co.uk/shared/bsp/search2/
advanced/news_ifs.stm
pus (Fiscus et al, 1999). Each TDT topic has a
topic label, such as Accidents or Natural Disas-
ters.4 These categories are broader than our do-
mains. Thus, we manually filtered the TDT topics
relevant to our four training domains (e.g., Acci-
dents matching Airplane Crashes). In this way, we
obtained TDT document clusters for 2 instances
of airplane crashes, 3 instances of earthquakes, 6
instances of presidential elections and 3 instances
of terrorist attacks. The number of the documents
corresponding to the instances varies greatly (from
two documents for one of the earthquakes up to
156 documents for one of the terrorist attacks).
This variation in the number of documents per
topic is typical for the TDT corpus. Many of the
current approaches of domain modeling collapse
together different instances and make the decision
on what information is important for a domain
based on this generalized corpus (Collier, 1998;
Barzilay and Lee, 2003; Sudo et al, 2003). We,
on the other hand, propose to cross-examine these
instances keeping them separated. Our goal is to
eliminate dependence on how well the corpus is
balanced and to avoid the possibility of greater
impact on the domain template of those instances
which have more documents.
5 Creating Templates
In this work we build domain templates around
verbs which are estimated to be important for the
domains. Using verbs as the starting point we
identify semantic dependencies within sentences.
In contrast to deep semantic analysis (Fillmore
and Baker, 2001; Gildea and Jurafsky, 2002; Prad-
han et al, 2004; Harabagiu and La?ca?tus?u, 2005;
Palmer et al, 2005) we rely only on corpus statis-
tics. We extract the most frequent syntactic sub-
trees which connect verbs to the lexemes used in
the same subtrees. These subtrees are used to cre-
ate domain templates.
For each of the four domains described in Sec-
tion 4, we automatically create domain templates
using the following algorithm.
Step 1: Estimate what verbs are important for
the domain under investigation. We initiate our
algorithm by calculating the probabilities for all
the verbs in the document collection for one do-
main ? e.g., the collection containing all the in-
stances in the domain of airplane crashes. We
4In our experiments we analyze TDT topics used in
TDT-2 and TDT-4 evaluations.
209
discard those verbs that are stop words (Salton,
1971). To take into consideration the distribution
of a verb among different instances of the domain,
we normalize this probability by its VIF value
(verb instance frequency), specifying in how many
domain instances this verb appears.
Score(vbi) =
countvbi?
vbj?comb coll countvbj
? VIF(vbi) (1)
VIF(vbi) = # of domain instances containing vbi# of all domain instances (2)
These verbs are estimated to be the most impor-
tant for the combined document collection for all
the domain instances. Thus, we build the domain
template around these verbs. Here are the top ten
verbs for the terrorist attack domain:
killed, told, found, injured, reported,
happened, blamed, arrested, died, linked.
Step 2: Parse those sentences which contain the
top 50 verbs. After we identify the 50 most impor-
tant verbs for the domain under analysis, we parse
all the sentences in the domain document collec-
tion containing these verbs with the Stanford syn-
tactic parser (Klein and Manning, 2002).
Step 3: Identify most frequent subtrees containing
the top 50 verbs. A domain template should con-
tain not only the most important actions for the do-
main, but also the entities that are linked to these
actions or to each other through these actions. The
lexemes referring to such entities can potentially
be used within the domain template slots. Thus,
we analyze those portions of the syntactic trees
which contain the verbs themselves plus other lex-
emes used in the same subtrees as the verbs. To do
this we use FREQuent Tree miner.5 This software
is an implementation of the algorithm presented
by (Abe et al, 2002; Zaki, 2002), which extracts
frequent ordered subtrees from a set of ordered
trees. Following (Sudo et al, 2003) we are inter-
ested only in the lexemes which are near neighbors
of the most frequent verbs. Thus, we look only for
those subtrees which contain the verbs themselves
and from four to ten tree nodes, where a node is
either a syntactic tag or a lexeme with its tag. We
analyze not only NPs which correspond to the sub-
ject or object of the verb, but other syntactic con-
stituents as well. For example, PPs can potentially
link the verb to locations or dates, and we want to
include this information into the template. Table 1
contains a sample of subtrees for the terrorist at-
tack domain mined from the sentences containing
5http://chasen.org/?taku/software/freqt/
nodes subtree
8 (SBAR(S(VP(VBD killed)(NP(QP(IN at))(NNS people)))))
8 (SBAR(S(VP(VBD killed)(NP(QP(JJS least))(NNS people)))))
5 (VP(ADVP)(VBD killed)(NP(NNS people)))
6 (VP(VBD killed)(NP(ADJP(JJ many))(NNS people)))
5 (VP(VP(VBD killed)(NP(NNS people))))
7 (VP(ADVP(NP))(VBD killed)(NP(CD 34)(NNS people)))
6 (VP(ADVP)(VBD killed)(NP(CD 34)(NNS people)))
Table 1: Sample subtrees for the terrorist attack domain.
the verb killed. The first column of Table 1 shows
how many nodes are in the subtree.
Step 4: Substitute named entities with their re-
spective tags. We are interested in analyzing a
whole domain, not just an instance of this do-
main. Thus, we substitute all the named entities
with their respective tags, and all the exact num-
bers with the tag NUMBER. We speculate that sub-
trees similar to those presented in Table 1 can
be extracted from a document collection repre-
senting any instance of a terrorist attack, with the
only difference being the exact number of causal-
ities. Later, however, we analyze the domain in-
stances separately to identity information typi-
cal for the domain. The procedure of substitut-
ing named entities with their respective tags previ-
ously proved to be useful for various tasks (Barzi-
lay and Lee, 2003; Sudo et al, 2003; Filatova and
Prager, 2005). To get named entity tags we used
BBN?s IdentiFinder (Bikel et al, 1999).
Step 5: Merge together the frequent subtrees. Fi-
nally, we merge together those subtrees which
are identical according to the information encoded
within them. This is a key step in our algorithm
which allows us to bring together subtrees from
different instances of the same domain. For exam-
ple, the information rendered by all the subtrees
from the bottom part of Table 1 is identical. Thus,
these subtrees can be merged into one which con-
tains the longest common pattern:
(VBD killed)(NP(NUMBER)(NNS people))
After this merging procedure we keep only those
subtrees for which each of the domain instances
has at least one of the subtrees from the initial set
of subtrees. This subtree should be used in the in-
stance at least twice. At this step, we make sure
that we keep in the template only the information
which is generally important for the domain rather
than only for a fraction of instances in this domain.
We also remove all the syntactic tags as we want
to make this pattern as general for the domain as
possible. A pattern without syntactic dependencies
contains a verb together with a prospective tem-
210
plate slot corresponding to this verb:
killed: (NUMBER) (NNS people)
In the above example, the prospective template
slots appear after the verb killed. In other cases the
domain slots appear in front of the verb. Two ex-
amples of such slots, for the presidential election
and earthquake domains, are shown below:
(PERSON) won
(NN earthquake) struck
The above examples show that it is not enough to
analyze only named entities, general nouns con-
tain important information as well. We term the
structure consisting of a verb together with the as-
sociated slots a slot structure. Here is a part of the
slot structure we get for the verb killed after cross-
examination of the terrorist attack instances:
killed (NUMBER) (NNS people)
(PERSON) killed
(NN suicide) killed
Slot structures are similar to verb frames, which
are manually created for the PropBank annota-
tion (Palmer et al, 2005).6 An example of the
PropBank frame for the verb to kill is:
Roleset kill.01 ?cause to die?:
Arg0:killerArg1:corpseArg2:instrument
The difference between the slot structure extracted
by our algorithm and the PropBank frame slots is
that the frame slots assign a semantic role to each
slot, while our algorithm gives either the type of
the named entity that should fill in this slot or puts
a particular noun into the slot (e.g., ORGANIZA-
TION, earthquake, people). An ideal domain tem-
plate should include semantic information but this
problem is outside of the scope of this paper.
Step 6: Creating domain templates. After we get
all the frequent subtrees containing the top 50 do-
main verbs, we merge all the subtrees correspond-
ing to the same verb and create a slot structure for
every verb as described in Step 5. The union of
such slot structures created for all the important
verbs in the domain is called the domain template.
From the created templates we remove the slots
which are used in all the domains. For example,
(PERSON) told.2
The presented algorithm can be used to create a
template for any domain. It does not require pre-
defined domain or world knowledge. We learn do-
main templates from cross-examining document
collections describing different instances of the
domain of interest.
6http://www.cs.rochester.edu/?gildea/Verbs/
6 Evaluation
The task we deal with is new and there is no well-
defined and standardized evaluation procedure for
it. Sudo et al (2003) evaluated how well their
IE patterns captured named entities of three pre-
defined types. We are interested in evaluating how
well we capture the major actions as well as their
constituent parts.
There is no set of domain templates which are
built according to a unique set of principles against
which we could compare our automatically cre-
ated templates. Thus, we need to create a gold
standard. In Section 6.1, we describe how the gold
standard is created. Then, in Section 6.2, we eval-
uate the quality of the automatically created tem-
plates by extracting clauses corresponding to the
templates and verifying how many answers from
the questions in the gold standard are answered by
the extracted clauses.
6.1 Stage 1. Information Included into
Templates: Interannotator Agreement
To create a gold standard we asked people to create
a list of questions which indicate what is important
for the domain description. Our decision to aim
for the lists of questions and not for the templates
themselves is based on the following considera-
tions: first, not all of our subjects are familiar with
the field of IE and thus, do not necessarily know
what an IE template is; second, our goal for this
evaluation is to estimate interannotator agreement
for capturing the important aspects for the domain
and not how well the subjects agree on the tem-
plate structure.
We asked our subjects to think of their expe-
rience of reading newswire articles about various
domains.7 Based on what they remember from this
experience, we asked them to come up with a list
of questions about a particular domain. We asked
them to come up with at most 20 questions cover-
ing the information they will be looking for given
an unseen news article about a new event in the
domain. We did not give them any input informa-
tion about the domain but allowed them to use any
sources to learn more information about the do-
main.
We had ten subjects, each of which created one
list of questions for one of the four domains under
7We thank Rod Adams, Cosmin-Adrian Bejan, Sasha
Blair-Goldensohn, Cyril Cerovic, David Elson, David Evans,
Ovidiu Fortu, Agustin Gravano, Lokesh Shresta, John Yundt-
Pacheco and Kapil Thadani for the submitted questions.
211
Jaccard metric
Domain subj1 and subj1 and subj2 and
subj2 (and subj3) MUC MUC
Airplane crash 0.54 - -
Earthquake 0.68 - -
Presidential Election 0.32 - -
Terrorist Attack 0.50 0.63 0.59
Table 2: Creating gold standard. Jaccard metric values for in-
terannotator agreement.
analysis. Thus, for the earthquake and terrorist at-
tack domains we got two lists of questions; for the
airplane crash and presidential election domains
we got three lists of questions.
After the questions lists were created we studied
the agreement among annotators on what infor-
mation they consider is important for the domain
and thus, should be included in the template. We
matched the questions created by different anno-
tators for the same domain. For some of the ques-
tions we had to make a judgement call on whether
it is a match or not. For example, the following
question created by one of the annotators for the
earthquake domain was:
Did the earthquake occur in a well-known area
for earthquakes (e.g. along the San Andreas
fault), or in an unexpected location?
We matched this question to the following three
questions created by the other annotator:
What is the geological localization?
Is it near a fault line?
Is it near volcanoes?
Usually, the degree of interannotator agreement
is estimated using Kappa. For this task, though,
Kappa statistics cannot be used as they require
knowledge of the expected or chance agreement,
which is not applicable to this task (Fleiss et al,
1981). To measure interannotator agreement we
use the Jaccard metric, which does not require
knowledge of the expected or chance agreement.
Table 2 shows the values of Jaccard metric for in-
terannotator agreement calculated for all four do-
mains. Jaccard metric values are calculated as
Jaccard(domaind) = |QS
d
i ? QSdj |
|QSdi ? QSdj |
(3)
where QSdi and QSdj are the sets of questions cre-
ated by subjects i and j for domain d. For the air-
plane crash and presidential election domains we
averaged the three pairwise Jaccard metric values.
The scores in Table 2 show that for some do-
mains the agreement is quite high (e.g., earth-
quake), while for other domains (e.g., presiden-
tial election) it is twice as low. This difference
in scores can be explained by the complexity of
the domains and by the differences in understand-
ing of these domains by different subjects. The
scores for the presidential election domain are pre-
dictably low as in different countries the roles of
presidents are very different: in some countries the
president is the head of the government with a lot
of power, while in other countries the president is
merely a ceremonial figure. In some countries the
presidents are elected by general voting while in
other countries, the presidents are elected by par-
liaments. These variations in the domain cause the
subjects to be interested in different issues of the
domain. Another issue that might influence the in-
terannotator agreement is the distribution of the
presidential election process in time. For example,
one of our subjects was clearly interested in the
pre-voting situation, such as debates between the
candidates, while another subject was interested
only in the outcome of the presidential election.
For the terrorist attack domain we also com-
pared the lists of questions we got from our sub-
jects with the terrorist attack template created by
experts for the MUC competition. In this template
we treated every slot as a separate question, ex-
cluding the first two slots which captured informa-
tion about the text from which the template fillers
were extracted and not about the domain. The re-
sults for this comparison are included in Table 2.
Differences in domain complexity were stud-
ied by IE researchers. Bagga (1997) suggests a
classification methodology to predict the syntac-
tic complexity of the domain-related facts. Hut-
tunen et al (2002) analyze how component sub-
events of the domain are linked together and dis-
cuss the factors which contribute to the domain
complexity.
6.2 Stage 2. Quality of the Automatically
Created Templates
In section 6.1 we showed that not all the domains
are equal. For some of the domains it is much eas-
ier to come to a consensus about what slots should
be present in the domain template than for others.
In this section we describe the evaluation of the
four automatically created templates.
Automatically created templates consist of slot
structures and are not easily readable by human
annotators. Thus, instead of direct evaluation of
the template quality, we evaluate the clauses ex-
tracted according to the created templates and
212
check whether these clauses contain the answers
to the questions created by the subjects during the
first stage of the evaluation. We extract the clauses
corresponding to the test instances according to
the following procedure:
1. Identify all the simple clauses in the docu-
ments corresponding to a particular test in-
stance (respective TDT topic). For example,
for the sentence
Her husband, Robert, survived Thursday?s
explosion in a Yemeni harbor that killed at
least six crew members and injured 35.
only one part is output:
that killed at least six crew members and
injured 35
2. For every domain template slot check all the
simple clauses in the instance (TDT topic)
under analysis. Find the shortest clause (or
sequence of clauses) which includes both the
verb and other words extracted for this slot in
their respective order. Add this clause to the
list of extracted clauses unless this clause has
been already added to this list.
3. Keep adding clauses to the list of extracted
clauses till all the template slots are analyzed
or the size of the list exceeds 20 clauses.
The key step in the above algorithm is Step 2. By
choosing the shortest simple clause or sequence
of simple clauses corresponding to a particular
template slot, we reduce the possibility of adding
more information to the output than is necessary
to cover each particular slot.
In Step 3 we keep only the first twenty clauses
so that the length of the output which potentially
contains an answer to the question of interest is not
larger than the number of questions provided by
each subject. The templates are created from the
slot structures extracted for the top 50 verbs. The
higher the estimated score of the verb (Eq. 1) for
the domain the closer to the top of the template the
slot structure corresponding to this verb will be.
We assume that the important information is more
likely to be covered by the slot structures that are
placed near the top of the template.
The evaluation results for the automatically cre-
ated templates are presented in Figure 1. We cal-
culate what average percentage of the questions is
covered by the outputs created according to the
domain templates. For every domain, we present
the percentage of the covered questions separately
for each annotator and for the intersection of ques-
tions (Section 6.1).
0.00%10.00%
20.00%30.00%
40.00%50.00%
60.00%70.00%
80.00%
Attack Earthquake Presidentialelection Plane crash
IntersectSubj1Subj2Subj3
Figure 1: Evaluation results.
For the questions common for all the annota-
tors we capture about 70% of the answers for
three out of four domains. After studying the re-
sults we noticed that for the earthquake domain
some questions did not result in a template slot
and thus, could not be covered by the extracted
clauses. Here are two of such questions:
Is it near a fault line?
Is it near volcanoes?
According to the template creation procedure,
which is centered around verbs, the chances that
extracted clauses would contain answers to these
questions are low. Indeed, only one of the three
sentence sets extracted for the three TDT earth-
quake topics contain an answer to one of these
questions.
Poor results for the presidential election domain
could be predicted from the Jaccard metric value
for interannotator agreement (Table 2). There is
considerable discrepancy in the questions created
by human annotators which can be attributed to the
great variation in the presidential election domain
itself. It must be also noted that most of the ques-
tions created for the presidential election domain
were clearly referring to the democratic election
procedure, while some of the TDT topics catego-
rized as Elections were about either election fraud
or about opposition taking over power without the
formal resignation of the previous president.
Overall, this evaluation shows that using au-
tomatically created domain templates we extract
sentences which contain a substantial part of the
important information expressed in questions for
that domain. For those domains which have small
diversity our coverage can be significantly higher.
7 Conclusions
In this paper, we presented a robust method for
data-driven discovery of the important fact-types
213
for a given domain. In contrast to supervised meth-
ods, the fact-types are not pre-specified. The re-
sulting slot structures can subsequently be used
to guide the generation of responses to questions
about new instances of the same domain. Our ap-
proach features the use of corpus statistics derived
from both lexical and syntactic analysis across
documents. A comparison of our system output
for four domains of interest shows that our ap-
proach can reliably predict the majority of infor-
mation that humans have indicated are of interest.
Our method is flexible: analyzing document col-
lections from different time periods or locations,
we can learn domain descriptions that are tailored
to those time periods and locations.
Acknowledgements. We would like to thank Re-
becca Passonneau and Julia Hirschberg for the
fruitful discussions at the early stages of this work;
Vasilis Vassalos for his suggestions on the eval-
uation instructions; Michel Galley, Agustin Gra-
vano, Panagiotis Ipeirotis and Kapil Thadani for
their enormous help with evaluation.
This material is based upon work supported
in part by the Advanced Research Devel-
opment Agency (ARDA) under Contract No.
NBCHC040040 and in part by the Defense Ad-
vanced Research Projects Agency (DARPA) under
Contract No. HR0011-06-C-0023. Any opinions,
findings and conclusions expressed in this mate-
rial are those of the authors and do not necessarily
reflect the views of ARDA and DARPA.
References
Kenji Abe, Shinji Kawasoe, Tatsuya Asai, Hiroki Arimura,
and Setsuo Arikawa. 2002. Optimized substructure dis-
covery for semi-structured data. In Proc. of PKDD.
Amit Bagga. 1997. Analyzing the complexity of a domain
with respect to an Information Extraction task. In Proc.
7th MUC.
Regina Barzilay and Lillian Lee. 2003. Learning to
paraphrase: An unsupervised approach using multiple-
sequence alignment. In Proc. of HLT/NAACL.
Daniel Bikel, Richard Schwartz, and Ralph Weischedel.
1999. An algorithm that learns what?s in a name. Ma-
chine Learning Journal Special Issue on Natural Lan-
guage Learning, 34:211?231.
Sasha Blair-Goldensohn, Kathleen McKeown, and An-
drew Hazen Schlaikjer, 2004. Answering Definitional
Questions: A Hybrid Approach. AAAI Press.
Robin Collier. 1998. Automatic Template Creation for Infor-
mation Extraction. Ph.D. thesis, University of Sheffield.
Pablo Duboue and Kathleen McKeown. 2003. Statistical
acquisition of content selection rules for natural language
generation. In Proc. of EMNLP.
Elena Filatova and Vasileios Hatzivassiloglou. 2003.
Domain-independent detection, extraction, and labeling of
atomic events. In Proc. of RANLP.
Elena Filatova and John Prager. 2005. Tell me what you do
and I?ll tell you what you are: Learning occupation-related
activities for biographies. In Proc. of EMNLP/HLT.
Charles Fillmore and Collin Baker. 2001. Frame semantics
for text understanding. In Proc. of WordNet and Other
Lexical Resources Workshop, NAACL.
Jon Fiscus, George Doddington, John Garofolo, and Alvin
Martin. 1999. NIST?s 1998 topic detection and tracking
evaluation (TDT2). In Proc. of the 1999 DARPA Broad-
cast News Workshop, pages 19?24.
Joseph Fleiss, Bruce Levin, and Myunghee Cho Paik, 1981.
Statistical Methods for Rates and Proportions. J. Wiley.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic la-
beling of semantic roles. Computational Linguistics,
28(3):245?288.
Sanda Harabagiu and Finley La?ca?tus?u. 2005. Topic themes
for multi-document summarization. In Proc. of SIGIR.
Sanda Harabagiu and Steven Maiorano. 2002. Multi-docu-
ment summarization with GISTexter. In Proc. of LREC.
Jerry Hobbs and David Israel. 1994. Principles of template
design. In Proc. of the HLT Workshop.
Silja Huttunen, Roman Yangarber, and Ralph Grishman.
2002. Complexity of event structure in IE scenarios. In
Proc. of COLING.
Dan Klein and Christopher Manning. 2002. Fast exact infer-
ence with a factored model for natural language parsing.
In Proc. of NIPS.
Hans Luhn. 1957. A statistical approach to mechanized en-
coding and searching of literary information. IBM Journal
of Research and Development, 1:309?317.
Elaine Marsh and Dennis Perzanowski. 1997. MUC-7 eval-
uation of IE technology: Overview of results. In Proc. of
the 7th MUC.
Boyan Onyshkevych. 1994. Issues and methodology for
template design for information extraction system. In
Proc. of the HLT Workshop.
Martha Palmer, Dan Gildea, and Paul Kingsbury. 2005. The
Proposition Bank: An annotated corpus of semantic roles.
Computational Linguistics, 31(1):71?106.
Sameer Pradhan, Wayne Ward, Kadri Hacioglu, James Mar-
tin, and Daniel Jurafsky. 2004. Shallow semantic parsing
using support vector machines. In Proc. of HLT/NAACL.
Dragomir Radev and Kathleen McKeown. 1998. Gener-
ating natural language summaries from multiple on-line
sources. Computational Linguistics, 24(3):469?500.
Ellen Riloff and Mark Schmelzenbach. 1998. An empirical
approach to conceptual case frame acquisition. In Proc. of
the 6th Workshop on Very Large Corpora.
Gerard Salton, 1971. The SMART retrieval system. Prentice-
Hall, NJ.
Kiyoshi Sudo, Satoshi Sekine, and Ralph Grishman. 2003.
An improved extraction pattern representation model for
automatic IE pattern acquisition. In Proc. of ACL.
Ralph Weischedel, Jinxi Xu, and Ana Licuanan, 2004. Hy-
brid Approach to Answering Biographical Questions.
AAAI Press.
Michael White, Tanya Korelsky, Claire Cardie, Vincent Ng,
David Pierce, and Kiri Wagstaff. 2001. Multi-document
summarization via information extraction. In Proc. of
HLT.
Mohammed Zaki. 2002. Efficiently mining frequent trees in
a forest. In Proc. of SIGKDD.
Liang Zhou, Miruna Ticrea, and Eduard Hovy. 2004. Multi-
document biography summarization. In Proc. of EMNLP.
214
Event-Based Extractive Summarization
Elena Filatova
Department of Computer Science
Columbia University
New York, NY 10027, USA
filatova@cs.columbia.edu
Vasileios Hatzivassiloglou
Center for Computational Learning Systems
Columbia University
New York, NY 10027, USA
vh@cs.columbia.edu
Abstract
Most approaches to extractive summarization define
a set of features upon which selection of sentences
is based, using algorithms independent of the fea-
tures themselves. We propose a new set of features
based on low-level, atomic events that describe rela-
tionships between important actors in a document or
set of documents. We investigate the effect this new
feature has on extractive summarization, compared
with a baseline feature set consisting of the words
in the input documents, and with state-of-the-art
summarization systems. Our experimental results
indicate that not only the event-based features of-
fer an improvement in summary quality over words
as features, but that this effect is more pronounced
for more sophisticated summarization methods that
avoid redundancy in the output.
1 Introduction
The main goal of extractive summarization can be
concisely formulated as extracting from the input
pieces of text which contain the information about
the most important concepts mentioned in the input
text or texts. This definition conceals a lot of impor-
tant issues that should be taken into consideration
in the process of summary construction. First, it is
necessary to identify the important concepts which
should be described in the summary. When those
important concepts are identified then the process
of summarization can be presented as:
1. Break the input text into textual units (sen-
tences, paragraphs, etc.).
2. See what concepts each textual unit covers.
3. Choose a particular textual unit for the output
according to the concepts present in all textual
units.
4. Continue choosing textual units until reaching
the desired length of the summary.
Some current summarization systems add a clus-
tering step, substituting the analysis of all the textual
units by the analysis of representative units from
each cluster. Clustering is helpful for avoiding rep-
etitions in the summary.
In this paper we propose a new representation
for concepts and correspondingly a new feature on
which summarization can be based. We adapt the
algorithm we proposed earlier (Filatova and Hatzi-
vassiloglou, 2003) for assigning to each sentence
a list of low-level, atomic events. These events
capture information about important named entities
for the input text or texts, and the relationships be-
tween these named entities. We also discuss a gen-
eral model which treats summarization as a three-
component problem, involving the identification of
the textual units into which the input text should
be broken and which are later used as the con-
stituent parts of the final summary, the textual fea-
tures which are associated with the important con-
cepts described in the input text, and the appropri-
ate algorithm for selecting the textual units to be in-
cluded into the summary.
We focus on the latter two of those steps and ex-
plore interdependencies between the choice of fea-
tures (step 2) and selection algorithm (step 3). We
experimentally test our hypothesis that event-based
features are helpful for summarization by compar-
ing the performance of three sentence selection al-
gorithms when we use such features versus the case
where we use another, widely used set of textual
features: the words in the input texts, weighted by
their tf*idf scores. The results establish that for
the majority of document sets in our test collection,
events outperform tf*idf for all algorithms consid-
ered. Furthermore, we show that this benefit is more
pronounced when the selection algorithm includes
steps to address potential repetition of information
in the output summary.
2 General Summarization Model
Many summarization systems (e.g., (Teufel and
Moens, 1997; McKeown et al, 1999; Lin and Hovy,
2000)) include two levels of analysis: the sentence
level, where every textual unit is scored according to
c1 c2 c3 c4 c5
t1 1 1 0 1 1
t2 1 0 0 1 0
t3 0 1 0 0 1
t4 1 0 1 1 1
Table 1: Matrix for Summarization Model
the concepts or features it covers, and the text level,
where, before being added to the final output, tex-
tual units are compared to each other on the basis of
those features.
In Section 1 we presented a four-step pipeline
for extractive summarization; existing summariza-
tion systems largely follow this pipeline, although
they introduce different approaches for every step
in it. We suggest a model that describes the extrac-
tive summarization task in general terms. Consider
the matrix in Table 1.
Rows of this matrix represent all textual units into
which the input text is divided. Columns represent
the concepts discovered for the input text. Every
concept is either absent or present in a given textual
unit. Each concept ci has also an associated weight
wi indicating the importance of this concept. These
weights can be used for scoring the textual units.
Thus, the input text and the important informa-
tion in it is mapped onto an m?n matrix. Using the
above matrix it is possible to formulate the extrac-
tive summarization problem as extracting the mini-
mal amount of textual units which cover all the con-
cepts that are interesting or important. To account
for the cost of long summaries, we can constrain the
total length of the summary, or balance it against the
total weight of covered concepts.
The presented model can be also used for com-
paring summaries consisting of different textual
units. For example, a summary consisting only of
textual unit t1 renders the same information as the
summary consisting of textual units t2 and t3. Both
these summaries cover the same set of concepts,
namely c1, c2 and c3. We explore properties of
this model in more detail in (Filatova and Hatzivas-
siloglou, 2004).
3 Associating Concepts with Features
Before extracting a summary, it is necessary to de-
fine what concepts in the input text are important
and should be covered by the output text. There is
no exact definition or even agreement between dif-
ferent approaches on what an important concept is.
In order to use the model of Section 2 one has to
approximate the notion of ?concept? with some tex-
tual features.
Current summarization approaches use text fea-
tures which give high scores to the textual units that
contain important information, and low scores to
those textual units which are not highly likely to
contain information worth to be included in the final
output.
There exist approaches that deal mainly with lex-
ical features, like tf*idf weighing of words in the
input text(s), words used in the titles and section
headings (Luhn, 1958; Edmundson, 1968), or the
presence or absence of certain cue phrases like sig-
nificant, important, and in conclusion (Kupiec et
al., 1995; Teufel and Moens, 1997). Other sys-
tems exploit the co-occurrence of particular con-
cepts (Barzilay and Elhadad, 1997; Lin and Hovy,
2000) or syntactic constraints between concepts
(McKeown et al, 1999). Concepts do not have to be
directly observable as text snippets?they can rep-
resent abstract properties that particular text units
may or may not satisfy, for example, status as a first
sentence in a paragraph or generally position in the
source text (Baxendale, 1958; Lin and Hovy, 1997).
Some summarization systems assume that the im-
portance of a sentence is derivable from a rhetorical
representation of the source text (Marcu, 1997).
The matrix representation of the previous section
offers a way to formalize the sharing of information
between textual units at the individual feature level.
Thus, this representation is most useful for content-
related concepts that should not be repeated in the
summary. The representation can however handle
independent features such as sentence position by
encoding them separately for each textual unit.
4 Atomic Events
Atomic events link major constituent parts of the
actions described in a text or collection of texts
through the verbs or action nouns labeling the event
itself. The idea behind this technique is that the
major constituent parts of events (participants, lo-
cations, times) are usually realized in text as named
entities. The more important the constituent part,
the more often the corresponding named entity is
mentioned.
Not all the constituent parts of events need to be
represented by named entities. For example, in an
airline crash it is important to report information
about the passengers and the crew. These are not
marked by named entities but are highly likely to be
among the most frequently used nouns. Thus, we
add the top ten most frequent nouns to the list of
named entities.
We use the algorithm for atomic event extraction
proposed in (Filatova and Hatzivassiloglou, 2003).
It involves the following steps:
1. Analyze each input sentence1 one at a time; ig-
nore sentences that do not contain at least two
named entities or frequent nouns.
2. Extract all the possible pairs of named enti-
ties/frequent nouns in the sentence, preserving
their order and all the words in between. We
call such pairs of named entities relations, and
the words in-between the named entities in a
relation connectors.
3. For each relation, count how many times this
relation is used in the input text(s).
4. Keep only connectors that are content verbs
or action nouns, according to WordNet?s (Fell-
baum, 1998) noun hierarchy. For each connec-
tor calculate how many times it is used for the
extracted relation.
After calculating the scores for all relations and
all connectors within each relation, we calculate
their normalized scores The normalized relation
score is the ratio of the count for the current rela-
tion (how many times we see the relation within a
sentence in the input) over the overall count of all
relations. The normalized connector score is the ra-
tio of the count for the current connector (how many
times we see this connector for the current relation)
over the overall count for all connectors for this re-
lation.
Thus, out of the above procedural definition, an
atomic event is a triplet of two named entities (or
frequent nouns) connected by a verb or an action-
denoting noun. To get a score for the atomic event
we multiply the normalized score for the relation by
the normalized score for the connector. The score
indicates how important the triplet is overall.
In the above approach to event detection we do
not address co-reference, neither we merge together
the triplets which describe the same event using
paraphrases, inflected forms and syntactic variants
(e.g., active/passive voice). Our method uses rel-
atively simple extraction techniques and shallow
statistics, but it is fully automatic and can serve as a
first approximation of the events in the input text(s).
Our approach to defining events is not the only
one proposed?this is a subject with substantial
work in linguistics, information retrieval, and infor-
mation extraction. In linguistics, events are often
defined at a fine-grained level as a matrix verb or a
single action noun like ?war? (Pustejovsky, 2000).
In contrast, recent work in information retrieval
1We earlier showed empirically (Filatova and Hatzivas-
siloglou, 2003) that a description of a single event is usually
bound within one sentence.
within the TDT framework has taken event to mean
essentially ?narrowly defined topic for search? (Al-
lan et al, 1998). Finally, for the information extrac-
tion community an event represents a template of re-
lationships between participants, times, and places
(Marsh and Perzanowski, 1997). It may be possible
to use these alternative models of events as a source
of content features.
We earlier established empirically (Filatova and
Hatzivassiloglou, 2003) that this technique for
atomic event extraction is useful for delineating the
major participants and their relationships from a set
of topically related input texts. For example, from a
collection of documents about an airplane crash the
algorithm assigns the highest score to atomic events
that link together the name of the airline, the source
and destination airports and the day when the crash
happened through the verb crashed or its synonyms.
It is thus plausible to explore the usefulness of these
event triplets as the concepts used in the model of
Section 2.
5 Textual Unit Selection
We have formulated the problem of extractive sum-
marization in terms of the matrix model, stating
that mapping concepts present in the input text onto
the textual units out of which the output is con-
structed can be accomplished by extracting the min-
imal amount of textual units which either cover
most of the important concepts. Every time we add
a new textual unit to the output it is possible to judge
what concepts in it are already covered in the final
summary. This observation can be used to avoid re-
dundancy: before adding a candidate textual unit to
the output summary, we check whether it contains
enough new important concepts.
We describe in this section several algorithms
for selecting appropriate textual units for the output
summary. These algorithms differ on whether they
take advantage of the redundancy reduction prop-
erty of our model, and on whether they prioritize im-
portant concepts individually or collectively. They
share, however, a common property: all of them op-
erate independently of the features chosen to repre-
sent important concepts, and thus can be used with
both our event-based features and other feature sets.
The comparison of the results allows us to empir-
ically determine whether event-based features can
help in summarization.
5.1 Static Greedy Algorithm
Our first text unit selection algorithm does not sup-
port any mechanism for avoiding redundant infor-
mation in the summary. Instead, it rates each textual
unit independently. Textual units are included in the
summary if and only if they cover lots of concepts.
More specifically,
1. For every textual unit, calculate the weight of
this textual unit as the sum of the weights of all
the concepts covered by this textual unit.
2. Choose the textual unit with the maximum
weight and add it to the final output.
3. Continue extracting other textual units in order
of total weight till we get the summary of the
desired length.
5.2 Avoiding Redundancy in the Summary
Two popular techniques for avoiding redundancy
in summarization are Maximal Marginal Relevance
(MMR) (Goldstein et al, 2000) and clustering
(McKeown et al, 1999). In MMR the determination
of redundancy is based mainly on the textual over-
lap between the sentence that is about to be added to
the output and the sentences that are already in the
output. Clustering offers an alternative: before start-
ing the selection process, the summarization system
clusters the input textual units. This step allows an-
alyzing one representative unit from each cluster in-
stead of all textual units.
We take advantage of the model matrix of Sec-
tion 2 to explore another way to avoid redundancy.
Rather than making decisions for each textual unit
independently, as in our Static Greedy Algorithm,
we globally select the subset of textual units that
cover the most concepts (i.e., information) present
in the input. Then our task becomes very similar to
a classic theory problem, Maximum Coverage.
Given C , a finite set of weighted elements, a col-
lection T of subsets of C , and a parameter k, the
maximum coverage problem is to find k members
of T such that the total weight of the elements cov-
ered (i.e., belonging to the k members of the solu-
tion) is maximized. This problem is NP-hard, as it
can be reduced to the well-known set cover problem
(Hochbaum, 1997). Thus, we know only approxi-
mation algorithms solving this problem in polyno-
mial time.
Hochbaum (1997) reports that a greedy algorithm
is the best possible polynomial approximation algo-
rithm for this problem. This algorithm iteratively
adds to the solution S the set ti ? T that locally
maximizes the increase in the total weight of ele-
ments covered by S ? ti. The algorithm gives a so-
lution with weight at least 1/(1 ? e) of the optimal
solution?s total weight.
5.3 Adaptive Greedy Algorithm
The greedy algorithm for the maximum coverage
problem is not directly applicable to summariza-
tion, because the formulation of maximum cover-
age assumes that any combination of k sets ti (i.e.,
k sentences) is equally good as long as they cover
the same total weight of concepts. A more realistic
limitation for the summarization task is to aim for a
fixed total length of the summary, rather than a fixed
total number of sentences; this approach has been
adopted in several evaluation efforts, including the
Document Understanding Conferences (DUC). We
consequently modify the greedy algorithm for the
maximum coverage problem to obtain the following
adaptive greedy algorithm for summarization:
1. For each textual unit calculate its weight as the
sum of weights of all concepts it covers.
2. Choose the textual unit with the maximum
weight and add it to the output. Add the con-
cepts covered by this textual unit to the list of
concepts covered in the final output.
3. Recalculate the weights of the textual units:
subtract from each unit?s weight the weight of
all concepts in it that are already covered in the
output.
4. Continue extracting text units in order of their
total weight (going back to step 2) until the
summary is of the desired length.
5.4 Modified Adaptive Greedy Algorithm
The adaptive greedy algorithm described above pri-
oritizes sentences according to the total weight of
concepts they cover. While this is a reasonable ap-
proach, an alternative is to give increased priority to
concepts that are individually important, so that sen-
tences mentioning them have a chance of being in-
cluded in the output even if they don?t contain other
important concepts. We have developed the fol-
lowing variation of our adaptive greedy algorithm,
termed the modified greedy algorithm:
1. For every textual unit calculate its weight as
the sum of weights of all concepts it covers.
2. Consider only those textual units that contain
the concept with the highest weight that has not
yet been covered. Out of these, choose the one
with highest total weight and add it to the final
output. Add the concepts which are covered by
this textual unit to the list of concepts covered
in the final output.
3. Recalculate the weights of the textual units:
subtract from each unit?s weight the weight of
all concepts in it that are already covered in the
output.
4. Continue extracting textual units, going back
to step 2 each time, until we get a summary of
the desired length.
The modified greedy algorithm has the same
mechanism for avoiding redundancy as the adaptive
greedy one, while according a somewhat different
priority to individual sentences (weight of most im-
portant concepts versus just total weight).
6 Experiments
We chose as our input data the document sets
used in the evaluation of multidocument summa-
rization during the first Document Understanding
Conference (DUC), organized by NIST (Harman
and Marcu, 2001). This collection contains 30 test
document sets, each with approximately 10 news
stories on different events; document sets vary sig-
nificantly in their internal coherence. For each doc-
ument set three human-constructed summaries are
provided for each of the target lengths of 50, 100,
200, and 400 words. We selected DUC 2001 be-
cause ideal summaries are available for multiple
lengths.
Concepts and Textual Units Our textual units
are sentences, while the features representing con-
cepts are either atomic events, as described in Sec-
tion 4, or a fairly basic and widely used set of
lexical features, namely the list of words present
in each input text. The algorithm for extracting
event triplets assigns a weight to each such triplet,
while for words we used as weights their tf*idf val-
ues, taking idf values from http://elib.cs.
berkeley.edu/docfreq/.
Evaluation Metric Given the difficulties in com-
ing up with a universally accepted evaluation mea-
sure for summarization, and the fact that obtain-
ing judgments by humans is time-consuming and
labor-intensive, we adopted an automated pro-
cess for comparing system-produced summaries to
?ideal? summaries written by humans. The method,
ROUGE (Lin and Hovy, 2003), is based on n-gram
overlap between the system-produced and ideal
summaries. As such, it is a recall-based measure,
and it requires that the length of the summaries be
controlled to allow meaningful comparisons.
ROUGE can be readily applied to compare the
performance of different systems on the same set
of documents, assuming that ideal summaries are
available for those documents. At the same time,
ROUGE evaluation has not yet been tested exten-
sively, and ROUGE scores are difficult to interpret
as they are not absolute and not comparable across
source document sets.
50 100 200 400
events better 53.3% 63.3% 80.0% 80.0%
tf*idf better 23.3% 26.7% 20.0% 20.0%
equal 23.3% 10.0% 0.0% 0.0%
Table 2: Static greedy algorithm, events versus
tf*idf
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0.45
1 3 5 7 9 11 13 15 17 19 21 23 25 27 29
DUC document sets
R
O
U
GE
 
s
c
o
re
s
events
tf*idf
Figure 1: ROUGE scores for 400-word summaries
for static greedy algorithm, events versus tf*idf
50 100 200 400
events better 53.3% 66.7% 86.7% 80.0%
tf*idf better 23.3% 20.0% 13.3% 20.0%
equal 23.3% 13.3% 0.0% 0.0%
Table 3: Adaptive greedy algorithm, events versus
tf*idf
In our comparison, we used as reference sum-
maries those created by NIST assessors for the DUC
task of generic summarization. The human annota-
tors may not have created the same models if asked
for summaries describing the major events in the in-
put texts instead of generic summaries.
Summary Length For a given set of features and
selection algorithm we get a sorted list of sen-
tences extracted according to that particular algo-
rithm. Then, for each DUC document set we create
four summaries of length 50, 100, 200, and 400. In
all the suggested methods a whole sentence is added
at every step. We extracted exactly 50, 100, 200,
and 400 words out of the top sentences (truncating
the last sentence if necessary).
6.1 Results: Static Greedy Algorithm
In our first experiment we use the static greedy al-
gorithm to create summaries of various lengths. Ta-
ble 2 shows in how many cases out of the 30 docu-
ment sets the summary created according to atomic
events receives a higher or lower ROUGE score
than the summary created according to tf*idf fea-
tures (rows ?events better? and ?tf*idf better? re-
spectively). Row equal indicates how many of the
30 cases both systems produce results with the same
ROUGE score. We chose to report the number of
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0.45
1 3 5 7 9 11 13 15 17 19 21 23 25 27 29
DUC document sets
R
O
U
GE
 
s
c
o
re
s
events
tf*idf 
Figure 2: ROUGE scores for 400-word summaries
for adaptive greedy algorithm, events versus tf*idf
times each system is better rather than the average
ROUGE score in each case because ROUGE scores
depend on each particular document set.
It is clear from Table 2 that the summaries cre-
ated using atomic events are better in the majority
of cases than the summaries created using tf*idf.
Figure 1 shows ROUGE scores for 400-word sum-
maries. Although in most cases the performance of
the event-based summarizer is higher than the per-
formance based on tf*idf scores, for some docu-
ment sets tf*idf gives the better scores. This phe-
nomenon can be explained through an additional
analysis of document sets according to their inter-
nal coherence. Atomic event extraction works best
for a collection of documents with well-defined con-
stituent parts of events and where documents are
clustered around one specific major event. For such
document sets atomic events are good features for
basing the summary on. In contrast, some DUC
2001 document sets describe a succession of mul-
tiple events linked in time or of different events of
the same type (e.g., Clarence Thomas? ascendancy
to the Supreme Court, document set 7 in Figure 1,
or the history of airplane crashes, document set 30
in Figure 1). In such cases, a lot of different par-
ticipants are mentioned with only few common ele-
ments (e.g., Clarence Thomas himself). Thus, most
of the atomic events have similar low weights and
it is difficult to identify those atomic events that can
point out the most important textual units.
6.2 Results: Adaptive Greedy Algorithm
For the second experiment we used the adaptive
greedy algorithm, which accounts for information
overlap across sentences in the summary. As in
the case of the simpler static greedy algorithm, we
observe that events lead to a better performance in
most document sets than tf*idf (Table 3). Table 3
is in fact similar to Table 2, with slightly increased
numbers of document sets for which events receive
higher ROUGE scores for the 100 and 200-word
50 100 200 400
static better 0.0% 3.3% 20.0% 23.3%
adaptive better 10.0% 16.7% 26.6% 40.0%
equal 90.0% 80.0% 53.3% 36.7%
Table 4: Adaptive greedy algorithm versus static
greedy algorithm, using events as features
-0.15
-0.1
-0.05
0
0.05
0.1
0.15
0.2
1 3 5 7 9 11 13 15 17 19 21 23 25 27 29
DUC document sets
R
O
U
GE
 
s
c
o
re
 
ga
in
adaptive
static
Figure 3: Gain in ROUGE scores (400-word sum-
maries) when using events instead of tf*idf for the
static and adaptive greedy algorithms
50 100 200 400
static better 3.3% 26.7% 43.3% 50.0%
adaptive better 3.3% 13.3% 30.0% 50.0%
equal 93.3% 60.0% 26.7% 0.0%
Table 5: Adaptive greedy algorithm versus static
greedy algorithm, using tf*idf as features
summaries. It is interesting to see that the differ-
ence between the ROUGE scores for the summariz-
ers based on atomic events and tf*idf features be-
comes more distinct when the adaptive greedy al-
gorithm is used; Figure 2 shows this for 400-word
summaries.
As Table 4 shows, the usage of the adaptive
greedy algorithm improves the performance of a
summarizer based on atomic events in comparison
to the static greedy algorithm. In contrast, the re-
verse is true when tf*idf is used (Table 5). Figure 3
shows the change in ROUGE scores that the intro-
duction of the adaptive algorithm offers for 400-
word summaries. This indicates that tf*idf is not
compatible with our information redundancy com-
ponent; a likely explanation is that words are corre-
lated, and the presence of an important word makes
other words in the same sentence also potentially
important, a fact not captured by the tf*idf feature.
Events, on the other hand, exhibit less of a depen-
dence on each other, since each triplet captures a
specific interaction between two entities.
6.3 Results: Modified Greedy Algorithm
In the case of the modified adaptive greedy algo-
rithm we see improvement in performance in com-
50 100 200 400
static better 43.3% 43.3% 36.7% 43.3%
modified better 43.3% 56.7% 63.3% 56.7%
equal 13.3% 0.0% 0.0% 0.0%
Table 6: Modified adaptive greedy algorithm versus
static greedy algorithm, using events as features
50 100 200 400
static better 6.7% 26.7% 36.7% 26.7%
modified better 30.0% 40.0% 56.7% 73.3%
equal 63.3% 33.3% 6.7% 0.0%
Table 7: Modified adaptive greedy algorithm versus
static greedy algorithm, using tf*idf as features
50 100 200 400
events better 56.7% 70.0% 80.0% 66.6%
tf*idf better 33.3% 30.0% 20.0% 33.3%
equal 10.0% 0.0% 0.0% 0.0%
Table 8: Modified adaptive greedy algorithm, events
versus tf*idf
parison with the summarizers using the static greedy
algorithm for both events and tf*idf (Tables 6 and
7). In other words, the prioritization of individ-
ual important concepts addresses the correlation be-
tween words and allows the summarizer to benefit
from redundancy reduction even when using tf*idf
as the features. The modified adaptive algorithm of-
fers a slight improvement in ROUGE scores over
the unmodified adaptive algorithm. Also, as Table 8
makes clear, events remain the better feature choice
over tf*idf.
6.4 Results: Comparison with DUC systems
For our final experiment we used the 30 test doc-
ument sets provided for DUC 2003 competition,
for which the summaries produced by participat-
ing summarization systems were also released. In
DUC 2003 the task was to create summaries only of
length 100.
We calculated ROUGE scores for the released
summaries created by DUC participants and com-
pared them to the scores of our system with atomic
events as features and adaptive greedy algorithm as
the filtering method. In 14 out of 30 cases our sys-
tem outperforms the median of the scores of all the
15 participating systems over that specific document
set. We view this comparison as quite encourag-
ing, as our system does not employ any of the ad-
ditional features (such as sentence position or time
information) used by the best DUC summarization
systems, nor was it adapted to the DUC domain.
Again, the suitability (and relative performance) of
the event-based summarizer varies according to the
type of documents being summarized, indicating
that using our approach for a subset of document
sets is more appropriate. For example, our system
scored below all the other systems for the docu-
ment set about a meteor shower, which included a
lot of background information and no well-defined
constituents of events. On the contrary, our sys-
tem performed better than any DUC system for the
document set describing an abortion-related murder,
where it was clear who was killed and where and
when it happened.
7 Conclusion
We have introduced atomic events as a feature that
can be automatically extracted from text and used
for summarization, and described algorithms that
utilize this feature to select sentences for the sum-
mary while minimizing the overlap of information
in the output. Our experimental results indicate that
events are indeed an effective feature, at least in
comparison with words in the input texts that form
the basis of many of current summarizers? feature
sets. With all three of our summarization algo-
rithms, we achieved a gain in performance when
using events. This gain was actually more pro-
nounced with the more sophisticated sentence se-
lection methods, establishing that events also ex-
hibit less interdependence than features based di-
rectly on words. The advantage was also larger in
longer summaries.
Our approach to defining and extracting events
can be improved in many ways. We are currently
looking at ways of matching connectors that are
similar in meaning, representing paraphrases of the
same event, and methods for detecting and prioritiz-
ing special event components such as time and loca-
tion phrases. We are also considering merging infor-
mation across many related atomic events to a more
structured representation for each event, and allow-
ing for partial matches between such structures and
input sentences.
8 Acknowledgements
We wish to thank Rocco Servedio and Mihalis
Yannakakis for valuable discussions of theoreti-
cal foundations of the set cover problem. We
also thank Kathy McKeown and Noemie Elhadad
for comments on an earlier version. This work
was supported by ARDA under Advanced Question
Answering for Intelligence (AQUAINT) project
MDA908-02-C-0008. Any opinions, findings, or
recommendations are those of the authors.
References
James Allan, Jaime Carbonell, George Dodding-
ton, Jonathan Yamron, and Yiming Yang. 1998.
Topic detection and tracking plot study: Final re-
port. In Proceedings of the DARPA Broadcast
News Transscription Workshop, April.
Regina Barzilay and Michael Elhadad. 1997. Us-
ing lexical chains for text summarization. In Pro-
ceedings of the ACL/EACL 1997 Workshop on
Intelligent Scalable Text Summarizaion, Madrid,
Spain, July.
P. B. Baxendale. 1958. Machine-made index for
technical literature?An experiment. IBM Jour-
nal of Research and Development, 2:354?361.
H. P. Edmundson. 1968. New methods in automatic
extracting. Journal of the Association for Com-
puting Machinary, 23(1):264?285, April.
Christiane Fellbaum, editor. 1998. WordNet: An
Electronic Lexical Database. MIT Press.
Elena Filatova and Vasileios Hatzivassiloglou.
2003. Domain-independent detection, extraction,
and labeling of atomic events. In Proceedings
of RANLP, pages 145?152, Borovetz, Bulgaria,
September.
Elena Filatova and Vasileios Hatzivassiloglou.
2004. A formal model for information selection
in multi-sentence text extraction. In Proceedings
of COLING, Geneva, Switzerland, August.
Jade Goldstein, Vibhu Mittal, Jaime Carbonell, and
Jamie Callan. 2000. Creating and evaluating
multi-document sentence extract summaries. In
Proceedings of the 9th CIKM Conference, pages
165?172.
Donna Harman and Daniel Marcu, editors. 2001.
Proceedings of the Document Understanding
Conference (DUC). NIST, New Orleans, USA,
September.
Dorit S. Hochbaum. 1997. Approximating cov-
ering and packing problems: Set cover, vertex
cover, independent set, and related problems. In
Dorit S. Hochbaum, editor, Approximation Al-
gorithms for NP-hard Problems, pages 94?143.
PWS Publishing Company, Boston, MA.
Julian Kupiec, Jan Pedersen, and Francine Chen.
1995. A trainable document summarizer. In Pro-
ceedings of the 18th ACM SIGIR Conference,
pages 68?73, Seattle, Washington, May.
Chin-Yew Lin and Eduard Hovy. 1997. Identify-
ing topic by position. In Proceedings of the 5th
ANLP Conference, Washington, DC.
Chin-Yew Lin and Eduard Hovy. 2000. The au-
tomated acquisition of topic signatures for text
summarization. In Proceedings of the COLING
Conference, Saarbru?cken, Germany, July.
Chin-Yew Lin and Eduard Hovy. 2003. Auto-
matic evaluation of summaries using n-gram co-
occurrence statistics. In Proceedings of HLT-
NAACL, Edmonton, Canada, May.
H. P. Luhn. 1958. The automatic creation of lit-
erature abstracts. IBM Journal of Research and
Development, 2(2):159?165, April.
Daniel Marcu. 1997. From discourse struc-
tures to text summaries. In Proceedings of the
ACL/EACL 1997 Workshop on Intelligent Scal-
able Text Summarizaion, pages 82?88, Madrid,
Spain, July.
E. Marsh and D. Perzanowski. 1997. MUC-7 eval-
uation of IE technology: Overview of results. In
Proceedings of MUC-7.
Kathleen R. McKeown, Judith L. Klavans, Vasileios
Hatzivassiloglou, Regina Barzilay, and Eleazar
Eskin. 1999. Towards multidocument sum-
marization by reformulation: Progress and
prospects. In Proceedings of AAAI.
James Pustejovsky, 2000. Events and the Seman-
tics of Opposition, pages 445?482. CSLI Publi-
cations.
Simone Teufel and Marc Moens. 1997. Sentence
extraction as a classification task. In Proceed-
ings of the ACL/EACL 1997 Workshop on Intelli-
gent Scalable Text Summarization, pages 58?65,
Madrid, Spain, July.
Proceedings of CLIAWS3, Third International Cross Lingual Information Access Workshop, pages 30?37,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Directions for Exploiting Asymmetries in Multilingual Wikipedia
Elena Filatova
Computer and Information
Sciences Department
Fordham University
Bronx, NY 10458, USA
filatova@cis.fordham.edu
Abstract
Multilingual Wikipedia has been used exten-
sively for a variety Natural Language Pro-
cessing (NLP) tasks. Many Wikipedia entries
(people, locations, events, etc.) have descrip-
tions in several languages. These descriptions,
however, are not identical. On the contrary,
descriptions in different languages created for
the same Wikipedia entry can vary greatly in
terms of description length and information
choice. Keeping these peculiarities in mind is
necessary while using multilingual Wikipedia
as a corpus for training and testing NLP ap-
plications. In this paper we present prelimi-
nary results on quantifying Wikipedia multi-
linguality. Our results support the observation
about the substantial variation in descriptions
of Wikipedia entries created in different lan-
guages. However, we believe that asymme-
tries in multilingual Wikipedia do not make
Wikipedia an undesirable corpus for NLP ap-
plications training. On the contrary, we out-
line research directions that can utilize multi-
lingual Wikipedia asymmetries to bridge the
communication gaps in multilingual societies.
1 Introduction
Multilingual parallel corpora such as translations of
fiction, European parliament proceedings, Canadian
parliament proceedings, the Dutch parallel corpus
are being used for training machine translation and
paraphrase extraction systems. All of these corpora
are parallel corpora.
Parallel corpora contain the same information
translated from one language (the source language
of the text) into a set of pre-specified languages with
the goal of preserving the information covered in
the source language document. Translators work-
ing with fiction also carefully preserve the stylistic
details of the original text.
Parallel corpora are a valuable resource for train-
ing NLP tools. However, they exist only for a small
number of language pairs and usually in a specific
context (e.g., legal documents, parliamentary notes).
Recently NLP community expressed a lot of interest
in studying other types of multilingual corpora.
The largest multilingual corpus known at the mo-
ment is World Wide Web (WWW). One part of par-
ticular interest is the on-line encyclopedia-style site,
Wikipedia.1 Most Wikipedia entries (people, loca-
tions, events, etc.) have descriptions in different lan-
guages. However, Wikipedia is not a parallel cor-
pus as these descriptions are not translations of a
Wikipedia article from one language into another.
Rather, Wikipedia articles in different languages are
independently created by different users.
Wikipedia does not have any filtering on who can
write and edit Wikipedia articles. In contrast to pro-
fessional encyclopedias (like Encyclopedia Britan-
nica), Wikipedia authors and editors are not nec-
essarily experts in the field for which they create
and edit Wikipedia articles. The trustworthiness
of Wikipedia is questioned by many people (Keen,
2007).
The multilinguality of Wikipedia makes this situ-
ation even more convoluted as the sets of Wikipedia
contributors for different languages are not the same.
1http://www.wikipedia.org/
30
Moreover, these sets might not even intersect. It
is unclear how similar or different descriptions of
a particular Wikipedia entry in different languages
are. Knowing that there are differences in descrip-
tions for the same entry and the ability to identify
these differences is essential for successful commu-
nication in multilingual societies.
In this paper we present a preliminary study of the
asymmetries in a subset of multilingual Wikipedia.
We analyze the number of languages in which the
Wikipedia entry descriptions are created; and the
length variation for the same entry descriptions cre-
ated in different languages. We believe that this in-
formation can be helpful for understanding asymme-
tries in multilingual Wikipedia. These asymmetries,
in turn, can be used by NLP researchers for training
summarization systems, and contradiction detection
systems.
The rest of the paper is structured as follows. In
Section 2 we describe related work, including the
work on utilizing parallel corpora. In Section 3
we provide examples of our analysis for several
Wikipedia entries. In Section 4 we describe our cor-
pus, and the systematic analysis performed on this
corpus. In Section 5 we draw conclusions based on
the collected statistics and outline avenues for our
future research.
2 Related Work
There exist several types of multilingual corpora
(e.g., parallel, comparable) that are used in the NLP
community. These corpora vary in their nature ac-
cording to the tasks for which these corpora were
created.
Corpora developed for multilingual and cross-
lingual question-answering (QA), information re-
trieval (IR), and information extraction (IE) tasks
are typically compilations of documents on related
subjects written in different languages. Documents
in such corpora rarely have counterparts in all the
languages presented in the corpus (CLEF, 2000;
Magnini et al, 2003).
Parallel multilingual corpora such as Canadian
parliament proceedings (Germann, 2001), European
parliament proceedings (Koehn, 2005), the Dutch
parallel corpus (Macken et al, 2007), JRC-ACQUIS
Multilingual Parallel Corpus (Steinberger et al,
2006), and so on contain documents that are exact
translations of the source documents.
Understanding the corpus nature allows systems
to utilize different aspects of multilingual corpora.
For example, Barzilay et al (2001) use several trans-
lations of the French text of Gustave Flaubert?s
novel Madame Bovary into English to mine a corpus
of English paraphrases. Thus, they utilize the cre-
ativity and language expertise of professional trans-
lators who used different wordings to convey not
only the meaning but also the stylistic peculiarities
of Flaubert?s French text into English.
Parallel corpora are a valuable resource for train-
ing NLP tools. However, they exist only for a small
number of language pairs and usually in a specific
context (e.g., legal documents, parliamentary notes).
Recently NLP community expressed a lot of inter-
est in studying comparable corpora. Workshops on
building and using comparable corpora have become
a part of NLP conferences (LREC, 2008; ACL,
2009). A comparable corpus is defined as a set of
documents in one to many languages, that are com-
parable in content and form in various degrees and
dimensions.
Wikipedia entries can have descriptions in several
languages independently created for each language.
Thus, Wikipedia can be considered a comparable
corpus.
Wikipedia is used in QA for answer extraction
and verification (Ahn et al, 2005; Buscaldi and
Rosso, 2006; Ko et al, 2007). In summarization,
Wikipedia articles structure is used to learn the fea-
tures for summary generation (Baidsy et al, 2008).
Several NLP systems utilize the Wikipedia multi-
linguality property. Adafre et al (2006) analyze the
possibility of constructing an English-Dutch parallel
corpus by suggesting two ways of looking for sim-
ilar sentences in Wikipedia pages (using matching
translations and hyperlinks). Richman et al (2008)
utilize multilingual characteristics of Wikipedia to
annotate a large corpus of text with Named Entity
tags. Multilingual Wikipedia has been used to fa-
cilitate cross-language IR (Scho?nhofen et al, 2007)
and to perform cross-lingual QA (Ferra?ndez et al,
2007).
One of the first attempts to analyze similarities
and differences in multilingual Wikipedia is de-
scribed in Adar et al (2009) where the main goal
31
is to use self-supervised learning to align or/and cre-
ate new Wikipedia infoboxes across four languages
(English, Spanish, French, German). Wikipedia
infoboxes contain a small number of facts about
Wikipedia entries in a semi-structured format.
3 Analysis of Multilingual Wikipedia
Entry Examples
Wikipedia is a resource generated by collaborative
effort of those who are willing to contribute their ex-
pertise and ideas about a wide variety of subjects.
Wikipedia entries can have descriptions in one or
several languages. Currently, Wikipedia has articles
in more than 200 languages. Table 1 presents infor-
mation about the languages that have the most ar-
ticles in Wikipedia: the number of languages, the
language name, and the Internet Engineering Task
Force (IETF) standard language tag.2
English is the language having the most number
of Wikipedia descriptions, however, this does not
mean that all the Wikipedia entries have descriptions
in English. For example, entries about people, lo-
cations, events, etc. famous or/and important only
within a community speaking in a particular lan-
guage are not likely to have articles in many lan-
guages. Below, we list a few examples that illustrate
this point. Of course, more work is required to quan-
tify the frequency of such entries.
? the Wikipedia entry about Mexican singer and
actress Roc??o Banquells has only one descrip-
tion: in Spanish;
? the Wikipedia entry about a mountain ski re-
sort Falakro in northern Greece has descrip-
tions in four languages: Bulgarian, English,
Greek, Nynorsk (one of the two official Nor-
wegian standard languages);
? the Wikipedia entry about Prioksko-Terrasny
Nature Biosphere Reserve, a Russia?s small-
est nature reserve, has descriptions in two lan-
guages: Russian and English;
2http://en.wikipedia.org/wiki/List_of_
Wikipedias
Wikipedia is changing constantly. All the quotes and examples
from Wikipedia presented and analyzed in this paper were
collected on February 10, 2009, between 14:00 and 21:00 PST.
Number or Articles Language IETF Tag
2,750,000+ English en
750,000+ German de
French fr
500,000+ Japanese jp
Polish pl
Italian it
Dutch nl
Table 1: Language editions of Wikipedia by number of
articles.
? the Wikipedia entry about a Kazakhstani fig-
ure skater Denis Ten who is of partial Korean
descent has descriptions in four languages: En-
glish, Japanese, Korean, and Russian.
At the same time, Wikipedia entries that are im-
portant or interesting for people from many commu-
nities speaking different languages have articles in
a variety of languages. For example, Newton?s law
of universal gravitation is a fundamental nature law
and has descriptions in 30 languages. Interestingly,
the Wikipedia entry about Isaac Newton who first
formulated the law of universal gravitation and who
is know all over the world has descriptions in 111
different languages.
However, even if a Wikipedia entry has arti-
cles in many languages, the information covered by
these articles can differ substantially. The two main
sources of differences are:
? the amount of the information covered by the
Wikipedia articles (the length of the Wikipedia
articles);
? the choice of the information covered by the
Wikipedia articles.
For example, Wikipedia entry about Isadora Dun-
can has descriptions in 44 languages. The length of
the descriptions about Isadora Duncan is different
for every language: 127 sentences for the article in
English; 77 - for French; 37 - for Russian, 1 - for
Greek, etc. The question arises: whether a shorter
article can be considered a summary of a longer arti-
cle, or whether a shorter article might contain infor-
mation that is either not covered in a longer article
or contradicts the information in the longer article.
32
Isadora Duncan was a American-born dancer who
was very popular in Europe and was married to a
Russian poet, Sergey Esenin. Certain amount of in-
formation facts (i.e., major biography dates) about
Isadora Duncan are repeated in the articles in ev-
ery language. However, shorter articles are not nec-
essarily summaries of longer articles. For exam-
ple, the article in Russian that is almost four time
shorter than the articles in English, contains infor-
mation that is not covered in the articles written in
English. The same can be noted about articles in
French and Spanish.
In this paper, we analyze the distribution of lan-
guages used in Wikipedia for the list of 48 people in
the DUC 2004 biography generation task. We ana-
lyze, the number of languages that contain articles
for each of the 48 DUC 2004 people. We also ana-
lyze the distribution of the lengths for the descrip-
tions in different languages. We believe that this
statistics is important for the understanding of the
Wikipedia multilinguality nature and can be used by
many NLP applications. Several NLP applications
that can leverage this information are listed in Sec-
tion 5.
4 Analysis of Wikipedia Multilinguality
In this paper, we propose a framework to quantify
the multilinguality aspect of Wikipedia. In the cur-
rent work we use a small portion of Wikipedia. Ana-
lyzing only a portion of Wikipedia allows us to com-
pare in detail the multilinguality aspect for all the
Wikipedia entries in our data set.
4.1 Data Set
For our analysis, we used the list of people created
for the Task 5 of DUC 2004: biography generation
task (48 people).3
First, we downloaded from Wikipedia all the arti-
cles in all the languages corresponding to each per-
son from the DUC 2004 evaluation set. For our
analysis we used Wikitext, the text that is used by
Wikipedia authors and editors. Wikitext complies
with the wiki markup language and can be pro-
cessed by the Wikimedia content manager system
into HTML which can then be viewed in a browser.
This is the text that can be obtained through the
3http://duc.nist.gov/duc2004/tasks.html/
Wikipedia dumps.4 For our analysis we removed
from the wikitext all the markup tags and tabular in-
formation (e.g., infoboxes and tables) and kept only
plain text. There is no commonly accepted standard
wikitext language, thus our final text had a certain
amount of noise which, however, does not affect the
conclusions drawn from our analysis.
For this work, for each Wikipedia entry (i.e.,
DUC 2004 person) we downloaded the correspond-
ing descriptions in all the languages, including sim-
ple English, Esperanto, Latin, etc. To facilitate the
comparison of descriptions written in different lan-
guages we used the Google machine translation sys-
tem5 to translate the downloaded descriptions into
English. The number of languages currently covered
by the Google translation system (41 language) is
smaller than the number of languages in which there
exist Wikipedia articles (265 languages). However,
we believe that using for cross-lingual analysis de-
scriptions only in those languages that can be han-
dled by the Google translation system does not af-
fect the generality of our conclusions.
4.2 Data Processing Tools
After the Wikipedia descriptions for each person
from the DUC 2004 set were collected and trans-
lated, we divided the description texts into sentences
using the LingPipe sentence chunker (Alias-i, 2009).
We apply sentence splitter only to the English lan-
guage documents: either originally created in En-
glish or translated into English by the Google trans-
lation system.
4.3 Data Analysis
As mentioned in Section 1, the goal of the analysis
described in this paper is to quantify the language
diversity in Wikipedia entry descriptions.
We chose English as our reference and, for each
DUC 2004 person, compared a description of this
person in English against the descriptions of this per-
son in other languages.
Language count: In Figure 1, we present infor-
mation about descriptions in how many languages
are created in Wikipedia for each person from the
DUC 2004 set. All the people from the DUC 2004
4http://download.wikimedia.org/
5http://translate.google.com/
33
Figure 1: Number of languages for DUC 2004 people Wikipedia entries.
set have descriptions in English. The results in
Figure 1 are presented in sorted order: from the
Wikipedia entries with the largest number of de-
scriptions (languages covered) to the Wikipedia en-
tries with the smallest number of descriptions (lan-
guages covered). Five people from the DUC 2004
set have only one description (English). The per-
son who has descriptions in the most number of
languages for our data set is the former Secretary-
General of the United Nations Kofi Annan (86 lan-
guages). Figure 1 also has information about de-
scriptions in how many languages were translated
into English (handled by the Google translation sys-
tem).
Despite the fact that English is the language hav-
ing descriptions for more Wikipedia entries than any
other language, it does not always provide the great-
est coverage for Wikipedia entries. To show this
we analyzed the length of Wikipedia entry descrip-
tions for the people from the DUC 2004 set. For our
analysis, the length of a description is equal to the
number of sentences in this description. To count
the number of sentences in the uniform way for as
many languages as possible we used translations of
Wikipedia description from languages that are cur-
rently handled by the Google translation system into
English. Those five people from the DUC 2004 set
that have descriptions only in English are excluded
from this analysis. Thus, in the data set for the next
analysis we have 43 data points.
Sentence count: For every Wikipedia entry (per-
son from the DUC 2004 set), we count the length
of the descriptions originally created in English or
translated into English by the Google translation
system. In Figure 2, we present information about
the length of the Wikipedia entity descriptions for
English and for the language other than English
with the maximum description length. The results
in Figure 2 are presented in sorted order: from
the Wikipedia entry with the maximal longest de-
scription in the language other than English to the
Wikipedia entry with the minimal longest descrip-
tion in the language other than English for our data
set. This sorted order does not correspond to the
sorted order from Figure 1. It is interesting so see
that the sorted order in Figure 2 does not correlate
to the length distribution of English descriptions for
our data set.
Obviously, the descriptions in English are not al-
34
Figure 2: Number of sentences in the English description and the longest non-English description.
ways the longest ones. To be precise for 17 out of
43 people from the DUC 2004 set, the corresponding
Wikipedia description in English was not the longest
one. In several cases, the length of the description
in English is several times shorter than the length
of the longest (non-English) description. For exam-
ple, the description of Gu?nter Grass in German has
251 sentences while his description in English has
74 sentences.
It is safe to assume that longer descriptions
have more information than shorter descriptions
and 17 out of 43 English language descriptions of
Wikipedia entries in our data set can be naturally
extended with the information covered in the de-
scriptions in other languages. Thus, multilingual
Wikipedia gives a straight-forward way of extend-
ing Wikipedia entry descriptions.
It must be noted that the average length of
Wikipedia descriptions (also presented on Figure 2)
is very short. Thus, many descriptions for Wikipedia
entries are quite short. The question arises how well
the information covered in short descriptions corre-
sponds to the information covered in long descrip-
tions.
Correlation Analysis: In this paper, we present
analysis for a small portion of Wikipedia. Currently,
Wikipedia has more than more than 2, 750, 000 ar-
ticles in English alone. Thus, the question arises
whether our analysis can be used without loss of
generality for the complete Wikipedia (i.e., all de-
scriptions for all Wikipedia entries).6 To check
this we analyzed the correspondence of how many
Wikipedia entry descriptions are there for each lan-
guage. For the Wikipedia subset corresponding
to the people from the DUC 2004 set we simply
counted how many Wikipedia entries have descrip-
tions in each language. For the complete set of
Wikipedia descriptions we used the Wikipedia size
numbers from the List of Wikipedias page.7 Af-
ter getting the Wikipedia size numbers we kept the
data only for those languages that are used for de-
scriptions of Wikipedia entries corresponding to the
DUC 2004 people.
To compute correlation between these two lists of
numbers we ranked numbers in each of these lists.
The Rank (Spearman) Correlation Coefficient for
6It must be noted that the notion of complete Wikipedia is
elusive as Wikipedia is changing constantly.
7http://en.wikipedia.org/wiki/List_of_
Wikipedias
35
the above two ranked lists is equal to 0.763 which
shows a high correlation between the two ranked
lists. Thus, the preliminary analysis presented in
work can be a good predictor for the descriptions?
length distribution across descriptions in the com-
plete multilingual Wikipedia.
5 Conclusions and Future Work
In this papers we presented a way of quantify-
ing multilingual aspects of Wikipedia entry descrip-
tions. We showed that despite the fact that English
has descriptions for the most number of Wikipedia
entries across all languages, English descriptions
can not always be considered as the most detailed
descriptions. We showed that for many Wikipedia
entries, descriptions in the languages other than En-
glish are much longer than the corresponding de-
scriptions in English.
Our estimation is that even though Wikipedia en-
try descriptions created in different languages are
not identical, they are likely to contain informa-
tion facts that appear in descriptions in many lan-
guages. One research direction that we are inter-
ested in pursuing is investigating whether the infor-
mation repeated in multiple descriptions of a partic-
ular entry corresponds to the pyramid summariza-
tion model (Teufel and Halteren, 2004; Nenkova et
al., 2007). In case of the positive answer to this
question, multilingual Wikipedia can be used as a
reliable corpus for learning summarization features.
Also, our preliminary analysis shows that
Wikipedia entry descriptions might contain informa-
tion that contradicts information presented in the en-
try descriptions in other languages. Even the choice
of a title for a Wikipedia entry can provide inter-
esting information. For example, the title for the
Wikipedia entry about Former Yugoslav Republic of
Macedonia in English, German, Italian, and many
other languages uses the term Republic of Macedo-
nia or simply Macedonia. However, Greece does not
recognize this name, and thus, the title of the corre-
sponding description in Greek has a complete formal
name of the country: Former Yugoslav Republic of
Macedonia.
Multilingual Wikipedia is full of information
asymmetries. Studying information asymmetries in
multilingual Wikipedia can boost research in new
information and contradiction detection. At the
same time, information symmetries in multilingual
Wikipedia can be used for learning summarization
features.
References
ACL. 2009. Workshop on building and using compara-
ble corpora: from parallel to non-parallel corpora.
Sisay Fissaha Adafre and Maarten de Rijke. 2006. Find-
ing similar sentences across multiple languages in
wikipedia. In Proceedings of the Conference of the
European Chapter of the Association for Computa-
tional Linguistics, Workshop on New Text ? Wikis and
blogs and other dynamic text sources, Trento, Italy,
April.
Eytan Adar, Michael Skinner, and Dan Weld. 2009.
Information arbitrage in multi-lingual Wikipedia. In
Proceedings of the Second ACM International Con-
ference on Web Search and Data Mining, Barcelona,
Spain, February.
David Ahn, Valentin Jijkoun, Gilad Mishne, Karin
Mu?ller, Maarten de Rijke, and Stefan Schlobach.
2005. Using Wikipedia at the TREC QA track. In
Proceedings of the Text REtrieval Conference (TREC
2004).
Alias-i. 2009. Lingpipe 3.7.0. (accessed January 19,
2009). http://alias-i.com/lingpipe.
Fadi Baidsy, Julia Hirschberg, and Elena Filatova. 2008.
An unsupervised approach to biography production
using wikipedia. In Proceedings of the 46th Annual
Meeting of the Association for Computational Linguis-
tics (ACL-2008), Columbus, OH, USA, July.
Regina Barzilaya and Kathleen McKeown. 2001. Ex-
tracting paraphrases from a parallel corpus. In Pro-
ceedings of the 39th Annual Meeting of the Association
for Computational Linguistics (ACL-2001), Toulouse,
France, July.
Davide Buscaldi and Paolo Rosso. 2006. Mining knowl-
edge from wikipedia for the question answering task.
In Proceedings of The Fifth international Conference
on Language Resources and Evaluation (LREC-2006),
Genoa, Italy, May.
CLEF. 2000. Cross-language evaluation forum (CLEF).
http://www.clef-campaign.org.
Sergio Ferra?ndez, Antonio Toral, ?Oscar Ferra?ndez, Anto-
nio Ferra?ndez, and Rafael Munoz. 2007. Applying
Wikipedia?s multilingual knowledge to cross-lingual
question answering. Lecture Notes in Computer Sci-
ence (LNCS): Natural Language Processing and In-
formation Systems, 4592:352?363.
Ulrich Germann. 2001. Aligned hansards of
the 36th parliament of Canada. Website.
36
http://www.isi.edu/natural-language/
download/hansard/.
Andrew Keen. 2007. The Cult of the Amateur: How
Today?s Internet is Killing Our Culture. Doubleday
Business.
Jeongwoo Ko, Teruko Mitamura, and Eric Nyberg. 2007.
Language-independent probabilistic answer ranking
for multilingual question answering. In Proceed-
ings of the 45th Annual Meeting of the Association
for Computational Linguistics (ACL-2007), Prague,
Czech Republic, June.
Philipp Koehn. 2005. Europarl: A parallel corpus for sta-
tistical machine translation. In Proceedings of the Ma-
chine Translation Summit (MT-2005), Phuket Island,
Thailand, September.
LREC. 2008. Workshop on building and using compara-
ble corpora.
Lieve Macken, Julia Trushkina, and Lidia Rura. 2007.
Dutch Parallel Corpus: MT corpus and translator?s
aid. In Proceedings of the Eleventh Machine Transla-
tion Summit (MT-2007), pages 313?320, Copenhagen,
Denmark, September.
Bernardo Magnini, Simone Romagnoli, and Ro Vallin.
2003. Creating the DISEQuA corpus: A test set
for multilingual question answering. In Proceedings
of the Cross-Lingual Evaluation Forum (CLEF-2003),
Trondheim, Norway, August.
Ani Nenkova, Rebecca Passonneau, and Kathleen McK-
eown. 2007. The Pyramid method: Incorporating hu-
man content selection variation in summarization eval-
uation. ACM Transactions on Speech and Language
Processing, 4(2).
Alexander Richman and Patrick Schone. 2008. Mining
Wiki resources for multilingual named entity recogni-
tion. In Proceedings of the 46th Annual Meeting of
the Association for Computational Linguistics (ACL-
2008), Columbus, OH, USA, July.
Pe?ter Scho?nhofen, Andra?s Benczu?r, Istva?n B??ro?, and
Ka?roly Csaloga?ny. 2007. Performing cross-language
retrieval with wikipedia. In Proceedings of the Work-
ing Notes for the CLEF 2007 Workshop, Budapest,
Hungary, September.
Ralf Steinberger, Bruno Pouliquen, Anna Widiger,
Camelia Ignat, Tomaz? Erjavec, Dan Tufis, and Da?niel
Varga. 2006. The JRC-Acquis: A multilingual
aligned parallel corpus with 20+ languages. In Pro-
ceedings of The Fifth international Conference on
Language Resources and Evaluation (LREC-2006),
Genoa, Italy, May.
Simone Teufel and Hans Van Halteren. 2004. Evaluating
information content by factoid analysis: Human anno-
tation and stability. In Proceedings of the 42th Annual
Meeting of the Association for Computational Linguis-
tics (ACL-2004), Barcelona, Spain, July.
37
Proceedings of ACL-08: HLT, pages 807?815,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
An Unsupervised Approach to Biography Production using Wikipedia
Fadi Biadsy,? Julia Hirschberg? and Elena Filatova*
?Department of Computer Science
Columbia University, New York, NY 10027, USA
{fadi,julia}@cs.columbia.edu
*InforSense LLC
Cambridge, MA 02141, USA
efilatova@inforsense.com
Abstract
We describe an unsupervised approach to
multi-document sentence-extraction based
summarization for the task of producing
biographies. We utilize Wikipedia to auto-
matically construct a corpus of biographical
sentences and TDT4 to construct a corpus
of non-biographical sentences. We build a
biographical-sentence classifier from these
corpora and an SVM regression model for
sentence ordering from the Wikipedia corpus.
We evaluate our work on the DUC2004
evaluation data and with human judges.
Overall, our system significantly outperforms
all systems that participated in DUC2004,
according to the ROUGE-L metric, and is
preferred by human subjects.
1 Introduction
Producing biographies by hand is a labor-intensive
task, generally done only for famous individuals.
The process is particularly difficult when persons of
interest are not well known and when information
must be gathered from a wide variety of sources. We
present an automatic, unsupervised, multi-document
summarization (MDS) approach based on extractive
techniques to producing biographies, answering the
question ?Who is X??
There is growing interest in automatic MDS in
general due in part to the explosion of multilingual
and multimedia data available online. The goal of
MDS is to automatically produce a concise, well-
organized, and fluent summary of a set of docu-
ments on the same topic. MDS strategies have been
employed to produce both generic summaries and
query-focused summaries. Due to the complexity
of text generation, most summarization systems em-
ploy sentence-extraction techniques, in which the
most relevant sentences from one or more docu-
ments are selected to represent the summary. This
approach is guaranteed to produce grammatical sen-
tences, although they must subsequently be ordered
appropriately to produce a coherent summary.
In this paper we describe a sentence-extraction
based MDS procedure to produce biographies from
online resources automatically. We make use of
Wikipedia, the largest free multilingual encyclope-
dia on the internet, to build a biographical-sentence
classifier and a component for ordering sentences in
the output summary. Section 2 presents an overview
of our system. In Section 3 we describe our cor-
pus and in Section 4 we discuss the components of
our system in more detail. In Section 5, we present
an evaluation of our work on the Document Under-
standing Conference of 2004 (DUC2004), the biog-
raphy task (task 5) test set. In Section 6 we com-
pare our research with previous work on biography
generation. We conclude in Section 7 and identify
directions for future research.
2 System Overview
In this section, we present an overview of our biog-
raphy extraction system. We assume as input a set of
documents retrieved by an information retrieval en-
gine from a query consisting of the name of the per-
son for whom the biography is desired. We further
assume that these documents have been tagged with
Named Entities (NE)s with coreferences resolved
807
using a system such as NYU?s 2005 ACE system
(Grishman et al, 2005), which we used for our ex-
periments. Our task is to produce a concise biogra-
phy from these documents.
First, we need to select the most ?important? bio-
graphical sentences for the target person. To do so,
we first extract from the input documents all sen-
tences that contain some reference to the target per-
son according to the coreference assignment algo-
rithm; this reference may be the target?s name or
a coreferential full NP or pronominal referring ex-
pression, such as the President or he. We call these
sentences hypothesis sentences. We hypothesize that
most ?biographical? sentences will contain a refer-
ence to the target. However, some of these sentences
may be irrelevant to a biography; therefore, we filter
them using a binary classifier that retains only ?bio-
graphical? sentences. These biographical sentences
may also include redundant information; therefore,
we cluster them and choose one sentence from each
cluster to represent the information in that cluster.
Since some of these sentences have more salient bi-
ographical information than others and since manu-
ally produced biographies tend to include informa-
tion in a certain order, we reorder our summary sen-
tences using an SVM regression model trained on
biographies. Finally, the first reference to the tar-
get person in the initial sentence in the reordering
is rewritten using the longest coreference in our hy-
pothesis sentences which contains the target?s full
name. We then trim the output to a threshold to pro-
duce a biography of a certain length for evaluation
against the DUC2004 systems.
3 Training Data
One of the difficulties inherent in automatic biog-
raphy generation is the lack of training data. One
might collect training data by manually annotating
a suitable corpus containing biographical and non-
biographical data about a person, as in (Zhou et al,
2004). However, such annotation is labor intensive.
To avoid this problem, we adopt an unsupervised ap-
proach. We use Wikipedia biographies as our corpus
of ?biographical? sentences. We collect our ?non-
biographical? sentences from the English newswire
documents in the TDT4 corpus.1 While each corpus
1http://projects.ldc.upenn.edu/TDT4
may contain positive and negative examples, we as-
sume that most sentences in Wikipedia biographies
are biographical and that the majority of TDT4 sen-
tences are non-biographical.
3.1 Constructing the Biographical Corpus
To automatically collect our biographical sentences,
we first download the xml version of Wikipedia
and extract only the documents whose authors used
the Wikipedia biography template when creating
their biography. There are 16,906 biographies in
Wikipedia that used this template. We next apply
simple text processing techniques to clean the text.
We select at most the first 150 sentences from each
page, to avoid sentences that are not critically impor-
tant to the biography. For each of these sentences we
perform the following steps:
1. We identify the biography?s subject from its ti-
tle, terming this name the ?target person.?
2. We run NYU?s 2005 ACE system (Grish-
man et al, 2005) to tag NEs and do coref-
erence resolution. There are 43 unique NE
tags in our corpora, including PER Individual,
ORG Educational, and so on, and TIMEX tags
for all dates.
3. For each sentence, we replace each NE by its
tag name and type ([name-type subtype]) as as-
signed by the NYU tagger. This modified sen-
tence we term a class-based/lexical sentence.
4. Each non-pronominal referring expression
(e.g., George W. Bush, the US president) that
is tagged as coreferential with the target per-
son is replaced by our own [TARGET PER] tag
and every pronoun P that refers to the target
person is replaced by [TARGET P], where P is
the pronoun itself. This allows us to general-
ize our sentences while retaining a) the essen-
tial distinction between this NE (and its role in
the sentence) and all other NEs in the sentence,
and b) the form of referring expressions.
5. Sentences containing no reference to the tar-
get person are assumed to be irrelevant and re-
moved from the corpus, as are sentences with
808
fewer than 4 tokens; short sentences are un-
likely to contain useful information beyond the
target reference.
For example, given sentences from the Wikipedia
biography of Martin Luther King, Jr. we produce
class-based/lexical sentences as follows:
Martin Luther King, Jr., was born on January 15, 1929, in Atlanta,
Georgia. He was the son of Reverend Martin Luther King, Sr. and
Alberta Williams King. He had an older sister, Willie Christine
(September 11, 1927) and a younger brother, Albert Daniel.
[TARGET PER], was born on [TIMEX], in [GPE PopulationCenter].
[TARGET HE] was the son of [PER Individual] and [PER Individual].
[TARGET HE] had an older sister, [PER Individual] ([TIMEX]) and a
younger brother, [PER Individual].
3.2 Constructing the Non-Biographical Corpus
We use the TDT4 corpus to identify non-
biographical sentences. Again, we run NYU?s 2005
ACE system to tag NEs and do coreference resolu-
tion on each news story in TDT4. Since we have
no target name for these stories, we select an NE
tagged as PER Individual at random from all NEs in
the story to represent the target person. We exclude
any sentence with no reference to this target person
and produce class-based/lexical sentences as above.
4 Our Biography Extraction System
4.1 Classifying Biographical Sentences
Using the biographical and non-biographical cor-
pora described in Section 3, we train a binary classi-
fier to determine whether a new sentence should be
included in a biography or not. For our experiments
we extracted 30,002 sentences from Wikipedia bi-
ographies and held out 2,108 sentences for test-
ing. Similarly. we extracted 23,424 sentences from
TDT4, and held out 2,108 sentences for testing.
For each sentence, we then extract the frequency of
three class-based/lexical features ? unigram, bia-
gram, and trigram ? and two POS features ? the
frequency of unigram and bigram POS. To reduce
the dimensionality of our feature space, we first sort
the features in decreasing order of Chi-square statis-
tics computed from the contingency tables of the ob-
served frequencies from the training data. We then
take the highest 30-80% features, where the num-
ber of features used is determined empirically for
Classifier Accuracy F-Measure
SVM 87.6% 0.87
M. na??ve Bayes 84.1% 0.84
C4.5 81.8% 0.82
Table 1: Binary classification results: Wikipedia bi-
ography class-based/lexical sentences vs. TDT4 class-
based/lexical sentences
each feature type. This process identifies features
that significantly contribute to the classification task.
We extract 3K class-based/lexical unigrams, 5.5K
bigrams, 3K trigrams, 20 POS unigrams, and 166
POS bigrams.
Using the training data described above, we ex-
perimented with three different classification algo-
rithms using the Weka machine learning toolkit
(Witten et al, 1999): multinomial na??ve Bayes,
SVM with linear kernel, and C4.5. Weka also pro-
vides a classification confidence score that repre-
sents how confident the classifier is on each classi-
fied sample, which we will make use of as well.
Table 1 presents the classification results on our
4,216 held-out test-set sentences. These results are
quite promising. However, we should note that they
may not necessarily represent the successful clas-
sification of biographical vs. non-biographical sen-
tences but rather the classification of Wikipedia sen-
tences vs. TDT4 sentences. We will validate these
results for our full systems in Section 5.
4.2 Removing Redundant Sentences
Typically, redundancy removal is a standard com-
ponent in MDS systems. In sentence-extraction
based summarizers, redundant sentences are defined
as those which include the same information with-
out introducing new information and identified by
some form of lexically-based clustering. We use
an implementation of a single-link nearest neighbor
clustering technique based on stem-overlap (Blair-
Goldensohn et al, 2004b) to cluster the sentences
classified as biographical by our classifier, and then
select the sentence from each cluster that maximizes
the confidence score returned by the classifier as the
representative for that cluster.
4.3 Sentence Reordering
It is essential for MDS systems in the extraction
framework to choose the order in which sentences
809
should be presented in the final summary. Present-
ing more important information earlier in a sum-
mary is a general strategy for most domains, al-
though importance may be difficult to determine re-
liably. Similar to (Barzilay and Lee, 2004), we au-
tomatically learn how to order our biographical sen-
tences by observing the typical order of presentation
of information in a particular domain. We observe
that our Wikipedia biographies tend to follow a gen-
eral presentation template, in which birth informa-
tion is mentioned before death information, infor-
mation about current professional position and af-
filiations usually appear early in the biography, and
nuclear family members are typically mentioned be-
fore more distant relations. Learning how to order
information from these biographies however would
require that we learn to identify particular types of
biographical information in sentences.
We directly use the position of each sentence in
each Wikipedia biography as a way of determin-
ing where sentences containing similar information
about different target individuals should appear in
their biographies. We represent the absolute posi-
tion of each sentence in its biography as an inte-
ger and train an SVM regression model with RBF
kernel, from the class/lexical features of the sen-
tence to its position. We represent each sentence by
a feature vector whose elements correspond to the
frequency of unigrams and bigrams of class-based
items (e.g., GPE, PER) (cf. Section 3) and lexical
items; for example, the unigrams born, became, and
[GPE State-or-Province], and the bigrams was born,
[TARGET PER] died and [TARGET PER] joined
would be good candidates for such features.
To minimize the dimensionality of our regres-
sion space, we constrained our feature choice to
those features that are important to distinguish bi-
ographical sentences, which we term biographical
terms. Since we want these biographical terms to
impact the regression function, we define these to
be phrases that consist of at least one lexical item
that occurs in many biographies but rarely more than
once in any given biography. We compute the bio-
graphical term score as in the following equation:
bio score(t)= | Dt || D | ?
?
d?Dt(1?
n(t)d
maxt(n(t)d) )
| D | (1)
where D is the set of 16,906 Wikipedia biographies,
n(t)d is the number of occurrences of term t in doc-
ument d, and Dt = {d ? D : t ? d}. The left factor
represents the document frequency of term t, and the
right factor calculates how infrequent the term is in
each biography that contains t at least once.2 We or-
der the unigrams and bigrams in the biographies by
their biographical term scores and select the high-
est 1K unigrams and 500 bigrams; these thresholds
were determined empirically.
4.4 Reference Rewriting
We observe that news articles typically mention bio-
graphical information that occurs early in Wikipedia
biographies when they mention individuals for the
first time in a story (e.g. Stephen Hawking, the Cam-
bridge University physicist). We take advantage of
the fact that the coreference resolution system we
use tags full noun phrases including appositives as
part of NEs. Therefore, we initially search for the
sentence that contains the longest identified NE (of
type PER) that includes the target person?s full name
and is coreferential with the target according to the
reference resolution system; we denote this NE NE-
NP. If this sentence has already been classified as
a biographical sentence by our classifier, we simply
boost its rank in the summary to first. Otherwise,
when we order our sentences, we replace the refer-
ence to the target person in the first sentence by NE-
NP. For example, if the first sentence in the biogra-
phy we have produced for Jimmy Carter is He was
born in 1947 and a sentence not chosen for inclusion
in our biography Jimmy Carter, former U.S. Presi-
dent, visited the University of California last year.
contains the NE-NP, and Jimmy Carter and He are
coreferential, then the first sentence in our biography
will be rewritten as Jimmy Carter, former U.S. Presi-
dent, was born in 1947. Note that, in the evaluations
presented in Section 5, sentence order was modified
by this process in only eight summaries.
5 Evaluation
To evaluate our biography generation system, we use
the document sets created for the biography evalua-
2We considered various approaches to feature selection here,
such as comparing term frequency between our biographical
and non-biographical corpora. However, terms such as killed
and died, which are useful biographical terms, also occur fre-
quently in our non-biographical corpus.
810
ROUGE-L Average_F
0.25
0.275
0.3
0.325
0.35
0 1 2 3 4 5 6 7 8 9 10 11 12
SVM 
reg. onlytop-DUC2004 C4.5 SVM SVM  + SVM reg. MNB +SVM reg
 
MNBC4.5  +  SVM reg. SVM + baseline orderC4.5 +   baseline order MNB +   baseline order
Figure 1: Comparing our approaches against the top performing system in DUC2004 according to ROUGE-L (dia-
mond).
tion (task 5) of DUC2004.3 The task for systems
participating in this evalution was ? Given each doc-
ument cluster and a question of the form ?Who is
X??, where X is the name of a person or group of
people, create a short summary (no longer than 665
bytes) of the cluster that responds to the question.?
NIST assessors chose 50 clusters of TREC docu-
ments such that all the documents in a given cluster
provide at least part of the answer to this question.
Each cluster contained on average 10 documents.
NIST had 4 human summaries written for each clus-
ter. A baseline summary was also created for each
cluster by extracting the first 665 bytes of the most
recent document in the cluster. 22 systems partici-
pated in the competition, producing a total of 22 au-
tomatic summaries (restricted to 665 bytes) for each
cluster. We evaluate our system against the top per-
forming of these 22 systems, according to ROUGE-
L, which we denote top-DUC2004.4
5.1 Automatic Evaluation Using ROUGE
As noted in Section 4.1, we experimented with a
number of learning algorithms when building our
biographical-sentence classifier. For each machine
learning algorithm tested, we build a system that ini-
tially classifies the input list of sentences into bio-
graphical and non-biographical sentences and then
3http://duc.nist.gov/duc2004
4Note that this system out-performed 19 of the 22 systems
on ROUGE-1 and 20 of 22 on ROUGE-L and ROUGE-W-1.2
(p < .05) (Blair-Goldensohn et al, 2004a). No ROUGE metric
produced scores where this system scored significantly worse
than any other system. See Figure 2 below for a comparison
of all DUC2004 systems with our top system where all systems
are evaluated using ROUGE-L-1.5.5.
removes redundant sentences. Next, we produce
three versions of each system: one which imple-
ments a baseline ordering procedure, in which sen-
tences from the clusters are ordered by their ap-
pearance in their source document (e.g. any sen-
tence which occurred first in its original document
is placed first in the summary, with ties ordered ran-
domly within the set), a second which orders the
biographical sentences by the confidence score ob-
tained from the classifier, and a third which uses the
SVM regression as the reordering component. Fi-
nally, we run our reference rewriting component on
each and trim the output to 665 bytes.
We evaluate first using the ROUGE-L metric (Lin
and Hovy, 2003) with a 95% (ROUGE computed)
confidence interval for all systems and compared
these to the ROUGE-L score of the best-performing
DUC2004 system.5 The higher the ROUGE score,
the closer the summary is to the DUC2004 human
reference summaries. As shown in Figure 1, our
best performing system is the multinomial na??ve
Bayes classifier (MNB) using the classifier confi-
dence scores to order the sentences in the biography.
This system significantly outperforms the top ranked
DUC2004 system (top-DUC2004).6 The success of
this particularly learning algorithm on our task may
be due to: (1) the nature of our feature space ? n-
gram frequencies are modeled properly by a multi-
nomial distribution; (2) the simplicity of this classi-
fier particularly given our large feature dimensional-
5We used the same version (1.5.5) of the ROUGE metric to
compute scores for the DUC systems and baseline also.
6Significance for each pair of systems was determined by
paired t-test and calculated at the .05 significance level.
811
ity; and (3) the robustness of na??ve Bayes with re-
spect to noisy data: Not all sentences in Wikipedia
biographies are biographical sentences and some
sentences in TDT4 are biographical.
While the SVM regression reordering component
has a slight negative impact on the performance
of the MNB system, the difference between the
two versions is not significant. Note however, that
both the C4.5 and the SVM versions of our system
are improved by the SVM regression sentence re-
ordering. While neither performs better than top-
DUC2004 without this component, the C4.5 system
with SVM reordering is significantly better than top-
DUC2004 and the performance of the SVM sys-
tem with SVM regression is comparable to top-
DUC2004. In fact, when we use only the SVM
regression model to rank the hypothesis sentences,
without employing any classifier, then remove re-
dundant sentences, rewrite and trim the results, we
find that, interestingly, this approach also outper-
forms top-DUC2004, although the difference is not
statistically significant. However, we believe that
this is an area worth pursuing in future, with more
sophisticated features.
The following biography of Brian Jones was pro-
duced by our MNB system and then the sentences
were ordered using the SVM regression model:
Born in Bristol in 1947, Brian Jones, the co-pilot on the
Breitling mission, learned to fly at 16, dropping out of
school a year later to join the Royal Air Force. After earn-
ing his commercial balloon flying license, Jones became
a ballooning instructor in 1989 and was certified as an ex-
aminer for balloon flight licenses by the British Civil Avi-
ation Authority. He helped organize Breitling?s most re-
cent around-the-world attempts, in 1997 and 1998. Jones,
52, replaces fellow British flight engineer Tony Brown.
Jones, who is to turn 52 next week, is actually the team?s
third co-pilot. After 13 years of service, he joined a cater-
ing business and, in the 1980s,...
Figure 2 illustrates the performance of our MNB
system with classifier confidence score sentence or-
dering when compared to mean ROUGE-L-1.5.5
scores of DUC2004 human-generated summaries
and the 22 DUC2004 systems? summaries across all
summary tasks. Human summaries are labeled A-
H, DUC2004 systems 1-22, and our MNB system
is marked by the rectangle. Results are sorted by
mean ROUGE-L score. Note that our system perfor-
mance is actually comparable in ROUGE-L score to
one of the human summary generators and is signif-
icantly better that all DUC2004 systems, including
top-DUC2004, which is System 1 in the figure.
5.2 Manual Evaluation
ROUGE evaluation is based on n-gram overlap be-
tween the automatically produced summary and the
human reference summaries. Thus, it is not able to
measure how fluent or coherent a summary is. Sen-
tence ordering is one factor in determining fluency
and coherence. So, we conducted two experiments
to measure these qualities, one comparing our top-
performing system according to ROUGE-L score
(MNB) vs. the top-performing DUC2004 system
(top-DUC2004) and another comparing our top sys-
tem with two different ordering methods, classifier-
based and SVM regression.7 In each experiment,
summaries were trimmed to 665 bytes.
In the first experiment, three native American En-
glish speakers were presented with the 50 questions
(Who is X?). For each question they were given a
pair of summaries (presented in random order): one
was the output of our MNB system and the other
was the summary produced by the top-DUC2004
system. Subjects were asked to decide which sum-
mary was more responsive in form and content to the
question or whether both were equally responsive.
85.3% (128/150) of subject judgments preferred one
summary over the other. 100/128 (78.1%) of these
judgments preferred the summaries produced by our
MNB system over those produced by top-DUC2004.
If we compute the majority vote, there were 42/50
summaries in which at least two subjects made the
same choice. 37/42 (88.1%) of these majority judg-
ments preferred our system?s summary (using bino-
mial test, p = 4.4e?7). We used the weighted kappa
statistic with quadratic weighting (Cohen, 1968)
to determine the inter-rater agreement, obtaining a
mean pairwise ? of 0.441.
Recall from Section 5.1 that our SVM regression
reordering component slightly decreases the aver-
age ROUGE score (although not significantly) for
our MNB system. For our human evaluations, we
decided to evaluate the quality of the presentation
of our summaries with and without this compo-
7Note that top-DUC2004 was ranked sixth in the DUC 2004
manual evaluation, with no system performing significantly
better for coverage and only 1 system performing significantly
better for responsiveness.
812
ROUGE-L Average_F
0.2
0.25
0.3
0.35
0.4
0.45
B E H G F A D C * 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 BL 18 19 20 21 22
Figure 2: ROUGE-L scores for DUC2004 human summaries (A-H), our MNB system (rectangle), and the DUC2004
competing systems (1-22 anonymized), with the baseline system labeled BL.
nent to see if this reordering component affected hu-
man judgments even if it did not improve ROUGE
scores. For each question, we produced two sum-
maries from the sentences classified as biographi-
cal by the MNB classifier, one ordered by the con-
fidence score obtained by the MNB, in decreasing
order, and the other ordered by the SVM regression
values, in increasing order. Note that, in three cases,
the summary sentences were ordered identically by
both procedures, so we used only 47 summaries
for this evaluation. Three (different) native Amer-
ican English speakers were presented with the 47
questions for which sentence ordering differed. For
each question they were given the two summaries
(presented in random order) and asked to determine
which biography they preferred.
We found inter-rater agreement for these judg-
ments using Fleiss? kappa (Fleiss, 1971) to be only
moderate (?=0.362). However, when we computed
the majority vote for each question, we found that
61.7% (29/47) preferred the SVM regression order-
ing over the MNB classifier confidence score order-
ing. Although this difference is not statistically sig-
nificant, again we find the SVM regression ordering
results encouraging enough to motivate our further
research on improving such ordering procedures.
6 Related Work
The DUC2004 system achieving the highest over-
all ROUGE score, our top-DUC2004 in Section 5,
was Blair-Goldensohn et al (2004a)?s DefScriber,
which treats ?Who is X?? as a definition question
and targets definitional themes (e.g. genus-species)
found in the input document collections which in-
clude references to the target person. Extracted sen-
tences are then rewritten using a reference rewriting
system (Nenkova and McKeown, 2003) which at-
tempts to shorten subsequent references to the tar-
get. Sentences are ordered in the summary based
on a weighted combination of topic centrality, lex-
ical cohesion, and topic coverage scores. A simi-
lar approach is explored in Biryukov et al (2005),
which uses Topic Signatures (Lin and Hovy, 2000)
constructed around the target individual?s name to
identify sentences to be included in the biography.
Zhou et al (2004)?s biography generation system,
like ours, trains biographical and non-biographical
sentence classifiers to select sentences to be included
in the biography. Their system is trained on a hand-
annotated corpus of 130 biographies of 12 people,
tagged with 9 biographical elements (e.g., bio, ed-
ucation, nationality) and uses binary unigram and
bigram lexical and unigram part-of-speech features
for classification. Duboue et al (2003) also ad-
dress the problem of learning content selection rules
for biography. They learn rules from two corpora,
a semi-structured corpus with lists of biographical
facts about show business celebrities and a corpus
of free-text biographies about the same celebrities.
Filatova et al (2005) learn text features typical
of biographical descriptions by deducing biograph-
ical and occupation-related activities automatically
by compariing descriptions of people with differ-
ent occupations. Weischedel et al (2004) models
kernel-fact features typical for biographies using lin-
guistic and semantic processing. Linguistic features
813
are derived from predicate-argument structures de-
duced from parse trees, and semantic features are the
set of biography-related relations and events defined
in the ACE guidelines (Doddington et al, 2004).
Sentences containing kernel facts are ranked using
probabilities estimated from a corpus of manually
created biographies, including Wikipedia, to esti-
mate the conditional distribution of relevant material
given a kernel fact and a background corpus.
The problem of ordering sentences and preserv-
ing coherence in MDS is addressed by Barzi-
lay et al (2001), who combine chronological order-
ing of events with cohesion metrics. SVM regres-
sion has recently been used by (Li et al, 2007) for
sentence ranking for general MDS. The authors cal-
culated a similarity score for each sentence to the
human summaries and then regress numeric features
(e.g., the centroid) from each sentence to this score.
Barzilay and Lee (2004) use HMMs to capture topic
shift within a particular domain; sequence of topic
shifts then guides the subsequent ordering of sen-
tences within the summary.
7 Discussion and Future Work
In this paper, we describe a MDS system for produc-
ing biographies, given a target name. We present an
unsupervised approach using Wikipedia biography
pages and a general news corpus (TDT4) to automat-
ically construct training data for our system. We em-
ploy a NE tagger and a coreference resolution sys-
tem to extract class-based and lexical features from
each sentence which we use to train a binary classi-
fier to identify biographical sentences. We also train
an SVM regression model to reorder the sentences
and then employ a rewriting heuristic to create the
final summary.
We compare versions of our system based upon
three machine learning algorithms and two sentence
reordering strategies plus a baseline. Our best per-
forming system uses the multinomial na??ve Bayes
(MNB) classifier with classifier confidence score re-
ordering. However, our SVM regression reorder-
ing improves summaries produced by the other two
classifiers and is preferred by human judges. We
compare our MNB system on the DUC2004 bi-
ography task (task 5) to other DUC2004 systems
and to human-generated summaries. Our system
out-performs all DUC2004 systems significantly,
according to ROUGE-L-1.5.5. When presented
with summaries produced by our system and sum-
maries produced by the best-performing (according
to ROUGE scores) of the DUC2004 systems, human
judges (majority vote of 3) prefer our system?s bi-
ographies in 88.1% of cases.
In addition to its high performance, our approach
has the following advantages: It employs no manual
annotation but relies upon identifying appropriately
different corpora to represent our training corpus.
It employs class-based as well as lexical features
where the classes are obtained automatically from
an ACE NE tagger. It utilizes automatic corefer-
ence resolution to identify sentences containing ref-
erences to the target person. Our sentence reorder-
ing approaches make use of either classifier confi-
dence scores or ordering learned automatically from
the actual ordering of sentences in Wikipedia biogra-
phies to determine the order of presentation of sen-
tences in our summaries.
Since our task is to produce concise summaries,
one focus of our future research will be to simplify
the sentences we extract before classifying them
as biographical or non-biographical. This proce-
dure should also help to remove irrelevant informa-
tion from sentences. Recall that our SVM regres-
sion model for sentence ordering was trained using
only biographical class-based/lexical items. In fu-
ture, we would also like to experiment with more
linguistically-informed features. While Wikipedia
does not enforce any particular ordering of infor-
mation in biographies, and while different biogra-
phies may emphasize different types of information,
it would appear that the success of our automatically
derived ordering procedures may capture some un-
derlying shared view of how biographies are written.
The same underlying views may also apply to do-
mains such as organization descriptions or types of
historical events. In future we plan to explore such a
generalization of our procedures to such domains.
Acknowledgments
We thank Kathy McKeown, Andrew Rosenberg, Wisam Dakka, and the
Speech and NLP groups at Columbia for useful discussions. This mate-
rial is based upon work supported by the Defense Advanced Research
Projects Agency (DARPA) under Contract No. HR001106C0023 (ap-
proved for public release, distribution unlimited). Any opinions, find-
ings and conclusions or recommendations expressed in this material are
those of the authors and do not necessarily reflect the views of DARPA.
814
References
Regina Barzilay and Lillian Lee. 2004. Catching the
drift: Probabilistic content models, with applications
to generation and summarization. In Proceedings of
NAACL-HLT.
Regina Barzilay, Noemie Elhadad, and Kathleen McKe-
own. 2001. Sentence ordering in multidocument sum-
marization. In Proceedings of the First Human Lan-
guage Technology Conference, San Diego, California.
Maria Biryukov, Roxana Angheluta, and Marie-Francine
Moens. 2005. Multidocument question answering
text summarization using topic signatures. In Pro-
ceedings of the 5th Dutch-Belgium Information Re-
trieval Workshop, Utrecht, the Netherlands.
Sasha Blair-Goldensohn, David Evans, Vasileios Hatzi-
vassiloglou, Kathleen McKeown, Ani Nenkova, Re-
becca Passonneau, Barry Schiffman, Andrew Schlaik-
jer, Advaith Siddharthan, and Sergey Siegelman.
2004a. Columbia University at DUC 2004. In Pro-
ceedings of the 4th Document Understanding Confer-
ence, Boston, Massachusetts, USA.
Sasha Blair-Goldensohn, Kathy McKeown, and Andrew
Schlaikjer. 2004b. Answering definitional questions:
A hybrid approach. In Mark Maybury, editor, New
Directions In Question Answering, chapter 4. AAAI
Press.
J. Cohen. 1968. Weighted kappa: Nominal scale agree-
ment with provision for scaled disagreement or partial
credit. volume 70, pages 213?220.
George Doddington, Alexis Mitchell, Mark Przybocki,
Lance Ramshaw, Stephanie Strassel, and Ralph
Weischedel. 2004. The automatic content extraction
program - tasks, data, and evaluation. In Proceedings
of the LREC Conference, Canary Islands, Spain, July.
Pablo Duboue and Kathleen McKeown. 2003. Statistical
acquisition of content selection rules for natural lan-
guage generation. In Proceedings of the Conference
on Empirical Methods for Natural Language Process-
ing, pages 121?128, Sapporo, Japan, July.
Elena Filatova and John Prager. 2005. Tell me what
you do and I?ll tell you what you are: Learning
occupation-related activities for biographies. In Pro-
ceedings of the Joint Human Language Technology
Conference and Conference on Empirical Methods in
Natural Language Processing, pages 113?120, Van-
couver, Canada, October.
J. L. Fleiss. 1971. Measuring nominal scale agreement
among many raters. volume 76, No. 5, pages 378?382.
Ralph Grishman, David Westbrook, and Adam Meyers.
2005. Nyu?s english ace 2005 system description. In
ACE 05 Evaluation Workshop, Gaithersburg, MD.
Sujian Li, You Ouyang, Wei Wang, and Bin Sun. 2007.
Multi-document summarization using support vector
regression. In http://duc.nist.gov/pubs/2007papers.
Chin-Yew Lin and Eduard Hovy. 2000. The auto-
mated acquisition of topic signatures for text summa-
rization. In Proceedings of the 18th International Con-
ference on Computational Linguistics, pages 495?501,
Saarbru?cken, Germany, July.
Chin-Yew Lin and Eduard Hovy. 2003. Automatic evalu-
ation of summaries using n-gram co-occurrence statis-
tics. In Proceedings of the 2003 Language Technology
Conference, Edmonton, Canada.
Ani Nenkova and Kathleen McKeown. 2003. References
to named entities: A corpus study. In Proceedings
of the Joint Human Language Technology Conference
and North American chapter of the Association for
Computational Linguistics Annual Meeting, Edmon-
ton, Canada, May.
Ralph Weischedel, Jinxi Xu, and Ana Licuanan. 2004. A
hybrid approach to answering biographical questions.
In Mark Maybury, editor, New Directions In Question
Answering, chapter 5. AAAI Press.
I. Witten, E. Frank, L. Trigg, M. Hall, G. Holmes, and
S. Cunningham. 1999. Weka: Practical machine
learning tools and techniques with java implementa-
tion. In International Workshop: Emerging Knowl-
edge Engineering and Connectionist-Based Informa-
tion Systems, pages 192?196.
Liang Zhou, Miruna Ticrea, and Eduard Hovy. 2004.
Multi-document biography summarization. In Pro-
ceedings of the Conference on Empirical Methods
in Natural Language Processing, pages 434?441,
Barcelona, Spain.
815
Proceedings of the NAACL HLT 2010 Fifth Workshop on Innovative Use of NLP for Building Educational Applications, pages 45?48,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Rethinking Grammatical Error Annotation and Evaluation with the
Amazon Mechanical Turk
Joel R. Tetreault
Educational Testing Service
Princeton, NJ, 08540, USA
JTetreault@ets.org
Elena Filatova
Fordham University
Bronx, NY, 10458, USA
filatova@fordham.edu
Martin Chodorow
Hunter College of CUNY
New York, NY, USA
martin.chodorow@
hunter.cuny.edu
Abstract
In this paper we present results from two pi-
lot studies which show that using the Amazon
Mechanical Turk for preposition error anno-
tation is as effective as using trained raters,
but at a fraction of the time and cost. Based
on these results, we propose a new evaluation
method which makes it feasible to compare
two error detection systems tested on different
learner data sets.
1 Introduction
The last few years have seen an explosion in the de-
velopment of NLP tools to detect and correct errors
made by learners of English as a Second Language
(ESL). While there has been considerable empha-
sis placed on the system development aspect of the
field, with researchers tackling some of the tough-
est ESL errors such as those involving articles (Han
et al, 2006) and prepositions (Gamon et al, 2008),
(Felice and Pullman, 2009), there has been a woeful
lack of attention paid to developing best practices for
annotation and evaluation.
Annotation in the field of ESL error detection has
typically relied on just one trained rater, and that
rater?s judgments then become the gold standard for
evaluating a system. So it is very rare that inter-rater
reliability is reported, although, in other NLP sub-
fields, reporting reliability is the norm. Time and
cost are probably the two most important reasons
why past work has relied on only one rater because
using multiple annotators on the same ESL texts
would obviously increase both considerably. This is
especially problematic for this field of research since
some ESL errors, such as preposition usage, occur at
error rates as low as 10%. This means that to collect
a corpus of 1,000 preposition errors, an annotator
would have to check over 10,000 prepositions.1
(Tetreault and Chodorow, 2008b) challenged the
view that using one rater is adequate by showing
that preposition usage errors actually do not have
high inter-annotator reliability. For example, trained
raters typically annotate preposition errors with a
kappa around 0.60. This low rater reliability has
repercussions for system evaluation: Their experi-
ments showed that system precision could vary as
much as 10% depending on which rater?s judgments
they used as the gold standard. For some grammat-
ical errors such as subject-verb agreement, where
rules are clearly defined, it may be acceptable to
use just one rater. But for usage errors, the rules
are less clearly defined and two native speakers can
have very different judgments of what is acceptable.
One way to address this is by aggregating a multi-
tude of judgments for each preposition and treating
this as the gold standard, however such a tactic has
been impractical due to time and cost limitations.
While annotation is a problem in this field, com-
paring one system to another has also been a major
issue. To date, none of the preposition and article
error detection systems in the literature have been
evaluated on the same corpus. This is mostly due to
the fact that learner corpora are difficult to acquire
(and then annotate), but also to the fact that they are
1(Tetreault and Chodorow, 2008b) report that it would take
80hrs for one of their trained raters to find and mark 1,000
preposition errors.
45
usually proprietary and cannot be shared. Examples
include the Cambridge Learners Corpus2 used in
(Felice and Pullman, 2009), and TOEFL data, used
in (Tetreault and Chodorow, 2008a). This makes it
difficult to compare systems since learner corpora
can be quite different. For example, the ?difficulty?
of a corpus can be affected by the L1 of the writ-
ers, the number of years they have been learning En-
glish, their age, and also where they learn English (in
a native-speaking country or a non-native speaking
country). In essence, learner corpora are not equal,
so a system that performs at 50% precision in one
corpus may actually perform at 80% precision on
a different one. Such an inability to compare sys-
tems makes it difficult for this NLP research area to
progress as quickly as it otherwise might.
In this paper we show that the Amazon Mechani-
cal Turk (AMT), a fast and cheap source of untrained
raters, can be used to alleviate several of the evalua-
tion and annotation issues described above. Specifi-
cally we show:
? In terms of cost and time, AMT is an effec-
tive alternative to trained raters on the tasks of
preposition selection in well-formed text and
preposition error annotation in ESL text.
? With AMT, it is possible to efficiently collect
multiple judgments for a target construction.
Given this, we propose a new method for evalu-
ation that finally allows two systems to be com-
pared to one another even if they are tested on
different corpora.
2 Amazon Mechnical Turk
Amazon provides a service called the Mechani-
cal Turk which allows requesters (companies, re-
searchers, etc.) to post simple tasks (known as Hu-
man Intelligence Tasks, or HITs) to the AMT web-
site for untrained raters to perform for payments as
low as $0.01 in many cases (Sheng et al, 2008).
Recently, AMT has been shown to be an effective
tool for annotation and evalatuation in NLP tasks
ranging from word similarity detection and emotion
detection (Snow et al, 2008) to Machine Transla-
tion quality evaluation (Callison-Burch, 2009). In
these cases, a handful of untrained AMT workers
2http://www.cambridge.org/elt
(or Turkers) were found to be as effective as trained
raters, but with the advantage of being considerably
faster and less expensive. Given the success of us-
ing AMT in other areas of NLP, we test whether we
can leverage it for our work in grammatical error de-
tection, which is the focus of the pilot studies in the
next two sections.
The presence of a gold standard in the above pa-
pers is crucial. In fact, the usability of AMT for text
annotation has been demostrated in those studies by
showing that non-experts? annotation converges to
the gold standard developed by expert annotators.
However, in our work we concentrate on tasks where
there is no single gold standard, either because there
are multiple prepositions that are acceptable in a
given context or because the conventions of preposi-
tion usage simply do not conform to strict rules.
3 Selection Task
0.60
0.65
0.70
0.75
0.80
0.85
0.90
1 2 3 4 5 6 7 8 9 10
Kap
pa
Number of Turkers
Writer vs. AMTRater 1 vs. AMTRater 2 vs. AMT
Figure 1: Error Detection Task: Reliability of AMT as a
function of number of judgments
Typically, an early step in developing a preposi-
tion or article error detection system is to test the
system on well-formed text written by native speak-
ers to see how well the system can predict, or select,
the writer?s preposition given the context around
the preposition. (Tetreault and Chodorow, 2008b)
showed that trained human raters can achieve very
high agreement (78%) on this task. In their work, a
rater was shown a sentence with a target preposition
replaced with a blank, and the rater was asked to se-
lect the preposition that the writer may have used.
We replicate this experiment not with trained raters
but with the AMT to answer two research questions:
1. Can untrained raters be as effective as trained
46
raters? 2. If so, how many raters does it take to
match trained raters?
In the experiment, a Turker was presented with
a sentence from Microsoft?s Encarta encyclopedia,
with one preposition in that sentence replaced with
a blank. There were 194 HITs (sentences) in all, and
we requested 10 Turker judgments per HIT. Some
Turkers did only one HIT, while others completed
more than 100, though none did all 194. The Turk-
ers? performance was analyzed by comparing their
responses to those of two trained annotators and to
the Encarta writer?s preposition, which was consid-
ered the gold standard in this task. Comparing each
trained annotator to the writer yielded a kappa of
0.822 and 0.778, and the two raters had a kappa of
0.742. To determine how many Turker responses
would be required to match or exceed these levels of
reliability, we randomly selected samples of various
sizes from the sets of Turker responses for each sen-
tence. For example, when samples were of size N =
4, four responses were randomly drawn from the set
of ten responses that had been collected. The prepo-
sition that occurred most frequently in the sample
was used as the Turker response for that sentence. In
the case of a tie, a preposition was randomly drawn
from those tied for most frequent. For each sample
size, 100 samples were drawn and the mean values
of agreement and kappa were calculated. The reli-
ability results presented in Table 1 show that, with
just three Turker responses, kappa with the writer
(top line) is comparable to the values obtained from
the trained annotators (around 0.8). Most notable is
that with ten judgments, the reliability measures are
much higher than those of the trained annotators. 3
4 Error Detection Task
While the previous results look quite encouraging,
the task they are based on, preposition selection in
well-formed text, is quite different from, and less
challenging than, the task that a system must per-
form in detecting errors in learner writing. To exam-
ine the reliability of Turker preposition error judg-
ments, we ran another experiment in which Turkers
were presented with a preposition highlighted in a
sentence taken from an ESL corpus, and were in-
3We also experimented with 50 judgments per sentence, but
agreement and kappa improved only negligibly.
structed to judge its usage as either correct, incor-
rect, or the context is too ungrammatical to make
a judgment. The set consisted of 152 prepositions
in total, and we requested 20 judgments per prepo-
sition. Previous work has shown this task to be a
difficult one for trainer raters to attain high reliabil-
ity. For example, (Tetreault and Chodorow, 2008b)
found kappa between two raters averaged 0.630.
Because there is no gold standard for the er-
ror detection task, kappa was used to compare
Turker responses to those of three trained anno-
tators. Among the trained annotators, inter-kappa
agreement ranged from 0.574 to 0.650, for a mean
kappa of 0.606. In Figure 2, kappa is shown for the
comparisons of Turker responses to each annotator
for samples of various sizes ranging from N = 1 to
N = 18. At sample size N = 13, the average kappa is
0.608, virtually identical to the mean found among
the trained annotators.
0.40
0.45
0.50
0.55
0.60
0.65
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18
Kap
pa
Number of Turkers
Rater 1 vs. AMTRater 2 vs. AMTRater 3 vs. AMTMean
Figure 2: Error Detection Task: Reliability of AMT as a
function of number of judgments
5 Rethinking Evaluation
We contend that the Amazon Mechanical Turk can
not only be used as an effective alternative annota-
tion source, but can also be used to revamp evalu-
ation since multiple judgments are now easily ac-
quired. Instead of treating the task of error detection
as a ?black or white? distinction, where a preposi-
tion is either correct or incorrect, cases of prepo-
sition use can now be grouped into bins based on
the level of agreement of the Turkers. For example,
if 90% or more judge a preposition to be an error,
47
Task # of HITs Judgments/HIT Total Judgments Cost Total Cost # of Turkers Total Time
Selection 194 10 1,940 $0.02 $48.50 49 0.5 hours
Error Detection 152 20 3,040 $0.02 $76.00 74 6 hours
Table 1: AMT Experiment Statistics
the high agreement is strong evidence that this is a
clear case of an error. Conversely, agreement lev-
els around 50% would indicate that the use of a par-
ticular preposition is highly contentious, and, most
likely, it should not be flagged by an automated er-
ror detection system.
The current standard method treats all cases of
preposition usage equally, however, some are clearly
harder to annotate than others. By breaking an eval-
uation set into agreement bins, it should be possible
to separate the ?easy? cases from the ?hard? cases
and report precision and recall results for the differ-
ent levels of human agreement represented by differ-
ent bins. This method not only gives a clearer pic-
ture of how a system is faring, but it also ameliorates
the problem of cross-system evaluation when two
systems are evaluated on different corpora. If each
evaluation corpus is annotated by the same number
of Turkers and with the same annotation scheme, it
will now be possible to compare systems by sim-
ply comparing their performance on each respective
bin. The assumption here is that prepositions which
show X% agreement in corpus A are of equivalent
difficulty to those that show X% agreement in cor-
pus B.
6 Discussion
In this paper, we showed that the AMT is an ef-
fective tool for annotating grammatical errors. At
a fraction of the time and cost, it is possible to
acquire high quality judgments from multiple un-
trained raters without sacrificing reliability. A sum-
mary of the cost and time of the two experiments
described here can be seen in Table 1. In the task of
preposition selection, only three Turkers are needed
to match the reliability of two trained raters; in the
more complicated task of error detection, up to 13
Turkers are needed. However, it should be noted
that these numbers can be viewed as upper bounds.
The error annotation scheme that was used is a very
simple one. We intend to experiment with different
guidelines and instructions, and to screen (Callison-
Burch, 2009) and weight Turkers? responses (Snow
et al, 2008), in order to lower the number of Turk-
ers required for this task. Finally, we will look at
other errors, such as articles, to determine howmany
Turkers are necessary for optimal annotation.
Acknowledgments
We thank Sarah Ohls and Waverely VanWinkle for
their annotation work, and Jennifer Foster and the
two reviewers for their comments and feedback.
References
Chris Callison-Burch. 2009. Fast, cheap, and creative:
Evaluating translation quality using Amazon?s Me-
chanical Turk. In EMNLP.
Rachele De Felice and Stephen G. Pullman. 2009. Auto-
matic detection of preposition errors in learner writing.
CALICO Journal, 26(3).
Michael Gamon, Jianfeng Gao, Chris Brockett, Alex
Klementiev, William B. Dolan, Dmitriy Belenko, and
Lucy Vanderwende. 2008. Using contextual speller
techniques and language modeling for esl error cor-
rection. In Proceedings of IJCNLP, Hyderabad, India,
January.
Na-Rae Han, Martin Chodorow, and Claudia Leacock.
2006. Detecting errors in English article usage by
non-native speakers. Natural Language Engineering,
12:115?129.
Victor Sheng, Foster Provost, and Panagiotis Ipeirotis.
2008. Get another label? Improving data quality and
data mining using multiple, noisy labelers. In Pro-
ceeding of ACM SIGKDD, Las Vegas, Nevada, USA.
Rion Snow, Brendan O?Connor, Daniel Jurafsky, and An-
drew Ng. 2008. Cheap and fast ? but is it good?
evaluating non-expert annotations for natural language
tasks. In EMNLP.
Joel R. Tetreault and Martin Chodorow. 2008a. The ups
and downs of preposition error detection in ESL writ-
ing. In COLING.
Joel Tetreault and Martin Chodorow. 2008b. Native
Judgments of non-native usage: Experiments in prepo-
sition error detection. In COLING Workshop on Hu-
man Judgments in Computational Linguistics.
48
