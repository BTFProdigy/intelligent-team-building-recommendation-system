Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 272?276,
Prague, June 2007. c?2007 Association for Computational Linguistics
PU-BCD: Exponential Family Models for the Coarse- and Fine-Grained
All-Words Tasks
Jonathan Chang
Princeton University
Department of Electrical Engineering
jcone@princeton.edu
Miroslav Dud??k, David M. Blei
Princeton University
Department of Computer Science
{mdudik,blei}@cs.princeton.edu
Abstract
This paper describes an exponential family
model of word sense which captures both
occurrences and co-occurrences of words
and senses in a joint probability distribution.
This statistical framework lends itself to the
task of word sense disambiguation. We eval-
uate the performance of the model in its par-
ticipation on the SemEval-2007 coarse- and
fine-grained all-words tasks under a variety
of parameters.
1 Introduction
This paper describes an exponential family model
suited to performing word sense disambiguation.
Exponential family models are a mainstay of mod-
ern statistical modeling (Brown, 1986) and they are
widely and successfully used for example in text
classification (Berger et al, 1996). In statistical
machine learning research, a general methodology
and many algorithms were developed for undirected
graphical model representation of exponential fam-
ilies (Jordan, 2004), providing a solid basis for effi-
cient inference.
Our model differs from other probabilistic mod-
els used for word sense disambiguation in that it
captures not only word-sense co-occurrences but
also contextual sense-sense co-occurrences, thereby
breaking the na??ve Bayes assumption. Although
spare in the types of features, the model is extremely
expressive. Our model has parameters that control
for word-sense interaction and sense-sense similar-
ity, allowing us to capture many of the salient fea-
tures of word and sense use. After fitting the param-
eters of our model from a labeled corpus, the task
of word sense disambiguation immediately follows
by considering the posterior distribution of senses
given words.
We used this model to participate in SemEval-
2007 on the coarse- and fine-grained all-words tasks.
In both of these tasks, a series of sentences are
given with certain words tagged. Each competing
system must assign a sense from a sense inventory
to the tagged words. In both tasks, performance
was gauged by comparing the output of each system
to human-tagged senses. In the fine-grained task,
precision and recall were simply and directly com-
puted against the golden annotations. However, in
the coarse-grained task, the sense inventory was first
clustered semi-automatically with each cluster rep-
resenting an equivalence class over senses (Navigli,
2006). Precision and recall were computed against
equivalence classes.
This paper briefly derives the model and then
explores its properties for WSD. We show how
common algorithms, such as ?dominant sense? and
?most frequent sense,? can be expressed in the ex-
ponential family framework. We then proceed to
present an evaluation of the developed techniques on
the SemEval-2007 tasks in which we participated.
2 The model
We describe an exponential family model for word
sense disambiguation. We posit a joint distribution
over words w and senses s.
2.1 Notation
We define a document d to be a sequence of words
from some lexicon W; for the participation in this
contest, a document consists of a sentence. Associ-
ated with each word is a sense from a lexicon S. In
272
this work, our sense lexicon is the synsets of Word-
Net (Fellbaum and Miller, 2003), but our methods
easily generalize to other sense lexicons, such as
VerbNet (Kipper et al, 2000).
Formally, we denote the sequence of words in a
document d by wd = (wd,1, . . . , wd,nd) and the se-
quence of synsets by sd = (sd,1, sd,2, . . . , sd,nd),
where nd denotes the number of words in the docu-
ment. A corpus D is defined as a collection of doc-
uments. We also write w ? s if w can be used to
represent sense s.
2.2 An exponential family of words and senses
We turn our attention to an exponential family
of words and senses. The vector of parameters
? = (?,?) consists of two blocks capturing depen-
dence on word-synset co-occurrences, and synset
co-occurrences.
p?,n(s,w)
= exp
{?
i?wi,si +
?
i,j ?si,sj
}/
Z?,n .
(1)
The summations are first over all positions in the
document, 1 ? i ? n, and then over all pairs of
positions in the document, 1 ? i, j ? n. We discuss
parameters of our exponential model in turn.
Word-sense parameters ? Using parameters ?
alone, it is possible to describe an arbitrary context
independent distribution between a word and its as-
signed synset.
Sense co-occurrence parameters? Parameters?
are the only parameters that establish the depen-
dence of sense on its context. More specifically,
they capture co-occurrences of synset pairs within a
context. Larger values favor, whereas smaller values
disfavor each pair of synsets.
3 Parameter estimation
With the model in hand, we need to address two
problems in order to use it for problems such as
WSD. First, in parameter estimation, we find values
of the parameters that explain a labeled corpus, such
as SemCor (Miller et al, 1993). Once the parame-
ters are fit, we use posterior inference to compute the
posterior probability distribution of a set of senses
given a set of unlabeled words in a context, p(s |w).
This distribution is used to predict the senses of the
words.
In this section, it will be useful to introduce the
notation p?(s, w) to denote the empirical probabili-
ties of observing the word-sense pair s, w in the en-
tire corpus:
p?(s, w) =
?
d,i ?(sd,i, s)?(wd,i, w)/
?
d nd ,
where ?(x, y) = 1 if x = y and 0 otherwise.
Similarly, we will define p?(s) to denote the empiri-
cal probability of observing a sense s over the entire
corpus:
p?(s) =
?
d,i ?(sd,i, s)/
?
d nd .
3.1 Word-sense parameters ?
Fallback Let ?WNw,s = 0 if w ? s and ?
WN
w,s = ??
otherwise. This simply sets to zero the probability of
assigning a word w to a synset s when w 6? s while
making all w ? s equally likely as an assignment
to s. This forces the model to rely entirely on ?
for inference. If ? is also set to 0, this then forces
the system to fall back onto its arbitrary tie-breaking
mechanism such as choosing randomly or choosing
the first sense.
Most-frequent synset One approach to disam-
biguation is the technique of choosing the most fre-
quently occurring synset which the word may ex-
press. This can be implemented within the model by
setting ?w,s = ?MFSw,s ? ln p?(s) if w ? s and ??
otherwise.
MLE Given a labeled corpus, we would like to
find the corresponding parameters that maximize
likelihood of the data. Equivalently, we would like
to maximize the log likelihood
L(?) =
?
d
[?
i?wd,i,sd,i +
?
i,j ?sd,i,sd,j ? lnZ?,nd
]
.
(2)
In this section, we consider a simple case when it
is possible to estimate parameters maximizing the
likelihood exactly, i.e., the case where our model
depends only on word-synset co-occurrences and is
parametrized solely by ? (setting ? = 0).
Using Eq. (1), with ? = 0, we obtain
p?(sD,wD) =
exp
{?
d,i?wd,i,sd,i
}
?
d Z?,nd
.
273
Thus, p?(sD,wD) can be viewed as a multino-
mial model with
?
d nd trials and |S| outcomes,
parametrized by ?w,s. The maximum likelihood es-
timates in this model are ??w,s ? ln p?(s, w).
This setting of the parameters corresponds pre-
cisely to the dominant-sensemodel (McCarthy et al,
2004). The resulting model is thus
p?,n(s,w) =
?
i p?(si, wi) . (3)
3.2 Sense co-occurrence parameters ?
Unlike ?, it is impossible to find a closed-form so-
lution for the maximum-likelihood settings of ?.
Therefore, we turn to intuitive methods.
Observed synset co-occurrence One natural ad
hoc statistic to use to compute the parameters ? are
the empirical sense co-occurrences. In particular, we
may set
?si,sj = ?
SF
si,sj ? ln p?(si, sj) . (4)
We will observe in section 5 that the performance
of ? = ?SF actually degrades the performance of
the system, especially when combined with ? = ??.
This can be understood as a by-product of an un-
sympathetic interaction between ? and ?. In other
words, ? and ? overlap; by favoring a sense pair the
model will also implicitly favor each of the senses in
the pair.
Discounted observed synset co-occurrence As
we noted earlier, the combination ? = ??,? = ?SF
actually performs worse than ? = ??,? = 0.
In order to cancel out the aforementioned over-
lap effect, we attempt to compute the number of
co-occurrences beyond what the occurrences them-
selves would imply. To do so, we set
? = ?DSF ? ln
p?(si, sj)
p?(si)p?(sj)
, (5)
a quantity which finds an analogue in the notion of
mutual information. We will see shortly that such
a setting of ? will allow sense co-occurrence to im-
prove disambiguation performance.
4 Word Sense Disambiguation
Finally, we describe how to perform WSD using the
exponential family model. Our goal is to assign a
synset si to every word wi in an unlabeled document
d of length n. In this setting, the synsets are hidden
variables. Thus, we assign synsets according to their
posterior probability given the observed words:
s? = argmax
s?Sn
p?,n(s,w)
?
s? p?,n(s
?,w)
,
where the sum is over all possible sequences of
synsets. This combinatorial sum renders exact infer-
ence computationally intractable. We discuss how to
obtain the sense assignment using approximate in-
ference.
4.1 Variational Inference
To approximate the posterior over senses, we use
variational inference (Jordan et al, 1999). In vari-
ational inference, one first chooses a family of
distributions for which inference is computationlly
tractable. Then the distribution in that family which
best approximates the posterior distribution of inter-
est is found.
For our purposes, it is convenient to select q from
the family of factorized multinomial distributions:
q(s) =
?
i
qi(si) ,
where each qi(si) is a multinomial distribution
over all possible senses. Observe that finding s? is
much simpler using q(s): one can find the argmax
of each individual qi independently.
It can be shown that the multinomial which mini-
mizes the KL-divergence must satisfy:
qi(si) ? exp
?
?
?
?wi,si +
?
j 6=i
?
sj
qj(sj)?si,sj
?
?
?
(6)
a system of transcendental equations which can
be solved iteratively to find q. This q is then used to
efficiently perform inference and hence disambigua-
tion.
5 Evaluation
This section evaluates the performance of the model
and the techniques described in the previous sec-
tions with respect to the coarse- and fine-grained all-
words tasks at SemEval-2007.
In order to train the parameters, we trained our
model in a supervised fashion on SemCor (Miller et
274
? = ?WN ? = ?MFS ? = ??
? = 0 52.0% 45.8% 51.2%
? = ?SF 48.8% 45.3% 52.5%
? = ?DSF 47.0% 44.6% 54.2%
Table 1: Precision for the fine-grained all-words task. The results corresponding to the bolded value was
submitted to the competition.
al., 1993) with Laplace smoothing for parameter es-
timates. We utilized the POS tagging and lemma-
tization given in the coarse-grained all-words test
set. Wherever a headword was tagged differently
between the two test sets, we produced an answer
only for the coarse-grained test and not for the fine-
grained one. This led to responses on only 93.9% of
the fine-grained test words. Of the 6.1% over which
no response was given, 5.3% were tagged as ?U? in
the answer key.
In order to break ties between equally likely
senses, for the fine-grained test, the system returned
the first one returned in WordNet?s sense inventory
for that lemma. For the coarse-grained test, an arbi-
trary sense was returned in case of ties.
The precision results given in this section are over
polysemous words (of all parts of speech) for which
our system gave an answer and for which the answer
key was not tagged with ?U.?
5.1 Fine-grained results (Task 17)
The fine-grained results over all permutations of the
parameters mentioned in Section 3 are given in Ta-
ble 1. Note here that the baseline number of ? =
0,? = ?WN given in the upper-left is equivalent to
simply choosing the first WordNet sense. Notably,
such a simple configuration of the model outper-
forms all but two other of the other parameter set-
tings.
When any sort of nonzero sense co-occurrence
parameter is used with ? = ?WN, the performance
degrades dramatically, to 48.8% and 47.0% for ?SF
and ?DSF respectively. Since the discounting scheme
was devised to positively interact with ? = ??, it is
no surprise that it does poorly when ? is not set in
such a way. And as mentioned previously, na??vely
setting ? to ?SF improperly conflates ? and ?, yield-
ing a poor result.
When ? = ?MFS is used, the precision is
even lower, dropping to 45.8% when no sense co-
occurrence information is used. And similarly to
? = ?WN, any nonzero ? significantly degrades per-
formance. This seems to indicate the most-frequent
synset, as predicted by our earlier analysis, is an in-
ferior technique.
Finally, when? = ?? is used (i.e. dominant sense),
the precision is 51.2%, slightly lower than but nearly
on par with that of the baseline. When sense co-
occurrence parameters are added, the performance
increases. For ?SF, a precision of 52.5% is achieved;
a precision above the baseline. But again, because of
the interaction between ? and ?, here we expect it
to be possible to improve upon this performance.
And indeed, when ? = ?DSF, the highest value
of the entire table, 54.2% is achieved. This is a sig-
nificant improvement over the baseline and demon-
strates that our intuitively appealing mutual informa-
tion discounting mechanism allows for ? and ? to
work cooperatively.
5.2 Coarse-grained results (Task 7)
In order to perform the coarse-grained task, our sys-
tem first determined the set of sense equivalence
classes. We denote a sense equivalence class by k,
where k is some sense key member of the class. The
equivalence classes were created according to the
following constraints:
? Each sense key k may only belong to one
equivalence class k.
? All sense keys referring to the same sense s
must belong in the same class.
? All sense keys clustered together must belong
in the same class.
Once the clustering is complete, we can proceed
exactly as we did in the previous sections, while re-
placing all instances of s with k. Thus, training
in this case was performed on a SemCor where all
275
the senses were mapped back to their corresponding
sense equivalence classes.
The model fared considerably worse on the
coarse-grained all-words task. The precision of the
system as given by the scorer was 69.7% and the
recall 62.8%. These results, while naturally much
higher than those for the fine-grained test, are low by
coarse-grained standards. While the gold standard
was not available for comparison for these results,
there are two likely causes of the lower performance
on this task.
The first is that ties were not adjudicated by
choosing the first WordNet sense. Instead, an ar-
bitrary sense was chosen thereby pushing cases in
which the model is unsure from the baseline to the
much lower random precision rate. The second is the
same number of documents are mapped to a smaller
number of ?senses? (i.e. sense equivalence classes),
the number of parameters is greatly reduced. There-
fore, the expressive power of each parameter is di-
luted because it must be spread out across all senses
within the equivalence class.
We believe that both of these issues can be eas-
ily overcome and we hope to do so in future work.
Furthermore, while the model currently captures the
most salient features for word sense disambiguation,
namely word-sense occurrence and sense-sense co-
occurrence, it would be simple to extend the model
to include a larger number of features (e.g. syntactic
features).
6 Conclusion
In summary, this paper described our participation in
the the SemEval-2007 coarse- and fine-grained all-
words tasks. In particular, we described an exponen-
tial family model of word sense amenable to the task
of word sense disambiguation. The performance of
the model under a variety of parameter settings was
evaluated on both tasks and the model was shown to
be particularly effective on the fine-grained task.
7 Acknowledgments
The authors would like to thank Christiane Fell-
baum, Daniel Osherson, and the members of the
CIMPL group for their helpful contributions. This
research was supported by a grant from Google Inc.
and by NSF grant CCR-0325463.
References
Adam L. Berger, Vincent J. Della Pietra, and Stephen A.
Della Pietra. 1996. A maximum entropy approach to natural
language processing. Computational Linguistics, 22(1):39?
71.
Lawrence D. Brown. 1986. Fundamentals of Statistical Expo-
nential Families. Institute of Mathematical Statistics, Hay-
ward, CA.
Christiane Fellbaum and George A. Miller. 2003. Mor-
phosemantic links in WordNet. Traitement automatique de
langue.
Michael I. Jordan, Zoubin Ghahramani, Tommi Jaakkola, and
Lawrence K. Saul. 1999. An introduction to varia-
tional methods for graphical models. Machine Learning,
37(2):183?233.
Michael I. Jordan. 2004. Graphical models. Statistical Science,
19(1):140?155.
Karin Kipper, Hoa Trang Dang, and Martha Palmer. 2000.
Class-Based Construction of a Verb Lexicon. Proceedings
of the Seventeenth National Conference on Artificial Intelli-
gence and Twelfth Conference on Innovative Applications of
Artificial Intelligence table of contents, pages 691?696.
Diana McCarthy, Rob Koeling, Julie Weeds, and John Carroll.
2004. Finding predominant senses in untagged text. In
Proceedings of the 42nd Annual Meeting of the Association
for Computational Linguistics, pages 280?287, Barcelona,
Spain.
George A. Miller, Claudia Leacock, Randee Tengi, and Ross T.
Bunker. 1993. A semantic concordance. In 3rd DARPA
Workshop on Human Language Technology.
Roberto Navigli. 2006. Meaningful clustering of senses helps
boost word sense disambiguation performance. In COLING-
ACL 2006, pages 105?112, July.
276
Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon?s Mechanical Turk, pages 131?138,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Not-So-Latent Dirichlet Allocation:
Collapsed Gibbs Sampling Using Human Judgments
Jonathan Chang
Facebook
1601 S. California Ave.
Palo Alto, CA 94304
jonchang@facebook.com
Abstract
Probabilistic topic models are a popular tool
for the unsupervised analysis of text, providing
both a predictive model of future text and a la-
tent topic representation of the corpus. Recent
studies have found that while there are sugges-
tive connections between topic models and the
way humans interpret data, these two often dis-
agree. In this paper, we explore this disagree-
ment from the perspective of the learning pro-
cess rather than the output. We present a novel
task, tag-and-cluster, which asks subjects to
simultaneously annotate documents and cluster
those annotations. We use these annotations
as a novel approach for constructing a topic
model, grounded in human interpretations of
documents. We demonstrate that these topic
models have features which distinguish them
from traditional topic models.
1 Introduction
Probabilistic topic models have become popular tools
for the unsupervised analysis of large document
collections (Deerwester et al, 1990; Griffiths and
Steyvers, 2002; Blei and Lafferty, 2009). These mod-
els posit a set of latent topics, multinomial distribu-
tions over words, and assume that each document
can be described as a mixture of these topics. With
algorithms for fast approximate posterior inference,
we can use topic models to discover both the topics
and an assignment of topics to documents from a
collection of documents. (See Figure 1.)
These modeling assumptions are useful in the
sense that, empirically, they lead to good models
of documents (Wallach et al, 2009). However, re-
cent work has explored how these assumptions corre-
spond to humans? understanding of language (Chang
et al, 2009; Griffiths and Steyvers, 2006; Mei et al,
2007). Focusing on the latent space, i.e., the inferred
mappings between topics and words and between
documents and topics, this work has discovered that
although there are some suggestive correspondences
between human semantics and topic models, they are
often discordant.
In this paper we build on this work to further
explore how humans relate to topic models. But
whereas previous work has focused on the results
of topic models, here we focus on the process by
which these models are learned. Topic models lend
themselves to sequential procedures through which
the latent space is inferred; these procedures are in
effect programmatic encodings of the modeling as-
sumptions. By substituting key steps in this program
with human judgments, we obtain insights into the
semantic model conceived by humans.
Here we present a novel task, tag-and-cluster,
which asks subjects to simultaneously annotate a doc-
ument and cluster that annotation. This task simulates
the sampling step of the collapsed Gibbs sampler (de-
scribed in the next section), except that the posterior
defined by the model has been replaced by human
judgments. The task is quick to complete and is
robust against noise. We report the results of a large-
scale human study of this task, and show that humans
are indeed able to construct a topic model in this fash-
ion, and that the learned topic model has semantic
properties distinct from existing topic models. We
also demonstrate that the judgments can be used to
guide computer-learned topic models towards models
which are more concordant with human intuitions.
2 Topic models and inference
Topic models posit that each document is expressed
as a mixture of topics. These topic proportions are
drawn once per document, and the topics are shared
across the corpus. In this paper we focus on Latent
Dirichlet alocation (LDA) (Blei et al, 2003) a topic
model which treats each document?s topic assign-
ment as a multinomial random variable drawn from
a symmetric Dirichlet prior. LDA, when applied to a
collection of documents, will build a latent space: a
collection of topics for the corpus and a collection of
topic proportions for each of its documents.
131
computer, 
technology, 
system, 
service, site, 
phone, 
internet, 
machine
play, film, 
movie, theater, 
production, 
star, director, 
stage
sell, sale, 
store, product, 
business, 
advertising, 
market, 
consumer
TOPIC 1
TOPIC 2
TOPIC 3
(a) Topics
Forget the 
Bootleg, Just 
Download the 
Movie Legally
Multiplex Heralded 
As Linchpin To 
Growth
The Shape of 
Cinema, 
Transformed At 
the Click of a 
Mouse A Peaceful Crew 
Puts Muppets 
Where Its Mouth Is
Stock Trades: A 
Better Deal For 
Investors Isn't 
Simple
Internet portals 
begin to distinguish 
among themselves 
as shopping malls
Red Light, Green 
Light: A 
2-Tone L.E.D. to 
Simplify Screens
TOPIC 2
"BUSINESS"
TOPIC 3
"ENTERTAINMENT"
TOPIC 1
"TECHNOLOGY"
(b) Document Assignments to Topics
Figure 1: The latent space of a topic model consists of topics, which are distributions over words, and a distribution
over these topics for each document. On the left are three topics from a fifty topic LDA model trained on articles from
the New York Times. On the right is a simplex depicting the distribution over topics associated with seven documents.
The line from each document?s title shows the document?s position in the topic space.
LDA can be described by the following generative
process:
1. For each topic k,
(a) Draw topic ?k ? Dir(?)
2. For each document d,
(a) Draw topic proportions ?d ? Dir(?)
(b) For each word wd,n,
i. Draw topic assignment zd,n?Mult(?d)
ii. Draw word wd,n ? Mult(?zd,n)
This process is depicted graphically in Figure 2. The
parameters of the model are the number of topics,
K, as well as the Dirichlet priors on the topic-word
distributions and document-topic distributions, ? and
?. The only observed variables of the model are
the words, wd,n. The remaining variables must be
learned.
There are several techniques for performing pos-
terior inference, i.e., inferring the distribution over
hidden variables given a collection of documents, in-
cluding variational inference (Blei et al, 2003) and
Gibbs sampling (Griffiths and Steyvers, 2006). In the
sequel, we focus on the latter approach.
Collapsed Gibbs sampling for LDA treats the topic-
word and document-topic distributions, ?d and ?k, as
nuisance variables to be marginalized out. The poste-
rior distribution over the remaining latent variables,
D
N
?
d
?
?
k
?
z
d,n
w
d,n
K
Figure 2: A graphical model depiction of latent Dirichlet
allocation (LDA). Plates denote replication. The shaded
circle denotes an observed variable and unshaded circles
denote hidden variables.
the topic assignments zd,n, can be expressed as
p(z|?, ?,w) ?
?
d
?
k
[
?(nd,k + ?k)
?(?wd,n + nwd,n,k)
?(
?
w nw,k + ?w)
]
,
where nd,k denotes the number of words in document
d assigned to topic k and nw,k the number of times
word w is assigned to topic k. This leads to the
sampling equations,
p(zd,i = k|?, ?,w, zy) ?
(n?d,id,k + ?k)
?wd,i + n
?d,i
wd,i,k
?
w n
?d,i
w,k + ?w
, (1)
where the superscript ?d, i indicates that these statis-
132
tics should exclude the current variable under consid-
eration, zd,i.
In essence, the model performs inference by look-
ing at each word in succession, and probabilistically
assigning it to a topic according to Equation 1. Equa-
tion 1 is derived through the modeling assumptions
and choice of parameters. By replacing Equation 1
with a different equation or with empirical observa-
tions, we may construct new models which reflect
different assumptions about the underlying data.
3 Constructing topics using human judg-
ments
In this section we propose a task which creates a
formal setting where humans can create a latent
space representation of the corpus. Our task, tag-and-
cluster, replaces the collapsed Gibbs sampling step
of Equation 1 with a human judgment. In essence,
we are constructing a gold-standard series of samples
from the posterior.1
Figure 3 shows how tag-and-cluster is presented
to users. The user is shown a document along with its
title; the document is randomly selected from a pool
of available documents. The user is asked to select
a word from the document which is discriminative,
i.e, a word which would help someone looking for
the document find it. Once the word is selected, the
user is then asked to assign the word to the topic
which best suits the sense of the word used in the
document. Users are specifically instructed to focus
on the meanings of words, not their syntactic usage
or orthography.
The user assigns a word to a topic by selecting an
entry out of a menu of topics. Each topic is repre-
sented by the five words occuring most frequently in
that topic. The order of the topics presented to the
user is determined by the number of words in that
document already assigned to each topic. Once an
instance of a word in a document has been assigned,
it cannot be reassigned and will be marked in red
when subsequent users encounter this document. In
practice, we also prohibit users from selecting infre-
quently occurring words and stop words.
1Additionally, since Gibbs sampling is by nature stochastic,
we believe that the task is robust against small perturbations in
the quality of the assignments, so long as in aggregate they tend
toward the mode.
4 Experimental results
We conducted our experiments using Amazon Me-
chanical Turk, which allows workers (our pool of
prospective subjects) to perform small jobs for a fee
through a Web interface. No specialized training
or knowledge is typically expected of the workers.
Amazon Mechanical Turk has been successfully used
in the past to develop gold-standard data for natural
language processing (Snow et al, 2008).
We prepare two randomly-chosen, 100-document
subsets of English Wikipedia. For convenience, we
denote these two sets of documents as set1 and set2.
For each document, we keep only the first 150 words
for our experiments. Because of the encyclopedic
nature of the corpus, the first 150 words typically
provides a broad overview of the themes in the article.
We also removed from the corpus stop words and
words which occur infrequently2, leading to a lexicon
of 8263 words. After this pruning set1 contained
11614 words and set2 contained 11318 words.
Workers were asked to perform twenty of the tag-
gings described in Section 3 for each task; workers
were paid $0.25 for each such task. The number of
latent topics, K, is a free parameter. Here we explore
two values of this parameter, K = 10 and K = 15,
leading to a total of four experiments ? two for each
set of documents and two for each value of K.
4.1 Tagging behavior
For each experiment we issued 100 HITs, leading to
a total of 2000 tags per experiment. Figure 4 shows
the number of HITs performed per person in each
experiment. Between 12 and 30 distinct workers par-
ticipated in each experiment. The number of HITs
performed per person is heavily skewed, with the
most active participants completing an order of mag-
nitude more HITs than other particpants.
Figure 5 shows the amount of time taken per tag,
in log seconds. Each color represents a different
experiment. The bulk of the tags took less than a
minute to perform and more than a few seconds.
4.2 Comparison with LDA
Learned topics As described in Section 3, the tag-
and-cluster task is a way of allowing humans to con-
2Infrequently occurring words were identified as those ap-
pearing fewer than eight times on a larger collection of 7726
articles.
133
Figure 3: Screenshots of our task. In the center, the document along with its title is shown. Words which cannot be
selected, e.g., distractors and words previously selected, are shown in red. Once a word is selected, the user is asked to
find a topic in which to place the word. The user selects a topic by clicking on an entry in a menu of topics, where each
topic is expressed by the five words which occur most frequently in that topic.
0 5 10 15 20 25 30
0
5
10
15
20
25
(a) set1, K = 10
0 2 4 6 8 10 12
0
10
20
30
40
50
60
(b) set1, K = 15
0 5 10 15 20
0
10
20
30
40
50
(c) set2, K = 10
0 5 10 15 20 25 30
0
5
10
15
20
(d) set2, K = 15
Figure 4: The number of HITs performed (y-axis) by each participant (x-axis). Between 12 and 30 people participated
in each experiment.
0.05 0.1 0.2 0.5 1 2 set1 set2
0
0.5
1
1.5
2
2.5
en
trop
y
?
(a) K = 10
0.05 0.1 0.2 0.5 1 2 set1 set2
0
0.5
1
1.5
2
2.5
3
en
trop
y
?
(b) K = 15
Figure 6: A comparison of the entropy of distributions drawn from a Dirichlet distribution versus the entropy of the
topic proportions inferred by workers. Each column of the boxplot shows the distribution of entropies for 100 draws
from a Dirichlet distribution with parameter ?. The two rightmost columns show the distribution of the entropy of the
topic proportions inferred by workers on set1 and set2. The ? of workers typically falls between 0.2 and 0.5.
134
Table 1: The five words with the highest probability mass in each topic inferred by humans using the task described in
Section 3. Each subtable shows the results for a particular experimental setup. Each row is a topic; the most probable
words are ordered from left to right.
(a) set1, K = 10
railway lighthouse rail huddersfield station
school college education history conference
catholic church film music actor
runners team championships match racing
engine company power dwight engines
university london british college county
food novel book series superman
november february april august december
paint photographs american austin black
war history army american battle
(b) set2, K = 10
president emperor politician election government
american players swedish team zealand
war world navy road torpedo
system pop microsoft music singer
september 2007 october december 1999
television dog name george film
people malay town tribes cliff
diet chest enzyme hair therapy
british city london english county
school university college church center
(c) set1, K = 15
australia knee british israel set
catholic roman island village columbia
john devon michael austin charles
school university class community district
november february 2007 2009 2005
lighthouse period architects construction design
railway rail huddersfield ownership services
cyprus archdiocese diocese king miss
carson gordon hugo ward whitney
significant application campaign comic considered
born london american england black
war defense history military artillery
actor film actress band designer
york michigan florida north photographs
church catholic county 2001 agricultural
(d) set2, K = 15
music pop records singer artist
film paintings movie painting art
school university english students british
drama headquarters chess poet stories
family church sea christmas emperor
dog broadcast television bbc breed
champagne regular character characteristic common
election government parliament minister politician
enzyme diet protein hair oxygen
war navy weapons aircraft military
september october december 2008 1967
district town marin america american
car power system device devices
hockey players football therapy champions
california zealand georgia india kolkata
0 10 20 30 40
?70000
?65000
?60000
?55000
?50000
log li
kelih
ood
iteration
(a) set1, K = 10
0 10 20 30 40
?70000
?65000
?60000
?55000
?50000
?45000
log li
kelih
ood
iteration
(b) set2, K = 10
0 10 20 30 40
?70000
?65000
?60000
?55000
?50000
log li
kelih
ood
iteration
(c) set1, K = 15
0 10 20 30 40
?70000
?65000
?60000
?55000
?50000
?45000
log li
kelih
ood
iteration
(d) set2, K = 15
Figure 7: The log-likelihood achieved by LDA as a function of iteration. There is one series for each value of
? ? {0.05, 0.1, 0.2, 0.5, 1.0, 2.0} from top to bottom.
135
Table 2: The five words with the highest probability mass in each topic inferred by LDA, with ? = 0.2. Each subtable
shows the results for a particular experimental setup. Each row is a topic; the most probable words are ordered from left
to right.
(a) set1, K = 10
born 2004 team award sydney
regiment army artillery served scouting
line station main island railway
region street located site knee
food february conference day 2009
pride greek knowledge portland study
catholic church roman black time
class series film actor engine
travel human office management defense
school born war world university
(b) set2, K = 10
september english edit nord hockey
black hole current england model
training program war election navy
school university district city college
family word international road japan
publication time day india bridge
born pop world released march
won video microsoft project hungary
film hair bank national town
people name french therapy artist
(c) set1, K = 15
time michael written experience match
line station railway branch knowledge
film land pass set battle
william florida carson virginia newfoundland
war regiment british army south
reaction terminal copper running complex
born school world college black
food conference flight medium rail
township scouting census square county
travel defense training management edges
series actor engine november award
pride portland band northwest god
team knee 2004 sydney israel
catholic located site region church
class february time public king
(d) set2, K = 15
family protein enzyme acting oxygen
england producer popular canadian sea
system death artist running car
character series dark main village
english word publication stream day
training program hair students electrical
district town city local kolkata
september edit music records recorded
black pop bank usually hole
people choir road diet related
war built navy british service
center million cut champagne players
born television current drama won
school university college election born
film nord played league hockey
0 500 1000 1500 2000
?2
0
2
4
6
8
10
12
log(
seco
nds 
betw
ee
n ta
gs)
Figure 5: The distribution of times taken per HIT. Each se-
ries represents a different experiment. The bulk of the tags
took less than one minute and more than a few seconds.
struct a topic model. One way of visualizing a learned
topic model is by examining its topics. Table 1 shows
the topics constructed by human judgments. Each
subtable shows a different experimental setup and
each row shows an individual topic. The five most
frequently occurring words in each topic are shown,
ordered from left to right.
Many of the topics inferred by humans have
straightforward interpretations. For example, the
{november, february, april, august,
december} topic for the set1 corpus with K = 10
is simply a collection of months. Similar topics
(with years and months combined) can be found
in the other experimental configurations. Other
topics also cover specific semantic domains,
such as {president, emperor, politican,
election, government} or {music, pop,
records, singer, artist}. Several of the
topics are combinations of distinct concepts,
such as {catholic, church, film, music,
actor}, which is often indicative of the number of
clusters, K, being too low.
136
Table 2 shows the topics learned by LDA un-
der the same experimental conditions, with the
Dirichlet hyperparameter ? = 0.2 (we justify
this choice in the following section). These
topics are more difficult to interpret than the
ones created by humans. Some topics seem
to largely make sense except for some anoma-
lous words, such as {district, town, city,
local, kolkata} or {school, university,
college, election, born}. But the small
amount of data means that it is difficult for a model
which does not leverage prior knowledge to infer
meaningful topic. In contrast, several humans, even
working independently, can leverage prior knowledge
to construct meaningful topics with little data.
There is another qualitative difference between
the topics found by the tag-and-cluster task and
LDA. Whereas LDA must rely on co-occurrence,
humans can use ontological information. Thus, a
topic which has ontological meaning, such as a list
of months, may rarely be discovered by LDA since
the co-occurrence patterns of months do not form a
strong pattern. But users in every experimental con-
figuration constructed this topic, suggesting that the
users were consistently leveraging information that
would not be available to LDA, even with a larger
corpus.
Hyperparameter values A persistent question
among practitioners of topic models is how to set or
learn the value of the hyperparameter ?. ? is a Dirich-
let parameter which acts as a control on sparsity ?
smaller values of ? lead to sparser document-topic
distributions. By comparing the sparsity patterns of
human judgments to those of LDA for different set-
tings of ?, we can infer the value of ? that would
best match human judgments.
Figure 6 shows a boxplot comparison of the en-
tropy of draws from a Dirichlet distribution (the gen-
erative process in LDA), versus the observed en-
tropy of the models learned by humans. The first
six columns show the distributions for the Dirichlet
draws for various values of ?; the last two columns
show the observed entropy distributions on the two
corpora, set1 and set2.
The empirical entropy distributions across the cor-
pora are comparable to those of a Dirichlet distri-
bution with ? between approximately 0.2 and 0.5.
Table 3: The five words with the highest probability mass
in each topic inferred by LDA on set1 with ? = 0.2,
K = 10, and initialized using human judgments. Each
row is a topic; the most probable words are ordered from
left to right.
line station lighthouse local main
school history greek knowledge university
catholic church city roman york
team club 2004 scouting career
engine knee series medium reaction
located south site land region
food film conference north little
february class born august 2009
pride portland time northwest june
war regiment army civil black
These settings of ? are slightly higher than, but still in
line with a common rule-of-thumb of ? = 1/K. Fig-
ure 7 shows the log-likelihood, a measure of model
fit, achieved by LDA for each value of ?. Higher
log-likelihoods indicate better fits. Commensurate
with the rule of thumb, using log-likelihoods to se-
lect ? would encourage values smaller than human
judgments.
However, while the entropy of the Dirichlet draws
increases significantly when the number of clusters
is increased from K = 10 to K = 15, the entropies
assigned by humans does not vary as dramatically.
This suggests that for any given document, humans
are likely to pull words from only a small number of
topics, regardless of how many topics are available,
whereas a model will continue to spread probability
mass across all topics even as the number of topics
increases.
Improving LDA using human judgments The re-
sults of the previous sections suggests that human be-
havior differs from that of LDA, and that humans con-
ceptualize documents in ways LDA does not. This
motivates using the human judgments to augment the
information available to LDA. To do so, we initialize
the topic assignments used by LDA?s Gibbs sampler
to those made by humans. We then run the LDA
sampler till convergence. This provides a method to
weakly incorporate human knowledge into the model.
Table 3 shows the topics inferred by LDA when
initialized with human judgments. These topics re-
semble those directly inferred by humans, although
as we predicted in the previous sections, the topic
consisting of months has largely disappeared. Other
semantically coherent topics, such as {located,
137
0 10 20 30 40
?70000
?65000
?60000
?55000
?50000
log 
like
liho
od
iteration
Figure 8: The log likelihood achieved by LDA on set1
with ? = 0.2, K = 10, and initialized using human
judgments (blue). The red line shows the log likelihood
without incorporating human judgments. LDA with hu-
man judgments dominates LDA without human judgments
and helps the model converge more quickly.
south, site, land, region}, have appeared
in its place.
Figure 8 shows the log-likelihood course of LDA
when initialized by human judgments (blue), versus
LDA without human judgments (red). Adding hu-
man judgments strictly helps the model converge to
a higher likelihood and converge more quickly. In
short, incorporating human judgments shows promise
at improving both the interpretability and conver-
gence of LDA.
5 Discussion
We presented a new method for constructing topic
models using human judgments. Our approach re-
lies on a novel task, tag-and-cluster, which asks
users to simultaneously annotate a document with
one of its words and to cluster those annotations. We
demonstrate using experiments on Amazon Mechan-
ical Turk that our method constructs topic models
quickly and robustly. We also show that while our
topic models bear many similarities to traditionally
constructed topic models, our human-learned topic
models have unique features such as fixed sparsity
and a tendency for topics to be constructed around
concepts which models such as LDA typically fail to
find.
We also underscore that the collapsed Gibbs sam-
pling framework is expressive enough to use as the
basis for human-guided topic model inference. This
may motivate, as future work, the construction of dif-
ferent modeling assumptions which lead to sampling
equations which more closely match the empirically
observed sampling performed by humans. In effect,
our method constructs a series of samples from the
posterior, a gold standard which future topic models
can aim to emulate.
Acknowledgments
The author would like to thank Eytan Bakshy and
Professor Jordan Boyd-Graber-Ying for their helpful
comments and discussions.
References
David Blei and John Lafferty, 2009. Text Mining: Theory
and Applications, chapter Topic Models. Taylor and
Francis.
David Blei, Andrew Ng, and Michael Jordan. 2003. La-
tent Dirichlet alocation. JMLR, 3:993?1022.
Jonathan Chang, Jordan Boyd-Graber, Sean Gerrish,
Chong Wang, and David M. Blei. 2009. Reading
tea leaves: How humans interpret topic models. In
NIPS.
Scott Deerwester, Susan Dumais, Thomas Landauer,
George Furnas, and Richard Harshman. 1990. Index-
ing by latent semantic analysis. Journal of the Ameri-
can Society of Information Science, 41(6):391?407.
Thomas L. Griffiths and Mark Steyvers. 2002. A prob-
abilistic approach to semantic representation. In Pro-
ceedings of the 24th Annual Conference of the Cogni-
tive Science Society.
T. Griffiths and M. Steyvers. 2006. Probabilistic topic
models. In T. Landauer, D. McNamara, S. Dennis, and
W. Kintsch, editors, Latent Semantic Analysis: A Road
to Meaning. Laurence Erlbaum.
Qiaozhu Mei, Xuehua Shen, and ChengXiang Zhai. 2007.
Automatic labeling of multinomial topic models. In
KDD.
R. Snow, Brendan O?Connor, D. Jurafsky, and A. Ng.
2008. Cheap and fast?but is it good? Evaluating
non-expert annotations for natural language tasks. In
EMNLP.
Hanna M. Wallach, Iain Murray, Ruslan Salakhutdinov,
and David Mimno. 2009. Evaluation methods for topic
models. In ICML.
138
