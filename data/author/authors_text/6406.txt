Proceedings of the Second Workshop on Statistical Machine Translation, pages 33?39,
Prague, June 2007. c?2007 Association for Computational Linguistics
Can We Translate Letters?
David Vilar, Jan-T. Peter and Hermann Ney
Lehrstuhl fu?r Informatik 6
RWTH Aachen University
D-52056 Aachen, Germany
{vilar,peter,ney}@cs.rwth-aachen.de
Abstract
Current statistical machine translation sys-
tems handle the translation process as the
transformation of a string of symbols into
another string of symbols. Normally the
symbols dealt with are the words in differ-
ent languages, sometimes with some addi-
tional information included, like morpho-
logical data. In this work we try to push
the approach to the limit, working not on the
level of words, but treating both the source
and target sentences as a string of letters.
We try to find out if a nearly unmodified
state-of-the-art translation system is able to
cope with the problem and whether it is ca-
pable to further generalize translation rules,
for example at the level of word suffixes and
translation of unseen words. Experiments
are carried out for the translation of Catalan
to Spanish.
1 Introduction
Most current statistical machine translation systems
handle the translation process as a ?blind? transfor-
mation of a sequence of symbols, which represent
the words in a source language, to another sequence
of symbols, which represent words in a target lan-
guage. This approach allows for a relative simplic-
ity of the models, but also has drawbacks, as re-
lated word forms, like different verb tenses or plural-
singular word pairs, are treated as completely differ-
ent entities.
Some efforts have been made e.g. to integrate
more information about the words in the form of Part
Of Speech tags (Popovic? and Ney, 2005), using addi-
tional information about stems and suffixes (Popovic?
and Ney, 2004) or to reduce the morphological vari-
ability of the words (de Gispert, 2006). State of the
art decoders provide the ability of handling different
word forms directly in what has been called factored
translation models (Shen et al, 2006).
In this work, we try to go a step further and treat
the words (and thus whole sentences) as sequences
of letters, which have to be translated into a new se-
quence of letters. We try to find out if the trans-
lation models can generalize and generate correct
words out of the stream of letters. For this approach
to work we need to translate between two related
languages, in which a correspondence between the
structure of the words can be found.
For this experiment we chose a Catalan-Spanish
corpus. Catalan is a romance language spoken in the
north-east of Spain and Andorra and is considered
by some authors as a transitional language between
the Iberian Romance languages (e.g. Spanish) and
Gallo-Romance languages (e.g. French). A common
origin and geographic proximity result in a similar-
ity between Spanish and Catalan, albeit with enough
differences to be considered different languages. In
particular, the sentence structure is quite similar in
both languages and many times a nearly monotoni-
cal word to word correspondence between sentences
can be found. An example of Catalan and Spanish
sentences is given in Figure 1.
The structure of the paper is as follows: In Sec-
tion 2 we review the statistical approach to machine
translation and consider how the usual techniques
can be adapted to the letter translation task. In Sec-
33
Catalan Perque` a mi m?agradaria estar-hi dues, una o dues setmanes, me?s o menys, depenent del
preu i cada hotel.
Spanish Porque a m?? me gustar??a quedarme dos, una o dos semanas, ma?s o menos, dependiendo del
precio y cada hotel.
English Because I would like to be there two, one or two weeks, more or less, depending on the
price of each hotel.
Catalan Si baixa aqu?? tenim una guia de la ciutat que li podem facilitar en la que surt informacio?
sobre els llocs me?s interessants de la ciutat.
Spanish Si baja aqu?? tenemos una gu??a de la ciudad que le podemos facilitar en la que sale infor-
macio?n sobre los sitios ma?s interesantes de la ciudad.
English If you come down here we have a guide book of the city that you can use, in there is
information about the most interesting places in the city.
Figure 1: Example Spanish and Catalan sentences (the English translation is provided for clarity).
tion 3 we present the results of the letter-based trans-
lation and show how to use it for improving transla-
tion quality. Although the interest of this work is
more academical, in Section 4 we discuss possible
practical applications for this approach. The paper
concludes in Section 5.
2 From Words To Letters
In the standard approach to statistical machine trans-
lation we are given a sentence (sequence of words)
fJ1 = f1 . . . fJ in a source language which is to be
translated into a sentence e?I1 = e?1 . . . e?I in a target
language. Bayes decision rule states that we should
choose the sentence which maximizes the posterior
probability
e?I1 = argmax
eI1
p(eI1|fJ1 ) , (1)
where the argmax operator denotes the search pro-
cess. In the original work (Brown et al, 1993) the
posterior probability p(eI1|fJ1 ) is decomposed fol-
lowing a noisy-channel approach, but current state-
of-the-art systems model the translation probabil-
ity directly using a log-linear model(Och and Ney,
2002):
p(eI1|fJ1 ) =
exp
(
?M
m=1 ?mhm(eI1, fJ1 )
)
?
e?I1
exp
(
?M
m=1 ?mhm(e?I1, fJ1 )
) ,
(2)
with hm different models, ?m scaling factors and
the denominator a normalization factor that can be
ignored in the maximization process. The ?m are
usually chosen by optimizing a performance mea-
sure over a development corpus using a numerical
optimization algorithm like the downhill simplex al-
gorithm (Press et al, 2002).
The most widely used models in the log lin-
ear combination are phrase-based models in source-
to-target and target-to-source directions, ibm1-like
scores computed at phrase level, also in source-to-
target and target-to-source directions, a target lan-
guage model and different penalties, like phrase
penalty and word penalty.
This same approach can be directly adapted to the
letter-based translation framework. In this case we
are given a sequence of letters FJ1 corresponding
to a source (word) string fJ1 , which is to be trans-
lated into a sequence of letters EI1 corresponding to
a string eI1 in a target language. Note that in this case
whitespaces are also part of the vocabulary and have
to be generated as any other letter. It is also impor-
tant to remark that, without any further restrictions,
the word sequences eI1 corresponding to a generated
letter sequence EI1 are not even composed of actual
words.
2.1 Details of the Letter-Based System
The vocabulary of the letter-based translation sys-
tem is some orders of magnitude smaller than the
vocabulary of a full word-based translation system,
at least for European languages. A typical vocabu-
lary size for a letter-based system would be around
70, considering upper- and lowercase letter, digits,
34
whitespace and punctuation marks, while the vocab-
ulary size of a word-based system like the ones used
in current evaluation campaigns is in the range of
tens or hundreds of thousands words. In a normal
situation there are no unknowns when carrying out
the actual translation of a given test corpus. The sit-
uation can be very different if we consider languages
like Chinese or Japanese.
This small vocabulary size allows us to deal with
a larger context in the models used. For the phrase-
based models we extract all phrases that can be used
when translating a given test corpus, without any
restriction on the length of the source or the tar-
get part1. For the language model we were able to
use a high-order n-gram model. In fact in our ex-
periments a 16-gram letter-based language model is
used, while state-of-the-art translation systems nor-
mally use 3 or 4-grams (word-based).
In order to better try to generate ?actual words?
in the letter-based system, a new model was added
in the log-linear combination, namely the count of
words generated that have been seen in the training
corpus, normalized with the length of the input sen-
tence. Note however that this models enters as an ad-
ditional feature function in the model and it does not
constitute a restriction of the generalization capabil-
ities the model can have in creating ?new words?.
Somehow surprisingly, an additional word language
model did not help.
While the vocabulary size is reduced, the average
sentence length increases, as we consider each let-
ter to be a unit by itself. This has a negative impact
in the running time of the actual implementation of
the algorithms, specially for the alignment process.
In order to alleviate this, the alignment process was
split into two passes. In the first part, a word align-
ment was computed (using the GIZA++ toolkit (Och
and Ney, 2003)). Then the training sentences were
split according to this alignment (in a similar way to
the standard phrase extraction algorithm), so that the
length of the source and target part is around thirty
letters. Then, a letter-based alignment is computed.
2.2 Efficiency Issues
Somewhat counter-intuitively, the reduced vocabu-
lary size does not necessarily imply a reduced mem-
1For the word-based system this is also the case.
ory footprint, at least not without a dedicated pro-
gram optimization. As in a sensible implementa-
tions of nearly all natural language processing tools,
the words are mapped to integers and handled as
such. A typical implementation of a phrase table is
then a prefix-tree, which is accessed through these
word indices. In the case of the letter-based transla-
tion, the phrases extracted are much larger than the
word-based ones, in terms of elements. Thus the to-
tal size of the phrase table increases.
The size of the search graph is also larger for
the letter-based system. In most current systems
the generation algorithm is a beam search algorithm
with a ?source synchronous? search organization.
As the length of the source sentence is dramatically
increased when considering letters instead of words,
the total size of the search graph is also increased, as
is the running time of the translation process.
The memory usage for the letter system can ac-
tually be optimized, in the sense that the letters can
act as ?indices? themselves for addressing the phrase
table and the auxiliary mapping structure is not nec-
essary any more. Furthermore the characters can be
stored in only one byte, which provides a signifi-
cant memory gain over the word based system where
normally four bytes are used for storing the indices.
These gains however are not expected to counteract
the other issues presented in this section.
3 Experimental Results
The corpus used for our experiment was built in the
framework of the LC-STAR project (Conejero et al,
2003). It consists of spontaneous dialogues in Span-
ish, Catalan and English2 in the tourism and travel-
ling domain. The test corpus (and an additional de-
velopment corpus for parameter optimization) was
randomly extracted, the rest of the sentences were
used as training data. Statistics for the corpus can
be seen in Table 1. Details of the translation system
used can be found in (Mauser et al, 2006).
The results of the word-based and letter-based
approaches can be seen in Table 2 (rows with la-
bel ?Full Corpus?). The high BLEU scores (up to
nearly 80%) denote that the quality of the trans-
lation is quite good for both systems. The word-
2The English part of the corpus was not used in our experi-
ments.
35
Spanish Catalan
Training Sentences 40 574
Running Words 482 290 485 514
Vocabulary 14 327 12 772
Singletons 6 743 5 930
Test Sentences 972
Running Words 12 771 12 973
OOVs [%] 1.4 1.3
Table 1: Corpus Statistics
based system outperforms the letter-based one, as
expected, but the letter-based system also achieves
quite a good translation quality. Example transla-
tions for both systems can be found in Figure 2. It
can be observed that most of the words generated
by the letter based system are correct words, and in
many cases the ?false? words that the system gen-
erates are very close to actual words (e.g. ?elos? in-
stead of ?los? in the second example of Figure 2).
We also investigated the generalization capabili-
ties of both systems under scarce training data con-
ditions. It was expected that the greater flexibility
of the letter-based system would provide an advan-
tage of the approach when compared to the word-
based approach. We randomly selected subsets of
the training corpus of different sizes ranging from
1 000 sentences to 40 000 (i.e. the full corpus) and
computed the translation quality on the same test
corpus as before. Contrary to our hopes, however,
the difference in BLEU score between the word-
based and the letter-based system remained fairly
constant, as can be seen in Figure 3, and Table 2
for representative training corpus sizes.
Nevertheless, the second example in Figure 2 pro-
vides an interesting insight into one of the possi-
ble practical applications of this approach. In the
example translation of the word-based system, the
word ?centreamericans? was not known to the sys-
tem (and has been explicitly marked as unknown in
Figure 2). The letter-based system, however, was
able to correctly learn the translation from ?centre-?
to ?centro-? and that the ending ?-ans? in Catalan
is often translated as ?-anos? in Spanish, and thus
a correct translation has been found. We thus chose
to combine both systems, the word-based system do-
ing most of the translation work, but using the letter-
based system for the translation of unknown words.
The results of this combined approach can be found
in Table 2 under the label ?Combined System?. The
combination of both approaches leads to a 0.5% in-
crease in BLEU using the full corpus as training ma-
terial. This increase is not very big, but is it over a
quite strong baseline and the percentage of out-of-
vocabulary words in this corpus is around 1% of the
total words (see Table 1). When the corpus size is
reduced, the gain in BLEU score becomes more im-
portant, and for the small corpus size of 1 000 sen-
tences the gain is 2.5% BLEU. Table 2 and Figure 3
show more details.
4 Practical Applications
The approach described in this paper is mainly of
academical interest. We have shown that letter-
based translation is in principle possible between
similar languages, in our case between Catalan and
Spanish, but can be applied to other closely related
language pairs like Spanish and Portuguese or Ger-
man and Dutch. The approach can be interesting for
languages where very few parallel training data is
available.
The idea of translating unknown words in a letter-
based fashion can also have applications to state-of-
the-art translation systems. Nowadays most auto-
matic translation projects and evaluations deal with
translation from Chinese or Arabic to English. For
these language pairs the translation of named en-
tities poses an additional problem, as many times
they were not previously seen in the training data
and they are actually one of the most informative
words in the texts. The ?translation? of these enti-
ties is in most cases actually a (more or less pho-
netic) transliteration, see for example (Al-Onaizan
and Knight, 2002). Using the proposed approach for
the translation of these words can provide a tighter
integration in the translation process and hopefully
increase the translation performance, in the same
way as it helps for the case of the Catalan-Spanish
translation for unseen words.
Somewhat related to this problem, we can find an
additional application in the field of speech recog-
nition. The task of grapheme-to-phoneme conver-
sion aims at increasing the vocabulary an ASR sys-
tem can recognize, without the need for additional
36
BLEU WER PER
Word-Based System Full Corpus 78.9 11.4 10.6
10k 74.0 13.9 13.2
1k 60.0 21.3 20.1
Letter-Based System Full Corpus 72.9 14.7 13.5
10k 69.8 16.5 15.1
1k 55.8 24.3 22.8
Combined System Full Corpus 79.4 11.2 10.4
10k 75.2 13.4 12.6
1k 62.5 20.2 19.0
Table 2: Translation results for selected corpus sizes. All measures are percentages.
Source (Cat) Be?, en principi seria per a les vacances de Setmana Santa que so?n les segu?ents que tenim
ara, entrant a juliol.
Word-Based Bueno, en principio ser??a para las vacaciones de Semana Santa que son las siguientes que
tenemos ahora, entrando en julio.
Letter-Based Bueno, en principio ser??a para las vacaciones de Semana Santa que son las siguientes que
tenemos ahora, entrando bamos en julio .
Reference Bueno, en principio ser??a para las vacaciones de Semana Santa que son las siguientes que
tenemos ahora, entrando julio.
Source (Cat) Jo li recomanaria per exemple que intente?s apropar-se a algun pa??s ve?? tambe? com poden ser
els pa??sos centreamericans, una mica me?s al nord Panama?.
Word-Based Yo le recomendar??a por ejemplo que intentase acercarse a algu?n pa??s vecino tambie?n como
pueden ser los pa??ses UNKNOWN centreamericans, un poco ma?s al norte Panama?.
Letter-Based Yo le recomendar??a por ejemplo que intentaseo acercarse a algu?n pa??s ve?? tambie?n como
pueden ser elos pa??ses centroamericanos, un poco ma?s al norte Panama?.
Combined Yo le recomendar??a por ejemplo que intentase acercarse a algu?n pa??s vecino tambie?n como
pueden ser los pa??ses centroamericanos, un poco ma?s al norte Panama?.
Reference Yo le recomendar??a por ejemplo que intentase acercarse a algu?n pa??s vecino tambie?n como
pueden ser los pa??ses centroamericanos, un poco ma?s al norte Panama?.
Figure 2: Example translations of the different approaches. For the word-based system an unknown word
has been explicitly marked.
37
 50
 55
 60
 65
 70
 75
 80
 0  5000  10000  15000  20000  25000  30000  35000  40000
Word-Based
Letter-Based
Combined
Figure 3: Translation quality depending of the corpus size.
acoustic data. The problem can be formulated as a
translation from graphemes (?letters?) to a sequence
of graphones (?pronunciations?), see for example
(Bisani and Ney, 2002). The proposed letter-based
approach can also be adapted to this task.
Lastly, a combination of both, word-based and
letter-based models, working in parallel and perhaps
taking into account additional information like base
forms, can be helpful when translating from or into
rich inflexional languages, like for example Spanish.
5 Conclusions
We have investigated the possibility of building a
letter-based system for translation between related
languages. The performance of the approach is quite
acceptable, although, as expected, the quality of the
word-based approach is superior. The combination
of both techniques, however, allows the system to
translate words not seen in the training corpus and
thus increase the translation quality. The gain is spe-
cially important when the training material is scarce.
While the experiments carried out in this work are
more interesting from an academical point of view,
several practical applications has been discussed and
will be the object of future work.
Acknowledgements
This work was partly funded by the Deutsche
Forschungsgemeinschaft (DFG) under the project
?Statistische Textu?bersetzung? (NE 572/5-3).
References
Yaser Al-Onaizan and Kevin Knight. 2002. Machine
transliteration of names in arabic text. In Proceed-
ings of the ACL-02 workshop on Computational ap-
proaches to semitic languages, pages 1?13, Morris-
town, NJ, USA. Association for Computational Lin-
guistics.
Max Bisani and Hermann Ney. 2002. Investigations
on joint-multigram models for grapheme-to-phoneme
conversion. In Proceedings of the 7th International
Conference on Spoken Language Processing, vol-
ume 1, pages 105?108, Denver, CO, September.
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. 1993. The mathemat-
ics of statistical machine translation: Parameter esti-
38
mation. Computational Linguistics, 19(2):263?311,
June.
D. Conejero, J. Gimnez, V. Arranz, A. Bonafonte, N. Pas-
cual, N. Castell, and A. Moreno. 2003. Lexica and
corpora for speech-to-speech translation: A trilingual
approach. In European Conf. on Speech Commu-
nication and Technology, pages 1593?1596, Geneva,
Switzerland, September.
Adria` de Gispert. 2006. Introducing Linguistic Knowl-
edge into Statistical Machine Translation. Ph.D. the-
sis, Universitat Polite`cnica de Catalunya, Barcelona,
October.
Arne Mauser, Richard Zens, Evgeny Matusov, Sas?a
Hasan, and Hermann Ney. 2006. The RWTH Statisti-
cal Machine Translation System for the IWSLT 2006
Evaluation. In Proc. of the International Workshop on
Spoken Language Translation, pages 103?110, Kyoto,
Japan.
Franz Josef Och and Hermann Ney. 2002. Discrimina-
tive training and maximum entropy models for statis-
tical machine translation. In Proc. of the 40th Annual
Meeting of the Association for Computational Linguis-
tics (ACL), pages 295?302, Philadelphia, PA, July.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19?51, March.
Maja Popovic? and Hermann Ney. 2004. Towards the
Use of Word Stems and Suffixes for Statistical Ma-
chine Translation. In 4th International Conference on
Language Resources and Evaluation (LREC), pages
1585?1588, Lisbon, Portugal, May.
Maja Popovic? and Hermann Ney. 2005. Exploiting
Phrasal Lexica and Additional Morpho-syntactic Lan-
guage Resources for Statistical Machine Translation
with Scarce Training Data. In 10th Annual Conference
of the European Association for Machine Translation
(EAMT), pages 212?218, Budapest, Hungary, May.
William H. Press, Saul A. Teukolsky, William T. Vetter-
ling, and Brian P. Flannery. 2002. Numerical Recipes
in C++. Cambridge University Press, Cambridge,
UK.
Wade Shen, Richard Zens, Nicola Bertoldi, and Marcello
Federico. 2006. The JHU Workshop 2006 IWSLT
System. In Proc. of the International Workshop on
Spoken Language Translation, pages 59?63, Kyoto,
Japan.
39
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 322?332,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Advancements in Reordering Models for Statistical Machine Translation
Minwei Feng and Jan-Thorsten Peter and Hermann Ney
Human Language Technology and Pattern Recognition
Computer Science Department, RWTH Aachen University, Aachen, Germany
<surname>@cs.rwth-aachen.de
Abstract
In this paper, we propose a novel re-
ordering model based on sequence label-
ing techniques. Our model converts the
reordering problem into a sequence label-
ing problem, i.e. a tagging task. Results
on five Chinese-English NIST tasks show
that our model improves the baseline sys-
tem by 1.32 BLEU and 1.53 TER on av-
erage. Results of comparative study with
other seven widely used reordering mod-
els will also be reported.
1 Introduction
The systematic word order difference between two
languages poses a challenge for current statistical
machine translation (SMT) systems. The system
has to decide in which order to translate the given
source words. This problem is known as the re-
ordering problem. As shown in (Knight, 1999), if
arbitrary reordering is allowed, the search problem
is NP-hard.
Many ideas have been proposed to address
the reordering problem. Within the phrase-based
SMT framework there are mainly three stages
where improved reordering could be integrated:
In the preprocessing: the source sentence is re-
ordered by heuristics, so that the word order of
source and target sentences is similar. (Wang et
al., 2007) use manually designed rules to reorder
parse trees of the source sentences. Based on shal-
low syntax, (Zhang et al, 2007) use rules to re-
order the source sentences on the chunk level and
provide a source-reordering lattice instead of a sin-
gle reordered source sentence as input to the SMT
system. Designing rules to reorder the source sen-
tence is conceptually clear and usually easy to im-
plement. In this way, syntax information can be in-
corporated into phrase-based SMT systems. How-
ever, one disadvantage is that the reliability of the
rules is often language pair dependent.
In the decoder: we can add constraints or mod-
els into the decoder to reward good reordering op-
tions or penalize bad ones. For reordering con-
straints, early work includes ITG constraints (Wu,
1995) and IBM constraints (Berger et al, 1996).
(Zens and Ney, 2003) did comparative study over
different reordering constraints. This paper fo-
cuses on reordering models. For reordering mod-
els, we can further roughly divide the existing
methods into three genres:
? The reordering is a classification problem.
The classifier will make decision on next
phrase?s relative position with current phrase.
The classifier can be trained with maximum
likelihood like Moses lexicalized reordering
(Koehn et al, 2007) and hierarchical lexical-
ized reordering model (Galley and Manning,
2008) or be trained under maximum entropy
framework (Zens and Ney, 2006).
? The reordering is a decoding order problem.
(Marin?o et al, 2006) present a translation
model that constitutes a language model of
a sort of bilanguage composed of bilingual
units. From the reordering point of view, the
idea is that the correct reordering is a suit-
able order of translation units. (Feng et al,
2010) present a simpler version of (Marin?o et
al., 2006)?s model which utilize only source
words to model the decoding order.
? The reordering can be solved by outside
heuristics. We can put human knowledge into
the decoder. For example, the simple jump
model using linear distance tells the decoder
that usually the long range reordering should
be avoided. (Cherry, 2008) uses information
from dependency trees to make the decod-
ing process keep syntactic cohesion. (Feng
et al, 2012) present a method that utilizes
predicate-argument structures from semantic
role labeling results as soft constraints.
In the reranking framework: in principle, all
322
the models in previous category can be used in
the reranking framework, because in the rerank-
ing we have all the information (source and tar-
get words/phrases, alignment) about the transla-
tion process. (Och et al, 2004) describe the use of
syntactic features in the rescoring step. However,
they report the syntactic features contribute very
small gains. One disadvantage of carrying out re-
ordering in reranking is the representativeness of
the N-best list is often a question mark.
In this paper, we propose a novel tagging style
reordering model which is under the category
?The reordering is a decoding order problem?.
Our model converts the decoding order problem
into a sequence labeling problem, i.e. a tagging
task. The remainder of this paper is organized
as follows: Section 2 introduces the basement
of this research: the principle of statistical ma-
chine translation. Section 3 describes the proposed
model. Section 4 briefly describes several reorder-
ing models with which we compare our method.
Section 5 provides the experimental configuration
and results. Conclusion will be given in Section 6.
2 Translation System Overview
In statistical machine translation, we are given a
source language sentence fJ1 = f1 . . . fj . . . fJ .
The objective is to translate the source into a tar-
get language sentence eI1 = e1 . . . ei . . . eI . The
strategy is to choose the target sentence with the
highest probability among all others:
e?I?i = argmaxI,eI1
{Pr(eI1|fJ1 )} (1)
We model Pr(eI1|fJ1 ) directly using a log-linear
combination of several models (Och and Ney,
2002):
Pr(eI1|fJ1 ) =
exp
( M?
m=1
?mhm(eI1, fJ1 )
)
?
I? ,e? I
?
1
exp
( M?
m=1
?mhm(e?I
?
1 , fJ1 )
)
(2)
The denominator is to make the Pr(eI1|fJ1 ) to be a
probability distribution and it depends only on the
source sentence fJ1 . For search, the decision rule
is simply:
e?I?i = argmaxI,eI1
{ M?
m=1
?mhm(eI1, fJ1 )
}
(3)
The model scaling factors ?M1 are trained with
Minimum Error Rate Training (MERT). In this pa-
per, the phrase-based machine translation system
is utilized (Och et al, 1999; Zens et al, 2002;
Koehn et al, 2003).
3 Tagging-style Reordering Model
In this section, we describe the proposed novel
model. First we will describe the training process.
Then we explain how to use the model in the de-
coder.
3.1 Modeling
Figure 1 shows the modeling steps. The first step
is word alignment training. Figure 1(a) is an ex-
ample after GIZA++ training. If we regard this
alignment as a translation result, i.e. given the
source sentence f71 , the system translates it into
the target sentence e71, then the alignment link set
{a1 = 3, a3 = 2, a4 = 4, a4 = 5, a5 = 7, a6 =
6, a7 = 6} reveals the decoding process, i.e. the
alignment implies the order in which the source
words should be translated, e.g. the first generated
target word e1 has no alignment, we can regard it
as a translation from a NULL source word; then
the second generated target word e2 is translated
from f3. We reorder the source side of the align-
ment to get Figure 1(b). Figure 1(b) implies the
source sentence decoding sequence information,
which is depicted in Figure 1(c). Using this ex-
ample we describe the strategies we used for spe-
cial cases in the transformation from Figure 1(b)
to Figure 1(c):
? ignore the unaligned target word, e.g. e1
? the unaligned source word should follow its
preceding word, the unaligned feature is kept
with a ? symbol, e.g. f?2 is after f1
? when one source word is aligned to multi-
ple target words, only keep the alignment that
links the source word to the first target word,
e.g. f4 is linked to e5 and e6, only f4 ? e5
is kept. In other words, we use this strategy
to guarantee that every source word appears
only once in the source decoding sequence.
? when multiple source words are aligned to
one target word, put together the source
words according to their original relative po-
sitions, e.g. e6 is linked to f6 and f7. So in
the decoding sequence, f6 is before f7.
Now Figure 1(c) shows the original source sen-
tence and its decoding sequence. By using the
strategies above, it is guaranteed that the source
sentence and its decoding sequence have the ex-
323
f1 f2 f3 f4 f5 f6 f7
e1 e2 e3 e4 e5 e6 e7
(a)
f3 f1 f2 f4 f6 f7 f5
e1 e2 e3 e4 e5 e6 e7
(b)
f1 f?2 f3 f4 f5 f6 f7
f3 f1 f2 f4 f6 f7 f5
(c)
f1 f?2 f3 f4 f5 f6 f7
+1 +1 ?2 0 +2 ?1 ?1
(d)
BEGIN-Rmono Unalign Lreorder-Rmono Lmono-Rmono Lmono-Rreorder Lreorder-Rmono END-Lmono
f1 f?2 f3 f4 f5 f6 f7
(e)
Figure 1: modeling process illustration.
actly same length. Hence the relation can be mod-
eled by a function F (f) which assigns a value for
each source word f . Figure 1(d) manifests this
function. The positive function values mean that
compared to the original position in the source
sentence, its position in the decoding sequence
should move rightwards. If the function value is
0, the word?s position in original source sentence
and its decoding sequence is same. For example,
f1 is the first word in the source sentence but it is
the second word in the decoding sequence. So its
function value is +1 (move rightwards one posi-
tion).
Now Figure 1(d) converts the reordering prob-
lem into a sequence labeling or tagging problem.
To make the computational cost to a reasonable
level, we do a final step simplification in Figure
1(e). Suppose the longest sentence length is 100,
then according to Figure 1(d), there are 200 tags
(from -99 to +99 plus the unalign tag). As we will
see later, this number is too large for our task. We
instead design nine tags. For a source word fj in
one source sentence fJ1 , the tag of fj will be one
of the following:
Unalign fj is an unaligned source word
BEGIN-Rmono j = 1 and fj+1 is translated af-
ter fj (Rmono for right monotonic)
BEGIN-Rreorder j = 1 and fj+1 is translated
before fj (Rreorder for right reordered)
END-Lmono j = J and fj?1 translated before
fj (Lmono for left monotonic)
END-Lreorder j = J and fj?1 translated after
fj (Lreorder for left reordered)
Lmono-Rmono 1 < j < J and fj?1 translated
before fj and fj translated before fj+1
Lreorder-Rmono 1 < j < J and fj?1 translated
after fj and fj translated before fj+1
Lmono-Rreorder 1 < j < J and fj?1 translated
before fj and fj translated after fj+1
Lreorder-Rreorder 1 < j < J and fj?1 trans-
lated after fj and fj translated after fj+1
Up to this point, we have converted the reorder-
ing problem into a tagging problem with nine tags.
The transformation in Figure 1 is conducted for
all the sentence pairs in the bilingual training cor-
pus. After that, we have built an ?annotated? cor-
pus for the training. For this supervised learning
task, we choose the approach conditional random
fields (CRFs) (Lafferty et al, 2001; Sutton and
Mccallum, 2006; Lavergne et al, 2010) and recur-
rent neural network (RNN) (Elman, 1990; Jordan,
1990; Lang et al, 1990).
For the first method, we adopt the linear-chain
CRFs. However, even for the simple linear-chain
CRFs, the complexity of learning and inference
grows quadratically with respect to the number of
output labels and the amount of structural features
which are with regard to adjacent pairs of labels.
Hence, to make the computational cost as low as
possible, two measures have been taken. Firstly,
as described above we reduce the number of tags
to nine. Secondly, we add source sentence part-of-
speech (POS) tags to the input. For features with
window size one to three, both source words and
its POS tags are used. For features with window
size four and five, only POS tags are used.
As the second method, we use recurrent neu-
ral network (RNN). RNN is closely related with
Multilayer Perceptrons (MLP) (Rumelhart et al,
1986), but the output of one ore more hidden lay-
ers is reused as additional inputs for the network in
the next time step. This structure allows the RNN
to learn whole sequences without restricting itself
to a fixed input window. A plain RNN has only ac-
cess to the previous events in the input sequence.
Hence we adopt the bidirectional RNN (BRNN)
(Schuster and Paliwal, 1997) which reads the input
sequence from both directions before making the
prediction. The long short-term memory (LSTM)
(Hochreiter and Schmidhuber, 1997) is applied to
324
counter the effects that long distance dependen-
cies are hard to learn with gradient descent. This
is often referred to as vanishing gradient problem
(Bengio et al, 1994).
3.2 Decoding
Once the model training is finished, we make in-
ference on develop and test corpora which means
that we get the labels of the source sentences that
need to be translated. In the decoder, we add
a new model which checks the labeling consis-
tency when scoring an extended state. During
the search, a sentence pair (fJ1 , eI1) will be for-
mally splitted into a segmentation SK1 which con-
sists of K phrase pairs. Each sk = (ik; bk, jk)
is a triple consisting of the last position ik of
the kth target phrase e?k. The start and end po-
sition of the kth source phrase f?k are bk and jk.
Suppose the search state is now extended with a
new phrase pair (f?k, e?k): f?k := fbk . . . fjk and
e?k := eik?1+1 . . . eik . We have access to the
old coverage vector, from which we know if the
new phrase?s left neighboring source word fbk?1
and right neighboring source word fjk+1 have
been translated. We also have the word alignment
within the new phrase pair, which is stored dur-
ing the phrase extraction process. Based on the
old coverage vector and alignment, we can repeat
the transformation in Figure 1 to calculate the la-
bels for the new phrase. The added model will
then check the consistence between the calculated
labels and the labels predicted by the reordering
model. The number of source words that have in-
consistent labels is the penalty and is then added
into the log-linear framework as a new feature.
4 Comparative Study
The second part of this paper is comparative study
on reordering models. Here we briefly describe
those models which will be compared to later.
4.1 Moses lexicalized reordering model
A B
Figure 2: lexicalized reordering model illustration.
Moses (Koehn et al, 2007) contains a word-
based orientation model, which has three types of
reordering: (m) monotone order, (s) switch with
previous phrase and (d) discontinuous. Figure 2
is an example. The definitions of reordering types
are as follows:
monotone for current phrase, if a word alignment
to the bottom left (point A) exists and there is no
word alignment point at the bottom right position
(point B) .
swap for current phrase, if a word alignment to
the bottom right (point B) exists and there is no
word alignment point at the bottom left position
(point A) .
discontinuous all other cases
Our implementation is same with the default
behavior of Moses lexicalized reordering model.
We count how often each extracted phrase pair is
found with each of the three reordering types. The
add-0.5 smoothing is then applied. Finally, the
probability is estimated with maximum likelihood
principle.
4.2 Maximum entropy reordering model
Figure 3 is an illustration of (Zens and Ney, 2006) .
j is the source word position which is aligned to
the last target word of the current phrase. j? is
the last source word position of the current phrase.
j?? is the source word position which is aligned to
the first target word position of the next phrase.
(Zens and Ney, 2006) proposed a maximum en-
tropy classifier to predict the orientation of the
next phrase given the current phrase. The orien-
tation class cj,j? ,j?? is defined as:
cj,j? ,j??=
?
?
?
left, if j??<j
right, if j??>j and j?? ? j?>1
monotone, if j??>j and j?? ? j?=1
(4)
The orientation probability is modeled in a log-
linear framework using a set of N feature func-
tions hn(fJ1 , eI1, i, j, cj,j? ,j?? ), n = 1, . . . , N . Thewhole model is:
p?N1 (cj,j? ,j?? |f
J
1 , eI1, i, j)
=
exp(
N?
n=1
?nhn(fJ1 ,eI1,i,j,cj,j? ,j?? ))
?
c?
exp(
N?
n=1
?nhn(fJ1 ,eI1,i,j,c
? ))
(5)
Different features can be used, we use the source
and target word features to train the model.
325
Figure 3: phrase orientation: left, right and monotone. j is the source word position aligned to the last target word of current
phrase. j? is the last source word position of current phrase. j?? is the source word position aligned to the first target word
position of the next phrase.
f1 f2 f3 f4 f5 f6 f7
e1 e2 e3 e4 e5 e6 e7
Figure 4: bilingual LM illustration. The bilingual sequence
is e1 , e2 f3 , e3 f1 , e4 f4 , e5 f4 , e6 f6 f7 , e7 f5 .
4.3 Bilingual LM
The previous two models belong to ?The reorder-
ing is a classification problem?. Now we turn
to ?The reordering is a decoding order problem?.
(Marin?o et al, 2006) implement a translation
model using n-grams. In this way, the translation
system can take full advantage of the smoothing
and consistency provided by standard back-off n-
gram models. Figure 4 is an example. The in-
terpretation is that given the sentence pair (f71 , e71)
and its alignment, the correct translation order is
e1 , e2 f3 , e3 f1 , e4 f4 , e5 f4 , e6 f6 f7 , e7 f5 .
Notice the bilingual units have been ordered ac-
cording to the target side, as the decoder writes
the translation in a left-to-right way. Using the ex-
ample we describe the strategies used for special
cases:
? keep the unaligned target word, e.g. e1
? remove the unaligned source word, e.g. f2
? when one source word aligned to multiple
target words, duplicate the source word for
each target word, e.g. e4 f4 , e5 f4
? when multiple source words aligned to one
target word, put together the source words for
that target word, e.g. e6 f6 f7
After the operation in Figure 4 was done for
all bilingual sentence pairs, we get a decoding
sequence corpus. We build a 9-gram LM us-
ing SRILM toolkit (Stolcke, 2002) with modified
Kneser-Ney smoothing.
The model is added as an additional feature in
Equation (2). To use the bilingual LM, the search
state must be augmented to keep the bilingual unit
decoding sequence. In search, the bilingual LM
is applied similar to the standard target side LM.
The bilingual sequence of phrase pairs will be ex-
tracted using the same strategy in Figure 4 . Sup-
pose the search state is now extended with a new
phrase pair (f? , e?). F? is the bilingual sequence for
the new phrase pair (f? , e?) and F? i is the ith unit
within F? . F? ? is the bilingual sequence history
for current state. We compute the feature score
hbilm(F? , F?
?) of the extended state as follows:
hbilm(F? , F?
?)=? ?
|F? |?
i=1
log p(F? i|F? ? , F? 1, ? ? ? , F? i?1)
(6)
? is the scaling factor for this model. |F? | is the
length of the bilingual decoding sequence.
4.4 Source decoding sequence LM
(Feng et al, 2010) present an simpler version of
the above bilingual LM where they use only the
source side to model the decoding order. The
source word decoding sequence in Figure 4 is then
f3 , f1 , f2 , f4 , f6 , f7 , f5 . We also build a 9-gram
LM based on the source word decoding sequences.
The usage of the model is same as bilingual LM.
4.5 Syntactic cohesion model
The previous two models belong to ?The reorder-
ing is a decoding order problem?. Now we turn to
?The reordering can be solved by outside heuris-
tics?. (Cherry, 2008) proposed a syntactic cohe-
sion model. The core idea is that the syntactic
structure of the source sentence should be pre-
served during translation. This structure is repre-
sented by a source sentence dependency tree. The
algorithm is as follows: given the source sentence
and its dependency tree, during the translation pro-
cess, once a hypothesis is extended, check if the
source dependency tree contains a subtree T such
that:
326
? Its translation is already started (at least one
node is covered)
? It is interrupted by the new added phrase (at
least one word in the new source phrase is not
in T )
? It is not finished (after the new phrase is
added, there is still at least one free node in
T )
If so, we say this hypothesis violates the subtree
T , and the model returns the number of subtrees
that this hypothesis violates.
4.6 Semantic cohesion model
(Feng et al, 2012) propose two structure features
from semantic role labeling (SRL) results. Simi-
lar to the previous model, the SRL information is
used as soft constraints. During decoding process,
the first feature will report how many event layers
that one search state violates and the second fea-
ture will report the amount of semantic roles that
one search state violates. In this paper, the two
features have been used together. So when the se-
mantic cohesion model is used, both features will
be triggered.
4.7 Tree-based jump model
(Wang et al, 2007) present a pre-reordering
method for Chinese-English translation task. In
Section 3.6 of (Zhang, 2013), instead of doing
hard reordering decision, the author uses the rules
as soft constraints in the decoder. In this paper,
we use the similar method as described in (Zhang,
2013). Our strategy is: firstly, we parse the source
sentences to get constituency trees. Then we ma-
nipulate the trees using heuristics described by
(Wang et al, 2007) . The leaf nodes in the revised
tree constitute the reordered source sentence. Fi-
nally, in the log-linear framework (Equation 2) a
new jump model is added which uses the reordered
source sentence to calculate the cost. For example,
the original sentence f1f2f3f4f5 is now converted
by rules into the new sentence f1f5f3f2f4 . For
decoding, we still use the original sentence. Sup-
pose previously translated source phrase is f1 and
the current phrase is f5 . Then the standard jump
model gives cost qDist = 4 and the new tree-based
jump model will return a cost qDist new = 1 .
5 Experiments
In this section, we describe the baseline setup, the
CRFs training results, the RNN training results
and translation experimental results.
5.1 Experimental Setup
Our baseline is a phrase-based decoder, which in-
cludes the following models: an n-gram target-
side language model (LM), a phrase translation
model and a word-based lexicon model. The latter
two models are used for both directions: p(f |e)
and p(e|f). Additionally we use phrase count
features, word and phrase penalty. The reorder-
ing model for the baseline system is the distance-
based jump model which uses linear distance.
This model does not have hard limit. We list the
important information regarding the experimental
setup below. All those conditions have been kept
same in this work.
? lowercased training data from the GALE task
(Table 1, UN corpus not included)
alignment trained with GIZA++
? tuning corpus: NIST06
test corpora: NIST02 03 04 05 and 08
? 5-gram LM (1 694 412 027 running words)
trained by SRILM toolkit (Stolcke, 2002)
with modified Kneser-Ney smoothing
training data: target side of bilingual data.
? BLEU (Papineni et al, 2001) and TER
(Snover et al, 2005) reported
all scores calculated in lowercase way.
? Wapiti toolkit (Lavergne et al, 2010) used for
CRFs; RNN is built by the RNNLIB toolkit.
Chinese English
Sentences 5 384 856
Running Words 115 172 748 129 820 318
Vocabulary 1 125 437 739 251
Table 1: translation model and LM training data statistics
Table 1 contains the data statistics used for
translation model and LM. For the reordering
model, we take two further filtering steps. Firstly,
we delete the sentence pairs if the source sentence
length is one. When the source sentence has only
one word, the translation will be always mono-
tonic and the reordering model does not need to
learn this. Secondly, we delete the sentence pairs if
the source sentence contains more than three con-
tiguous unaligned words. When this happens, the
sentence pair is usually low quality hence not suit-
able for learning. The main purpose of the two
filtering steps is to further lay down the computa-
tional burden. The label distribution is depicted in
Figure 5. We can see that most words are mono-
tonic. We then divide the corpus to three parts:
327
0 0.5 1 1.5 2 2.5 3 3.5 4 4.5 5?107
BEGIN-RmonoBEGIN-Rreorder
END-LmonoEND-Lreorder
Lmono-RmonoLmono-Rreorder
Lreorder-RmonoLreorder-Rreorder
UNALIGN
Amount of Tags
Figure 5: Tags distribution illustration.
train, validation and test. The source side data
statistics for the reordering model training is given
in Table 2 (target side has only nine labels).
train validation test
Sentences 2 973 519 400 000 400 000
Running Words 62 263 295 8 370 361 8 382 086
Vocabulary 454 951 149686 150 007
Table 2: tagging-style model training data statistics
5.2 CRFs Training Results
The toolkit Wapiti (Lavergne et al, 2010) is used
in this paper. We choose the classical optimization
algorithm limited memory BFGS (L-BFGS) (Liu
and Nocedal, 1989). For regularization, Wapiti
uses both the `1 and `2 penalty terms, yielding the
elastic-net penalty of the form
?1? ? ? ?1 +
?2
2 ? ? ? ?
2
2 (7)
In this work, we use as many features as pos-
sible because `1 penalty ?1 ? ? ?1 is able to
yield sparse parameter vectors, i.e. using a `1
penalty term implicitly performs the feature selec-
tion. The computational costs are given here: on
a cluster with two AMD Opteron(tm) Processor
6176 (total 24 cores), the training time is about 16
hours, peak memory is around 120G. Several ex-
periments have been done to find the suitable hy-
perparameter ?1 and ?2, we choose the model with
lowest error rate on validation corpus for trans-
lation experiments. The error rate of the chosen
model on test corpus (the test corpus in Table 2)
is 25.75% for token error rate and 69.39% for se-
quence error rate. Table 3 is the feature template
we set initially which generates 722 999 637 fea-
tures. Some examples are given in Table 4. After
training 36 902 363 features are kept.
5.3 RNN Training Results
We also applied RNN to the task as an alternative
approach to CRFs. The here used RNN implemen-
tation is RNNLIB which has support for long short
term memory (LSTM) (Graves, 2008). We used
a one of k encoding for the input word and also
for the labels. After testing several configurations
over the validation corpus we used a network with
Feature Templates
1-gram source word features
x[-4,0], x[-3,0], x[-2,0], x[-1,0]
x[0,0], x[1,0], x[2,0], x[3,0], x[4,0]
1-gram source POS features
x[-4,1], x[-3,1], x[-2,1], x[-1,1]
x[0,1], x[1,1], x[2,1], x[3,1], x[4,1]
2-gram source word features
x[-1,0]/x[0,0], x[ 0,0]/x[1,0]
x[-1,1]/x[0,1], x[0,1]/x[1,1]
3-gram source word features
x[-1,0]/x[0,0]/x[1,0]
x[-2,0]/x[-1,0]/x[0,0]
x[0,0]/x[1,0]/x[2,0]
3-gram source POS features
x[0,1]/x[1,1]/x[2,1]
x[-2,1]/x[-1,1]/x[0,1]
x[-1,1]/x[0,1]/x[1,1]
4-gram source POS features
x[0,1]/x[1,1]/x[2,1]/x[3,1]
x[0,1]/x[-1,1]/x[-2,1]/x[-3,1]
x[-1,1]/x[0,1]/x[1,1]/x[2,1]
x[-2,1]/x[-1,1]/x[0,1]/x[1,1]
5-gram source POS features
x[0,1]/x[1,1]/x[2,1]/x[3,1]/x[4,1]
x[-4,1]/x[-3,1]/x[-2,1]/x[-1,1]/x[0,1]
x[-2,1]/x[-1,1]/x[0,1]/x[1,1]/x[2,1]
bigram output label feature
x[-1,2]/x[0,2]
Table 3: feature templates for CRFs training
Words POS Label
?? P BEGIN-Rmono
? DT Lmono-Rmono
? M Lmono-Rmono
?? NN Lmono-Rmono
, PU Lmono-Rmono
?? PN Lmono-Rmono
? VC UNALIGN  Current label
?? VV Lmono-Rmono
??? NN Lmono-Rmono
? DEC UNALIGN
? PU END-Lmono
Table 4: feature examples. x[row,col] specifies a token in the
input data. row specfies the relative position from the cur-
rent label and col specifies the absolute position of the col-
umn. So for the current lable in this table, x[?1, 2]/x[0, 2]
is Lmono-Rmono/UNALIGN and x[?1, 1]/x[0, 1]/x[1, 1] is
PN/VC/VV.
LSTM 200 nodes in the hidden layer. The RNN
has a token error rate of 27.31% and a sentence
error rate of 77.00% over the test corpus in Ta-
ble 2. The RNN is trained on a similar computer
as above. RNNLIB utilizes only one thread. The
training time is about three and a half days and
peak memory consumption is 1G .
5.4 Comparison of CRFs and RNN errors
CRFs performs better than RNN (token error rate
25.75% vs 27.31%). Both error rate values are
much higher than what we usually see in part-of-
speech tagging task. The main reason is that the
?annotated? corpus is converted from word align-
ment which contains lots of error. However, as we
328
hhhhhhhhhhReference
Prediction Unalign BEGIN-Rm BEGIN-Rr END-Lm END-Lr Lm-Rm Lr-Rm Lm-Rr Lr-Rr
Unalign 687724 15084 850 7347 716 493984 107364 43457 9194
BEGIN-Rmono 3537 338315 6209 0 0 0 0 0 0
BEGIN-Rreorder 419 12557 17054 0 0 0 0 0 0
END-Lmono 1799 0 0 365635 3196 0 0 0 0
END-Lreorder 510 0 0 5239 7913 0 0 0 0
Lmomo-Rmono 188627 0 0 0 0 4032738 176682 150952 13114
Lreorder-Rmono 88177 0 0 0 0 369232 433027 27162 15275
Lmomo-Rreorder 32342 0 0 0 0 268570 24558 296033 10645
Lreorder-Rreorder 9865 0 0 0 0 34746 20382 16514 45342
Recall 50.36% 97.20% 56.79% 98.65% 57.92% 88.40% 46.42% 46.83% 35.74%
Precision 67.89% 92.45% 70.73% 96.67% 66.92% 77.56% 56.83% 55.42% 48.46%
Table 5: CRF Confusion Matrix. Abbreviations: Lmono(Lm) Lreorder(Lr) Rmono(Rm) Rreorder(Rr)
hhhhhhhhhhReference
Prediction Unalign BEGIN-Rm BEGIN-Rr END-Lm END-Lr Lm-Rm Lr-Rm Lm-Rr Lr-Rr
Unalign 589100 17299 901 7870 1000 639555 82413 24277 3305
BEGIN-Rmono 1978 339686 6397 0 0 0 0 0 0
BEGIN-Rreorder 186 13812 16032 0 0 0 0 0 0
END-Lmono 2258 0 0 364121 4251 0 0 0 0
END-Lreorde 699 0 0 4693 8269 1 0 0 0
Lmomo-Rmono 142777 1 0 0 0 4232113 105266 78692 3264
Lreorder-Rmono 96278 0 1 0 0 491989 323272 14635 6698
Lmomo-Rreorder 31118 0 0 0 0 380483 18144 198068 4335
Lreorder-Rreorder 12366 0 1 0 0 50121 25196 17008 22157
Recall 43.13% 97.59% 53.39% 98.24% 60.53% 92.77% 34.65% 31.33% 17.47%
Precision 67.19% 91.61% 68.71% 96.66% 61.16% 73.04% 58.32% 59.54% 55.73%
Table 6: RNN Confusion Matrix. Abbreviations: Lmono(Lm) Lreorder(Lr) Rmono(Rm) Rreorder(Rr)
will show later, the model trained with both CRFs
and RNN help to improve the translation quality.
Table 5 and Table 6 demonstrate the confusion
matrix of the CRFs and RNN errors over the test
corpus. The rows represent the correct tag that the
classifier should have predicted and the columns
are the actually predicted tags. E.g. the number
687724 in first row and first column of Table 5
tells that there are 687724 correctly labeled Un-
align tags. The number 15084 in first row and
second column of Table 5 represents that there are
15084 Unalign tags labeled incorrectly to Begin-
Rmono. Therefore, numbers on the diagonal from
the upper left to the lower right corner represent
the amount of correctly classified tags and all other
numbers show the amount of false labels. The
many zeros show that both classifier rarely make
mistake for the label ?BEGIN-?? which only oc-
cur at the beginning of a sentence. The same is
true for the ?END-?? labels.
5.5 Translation Results
Results are summarized in Table 7. Please read
the caption for the meaning of abbreviations. An
Index column is added for score reference conve-
nience (B for BLEU; T for TER). For the proposed
model, significance testing results on both BLEU
and TER are reported (B2 and B3 compared to B1,
T2 and T3 compared to T1). We perform bootstrap
resampling with bounds estimation as described
in (Koehn, 2004). The 95% confidence threshold
(denoted by ? in the table) is used to draw signifi-
cance conclusions. We add a column avg. to show
the average improvements.
From Table 7 we see that the proposed reorder-
ing model using CRFs improves the baseline by
0.98 BLEU and 1.21 TER on average, while the
proposed reordering model using RNN improves
the baseline by 1.32 BLEU and 1.53 TER on av-
erage. For line B2 B3 and T2 T3, most scores
are better than their corresponding baseline values
with more than 95% confidence. The results show
that our proposed idea improves the baseline sys-
tem and RNN trained model performs better than
CRFs trained model, in terms of both automatic
measure and significance test. To investigate why
RNN has lower performance for the tagging task
but achieves better BLEU, we build a 3-gram LM
on the source side of the training corpus in Table
2 and perplexity values are listed in Table 8. The
perplexity of the test corpus for reordering model
comparison is much lower than those NIST cor-
pora for translation experiments. In other words,
there exists mismatch of the data for reordering
model training and actual MT data. This could
explain why CRFs is superior to RNN for labeling
problem while RNN is better for MT tasks.
For the comparative study, the best method is
the tree-based jump model (JUMPTREE). Our
proposed model ranks the second position. The
difference is tiny: on average only 0.08 BLEU (B3
and B10) and 0.15 TER (T3 and T10). Even with
329
Systems NIST02 NIST03 NIST04 NIST05 NIST08 avg. Index
BLEU scores
baseline 33.60 34.29 35.73 32.15 26.34 - B1
baseline+CRFs 34.53 35.19 36.56? 33.30? 27.41? 0.98 B2
baseline+RNN 35.30? 35.34? 37.03? 33.80? 27.23? 1.32 B3
baseline+LRM 34.87 34.90 36.40 33.43 27.45 0.99 B4
baseline+MERO 34.91 34.83 36.29 33.69 26.66 0.85 B5
baseline+BILM 35.21 35.00 36.83 33.64 27.39 1.19 B6
baseline+SRCLM 34.55 34.52 36.18 32.84 27.03 0.50 B7
baseline+SRL 35.05 34.93 36.71 33.22 26.89 0.93 B8
baseline+SC 34.96 34.52 36.37 33.35 26.90 0.79 B9
baseline+JUMPTREE 35.10 35.53 37.12 34.18 27.19 1.40 B10
baseline+LRM+MERO+BILM+SRCLM+SRL+SC+JUMPTREE 36.77 36.16 38.10 35.67 28.52 2.62 B11
baseline+LRM+MERO+BILM+SRCLM+SRL+SC+JUMPTREE+RNN 36.99 37.00 38.79 35.86 28.99 3.10 B12
TER scores
baseline 61.36 60.48 59.12 60.94 65.17 - T1
baseline+CRFs 60.14? 58.91? 57.91? 59.77? 64.30? 1.21 T2
baseline+RNN 59.38? 58.87? 57.60? 59.56? 63.99? 1.53 T3
baseline+LRM 60.07 59.08 58.42 59.74 64.50 1.05 T4
baseline+MERO 60.19 59.58 58.51 59.49 64.68 0.92 T5
baseline+BILM 60.23 59.93 58.59 60.09 64.72 0.70 T6
baseline+SRCLM 60.27 59.55 58.40 60.16 64.61 0.82 T7
baseline+SRL 60.05 59.55 58.14 59.69 64.74 0.98 T8
baseline+SC 59.90 59.37 58.27 59.69 64.44 1.08 T9
baseline+JUMPTREE 59.53 58.54 57.67 58.90 64.04 1.68 T10
baseline+LRM+MERO+BILM+SRCLM+SRL+SC+JUMPTREE 59.16 57.84 56.83 58.03 63.20 2.40 T11
baseline+LRM+MERO+BILM+SRCLM+SRL+SC+JUMPTREE+RNN 58.67 57.67 56.27 58.00 63.09 2.67 T12
Table 7: Experimental results. CRFs and RNN mean the tagging-style model trained with CRFs or RNN; LRM for lexicalized
reordering model (Koehn et al, 2007) ; MERO for maximum entropy reordering model (Zens and Ney, 2006) ; BILM for
bilingual language model (Marin?o et al, 2006) and SRCLM for its simpler version source decoding sequence model (Feng et
al., 2010) ; SC for syntactic cohesion model (Cherry, 2008) ; SRL for semantic cohesion model (Feng et al, 2012); JUMPTREE
for our tree-based jump model based on (Wang et al, 2007).
Running Words OOV Perplexity
Test in Table 2 8 382 086 33854 74.364
NIST02 22 749 195 176.806
NIST03 24 180 290 274.679
NIST04 49 612 320 170.507
NIST05 29 966 228 279.402
NIST08 32 502 511 408.067
Table 8: perplexity
a strong system (B11 and T11), our model is still
able to provide improvements (B12 and T12).
6 Conclusion
In this paper, a novel tagging style reordering
model has been proposed. By our method, the re-
ordering problem is converted into a sequence la-
beling problem so that the whole source sentence
is taken into consideration for reordering decision.
By adding an unaligned word tag, the unaligned
word phenomenon is automatically implanted in
the proposed model. The model is utilized as soft
constraints in the decoder. In practice, we do not
experience decoding memory increase nor speed
slow down.
We choose CRFs and RNN to accomplish the
sequence labeling task. The CRFs achieves lower
error rate on the tagging task but RNN trained
model is better for the translation task. Experi-
mental results show that our model is stable and
improves the baseline system by 0.98 BLEU and
1.21 TER (trained by CRFs) and 1.32 BLEU and
1.53 TER (trained by RNN). Most of the scores
are better than their corresponding baseline values
with more than 95% confidence. We also compare
our method with several other popular reorder-
ing models. Our model ranks the second position
which is slightly worse than the tree-based jump
model. However, the tree-based jump model re-
lies on manually designed reordering rules which
does not exist for many language pairs while our
model can be easily adapted to other translation
tasks. We also show that the proposed model is
able to improve a very strong baseline system.
The main contributions of the paper are: pro-
pose the tagging-style reordering model and im-
prove the translation quality; compare two se-
quence labeling techniques CRFs and RNN; com-
pare our method with seven other reordering mod-
els. To our best knowledge, it is the first time that
the above two comparisons have been reported .
Acknowledgments
This work was partly realized as part of the
Quaero Programme, funded by OSEO, French
State agency for innovation, and also partly based
upon work supported by the Defense Advanced
Research Projects Agency (DARPA) under Con-
tract No. HR0011-12-C-0015. Any opinions,
findings and conclusions or recommendations ex-
pressed in this material are those of the authors and
do not necessarily reflect the views of the Defense
Advanced Research Projects Agency (DARPA).
330
References
Yoshua Bengio, Patrice Simard, and Paolo Frasconi.
1994. Learning long-term dependencies with gradi-
ent descent is difficult. IEEE Transactions on Neu-
ral Networks, 5(2):157?166, March.
Adam Berger, Peter F. Brown, Stephen A. Pietra, Vin-
cent J. Pietra, Andrew S. Kehler, and Robert L.
Mercer. 1996. Language translation apparatus and
method of using Context-Based translation models.
United States Patent, No. 5,510,981.
Colin Cherry. 2008. Cohesive phrase-based decoding
for statistical machine translation. In Proceedings
of Annual Meeting of the Association for Computa-
tional Linguistics: Human Language Technologies
(ACL: HLT), pages 72?80, Columbus, Ohio, USA,
June. Association for Computational Linguistics.
Jeffrey L. Elman. 1990. Finding structure in time.
Cognitive Science, 14(2):179?211.
Minwei Feng, Arne Mauser, and Hermann Ney. 2010.
A source-side decoding sequence model for statisti-
cal machine translation. In Proceedings of the As-
sociation for Machine Translation in the Americas
(AMTA), Denver, CO, USA, October.
Minwei Feng, Weiwei Sun, and Hermann Ney. 2012.
Semantic cohesion model for phrase-based SMT.
In Proceedings of the International Conference on
Computational Linguistics (COLING), pages 867?
878, Mumbai, India, December. The COLING 2012
Organizing Committee.
Michel Galley and Christopher D. Manning. 2008.
A simple and effective hierarchical phrase reorder-
ing model. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing
(EMNLP), pages 848?856, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Alex Graves. 2008. Supervised Sequence Labelling
with Recurrent Neural Networks. Ph.D. thesis,
Technical University of Munich, July.
Sepp Hochreiter and Ju?rgen Schmidhuber. 1997.
Long short-term memory. Neural Computation,
9(8):1735?1780, November.
Michael I. Jordan. 1990. Attractor dynamics and
parallelism in a connectionist sequential machine.
IEEE Computer Society Neural Networks Technol-
ogy Series, pages 112?127.
Kevin Knight. 1999. Decoding complexity in word-
replacement translation models. Computational
Linguistics, 25(4):607?615.
Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003.
Statistical phrase-based translation. In Proceedings
of the Conference of the North American Chapter
of the Association for Computational Linguistics on
Human Language Technology (NAACL) - Volume 1,
pages 48?54, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
Proceedings of the Annual Meeting of the Associ-
ation for Computational Linguistics (ACL), demon-
stration session, pages 177?180, Prague, Czech Re-
public, June.
Philipp Koehn. 2004. Statistical significance tests
for machine translation evaluation. In Proceedings
of the Conference on Empirical Methods in Natu-
ral Language Processing (EMNLP), pages 388?395,
Barcelona, Spain, July. Association for Computa-
tional Linguistics.
John D. Lafferty, Andrew McCallum, and Fernando
C. N. Pereira. 2001. Conditional random fields:
Probabilistic models for segmenting and labeling se-
quence data. In Proceedings of the Eighteenth Inter-
national Conference on Machine Learning (ICML),
pages 282?289, San Francisco, CA, USA. Morgan
Kaufmann Publishers Inc.
Kevin J. Lang, Alex H. Waibel, and Geoffrey E. Hin-
ton. 1990. A time-delay neural network architec-
ture for isolated word recognition. Neural networks,
3(1):23?43, January.
Thomas Lavergne, Olivier Cappe?, and Franc?ois Yvon.
2010. Practical very large scale CRFs. In Proceed-
ings the 48th Annual Meeting of the Association for
Computational Linguistics (ACL), pages 504?513.
Association for Computational Linguistics, July.
Dong C. Liu and Jorge Nocedal. 1989. On the lim-
ited memory bfgs method for large scale optimiza-
tion. Mathematical Programming, 45(3):503?528,
December.
Jose? B. Marin?o, Rafael E. Banchs, Josep Maria Crego,
Adria` de Gispert, Patrik Lambert, Jose? A. R. Fonol-
losa, and Marta R. Costa-Jussa`. 2006. N-gram-
based machine translation. Computational Linguis-
tics, 32(4):527?549, December.
Franz J. Och and Hermann Ney. 2002. Discriminative
training and maximum entropy models for statisti-
cal machine translation. In Proceedings of Annual
Meeting of the Association for Computational Lin-
guistics (ACL), pages 295?302, Philadelphia, Penn-
sylvania, USA, July.
Franz J. Och, Christoph Tillmann, and Hermann Ney.
1999. Improved alignment models for statistical
machine translation. In Proceedings of the Joint
SIGDAT Conference on Empirical Methods in Nat-
ural Language Processing and Very Large Cor-
pora (EMNLP), pages 20?28, University of Mary-
land, College Park, MD, USA, June. Association for
Computational Linguistics.
331
Franz J. Och, Daniel Gildea, Sanjeev Khudanpur,
Anoop Sarkar, Kenji Yamada, Alex Fraser, Shankar
Kumar, Libin Shen, David Smith, Katherine Eng,
Viren Jain, Zhen Jin, and Dragomir Radev. 2004.
A smorgasbord of features for statistical machine
translation. In Proceedings of the Conference
on Statistical Machine Translation at the North
American Chapter of the Association for Compu-
tational Linguistics on Human Language Technol-
ogy (NAACL-HLT), pages 161?168, Boston, Mas-
sachusetts, USA, May. Association for Computa-
tional Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2001. Bleu: a method for automatic eval-
uation of machine translation. IBM Research Re-
port, RC22176 (W0109-022), September.
David. E. Rumelhart, Geoffrey E. Hinton, and
Ronald J. Williams. 1986. Learning internal repre-
sentations by error propagation. In David E. Rumel-
hart and James L. McClelland, editors, Parallel dis-
tributed processing: explorations in the microstruc-
ture of cognition, vol. 1, pages 318?362. MIT Press,
Cambridge, MA, USA.
Mike Schuster and Kuldip K. Paliwal. 1997. Bidirec-
tional recurrent neural networks. IEEE Transactions
on Signal Processing, 45(11):2673?2681, Novem-
ber.
Matthew Snover, Bonnie Dorr, Richard Schwartz, John
Makhoul, Linnea Micciulla, and Ralph Weischedel.
2005. A study of translation error rate with targeted
human annotation. Technical Report LAMP-TR-
126, CS-TR-4755, UMIACS-TR-2005-58, Univer-
sity of Maryland, College Park, MD.
Andreas Stolcke. 2002. Srilm - an extensible lan-
guage modeling toolkit. In Proceedings of Interna-
tional Conference on Spoken Language Processing
(ICSLP), pages 901?904, Denver, Colorado, USA,
September. ISCA.
Charles Sutton and Andrew Mccallum, 2006. In-
troduction to Conditional Random Fields for Rela-
tional Learning, pages 93?128. MIT Press.
Chao Wang, Michael Collins, and Philipp Koehn.
2007. Chinese syntactic reordering for statis-
tical machine translation. In Proceedings of
the Conference on Empirical Methods in Natural
Language Processing (EMNLP), pages 737?745,
Prague, Czech Republic, June. Association for Com-
putational Linguistics.
Dekai Wu. 1995. Stochastic inversion transduction
grammars with application to segmentation, brack-
eting, and alignment of parallel corpora. In Pro-
ceedings of the 14th international joint conference
on Artificial intelligence (IJCAI) - Volume 2, pages
1328?1335, San Francisco, CA, USA, August. Mor-
gan Kaufmann Publishers Inc.
Richard Zens and Hermann Ney. 2003. A compara-
tive study on reordering constraints in statistical ma-
chine translation. In Proceedings of the 41st Annual
Meeting on Association for Computational Linguis-
tics (ACL) - Volume 1, pages 144?151, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
Richard Zens and Hermann Ney. 2006. Discrimina-
tive reordering models for statistical machine trans-
lation. In Proceedings of the Workshop on Statistical
Machine Translation at the North American Chapter
of the Association for Computational Linguistics on
Human Language Technology (NAACL-HLT), pages
55?63, New York City, NY, June. Association for
Computational Linguistics.
Richard Zens, Franz J. Och, and Hermann Ney. 2002.
Phrase-based statistical machine translation. In Ger-
man Conference on Artificial Intelligence, pages 18?
32. Springer Verlag, September.
Yuqi Zhang, Richard Zens, and Hermann Ney. 2007.
Chunk-level reordering of source language sen-
tences with automatically learned rules for statistical
machine translation. In Proceedings of the Work-
shop on Syntax and Structure in Statistical Transla-
tion at the North American Chapter of the Associa-
tion for Computational Linguistics on Human Lan-
guage Technology (NAACL-HLT)/Association for
Machine Translation in the Americas (AMTA), pages
1?8, Morristown, NJ, USA, April. Association for
Computational Linguistics.
Yuqi Zhang. 2013. The Application of Source Lan-
guage Information in Chinese-English Statistical
Machine Translation. Ph.D. thesis, Computer Sci-
ence Department, RWTH Aachen University, May.
332
Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 193?199,
Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational Linguistics
The RWTH Aachen Machine Translation System for WMT 2013
Stephan Peitz, Saab Mansour, Jan-Thorsten Peter, Christoph Schmidt,
Joern Wuebker, Matthias Huck, Markus Freitag and Hermann Ney
Human Language Technology and Pattern Recognition Group
Computer Science Department
RWTH Aachen University
D-52056 Aachen, Germany
<surname>@cs.rwth-aachen.de
Abstract
This paper describes the statistical ma-
chine translation (SMT) systems devel-
oped at RWTH Aachen University for
the translation task of the ACL 2013
Eighth Workshop on Statistical Machine
Translation (WMT 2013). We partici-
pated in the evaluation campaign for the
French-English and German-English lan-
guage pairs in both translation directions.
Both hierarchical and phrase-based SMT
systems are applied. A number of dif-
ferent techniques are evaluated, including
hierarchical phrase reordering, translation
model interpolation, domain adaptation
techniques, weighted phrase extraction,
word class language model, continuous
space language model and system combi-
nation. By application of these methods
we achieve considerable improvements
over the respective baseline systems.
1 Introduction
For the WMT 2013 shared translation task1
RWTH utilized state-of-the-art phrase-based and
hierarchical translation systems as well as an in-
house system combination framework. We give
a survey of these systems and the basic meth-
ods they implement in Section 2. For both
the French-English (Section 3) and the German-
English (Section 4) language pair, we investigate
several different advanced techniques. We con-
centrate on specific research directions for each
of the translation tasks and present the respec-
tive techniques along with the empirical results
they yield: For the French?English task (Sec-
tion 3.2), we apply a standard phrase-based sys-
tem with up to five language models including a
1http://www.statmt.org/wmt13/
translation-task.html
word class language model. In addition, we em-
ploy translation model interpolation and hierarchi-
cal phrase reordering. For the English?French
task (Section 3.1), we train translation mod-
els on different training data sets and augment
the phrase-based system with a hierarchical re-
ordering model, a word class language model,
a discriminative word lexicon and a insertion
and deletion model. For the German?English
(Section 4.3) and English?German (Section 4.4)
tasks, we utilize morpho-syntactic analysis to pre-
process the data (Section 4.1), domain-adaptation
(Section 4.2) and a hierarchical reordering model.
For the German?English task, an augmented hi-
erarchical phrase-based system is set up and we
rescore the phrase-based baseline with a continu-
ous space language model. Finally, we perform a
system combination.
2 Translation Systems
In this evaluation, we employ phrase-based trans-
lation and hierarchical phrase-based translation.
Both approaches are implemented in Jane (Vilar et
al., 2012; Wuebker et al, 2012), a statistical ma-
chine translation toolkit which has been developed
at RWTH Aachen University and is freely avail-
able for non-commercial use.2
2.1 Phrase-based System
In the phrase-based decoder (source cardinality
synchronous search, SCSS), we use the standard
set of models with phrase translation probabilities
and lexical smoothing in both directions, word and
phrase penalty, distance-based distortion model,
an n-gram target language model and three bi-
nary count features. Optional additional models
used in this evaluation are the hierarchical reorder-
ing model (HRM) (Galley and Manning, 2008), a
word class language model (WCLM) (Wuebker et
2http://www.hltpr.rwth-aachen.de/jane/
193
al., 2012), a discriminative word lexicon (DWL)
(Mauser et al, 2009), and insertion and deletion
models (IDM) (Huck and Ney, 2012). The param-
eter weights are optimized with minimum error
rate training (MERT) (Och, 2003). The optimiza-
tion criterion is BLEU.
2.2 Hierarchical Phrase-based System
In hierarchical phrase-based translation (Chiang,
2007), a weighted synchronous context-free gram-
mar is induced from parallel text. In addition to
continuous lexical phrases, hierarchical phrases
with up to two gaps are extracted. The search is
carried out with a parsing-based procedure. The
standard models integrated into our Jane hierar-
chical systems (Vilar et al, 2010; Huck et al,
2012c) are: phrase translation probabilities and
lexical smoothing probabilities in both translation
directions, word and phrase penalty, binary fea-
tures marking hierarchical phrases, glue rule, and
rules with non-terminals at the boundaries, four
binary count features, and an n-gram language
model. Optional additional models comprise IBM
model 1 (Brown et al, 1993), discriminative word
lexicon and triplet lexicon models (Mauser et al,
2009; Huck et al, 2011), discriminative reordering
extensions (Huck et al, 2012a), insertion and dele-
tion models (Huck and Ney, 2012), and several
syntactic enhancements like preference grammars
(Stein et al, 2010) and soft string-to-dependency
features (Peter et al, 2011). We utilize the cube
pruning algorithm for decoding (Huck et al, 2013)
and optimize the model weights with MERT. The
optimization criterion is BLEU.
2.3 System Combination
System combination is used to produce consensus
translations from multiple hypotheses generated
with different translation engines. First, a word
to word alignment for the given single system hy-
potheses is produced. In a second step a confusion
network is constructed. Then, the hypothesis with
the highest probability is extracted from this con-
fusion network. For the alignment procedure, one
of the given single system hypotheses is chosen as
primary system. To this primary system all other
hypotheses are aligned using the METEOR (Lavie
and Agarwal, 2007) alignment and thus the pri-
mary system defines the word order. Once the
alignment is given, the corresponding confusion
network is constructed. An example is given in
Figure 1.
The model weights of the system combination
are optimized with standard MERT on 100-best
lists. For each single system, a factor is added to
the log-linear framework of the system combina-
tion. Moreover, this log-linear model includes a
word penalty, a language model trained on the in-
put hypotheses, a binary feature which penalizes
word deletions in the confusion network and a pri-
mary feature which marks the system which pro-
vides the word order. The optimization criterion is
4BLEU-TER.
2.4 Other Tools and Techniques
We employ GIZA++ (Och and Ney, 2003) to train
word alignments. The two trained alignments are
heuristically merged to obtain a symmetrized word
alignment for phrase extraction. All language
models (LMs) are created with the SRILM toolkit
(Stolcke, 2002) and are standard 4-gram LMs
with interpolated modified Kneser-Ney smooth-
ing (Kneser and Ney, 1995; Chen and Goodman,
1998). The Stanford Parser (Klein and Manning,
2003) is used to obtain parses of the training data
for the syntactic extensions of the hierarchical sys-
tem. We evaluate in truecase with BLEU (Papineni
et al, 2002) and TER (Snover et al, 2006).
2.5 Filtering of the Common Crawl Corpus
The new Common Crawl corpora contain a large
number of sentences that are not in the labelled
language. To clean these corpora, we first ex-
tracted a vocabulary from the other provided cor-
pora. Then, only sentences containing at least
70% word from the known vocabulary were kept.
In addition, we discarded sentences that contain
more words from target vocabulary than source
vocabulary on the source side. These heuristics
reduced the French-English Common Crawl cor-
pus by 5,1%. This filtering technique was also ap-
plied on the German-English version of the Com-
mon Crawl corpus.
3 French?English Setups
We trained phrase-based translation systems for
French?English and for English?French. Cor-
pus statistics for the French-English parallel data
are given in Table 1. The LMs are 4-grams trained
on the provided resources for the respective lan-
guage (Europarl, News Commentary, UN, 109,
Common Crawl, and monolingual News Crawl
194
0
1
5:
th
at
/1
7:
th
is/
3
2
3:
is/
3
8:
w
as
/1
3
0:
*E
PS
*/
3
4:
it/
1
4
0:
*E
PS
*/
3
2:
in
/1
5
0:
*E
PS
*/
3
6:
th
e/
1
6
0:
*E
PS
*/
1
1:
fu
tu
re
/3
Figure 1: Confusion network of four different hypotheses.
Table 1: Corpus statistics of the preprocessed
French-English parallel training data. EPPS de-
notes Europarl, NC denotes News Commentary,
CC denotes Common Crawl. In the data, numeri-
cal quantities have been replaced by a single cate-
gory symbol.
French English
EPPS Sentences 2.2M
+ NC Running Words 64.7M 59.7M
Vocabulary 153.4K 132.2K
CC Sentences 3.2M
Running Words 88.1M 80.9.0M
Vocabulary 954.8K 908.0K
UN Sentences 12.9M
Running Words 413.3M 362.3M
Vocabulary 487.1K 508.3K
109 Sentences 22.5M
Running Words 771.7M 661.1M
Vocabulary 1 974.0K 1 947.2K
All Sentences 40.8M
Running Words 1 337.7M 1 163.9M
Vocabulary 2 749.8K 2 730.1K
language model training data).3
3.1 Experimental Results English?French
For the English?French task, separate translation
models (TMs) were trained for each of the five
data sets and fed to the decoder. Four additional
indicator features are introduced to distinguish the
different TMs. Further, we applied the hierar-
chical reordering model, the word class language
model, the discriminative word lexicon, and the
insertion and deletion model. Table 2 shows the
results of our experiments.
As a development set for MERT, we use new-
stest2010 in all setups.
3.2 Experimental Results French?English
For the French?English task, a translation model
(TM) was trained on all available parallel data.
For the baseline, we interpolated this TM with
3The parallel 109 corpus is often also referred to as WMT
Giga French-English release 2.
an in-domain TM trained on EPPS+NC and em-
ployed the hierarchical reordering model. More-
over, three language models were used: The first
language model was trained on the English side
of all available parallel data, the second one on
EPPS and NC and the third LM on the News Shuf-
fled data. The baseline was improved by adding a
fourth LM trained on the Gigaword corpus (Ver-
sion 5) and a 5-gram word class language model
trained on News Shuffled data. For the WCLM,
we used 50 word classes clustered with the tool
mkcls (Och, 2000). All results are presented in Ta-
ble 3.
4 German?English Setups
For both translation directions of the German-
English language pair, we trained phrase-based
translation systems. Corpus statistics for German-
English can be found in Table 4. The language
models are 4-grams trained on the respective tar-
get side of the bilingual data as well as on the
provided News Crawl corpus. For the English
language model the 109 French-English, UN and
LDC Gigaword Fifth Edition corpora are used ad-
ditionally.
4.1 Morpho-syntactic Analysis
In order to reduce the source vocabulary size for
the German?English translation, the German text
is preprocessed by splitting German compound
words with the frequency-based method described
in (Koehn and Knight, 2003). To further reduce
translation complexity, we employ the long-range
part-of-speech based reordering rules proposed by
Popovic? and Ney (2006).
4.2 Domain Adaptation
This year, we experimented with filtering and
weighting for domain-adaptation for the German-
English task. To perform adaptation, we define a
general-domain (GD) corpus composed from the
news-commentary, europarl and Common Crawl
corpora, and an in-domain (ID) corpus using
a concatenation of the test sets (newstest{2008,
2009, 2010, 2011, 2012}) with the correspond-
ing references. We use the test sets as in-domain
195
Table 2: Results for the English?French task (truecase). newstest2010 is used as development set.
BLEU and TER are given in percentage.
newstest2008 newstest2009 newstest2010 newstest2011 newstest2012
English?French BLEU TER BLEU TER BLEU TER BLEU TER BLEU TER
TM:EPPS + HRM 22.9 63.0 25.0 60.0 27.8 56.7 28.9 54.4 27.2 57.1
TM:UN + HRM 22.7 63.4 25.0 60.0 28.3 56.4 29.5 54.2 27.3 57.1
TM:109 + HRM 23.5 62.3 26.0 59.2 29.6 55.2 30.3 53.3 28.0 56.4
TM:CC + HRM 23.5 62.3 26.2 58.8 29.2 55.3 30.3 53.3 28.2 56.0
TM:NC 21.0 64.8 22.3 61.6 25.6 58.7 26.9 56.6 25.7 58.5
+ HRM 21.5 64.3 22.6 61.2 26.1 58.4 27.3 56.1 26.0 58.2
+ TM:EPPS,CC,UN 23.9 61.8 26.4 58.6 29.9 54.7 31.0 52.7 28.6 55.6
+ TM:109 24.0 61.5 26.5 58.4 30.2 54.2 31.1 52.3 28.7 55.3
+ WCLM, DWL, IDM 24.0 61.6 26.5 58.3 30.4 54.0 31.4 52.1 28.8 55.2
Table 3: Results for the French?English task (truecase). newstest2010 is used as development set.
BLEU and TER are given in percentage.
newstest2010 newstest2011 newstest2012
French?English BLEU TER BLEU TER BLEU TER
SCSS baseline 28.1 54.6 29.1 53.3 - -
+ GigaWord.v5 LM 28.6 54.2 29.6 52.9 29.6 53.3
+ WCLM 29.1 53.8 30.1 52.5 29.8 53.1
(newswire) as the other corpora are coming from
differing domains (news commentary, parliamen-
tary discussions and various web sources), and on
initial experiments, the other corpora did not per-
form well when used as an in-domain representa-
tive for adaptation. To check whether over-fitting
occurs, we measure the results of the adapted
systems on the evaluation set of this year (new-
stest2013) which was not used as part of the in-
domain set.
The filtering experiments are done similarly to
(Mansour et al, 2011), where we compare filtering
using LM and a combined LM and IBM Model 1
(LM+M1) based scores. The scores for each sen-
tence pair in the general-domain corpus are based
on the bilingual cross-entropy difference of the
in-domain and general-domain models. Denoting
HLM (x) as the cross entropy of sentence x ac-
cording to LM , then the cross entropy difference
DHLM (x) can be written as:
DHLM (x) = HLMID(x)?HLMGD(x)
The bilingual cross entropy difference for a sen-
tence pair (s, t) in the GD corpus is then defined
by:
DHLM (s) + DHLM (t)
For IBM Model 1 (M1), the cross-entropy
HM1(s|t) is defined similarly to the LM cross-
entropy, and the resulting bilingual cross-entropy
difference will be of the form:
DHM1(s|t) + DHM1(t|s)
The combined LM+M1 score is obtained by
summing the LM and M1 bilingual cross-entropy
difference scores. To perform filtering, the GD
corpus sentence pairs are scored by the appropri-
ate method, sorted by the score, and the n-best sen-
tences are then used to build an adapted system.
In addition to adaptation using filtering, we ex-
periment with weighted phrase extraction similar
to (Mansour and Ney, 2012). We differ from their
work by using a combined LM+M1 weight to per-
form the phrase extraction instead of an LM based
weight. We use a combined LM+M1 weight as
this worked best in the filtering experiments, mak-
ing scoring with LM+M1 more reliable than LM
scores only.
4.3 Experimental Results German?English
For the German?English task, the baseline is
trained on all available parallel data and includes
the hierarchical reordering model. The results of
the various filtering and weighting experiments are
summarized in Table 5.
196
Table 5: German-English results (truecase). BLEU and TER are given in percentage. Corresponding
development set is marked with *. ? labels the single systems selected for the system combination.
newstest2009 newstest2010 newstest2011 newstest2012 newstest2013
German?English BLEU TER BLEU TER BLEU TER BLEU TER BLEU TER
SCSS baseline 21.7 61.1 24.8* 58.9* 22.0 61.1 23.4 60.0 26.1 56.4
LM 800K-best 21.6 60.5 24.7* 58.3* 22.0 60.5 23.6 59.7 - -
LM+M1 800K-best 21.4 60.5 24.7* 58.1* 22.0 60.4 23.7 59.2 - -
(LM+M1)*TM 22.1 60.2 25.4* 57.8* 22.5 60.1 24.0 59.1 - -
(LM+M1)*TM+GW 22.8 59.5 25.7* 57.2* 23.1 59.5 24.4 58.6 26.6 55.5
(LM+M1)*TM+GW? 22.9* 61.1* 25.2 59.3 22.8 61.5 23.7 60.8 26.4 57.1
SCSS baseline 22.6* 61.6* 24.1 60.1 22.1 62.0 23.1 61.2 - -
CSLM rescoring? 22.0 60.4 25.1* 58.3* 22.4 60.2 23.9 59.3 26.0 56.0
HPBT? 21.9 60.4 24.9* 58.2* 22.3 60.3 23.6 59.6 25.9 56.3
system combination - - - - 23.4* 59.3* 24.7 58.5 27.1 55.3
Table 6: English-German results (truecase). newstest2009 was used as development set. BLEU and TER
are given in percentage.
newstest2008 newstest2009 newstest2010 newstest2011 newstest2012
English?German BLEU TER BLEU TER BLEU TER BLEU TER BLEU TER
SCSS baseline 14.9 70.9 14.9 70.4 16.0 66.3 15.4 69.5 15.7 67.5
LM 800K-best 15.1 70.9 15.1 70.3 16.2 66.3 15.6 69.4 15.9 67.4
(LM+M1) 800K-best 15.8 70.8 15.4 70.0 16.2 66.2 16.0 69.3 16.1 67.4
(LM+M1) ifelse 16.1 70.6 15.7 69.9 16.5 66.0 16.2 69.2 16.3 67.2
Table 4: Corpus statistics of the preprocessed
German-English parallel training data (Europarl,
News Commentary and Common Crawl). In the
data, numerical quantities have been replaced by a
single category symbol.
German English
Sentences 4.1M
Running Words 104M 104M
Vocabulary 717K 750K
For filtering, we use the 800K best sentences
from the whole training corpora, as this se-
lection performed best on the dev set among
100K,200K,400K,800K,1600K setups. Filtering
seems to mainly improve on the TER scores, BLEU
scores are virtually unchanged in comparison to
the baseline. LM+M1 filtering improves further
on TER in comparison to LM-based filtering.
The weighted phrase extraction performs best
in our experiments, where the weights from the
LM+M1 scoring method are used. Improvements
in both BLEU and TER are achieved, with BLEU
improvements ranging from +0.4% up-to +0.6%
and TER improvements from -0.9% and up-to -
1.1%.
As a final step, we added the English Gigaword
corpus to the LM (+GW). This resulted in further
improvements of the systems.
In addition, the system as described above was
tuned on newstest2009. Using this development
set results in worse translation quality.
Furthermore, we rescored the SCSS baseline
tuned on newstest2009 with a continuous space
language model (CSLM) as described in (Schwenk
et al, 2012). The CSLM was trained on the eu-
roparl and news-commentary corpora. For rescor-
ing, we used the newstest2011 set as tuning set and
re-optimized the parameters with MERT on 1000-
best lists. This results in an improvement of up to
0.8 points in BLEU compared to the baseline.
We compared the phrase-based setups with a
hierarchical translation system, which was aug-
mented with preference grammars, soft string-
to-dependency features, discriminative reordering
extensions, DWL, IDM, and discriminative re-
197
ordering extensions. The phrase table of the hier-
archical setup has been extracted from News Com-
mentary and Europarl parallel data only (not from
Common Crawl).
Finally, three setups were joined in a system
combination and we gained an improvement of up
to 0.5 points in BLEU compared to the best single
system.
4.4 Experimental Results English?German
The results for the English?German task are
shown in Table 6. While the LM-based filter-
ing led to almost no improvement over the base-
line, the LM+M1 filtering brought some improve-
ments in BLEU. In addition to the sentence fil-
tering, we tried to combine the translation model
trained on NC+EPPS with a TM trained on Com-
mon Crawl using the ifelse combination (Mansour
and Ney, 2012). This combination scheme con-
catenates both TMs and assigns the probabilities
of the in-domain TM if it contains the phrase,
else it uses the probabilities of the out-of-domain
TM. Appling this method, we achieved further im-
provements.
5 Conclusion
For the participation in the WMT 2013 shared
translation task, RWTH experimented with both
phrase-based and hierarchical translation systems.
Several different techniques were evaluated and
yielded considerable improvements over the re-
spective baseline systems as well as over our last
year?s setups (Huck et al, 2012b). Among these
techniques are a hierarchical phrase reordering
model, translation model interpolation, domain
adaptation techniques, weighted phrase extraction,
a word class language model, a continuous space
language model and system combination.
Acknowledgments
This work was achieved as part of the Quaero Pro-
gramme, funded by OSEO, French State agency
for innovation.
References
Peter F. Brown, Stephen A. Della Pietra, Vincent J.
Della Pietra, and Robert L. Mercer. 1993. The
Mathematics of Statistical Machine Translation: Pa-
rameter Estimation. Computational Linguistics,
19(2):263?311, June.
Stanley F. Chen and Joshua Goodman. 1998. An
Empirical Study of Smoothing Techniques for Lan-
guage Modeling. Technical Report TR-10-98, Com-
puter Science Group, Harvard University, Cam-
bridge, Massachusetts, USA, August.
David Chiang. 2007. Hierarchical Phrase-Based
Translation. Computational Linguistics, 33(2):201?
228.
Michel Galley and Christopher D. Manning. 2008. A
Simple and Effective Hierarchical Phrase Reorder-
ing Model. In Proceedings of the 2008 Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 847?855, Honolulu, Hawaii, USA,
October.
Matthias Huck and Hermann Ney. 2012. Insertion and
Deletion Models for Statistical Machine Translation.
In Proceedings of the North American Chapter of the
Association for Computational Linguistics - Human
Language Technologies conference, pages 347?351,
Montre?al, Canada, June.
Matthias Huck, Saab Mansour, Simon Wiesler, and
Hermann Ney. 2011. Lexicon Models for Hierar-
chical Phrase-Based Machine Translation. In Pro-
ceedings of the International Workshop on Spoken
Language Translation (IWSLT), pages 191?198, San
Francisco, California, USA, December.
Matthias Huck, Stephan Peitz, Markus Freitag, and
Hermann Ney. 2012a. Discriminative Reordering
Extensions for Hierarchical Phrase-Based Machine
Translation. In 16th Annual Conference of the Eu-
ropean Association for Machine Translation, pages
313?320, Trento, Italy, May.
Matthias Huck, Stephan Peitz, Markus Freitag, Malte
Nuhn, and Hermann Ney. 2012b. The RWTH
Aachen Machine Translation System for WMT
2012. In NAACL 2012 Seventh Workshop on
Statistical Machine Translation, pages 304?311,
Montre?al, Canada, June.
Matthias Huck, Jan-Thorsten Peter, Markus Freitag,
Stephan Peitz, and Hermann Ney. 2012c. Hierar-
chical Phrase-Based Translation with Jane 2. The
Prague Bulletin of Mathematical Linguistics, 98:37?
50, October.
Matthias Huck, David Vilar, Markus Freitag, and
Hermann Ney. 2013. A Performance Study of
Cube Pruning for Large-Scale Hierarchical Machine
Translation. In Proceedings of the NAACL 7thWork-
shop on Syntax, Semantics and Structure in Statis-
tical Translation, pages 29?38, Atlanta, Georgia,
USA, June.
Dan Klein and Christopher D. Manning. 2003. Accu-
rate Unlexicalized Parsing. In Proc. of the 41th An-
nual Meeting of the Association for Computational
Linguistics (ACL), pages 423?430, Sapporo, Japan,
July.
198
Reinhard Kneser and Hermann Ney. 1995. Im-
proved Backing-Off for M-gram Language Model-
ing. In Proceedings of the International Conference
on Acoustics, Speech, and Signal Processing, vol-
ume 1, pages 181?184, May.
Philipp Koehn and Kevin Knight. 2003. Empirical
Methods for Compound Splitting. In Proceedings of
European Chapter of the ACL (EACL 2009), pages
187?194.
Alon Lavie and Abhaya Agarwal. 2007. METEOR:
An Automatic Metric for MT Evaluation with High
Levels of Correlation with Human Judgments. In
ACL 2007 Second Workshop on Statistical Machine
Translation, pages 228?231, Prague, Czech Repub-
lic, June.
Saab Mansour and Hermann Ney. 2012. A Simple and
Effective Weighted Phrase Extraction for Machine
Translation Adaptation. In Proceedings of the Inter-
national Workshop on Spoken Language Translation
(IWSLT), pages 193?200, Hong Kong, December.
Saab Mansour, Joern Wuebker, and Hermann Ney.
2011. Combining Translation and Language Model
Scoring for Domain-Specific Data Filtering. In Pro-
ceedings of the International Workshop on Spoken
Language Translation (IWSLT), pages 222?229, San
Francisco, California, USA, December.
Arne Mauser, Sas?a Hasan, and Hermann Ney. 2009.
Extending Statistical Machine Translation with Dis-
criminative and Trigger-Based Lexicon Models. In
Proc. of the Conf. on Empirical Methods for Natu-
ral Language Processing (EMNLP), pages 210?218,
Singapore, August.
Franz Josef Och and Hermann Ney. 2003. A System-
atic Comparison of Various Statistical Alignment
Models. Computational Linguistics, 29(1):19?51,
March.
Franz Josef Och. 2000. mkcls: Training
of word classes for language modeling.
http://www.hltpr.rwth-aachen.de/
web/Software/mkcls.html.
Franz Josef Och. 2003. Minimum Error Rate Training
in Statistical Machine Translation. In Proc. of the
41th Annual Meeting of the Association for Compu-
tational Linguistics (ACL), pages 160?167, Sapporo,
Japan, July.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a Method for Automatic
Evaluation of Machine Translation. In Proceed-
ings of the 41st Annual Meeting of the Associa-
tion for Computational Linguistics, pages 311?318,
Philadelphia, Pennsylvania, USA, July.
Jan-Thorsten Peter, Matthias Huck, Hermann Ney, and
Daniel Stein. 2011. Soft String-to-Dependency
Hierarchical Machine Translation. In International
Workshop on Spoken Language Translation, pages
246?253, San Francisco, California, USA, Decem-
ber.
Maja Popovic? and Hermann Ney. 2006. POS-based
Word Reorderings for Statistical Machine Transla-
tion. In International Conference on Language Re-
sources and Evaluation, pages 1278?1283, Genoa,
Italy, May.
Holger Schwenk, Anthony Rousseau, and Mohammed
Attik. 2012. Large, Pruned or Continuous Space
Language Models on a GPU for Statistical Machine
Translation. In NAACL-HLT 2012 Workshop: Will
We Ever Really Replace the N-gram Model? On the
Future of Language Modeling for HLT, pages 11?
19, Montre?al, Canada, June.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A Study of
Translation Edit Rate with Targeted Human Annota-
tion. In Proceedings of the 7th Conference of the As-
sociation for Machine Translation in the Americas,
pages 223?231, Cambridge, Massachusetts, USA,
August.
Daniel Stein, Stephan Peitz, David Vilar, and Hermann
Ney. 2010. A Cocktail of Deep Syntactic Fea-
tures for Hierarchical Machine Translation. In Conf.
of the Association for Machine Translation in the
Americas (AMTA), Denver, Colorado, USA, Octo-
ber/November.
Andreas Stolcke. 2002. SRILM ? An Extensible Lan-
guage Modeling Toolkit. In Proc. of the Int. Conf.
on Speech and Language Processing (ICSLP), vol-
ume 2, pages 901?904, Denver, Colorado, USA,
September.
David Vilar, Daniel Stein, Matthias Huck, and Her-
mann Ney. 2010. Jane: Open Source Hierarchi-
cal Translation, Extended with Reordering and Lex-
icon Models. In ACL 2010 Joint Fifth Workshop on
Statistical Machine Translation and Metrics MATR,
pages 262?270, Uppsala, Sweden, July.
David Vilar, Daniel Stein, Matthias Huck, and Her-
mann Ney. 2012. Jane: an advanced freely avail-
able hierarchical machine translation toolkit. Ma-
chine Translation, 26(3):197?216, September.
Joern Wuebker, Matthias Huck, Stephan Peitz, Malte
Nuhn, Markus Freitag, Jan-Thorsten Peter, Saab
Mansour, and Hermann Ney. 2012. Jane 2:
Open Source Phrase-based and Hierarchical Statis-
tical Machine Translation. In International Confer-
ence on Computational Linguistics, pages 483?491,
Mumbai, India, December.
199
