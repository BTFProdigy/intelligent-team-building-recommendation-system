Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 1011?1019,
Beijing, August 2010
Explore the Structure of Social Tags by Subsumption Relations
Xiance Si, Zhiyuan Liu, Maosong Sun
Department of Computer Science and Technology
State Key Lab on Intelligent Technology and Systems
National Lab for Information Science and Technology
Tsinghua University
{sixiance,lzy.thu}@gmail.com, sms@tsinghua.edu.cn
Abstract
Thanks to its simplicity, social tagging
system has accumulated huge amount of
user contributed tags. However, user
contributed tags lack explicit hierarchi-
cal structure, while many tag-based ap-
plications would benefit if such a struc-
ture presents. In this work, we explore
the structure of tags with a directed and
easy-to-evaluate relation, named as the
subsumption relation. We propose three
methods to discover the subsumption rela-
tion between tags. Specifically, the tagged
document?s content is used to find the re-
lations, which leads to better result. Be-
sides relation discovery, we also propose
a greedy algorithm to eliminate the re-
dundant relations by constructing a Lay-
ered Directed Acyclic Graph (Layered-
DAG) of tags. We perform quantita-
tive evaluations on two real world data
sets. The results show that our methods
outperform hierarchical clustering-based
approach. Empirical study of the con-
structed Layered-DAG and error analysis
are also provided.
1 Introduction
In this work, we aim at exploring the structure of
social tags. Social tagging is widely used in Web-
based services, in which a user could use any word
to annotate an object. Thanks to its simplicity, ser-
vices with social tagging features have attracted a
lot of users and have accumulated huge amount of
annotations. However, comparing to taxonomies,
social tagging has an inherent shortcoming, that
Figure 1: Examples of (a) flat tag cloud, (b) hier-
archical clusters, and (c) subsumption relations.
there is no explicit hierarchical relations between
tags. Figure 1 (a) shows an example of the com-
monly used flat tag cloud, in which only the pop-
ularity of a tag is concerned. Kome et al (2005)
argued that implicit hierarchical relations exist in
social tags. Previous literature shows that orga-
nizing tags in hierarchical structures will help tag-
based Information Retrieval applications (Begel-
man et al, 2006; Brooks and Montanez, 2006).
Hierarchical clustering could reveal the simi-
larity relations of tags. Figure 1 (b) shows an
example of a typical hierarchical clustering of
tags. While clusters can capture similarity be-
tween tags, problems still remain: First, clusters
mix different relations, such as synonyms and hy-
pernyms. Second, clusters also ignore the direc-
tion of relations, for example, the direction in
browser ? firefox. Third, it is hard to evalu-
ate the correctness of clustering. Specifically, it
is hard to tell if two tags are similar or not. In
practice, directed and easy-to-evaluate relations
between tags are preferred, such as Figure 1 (c).
In this work, we explore the structure of so-
cial tags by discovering a directed and easy-to-
evaluate relation between tags, named subsump-
tion relation. A tag ta subsumes tb, if and only
if wherever tb is used, we can also replace it
1011
with ta. Unlike similar-to, subsumption relation
is asymmetric, and its correctness is easier to as-
sess. Then, we propose three ways to discover the
subsumption relations, through tag-tag, tag-word
and tag-reason co-occurrences respectively. In the
third way, A tag?s reason is defined as the word
in the content that explains the using of the tag.
We employ the Tag Allocation Model (TAM) pro-
posed by Si et al (2010) to find the reason for
each tag. Besides subsumption relation discov-
ery, we also propose a greedy algorithm to remove
the redundant relations. The removal is done by
constructing a Layered Directed Acyclic Graph
(Layered-DAG) of tags with the subsumption re-
lations.
We carried out the experiments on two real
world data sets. The results of quantitative evalu-
ation showed that tag-reason based approach out-
performed other two methods and a commonly
used hierarchical clustering-based method. We
also do empirical study on the output of Layered-
DAG construction.
The contribution of this paper can be summa-
rized as follows:
1. We explore the structure of social tags by
a clearly defined subsumption relation. We
propose methods to discover the subsump-
tion relation automatically, leveraging both
the co-occurred tags and the content of an-
notated document.
2. We propose an algorithm to eliminate the re-
dundant relations by constructing a Layered-
DAG of tags.
3. We perform both empirical and quantitative
evaluation of proposed methods on two real
world data sets.
The rest of the paper is organized as follows:
Section 2 surveys the related work; Section 3 de-
fines the subsumption relation we used, and pro-
poses methods for relation discovery; Section 4
proposes a greedy algorithm for Layered-DAG
construction; Section 5 explains the experimen-
tal settings and shows the evaluation results. Sec-
tion 6 concludes the paper.
2 Related Work
To explore the hierarchical relations between tags,
an intuitive way is to cluster the tags into hier-
archical clusters. Wu et al (2006b) used a fac-
torized model, namely Latent Semantic Analy-
sis, to group tags into non-hierarchical topics for
better recommendation. Brooks et al (2006) ar-
gued that performing Hierarchical Agglomerative
Clustering (HAC) on tags can improve the col-
laborative tagging system. Later, HAC on tags
was also used for improving personalized recom-
mendation (Shepitsen et al, 2008). Heymann et
al. (2006) clustered tags into a tree by a similarity-
based greedy tree-growing method. They evalu-
ated the obtained trees empirically, and reported
that the method is simple yet powerful for orga-
nizing tags with hierarchies. Based on Heymann
et al?s work, Schwarzkopf et al (2007) proposed
an approach for modeling users with the hierarchy
of tags. Begelman et al (2006) used top-down hi-
erarchical clustering, instead of bottom-up HAC,
to organize tags, and argued that tag hierarchies
improve user experiences in their system. Most
of the hierarchical clustering algorithms rely on
the symmetric similarity between tags, while the
discovered relations are hard to evaluate quantita-
tively, since one cannot distinguish similar from
not-similar with a clear boundary.
People have also worked on bridging social tag-
ging systems and ontologies. An ontology defines
relations between entities. Peter Mika (2005) pro-
posed an extended scheme of social tagging that
includes actors, concepts and objects, and used
tag co-occurrences to construct an ontology from
social tags. Wu et al (2006a) used hierarchical
clustering to build ontology from tags that also
use similar-to relations. Later, ontology schemes
that fits social tagging system were proposed, such
as (Van Damme et al, 2007) and (Echarte et
al., 2007), which mainly focused on the relation
between tags, objects and users, rather than be-
tween tags themselves. Alexandre Passant (2007)
mapped tags to domain ontologies manually to
improve information retrieval in social media. To
construct tag ontology automatically, Angeletou
et al (2007) used ontologies built by domain ex-
perts to find relations between tags, but observed
a very low coverage. Specia et al (2007) pro-
posed an integrated framework for organizing tags
by existing ontologies, but no experiment was per-
formed. Kim et al (2008) summarized the state-
1012
of-the-art methods to model tags with semantic
annotations.
Before social tagging was invented, Sanderson
et al (1999) proposed to use subsumption relation
to organize words in text hierarchically. Schmitz
et al (2006) followed the idea to use subsumption
relation for organizing Flickr 1 tag, where tag-tag
co-occurrences are used for discover the relations.
We follow the idea of subsumption relation in this
paper, and explore alternative ways for relation
discovery.
3 Subsumption Relations in Tags
In this section, we define the subsumption relation
used in our study, and propose three methods to
discover the subsumption relations.
3.1 Definitions
First, we introduce the symbols used through out
the paper: A tag is denoted as t ? T , where T is
the set of all tags. To distinguish from words, we
use fixed-width to represent the example tags.
An annotated document is denoted as d ? D,
where D is the set of all documents. The words
in d are denoted as a set {wdi}, where i ? [1, |d|],
and |d| is the number of words in d.
Inspired by (Sanderson and Croft, 1999), we
define the subsumption relation between ta and tb
as follows: ta subsumes tb, means that wherever
the tag tb is used, ta can also be used without
ambiguity. The subsumption relation between ta
and tb is denoted as ta ?s tb.
Subsumption relation is directional, that is,
ta ?s tb does not imply tb ?s ta. For ex-
ample, literature ?s chineseliterature,
since for any document annotated with
chineseliterature, we can also annotate
it with literature. However, if we swapped the
two tags, the statement would not hold.
Subsumption relation is more strict than simi-
larity. For example, during the time of Haiti earth-
quake, the tag earthquake is close to haiti in
similarity, but none of them implies the use of the
other one: document annotated with earthquake
may refer to the earthquake in China, while docu-
1http://www.flickr.com. An image sharing site that allows
users to annotate images with tags
ment annotated with haiti may mean the travel-
ing experience in Haiti.
Note that the subsumption has transitivity prop-
erty, that ta ?s tb and tb ?s tc means ta ?s
tc, which corresponds to our intuition. For in-
stance, naturaldisaster ?s earthquake and
disaster?snaturaldisaster means disaster
?searthquake.
3.2 Discover Subsumption Relation
We discover the subsumption relations by estimat-
ing the probability p(ta|tb). The motivation is, if
ta ?s tb and tb is used, it would be more likely to
see ta. So, by sorting all (ta, tb) pairs by p(ta|tb)
in descending order, top-ranked pairs are more
likely to have subsumption relations.
In this work, we present three methods to esti-
mate the probability p(ta|tb), using tag-tag, tag-
word and tag-reason co-occurrences respectively.
By using tag-word and tag-reason co-occurrences,
we leverage the content of the annotated docu-
ment for subsumption relation discovery.
3.2.1 Tag-Tag Co-occurrences Approach
The most intuitive way to estimate p(ta|tb) is
via tag-tag co-occurrences. Specifically, we use
the following formula:
p(ta|tb) =
Nd(ta, tb)
Nd(tb)
, (1)
where Nd(ta, tb) is the number of documents that
are annotated by both ta and tb, and Nd(tb) is the
number of documents annotated by tb. We de-
note the tag-tag co-occurrences approach as TAG-
TAG.
The use of TAG-TAG can be found in previous
literature for organizing tags for photos(Schmitz,
2006). One of TAG-TAG?s benefits is that it does
not rely on the content of the annotated document,
thus it can be applied to tags for non-text objects,
such as images and music. However, when com-
ing to text documents, this benefit is also a short-
coming, that TAG-TAG makes no use of the con-
tent when it is available.
Using TAG-TAG for subsumption relation dis-
covery relies on an implication, that if a user has
annotated d with tb, he would also annotate all
tags that subsumes tb. The implication may not
always hold in real world situations. For example,
1013
a novel reader would use tags such as scifi and
mystery to organize his collections, but he is not
likely to annotate each of his collection as novel
or book, since they are too obvious for him. We
name the problem as the omitted-tag problem.
3.2.2 Tag-Word Co-occurrences Approach
When the content of the annotated document
is available, using it for estimating p(ta|tb) is a
natural thought. The content is expected to be
complete and information-rich whether or not the
user has omitted any tags. We use the follow-
ing formula to estimate p(ta|tb) by tag-word co-
occurrences:
p(ta|tb) =
?
w?W
p(ta|w)p(w|tb)
=
?
w?W
Nd(ta, w)
Nd(w)
Nd(tb, w)
Nd(tb)
, (2)
where Nd(ta, w) is the number of documents that
contains both tag ta and word w, and Nd(w) is
the number of documents that contains the word
w. We denote this approach as TAG-WORD.
Instead of computing tag-tag co-occurrences
directly, TAG-WORD uses words in the document
as a bridge to estimate p(ta|tb). By introduc-
ing words, the estimation is less affected by the
omitted-tag problem, Take the novel reader exam-
ple again: Although he does not use the tag novel
too often, the words in book descriptions would
suggest the using of novel, according to all other
documents annotated by novel.
While using the content may weaken the
omitted-tag problem, it also brings the noise in
text to the estimation. Not every word in the con-
tent is related to one of the tags. To the oppo-
site, most words are functional words or that about
other aspects of the document. p(ta|tb) estimated
by using all words may largely depends on these
irrelevant words.
3.2.3 Tag-Reason Co-occurrences Approach
To focus on the words that are highly relevant
to the interested tags, we propose the third method
that uses tag-reason co-occurrences. The reason is
defined as the word(s) that can explain the using
of a tag in the document. For example, the tag
scifi for a book could be explained by the words
?robot?, ?Asimov? in the book description. If the
reason of each tag could be identified, the noise in
content-based p(ta|tb) could be reduced.
Si et al (2010) proposed a probabilistic model
for content-based social tags, named Tag Allo-
cation Model (TAM). TAM introduces a latent
variable r for each tag in the data set, known
as the reason variable. The value of r can be a
word in the corresponding document, or a global
noise variable ?. Allowing the reason of tags to
be a global noise makes TAM deal with content-
irrelevant tags and mistakenly annotated tags ef-
fectively. The likelihood that a document d is an-
notated by tag t is given as:
p(t|d) =
?
w?d
p(t|r = w)p(r = w|d)p(s = 0)
+ p(t|?)p(r = ?)p(s = 1), (3)
where r is the reason of the tag t, r ? {wdi|i ?
[0, |d|]} ? {?}, ? is the global noise variable. s is
the source of reason t, s = 0 means the source is
the content of the document, while s = 1 means
the source is the global noise variable ?. TAM
can be trained use Gibbs sampling method. For
the details of TAM, please refer to (Si and Sun,
2010).
With a trained TAM, we can infer p(t|r), the
probability of seeing a tag t when using r as the
reason, and p(r|t), the probability of choosing r
as the reason for tag t. With these probabilities,
we can estimate p(ta|tb) by
p(ta|tb) =
?
r?W
p(ta|r)p(r|tb). (4)
Note that we use only word reasons (r ? W ),
ignoring the noise reason ? completely. We de-
note this approach as TAG-REASON.
With the help of TAM, TAG-REASON cov-
ers the problems of the TAG-WORD method in
two aspects: First, instead of using all words,
TAG-REASON emphasizes on the really relevant
words, which are the reasons identified by TAM.
Second, by ignoring the noise variable ?, TAG-
REASON is less affected by the content-irrelevant
noise tags, such as thingstodo or myown.
After p(ta|tb) is estimated for each (ta, tb) ?
T ?T , we use the top-n pairs with largest p(ta|tb)
1014
Figure 2: DAG and Layered-DAG
as the final set of discovered subsumption rela-
tions.
4 Remove Redundancy with
Layered-DAG Construction
The discovered subsumption relations connect all
tags into a directed graph G = {V,E}, where V
is the set of nodes, with each node is a tag; E is
the set of edges, an edge eta,tb from ta to tb means
ta ?s tb. Furthermore, we define the weight of
each edge we as the probability p(ta|tb).
Recalling that subsumption relation has transi-
tivity property, to avoid the cyclic references in G,
we would like to turn G into a Directed Acyclic
Graph (DAG). Further, DAG may also contains
redundant information. Figure 2 (a) shows a part
of a DAG. Note the edge marked as ?*?, which
is perfectly correct, but does not provide extra
information, since literature ?s novel and
novel?s scifi-novel have already implied that
literature?s novel. We would like to remove
these redundant relations, turning a DAG into the
form of Figure 2 (b).
We define Layered-DAG formally as follows:
For a DAG G, when given any pair of nodes, if ev-
ery path that can connect them has equal length, G
is a Layered-DAG. Layered-DAG prohibits edges
that link cross layers, such like edge ?*? in Fig-
ure 2 (a). Constructing a Layered-DAG from the
discovered relations can eliminate the redundant
information.
Given a set of subsumption relations, multiple
Layered-DAGs may be constructed. In particular,
we want to find the Layered-DAG that maximizes
the sum of all edges? weights. Weight maximiza-
tion implies two concerns: First, when we need
to remove a relation to resolve the conflicts or re-
dundancy, the one with lower weight is prefered.
Layered-DAG Construction Algorithm
Input: A set of weighted relations, R = {ta ?s tb|ta ? T, tb ? T},
wta?stb > 0
Output: A Layered-DAG of tags G? = {V ?, E?}
1: V ? = {}
2: while R 6= ?
3: if V ? = ?
4: choose ta ?s tb ? R with highest weight.
5: E? ? ta ?s tb
6: V ? ? ta, V ? ? tb.
7: remove ta ?s tb from R.
8: else
9: C ? {ta ?s tb|ta ?s tb ? R, {ta, tb} ? V ? 6= ?}
10: for ta ?s tb ? C in descending weight order
11: if adding ta ?s tb to G? keeps G? a Layered-DAG.
12: E? ? ta ?s tb
13: V ? ? ta, V ? ? tb.
14: break
15: endif
16: remove ta ?s tb from R.
17: endfor
18: endif
19: endwhile
20: output G?
Figure 3: A greedy algorithm for constructing
Layered-DAG of tags
Second, when more than one valid Layered-DAGs
are available, we want to use the one that contains
as many edges as possible.
Finding and proving an optimal algorithm for
maximum Layered-DAG construction are beyond
the scope of this paper. Here we present a greedy
algorithm that works well in practice, as described
in Figure 3.
The proposed algorithm starts with a minimal
Layered-DAG G? that contains only the high-
est weighted relation in R (Steps 1-8). Then, it
moves an edge in G to G? once a time, ensuring
that adding the new edge still keeps G? a valid
Layered-DAG (Step 11), and the new edge has the
highest weights among all valid candidates (Steps
9-10).
5 Experiments
In this section, we show the experimental results
of proposed methods. Specifically, we focus on
the following points:
? The quality of discovered subsumption rela-
tions by different methods.
? The characteristics of wrong subsumption re-
lations discovered.
? The effect of Layered-DAG construction on
the quality of relations.
? Empirical study of the resulted Layered-
DAG.
1015
Name N N?tag N?content
BLOG 100,192 2.78 332.87
BOOK 110,371 8.51 204.76
Table 1: Statistics of the data sets. N is the num-
ber of documents. N?tag is the mean number of
tags per document. N?content is the mean number
of words per document.
5.1 Data Sets
We use two real world social tagging data sets.
The first data set, named BLOG, is a collection
of blog posts annotated by blog authors, which
is crawled from the web. The second data set,
named BOOK, is from a book collecting and shar-
ing site2, which contains description of Chinese
books and user contributed tags. Table 1 lists the
basic statistics of the data sets.
The two data sets have different characteristics.
Documents in BLOG are longer, not well written,
and the number of tags per document is small. To
the opposite, documents in BOOK are shorter but
well written, and there are more tags for each doc-
ument.
5.2 Discovered Subsumption Relations
5.2.1 Experimental Settings
For BLOG, we use the tags that have been used
more than 10 times; For BOOK, we use the tags
that have been used more than 50 times. We per-
form 100 iterations of Gibbs sampling when train-
ing the TAM model, with first 50 iterations as
the burn-in iterations. All the estimation meth-
ods require proper smoothing. Here we use ad-
ditive smoothing for all methods, which adds a
very small number (0.001 in our case) to all raw
counts. Sophisticated smoothing method could be
employed, but is out of the scope of this paper.
5.2.2 Evaluation
We use precision and coverage to evaluate the
discovered relations at any given cut-off threshold
n. First, we sort the discovered relations by their
weights in descending order. Then, we take the
top-n relations, discarding the others. For the re-
maining relations, precision is computed as Nc/n,
Nc is the number of correct relations in the top-n
2http://www.douban.com
list; coverage is computed as Nt/|T |, where Nt is
the number of unique tags appeared in the top-n
list, and |T | is the total number of tags.
To get Nc, the number of correct relations, we
need a standard judgement of the correctness of
relations, which involves human labeling. To min-
imize the bias in human assessment, we use pool-
ing, which is a widely accepted method in Infor-
mation Retrieval research (Voorhees and Harman,
2005). Pooling works as follows: First, relations
obtained by different methods are mixed together,
creating a pool of relations. Second, the pool is
shuffled, so that the labeler cannot identify the
source of a single relation. Third, annotators are
requested to label the relations in the pool as cor-
rect or incorrect, based on the definition of sub-
sumption relation. After all relations in the pool
are labeled, we use them as the standard judge-
ment to evaluate each method?s output.
Precision measures the proportion of correct re-
lations, while coverage measures the proportion of
tags that are connected by the relations. The cut-
off threshold n affects both precision and cover-
age: the larger the n, the lower the precision, and
the higher the coverage.
5.2.3 Baseline methods
Besides TAG-TAG, TAG-WORD and TAG-
REASON, we also include the method described
in (Heymann and Garcia-Molina, 2006) as a
baseline, denoted as HEYMANN. HEYMANN
method was designed to find similar-to relation
rather than subsumption relation. The similar-to
relation is symmetric, while subsumption relation
is more strict and asymmetric. In our experiments,
we use the same evaluation process to evalu-
ate TAG-TAG, TAG-WORD, TAG-REASON and
HEYMANN, in which only subsumption relations
will be marked as correct.
5.2.4 Results
For each method, we set the cut-off threshold
n from 1 to 500, so as to plot the psrecision-
coverage curves. The result is shown in Figure 4.
The larger the area under the curve, the better the
method?s performance.
We have three observations from Figure 4.
First, TAG-REASON has the best performance
1016
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  0.05  0.1  0.15  0.2  0.25  0.3  0.35  0.4
Pr
ec
is
io
n
Coverage
TAG-REASON
TAG-TAG
TAG-WORD
HEYMANN
(a) BLOG
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  0.05  0.1  0.15  0.2  0.25  0.3  0.35
Pr
ec
is
io
n
Coverage
TAG-REASON
TAG-TAG
TAG-WORD
HEYMANN
(b) BOOK
Figure 4: The precision and coverage of TAG-TAG, TAG-WORD, TAG-REASON and HEYMANN
methods. The larger the area under the curve, the better the result. The cut-off threshold n ? [1, 500].
BLOG BOOK
Insufficient Reversed Irrelevant Insufficient Reversed Irrelevant
childedu?s father stock?s security travel?sbuilding textbook?s exam English?s foreignlang japan?slightnovel
childedu?s grandma stock?s financial emotion?stime history?s military biography?speople building?stextbook
emotion?swarm delicious?staste emotion?soriginal piano?sscores jpbuilding?s jpculture sales?sO
childedu?schild delicious?sfood culture?sspring history?sculture novel?spureliterature japan?s shower
education?schild earthquake?sdisaster poem?snight novel?slove ancientgreek?sgreek photo?sumbrella
Total 52% Total 14% Total 34% Total 37% Total 48% Total 15%
Table 2: Examples of mistakes and the percentage of each mistake type.
on both data sets: On the BOOK data set, TAG-
REASON outperforms others by a marked mar-
gin; On the BLOG data set, TAG-REASON has
higher precision when coverage is smaller (which
means within top-ranked relations), and has com-
parable precision to TAG-TAG when coverage
increases. Second, similarity-based clustering
method (namely HEYMANN) performed worse
than others, suggesting it may not be adequate for
discovering subsumption relation. Third, while
also using content information, TAG-WORD per-
forms poorer than both TAG-REASON and TAG-
TAG, which suggests that noise in the content
would prevent TAG-WORD from getting the cor-
rect estimation of p(ta|tb).
To summarize, by leveraging relevant con-
tent, TAG-REASON could discover better sub-
sumption relations than just using tag-tag co-
occurrences and similarity-based hierarchical
clustering.
5.2.5 Mistakes in Discovered Relations
We also studied the type of mistakes in sub-
sumption relation discovery. To our observation, a
mistakenly discovered relation ta ?s tb falls into
one of the following categories:
1. insufficient ta relates with tb, but using tb
does not implies the using of ta in all cases.
2. reversed tb ?s ta is correct, while ta ?s tb
is not.
3. irrelevant There is no obvious connection
between ta and tb.
We collected all incorrect relations discovered
by the TAG-REASON method. Then, the type of
mistake for each relation is labeled manually. The
result is shown in Table 2, along with selected ex-
amples of each type.
Table 2 shows different error patterns for
BLOG and BOOK. In BLOG, most of the
mistakes are of the type insufficient. Taking
?education?s child? for example, annotating a
document as child does not imply that it is about
child education, it may about food or clothes for
a child. In BOOK, most of the mistakes are re-
versed mistakes, which is a result of the omitted-
tag problem discussed in Section 3.2.1.
1017
Figure 5: Part of the constructed Layered-DAG from the BOOK data set.
BLOG BOOK
Method Precision Coverage Precision Coverage
TAG-TAG ?4.7% +7.9% ?7.4% +12.5%
TAG-WORD 0% 0% ?9.0% +2.2%
TAG-REASON ?3.6% +5.4% ?0.9% +5.4%
Table 3: The effects on precision and coverage by
Layered-DAG construction
5.3 Layered-DAG Construction
Using the algorithm introduced in Section 4, we
constructed Layered-DAGs from the discovered
relations. Constructing Layered-DAG will re-
move certain relations, which will decrease the
precision and increase the coverage. Table 3
shows the changes of precision and coverage
brought by Layered-DAG construction. In most
of the cases, the increasing of coverage is more
than the decreasing of precision.
As a representative example, we show part of
a constructed Layered-DAG from the BOOK data
set in Figure 5, since the whole graph is too big to
fit in the paper. All tags in Chinese are translated
to English.
6 Conclusion and Future Work
In this paper, we explored the structure of social
tags by discovering subsumption relations. First,
we defined the subsumption relation ta ?s tb
as ta can be used to replace tb without ambigu-
ity. Then, we cast the subsumption relation iden-
tification problem to the estimation of p(ta|tb).
We proposed three methods, namely TAG-TAG,
TAG-WORD and TAG-REASON, while the last
two leverage the content of document to help esti-
mation. We also proposed an greedy algorithm for
constructing a Layered-DAG from the discovered
relations, which helps minimizing redundancy.
We performed experiments on two real world
data sets, and evaluated the discovered subsump-
tion relations quantitatively by pooling. The
results showed that the proposed methods out-
perform similarity-based hierarchical clusteing
in finding subsumption relations. The TAG-
REASON method, which uses only the relevant
content to the tags, has the best performance. Em-
pirical study showed that Layered-DAG construc-
tion works effectively as expected.
The results suggest two directions for future
work: First, more ways for p(ta|tb) estima-
tion could be explored, for example, combining
TAG-TAG and TAG-REASON; Second, external
knowledge, such as the Wikipedia and the Word-
Net, could be exploited as background knowledge
to improve the accuracy.
ACKNOWLEDGEMENTS
This work is supported by the National Science
Foundation of China under Grant No. 60873174
and the National 863 High-Tech Program of China
under Grant No. 2007AA01Z148. We also thank
Douban Inc.(www.douban.com) for providing the
DOUBAN data set, and Shoukun Wang, Guozhu
Wen et al of Douban Inc. for insightful discus-
sion.
1018
References
Angeletou, S., M. Sabou, L. Specia, and E. Motta.
2007. Bridging the gap between folksonomies and
the semantic web: An experience report. In Work-
shop: Bridging the Gap between Semantic Web and
Web, volume 2. Citeseer.
Begelman, Grigory, Keller, and F. Smadja. 2006. Au-
tomated tag clustering: Improving search and explo-
ration in the tag space. In Collaborative Web Tag-
ging Workshop, 15 th International World Wide Web
Conference.
Brooks, Christopher H. and Nancy Montanez. 2006.
Improved annotation of the blogosphere via auto-
tagging and hierarchical clustering. In WWW ?06:
Proceedings of the 15th international conference on
World Wide Web, pages 625?632, New York, NY,
USA. ACM.
Echarte, F., J. J. Astrain, A. Co?rdoba, and J. Villadan-
gos. 2007. Ontology of folksonomy: A New mod-
eling method. Proceedings of Semantic Authoring,
Annotation and Knowledge Markup (SAAKM).
Heymann, Paul and Hector Garcia-Molina. 2006. Col-
laborative creation of communal hierarchical tax-
onomies in social tagging systems. Technical Re-
port 2006-10, Stanford University, April.
Kim, Hak L., Simon Scerri, John G. Breslin, Stefan
Decker, and Hong G. Kim. 2008. The state of the
art in tag ontologies: a semantic model for tagging
and folksonomies. In DCMI ?08: Proceedings of
the 2008 International Conference on Dublin Core
and Metadata Applications, pages 128?137. Dublin
Core Metadata Initiative.
Kome, Sam H. 2005. Hierarchical subject relation-
ships in folksonomies. Master?s thesis, University
of North Carolina at Chapel Hill, November.
Mika, P. 2005. Ontologies are us: A unified model of
social networks and semantics. The Semantic Web?
ISWC 2005, pages 522?536.
Passant, Alexandre. 2007. Using ontologies to
strengthen folksonomies and enrich information re-
trieval in weblogs. In Proceedings of International
Conference on Weblogs and Social Media.
Sanderson, M. and B. Croft. 1999. Deriving concept
hierarchies from text. In Proceedings of the 22nd
annual international ACM SIGIR conference on Re-
search and development in information retrieval,
pages 206?213. ACM.
Schmitz, P. 2006. Inducing ontology from flickr
tags. In Collaborative Web Tagging Workshop at
WWW2006, Edinburgh, Scotland, pages 210?214.
Citeseer.
Schwarzkopf, E., D. Heckmann, and D. Dengler.
2007. In Workshop on Data Mining for User Mod-
eling, ICUM?07, page 63. Citeseer.
Shepitsen, Andriy, Jonathan Gemmell, Bamshad
Mobasher, and Robin Burke. 2008. Personalized
recommendation in collaborative tagging systems
using hierarchical clustering. In Proceedings of
ACM RecSys?08.
Si, Xiance and Maosong Sun. 2010. Tag allocation
model: Modeling noisy social annotations by reason
finding. In Proceedings of 2010 IEEE/WIC/ACM
International Conferences on Web Intelligence and
Intelligent Agent Technology.
Specia, Lucia and Enrico Motta. 2007. Integrating
folksonomies with the semantic web. pages 624?
639.
Van Damme, C., M. Hepp, and K. Siorpaes. 2007.
Folksontology: An integrated approach for turning
folksonomies into ontologies. Bridging the Gap be-
tween Semantic Web and Web, 2:57?70.
Voorhees, E.M. and D.K. Harman. 2005. TREC: Ex-
periment and evaluation in information retrieval.
MIT Press.
Wu, Harris, Mohammad Zubair, and Kurt Maly.
2006a. Harvesting social knowledge from folk-
sonomies. In HYPERTEXT ?06: Proceedings of the
seventeenth conference on Hypertext and hyperme-
dia, pages 111?114, New York, NY, USA. ACM.
Wu, Xian, Lei Zhang, and Yong Yu. 2006b. Exploring
social annotations for the semantic web. In WWW
?06: Proceedings of the 15th international con-
ference on World Wide Web, pages 417?426, New
York, NY, USA. ACM.
1019
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1723?1732,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Deceptive Answer Prediction with User Preference Graph
Fangtao Li?, Yang Gao?, Shuchang Zhou??, Xiance Si?, and Decheng Dai?
?Google Research, Mountain View
?State Key Laboratory of Computer Architecture, Institute of Computing Technology, CAS
{lifangtao,georgezhou,sxc,decheng}@google.com
?Department of Computer Science and Technology, Tsinghua University
gao young@163.com
Abstract
In Community question answering (QA)
sites, malicious users may provide decep-
tive answers to promote their products or
services. It is important to identify and fil-
ter out these deceptive answers. In this
paper, we first solve this problem with
the traditional supervised learning meth-
ods. Two kinds of features, including tex-
tual and contextual features, are investi-
gated for this task. We further propose
to exploit the user relationships to identify
the deceptive answers, based on the hy-
pothesis that similar users will have simi-
lar behaviors to post deceptive or authentic
answers. To measure the user similarity,
we propose a new user preference graph
based on the answer preference expressed
by users, such as ?helpful? voting and
?best answer? selection. The user prefer-
ence graph is incorporated into traditional
supervised learning framework with the
graph regularization technique. The ex-
periment results demonstrate that the user
preference graph can indeed help improve
the performance of deceptive answer pre-
diction.
1 Introduction
Currently, Community QA sites, such as Yahoo!
Answers1 and WikiAnswers2, have become one of
the most important information acquisition meth-
ods. In addition to the general-purpose web search
engines, the Community QA sites have emerged as
popular, and often effective, means of information
seeking on the web. By posting questions for other
participants to answer, users can obtain answers
to their specific questions. The Community QA
1http://answers.yahoo.com
2http://wiki.answers.com
sites are growing rapidly in popularity. Currently
there are hundreds of millions of answers and mil-
lions of questions accumulated on the Community
QA sites. These resources of past questions and
answers are proving to be a valuable knowledge
base. From the Community QA sites, users can di-
rectly get the answers to meet some specific infor-
mation need, rather than browse the list of returned
documents to find the answers. Hence, in recent
years, knowledge mining in Community QA sites
has become a popular topic in the field of artifi-
cial intelligence (Adamic et al, 2008; Wei et al,
2011).
However, some answers may be deceptive. In
the Community QA sites, there are millions of
users each day. As the answers can guide the
user?s behavior, some malicious users are moti-
vated to give deceptive answers to promote their
products or services. For example, if someone
asks for recommendations about restaurants in the
Community QA site, the malicious user may post a
deceptive answer to promote the target restaurant.
Indeed, because of lucrative financial rewards, in
several Community QA sites, some business own-
ers provide incentives for users to post deceptive
answers for product promotion.
There are at least two major problems that the
deceptive answers cause. On the user side, the
deceptive answers are misleading to users. If
the users rely on the deceptive answers, they will
make the wrong decisions. Or even worse, the pro-
moted link may lead to illegitimate products. On
the Community QA side, the deceptive answers
will hurt the health of the Community QA sites. A
Community QA site without control of deceptive
answers could only benefit spammers but could
not help askers at all. If the asker was cheated by
the provided answers, he will not trust and visit
this site again. Therefore, it is a fundamental task
to predict and filter out the deceptive answers.
In this paper, we propose to predict deceptive
1723
answer, which is defined as the answer, whose pur-
pose is not only to answer the question, but also
to promote the authors? self-interest. In the first
step, we consider the deceptive answer prediction
as a general binary-classification task. We extract
two types of features: one is textual features from
answer content, including unigram/bigram, URL,
phone number, email, and answer length; the other
is contextual features from the answer context, in-
cluding the relevance between answer and the cor-
responding question, the author of the answer, an-
swer evaluation from other users and duplication
with other answers. We further investigate the user
relationship for deceptive answer prediction. We
assume that similar users tend to have similar be-
haviors, i.e. posting deceptive answers or post-
ing authentic answers. To measure the user rela-
tionship, we propose a new user preference graph,
which is constructed based on the answer evalu-
ation expressed by users, such as ?helpful? vot-
ing and ?best answer? selection. The user prefer-
ence graph is incorporated into traditional super-
vised learning framework with graph regulariza-
tion, which can make answers, from users with
same preference, tend to have the same category
(deceptive or authentic). The experiment results
demonstrate that the user preference graph can fur-
ther help improve the performance for deceptive
answer prediction.
2 Related Work
In the past few years, it has become a popular task
to mine knowledge from the Community QA sites.
Various studies, including retrieving the accumu-
lated question-answer pairs to find the related an-
swer for a new question, finding the expert in a
specific domain, summarizing single or multiple
answers to provide a concise result, are conducted
in the Community QA sites (Jeon et al, 2005;
Adamic et al, 2008; Liu et al, 2008; Song et
al., 2008; Si et al, 2010a; Figueroa and Atkin-
son, 2011). However, an important issue which
has been neglected so far is the detection of decep-
tive answers. If the acquired question-answer cor-
pus contains many deceptive answers, it would be
meaningless to perform further knowledge mining
tasks. Therefore, as the first step, we need to pre-
dict and filter out the deceptive answers. Among
previous work, answer quality prediction (Song et
al., 2010; Harper et al, 2008; Shah and Pomer-
antz, 2010; Ishikawa et al, 2010) is most related to
the deceptive answer prediction task. But these are
still significant differences between two tasks. An-
swer quality prediction measures the overall qual-
ity of the answers, which refers to the accuracy,
readability, completeness of the answer. While
the deceptive answer prediction aims to predict if
the main purpose of the provided answer is only
to answer the specific question, or includes the
user?s self-interest to promote something. Some
of the previous work (Song et al, 2010; Ishikawa
et al, 2010; Bian et al, 2009) views the ?best
answer? as high quality answers, which are se-
lected by the askers in the Community QA sites.
However, the deceptive answer may be selected as
high-quality answer by the spammer, or because
the general users are mislead. Meanwhile, some
answers from non-native speakers may have lin-
guistic errors, which are low-quality answers, but
are still authentic answers. Our experiments also
show that answer quality prediction is much dif-
ferent from deceptive answer prediction.
Previous QA studies also analyze the user graph
to investigate the user relationship (Jurczyk and
Agichtein, 2007; Liu et al, 2011). They mainly
construct the user graph with asker-answerer rela-
tionship to estimate the expertise score in Commu-
nity QA sites. They assume the answerer is more
knowledgeable than the asker. However, we don?t
care which user is more knowledgeable, but are
more likely to know if two users are both spam-
mers or authentic users. In this paper, we pro-
pose a novel user preference graph based on their
preference towards the target answers. We assume
that the spammers may collaboratively promote
the target deceptive answers, while the authen-
tic users may generally promote the authentic an-
swers and demote the deceptive answers. The user
preference graph is constructed based on their an-
swer evaluation, such as ?helpful? voting or ?best
answer? selection.
3 Proposed Features
We first view the deceptive answer prediction as a
binary-classification problem. Two kinds of fea-
tures, including textual features and contextual
features, are described as follows:
3.1 Textual Features
We first aim to predict the deceptive answer by an-
alyzing the answer content. Several textual fea-
tures are extracted from the answer content:
3.1.1 Unigrams and Bigrams
The most common type of feature for text classi-
fication is the bag-of-word. We use an effective
1724
feature selection method ?2 (Yang and Pedersen,
1997) to select the top 200 unigrams and bigrams
as features. The top ten unigrams related to decep-
tive answers are shown on Table 1. We can see that
these words are related to the intent for promotion.
professional service advice address
site telephone therapy recommend
hospital expert
Table 1: Top 10 Deceptive Related Unigrams
3.1.2 URL Features
Some malicious users may promote their products
by linking a URL. We find that URL is good indi-
cator for deceptive answers. However, some URLs
may provide the references for the authentic an-
swers. For example, if you ask the weather in
mountain view, someone may just post the link
to ?http://www.weather.com/?. Therefore, besides
the existence of URL, we also use the following
URL features:
1). Length of the URLs: we observe that the
longer urls are more likely to be spam.
2). PageRank Score: We employ the PageRank
(Page et al, 1999) score of each URL as popularity
score.
3.1.3 Phone Numbers and Emails
There are a lot of contact information mentioned
in the Community QA sites, such as phone num-
bers and email addresses, which are very likely to
be deceptive, as good answers are found to be less
likely to refer to phone numbers or email addresses
than the malicious ones. We extract the number of
occurrences of email and phone numbers as fea-
tures.
3.1.4 Length
We have also observed some interesting patterns
about the length of answer. Deceptive ones tend
to be longer than authentic ones. This can be ex-
plained as the deceptive answers may be well pre-
pared to promote the target. We also employ the
number of words and sentences in the answer as
features.
3.2 Contextual Features
Besides the answer textual features, we further in-
vestigate various features from the context of the
target answer:
3.2.1 Question Answer Relevance
The main characteristic of answer in Community
QA site is that the answer is provided to answer
the corresponding question. We can use the corre-
sponding question as one of the context features by
measuring the relevance between the answer and
the question. We employ three different models
for Question-Answer relevance:
Vector Space Model
Each answer or question is viewed as a word
vector. Given a question q and the answer a, our
vector model uses weighted word counts(e.g.TF-
IDF) as well as the cosine similarity (q ? a) of
their word vectors as relevant function (Salton and
McGill, 1986). However, vector model only con-
sider the exact word match, which is a big prob-
lem, especially when the question and answer are
generally short compared to the document. For ex-
ample, Barack Obama and the president of the US
are the same person. But the vector model would
indicate them to be different. To remedy the word-
mismatch problem, we also look for the relevance
models in higher semantic levels.
Translation Model
A translation model is a mathematical model in
which the language translation is modeled in a sta-
tistical way. The probability of translating a source
sentence (as answer here) into target sentence (as
question here) is obtained by aligning the words
to maximize the product of all the word probabil-
ities. We train a translation model (Brown et al,
1990; Och and Ney, 2003) using the Community
QA data, with the question as the target language,
and the corresponding best answer as the source
language. With translation model, we can com-
pute the translation score for new question and an-
swer.
Topic Model
To reduce the false negatives of word mismatch
in vector model, we also use the topic models to
extend matching to semantic topic level. The topic
model, such as Latent Dirichlet Allocation (LDA)
(Blei et al, 2003), considers a collection of doc-
uments with K latent topics, where K is much
smaller than the number of words. In essence,
LDA maps information from the word dimen-
sion to a semantic topic dimension, to address the
shortcomings of the vector model.
3.2.2 User Profile Features
We extract several user?s activity statistics to con-
struct the user profile features, including the level
1725
of the user in the Community QA site, the number
of questions asked by this user, the number of an-
swers provided by this user, and the best answer
ratio of this user.
3.2.3 User Authority Score
Motivated by expert finding task (Jurczyk and
Agichtein, 2007; Si et al, 2010a; Li et al, 2011),
the second type of author related feature is author-
ity score, which denotes the expertise score of this
user. To compute the authority score, we first con-
struct a directed user graph with the user interac-
tions in the community. The nodes of the graph
represent users. An edge between two users in-
dicates a contribution from one user to the other.
Specifically, on a Q&A site, an edge from A to
B is established when user B answered a question
asked by A, which shows user B is more likely to
be an expert than A. The weight of an edge indi-
cates the number of interactions. We compute the
user?s authority score (AS) based on the link anal-
ysis algorithm PageRank:
AS(ui) =
1? d
N + d
?
uj?M(ui)
AS(uj)
L(uj)
(1)
where u1, . . . , uN are the users in the collection,
N is the total number of users, M(ui) is the set
of users whose answers are provided by user ui,
L(ui) is the number of users who answer ui?s
questions, d is a damping factor, which is set as
0.85. The authority score can be computed itera-
tively with random initial values.
3.2.4 Robot Features
The third type of author related feature is used for
detecting whether the author is a robot, which are
scripts crafted by malicious users to automatically
post answers. We observe that the distributions of
the answer-posting time are very different between
general user and robot. For example, some robots
may make posts continuously and mechanically,
hence the time increment may be smaller that hu-
man users who would need time to think and pro-
cess between two posts. Based on this observa-
tion, we design an time sequence feature for robot
detection. For each author, we can get a list of
time points to post answers, T = {t0, t1, ..., tn},
where ti is the time point when posting the ith an-
swer. We first convert the time sequence T to time
interval sequence ?T = {?t0,?t1, ...,?tn?1},
where ?ti = ti+1 ? ti. Based on the interval
sequences for all users, we then construct a ma-
trix Xm?b whose rows correspond to users and
columns correspond to interval histogram with
predefined range. We can use each row vector as
time sequence pattern to detect robot. To reduce
the noise and sparse problem, we use the dimen-
sion reduction techniques to extract the latent se-
mantic features with Singular Value Decomposi-
tion (SVD) (Deerwester et al, 1990; Kim et al,
2006).
3.2.5 Evaluation from Other Users
In the Community QA sites, other users can ex-
press their opinions or evaluations on the answer.
For example, the asker can choose one of the an-
swers as best answer. We use a bool feature to de-
note if this answer is selected as the best answer.
In addition, other users can label each answer as
?helpful? or ?not helpful?. We also use this helpful
evaluation by other users as the contextual feature,
which is defined as the ratio between the number
of ?helpful? votes and the number of total votes.
3.2.6 Duplication with Other Answers
The malicious user may post the pre-written prod-
uct promotion documents to many answers, or just
change the product name. We also compute the
similarity between different answers. If the two
answers are totally same, but the question is differ-
ent, these answer is potentially as a deceptive an-
swer. Here, we don?t want to measure the semantic
similarity between two answers, but just measure
if two answers are similar to the word level, there-
fore, we apply BleuScore (Papineni et al, 2002),
which is a standard metric in machine translation
for measuring the overlap between n-grams of two
text fragments r and c. The duplication score of
each answer is the maximum BleuScore compared
to all other answers.
4 Deceptive Answer Prediction with User
Preference Graph
Besides the textual and contextual features, we
also investigate the user relationship for decep-
tive answer prediction. We assume that similar
users tend to perform similar behaviors (posting
deceptive answers or posting authentic answers).
In this section, we first show how to compute the
user similarity (user preference graph construc-
tion), and then introduce how to employ the user
relationship for deceptive answer prediction.
4.1 User Preference Graph Construction
In this section, we propose a new user graph to de-
scribe the relationship among users. Figure 1 (a)
shows the general process in a question answering
1726
Question 
Answer1 
Answer2 
Best Answer 
u1 
u2 
u3 
u4 
u5 
u6 
(a) Question Answering (b) User Preference Relation (c) User Preference Graph
Figure 1: User Preference Graph Construction
thread. The asker, i.e. u1, asks a question. Then,
there will be several answers to answer this ques-
tion from other users, for example, answerers u2
and u3. After the answers are provides, users can
also vote each answer as ?helpful? or ?not help-
ful? to show their evaluation towards the answer .
For example, users u4, u5 vote the first answer as
?not helpful?, and user u6 votes the second answer
as ?helpful?. Finally, the asker will select one an-
swer as the best answer among all answers. For
example, the asker u1 selects the first answer as
the ?best answer?.
To mine the relationship among users, previous
studies mainly focus on the asker-answerer rela-
tionship (Jurczyk and Agichtein, 2007; Liu et al,
2011). They assume the answerer is more knowl-
edgeable than the asker. Based on this assump-
tion, they can extract the expert in the commu-
nity, as discussed in Section 3.2.3. However, we
don?t care which user is more knowledgeable, but
are more interested in whether two users are both
malicious users or authentic users. Here, we pro-
pose a new user graph based on the user prefer-
ence. The preference is defined based on the an-
swer evaluation. If two users show same pref-
erence towards the target answer, they will have
the user-preference relationship. We mainly use
two kinds of information: ?helpful? evaluation and
?best answer? selection. If two users give same
?helpful? or ?not helpful? to the target answer, we
view these two users have same user preference.
For example, user u4 and user u5 both give ?not
helpful? evaluation towards the first answer, we
can say that they have same user preference. Be-
sides the real ?helpful? evaluation, we also assume
the author of the answer gives the ?helpful? evalu-
ation to his or her own answer. Then if user u6 give
?helpful? evaluation to the second answer, we will
view user u6 has same preference as user u3, who
is the author of the second answer. We also can ex-
tract the user preference with ?best answer? selec-
tion. If the asker selects the ?best answer? among
all answers, we will view that the asker has same
preference as the author of the ?best answer?. For
example, we will view user u1 and user u2 have
same preference.
Based on the two above assumptions, we can
extract three user preference relationships (with
same preference) from the question answering ex-
ample in Figure 1 (a): u4 ? u5, u3 ? u6, u1 ? u2,
as shown in Figure1 (b). After extracting all user
preference relationships, we can construct the user
preference graph as shown in Figure 1 (c). Each
node represents a user. If two users have the user
preference relationship, there will be an edge be-
tween them. The edge weight is the number of
user preference relationships.
In the Community QA sites, the spammers
mainly promote their target products by promoting
the deceptive answers. The spammers can collab-
oratively make the deceptive answers look good,
by voting them as high-quality answer, or select-
ing them as ?best answer?. However, the authen-
tic users generally have their own judgements to
the good and bad answers. Therefore, the evalu-
ation towards the answer reflects the relationship
among users. Although there maybe noisy rela-
tionship, for example, an authentic user may be
cheated, and selects the deceptive answer as ?best
answer?, we hope the overall user preference rela-
tion can perform better results than previous user
interaction graph for this task.
1727
4.2 Incorporating User Preference Graph
To use the user graph, we can just compute the
feature value from the graph, and add it into the
supervised method as the features introduced in
Section 3. Here, we propose a new technique to
employ the user preference graph. We utilize the
graph regularizer (Zhang et al, 2006; Lu et al,
2010) to constrain the supervised parameter learn-
ing. We will introduce this technique based on
a commonly used model f(?), the linear weight
model, where the function value is determined by
linear combination of the input features:
f(xi) = wT ? xi =
?
k
wk ? xik (2)
where xi is a K dimension feature vector for the
ith answer, the parameter value wk captures the
effect of the kth feature in predicting the deceptive
answer. The best parameters w? can be found by
minimizing the following objective function:
?1(w) =
?
i
L(wTxi, yi) + ? ? |w|2F (3)
where L(wTxi, yi) is a loss function that mea-
sures discrepancy between the predicted label
wT ? xi and the true label yi, where yi ?
{+1,?1}. The common used loss functions in-
clude L(p, y) = (p?y)2 (least square), L(p, y) =
ln (1 + exp (?py)) (logistic regression). For sim-
plicity, here we use the least square loss function.
|w|2F =
?
k w2k is a regularization term defined
in terms of the Frobenius norm of the parameter
vector w and plays the role of penalizing overly
complex models in order to avoid fitting.
We want to incorporate the user preference re-
lationship into the supervised learning framework.
The hypothesis is that similar users tend to have
similar behaviors, i.e. posting deceptive answers
or authentic answers. Here, we employ the user
preference graph to denote the user relationship.
Based on this intuition, we propose to incorporate
the user graph into the linear weight model with
graph regularization. The new objective function
is changed as:
?2(w) =
?
i
L(wTxi, yi) + ? ? |w|2F +
?
?
ui,uj?Nu
?
x?Aui ,y?Auj
wui,uj (f(x)? f(y))2 (4)
where Nu is the set of neighboring user pairs in
user preference graph, i.e, the user pairs with same
preference. Aui is the set of all answers posted by
user ui. wui,uj is the weight of edge between ui
and uj in user preference graph. In the above ob-
jective function, we impose a user graph regular-
ization term
?
?
ui,uj?Nu
?
x?Aui ,y?Auj
wui,uj (f(x)? f(y))2
to minimize the answer authenticity difference
among users with same preference. This regu-
larization term smoothes the labels on the graph
structure, where adjacent users with same prefer-
ence tend to post answers with same label.
5 Experiments
5.1 Experiment Setting
5.1.1 Dataset Construction
In this paper, we employ the Confucius (Si et
al., 2010b) data to construct the deceptive an-
swer dataset. Confucius is a community question
answering site, developed by Google. We first
crawled about 10 million question threads within
a time range. Among these data, we further sam-
ple a small data set, and ask three trained annota-
tors to manually label the answer as deceptive or
not. If two or more people annotate the answer as
deceptive, we will extract this answer as a decep-
tive answer. In total, 12446 answers are marked
as deceptive answers. Similarly, we also manu-
ally annotate 12446 authentic answers. Finally,
we get 24892 answers with deceptive and authen-
tic labels as our dataset. With our labeled data,
we employ supervised methods to predict decep-
tive answers. We conduct 5-fold cross-validation
for experiments. The larger question threads data
is employed for feature learning, such as transla-
tion model, and topic model training.
5.1.2 Evaluation Metrics
The evaluation metrics are precision, recall and
F -score for authentic answer category and de-
ceptive answer category: precision = Sp?ScSp ,
recall = Sp?ScSc , and F = 2?precision?recallprecision+recall , where
Sc is the set of gold-standard positive instances for
the target category, Sp is the set of predicted re-
sults. We also use the accuracy as one metric,
which is computed as the number of answers pre-
dicted correctly, divided by the number of total an-
swers.
1728
Deceptive Answer Authentic Answer Overall
Prec. Rec. F-Score Prec. Rec. F-Score Acc.
Random 0.50 0.50 0.50 0.50 0.50 0.50 0.50
Unigram/Bigram (UB) 0.61 0.71 0.66 0.66 0.55 0.60 0.63
URL 0.93 0.26 0.40 0.57 0.98 0.72 0.62
Phone/Mail 0.94 0.15 0.25 0.53 0.99 0.70 0.57
Length 0.56 0.91 0.69 0.76 0.28 0.41 0.60
All Textual Features 0.64 0.67 0.66 0.66 0.63 0.64 0.65
QA Relevance 0.66 0.57 0.61 0.62 0.71 0.66 0.64
User Profile 0.62 0.53 0.57 0.59 0.67 0.63 0.60
User Authority 0.54 0.80 0.65 0.62 0.33 0.43 0.56
Robot 0.66 0.62 0.64 0.61 0.66 0.64 0.64
Answer Evaluation 0.55 0.53 0.54 0.55 0.57 0.56 0.55
Answer Duplication 0.69 0.71 0.70 0.70 0.68 0.69 0.69
All Contextual Feature 0.78 0.74 0.76 0.75 0.79 0.77 0.77
Textutal + Contextual 0.80 0.82 0.81 0.82 0.79 0.80 0.81
Table 2: Results With Textual and Contextual Features
5.2 Results with Textual and Contextual
Features
We tried several different classifiers, including
SVM, ME and the linear weight models with least
square and logistic regression. We find that they
can achieve similar results. For simplicity, the lin-
ear weight with least square is employed in our
experiment. Table 2 shows the experiment results.
For textual features, it achieves much better re-
sult with unigram/bigram features than the ran-
dom guess. This is very different from the an-
swer quality prediction task. The previous stud-
ies (Jeon et al, 2006; Song et al, 2010) find that
the word features can?t improve the performance
on answer quality prediction. However, from Ta-
ble 1, we can see that the word features can pro-
vide some weak signals for deceptive answer pre-
diction, for example, words ?recommend?, ?ad-
dress?, ?professional? express some kinds of pro-
motion intent. Besides unigram and bigram, the
most effective textual feature is URL. The phone
and email features perform similar results with
URL. The observation of length feature for decep-
tive answer prediction is very different from previ-
ous answer quality prediction. For answer quality
prediction, length is an effective feature, for exam-
ple, long-length provides very strong signals for
high-quality answer (Shah and Pomerantz, 2010;
Song et al, 2010). However, for deceptive answer
prediction, we find that the long answers are more
potential to be deceptive. This is because most of
deceptive answers are well prepared for product
promotion. They will write detailed answers to at-
tract user?s attention and promote their products.
Finally, with all textual features, the experiment
achieves the best result, 0.65 in accuracy.
For contextual features, we can see that, the
most effective contextual feature is answer dupli-
cation. The malicious users may copy the pre-
pared deceptive answers or just simply edit the tar-
get name to answer different questions. Question-
answer relevance and robot are the second most
useful single features for deceptive answer predic-
tion. The main characteristics of the Community
QA sites is to accumulate the answers for the tar-
get questions. Therefore, all the answers should be
relevant to the question. If the answer is not rel-
evant to the corresponding question, this answer
is more likely to be deceptive. Robot is one of
main sources for deceptive answers. It automat-
ically post the deceptive answers to target ques-
tions. Here, we formulate the time series as in-
terval sequence. The experiment result shows that
the robot indeed has his own posting behavior pat-
terns. The user profile feature also can contribute
a lot to deceptive answer prediction. Among the
user profile features, the user level in the Com-
munity QA site is a good indicator. The other
two contextual features, including user authority
and answer evaluation, provide limited improve-
ment. We find the following reasons: First, some
malicious users post answers to various questions
for product promotion, but don?t ask any question.
From Equation 1, when iteratively computing the
1729
Deceptive Answer Authentic Answer Overall
Prec. Rec. F-Score Prec. Rec. F-Score Acc.
Interaction Graph as Feature 0.80 0.82 0.81 0.82 0.79 0.80 0.81
Interaction Graph as Regularizer 0.80 0.83 0.82 0.82 0.80 0.81 0.82
Preference Graph as Feature 0.79 0.83 0.81 0.82 0.78 0.80 0.81
Preference Graph as Regularizer 0.83 0.86 0.85 0.85 0.83 0.84 0.85
Table 3: Results With User Preference Graph
final scores, the authority scores for these mali-
cious users will be accumulated to large values.
Therefore, it is hard to distinguish whether the
high authority score represents real expert or mali-
cious user. Second, the ?best answer? is not a good
signal for deceptive answer prediction. This may
be selected by malicious users, or the authentic
asker was misled, and chose the deceptive answer
as ?best answer?. This also demonstrates that the
deceptive answer prediction is very different from
the answer quality prediction. When combining
all the contextual features, it can achieve the over-
all accuracy 0.77, which is much better than the
textual features. Finally, with all the textual and
contextual features, we achieve the overall result,
0.81 in accuracy.
5.3 Results with User Preference Graph
Table 3 shows the results with user preference
graph. We compare with several baselines. Inter-
action graph is constructed by the asker-answerer
relationship introduced in Section 3.2.3. When
using the user graph as feature, we compute the
authority score for each user with PageRank as
shown in Equation 1. We also incorporating the
interaction graph with a regularizer as shown in
Equation 4. Note that we didn?t consider the edge
direction when using interaction graph as a regu-
larizer. From the table, we can see that when in-
corporating user preference graph as a feature, it
can?t achieve a better result than the interaction
graph. The reason is similar as the interaction
graph. The higher authority score may boosted
by other spammer, and can?t be a good indica-
tor to distinguish deceptive and authentic answers.
When we incorporate the user preference graph
as a regularizer, it can achieve about 4% further
improvement, which demonstrates that the user
evaluation towards answers, such as ?helpful? vot-
ing and ?best answer? selection, is a good signal
to generate user relationship for deceptive answer
prediction, and the graph regularization is an ef-
fective technique to incorporate the user prefer-
ence graph. We also analyze the parameter sen-
10?5 10?4 10?3 10?2 10?1 1000.76
0.78
0.8
0.82
0.84
0.86
0.88
Acc
urac
y
 
 General supervised methodInteraction Graph as RegularizerPreference Graph as Regularizer
Figure 2: Results with different values of ?
sitivity. ? is the tradeoff weight for graph regular-
ization term. Figure 2 shows the results with dif-
ferent values of ?. We can see that when ? ranges
from 10?4 ? 10?2, the deceptive answer predic-
tion can achieve best results.
6 Conclusions and Future Work
In this paper, we discuss the deceptive answer
prediction task in Community QA sites. With
the manually labeled data set, we first predict the
deceptive answers with traditional classification
method. Two types of features, including textual
features and contextual features, are extracted and
analyzed. We also introduce a new user prefer-
ence graph, constructed based on the user evalua-
tions towards the target answer, such as ?helpful?
voting and ?best answer? selection. A graph reg-
ularization method is proposed to incorporate the
user preference graph for deceptive answer predic-
tion. The experiments are conducted to discuss
the effects of different features. The experiment
results also show that the method with user pref-
erence graph can achieve more accurate results for
deceptive answer prediction.
In the future work, it is interesting to incorpo-
rate more features into deceptive answer predic-
tion. It is also important to predict the deceptive
question threads, which are posted and answered
both by malicious users for product promotion.
Malicious user group detection is also an impor-
tant task in the future.
1730
References
Lada A. Adamic, Jun Zhang, Eytan Bakshy, and
Mark S. Ackerman. 2008. Knowledge sharing
and yahoo answers: everyone knows something. In
Proceedings of the 17th international conference on
World Wide Web, WWW ?08, pages 665?674, New
York, NY, USA. ACM.
Jiang Bian, Yandong Liu, Ding Zhou, Eugene
Agichtein, and Hongyuan Zha. 2009. Learning to
recognize reliable users and content in social media
with coupled mutual reinforcement. In Proceedings
of the 18th international conference on World wide
web, WWW ?09, pages 51?60, NY, USA. ACM.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet alocation. J. Mach. Learn.
Res., 3:993?1022, March.
Peter F. Brown, John Cocke, Stephen A. Della Pietra,
Vincent J. Della Pietra, Fredrick Jelinek, John D.
Lafferty, Robert L. Mercer, and Paul S. Roossin.
1990. A statistical approach to machine translation.
Comput. Linguist., 16:79?85, June.
S. Deerwester, S.T. Dumais, G.W. Furnas, T.K. Lan-
dauer, and R. Harshman. 1990. Indexing by latent
semantic analysis. Journal of the American society
for information science, 41(6):391?407.
A. Figueroa and J. Atkinson. 2011. Maximum entropy
context models for ranking biographical answers to
open-domain definition questions. In Twenty-Fifth
AAAI Conference on Artificial Intelligence.
F. Maxwell Harper, Daphne Raban, Sheizaf Rafaeli,
and Joseph A. Konstan. 2008. Predictors of answer
quality in online q&a sites. In Proceedings of the
twenty-sixth annual SIGCHI conference on Human
factors in computing systems, CHI ?08, pages 865?
874, New York, NY, USA. ACM.
Daisuke Ishikawa, Tetsuya Sakai, and Noriko Kando,
2010. Overview of the NTCIR-8 Community QA Pi-
lot Task (Part I): The Test Collection and the Task,
pages 421?432. Number Part I.
Jiwoon Jeon, W. Bruce Croft, and Joon Ho Lee. 2005.
Finding similar questions in large question and an-
swer archives. In Proceedings of the 14th ACM
CIKM conference, 05, pages 84?90, NY, USA.
ACM.
J. Jeon, W.B. Croft, J.H. Lee, and S. Park. 2006. A
framework to predict the quality of answers with
non-textual features. In Proceedings of the 29th an-
nual international ACM SIGIR conference on Re-
search and development in information retrieval,
pages 228?235. ACM.
P. Jurczyk and E. Agichtein. 2007. Discovering au-
thorities in question answer communities by using
link analysis. In Proceedings of the sixteenth ACM
CIKM conference, pages 919?922. ACM.
H. Kim, P. Howland, and H. Park. 2006. Dimension
reduction in text classification with support vector
machines. Journal of Machine Learning Research,
6(1):37.
Fangtao Li, Minlie Huang, Yi Yang, and Xiaoyan Zhu.
2011. Learning to identify review spam. In Pro-
ceedings of the Twenty-Second international joint
conference on Artificial Intelligence-Volume Volume
Three, pages 2488?2493. AAAI Press.
Yuanjie Liu, Shasha Li, Yunbo Cao, Chin-Yew Lin,
Dingyi Han, and Yong Yu. 2008. Understand-
ing and summarizing answers in community-based
question answering services. In Proceedings of the
22nd International Conference on Computational
Linguistics - Volume 1, COLING ?08, pages 497?
504, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
Jing Liu, Young-In Song, and Chin-Yew Lin. 2011.
Competition-based user expertise score estimation.
In Proceedings of the 34th international ACM SI-
GIR conference on Research and development in In-
formation Retrieval, pages 425?434. ACM.
Yue Lu, Panayiotis Tsaparas, Alexandros Ntoulas, and
Livia Polanyi. 2010. Exploiting social context for
review quality prediction. In Proceedings of the
19th international conference on World wide web,
pages 691?700. ACM.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Comput. Linguist., 29:19?51, March.
Lawrence Page, Sergey Brin, Rajeev Motwani, and
Terry Winograd. 1999. The pagerank citation rank-
ing: Bringing order to the web. Technical Report
1999-66, Stanford InfoLab, November. SIDL-WP-
1999-0120.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of
the 40th Annual Meeting on Association for Com-
putational Linguistics, ACL ?02, pages 311?318,
Stroudsburg, PA, USA. ACL.
Gerard Salton and Michael J. McGill. 1986. Intro-
duction to Modern Information Retrieval. McGraw-
Hill, Inc., New York, NY, USA.
Chirag Shah and Jefferey Pomerantz. 2010. Evaluat-
ing and predicting answer quality in community qa.
In Proceedings of the 33rd international ACM SIGIR
conference on Research and development in infor-
mation retrieval, SIGIR ?10, pages 411?418, New
York, NY, USA. ACM.
X. Si, Z. Gyongyi, and E. Y. Chang. 2010a. Scal-
able mining of topic-dependent user reputation for
improving user generated content search quality. In
Google Technical Report.
1731
Xiance Si, Edward Y. Chang, Zolta?n Gyo?ngyi, and
Maosong Sun. 2010b. Confucius and its intelli-
gent disciples: integrating social with search. Proc.
VLDB Endow., 3:1505?1516, September.
Young-In Song, Chin-Yew Lin, Yunbo Cao, and Hae-
Chang Rim. 2008. Question utility: a novel static
ranking of question search. In Proceedings of the
23rd national conference on Artificial intelligence
- Volume 2, AAAI?08, pages 1231?1236. AAAI
Press.
Y.I. Song, J. Liu, T. Sakai, X.J. Wang, G. Feng, Y. Cao,
H. Suzuki, and C.Y. Lin. 2010. Microsoft research
asia with redmond at the ntcir-8 community qa pilot
task. In Proceedings of NTCIR.
Wei Wei, Gao Cong, Xiaoli Li, See-Kiong Ng, and
Guohui Li. 2011. Integrating community question
and answer archives. In AAAI.
Y. Yang and J.O. Pedersen. 1997. A compara-
tive study on feature selection in text categoriza-
tion. In MACHINE LEARNING-INTERNATIONAL
WORKSHOP THEN CONFERENCE-, pages 412?
420. MORGAN KAUFMANN PUBLISHERS.
Tong Zhang, Alexandrin Popescul, and Byron Dom.
2006. Linear prediction models with graph regu-
larization for web-page categorization. In Proceed-
ings of the 12th ACM SIGKDD international con-
ference on Knowledge discovery and data mining,
pages 821?826. ACM.
1732
