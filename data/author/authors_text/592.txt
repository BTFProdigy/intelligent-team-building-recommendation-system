IBM MASTOR SYSTEM: Multilingual Automatic Speech-to-speech Translator * 
 
Yuqing Gao, Liang Gu, Bowen Zhou, Ruhi Sarikaya, Mohamed Afify, Hong-Kwang Kuo, 
Wei-zhong Zhu, Yonggang Deng, Charles Prosser, Wei Zhang and Laurent Besacier 
IBM T. J. Watson Research Center, Yorktown Heights, NY 10598 
 
ABSTRACT 
In this paper, we describe the IBM MASTOR, a speech-to-speech 
translation system that can translate spontaneous free-form 
speech in real-time on both laptop and hand-held PDAs. Chal-
lenges include speech recognition and machine translation in 
adverse environments, lack of training data and linguistic re-
sources for under-studied languages, and the need to rapidly de-
velop capabilities for new languages. Another challenge is de-
signing algorithms and building models in a scalable manner to 
perform well even on memory and CPU deficient hand-held com-
puters. We describe our approaches, experience, and success in 
building working free-form S2S systems that can handle two 
language pairs (including a low-resource language). 
 
1. INTRODUCTION 
Automatic speech-to-speech (S2S) translation breaks down com-
munication barriers between people who do not share a common 
language and hence enable instant oral cross-lingual communica-
tion for many critical applications such as emergency medical 
care. The development of an accurate, efficient and robust S2S 
translation system poses a lot of challenges. This is especially 
true for colloquial speech and resource deficient languages. 
The IBM MASTOR speech-to-speech translation system has been 
developed for the DARPA CAST and Transtac programs whose 
mission is to develop technologies that enable rapid deployment 
of real-time S2S translation of low-resource languages on port-
able devices. It originated from the IBM MARS S2S system 
handling the air travel reservation domain described in [1], which 
was later significantly improved in all components, including 
ASR, MT and TTS, and later evolved into the MASTOR multi-
lingual S2S system that covers much broader domains such as 
medical treatment and force protection [2,3]. More recently, we 
have further broadened our experience and efforts to very rapidly 
develop systems for under-studied languages, such as regional 
dialects of Arabic. The intent of this program is to provide lan-
guage support to military, medical and humanitarian personnel 
during operations in foreign territories, by deciphering possibly 
critical language communications with a two-way real-time 
speech-to-speech translation system designed for specific tasks 
such as medical triage and force protection.  
The initial data collection effort for the project has shown that the 
domain of force protection and medical triage is, though limited, 
rather broad. In fact, the definition of domain coverage is tough 
when the speech from responding foreign language speakers are 
concerned, as their responses are less constrained and may in-
clude out-of-domain words and concepts. Moreover, flexible 
casual or colloquial speaking style inevitably appears in the hu-
man-to-human conversational communications. Therefore, the 
project is a great challenge that calls for major research efforts. 
Among all the challenges for speech recognition and translation 
for under-studied languages, there are two main issues: 1) Lack of 
appropriate amount of speech data that represent the domain of 
interest and the oral language spoken by the target speakers, re-
sulting in difficulties in accurate estimation of statistical models 
for speech recognition and translation. 2) Lack of linguistic 
knowledge realization in spelling standards, transcriptions, lexi-
cons and dictionaries, or annotated corpora. Therefore, various 
different approaches have to be explored.  
Another critical challenge is to embed complicated algorithms 
and programs into small devices for mobile users. A hand-held 
computing device may have a CPU of 256MHz and 64MB mem-
ory; to fit the programs, as well as the models and data files into 
this memory and operate the system in real-time are tremendous 
challenges [4]. 
In this paper, we will describe the overall framework of the 
MASTOR system and our approaches for each major component, 
i.e., speech recognition and translation. Various statistical ap-
proaches [5,6,7,8] are explored and used to solve different techni-
cal challenges. We will show how we addressed the challenges 
that arise when building automatic speech recognition (ASR) and 
machine translation (MT) for colloquial Arabic on both the laptop 
and handheld PDA platforms. 
 
2. SYSTEM OVERVIEW 
The general framework of our speech translation system is illus-
trated in Figure 1. The general framework of our MASTOR sys-
tem has components of ASR, MT and TTS. The cascaded ap-
proach allows us to deploy the power of the existing advanced 
speech and language processing techniques, while concentrating 
on the unique problems in speech-to-speech translation. Figure 2 
illustrates the MASTOR GUI (Graphic User Interface) on laptop 
and PDA, respectively. 
Acoustic models for English and Mandarin baseline are devel-
oped for large-vocabulary continuous speech and trained on over 
200 hours of speech collected from about 2000 speakers for each 
language. However, the Arabic dialect speech recognizer was 
only trained using about 50 hours of dialectal speech.  The train-
ing data for Arabic consists of about 200K short utterances. Large 
efforts were invested in initial cleaning and normalization of the 
training data because of large number of irregular dialectal words 
and variations in spellings. We experimented with three ap-
proaches for pronunciation and acoustic modeling: i.e. grapheme, 
phonetic, and context-sensitive grapheme as will be described in 
ASR TTS 
Statistical NLU/NLG 
based MT 
Figure 1 IBM MASTOR Speech-to-Speech Translation System 
Statistical MT using 
WFST/SIPL  
* Thanks to DARPA for funding 
section 3.A. We found that using context-sensitive pronunciation 
rules reduces the WER of the grapheme based acoustic model by 
about 3% (from 36.7% to 35.8%). Based on these results, we 
decided to use context-sensitive grapheme models in our system.  
The Arabic language model (LM) is an interpolated model con-
sisting of a trigram LM, a class-based LM and a morphologically 
processed LM, all trained from a corpus of a few hundred thou-
sand words. We also built a compact language model for the 
hand-held system, where singletons are eliminated and bigram 
and trigram counts are pruned with increased thresholds. The LM 
footprint size is 10MB. 
There are two approaches for translation. The concept based ap-
proach uses natural language understanding (NLU) and natural 
language generation models trained from an annotated corpus. 
Another approach is the phrase-based finite state transducer 
which is trained using an un-annotated parallel corpus. 
A trainable, phrase-splicing and variable substitution TTS system 
is adopted to synthesize speech from translated sentences, which 
has a special ability to generate speech of mixed languages seam-
lessly [9]. In addition, a small footprint TTS is developed for the 
handheld devices using embedded concatenative TTS technolo-
gies.[10] 
Next, we will describe our approaches in automatic speech recog-
nition and machine translation in greater detail. 
 
3. AUTOMATIC SPEECH RECOGNITION 
A. Acoustic Models 
Acoustic models and the pronunciation dictionary greatly influ-
ence the ASR performance. In particular, creating an accurate 
pronunciation dictionary poses a major challenge when changing 
the language. Deriving pronunciations for resource rich languages 
like English or Mandarin is relatively straight forward using ex-
isting dictionaries or letter to sound models. In certain languages 
such as Arabic and Hebrew, the written form does not typically 
contain short vowels which a native speaker can infer from con-
text. Deriving automatic phonetic transcription for speech corpora 
is thus difficult. This problem is even more apparent when con-
sidering colloquial Arabic, mainly due to the large number of 
irregular dialectal words. 
One approach to overcome the absence of short vowels is to use 
grapheme based acoustic models. This leads to straightforward 
construction of pronunciation lexicons and hence facilitates 
model training and decoding. However, the same grapheme may 
lead to different phonetic sounds depending on its context. This 
results in less accurate acoustic models. For this reason we ex-
perimented with two other different approaches. The first is a full 
phonetic approach which uses short vowels, and the second uses 
context-sensitive graphemes for the letter "A" (Alif) where two 
different phonemes are used for "A" depending on its position in 
the word. 
Using phoneme based pronunciations would require vowelization 
of every word. To perform vowelization, we used a mix of dic-
tionary search and a statistical approach. The word is first 
searched in an existing vowelized dictionary, and if not found it is 
passed to the statistical vowelizer [11].  Due to the difficulties in 
accurately vowelizing dialectal words, our experiments have not 
shown any improvements using phoneme based ASR compared 
to grapheme based.  
Speech recognition for both the laptop and hand-held systems is 
based on the IBM ViaVoice engine. This highly robust and effi-
cient framework uses rank based acoustic scores [12] which are 
derived from tree-clustered context dependent Gaussian models. 
These acoustic scores together with n-gram LM probabilities are 
incorporated into a stack based search algorithm to yield the most 
probable word sequence given the input speech. 
The English acoustic models use an alphabet of 52 phones. Each 
phone is modeled with a 3-state left-to-right hidden Markov 
model (HMM). The system has approximately 3,500 context-
dependent states modeled using 42K Gaussian distributions and 
trained using 40 dimensional features. The context-dependent 
states are generated using a decision-tree classifier. The collo-
quial Arabic acoustic models use about 30 phones that essentially 
correspond to graphemes in the Arabic alphabet. The colloquial 
Arabic HMM structure is the same as that of the English model. 
The Arabic acoustic models are also built using 40 dimensional 
features. The compact model for the PDA has about 2K leaves 
and 28K Gaussian distributions.  The laptop version has over 3K 
leaves and 60K Gaussians. All acoustic models are trained using 
discriminative training [13]. 
B. Language Modeling   
Language modeling (LM) of the probability of various word se-
quences is crucial for high-performance ASR of free-style open-
 
   
 
 
Figure 2  IBM MASTOR system in Windows XP and Win-
dows CE 
ended coversational systems. Our approaches to build statistical 
tri-gram LMs fall into three categories: 1) obtaining additional 
training material automatically; 2) interpolating domain-specific 
LMs with other LMs; 3) improving distribution estimation ro-
bustness and accuracy with limited in-domain resources. Auto-
matic data collection and expansion is the most straight-forward 
way to achieve efficient LM, especially when little in-domain 
data is available. For resource-rich languages such as English and 
Chinese, we retrieve additional data from the World Wide Web 
(WWW) to enhance our limited domain specific data, which 
shows significant improvement [6]. 
In Arabic, words can take prefixes and suffixes to generate new 
words which are semantically related to the root form of the word 
(stem). As a result, the vocabulary size in Arabic can become 
very large even for specific domains. To alleviate this problem, 
we built a language model on morphologically tokenized data by 
applying morphological analysis and hence splitting some of the 
words into prefix+stem+suffix, prefix+stem or stem+suffix forms. 
We refer the reader to [14] to learn more about the morphological 
tokenization algorithm. Morphological analysis reduced the vo-
cabulary size by about 30% without sacrificing the coverage. 
More specifically, in our MASTOR system, the English language 
model has two components that are linearly interpolated. The first 
one is built using in-domain data. The second component acts as a 
background model and is built using a very large generic text 
inventory that is domain independent. The language model counts 
are also pruned to control the size of this background model. The 
colloquial Arabic language model for our laptop system is com-
posed of three components that are linearly interpolated. The first 
one is the basic word tri-gram model. The second one is a class 
based language model with 13 classes that covers names for Eng-
lish and Arabic, numbers, months, days, etc. The third one is the 
morphological language model described above. 
4. SPEECH TRANSLATION 
A. NLU/NLG-based Speech Translation 
One of the translation algorithms we proposed and applied in 
MASTOR is the statistical translation method based on natural 
language understanding (NLU) and natural language generation 
(NLG). Statistical machine translation methods translate a sen-
tence W in the source language into a sentence A in the target 
language by using a statistical model that estimates the probabil-
ity of A given W, i.e. ( )WAp . Conventionally, ( )WAp  is opti-
mized on a set of pairs of sentences that are translations of one 
another. To alleviate this data sparseness problem and, hence, 
enhance both the accuracy and robustness of estimating ( )WAp , 
we proposed a statistical concept-based machine translation para-
digm that predicts A with not only W but also the underlying con-
cepts embedded in W and/or A. As a result, the optimal sentence 
A is picked by first understanding the meaning of the source sen-
tence W.  
Let C denote the concepts in the source language and S denote the 
concepts in the target language, our proposed statistical concept-
based algorithm should select a word sequence A? as 
( ) ( ) ( ) ( )
??
?
??
?
== ? WCpWCSpWCSApWApA
CSAA
,,,maxargmaxarg?
,
 , 
where the conditional probabilities ( )WCp , ( )WCSp ,  and 
( )WCSAp ,,  are estimated by the Natural Language Understand-
ing (NLU), Natural Concept Generation (NCG) and Natural 
Word Generation (NWG) procedures, respectively. The probabil-
ity distributions are estimated and optimized upon a pre-annotated 
bilingual corpus. In our MASTOR system, ( )WCp  is estimated 
by a decision-tree based statistical semantic parser, and 
( )WCSp ,  and ( )WCSAp ,,  are estimated by maximizing the 
conditional entropy as depicted in [2] and [7], respectively. 
We are currently developing a new translation method that unifies 
statistical phrase-based translation models and the above 
NLU/NLG based approach. We will discuss this work in future 
publications. 
 
B. Fast and Memory Efficient Machine Translation Using SIPL 
Another translation method we proposed in MASTOR is based on 
the Weighted Finite-State Transducer (WFST). In particular, we 
developed a novel phrase-based translation framework using 
WFSTs that achieves both memory efficiency and fast speed, 
which is suitable for real time speech-to-speech translation on 
scalable computational platforms. In the proposed framework [15] 
which we refer to as Statistical Integrated Phrase Lattices (SIPLs), 
we statically construct a single optimized WFST encoding the 
entire translation model. In addition, we introduce a Viterbi de-
coder that can combine the translation model and language model 
FSTs with the input lattice efficiently, resulting in translation 
speeds of up to thousands of words per second on a PC and hun-
dred words per second on a PDA device. This WFST-based ap-
proach is well-suited to devices with limited computation and 
memory. We achieve this efficiency by using methods that allow 
us to perform more composition and graph optimization offline 
(such as, the determinization of the phrase segmentation trans-
ducer P) than in previous work, and by utilizing a specialized 
decoder involving multilayer search.  
During the offline training, we separate the entire translation lat-
tice H into two pieces: the language model L and the translation 
model M: 
( )( )( )WTPDetMinMinM =  
where   is the composition operator, Min  denotes the 
minimization operation, and Det  denotes the determinization 
operation; T is the phrase translation transducer, and W is the 
phrase-to-word transducer. Due to the determinizability of P, M 
can be computed offline using a moderate amount of memory. 
The translation problem can be framed as finding the best path in 
the full search lattice given an input sentence/automaton I. To 
address the problem of efficiently computing LMI  , we have 
developed a multilayer search algorithm. 
Specifically, we have one layer for each of the input FSM's: I, L, 
and M. At each layer, the search process is performed via a state 
traversal procedure starting from the start state 0s

, and consum-
ing an input word in each step in a left-to-right manner.  
We represent each state s in the search space using the following 
7-tuple: Is , Ms , Ls , Mc , Lc , h
 
, prevs , where Is , Ms , and 
Ls record the current state in each input FSM; Mc and Lc  record 
the accumulated cost in L and M in the best path up to this point; 
h
 
 records the target word sequence labeling the best path up to 
this point; and prevs  records the best previous state. 
To reduce the search space, two active search states are merged 
whenever they have identical Is , Ms , and Ls values; the re-
maining state components are inherited from the state with lower 
cost.  In addition, two pruning methods, histogram pruning and 
threshold or beam pruning, are used to achieve the desired bal-
ance between translation accuracy and speed. 
To provide the decoder for the PDA devices as well that lacks a 
floating-point processor, the search algorithm is implemented 
using fixed-point arithmetic. 
 
 
5. CONCLUSION 
We described the framework of the IBM MASTOR system, the 
various technologies used in building major components for lan-
guages with different levels of data resources. The technologies 
have shown successes in building real-time S2S systems on both 
laptop and small computation resource platforms for two lan-
guage pairs, English-Mandarin Chinese, and English-Arabic dia-
lect. In the latter case, we also developed approaches which lead 
to very rapid (in the matter of 3-4 months) development of sys-
tems using very limited language and domain resources. We are 
working on improving spontaneous speech recognition accuracy 
and more naturally integrating two translation approaches.  
 
6. ACKNOWLEDGEMENT 
The authors sincerely thank Drs. Yoshinori Tahara, Fu-hua Liu, 
Yongxing Li, Etienne Marcheret, Raimo Bakis, Ellen Eide, Burn 
Lewis, Tony Lee, Ossama Emam, and Lubos Ures for their help 
and contributions to the MASTOR S2S system. 
 
7. REFERENCES 
[1] Y. Gao et al ?MARS: A Statistical Semantic Parsing and Generation 
Based Multilingual Automatic tRanslation System,? Machine Trans-
lation, vol. 17, pp.185-212, 2004. 
[2] L. Gu et al ?Improving Statistical Natural Concept Generation in 
Interlingua-based Speech-to-Speech Translation,? in Proc. Eu-
rospeech?2003, pp.2769-2772. 
[3] F.-H. Liu, ?Robustness in Speech-to-Speech Translation,? in Proc. 
Eurospeech?2003, pp.2797-2800. 
[4] B. Zhou et al ?Two-way speech-to-speech translation on handheld 
devices,? in Proc. ICSLP'04, South Korea, Oct, 2004. 
[5] H. Erdogan et al ?Using Semantic Analysis to Improve Speech 
Recognition Performance,? Computer Speech and Language, vol.19, 
pp.321-343, 2005. 
[6] R. Sarikaya, et al  ?Rapid Language Model Development Using 
External Resources for New Spoken Dialog Domains,? in Proc. 
ICASSP'05, Philadelphia, PA, Mar, 2005. 
[7] L. Gu et al ?Concept-based Speech-to-Speech Translation using 
Maximum Entropy Models for Statistical Natural Concept Genera-
tion,? IEEE Trans. Speech and Audio Processing, vol.14, no.2, 
pp.377-392, March, 2006. 
[8] B. Zhou et al ?Constrained phrase-based translation using weighted 
finite-state transducers,? in Proc. ICASSP'05, Philadelphia, Mar, 
2005. 
[9] E. Eide et al ?Recent Improvements to the IBM Trainable Speech 
Synthesis System,? in Proc.  ICASSP, Hong Kong, China, 2003. 
[10] Dan Chazan et al ?Reducing the Footprint of the IBM Trainable 
Speech Synthesis System,? in ICSLP-2002, pp.2381-2384  
[11] R. Sarikaya et al ?Maximum Entropy Based Vowelization of Ara-
bic,? Interspeech2006 (submitted for publication). 
[12] L.R. Bahl, et al ?Robust methods for using context-dependent fea-
tures and models in a continuous speech recognizer,? in Proc. 
ICASSP, 1994 
[13] D. Povey & P.C. Woodland, ?Minimum Phone Error and I-
Smoothing for Improved Discriminative Training,? In Proc. ICASSP, 
Orlando, 2002. 
[14] M. Afify et.al, ?On the Use of Morphological Analysis for Dialectal 
Arabic Speech Recognition,? Interspeech 2006 (submitted for publi-
cation). 
[15] B. Zhou, S. Chen, and Y. Gao, ?Fast Machine Translation Using 
Statistical Integrated Phrase Lattices,? submitted to COL-
ING/ACL'2006. 
 
 
Proceedings of the Fourth Workshop on Statistical Machine Translation , pages 165?172,
Athens, Greece, 30 March ? 31 March 2009. c?2009 Association for Computational Linguistics
Mining a comparable text corpus for a Vietnamese - French  
statistical machine translation system 
 
 
Thi-Ngoc-Diep Do *,**, Viet-Bac Le *, Brigitte Bigi*,  
Laurent Besacier*, Eric Castelli** 
*LIG Laboratory, CNRS/UMR-5217, Grenoble, France 
** MICA Center, CNRS/UMI-2954, Hanoi, Vietnam 
thi-ngoc-diep.do@imag.fr 
 
 
 
 
Abstract 
This paper presents our first attempt at con-
structing a Vietnamese-French statistical 
machine translation system. Since Vietnam-
ese is an under-resourced language, we con-
centrate on building a large Vietnamese-
French parallel corpus. A document align-
ment method based on publication date, spe-
cial words and sentence alignment result is 
proposed. The paper also presents an appli-
cation of the obtained parallel corpus to the 
construction of a Vietnamese-French statis-
tical machine translation system, where the 
use of different units for Vietnamese (sylla-
bles, words, or their combinations) is dis-
cussed. 
1 Introduction 
Over the past fifty years of development, ma-
chine translation (MT) has obtained good results 
when applied to several pairs of languages such 
as English, French, German, Japanese, etc. How-
ever, for under-resourced languages, it still re-
mains a big gap. For instance, although 
Vietnamese is the 14th widely-used language in 
the world, research on MT for Vietnamese is 
very rare.  
The earliest MT system for Vietnamese is the 
system from the Logos Corporation, developed 
as an English-Vietnamese system for translating 
aircraft manuals during the 1970s (Hutchins, 
2001). Until now, in Vietnam, there are only four 
research groups working on MT for Vietnamese-
English (Ho, 2005). However the results are still 
modest. 
MT research on Vietnamese-French occurs 
even more rarely. Doan (2001) proposed a trans-
lation module for Vietnamese within ITS3, a 
multilingual MT system based on the classical 
analysis-transfer-generation approach. Nguyen 
(2006) worked on Vietnamese language and 
Vietnamese-French text alignment. But no com-
plete MT system for this pair of languages has 
been published so far.  
There are many approaches for MT: rule-based 
(direct translation, interlingua-based, transfer-
based), corpus-based (statistical, example-based) 
as well as hybrid approaches. We focus on build-
ing a Vietnamese-French statistical machine 
translation (SMT) system. Such an approach re-
quires a parallel bilingual corpus for source and 
target languages. Using this corpus, we build a 
statistical translation model for source/target lan-
guages and a statistical language model for target 
language. Then the two models and a search 
module are used to decode the best translation 
(Brown et al, 1993; Koehn et al, 2003). 
Thus, the first task is to build a large parallel 
bilingual text corpus. This corpus can be de-
scribed as a set of bilingual sentence pairs. At the 
moment, such a large parallel corpus for Viet-
namese-French is unavailable. (Nguyen, 2006) 
presents a Vietnamese-French parallel corpus of 
law and economics documents. Our SMT system 
was trained using Vietnamese-French news cor-
pus created by mining a comparable bilingual 
text corpus from the Web.  
Section 2 presents the general methodology of 
mining a comparable text corpus. We present an 
overview of document alignment methods and 
sentence alignment methods, and discuss the 
document alignment method we utilized, which 
is based on publishing date, special words, and 
sentence alignment results. Section 3 describes 
our experiments in automatically mining a multi-
lingual news website to create a Vietnamese-
French parallel text corpus. Section 4 presents 
165
our application to rapidly build Vietnamese-
French SMT systems using the obtained parallel 
corpus, where the use of different units for Viet-
namese (syllables, words, or their combination) is 
discussed. Section 5 concludes and discusses fu-
ture work. 
2 Mining a comparable text corpus  
In (Munteanu and Daniel Marcu, 2006), the au-
thors present a method for extracting parallel 
sub-sentential fragments from comparable bilin-
gual corpora. However this method is in need of 
an initial parallel bilingual corpus, which is not 
available for the pair of language Vietnamese-
French (in the news domain). 
The overall process of mining a bilingual text 
corpus which is used in a SMT system typically 
takes five following steps (Koehn, 2005): raw 
data collection, document alignment, sentence 
splitting, tokenization and sentence alignment. 
This section presents the two main steps: docu-
ment alignment and sentence alignment. We also 
discuss the proposed document alignment 
method. 
2.1 Document alignment 
Let S1 be set of documents in language L1; let S2 
be set of documents in language L2. Extracting 
parallel documents or aligning documents from 
the two sets S1, S2 can be seen as finding the 
translation document D2 (in the set S2) of a 
document D1 (in the set S1). We call this pair of 
documents D1-D2 a parallel document pair 
(PDP). 
For collecting bilingual text data for the two 
sets S1, S2, the Web is an ideal source as it is 
large, free and available (Kilgarriff and Grefen-
stette, 2003). For this kind of data, various meth-
ods to align documents have been proposed. 
Documents can be simply aligned based on the 
anchor link, the clue in URL (Kraaij et al, 2003) 
or the web page structure (Resnik and Smith, 
2003). However, this information is not always 
available or trustworthy. The titles of documents 
D1, D2 can also be used (Yang and Li, 2002), but 
sometimes they are completely different. 
Another useful source of information is invari-
ant words, such as named entities, dates, and 
numbers, which are often common in news data. 
We call these words special words. (Patry and 
Langlais, 2005) used numbers, punctuation, and 
entity names to measure the parallelism between 
two documents. The order of this information in 
document is used as an important criterion. How-
ever, this order is not always respected in a PDP 
(see an example in Table 1). 
 
French document Vietnamese document 
      Selon l'Administration 
nationale du tourisme, les 
voyageurs en provenance de 
l'Asie du Nord-Est (Japon, 
R?publique de Cor?e,...) 
repr?sentent 33%, de l'Eu-
rope, 16%, de l'Am?rique 
du Nord, 13%, d'Australie 
et de Nouvelle-Z?lande, 6%. 
     En outre, depuis le d?but 
de cette ann?e, environ 2,8 
millions de touristes ?tran-
gers ont fait le tour du Viet-
nam, 78% d'eux sont venus 
par avion.  
     Cela t?moigne d'un af-
flux des touristes riches au 
Vietnam.? 
     Trong s? g?n 2,8 tri?u 
l??t kh?ch qu?c t? ??n Vi?t 
Nam t? ??u n?m ??n nay, 
l??ng kh?ch ??n b?ng 
???ng h?ng kh?ng v?n 
chi?m ch? ??o v?i kho?ng 
78%.  
     ?i?u n?y cho th?y, d?ng 
kh?ch du l?ch ch?t l??ng 
cao ??n Vi?t Nam t?ng 
nhanh. 
     Theo th?ng k? th? kh?ch 
qu?c t? v?o Vi?t Nam cho 
th?y kh?ch ??ng B?c ? 
(Nh?t B?n, H?n Qu?c) 
chi?m t?i 33%, ch?u ?u 
chi?m 16%, B?c M? 13%, 
?xtr?ylia v? Niu Dil?n 
chi?m 6%.? 
Table 1. An example of a French-Vietnamese 
parallel document pair in our corpus. 
2.2 Sentence alignment 
From a PDP D1-D2, the sentence alignment 
process identifies parallel sentence pairs (PSPs) 
between two documents D1 and D2. For each 
D1-D2, we have a set SenAlignmentD1-D2 of 
PSPs. 
SenAlignmentD1-D2 = {?sen1-sen2?| sen1 is 
zero/one/many sentence(s) in document D1, 
sen2 is zero/one/many sentence(s) in docu-
ment D2, sen1-sen2 is considered as a 
PSP}. 
We call a PSP sen1-sen2 alignment type m:n 
when sen1 contains m consecutive sentences and 
sen2 contains n consecutive sentences. 
Several automatic sentence alignment ap-
proaches have been proposed based on sentence 
length (Brown et al, 1991) and lexical informa-
tion (Kay and Roscheisen, 1993). A hybrid ap-
proach is presented in (Gale and Church, 1993) 
whose basic hypothesis is that ?longer sentences 
in one language tend to be translated into longer 
sentences in the other language, and shorter sen-
tences tend to be translated into shorter sen-
tences?. Some toolkits such as Hunalign1 and 
Vanilla2 implement these approaches. However, 
they tend to work best when documents D1, D2 
contain few sentence deletions and insertions, 
and mainly contain PSPs of type 1:1. 
                                                          
1
 http://mokk.bme.hu/resources/hunalign 
2
 http://nl.ijs.si/telri/Vanilla/ 
166
Ma (2006) provides an open source software 
called Champollion1 to solve this limitation. 
Champollion permits alignment type m:n (m, n = 
0,1,2,3,4), so the length of sentence does not play 
an important role. Champollion uses also lexical 
information (lexemes, stop words, bilingual dic-
tionary, etc.) to align sentences. Champollion can 
easily be adapted to new pairs of languages. 
Available language pairs in Champollion are 
English-Arabic and English-Chinese (Ma, 2006).  
2.3 Our document alignment method 
Figure 1 describes our methodology for docu-
ment alignment. For each document D1 in the set 
S1, we find the aligned document D2 in the set 
S2.  
We propose to use publishing date, special 
words, and the results of sentence alignment to 
discover PDPs. First, the publishing date is used 
to reduce the number of possible documents D2. 
Then we use a filter based on special words con-
tained in the documents to determine the candi-
date documents D2. Finally, we eliminate 
candidates in D2 based on the combination of 
document length information and lexical infor-
mation, which are extracted from the results of 
sentence alignment. 
 
 
Figure 1. Our document alignment scheme. 
2.3.1 The first filter: publishing date 
We assume that the document D2 is translated 
and published at most n days after the publishing 
date of the original document. We do not know 
whether D1 or D2 is the original document, so 
                                                          
1
 http://champollion.sourceforge.net 
we assume that D2 is published n days before or 
after D1. After filtering by publishing date crite-
rion, we obtain a subset S2? containing possible 
documents D2. 
2.3.2 The second filter: special words  
In our case, the special words are numbers and 
named entities. Not only numbers (0-9) but also 
attached symbols (?$?, ?%?, ???, ?,?, ?.??) are 
extracted from documents, for example: 
?12.000$?; ?13,45?; ?50%?;? Named entities 
are specified by one or several words in which 
the first letter of each word is upper case, e.g. 
?Paris?, ?Nations Unies? in French.  
While named entities in language L1 are usu-
ally translated into the corresponding names in 
language L2, in some cases the named entities in 
L1 (such as personal names or organization 
names) do not change in L2. In particular, many 
Vietnamese personal names are translated into 
other languages by removal of diacritical marks 
(see examples in Table 2). 
 
 French Vietnamese Vietnamese 
-Removed 
diacritic 
Nations 
Unies 
Li?n H?p 
Qu?c 
Lien Hop 
Quoc 
Changed 
France Ph?p Phap 
ASEAN ASEAN ASEAN 
Nong Duc 
Manh 
N?ng ??c 
M?nh 
Nong Duc 
Manh 
Not 
changed 
Dien Bien ?i?n Bi?n Dien Bien 
Table 2. Some examples of named entities in 
French-Vietnamese. 
 
All special words are extracted from document 
D1. This gives a list of special words w1,w2,?wn. 
For each special word, we search in the set S2? 
documents D2 which contain this special word. 
For each word, we obtain a list of documents D2. 
The document D2 which has the biggest number 
of appearance in all lists is chosen. It is the 
document containing the highest number of spe-
cial words. We can find zero, one or several 
documents which are satisfactory. We call this 
set of documents set S2?? (see in Figure 2). 
The way that we use special words is different 
from the way used in (Patry and Langlais, 2005). 
We do not use punctuation as special words. We 
use the attached symbols (?$?, ?%?, ???, ?) with 
the number. Furthermore, in our method, the or-
der of special words in documents is not impor-
tant, and if a special word appears several times 
in a document, it does not affect the result. 
 
S1 S2 
Filter by publishing date  
(?n days) 
S2?
S2??
?S2?  D2 },ent{SenAlignm D2-D1 ?
Filter by special words 
 (numbers+ named entities) 
Align sentences 
Filter SenAlignment 
(use ?, ?) 
sen2}-{sen1D2}-{D1 +
D1 D2
167
  
Figure 2. Using special words to filter documents 
D2. 
2.3.3 The third filter: sentence alignments  
As mentioned in section 2.3.2, for each document 
D1, we discover a set S2??, which contains zero, 
one or several documents D2. When we continue 
to align sentences for each PDP D1-D2, we get a 
lot of low quality PSPs. The results of sentence 
alignment allow us to further filter the documents 
D2. 
After aligning sentences, we have a set of 
PSPs, SenAlignmentD1-D2, for each PDP D1-D2. 
We add two rules to filter documents D2.  
When D1-D2 is not a true PDP, it is hard to 
find out PSPs. So we note the number of PSPs in 
the set SenAlignmentD1-D2 by 
card(SenAlignmentD1-D2). The number of sentence 
pairs which can not find their alignment partner 
(when sen1 or sen2 is ?null?) is noted by 
nbr_omitted(SenAlignmentD1-D2).  
When ?>)ignmentcard(SenAl
)mentd(SenAlignnbr_omitte
D2-D1
D2-D1
 , this 
PDP D1-D2 will be eliminated. 
This first rule also deals with the problem of 
document length, sentence deletions and sentence 
insertions. 
The second rule makes use of lexical informa-
tion. For each PSP, we add two scores xL1 and xL2 
for sen1 and sen2.  
i
i
Li
seninwordsofnumber
seninwordstranslatedofnumber
x
????
?????
=
 
Translated words are words having translation 
equivalents in the other sentence. In this rule, we 
do not take into account the stop words. Table 3 
shows an example for calculating two scores xL1 
and xL2  for a PSP.  
In the second rule, when all PSPs in Se-
nAlignmentD1-D2 have two scores xL1 and xL2 that 
are both smaller than ?, this PDP D1-D2 will be 
eliminated. This rule removes the low quality 
PDP which creates a set of low quality PSPs. 
sen1 (in French) : ils ont ?chang? leurs opinions pour 
parvenir ? la signature de documents constituant la base 
du d?veloppement et de l' intensification de la coop?ra-
tion en ?conomie en commerce et en investissement ainsi 
que celles dans la culture le sport et le tourisme entre les 
deux pays 
sen2 (in Vietnamese) : hai b?n ?? ti?n_h?nh trao_??i ?? 
k?_k?t c?c v?n_b?n l?m c?_s? cho vi?c m?_r?ng v? 
t?ng_c??ng quan_h? h?p_t?c kinh_t? th??ng_m?i 
??u_t? v?n_ho? th?_thao v? du_l?ch gi?a hai n??c 
Translated words :  
??chan-
ger:trao_??i? ;?base:c?_s??,?intensification:t?ng_c??n
g? ;?coop?ration:h?p_t?c?,??conomie:kinh_t?? ; inves-
tissement:??u_t??,?sport:th?_thao? ; ?tou-
risme :du_l?ch? ; ?pays:n??c? 
Number of non-stop words in sen1 19  
Number of non-stop words in sen2 21 
Number of translated words 9 
xL1 = 9/19=0.47 ; xL2 = 9/21=0.43 
Table 3. Example for calculating two scores xL1 
and xL2. 
 
After using three filters based on information 
of publishing date, special words, and the results 
of sentence alignment, we have a corpus of 
PDPs, and also a corpus of corresponding PSPs. 
To ensure the quality of output PSPs, we can 
continue to filter PSPs. For example, we can keep 
only the PSPs whose scores (xL1 and xL2) are 
higher than a threshold. 
3 Experiments 
3.1 Characteristics of Vietnamese 
The basic unit of the Vietnamese language is syl-
lable. In writing, syllables are separated by a 
white space. One word corresponds to one or 
more syllables (Nguyen, 2006). Table 4 presents 
an example of a Vietnamese sentence segmented 
into syllables and words.  
 
Vietnamese sentence: Th?nh ph? hy v?ng s? ??n nh?n 
kho?ng 3 tri?u kh?ch du l?ch n??c ngo?i trong n?m nay 
Segmentation in syllables: Th?nh | ph? | hy | v?ng | s? | 
??n | nh?n | kho?ng | 3 | tri?u | kh?ch | du | l?ch | n??c | 
ngo?i | trong | n?m | nay 
Segmentation in words: Th?nh_ph? | hy_v?ng | s? | 
??n_nh?n | kho?ng | 3 | tri?u | kh?ch_du_l?ch | 
n??c_ngo?i | trong | n?m | nay 
Corresponding English sentence: The city is expected to 
receive 3 million foreign tourists this year 
Table 4. An example of a Vietnamese sentence 
segmented into syllables and words. 
 
In Vietnamese, words do not change their 
form. Instead of conjugation for verb, noun or 
adjective, Vietnamese language uses additional 
words, such as ?nh?ng?, ?c?c? to express the plu-
D1 
Extract  
special words 
w1?wn  
 find w1 in S2?   doc1, doc3, doc5 
 find w2 in S2?   doc3, doc4, doc5 
 find w3 in S2?   doc3, doc5 
 ?                     ? 
doc1: 1 time 
doc3: 3 times 
doc4: 1 time 
doc5: 3 times 
 Count 
Choose 
the max 
S2?? 
{doc3, 
doc5} 
168
ral; ????, ?s?? to express the past tense and the 
future. The syntactic functions are also deter-
mined by the order of words in the sentence 
(Nguyen, 2006).  
3.2 Data collecting  
In order to build a Vietnamese-French parallel 
text corpus, we applied our proposed methodol-
ogy to mine a comparable text corpus from a 
Vietnamese daily news website, the Vietnam 
News Agency1 (VNA). This website contains 
news articles written in four languages (Vietnam-
ese, English, French, and Spanish) and divided in 
9 categories including ?Politics - Diplomacy?, 
?Society - Education?, ?Business - Finance?, 
?Culture - Sports?, ?Science - Technology?, 
?Health?, ?Environment?, ?Asian corner? and 
?World?. However, not all of the Vietnamese 
articles have been translated into the other three 
languages. The distribution of the amount of data 
in four languages is shown in figure 3. 
 
 
Figure 3. Distribution of the amount of data for 
each language on VNA website. 
 
Each document (i.e., article) can be obtained 
via a permanent URL link from VNA. To date, 
we have obtained about 121,000 documents in 
four languages, which are gathered from 12 April 
2006 to 14 August 2008; each document con-
tains, on average, 10 sentences, with around 30 
words per sentence. 
3.3 Data pre-processing 
We splitted the collected data into 2 sets. The 
development set, designated SDEV, contained 
1000 documents, was used to tune the mining 
system parameters. The rest of data, designated 
STRAIN, was used as a training set, where the esti-
mated parameters were applied to build the entire 
corpus. We applied the following pre-process to 
each set SDEV and STRAIN: 
1. Extract contents from documents. 
                                                          
1
 http://www.vnagency.com.vn/ 
2. Classify documents by language (using 
TextCat2, an n-gram based language identi-
fication). 
3. Process and clean both Vietnamese and 
French documents by using the CLIPS-Text-
Tk toolkit (LE et al, 2003): convert html to 
text file, convert character code, segment 
sentence, segment word. The resulting clean 
corpora are S1 (for French) and S2 (for 
Vietnamese). 
3.4  Parameters estimation 
Our proposed document alignment method was 
applied to the sets S1 and S2 extracted from the 
set SDEV. To filter by publishing date, we as-
sumed that n=2.  
The second filter was implemented on the set 
S1 and the new set S2* which was created by re-
moving diacritical marks from the set S2 (in the 
case of Vietnamese).  
The sentence alignment process was imple-
mented by using data from sets S1, S2 and the 
Champollion toolkit. We adapted Champollion to 
Vietnamese-French by changing some parame-
ters: the ratio of French word to Vietnamese 
translation word is set to 1.2, penalty for align-
ment type 1-1 is set to 1, for type 0-1 to 0.8, for 
type 2-1, 1-2 and 2-2 to 0.75, and we did not use 
the other types (see more in (Ma, 2006)). After 
using two filters, the result data is shown in Table 
5. The true PDPs were manually extracted. 
 
SDEV - Number of documents: 1000 
- Number of French documents: 173 
- Number of Vietnamese documents: 348 
- Number of true PDPs: 129 
S2?? - Number of found PDPs: 379 
- Number of hits PDPs: 129 
- Precision = 34.04% , Recall = 100% 
Table 5. Result data after using two filters. 
  
The third filter was applied in which ? was set 
to (0.4, 0.5, 0.6, 0.7) and ? was set to (0.1, 0.15, 
0.2, 0.25, 0.3, 0.35, 0.4). The precision and recall 
were calculated according to our true PDPs and 
the F-measure (F1 score) was estimated. 
F-measure 
    ?   
?  0.1 0.15 0.2 0.25 0.3 0.35 0.4 
0.4 0.69 0.71 0.71 0.60 0.48 0.36 0.21 
0.5 0.76 0.79 0.77 0.65 0.52 0.39 0.23 
0.6 0.77 0.83 0.82 0.70 0.56 0.41 0.26 
0.7 0.75 0.84 0.83 0.73 0.59 0.44 0.27 
Table 6. Filter result with different values of ? 
and ? on the SDEV. 
                                                          
2
 http://www.let.rug.nl/~vannoord/TextCat/ 
169
From the results mentioned in Table 6, we 
chose ?=0.7 and ?=0.15. 
3.5 Mining the entire corpus 
We applied the same methodology with the pa-
rameters estimated in section 3.4 to the set 
STRAIN. The obtained corpus is presented in Table 
7.  
 
STRAIN - Number of documents: 120,218 
- Number of French documents: 20,884 
- Number of Vietnamese documents: 
54,406 
Entire 
corpus 
- Number of PDPs: 12,108 
- Number of PSPs: 50,322 
Table 7. The obtained corpus from STRAIN. 
4 Application: a Vietnamese - French 
statistical machine translation system  
With the obtained parallel corpus, we attempted 
to rapidly build a SMT system for Vietnamese-
French. The system was built using the Moses 
toolkit1. The Moses toolkit contains all of the 
components needed to train both the translation 
model and the language model. It also contains 
tools for tuning these models using minimum 
error rate training and for evaluating the transla-
tion result using the BLEU score (Koehn et al, 
2007). 
4.1 Preparing data 
From the entire corpus, we chose 50 PDPs (351 
PSPs) for developing (Dev), 50 PDPs (384 PSPs) 
for testing (Tst), with the rest PDPs (49,587 
PSPs) reserved for training (Trn).  
Concerning the developing and testing PSPs, 
we manually verified and eliminated low quality 
PSPs, which produced 198 good quality PSPs for 
developing and 210 good quality PSPs for test-
ing. The data used to create the language model 
were extracted from 49,587 PSPs of the training 
set. 
4.2 Baseline system  
We built translation systems in two translation 
directions: French to Vietnamese (FV) and 
Vietnamese to French (VF). The Vietnamese 
data were segmented into either words or sylla-
bles. So we first have four translation systems. 
We removed sentences longer than 100 
words/syllables from the training and develop-
                                                          
1
 http://www.statmt.org/moses/ 
ment sets according to the Moses condition (so 
the number of PSPs used in the training set dif-
fers slightly between systems). All words found 
are implicitly added to the vocabulary. 
 
System Direction Vietnamese is 
segmented into Nbr of PSPs 
S1FV FV 
S1VF VF 
Syllable 
Training: 47,081 
Developing: 198 
Testing:        210 
S2FV FV 
S2VF VF 
Word 
Training: 48,864 
Developing: 198 
Testing:        210 
 
System Set - 
Language 
Nbr. of vocab  
(K) 
Nbr. of running 
words/syllables 
(K) 
Fr 38.6 1783.6 Trn Vn 21.9 2190.2 
Fr 1.8 6.3 Dev Vn 1.2 6.9 
Fr 1.9 6.4 
S1FV 
S1VF 
 
Tst Vn 1.3 7.1 
Fr 39.7 1893 Trn Vn 33.4 1629 
Fr 1.8 6.3 Dev Vn 1.5 4.8 
Fr 1.9 6.3 
S2FV 
S2VF 
 
Tst Vn 1.6 4.9 
Table 8. Our four translation systems. 
 
We obtained the performance results for those 
systems in Table 9. In the case of the systems 
where Vietnamese was segmented into words, 
the Vietnamese sentences were changed back to 
syllable representation before calculating the 
BLEU scores, so that all the BLEU scores evalu-
ated can be compared to each other. 
 
 S1FV S1VF S2FV S2VF 
BLEU  0.40 0.31 0.40 0.30 
Table 9. Evaluation of SMTs on the Tst set. 
 
The BLEU scores for French to Vietnamese 
translation direction are around 0.40 and the 
BLEU scores for Vietnamese to French transla-
tion direction are around 0.31, which is encour-
aging as a first result. Moreover, only one 
reference was used to estimate BLEU scores in 
our experiments. It is also interesting to note that 
segmenting Vietnamese sentences into words or 
syllables does not significantly change the per-
formance for both translation directions. An ex-
ample of translation from four systems is 
presented in Table 10.  
 
 
 
170
Given a pair of parallel sentences 
FR: selon le d?partement de gestion des travailleurs 
? l' ?tranger le qatar est un march? prometteur et 
n?cessite une grande quantit? de travailleurs ?tran-
gers 
VNsyl : theo c?c qu?n l? lao ??ng ngo?i n??c cata 
l? th? tr??ng ??y ti?m n?ng v? c? nhu c?u l?n lao 
??ng n??c ngo?i 
VNword : theo c?c qu?n_l? lao_??ng ngo?i n??c 
cata l? th?_tr??ng ??y ti?m_n?ng v? c? nhu_c?u l?n 
lao_??ng n??c_ngo?i 
S1FV Input: FR              Reference: VNsyl 
Output: theo c?c qu?n l? lao ??ng ? n??c 
ngo?i ph?a cata l? m?t th? tr??ng ??y ti?m 
n?ng v? c?n m?t l??ng l?n lao ??ng n??c 
ngo?i 
S2FR Input: FR              Reference: VNword 
Output: theo th?ng_k? c?a c?c qu?n_l? 
lao_??ng ngo?i n??c cata l? m?t 
th?_tr??ng ??y ti?m_n?ng v? c?n c? s? l?n 
l??ng lao_??ng n??c_ngo?i 
S1VF Input: VNsyl         Reference: FR 
Output: selon le d?partement de gestion 
des travailleurs ?trangers cata ?tait un mar-
ch? plein de potentialit?s et aux besoins 
importants travailleurs ?trangers 
S2VF Input: VNword      Reference: FR 
Output : selon le d?partement de gestion 
des travailleurs ?trangers cata march? plein 
de potentialit?s et la grande travailleurs 
?trangers 
Table 10 : Example of translation from systems. 
4.3 Combining word- and syllable-based 
systems 
We performed another experiment on combining 
syllable and word units on the Vietnamese side. 
We carried out the experiment on the Vietnamese 
to French translation direction only. In fact, the 
Moses toolkit supports the combination of 
phrase-tables. The phrase-tables of the system 
S1VF (Tsyl) and system S2VF (Tword) were used. 
Another phrase-table (Tword*) was created from 
the Tword, in which all words in the phrase table 
were changed back into syllable representation 
(in this latter case, the word segmentation infor-
mation was used during the alignment process 
and the phrase table construction, while the unit 
kept at the end remains the syllable). The combi-
nations of these three phrase-tables were also 
created (by simple concatenation of the phrase 
tables). The Vietnamese input for this experiment 
was either in word or in syllable representation. 
As usual, the developing set was used for tuning 
the log-linear weights and the testing set was 
used to estimate the BLEU score. The obtained 
results are presented in Table 11. Some perform-
ances are marked as X since those combinations 
of input and phrase table do not make sense (for 
instance the combination of input in words and 
syllable-based phrase table). 
 
Input in syllable Input in word Phrase-tables 
used Dev Tst Dev Tst 
Tsyl 0.35 0.31 X X 
Tword X X 0.35 0.30 
Tword* 0.37 0.31 X X 
Tsyl + Tword 0.35 0.31 0.36 0.30 
Tsyl + Tword* 0.38 0.32 X X 
Tword + Tword* 0.37 0.30 0.36 0.30 
Table 11: The BLEU scores obtained from com-
bination of phrase-tables on Dev set and Tst set 
(Vietnamese to French machine translation). 
 
These results show that the performance can 
be improved by combining information from 
word and syllable representations of Vietnamese. 
(BLEU improvement from 0.35 to 0.38 on the 
Dev set and from 0.31 to 0.32 on the Tst set). In 
the future, we will analyze more the combination 
of syllable and word units for Vietnamese MT 
and we will investigate the use of confusion net-
works as an MT input, which have the advantage 
to keep both segmentations (word, syllable) into 
a same structure. 
4.4 Comparing with Google Translate1 
Google Translate system has recently supported 
Vietnamese. In most cases, it uses English as an 
intermediary language. For the first comparative 
evaluation, some simple tests were carried out. 
Two sets of data were used: in domain data set 
(the Tst set in section 4.2) and out of domain data 
set. The latter was obtained from a Vietnamese-
French bilingual website2 which is not a news 
website. After pre-processing and aligning manu-
ally, we obtained 100 PSPs in the out of domain 
data set. In these tests, the Vietnamese data were 
segmented into syllables. Both data sets were 
inputted to our translation systems (S1FV, S1VF) 
and the Google Translate system. The outputs of 
Google Translate system were post-processed 
(lowercased) and then the BLEU scores were 
estimated. Table 12 presents the results of these 
tests. While our system is logically better for in 
domain data set, it is also slightly better than 
Google for out of domain data set.  
 
                                                          
1
 http://translate.google.com 
2
 http://www.ambafrance-vn.org 
171
BLEU score  Direction Our system Google 
FV 0.40  0.25 In domain 
(210 PSPs) VF 0.31  0.16 
FV 0.25 0.24 Out of domain 
(100 PSPs) VF 0.20 0.16 
Table 12: Comparing with Google Translate. 
5 Conclusions and perspectives 
In this paper, we have presented our work on 
mining a comparable Vietnamese-French corpus 
and our first attempts at Vietnamese-French 
SMT. The paper has presented our document 
alignment method, which is based on publication 
date, special words and sentence alignment re-
sult. The proposed method is applied to Vietnam-
ese and French news data collected from VNA. 
For Vietnamese and French data, we obtained 
around 12,100 parallel document pairs and 
50,300 parallel sentence pairs. This is our first 
Vietnamese-French parallel bilingual corpus. We 
have built SMT systems using Moses. The BLEU 
scores for French to Vietnamese translation sys-
tems and Vietnamese to French translation sys-
tems were 0.40 and 0.31 in turn. Moreover, 
combining information from word and syllable 
representations of Vietnamese can be useful to 
improve the performance of Vietnamese MT sys-
tem.  
In the future, we will attempt to increase the 
corpus size (by using unsupervised SMT for in-
stance) and investigate further the use of different 
Vietnamese lexical units (syllable, word) in a MT 
system.  
References  
Brown, Peter F., Jennifer C. Lai and Robert L. Mer-
cer. 1991. Aligning sentences in parallel corpora. 
Proceedings of 47th Annual Meeting of the Asso-
ciation for Computational Linguistics. 
Brown, Peter F., Stephen A. Della Pietra, Vincent J. 
Della Pietra and Robert L. Mercer. 1993. The 
Mathematics of Statistical Machine Translation: 
Parameter Estimation. Computational Linguistics. 
Vol. 19, no. 2. 
Doan, Nguyen Hai. 2001. Generation of Vietnamese 
for French-Vietnamese and English-Vietnamese 
Machine Translation. ACL, Proceedings of the 8th 
European workshop on Natural Language Genera-
tion. 
Gale, William A. and Kenneth W. Church. 1993. A 
program for aligning sentences in bilingual cor-
pora. Proceedings of the 29th annual meeting on 
Association for Computational Linguistics. 
Ho, Tu Bao. 2005. Current Status of Machine Trans-
lation Research in Vietnam Towards Asian wide 
multi language machine translation project. Viet-
namese Language and Speech Processing Work-
shop. 
Hutchins, W.John. 2001. Machine translation over 
fifty years. Histoire, epistemologie, langage: HEL, 
ISSN 0750-8069, Vol. 23, N? 1, 2001 , pages. 7-32.  
Kay, Martin and Martin Roscheisen. 1993. Text - 
translation alignment. Association for Computa-
tional Linguistics. 
Kilgarriff, Adam and Gregory Grefenstette. 2003. 
Introduction to the Special Issue on the Web as 
Corpus. Computational Linguistics, volume 29. 
Koehn, Philipp, Franz Josef Och and Daniel Marcu. 
2003. Statistical phrase-based translation. Confer-
ence of the North American Chapter of the Asso-
ciation for Computational Linguistics on Human 
Language Technology - Volume 1. 
Koehn, Philipp. 2005. Europarl: A Parallel Corpus 
for Statistical Machine Translation. Machine 
Translation Summit.  
Koehn, Philipp, Hieu Hoang, Alexandra Birch, Chris 
Callison-Burch, Richard Zens, Marcello Federico, 
Nicola Bertoldi, Brooke Cowan, Wade Shen and 
Christine Moran. 2007. Moses: Open Source Tool-
kit for Statistical Machine Translation. Proceedings 
of the ACL.  
Kraaij, Wessel, Jian-Yun Nie and Michel Simard. 
2003. Embedding web-based statistical translation 
models in cross-language information retrieval. 
Computational Linguistics,  Volume 29 ,  Issue 3. 
LE, Viet Bac, Brigitte Bigi, Laurent Besacier and Eric 
Castelli. 2003. Using the Web for fast language 
model construction in minority languages. Eu-
rospeech'03. 
Ma, Xiaoyi. 2006. Champollion: A Robust Parallel 
Text Sentence Aligner. LREC: Fifth International 
Conference on Language Resources and Evalua-
tion.  
Munteanu, Dragos Stefan and Daniel Marcu. 2006. 
Extracting parallel sub-sentential fragments from 
non-parallel corpora . 44th annual meeting of the 
Association for Computational Linguistics 
Nguyen, Thi Minh Huyen. 2006. Outils et ressources 
linguistiques pour l'alignement de textes multilin-
gues fran?ais-vietnamiens. Th?se pr?sent?e pour 
l?obtention du titre de Docteur de l?Universit? Hen-
ri Poincar?, Nancy 1 en Informatique.  
Patry, Alexandre and Philippe Langlais. 2005. Para-
docs: un syst?me d?identification automatique de 
documents parall?les. 12e Conference sur le Trai-
tement Automatique des Langues Naturelles. 
Dourdan, France.  
Resnik, Philip and Noah A. Smith. 2003. The Web as 
a Parallel Corpus. Computational Linguistics.  
Yang, Christopher C. and Kar Wing Li. 2002. Mining 
English/Chinese Parallel Documents from the 
World Wide Web. Proceedings of the 11th Interna-
tional World Wide Web Conference, Honolulu, 
USA.  
172
Proceedings of the 2009 Named Entities Workshop, ACL-IJCNLP 2009, pages 177?185,
Suntec, Singapore, 7 August 2009. c?2009 ACL and AFNLP
A Hybrid Model for Urdu Hindi Transliteration 
 
 
Abbas Malik Laurent Besacier Christian Boitet 
GETALP, Laboratoire d?Informatique Grenoble (LIG) 
Universit? Joseph Fourier 
Abbas.Malik, Laurent.Besacier, 
Christian.Boitet@imag.fr 
Pushpak Bhattacharyya 
IIT Bombay 
 
pb@cse.iitb.ac.in 
 
  
 
Abstract 
We report in this paper a novel hybrid ap-
proach for Urdu to Hindi transliteration that 
combines finite-state machine (FSM) based 
techniques with statistical word language 
model based approach. The output from the 
FSM is filtered with the word language model 
to produce the correct Hindi output. The main 
problem handled is the case of omission of di-
acritical marks from the input Urdu text. Our 
system produces the correct Hindi output even 
when the crucial information in the form of di-
acritic marks is absent. The approach improves 
the accuracy of the transducer-only approach 
from 50.7% to 79.1%. The results reported 
show that performance can be improved using 
a word language model to disambiguate the 
output produced by the transducer-only ap-
proach, especially when diacritic marks are not 
present in the Urdu input. 
1 Introduction 
Transliteration is a process to transcribe a word 
written in one language, in another language by 
preserving its articulation. It is crucial for han-
dling out-of-vocabulary (OOV) words in differ-
ent domains of Natural Language Processing 
(NLP), especially in Machine Translation 
(Knight and Graehl, 1998; Knight and Stall, 
1998; Paola and Sanjeev, 2003), Cross-Lingual 
Information Retrieval (Pirkola et al, 2003), the 
development of multi-lingual resources (Yan et 
al., 2003) and multi-lingual text and speech 
processing. It is also useful for Inter-dialectal 
translation without lexical changes and some-
times it is mandatory when the dialects in ques-
tion use mutually incomprehensible writing sys-
tems. Such cases exists in Malay (written in 2 
different scripts), Turkish (2 scripts), Kurdish (3 
scripts), Hindi/Urdu (2 scripts), Punjabi (2 
scripts), etc., where words are transliterated from 
one script to the other, irrespective of their type 
(noun, verb, etc., and not only proper nouns and 
unknown words). In this study, we will focus on 
Hindi/Urdu example. 
Hindi and Urdu are written in two mutually 
incomprehensible scripts, Devanagari and Urdu 
script ? a derivative of Persio-Arabic script re-
spectively. Hindi and Urdu are the official lan-
guages of India and the later is also the National 
language of Pakistan (Rahman, 2004). Table 1 
gives an idea about the number of speakers of 
Hindi and Urdu. 
 
 Native Speaker
2nd Lang. 
Speaker Total 
Hindi 366 487 853 
Urdu 60.29 104 164.29 
Total 426.29 591 1,017.29 
Source: (Grimes, 2000) all numbers are in millions 
Table 1: Hindi and Urdu Speakers 
Notwithstanding the transcriptional differences, 
Hindi and Urdu share phonology, grammar, 
morphology, literature, cultural heritage, etc. 
People from Hindi and Urdu communities can 
understand the verbal expressions of each other 
but the written expression of one community is 
alien to the other community. 
A finite-state transliteration model for Hindi 
and Urdu transliteration using the Universal In-
termediate Transcription (UIT ? a pivot between 
the two scripts) was proposed by Malik et al 
(2008). The non-probabilistic finite-state model 
is not powerful enough to solve all problems of 
Hindi ? Urdu transliteration. We visit and ana-
lyze Hindi ? Urdu transliteration problems in 
the next section and show that the solution of 
these problems is beyond the scope of a non-
probabilistic finite-state transliteration model. 
177
Following this, we show how a statistical model 
can be used to solve some of these problems, 
thereby enhancing the capabilities of the finite-
state model. 
Thus, we propose a hybrid transliteration 
model by combining the finite-state model and 
the statistical word language model for solving 
Hindi ? Urdu transliteration problems, dis-
cussed in section 2. Section 3 will throw light on 
the proposed model, its different components and 
various steps involved in its construction. In sec-
tion 4, we will report and various aspects of dif-
ferent experiments and their results. Finally, we 
will conclude this study in section 5. 
2 Hindi Urdu Transliteration 
In this section, we will analyze Hindi ? Urdu 
transliteration problems and will concentrate on 
Urdu to Hindi transliteration only due to shortage 
of space and will discuss the reverse translitera-
tion later. Thus, the remainder of the section ana-
lyzes the problems from Urdu to Hindi translite-
ration. 
2.1 Vowel, Yeh (?) and Waw (?) 
Urdu is written in a derivation of Persio-Arabic 
script. Urdu vowels are represented with the help 
of four long vowels Alef-madda (?), Alef (?), 
Waw (?), Yeh (?) and diacritical marks. One 
vowel can be represented in many ways depend-
ing upon its context or on the origin of the word, 
e.g. the vowel [?] is represented by Alef-madda 
(?) at the beginning of a word, by Alef (?) in the 
middle of a word and in some Persio-Arabic loan 
word, it is represented by the diacritical mark 
Khari Zabar (G?). Thus Urdu has very complex 
vowel system, for more details see Malik et al 
(2008). Urdu contains 10 vowels, and 7 of them 
also have their nasalization forms (Hussain, 
2004; Khan, 1997) and 15 diacritical marks. 
Thou diacritical marks form the cornerstone of 
the Urdu vowel system, but are sparingly used 
(Zia, 1999). They are vital for the correct Urdu to 
Hindi transliteration using the finite-state transli-
teration model. The accuracy of the finite-state 
transliteration model decreases from above 80% 
to 50% in the absence of diacritical marks. Fig-
ure 1 shows two example Urdu phrases (i) with 
and (ii) without the diacritical marks and their 
Hindi transliteration using the finite-state transli-
teration model. Due to the absence of Zabar (F?) 
in the first and the last words in (1)(ii) and in the 
5th word in (2)(ii), vowels ? ? [?] and ? [?] are 
transliterated into vowels ?? [e] and ? [o] re-
spectively. Similarly, due to the absence of Pesh 
( E?) and Zer (G?) in 3rd and 4th words respectively 
in (1)(ii), both vowels ? ? [?] and ?? [?] are con-
verted into the vowel [?]. All wrongly converted 
words are underlined. 
 
(1)  (i) ??? ???? ?? ???? ????? ??? ????? ???? 
 (ii) ?? ??? ???? ??? ??? ?? ??? ???? 
(i) ??? ?? ???? ???? ??? ???? ???? ?? 
?? ??? ???? ??? ??? ??? ?? ??? (ii) 
I have not done a lot of work 
(2) (i) ????? ?? ?? ?????? ???? ??? ?? ???? ???????????  
  (ii) ????? ??? ?? ??? ??????? ??? ?? ??? ???  
(i) ??????? ??? ?? ?? ?? ????? ??? ?? ?? 
?? ?? ??? ???? ?? ?? ?? ??? ??? ??? (ii) 
Both at the central level and at the state level 
Figure 1: Example Urdu Phrases 
In Hindi, each vowel is represented by a cha-
racter and a vowel sign except the vowel [?], 
which is only represented by the character ? and 
do not have a vowel sign (Malik et al, 2008). 
Table 2 gives all vowel conversion problems. 
 
Sr. IPA 
Vowel 
Conversion 
Problems 
Hindi 
1 ? ? ? ? ? or ?? ? ? or 0* 
2 ? ? ? ? ? or ?? ? ? or 0* 
3 i i ? e ? or ?? ? ? or ?? 
4 ? ? ? e ? or ?? ? ? or ?? 
5 u u ? o ? or ?? ? ? or ?? 
6 ? ? ? o ? or ?? ? ? or ??
7 j j ? e ? ? ?? 
8 v v ? o ? ? ?? 
* Zero (0) means deleted. 
Table 2: Vowel Problems from Urdu to Hindi 
Long vowels Yeh (?) [j] and Waw (?) [v] are 
also used as consonants and certain contextual 
rules help us to decide whether they are used as a 
consonant or as a vowel, e.g., Yeh (?) and Waw 
(?) are used as consonants at the start of a word 
and after the long vowel Alef-madda (?), etc. Fi-
178
nite-state transliteration model can exploit such 
contextual rules but it is not possible to decide 
Yeh (?) and Waw (?) as consonants in the ab-
sence of diacritics. Thus a finite-state translitera-
tion model wrongly converts consonant Yeh (?) 
and Waw (?) into vowels ?? [e] and ?? [o], also 
given in Table 2, instead of consonants Ya (?) 
and Wa (?) respectively, e.g., in the word ????? 
(prince) [k??vr], Waw is wrongly converted into 
the vowel [o] due to the absence of Zabar ( F?) 
after it and the word becomes [k?nor], which is 
not a valid word of Hindi/Urdu. 
2.2 Native Sounds 
The Hindi writing system contains some native 
sounds/characters, e.g., vocalic R (?) [r?], retrof-
lex form of Na (?) [?], etc. On the other hand 
Urdu does not have their equivalents. Thus 
words containing such sounds are transcribed in 
Urdu with their approximate phonetic equiva-
lents. All such cases are problematic for Urdu to 
Hindi transliteration and are given in Table 3. 
 
Sr. IPA Hindi Urdu 
1 r? ? or ?? ? [r] 
2 ? ? ? [n] 
3 ? ? ? [?] 
4 Half h ?? ? [h] 
Table 3: Sounds of Sanskrit Origin 
2.3 Conjunct Form 
The Hindi alphabet is partly syllabic because 
each consonant inherits the vowel [?]. Two or 
more consonants may be combined together to 
form a cluster called Conjunct that marks the 
absence of the inherited vowel [?] between con-
sonants (Kellogg, 1872; Montaut, 2004). Con-
junction is also used to represent the gemination 
of a consonant, e.g., ?[k]+??+?[k]=???[kk] 
where ?? is the conjunct marker and aspiration of 
some consonants like ? [n], ? [m], ? [r] and ? 
[l] when used as conjunction with ? [h], e.g., 
?[n] + ?? + ?[h] = ???[nh]. Conjunction has a spe-
cial meaning but native speakers use conjunct 
forms without any explicit rule (Montaut, 2004). 
On the other hand, Urdu uses Jazam ( H? ? a 
diacritic) and Shadda (H?) to mark the absence of 
a vowel between two consonants and gemination 
of a consonant respectively. In the absence of 
these diacritics in the input Urdu text, it is not 
possible to decide on the conjunct form of con-
sonants except in the case of aspiration. In Urdu, 
aspiration of a consonant is marked with the spe-
cial character Heh-Doachashmee (?) (Malik et 
al., 2008), thus a finite-state transducer can easi-
ly decide about the conjunction for aspiration 
with a simple contextual rule, e.g. the word ????? 
(bride) [d??lhn] is correctly transliterated by our 
finite-state transliteration model into ??????. 
2.4 Native Hindi Spellings and Sanskritized 
Vocabulary 
Sanskrit highly influences Hindi and especially 
its vocabulary. In some words of Sanskrit origin, 
the vowel ?? [i] and ?? [u] are transcribed as ?? 
[?] and ?? [?] respectively at the end of a word. 
Javaid and Ahmed (2009) have pointed to this 
issue in these words ?Hindi language can have 
words that end on short vowel??. Table 4 gives 
some examples of such native words. On the 
other hand in Urdu, short vowels can never come 
at the end of a word (Javaid and Ahmed, 2009; 
Malik et al, 2008). 
 
Vowel Examples 
?? [i] 
??????? ? ????? (person) [vj?kti] 
??????? ? ??????? (culture) [s??skr?t?i] 
???????? ? ??????? (high) [???ko?i] 
?? [u] 
???? ? ????? (for) [het?u] 
????? ?? ????? (but) [k?nt?u] 
???? ? ?????? (metal) [d??t?u] 
Table 4: Hindi Word with Short vowel at End 
It is clear from above examples that short vowels 
at the end of a Hindi word can easily be translite-
rated in Urdu using a contextual rule of a finite-
state transducer, but it is not possible to do so for 
Urdu to Hindi transliteration using a non-
probabilistic finite-state transliteration model. 
Thus Urdu to Hindi transliteration can also be 
179
considered as a special case of Back Translitera-
tion. 
In some words, the vowel ?? [u] is written as 
the vowel ?? [?], e.g., ??? ? ????? or ??? ? ???? (to be) 
[hue], ??????? (name of a city) [r???npur]. 
Some of these cases are regular and can be im-
plemented as contextual rules in a finite-state 
transducer but it is not possible in every case. 
2.5 Ain (?) 
Ain (? ? glottal stop) exists in the Arabic alpha-
bet and native Arabic speakers pronounce it 
properly. Urdu also has adopted Ain (?) in its 
alphabet as well as Arabic loan words but native 
speakers of the sub-continent cannot produce its 
sound properly, rather they produce a vowel 
sound by replacing Ain (?) with Alef (?). The 
Hindi alphabet follows one character for one 
sound rule and it does not have any equivalent of 
Ain (?). Then, Ain (?) in Urdu words is tran-
scribed in Hindi by some vowel representing the 
pronunciation of the word by native sub-
continent speakers. Thus it is always translite-
rated in some vowel in Hindi. For example, Ain 
(?) gives the sound of the vowel [?] in ????  ? 
???? (strange) [??ib] and the vowel [?] with 
and without Alef (?) in words ???  ? ?? (com-
mon) [?m] and ???  ? ??? (after) [b?d?] respective-
ly. In some words, Ain (?) is not pronounced at 
all and should be deleted while transliterating 
from Urdu to Hindi, e.g., ??????  ? ???? (to start) 
[??ru], etc. Conversion of Ain (?) is a big prob-
lem for transliteration. 
2.6 Nasalization 
Noonghunna (?) [?] is the nasalization marker of 
vowels in Urdu. Interestingly, it is only used to 
nasalize a vowel at the end of a word. In the 
middle of a word, Noon (?) [n] is used to mark 
the nasalization of a vowel and it is also used as 
a consonant. It is difficult to differentiate be-
tween nasalized and consonant Noon (?). There 
are certain contextual rules that help to decide 
that Noon (?) is used as a consonant or a nasali-
zation marker, but it not possible in all cases. 
2.7 Persio-Arabic Vocabulary 
Urdu borrows a considerable portion of it voca-
bulary from Persian and Arabic and translitera-
tion of these words in Hindi is not regular. Table 
5 explains it with few examples. 
 
Urdu 
Hindi 
FST Conversion Correct 
??????  
?????? 
(surely) 
?????? 
[b?lk?l] 
???????? 
???????? 
(with reference of) 
???????? 
[b?lv?st??] 
?? ????????
????????? 
(in fact) 
???????? 
[f?lh?qiq?t]
Table 5: Persio-Arabic Vocabulary in Urdu 
3 Hybrid Transliteration Model 
The analysis of the previous section clearly 
shows that solution of these problems is beyond 
the scope of the non-probabilistic Hindi Urdu 
Finite-state transliteration model (Malik et al, 
2008). We propose a hybrid transliteration model 
that takes the input Urdu text and converts it in 
Hindi using the Finite-state Transliteration Mod-
el (Malik et al 2008). After that, it tries to cor-
rect the orthographic errors in the transducer-
only Hindi output string using a statistical word 
language model for Hindi with the help of a 
Hindi Word Map described later. The approach 
used is rather similar to what is done in text re-
capitalization (Stolcke et al 1998) for instance. 
 
Figure 2: Hybrid Transliteration Model for Urdu 
Hindi 
Normally, the Urdu text does not contain neces-
sary diacritical marks that are mandatory for the 
correct transliteration by the finite-state compo-
nent Urdu Hindi Transliteration 
180
Finite-state Machine (UHT-FSM), 
described by Malik et al (2008). The proposed 
hybrid model focuses on the correct translitera-
tion of Urdu texts without diacritical marks. Fig-
ure 2 gives the proposed Model architecture. 
3.1 Preprocessing UHT-FSM Output 
The goal of this pre-processing is to generate a 
more ?normalized? (and consequently more am-
biguous) form of Hindi, e.g. pre-processing 
transforms both corpus words ?? (this) [?s] and 
?? (that) [?s] (if encountered in the UHT-FSM 
Hindi output) into the default input Hindi word 
??* [?s] (not a valid Hindi word but is a finite-
state transliteration of the input Urdu word ??, a 
word without diacritical marks). Thus pre-
processing is vital for establishing connections 
between the UHT-FSM Hindi output words 
(from the Urdu input without diacritical marks) 
and the Hindi corpus words. In the example 
above, the word ??* [?s] is aligned to two Hin-
di corpus words. All such alignments are record-
ed in the Hindi Word Map. This ambiguity will 
be solved by the Hindi word language 
model, trained on a large amount of Hindi data. 
Thus pre-processing is a process that establishes 
connections between the most likely expected 
input Hindi word forms (UHT-FSM Hindi output 
from the Urdu input without diacritical marks) 
and the correct Hindi word forms (words that are 
present in the Hindi corpus). 
The Preprocessing component is a finite-
state transducer that normalizes the Hindi output 
of UHT-FSM component for the Hindi word 
language model. The transducer converts all 
cases of gemination of consonants into a simple 
consonant. For example, the UHT-FSM converts 
the Urdu word ??? (God) [r?bb] into ???? and the 
Preprocessing converts it into ?? [rb]. The 
transducer also removes the conjunct marker (??) 
from the output of the UHT-FSM except when it 
is preceded by one of the consonant from the set 
{? [r], ? [l], ? [m], ? [n]} and also followed by 
the consonant ? [h] (first 3 lines of Figure 3), 
e.g., UHT-FSM converts the Urdu words ?????? 
(Hindi) [h?ndi] and ????? (bride) [d??lhn] into ?????? 
and ?????? respectively and the Preprocess-
ing component converts them into ????? (re-
moves ??) and ?????? (no change). Actually, Pre-
processing deteriorates the accuracy of the output 
of the UHT-FSM component. We will come back 
to this point with exact figures in the next sec-
tion. 
The code of the finite-state transducer is given 
in XFST (Beesley and Karttunen, 2003) style in 
Figure 3. In XFST, the rules are applied in re-
verse order due to XFST?s transducer stack, i.e. a 
rule written at the end of the XFST script file 
will apply first and so on. 
 
read regex [? ?-> 0 || [? - [? | ? | ? | ?]] _ [? - 
?]]; 
read regex [?? -> 0 || [? | ? | ? | ?] _ [? - ?]]; 
read regex [?? -> 0 || [? - [? | ? | ? | ?]] _ [?]]; 
read regex [[? ? ??] -> ?, [? ? ??] -> ?, 
[? ? ??] -> ?, [? ? ??] -> ?, [? ? ??] -> ?, 
[? ?? ?] -> ?, [? ? ??] -> ?, [? ? ??] -> ?, 
[? ? ??] -> ?, [? ? ??] -> ?, [? ? ??] -> ?, [? ? ?
?] -> ?, [? ?? ?] -> ?, [? ? ??] -> ?, [? ? ??] 
-> ?, [? ? ??] -> ?, [? ? ??] -> ?, [? ? ??] -> 
?, [? ? ??] -> ?, [? ? ??] -> ?, [? ? ??] -> ?, 
[? ? ??] -> ?, [? ? ??] -> ?, [? ? ??] -> ?, 
[? ?? ?] -> ?, [? ?? ?] -> ?, [? ?? ?] -> ?, 
[? ?? ?] -> ?, [? ?? ?] -> ?, [? ?? ?] -> ?, 
[? ?? ?] -> ?, [? ?? ?] -> ?, [? ?? ?] -> ?, 
[? ?? ?] -> ?, [? ?? ?] -> ?, [? ? ??] -> ?]; 
Figure 3: Preprocessing Transducer 
3.2 Hindi Word Language Model 
The Hindi Word Language Model is an 
important component of the hybrid transliteration 
model. For the development of our statistical 
word language model, we have used the Hindi 
Corpus freely available from the Center for In-
dian Language Technology1, Indian Institute of 
Technology Bombay (IITB), India. 
First, we extracted all Hindi sentences from 
the Hindi corpus. Then we removed all punctua-
tion marks from each sentence. Finally, we add-
ed ?<s>? and ?</s>? tags at the start and at the 
end of each sentence. We trained a tri-gram 
Hindi Word Language Model with the 
SRILM (Stolcke, 2002) tool. The processed Hin-
di corpus data contains total 173,087 unique sen-
                                                 
1 http://www.cfilt.iitb.ac.in/ 
181
tences and more than 3.5 million words. The 
SRILM toolkit command ?disambig? is used to 
generate the final Hindi output using the statis-
tical word language model for Hindi and the 
Hindi Word Map described in the next section.  
3.3 Hindi Word Map 
The Hindi Word Map is another very important 
component of the proposed hybrid transliteration 
model. It describes how each ?normalized? Hindi 
word that can be seen after the Preprocess-
ing step and can be converted to one or several 
correct Hindi words, the final decision being 
made by the statistical word language model for 
Hindi. We have developed it from the same 
processed Hindi corpus data that was used to 
build the Hindi Word Language Model. 
We extracted all unique Hindi words (120,538 
unique words in total). 
The hybrid transliteration model is an effort to 
correctly transliterate the input Urdu text without 
diacritical marks in Hindi. Thus we take each 
unique Hindi word and try to generate all possi-
ble Hindi word options that can be given as input 
to the Hindi Word Language Model 
component for the said word. Consider the Urdu 
word ??? (God) [r?bb]; its correct Hindi spel-
lings are ????. If we remove the diacritical mark 
Shadda (H?) after the last character of the word, 
then the word becomes ?? and UHT-FSM trans-
literates it in ??*. Thus the Hindi Word 
Language Model will encounter either ???? or 
??* for the Hindi word ???? (two possible word 
options). In other words, the Hindi Word Map is 
a computational model that records all possible 
alignments between the ?normalized? or pre-
processed words (most likely input word forms) 
and the correct Hindi words from the corpus. 
We have applied a finite-state transducer that 
generates all possible word options for each 
unique Hindi word. We cannot give the full 
XFST code of the ?Default Input Creator? due to 
space shortage, but a sample XFST code is given 
in Figure 4. If the Urdu input contains all neces-
sary diacritical marks, then pre-processing of the 
output of the UHT-FSM tries to remove the effect 
of some of these diacritical marks from the Hindi 
output. In the next section, we will show that 
actually it increases the accuracy at the end. 
 
 
define CONSONANTS [? | ? | ? | ? | ? | ? | 
? | ? | ? | ? | ? | ? | ? | ? | ? | ? | ? | ? | ? | 
? | ? | ? | ? | ? | ? | ? | ? | ? | ? | ? | ? | ? | 
? | ? | ? | ? | ? | ? | ? | ?]; 
? 
read regex [?? (->) ?,? ?? (->) ?,? ?? (->) ??, ?? 
(->) ??, ?? (->) 0, ? ?(->) 0 || [CONSONANTS] 
_ ]; 
read regex [?? (->) ? ?|| [CONSONANTS] _ [? -
 .#.]]; 
read regex [?? -> ??, ? ? -> ??, ? ? -> ? ? || 
[CONSONANTS] _ .#.]; 
? 
Figure 4: Default Input Creator Transducer 
Practically, the Hindi Word Map is a file in 
which each line contains a possible input word to 
Hindi Word Language Model, followed 
by a list of one (see line 3 of Figure 5) or more 
(see line 1 of Figure 5) words from the corpus 
that are associated with this possible input word. 
The ?Default Input Creator? transducer has 
generated in total 961,802 possible input words 
for 120,538 unique Hindi words. For implemen-
tation reasons, we also added non-ambiguous 
pair entries in the word map (see line 2 of Figure 
5), thus the initial word map contains in total 
1,082,340 entries. We extract unique option 
words and finally, Hindi Word Map contains in 
total 962,893 entries. Some examples from Hindi 
Word Map file are given in Table 5. 
 
(1) ???? ???? ???? 
(2) ???? ???? 
(3) ?? ???? 
(4) ???????? ????????? ?????????? 
(5) ?? ?? ?? 
Figure 5: Sample Hindi Word Map 
4 Test and Results 
For testing purposes, we extracted 200 Hindi 
sentences from the Hindi corpus before removing 
punctuation marks. These sentences were of 
course removed from the training corpus used to 
build the statistical word language model for 
Hindi. First we converted these 200 Hindi sen-
tences in Urdu using Hindi Urdu Finite-state 
transliteration model (Malik et al, 2008). Trans-
182
literated Urdu sentences were post edited ma-
nually for any error and we also made sure that 
the Urdu text contained all diacritical marks. 200 
original Hindi sentences served as Hindi refer-
ence for evaluation purposes. 
From the post-edited Urdu sentences, we de-
veloped two test corpora. The first test corpus 
was the Urdu test with all diacritical marks. In 
the second test corpus, all diacritical marks were 
removed. We calculated both word level and 
character level accuracy and error rates using the 
SCLITE 2  tool. Our 200 sentence test contains 
4,250 words and 16,677 characters in total. 
4.1 Test: UHT-FSM 
First we converted both Urdu test data using 
UHT-FSM only and compared the transliterated 
Hindi texts with the Hindi reference. UHT-FSM 
shows a word error rate of 21.5% and 51.5% for 
the Urdu test data with and without diacritics 
respectively. Results are given in Table 6, row 1. 
 
Urdu Test Data With diacritics 
Without 
diacritics 
UHT-FSM 
Accuracy/Error 
80.7% / 
21.5% 
50.7% / 
51.5% 
UHT-FSM + 
HLM 
82.6% / 
19.6% 
79.1% / 
23.1% 
UHT-FSM + 
PrePro 
67.5% / 
32.4% 
50.7% / 
51.5% 
UHT-FSM + 
PrePro + HLM 
85.8% / 
16.4% 
79.1% / 
23.1% 
Table 6: Word Level Results 
These results support our claims that the absence 
of diacritical marks considerably increases the 
error rate. 
4.2 Test: UHT-FSM + Hindi Language 
Model 
Both outputs of UHT-FSM are first passed direct-
ly to Hindi Word Language Model with-
out preprocessing. The Hindi Word Lan-
guage Model converts UHT-FSM Hindi out-
put in the final Hindi output with the help of 
Hindi Word Map. 
Two final outputs were again compared with 
the Hindi reference and results are given in Table 
6, row 2. For Urdu test data without diacritics, 
error rate decreased by 28.4% due to the Hindi 
Word Language Model and Hindi Word 
                                                 
2 http://www.itl.nist.gov/iad/mig//tools/ 
Map as compared to the UHT-FSM error rate. 
The Hindi Word Language Model also decreases 
the error rate by 1.9% for the Urdu test data with 
diacritics. 
4.3 Test: UHT-FSM + Preprocessing 
In this test, both outputs of UHT-FSM were pre-
processed and the intermediate Hindi outputs 
were compared with the Hindi reference. Results 
are given in Table 6, row 3. After the comparison 
of results of row 1 and row 3, it is clear that pre-
processing deteriorates the accuracy of Urdu test 
data with diacritics and does not have any effect 
on Urdu test data without diacritics. 
4.4 Test: UHT-FSM + Preprocessing + 
Hindi Language Model 
Preprocessed UHT-FSM Hindi outputs of the test 
of Section 4.3 were passed to the Hindi Word 
Language Model that produced final Hindi 
outputs with the help of the Hindi Word Map. 
Results are given in Table 6, row 4. They show 
that the Hindi Word Language Model 
increases the accuracy by 5.1% and 18.3% when 
compared with the accuracy of UHT-FSM and 
UHT-FSM + Preprocessing tests respectively, for 
the Urdu test data with diacritical marks. 
For the Urdu test data without diacritical 
marks, the Hindi Word Language Model 
increases the accuracy rate by 28.3% in compari-
son to the accuracy of the UHT-FSM output 
(whether pre-processed or not). 
4.5 Character Level Results 
All outputs of tests of Sections 4.1, 4.2, 4.3 and 
4.4 and the Hindi reference are processed to cal-
culate the character level accuracy and error 
rates. Results are given in Table 7. 
 
Urdu Test 
Data 
With 
diacritics 
Without 
diacritics 
UHT-FSM 94.1% / 6.5% 77.5% / 22.6%
UHT-FSM + 
HLM 94.6% / 6.1% 89.8% / 10.7 
UHT-FSM + 
PreP 87.5% / 13.0% 77.5% / 22.6 
UHT-FSM + 
PreP + HLM 94.5% / 6.1% 89.8% / 10.7 
Table 7: Character Level Results 
183
4.6 Results and Examples 
The Hindi Word Language Model in-
creases the accuracy of Urdu Hindi translitera-
tion, especially for the Urdu input without dia-
critical marks. 
Consider the examples of Figure 7. Figure 1 is 
reproduced here by adding the Hindi translitera-
tion of example sentences using the proposed 
hybrid transliteration model and Hindi reference. 
 
(1)  (i) ?? ???? ????? ??? ????? ???? ??? ???? 
 (ii) ??? ?? ??? ???? ??? ???? ??? ?? 
(i) ??? ?? ???? ???? ??? ???? ???? ?? 
?? ??? ???? ??? ??? ??? ?? ???(ii)  
I have not done a lot of work
Output of Hybrid Transliteration Model 
(i) ??? ?? ???? ???? ??? ???? ???? ?? 
(ii) ??? ?? ???? ???? ??? ???? ???? ?? 
Hindi Reference 
????? ???? ???? ??? ???? ???? ?? 
(2) (i) ????? ?? ?? ?????? ???? ????? ?? ?? ???????????  
  (ii) ??????? ??? ?? ??? ??? ????? ??? ?? ???  
(i) ??????? ??? ?? ?? ?? ????? ??? ?? ?? 
 ?? ??? ???? ?? ?? ?? ??? ??? ???(ii) 
?? 
Both at the central level and at the state level
Output of Hybrid Transliteration Model 
(i) ??????? ??? ?? ?? ?? ????? ??? ?? ?? 
(ii) ??? ??? ??? ?? ?? ?? ????? ??? ?? ?? 
Hindi Reference 
??????? ??? ?? ?? ?? ????? ??? ?? ?? 
Figure 7: Examples 
By comparing Hindi outputs of Hindi Word 
Language Model with the Hindi reference, 
only the first word of (2)(ii) is wrong and other 
errors due to the absence of diacritical marks in 
the source Urdu sentences are corrected properly. 
5 Conclusion 
From the test results of the previous section we 
can conclude that the statistical word language 
model increases the accuracy of Urdu to Hindi 
transliteration, especially for Urdu input text 
without diacritical marks. The proposed Hybrid 
Transliteration Model improves the accuracy and 
produces the correct Hindi output even when the 
crucial information in the form of diacritical 
marks is absent. It increases the accuracy by 
28.3% in comparison to our previous Finite-state 
Transliteration Model. This study also shows that 
diacritical marks are crucial and necessary for 
Hindi Urdu transliteration. 
References  
Beesley, Kenneth R. and Karttunen, Lauri. 2003. Fi-
nite State Morphology, CSLI Publication, USA. 
Grimes, Barbara F. (ed). 2000. Pakistan, in Ethnolo-
gue: Languages of the World, 14th Edition Dallas, 
Texas; Summer Institute of Linguistics, pp: 588-
598. 
Hussain, Sarmad. 2004. Letter to Sound Rules for 
Urdu Text to Speech System, proceedings of Work-
shop on Computational Aproaches to Arabic 
Script-based Languages, COLING 2004, Geneva, 
Switzerland. 
Jawaid, Bushra and Tafseer Ahmed. 2009. Hindi to 
Urdu Conversion: Beyond Simple Transliteration, 
in proceedings of Conference on Language & 
Technology, Lahore, Pakistan. 
Kellogg, Rev. S. H. 1872. A Grammar of Hindi Lan-
guage, Delhi, Oriental Book reprints. 
Khan, Mehboob Alam. 1997. ????? ?? ???? ???? (Sound 
System in Urdu), National Language Authority, 
Pakistan 
Knight, K. and Graehl, J. 1998. Machine Translitera-
tion, Computational Linguistics, 24(4). 
Knight, K. and Stall, B. G. 1998. Transliterating 
Names and Technical Terms in Arabic Text, pro-
ceedings of COLING/ACL Workshop on Compu-
tational Approaches to Semitic Languages. 
Malik, M. G. Abbas. Boitet, Christian. Bhattcharyya, 
Pushpak. 2008. Hindi Urdu Machine Translitera-
tion using Finite-state Transducers, proceedings of 
COLING 2008, Manchester, UK. 
Montaut, A. 2004. A Linguistic Grammar of Hindi, 
Studies in Indo-European Linguistics Series, Mun-
chen, Lincom Europe. 
Paola, V. and Sanjeev, K. 2003. Transliteration of 
Proper Names in Cross-language Application, pro-
ceedings of 26th Annual International ACM SIGIR 
Conference on Research and Development in In-
formation Retrieval, Toronto, Canada. 
Pirkola, A. Toivonen, J. Keshustalo, H. Visala, K. and 
Jarvelin, K. 2003. Fuzzy Translation of Cross-
lingual Spelling Variants, proceedings of 26th An-
nual International ACM SIGIR Conference on Re-
search and Development in Information Retrieval, 
Toronto, Canada. 
Rahman, Tariq. 2004. Language Policy and Localiza-
tion in Pakistan: Proposal for a Paradigmatic 
184
Shift, Crossing the Digital Divide, SCALLA Con-
ference on Computational Linguistics. 
Stolcke, A. 2002. SRILM ? An Extensible Language 
Modeling Toolkit, in proceedings of International 
Conference on Spoken Language Processing. 
Stolcke, A. Shriberg, E. Bates, R. Ostendorf, M. Hak-
kani, D. Plauche, M. Tur, G. and Lu, Y. 1998. Au-
tomatic Detection of Sentence Boundaries and Dis-
fluencies based on Recognized Words. Proceedings 
of International Conference on Spoken Language 
Processing (ICSLP), Sydney, Australia. 
Yan, Qu. Gregory, Grefenstette. and David A. Evans. 
2003. Automatic Transliteration for Japanese-to-
English Text Retrieval. In proceedings of the 26th 
annual international ACM SIGIR conference on 
Research and Development in Information Retriev-
al, pp: 353 ? 360. 
Zia, Khaver. 1999. Standard Code Table for Urdu. 
Proceedings of 4th Symposium on Multilingual In-
formation Processing (MILIT-4), Yangon, Myan-
mar, CICC, Japan. 
 
185
Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 161?166,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
The LIG machine translation system for WMT 2010
Marion Potet, Laurent Besacier and Herve? Blanchon
LIG Laboratory, GETALP Team
University Joseph Fourier, Grenoble, France.
Marion.Potet@imag.fr
Laurent.Besacier@imag.fr
Herve.Blanchon@imag.fr
Abstract
This paper describes the system submit-
ted by the Laboratory of Informatics of
Grenoble (LIG) for the fifth Workshop
on Statistical Machine Translation. We
participated to the news shared transla-
tion task for the French-English language
pair. We investigated differents techniques
to simply deal with Out-Of-Vocabulary
words in a statistical phrase-based ma-
chine translation system and analyze their
impact on translation quality. The final
submission is a combination between a
standard phrase-based system using the
Moses decoder, with appropriate setups
and pre-processing, and a lemmatized sys-
tem to deal with Out-Of-Vocabulary con-
jugated verbs.
1 Introduction
We participated, for the first time, to the shared
news translation task of the fifth Workshop on Ma-
chine Translation (WMT 2010) for the French-
English language pair. The submission was
performed using a standard phrase-based trans-
lation system with appropriate setups and pre-
processings in order to deal with system?s un-
known words. Indeed, as shown in (Carpuat,
2009), (Habash, 2008) and (Niessen, 2004), han-
dling Ou-of-Vocabulary words with techniques
like lemmatization, phrase table extension or mor-
phological pre-processing is a way to improve
translation quality. After a short presentation of
our baseline system setups we discuss the effect
of Out-Of-Vocabulary words in the system and in-
troduce some ideas we chose to implement. In the
last part, we evaluate their impact on translation
quality using automatic and human evaluations.
2 Baseline System Setup
2.1 Used Resources
We used the provided Europarl and News par-
allel corpora (total 1,638,440 sentences) to train
the translation model and the News monolin-
gual corpora (48,653,884 sentences) to train the
language model. The 2008 News test corpora
(news-test2008; 2,028 sentences) was used to tune
the produced system and last year?s test corpora
(news-test2009; 3,027 sentences) was used for
evaluation purposes. These corpora will be ref-
ered to as Dev and Test later in the paper. As pre-
processing steps, we applied the PERL scripts pro-
vided with the corpora to lowercase and tokenise
the data.
2.2 Language modeling
The target language model is a standard n-gram
language model trained using the SRI language
modeling toolkit (Stocke, 2002) on the news
monolingual corpus. The smoothing technique we
applied is the modified Kneser-Ney discounting
with interpolation.
2.3 Translation modeling
The translation model was trained using the par-
allel corpus described earlier (Europarl+News).
First, the corpus was word aligned and then, the
pairs of source and corresponding target phrases
were extracted from the word-aligned bilingual
training corpus using the scripts provided with
the Moses decoder (Koehn et al, 2007). The re-
sult is a phrase-table containing all the aligned
phrases. This phrase-table, produced by the trans-
lation modeling, is used to extract several transla-
tions models. In our experiment we used thirteen
standard translation models: six distortion models,
a lexicon word-based and a phrase-based transla-
tion model for both direction, and a phrase, word
and distortion penalty.
161
2.4 Tuning and decoding
For the decoding (i.e. translation of the test
set), the system uses a log-linear combination of
the previous target language model and the thir-
teen translation models extracted from the phrase-
table. As the system can be beforehand tuned by
adjusting log-linear combination weights on a de-
velopement corpus, we used the Minimum Error
Rate Training (MERT) method, by (Och, 2003).
3 Ways of Improvements
3.1 Discussion about Out-Of-Vocabulary
words in PBMT systems
Phrase-based statistical machine translation
(PBMT) use phrases as units in the translation
process. A phrase is a sequence of n consecutive
words known by the system. During the training,
these phrases are automaticaly learned and each
source phrase is mapped with its corresponding
target phrase. Throughout test set decoding, a
word not being part of this vocabulary list is
labeled as ?Out-Of-Vocabulary? (OOV) and, as it
doesn?t appear in the translation table, the system
is unable to translate it. During the decoding,
Out-Of-Vocabulary words lead to ?broken?
phrases and degrade translation quality. For these
reasons, we present some techniques to handle
Out-Of-Vocabulary words in a PBMT system and
combine these techniques before evaluating them.
In a preliminary study, we automatically ex-
tracted and manually analyzed OOVs of a 1000
sentences sample extracted from the test cor-
pus (news-test2009). There were altogether 487
OOVs tokens wich include 64.34% proper nouns
and words in foreign languages, 17.62% common
nouns, 15.16% conjugated verbs, 1.84% errors in
source corpus and 1.02% numbers. Note that, as
our system is configured to copy systematically
the OOVs in the produced translated sentence, the
rewriting of proper nouns and words in foreign
language is straightforward in that case. However,
we still have to deal with common nouns and con-
jugated verbs.
Initial sentence:
?Cela ne marchera pas? souligna-t-il par la suite.
Normalised sentence:
?Cela ne marchera pas? il souligna par la suite
Figure 1: Normalisation of the euphonious ?t?
3.2 Term expansion with dictionary
The first idea is to expand the vocabulary size,
more specifically minimizing Out-Of-Vocabulary
common nouns adding a French-English dictio-
nary during the training process. In our experi-
ment, we used a free dictionnary made available
by the Wiktionary1 collaborative project (wich
aims to produce free-content multilingual dictio-
naries). The provided dictionnary, containing
15,200 entries, is added to the bilingual training
corpus before phrase-table extraction.
3.3 Lemmatization of the French source
verbs
To avoid Out-Of-Vocabulary conjugated verbs one
idea is to lemmatize verbs in the source train-
ing and test corpus to train a so-called lemma-
tized system. We used the freely available French
lemmatiser LIA TAGG (Be?chet, 2001). But, ap-
plying lemmatization leads to a loss of informa-
tion (tense, person, number) which may affect
deeply the translation quality. Thus, we decided
to use the lematized system only when OOV verbs
are present in the source sentence to be trans-
lated. Consequently, we differentiate two kinds
of sentences: -sentences containing at least one
OOV conjugated verb, and - sentences which do
not have any conjugated verb (these latter sen-
tences obviously don?t need any lemmatization!).
Thereby, we decided to build a combined trans-
lation system which call the lemmatized system
only when the source sentence contains at least
one Out-Of-Vocabulary conjugated verb (other-
wise, the sentence will be translated by the stan-
dard system). To detect sentences with Out-Of-
Vocabulary conjugated verb we translate each sen-
tence with both systems (lemmatized and stan-
dard), count OOV and use the lemmatized transla-
tion only if it contains less OOV than the standard
translation. For example, a translation containing
k Out-Of-Vocabulary conjugated verbs and n oth-
ers Out-Of-Vocabulary words (in total k+n OOV)
with the standard system, contains, most probably,
only n Out-Of-Vocabulary words with the lemma-
tised system because the conjugated verbs will be
lemmatized, recognized and translated by the sys-
tem.
1http://wiki.webz.cz/dict/
162
3.4 Normalization of a special French form
We observed, in the French source corpra, a spe-
cial French form which generates almost always
Out-Of-Vocabulary words in the English transla-
tion. The special French form, named euphonious
?t?, consists of adding the letter ?t? between a verb
(ended by ?a?, ?e? or ?c?) and a personal pronoun
and, then, inverse them in order to facilitate the
prononciation. The sequence is represented by:
verb-t-pronoun like annonca-t-elle, arrive-t-il, a-
t-on, etc. This form concerns 1.75% of the French
sentences in the test corpus whereas these account
for 0.66% and 0.78% respetively in the training
and the developement corpora. The normalized
proposed form, illustrated below in figure 1, con-
tains the subject pronoun (in first posistion) and
the verb (in the second position). This change has
no influence on the French source sentence and ac-
cordingly on the correctness and fluency of the En-
glish translation.
3.5 Adaptation of the language model
Finally, for each system, we decided to apply dif-
ferent language models and to look at those who
perfom well. In addition to the 5-gram language
model, we trained and tested 3-gram and 4-gram
language models with two different kinds of vo-
cabularies : - the first one (conventional, refered to
as n-gram in table 3) contains an open-vocabulary
extracted from the monolingual English training
data, and - the second one (refered to as n-gram-
vocab in table 3) contains a closed-vocabulary ex-
tracted from the English part of the bilingual train-
ing data. In both cases, language model probabil-
ities are trained from the monolingual LM train-
ing data but, in the second case, the lexicon is re-
stricted to the one of the phrase-table.
4 Experimental results
In the automatic evaluation, the reported evalu-
ation metric is the BLEU score (Papineni et al,
2002) computed by MTEval version 13a. The re-
sults are reported in table 1. Note that in our ex-
periments, according to the resampling method of
(Koehn, 2004), there are significative variations
(improvement or deterioration), with 95% cer-
tainty, only if the difference between two BLEU
scores represent, at least, 0.33 points. To complete
this automatic evaluation, we performed a human
analysis of the systems outputs.
4.1 Standard systems
4.1.1 Term expansion with dictionary
Regarding the results of automatic evaluation (ta-
ble 1, system (2)), adding the dictionary do not
leads to a significant improvement. The OOV
rate and system perplexity are reduced but, ignor-
ing the tuned system which presents lower per-
formance, the BLEU score decreases significatly
on the test set. The BLEU score of the system
augmented with the dictionary is 24.50 whereas
the baseline one is 24.94. So we can conclude
that there is not a meaningfull positive contribu-
tion, probably because the size of the dictionary
is very small regarding the bilingual training cor-
pus. We found out very few Out-Of-Vocabulary
words of the standard system recognized by the
system with the dictionary, see figure 2 for exam-
ple (among them : coupon, cafard, blonde, retar-
dataire, me?dicaments, pamplemousse, etc.). But,
as the dictionnary is very small, most OOV com-
mon words like ho?tesse and clignotant are still un-
known. Regarding the output sentences, we note
that there are very few differences and the quality
is equivalent. The dictionary used is to small to
extend the system?s vocabulary and most of words
still Out-Of-Vocabulary are conjugated verbs and
unrecognized forms.
Baseline system:
A cafard fled before the danger, but if he felt fear?
System with dictionary:
A blues fled before the danger, but if he felt fear?
Figure 2: Example of sentence with an OOV com-
mon noun
4.1.2 Normalisation of special French form
Considering the BLEU score, the normalization of
French euphonious ?t? have, apparently, very few
repercussion on the translation result (table 1, sys-
tem (3)) but the human analysis indicates that, in
our context, the normalisation of euphonious ?t?
brings a clear improvement as seen in example 3.
Consequently, this preprocessing is kept in the fi-
nal system.
4.1.3 Tuning
We can see in table 1 that the usual tuning with
Minimum Error Rate Training algorithm deterio-
rates systematically performance scores on the test
set, for all systems. This can be explained by the
163
System OOVs ppl Dev score Test score
(1) Baseline 2.32% 207 29.72 (19.93) 23.77 (24.94)
(2) + dictionary 2.30% 204 30.01 (23.92) 24.32 (24.50)
(3) + normalization 2.31% 204 30.07 (19.90) 23.99 (24.98)
(4) + normalization + Dev data 2.30% 204 / (/) / (25,05)
Table 1: Standard systems BLEU scores with tuning (without tuning)/ LM 5-gram
Baseline system:
?It will not work? souligna-t-il afterwards.
System with normalisation:
?It will not work? he stressed afterwards.
Figure 3: Example of sentence with a ?verb-t-
pronoun? form
gap between the developement and test corpora (ie
the Dev set may be not representative of the Test
set). So, even if it is recommanded in the standard
process, we do not tune our system (we use the de-
fault weights proposed by the Moses decoder) and
add the developement corpus to train it. In this
case, the training set contains 1,640,468 sentences
(the initial 1,638,440 sentences and the 2,028 sen-
tences of the developement set). This slightly im-
proves the system (from 24.98, the BLEU score
raise to 25,05 after adding the developpement set
to the training).
4.2 Lemmatised systems
Results of lemmatised systems are reported on ta-
ble 2. First, we can notice that, in this particular
case, the tuning (with MERT method) is manda-
tory to adapt the weights of the log linear model.
Our analysis of the tuned weight of the lemma-
tised system shows that, in particular, the word
penalty model has a very low weight (this favours
short sentences) and the lexical word-based trans-
lation models have a very low weight (no use of
the lexical translation probability). We also no-
tice that the lemmatization leads to a real drop-off
of OOV rate (fall from 2.32% for the baseline, to
2.23% for the lemmatized system) and perplexity
(fall from 207 for the baseline, to 178 for the lem-
matized system). We can observe a clear decrease
of the performance with the lemmatized system
(BLEU score of 20.50) compared with a non-
lemmatized one (BLEU score of 24.94). This can
be significatively improved applying euphonious
?t? normalization to the source data (BLEU score
of 22.14). Almost all French OOV conjugated
verbs with the standard system were recognized
by the lemmatized one (trierait, joues, testaient,
immerge?e, e?conomiseraient, baisserait, pre?pares,
etc.) but the small decrease of the translation qual-
ity can be explained, among other things, by sev-
eral tense errors. See illustration in figure 4. So,
we conclude that the systematic normalization of
French verbs, as a pre-process, reduce the Out-Of-
Vocabulary conjugated verbs but decrease slighly
the final translation quality. The use of such a sys-
tem is helpfull especially when the sentence con-
tains conjugated verbs (see example 5).
4.3 Adaptation of the language model
We applied five differents language models (3-
gram and 4-gram language models with selected
vocabulary or not and a 5-gram language model)
to the four standard systems and the two lemma-
tised one. The results, reported in table 3, show
that BLEU score can be significantly different de-
pending on the language model used. For exam-
ple, the fifth system (5) obtained a BLEU score of
21.48 with a 3-gram language model and a BLEU
score of 22.84 with a 4-gram language model. We
can also notice that five out of our six systems out-
perform using a language model with selected vo-
cabulary (n-gram-vocab). One possible explana-
tion is that with LM using selected vocabulary (n-
gram-vocab), there is no loss of probability mass
for english words not present in the translation ta-
ble.
4.4 Final combined system
Considering the previous observations, we believe
that the best choice is to apply the lemmatized
system only if necessary i.e. only if the sentence
contains OOV conjugated verbs, otherwise, a stan-
dard system should be used. We consider system
(4), with 4-gram-vocab language model (selected
vocabulary) without tuning, as the best standard
system and system (6), with 3-gram-vocab lan-
guage model (selected vocabulary) not tuned ei-
ther, as the best lemmatized system. The final
164
System OOVs ppl Dev score Test score
(5) lemmatization 2.23% 178 20.97 (8.57) 20.50 (8.56)
(6) lemmatization + normalization 2.18% 175 27.81 (9.20) 22.14 (10.82)
Table 2: Lemmatised systems BLEU scores with tuning (without tuning)/ LM 5-gram
Baseline system: You will be limited by the absence of exit for headphones.
Lemmatised system: You are limited by the lack of exit for ordinary headphones.
reference: You will be limited by the absence of output on ordinary headphones.
Figure 4: Example of sentences without OOV verbs
system translations are those of the lemmatized
system (6) when we translate sentences with one
or more Out-Of-Vocabulary conjugated verbs and
those of the un-lemmatized system (4) otherwise.
Around 6% of test set sentences were translated
by the lemmatized system. Considering the results
reported in table 4, the combined system?s BLEU
score is comparable to the standard one (25.11
against 25.17).
System Test score sentences
(4) Standard sys. 25.17 94 %
(6) Lemmatised sys. 22.89 6%
(7) Combined 25.11 100 %
Table 4: Combined system?s results and % trans-
lated sentences by each system
5 Human evaluation
We compared two data set. The first set (selected
sent.) contains 301 sentences selected from test
data by the combined system (7) to be translated
by the lemmatized system (6) whereas the second
set (random sent.) contains 301 sentences ran-
domly picked up. The latter is our control data set.
We compared for both groups the translation hy-
pothesis given by the lemmatized system and the
standard one.
We performed a subjective evaluation with the
NIST five points scales to measure fluency and ad-
equacy of each sentences through SECtra w inter-
face (Huynh et al, 2009).We involved a total of 6
volunteers judges (3 for each set). We evaluated
the inter-annotator agreement using a generalized
version of Kappa. The results show a slight to fair
agreement according (Landis, 1977).
The evaluation results, detailled in table 5 and 6,
showed that both fluency and adequacy were im-
proved using our combined system. Indeed, for a
random input (random sent.), the lemmatized sys-
tem lowers the translations quality (fluency and
adequacy are degraded for, respectively, 35.8%
and 37.5% of the sentences), while it improves
the quality for sentences selected by the combined
system (for ?selected sent.?, fluency and adequacy
are improved or stable for 81% of the sentences).
Adequacy selected sent. random sent.
(6) ? (4) 81% 62.4%
(6) < (4) 18.9% 37.5%
Table 5: Subjective evaluation of sentences ade-
quacy ((6) lemmatized system - (4) standard sys-
tem)
Fluency selected sent. random sent.
(6) ? (4) 81% 64.1%
(6)<(4) 18.9% 35.8%
Table 6: Subjective evaluation of sentences flu-
ency ((6) lemmatized system - (4) standard sys-
tem)
6 Conclusion and Discussion
We have described the system used for our sub-
mission to the WMT?10 shared translation task for
the French-English language pair.
We propose dsome very simple techniques to
improve rapidely a statistical machine translation.
Those techniques particularly aim at handling
Out-Of-Vocabulary words in statistical phrase-
based machine translation and lead an improved
fluency in translation results. The submited sys-
tem (see section 4.4) is a combination between a
standard system and a lemmatized system with ap-
propriate setup.
165
Baseline system: At the end of trade, the stock market in the negative bascula.
Lemmatised system: At the end of trade, the stock market exchange stumbled into the negative.
Baseline system: You can choose conseillera.
Lemmatised system: We would advise you, how to choose.
Figure 5: Example of sentences with OOV conjugated verbs
System 3-gram 3-gram-vocab 4-gram 4-gram-vocab 5-gram
(1) 24.60 24.95 24.94 25.11 24.94
(2) 25.14 25.17 24.50 23.49 24.50
(3) 24.88 25.00 24.98 25.15 24.98
(4) 24.92 24.99 25.05 25.17 25.05
(5) 21.48 19.48 22.84 20.18 20.50
(6) 22.60 22.89 22.14 22.24 22.14
Table 3: Systems?s results on test set with differents language models
This system evaluation showed a positive influ-
ence on translation quality, indeed, while the im-
provements on automatic metrics are small, man-
ual inspection suggests a significant improvements
of translation fluency and adequacy.
In future work, we plan to investigate and de-
velop more sophisticated methods to deal with
Out-Of-Vocabulary words, still relying on the an-
alyze of our system output. We believe, for ex-
ample, that an appropriate way to use the dictio-
nary, a sensible pre-processings of French source
texts (in particular normalization of some specific
French forms) and a factorial lemmatization with
the tense information can highly reduce OOV rate
and improve translation quality.
References
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-
dra Constantin, Evan Herbst. 2007. Moses: Open
Source Toolkit for Statistical Machine Translation.
Annual Meeting of the Association for Computa-
tional Linguistics (ACL). Prague, Czech Republic.
Papineni K., Roukos S., Ward T., and Zhu W.J. 2002.
BLEU: a method for automatic evaluation of ma-
chine translation. ACL-2002: 40th Annual meeting
of the Association for Computational Linguistics,
pp. 311?318. Philadelphia, Pennsylvania, USA.
Andreas Stolcke. 2002. SRILM - An Extensible Lan-
guage Modeling Toolkit. International Conference
on Spoken Language Processing, Vol. 2, pp 901?
904. Denver, Colorado, USA.
Frederic Be?chet. 2001. LIA TAGG. http://old.lia.univ
-avignon.fr/chercheurs/bechet/download fred.html.
Franz Josef Och. 2003. Minimum error rate train-
ing for statistical machine translation. Annual Meet-
ing of the Association for Computational Linguistics
(ACL). Sapporo, July.
Philipp Koehn. 2004. Statistical significance tests
for machine translation evaluation. conference on
Empirical Methods in Natural Language Processing
(EMNLP), pp 388?395. Barcelona, Spain.
Marine Carpuat. 2009. Toward Using Morphology in
French-English Phrase-based SMT. Workshop on
Machine Translation in European Association for
Computational Linguistics (EACL-WMT), pp 150?
154. Athens, Greece.
Sonja Niessen and Hermann Ney. 2004. Statistical
Machine Translation with Scarce Resources Using
Morpho-syntactic Information. Computational Lin-
guistics, vol. 30, pp 181?204.
Nizar Habash. 2008. Four techniques for Online
Handling of Out-Of-Vocabulary Words in Arabic-
English Statistical Machine Translation. Human
Language Technology Workshop in Association for
Computational Linguistics, (ACL-HTL), pp 57?60.
Columbus, Ohio, USA.
Landis J. R. and Koch G. G.. 1977. The Measurement
of Observer Agreement for Categorical Data. Bio-
metrics, vol. 33, pp. 159?174.
Herve? Blanchon, Christian Boitet and Cong-Phap
Huynh. 2009. A Web Service Enabling Grad-
able Post-edition of Pre-translations Produced by
Existing Translation Tools: Practical Use to Provide
High-quality Translation of an Online Encyclopedia.
MT Summit XII, Beyond Translation Memories: New
Tools for Translators Workshop, pp 20?27. Ottawa,
Canada.
166
Proceedings of the 1st Workshop on South and Southeast Asian Natural Language Processing (WSSANLP), pages 1?7,
the 23rd International Conference on Computational Linguistics (COLING), Beijing, August 2010
Boosting N-gram Coverage for Unsegmented Languages  Using 
Multiple Text Segmentation Approach
Solomon Teferra Abate
LIG Laboratory, 
CNRS/UMR-5217
solomon.abate@imag.fr
Laurent Besacier
LIG Laboratory, 
CNRS/UMR-5217
laurent.besacier@imag.fr
Sopheap Seng
LIG Laboratory, 
CNRS/UMR-5217
MICA Center, CNRS/UMI-
2954
sopheap.seng@imag.fr
Abstract
Automatic  word  segmentation  errors, 
for  languages  having  a  writing  system 
without word boundaries, negatively af-
fect the performance of language mod-
els.  As a solution, the use of  multiple, 
instead of unique, segmentation has re-
cently  been  proposed.  This  approach 
boosts  N-gram  counts  and  generates 
new  N-grams.  However,  it  also  pro-
duces bad N-grams that  affect the lan-
guage models' performance. In this pa-
per, we study more deeply the contribu-
tion  of  our  multiple  segmentation  ap-
proach  and experiment on an efficient 
solution to minimize the effect of adding 
bad N-grams.
1 Introduction
A language model  is  a  probability assignment 
over  all  possible  word  sequences  in  a  natural 
language. It assigns a relatively large probabili-
ty to meaningful, grammatical, or frequent word 
sequences and a low probability or a zero proba-
bility  to  nonsensical,  ungrammatical  or  rare 
ones.  The statistical  approach used in N-gram 
language modeling requires a large amount of 
text data in order to make an accurate estimation 
of probabilities. These data are not available in 
large  quantities  for  under-resourced  languages 
and the lack of text data has a direct impact on 
the performance of language models. While the 
word is  usually  a  basic  unit  in  statistical  lan-
guage  modeling,  word  identification  is  not  a 
simple  task  even  for  languages  that  separate 
words by a special character (a white space in 
general).  For  unsegmented  languages,  which 
have a writing system without obvious word de-
limiters, the N-grams of words are usually esti-
mated  from  the  text  corpus  segmented  into 
words employing automatic methods. Automat-
ic segmentation of text is not a trivial task and 
introduces errors due to the ambiguities in natu-
ral language and the presence of out of vocabu-
lary words in the text. 
While the lack of text resources has a nega-
tive  impact  on  the  performance  of  language 
models,  the  errors  produced by the  word seg-
mentation make those data even less usable. The 
word N-grams not found in the training corpus 
could be due not only to the errors introduced 
by the automatic segmentation but  also to the 
fact  that  a  sequence  of  characters  could  have 
more than one correct segmentation. 
In  previous  article  (Seng  et  al.,  2009),  we 
have proposed a method to estimate an N-gram 
language  model  from  the  training  corpus  on 
which each sentence is segmented into multiple 
ways instead of a unique segmentation. The ob-
jective of multiple segmentation is to generate 
more N-grams from the training corpus to use in 
language modeling. It was possible to show that 
this  approach  generates  more  N-grams  (com-
pared  to  the  classical  dictionary-based  unique 
segmentation method) that are potentially useful 
and relevant in language modeling. The applica-
tion of multiple segmentation in language mod-
eling  for  Khmer  and  Vietnamese  showed im-
provement in terms of tri-gram hits and recogni-
tion error rate in Automatic Speech Recognition 
(ASR) systems. 
This work is a continuation of our previous 
work on the use of multiple segmentation. It is 
conducted on Vietnamese only. A close analysis 
of N-gram counts shows that the approach has 
in fact two contributions: boosting the N-gram 
1
counts that are generated with first best segmen-
tation  and  generating  new N-grams.  We have 
also identified that there are N-grams that nega-
tively  affect  the  performance  of  the  language 
models. In this paper, we study the contribution 
of boosting N-gram counts and  of new N-grams 
to the performance of the language models and 
consequently  to  the  recognition  performance. 
We also present experiments where rare or bad 
N-grams are cut off in order to minimize their 
negative effect on the performance of the lan-
guage models.
The paper is organized as follows: section 2 
presents the theoretical background of our mul-
tiple  segmentation  approach;  in  section  3  we 
point out the set up of our experiment; in sec-
tion 4 we present the results of our detailed sta-
tistical analysis of N-grams generated by multi-
ple  segmentation  systems.  Section  5  presents 
the  evaluation  results  of  our  language  models 
for  ASR  and  finally,  we  give  concluding  re-
marks.
2 Multiple Text Segmentation
Text segmentation is a fundamental task in nat-
ural language processing (NLP). Many NLP ap-
plications require the input text segmented into 
words before making further progress because 
the word is considered the basic semantic unit in 
natural  languages.  For unsegmented languages 
segmenting text into words is not a trivial task. 
Because of ambiguities in human languages, a 
sequence  of  characters  may  be  segmented  in 
more than one way to  produce a  sequence of 
valid words.  This is due to the fact  that there 
are different segmentation conventions and the 
definition of  word in a language is  often am-
biguous. 
Text  segmentation  techniques  generally  use 
an  algorithm  which  searches  in  the  text  the 
words corresponding to those in a dictionary. In 
case of ambiguity, the algorithm selects the one 
that  optimizes  a  parameter  dependent  on  the 
chosen  strategy.  The  most  common optimiza-
tion strategies consist of maximizing the length 
of  words  (?longest  matching?)  or  minimizing 
the  number  of  words  in  the  entire  sentence 
(?maximum matching?). These techniques rely 
heavily on the availability and the quality of the 
dictionaries and while it is possible to automati-
cally generate a dictionary from an unsegment-
ed text corpus using unsupervised methods, dic-
tionaries are often created manually. The state-
of-the-art methods generally use a combination 
of hand-crafted, dictionary and statistical tech-
niques to obtain a better result. However, statis-
tical  methods  need  a  large  corpus  segmented 
manually  beforehand.  Statistical  methods  and 
complex training methods are not appropriate in 
the context of under-resourced languages as the 
resources  needed to  implement  these  methods 
do not exist. For an under-resourced language, 
we seek segmentation methods that allow better 
exploitation of the limited resources. In our pre-
vious paper (Seng et al, 2009) we have indicat-
ed the  problems of  existing text  segmentation 
approaches  and  introduced  a  weighted  finite 
state  transducer  (WFST)  based  multiple  text 
segmentation algorithm.
Our approach is implemented using the AT & 
T FSM Toolkit (Mohri et al, 1998). The algo-
rithm is inspired with the work on the segmen-
tation of Arabic words (Lee et al, 2003).  The 
multiple segmentation of a sequence of charac-
ters is made using the composition of three con-
trollers.  Given  a  finite  list  of  words  we  can 
build a finite state transducer M (or word trans-
ducer) that, once composed with an acceptor I 
of the input string that represent a single charac-
ter  with  each  arc,  generates  a  lattice  of  the 
words that represent all of the possible segmen-
tations. To handle out-of-vocabulary entries, we 
make a model of any string of characters by a 
star closure operation over all the possible char-
acters.  Thus,  the  unknown  word  WFST  can 
parse any sequence of characters and generate a 
unique  unk word symbol. The word transducer 
can,  therefore,  be  described  in  terms  of  the 
WFST  operations  as  M  =  (WD   UNK)+?  
where WD is a WFST that represents the dictio-
nary  and  UNK represents  the  unknown  word 
WFST. Here,  and + are the union and Kleene?  
?+? closure operations. A language model L is 
used to score the lattice of all possible segmen-
tations obtained by the composition of our word 
transducer M and the input string I. A language 
model  of  any  order  can  be  represented  by  a 
WFST. In our case, it is important to note that 
only a simple uni-gram language model is used. 
The uni-gram model is estimated from a small 
training  corpus  segmented  automatically  into 
words  using  a  dictionary  based  method.  The 
composition  of  the  sequence  of  input  string  I 
2
with the word transducer M yields a transducer 
that represents all possible segmentations. This 
transducer is then composed with the language 
model  L,  resulting  in  a  transducer  that  repre-
sents  all  possible  segmentations  for  the  input 
string  I,  scored  according  to  L.  The  highest 
scoring paths of the compound transducer is the 
segmentation m that can be defined as:
P ?m ?=maxP ?mk ?  
The segmentation procedure can then be ex-
pressed formally as:
 m=bestpath ? I?M?L ?
where ? is the composition operator. The N-
best segmentations are obtained by decoding the 
final lattice to output the N-best highest scoring 
paths and will be used for the N-gram count.
3 Experimental Setup
3.1 Language Modeling
First,  it  is  important  to  note  that  Vietnamese 
texts are naturally segmented into syllables (not 
words). Each  syllable  tends  to  have  its  own 
meaning and thus  a  strong identity.  However, 
the  Vietnamese  monosyllable  is  not  automati-
cally a word as we would define a word in Eng-
lish. Often, two syllables go together to form a 
single word, which can be identified by the way 
it  functions  grammatically  in  a  sentence.  To 
have a word-based language model, word seg-
mentation would, therefore, be a must in Viet-
namese.
A Vietnamese training corpus that contains 3 
millions sentences from broadcast news domain 
has been used in this experiment. A Vietnamese 
dictionary of 30k words has been used both for 
the  segmentation  and  counting  the  N-grams. 
Therefore, in the experiments, the ASR vocabu-
lary always remains the same and only the lan-
guage model is changing. The segmentation of 
the  corpus  with  dictionary  based,  ?longest 
matching? unique segmentation method gives a 
corpus  of  46  millions  words.  A  development 
corpus of 1000 sentences, which has been seg-
mented automatically to obtain 44k words, has 
been used to evaluate the tri-gram hits and the 
perplexity.  The performance of  each language 
model  produced will  be evaluated in  terms of 
the tri-gram hits and perplexity on the develop-
ment corpus and in terms of ASR performance 
on a separate speech test set (different from the 
development set).
First of all, a language model named lm_1 is 
trained using the SRILM toolkit (Stolcke 2002) 
from  the  first  best  segmentation  (Segmul1), 
which has the highest scoring paths (based on 
the transducer explained in section 2) of  each 
sentence in the whole corpus. Then,  additional 
language  models  have  been  trained  using  the 
corpus  segmented  with  N-best  segmentation: 
the number of N-best segmentations to generate 
for each sentence is fixed to 2, 5, 10, 50, 100 
and 1000. The resulting texts are named accord-
ingly  as  Segmul2,  Segmul5,  Segmul10,  Seg-
mul50,  Segmul100,  Segmul1000.  Using  these 
as  training  data,  we  have  developed  different 
language models. Note that a tri-gram that ap-
pears several times in multiple segmentations of 
a single sentence has a count set to one. 
3.2 ASR System
Our automatic speech recognition systems use 
the CMU?s Sphinx3 decoder. The decoder uses 
Hidden Markov Models (HMM) with continu-
ous  output  probability  density  functions.  The 
model topology is a 3-state, left-to-right HMM 
with 16 Gaussian mixtures per state.  The pre-
processing of the system consists of extracting a 
39 dimensional  features vector  of  13 MFCCs, 
the  first  and  second  derivatives.  The  CMU?s 
SphinxTrain has been used to train the acoustic 
models used in our experiment. 
The  Vietnamese  acoustic  modeling  training 
corpus is  made up of  14 hours  of  transcribed 
read  speech.  More  details  on  the  automatic 
speech recognition system for Vietnamese lan-
guage can be found in (Le et al, 2008). While 
the evaluation metric WER (Word Error Rate) 
is  generally used to evaluate and compare the 
performance  of  the  ASR  systems,  this  metric 
does not fit well for unsegmented languages be-
cause the errors introduced during the segmen-
tation of the references and the output hypothe-
sis may prevent a fair comparison of different 
ASR system outputs.  We,  therefore,  used  the 
Syllable Error Rate (SER) as Vietnamese text is 
composed  of  syllables  naturally  separated  by 
white space. The automatic speech recognition 
is  done  on  a  test  corpus  of  270  utterances 
(broadcast news domain). 
3
4 Statistical  Analysis  of  N-grams  in 
Multiple Text Segmentation
The  change  in  the  N-gram  count  that  results 
from  multiple  segmentation  is  two  fold:  first 
there is a boosting of the counts of the N-grams 
that  are  already found with the first  best  seg-
mentation,  and  secondly  new  N-grams  are 
added.  As  we have made a  closed-vocabulary 
counting, there are no new uni-grams resulting 
from  multiple segmentation. For the counting, 
the SRILM toolkit  (Stolcke 2002) is used set-
ting the -gtnmin option to zero so that all the N-
gram counts can be considered.
Figure  1  shows  the  distribution  of  tri-gram 
counts for the unique and multiple segmentation 
of the training corpus.  It  can be seen that  the 
majority  of  the  tri-grams  have  counts  in  the 
range of one to three.
Figure 1: Distribution of tri-gram counts
The boosting (the counts of the tri-grams that 
are already found with the first best segmenta-
tion) effect of the multiple segmentation is indi-
cated in table 1. We can see from the table that 
Segmul2, for example, reduced the  number of 
rare tri-grams (count range 1-3) from 19.04 to 
16.15  million.  Consequently,  the  ratio  of  rare 
tri-grams to all tri-grams that are in Segmul1 is 
reduced  from 94% (19.04/20.31*100)  of  Seg-
mul1  only  to  79%  (15.96/20.31*100)  by  the 
boosting effect of Segmul1000, which increased 
the number of tri-grams with count range of 4-9 
from 0.91M to 3.34M. This implies, in the con-
text of under-resourced languages, that multiple 
segmentation  is  boosting  the  N-gram  counts. 
However, one still has to verify if this boosting 
is relevant or not for ASR.
Multiple
Seg.
Counts Range
1?3 
(M)
4-9 
(M) 
10-99
(M)
100-999
(M)
?1000
(M)
Segmul1 19.04 0.91 0.34 0.016 0.00054
Segmul2 16.15 3.23 0.89 0.043 0.0017
Segmul5 16.06 3.28 0.92 0.045 0.0017
Segmul10 16.03 3.30 0.93 0.045 0.0017
Segmul50 15.99 3.33 0.95 0.046 0.0017
Segmul100 15.98 3.33 0.95 0.046 0.0017
Segmul1000 15.96 3.34 0.96 0.046 0.0017
Table 1. boosting tri-gram counts 
We have also analyzed the statistical behav-
ior of the newly added tri-grams with regard to 
their count distribution (see figure 2). As we can 
see from the figure, the distribution of the new 
tri-grams is somehow similar to the distribution 
of the whole tri-grams that is indicated in figure 
1. 
As  shown  in  table  2,  the  total  number  of 
newly  added  tri-grams  is  around  15  millions. 
We can see from the table that the rate of new 
tri-gram contribution of each segmentation in-
creases as N increases in the N-best segmenta-
tion. However, as it is indicated in figure 2, the 
major  contribution  is  in  the  area  of  rare  tri-
grams.
Figure 2: Distribution of new tri-gram counts
1?3 4?9 10?99 100-999 ?1000
0
5,000,000
10,000,000
15,000,000
20,000,000
25,000,000
30,000,000
35,000,000
40,000,000
Segmul1
Segmul2
Segmul5
Segmul10
Segmul50
Segmul100
Segmul1000
Counts Range
No
. o
f tr
i-g
ra
m s
1?3 4?9 10?99 100-999 ?1000
0
2,000,000
4,000,000
6,000,000
8,000,000
10,000,000
12,000,000
14,000,000
16,000,000
Segmul 2
Segmul 5
Segmul 10
Segmul 50
Segmul 100
Segmul 1000
Counts Range
Nu
mb
er
 of
 tri
-g
ra
ms
4
Mul. Segmentation No. %
Segmul2 4,125,881 26,05
Segmul5 8,249,684 52,09
Segmul10 10,355,433 65,39
Segmul50 13,002,700 82,11
Segmul100 14,672,827 92,65
Segmul1000 15,836,120 100,0
Table 2. tri-gram contribution of multiple seg-
mentation
5 Experimental Results
In this section we present the various language 
models  we  have  developed  and  their  perfor-
mance in terms of perplexity, tri-gram hits and 
ASR performance (syllable error rate).
We use the results obtained with the method 
presented in (Seng et al, 2009) as baseline. This 
method  consists  in  re-estimating  the  N-gram 
counts  using the  multiple  segmentation of  the 
training data and add one to the count of a tri-
gram that appears several times in multiple seg-
mentations of a single sentence. These baseline 
results  are  presented  in  Table  3. The  results 
show an increase of the tri-gram coverage and 
slight improvements of the ASR performance.
Language 
Models
3gs(M) 3g hit(% ) Ppl SER 
Lm_1 20.31 46.9 126.6 27
lm_2 24.06 48.6 118.1 26.2
Lm_5 28.92 49.2 125.9 27
Lm_10 32.82 49.4 129.0 26.5
Lm_50 34.20 49.7 133.4 26.7
lm_100 34.93 49.7 134.8 26.9
lm_1000 36.11 49.88 137.7 27.3
Table 3. Results of experiments using the base-
line method presented in (Seng et al, 2009)
5.1 Separate  effect  of  boosting  tri-gram 
counts
To see  the  effect  of  boosting  tri-gram counts 
only,  we  have  updated  the  counts  of  the  tri-
grams  obtained  from  the  1-best  segmentation 
(baseline  approach)  by  the  tri-gram counts  of 
different  multiple  segmentations.  Note  that  no 
new tri-grams are added here, and we evaluate 
only the effect  and,  therefore,  the tri-gram hit 
remains the same as that of lm_1.
We have then developed different  language 
models using the uni-gram and bi-gram counts 
of  the first  best  segmentation and the updated 
trigram counts after multiple segmentation. The 
performance of the language models have been 
evaluated in terms of perplexity and their contri-
bution  to  the  performance  improvement  of  a 
speech recognition system.  We have observed 
(detailed  results  are  not  reported  here)  that 
boosting only the tri-gram counts has not con-
tributed any improvement in the performance of 
the  language  models.  The  reason  is  probably 
due  to  the  fact  that  simply  updating  tri-gram 
counts without updating the uni-grams and the 
bi-grams lead to a biased and inefficient LM.
5.2 Separate effect of new tri-grams
To  explore  the  contributions  of  only  newly 
added tri-grams, we have added their counts to 
the N-gram counts of Segmul1. It is important 
to note that the model obtained in that case is 
different from the baseline model whose results 
are presented in Table 3 (the counts of the tri-
grams already found in the unique segmentation 
are different between models). As it is presented 
in table 4, including only the newly added tri-
grams  consistently  improved  tri-gram  hits, 
while the improvement in perplexity stopped at 
Segmul10. Moreover, the use of only new tri-
grams do not reduce the speech recognition er-
ror rate.
Language 
Models
3gs
(M)
3g 
hit(% )
ppl SER 
lm_1 20.3 46.9 126.6 27
lm_2_new 24.4 48.7 119.1 26.9
lm_5_new 28.6 49.0 122.5 27.8
lm_10_new 30.7 49.2 124.2 27.9
lm_50_new 33.3 49.4 126.8 27.8
lm_100_new 35 49.8 127.8 28
lm_1000_new 36.1 49.9 129.7 27.9
Table 4. Contributions of new tri-grams
5.3 Pooling unique and multiple segmenta-
tion models
We have developed language models by pooling 
unique and multiple segmentation models alto-
gether.  For  instance,  all  the  N-grams of  lm_5 
multiple  segmentation  are  pooled  with  all  N-
grams of lm_1 unique segmentation before esti-
mating the language model probabilities. In oth-
er  words,  ngram-count  command is  used with 
multiple count files. The results are presented in 
table 5.
As it can be noted from table 5, we have got a 
significant  improvement  in  all  the  evaluation 
criteria  as  compared  with  the  performance  of 
lm_1 that has perplexity of 126.6, tri-gram hit 
5
of 46.91% and SER of 27. The best result ob-
tained (25.4) shows a 0.8 absolute SER reduc-
tion  compared  to  the  best  result  presented  in 
(Seng et al, 2009).
Language 
Models
3gs
(M)
3g 
hit(% )
ppl SER 
lm_1 20.31 46.9 126.6 27
lm_2+lm_1 24.4 48.7 120.9 25.4
lm_5+lm_1 29.12 49.2 123.2 26.2
lm_10+lm_1 31.4 49.4 124.2 26
lm_50+lm_1 34.3 49.7 126 26
lm_100+lm_1 35 49.8 126.5 26.2
lm_1000+lm_1 36.2 49.9 128 26.2
Table 5. Performance with pooling
5.4 Cutting off rare tri-grams
With  the  assumption  that  bad  N-grams  occur 
rarely, we cut off rare tri-grams from the counts 
in developing language models. We consider all 
tri-grams with a count of 1 to be rare. Our hope, 
here, is that using this cut off we will remove 
bad  N-grams  introduced  by  the  multiple  seg-
mentation approach, while keeping correct new 
N-grams in the model. Table 6 shows the per-
formance  of  the  language  models  developed 
with or without tri-gram cut off for the baseline 
method (the results presented on the lines indi-
cating All3gs are the same as the ones presented 
in Table 3) . 
Language models Evaluation Criteria
3gs
(M)
3g hit 
(%)
ppl SER
lm_1 All 3gs 20.31 46.91 126.6 27
Cut off 4.17 38.09 129.3 26.6
lm_2 All 3gs 24.06 48.6 118.1 26.2
Cut off 5.11 39.6 121.0 26.7
lm_5 All 3gs 28.92 49.2 125.9 27
Cut off 6.4 40.11 129.2 26.6
lm_10 All 3gs 32.82 49.41 129.0 26.5
Cut off 6.98 40.27 132.4 26.6
lm_50 All 3gs 34.20 49.68 133.4 26.7
Cut off 7.8 40.51 136.9 26.9
lm_100 All 3gs 34.93 49.74 134.8 26.9
Cut off 7.98 40.59 138.4 26.8
lm_1000 All 3gs 36.11 49.88 137.7 27.3
Cut off 8.33 40.71 141.3 26.8
Table 6. Performance with cut off.
The result shows that cutting off reduced the 
number of tri-grams highly (4 tri-grams over 5 
are removed in that case). It, therefore, reduces 
the  size  of  the  language  models  significantly. 
Although  the  results  obtained  are  not  conclu-
sive, a reduction of  recognition error rate has 
been  observed  in  four  out  of  the  seven  cases 
while the perplexity increased and the tri-gram 
hits decreased in all cases. 
5.5 Hybrid  of  pooling  and  cutting  off 
methods
As it has been already indicated, cutting off in-
creased the perplexity of  the language models 
and decreased the tri-gram hits. To reduce the 
negative  effect  of  cutting  off  on  tri-gram hits 
and  perplexity,  we  have  developed  language 
models using both pooling and cut off methods. 
We then cut off tri-grams of count 1 from the 
pooled N-grams. The result, as presented in ta-
ble 7, shows that we can gain significant reduc-
tion in recognition error rate and  improvement 
in tri-gram hits as compared to lm_1 that is de-
veloped with cut off, even if no improvement in 
perplexity is observed. 
The best  result  obtained (25.9) shows a 0.3 
absolute  SER reduction  compared  to  the  best 
system presented in (Seng et al, 2009).
Language Models 3gs
(M)
3g hit
(% )
ppl SE
R 
lm_1 (no cutoff) 20.3 46.9 126.6 27
lm_1 (cutoff) 4.2 38.1 129.3 26.6
lm_2+lm_1 (cutoff) 5.2 39.7 126.4 26.8
lm_5+lm_1 (cutoff) 6.4 40.2 129.5 25.9
lm_10+lm_1 (cutoff) 7.0 40.3 131.1 26.3
lm_50+lm_1 (cutoff) 7.8 40.5 133.5 26.4
lm_100+lm_1 (cutoff) 8.0 40.6 134.3 26.4
lm_1000+lm_1 (cutoff) 8.3 40.7 161.5 26.7
Table 7. Performance with hybrid method
6 Conclusion
The  two  major  contributions  of  multiple  seg-
mentation are generation of new N-grams and 
boosting N-gram counts of those found in first 
best  segmentation.  However,  it  also  produces 
bad N-grams that affect the performance of lan-
guage models. In this paper, we studied the con-
tribution  of  multiple  segmentation  approach 
more deeply and conducted experiments on effi-
cient solutions to minimize the effect of adding 
bad N-grams. Since only boosting the tri-gram 
counts  of  first  best  segmentation  and  adding 
only new tri-grams did not  reduce recognition 
error  rate,  we  have  proposed  to  pool  all  N-
grams of N-best  segmentations to that  of  first 
best  segmentation  and  got  a  significant  im-
provement in perplexity and tri-gram hits from 
6
which we obtained the maximum (0.8 absolute) 
reduction in recognition error rate. 
To   minimize  the  effect  of  adding  bad  N-
grams,  we  have  cut  off  rare  tri-grams in  lan-
guage modeling and got  reduction in  recogni-
tion error rate. The significant reduction of tri-
grams that  resulted  from the  cut  off  revealed 
that the majority of tri-grams generated by mul-
tiple  segmentation  have  counts  1.  Cutting  off 
such a big portion of the trigrams reduced tri-
gram hits and as a solution, we  proposed a hy-
brid of both pooling  and cutting off tri-grams 
from which we obtained a significant reduction 
in recognition error rate. 
It  is  possible  to  conclude  that  our  methods 
make the multiple segmentation approach more 
useful by minimizing the effect of bad N-grams 
that it generates and utilizing the contribution of 
different multiple segmentations. 
However,  we  still  see  rooms  for  improve-
ment.  A systematic selection of new tri-grams 
(for example, based on the probabilities of the 
N-grams and/or application of simple linguistic 
criteria  to  evaluate  the  usefulness  of  new tri-
grams), with the aim of reducing bad tri-grams, 
might lead to performance improvement. Thus, 
we will do experiments in this line. We will also 
apply these methods to other languages, such as 
Khmer.
References
Lee, Young-Suk, Papineni, Kishore, Roukos, Salim 
Emam,  Ossama  and  Hassan,  Hany.  2003.  Lan-
guage model based arabic word segmentation. In 
Proceedings of the ACL?03, pp. 399?406. 
Le,  Viet-Bac,  Besacier,  Laurent,  Seng,  Sopheap, 
Bigi,  Brigite and Do, Thi-Ngoc-Diep. 2008.  Re-
cent  advances  in  automatic  speech  recognition  
for vietnamese. SLTU?08, Hanoi Vietnam. 
Mohri,  Mehryar,  Fernando  C.  N.  Pereira,  and 
Michael Riley, ?A rational design for a weighted 
finite-state transducer library,? in Lecture Notes in 
Computer Science. Springer, 1998, pp. 144?158. 
Seng,  Sopheap,  Besacier,  Laurent,  Bigi,  Brigitte, 
Castelli,  Eric.  2009.  Multiple Text Segmentation  
for  Statistical  Language  Modeling. InterSpeech, 
Brighton, UK,  
Stolcke, Andreas.  2002. SRILM: an extensible lan-
guage  modeling  toolkit.  Proceedings  of  Interna-
tional Conference on Spoken Language Process-
ing, volume II, 901?904 . 129.88.65.115
7
Proceedings of the 6th Workshop on Statistical Machine Translation, pages 440?446,
Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational Linguistics
The LIGA (LIG/LIA) Machine Translation System for WMT 2011
Marion Potet1, Raphae?l Rubino2, Benjamin Lecouteux1, Ste?phane Huet2,
Herve? Blanchon1, Laurent Besacier1 and Fabrice Lefe`vre2
1UJF-Grenoble1, UPMF-Grenoble2
LIG UMR 5217
Grenoble, F-38041, France
FirstName.LastName@imag.fr
2Universite? d?Avignon
LIA-CERI
Avignon, F-84911, France
FirstName.LastName@univ-avignon.fr
Abstract
We describe our system for the news com-
mentary translation task of WMT 2011. The
submitted run for the French-English direction
is a combination of two MOSES-based sys-
tems developed at LIG and LIA laboratories.
We report experiments to improve over the
standard phrase-based model using statistical
post-edition, information retrieval methods to
subsample out-of-domain parallel corpora and
ROVER to combine n-best list of hypotheses
output by different systems.
1 Introduction
This year, LIG and LIA have combined their efforts
to produce a joint submission to WMT 2011 for the
French-English translation task. Each group started
by developing its own solution whilst sharing re-
sources (corpora as provided by the organizers but
also aligned data etc) and acquired knowledge (cur-
rent parameters, effect of the size of n-grams, etc.)
with the other. Both LIG and LIA systems are stan-
dard phrase-based translation systems based on the
MOSES toolkit with appropriate carefully-tuned se-
tups. The final LIGA submission is a combination
of the two systems.
We summarize in Section 2 the resources used
and the main characteristics of the systems. Sec-
tions 3 and 4 describe the specificities and report
experiments of resp. the LIG and the LIA system.
Section 5 presents the combination of n-best lists
hypotheses generated by both systems. Finally, we
conclude in Section 6.
2 System overview
2.1 Used data
Globally, our system1 was built using all the French
and English data supplied for the workshop?s shared
translation task, apart from the Gigaword monolin-
gual corpora released by the LDC. Table 1 sums up
the used data and introduces designations that we
follow in the remainder of this paper to refer to cor-
pora. Four corpora were used to build translation
models: news-c, euro, UN and giga, while three
others are employed to train monolingual language
models (LMs). Three bilingual corpora were de-
voted to model tuning: test09 was used for the de-
velopment of the two seed systems (LIG and LIA),
whereas test08 and testcomb08 were used to tune the
weights for system combination. test10 was finally
put aside to compare internally our methods.
2.2 LIG and LIA system characteristics
Both LIG and LIA systems are phrase-based trans-
lation models. All the data were first tokenized with
the tokenizer provided for the workshop. Kneser-
Ney discounted LMs were built from monolingual
corpora using the SRILM toolkit (Stolcke, 2002),
while bilingual corpora were aligned at the word-
level using GIZA++ (Och and Ney, 2003) or its
multi-threaded version MGIZA++ (Gao and Vogel,
2008) for the large corpora UN and giga. Phrase
table and lexicalized reordering models were built
with MOSES (Koehn et al, 2007). Finally, 14 fea-
tures were used in the phrase-based models:
1When not specified otherwise ?our? system refers to the
LIGA system.
440
CORPORA DESIGNATION SIZE (SENTENCES)
English-French Bilingual training
News Commentary v6 news-c 116 k
Europarl v6 euro 1.8 M
United Nation corpus UN 12 M
109 corpus giga 23 M
English Monolingual training
News Commentary v6 mono-news-c 181 k
Shuffled News Crawl corpus (from 2007 to 2011) news-s 25 M
Europarl v6 mono-euro 1.8 M
Development
newstest2008 test08 2,051
newssyscomb2009 testcomb09 502
newstest2009 test09 2,525
Test
newstest2010 test10 2,489
Table 1: Used corpora
? 5 translation model scores,
? 1 distance-based reordering score,
? 6 lexicalized reordering score,
? 1 LM score and
? 1 word penalty score.
The score weights were optimized on the test09 cor-
pus according to the BLEU score with the MERT
method (Och, 2003). The experiments led specifi-
cally with either LIG or LIA system are respectively
described in Sections 3 and 4. Unless otherwise
indicated, all the evaluations were performed using
case-insensitive BLEU and were computed with the
mteval-v13a.pl script provided by NIST. Ta-
ble 2 summarizes the differences between the final
configuration of the systems.
3 The LIG machine translation system
LIG participated for the second time to the WMT
shared news translation task for the French-English
language pair.
3.1 Pre-processing
Training data were first lowercased with the PERL
script provided for the campaign. They were also
processed in order to normalize a special French
form (named euphonious ?t?) as described in (Potet
et al, 2010).
The baseline system was built using a 4-gram LM
trained on the monolingual corpora provided last
year and translation models trained on news-c and
euro (Table 3, System 1). A significant improve-
ment in terms of BLEU is obtained when taking into
account a third corpus, UN, to build translation mod-
els (System 2). The next section describes the LMs
that were trained using the monolingual data pro-
vided this year.
3.2 Language model training
Target LMs are standard 4-gram models trained
on the provided monolingual corpus (mono-news-c,
mono-euro and news-s). We decided to test two dif-
ferent n-gram cut-off settings. The fist set has low
cut-offs: 1-2-3-3 (respectively for 1-gram, 2-gram,
3-gram and 4-gram counts), whereas the second one
(LM2) is more aggressive: 1-5-7-7. Experiment re-
sults (Table 3, Systems 3 and 4) show that resorting
to LM2 leads to an improvement of BLEU with re-
spect to LM1. LM2 was therefore used in the sub-
sequent experiments.
441
FEATURES LIG SYSTEM LIA SYSTEM
Pre-processing Text lowercased Text truecasedNormalization of French euphonious
?t?
Reaccentuation of French words start-
ing with a capital letter
LM Training on mono-news-c, news-s and
mono-euro
Training on mono-news-c and news-s
4-gram models 5-gram models
Translation model
Training on news-c, euro and UN Training on 10 M sentence pairs se-
lected in news-c, euro, UN and giga
Phrase table filtering
Use of -monotone-at-punctuation op-
tion
Table 2: Distinct features between final configurations retained for the LIG and LIA systems
3.3 Translation model training
Translation models were trained from the parallel
corpora news-c, euro and UN. Data were aligned
at the word-level and then used to build standard
phrase-based translation models. We filtered the ob-
tained phrase table using the method described in
(Johnson et al, 2007). Since this technique drasti-
cally reduces the size of the phrase table, while not
degrading (and even slightly improving) the results
on the development and test corpora (System 6), we
decided to employ filtered phrase tables in the final
configuration of the LIG system.
3.4 Tuning
For decoding, the system uses a log-linear com-
bination of translation model scores with the LM
log-probability. We prevent phrase reordering over
punctuation using the MOSES option -monotone-at-
punctuation. As the system can be beforehand tuned
by adjusting the log-linear combination weights on
a development corpus, we used the MERT method
(System 5). Optimizing weights according to BLEU
leads to an improvement with respect to the sys-
tem with MOSES default value weights (System 5
vs System 4).
3.5 Post-processing
We also investigated the interest of a statistical
post-editor (SPE) to improve translation hypotheses.
About 9,000 sentences extracted from the news do-
main test corpora of the 2007?2009 WMT transla-
tion tasks were automatically translated by a sys-
tem very similar to that described in (Potet et al,
2010), then manually post-edited. Manual correc-
tions of translations were performed by means of the
crowd-sourcing platform AMAZON MECHANICAL
TURK2 ($0.15/sent.). These collected data make
a parallel corpus whose source part is MT output
and target part is the human post-edited version of
MT output. This are used to train a phrase-based
SMT (with Moses without the tuning step) that au-
tomatically post-edit the MT output. That aims at
learning how to correct translation hypotheses. Sys-
tem 7 obtained when post-processing MT 1-best out-
put shows a slight improvement. However, SPE was
not used in the final LIG system since we lacked
time to apply SPE on the N-best hypotheses for the
development and test corpora (the N-best being nec-
essary for combination of LIG and LIA systems).
Ths LIGA submission is thus a constrained one.
3.6 Recasing
We trained a phrase-based recaser model on the
news-s corpus using the provided MOSES scripts
and applied it to uppercase translation outputs. A
common and expected loss of around 1.5 case-
sensitive BLEU points was observed on the test cor-
pus (news10) after applying this recaser (System 7)
with respect to the score case-insensitive BLEU pre-
viously measured.
2http://www.mturk.com/mturk/welcome
442
? SYSTEM DESCRIPTION BLEU SCORE
test09 test10
1 Training: euro+news-c 24.89 26.01
2 Training: euro+news-c+UN 25.44 26.43
3 2 + LM1 24.81 27.19
4 2 + LM2 25.37 27.25
5 4 + MERT on test09 26.83 27.53
6 5 + phrase-table filtering 27.09 27.64
7 6 + SPE 27.53 27.74
8 6 + recaser 24.95 26.07
Table 3: Incremental improvement of the LIG system in
terms of case-insensitive BLEU (%), except for line 8
where case-sensitive BLEU (%) are reported
4 The LIA machine translation system
This section describes the particularities of the MT
system which was built at the LIA for its first partic-
ipation to WMT.
4.1 System description
The available corpora were pre-processed using
an in-house script that normalizes quotes, dashes,
spaces and ligatures. We also reaccentuated French
words starting with a capital letter. We significantly
cleaned up the crawled parallel giga corpus, keeping
19.3 M of the original 22.5 M sentence pairs. For ex-
ample, sentence pairs with numerous numbers, non-
alphanumeric characters or words starting with cap-
ital letters were removed. The whole training ma-
terial is truecased, meaning that the words occur-
ing after a strong punctuation mark were lowercased
when they belonged to a dictionary of common all-
lowercased forms; the others were left unchanged.
The training of a 5-gram English LM was re-
strained to the news corpora mono-news-c and news-
s that we consider large enough to ignore other data.
In order to reduce the size of the LM, we first limited
the vocabulary of our model to a 1 M word vocabu-
lary taking the most frequent words in the news cor-
pora. We also resorted to cut-offs to discard infre-
quent n-grams (2-2-3-5 thresholds on 2- to 5-gram
counts) and uses the SRILM option prune, which
allowed us to train the LM on large data with 32 Gb
RAM.
Our translation models are phrase-based models
(PBMs) built with MOSES with the following non-
default settings:
? maximum sentence length of 80 words,
? limit on the number of phrase translations
loaded for each phrase fixed to 30.
Weights of LM, phrase table and lexicalized re-
ordering model scores were optimized on the devel-
opment corpus thanks to the MERT algorithm.
Besides the size of used data, we experimented
with two advanced features made available for
MOSES. Firstly, we filtered phrase tables using the
default setting -l a+e -n 30. This dramatically
reduced phrase tables by dividing their size by a
factor of 5 but did not improve our best configu-
ration from the BLEU score perspective (Table 4,
line 1); the method was therefore not kept in the
LIA system. Secondly, we introduced reordering
constraints in order to consider quoted material as
a block. This method is particularly useful when ci-
tations included in sentences have to be translated.
Two configurations were tested: zone markups in-
clusion around quotes and wall markups inclusion
within zone markups. However, the measured gains
were finally too marginal to include the method in
the final system.
4.2 Parallel corpus subsampling
As the only news parallel corpus provided for the
workshop contains 116 k sentence pairs, we must
resort to parallel out-of-domain corpora in order to
build reliable translation models. Information re-
trieval (IR) methods have been used in the past to
subsample parallel corpora. For example, Hilde-
brand et al (2005) used sentences belonging to the
development and test corpora as queries to select the
k most similar source sentences in an indexed paral-
lel corpus. The retrieved sentence pairs constituted
a training corpus for the translation models.
The RALI submission for WMT10 proposed a
similar approach that builds queries from the mono-
lingual news corpus in order to select sentence pairs
stylistically close to the news domain (Huet et al,
2010). This method has the major interest that it
does not require to build a new training parallel
corpus for each news data set to translate. Fol-
lowing the best configuration tested in (Huet et al,
443
2010), we index the three out-of-domain corpora us-
ing LEMUR3, and build queries from English news-s
sentences where stop words are removed. The 10 top
sentence pairs retrieved per query are selected and
added to the new training corpus if they are not re-
dundant with a sentence pair already collected. The
process is repeated until the training parallel cor-
pus reaches a threshold over the number of retrieved
pairs.
Table 4 reports BLEU scores obtained with the
LIA system using the in-domain corpus news-c and
various amounts of out-of-domain data. MERT was
re-run for each set of training data. The first four
lines display results obtained with the same num-
ber of sentence pairs, which corresponds to the
size of news-c appended to euro. The experiments
show that using euro instead of the first sentences of
UN and giga significantly improves BLEU scores,
which indicates the better adequacy of euro with re-
spect to the test10 corpus. The use of the IR method
to select sentences from euro, UN and giga leads to
a similar BLEU score to the one obtained with euro.
The increase of the collected pairs up to 3 M pairs
generates a significant improvement of 0.9 BLEU
point. A further rise of the amount of collected
pairs does not introduce a major gain since retriev-
ing 10 M sentence pairs only augments BLEU from
29.1 to 29.3. This last configuration which leads to
the best BLEU was used to build the final LIA sys-
tem. Let us note that 2 M, 3 M and 15 M queries
were required to respectively obtain 3 M, 5 M and
10 M sentence pairs because of the removal of re-
dundant sentences in the increased corpus.
For a matter of comparison, a system was also
built taking into account all the training material,
i.e. 37 M sentence pairs4. This last system is out-
performed by our best system built with IR and has
finally close performance to the one obtained with
news-c+euro relatively to the quantity of used data.
5 The system combination
System combination is based on the 500-best out-
puts generated by the LIA and the LIG systems.
3www.lemurproject.org
4For this experiment, the data were split into three parts
to build independent alignment models: news-c+euro, UN and
giga, and they were joined afterwards to build translation mod-
els.
USED PARALLEL CORPORA FILTERING
without with
news-c + euro (1.77 M) 28.1 28.0
news-c + 1.77 M of UN 27.2 -
news-c + 1.77 M of giga 27.1 -
news-c + 1.77 M with IR 28.2 -
news-c + 3 M with IR 29.1 29.0
news-c + 5 M with IR 28.8 -
news-c + 10 M with IR 29.3 29.2
All data 28.9 29.0
Table 4: BLEU (%) on test10 measured with the LIA
system using different training parallel corpora
They both used the MOSES option distinct, en-
suring that the hypotheses produced for a given sen-
tence are different inside an N-best list. Each N-best
list is associated with a set of 14 scores and com-
bined in several steps.
The first step takes as input lowercased 500-best
lists, since preliminary experiments have shown a
better behavior using only lowercased output (with
cased output, combination presents some degrada-
tions). The score combination weights are opti-
mized on the development corpus, in order to max-
imize the BLEU score at the sentence level when
N-best lists are reordered according to the 14 avail-
able scores. To this end, we resorted to the SRILM
nbest-optimize tool to do a simplex-based
Amoeba search (Press et al, 1988) on the error func-
tion with multiple restarts to avoid local minima.
Once the optimized feature weights are com-
puted independently for each system, N-best lists
are turned into confusion networks (Mangu et al,
2000). The 14 features are used to compute poste-
riors relatively to all the hypotheses in the N-best
list. Confusion networks are computed for each sen-
tence and for each system. In Table 5 we present
the ROVER (Fiscus, 1997) results for the LIA and
LIG confusion networks (LIA CNC and LIG CNC).
Then, both confusion networks computed for each
sentence are merged into a single one. A ROVER
is applied on the combined confusion network and
generates a lowercased 1-best.
The final step aims at producing cased hypothe-
ses. The LIA system built from truecased corpora
achieved significantly higher performance than the
444
LIG LIA LIG CNC LIA CNC LIG+LIA
case-insensitive test10 27.6 29.3 28.1 29.4 29.7
BLEU test11 28.5 29.4 28.5 29.3 29.9
case-sensitive test10 26.1 28.4 27.0 28.4 28.7
BLEU test11 26.9 28.4 27.5 28.4 28.8
Table 5: Performance measured before and after combining systems
LIG system trained on lowercased corpora (Table 5,
two last lines). In order to get an improvement when
combining the outputs, we had to adopt the follow-
ing strategy. The 500-best truecased outputs of the
LIA system are first merged in a word graph (and
not a mesh lattice). Then, the lowercased 1-best
previously obtained with ROVER is aligned with the
graph in order to find the closest existing path, which
is equivalent to matching an oracle with the graph.
This method allows for several benefits. The new
hypothesis is based on a ?true? decoding pass gener-
ated by a truecased system and discarded marginal
hypotheses. Moreover, the selected path offers a
better BLEU score than the initial hypothesis with
and without case. This method is better than the one
which consists of applying the LIG recaser (section
3.6) on the combined (un-cased) hypothesis.
The new recased one-best hypothesis is then used
as the final submission for WMT. Our combination
approach improves on test11 the best single sys-
tem by 0.5 case-insensitive BLEU point and by 0.4
case-sensitive BLEU (Table 5). However, it also in-
troduces some mistakes by duplicating in particular
some segments. We plan to apply rules at the seg-
ment level in order to reduce these artifacts.
6 Conclusion
This paper presented two statistical machine trans-
lation systems developed at different sites using
MOSES and the combination of these systems. The
LIGA submission presented this year was ranked
among the best MT system for the French-English
direction. This campaign was the first shot for LIA
and the second for LIG. Beside following the tradi-
tional pipeline for building a phrase-based transla-
tion system, each individual system led to specific
works: LIG worked on using SPE as post-treatment,
LIA focused on extracting useful data from large-
sized corpora. And their combination implied to ad-
dress the interesting issue of matching results from
systems with different casing approaches.
WMT is a great opportunity to chase after perfor-
mance and joining our efforts has allowed to save
considerable amount of time for data preparation
and tuning choices (even when final decisions were
different among systems), yet obtaining very com-
petitive results. This year, our goal was to develop
state-of-the-art systems so as to investigate new ap-
proaches for related topics such as translation with
human-in-the-loop or multilingual interaction sys-
tems (e.g. vocal telephone information-query di-
alogue systems in multiple languages or language
portability of such systems).
References
Jonathan G. Fiscus. 1997. A post-processing system to
yield reduced word error rates:recognizer output vot-
ing error reduction (ROVER). In Proceedings of the
IEEE Workshop on Automatic Speech Recognition and
Understanding, pages 347?354, Santa Barbara, CA,
USA.
Qin Gao and Stephan Vogel. 2008. Parallel implemen-
tations of word alignment tool. In Proceedings of the
ACL Workshop: Software Engineering, Testing, and
Quality Assurance for Natural Language Processing,
pages 49?57, Columbus, OH, USA.
Almut Silja Hildebrand, Matthias Eck, Stephan Vogel,
and Alex Waibel. 2005. Adaptation of the translation
model for statistical machine translation based on in-
formation retrieval. In Proceedings of the 10th confer-
ence of the European Association for Machine Trans-
lation (EAMT), Budapest, Hungary.
Ste?phane Huet, Julien Bourdaillet, Alexandre Patry, and
Philippe Langlais. 2010. The RALI machine trans-
lation system for WMT 2010. In Proceedings of the
ACL Joint 5th Workshop on Statistical Machine Trans-
lation and Metrics (WMT), Uppsala, Sweden.
Howard Johnson, Joel Martin, George Foster, and Roland
445
Kuhn. 2007. Improving translation quality by dis-
carding most of the phrasetable. In Proceedings of
the Joint Conference on Empirical Methods in Nat-
ural Language Processing and Computational Natu-
ral Language Learning (EMNLP-CoNLL), pages 967?
975, Prague, Czech Republic, jun.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In Proceed-
ings of the 45th Annual Meeting of the Association
for Computational Linguistics (ACL), Companion Vol-
ume, pages 177?180, Prague, Czech Republic, June.
Lidia Mangu, Eric Brill, and Andreas Stolcke. 2000.
Finding consensus in speech recognition: Word error
minimization and other applications of confusion net-
works. Computer Speech and Language, 14(4):373?
400.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19?51.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proceedings of
the 41st Annual Meeting on Association for Computa-
tional Linguistics (ACL), Sapporo, Japan.
Marion Potet, Laurent Besacier, and Herve? Blanchon.
2010. The LIG machine translation for WMT 2010.
In Proceedings of the ACL Joint 5th Workshop on Sta-
tistical Machine Translation and Metrics (WMT), Up-
psala, Sweden.
William H. Press, Brian P. Flannery, Saul A. Teukolsky,
and William T. Vetterling. 1988. Numerical Recipes
in C: The Art of Scientific Computing. Cambridge
University Press.
Andreas Stolcke. 2002. SRILM ? an extensible lan-
guage modeling toolkit. In Proceedings of the 7th In-
ternational Conference on Spoken Language Process-
ing (ICSLP), Denver, CO, USA.
446
JEP-TALN-RECITAL 2012, Atelier TALAf 2012: Traitement Automatique des Langues Africaines, pages 53?62,
Grenoble, 4 au 8 juin 2012. c?2012 ATALA & AFCP
Analyse des performances de mod?les de langage sub-lexicale
pour des langues peu-dot?es ? morphologie riche
Hadrien Gelas1, 2 Solomon Teferra Abate2
Laurent Besacier2 Fran?ois Pellegrino1
(1) Laboratoire Dynamique Du Langage, CNRS - Universit? de Lyon, France
(2) Laboratoire Informatique de Grenoble, CNRS - Universit? Joseph Fourier Grenoble 1, France
{hadrien.gelas, francois.pellegrino}@univ-lyon2.fr
{solomon.abate, laurent.besacier@imag.fr}
R?SUM?
Ce papier ?tudie l?impact de l?utilisation d?unit?s sous-lexicales sur les performances d?un syst?me
de RAP pour deux langues africaines peu-dot?es et morphologiquement riches (l?amharique et le
swahili). Deux types de sous-unit?s sous-lexicales sont consider?s : la syllabe et le morph?me,
ce dernier ?tant obtenu de mani?re supervis?e ou non-supervis?e. La reconstruction en mots
? partir de sorties de RAP en syllabes ou morph?mes est aussi prise en compte. Pour les deux
langues, les meilleurs r?sultats sont obtenus avec les morph?mes non-supervis?s. Le taux
d?erreur de mots est grandement r?duit pour la reconnaissance de l?amharique dont les donn?es
d?entrainement du LM sont tr?s faibles (2,3M de mots). Les scores pour la RAP du swahili sont
aussi am?lior?s (28M de mots pour l?entrainement). Il est ausi pr?sent? une analyse d?taill?e de
la reconstruction des mots hors vocabulaires, un pourcentage important de ceux-ci (jusqu?? 75%
pour l?amharique) sont retrouv?s ? l?aide de mod?les de langage ? base de morph?mes et la
m?thode de reconstruction appropri?e.
ABSTRACT
Performance analysis of sub-word language modeling for under-resourced languages with
rich morphology : case study on Swahili and Amharic
This paper investigates the impact on ASR performance of sub-word units for two under-
resourced african languages with rich morphology (Amharic and Swahili). Two subword
units are considered : syllable and morpheme, the latter being obtained in a supervised or
unsupervised way. The important issue of word reconstruction from the syllable (or morpheme)
ASR output is also discussed. For both languages, best results are reached with morphemes
got from unsupervised approach. It leads to very significant WER reduction for Amharic ASR
for which LM training data is very small (2.3M words) and it also slightly reduces WER over
a Word-LM baseline for Swahili ASR (28M words for LM training). A detailed analysis of the
OOV word reconstruction is also presented ; it is shown that a high percentage (up to 75% for
Amharic) of OOV words can be recovered with morph-based language model and appropriate
reconstruction method.
MOTS-CL?S : Mod?le de langage, Morph?me, Hors vocabulaire, Langues peu-dot?es.
KEYWORDS: Language model, Morpheme, Out-of-Vocabulary , Under-resourced languages.
53
1 Introduction
Due to world?s globalisation and answering the necessity of bridging the numerical gap with
the developing world, speech technology for under-resourced languages is a challenging issue.
Applications and usability of such tools in developing countries are proved to be numerous
and are highlighted for information access in Sub-Saharan Africa (Barnard et al, 2010a,b),
agricultural information in rural India (Patel et al, 2010), or health information access by
community health workers in Pakistan (Kumar et al, 2011).
In order to provide a totally unsupervised and language independent methodology to develop an
automatic speech recognition (ASR) system, some particular language characteristics should be
taken into account. Such specific features as tones ((Lei et al, 2006) on Mandarin Chinese) or
writing systems without explicit word boundaries ((Seng et al, 2008) on Khmer) need a specific
methodology adaptation. This is especially true when dealing with under-resourced languages,
where only few data are available.
During recent years, many studies tried to deal with morphologically rich languages (whether
they are agglutinative, inflecting and compounding languages) in NLP (Sarikaya et al, 2009).
Such a morphology results in data sparsity and in a degraded lexical coverage with a similar
lexicon size than state-of-the-art speech recognition setup (as one for English). It yields high
Out-of-Vocabulary (OOV) rates and degrades Word-Error rate (WER) as each OOV words will not
be recognized but can also affect their surrounding words and strongly increase WER.
When the corpus size is limited, a common approach to overcome the limited lexical coverage
is to segment words in sub-word units (morphemes or syllables). Segmentation in morphemes
can be obtained in a supervised or unsupervised manner. Supervised approaches were mainly
used through morphological analysers built on carefully annotated corpora requiring impor-
tant language-specific knowledge (as in (Ar?soy et al, 2009)). Unsupervised approaches are
language-independent and do not require any linguistic-knowledge. In (Kurimo et al, 2006),
several unsupervised algorithms have been compared, including their own public method called
Morfessor ((Creutz et Lagus, 2005)) for two ASR tasks in Turkish and Finnish (see also (Hirsimaki
et al, 2009) for a recent review of morh-based approaches). The other sub-word type that is also
utilized for reducing high OOV rate is the syllable. Segmentation is mainly rule-based and was
used in (Shaik et al, 2011b) and (Shaik et al, 2011a), even if outperformed in WER by ASR
morpheme-based recognition for Polish and German.
In this work, we investigate those different methodologies and see how to apply them for two
different speech recognition tasks : read speech ASR in Amharic and broadcast speech trans-
cription in Swahili. These tasks represents two different profiles of under-resourced languages
cases. Amharic with an acoustic model (AM) trained on 20h of read-speech but limited text data
(2.3M) and on the opposite, Swahili with a weaker acoustic model (12h of broadcast news from
internet mixing genre and quality) but a more robust LM (28M words of web-mining news, still
without any adaptation to spoken broadcast news). If such study on sub-unit has already been
conducted on Amharic (Pellegrini et Lamel, 2009), no prior work are known to us for Swahili.
But, the main goal of this study is to better understand what does really impact performance
of ASR using sub-word unit through a comparison of different methodologies. Both supervised
and unsupervised segmentation strategies are explored as well as different approaches to tag
segmentation.
54
The next section describes the target languages and the available corpora. Then, we introduce
several segmentation approaches in section 3. Section 4 presents the analysis of experimental
results for Swahili and Amharic while section 5 concludes this work.
2 Experiment description
2.1 Languages
Amharic is a Ethio-Semitic language from the Semitic branch of the Afroasiatic super family. It
is related to Hebrew, Arabic, and Syrian. According to the 1998 census, it is spoken by over
17 million people as a first language and by over 5 million as a second language throughout
Ethiopia. Amharic is also spoken in other countries such as Egypt, Israel and the United States.
It has its own writing system which is syllabary. It exhibits non-concatenative, inflectional and
derivational morphology. Like other Semitic languages such as Arabic, Amharic exhibits a root-
pattern morphological phenomenon. Case, number, definiteness, and gender-marking affixes
inflect nouns. Some adverbs can be derived from adjectives but adverbs are not inflected. Nouns
are derived from other basic nouns, adjectives, stems, roots, and the infinitive form of a verb is
obtained by affixation and intercalation.
Swahili is a Bantu language often used as a vehicular language in a wide area of East Africa. It
is not only the national language of Kenya and Tanzania but it is also spoken in different parts
of Democratic Republic of Congo, Mozambique, Somalia, Uganda, Rwanda and Burundi. Most
estimations give over 50 million speakers (with only less than 5 million native speakers). It has
many typical Bantu features, such as noun class and agreement systems and complex verbal
morphology. Structurally, it is often considered as an agglutinative language (Marten, 2006).
2.2 Speech corpora description
Both Amharic and a small part of Swahili training audio corpora were collected following the
same protocol. Texts were extracted from news websites and segmented by sentence. Native
speakers were recorded using a self-paced reading interface (with possible rerecordings). The
Amharic speech corpus (Abate et al, 2005) consists of 20 hours of training speech collected
from 100 speakers who read a total of 10,850 sentences. Swahili corpus corresponds to 2 hours
and a half read by 5 speakers (3 male and 2 female) along with almost 10 hours of web-mining
broadcast news representing various types of recording quality (noisy speech, telephone speech,
studio speech) and speakers. They were transcribed using a collaborative transcription process
based on the use of automatic pre-transcriptions to increase productivity gains (See details in
(Gelas et al, 2012)). Test corpora are made of 1.5 hours (758 sentences) of read speech for
Amharic and 2 hours (1,997 sentences) of broadcast news for Swahili.
55
2.3 Text corpora description
We built all statistical N-gram language model (LM) using the SRI 1 language model toolkit.
Swahili text corpus is made of data collected from 12 news websites (over 28M words). To
generate a pronunciation dictionary, we extracted the 65k most frequent words from the text
corpus and automatically created pronunciations taking benefit of the regularity of the grapheme
to phoneme conversion in Swahili. The same methodology and options have been applied to all
sub-words LM. For Amharic, we have used the data (2.3M words text) described in (Tachbelie
et al, 2010).
3 Segmenting text data
3.1 Unsupervised morphemic segmentation
For the unsupervised word segmentation, we used a publicly available tool called Morfessor 2.
Its data-driven approach learns a sub-word lexicon from a training corpus of words by using a
Minimum Description Length (MDL) algorithm (Creutz et Lagus, 2005). It has been used with
default options and without any adaptation.
3.2 Supervised morphemic and syllabic segmentation
For Amharic, we used the manually-segmented text described in (Tachbelie et al, 2011a) to train
an FSM-based segmenter (a composition of morpheme transducer and 12gram consonant vowel
syllable-based language model) using the AT&T FSM Library (FiniteState Machine Library) and
GRM Library (Grammar Library)(Mohri et al, 1998). The trained segmenter with the language
model is applied to segment the whole text mentioned in (Tachbelie et al, 2010).
The supervised decomposition for Swahili is performed with the public Part-Of-Speech tagger
named TreeTagger 3. It is using the parameters available for Swahili to extract sub-word units.
As for as syllable segmentation is concerned, we designed rule-based algorithms following
structural and phonological restrictions of the respective languages.
3.3 Segmentation tagging and vocabulary size
While working on sub-word unit, one should think on how to incorporate the segmentation
information. Morphological information can be included within factored LM as in (Tachbelie
et al, 2011b) or directly as a full unit in the n-gram LM itself. By choosing the latter, the ASR
decoder output is a sequence of sub-word units and an additional step is needed to recover
1. www.speech.sri.com/projects/srilm/
2. The unit obtained with Morfessor is referred here as morpheme even if it do not automatically corresponds to the
linguistic definition of morpheme (the smallest semantically meaningful unit)
3. www.ims.uni-stuttgart.de/projekte/corplex/TreeTagger/DecisionTree
Tagger.html
56
words from sub-units. In (Diehl et al, 2011), a n-gram SMT-based morpheme-to-word conversion
approach is proposed.
In this work, we evaluate how the recognition performance is affected by different ways of
tagging the segmentation information straightly in the training text. In (Ar?soy et al, 2009), it is
noticed that this aspect need to be considered as it impacts WER. In (Guijarrubia et al, 2009),
a similar methodology is applied without reading any conclusion since a too small and easy
recognition task was performed.
Three distinct types of tagging are evaluated here :
? UNIT_AFX : A morpheme boundary (MB) is added on left and/or right side of segmentation
leaving the (so-called with Morfessor) root alone. To rebuild up to words, we reconnect every
units containing MB to the one next to it.
(ex. kiMB tabu? kitabu)
? UNIT_ALL : A MB tag is added on each side of segmentation, in other words, we add to the
lexicon the information to distinguish roots from their context (we can get up to four different
entries for a same root : ROOT, MBROOT, ROOTMB, MBROOTMB). To rebuild, we reconnect
every time two MB appearing consecutively.
(ex. kiMB MBtabu? kitabu)
? UNIT_POS : For syllables, we add to the unit the position of the syllable in the word.
(ex. 1ki 2ta 3bu? kitabu)
In table 1, it is shown that each choice has an influence on the size of the full text vocabulary
and thus on the lexical coverage of the 65k lexicon. As expected from a language with rich
morphology, the word baseline 65k lexicon shows a dramatically low lexical coverage (13.95%).
For the same text information, syllables logically reduce the size of vocabulary and got a full
theoretical lexical coverage without reaching the 65k limits, but with the cost of really short
length unit. Concerning both morpheme segmentation types, as expected the supervised approach
leads to a larger number of units than the unsupervised statistical approach, the latter leads to a
better theoretical lexical coverage. The average token length do not reduce much compared to
word unit as most frequent words are already short mono-morphemic grammatical words. The
influence of different tagging techniques is also shown on the same table. Detailed comments on
WER will be given in 4.2.
FullVoc 65k Cov. Token WER
LM (%) (%) length (%)
Word 100 13.95 5.5 35.7
Syl_Pos (V=27k) 5.79 100 2.0 51.7
Treetag_All 79.38 17.57 4.4 44.7
Treetag_Afx 78.61 17.74 4.4 43.3
Morf_All 45.24 30.83 5.3 34.8
Morf_Afx 38.07 36.64 5.3 35.4
TABLE 1 ? Swahili - Size of full text corpus vocabulary in comparison with a word level baseline
(FullVoc) ; lexical coverage of a 65k lexicon on the full vocabulary (65k Cov.) ; average token length
in character for the text corpus ; word error rate depending on the choice of unit and segmentation
tag (WER), all systems using 3gram LM and 65k lexicon except when specified
57
4 Results
4.1 ASR system description
We used SphinxTrain 4 toolkit from Sphinx project for building Hidden Markov Models (HMM)
based acoustic models (AMs) for Swahili. With the speech training database described in 2.2,
we trained a context-dependent model with 3,000 tied states. The acoustic models have 36 and
40 phones for Swahili and Amharic, respectively. We used the HDecode decoder of the HTK for
Amharic. The Amharic acoustic model is more precisely described in (Tachbelie et al, 2010).
4.2 Analysis of Sub-word units performance for Swahili
Comparing all results for Swahili broadcast speech transcription task (table 1), Morfessor based
segmentation ASR system is the only one, with 34.8% WER, performing significantly better than
the 35.7% word baseline. As in (Ar?soy et al, 2009) and (Hirsimaki et al, 2006), segmentation
based on a morphological analyser reaches lower results (43.3% WER) than words and unsu-
pervised based segmentation. Finally, rule-based syllabic system have the worst performance
with 51.7% WER. Those scores in table 1 gives a good indication on how to choose the most
performing unit. It seems that one need to balance and optimise two distinct criteria : n-gram
length coverage and lexical coverage.
The importance of n-gram length coverage can be seen with poor performance of too short
units, like syllables in this work. A syllable trigram (average 6.0 character-long) is approximately
equivalent to a word unigram in Swahili (average 5.5 character-long), thus such a short trigram
length is directly impacting ASR system performance even if lexical coverage is maximized
(100%). The importance to use higher order n-gram LM when dealing with short units is also
shown in (Hirsimaki et al, 2009). However, if a lattice rescoring framework is often used, it is
difficult to recover enough information if the first trigram pass do not perform well enough. It is
then recommended to directly implement the higher order n-gram LM in the decoder.
In the same time, a larger lexical coverage (lex.cov.), allows better performance if not used
with too short units as shows the difference of performance between word-based LM (13.95%
lex.cov. and 35.7% WER) and Morfessor-based LM (30.83% lex.cov. and 34.8% WER), both
having similar average token lengths.
Concerning the different tagging techniques, they have an impact on WER. The better choice
seems to be influenced by the lexical coverage. When lexical coverage is good enough (Morfessor-
based system), one can get advantage of having more different and precise contexts (tag on all
units, separating roots alone and roots with affixes in the vocabulary and on n-gram estimations),
whereas for low lexical coverage (TreeTagger-based system), having more various words is better
(tag only on affixes, regrouping all same roots together allowing more distinct units in the
lexicon).
4. cmusphinx.sourceforge.net/
58
4.3 Sub-word units performance for Amharic
For the read speech recognition task for Amharic, only the best performing systems are presented
in table 2. Similar trend is found concerning the tagging techniques (better systems are tagged
ALL for Morfessor and tagged AFX for FSM) and by the fact that Morfessor system outperforms
the others. Even if the unit length in Morfessor is 40% shorter than average word length, it
gets important benefits from a 100% lexical coverage of the training corpus. However, for this
task, the supervised segmentation (FSM) has better results than word baseline system. It can be
explained by a slightly increased lexical coverage and still a reasonable token length. Through
this task, we also considered several vocabulary sizes. Results show that WER greatly benefits
from sub-units in smaller lexicon tasks. Finally, as for Amharic sub-word units being notably
shorter than word units, we rescored output lattices from the trigram LM system with a 5gram
LM. It leads to an absolute WER decrease of 2.0% for Morfessor.
65k Cov. Token Word Error Rate (%)
LM (%) length 5K 20K 65K
Word_3g 30.79 8.3 52.4 29.6 15.9
FSM_Afx_3g 45.13 6.3 39.3 20.8 12.2
FSM_Afx_5g 45.13 6.3 39.1 20.3 11.4
Morf_All-3g 100 4.9 36.7 14.8 9.9
Morf_All-5g 100 4.9 34.9 12.6 7.9
TABLE 2 ? Amharic - Lexical coverage of a 65k lexicon on the full vocabulary (65k Cov.) ; average
token length in the whole text corpus ; word error rate depending on the choice of unit, segmentation
tag and vocabulary size
4.4 OOV benefits of using sub-word units
Making good use of sub-word units for ASR has been proved efficient in many research to
recognize OOV words over baseline word LMs (as in (Shaik et al, 2011a)). Table 3 presents
the different OOV rates considering both token and type for each LM (OOV morphemes for
Morfessor-based LM). We also present the proportion of correctly recognized words (COOV)
which were OOVs in the word baseline LM. Results show important OOV rate reduction and
correctly recognised OOV rate for both languages (Morfessor-based outputs). For Amharic, the
difference of COOV rate between each lexicon is correlated with the possible OOVs each system
can recognized.
Swahili obtain less benefits for COOV. It can be explained by the specificity of the broadcast news
task, leading to important OOV entity names or proper names (the 65k Morfessor-based lexicon
is still having 11.36% of OOV types). But if we consider only the OOVs that can possibly be
recognized (i.e. only those which are not also OOVs in the Morfessor-based lexicon), 36.04% of
them are rebuilt. Due to decoder limitations we restrained this study to a 65k lexicon, but for a
Swahili 200k word vocabulary we get a type OOV rate of 12.46% and still 10.28% with a full
vocab (400k). Those numbers are really close to those obtained with the 65k Morfessor lexicon
and could only be reached with the cost of more computational power and less robust LM. In the
59
OOV (%) OOV (%) COOV (%)
LM Token Type
Amharic
Word-5k 35.21 57.14 -
Word-20k 19.48 32.18 -
Word-65k 9.06 14.99 -
Morf_All-5k 13.67 40.58 33.76
Morf_All-20k 2.50 7.88 66.95
Morf_All-65k 0.12 2.81 75.30
Swahili
Word-65k 5.73 19.17 -
Morf_All-65k 3.67 11.36 8.77
TABLE 3 ? Amharic and Swahili - Token and type OOV rate in test reference transcriptions depending
on LM (OOV morphemes for Morfessor-based LM) ; correctly recognised baseline OOV words rate in
ASR outputs (COOV)
same time, growing Morfessor lexicon to 200k would be more advantageous as it reduces the
type OOV rate to 1.61%.
While using sub-word system outputs rebuilt to word level reduces OOV words, in contrary, it can
also generate non words by ungrammatical or non-sense concatenation. We checked the 5029
words generated by the best Amharic Morfessor output to see if they exist in the full training
text vocabulary. It appears that only 37 are non-words (33 after manual validation). Among
those 33, there were 26 isolated affixes and 7 illegal concatenations, all due to poor acoustic
estimation from the system. Considering this small amount of non-words and with no possibility
to retrieve good ones in lattices, we did not process to constraint illegal concatenation as in
(Ar?soy et Sara?lar, 2009).
5 Conclusion
We investigated the use of sub-word units in n-gram language modeling through different metho-
dologies. The best results are obtained using unsupervised segmentation with Morfessor. This
tool outperforms supervised methodologies (TreeTagger, FSM or rule-based syllables) because
the choice of sub-word units optimise two essential criteria which are n-gram length coverage
and lexical coverage. In the same time, it appears that the way one implements the segmentation
information affects the speech recognition performance. As expected, using sub-word units brings
major benefits to the OOV problem. It shows to be effective in two very different tasks for two
under-resourced African languages with rich morphology (one being highly inflectional, Amharic
and the other being agglutinative, Swahili). The Amharic read speech recognition task, get the
more advantages of it, since the word baseline LM suffers from data sparsity. But results are also
improved for a broadcast speech transcription task for Swahili.
60
R?f?rences
ABATE, S., MENZEL, W. et TAFILA, B. (2005). An Amharic speech corpus for large vocabulary
continuous speech recognition. In Interspeech, pages 67?76.
ARISOY, E., CAN, D., PARLAK, S., SAK, H. et SARA?LAR, M. (2009). Turkish broadcast news
transcription and retrieval. Audio, Speech, and Language Processing, IEEE Transactions on,
17(5):874?883.
ARISOY, E. et SARA?LAR, M. (2009). Lattice extension and vocabulary adaptation for Turkish
LVCSR. Audio, Speech, and Language Processing, IEEE Transactions on, 17(1):163?173.
BARNARD, E., DAVEL, M. et van HUYSSTEEN, G. (2010a). Speech technology for information
access : a South African case study. In AAAI Symposium on Artificial Intelligence, pages 22?24.
BARNARD, E., SCHALKWYK, J., van HEERDEN, C. et MORENO, P. (2010b). Voice search for develop-
ment. In Interspeech.
CREUTZ, M. et LAGUS, K. (2005). Unsupervised morpheme segmentation and morphology
induction from text corpora using morfessor 1.0. Rapport technique, Computer and Information
Science, Report A81, Helsinki University of Technology.
DIEHL, F., GALES, M., TOMALIN, M. et WOODLAND, P. (2011). Morphological decomposition in
Arabic ASR systems. Computer Speech & Language.
GELAS, H., BESACIER, L. et PELLEGRINO, F. (2012). Developments of swahili resources for an
automatic speech recognition system. In SLTU.
GUIJARRUBIA, V., TORRES, M. et JUSTO, R. (2009). Morpheme-based automatic speech recognition
of basque. Pattern Recognition and Image Analysis, pages 386?393.
HIRSIMAKI, T., CREUTZ, M., SIIVOLA, V., KURIMO, M., VIRPIOJA, S. et PYLKKONEN, J. (2006). Unlimi-
ted vocabulary speech recognition with morph language models applied to Finnish. Computer
Speech & Language.
HIRSIMAKI, T., PYLKKONEN, J. et KURIMO, M. (2009). Importance of high-order n-gram models in
morph-based speech recognition. Audio, Speech, and Language Processing, IEEE Transactions on,
17(4):724?732.
KUMAR, A., TEWARI, A., HORRIGAN, S., KAM, M., METZE, F. et CANNY, J. (2011). Rethinking speech
recognition on mobile devices. In IUI4DR. ACM.
KURIMO, M., CREUTZ, M., VARJOKALLIO, M., ARISOY, E. et SARACLAR, M. (2006). Unsupervised
segmentation of words into morphemes?morpho challenge 2005, application to automatic
speech recognition. In Interspeech.
LEI, X., SIU, M., HWANG, M., OSTENDORF, M. et LEE, T. (2006). Improved tone modeling for
Mandarin broadcast news speech recognition. In Interspeech.
MARTEN, L. (2006). Swahili. In BROWN, K., ?diteur : The Encyclopedia of Languages and Linguistics,
2nd ed., volume 12, pages 304?308. Oxford : Elsevier.
MOHRI, M., PEREIRA, F. et RILEY, M. (1998). A rational design for a weighted finite-state
transducer library. In Lecture Notes in Computer Science, pages 144?158. Springer.
PATEL, N., CHITTAMURU, D., JAIN, A., DAVE, P. et PARIKH, T. (2010). Avaaj otalo : a field study of
an interactive voice forum for small farmers in rural India. In CHI, pages 733?742. ACM.
61
PELLEGRINI, T. et LAMEL, L. (2009). Automatic word decompounding for ASR in a morphologically
rich language : Application to Amharic. Audio, Speech, and Language Processing, IEEE Transactions
on, 17(5):863?873.
SARIKAYA, R., KIRCHHOFF, K., SCHULTZ, T. et HAKKANI-TUR, D. (2009). Introduction to the special
issue on processing morphologically rich languages. Audio, Speech, and Language Processing,
IEEE Transactions on, 17(5).
SENG, S., SAM, S., BESACIER, L., BIGI, B. et CASTELLI, E. (2008). First broadcast news transcription
system for Khmer language. In LREC.
SHAIK, M., MOUSA, A., SCHLUTER, R. et NEY, H. (2011a). Hybrid language models using mixed
types of sub-lexical units for open vocabulary German LVCSR. In Interspeech.
SHAIK, M., MOUSA, A., SCHLUTER, R. et NEY, H. (2011b). Using morpheme and syllable based
sub-words for Polish LVCSR. In ICASSP.
TACHBELIE, M., ABATE, S. et BESACIER, L. (2011a). Part-of-speech tagging for under-resourced
and morphologically rich languages - the case of Amharic. In HLTD.
TACHBELIE, M., ABATE, S. et MENZEL, W. (2010). Morpheme-based automatic speech recognition
for a morphologically rich language - amharic. In SLTU.
TACHBELIE, M., ABATE, S. et MENZEL, W. (2011b). Morpheme-based and factored language
modeling for Amharic speech recognition. In VETULANI, Z., ?diteur : Human Language Technology.
Challenges for Computer Science and Linguistics, volume 6562 de Lecture Notes in Computer
Science, pages 82?93. Springer.
62
Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 386?391,
Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational Linguistics
LIG System for WMT13 QE Task: Investigating the Usefulness of
Features in Word Confidence Estimation for MT
Ngoc-Quang Luong Benjamin Lecouteux
LIG, Campus de Grenoble
41, Rue des Mathe?matiques,
UJF - BP53, F-38041 Grenoble Cedex 9, France
{ngoc-quang.luong,laurent.besacier,benjamin.lecouteux}@imag.fr
Laurent Besacier
Abstract
This paper presents the LIG?s systems
submitted for Task 2 of WMT13 Qual-
ity Estimation campaign. This is a
word confidence estimation (WCE) task
where each participant was asked to la-
bel each word in a translated text as
a binary ( Keep/Change) or multi-class
(Keep/Substitute/Delete) category. We in-
tegrate a number of features of various
types (system-based, lexical, syntactic and
semantic) into the conventional feature
set, for our baseline classifier training.
After the experiments with all features,
we deploy a ?Feature Selection? strategy
to keep only the best performing ones.
Then, a method that combines multiple
?weak? classifiers to build a strong ?com-
posite? classifier by taking advantage of
their complementarity is presented and ex-
perimented. We then select the best sys-
tems for submission and present the offi-
cial results obtained.
1 Introduction
Recently Statistical Machine Translation (SMT)
systems have shown impressive gains with many
fruitful results. While the outputs are more accept-
able, the end users still face the need to post edit
(or not) an automatic translation. Then, the issue
is to be able to accurately identify the correct parts
as well as detecting translation errors. If we fo-
cus on errors at the word level, the issue is called
Word-level Confidence Estimation (WCE).
In WMT 2013, a shared task about quality esti-
mation is proposed. This quality estimation task is
proposed at two levels: word-level and sentence-
level. Our work focuses on the word-level qual-
ity estimation (named Task 2). The objective is to
highlight words needing post-edition and to detect
parts of the sentence that are not reliable. For the
task 2, participants produce for each token a label
according to two sub-tasks:
? a binary classification: good (keep) or bad
(change) label
? a multi-class classification: the label refers to
the edit action needed for the token (i.e. keep,
delete or substitute).
Various approaches have been proposed for
WCE: Blatz et al (2003) combine several features
using neural network and naive Bayes learning al-
gorithms. One of the most effective feature combi-
nations is the Word Posterior Probability (WPP) as
proposed by Ueffing et al (2003) associated with
IBM-model based features (Blatz et al, 2004).
Ueffing and Ney (2005) propose an approach for
phrase-based translation models: a phrase is a se-
quence of contiguous words and is extracted from
word-aligned bilingual training corpus. The con-
fidence value of each word is then computed by
summing over all phrase pairs in which the tar-
get part contains this word. Xiong et al (2010)
integrate target word?s Part-Of-Speech (POS) and
train them by Maximum Entropy Model, allow-
ing significative gains compared to WPP features.
Other approaches are based on external features
(Soricut and Echihabi, 2010; Felice and Specia,
2012) allowing to deal with various MT systems
(e.g. statistical, rule based etc.).
In this paper, we propose to use both internal
and external features into a conditionnal random
fields (CRF) model to predict the label for each
word in the MT hypothesis. We organize the arti-
cle as follows: section 2 explains all the used fea-
tures. Section 3 presents our experimental settings
and the preliminary experiments. Section 4 ex-
plores a feature selection refinement and the sec-
tion 5 presents work using several classifiers asso-
ciated with a boosting decision. Finally we present
386
our systems submissions and propose some con-
clusions and perspectives.
2 Features
In this section, we list all 25 types of features for
building our classifier (see a list in Table 3). Some
of them are already used and described in detail in
our previous paper (Luong, 2012), where we deal
with French - English SMT Quality Estimation.
WMT13 was a good chance to re-investigate their
usefulness for another language pair: English-
Spanish, as well as to compare their contributions
with those from other teams. We categorize them
into two types: the conventional features, which
are proven to work efficiently in numerous CE
works and are inherited in our systems, and the
LIG features which are more specifically sug-
gested by us.
2.1 The conventional features
We describe below the conventional features we
used. They can be found in some previous papers
dealing with WCE.
? Target word features: the target word itself;
the bigram (trigram) it forms with one (two)
previous and one (two) following word(s); its
number of occurrences in the sentence.
? Source word features: all the source words
that align to the target one, represented in
BIO1 format.
? Source alignment context features: the com-
binations of the target word and one word be-
fore (left source context) or after (right source
context) the source word aligned to it.
? Target algnment context features: the com-
binations of the source word and each word
in the window ?2 (two before, two after) of
the target word.
? Target Word?s Posterior Probability (WPP).
? Backoff behaviour: a score assigned to the
word according to how many times the target
Language Model has to back-off in order to
assign a probability to the word sequence, as
described in (Raybaud et al, 2011).
1http://www-tsujii.is.s.u-tokyo.ac.jp/GENIA/tagger/
? Part-Of-Speech (POS) features (using Tree-
Tagger2 toolkit): The target word?s POS; the
source POS (POS of all source words aligned
to it); bigram and trigram sequences between
its POS and the POS of previous and follow-
ing words.
? Binary lexical features that indicate whether
the word is a: stop word (based on the stop
word list for target language), punctuation
symbol, proper name or numerical.
2.2 The LIG features
? Graph topology features: based on the N-best
list graph merged into a confusion network.
On this network, each word in the hypothesis
is labelled with its WPP, and belongs to one
confusion set. Every completed path passing
through all nodes in the network represents
one sentence in the N-best, and must con-
tain exactly one link from each confusion set.
Looking into a confusion set, we find some
useful indicators, including: the number of
alternative paths it contains (called Nodes),
and the distribution of posterior probabili-
ties tracked over all its words (most interest-
ing are maximum and minimum probabilities,
called Max and Min).
? Language Model (LM) features: the ?longest
target n-gram length? and ?longest source n-
gram length?(length of the longest sequence
created by the current target (source aligned)
word and its previous ones in the target
(source) LM). For example, with the tar-
get word wi: if the sequence wi?2wi?1wi
appears in the target LM but the sequence
wi?3wi?2wi?1wi does not, the n-gram value
for wi will be 3.
? The word?s constituent label and its depth in
the tree (or the distance between it and the
tree root) obtained from the constituent tree
as an output of the Berkeley parser (Petrov
and Klein, 2007) (trained over a Spanish tree-
bank: AnCora3).
? Occurrence in Google Translate hypothesis:
we check whether this target word appears in
the sentence generated by Google Translate
engine for the same source.
2http://www.ims.uni-stuttgart.de/projekte/corplex/TreeTagger/
3http://clic.ub.edu/corpus/en/ancora
387
? Polysemy Count: the number of senses of
each word given its POS can be a reliable in-
dicator for judging if it is the translation of
a particular source word. Here, we investi-
gate the polysemy characteristic in both tar-
get word and its aligned source word. For
source word (English), the number of senses
can be counted by applying a Perl exten-
sion named Lingua::WordNet4, which pro-
vides functions for manipulating the Word-
Net database. For target word (Spanish), we
employ BabelNet5 - a multilingual semantic
network that works similarly to WordNet but
covers more European languages, including
Spanish.
3 Experimental Setting and Preliminary
Experiment
The WMT13 organizers provide two bilingual
data sets, from English to Spanish: the training
and the test ones. The training set consists of
803 MT outputs, in which each token is anno-
tated with one appropriate label. In the binary
variant, the words are classified into ?K? (Keep)
or ?C? (Change) label, meanwhile in the multi-
class variant, they can belong to ?K? (Keep), ?S?
(Substitution) or ?D? (Deletion). The test set con-
tains 284 sentences where all the labels accompa-
nying words are hidden. For optimizing parame-
ters of the classifier, we extract 50 sentences from
the training set to form a development set. Since
a number of repetitive sentences are observed in
the original training set, the dev set was carefully
chosen to ensure that there is no overlap with the
new training set (753 sentences), keeping the tun-
ing process accurate. Some statistics about each
set can be found in Table 1.
Motivated by the idea of addressing WCE as
a sequence labeling task, we employ the Con-
ditional Random Fields (CRF) model (Lafferty
et al, 2001) and the corresponding WAPITI toolkit
(Lavergne et al, 2010) to train our classifier. First,
we experiment with the combination of all fea-
tures. For the multi-class system, WAPITI?s de-
fault configuration is applied to determine the la-
bel, i.e. label which has the highest score is as-
signed to word. In case of the binary system,
the classification task is then conducted multiple
times, corresponding to a threshold increase from
4http://search.cpan.org/dist/Lingua-Wordnet/Wordnet.pm
5http://babelnet.org
0.300 to 0.975 (step = 0.025). When threshold =
?, all words in the test set which the probability of
?K? class > ? will be labelled as ?K?, and oth-
erwise, ?C?. The values of Precision (Pr), Recall
(Rc) and F-score (F) for K and C label are tracked
along this threshold variation, allowing us to se-
lect the optimal threshold that yields the highest
Favg = F (K)+F (C)2 .
Results for the all-feature binary system
(ALL BIN) at the optimal threshold (0.500) and
the multi-class one (ALL MULT) at the default
threshold, obtained on our dev set, are shown
in Table 2. We can notice that with ALL BIN,
?K? label scores are very promising and ?C? la-
bel reaches acceptable performance. In case of
ALL MULT we obtain the almost similar above
performance for ?K? and ?S?, respectively, ex-
cept the disappointing scores for ?D? (which can
be explained by the fact that very few instances of
?D? words (4%) are observed in the training cor-
pus).
Data set Train Dev Test
#segments 753 50 284
#distinct segments 400 50 163
#words 18435 1306 7827
%K : %C 70: 30 77: 23 -
%K: %S: %D 70:26:4 77:19:4 -
Table 1: Statistics of training, dev and test sets
System Label Pr(%) Rc(%) F(%)
ALL BIN K 85.79 84.68 85.23
C 50.96 53.16 52.04
ALL MULT K 85.30 84.00 84.65
S 43.89 49.00 46.31
D 7.90 6.30 7.01
Table 2: Average Pr, Rc and F for labels of all-
feature binary and multi-class systems, obtained
on dev set.
4 Feature Selection
In order to improve the preliminary scores of all-
feature systems, we conduct a feature selection
which is based on the hypothesis that some fea-
tures may convey ?noise? rather than ?informa-
tion? and might be the obstacles weakening the
other ones. In order to prevent this drawback,
we propose a method to filter the best features
388
based on the ?Sequential Backward Selection? al-
gorithm6. We start from the full set of N features,
and in each step sequentially remove the most use-
less one. To do that, all subsets of (N-1) fea-
tures are considered and the subset that leads to
the best performance gives us the weakest feature
(not involved in the considered set). This proce-
dure is also called ?leave one out? in the litera-
ture. Obviously, the discarded feature is not con-
sidered in the following steps. We iterate the pro-
cess until there is only one remaining feature in
the set, and use the following score for compar-
ing systems: Favg(all) = Favg(K)+Favg(C)2 , where
Favg(K) and Favg(C) are the averaged F scores
for K and C label, respectively, when threshold
varies from 0.300 to 0.975. This strategy enables
us to sort the features in descending order of im-
portance, as displayed in Table 3. Figure 1 shows
the evolution of the performance as more and more
features are removed.
Rank Feature name Rank Feature name
1 Source POS 14? Distance to root
2? Occur in Google Trans. 15 Backoff behaviour
3? Nodes 16? Constituent label
4 Target POS 17 Proper name
5 WPP 18 Number of occurrences
6 Left source context 19? Min
7 Right target context 20? Max
8 Numeric 21 Left target context
9? Polysemy (target) 22? Polysemy (source)
10 Punctuation 23? Longest target gram length
11 Stop word 24? Longest source gram length
12 Right source context 25 Source Word
13 Target Word
Table 3: The rank of each feature (in term of use-
fulness) in the set. The symbol ?*? indicates our
proposed features.
Observations in 10-best and 10-worst perform-
ing features in Table 3 suggest that numerous fea-
tures extracted directly from SMT system itself
(source and target POS, alignment context infor-
mation, WPP, lexical properties: numeric, punc-
tuation) perform very well. Meanwhile, opposite
from what we expected, those from word statis-
tical knowledge sources (target and source lan-
guage models) are likely to be much less ben-
eficial. Besides, three of our proposed features
appear in top 10-best. More noticeable, among
them, the first-time-experimented feature ?Occur-
rence in Google Translation hypothesis? is the
most prominent (rank 2), implying that such an on-
line MT system can be a reliable reference channel
for predicting word quality.
6http://research.cs.tamu.edu/prism/lectures/pr/pr l11.pdf
Figure 1: Evolution of system performance
(Favg(all)) during Feature Selection process, ob-
tained on dev set
The above selection process also brings us the
best-performing feature set (Top 20 in Table 3).
The binary classifier built using this optimal sub-
set of features (FS BIN) reaches the optimal per-
formance at the threshold value of 0.475, and
slightly outperforms ALL BIN in terms of F scores
(0.46% better for ?K? and 0.69% better for ?C?).
We then use this set to build the multi-class one
(FS MULT) and the results are shown to be a
bit more effective compare to ALL MULT (0.37%
better for ?K?, 0.80% better for ?S? and 0.15%
better for ?D?). Detailed results of these two sys-
tems can be found in Table 4.
In addition, in Figure 1, when the size of fea-
ture set is small (from 1 to 7), we can observe
sharply the growth of system scores for both la-
bels. Nevertheless the scores seem to saturate as
the feature set increases from the 8 up to 25. This
phenomenon raises a hypothesis about the learn-
ing capability of our classifier when coping with
a large number of features, hence drives us to an
idea for improving the classification scores. This
idea is detailed in the next section.
System Label Pr(%) Rc(%) F(%)
FS BIN K 85.90 85.48 85.69
C 52.29 53.17 52.73
FS MULT K 85.05 85.00 85.02
S 45.36 49.00 47.11
D 9.1 5.9 7.16
Table 4: The Pr, Rc and F for labels of binary and
multi-class system built from Top 20 features, at
the optimal threshold value, obtained on dev set
389
5 Using Boosting technique to improve
the system?s performance
In this section, we try to answer to the following
question: if we build a number of ?weak? (or ?ba-
sic?) classifiers by using subsets of our features
and a machine learning algorithm (such as Boost-
ing), would we get a single ?strong? classifier?
When deploying this idea, our hope is that multi-
ple models can complement each other as one fea-
ture set might be specialized in a part of the data
where the others do not perform very well.
First, we prepare 23 feature subsets
(F1, F2, ..., F23) to train 23 basic classifiers,
in which: F1 contains all features, F2 is the Top
20 in Table 3 and Fi (i = 3..23) contains 9
randomly chosen features. Next, a 7-fold cross
validation is applied on our training set. We
divide it into 7 subsets (S1, S2, . . . , S7). Each
Si (i = 1..6) contains 100 sentences, and the
remaining 153 sentences constitute S7. In the
loop i (i = 1..7), Si is used as the test set and
the remaining data is trained with 23 feature
subsets. After each loop, we obtain the results
from 23 classifiers for each word in Si. Finally,
the concatenation of these results after 7 loops
gives us the training data for Boosting. Therefore,
the Boosting training file has 23 columns, each
represents the output of one basic classifier for
our training set. The detail of this algorithm is
described below:
Algorithm to build Boosting training data
for i :=1 to 7 do
begin
TrainSet(i) := ?Sk (k = 1..7, k 6= i)
TestSet(i) := Si
for j := 1 to 23 do
begin
Classifier Cj := Train TrainSet(i) with Fj
Result Rj := Use Cj to test Si
Column Pj := Extract the ?probability of word
to be G label? in Rj
end
Subset Di (23 columns) := {Pj} (j = 1..23)
end
Boosting training set D := ?Di (i = 1..7)
Next, the Bonzaiboost toolkit7 (which imple-
ments Boosting algorithm) is used for building
Boosting model. In the training command, we in-
voked: algorithm = ?AdaBoost?, and number of
iterations = 300. The Boosting test set is prepared
as follows: we train 23 feature subsets with the
training set to obtain 23 classifiers, then use them
7http://bonzaiboost.gforge.inria.fr/x1-20001
to test our dev set, finally extract the 23 probabil-
ity columns (like in the above pseudo code). In the
testing phase, similar to what we did in Section 4,
the Pr, Rc and F scores against threshold variation
for ?K? and ?C? labels are tracked, and those cor-
responding to the optimal threshold (0.575 in this
case) are represented in Table 5.
System Label Pr(%) Rc(%) F(%)
BOOST BIN K 86.65 84.45 85.54
C 51.99 56.48 54.15
Table 5: The Pr, Rc and F for labels of Boosting
binary classifier (BOOST BIN)
The scores suggest that using Boosting algo-
rithm on our CRF classifiers? output accounts
for an efficient way to make them predict better:
on the one side, we maintain the already good
achievement on K class (only 0.15% lost), on the
other side we gain 1.42% the performance in C
class. It is likely that Boosting enables different
models to better complement each other, in terms
of the later model becomes experts for instances
handled wrongly by the previous ones. Another
advantage is that Boosting algorithm weights each
model by its performance (rather than treating
them equally), so the strong models (come from
all features, top 20, etc.) can make more dominant
impacts than the rest.
6 Submissions and Official Results
After deploying several techniques to improve the
system?s prediction capability, we select two bests
of each variant (binary and multi-class) to sub-
mit. For the binary task, the submissions in-
clude: the Boosting (BOOST BIN) and the Top
20 (FS BIN) system. For the multi-class task, we
submit: the Top 20 (FS MULT) and the all-feature
(ALL MULT) one. Before the submission, the
training and dev sets were combined to re-train
the prediction models for FS BIN, FS MULT and
ALL MULT. Table 6 reports the official results
obtained by LIG at WMT 2013, task 2. We ob-
tained the best performance among 3 participants.
These results confirm that the feature selection
strategy is efficient (FS MULT slightly better than
ALL MULT) while the contribution of Boosting
is unclear (BOOST BIN better than FS BIN if F-
measure is considered but worse if Accuracy is
considered - the difference is not significant).
390
System Pr Rc F Acc
BOOST BIN 0.777882 0.884325 0.827696 0.737702
FS BIN 0.788483 0.864418 0.824706 0.738213
FS MULT - - - 0.720710
ALL MULT - - - 0.719177
Table 6: Official results of the submitted systems, obtained on test set
7 Discussion and Conclusion
In this paper, we describe the systems submitted
for Task 2 of WMT13 Quality Estimation cam-
paign. We cope with the prediction of quality
at word level, determining whether each word
is ?good? or ?bad? (in the binary variant), or is
?good?, or should be ?substitute? or ?delete? (in
the multi-class variant). Starting with the ex-
isting word features, we propose and add vari-
ous of novel ones to build the binary and multi-
class baseline classifier. The first experiment?s re-
sults show that precision and recall obtained in
?K? label (both in binary and multi-class sys-
tems) are very encouraging, and ?C? (or ?S?) la-
bel reaches acceptable performance. A feature se-
lection strategy is then deployed to enlighten the
valuable features, find out the best performing sub-
set. One more contribution we made is the proto-
col of applying Boosting algorithm, training mul-
tiple ?weak? classifiers, taking advantage of their
complementarity to get a ?stronger? one. These
techniques improve gradually the system scores
(measure with F score) and help us to choose the
most effective systems to classify the test set.
In the future, this work can be extended in the
following ways. Firstly, we take a deeper look into
linguistic features of word, such as the grammar
checker, dependency tree, semantic similarity, etc.
Besides, we would like to reinforce the segment-
level confidence assessment, which exploits the
context relation between surrounding words to
make the prediction more accurate. Moreover, a
methodology to evaluate the sentence confidence
relied on the word- and segment- level confidence
will be also deeply considered.
References
John Blatz, Erin Fitzgerald, George Foster, Simona Gan-
drabur, Cyril Goutte, Alex Kulesza, Alberto Sanchis, and
Nicola Ueffing. Confidence estimation for machine trans-
lation. Technical report, JHU/CLSP Summer Workshop,
2003.
John Blatz, Erin Fitzgerald, George Foster, Simona Gan-
drabur, Cyril Goutte, Alex Kulesza, Alberto Sanchis, and
Nicola Ueffing. Confidence estimation for machine trans-
lation. In Proceedings of COLING 2004, pages 315?321,
Geneva, April 2004.
Mariano Felice and Lucia Specia. Linguistic features for
quality estimation. In Proceedings of the 7th Workshop on
Statistical Machine Translation, pages 96?103, Montreal,
Canada, June 7-8 2012.
John Lafferty, Andrew McCallum, and Fernando Pereira.
Conditional random fields: Probabilistic models for seg-
menting et labeling sequence data. In Proceedings of
ICML-01, pages 282?289, 2001.
Thomas Lavergne, Olivier Cappe?, and Franc?ois Yvon. Practi-
cal very large scale crfs. In Proceedings of the 48th Annual
Meeting of the Association for Computational Linguistics,
pages 504?513, 2010.
Ngoc-Quang Luong. Integrating lexical, syntactic and
system-based features to improve word confidence estima-
tion in smt. In Proceedings of JEP-TALN-RECITAL, vol-
ume 3 (RECITAL), pages 43?56, Grenoble, France, June
4-8 2012.
Slav Petrov and Dan Klein. Improved inference for unlexical-
ized parsing. In Proceedings of NAACL HLT 2007, pages
404?411, Rochester, NY, April 2007.
S. Raybaud, D. Langlois, and K. Sma?? li. ?this sentence is
wrong.? detecting errors in machine - translated sentences.
In Machine Translation, pages 1?34, 2011.
Radu Soricut and Abdessamad Echihabi. Trustrank: Inducing
trust in automatic translations via ranking. In Proceedings
of the 48th ACL (Association for Computational Linguis-
tics), pages 612?621, Uppsala, Sweden, July 2010.
Nicola Ueffing and Hermann Ney. Word-level confidence
estimation for machine translation using phrased-based
translation models. In Proceedings of Human Lan-
guage Technology Conference and Conference on Empiri-
cal Methods in Natural Language Processing, pages 763?
770, Vancouver, 2005.
Nicola Ueffing, Klaus Macherey, and Hermann Ney. Con-
fidence measures for statistical machine translation. In
Proceedings of the MT Summit IX, pages 394?401, New
Orleans, LA, September 2003.
Deyi Xiong, Min Zhang, and Haizhou Li. Error detection
for statistical machine translation using linguistic features.
In Proceedings of the 48th Association for Computational
Linguistics, pages 604?611, Uppsala, Sweden, July 2010.
391
Workshop on Humans and Computer-assisted Translation, pages 1?9,
Gothenburg, Sweden, 26 April 2014.
c?2014 Association for Computational Linguistics
Word Confidence Estimation for SMT N-best List Re-ranking
Ngoc-Quang Luong Laurent Besacier
LIG, Campus de Grenoble
41, Rue des Math?ematiques,
UJF - BP53, F-38041 Grenoble Cedex 9, France
{ngoc-quang.luong,laurent.besacier,benjamin.lecouteux}@imag.fr
Benjamin Lecouteux
Abstract
This paper proposes to use Word Confi-
dence Estimation (WCE) information to
improve MT outputs via N-best list re-
ranking. From the confidence label as-
signed for each word in the MT hypoth-
esis, we add six scores to the baseline log-
linear model in order to re-rank the N-best
list. Firstly, the correlation between the
WCE-based sentence-level scores and the
conventional evaluation scores (BLEU,
TER, TERp-A) is investigated. Then, the
N-best list re-ranking is evaluated over dif-
ferent WCE system performance levels:
from our real and efficient WCE system
(ranked 1st during last WMT 2013 Quality
Estimation Task) to an oracle WCE (which
simulates an interactive scenario where a
user simply validates words of a MT hy-
pothesis and the new output will be auto-
matically re-generated). The results sug-
gest that our real WCE system slightly (but
significantly) improves the baseline while
the oracle one extremely boosts it; and bet-
ter WCE leads to better MT quality.
1 Introduction
A number of methods to improve MT hypothe-
ses after decoding have been proposed in the past,
such as: post-editing, re-ranking or re-decoding.
Post-editing (Parton et al., 2012) is a human-
inspired task where the machine post edits trans-
lations in a second automatic pass. In re-ranking
(Zhang et al., 2006; Duh and Kirchhoff, 2008;
Bach et al., 2011), more features are used along
with the multiple model scores for re-determining
the 1-best among N-best list. Meanwhile, re-
decoding process (Venugopal et al., 2007) inter-
venes directly into the decoder?s search graph (e.g.
adds more reward or penalty scores), driving it to
another better path.
This work aims at re-ranking the N-best list to im-
prove MT quality. Generally, during the transla-
tion task, the decoder traverses through paths in
its search space, computes the objective function
values for them and outputs the one with high-
est score as the best hypothesis. Besides, those
with lower scores can also be generated in a so-
called N-best list. The decoder?s function consists
of parameters from different models, such as trans-
lation, distortion, word penalties, reordering, lan-
guage models, etc. In the N-best list, although the
current 1-best beats the remains in terms of model
score, it might not be exactly the closest to the hu-
man reference. Therefore, adding more decoder
independent features would be expected to raise
up a better candidate. In this work, we build six
additional features based on the labels predicted
by our Word Confidence Estimation (WCE) sys-
tem, then integrate them with the existing decoder
scores for re-ranking hypotheses in the N-best
list. More precisely, in the second pass, our re-
ranker aggregates over decoder and WCE-based
weighted scores and utilizes the obtained sum to
sort out the best candidate. The novelty of this pa-
per lies on the following contributions: the corre-
lation between WCE-based sentence-level scores
and conventional evaluation scores (BLEU, TER,
TERp-A) is first investigated. Then, we conduct
the N-best list re-ranking over different WCE sys-
tem performance levels: starting by a real WCE,
passing through several gradually improved (sim-
ulated) systems and finally the ?oracle? one. From
these in-depth experiments, the role of WCE in
improving MT quality via re-ranking N-best list
is confirmed and reinforced.
The remaining parts of this article are organized
as follows: in section 2 we summarize some out-
standing approaches in N-best list re-ranking as
well as in WCE. Section 3 describes our WCE sys-
tem construction, followed by proposed features.
1
The experiments along with results and in-depth
analysis of WCE scores? contribution (as WCE
system gets better) are presented in Section 4 and
Section 5. The last section concludes the paper
and points out some ongoing work.
2 Related Work
2.1 N-best List Re-ranking
Walking through various related work concern-
ing this issue, we observe some prominent ideas.
The first attempt focuses on proposing additional
Language Models. Kirchhoff and Yang (2005)
train one word-based 4-gram model (with modi-
fied Kneser-Ney smoothing) and one factored tri-
gram one, then combine them with seven decoder
scores for re-ranking N-best lists of several SMT
systems. Their proposed LMs increase the transla-
tion quality of the baselines (measured by BLEU
score) from 21.6 to 22.0 (Finnish - English), or
from 30.5 to 31.0 (Spanish - English). Meanwhile,
Zhang et al. (2006) experiment a distributed LM
where each server, among the total of 150, hosts a
portion of the data and responses its client, allow-
ing them to exploit an extremely large corpus (2.7
billion word English Gigaword) for estimating N-
gram probability. The quality of their Chinese
- English hypotheses after the re-scoring process
by using this LM is improved 4.8% (from BLEU
31.44 to 32.64, oracle score = 37.48).
In one other direction, several authors propose to
replace the current linear scoring function used by
the decoder by more efficient functions. Sokolov
et al. (2012) learn their non-linear scoring function
in a learning-to-rank paradigm, applying Boosting
algorithm. Their gains on the WMT?{10, 11, 12}
are shown modest yet consistent and higher than
those based on linear scoring functions. Duh and
Kirchhoff (2008) use Minimum Error Rate Train-
ing (MERT) (Och, 2003) as a weak learner and
build their own solution, BoostedMERT, a highly-
expressive re-ranker created by voting among mul-
tiple MERT ones. Their proposed model dramat-
ically beats the decoder?s log-linear model (43.7
vs. 42.0 BLEU) in IWSLT 2007 Arabic - English
task. Applying solely goodness (the sentence con-
fidence) scores, Bach et al. (2011) obtain very con-
sistent TER reductions (0.7 and 0.6 on the dev and
test set) after a 5-list re-ranking for their Arabic -
English SMT hypotheses. This latter work is the
one that is the most related to our paper. However,
the major differences are: (1) our proposed sen-
tence scores are computed based on word confi-
dence labels; and (2) we perform an in-depth study
of the use of WCE for N-best reranking and assess
its usefulness in a simulated interactive scenario.
2.2 Word Confidence Estimation
Confidence Estimation (CE) is the task of iden-
tifying the correct parts and detecting the trans-
lation errors in MT output. If the error is pre-
dicted for each word, this becomes WCE. The in-
teresting uses of WCE include: pointing out the
words that need to be corrected by the post-editor,
telling readers about the reliability of a specific
portion, and selecting the best segments among
options from multiple translation systems for com-
bination.
Dealing with this problem, various approaches
have been proposed: Blatz et al. (2003) combine
several features using neural network and naive
Bayes learning algorithms. One of the most ef-
fective feature combinations is the Word Posterior
Probability (WPP) as suggested by Ueffing et al.
(2003) associated with IBM-model based features
(Blatz et al., 2004). Ueffing and Ney (2005)
propose an approach for phrase-based translation
models: a phrase is a sequence of contiguous
words and is extracted from the word-aligned
bilingual training corpus. The confidence value
of each word is then computed by summing over
all phrase pairs in which the target part contains
this word. Xiong et al. (2010) integrate target
word?s Part-Of-Speech (POS) and train them by
Maximum Entropy Model, allowing significative
gains in comparison to WPP features. The novel
features from source side, alignment context, and
dependency structure (Bach et al., 2011) help to
augment marginally in F-score as well as the Pear-
son correlation with human judgment. Other ap-
proaches are based on external features (Soricut
and Echihabi, 2010; Felice and Specia, 2012) al-
lowing to cope with various MT systems (e.g. sta-
tistical, rule based etc.). Among the numerous
WCE applications, we consider its contribution in
a specific step of SMT pipeline: N-best list re-
ranking. Our WCE system and the proposed re-
ranking features are presented in the next section.
3 Our Approach
Our approach can be expressed in three steps: in-
vestigate the potential of using word-level score in
N-best list re-ranking, build the WCE system and
2
extract additional features to integrate with the ex-
isting log-linear model.
3.1 Investigating the correlation between
?word quality? scores and other metrics
Firstly, we investigate the correlation between
sentence-level scores (obtained from WCE labels)
and conventional evaluation scores (BLEU (Pa-
pineni et al., 2002), TER and TERp-A (Snover
et al., 2008)). For each sentence, a word quality
score (WQS) is calculated by:
WQS =
#
??
G
??
(good) words
#words
(1)
In other words, we are trying to answer the fol-
lowing question: can the high percentage of ?G?
(good) words (predicted by WCE system) in a
MT output ensure its possibility of having a better
BLEU and low TER (TERp-A) value ? This inves-
tigation is a strong prerequisite for further exper-
iments in order to check that WCE scores do not
bring additional ?noise? to the re-ranking process.
In this experiment, we compute WQS over our en-
tire French - English data set (total of 10,881 1-
best translations) for which WCE oracle labels are
available (see Section 3.2 to see how they were ob-
tained). The results are plotted in Figure 1, where
the y axis shows the ?G? (good) word percent-
age, and the x axis shows BLEU (1a), TER (1b) or
TERp-A (1c) scores. It can be seen from Figure 1
that the major parts of points (the densest areas) in
all three cases conform the common tendency: In
Figure 1a, the higher ?G? percentage, the higher
BLEU is; on the contrary, in Figure 1b (Figure
1c), the higher ?G? percentage, the lower TER
(TERp-A) is. We notice some outliers, i.e. sen-
tences with most or almost words labeled ?good?,
yet still have low BLEU or high TER (TERp-A)
scores. This phenomenon is to be expected when
many (unknown) source words are not translated
or when the (unique) reference is simply too far
from the hypothesis. Nevertheless, the informa-
tion extracted from oracle WCE labels seems use-
ful to build an efficient re-ranker.
3.2 WCE System Preparation
Essentially, a WCE system construction consists
of two pivotal elements: the features (the SMT
system dependent or independent information
extracted for each word to represent its char-
acteristics) and the machine learning method
(to train the prediction model). Motivated
Figure 1: The correlation between WQS in a sen-
tence and its overall quality measured by : (a)
BLEU, (b) TER and (c) TERp-A metrics
by the idea of addressing WCE problem as
a sequence labeling process, we employ the
Conditional Random Fields (CRFs) for our model
training, with WAPITI toolkit (Lavergne et al.,
2010). Basically, CRF computes the probabil-
ity of the output sequence Y = (y
1
, y
2
, ..., y
N
)
given the input sequenceX = (x
1
, x
2
, ..., x
N
) by:
3
p?
(Y |X) =
1
Z
?
(X)
exp
{
K
?
k=1
?
k
F
k
(X,Y )
}
(2)
where F
k
(X,Y ) =
?
T
t=1
f
k
(y
t?1
, y
t
, x
t
);
{f
k
} (k = 1,K) is a set of feature functions;
{?
k
} (k = 1,K) are the associated parameter val-
ues; and Z
?
(x) is the normalization function.
In terms of features, a number of knowledge
sources are employed for extracting them, result-
ing in the major types listed below. We briefly
summarize them in this work, further details about
total of 25 features can be referred in (Luong et al.,
2013a).
? Target Side: target word; bigram (trigram)
backward sequences; number of occurrences
? Source Side: source word(s) aligned to the
target word
? Alignment Context: the combinations of the
target (source) word and all aligned source
(target) words in the window ?2
? Word posterior probability
? Pseudo-reference (Google Translate):
whether the current word appears in the
pseudo reference or not
1
?
? Graph topology: number of alternative paths
in the confusion set, maximum and minimum
values of posterior probability distribution
? Language model (LM) based: length of the
longest sequence of the current word and its
previous ones in the target (resp. source) LM.
For example, with the target word w
i
: if the
sequence w
i?2
w
i?1
w
i
appears in the target
LM but the sequence w
i?3
w
i?2
w
i?1
w
i
does
not, the n-gram value for w
i
will be 3.
? Lexical Features: word?s Part-Of-Speech
(POS); sequence of POS of all its aligned
source words; POS bigram (trigram) back-
ward sequences; punctuation; proper name;
numerical
? Syntactic Features: Null link; constituent la-
bel; depth in the constituent tree
? Semantic Features: number of word senses in
WordNet.
Interestingly, this feature set was also used in our
English - Spanish WCE system which got the first
1
This is our first-time experimented feature and does not
appear in (Luong et al., 2013a)
rank in WMT 2013 Quality Estimation Shared
Task (Luong et al., 2013b).
For building the WCE training and test sets, we
use a dataset of 10,881 French sentences (Potet
et al., 2012) , and apply a baseline SMT system
to generate hypotheses (1000-best list). Our base-
line SMT system (presented for WMT 2010 eval-
uation) keeps the Moses?s default setting (Koehn
et al., 2007): log-linear model with 14 weighted
feature functions. The translation model is trained
on the Europarl and News parallel corpora of
WMT10
2
evaluation campaign (1,638,440 sen-
tences). The target language model is trained by
the SRI language modeling toolkit (Stolcke, 2002)
on the news monolingual corpus (48,653,884 sen-
tences).
Translators were then invited to correct MT out-
puts, giving us the same amount of post editions
(Potet et al., 2012). The set of triples (source,
hypothesis, post edition) is then divided into the
training set (10000 first triples) and test set (881
remaining). To train the WCE model, we ex-
tract all above features for words of the 1-best hy-
potheses of the training set. For the test set, the
features are built for all 1000 best translations of
each source sentence. Another essential element
is the word?s confidence labels (or so-called WCE
oracle labels) used to train the prediction model
as well as to judge the WCE results. They are
set by using TERp-A toolkit (Snover et al., 2008)
in one of the following classes: ?I? (insertions),
?S? (substitutions), ?T? (stem matches), ?Y? (syn-
onym matches), ?P? (phrasal substitutions), ?E?
(exact matches) and then simplified into binary
class: ?G? (good word) or ?B? (bad word) (Lu-
ong et al., 2013a).
Once having the prediction model built with all
features, we apply it on the test set (881 x 1000
best = 881000 sentences) and get needed WCE la-
bels. Figure 2 shows an example about the classi-
fication results for one sentence. Comparing with
the reference labels, we can point out easily the
correct classifications for ?G? words (e.g. in case
of operation, added) and for ?B? words (e.g. is,
have), as well as classification errors (e.g. a, com-
bat). According to the Precision (Pr), Recall (Rc)
and F-score (F) shown in Table 1, our WCE sys-
tem reaches very promising performance in pre-
dicting ?G? label, and acceptable for ?B? label.
These labels will be used to calculate our proposed
2
http://www.statmt.org/wmt10/
4
Figure 2: Example of our WCE classification results for one MT hypothesis
features (section 3.3).
Label Pr(%) Rc(%) F(%)
Good (G) 84.36 91.22 87.65
Bad (B) 51.34 35.95 42.29
Table 1: Pr, Rc and F for ?G? and ?B? labels of
our WCE system
3.3 Proposed Features
Since the scores resulted from the WCE system
are for words, we have to synthesize them in sen-
tence level scores for integrating with the 14 de-
coder scores. Six proposed scores involve:
? The ratio of number of good words to total
number of words. (1 score)
? The ratio of number of good nouns (verbs) to
total number of nouns (verbs)
3
. (2 scores)
? The ratio of number of n consecutive good
word sequences to the total number of con-
secutive word sequences ; n=2, n=3 and n=4.
(3 scores)
For instance, in case of the hypothesis in Figure 2:
among the total of 18 words, we have 12 labeled
as ?G?; and 7 out of 17 word pairs (bigram) are
labeled as ?GG?, etc. Hence, some of the above
3
We decide not to experiment with adjectives, adverbs and
conjunctions since their number can be 0 in many cases.
scores can be written as:
#good words
#words
=
12
18
= 0.667
#good bigrams
#bigrams
=
7
17
= 0.4118
#good trigrams
#trigrams
=
3
16
= 0.1875
(3)
With the features simply derived from WCE labels
and not from CRF model scores (i.e. the probabil-
ity p(G), p(B)) , we expect to spread out the eval-
uation up to the ?oracle? setting, where the users
validate a word as ?G? or ?B? without providing
any confidence score.
4 Experiments
4.1 Experimental Settings
As described in Section 3.2, our SMT system gen-
erates 1000-best list for each source sentence, and
among them, the best hypothesis was determined
by using the objective function based on 14 de-
coder scores, including: 7 reordering scores, 1 lan-
guage model score, 5 translation model scores and
1 word penalty score. Initially, all six additional
WCE-based scores are weighted as 1.0. Then,
two optimization methods: MERT and Margin
Infused Relaxed Algorithm (MIRA) (Watanabe
et al., 2007) are applied to optimize the weights of
all 20 scores of the re-ranker. In both methods, we
carry out a 2-fold cross validation on the N-best
5
Systems MERT MIRA
BLEU TER TERp-A BLEU TER TERp-A
BL 52.31 0.2905 0.3058 50.69 0.3087 0.3036
BL+OR 58.10 0.2551 0.2544 55.41 0.2778 0.2682
BL+WCE 52.77 0.2891 0.3025 51.01 0.3055 0.3012
WCE + 25% 53.45 0.2866 0.2903 51.33 0.3010 0.2987
WCE + 50% 55.77 0.2730 0.2745 53.63 0.2933 0.2903
WCE + 75% 56.40 0.2687 0.2669 54.35 0.2848 0.2822
Oracle BLEU score BLEU=60.48
Table 2: Translation quality of the baseline system (only decoder scores) and that with additional scores
from real ?WCE? or ?oracle? WCE system
System MERT
Better Equivalent Worse
BL+WCE 159 601 121
BL+OR 517 261 153
WCE+25% 253 436 192
WCE+50% 320 449 112
WCE+75% 461 243 177
Table 3: Quality comparison (measured by TER) between the baseline and two integrated systems in
details (How many sentences are improved, kept equivalent or degraded, out of 881 test sentences?
test set. In other words, we split our N-best test
set into two equivalent subsets: S1 and S2. Play-
ing the role of a development set, S1 will be used
to optimize the 20 weights for re-ranking S2 (and
vice versa). Finally two result subsets (new 1-best
after re-ranking process) are merged for evalua-
tion. To better acknowledge the impact of the pro-
posed scores, we calculate them not only using our
real WCE system, but also using an oracle WCE
(further called ?WCE scores? and ?oracle scores?,
respectively). To summarize, we experiment with
the three following systems:
? BL: Baseline SMT system with 14 above de-
coder scores
? BL+WCE: Baseline + 6 real WCE scores
? BL+OR: Baseline + 6 oracle WCE scores
(simulating an interactive scenario).
4.2 Results and Analysis
The translation quality of BL, BL+WCE and
BL+OR, optimized by MERT and MIRA method
are reported in Table 2. Meanwhile, Table 3
depicts in details the number of sentences in
the two integrated systems which outperform, re-
main equivalent or degrade the baseline hypoth-
esis (when match against the references, mea-
sured by TER). It can be observed from Table
2 that the integration of oracle scores signifi-
cantly boosts the MT output quality, measured
by all three metrics and optimized by both meth-
ods employed. We gained 5.79 and 4.72 points
in BLEU score, by MERT and MIRA (respec-
tively). With TER, BL+OR helps to gain 0.03
point in both two methods. Meanwhile, in case of
TERp-A, the improvement is 0.05 point for MERT
and 0.03 point for MIRA. It is worthy to mention
that the possibility of obtaining such oracle labels
is definitely doable through a human-interaction
scenario (which could be built from a tool like
PET (Post-Editing Tool) (Aziz et al., 2012) for
instance). In such an environment, once having
the hypothesis produced by the first pass (trans-
lation task), the human editor could simply click
on words considered as bad (B), the other words
being implicitly considered as correct (G).
Breaking down the analysis into sentence level,
as described on Table 3, BL+OR (MERT) yields
nearly 59% (517 over 881) better outputs than the
baseline and only 17% of worse ones. Further-
more, Table 2 shows that in case of our test set, op-
timizing by MERT is pretty more beneficial than
MIRA (we do not have a clear explanation of this
yet).
For more insightful understanding about WCE
scores? acuteness, we make a comparison with
6
the most possible optimal BLEU score that could
be obtained from the N-best list. Applying the
sentence-level BLEU+1 (Nakov et al., 2012) met-
ric over candidates in the list, we are able to se-
lect the one with highest score and aggregate all
of them in an oracle-best translation; the result-
ing performance obtained is 60.48. This score
accounts for a fact that the simulated interactive
scenario (BL+OR) lacks only 2.38 points (in case
of MERT) to be optimal and clearly overpass the
baseline (8.17 points below the best score).
The contribution of a real WCE system seems
more modest: BL+WCE marginally increases
BLEU scores of BL (0.46 gain in case of opti-
mizing by MERT and 0.32 by MIRA). For both
TER and TERp-A metric, the progressions are
also negligible. To verify the significance of this
result, we estimate the p-value between BLEU of
BL+WCE system and BLEU of baseline BL rely-
ing on Approximate Randomization (AR) method
(Clark et al., 2011) which indicates if the improve-
ment yielded by the optimized system is likely
to be generated again by some random processes
(randomized optimizers). After various optimizer
runs, we selected randomly 5 optimizer outputs,
perform the AR test and obtain a p-value of 0.01.
This result reveals that the improvement yielded
by BL+WCE is significative although small, orig-
inated from the contribution of WCE score, not
by any optimizer variance. This modest but pos-
itive change in BLEU score using WCE features,
encourages us to investigate and analyze further
about WCE scores? impact, supposing WCE per-
formance is getting better. More in-depth analysis
is presented in the next section.
5 Further Understanding of WCE scores
role in N-best Re-ranking via
Improvement Simulation
We think it would be very interesting and useful
to answer the following question: do WCE scores
really effectively help to increase MT output qual-
ity when the WCE system is getting better and
better? To do this, our proposition is as follows:
firstly, by using the oracle labels, we filter out all
wrongly classified words in the test set and push
them into a temporary set, called T. Then, we cor-
rect randomly a percentage (25%, 50%, or 75%)
of labels in T. Finally, the altered T will be inte-
grated back with the correctly predicted part (by
the WCE system) in order to form a new ?simu-
lated? result set. This strategy results in three ?vir-
tual? WCE systems called ?WCE+N%? (N=25,
50 or 75), which use 14 decoder scores and 6 ?sim-
ulated? WCE scores. Table 4 shows the perfor-
mance of these systems in term of F score (%).
From each of the above systems, the whole exper-
System F(?G?) F(?B?) Overall F
WCE+25% 89.87 58.84 63.51
WCE+50% 93.21 73.09 76.11
WCE+75% 96.58 86.87 88.33
Oracle labels 100 100 100
Table 4: The performances (Fscore) of simulated
WCE systems
imental setting is identical to what we did with the
original WCE and oracle systems: six scores are
built and combined with existing 14 system scores
for each hypothesis in the N-best list. After that,
MERT and MIRA methods are invoked to opti-
mize their weights, and finally the reordering is
performed thanks to these scores and appropriate
optimal weights. The translation quality measured
by BLEU, TER and TERp-A after re-ranking us-
ing ?WCE+N%? (N=25,50,75) can be seen also
in Table 2. The number of translations which out-
perform, keep intact and decline in comparison to
the baseline are shown in Table 3 for MERT opti-
mization.
We note that all obtained scores fit our guess and
expectation: the better performance WCE system
reaches, the clearer its role in improving MT out-
put quality. Diminishing 25% of the wrongly pre-
dicted words leads to a gain 0.68 point (by MERT)
and 0.32 (by MIRA) in BLEU score. More sig-
nificant increases of BLEU 3.00 and BLEU 3.63
(MERT) can be achieved when prediction errors
are cut off up to 50% and 75%. Figure 3 presents
an overview of the results obtained and helps us
to predict the MT improvements expected if the
WCE system improves in the future. Table 5
shows several examples where WCE scores drive
SMT system to better reference-correlated hypoth-
esis. In the first example, the baseline generates
the hypothesis in which the source phrase ?pour
sa part? remains untranslated. On the contrary,
WCE+50% overcomes this drawback by result-
ing in a correct translation phrase: ?for his part?.
The latter translation needs only one edit opera-
tion (shift for ?Bettencourt-Meyers?) to become
its reference. In example 2, BL+OR selects the
7
Example 1 (from WCE+50%)
Source Pour sa part , l? avocat de Franc?oise Bettencourt-Meyers , Olivier
Metzner , s? est f?elicit?e de la d?ecision du tribunal .
Hypothesis (Baseline SMT) The lawyer of Bettencourt-Meyers Franc?oise , Olivier Metzner ,
welcomed the court ?s decision .
Hypothesis (SMT+WCE
scores)
For his part , the lawyer of Bettencourt-Meyers Franc?oise ,
Olivier Metzner , welcomed the court ?s decision .
Post-edition For his part , the lawyer of Franc?oise Bettencourt-Meyers ,
Olivier Metzner , welcomed the court ?s decision .
Example 2 (from BL+OR)
Source Pour l? otre , l? accord risque ? de creuser la tombe d? un tr`es
grand nombre de pme du secteur dans les 12 prochains mois ? .
Hypothesis (Baseline MT) For the otre the agreement is likely to deepen the grave of a very
large number of smes in the sector in the next 12 months ? .
Hypothesis (SMT+WCE
scores)
For the otre agreement , the risk ? digging the grave of a very
large number of medium-sized businesses in the next 12 months ?
.
Post-edition For the otre , the agreement risks ? digging the grave of a very
large number of small- and medium-sized businesses in the next
12 months ? .
Table 5: Examples of MT hypothesis before and after reranking using the additional scores from
WCE+50% (Example 1) and BL+OR (Example 2) system
Figure 3: Comparison of the performance of var-
ious systems: the integrations of WCE features,
which the quality increases gradually, lead to the
linear improvement of translation outputs.
better hypothesis, in which the phrases ?creuser
la tombe? and ??pme du secteur? are translated
into ?digging the grave? and ?medium-sized busi-
nesses?, respectively, better than those of the base-
line (?deepen the grave? and ?smes in the sec-
tor?).
6 Conclusions And Perspectives
So far, the word confidence scores have been
exploited in several applications, e.g. post-
editing, sentence quality assessment or multiple
MT-system combination, yet very few studies (ex-
cept Bach et al. (2011) ) propose to investigate
them for boosting MT quality. Thus, this pa-
per proposed several features extracted from a
WCE system and combined them with existing de-
coder scores for re-ranking N-best lists. Our WCE
model is built using CRFs, on a variety of types of
features for the French - English SMT task. Due
to its limitations in predicting translation errors
(?B? label), WCE scores ensure only a modest im-
provement in translation quality over the baseline
SMT. Nevertheless, further experiments about the
simulation of WCE performance suggest that such
types of score contribute dramatically if they are
built from an accurate WCE system. They also
show that with the help of an ?ideal? WCE, the
MT system reaches quite close to its most optimal
possible quality. These scores are totally indepen-
dent from the decoder, they can be seen as a way
to introduce lexical, syntactic and semantic infor-
mation (used for WCE) in a SMT pipeline.
As future work, we plan to focus on augmenting
our WCE performance using more linguistic fea-
tures as well as advanced techniques (feature se-
lection, Boosting method...). In the same time, we
would like to integrate the WCE scores in the de-
coder?s search graph to redirect the decoding pro-
cess (preliminary experiments, not reported here
yet, have shown that this is a very promising av-
enue of research).
8
References
Wilker Aziz, Sheila C. M. de Sousa, and Lucia Specia. Pet:
a tool for post-editing and assessing machine translation.
In Proceedings of the Eight International Conference on
Language Resources and Evaluation (LREC?12), Istanbul,
Turkey, May 23-25 2012.
Nguyen Bach, Fei Huang, and Yaser Al-Onaizan. Goodness:
A method for measuring machine translation confidence.
In Proceedings of the 49th Annual Meeting of the Associa-
tion for Computational Linguistics, pages 211?219, Port-
land, Oregon, June 19-24 2011.
John Blatz, Erin Fitzgerald, George Foster, Simona Gan-
drabur, Cyril Goutte, Alex Kulesza, Alberto Sanchis, and
Nicola Ueffing. Confidence estimation for machine trans-
lation. Technical report, JHU/CLSP Summer Workshop,
2003.
John Blatz, Erin Fitzgerald, George Foster, Simona Gan-
drabur, Cyril Goutte, Alex Kulesza, Alberto Sanchis, and
Nicola Ueffing. Confidence estimation for machine trans-
lation. In Proceedings of COLING 2004, pages 315?321,
Geneva, April 2004.
Jonathan Clark, Chris Dyer, Alon Lavie, and Noah Smith.
Better hypothesis testing for statistical machine transla-
tion: Controlling for optimizer instability. In Proceedings
of the Association for Computational Lingustics, 2011.
Kevin Duh and Katrin Kirchhoff. Beyond log-linear models:
Boosted minimum error rate training for n-best re-ranking.
In Proc. of ACL, Short Papers, 2008.
Mariano Felice and Lucia Specia. Linguistic features for
quality estimation. In Proceedings of the 7th Workshop on
Statistical Machine Translation, pages 96?103, Montreal,
Canada, June 7-8 2012.
Katrin Kirchhoff and Mei Yang. Improved language model-
ing for statistical machine translation. In Proceedings of
the ACL Workshop on Building and Using Parallel Texts,
pages 125?128, Ann Arbor, Michigan, June 2005.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin,
and Evan Herbst. Moses: Open source toolkit for statisti-
cal machine translation. In Proceedings of the 45th Annual
Meeting of the Association for Computational Linguistics,
pages 177?180, Prague, Czech Republic, June 2007.
Thomas Lavergne, Olivier Capp?e, and Franc?ois Yvon. Practi-
cal very large scale crfs. In Proceedings of the 48th Annual
Meeting of the Association for Computational Linguistics,
pages 504?513, 2010.
Ngoc Quang Luong, Laurent Besacier, and Benjamin Lecou-
teux. Word confidence estimation and its integration in
sentence quality estimation for machine translation. In
Proceedings of The Fifth International Conference on
Knowledge and Systems Engineering (KSE 2013), Hanoi,
Vietnam, October 17-19 2013a.
Ngoc Quang Luong, Benjamin Lecouteux, and Laurent Be-
sacier. LIG system for WMT13 QE task: Investigating the
usefulness of features in word confidence estimation for
MT. In Proceedings of the Eighth Workshop on Statisti-
cal Machine Translation, pages 396?391, Sofia, Bulgaria,
August 2013b. Association for Computational Linguistics.
Preslav Nakov, Francisco Guzman, and Stephan Vogel. Op-
timizing for sentence-level bleu+1 yields short transla-
tions. In Proceedings of COLING 2012, pages 1979?1994,
Mumbai, India, December 8 -15 2012.
Franz Josef Och. Minimum error rate training in statistical
machine translation. In Proceedings of the 41st Annual
Meeting of the Association for Computational Linguistics,
pages 160?167, July 2003.
Kishore Papineni, Salim Roukos, Todd Ard, and Wei-Jing
Zhu. Bleu: a method for automatic evaluation of machine
translation. In Proceedings of the 40th Annual Meeting of
the Association for Computational Linguistics, 2002.
Kristen Parton, Nizar Habash, Kathleen McKeown, Gonzalo
Iglesias, and Adri`a de Gispert. Can automatic post-editing
make mt more meaningful? In Proceedings of the 16th
EAMT, pages 111?118, Trento, Italy, 28-30 May 2012.
M Potet, R Emmanuelle E, L Besacier, and H Blanchon.
Collection of a large database of french-english smt out-
put corrections. In Proceedings of the eighth interna-
tional conference on Language Resources and Evaluation
(LREC), Istanbul, Turkey, May 2012.
Matthew Snover, Nitin Madnani, Bonnie Dorr, and Richard
Schwartz. Terp system description. In MetricsMATR
workshop at AMTA, 2008.
Artem Sokolov, Guillaume Wisniewski, and Francois Yvon.
Non-linear n-best list reranking with few features. In Pro-
ceedings of AMTA, 2012.
Radu Soricut and Abdessamad Echihabi. Trustrank: Inducing
trust in automatic translations via ranking. In Proceedings
of the 48th ACL (Association for Computational Linguis-
tics), pages 612?621, Uppsala, Sweden, July 2010.
Andreas Stolcke. Srilm - an extensible language model-
ing toolkit. In Seventh International Conference on Spo-
ken Language Processing, pages 901?904, Denver, USA,
2002.
Nicola Ueffing and Hermann Ney. Word-level confidence
estimation for machine translation using phrased-based
translation models. In Proceedings of Human Lan-
guage Technology Conference and Conference on Empiri-
cal Methods in Natural Language Processing, pages 763?
770, Vancouver, 2005.
Nicola Ueffing, Klaus Macherey, and Hermann Ney. Con-
fidence measures for statistical machine translation. In
Proceedings of the MT Summit IX, pages 394?401, New
Orleans, LA, September 2003.
Ashish Venugopal, Andreas Zollmann, and Stephan Vogel.
An efficient two-pass approach to synchronous-cfg driven
statistical mt. In Proceedings of Human Language Tech-
nologies 2007: The Conference of the North American
Chapter of the Association for Computational Linguistics,
April 2007.
Taro Watanabe, Jun Suzuki, Hajime Tsukada, and Hideki
Isozaki. Online large-margin training for statistical ma-
chine translation. In Proceedings of the 2007 Joint Con-
ference on Empirical Methods in Natural Language Pro-
cessing and Computational Natural Language Learning,
pages 64?773,, Prague, Czech Republic, June 2007.
Deyi Xiong, Min Zhang, and Haizhou Li. Error detection
for statistical machine translation using linguistic features.
In Proceedings of the 48th Association for Computational
Linguistics, pages 604?611, Uppsala, Sweden, July 2010.
Ying Zhang, Almut Silja Hildebrand, and Stephan Vogel.
Distributed language modeling for n-best list re-ranking.
In Proceedings of the 2006 Conference on Empirical
Methods in Natural Language Processing (EMNLP 2006),
pages 216?223, Sydney, July 2006.
9
Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 335?341,
Baltimore, Maryland USA, June 26?27, 2014.
c?2014 Association for Computational Linguistics
LIG System for Word Level QE task at WMT14
Ngoc-Quang Luong Laurent Besacier
LIG, Campus de Grenoble
41, Rue des Math?ematiques,
UJF - BP53, F-38041 Grenoble Cedex 9, France
{ngoc-quang.luong,laurent.besacier,benjamin.lecouteux}@imag.fr
Benjamin Lecouteux
Abstract
This paper describes our Word-level QE
system for WMT 2014 shared task on
Spanish - English pair. Compared to
WMT 2013, this year?s task is different
due to the lack of SMT setting information
and additional resources. We report
how we overcome this challenge to retain
most of the important features which
performed well last year in our system.
Novel features related to the availability of
multiple systems output (new point of this
year) are also proposed and experimented
along with baseline set. The system
is optimized by several ways: tuning
the classification threshold, combining
with WMT 2013 data, and refining
using Feature Selection strategy on our
development set, before dealing with the
test set for submission.
1 Introduction
1.1 Overview of task 2 in WMT14
This year WMT calls for methods which predict
the MT output quality at run-time, on both levels:
sentence (Task 1) and word (Task 2). Towards
a SMT system-independent and widely-applied
estimation, MT outputs are collected from
multiple translation means (machine and human),
therefore all SMT specific settings (and the
associated features that could have been extracted
from it) become unavailable. This initiative puts
more challenges on participants, yet motivates
number of SMT-unconventional approaches and
inspires the endeavors aiming at an ?Evaluation
For All?.
We focus our effort on Task 2 (Word-level QE),
where, unlike in WMT2013, participants are
requested to generate prediction labels for words
in three variants:
? Binary: words are judged as Good (no
translation error), or Bad (need for editing).
? Level 1: the Good class is kept intact,
whereas Bad one is further divided into
subcategories: Accuracy issue (the word does
not accurately reflect the source text) and
Fluency issue (the word does not relate to the
form or content of the target text).
? Multi-class: more detailed judgement, where
the translation errors are further decomposed
into 16 labels based on MQM
1
metric.
1.2 Related work
WMT 2013 witnessed several attempts dealing
with this evaluation type in its first launch. Han
et al. (2013); Luong et al. (2013) employed the
Conditional Random Fields (CRF) (Lafferty et al.,
2001) model as their Machine Learning method
to address the problem as a sequence labeling
task. Meanwhile, Bicici (2013) extended the
global learning model by dynamic training with
adaptive weight updates in the perceptron training
algorithm. As far as prediction indicators are
concerned, Bicici (2013) proposed seven word
feature types and found among them the ?common
cover links? (the links that point from the leaf
node containing this word to other leaf nodes
in the same subtree of the syntactic tree) the
most outstanding. Han et al. (2013) focused
only on various n-gram combinations of target
words. Inheriting most of previously-recognized
features, Luong et al. (2013) integrated a number
of new indicators relying on graph topology,
pseudo reference, syntactic behavior (constituent
label, distance to the semantic tree root) and
polysemy characteristic. Optimization endeavors
were also made to enhance the baseline, including
classification threshold tuning, feature selection
and boosting technique (Luong et al., 2013).
1
http://www.qt21.eu/launchpad/content/training
335
1.3 Paper outline
The rest of our paper is structured as follows:
in the next section, we describe 2014 provided
data for Task 2, and the additional data used
to train the system. Section 3 lists the entire
feature set, involving WMT 2013 set as well as
a new feature proposed for this year. Baseline
system experiments and methods for optimizing it
are furthered discussed in Section 4 and Section
5 respectively. Section 6 selects the most
outstanding system for submission. The last
section summarizes the approach and opens new
outlook.
2 Data and Supporting Resources
For English - Spanish language pair in Task 2,
the organizers released two bilingual data sets:
the training and the test ones. The training
set contains 1.957 MT outputs, in which each
token is annotated with one appropriate label.
In the binary variant, the words are classified
into ?OK? (no translation error) or ?BAD? (edit
operators needed) label. Meanwhile, in the level
1 variant, they belong to ?OK?, ?Accuracy?
or ?Fluency? (two latter ones are divided from
?BAD? label of the first subtask). In the last
variant, multi-class, beside ?Accuracy? and
?Fluency? we have further 15 labels based on
MQM metric: Terminology, Mistranslation,
Omission, Addition, Untranslated, Style/register,
Capitalization, Spelling, Punctuation,
Typography, Morphology (word form),
Part of speech, Agreement, Word order,
Function words, Tense/aspect/mood, Grammar
and Unintelligible. The test set consists of 382
sentences where all the labels accompanying
words are hidden. For optimizing parameters of
the classifier, we extract last 200 sentences from
the training set to form a development (dev) set.
Besides, the Spanish - English corpus provided in
WMT 2013 (total of 1087 tuples) is also exploited
to enrich our WMT 2014 system. Unfortunately,
2013 data can only help us in the binary variant,
due to the discrepancy in training labels. Some
statistics about each set can be found in Table 1.
In addition, additional (MT-independent)
resources are used for the feature extraction,
including:
? Spanish and English Word Language Models
(LM)
? Spanish and English POS Language Models
? Spanish - English 2013 MT system
On the contrary, no specific MT setting is provided
(e.g. the code to re-run Moses system like
WMT 2013), leading to the unavailability of some
crucial resources, such as the N-best list and
alignment information. Coping with this, we
firstly thought of using the Moses ?Constrained
Decoding? option as a method to tie our (already
available) decoder?s output to the given target
translations (this feature is supported by the
latest version of Moses (Koehn et al., 2007) in
2013). Our hope was that, by doing so, both
N-best list and alignment information would be
generated during decoding. But the decoder
failed to output all translations (only 1/4 was
obtained) when the number of allowed unknown
words (-max-unknowns) was set as 0. Switching
to non zero value for this option did not help
either since, even if more outputs were generated,
alignment information was biased in that case
due to additional/missing words in the obtained
MT output. Ultimately, we decided to employ
GIZA++ toolkit (Och and Ney, 2003) to obtain
at least the alignment information (and associated
features) between source text and target MT
output. However, no N-best list were extracted
nor available as in last year system. Nevertheless,
we tried to extract some features equivalent to
last year N-best features (details can be found in
Section 3.2).
3 Feature Extraction
In this section, we briefly list out all the
features used in WMT 2013 (Luong et al.,
2013) that were kept for this year, followed
by some proposed features taking advantage of
the provided resources and multiple translation
system outputs (for a same source sentence).
3.1 WMT13 features
? Source word features: all the source words
that align to the target one, represented in
BIO
2
format.
? Source alignment context features: the
combinations of the target word and one
word before (left source context) or after
(right source context) the source word
aligned to it.
2
http://www-tsujii.is.s.u-tokyo.ac.jp/GENIA/tagger/
336
Statistics WMT2014 WMT2013
train dev test train dev test
#segments 1757 200 382 753 50 284
#words 40975 6436 9613 18435 1306 7827
%G (OK) : %B (BAD) 67 : 33 58 : 42 - 70 : 30 77 : 23 75 : 25
Table 1: Statistics of corpora used in LIG?s system. We use the notion name+year to indicate the dataset.
For instance, train14 stands for the training set of WMT14
? Target alignment context features: the
combinations of the source word and each
word in the window ?2 (two before, two
after) of the target word.
? Backoff Behaviour: a score assigned to the
word according to how many times the target
Language Model has to back-off in order to
assign a probability to the word sequence, as
described in (Raybaud et al., 2011).
? Part-Of-Speech (POS) features (using
TreeTagger
3
toolkit): The target word?s POS;
the source POS (POS of all source words
aligned to it); bigram and trigram sequences
between its POS and the POS of previous
and following words.
? Binary lexical features that indicate whether
the word is a: stop word (based on the stop
word list for target language), punctuation
symbol, proper name or numerical.
? Language Model (LM) features: the ?longest
target n-gram length? and ?longest source
n-gram length?(length of the longest
sequence created by the current target
(source aligned) word and its previous ones
in the target (source) LM). For example,
with the target word w
i
: if the sequence
w
i?2
w
i?1
w
i
appears in the target LM but
the sequence w
i?3
w
i?2
w
i?1
w
i
does not, the
n-gram value for w
i
will be 3.
? The word?s constituent label and its depth in
the tree (or the distance between it and the
tree root) obtained from the constituent tree
as an output of the Berkeley parser (Petrov
and Klein, 2007) (trained over a Spanish
treebank: AnCora
4
).
? Occurrence in Google Translate hypothesis:
we check whether this target word appears in
3
http://www.ims.uni-stuttgart.de/projekte/corplex/TreeTagger/
4
http://clic.ub.edu/corpus/en/ancora
the sentence generated by Google Translate
engine for the same source.
? Polysemy Count: the number of senses of
each word given its POS can be a reliable
indicator for judging if it is the translation
of a particular source word. Here, we
investigate the polysemy characteristic in
both target word and its aligned source word.
For source word (English), the number
of senses can be counted by applying a
Perl extension named Lingua::WordNet
5
,
which provides functions for manipulating
the WordNet database. For target word
(Spanish), we employ BabelNet
6
- a
multilingual semantic network that works
similarly to WordNet but covers more
European languages, including Spanish.
3.2 WMT14 additional features
? POS?s LM based features: we exploit
the Spanish and English LMs of POS
tag (provided as additional resources for
this year?s QE tasks) for calculating the
maximum length of the sequences created
by the current target token?s POS and those
of previous ones. The same score for POS
of aligned source word(s) is also computed.
Besides, the back-off score for word?s POS
tag is also taken into consideration. Actually,
these feature types are listed in Section
3.1 for target word, and we proposed the
similar ones for POS tags. In summary, three
POS LM?s new features are built, including:
?longest target n-gram length?, ?longest
source n-gram length? and back-off score for
POS tag.
? Word Occurrence in multiple translations:
one novel point in this year?s shared task
is that the targets come from multiple MT
5
http://search.cpan.org/dist/Lingua-Wordnet/Wordnet.pm
6
http://babelnet.org
337
outputs (from systems or from humans) for
the same source sentences. Obviously, one
would have a ?natural? intuition that: the
occurrence of a word in all (or almost)
systems implies a higher likelihood of being
a correct translation. Relying on this
observation, we add a new binary-value
feature, telling whether the current token
can be found in more than N% (in our
experiments, we choose N = 50) out
of all translations generated for the same
source sentence. Here, in order to make
the judgments more accurate, we propose
several additional references besides those
provided in the corpora, coming from: (1)
Google Translate system, (2) The baseline
SMT engine provided for WMT2013 English
- Spanish QE task. These two MT outputs
are added to the already available MT outputs
of a given source sentence, before calculating
the (above described) binary feature.
4 Baseline Experiments and
Optimization Strategies
4.1 Machine Learning Method
Motivated by the idea of addressing Word
Confidence Estimation (WCE) problem as
a sequence labeling process, we employ the
Conditional Random Fields (CRFs) for our
model training, with WAPITI toolkit (Lavergne
et al., 2010). Let X = (x
1
, x
2
, . . . , x
N
) be the
random variable over data sequence to be labeled,
Y = (y
1
, y
2
, . . . , y
N
) be the output sequence
obtained after the labeling task. Basically, CRF
computes the probability of the output sequence
Y given the input sequence X by:
p
?
(Y |X) =
1
Z
?
(X)
exp
{
K
?
k=1
?
k
F
k
(X,Y )
}
(1)
where F
k
(X,Y ) =
?
T
t=1
f
k
(y
t?1
, y
t
, x
t
);
{f
k
} (k = 1,K) is a set of feature functions;
{?
k
} (k = 1,K) are the associated parameter
values; and Z
?
(x) is the normalization function.
In the training phase, we set the maximum
number of iterations, the stop window size,
and stop epsilon value at 200; 6 and 0.00005
respectively.
System Label Pr(%) Rc(%) F(%)
BL(bin) OK 66.67 81.92 73.51
Bad 60.69 41.92 49.58
BL(L1) OK 63.86 82.83 72.12
Accuracy 22.14 14.89 17.80
Fluency 50.40 27.98 35.98
BL(mult) OK 63.32 87.56 73.49
Fluency 14.44 10.10 11.88
Mistranslation 9.95 5.69 7.24
Terminology 3.62 3.89 3.75
Unintelligible 52.97 16.56 25.23
Agreement 5.93 11.76 7.88
Untranslated 5.65 7.76 6.53
Punctuation 56.97 25.82 35.53
BL+WMT OK 68.62 82.69 75.01
13(bin) Bad 64.38 45.73 53.47
Table 2: Average Pr, Rc and F for labels
of all-feature binary and multi-class systems,
obtained on our WMT 2014 dev set (200
sentences). In BL(multi), classes with zero value
for Pr or Rc will not be reported
4.2 Experimental Classifiers
We experiment with the following classifiers:
? BL(bin): all features (WMT14+WMT13)
trained on train14 only, using binary labels
(?OK? and ?BAD?)
? BL(L1): all features trained on train14 only,
using level 1 labels (?OK?, ?Accuracy?, and
?Fluency?)
? BL(mult): all features trained on train14
only, using 16 labels
? BL+WMT13(bin): all features trained on
train14 + {train+dev+test}13, using binary
labels.
System quality in Precision (Pr), Recall (Rc) and
F score (F) are shown in Table 2. It can be
observed that promising results are found in binary
variant where both BL(bin) and BL+WMT(bin)
are able to reach at least 50% F score in detecting
errors (BAD class), meanwhile the performances
in ?OK? class go far beyond (73.51% and 75.01%
respectively). Interestingly, the combination
with WMT13 data boosts the baseline prediction
capability in both labels: BL+WMT13(bin)
outperforms BL(bin) in 1.10% ( 3.89%) for OK
(BAD) label. Nevertheless, level 1 and multi-class
systems maintain only good score for ?OK? class.
In addition, BL(mult) seems suffer seriously
from its class imbalance, as well as the lack of
training data for each, resulting in the inability
of prediction for several among them (not all are
reported in Table 2 ).
338
4.3 Decision threshold tuning for binary task
In binary systems BL(bin) and
BL+WMT13(bin), we run the classification
task multiple times, corresponding to a decision
threshold increase from 0.300 to 0.975 (step
= 0.025). The values of Precision (Pr), Recall
(Rc) and F-score (F) for OK and BAD label are
tracked along this threshold variation, allowing
us to select the optimal threshold that yields the
highest F
avg
=
F (OK)+F (BAD)
2
. Figure 1 shows
that BL(bin) reaches the best performance at the
threshold value of 0.95, meanwhile the one for
BL+WMT13(bin) is 0.75. The latter threshold
(0.75) has been used for the primary system
submitted.
Figure 1: Decision threshold tuning on BL(bin)
and BL+WMT2013(bin)
4.4 Feature Selection
In order to improve the preliminary scores
of all-feature systems, we conduct a feature
selection which is based on the hypothesis
that some features may convey ?noise? rather
than ?information? and might be the obstacles
weakening the other ones. In order to prevent
this drawback, we propose a method to filter the
best features based on the ?Sequential Backward
Selection? algorithm
7
. We start from the full set of
N features, and in each step sequentially remove
the most useless one. To do that, all subsets of
(N-1) features are considered and the subset that
leads to the best performance gives us the weakest
feature (not involved in the considered set). This
procedure is also called ?leave one out? in the
literature. Obviously, the discarded feature is not
considered in the following steps. We iterate the
7
http://research.cs.tamu.edu/prism/lectures/pr/pr l11.pdf
process until there is only one remaining feature in
the set, and use the following score for comparing
systems: F
avg
(all) =
F
avg
(OK)+F
avg
(BAD)
2
,
where F
avg
(OK) and F
avg
(BAD) are the
averaged F scores for OK and BAD label,
respectively, when threshold varies from 0.300 to
0.975. This strategy enables us to sort the features
in descending order of importance, as displayed
in Table 3. Figure 2 shows the evolution of
the performance as more and more features are
removed. The feature selection is done from the
BL+WMT2013(bin) system.
We observe in Table 3 four valuable features
which appear in top 10 in both WMT13
and WMT14 systems: Source POS, Occur in
Google Translate, Left source context and Right
target context. Among our proposed features,
?Occurrence in multiple systems? is the most
outstanding one with rank 3, ?longest target POS
gram length? plays an average role with rank 12,
whereas ?longest source POS gram length? is
much less beneficial with the last position in the
list. Figure 2 reveals that the optimal subset of
features is the top 18 in Table 3, after discarding 6
weakest ones. This set will be used to train again
the classifiers in all subtasks and compare to the
baseline ones.
Figure 2: The evolution of the performance
as more and more features are removed (from
BL+WMT2013(bin) system)
5 Submissions
After finishing the optimization process and
comparing systems, we select two most
out-standing ones (of each subtask) for the
submission of this year?s shared task. They are
the following:
? Binary variant: BL+WMT13(bin) and
FS(bin) (feature selection from the same
corresponding system)
? Level 1 variant: BL(L1) and FS(L1) (feature
selection from the same corresponding
system)
339
Rank WMT2014 WMT2013
1 Target POS Source POS
2 Longest target gram length Occur in Google Translate
3 Occurrence in multiple systems Nodes
4 Target word Target POS
5 Occur in Google Translate WPP any
6 Source POS Left source context
7 Numeric Right target context
8 Polysemy count (target) Numeric
9 Left source context Polysemy count(target)
10 Right Target context Punctuation
11 Constituent label Stop word
12 Longest target POS gram length Right source context
13 Punctuation Target word
14 Stop word Distance to root
15 Number of occurrences Backoff behaviour
16 Left target context Constituent label
17 Backoff behaviour Proper name
18 Polysemy count (source) Number of occurrences
19 Source Word Min
20 Proper Name Max
21 Distance to root Left target context
22 Longest source gram length Polysemy count (source)
23 Right source context Longest target gram length
24 Longest source POS gram length Longest source gram length
25 Source Word
Table 3: The rank of each feature (in term of usefulness) in WMT2014 and WMT2013 systems. The
bold ones perform well in both cases. Note that feature sets are not exactly the same for 2013 and 2014
(see explanations in section 3).
? Multi-class variant: BL(mult) and
FS(mult) (feature selection from the
same corresponding system)
The official results can be seen in Table 4. This
year, in order to appreciate the translation error
detection capability of WCE systems, the official
evaluation metric used for systems ranking is the
average F score for all but the ?OK? class. For
the non-binary variant, this average is weighted
by the frequency of the class in the test data.
Nevertheless, we find the F scores for ?OK? class
are also informative, since they reflect how good
our systems are in identifying correct translations.
Therefore, both scores are reported in Table 4.
6 Conclusion and perspectives
We presented our preparation for this year?s shared
task on QE at word level, for the English - Spanish
language pair. The lack of some information
on MT system internals was a challenge. We
made efforts to maintain most of well-performing
System F(?OK?) (%) Average F(%)
FS(bin) (primary) 74.0961 0.444735
FS(L1) 73.9856 0.317814
FS(mult) 76.6645 0.204953
BL+WMT2013(bin) 74.6503 0.441074
BL(L1) 74.0045 0.317894
BL(mult) 76.6645 0.204953
Table 4: The F scores for ?OK? class and the
average F scores for the remaining classes (official
WMT14 metric) , obtained on test set.
2013 features, especially the source side ones,
and propose some novel features based on this
year?s corpus specificities, as well as combine
them with those of last year. Generally, our
results are not able to beat those in WMT13 for
the same language pair, yet still promising under
these constraints. As future work, we are thinking
of using more efficiently the existing references
(coming from provided translations and other
reliable systems) to obtain stronger indicators, as
340
well as examine other ML methods besides CRF.
References
Ergun Bicici. Referential translation machines
for quality estimation. In Proceedings of
the Eighth Workshop on Statistical Machine
Translation, pages 343?351, Sofia, Bulgaria,
August 2013. Association for Computational
Linguistics. URL http://www.aclweb.
org/anthology/W13-2242.
Aaron Li-Feng Han, Yi Lu, Derek F. Wong,
Lidia S. Chao, Liangye He, and Junwen Xing.
Quality estimation for machine translation
using the joint method of evaluation criteria
and statistical modeling. In Proceedings of
the Eighth Workshop on Statistical Machine
Translation, pages 365?372, Sofia, Bulgaria,
August 2013. Association for Computational
Linguistics. URL http://www.aclweb.
org/anthology/W13-2245.
Philipp Koehn, Hieu Hoang, Alexandra Birch,
Chris Callison-Burch, Marcello Federico,
Nicola Bertoldi, Brooke Cowan, Wade Shen,
Christine Moran, Richard Zens, Chris Dyer,
Ondrej Bojar, Alexandra Constantin, and
Evan Herbst. Moses: Open source toolkit for
statistical machine translation. In Proceedings
of the 45th Annual Meeting of the Association
for Computational Linguistics, pages 177?180,
Prague, Czech Republic, June 2007.
John Lafferty, Andrew McCallum, and
Fernando Pereira. Conditional random
fields: Probabilistic models for segmenting
et labeling sequence data. In Proceedings of
ICML-01, pages 282?289, 2001.
Thomas Lavergne, Olivier Capp?e, and Franc?ois
Yvon. Practical very large scale crfs. In
Proceedings of the 48th Annual Meeting of
the Association for Computational Linguistics,
pages 504?513, 2010.
Ngoc Quang Luong, Laurent Besacier, and
Benjamin Lecouteux. Word confidence
estimation and its integration in sentence
quality estimation for machine translation.
In Proceedings of the fifth international
conference on knowledge and systems
engineering (KSE), Hanoi, Vietnam, October
2013.
Franz Josef Och and Hermann Ney. A systematic
comparison of various statistical alignment
models. Computational Linguistics, 29(1):
19?51, 2003.
Slav Petrov and Dan Klein. Improved inference
for unlexicalized parsing. In Proceedings of
NAACL HLT 2007, pages 404?411, Rochester,
NY, April 2007.
S. Raybaud, D. Langlois, and K. Sma?? li. ?this
sentence is wrong.? detecting errors in machine
- translated sentences. In Machine Translation,
pages 1?34, 2011.
Matthew Snover, Nitin Madnani, Bonnie Dorr,
and Richard Schwartz. Terp system description.
In MetricsMATR workshop at AMTA, 2008.
341
