167
168
169
170
Generating Spatio-Temporal Descriptions in Pollen Forecasts
Ross Turner, Somayajulu Sripada and Ehud Reiter
Dept of Computing Science,
University of Aberdeen, UK
{rturner,ssripada,ereiter}@csd.abdn.ac.uk
Ian P Davy
Aerospace and Marine International,
Banchory, Aberdeenshire, UK
idavy@weather3000.com
Abstract
We describe our initial investigations into
generating textual summaries of spatio-
temporal data with the help of a prototype
Natural Language Generation (NLG) system
that produces pollen forecasts for Scotland.
1 Introduction
New monitoring devices such as remote sensing sys-
tems are generating vast amounts of spatio-temporal
data. These devices, coupled with the wider accessi-
bility of the data, have spurred large amounts of re-
search into how it can best be analysed. There has been
less research however, into how the results of the data
analysis can be effectively communicated. As part of
a wider research project aiming to produce textual re-
ports of complex spatio-temporal data, we have devel-
oped a prototype NLG system which produces textual
pollen forecasts for the general public.
Pollen forecast texts describe predicted pollen con-
centration values for different regions of a country.
Their production involves two subtasks; predicting
pollen concentration values for different regions of a
country, and describing these numerical values textu-
ally.In our work, we focus on the later subtask, tex-
tual description of spatio-temporally distributed pollen
concentration values. The subtask of predicting pollen
concentrations is carried out by our industrial collab-
orator, Aerospace and Marine International (UK) Ltd
(AMI).
A fairly substantial amount of work already exists
on weather forecast generation. A number of systems
have been developed and are currently in commercial
use with two of the most notable being FOG (Goldberg
et al, 1994) and MultiMeteo (Coch, 1998).
2 Knowledge Acquisition
Our knowledge acquisition activities consisted of cor-
pus studies and discussions with experts. We have
collected a parallel corpus (69 data-text pairs) of
pollen concentration data and their corresponding hu-
man written pollen reports which our industrial collab-
orator has provided for a local commercial television
station. The forecasts were written by two expert mete-
orologists, one of whom provided insight into how the
forecasts were written. An example of a pollen fore-
cast text is shown in Figure 1, its corresponding data is
shown in table 1. A pollen forecast in the map form is
shown in Figure 2.
?Monday looks set to bring another day of
relatively high pollen counts, with values up
to a very high eight in the Central Belt. Fur-
ther North, levels will be a little better at a
moderate to high five to six. However, even
at these lower levels it will probably be un-
comfortable for Hay fever sufferers.?
Figure 1: Human written pollen forecast text for the
pollen data shown in table 1
Figure 2: Pollen forecast map for the pollen data shown
in table 1
Analysis of a parallel corpus (texts and their under-
lying data) can be performed in two stages:
? In the first stage, traditional corpus analysis pro-
cedure outlined in (Reiter and Dale, 2000) and
(Geldof, 2003) can be used to analyse the pollen
forecast texts (the textual component of the paral-
lel corpus). This stage will identify the different
message types and uncover the sub language of
the pollen forecasts.
? In the second stage the more recent analysis meth-
ods developed in the SumTime project (Reiter et
163
ValidDate AreaID Value
27/06/2005 1 (North) 6
27/06/2005 2 (North West) 5
27/06/2005 3 (Central) 5
27/06/2005 4 (North East) 6
27/06/2005 5 (South West) 8
27/06/2005 6 (South East) 8
Table 1: Pollen Concentration Data for Scotland - Input
data for Figures 1 and 2
al., 2003) which exploit the availability of the un-
derlying pollen data corresponding to the forecast
texts can be used to map messages to input data
and also map parts of the sub language such as
words to the input data. Due to the fact that we
are modeling the task of automatically producing
pollen forecast texts from predicted pollen con-
centration values, knowledge of how to map in-
put data to messages and words/phrases is abso-
lutely necessary. Studies connecting language to
data are useful for understanding the semantics of
language in a more novel way than the traditional
logic-based formalisms (Roy and Reiter, 2005).
We have performed the first stage of the corpus anal-
ysis and part of the second stage so far. In the first
stage, we abstracted out the different message types
from the forecast texts (Reiter and Dale, 2000). These
are shown in Table 2. The main two message types
are forecast messages and trend messages. The for-
mer communicate the actual pollen forecast data (the
communicative goal) and the latter describe patterns in
pollen levels over time as shown in Figure 3
?Grass pollen counts continue to ease from
the recent high levels?
Figure 3: A trend message describing a fall in pollen
levels
Table 2 also shows three other identified message
types. We have ignored both the forecast explanation
and general message types in our system development
because they cannot be generated from pollen data
alone. For example, the explanation type messages ex-
plain the weather conditions responsible for the pollen
predictions. Hayfever messages in our system are rep-
resented as canned text. Examples of a forecast ex-
planation message and hayfever message are shown in
Figure 4 and Figure 5 respectively.
From our corpus analysis we have also been able to
learn the text structure for pollen forecasts. The fore-
casts normally start with a trend message and then in-
clude a number of forecast messages. Where hayfever
messages are present, they normally occur at the end of
the forecast.
Due to the fact that the input to our pollen text gen-
?Windier and wetter weather over last 24
hours has dampened down the grass pollen
count?
Figure 4: An example forecast explanation message
?Even though values are mostly low, those
sensitive to pollen may still be affected?
Figure 5: An example hayfever message
erator is the pollen data in numerical form, as part of
the second stage of the corpus analysis we need to map
the input data to the messages. In earlier ?numbers
to text? NLG systems such as SumTime (Sripada et
al., 2003) and TREND (Boyd, 1998), well known data
analysis techniques such as segmentation and wavelet
analysis were employed for this task. Since pollen data
is spatio-temporal we need to employ spatio-temporal
data analysis techniques to achieve this mapping. We
describe our method in the next section.
Our corpus analysis revealed that forecast texts con-
tain a rich variety of spatial descriptions for a location.
For example, the same region could be referred to by
it?s proper name e.g. ?Suthlerland and Caithness? or
by its? relation to a well known geographical landmark
e.g. ?North of the Great Glen? or simply by its? geo-
graphical location on the map e.g. ?the far North and
Northwest?. In the context of pollen forecasts which
describe spatio-temporal data, studying the semantics
of phrases or words used for describing locations or re-
gions is a challenge. We are currently analysing the
forecast texts along with the underlying data to under-
stand how spatial descriptions map to the underlying
data using the methods applied in the SumTime project
(Sripada et al, 2003).
As part of this analysis, in a seperate study, we asked
twenty four further education students in the Glasgow
area of Scotland a Geography question. The question
asked how many out of four major place names in Scot-
land did they consider to be in the south west of the
country. The answers we got back were very mixed
with a sizeable number of respondents deciding that
the only place we considered definitely not to be in the
south west of Scotland was in fact there.
3 Spatio-temporal Data Analysis
We have followed the pipeline architecture for text gen-
eration outlined in (Reiter and Dale, 2000). The mi-
croplanning and surface realisation modules from the
Sumtime project (Sripada et al, 2003) have largely
been reused. We have developed new data analysis
and document planning modules for the system and de-
scribe the data analysis module in the rest of this sec-
tion. The data analysis module performs segmentation
and trend detection on the data before providing the re-
sults as input to the Natural Language Generation Sys-
164
Message Type Data Dependency Corpus Coverage
Forecast Pollen data for day of forecast 100%
Trend Past/Future pollen forecasts 54%
Forecast Explanation Weather forecast for day of forecast 35%
Hayfever Pollen levels affect hay fever 23%
General General Domain Knowledge 17%
Table 2: Message Categorisation of the Pollen Corpus
tem. An example of the input data to our system is
shown in Table 1. Our data analysis is based on three
steps:-
1. segmentation of the geographic regions by their
non-spatial attributes (pollen values)
2. further segmentation of the segmented geographic
regions by their spatial attributes (geographic
proximity)
3. detection of trends in the generalised pollen level
for the whole region over time
3.1 Segmentation
The task of segmentation consists of two major sub-
tasks, clustering and classification (Miller and Han,
2001). Spatial clustering involves grouping objects into
similar subclasses, whereas spatial classification in-
volves finding a description for those subclasses which
differentiates the clustered objects from each other (Es-
ter et al, 1998).
Pollen values are measured on a scale of 1 to 10(low
to very high). We defined 4 initial categories for seg-
mentation, these are:-
1. VeryHigh - {8,9,10}
2. High - {6,7}
3. Moderate - {4,5}
4. Low - {1,2,3}
These categories proved rather rigid for our pur-
poses. This was due to the fact that human forecasters
take a flexible approach to classifying pollen values.
For example, in the corpus the pollen value of 4 could
be referred to as both a moderate level of pollen and a
low-to-moderate level of pollen. This lead us to define
3 further categories which are derived from our 4 initial
categories:-
5. LowModerate - {3,4}
6. ModerateHigh - {5,6}
7. HighVeryhigh - {7,8}
Thus, the initial segmentation of data carried out by
our system is a two stage process. Firstly regions are
clustered into the initial four categories by pollen value.
The second stage involves merging adjacent categories
that only contain regions with adjacent values. For ex-
ample if we take the input data from Table 1, after the
first stage we have the sets:-
? {{AreaID=2,Value=5},{AreaID=3,Value=5}}
? {{AreaID=1,Value=6},{AreaID=4,Value=6}}
? {{AreaID=5,Value=8},{AreaID=6,Value=8}}
In stage two we create the union of the moderate and
high sets to give:-
? {{AreaID=1,Value=6},{AreaID=2,Value=5},
{AreaID=3,Value=5},{AreaID=4,Value=6}}
? {{AreaID=5,Value=8},{AreaID=6,Value=8}}
Although this initial segmentation could be accom-
plished all in one step, completing it in two steps pro-
vided a more simple software engineering solution.
We can now carry out further segmentation of these
sets according to their spatial attributes. In our set of
regions with ModerateHigh pollen levels we can see
that AreaIDs 1,2,3,4 are in fact all spatial neighbours.
The north, north east and north west regions can be
described spatially as the northern part of the country.
Therefore we can now say that ?Pollen levels are at a
moderate to high 5 or 6 in the northern and central
parts of the country? . Similarly, as the two members of
our set containing regions with VeryHigh pollen levels
are also spatial neighbours we can also say that ?Pollen
levels are at a very high level 8 in the south of the coun-
try?. This process now yields the following two sets:-
? {{AreaID=1234,Value=[5,6]}}
? {{AreaID=56,Value=[8]}}
Our two sets we have now created can now be passed
to the Document Planner were they will be encapsu-
lated as individual Forecast messages.
3.2 Trend Detection
Trend detection in our system works by generalising
over all sets created by segmentation. From our two
sets we can say that generally pollen levels are high
over the whole of Scotland. Looking at the previous
days forecast we can detect a trend by comparing the
two generalisations. If the previous days forecast was
also high we can say ?pollen levels remain at the high
165
levels of yesterday?. By looking further back, and if
those previous days were also high, we can say ?pollen
levels remain at the high levels of recent days?. If the
previous days forecast was low, we can say ?pollen lev-
els have increased from yesterdays low levels?. Our
data analysis module then conveys the information that
there is a relation between the general pollen level
of today and the general pollen level of some recent
timescale to the Document Planner, which then encap-
sulates the information as a Trend message.
After the results of data analysis have been input into
the NLG pipeline the output in Figure 6 is produced.
?Grass pollen levels for Monday remain at
the moderate to high levels of recent days
with values of around 5 to 6 across most parts
of the country. However, in southern areas,
pollen levels will be very high with values of
8.?
Figure 6: The output text from our system for the input
data in Table 1
4 Evaluation
A demo of the pollen forecasting system can be found
on the internet at 1. The evaluation of the system is be-
ing carried out in two stages. The first stage has used
this demo to obtain feedback from expert meteorolo-
gists at AMI. We found the feedback on the system to
be very positive and hope to deploy the system for the
next pollen season. Two main areas identified for im-
provement of the generated texts:-
? Use of a more varied amount of referring expres-
sions for geographic locations.
? An ability to vary the length of the text dependent
on the context it was being used, i.e in a newspa-
per or being read aloud.
These issues will be dealt with subsequent releases
of the software. The second and more thorough evalu-
ation will be carried out when the system is deployed.
5 Further Research
The current work on pollen forecasts is carried out as
part of RoadSafe2 a collaborative research project be-
tween University of Aberdeen and Aerospace and Ma-
rine International (UK) Ltd. The main objective of
the project is to automatically generate road mainte-
nance instructions to ensure efficient and correct ap-
plication of salt and grit to the roads during the win-
ter. The core requirement of this project is to describe
spatio-temporal data of detailed weather and road sur-
face temperature predictions textually. In a previous
1www.csd.abdn.ac.uk/?rturner/cgi bin/pollen.html
2www.csd.abdn.ac.uk/?rturner/RoadSafe/
research project SumTime (Sripada et al, 2003) we
have developed techniques for producing textual sum-
maries of time series data. In RoadSafe we plan to ex-
tend these techniques to generate textual descriptions
of spatio-temporal data. Because the spatio-temporal
weather prediction data used in road maintenance ap-
plications is normally of the order of a megabyte, we
initially studied pollen forecasts which are based on
smaller spatio-temporal data sets. We will apply the
various techniques we have learnt from the study of
pollen forecasts to the spatio-temporal data from the
road maintenance application.
6 Summary
Automatically generating spatio-temporal descriptions
involves two main subtasks. The first subtask focuses
on the spatio-temporal analysis of the input data to
extract information required by the different message
types identified in the corpus analysis. The second sub-
task is to find appropriate linguistic form for the spatial
location or region information.
References
S. Boyd. 1998. Trend: a system for generating in-
telligent descriptions of time-series data. In IEEE
International Conference on Intelligent Processing
Systems (ICIPS1998).
J. Coch. 1998. Multimeteo: multilingual production
of weather forecasts. ELRA Newsletter, 3(2).
M. Ester, A. Frommelt, H. Kriegel, and J. Sander.
1998. Algorithms for characterization and trend de-
tection in spatial databases. In KDD, pages 44?50.
S. Geldof. 2003. Corpus analysis for nlg. cite-
seer.ist.psu.edu/583403.html.
E. Goldberg, N. Driedger, and R. Kittredge. 1994. Us-
ing natural-language processing to produce weather
forecasts. IEEE Expert, 9(2):45?53.
H. J. Miller and J. Han. 2001. Geographic Data Min-
ing and Knowledge Discovery. Taylor and Francis.
E. Reiter and R. Dale. 2000. Building Natural Lan-
guage Generation Systems. Cambridge University
Press.
E. Reiter, S. Sripada, and R. Robertson. 2003. Ac-
quiring correct knowledge for natural language gen-
eration. Journal of Artificial Intelligence Research,
18:491?516.
D. Roy and E. Reiter. 2005. Connecting language to
the world. Artificial Intelligence, 167:1?12.
S. Sripada, E. Reiter, and I. Davy. 2003. Sumtime-
mousam: Configurable marine weather forecast gen-
erator. Expert Update, 6:4?10.
166

c? 2002 Association for Computational Linguistics
Squibs and Discussions
Human Variation and Lexical Choice
Ehud Reiter? Somayajulu Sripada?
University of Aberdeen University of Aberdeen
Much natural language processing research implicitly assumes that word meanings are fixed in
a language community, but in fact there is good evidence that different people probably associate
slightly different meanings with words. We summarize some evidence for this claim from the
literature and from an ongoing research project, and discuss its implications for natural language
generation, especially for lexical choice, that is, choosing appropriate words for a generated text.
1. Introduction
A major task in natural language generation (NLG) is lexical choice, that is, choosing
lexemes (words) to communicate to the reader the information selected by the system?s
content determination module. From a semantic perspective lexical choice algorithms
are based on models of word meanings, which state when a word can and cannot be
used; of course, lexical choice algorithms may also consider syntactic constraints and
pragmatic features when choosing words.
Such models assume that it is possible to specify what a particular word means to
a particular user. However, both the cognitive science literature and recent experiments
carried out in the SUMTIME project at the University of Aberdeen, of which the current
authors are a part, suggest that this may be difficult to do because of variations among
people, that is, because the same word may mean different things to different people.
More precisely, although people may agree at a rough level about what a word means,
they may disagree about its precise definition, and in particular, to what objects or
events a word can be applied. This means that it may be impossible even in principle
to specify precise word meanings for texts with multiple readers, and indeed for texts
with a single reader, unless the system has access to an extremely detailed user model.
A corpus study in our project also showed that there were differences in which words
individuals used (in the sense that some words were used only by a subset of the
authors) and also in how words were orthographically realized (spelled).
This suggests that it may be risky for NLG systems (and indeed human authors) to
depend for communicative success on the human reader?s interpreting words exactly
as the system intends. This in turn suggests that perhaps NLG systems should be
cautious in using very detailed lexical models and also that it may be useful to add
some redundancy to texts in case the reader does not interpret a word as expected.
This is especially true in applications in which each user reads only one generated text;
if users read many generated texts, then perhaps over time they will learn about and
adapt to the NLG system?s lexical usage. Human variability also needs to be taken
into account by natural language processing (NLP) researchers performing corpora
analyses; such analyses should not assume that everyone uses identical rules when
making linguistic decisions.
? Department of Computing Science, University of Aberdeen, Aberdeen AB24 3UE, UK. E-mail:
ereiter@csd.abdn.ac.uk
? Department of Computing Science, University of Aberdeen, Aberdeen AB24 3UE, UK. E-mail:
ssripada@csd.abdn.ac.uk
546
Computational Linguistics Volume 28, Number 4
2. Evidence for Human Lexical Variation
2.1 Previous Research
Linguists have acknowledged that people may associate different meanings with the
same word. Nunberg (1978, page 81), for example, writes:
There is considerable variation among speakers in beliefs about what
does and does not constitute a member of the category. . . . Take jazz. I
may believe that the category includes ragtime, but not blues; you may
believe the exact opposite. After all, we will have been exposed to a
very different set of exemplars. And absent a commonly accepted au-
thority, we must construct our own theories of categories, most prob-
ably in the light of varying degrees of musical sophistication.
Many modern theories of mental categorization (Rosch 1978; Smith and Medin 1981)
assume that mental categories are represented by prototypes or exemplars. Therefore,
if different people are exposed to different category prototypes and exemplars, they
are likely to have different rules for evaluating category membership.
Parikh (1994) made a similar point and backed it up with some simple experimen-
tation. For example, he showed squares from the Munsell chart to participants and
asked them to characterize the squares as red or blue; different individuals character-
ized the squares in different ways. In another experiment he showed that differences
remained even if participants were allowed to associate fuzzy-logic-type truth values
with statements.
In the psychological community, Malt et al (1999) investigated what names par-
ticipants gave to real-world objects. For example, these researchers wished to know
whether participants would describe a pump-top hand lotion dispenser as a bottle or a
container. They were primarily interested in variations across linguistic communities,
but they also discovered that even within a linguistic community there were differ-
ences in how participants named objects. They state (page 242) that only 2 of the 60
objects in their study were given the same name by all of their 76 native-English-
speaker participants.
In the lexicographic community, field-workers for the Dictionary of American Re-
gional English (DARE) (Cassidy and Hall 1996) asked a representative set of Americans
to respond to fill-in-the-blank questionnaires. The responses they received revealed
substantial differences among participants. For example, there were 228 different re-
sponses to question B12, When the wind begins to increase, you say it?s , the most
common of which were getting windy and blowing up; and 201 different responses to
question B13, When the wind begins to decrease, you say it?s , the most common of
which were calming down and dying down.
2.2 SUMTIME Project
The SUMTIME project at the University of Aberdeen is researching techniques for gen-
erating summaries of time-series data.1 Much of the project focuses on content deter-
mination (see, for example, Sripada et al [2001]), but it is also examining lexical choice
algorithms for time-series summaries, which is where the work described in this ar-
ticle originated. To date, SUMTIME has primarily focused on two domains, weather
forecasts and summaries of gas turbine sensors, although we have recently started
1 See ?http://www.csd.abdn.ac.uk/research/sumtime? for general information about SUMTIME.
547
Reiter and Sripada Human Variation and Lexical Choice
work in a third domain as well, summaries of sensor readings in neonatal intensive
care units.
2.2.1 Gas Turbine Domain. In order to develop a lexicon for describing patterns in
gas turbine sensor data, we asked two experts to write short descriptions of 38 signal
fragments. (A signal fragment is an interval of a single time-series data channel.) The
descriptors were small, with an average size of 8.3 words. In no case did the experts
produce exactly the same descriptor for a fragment. Many of the differences simply
reflected usage of different words to express the same underlying concept; for example,
one expert typically used rise to describe what the other expert called increase. In other
cases the differences reflected different levels of detail. For example, one fragment was
described by expert A as Generally steady with a slow rising trend. Lots of noise and a few
small noise steps, whereas expert B used the shorter phrase Rising trend, with noise. Both
experts also had personal vocabulary; for example, the terms bathtub and dome were
used only by expert A, whereas the terms square wave and transient were used only by
expert B.
Most importantly from the perspective of this article, there were cases in which the
differences between the experts reflected a difference in the meanings associated with
words. For example, both experts used the word oscillation. Six signals were described
by both experts as oscillations, but two signals, including the one shown in Figure 1,
were described as oscillations only by expert B. We do not have enough examples
to solidly support hypotheses about why the experts agreed on application of the
term oscillation to some signals and disagreed on its application to others, but one
explanation that seems to fit the available data is that the experts agreed on applying
the term to signals that were very similar to a sine wave (which presumably is the
prototype [Rosch 1978] of an oscillation), but sometimes disagreed on its application
to signals that were less similar to a sine wave, such as Figure 1.
2.2.2 Meteorology Domain. In the meteorology domain we accumulated and ana-
lyzed a corpus of 1,099 human-written weather forecasts for offshore oil rigs, together
with the data files (produced by a numerical weather simulation) that the forecasters
examined when writing the forecasts. The forecasts were written by five different fore-
casters. A short extract from a typical forecast is shown in Figure 2; this text describes
changes in wind speed and direction predicted to occur two days after the forecast
was issued. An extract from the corresponding data file is shown in Table 1; it de-
scribes the predicted wind speed and direction from the numerical weather simulation
at three-hourly intervals.
As in the gas turbine domain, our corpus analysis showed that individual fore-
casters had idiosyncratic vocabulary that only they used. For example, one forecaster
used the verb freshening to indicate a moderate increase in wind speed from a low
or moderate initial value, but no other forecaster used this verb. There were also
Figure 1
Signal fragment (gas turbine exhaust temperature): Is this an oscillation?
548
Computational Linguistics Volume 28, Number 4
FORECAST 00-24 GMT, WEDNESDAY, 04-Oct 2000
WIND(10M): WSW 20-24 BACKING SSW 10-14 BY MIDDAY THEN
VEERING SW 24-28 BY EVENING
Figure 2
Wind (at 10m) extract from five-day weather forecast issued on October 2, 2000.
differences in orthography. For example, some forecasters lexicalized the four basic
directions as N, E, S, and W, whereas others used the lexicalizations N?LY, E?LY, S?LY,
and W?LY.
We performed a number of semantic analyses to determine when different fore-
casters used different words; these invariably showed differences between authors.
For example, we attempted to infer the meaning of time phrases such as by evening by
searching for the first data file record that matched the corresponding wind descriptor.
The forecast in Figure 2, for example, says that the wind will change to SSW 10?14
at the time suggested by BY MIDDAY. In the corresponding data shown in Table 1,
the first entry with a direction of SSW and a speed in the 10?14 range is 1200; hence
in this example the time phrase by midday is associated with the time 1200. A similar
analysis suggests that in this example the time phrase by evening is associated with the
time 0000 (on 5-10-00).
We repeated this procedure for every forecast in our corpus and statistically an-
alyzed the results to determine how individual forecasters used time phrases. More
details about the analysis procedure are given by Reiter and Sripada (2002). As re-
ported in that paper, the forecasters seemed to agree on the meaning of some time
phrases; for example, all forecasters predominantly used by midday to mean 1200.
They disagreed, however, on the use of other terms, including by evening. The use of
by evening is shown in Table 2; in particular, whereas forecaster F3 (the author of the
text in Figure 2) most often used this phrase to mean 0000, forecasters F1 and F4 most
often used this phrase to mean 1800. The differences between forecasters in their usage
of by evening are significant at p < .001 under both a chi-squared test (which treats
time as a categorical variable) and a one-way analysis of variance (which compares
the mean time for each forecaster; for this test we recoded the hour 0 as 24).
2.2.3 Knows Java Experiment. Some colleagues pointed out to us that meteorology
in particular was a domain with an established sublanguage and usage conventions,
whose words might correspond to technical terms, and wondered what would hap-
Table 1
Wind (at 10m) extract from October 2, 2000, data file (output of numerical weather model).
Day Hour Wind Direction Wind Speed
4-10-00 0 WSW 22
4-10-00 3 WSW 20
4-10-00 6 SW 16
4-10-00 9 SW 14
4-10-00 12 SSW 12
4-10-00 15 SSW 18
4-10-00 18 SSW 22
4-10-00 21 SSW 24
5-10-00 0 SW 26
549
Reiter and Sripada Human Variation and Lexical Choice
Table 2
How often by evening was used to refer to each time, for each forecaster (mode in boldface
font).
Hour F1 F2 F3 F4 F5 Total
0 5 35 1 3 44
3 1 1
6 1 1
9 0
12 1 1
15 5 2 3 10
18 19 3 1 22 4 49
21 7 5 22 3 6 43
Total 31 14 61 29 14 149
pen in a domain in which there was no established sublanguage and technical termi-
nology. We therefore performed a small experiment at the University of Aberdeen
in which we asked 21 postgraduate students and academic staff members to fill
out a questionnaire asking which of the following individuals they would regard as
knowing Java:2
? A cannot program in Java, but knows that Java is a popular
programming language.
? B cannot write a Java program from scratch, but can make very simple
changes to an existing Java program (such as changing a string constant
that specifies a URL).
? C can use a tool such as JBuilder to write a very simple Java program,
but cannot use control flow constructs such as while loops.
? D can write Java programs that use while loops, arrays, and the Java
class libraries, but only within one class; she cannot write a program that
consists of several classes.
? E can create complex Java programs and classes, but needs to
occasionally refer to documentation for details of the Java language and
class libraries.
Respondents could tick Yes, Unsure, or No. All 21 respondents ticked No for A and Yes
for E. They disagreed about whether B, C, and D could be considered to know Java; 3
ticked Yes for B, 5 ticked Yes for C, and 13 ticked Yes for D. In other words, even among
this relatively homogeneous group, there was considerable disagreement over what
the phrase knows Java meant in terms of actual knowledge of the Java programming
language.
2 The questionnaire in fact contained a sixth item: ?F can create complex Java libraries and almost never
needs to refer to documentation because she has memorised most of it.? However, a few participants
were unsure whether create complex Java libraries meant programming or meant assembling compiled
object files into a single archive file using a tool such as tar or jartool, so we dropped F from our study.
550
Computational Linguistics Volume 28, Number 4
3. Implications for Natural Language Generation
3.1 Lexical Choice
The previous section has argued that people in many cases do associate different
meanings with lexemes and phrases such as oscillation, by evening, and knows; and that
some words, such as bathtub and freshening, are used only by a subset of authors in a
particular domain. What impact does this have on lexical choice?
In applications in which users read only one generated text, it may be necessary
to restrict lexeme definitions to those that we expect all users to share. Indeed, essen-
tially this advice was given to us by a domain expert in an earlier project on generating
personalized smoking-cessation letters (Reiter, Robertson, and Osman 2000). In appli-
cations in which users read many generated texts over a period of time, however, an
argument could be made for using a richer vocabulary and set of lexeme definitions
and expecting users to adapt to and learn the system?s vocabulary and usage over the
course of time; it may be appropriate to add ?redundant? information to texts (sec-
tion 3.2) while the user is still learning the system?s lexical usage. This strategy has
some risks, but if successful, can lead to short and less awkward texts. Consistency is
essential if this strategy is followed; the system should not, for example, sometimes
use by evening to mean 1800 and sometimes use by evening to mean 0000.
We are not aware of previous research in lexical choice that focuses on dealing
with differences in the meanings that different human readers associate with words.
Perhaps the closest research strand is that which investigates tailoring word choice and
phrasing according to the expertise of the user (Bateman and Paris 1989; Reiter 1991;
McKeown, Robin, and Tanenblatt 1993). For example, the Coordinated Multimedia
Explanation Testbed (COMET) (McKeown, Robin, and Tanenblatt 1993) could generate
Check the polarity for skilled users and Make sure the plus on the battery lines up with the
plus on the battery compartment for unskilled users. General reviews of previous lexical
choice research in NLG are given by Stede (1995) and Wanner (1996); Zukerman and
Litman (2001) review research in user modeling and NLP.
In the linguistic community, Parikh (1994) has suggested that utility theory be
applied to word choice. In other words, if we know (1) the probability of a word?s
being correctly interpreted or misinterpreted and (2) the benefit to the user of correct
interpretation and the cost of misinterpretation, then we can compute an overall utility
to the user of using the word. This seems like an interesting theoretical model, but in
practice (at least in the applications we have looked at), whereas it may be just about
possible to get data on the likelihood of correct interpretation of a word, it is probably
impossible to calculate the cost of misinterpretation, because we do not have accurate
task models that specify exactly how the user will use the generated texts (and our
domain experts have told us that it is probably impossible to construct such models).
3.1.1 Near Synonyms. A related problem is choosing between near synonyms, that
is, words with similar meanings: for example, choosing between easing and decreasing
when describing changes in wind speed, or saw-tooth transient and shark-tooth transient
when describing gas turbine signals. The most in-depth examination of choosing be-
tween near synonyms was undertaken by Edmonds (1999), who essentially suggested
using rules based on lexicographic work such as Webster?s New Dictionary of Synonyms
(Gove 1984).
Edmonds was working on machine translation, not generating texts from nonlin-
guistic data, and hence was looking at larger differences than the ones with which
we are concerned. Indeed, dictionaries do not in general give definitions at the level
of detail required by SUMTIME; for example, we are not aware of any dictionary that
551
Reiter and Sripada Human Variation and Lexical Choice
defines oscillation in enough detail to specify whether it is appropriate to describe the
signal in Figure 1 as an oscillation. We also have some doubts, however, as to whether
the definitions given in synonym dictionaries such as Gove (1984) do indeed accu-
rately represent how all members of a language community use near synonyms. For
example, when describing synonyms and near synonyms of error, Gove (page 298)
states that faux pas is ?most frequently applied to a mistake in etiquette.? This seems
to be a fair match to DARE?s fieldwork question (see section 2.1) JJ41, An embarrassing
mistake: Last night she made an awful , and indeed DARE (volume 2, page 372)
states that faux pas was a frequent response to this question. DARE adds, however,
that faux pas was less often used in the South Midland region of the United States (the
states of Kentucky and Tennessee and some adjacent regions of neighboring states)
and also was less often used by people who lacked a college education. So although
faux pas might be a good lexicalization of a ?mistake in etiquette? for most Americans,
it might not be appropriate for all Americans; and, for example, if an NLG system
knew that its user was a non-college-educated man from Kentucky, then perhaps it
should consider using another word for this concept.
3.2 Redundancy
Another implication of human variation in word usage is that if there is a chance that
people may not interpret words as expected, it may be useful for an NLG system to
include extra information in the texts it generates, beyond what is needed if users could
be expected to interpret words exactly as the system intended. Indeed, unexpected
word interpretations could perhaps be considered to be a type of ?semantic? noise;
and as with all noise, redundancy in the signal (text) can help the reader recover the
intended meaning.
For example, the referring-expression generation model of Dale and Reiter (1995)
selects attributes to identify referents based on the assumption that the hearer will
interpret the attributes as the system expects. Assume, for instance, that there are
two books in focus, B1 and B2, and the system?s knowledge base records that B1 has
color red and B2 has color blue. Then, according to Dale and Reiter, the red book is a
distinguishing description that uniquely identifies B1.
Parikh, however, has shown that people can in fact disagree about which objects
are red and which are blue; if this is the case, then it is possible that the hearer will not
in fact be able to identify B1 as the referent after hearing the red book. To guard against
this eventuality, it might be useful to add additional information about a different
attribute to the referring expression; for example, if B1 is 100 pages and B2 is 1,000
pages, the system could generate the thin red book. This is longer than the red book and
thus perhaps may take longer to utter and comprehend, but its redundancy provides
protection against unexpected lexical interpretations.
3.3 Corpus Analysis
A final point is that differences between individuals should perhaps be considered
in general by people performing corpus analyses to derive rules for NLG systems
(Reiter and Sripada 2002). To take one randomly chosen example, Hardt and Rambow
(2001) suggest a set of rules for deciding on verb phrase (VP) ellipsis that are based
on machine learning techniques applied to the Penn Treebank corpus. These achieve
a 35% reduction in error rate against a baseline rule. They do not consider variation
in author, and we wonder if a considerable amount of the remaining error is due to
individual variation in deciding when to ellide a VP. It would be interesting to perform
a similar analysis with author specified as one of the features given to the machine
learning algorithm and see whether this improved performance.
552
Computational Linguistics Volume 28, Number 4
4. Natural Language Generation versus Human-Written Texts
To finish on a more positive note, human variation may potentially be an opportunity
for NLG systems, because they can guarantee consistency. In the weather-forecasting
domain we examined, for example, users receive texts written by all five forecasters,
which means that they may have problems reliably interpreting phrases such as by
evening; an NLG system, in contrast, could be programmed always to use this phrase
consistently. An NLG system could also be programmed to avoid idiosyncratic terms
with which users might not be familiar (bathtub, for example) and not to use terms in
cases in which people disagree about their applicability (e.g., oscillation for Figure 1).
Our corpus analyses and discussions with domain experts suggest that it is not always
easy for human writers to follow such consistency rules, especially if they have limited
amounts of time.
Psychologists believe that interaction is a key aspect of the process of humans
agreeing on word usage (Garrod and Anderson 1987). Perhaps a small group of people
who constantly communicate with each other over a long time period (presumably the
circumstances under which language evolved) will agree on word meanings. But in the
modern world it is common for human writers to write documents for people whom
they have never met or with whom they have never otherwise interacted, which may
reduce the effectiveness of the natural interaction mechanism for agreeing on word
meanings.
In summary, dealing with lexical variation among human readers is a challenge
for NLG systems and will undoubtably require a considerable amount of thought,
research, and data collection. But if NLG systems can do a good job of this, they
might end up producing superior texts to many human writers, which would greatly
enhance the appeal of NLG technology.
Acknowledgments
Our thanks to the many individuals who
have discussed this work with us (not all of
whom agree with our analysis!), including
Regina Barzilay, Ann Copestake, Robert
Dale, Phil Edmonds, Jim Hunter, Adam
Kilgarriff, Owen Rambow, Graeme Ritchie,
Rosemary Stevenson, Sandra Williams, and
Jin Yu. We are also grateful to the
anonymous reviewers for their helpful
comments. Special thanks to DARE editor
Joan Hall for providing us with the DARE
fieldwork data. Last but certainly not least,
this work would not have been possible
without the help of our industrial
collaborators at Intelligent Applications and
WNI/Oceanroutes. This work was
supported by the UK Engineering and
Physical Sciences Research Council
(EPSRC), under grant GR/M76681.
References
Bateman, John and Cecile Paris. 1989.
Phrasing a text in terms the user can
understand. In Proceedings of the 11th
International Joint Conference on Artificial
Intelligence (IJCAI-89), volume 2,
pages 1511?1517.
Cassidy, Frederick and Joan Hall, editors.
1996. Dictionary of American Regional
English. Belknap.
Dale, Robert and Ehud Reiter. 1995.
Computational interpretations of the
Gricean maxims in the generation of
referring expressions. Cognitive Science,
19:233?263.
Edmonds, Philip. 1999. Semantic
Representations of Near-Synonyms for
Automatic Lexical Choice. Ph.D. thesis,
Computer Science Department,
University of Toronto, Toronto.
Garrod, Simon and Anthony Anderson.
1987. Saying what you mean in dialogue:
A study in conceptual and semantic
co-ordination. Cognition, 27:181?218.
Gove, Philip, editor. 1984. Webster?s New
Dictionary of Synonyms. Merriam-Webster.
Hardt, Daniel and Owen Rambow. 2001.
Generation of VP-ellipsis: A corpus-based
approach. In Proceedings of the 39th Meeting
of the Association for Computation Linguistics
(ACL-01), pages 282?289.
Malt, Barbara, Steven Sloman, Silvia
Gennari, Meiyi Shi, and Yuan Wang. 1999.
Knowing versus naming: Similarity and
553
Reiter and Sripada Human Variation and Lexical Choice
the linguistic categorization of artifacts.
Journal of Memory and Language, 40:230?262.
McKeown, Kathleen, Jacques Robin, and
Michael Tanenblatt. 1993. Tailoring lexical
choice to the user?s vocabulary in
multimedia explanation generation. In
Proceedings of 31st Annual Meeting of the
Association for Computational Linguistics
(ACL93), pages 226?234.
Nunberg, Geoffrey. 1978. The Pragmatics of
Reference. University of Indiana
Linguistics Club, Bloomington.
Parikh, Rohit. 1994. Vagueness and utility:
The semantics of common nouns.
Linguistics and Philosophy, 17:521?535.
Reiter, Ehud. 1991. A new model of lexical
choice for nouns. Computational
Intelligence, 7(4):240?251.
Reiter, Ehud, Roma Robertson, and Liesl
Osman. 2000. Knowledge acquisition for
natural language generation. In
Proceedings of the First International
Conference on Natural Language Generation,
pages 217?215.
Reiter, Ehud and Somayajulu Sripada. 2002.
Should corpora texts be gold standards
for NLG? In Proceedings of the Second
International Conference on Natural Language
Generation, pages 97?104.
Rosch, Eleanor. 1978. Principles of
categorization. In E. Rosch and B. Lloyd,
editors, Cognition and Categorization.
Lawrence Erlbaum, Hillsdale, NJ,
pages 27?48.
Smith, Edward and Douglas Medin. 1981.
Categories and Concepts. Harvard
University Press, Cambridge.
Sripada, Somayajulu, Ehud Reiter, Jim
Hunter, and Jin Yu. 2001. A two-stage
model for content determination. In
Proceedings of ENLGW-2001, pages 3?10.
Stede, Manfred. 1995. Lexicalization in
natural language generation: A survey.
Artificial Intelligence Review, 8:309?336.
Wanner, Leo. 1996. Lexical choice in text
generation and machine translation.
Machine Translation, 11:3?35.
Zukerman, Ingrid and Diane Litman. 2001.
Natural language processing and user
modeling: Synergies and limitations. User
Modeling and User-Adapted Interaction,
11:129?158.
A Two-stage Model for Content Determination
Somayajulu G.
Sripada
Dept. of Comp. Sc.
Univ. of Aberdeen,
Aberdeen, UK
ssripada@csd.
abdn.ac.uk
Ehud Reiter
Dept. of Comp. Sc.
Univ. of Aberdeen,
Aberdeen, UK
ereiter@csd.abdn
.ac.uk
Jim Hunter
Dept. of Comp. Sc.
Univ. of Aberdeen,
Aberdeen, UK
jhunter@csd.abdn
.ac.uk
Jin Yu
Dept. of Comp. Sc.
Univ. of Aberdeen,
Aberdeen, UK
jyu@csd.abdn.ac
.uk
Abstract
In this paper we describe a two-stage
model for content determination in
systems that summarise time series
data. The first stage involves building a
qualitative overview of the data set, and
the second involves using this
overview, together with the actual data,
to produce summaries of the time-
series data.  This model is based on our
observations of how human experts
summarise time-series data.
1  Introduction
This paper addresses the problem of content
determination in data summarisation. Content
determination as the name indicates is the
process responsible for determining the content
of the texts generated by an NLG system (Reiter
and Dale 2000).  Although content-
determination is probably the most important
part of an NLG system from the end-user's
perspective, there is little agreement in the NLG
community as to how content-determination
should be done, with different systems adapting
widely varying approaches.  Also, algorithms
and architectures for content-determination
seem to often be based on the intuitions of
system developers, instead of on empirical
observations, although detailed content
determination rules are often based on corpus
analysis and interaction with experts.
In this paper we propose a general architecture
for content determination in data summarisation
systems which assumes that content
determination happens in two stages: first a
qualitative overview of the data is formed, and
second the content of the actual summaries is
decided upon.  This model is based on extensive
knowledge acquisition (KA) activies that we
have carried out in the SUMTIME  project
(Sripada, 2001), and also matches observations
made during KA activities carried out in the
STOP  project (Reiter et al 2000).  We have not
yet implemented this model, and indeed one of
the issues that we need to think about is to what
degree a content-determination strategy used by
human experts is also an appropriate one for a
computer NLG system.
2 Content Determination
Content determination is the task of deciding on
the information content of a generated text.  In
the three-stage pipeline model of Reiter and
Dale (2000), content determination is part of the
first stage, document planning, along with
document structuring (determining the textual
and rhetorical structure of a text).  Content
determination is extremely important to end
users; in most applications users probably prefer
a text which poorly expresses appropriate
content to a text which nicely expresses
inappropriate content.  From a theoretical
perspective content determination should
probably be based on deep reasoning about the
system's communicative goal, the user's
intentions, and the current context (Allen and
Perrault 1980), but this requires an enormous
amount of knowledge and reasoning, and is
difficult to do robustly in real applications.
In recent years many new content determination
strategies have been proposed, ranging from the
use of sophisticated signal-processing
techniques (Boyd 1997) to complex planning
algorithms (Mittal et al 1998) to systems which
exploit cognitive models of the user (Fiedler
1998).  However, most of these strategies have
only been demonstrated in one application.
Furthermore, as far as we can tell these
strategies are usually based on the intuition and
experiences of the developers.  While
realisation, microplanning, and document
structuring techniques are increasingly based on
analyses of how humans perform these tasks
(including corpus analysis, psycholinguistic
studies, and KA activities), most papers on
content determination make little reference to
how human experts determine the content of a
text.  Human experts are often consulted with
regard to the details of content rules, especially
when schemas are used for content
determination (Goldberg et al 1994, McKeown
et al 1994, Reiter et al 2000); but they rarely
seem to be consulted (as far as we can tell) when
deciding on the general algorithm or strategy to
use for content determination.
3  Summarising Time-Series Data
3.1  Text summaries of Time-Series Data
Time-series data is a collection of values of a set
of parameters over time.  Such data is very
common in the modern world, with its
proliferation of databases and sensors, and
humans frequently need to examine and make
inferences from time-series data.
Currently, human examination of time-series
data is generally done either by direct inspection
of the data (for small data sets), by graphical
visualisation, or by statistical analyses.
However, in some cases textual summaries of
time-series data are also useful.  For example,
newspapers regularly publish textual summaries
of weather predictions, the results of polls and
surveys, and stock market activity, instead of
just showing numbers and graphs.  This may be
because graphical depictions of time-series data
require time and skill to interpret, which is not
always available.   A doctor rushing to the side
of a patient who is suffering from a heart attack,
for example, may not have time to examine a set
of graphs of time-series data, and a newspaper
reader may not have the statistical knowledge
necessary to interpret raw poll results.
Perhaps the major problem today with textual
descriptions of time-series data is that they must
be produced manually, which makes them
expensive and also means they can not be
produced instantly.  Graphical depictions of
data, in contrast, can be produced quickly and
cheaply using off-the-shelf computer software;
this may be one reason why they are so popular.
If textual summaries of time-series data could be
automatically produced by software as cheaply
and as quickly as graphical depictions, then they
might be more widely used.
3.2 SUMTIME
The goal of the SUMTIME  project is to develop
better techniques for automatically generating
textual summaries of time-series data, in part by
integrating leading-edge NLG and time-series
analysis technology.  We are currently focusing
on two domains:
Meteorology ? producing weather forecasts
from numerical weather simulations.  This work
is done in collaboration with Weather News Inc
(WNI)/Oceanroutes, a leading meteorological
company.
Gas Turbines  ? summarising sensor readings
from a gas turbine.  This work is done in
collaboration with Intelligent Applications, a
leading developer of monitoring software for
gas turbines.
These domains are quite different in time-series
terms, not least in the size of the data set. A
typical weather forecast is based on tens of
values for tens of parameters, while a summary
of gas-turbine sensor readings may be based on
tens of thousands of values for hundreds of
parameters.  We hope that looking at such
different domains will help ensure that our
results are generalisable and not domain-
specific.  We will start working on a third
domain in 2002; this is likely to be a medical
one, perhaps (although this is not definite)
summarising sensor readings in neonatal
intensive care units.
The first year of SUMTIME  (which started in
April 2000) has mostly been devoted to
knowledge acquisition, that is to trying to
understand how human experts summarise time-
series data.  This was done using various
techniques, including corpus analysis,
observation of experts writing texts, analysis of
content rules suggested by experts, discussion
with experts, and think-aloud sessions, where
experts ?think aloud? while writing texts
(Sripada, 2001).
3.3  Example
The following table shows an example segment
of meteorological time series data, specifically
predicted wind speed and wind direction at an
offshore oil exploration site. The time field is
shown in ?day/hour? format.
Time
(day/hour)
Wind
Direction
Wind
Speed
Knots
05/06 SE 22
05/09 SE 24
05/12 SE 30
05/15 SE 25
05/18 SSE 28
05/21 SSE 22
06/00 SE 16
This data was summarised by WNI's human
forecasters as follows:
FORECAST 06-24 GMT,FRIDAY,05-Jan
2001
WIND(KTS)      CONF   HIGH
  10M: SE 20-25 OCCASIONALLY
       25-30, EASING 15-20 LATER
The above example is just a sample showing the
data and its corresponding forecast text for the
wind subsystem. Real weather forecast reports
are much longer and are produced from data
involving many more weather parameters than
just wind speed and wind direction.
4 Human Summarisation
4.1 Meteorology
In the domain of weather forecasting, we
observed how human experts carry out the task
of summarising weather data by video recording
a meteorologist thinking aloud while writing
weather forecasts. Details of the KA have been
described in Sripada (2001). Our observations
included the following:
1. In the case of weather forecasts, time-series
data represent the values of important
weather parameters (wind speed, direction,
temperature, rainfall), which collectively
describe a single system, the weather. It
seemed as though the expert was
constructing a mental picture of their source
using the significant patterns in time series.
Thus the first activity is that of data
interpretation to obtain a mental model of
weather.
2. The mental model of the weather is mostly
in terms of the elements/objects related to
atmosphere, like cold fronts and warm
fronts; it also seems to be qualitative instead
of numerical. In other words, it qualitatively
describes the meteorological state of the
atmosphere. The expert calls this an
?overview of the weather?.
3. Building the overview involves the task of
interpretation of the time series weather
data. While interpreting this data the expert
used his meteorological knowledge (which
includes his personal experience in
interpreting weather data) to arrive at an
overview of the weather. During this phase,
he appeared to be unconcerned about the
end user of the overview (see 4.1.1 below).
We call this process Domain Problem
Solving (DPS) where information is
processed using exclusively the domain
knowledge.
4. Forecasts are written after the forecaster gets
a clear mental picture (overview) of the
weather. Building the overview from the
data is an objective process which does not
depend on the forecast client (user), whereas
writing the forecast is subjective and varies
with client.
4.1.1  Examples
Two examples of the influence of the overview
on wind texts (Section 3.3) are:
1.  When very cold air flows over a warm sea,
surface winds may be underestimated by the
numerical weather model.  In such cases the
forecaster uses his ?overview of the weather?
to increase wind speeds and also perhaps
add other instability features to the forecast
such as squalls.
2.  If the data contains an outlier, such as a
wind direction which is always N except for
one time period in which it is NE, then the
expert uses the overview to decide if the
outlier is meteorologically plausible and
hence should be reported or if it is likely to
be an artefact of the simulation and hence
should not be reported.
The above examples involve reasoning about the
weather system.  Forecasters also consider user
goals and tasks, but this may be less affected by
the overview.  For example, in one think-aloud
session, the forecaster decided to use the phrase
20-24 to describe wind speed when the data file
predicted a wind speed of 19kt.  He explained to
us that he did this because he knew that oil-rig
staff used different operational procedures (for
example for docking supply boats) when the
wind exceeded 20kt, and he also knew that even
if the average wind speed in the period was
19kt, the actual speed was going to vary minute
by minute and often be above 20kt.  Hence he
decided to send a clear signal to the rig staff that
they should expect to use ?20kt or higher?
procedures, by predicting a wind speed of 20-24.
This reasoning about the user took place after
the overview had been created, and did not seem
to involve the overview.
4.2 Gas Turbine Sensors
Unlike the previous domain, in the domain of
gas turbine (GT), currently there are no textual
summaries of turbine data written by humans.
Thus we have asked the domain experts to
comment orally on the data. However, the
experts have attempted to summarise their
comments at the end of each session if they
found something worth summarising. Our
observations included:
1. The main task is to identify the abnormal
data and summarise it. However, an
abnormal trend in a specific channel might
have been caused due to a change in another
channel (for instance, an increase in the
output voltage can be explained with a
corresponding increase in the fuel input).
Thus individual channel data needs to be
interpreted in the context of the other
channels.
2. The expert agrees during the KA session
that he first analyses the data numerically to
obtain  qualitative trends relating to the GT
before generating comments. Therefore the
state of the GT that produced the data is
constructed through data interpretation and
the knowledge of the state is then used to
check if the turbine is in a healthy state or
not. Since GT is an artefact created by
humans it is possible to have a fairly
accurate model of states of a GT (unlike
weather!).
3. The phrases used by the expert often express
the trends in the data as if they were
physical happenings on the turbine, like
?running down? for a decreasing trend in
shaft speed data. This indicates that the
expert is merely expressing the state of the
GT. This in turn indicates that at the time
the summarisation is done, the mental model
of the state of the GT is available.
5 Evidence from Other Projects
After making the above observations, we
examined think-aloud transcripts from an earlier
project at the University of Aberdeen, STOP
(Reiter et al 2000), which involved building an
NLG system that produced smoking-cessation
letters from smoking questionnaires.   These
transcripts (from think-aloud sessions of doctors
and other health professionals manually writing
smoking-cessation letters) showed that in this
domain as well experts would usually first build
an overview (in this case, of the smoker) before
starting to determine the detailed content of a
letter. Below is an excerpt from one of the
transcripts of a KA session :
?   ?. The first thing I have got to do is to read
through the questionaire just to get some idea of
where he is at with his smoking. ?? ?
We did not investigate overview formation in
any detail in STOP, but the issue did come up
once in a general discussion with a doctor about
the think-aloud process.  This particular doctor
said that he built in his mind a mental image of
the smoker (including a guess at what he or she
looked like), and that he found this image very
useful in deciding how best to communicate
with the smoker.
In another work, RajuGuide, once again there is
evidence of an overview influencing content
determination (Sripada 1997). RajuGuide is a
system that generates route descriptions. At a
higer level of abstraction, RajuGuide has two
parts. The first part is responsible for planning
the route the user wanted. The second module is
responsible for generating the text describing the
route. The route computed by the first part,
which is in the form of a series of coordinates, is
not directly communicated to the user. Instead
the second part attempts to enrich the route
depending upon what the user already knows
and what additional information the knowledge
base knows for that particular route. We believe
that the route computed by the route planner is
the overview in this case and it drives the
content determination process in the second part.
6 Two-stage Model for content
determination
These observations have led us to make the
following hypotheses:
1. Humans form a qualitative overview of the
input data set.
2. Not all the information in the overview is
used in the text.
3. The overview is not dependent on pragmatic
factors such as the user?s taste, these are
considered at a later stage of the content
determination process.
Based on the above hypotheses, we propose a
two-stage model for content determination as
depicted in Figure 1. It is assumed that Domain
Data Source (DDS) is external to the text
generator. It has been assumed that a Domain
Problem Solver or Domain Reasoner (DR) is
available for data processing. This reasoning
module is essentially useful to draw inferences
while interpreting the input data set and
ultimately is responsible for generating the
overview. Communication Goal (CG) is the
input to the data summarisation system in
response to which it accesses DDS to produce
an overview of the data using the DR. In the
context of the overview produced by DR, the
Data Comprehension
Goal (Derived from
Comm. Goal)
Domain
Data Source
(DDS)
Domain
Reasoner (DR)
Data
Overview Communication
Reasoner (CR)
Final
Content
Communication
Goal
Constraints
due to User
Constraints due to other
pragmatic factors
Figure 1. Two stage model for content determination
Communication Reasoner (CR) system
generates the final content specification taking
into account the influence of the User
Constraints (UC) and other pragmatic factors.
This content is then sent to subsequent NLG
modules (not shown), such as microplanning
and surface realisation.
Our model has some similarities to the one
proposed by Barzilay et al (1998), in that the
Domain Reasoner uses general domain
knowledge similar to their RDK, while the
Communication Reasoner uses communication
knowledge similar to their CDK and DCK.
The central feature of the above model is the
idea of data overview and its effect on content
selection. One possible use of overviews is to
trigger context-dependent content rules.  The
time-series analysis part of SUMTIME  is largely
based on Shahar's model (1997), which makes
heavy use of such rules.  In Shahar's model
contexts are inferred by separate mechanisms;
we believe that these should be incorporated into
the overview, but this needs further
investigation.
At the current stage of our project we have only
a gross idea of what makes up the proposed data
overview. Our suspicion is that it is hard to
make a generic definition of the data overview
for all domains. Instead, we would like to
imagine the data overview as the result of
inferences made from the input data so as to
help in triggering the right content determination
rules. For example, in out meteorology domain,
the input time-series data comes from a
numerical weather prediction (NWP) model, but
even the most sophisticated NWP models do not
fully represent the real atmosphere ? all models
work with approximations. Thus the NWP data
displayed to the meteorologist is interpreted by
him to arrive at a conceptual model in his or her
head, which is the overview.
7  Issues with the two stage model
There are a number of issues that need to be
resolved with respect to the two-stage model
described above.
7.1 Is overview creation a human
artefact?
The main basis for including the overview in
two stage model has been the observation made
during the think aloud sessions that experts form
overviews before writing texts. Now it can be
argued that even if humans need an overview,
computer programs may not. Evidently, it is
hard to ever prove the contrary. But what can be
done is to show the advantages gained by a
computer program by using an overview for
content selection.
7.2 Does the overview have any other
utility than just providing context for
content determination rules?
We believe that the overview can play multiple
roles in the overall process of writing textual
forecasts. First, the overview can bring in
additional information into the text that is not
directly present in the underlying raw data. In
Reiter and Dale's (2000) terminology, overviews
are a technique for generating Computable Data
content, that is content which is not directly
present in the input data but can be computed or
inferred from it.  Such content provides much of
the value of summary texts.  Indeed, one could
argue that simple textual descriptions of a set of
data values without extra computed or inferred
content, such as those produced by TREND
(Boyd, 1997), might not be that much more
useful than a graph of the data.
The overview may also help in deciding how
reliable the input data is, which is especially
important in the meteorology domain, since the
data comes from an NWP simulation. This
could, for example, help the generation system
decide whether to use precise temporal terms
such as Midnight  or vague temporal terms such
as tonight. Again one could argue that the ability
to convey such uncertainty and reliability
information to a non-specialist is a key
advantage of textual summaries over graphs.
In general, the overview allows reasoning to be
carried out on the raw data and this will
probably be useful in many ways.
7.3  How is the overview related to the
domain ontology?
The basic concepts present in an overview may
be quite different from the basic concepts
present in a written text.  For example, the
overview built by our expert meteorologist was
based on concepts such as lapse rate (the rate at
which temperature varies with height),
movement of air masses, and atmospheric
stability.   However, the texts he wrote
mentioned none of these, instead it talked about
wind speed, wind direction, and showers.  In the
STOP  domain, overviews created by doctors
seemed to often contain many qualitative
psychological attributes (depression, self-
confidence, motivation to quit, etc) which were
not explicitly mentioned in the actual texts
written by the doctors.
This suggests that the conceptual ontology, that
is the specification of underlying concepts,
underlying the overview may be quite different
from the ontology underlying the actual texts.
The overview ontology includes concepts used
by experts when reasoning about a domain (such
as air masses or motivation), while the text
ontology includes concepts useful for
communicating information to the end user
(such as wind speed, or longer life expectancy).
7.4  What do experts think about the two-
stage model?
When the two stage model was reported back to
a WNI expert who participated in a think-aloud
session, the expert agreed that he does build an
overview (as he did during the KA session)
while writing forecasts, but felt that it?s use may
not be necessary for writing all forecasts. In his
opinion, the interpretation of most data sets
doesn?t require the use of the overview.
However, he was quick to add that the quality of
the forecasts can be improved by using
overviews which faciliate reasoning with the
weather data.
8  Evaluation
We are currently building a testbed system
called SUMTIME-MOUSAM  which will enable us
to test the hypotheses we have presented in this
paper and other hypotheses suggested by our
KA activities. SUMTIME-MOUSAM  is a
framework system that consists of
? "Infrastructure" software for accessing data
files, regression testing of new software
versions, etc.
? An ontology, which defines a conceptual
level of representation of texts.
? A corpus of human-written texts with their
corresponding conceptual representations
defined using the above ontology.
? Scoring software which compares the output
of a module (either at a conceptual or text
level) against the human corpus.
Because we are primarily interested in content
issues, it is important to evaluate our system at a
content level as well as at a text level.  To
support this, we are developing conceptual
representations of the texts we will be
generating, which can also be extracted from
human texts by manual analysis.
SUMTIME-MOUSAM  is currently best developed
in the area of producing wind texts.  In this area,
we have developed a conceptual representation
and manual annotation guide (with good inter-
annotator agreement, generally kappa values of
.9 or higher); built an initial software system to
automatically produce such texts based on a
threshold model without an overview; and
begun the process of analysing differences.  We
are currently working on extending SUMTIME-
MOUSAM  to other parts of weather forecasts,
such as statements describing clouds and
precipitation, and plan in the future to extend it
to the gas-turbine domain.
With regard to testing hypotheses specifically
about two-stage content determination (the
subject of this paper), our plan is as follows
1. Compare the output of the non-overview
based software to human summary texts,
and identify cases where an overview seems
to be used.
2. Ask human experts to build an overview
(using a GUI), modify our software to use
this overview when generating texts, and see
if this results in texts more similar to the
human texts.
3.  Attempt to automatically generate the
overview from the data, and again compare
the resultant texts to human texts.
At some point towards the end of SUMTIME, we
also hope to conduct user task evaluations.  For
example, we may show gas-turbine engineers
our summary texts and see if this helps them
detect problems in the gas turbine.
9  Conclusion
Our experience in three domains shows that
human experts build qualitative overviews when
writing texts, and that these overviews are used
by the experts for inference and to provide a
context for specific content rules.  We believe
that overviews could also be very useful in
computer NLG systems, and are currently
working on testing this hypothesis, as part of the
SUMTIME  project.
Acknowledgements
Many thanks to our collaborators at
WNI/Oceanroutes and Intelligent Applications,
especially Ian Davy, Dave Selway, Rob Milne,
and Jon Aylett; this work would not be possible
without them!  Thanks also to Sandra Williams
and the anonymous reviewers for their
comments on a draft of this paper.  This project
is supported by the UK Engineering and
Physical Sciences Research Council (EPSRC),
under grant GR/M76881.
References
Allen J. and Perrault C. R. (1980).  Analyzing
Intention in Utterances.  Artificial Intelligence,
26:1-33.
Barzilay R, McCullough D, Rambow O,
DeChristofaro J, Korelsky T, and Lavoie B (1998)
A New Approach to Expert System Explanations,
In Proceedings of INLG-1998,  pages 78-87.
Boyd S (1997). Detecting and Describing Patterns in
Time-varying Data Using Wavelets.  In Advances
in Intelligent Data Analysis: Reasoning About
Data, X Lui and P Cohen (Eds.), Lecture Notes in
Computer Science 1280, Springer Verlag.
Fiedler A (1998). Macroplanning with a Cognitive
Architecture for the Adaptive Explanation of
Proofs. In Proceedings of INLG-1998, pp 88-97.
Goldberg E, N Driedger and RL Kittredge (1994),
Using Natural-Language Processing to Produce
Weather Forecasts,  IEEE Expert,  9, 2, pp 45-53.
McKeown K, Kukich K, Shaw J (1994).  Practical
Issues in Automatic Document Generation.  In
Proceedings of ANLP-1994,  pp 7-14.
Mittal V, Moore J, Carenini G, and Roth S (1998).
Describing Complex Charts in Natural Language:
A Caption Generation System. Computational
Linguistics 24: 431-467.
Reiter E. and Dale R. (2000)  Building Natural
Language Generation Systems. Cambridge
University Press.
Reiter E., Robertson R. and Osman L. (2000)
Knowledge Acquisition for Natural Language
Generation.  In Proceedings of the First
International Conference on Natural Language
Generation (INLG-2000), 217-224 pp.
Shahar Y (1997), ?Framework for Knowledge-Based
Temporal Abstraction?, Artificial Intelligence
90:79-133..
Sripada S. G. (1997) Communicating Plans in
Natural Language: Planning and Realisation.
PhD Thesis,  Indian Institute of Technology,
Madras, India.
Sripada S. G. (2001) SUMTIME: Observations from
KA for Weather Domain.  Technical Report,
Computing Science Dept. Univ of Aberdeen,
Aberdeen AB24 3UE, UK. Awaiting approval
from industrial collaborators.
Learning the Meaning and Usage of Time Phrases from a Parallel Text-Data
Corpus
Ehud Reiter
Department of Computing Science
University of Aberdeen
ereiter@csd.abdn.ac.uk
Somayajulu Sripada
Department of Computing Science
University of Aberdeen
ssripada@csd.abdn.ac.uk
Abstract
We present an empirical corpus study of the
meaning and usage of time phrases in weather
forecasts; this is based on a novel corpus anal-
ysis technique where we align phrases from
the forecast text with data extracted from a nu-
merical weather simulation. Previous papers
have summarised this analysis and discussed
the substantial variations we discovered among
individual writers, which was perhaps our most
surprising finding. In this paper we describe
our analysis procedure and results in consid-
erably more detail, and also discuss our cur-
rent work on using parallel text-data corpora to
learn the meanings of other types of words.
1 Introduction
NLP systems that interact with the world often need mod-
els of what words mean in terms of the non-linguistic
world. In this paper, we describe how we have deter-
mined the meaning of time phrases in weather forecasts
by analysing a parallel corpus of (A) manually-written
weather forecast texts and (B) the numerical data (from a
weather simulation) that the human forecasters examined
when writing the textual forecasts. The analysis proce-
dure first aligns (associates) text fragments with data seg-
ments, and then infers the meaning of each time phrase by
statistically analysing the time of data segments that are
aligned to textual phrases that contain this time phrase.
This is broadly similar in concept to the use of parallel
multilingual corpora in machine translation (Brown et al,
1990), except that our parallel corpus consists of texts and
underlying numeric data, not texts and their translations.
In other words, we are trying to learn what words mean
in terms of non-linguistic data, not the best translations
of words in another language.
Probably the biggest surprise in our analysis was the
substantial variation we saw between individuals. For
example, by evening apparently meant 1800 to some peo-
ple, but 0000 to others. Although the possibility of such
variation in individual idiolects has been acknowledged
in the past (for example, (Nunberg, 1978; Parikh, 1994)),
it seems to be ignored by most recent work on lexical se-
mantics.
We have published other papers that have summarised
our key findings, notably variation between individu-
als (Reiter and Sripada, 2002a; Reiter and Sripada,
2002b); and also described the corpus itself (Sripada et
al., 2003b). The purpose of this paper is to describe
our analysis procedure (including alignment) and results
in detail, and to also discuss our current work on using
parallel text-data corpora to learn the meanings of other
types of words.
2 Previous Research
Linguists and lexicographers have used a number of dif-
ferent techniques to determine the meanings of words.
These include asking native-speaker informants to judge
the acceptability and oddness of test sentences (Cruse,
1986); defining word senses via lexicographic analysis
of citations and corpora (Landau, 1984); and asking sub-
jects to respond to ?fill in the blank? questions (Cassidy
and Hall, 1996). These techniques have focused purely
on texts, and have not analysed how texts and words re-
late to non-linguistic representations of the meanings of
a text, which is our focus.
Psychologists interested in categorisation have done
formal experiments to determine which objects human
subjects consider to be in a mental category (Rosch,
1978; Malt et al, 1999). If we assume that the mean-
ing of a word is one or more mental categories, then this
research has shed considerable light on what words mean.
However, like all psychological research, it has examined
language usage in an artificial experimental context, not
naturally occurring language.
In the NLP community, models of word meanings are
typically either entered by a user or developer (for exam-
ple in Microsoft?s English Query natural-language inter-
day hour wind dir wind speed
25-10-00 0 SSW 12
25-10-00 3 SSE 11
25-10-00 6 ESE 18
25-10-00 9 ESE 16
25-10-00 12 E 15
25-10-00 15 ENE 15
25-10-00 18 ENE 18
25-10-00 21 NNE 20
26-10-00 0 NNW 26
Table 1: Wind (at 10m) extract from 24-Oct-00 data file
face) or derived from a hand-built knowledge base (eg,
(Reiter, 1991)). There is growing interest in trying to
learn word meanings from parallel text-data corpora, for
example (Siskind, 2001; Barzilay and Lee, 2002; Roy,
2002). We believe our work is unusual because we are
using naturally occurring texts and data. Siskind (2001),
in contrast, used data which was explicitly created for his
experiments; Barzilay and Lee (2002) used texts which
subjects had written for a previous experiment; and Roy
(2002) used both data and texts that were created for his
experiments.
3 SumTime Project and Corpora
The SUMTIME project is investigating better technology
for building software systems that automatically gener-
ate textual summaries of time-series data. One of the
domains SUMTIME is working in is weather forecasts,
and in this domain we acquired a corpus of 1119 weather
forecasts (for off-shore oil rigs) written by five profes-
sional meteorologists (Sripada et al, 2002; Sripada et al,
2003b). The reports were primarily based on the output
of a numerical weather simulation, and our corpus con-
tains this information as well as the forecast texts. Each
forecast is roughly 400 words long, giving a total corpus
size of about 400,000 words. The forecasts are split into
an initial section which gives an overview of the weather,
and then additional sections which give detailed forecasts
for different periods of time. Figure 1 shows an example
extract from a forecast text; this is the detailed descrip-
tion of predicted weather on 25 Oct 2000, from a forecast
issued at 3AM on 24 Oct 2000.
Much of our analysis has focused on statements de-
scribing predicted wind speed and direction at 10 meters
altitude during the first 72 hours after the forecast was is-
sued. In other words, the WIND(10M) field from the de-
tailed weather descriptions up to 3 days after the forecast
was issued. One reason for focusing on wind statements
is that they are based fairly directly on two fields from
the data files, predicted wind direction and speed; the re-
lationship between some of the other statements (such as
weather) and the data files is more complex. The pre-
dicted wind (at 10m) speed and direction on 25 Oct 2000,
from the 24 Oct 2000 data file, is shown in Table 1. This
is the primary information that the meteorologists looked
at when writing the wind statement in Figure 1, although
they also have access to other information sources, such
as satellite weather photographs.
Each forecast contains 3 such wind statements, with an
average length of approximately 10 words; hence there
are about 30,000 words in our wind-statement subcorpus.
This of course is very small compared to many text-only
corpora such as the British National Corpus (BNC), but
we believe that our weather forecast corpus is one of the
largest parallel text-data corpora in existence.
4 Analysis Procedure for Time Phrases
One of SUMTIME?s research goals is to learn the meaning
of time phrases; in other words, what a forecaster meant
when he used a time phrase such as by evening or af-
ter midnight. We also wished to learn which time phrase
should be included in a computer-generated weather fore-
cast text to indicate a time; for example, which time
phrase should be used to indicate a change in the weather
at 1200. Note that it is rare for weather forecasts to ex-
plicitly mention numerical times such as 1200, and also
that although there are standard terminologies for some
meteorological phenomena such as cloud cover and pre-
cipitation, we are not aware of any standard terminologies
for the use of time phrases in weather forecasts.
We performed this analysis as follows. First we ex-
tracted the wind at 10 meters statements for the next 72
hours from all forecasts in our corpus, and parsed these
texts with a simple parser tuned to the linguistic structure
of these texts. The parser essentially broke sentences up
into individual phrases, and then recorded the speed, di-
rection, and time phrase mentioned in each such phrase,
along with other information (such as verb) which was
not used in the analysis described here. For example the
WIND (10M) statement from Figure 1 was broken up by
the parser into four wind phrases:
1. SSW 12-16
(speed:12-16, direction:SSW, timephrase: none)
2. BACKING ESE 16-20 IN THE MORNING,
(speed:16-20, direction:ESE, timephrase: IN THE
MORNING)
3. BACKING NE EARLY AFTERNOON
(speed:(16-20), direction:NE, timephrase: EARLY
AFTERNOON)
4. THEN NNW 24-28 LATE EVENING
(speed:24-28, direction:NNW, timephrase: LATE
EVENING)
FORECAST 00-24 GMT, WEDNESDAY, 25-Oct 2000
WIND(10M): SSW 12-16 BACKING ESE 16-20 IN THE MORNING, BACKING
NE EARLY AFTERNOON THEN NNW 24-28 LATE EVENING
(50M): SSW 15-20 BACKING ESE 20-25 IN THE MORNING, BACKING
NE EARLY AFTERNOON THEN NNW 30-35 LATE EVENING
SIG WAVE: 2.0-2.5 RISING 3.0-3.5 BY AFTERNOON
MAX WAVE: 3.0-4.0 RISING 5.0-5.5 BY AFTERNOON
WEATHER: RAIN SOON, CLEARING TO SHOWERS IN THE EVENING
VIS: GOOD BECOMING MODERATE IN RAIN
Figure 1: Extract from 5-day forecast issued on 24-Oct-00
If a wind phrase did not specify speed or direction, the
parser assumed that this was unchanged from the pre-
vious wind phrase; such elision is common in weather
forecast texts. Thus, for example, the speed recorded for
BACKING NE EARLY AFTERNOON is 16-20, which
is the speed from the previous phrase (BACKING ESE
16-20 IN THE MORNING). Our parser successfully
parsed 3225 of the 3357 WIND(10M) statements; 132
(4%) of the statements could not be parsed. The parser
produced 8198 wind phrases in total.
From these 8198 wind phrases we selected those
phrases which (a) included a time phrase, (b) did not
use a qualifier such as mainly or occasionally, (c) did not
specify that wind speed or direction was variable, (d) for
which we had the corresponding data files, and (e) for
which we knew the forecast author. There were 3654
such phrases. The majority (4014 phrases) of the elim-
inated phrases did not specify a time phrase, such as the
first phrase (SSW 12-16) in the above example.
We next associated each wind phrase with an entry in
the corresponding data file. In other words, we aligned
the textual wind phrases with the numeric data file en-
tries. As in other uses of parallel corpora, good alignment
is essential in order for the results to be meaningful (Och
and Ney, 2000).
To associate data file entries with wind phrases, we
first searched the data file for entries which matched the
wind phrase. An entry matched if its speed was within the
range defined in the phrase, and if its direction was within
12 degrees of the direction mentioned in the phrase. In
343 cases, no data file entry matched the wind phrase. We
believe that such cases were mostly due to (a) forecasters
not literally reporting the data file, but instead adjusting
what they said based on their meteorological expertise
and on information not available to the numerical weather
simulation (such as satellite weather images); (b) fore-
casters reporting a simultaneous change in wind speed
and direction, when in fact speed and direction changed
at different times (this may be due to forecasters trying to
write texts quickly, so that they can use the most up-to-
date data (Reiter et al, 2003)); and (c) forecaster errors.
For example, the third phrase in our example, BACK-
ING NE EARLY AFTERNOON, does not match any of
the data file entries shown in Table 1. This could be be-
cause the forecaster decided that the numerical forecast
was underestimating the speed at which the wind was
shifting, and hence he believed that the wind would be
NE at 12 or 15, even though the data file predicted E and
ENE for these times. It could also be that the forecaster
made a mistake, and perhaps was intending to write ENE
but wrote NE instead because he was writing under time
pressure. In any case, wind phrases which did not match
any data file entries were dropped from our analysis.
Out of the 3311 matched wind phrases, 1434 (43%)
were unambiguous and only matched one data file en-
try. For example, the fourth wind phrase in our example,
THEN NNW 24-28 LATE EVENING, matches only one
data file entry, the one for 0000 on 26 Oct 2000.
1877 (57%) of the matched wind phrases were ambigu-
ous and matched more than one data file entry. Typically
this happened when the wind was changing slowly and
hence two or more adjacent data file entries matched the
wind phrase. In such cases we checked if one data file
entry had a speed which was was closer than the other
data files entries to the middle of the speed range in the
textual wind phrase. This heuristic produced a preferred
match for 1105 (33%) of the matched wind phrases, and
left 772 (23%) phrases as ambiguous and unmatched.
For example, the second wind phrase in our example,
BACKING ESE 16-20 IN THE MORNING, matches two
data file entries: 0600 (direction ESE, speed 18) and 0900
(direction ESE, speed 16). The midpoint of the 16-20
speed range reported in the forecast is 18, so our speed
heuristic matches this wind phrase to the 0600 data file
entry, since its speed is closer to the speed range midpoint
(indeed, it matches the midpoint).
We evaluated our alignment process by applying it to
the subset of wind phrases which used a time phrase
which we thought had a clear and unambiguous inter-
pretation, namely an absolute time (such as by 0600), by
time F1 F2 F3 F4 F5 total
0000 2 9 80 5 14 110
0300 1 1
0600 1 1
0900 0
1200 1 1
1500 2 1 1 2 6
1800 30 5 2 27 13 77
2100 13 6 8 2 11 40
total 37 22 91 34 42 236
Significance of differences: p   0.001
Table 2: Usage of by evening, by forecaster (mode in
bold)
time F1 F2 F3 F4 F5 total
0000 2 1 3
0300 1 1
0600 1 1
0900 3 1 7 2 13
1200 23 71 86 11 191
1500 7 1 9 5 2 24
1800 2 2 1 5
2100 1 1
total 34 1 85 103 16 239
Significance of differences: p  0.1
Table 3: Usage of by midday, by forecaster (mode in
bold)
midday (which means 1200), by midnight (which means
0000), and by end of period (which means either 0000 or
0600, depending on the forecast section). The matching
process was fairly accurate; in 86% of cases it produced
the expected meaning (such as 0000 for by midnight).
Clearly we would benefit from better matching and
alignment techniques, and we wonder if perhaps some of
the alignment techniques used for parallel multi-lingual
corpora (Och and Ney, 2000) could be adapted to help
align our text-data corpora. This is a topic we plan to
investigate in future research.
This matching/alignment procedure is different in de-
tail from the preliminary analysis procedure reported in
(Reiter and Sripada, 2002b). The procedure used in our
earlier paper aligned fewer phrases (1382 vs. 2539) and
had a higher error rate (22% vs. 14%), so it was inferior.
5 Results
We examined the association between time phrase and
time in the 2539 aligned (wind phrase, data file entry)
pairs. In this analysis, we regarded time phrases as dif-
ferent if they involved different head nouns (for example,
time F1 F2 F3 F4 F5 total
0000 215 9 15 239
0300 1 1
0600 0
0900 0
1200 1 1
1500 0
1800 0
2100 3 3 2 8
total 0 0 219 12 18 249
Significance of differences p   0.001 (ANOVA: p = 0.06)
Table 4: Usage of by late evening, by forecaster (mode in
bold)
by evening and by afternoon), different prepositions (for
example, midday and by midday) and/or different adjec-
tives (for example, by afternoon and by late afternoon).
However, we ignored determiners (for example, by this
evening was regarded as the same phrase as by evening).
Tables 2, 3, and 4 gives details of the usage of the three
most common non-contextual time phrases: by evening,
by midday, and by late evening. This tables also shows
the statistical significance of differences between fore-
casters, calculated with a a chi-square test (which treats
time as a categorical variable). As some colleagues
have expressed an interest in a one-way ANOVA analysis
(which compares mean time), we show this as well where
it gives a substantially different value from the chi-square
analysis. This data suggests that
 by evening means different things to different peo-
ple; for example, F1 and F4 primarily use this phrase
to mean 1800, while F3 primarily uses this phrase to
mean 0000.
 by midday was used in a very similar way by all fore-
casters (ignoring F2, who only used the term once).
 by late evening was used by all forecasters (who
used this term) primarily to mean 0000. However,
the usages of the different forecasters was still sig-
nificantly different. This reflects a difference in the
distribution of usage; in particular, F3 almost always
(98% of cases) used this phrase to mean 0000, while
F4 and F5 used this phrase to mean 0000 in about
80% of cases.
These patterns are replicated across the corpus: some
phrases (such as by midday and by morning) are used in
the same way by all forecasters; some phrases (such as
by evening and by late morning) are used in very differ-
ent ways by the forecasters; and some phrases (such as by
late evening and by midnight) have the same core mean-
ing (eg, 0000) but different distributions around the core.
We have, incidentally, looked for seasonal variations in
meaning (for example, by evening meaning one thing in
the winter and another in the summer), but we have found
no evidence of such variation.
Roy (2002) has also noted variation in the meanings
that individuals assign to words, in his parallel text-data
study of object descriptions. For example, one object
might be described as having the colour pink by one sub-
ject, but other subjects might have problems identifying
the object when it was described as pink, because they did
not consider it to have this colour.
Table 5 presents the most common time-phrase used
by each forecaster for each time, including context-
dependent phrases such as later. This highlights ma-
jor ?stylistic? differences between forecasters in terms of
which time phrases they prefer to use. For example, F1
and F2 make heavy use of contextual time phrases such
as later and soon, while F5 (and to a lesser extent F4)
seem to prefer to avoid such terms. It is also interest-
ing that contextual time phrases are especially commonly
used to refer to the time 0300. We wonder if this could
reflect a ?lexical gap? in English; there are no commonly
used time phrases in English for times around 0300, and
perhaps this encourages the forecasters to use contextual
time phrases to refer to 0300.
Table 6 presents the most common (mode) meaning
of non-contextual time phrases, for each forecaster. Per-
haps not surprisingly, the greatest variability occurs when
a time phrase denoting a time period (morning, afternoon,
or evening) occurs without being modified by an adjec-
tive (early, mid, or late). The data also suggests that the
forecasters may disagree about the meaning of morning,
with F4 in particular considering morning to be the period
0300-0900, while F5 considers morning to be the period
0600-1200.
6 Current and Future Work
6.1 Verb Choice
We would like to use our corpus to learn choice rules
for verbs which are near-synonyms (Edmonds and Hirst,
2002). We are currently attempting to learn rules which
predict which of three possible verbs ? decreasing, eas-
ing, and falling ? are used when the wind speed de-
creases.
We have conducted two experiments. The first was a
semantic analysis, where we attempted to learn a choice
rule based on features extracted from the numerical data.
To do this, we used our aligned corpus to extract seman-
tic features which we thought could be relevant to this
decision (such as the amount by which the wind speed
has decreased), and then analysed this with Ripper (Co-
hen, 1995). This gave the rules shown in Figure 2; these
again show substantial variation between individual fore-
verb F1 F2 F3 F4 F5 total
decreasing 0 0 3 2 0 5
easing 1 19 0 0 0 20
falling 4 0 61 0 0 65
Table 7: Choice of wind decrease verb when subsequent
word is variable, by forecaster (mode in bold)
casters. These rules are mildly effective; 10-fold cross
validation error is 25%, compared to a baseline error rate
of 33% from always choosing the most common verb
(easing). These rules suggest that at least for some fore-
casters, decreasing suggests a larger change in the wind
speed than easing; this is the sort of near-synonym con-
notational difference that we expected to find. More sur-
prisingly (at least to us), the presence of forecast date in
some of the rules suggests that forecasters change how
they write over time. Perhaps in retrospect this should
not have been a surprise, because we have also observed
changes over time in how people write in a previous
project (Reiter et al, 2000).
We also analysed collocation effects, that is whether
we could predict verb choice based purely on the words
immediately preceding and following the verb (and hence
ignoring the numerical prediction data). This was done
on the complete corpus (not just verbs that were part of
successfully aligned phrases). It is difficult to directly
compare the collocation analysis with the semantic one
due to differences in the corpora texts used, but in gen-
eral terms the reduction in baseline error rate seems com-
parable to the semantic analysis. Some of the collocation
effects were both strong and forecaster-dependent. For
example, Table 7 shows the choice of wind decrease verb
when the word following the verb was variable (indicat-
ing wind direction was variable). In this context, fore-
casters F1 and F3 usually used falling; F2 always used
easing; and F4 always used decreasing (F5 never used
variable in his forecasts). Similar individual differences
were observed in other collocations. For example, when
the word preceding the verb was gradually, F3, F4, and
F5 preferred decreasing, but F2 always used easing (F1
never used gradually in his forecasts).
In summary, it seems that the choice between the near
synonyms decreasing, easing, and falling depends on
 semantics: how much the actual wind speed has
changed;

collocation: immediately preceding and following
words in the sentence;

author: which forecaster wrote this particular text;
 date: when the text was written.
time F1 F2 F3 F4 F5 all
0000 later later by late evening by midnight in evening later
0300 later soon soon soon tonight soon
0600 later overnight soon by morning later in period later
0900 soon soon soon by midday in morning soon
1200 by midday soon by midday by midday in morning by midday
by by mid by mid early by mid
1500 afternoon soon afternoon afternoon afternoon afternoon
1800 by evening by evening by late afternoon by evening by evening by evening
in evening/ later/
2100 later later by evening later by evening by evening
bold font means this phrases was at least twice as common as the second-most common term.
X/Y means X and Y were equally common
Table 5: Most common time-phrases for each time, by forecaster
Choose decreasing if
Forecaster rule
F1 never
F2 never
F3 speed change    10 knots AND time interval    15 hours
F4 speed change    9 knots OR forecast date    2-November-2001
F5 forecast date    30 March 2001
Otherwise choose easing
Figure 2: Verb choice rule based on data features
All of these factors are important, and in particular the
kind of semantic differences investigated by (Edmonds
and Hirst, 2002) are only one factor among many, and do
not dominate the choice decision. We plan to continue
working on this and other analyses of near-synonyms,
and obtain a better idea of how these factors interact.
6.2 Other corpora
In addition to the weather corpus, the SUMTIME project
also has access to a parallel text-data corpus of doc-
tors describing signal data from a neonatal intensive care
unit (Alberdi et al, 2001). We would like to analyse
this corpus to determine the meanings of words such as
steady and oscillations. However, a preliminary analy-
sis has suggested to us that we cannot conduct such an
analysis until we remove non-physiological events from
the data (Sripada et al, 2003a). For example, a doctor
may describe a signal as steady even when it contains a
large spike, if the doctor believes the spike is due to a
non-physiological event (such as a sensor falling off the
baby and then being replaced by a nurse). Hence non-
physiological events (known in this domain as ?artifacts?)
must be removed from the data before it is possible to
analyse word meaning. We are currently working on ar-
tifact removal, and once this is complete we will start our
analysis of word meanings.
SUMTIME is also working on generating textual sum-
maries of gas turbine sensor data (Yu et al, 2003). Un-
fortunately in this domain, as in many other NLG appli-
cations (Reiter et al, 2003), there is no existing corpus
of manually written texts describing the input data. We
have explicitly asked two experts to write descriptions
of 38 signal fragments. This very small corpus showed
that once again there were major differences between in-
dividuals (Reiter and Sripada, 2002a), but the corpus is
too small to allow meaningful statistical analysis of word
meanings.
7 Conclusion
To conclude, we believe that parallel text-data corpora
are a valuable resource for investigating lexical seman-
tic and pragmatic issues, and can help shed valuable light
on the fundamental question of how words relate to the
non-linguistic world. We have described in detail how we
have used such a corpus to analyse the meaning and usage
of time phrases in weather forecasts, and also sketched
our current work on other analyses of text-data corpora.
We hope that other researchers interested in semantics
and pragmatics will find our techniques interesting, and
consider whether they might be useful in exploring other
semantic and pragmatic questions about word meaning
usage.
phrase F1 F2 F3 F4 F5 combined
after midnight 0600 0600
afternoon * 1630 1630
around midday * * * 1200
by 0600 0600 0600
by afternoon 1500 1200 1200 1200 1200
by early afternoon * * * 1330 1330
by early evening 1800 * 1800
by early morning 0300 0300
by evening 1800 0000 0000 1800 0000 0000
by late afternoon 1800 * 1800
by late evening 0000 0000 0000 0000
by late morning * 0900 1200 1030
by mid afternoon 1500 1500 * 1500
by mid evening * 2100 * 2100
by mid morning * * * 0900
by midday 1200 * 1200 1200 1200 1200
by midnight 0000 0000 0000 0000
by morning 0600 0600 * 0600
during afternoon * * 1500
during evening 0000 0000 0000 0000
during morning * 1030 * 0900 0900
early afternoon * * * 1500 1500
early evening * 1800 1800
evening 1612 2100 0000 0000
from midday 1200 1200
in afternoon * * 1800 1800
in evening 0000 0000 0000
in morning 1200 1200
late evening * 0000 0000 0000
later in evening 0000 0000
later in night 0600 0600
mid morning * * * 0900
overnight 0600 0600 * 0600
tonight * 0000 0000
* means phrase was used fewer than five times by this forecaster.
Phrases used less than 5 times by all forecasters combined are omitted.
Contextual phrases (such as later) are also omitted.
If 2 or more times are equally common, their average is shown.
Table 6: Most common (mode) usage of time phrases, by forecaster
Acknowledgements
Our thanks to the many individuals who have discussed
this work with us, of which there are too many to list
here. Special thanks to our industrial collaborators at
WNI/Oceanroutes, without whom this work would have
been impossible. This work was supported by the UK En-
gineering and Physical Sciences Research Council (EP-
SRC), under grant GR/M76881.
References
Eugenio Alberdi, Julie-Clare Becher, Ken Gilhooly, Jim
Hunter, Robert Logie, Andy Lyon, Neil McIntosh, and
Jan Reiss. 2001. Expertise and the interpretation of
computerized physiological data: implications for the
design of computerized monitoring in neonatal inten-
sive care. International Journal of Human-Computer
Studies, 55:191?216.
Regina Barzilay and Lillian Lee. 2002. Bootstrap-
ping lexical choice via multiple sequence alignment.
In Proceedings of the 2002 Conference on Empirical
Methods in Natural Language Processing (EMNLP-
2002).
Peter F. Brown, John Cocke, Stephen Della Pietra, Vin-
cent J. Della Pietra, Frederick Jelinek, John D. Laf-
ferty, Robert L. Mercer, and Paul S. Roossin. 1990. A
statistical approach to machine translation. Computa-
tional Linguistics, 16(2):79?85.
Frederick Cassidy and Joan Hall, editors. 1996. Dictio-
nary of American Regional English. Belknap.
William Cohen. 1995. Fast effective rule induction.
In Proc. 12th International Conference on Machine
Learning, pages 115?123. Morgan Kaufmann.
D. Cruse. 1986. Lexical Semantics. Cambridge Univer-
sity Press.
Philip Edmonds and Graeme Hirst. 2002. Near-
synonymy and lexical choice. Computational Linguis-
tics, pages 105?144.
Sidney Landau. 1984. Dictionaries: the art and craft of
lexicography. Scribner.
Barbara Malt, Steven Sloman, Silvia Gennari, Meiyi Shi,
and Yuan Wang. 1999. Knowing versus naming:
Similarity and the linguistic categorization of artifacts.
Journal of Memory and Language, 40:230?262.
Geoffrey Nunberg. 1978. The Pragmatics of Reference.
University of Indiana Linguistics Club, Bloomington,
Indiana.
Franz Och and Herman Ney. 2000. A comparison of
alignment models for statistical machine translation.
In Proceedings of the 18th International Conference
on Computational Linguistics (COLING-2000), pages
1086?1090.
Rohit Parikh. 1994. Vagueness and utility: The seman-
tics of common nouns. Linguistics and Philosophy,
17:521?535.
Ehud Reiter and Somayajulu Sripada. 2002a. Human
variation and lexical choice. Computational Linguis-
tics, 28:545?553.
Ehud Reiter and Somayajulu Sripada. 2002b. Should
corpora texts be gold standards for NLG? In Proceed-
ings of the Second International Conference on Natu-
ral Language Generation, pages 97?104.
Ehud Reiter, Roma Robertson, and Liesl Osman. 2000.
Knowledge acquisition for natural language genera-
tion. In Proceedings of the First International Con-
ference on Natural Language Generation, pages 217?
215.
Ehud Reiter, Somayajulu Sripada, and Roma Robertson.
2003. Acquiring correct knowledge for natural lan-
guage generation. Journal of Artificial Intelligence Re-
search. Forthcoming.
Ehud Reiter. 1991. A new model of lexical choice for
nouns. Computational Intelligence, 7(4):240?251.
Eleanor Rosch. 1978. Principles of categorization. In
E. Rosch and B. Lloyd, editors, Cognition and Catego-
rization, pages 27?48. Lawrence Erlbaum, Hillsdale,
NJ.
Deb Roy. 2002. Learning visually grounded words and
syntax for a scene description task. Computer Speech
and Language, 16:353?385.
Jeffrey Siskind. 2001. Grounding the lexical semantics
of verbs in visual perspection using force dynamics
and event logic. Journal of Artificial Intelligence Re-
search, 15:31?90.
Somayajulu Sripada, Ehud Reiter, Jim Hunter, and Jin
Yu. 2002. SUMTIME-METEO: Parallel corpus of
naturally occurring forecast texts and weather data.
Technical Report AUCS/TR0201, Computing Science
Dept, Univ of Aberdeen, Aberdeen AB24 3UE, UK.
Somayajulu Sripada, Ehud Reiter, Jim Hunter, and Jin
Yu. 2003a. Exploiting a parallel text-data corpus.
In Proceedings of Corpus Linguistics 2003. UCREL,
Lancaster University, UK.
Somayajulu Sripada, Ehud Reiter, Jim Hunter, and Jin
Yu. 2003b. Summarising neonatal time-series data. In
Proceedings of EACL-2003. Forthcoming.
Jin Yu, Ehud Reiter, Jim Hunter, and Somayajulu Sri-
pada. 2003. Sumtime-turbine: A knowledge-based
system to communicate gas turbine time-series data.
In Proceedings of IEA/AIE-2003.
Abstract 
Post-editing is commonly performed on computer-
generated texts, whether from Machine Translation 
(MT) or NLG systems, to make the texts accept-
able to end users. MT systems are often evaluated 
using post-edit data.  In this paper we describe our 
experience of using post-edit data to evaluate 
SUMTIME-MOUSAM, an NLG system that pro-
duces marine weather forecasts. 
1 Introduction 
Natural Language Generation (NLG) systems must of 
course be evaluated, like all NLP systems. Previous work on 
NLG evaluation has focused on either experiments con-
ducted with users who read the generated texts, or on com-
parisons of generated texts to corpora of human-written 
texts.  In this paper we describe an evaluation technique, 
which looks at how much humans need to post-edit gener-
ated texts before they are released to users.  Post-edit 
evaluations are common in machine translation, but we be-
lieve that ours is the first large-scale post-edit evaluation of 
an NLG system.  
The system being evaluated is SUMTIME-MOUSAM [Sri-
pada et al 2003], an NLG system, which generates marine 
weather forecasts from Numerical Weather Prediction 
(NWP) data. SUMTIME-MOUSAM is operational and is used 
by Weathernews (UK) Ltd to generate 150 draft forecasts 
per day, which are post-edited by Weathernews forecasters 
before being released to clients. 
2 Background 
2.1 Evaluating NLG Systems  
Common evaluation techniques for NLG systems [Mellish 
and Dale, 1998] include:  
? Showing generated texts to users, and measuring how 
effective they are at achieving their goal, compared to 
some control text (for example, [Young, 1999]) 
? Asking experts to rate computer-generated texts in 
various ways, and comparing this to their rating of 
manually authored texts (for example, [Lester and 
Porter, 1997]) 
? Automatically comparing generated texts to a corpus of 
human authored texts (for example, [Bangalore et al 
2000]). 
Each of these techniques is effective under different ap-
plication contexts in which NLG systems operate. For in-
stance, a corpus based technique is effective when a high 
quality corpus is available. The appeal of post-edit evalua-
tion as done with SUMTIME-MOUSAM is that (A) the edits 
should indicate actual mistakes instead of just differences in 
how things can be said and (B) the amount of post-editing 
required is a very important practical measure of how useful 
the system is to real users (forecasters in our case). 
Post-edit evaluations are a standard technique in Machine 
Translation [Hutchins and Somers, 1992]. The only previ-
ous use of post-edit evaluation in NLG that we are aware of 
is Mitkov and An Ha [2003], but their evaluation is rela-
tively small, and they give little information about it. 
2.2 SUMTIME-MOUSAM 
SUMTIME-MOUSAM [Sripada et al 2003] is an NLG system 
that generates textual weather forecasts from numerical 
weather prediction (NWP) data.  The forecasts are marine 
forecasts for offshore oilrigs.  Table 1 shows a small extract 
from the NWP data for 12-06-2002, and Table 2 shows part 
of the textual forecast that SUMTIME-MOUSAM generates 
from the NWP data.  The Wind statements in Table 2 are 
mostly based on the NWP data in Table 1.  
 
Time Wind 
Dir 
Wind Spd 
10m 
Wind Spd 
50m 
Gust 
10m 
Gust 
50m 
06:00 W 10.0 12.0 12.0 16.0 
09:00 W 11.0 14.0 14.0 17.0 
12:00 WSW 10.0 12.0 12.0 16.0 
15:00 SW 7.0 9.0 9.0 11.0 
18:00 SSW 8.0 10.0 10.0 12.0 
21:00 S 9.0 11.0 11.0 14.0 
00:00 S 12.0 15.0 15.0 19.0 
 
Table 1. Weather Data produced by an NWP model for 12-
Jun 2002 
 
Evaluation of an NLG System using Post-Edit Data: Lessons Learnt 
Somayajulu G. Sripada and Ehud Reiter and Lezan Hawizy 
Department of Computing Science 
University of Aberdeen 
Aberdeen, AB24 3UE, UK 
{ssripada,ereiter,lhawizy}@csd.abdn.ac.uk 
 SUMTIME-MOUSAM generates texts in three stages 
[Reiter and Dale, 2000]. 
Document Planning: Text structure is specified by 
Weathernews, via a control file.  The key content-
determination task is selecting ?important? or ?significant? 
data points from the underlying weather data to be included 
in the forecast text. SUMTIME-MOUSAM uses a bottom-up 
segmentation algorithm for this task [Sripada et al 2002].  
Micro-planning: The key decisions here are lexical selec-
tion, aggregation, and ellipsis. SUMTIME-MOUSAM uses 
rules for this that are derived from corpus analysis and other 
knowledge acquisition activities [Reiter et al 2003; Sripada 
et al 2003]. 
Realization: SUMTIME-MOUSAM uses a simple realiser 
that is tuned to the Weathernews weather sublanguage. 
SUMTIME-MOUSAM is partially controlled by a control 
data file that Weathernews can edit.  For example, this file 
specifies error function data that controls the segmentation 
process for content determination. The error function data 
decides the level of abstraction achieved by the segmenta-
tion process ? the larger the error function value the higher 
the level of abstraction achieved by segmentation. 
2.3 SUMTIME-MOUSAM at Weathernews 
Weathernews (UK) Ltd, a private sector weather services 
company, uses SUMTIME-MOUSAM to generate draft fore-
casts.  The process is illustrated in Figure 1.  Forecasters 
load the NWP data for the forecast into Marfors, which is 
Weathernews? forecasting tool. Using Marfors, forecasters 
edit the NWP data, using their meteorological expertise and 
additional information such as satellite weather maps. They 
then invoke SUMTIME-MOUSAM to generate an initial draft 
of the forecast. This initial draft helps the forecaster under-
stand the NWP data, and often suggests further edits to the 
NWP data.  The generate-and-edit-data process may be re-
peated.  When the forecaster is satisfied with the NWP data, 
he invokes SUMTIME-MOUSAM again to generate a final 
draft textual forecast, marked ?Pre-edited Text? in Figure 1. 
The forecaster then uses Marfors to post-edit the textual 
forecast.  When the forecaster is done, Marfors assembles 
the complete forecast from the individual fields, and sends it 
to the customer. 
 
Section 2. FORECAST 6 - 24 GMT, Wed 12-Jun 2002 
Field Text 
WIND(KTS) 10M W 8-13 backing SW by mid after-
noon and S 10-15 by midnight. 
WIND(KTS) 50M W 10-15 backing SW by mid after-
noon and S 13-18 by midnight. 
WAVES(M) 
SIG HT 
0.5-1.0 mainly SW swell. 
WAVES(M) 
MAX HT 
1.0-1.5 mainly SW swell falling 1.0 
or less mainly SSW swell by after-
noon, then rising 1.0-1.5 by mid-
night. 
WAVE PERIOD 
(SEC) 
Wind wave 2-4 mainly 6 second 
SW swell. 
WINDWAVE 
PERIOD (SEC) 
2-4. 
SWELL PERIOD 
(SEC) 
5-7. 
WEATHER Mainly cloudy with light rain 
showers becoming overcast around 
midnight. 
VISIBILITY 
(NM) 
Greater than 10. 
AIR TEMP(C) 8-10 rising 9-11 around midnight. 
CLOUD 
(OKTAS/FT) 
4-6 ST/SC 400-600 lifting 6-8 
ST/SC 700-900 around midnight. 
 
Table 2. Extract from SUMTIME-MOUSAM Forecast Pro-
duced for 12-Jun 2002 (AM). 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 1. Schematic Showing SUMTIME-MOUSAM Used at Weathernews 
Post-
edited 
Text  
Marfors 
Data Editor 
Marfors Text Editor 
SUMTIME-
MOUSAM 
Marfors 
Data Editor 
Pre-edited 
Text SUMTIME-
MOUSAM 
Text 1 Data 1 
Edited Data 
NWP Data 
 Note that SUMTIME-MOUSAM is used for two purposes 
by Weathernews; to help forecasters understand and there-
fore edit the NWP data, and to help generate texts for cus-
tomers.  In this paper we focus on evaluating the second 
usage of the system (generating texts for customers). 
When a forecast is complete, Marfors saves the final ed-
ited NWP data, marked ?Edited data? in Figure 1 and the 
final edited forecast marked ?Post-edited Text? into a data-
base.  This data is forwarded to us for 150 sites per day; this 
is the basis of our post-edit evaluation.  Marfors does not 
directly save the SUMTIME-MOUSAM text that forecasters 
edit (?Pre-edited Text? in Figure 1), but we can reconstruct 
this text by running the system on the final edited NWP 
data. 
3 Post-Edit Evaluation 
3.1 Data 
The evaluation was carried out on 2728 forecasts, collected 
during period June to August 2003.  Each forecast was 
roughly of 400 words, so there are about 1 million words in 
all in the corpus. 
For each forecast, we have the following data 
 
? Data: The final edited NWP data 
? Pre-edit text: The final draft forecast produced by 
SUMTIME-MOUSAM, which we reconstruct as de-
scribed in Section 2.3. 
? Post-edit text: The manually post-edited forecast, 
which was sent to the client. 
? Background information: includes date, location, and 
forecaster 
 We do not currently use the NWP data (other than for 
reconstructing SUMTIME-MOUSAM texts), although we 
hope in the future to include it in our analyses, in a manner 
roughly analogous to Reiter and Sripada [2003]. This data 
set continues to grow, we receive approximately 150 new 
forecasts per day. 
3.2 Analysis Procedure 
The following procedure is performed automatically by a 
software tool. First, we perform some data transformation 
and cleaning.  This includes breaking sentences up into 
phrases, where each phrase describes the weather at one 
point in time. 
For example, the pre-edit text in Figure 2 would be bro-
ken up into three phrases: 
 
A1 SW 20-25 
A2 backing SSW 28-33 by midday 
A3 then gradually increasing 34-39 by midnight 
 
 
 
 
 
 
 
 
 
 
Figure 2. Example pre-edit and post-edit texts from the post-
edit corpus 
 
The Figure 2 post-edit text is divided into two phrases: 
 
B1 SW 22-27 
B2 gradually increasing SSW 34-39 
 
The second step is to align phrases from these two tables 
as a preparation for comparison in the next step. Alignment 
is a complex activity and is described in detail next. To start 
with we generate an exhaustive list of all the possible com-
binations of phase alignments. 
For example, consider the texts in Figure 2. Here we gen-
erate the following list of possible alignments: 
 
{(A1, B1), (A1, B2), (A2, B1), (A2, B2), (A3, B1), (A3, 
B2)} 
 
  Next, we compute match scores for each of these possi-
ble alignments and use them for selecting the right align-
ments. For each unedited phrase Ai, the alignment with the 
highest matching score is selected. For the purpose of com-
puting the match scores, phrases are parsed using ?parts of 
speech? designed for weather sublanguage such as direction, 
speed and time. The total match score of a pair of phrases is 
computed as the sum of the match scores for their constitu-
ents. Match score (MS) for a pair of constituents depends 
upon their part of speech and also their degree of match. MS 
is defined as a product of two terms as explained below: 
? Match score due to degree of match: we assign a match 
score of 2 for exact matches, 1 for partial matches and 
0 for mismatches. 
? Weight factor denoting importance of constituents for 
alignment: Constituents belonging to certain parts of 
speech (POS) are more significant for alignment than 
others. For example, times are more significant for 
alignment than verbs. Also weights are varied for the 
same POS based on its context in the phrase. For ex-
ample, direction receives higher weight if it occurs in 
a phrase without a time or speed. This is because in 
such phrases direction is the only means for align-
ment. 
Continuing with our example sentences in Figure 2, we 
show below how we find an alignment for A3. As described 
earlier, A3 can be aligned to either B1 or B2. The MS for 
(A3, B1) is zero as shown in Table 3. 
 
A. Pre-edit Text: SW 20-25 backing SSW 28-33 by 
midday, then gradually increasing 34-39 by midnight. 
 
B. Post-edit Text: SW 22-27 gradually increasing 
SSW 34-39. 
 
POS A3 B1 MS 
conjunction Then <none> 0 
Adverb Gradually <none> 0 
Verb Increasing <none> 0 
Direction <none> SW 0 
Speed range 34-39 22-27 0 
Time By midnight <none> 0 
 
Table 3 Match Score for A3 and B1 
 
The MS for (A3, B2) is 2*(2*w1+w2) where w1 is the 
weight for Adverb/verb and w2 (>w1) for speed as shown in 
Table 4. Based on the match scores computed above A3 is 
aligned with B2. Similarly A1 is aligned with B1. A2 is 
unaligned, and treated as a deleted phrase. 
 
POS A3 B2 MS 
conjunction Then <none> 0 
Adverb Gradually Gradually w1*2 
Verb Increasing Increasing w1*2 
Direction <none> SSW 0 
Speed range 34-39 34-39 w2*2 
Time By midnight <none> 0 
 
Table 4. Match Score for A3 and B2 
 
The third step is to compare aligned phrases, such as A1 
and B1.  One evaluation metric is based on comparing 
aligned phrases as a whole. Here we simply record ?match? 
or ?mismatch?.  For example, both (A1, B1) and (A3, B2) 
are mismatches. We then compare constituents in the 
phrases to determine more details about the mismatches.  
For this detailed comparison we use the domain-specific 
part-of-speech tags described earlier. Each part-of-speech 
should occur at most once in a phrase (in our weather sub-
language), so we simply align on the basis of the tag. After 
constituents are aligned, we label each pre-edit/post-edit 
pair as match, replace, add, or delete. For example, A and B 
are analysed as in Table 5. 
 
POS A B label 
Direction SW SW match 
Speed 20-25 22-27 replace 
    
Conjunction then <none> delete 
Adverb gradually gradually match 
Verb increasing increasing match 
Direction <none> SSW add 
Speed 34-39 34-39 match 
Time by midnight <none> delete 
 
Table 5. Detailed Edit Analysis 
3.3 Analysis of Results 
We processed 2728 forecast pairs (pre-edited and post-
edited). These were divided into 73041 phrases. Out of 
these, the alignment procedure failed to align 7608 (10%) 
phrases. For instance, in the example of Section 3.2, phrase 
A2 was not aligned with any B phrase.  Alignment failure 
generally indicates that the forecaster is unhappy with 
SUMTIME-MOUSAM?s segmentation that is with the sys-
tem?s content determination. We have manually analysed 
some of these cases, and in general it seems the forecasters 
are performing more sophisticated data analysis than 
SUMTIME-MOUSAM, and are also more sensitive to which 
changes are significant enough to be reported to the user.  
We have manually inspected alignment quality of 100 
random phrase pairs to determine cases where our alignment 
procedure erroneously aligned phrases. We found one case 
of improper alignment. The pre-edited phrase ?soon becom-
ing variable? has not been aligned to its corresponding iden-
tical post-edited phrase. Inspection of the rest of the corpus 
showed that this error repeated 54 times in the whole cor-
pus. These cases have been classified as alignment failures 
and therefore do not affect the post-edit analysis. 
 
Time (Hours) Direction Speed 
00 ESE 12 
03 ESE 12 
06 ESE 11 
09 ESE 11 
12 ESE 10 
15 ESE 8 
18 ESE 9 
21 ESE 11 
24 ESE 13 
 
Table 6. Wind 10m data for 14 Jul 2003 
 
For example, consider the Wind 10m data shown in Table 
6. Our content determination algorithm first segments the 
data in table 6 (see Sripada et al[2002] for more details). 
Segmentation is the process of fitting straight lines to a data 
set in such a way that a minimum error is introduced by the 
lines. Since the direction data is constant at ESE, there is 
only one segment for this data. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 3. Segmentation of Wind speed data shown in Ta-
ble 6. 
Wind speed data however is segmented by two lines as 
shown in Figure 3, one line joining the point (0,12) with 
(15,8) and the second joining the point (15,8) with (24,13). 
Our content selection algorithm therefore selects data points 
Segmentation of Wind 10m data
0
2
4
6
8
10
12
14
0 3 6 9 12 15 18 21 24
Time
W
in
d 
Sp
e
e
d
(0,12), (15,8) and (24,13) to be included in the forecast. In 
this case our system produced: 
 
?ESE 10-15 gradually easing 10 or less by mid afternoon 
then increasing 11-16 by midnight? 
However, forecasters view this data as a special case and 
don?t segment it the way we do. Here the wind speed is al-
ways in the range of ?10-15? except at 1500 and 1800 hours. 
Therefore they mention the change as an additional informa-
tion to an otherwise constant wind speed. In this case, the 
forecaster edited text is: 
 
?ESE 10-15 decreasing 10 or less for a time later?. 
Talking about the segmentation differences, one of the 
forecasters at Weathernews told us that another factor af-
fecting segmentation is related to the end user. End users of 
the marine forecasts are oil company staff who schedule 
activities on the oilrigs in the North Sea. Over the years 
forecasters at Weathernews have acquired a good under-
standing of the informational needs of the oil company staff. 
So they use the forecast statements as messages to the end 
users about the weather and know what kind of messages 
will be useful to the end users. In the example texts shown 
in Figure 2 the forecaster could have thought that the impor-
tant message to communicate about wind is that it is in-
creasing monotonically and is likely to be in the range be-
tween 22 (the actual initial wind speed) and 39. Everything 
else distracts this primary message and therefore needs to be 
avoided. Once again there is post segmentation reasoning 
used by the forecasters. We are investigating better pattern 
matching techniques and better user models to improve our 
content selection. 
 
S. No. Mismatch Type Freq. % 
1. Ellipses (word additions 
and deletions) 
35874 65 
2. Data Related Replacements 
(range and direction re-
placements) 
10781 20 
3. Lexical Replacements 8264 15 
 Total 54919  
 
Table 7. Results of the Evaluation showing summary cate-
gories and their frequencies 
Going back to the successfully aligned phrases, 43914 
(60%) are perfect matches, and the remaining 21519 (30%) 
are mismatches.  Table 7 summarises the mismatches.  
Here, each mismatch is classified as 
? Ellipses: additions and deletions.  For example, delet-
ing the time phrase by midnight in the (A3, B2) pair.  
These generally indicate problems with SUMTIME-
MOUSAM?s aggregation and ellipsis. 
? Data replacements: changes (replaces) to constituents 
that directly convey NWP data, such as wind speed 
and direction.  For example, changing 20-25 to 22-27 
in the (A1, B1) pair.  These can indicate content prob-
lems.  They also occur when forecasters believe the 
NWP data is incorrect but decide to just correct the 
forecast text and not the data (eg, skip generate-and-
edit step described in section 2.3). 
? Lexical replacements: All other changes (replaces). For 
example, if the conjunction ?then? was replaced by 
?and?.  This generally indicates a problem in 
SUMTIME-MOUSAM?s lexicalisation strategy. 
For each pair of phrases compared in the evaluation, we 
have counted the number of times each edit operation such 
as add, delete and replace is performed by forecasters. For 
example consider the two phrase pairs shown in Table 5. 
For the first phrase pair of ?SW 20-25? and ?SW 22-27? fore-
casters performed zero add, zero delete and one replace 
operation (?20-25? is replaced by ?22-27?). For the second 
phrase pair of ?then gradually increasing 34-39 by mid-
night? and ?gradually increasing SSW 34-39? forecasters 
performed one add (added ?SSW?), two delete (deleted 
?then? and ?by midnight?) and zero replace operations. We 
hypothesized that forecasters were making significantly 
more add and delete operations than replace operations. For 
verifying this, we have performed a pairwise t-test. Vari-
able1 for the t-test represents the sum of the counts of add 
and delete operations for each pair of phrases. Variable2 
represents the count of replace operations. For example, for 
the two phrase pairs shown in Table 5, variable1 has values 
of zero and three where as variable2 has values of one and 
zero. This test showed (with a p value less than 10-20) that 
forecasters were performing more additions and deletions 
than replacements. In other words, ellipsis is the main prob-
lem in our system. Most (25235 out of 35874, 70%) of these 
errors are deletions, where the forecaster deletes words from 
SUMTIME-MOUSAM?s texts. 
A manual analysis of some ellipsis cases has highlighted 
some general phenomena.  First of all, many ellipsis cases 
are ?downstream? consequences of earlier changes. For ex-
ample, if we look at the (A3, B2) pair above, this contains 
three ellipsis changes: then was deleted, SSW was added, 
and by midnight was deleted.  The first two of these changes 
are a direct consequence of the deletion of phrase A2.  If 
SUMTIME-MOUSAM?s content determination system was 
changed so that it did not generate A2, then the micro plan-
ner would have expressed A3 as gradually increasing SSW 
34-39 by midnight, which is identical to B2 except for by 
midnight. 
The deletion of by midnight is an example of another 
common phenomenon, which is disagreement among indi-
viduals as to how text should be written.  As described in 
[Reiter et al 2003], some forecasters elide the last time 
phrase in simple sentences such as this one, and some do 
not.  An earlier version of SUMTIME-MOUSAM in fact 
would have elided this time phrase, but we changed the be-
havior of the system in this regard after consultation.  Ellip-
sis errors are inevitable in cases where the different fore-
casters disagree about when to elide.  However, since post-
editors can delete words more quickly than they can add 
words, it probably makes sense from a practical perspective 
to be conservative about elision, and only elide in unambi-
guous cases. We will not further discuss data replacement 
errors, since they reflect either content problems or cases 
where NWP data was not corrected at the input time but 
edited directly in the final text. 
We have discussed lexical replacement errors in detail 
elsewhere [Reiter and Sripada, 2002].  In general terms, 
some errors reflect problems with SUMTIME-MOUSAM; for 
example, the system overuses then as a connective, so fore-
casters often replaced then by alternative connectives such 
as and.  However, many lexical replacement errors simply 
reflected the lexical preferences of individual forecasters 
[Reiter and Sripada, 2002].  For example, SUMTIME-
MOUSAM always uses the verb easing to indicate a reduc-
tion in wind speed.  Most forecasters were happy with this, 
but 3 individuals usually changed this to decreasing. 
A general observation is that some forecasters post-edited 
texts much more than others.  For example, while overall 
28% of phrases were edited, edit rates by individual fore-
casters varied from 4% to 93%.  We do not know why edit 
rates vary so much, although it may be significant the indi-
vidual with the highest (93%) edit rate is one of the most 
experienced forecasters, who takes well-justified pride in 
producing well-crafted forecasts. 
Summarizing the results of our evaluation: 
1. SUMTIME-MOUSAM?s content determination can defi-
nitely be improved, by using more sophisticated segmenta-
tion techniques. 
2. SUMTIME-MOUSAM?s micro-planner can certainly be 
improved in places, for example by varying connectives.  
However, many post-edits are due to individual differences, 
which we cannot do anything about. 
We are currently carrying out another evaluation of SUM-
TIME-MOUSAM by the end users, oilrig staff and other ma-
rine staff who regularly read weather forecasts. In this study 
we compare user?s comprehension of weather information 
from human written and computer generated forecast texts. 
We also measure user ratings (preference) of human written 
and computer generated texts. Preliminary results from our 
study indicate that users make fewer mistakes on compre-
hension questions when they are shown texts that use com-
puter generated words with human selected content. Gener-
ally users seem to prefer computer generated texts to human 
written texts given the same underlying weather data. 
4 Lessons from our Post-Edit Evaluation 
As stated in Section 2.1, we were attracted to post-edit 
evaluation because we believed that (A) people would only 
edit things that were clearly wrong; and (B) post-editing was 
an important usefulness metric from the perspective of our 
users (forecasters). 
Looking back, (B) was certainly true.  The amount of 
post-editing that generated texts require is a crucial compo-
nent of the cost of using SUMTIME-MOUSAM, and hence of 
the attractiveness of the system to users (forecasters). Al-
though we have not measured the time required for perform-
ing post-edits, we have used edit-distance measures used in 
MT evaluations as an approximate cost metric. We have 
computed our cost metric by setting different cost (weight) 
values to different edit operations. Cost of add and replace 
operations is set to 5 and cost of delete is set to 1 as used in 
Su et al[1992]. The ratio of the cost of edits and the cost of 
writing the entire forecast manually (adding all the words) is 
computed to be 0.15.  (A) however was perhaps less true 
than we had hoped. Wagner [1998] also described post-
edited texts in MT as at times noisy. Our analysis of manu-
ally written forecasts [Reiter and Sripada, 2002] had high-
lighted a number of ?noise? elements that made it more dif-
ficult to extract information from such corpora. Basically 
there are many ways of communicating information in text, 
and the fact that a generated text doesn?t match a corpus text 
does not mean that the generated text is wrong.  We as-
sumed that people would only post-edit mistakes, where the 
generated text was wrong or sub-optimal, and hence post-
edit data would be better for evaluation purposes than cor-
pus comparisons. 
In fact, however, there were many justifications for post-
edits: 
1. Fixing problems in the generated texts (such as 
overuse of then);  
2. Refining/optimizing the texts (such as using for 
a time);  
3. Individual preferences (such as easing vs de-
creasing); and  
4. Downstream consequences of earlier changes 
(such as introducing SSW in B2, in the example 
of Section 3.2).  
We wanted to use our post-edit data to improve the sys-
tem, not just to quantify its performance, and we discovered 
that we could not do this without attempting to analyze why 
post-edits were made.  Probably the best way of doing this 
was to discuss post-edits with the forecasters.  Alternatively, 
we could have asked forecasters to fill in problem sheets to 
capture their explanation of post-edits. Such feedback from 
the forecasters would have allowed us to reason with post-
edit data to improve our system. In [Reiter et al 2003] we 
explained that we found that analysis of human-written cor-
pora was more useful if it was combined with directly work-
ing with domain experts; and essentially this (perhaps not 
surprisingly) is our conclusion about post-edit data as well.  
One of the lessons we learnt from this exercise has been 
that post-edit evaluations are useful to compute a cost metric 
to quantify the usefulness of a system. For example, as de-
scribed earlier, we have computed a cost metric, 0.15 signi-
fying the post-editing effort. Post-edit evaluations are also 
useful in revealing general problem areas in a system. For 
example, as described in section 3.3, our evaluation showed 
that ellipsis related problems are more serious in our system 
than others. However, post-edit evaluations are not affective 
in discovering specific problems in a system. The main rea-
son for this is that many post-edits, as stated earlier, do not 
actually fix problems in the generated text at all. The real 
post-edits that fixed problems in the generated text were 
buried among the other noisy post-edits. 
This lesson of course is the result of our method of post-
edit evaluation. Post-editing was not supported by 
SUMTIME-MOUSAM and forecasters used Marfors (see sec-
tion 2.3) to perform post-editing. Therefore, we had to ac-
cept the post-edit data with all the noise. In MT, post-editors 
often work under predefined guidelines on post-editing and 
also use post-editing tools.  For example, post-editing tools 
automatically revise texts to fix ?down-stream? conse-
quences of human edits. If post-edit tools are similarly inte-
grated into NLG systems, there is going to be a significant 
reduction in the number of noisy post-edits allowing us to 
focus on real post-edits. 
Because post-editing is subjective varying from individ-
ual to individual, we need to understand the post-editing 
behaviour of individuals to analyze the noisy post-edit data. 
Although we have data on forecaster variations in our post-
edit corpus, these variations have not been observed from 
different forecasters post-editing the same text. This we 
could have achieved by performing a pilot before the actual 
evaluation. For the pilot all the forecasters post-edit the 
same set of forecasts, thus revealing their individual prefer-
ences. Post-edit data from the pilot would have enabled us 
to factor out the effects of forecaster variation from the real 
evaluation data. As described above noise in the post-edit 
data can be reduced by using post-edit tools and by perform-
ing a pilot before the real evaluation. This means that post-
edit evaluations need preparation in the form of developing 
post-edit tools and carrying out pilot studies. This is another 
lesson we learnt from our current evaluation. 
Although analyzing the post-edit data was a major en-
deavour for us, the overall cost of post-edit evaluation was 
not much compared to the effort that would have been re-
quired to conduct end user experiments on 2728 texts.  Of 
course, this was only true because SUMTIME-MOUSAM 
texts were being post-edited in any case by Weathernews.  
The cost-effectiveness of post-edit evaluation is less clear if 
the evaluators must organize and pay for the post-editing, as 
Mitkov and An Ha [2003] did. In this context we should 
speculate that when more and more NLG systems are de-
ployed in the real world, post-editing will be accepted as a 
component in the process of automatic text generation much 
in the same way post-editing is now a part of MT. 
5 Conclusion 
Evaluation is a key aspect of NLG; we need to know how 
well theories and systems work.  We have used analysis of 
post-edits, a popular evaluation technique in machine trans-
lation, to evaluate SUMTIME-MOUSAM, an NLG system 
that generates marine weather forecasts.  We encountered 
some problems, such as the need to identify why post-edits 
were made which make post-edit data hard to discover spe-
cific clues for system improvement. However, post-edit 
evaluation can reveal problem areas in the system and also 
quantify system utility for real users. 
References 
[Bangalore et al, 2000] Srinivas Bangalore, Owen Ram-
bow, and Steve Whittaker. 2000. Evaluation metrics for 
generation. In Proc. of the First International Natural 
Language Generation Conference (INLG2000), Israel. 
[Hutchins and Somers, 1992] John Hutchins and Harold L. 
Somers, 1992. An Introduction to Machine Translation, 
Academic Press. 
[Lester and Porter, 1997] James Lester and Bruce Porter. 
1997. Developing and empirically evaluating robust 
explanation generators: The KNIGHT experiments. 
Computational Linguistics, 23-1:65-103. 
[Mellish and Dale, 1998] Chris Mellish and Robert Dale, 
1998. Evaluation in the context of natural language gen-
eration, Computer Speech and Language 12:349-373. 
[Mitkov and An Ha, 2003] Ruslan Mitkov and Le An Ha, 
2003. Computer-Aided Generation of Multiple-Choice 
Tests, In Proc. of the HLT-NAACL03 Workshop on 
Building Educational Applications Using NLP, Edmon-
ton, Canada, pp. 17-22. 
 [Reiter and Dale, 2000] Ehud Reiter and Robert Dale, 2000. 
Building Natural Language Generation Systems. Cam-
bridge University Press. 
[Reiter and Sripada, 2002] Ehud Reiter and Somayajulu G. 
Sripada, 2002. Human Variation and Lexical Choice. 
Computational Linguistics 28:545-553. 
[Reiter et al, 2003] Ehud Reiter, Somayajulu G. Sripada, 
and Roma Robertson, 2003. Acquiring Correct Knowl-
edge for Natural Language Generation. Journal of Artifi-
cial Intelligence Research, 18: 491-516, 2003. 
[Reiter and Sripada, 2003] Ehud Reiter and Somayajulu G. 
Sripada, 2003. Learning the Meaning and Usage of Time 
Phrases from a Parallel Text-Data Corpus. In Proc. of the 
HLT-NAACL03 Workshop on Learning Word Meaning 
from Non-Linguistic Data, pp 78-85. 
[Sripada et al, 2002] Somayajulu, G. Sripada, Ehud Reiter, 
Jim Hunter and Jin Yu. 2002 Segmenting Time Series 
for Weather Forecasting. In: Macintosh, A., Ellis, R. and 
Coenen, F. (ed) Proc. of ES2002, pp. 193-206. 
[Sripada et al, 2003] Somayajulu G. Sripada, Ehud Reiter, 
and Ian Davy, 2003. SUMTIME-MOUSAM: Configur-
able Marine Weather Forecast Generator. Expert Update, 
6(3):4-10. 
[Su et al, 1992] Keh-Yih Su, Ming-Wen Wu and Jing-Shin 
Chang, 1992, A new quantitative quality measure for 
machine translation systems. In Proceedings of 
COLING-92, Nantes, pp 433-439.  
[Wagner, 1998] Simone Wagner, 1998. Small Scale Evalua-
tion Methods In: Rita N?bel; Uta Seewald-Heeg (eds.): 
Evaluation of the Linguistic Performance of Machine 
Translation Systems. Proc. of the Workshop at the 
KONVENS-98. Bonn, pp 93-105. 
[Young, 1999] Michael Young, 1999. Using Grice?s maxim 
of quantity to select the content of plan description, Arti-
ficial Intelligence 115:215-256. 
Using Spatial Reference Frames to Generate Grounded Textual
Summaries of Georeferenced Data
Ross Turner, Somayajulu Sripada and Ehud Reiter
Dept of Computing Science,
University of Aberdeen, UK
{rturner,ssripada,ereiter}@csd.abdn.ac.uk
Ian P Davy
Aerospace and Marine Intl.,
Banchory, Aberdeenshire, UK
idavy@weather3000.com
Abstract
Summarising georeferenced (can be iden-
tified according to it?s location) data in
natural language is challenging because it
requires linking events describing its non-
geographic attributes to their underlying
geography. This mapping is not straightfor-
ward as often the only explicit geographic
information such data contains is latitude
and longitude. In this paper we present an
approach to generating textual summaries
of georeferenced data based on spatial ref-
erence frames. This approach has been im-
plemented in a data-to-text system we have
deployed in the weather forecasting domain.
1 Introduction
Data-to-text systems are NLG systems that gener-
ate texts from raw input data. Many examples of
such systems have been reported in the literature,
which have been applied in a number of domains and
to different types of input. For example, BabyTalk
(Portet et al, 2007) generates medical reports from
sensors monitoring a baby in a Neonatal Intensive
Care Unit, while (Hallett and Scott, 2005) describe a
system for generating reports from events in medical
records. SumTime (Reiter et al, 2005), (Coch, 1998)
and Fog (Goldberg et al, 1994) generate weather
forecasts from the output of weather computer sim-
ulation models, while (Iordanskaja et al, 1992) and
(Ro?sner, 1987) both generate summaries from em-
ployment statistics.
As the above examples show most work in data-to-
text up to now has concentrated almost exclusively
on time series data. Work on generating text from
spatial data has been reported in Coral (Dale et al,
2005), which generates route descriptions of a path
constructed from Geographical Information Systems
(GIS) datasets. Unlike the input to Coral however,
most georeferenced data contains only limited spatial
information(in many cases, only latitude and longi-
tude).
As (Roy and Reiter, 2005) point out, connecting
language to the non-linguistic world is an important
issue in Cognitive Science and Aritificial Intelligence;
moreover, geographic data is becoming increasingly
ubiquitous as the availability of low cost locational
devices such as GPS increases, and GIS become more
user friendly. Therefore, we believe exploring the
issue of generating textual reports grounded in real
world geographical data is an important challenge.
On a more practical level, it is also a natural next
step in the application of data-to-text technology to
apply it to geographically referenced data.
In the RoadSafe project described in the following
section, we have been investigating this issue in a
data-to-text system that generates road ice weather
forecasts. The subsequent focus of this paper is the
adaption of NLG techniques to the task of summaris-
ing georeferenced data. In particular, the incorpora-
tion of spatial reference frames to generate grounded
(from external GIS data sources) spatial references.
2 Background
Weather forecasting has been one of the most suc-
cessful and widely researched application domains for
NLG systems. The main novel aspect that sets Road-
Safe apart from other weather forecast generators
and indeed, other data-to-text systems, is it?s appli-
cation to spatio-temporal data. The input to Road-
Safe is generated by a road ice simulation model,
which outputs a large (in order of Megabytes) mul-
tivariate data set, shown in Figure 1.
The output of the model contains predicted mea-
surements of 9 meteorological parameters for 1000?s
of points across a road network, each measured at
20 minute intervals during a 24 hour forecast pe-
riod. A map of such a network, belonging to a lo-
cal council in the UK, is shown in Figure 2. This
model forms the basis of a road ice forecasting ser-
16
Figure 1: Part of a RoadSafe input data set show-
ing corresponding spatial and non-spatial attribute ta-
bles; T=Air Temperature (Deg C), W=Dew Point (Deg
C), R=Road Surface Temperature (Deg C), C=Weather
Code, D=Wind direction (Degrees), V=Mean wind
Speed (knots), G=Wind Gust (knots),S=Sky Cover (%),
P=Precipitation Water Equivalent (mm).
vice provided by Aerospace and Marine International
(AMI), which is delivered to local councils via an
online Road Weather Information System (RWIS).
This service provides road engineers with up to the
minute weather information using graphs, graphics
and textual reports that allows them to base their
road maintenance operations on during the winter
months. In RoadSafe we have been working on gen-
erating the textual reports, such as the one shown in
Figure 3, automatically from the model data.
The communicative goal of the textual reports is
to complement detailed tabular and graphical pre-
sentations of the model data with a more general
overview of the weather conditions. In the context
of our work this presents a number of challenges:
1. The input data has to be analysed, this is non-
Figure 2: Road Ice Model Data Points Map
trivial due to the complexity and size of the in-
put data.
2. Our system is required to achieve a huge
data/text compression ratio (Human authored
texts are short and concise summaries). There-
fore, content selection is a serious issue for our
system.
3. Describing the effect of the underlying geogra-
phy on weather conditions, such as ?possible gale
force gusts on higher ground?, is an integral part
of the communicative goal of the text. Infor-
mation containing such relationships is not ex-
plicit in the input data and therefore must be
grounded.
?Another night with all routes quickly falling be-
low zero this evening. Only isolated urban spots in
the south will only drop to around zero. Freezing
fog patches will become more widespread during the
night but thin a little tomorrow morning especially
in the south.?
Figure 3: Example Human Authored Corpus Text
3 Architecture
As noted in the previous section, the input data to
our system contains only limited spatial information:
a point identifier that ties the measurement site to
a particular route and a latitude longitude coordi-
nate. Therefore it is necessary for our system to per-
form additional spatial reasoning to characterise the
input in terms of its underlying geography. The ar-
chitecture of our system shown in Figure 4, extends
17
Figure 4: RoadSafe System Architecture
the architecture for data-to-text systems proposed
in (Reiter, 2007) to include this additional process-
ing. In Section 3.1 we explain some of the rationale
behind these design decisions based on observations
from our knowledge acquisition(KA) Studies. In Sec-
tions 3.2 and 3.3 we explain the additional modules
we have introduced in more detail.
3.1 Observations from Knowledge
Acquisition Studies
We have been working closely with experts at AMI
for a number of winters now in the development of
RoadSafe. During this time we have found that two
interrelated aspects in particular have influenced the
architecture of our system, which we describe next.
Spatial Reference Frames Frames of reference
in this context are a particular perspective in which
the domain can be observed. More precisely, they are
sets of related geographical features (such as elevated
areas) which partition the domain into meaningful
sub areas for descriptive purposes. In Levinson?s ter-
minology (Levinson, 2003), they are absolute refer-
ence systems as they employ fixed bearings. In the
RoadSafe domain we have identified 4 main spatial
frames of reference used by experts in our corpus de-
scribed in (Turner et al, 2008):
1. Altitude e.g. ?rain turning to snow on higher
ground?.
2. Absolute Direction e.g. ?some heavier bursts in
the north?.
3. Coastal Proximity e.g. ?strong winds along the
coast?.
4. Population e.g. e.g. ?Roads through the Urban
areas holding just above freezing?.
Communicative Purpose of Spatial Descrip-
tions From our studies we have found that experts
generally follow 4 steps when writing road ice fore-
casts:
1. Build frames of reference to geographical fea-
tures that may affect general weather condi-
tions.
2. Build an overview of the general weather pat-
tern.
3. Select important features to communicate from
the pattern.
4. Communicate the summary.
Building frames of reference to geographical fea-
tures is important for a human forecaster to be able
to take into account how the geography of the region
influences the general weather conditions. Under-
standing the weathers interaction with the terrain
enables them to make reliable meteorological infer-
ences. For example a spatial description such as ?rain
turning to snow in rural areas? may be geographically
accurate, but does not make sense meteorologically
as it is purely by chance that this event is occurring
at that location.
18
From a NLG system perspective it is important to
take into account the communicative purpose of spa-
tial descriptions in this context, which are express-
ing causality (the effect of geographical features on
weather conditions) rather than being purely loca-
tive. For example, changes in precipitation type are
more commonly seen in higher elevation areas where
the air temperature is generally lower, so a spatial de-
scription describing such an event should make use of
a reference frame that reflects this interaction. Simi-
larly, road surface temperatures are generally higher
in urban areas where there is a general population
effect. For a referring expression generation (REG)
strategy this means that this requires not only ade-
quate spatial representation and reasoning capabili-
ties about an objects location, but also additional in-
formation about an objects function in space. This is
a problem which has been acknowledged in the psy-
cholinguistic literature e.g. (Coventry and Garrod,
2004).
3.2 Geographic Characterisation
Geographic Characterisation is responsible for
grounding the location of the data by making the
relationship between it?s underlying geography ex-
plicit. As the first stage of data analysis it assigns
additional spatial properties to each data point by in-
tersecting the point with external GIS data sources
representing the frames of reference we have iden-
tified. For example after characterisation, the first
point in the spatial attribute table shown in Figure
1 is assigned values [0m,SSW,Urban,Coastal] to rep-
resent elevation, absolute compass direction, popula-
tion density of its immediate area and its proximity
to the coast respectively. This process is more com-
monly known as a form of data enrichment in the
Spatial Data Mining community (Miller and Han,
2001). In the scope of our work it is important for
two reasons: most importantly, it provides a set of
properties that are used by the REG module to gen-
erate spatial descriptions; secondly, these properties
can be taken into account by our analysis method
during the initial segmentation of the data.
3.3 Spatial Reasoner and Spatial Database
The spatial database provides a repository of geo-
graphic information. Frames of reference are stored
as thematic layers from various GIS data sources con-
sisting of sets of boundary objects. For example, al-
titude is represented as sets of polygons representing
altitude contours at a given resolution and popula-
tion is a set of town boundary polygons. The spatial
reasoning module provides a high level interface be-
tween the spatial database and the rest of the system.
It is responsible for performing geographic character-
isation and providing spatial query functionality to
the rest of the system.
4 Text Generation
In Section 2 we outlined 3 main challenges that our
system must address. Our approach to the first,
analysis of the input data, is described in (Turner
et al, 2007). In the following Sections 4.1 and 4.2,
we describe the approach taken by our text generator
to the former two: content selection and generating
spatial references.
4.1 Content Selection
The input to the document planning module of our
system is a series of meteorological events (such as
rises in temperature) describing each parameter over
specific periods of time and locations. The basic
events are generated by data analysis which are then
abstracted into higher level concepts by data inter-
pretation. As it is impossible to include all these
events in such a short summary our system also gen-
erates a table as well as text shown in Figure 5.
In our KA studies we have found experts use
a qualitative overview of weather conditions when
writing forecasts to perform this task, confirming
similar observations reported in (Sripada et al,
2001). We take the same approach as experts in
our system by including the internal information
of the table (generated by the data analysis mod-
ule) as input to document planning. This serves as
the overview for content selection and allows con-
struction of an initial document plan consisting of
overview event leaf nodes. An example of this struc-
ture for the system output shown in Figure 5 is given
in Figure 6. Each overview event corresponds to a
column (or columns in the case of snow and rain) in
the table if the column indicates a significant thresh-
old for the parameter it describes (i.e. yes for ice).
Figure 6: Overview event tree for the text output in Fig-
ure 5
19
Figure 5: Example system output with text and partial table
The next stage is to construct messages from the
leaf nodes of the document plan. This is done in a
top down fashion by further annotating the tree with
events from the input list. Additional events are se-
lected by using the information from the overview
events to retrieve them from the list. This has the
benefit of keeping the content of both text and ta-
ble consistent. The final tree comprises the input
to the microplanner where messages are realised as
sentences in the final text and typically contain two
events per message (as observed in our corpus). For
example the overview event describing Precip in Fig-
ure 6 is realised as two sentences in Figure 5: Win-
try precipitation will affect most routes throughout
the forecast period at first [overview event], falling
as snow flurries in some places above 300M at first
[event]. Snow spreading throughout the forecast pe-
riod to all areas [event] and persisting in some places
above 300M until end of period [event].
4.2 Generating Spatial References to
Geographic Areas
Approaches to REG to date have concentrated
on distinguishing descriptions (e.g. (Gatt and
van Deemter, 2007),(van Deemter, 2006),(Horacek,
2006),(Krahmer et al, 2003),(Dale and Reiter,
1995); more specifically that is given a domain, they
look to generate a description of a target object that
uniquely distinguishes it from all other objects within
that domain. In a large geographic environment
such as a road network consisting of 1000?s of points,
where the task is to refer to an event occurring at a
small subset of those points, it is impractical (gen-
erated descriptions may be long and complex) and
prohibitively expensive (large numbers of spatial re-
lations between objects may have to be computed) to
take this approach. A more practical approach is to
generate spatial descriptions in terms of regions that
are not strictly distinguishing (i.e. urban areas, high
ground) rather than in terms of the points contained
within that region. Indeed, this is the strategy em-
ployed by human authors in our corpus. Therefore,
in a description such as ?road surface temperatures
will fall below zero in some places in the south west?,
distractors can be defined as the set of points within
the south western boundary that do not satisfy this
premise.
The relaxation of the requirement to generate a
distinguishing description simplifies the REG task in
this context as a single referring expression may be
deemed acceptable to refer to a wide range of situa-
tions. For example, ?in some places in the south west?
could be used to refer to a large number of possible
subsets of points that fall within the south western
boundary of the network. A simple REG strategy
is to find the set of properties to use in a descrip-
tion that introduce the least number of distractors.
However, as mentioned previously in Section 3.1, an
added constraint is that a spatial description should
use an appropriate frame of reference in the context
of the event it is describing. For example, describing
a change in precipitation type using population as
20
a frame of reference (i.e ?rain turning snow in some
rural places?) is not a sound meteorological inference
because population density does not affect precipi-
tation. This could cause a reader to infer false im-
plicatures (Grice, 1975), and consequently lead to
unnecessary treatment of part of the road network
so should be avoided. To account for this, following
(Dale and Reiter, 1995) we include a preference set
of reference frames for each type of event that must
be described. Absence from the set signifies that the
specified frame of reference should not be used in
that context.
Recall from Section 3.2 that properties in this case
relate directly to sets of boundary objects within a
frame of reference. Our content selection module
takes as input a series of individual proportions de-
scribing the spatial distribution of each parameter
within each frame of reference at a particular time
point. A score is calculated for each set of properties
by averaging over the sum of proportions for each
frame of reference. An appropriate frame of refer-
ence is then selected by choosing the one with the
highest score from the preference set for the given
event. An example1 of the input for the generated
description ?falling as snow flurries in some places
above 300M at first? in Figure 5 is shown in Figure
7.
5 Evaluation
The system presented in this paper is in its first
incarnation, RoadSafe is still actively under devel-
opment in preparation for a full scale user evalua-
tion. We have been evaluating the quality of the
output of the current system using post edit tech-
niques and feedback from expert meteorologists at
AMI. Our prototype has been installed at AMI since
the start of the year and is being used to generate
draft road ice forecasts for one of their local council
clients. One forecast is generated per day which is
then post-edited by an on duty forecaster before it is
sent to the client. While common in Machine Trans-
lation post-edit evaluations are still relatively rare in
NLG. The only large scale post-edit evaluation of an
NLG system to our knowledge has been reported in
(Sripada et al, 2005).
Our current evaluation is small in comparison to
that evaluation; SumTime-Mousam, the system be-
ing evaluated in that work was generating 150 draft
forecasts per day. However, it does try to address
some of the problems the authors encountered during
that evaluation. The main issue outlined by (Sripada
1N.B. this example is taken from route network that is
land locked and therefore coastal proximity is not taken into
account in this case.
Parameter: Snow
Class: Flurries
Time point: 12:00 {
Reference Frame Boundary Proportion
Altitude
0m: 0.0
100m: 0.0
200m: 0.0
300m: 0.07
400m: 1.0
500m: 1.0
Direction
CentralNE: 0.0
CentralNW: 0.0
CentralSE: 0.0
CentralSW: 0.0
EastNorthEast: 0.0
EastSouthEast: 0.0
SouthSouthEast: 0.0
SouthSouthWest: 0.18
TrueNorthEast: 0.0
TrueSouthEast: 0.0
TrueSouthWest: 0.56
WestSouthWest: 0.23
Population
Rural: 0.02
Urban: 0.0
}
Figure 7: Example input to content selection for REG.
Proportions are number of points affected by snow within
given boundary at the specified time point. Scores by
Reference Frame: Altitude = 0.35, Direction = 0.07, Pop-
ulation = 0.01
et al, 2005) was that their analysis was post-hoc and
therefore not supported by authors or by an editing
tool, which made it difficult to analyse why post-edits
were made. We have accounted for this by including
post-editing as part of our development process and
making use of a simple online interface that allows
the editor to select check boxes as they edit and in-
sert any general comments they may have. Check
boxes record edit reasons at a high level, for exam-
ple content, sentence order, spatial description used
etc. This is because it is not reasonable to expect a
time-constrained forecaster to spend time recording
every edit he makes.
Another important lesson pointed out by (Sripada
et al, 2005) is the need for a pilot study to analyse
the post-edit behaviour of individual authors to ac-
count for noisy data. This is certainly worthwhile,
but is difficult to carry out in our domain where fore-
casters work in variable shift patterns and on vari-
able forecasting tasks at different times. Instead, we
21
have used feedback forms as a way to gain qualitative
data on both the general quality of the texts and the
post-editing process. We present our results in Sec-
tion 5.1. In Section 5.2 we provide some discussion
of the results and describe future work.
5.1 Results
Our post-edit corpus currently consists of 112 texts,
2 texts(1 generated,1 edited) for 56 forecast days.
Of the 56 generated texts 54 have been edited before
being released to the user. As a general evaluation
criterion, our generated texts are generally too long
with a mean word length of 72 (standard deviation
of 21) compared to a mean word length of 53 (stan-
dard deviation of 17). The mean word count differ-
ence per forecast is 21 (standard deviation of 15). In
general analysis of the corpus is difficult, as in some
cases (18) texts have been basically rewritten. This
is not reflecting the quality of the text as such, but
the fact that the author has access to other informa-
tion sources such as satellite maps, which can lead
him to draw different inferences to those in the raw
model data available to the system. Furthermore,
(Hopwood, 2004) acknowledge as ice prediction mod-
els have become increasingly advanced, the primary
added value provided by weather forecasters is to
function as quality control and error mitigation for
the model, using their insight and experience to make
amendments particularly on marginal nights (where
the road surface temperature may or may not fall be-
low zero). Such cases can only be considered as noise
for analysis purposes, and the fact that our system
cannot account for this without the additional infor-
mation has been acknowledged by all forecasters in
their editing comments and feedback forms.
Focusing on 74 real post-edits (not attributed to
model data) recorded in our corpus, they can be clas-
sified into the following broad error categories: con-
tent edits - 65% and microplanning edits 35%. One
major problem we have identified with the current
generated text is the way in which overview events
described in 4.1 are realised. Deletions of whole sen-
tences describing overview events such as the one
highlighted in bold in Figure 8 constitute over half
(52%) of content edits, which may help to explain
the large descrepency in word counts. Essentially
forecasters believe they can often communicate sim-
ilar information as subsequent statements about the
same parameter making the texts repetitive at times.
Therefore they suggest they should either be omit-
ted or be realised as more interpretative statements,
such as ?A marginal night for most routes? for the
omitted statement in Figure 8. Forecasters also of-
ten delete subsequent statements following overview
Generated Text:
?Road surface temperatures will reach
near critical levels on some routes from the
late evening until tomorrow morning. Rain
will affect all routes during the afternoon and
evening. Road surface temperatures will fall slowly
during the mid afternoon and evening, reaching
near critical levels in areas above 500M by 21:00.?
Post-edited Text:
?Rain will affect all routes during the after-
noon and evening. Road surface temperatures will
fall slowly during the mid afternoon and evening,
reaching near critical levels in areas above 500M by
21:00.?
Figure 8: Content selection post-edit example (road
surface temperature overview information removed)
Generated Text:
?Road surface temperatures will reach near
critical levels on some routes after midnight until
tomorrow morning. Rain will affect all routes
throughout the forecast period, falling as snow
in some places above 500M by 08:00. Snow
clearing by 08:00. Road surface temperatures
will fall slowly during the late evening and tonight,
reaching near critical levels in areas above 500M by
03:00.?
Post-edited Text:
?Road surface temperatures will reach near
critical levels on some routes after midnight until
tomorrow morning. Rain will affect all routes
during the forecast period, this may fall as sleet
later on highest ground before dying out.
Road surface temperatures will fall slowly during
the late evening and tonight, reaching near critical
levels in areas above 500M by 03:00.?
Figure 9: Microplanning post-edit example (lexicalisa-
tion and aggregation)
sentences when they describe an event (such as rain
turning heavy) occuring only at a small number of
locations. So the spatial extent of an event and not
only its meteorological importance should be con-
sidered during content selection. RoadSafe does not
currently include much domain reasoning at the doc-
ument planning level to be able to do this.
22
Microplanning edits, as highlighted in bold in Fig-
ure 9, are due to individual lexical choice or aggrega-
tion issues. In all questionnaires experts have com-
mented that the generated texts are grammatically
sound but could flow better. Aggregation is done
in a fairly basic fashion in our system at present as
is lexicalisation. There have been no edits to the
frame of reference used in the generated spatial de-
scriptions, which we have taken as indication that
our REG strategy works well.
5.2 Discussion
The general feedback to our system has been encour-
aging. In terms of the exploitability of the system in
its current form it has received mixed reviews from
4 forecasters: 1 forecaster rated the system as good
for content and very poor on fluency; 1 rated it as
ok for both; 1 forecaster rated it as poor for content
and ok for fluency; 1 forecaster rated it as poor for
both. Generally all forecasters believe the generated
texts should tell a more fluent story about weather
conditions with more causal linking between events.
In terms of the techniques and approach outlined in
this paper they have worked well, although as ac-
knowledged in the previous section more sophisti-
cated domain reasoning and aggregation techniques
are required if the text is to function as a concise
summary, and indeed reach the standard of human
authored texts.
Making the required improvements highlighted in
the previous section is the focus of current work. Af-
ter these improvements have been made we plan to
carry out an evaluation with users of the forecasts.
We hope to also extend the functionality of the sys-
tem by generating individual route forecasts, which
can be accessed interactively through the table.
6 Conclusions
We have presented an approach to generating ge-
ographically grounded summaries of georeferenced
data using spatial reference frames. This approach
has been implemented in a data-to-text system for
generating road ice forecasts. An important task in
summarising georeferenced data is to describe the
data in terms of its underlying geography it refer-
ences. This presents an interesting challenge for con-
ventional REG approaches as finding a distinguish-
ing description for large numbers of objects in geo-
graphic space is not practical. We have found char-
acterising the geography in terms of spatial reference
frames provides a good solution as it provides a flex-
ible representation to describe set of objects in terms
of geographic areas.
We have also implemented a simple top down
content selection approach based on the idea of
overview, taken from how we have observed ex-
perts commonly performing the summarisation task.
While this approach works well for content selection,
a post-edit evaluation with experts has highlighted
that realising the overview in the text can make texts
verbose and have the effect of making subsequent
statements describing related events in the discourse
sound repetitive. This is important as experts re-
quire a short concise summary of weather conditions.
Acknowledgments
Many thanks to our collaborators at Aerospace and
Marine International UK, especially Keith Thom-
son and the other Meteorologists, for their helpful
feedback and comments. The RoadSafe project is
supported jointly by Aerospace and Marine Inter-
national UK, and the UK Engineering and Physical
Sciences Research Council (EPSRC), under a CASE
PhD studentship.
References
J. Coch. 1998. Multimeteo: multilingual production of
weather forecasts. ELRA Newsletter, 3(2).
K. R. Coventry and S. C. Garrod. 2004. Saying, Seeing
and Acting: The Psychological Semantics of Spatial
Prepositions. Psychology Press.
R. Dale and E. Reiter. 1995. Computational interpreta-
tions of the gricean maxims in the generation of refer-
ring expressions. Cognitive Science, 19:233?263.
R Dale, S Geldof, and J-P Prost. 2005. Using natu-
ral language generation in automatic route description.
Journal of Research and Practice in Information Tech-
nology, 37(1):89?105.
A. Gatt and K. van Deemter. 2007. Lexical choice and
conceptual perspective in the generation of plural re-
ferring expressions. Journal of Logic, Language and
Information, 16:423?443.
E. Goldberg, N. Driedger, and R. Kittredge. 1994. Using
natural-language processing to produce weather fore-
casts. IEEE Expert, 9(2):45?53.
H. Grice. 1975. Logic and conversation. In P. Cole and
J. Morgan, editors, Syntax and Semantics, volume 3,
Speech Acts, pages 43?58. Academic Press: New York.
C. Hallett and D. Scott. 2005. Structural variation in
generated health reports. In Proceedings of the 3rd
International Workshop on Paraphrasing (IWP2005),
pages 33?40, Jeju Island, Republic of Korea.
Philip Hopwood. 2004. Improvements in road forecasting
techniques & their applications. In 12th International
Road Weather Conference, Bingen, Germany.
Helmut Horacek. 2006. Generating references to parts
of recursively structured objects. In Proceedings of
the 4th International Conference on Natural Language
Generation, pages 47?54.
23
Lidija Iordanskaja, Richard Kittredge, Benoit Lavoie,
and Alain Polgue`re. 1992. Generation of extended
bilingual statistical reports. COLING-92, pages 1019?
1023.
Emiel Krahmer, Sebastiaan van Erk, and Andr Verleg.
2003. Graph-based generation of referring expressions.
Computational Linguistics, 29(1):53?72.
Stephen C. Levinson. 2003. Space in language and cog-
nition: explorations in cognitive diversity. Cambridge
University Press, Cambridge.
Harvey J. Miller and Jiawei Han. 2001. Geographic data
mining and knowledge discovery: An overview. In Ge-
ographic Data Mining and Knowledge Discovery, chap-
ter 1, pages 1?32. Taylor & Francis.
F. Portet, E. Reiter, J. Hunter, and S. Sripada. 2007.
Automatic generation of textual summaries from
neonatal intensive care data. In 11th Conference on
Artificial Intelligence in Medicine (AIME 07), pages
227?236.
E. Reiter, S. Sripada, J. Hunter, J. Yu, and I. Davy.
2005. Choosing words in computer-generated weather
forecasts. In Artificial Intelligence, volume 67, pages
137?169.
E. Reiter. 2007. An architecture for data-to-text sys-
tems. In ENLG07, pages 97?104.
D. Ro?sner. 1987. The automated news agency: Sem-
tex: A text generator for german. In Natural Lan-
guage Generation: New Results in Artificial Intelli-
gence, Psychology, and Linguistics. Nijhoff.
D. Roy and E. Reiter. 2005. Connecting language to the
world. Artificial Intelligence, 167:1?12.
S. Sripada, E. Reiter, J. Hunter, and Jin Yu. 2001.
A two-stage model for content determination. In
ENLG2001, pages 3?10.
S Sripada, E Reiter, and L Hawizy. 2005. Evaluation of
an nlg system using post-edit data: Lessons learnt. In
10th European Workshop on Natural Language Gener-
ation.
R. Turner, S. Sripada, E. Reiter, and I. Davy. 2007. Se-
lecting the content of textual descriptions of geograph-
ically located events in spatio-temporal weather data.
In Applications and Innovations in Intelligent Systems
XV, pages 75?88.
R. Turner, S. Sripada, E. Reiter, and I. Davy. 2008.
Building a parallel spatio-temporal data-text cor-
pus for summary generation. In Proceedings of the
LREC2008 Workshop on Methodologies and Resources
for Processing Spatial Language, Marrakech, Morocco.
K van Deemter. 2006. Generating referring expressions
that involve gradable properties. Computational Lin-
guistics, 32:195?222.
24
INLG 2012 Proceedings of the 7th International Natural Language Generation Conference, pages 17?21,
Utica, May 2012. c?2012 Association for Computational Linguistics
MinkApp: Generating Spatio-temporal Summaries for Nature Conservation
Volunteers
Nava Tintarev, Yolanda Melero, Somayajulu Sripada,
Elizabeth Tait, Rene Van Der Wal, Chris Mellish
University of Aberdeen
{n.tintarev, y.melero, yaji.sripada,
elizbeth.tait, r.vanderwal, c.mellish@abdn.ac.uk}@abdn.ac.uk
Abstract
We describe preliminary work on generat-
ing contextualized text for nature conservation
volunteers. This Natural Language Genera-
tion (NLG) differs from other ways of describ-
ing spatio-temporal data, in that it deals with
abstractions on data across large geographi-
cal spaces (total projected area 20,600 km2),
as well as temporal trends across longer time
frames (ranging from one week up to a year).
We identify challenges at all stages of the clas-
sical NLG pipeline.
1 Introduction
We describe preliminary work on summarizing
spatio-temporal data, with the aim to generate con-
textualized feedback for wildlife management vol-
unteers. The MinkApp project assesses the use
of NLG to assist volunteers working on the Scot-
tish Mink Initiative (SMI). This participatory initia-
tive aims to safeguard riverine species of economic
importance (e.g., salmon and trout) and species of
nature conservation interest including water voles,
ground nesting birds and other species that are ac-
tively preyed upon by an invasive non-native species
- the American mink (Bryce et al, 2011).
2 Background
Our test ground is one of the world?s largest
community-based invasive species management
programs, which uses volunteers to detect, and sub-
sequently remove, American mink from an area of
Scotland set to grow from 10,000 km2 in 2010 to
20,600 km2 by the end of 2013 (Bryce et al, 2011).
Such a geographical expansion means that an in-
creasing share of the monitoring and control work is
undertaken by volunteers supported by a fixed num-
ber of staff. An important contribution of volunteers
is to help collect data over a large spatial scale.
Involving members of the public in projects such
as this can play a crucial role in collecting observa-
tional data (Silvertown, 2009). High profile exam-
ples of data-gathering programmes, labelled as cit-
izen science, include Galaxy Zoo and Springwatch
(Raddick et al, Published online 2010; Underwood
et al, 2008). However, in such long-term and wide
ranging initiatives, maintaining volunteer engage-
ment can be challenging and volunteers must get
feedback on their contributions to remain motivated
to participate (Silvertown, 2009). NLG may serve
the function of supplying this feedback.
3 Related work
We are particularly interested in summarizing raw
geographical and temporal data whose semantics
need to be computed at run time ? so called spatio-
temporal NLG. Such extended techniques are stud-
ied in data-to-text NLG (Molina and Stent, 2010;
Portet et al, 2009; Reiter et al, 2005; Turner et
al., 2008; Thomas et al, Published online 2010).
Generating text from spatio-temporal data involves
not just finding data abstractions, but also determin-
ing appropriate descriptors for them (Turner et al,
2008). Turner et. al (2008) present a case study in
weather forecast generation where selection of spa-
tial descriptors is partly based on domain specific
(weather related) links between spatial descriptors
17
and weather phenomena. In the current project we
see an opportunity to investigate such domain spe-
cific constraints in the selection of descriptors over
larger temporal and spatial scales.
4 Current Status
Over 600 volunteers currently notify volunteer man-
agers of their ongoing mink recording efforts. Our
work is informed by in-depth discussions and inter-
views with the volunteer managers, as well as 58
(ground level) volunteers? responses to a question-
naire about their volunteering experience. The set of
volunteers involves different people, such as conser-
vation professionals, rangers, landowners and farm-
ers with the degree of volunteer involvement varying
among them. Most volunteers check for sightings:
footprints on a floating platform with a clay-based
tracking plate (raft hereafter) readily used by mink,
or visual sightings on land or water. Others set and
check traps, and (much fewer volunteers) dispatch
trapped mink.1 In terms of feedback, volunteers cur-
rently receive regional quarterly newsletters, but tai-
lored and contextualized feedback is limited to spo-
radic personal communication, mostly via email.2
4.1 Why NLG in this context?
Where the initiative has been successful, mink sight-
ings are sparse. Such a lack of sightings can be de-
motivating for volunteers and leads to a situation in
which negative records are seldom recorded (Beirne,
2011). As one volunteer stated: ?Nothing much hap-
pens on my raft so my enthusiasm wanes.? Also,
73% of the volunteers who completed the ques-
tionnaire said they checked their raft at the recom-
mended frequency of every two weeks. Similarly,
72% said that they got in touch with their manager
rarely or only every couple of months ? when they
needed more clay or saw footprints. NLG based
feedback could motivate volunteers by informing
them about the value of negative records. If they
were to stop because of a lack of interest, mink are
likely to reinvade the area.
1Traps are only placed once a sighting has occurred. Once
placed, by law a trap must be checked daily.
2In this project, we are using a corpus based on newsletters
from the North Scotland Mink Project and the Cairngorms Wa-
ter Vole Conversation Project.
In addition, volunteers who work alone can be
isolated and lack natural mechanisms for informa-
tion exchange with peers. We postulate that giving
the volunteers contextualized feedback for an area
gives them a better feeling for their contribution to
the project and a better sense of how the initiative is
going overall. A need for this has already been felt
by volunteers: ?Knowing even more about progress
in the catchment would be good - and knowing in de-
tail about water vole returning and latest mink sight-
ings. It would be helpful to learn about other neigh-
boring volunteers captures sightings in ?real time?.?
5 Approach
In this section we describe the generation of text in
terms of a classic NLG pipeline, (Reiter and Dale,
2000), while addressing the additional tasks of in-
terpreting the input data (from volunteers) to mean-
ingful messages that achieve the desired communi-
cation goals: providing information to, as well as
motivating volunteers. The NLG system which will
generate these texts is actively under development.
5.1 Gold standard
Our nearest comparison is a corpus of domain spe-
cific conservation newsletters containing text such
as the one below. These newsletters give us an idea
of the type of structure and lexical choice applied
when addressing volunteers, using both temporal
and spatial summaries. However, these texts are not
contextualized, or adapted to a particular volunteer.
?With an ever expanding project area, we
are progressing exceptionally well achiev-
ing and maintaining areas free of breed-
ing mink through-out the North of Scot-
land. Currently, the upper Spey, upper
Dee and Ythan appear to be free of breed-
ing mink, with only a few transients pass-
ing through...?
We would like to improve on these existing texts
and aim to generate texts that are tailored and con-
sider the context of the volunteer. The text below is
developed from a template supplied from a volunteer
manager in the process of corpus collection. In the
following sections we describe the steps and chal-
lenges involved in the process of generating such a
text.
18
?Thank you for your helpful contribution!
You may have not seen any signs this time,
but in the last week two people in the Spey
catchment have seen footprints on their
rafts. This means there might be a female
with a litter in your neighborhood ? please
be on the lookout in the coming weeks!
Capturing her could mean removing up to
6 mink at once!?
5.2 Example input
The data we receive from volunteers includes pos-
itive and negative records from raft checks (every
14 days), visual sightings, and mink captures. Each
record contains a geographical reference (x and y co-
ordinate) and a timestamp. In addition, for trapped
mink we may know the sex (male, female, or un-
known) and age (juvenile, adult, or unknown).
5.3 Data analysis and interpretation
Spatial trends. The current version of the system
can reason over geographical information, defin-
ing various notions of neighborhood.3 For a given
point the following attributes can be used to describe
its neighborhood: geographical region (catchment
and subcatchment), Euclidean distance from another
point, and relative cardinal direction to another point
(north, south, east, west). The system reasons about
sightings and captures using facts such as:
? This point (on land or water) is in the Dee
catchment.
? Three neighbors have seen footprints (within a
given time window).
? One neighbor has caught a mink (within a given
time window).
? The nearest mink footprint is 15 km north east
of this point.
The definition of neighborhood will differ accord-
ing to domain specific factors. Euclidean distance
appears to be the most likely candidate for use, be-
cause sightings may belong to different geographic
3The reasoning is performed using the opensource GIS
Java library Geotools, http://geotools.org, retrieved
Jan 2012
regions (catchments) but be very close to each other.
More importantly, the definition of neighborhood is
likely to depend on the geographic region (e.g. ar-
eas differ in terms of mink population density with
mountainous regions less likely to be utilized than
coastal regions).
Temporal trends. Aside from geographic trends,
the system will also be used to portray temporal
trends. These look at the change in sightings be-
tween two time intervals, identifying it as a falling,
rising or steady trend in mink numbers. We are
primarily observing trends between different years,
but also taking into consideration the ecology of the
mink including their behavior in different seasons
and for quantification. For example, we need to be
able to decide if an increase from 0 to 5 mink sight-
ings in an area during breeding is worth mentioning
? most likely it is, as this a common size for a litter.
Another example is the definition of a ?cleared? area
- Example 1 below describes a stable zero trend over
a longer period of time.
...Currently, the upper Spey, upper Dee and Ythan
appear to be free of breeding mink...
(1)
5.4 Document planning
Content determination While useful on its own,
the text that could be generated from the data analy-
sis and interpretation described above is much more
useful when domain specific rules are applied. Ex-
ample 2 describes a significant year-on-year increase
for a given definition of neighborhood, during breed-
ing season.
IF ( (month >= 6 AND month <9)
AND sightingsLastYear(area) == 0
AND sightingsThisYear >= 5 )
THEN feedback +=
?It looks like the area has been reinvaded.
We should get ready to trap them to keep this
area mink free.?
(2)
Example rule 2 is applied in the breeding season (ca
June-Aug.). It will be given a score which signi-
fies its relative importance compared to other de-
rived content to allow prioritization. For example,
19
if there are both female and male captures in a re-
gion, it would be more important to speak about the
female capture. This is because the capture of breed-
ing mink has a much larger positive impact on the
success of the initiative.4 This importance should
be reflected in texts such as: ...Capturing her could
mean removing up to 6 mink at once!...
Document structuring Since our goal is to moti-
vate as well as inform, the structure of the text will
be affected. If we consider the example text in Sec-
tion 5.1, we can roughly divide it into three summary
types:
? Personal - ?Thank you for your helpful contri-
bution! You may have not seen any signs this
time.?
? Neighbor - ?In the last week two people in the
Spey catchment have seen small footprints on
their rafts.?
? Biology - ?There might be a female with a litter
in your neighborhood ... Capturing her could
mean removing up to 6 mink at once!?
If, in contrast to the previous example, a volun-
teer would capture a mink, then the neighborhood
summary can be used to emphasize the importance
of rare captures.
?IF currentMonth == August AND
capture == true AND nCapturesInSummer == 0?
(3)
The feedback for rule 3 might read something
like: ?Well done! So far, this was the only mink cap-
tured during the breeding season in the Spey catch-
ment!?
5.5 Microplanning
Microplanning will need to consider the aggrega-
tion of spatio-temporal data that happens on a deeper
level e.g., for a given catchment and year. This ag-
gregation is likely to result in a surface aggregation
as well deeper data aggregation, such as the catch-
ments in Example 1. In terms of lexical choice, the
system will have to use domain appropriate vocabu-
lary. The latter example refers to ?breeding mink?,
4Established adult females with litters.
which informs the reader that their capture has a
large impact on population control. Another exam-
ple of lexical choice may be ?quieter autumn? to de-
note a decrease in mink for an area.
The best way to communicate neighborhood to
volunteers is still an open question. The texts in
our corpus describe neighborhoods in terms of geo-
graphic regions (catchments and subcatchments, e.g.
Spey). However, Euclidean distance may be more
informative, in particular close to catchment bound-
aries.
6 Challenges
There are several key challenges when generating
motivating text for nature conservation volunteers,
using spatio-temporal NLG.
One challenge is to tailor feedback texts to in-
dividuals according to their motivations and infor-
mation needs. In line with previous research in
affective NLG (de Rosis and Grasso, 2000; Belz,
2003; Sluis and Mellish, 2010; Tintarev and Mas-
thoff, 2012; Mahamood and Reiter, 2011), we con-
tinue to study the factors which are likely to have
an effect on volunteer motivation. So far we have
worked together with volunteer managers. We col-
lected a corpus of texts, written by the managers,
that are tailored to motivate different volunteer per-
sonas, and conducted interviews and a focus group
with them. While we found that the mink managers
tailored texts to different personas, interviews indi-
cated that the biggest factor to tailor for was the def-
inition of neighborhood. Some volunteers are inter-
ested in a local update, while others are interested in
a larger overview.
A second, related challenge, regards correctly
defining the reasoning over spatio-temporal facts
e.g., quantifying the magnitude of significant
changes (increases and decreases in sightings and
captures) for different seasons, regions, and the time
frames over which they occur. We believe this will
lead to generating text referring to more compound
abstractions such as mink free areas, or re-invasion.
A final challenge brought out by the interviews
is to supply varied feedback that helps volunteers to
continue to learn about mink and their habitat. This
is a challenge for both content determination and mi-
croplanning.
20
References
Christopher Beirne. 2011. Novel use of mark-recapture
framework to study volunteer retention probabilities
within an invasive non-native species management
project reveals vocational and temporal trends. Mas-
ter?s thesis, University of Aberdeen.
Anja Belz. 2003. And now with feeling: Developments
in emotional language generation. Technical Report
ITRI-03-21, Information Technology Research Insti-
tute, University of Brighton.
Rosalind Bryce, Matthew K. Oliver, Llinos Davies, He-
len Gray, Jamie Urquhart, and Xavier Lambin. 2011.
Turning back the tide of american mink invasion at an
unprecedented scale through community participation
and adaptive management. Biological Conservation,
144:575?583.
Fiorella de Rosis and Floriana Grasso, 2000. Affective
Interactions, volume 1814 of Lecture Notes in Artifi-
cial Intelligence, chapter Affective Natural Language
Generation. Springer-Verlag.
Saad Mahamood and Ehud Reiter. 2011. Generating af-
fective natural language for parents of neonatal infants.
In ENLG.
Martin Molina and Amanda Stent. 2010. A knowledge-
based method for generating summaries of spatial
movement in geographic areas. International Journal
on Artificial Intelligence Tools, 19(3):393?415.
Francois Portet, Ehud Reiter, Albert Gatt, Jim Hunter,
Somayajulu Sripada, Yvonne Freer, and Cindy Sykes.
2009. Automatic generation of textual summaries
from neonatal intensive care data. Artificial Intelli-
gence, 173:789?816.
M. Jordan Raddick, Georgia Bracey, Pamela L. Gay,
Chris J. Lintott, Phil Murray, Kevin Schawinski,
Alexander S. Szalay, and Jan Vandenberg. Published
online 2010. Galaxy zoo: Exploring the motivations
of citizen science volunteers. Astronomy Education
Review, 9(1), 010103, doi:10.3847/AER2009036.
Ehud Reiter and Robert Dale. 2000. Building natural
language generation systems. Cambridge University
Press.
Ehud Reiter, Somayajulu Sripada, Jim Hunter, Jin Yu,
and Ian Davy. 2005. Choosing words in computer-
generated weather forecasts. Artificial Intelligence,
167:137?169.
Jonathan Silvertown. 2009. A new dawn for citizen sci-
ence. Trends in Ecology & Evolution, 24:467?471.
Ielka Van der Sluis and Chris Mellish, 2010. Empiri-
cal Methods in Natural Language Generation, volume
5980 of Lecture Notes in Computer Science, chap-
ter Towards Empirical Evaluation of Affective Tactical
NLG. Springer, Berlin / Heidelberg.
Kavita E. Thomas, Somayajulu Sripada, and Matthijs L.
Noordzij. Published online 2010. Atlas.txt: Ex-
ploring linguistic grounding techniques for commu-
nicating spatial information to blind users. Journal
of Universal Access in the Information Society, DOI
10.1007/s10209-010-0217-5.
Nava Tintarev and Judith Masthoff. 2012. Evaluating
the effectiveness of explanations for recommender sys-
tems: Methodological issues and empirical studies on
the impact of personalization. User Modeling and
User-Adapted Interaction, (to appear).
Ross Turner, Somayajulu Sripada, Ehud Reiter, and Ian
Davy. 2008. Using spatial reference frames to gener-
ate grounded textual summaries of georeferenced data.
In INLG.
Joshua Underwood, Hilary Smith, Rosemary Luckin, and
Geraldine Fitzpatrick. 2008. E-science in the class-
room towards viability. Computers & Education,
50:535?546.
21
Proceedings of the 8th International Natural Language Generation Conference, pages 1?5,
Philadelphia, Pennsylvania, 19-21 June 2014. c?2014 Association for Computational Linguistics
A Case Study: NLG meeting Weather Industry Demand for Quality 
and Quantity of Textual Weather Forecasts 
Somayajulu G Sripada, Neil Burnett and 
Ross Turner 
Arria NLG Plc 
{yaji.sripada,neil.burnett, 
ross.turner}@arria.com 
 John Mastin and Dave Evans 
Met Office 
{john.mastin, 
dave.evans}@metoffice.gov.uk 
  
 
  
 
Abstract 
In the highly competitive weather indus-
try, demand for timely, accurate and per-
sonalized weather reports is always on 
the rise. In this paper we present a case 
study where Arria NLG and the UK na-
tional weather agency, the Met Office 
came together to test the hypothesis that 
NLG can meet the quality and quantity 
demands of a real-world use case. 
1 Introduction 
Modern weather reports present weather predic-
tion information using tables, graphs, maps, 
icons and text. Among these different modalities 
only text is currently manually produced, con-
suming significant human resources. Therefore 
releasing meteorologists? time to add value else-
where in the production chain without sacrificing 
quality and consistency in weather reports is an 
important industry goal. In addition, in order to 
remain competitive, modern weather services 
need to provide weather reports for any geo-
location the end-user demands. As the quantity 
of required texts increases, manual production 
becomes humanly impossible. In this paper we 
describe a case study where data-to-text NLG 
techniques have been applied to a real-world use 
case involving the UK national weather service, 
the Met Office. In the UK, the Met Office pro-
vides daily weather reports for nearly 5000 loca-
tions which are available through its public web-
site. These reports contain a textual component 
that is not focused on the geo-location selected 
by the end-user, but instead describes the weath-
er conditions over a broader geographic region. 
This is done partly because the time taken to 
manually produce thousands of texts required 
would be in the order of weeks rather than 
minutes. In this case study a data-to-text NLG 
system was built to demonstrate that the site-
specific data could be enhanced with site-specific 
text for nearly 5000 locations. This system, run-
ning on a standard desktop, was tested to pro-
duce nearly 15000 texts (forecasts for 5000 loca-
tions for 3 days into the future) in less than a mi-
nute. After internally assessing the quality of 
machine-generated texts for nearly two years, the 
Met Office launched the system on their beta site 
(http://www.metoffice.gov.uk/public/weather/for
ecast-data2text/) in December 2013 for external 
assessment. A screenshot of the forecast for 
London Heathrow on 5th March 2014 is shown 
in Figure 1. In this figure, the machine-generated 
text is at the top of the table. Ongoing work has 
extended the processing capabilities of this sys-
tem to handle double the number of locations and 
an additional two forecast days. It has been 
found that the processing time scales linearly. 
2 Related Work 
Automatically producing textual weather fore-
casts has been the second favorite application for 
NLG, with 15 entries on Bateman and Zock?s list 
of NLG application domains (the domain of 
medicine comes on top with 19 entries) [Bate-
man and Zock, 2012]. NLG applications in the 
weather domain have a long history. FOG was an 
early landmark NLG system in the domain of 
weather reports [Goldberg et al, 1994]. Working 
as a module of the Forecast Production Assistant 
(FPA), FOG was operationally deployed at Envi-
ronment Canada to produce weather reports for 
the general public and also for marine users in 
both English and French. Using sampling and 
smoothing over space and time, FOG reduces 
raw data into a few significant events which are 
then organized and realized in textual form. 
MULTIMETEO is another industry deployed mul-
ti-lingual weather report generator [Coch 1998].  
The focus of MULTIMETEO is ?interactive genera-
tion via knowledge administration?.
1
Figure 1. Screenshot of Text-Enhanced Five-day Weather Forecast for London Heathrow on 5 March 2014 
showing only part of the data table 
 
Expert forecasters post-edit texts (interactivity) 
in their native language and this knowledge is 
then reused (knowledge administration) for au-
tomatically generating texts in other languages. It 
is claimed that such interactive generation is bet-
ter than using machine translation for multi-
lingual outputs. SUMTIME-MOUSAM is yet anoth-
er significant weather report generator that was 
operationally deployed to generate forecasts in 
English for oil company staff supporting oil rig 
operations in the North Sea [Sripada et al, 
2003a]. Adapting techniques used for time series 
segmentation, this project developed a frame-
work for data summarization in the context of 
NLG [Sripada et al, 2003b]. This time series 
summarization framework was later extended to 
summarizing spatio-temporal data in the ROAD-
SAFE system [Turner et al, 2008]. ROADSAFE too 
was used in an industrial context to produce 
weather reports (including text in English and a 
table) for road maintenance in winter months. 
The NLG system reported in the current case 
study builds upon techniques employed by earli-
er systems, particularly SUMTIME-MOUSAM and 
ROADSAFE.  
The main dimension on which the applica-
tion described in this paper differs most from the 
work cited previously is the quantity of textual 
weather forecasts that are generated. Previous 
work has either focused on summarising forecast 
sites collectively (in the case of FOG and ROAD-
SAFE), been limited in the number of sites fore-
cast for (15 in the case of MULTIMETEO) or lim-
ited in geographic extent (SUMTIME-MOUSAM 
concentrated on oil rig operations in the North 
Sea). This aspect of the system, amongst others, 
posed a number of challenges discussed in Sec-
tion 3.      
3 System Description 
For reasons of commercial sensitivity, the system 
description in this section is presented at an ab-
stract level. At the architecture level, our system 
uses the Arria NLG Engine that follows the 
standard five stage data-to-text pipeline [Reiter, 
2007]. The system integrates application specific 
modules with the generic reusable modules from 
the underlying engine. Input to the system is 
made up of three components: 
 
1. Weather prediction data consisting of sev-
eral weather parameters such as tempera-
ture, wind speed and direction, precipita-
tion and visibility at three hourly intervals; 
2. Daily summary weather prediction data 
consisting of average daily and nightly 
values for several weather parameters as 
above; and 
3. Seasonal averages (lows, highs and mean) 
for temperature. 
 
Because the system is built on top of the Ar-
ria NLG Engine, input data is configurable 
and not tied to file formats. The system can be 
configured to work with new data files with 
equivalent weather parameters as well as dif-
ferent forecast periods. In other words, the 
system is portable in principle for other use 
cases where site-specific forecasts are required 
from similar input data.  
2
3.1 Expressing Falling Prediction Quality for 
Subsequent Forecast Days 
As stated above, the system can be configured to 
generate forecast texts for a number of days into 
the future. Because prediction accuracy reduces 
going into the future, the forecast text on day 1 
should be worded differently from subsequent 
days where the prediction is relatively more un-
certain. An example output text for day 1 is 
shown in Figure 2 while Figure 3 shows the day 
3 forecast. Note the use of ?expected? to denote 
the uncertainty around the timing of the tempera-
ture peak. 
 
Staying dry and predominantly clear with only a 
few cloudy intervals through the night. A mild 
night with temperatures of 6C. Light winds 
throughout. 
 
Figure 2. Example output text for day 1 
 
Cloudy through the day. Mainly clear into the 
night. Highest temperatures expected during the 
afternoon in the region of 12C with a low of 
around 6C during the night. Light to moderate 
winds throughout. 
 
Figure 3. Example output text for day 3 
3.2 Lack of Corpus for System Development 
A significant feature of the system development 
has been to work towards a target text specifica-
tion provided by experts rather than extract such 
a specification from corpus texts, as is generally 
the case with most NLG system development 
projects. This is because expert forecasters do 
not write the target texts regularly; therefore, 
there is no naturally occurring target corpus. 
However, because of the specialized nature of 
the weather sublanguage (Weatherese), which 
has been well studied in the NLG community 
[Goldberg et al, 1994, Reiter et al 2005, Reiter 
and Williams 2010], it was possible to supple-
ment text specifications obtained from experts. 
In addition, extensive quality assessment (details 
in section 3.4) helped us to refine the system 
output to the desired levels of quality. 
3.3 Achieving Output Quantity 
The main requirements of the case study have 
been 1) build a NLG capability that produces the 
quantity of texts required and 2) achieve this 
quantity without sacrificing the quality expected 
from the Met Office. As stated previously, the 
quantity requirement has been met by generating 
15,000 texts in less than a minute, without need 
for high end computing infrastructure or parallel-
ization. Figure 4 is a box plot showing character 
lengths of forecast texts for an arbitrary set of 
inputs. The median length is 177 characters. The 
outliers, with length 1.5 times the interquartile 
range (1.5 * 134 = 201 characters) above the up-
per quartile or below the lower quartile, relate to 
sites experiencing particularly varied weather 
conditions. Feedback on the appropriateness of 
the text lengths is discussed in Section 3.4. 
 
Figure 4. Boxplot of forecast character length 
 
 
Figure 5. System Processing Time 
 
   The system has recently been extended to gen-
erate 50,000 texts without loss of performance. 
This extension has doubled the number of sites 
processed to 10,000 and extended the forecast to 
5 days. It has also increased the geographic ex-
tent of the system from UK only to worldwide, 
discussed in Section 3.5. The plot in Figure 5 
shows the relationship between processing time 
3
and the addition of new forecast sites. The results 
were obtained over 10 trials using a MacBook 
Pro 2.5 GHz Intel Core i5, running OS X 10.8 
with 4GB of RAM.  
3.4 Achieving Output Quality 
Achieving the required text quality was driven 
by expert assessment of output texts that oc-
curred over a period of two years. This is be-
cause experts had to ensure that the system out-
put was assessed over the entire range of weather 
conditions related to seasonal variations over the 
course of a year. The following comment about 
the output quality made by a Met Office expert 
summarizes the internal feedback: 
 
"They were very, very good and I got lots of ver-
bal feedback to that affect from the audience af-
terwards. Looking back after the weekend, the 
forecasts proved to be correct too! I've been 
looking at them at other times and I think they're 
brilliant." 
 
After successfully assessing the output quality 
internally, the Met Office launched the system on 
the Invent part of their website to collect end-
user assessments. Invent is used by the Met Of-
fice to test new technology before introducing 
the technology into their workflows. With the 
help of a short questionnaire 1  that collects as-
sessment of those end-users that use weather in-
formation for decision-making, quality assess-
ment is ongoing. The questionnaire had three 
questions related to quality assessment shown in 
Figures 6-8. In the rest of the section we describe 
the results of this questionnaire based on 35 re-
sponses received between 1st January 2014 and 
6th March 2014.  
The first question shown in Figure 6 relates to 
assessing the usefulness of textual content in 
helping the end-user understand a weather report 
better. Out of the 35 respondents, 34 (97%) an-
swered ?yes? and 1 (3%) answered ?no? for the 
question in Figure 6. The second question shown 
in Figure 7 relates to assessing if the text size is 
optimal for this use case. Here, out of the 35 re-
spondents, 26 (74%) felt the text is ?about right? 
size, 7 (20%) felt it is either ?too short? or ?too 
long? and 2 (6%) were ?unsure?. The third ques-
tion shown in Figure 8 relates to finding out if 
the end-user might want a forecast that includes 
textual content. Here, 32 (91%) wanted textual 
content while 3 (9%) did not want it.  
                                                 
1
http://www.metoffice.gov.uk/invent/feedback 
The Met Office is currently evaluating the new 
capability based upon the feedback received and 
how it can be applied to meet the demands of 
users across their portfolio of products. 
Did you find the text on the weather forecast 
page helped you to understand the forecast bet-
ter? * 
Yes 
No 
Figure 6. Question about textual content help-
ing the end-user understand the forecast better 
How did you find the text used? * 
Too short 
About right 
Too long 
Unsure / don't know 
Figure 7. Question about length of the forecast 
text 
Would you recommend this feature? * 
Yes 
No 
Figure 8. Question about the end-user?s opin-
ion on textual content as part of a weather report 
 
The questionnaire also asked for free text 
comments. An example of one such comment is: 
 
"Succinct and clear text. Contains all the im-
portant features and is well presented. Saves us 
having to summarise the visual descriptions our-
selves (or rather helps to shape our conclusions 
about the 24 hour weather pattern)." 
    
   A big challenge during the development of 
such a system is providing quality assurance 
4
when generating such a large volume of texts. A 
number of automated checks had to be applied to 
the complete output during system testing as well 
as targeted sampling of input data to produce a 
representative sample of outputs for manual as-
sessment. 
3.5 Extending the Geographic Extent 
Extending the scope of the system from UK-only 
sites to handling worldwide locations brings sub-
tle challenges in addition to scaling the system, 
principally: 
 
1. handling time zone changes; and 
2. adapting to different climates. 
 
In the case of point 1 above, time descriptions 
can become ambiguous where the sunrise and 
sunset time vary across geographies. Such times 
need to be carefully observed to avoid generating 
words such as ?sunny? after dark. For point 2, 
general terminologies relating to description of 
temperatures cannot be universally applied 
across locations. For example, the meaning of 
terms such as ?cool? differs at locations within 
the tropics versus locations north (or south) of 45 
degrees of latitude. 
4 Conclusion 
We have presented a case study describing an 
application of NLG technology deployed at the 
Met Office. The system has been developed to 
meet the text production requirements for thou-
sands of forecast locations that could not have 
been sustainable with human resources. The 
software can write a detailed five-day weather 
forecast for 10,000 locations worldwide in under 
two minutes. It would take a weather forecaster 
months to create the equivalent quantity of out-
put.  
In meeting the requirements of this particular 
use case a number of challenges have had to be 
met. Principally, these challenges have been fo-
cused upon processing speed and output text 
quality. While we have managed to achieve the 
required processing performance relatively 
quickly without the need for large amounts of 
computing resources or high-end computing in-
frastructure, ensuring the necessary output quali-
ty has been a longer process due to the high op-
erating standards required and the high resource 
cost of quality assurance when delivering texts at 
such scale.    
This application of NLG technology to site-
specific weather forecasting has potential for a 
number of enhancements to the type of weather 
services that may be provided in the future, most 
notably the opportunity for very geographically 
localized textual forecasts that can be updated 
immediately as the underlying numerical weather 
prediction data is produced.   
References 
E. Goldberg, N. Driedger, and R. Kittredge. 
Using Natural-Language Processing to Produce 
Weather Forecasts. 
IEEE Expert, 9(2):45--53, 1994. 
J. Coch. Interactive generation and knowledge admin-
istration in MultiMeteo.  
In Proceedings of the Ninth International Work-
shop on Natural Language Generation, pages 300--
303, Niagara-on-the-lake, Ontario, Canada, 1998. 
software demonstration. 
Bateman J and Zock M, (2012) Bateman/Zock list of 
NLG systems, http://www.nlg-wiki.org/systems/. 
S. Sripada, E. Reiter, and I. Davy, (2003a) 
?SumTime-Mousam: Configurable Marine Weather 
Forecast Generator?, Expert Update, 6(3), pp 4-10, 
(2003)  
S. Sripada, E. Reiter, J. Hunter and J. Yu (2003b). 
Generating English Summaries of Time Series Da-
ta using the Gricean Maxims. In Proceedings of 
KDD 2003, pp 187-196. 
E. Reiter, S. Sripada, J. Hunter, J. Yu and Ian Davy 
(2005). Choosing Words in Computer-Generated 
Weather Forecasts. Artificial Intelligence. 167(1-
2):137-169 
E. Reiter (2007). An architecture for data-to-text sys-
tems, In ENLG 07, pp97-104. 
Reiter, Ehud and Williams, Sandra (2010). Generating 
texts in different styles. In: Argamon, Shlomo; 
Burns, Kevin and Dubnov, Shlomo eds. The Struc-
ture of Style: Algorithmic Approaches to Manner 
and Meaning. Heidelberg: Springer, pp. 59?78 
E. Reiter, S. Sripada, J. Hunter, J. Yu, and Ian Da-
vy(2005). Choosing words in computer-generated 
weather forecasts. Artificial Intelligence. 167(1-
2):137-169 (2005) 
R. Turner, S. Sripada, E. Reiter, & I. Davy  
(2008). Using spatial reference frames to generate 
grounded textual summaries of geo-referenced da-
ta. Proceedings of the INLG 2008, Salt Fork, Ohio. 
5
Proceedings of the 8th International Natural Language Generation Conference, pages 93?94,
Philadelphia, Pennsylvania, 19-21 June 2014. c?2014 Association for Computational Linguistics
Adapting SimpleNLG for Brazilian Portuguese realisation
Rodrigo de Oliveira
Department of Computing Science
University of Aberdeen
Aberdeen, UK, AB24 3UE
rodrigodeoliveira@abdn.ac.uk
Somayajulu Sripada
Department of Computing Science
University of Aberdeen
Aberdeen, UK, AB24 3UE
yaji.sripada@abdn.ac.uk
Abstract
This paper describes the ongoing imple-
mentation and the current coverage of
SimpleNLG-BP, an adaptation of Simple-
NLG-EnFr (Vaudry and Lapalme, 2013)
for Brazilian Portuguese.
1 Introduction
Realisation is the last step in natural language gen-
eration (NLG) systems, so the goal of a realisation
engine is to output text. SimpleNLG is a Java li-
brary that employs morphological, syntactic and
orthographical operations on non-linguistic input
to output well-formed sentences in English. Sim-
pleNLG-EnFr (Vaudry and Lapalme, 2013) is an
adaptation of SimpleNLG for French. This paper
describes the current state of SimpleNLG-BP1, an
adaptation of SimpleNLG-EnFr for realisation in
Brazilian Portuguese.
2 Recycling SimpleNLG-EnFr
To implement SimpleNLG-BP, we opted to extend
SimpleNLG-EnFr instead of the original Simple-
NLG. The main reason was the linguistic phe-
nomenon of preposition contraction, which is
what happens in da mesa (of the table): da is the
fusion of de (of ) with a (the.FEM.SNG). Because
preposition contraction happens in French but not
in English, we simply adapted the algorithm in
SimpleNLG-EnFr to suit Brazilian Portuguese.
3 Coverage of SimpleNLG-BP
As of submission date of this paper (May 23,
2014), almost all efforts in implementing Sim-
pleNLG-BP focused on morphological opera-
tions, as described in Moderna Grama?tica Por-
tuguesa (Bechara, 2009). However, a testbed
1The source code for SimpleNLG-BP can be found at
https://github.com/rdeoliveira/simplenlg-en-fr-pt .
of 43 instances including full sentences in non-
interrogative form and isolated phrases could be
successfully generated by SimpleNLG-BP.
3.1 Morphology
Morphological operations in the current state of
SimpleNLG-BP tackle 3 phrase types: noun
phrases, preposition phrases and verb phrases.
3.1.1 Pluralisation of nouns
Pluralisation rules in Brazilian Portuguese nor-
mally add a final -s to nouns, but word-internal
modifications may also be applied, depending on
the word?s stress, last vowel and/or ending. Pos-
sible noun endings in Brazilian Portuguese are: -l,
-m, -n, -r, -s, -x, -z and vowels. SimpleNLG-BP
currently includes all pluralisation rules for nouns
ending in -m, -r, -s, -x or most vowels, but only
some rules for endings -l, -n, -z and -a?o. The plu-
ralisation algorithm will still attempt to pluralise
any string, which is useful to handle neologisms.
3.1.2 Preposition contraction
Similar to French, Brazilian Portuguese provides
a morphophonological mechanism to contract
words in preposition phrases. The prepositions
that undergo contraction are a (by, to), em (in, or,
at), de (from, of ) and por (through, by) ? or prepo-
sition complexes ending in those, such as atra?s
de (behind) or em frente a (in front of ). When
these precede a determiner or adverb, preposition
and following item combine to form a single word.
Take as (the.FEM.PLR), for instance. If it appears
in a preposition phrase after a, em, de or por, the
result will be a`s, nas, das and pelas, respectively.
Note that desde (since) ends with -de but does not
undergo contraction. The same applies for contra
(against) and para (to, for); both end in -a but do
not undergo contraction.
93
3.1.3 Verb conjugation
English systematically combines all 3 tenses ?
past, present and future ? to perfective and/or pro-
gressive aspects. This gives English a total of 12
possible combinations for the same verb, person
and number. Subjunctive or imperative moods are
of little concern to English, since base forms of
verbs are usually identical to non-indicative forms.
Brazilian Portuguese may be said to express
the same number of tenses, aspects and moods.
In practice, this does not apply. Perfectiveness
in Brazilian Portuguese traditional grammars is
seen as a 3-element set ? perfective, imperfec-
tive and pluperfective ? which apply only to the
past tense. English uses perfectiveness across all
3 tenses (had done, have done, will have done).
Moreover, subjunctive forms in Brazilian Portu-
guese are morphologically distinct from indicative
forms. Conditional is not built by adding an un-
changeable auxiliary (e.g. would), but by mor-
phology as well. Finally, infinitive forms of verbs
may be conjugated or not. Thus, it was more
practical to implement tense in SimpleNLG-BP as
a 10-element set ? past, present, future, imper-
fect, pluperfect, conditional, subjunctive present,
subjunctive imperfect, subjunctive future and per-
sonal infinitive ? where each tense may already
pack some sense of aspect and mood.
Nevertheless, we implement aspect as a sepa-
rate 3-element set, to be optionally declared as
verb features, in order to trigger verb periphrasis
formation. Modern Brazilian Portuguese uses verb
periphrases extensively; e.g. the periphrastic form
tinha feito (had done) is normally used instead
of the single-verb form fizera (also had done).
SimpleNLG-BP associates ter (have) to perfec-
tiveness and estar (be) to progressiveness, thereby
resembling the grammar of English and preserv-
ing most of the optional verb-phrase features used
in the original SimpleNLG. Additionally, we in-
cluded prospectiveness in the aspect set (as sug-
gested by Bechara (2009) pp. 214-215) to gener-
ate periphrases that express future by means of the
auxiliary ir (go). With a 3-element aspect set and
a 10-element tense set, SimpleNLG-BP is able to
build 80 different forms2 for the same verb, per-
son and number. Additionally, negative, passive
and modalised verb phrases are also supported.
Modals generate prepositions automatically, if re-
2Even though 22 of these don?t seem to be used by Bra-
zilian Portuguese speakers.
quired, such as dar (be able to) and acabar (end),
whose prepositions are para and de respectively.
As far as subject-verb agreement, if the verb to
be conjugated exists in the default lexicon file, the
final string is simply retrieved; if not, a conjuga-
tion algorithm attempts to inflect the verb. For
SimpleNLG-BP, we compiled an XML lexicon file
out of DELAF PB (Muniz, 2004), an 880,000-
entry lexicon of inflected words in Brazilian Por-
tuguese. The original file became too large at first
? 1,029,075 lines, 45.4MB ? which turned out to
be an issue. A default run of SimpleNLG com-
piles the default lexicon file a priori to store it in
memory, so a single run (e.g. 1 test case) took an
average of 2.5 seconds, just to build the lexicon
onto memory. Since an inefficiency of that dimen-
sion can be prohibitive in some practical contexts,
we compiled a smaller list of 57 irregular verbs
in Brazilian Portuguese plus personal pronouns,
which became only 4,075-line long (167KB) and
takes only 0.17 seconds for compilation in aver-
age. SimpleNLG-BP includes both the lexicon file
and the lexicon compiler, if one wishes to modify
the default lexicon.
4 Summary
We described SimpleNLG-BP, an ongoing adap-
tation of SimpleNLG for Brazilian Portuguese,
which currently supports noun pluralisation,
preposition contractions and verb conjugation, and
includes a lexicon file and a lexicon compiler.
Acknowledgements
The first author of this paper thanks Ar-
ria/Data2Text Limited for funding his doctoral re-
search at the University ofAberdeen.
References
Evanildo Bechara. 2009. Moderna Grama?tica Por-
tuguesa. Nova Fronteira & Lucerna, Rio de Janeiro,
37 edition.
Marcelo Caetano Martins Muniz. 2004. A construc?a?o
de recursos lingu???stico-computacionais para o por-
tugue?s do Brasil: o projeto Unitex-PB. Master?s the-
sis, USP.
Pierre-Luc Vaudry and Guy Lapalme. 2013. Adapt-
ing SimpleNLG for bilingual English-French real-
isation. In 14th European Conference on Natural
Language Generation, pages 183?187, Sofia, Bul-
garia.
94
Proceedings of the 8th International Natural Language Generation Conference, pages 113?117,
Philadelphia, Pennsylvania, 19-21 June 2014. c?2014 Association for Computational Linguistics
Determining Content for Unknown Users: Lessons from 
           the MinkApp Case Study 
Gemma Webster, Somayajulu G. Sripada, Chris Mellish, Yolanda Melero, Koen Arts, 
Xavier Lambin, Rene Van Der Wal  
University of Aberdeen 
{gwebster, yaji.sripada, c.mellish, y.melero, k.arts, x.lambin, r.vanderwal}@abdn.ac.uk  
 
Abstract 
If an NLG system needs to be put in 
place as soon as possible it is not always 
possible to know in advance who the us-
ers of a system are or what kind of in-
formation will interest them. This paper 
describes the development of a system 
and contextualized text for unknown us-
ers. We describe the development, design 
and initial findings with a system for un-
known users that allows the users to de-
sign their own contextualised text. 
1 Introduction 
Requirements of an NLG system are derived 
commonly by analysing a gold standard corpus. 
Other knowledge acquisition (KA) techniques 
such as interviewing experts and end-users are 
also frequently employed. However, when these 
KA studies result in only a partial specification 
of the system requirements or complications 
make carrying out a detailed user study in the 
time available difficult, an initial system for un-
known users may need to be developed. The ini-
tial system needs to fulfil the known require-
ments making a number of assumptions to fill the 
gaps in the requirements. In this paper, we con-
centrate on the content determination problem 
for such a system. 
 
We encountered this particular problem when 
producing an initial NLG system to give feed-
back to volunteers submitting information about 
signs of American Mink, an invasive species in 
Scotland. Our response can be viewed, on one 
hand, as that of exposing an early prototype for 
evaluation in real use. On the other hand, it can 
be viewed as an approach to allowing users to 
?design their own contextualised text?. We ex-
pected that this approach would have a number 
of advantages. In the paper, we draw our conclu-
sions about how this worked out in our example 
application. 
2 Background - MinkApp 
The Scottish Mink Initiative (SMI) project aims 
to protect native wildlife by removing breeding 
American Mink (an invasive species) from the 
North of Scotland. SMI in the form discussed 
here was launched in May 2011 and ran until 
August 2013, after which it continued but on a 
much smaller funding base. SMI?s success and 
future rely on an ongoing network of volunteers 
from across Scotland to monitor the American 
mink population. During the period from 2011 to 
2013, these volunteers were coordinated by 4 and 
later 3 full-time Mink Control officers (MCOs) 
who had 2.5 year fixed term contracts, had no 
communal offices and were geographically lo-
cated across Scotland.  
At present volunteers are provided with rafts to 
monitor American mink. Rafts are simple devic-
es that float on water and are monitored by vol-
unteers who regularly check a clay pad for mink 
footprints. In the past, volunteers in turn reported 
signs or lack of signs to their corresponding 
MCO. Now volunteers can do the same through 
the MinkApp website, introduced in 2012, 
though some choose to continue to use the previ-
ous reporting method. The data should ideally be 
entered roughly every 10 days; it concerns either 
positive or negative records from raft checks, or 
visual sightings of mink and actual mink cap-
tures. The records contain geographical infor-
mation and a timestamp. MinkApp checks 
whether this data is complete and then informs 
the respective mink officer for that volunteer?s 
area and enters the data into the database.  
 
Volunteers used to receive a quarterly newsletter 
that had some regional specific content but was 
not volunteer specific. They could receive spo-
radic contact from their mink control officer in 
the form of a phone call or email. MinkApp al-
lowed an infrastructure to be developed to pro-
vide volunteers with specific and immediate 
113
feedback upon submission of their observations 
by means of contextualised feedback text. 
 
SMI?s funding base was severely reduced in Au-
gust 2013 and MinkApp has proven central to its 
endurance. Volunteer activities of the SMI are 
now supported by staff from 10 local rivers and 
fisheries trusts (as one of their many activities). 
This limited amount of staff time available could 
make the development of automatic personalised 
feedback generation vital to allow volunteers to 
have tailored information on the progress of the 
project and to keep volunteers engaged. 
3 The Problem - SMI Volunteers: The 
Unknown Users 
The nearest to a gold standard for what infor-
mation to offer was the corpus of newsletters 
containing information on the project as a whole. 
However, we learned that these newsletters were 
often not read and we have no way of judging 
their level of success. These newsletters, along 
with emails and discussions conducted with SMI 
employees on their interactions with volunteers, 
however, gave us ideas about potential content 
that could be selected and indication of potential 
lexical structure and word use when addressing 
volunteers.  
Although some SMI volunteers monitor mink as 
part of their job (e.g. gamekeepers), they could in 
fact be anyone with a desire to contribute to na-
ture conservation. Volunteers are located in very 
disparate geographical locations across Scotland, 
with no set gender or age range and so volun-
teers? motivations, computer skills and profes-
sions are mostly unknown. Because of the range 
of types of people who could in principle be vol-
unteers, they can be expected to be very varied. 
It is extremely difficult to contact all volunteers 
as each SMI catchment is managed and orga-
nized in different ways and volunteers are con-
tacted using different media e.g. mail, email, tel-
ephone, face-to-face. SMI is also careful to avoid 
attempting to contact volunteers too often, con-
scious that they are providing their services for 
free and should not be bothered unnecessarily.  
There is also some uncertainty about which vol-
unteers are active, as records are often partial or 
out of date. It is known anecdotally from MCOs 
that many volunteers are unwilling to use any 
kind of computer system and so it is unclear 
what kind of people will be reached through 
MinkApp. Finally, most observations of mink 
signs that arise are ?null records?, i.e. records of 
observing no mink prints on rafts. It is not known 
which volunteers will be sufficiently motivated 
to submit ?null records? and which will remain 
apparently inactive because they have nothing 
positive to report. 
So, even though there was a need for automati-
cally generated feedback now, there was a real 
question of who the readers would be and how to 
select the content to include in the feedback. 
4 Related Work 
A standard approach to establish user require-
ments for NLG is to assemble a corpus of hu-
man-authored texts and their associated inputs 
(Reiter & Dale, 2000). This can be the basis of 
deriving rules by hand, or one can attempt to rep-
licate content selection rules from the corpus by 
machine learning (Duboue & McKeown, 2003; 
Konstas & Lapata, 2012). To produce a useful 
corpus, however, one has to know one?s users or 
have reliable expert authors. 
 
As first pointed out by Levine et al. (1991), an 
NLG system that produces hypertext, rather than 
straight text, can avoid some content selection 
decisions, as the user makes some of these deci-
sions by selecting links to follow. A similar ad-
vantage applies to other adaptive hypertext sys-
tems (Brusilovsky, 2001).  Another general pos-
sibility is to allow users to design aspects of the 
texts they receive. For instance, ICONOCLAST 
(Power, Scott, & Bouayad-Agha, 2003) allows 
users to make choices about text style. However, 
relatively little is known about how such ap-
proaches work ?in the wild?. 
 
Various previous work has attempted to build 
models of users through observing interactions 
with an interface (Fischer, 2001). Alternatively, 
it is possible to explicitly ask questions to the 
user about their interests (Tintarev & Masthoff, 
2008), though this requires the users to have the 
time and motivation to take part in an initial ac-
tivity with no direct reward. 
 
Our approach can be seen to have similarities 
with hypertext generation, in that we are offering 
alternative texts to users, and non-invasive ap-
proaches to user modelling. 
114
5 Approach to Content Selection 
To overcome the ?unknown? user and ?unknown? 
feedback problem it was decided to implement a 
relatively quick exploratory tool that could be 
used to help understand user requirements, pro-
vide initial evaluation of feedback content and 
build an understanding of user interests. To 
achieve these aims we developed a tool that al-
lows users to generate their own text, selecting 
content from a larger set of possibilities. The in-
formation on the type of feedback generated by 
the user would allow us to investigate user stere-
otypes, their detection and the automatic adapta-
tion of content based on their interests 
(Zancanaro, Kuflik, Boger, Goren-Bar, & 
Goldwasser, 2007). 
5.1 Exploratory Tool - The Feedback Form 
The feedback form (Figure 1) is displayed to us-
ers of the MinkApp system once they have sub-
mitted a raft check. The form allows the user to 
select which raft they wish to have their feedback 
generated on from a list of the rafts they manage. 
The users have four types of information they 
can select to have feedback generated on: Signs 
(information on signs of mink reported through 
raft checks), Captures (information on mink cap-
tures), My Rafts (information on their personal 
raft checks and submission record) and Mink 
Ecology (information on mink behaviour and 
seasonality).  
Two of the four options, Signs and Captures, 
allow the user to select to what geographic scale 
they would like their feedback based on: the 
whole of the SMI project area, their river or their 
catchment ? the geographical region that they 
report to e.g. Aberdeenshire, Tayside etc.  
 
Once the user has made their selection the per-
sonalised feedback based on their choices is gen-
erated and displayed along with an option to rank 
how interesting they found this feedback or any 
comments they wish to make. The user can gen-
erate multiple texts in one session. All data from 
each click of an option, the generated text and 
user comments on the text are recorded.  
5.2 Generation of the paragraphs 
The structure of the text is separated out into 
self-contained paragraphs to allow analysis of 
what volunteers regularly view. For each type, 
the structure of the generated paragraph is de-
termined by a simple schema: 
Signs:  
Neighbourhood (based on user selection) ? In the 
Don catchment there have been 6 signs of mink 
reported over the past 12 months which is higher 
than the previous 12 months 
Additional Information / Motivation ? Mink are 
coming into your area to replace captured mink. 
This shows your area has good ecology for mink 
and it is important to keep monitoring. 
Personal ? There have been no signs of mink (in 
the form of either footprints or scat) in the past 
30 days. No signs of mink recently does not mean 
they are gone - remain vigilant. 
  
Captures: 
Neighbourhood (based on user selection) ? In the 
Spey catchment we have trapped 5 mink over the 
past 12 months which is lower than the previous 
12 months. 
Additional Information / Motivation ? Infor-
mation available on this year's captures: An 
adult female mink was captured on: 2014-02-19. 
 
My Rafts: 
Personal ?You have been very active over the 
past 60 days with 7 'no mink signs' reported and 
2 signs of mink (in the form of either footprints 
or scat) reported, the last of which was logged 
on 14 Sep 2013 23:00:00 GMT. 
Additional Information / Motivation ? Please 
keep checking your raft as this evidence means 
there are mink in your area. 
 
Mink Ecology: 
Temporal - We are in the normal mink breeding 
season!  
Motivation ? During the breeding season female 
mink will defend an area covering approximately 
1.5 miles.  
Additional Information - Female mink are small 
enough to fit into water vole burrows which they 
explore in search of prey.Did you know there can 
be brown, black, purple, white and silver mink 
which reflects the colours bred for fur? 
115
 
To produce the actual content to fill the slots of 
the schemas, the system was designed to reason 
over geographical location to allow examination 
of the various notions of neighbourhood 
(Tintarev et al 2012). The system also looks at 
temporal trends when developing text based on 
the number of record submissions for a given 
time. The system initially looks at record sub-
missions in the past week then opens out to a 
month, season and finally activity between the 
same seasons on different years. This use of 
temporal trends ensures volunteers are supplied 
with the most relevant (recent) mink activity in-
formation first in busy periods such as the breed-
ing season but ensures ?cleared? areas with little 
mink activity are still provided with informative 
feedback.  
6 Evaluation of the Feedback Approach 
We were initially apprehensive about how much 
usage the feedback system would get. MinkApp 
was launched through the SMI newsletters, but 
we knew that volunteers were not always receiv-
ing or reading these. Also it turned out that the 
initial estimate of active volunteers was over-
inflated. Indeed, initially the usage of MinkApp 
in general was much lower than was expected. 
So we worked hard to promote the system, for 
instance asking the fisheries trusts to actively ask 
any volunteers they had contact with if they had 
heard of MinkApp and to try to use it. As a re-
sult, we did manage to increase the system usage 
to a level where some initial conclusions can be 
drawn. 
MinkApp and specifically the feedback form use 
were monitored for 50 days (7 weeks). During 
this time 308 raft checks were submitted by vol-
unteers for 98 different rafts by 44 unique users. 
The feedback system was used by volunteers to 
generate 113 different texts about 36 different 
rafts. 32 out of the 44 (72.7%) of all MinkApp 
users requested generated feedback at least once.  
 
In 47% of the feedback form use sessions multi-
ple texts were generated and there are some par-
ticularly interesting use patterns: 
? ?Regular explorer?: One user accessed 
MinkApp seven times and generated 
feedback text on every use: 1 text, 3 
texts, 5 texts, 5 texts, 4 texts, 2 texts and 
1 text 
? ?Periodic explorer?: One user accessed 
MinkApp six times and generated at 
least one feedback text on every second 
use 
? ?Try once only?: The user who accessed 
MinkApp the most with eleven different 
sessions only generated feedback text on 
their first use of MinkApp.  
These different patterns of use require further 
investigation as the number of users using 
MinkApp increases. The patterns can be affected 
by idiosyncratic factors. For instance, one volun-
teer informed the project coordinator that they 
continually selected Captures within their area as 
they had caught a mink and their capture had not 
yet been added to the system - the volunteer was 
using the feedback form to monitor how long it 
took for mink capture data to appear in 
MinkApp.  
 
Of the four types of information available to vol-
unteers Signs was the most viewed although 
Captures was what SMI staff had felt volunteers 
would be most interested in. Signs had 56.6% of 
the overall use and catchment was the most 
widely selected option for geographic area for 
both Signs and Captures. However there was no 
clearly predominant second choice for infor-
mation option with Captures and My Rafts hav-
ing only 2.7% of a difference within their use. 
Mink Ecology was the least used category, partly 
to do with the lack of clarity in the name ?Mink 
Ecology?. Signs on a local geographical scale 
were the most common selection for volunteers 
but the actual use was not clear enough to sup-
port a fixed text type or removing other options. 
7 Conclusions 
The results of this initial study did support the 
value of feedback to volunteers (more directly 
than we would have been able to determine in 
advance) with 73% of volunteers choosing to 
generate feedback. The feedback enabled us to 
offer contextualized information to volunteers 
quickly, without initial extensive user studies, 
which was very important for supporting the 
continuation of SMI. 
The fact that the volunteer population was rela-
tively unknown meant that there were some un-
pleasant surprises in terms of uptake and interest. 
It was necessary to make special efforts to en-
courage participation to get larger numbers. 
116
When our system gets used over longer periods 
we might observe more meaningful patterns of 
behaviour. 
The patterns of interest we observed were noisy 
and were influenced by many contextual factors 
meaning there was little potential yet for statisti-
cal analysis or machine learning.  
8 Future Work 
In-depth analysis is required as more volunteers 
use MinkApp and the feedback form to fully un-
derstand patterns of behaviour. Additionally 
qualitative studies such as interviews with volun-
teers could help explain use and preferences. 
These studies could help us improve the feed-
back system and text to better suit the user?s 
needs. In the meantime, we have a working sys-
tem that offers choices to users to ?generate their 
own text? even though we had hoped to be able 
to tailor to individual volunteer preferences 
sooner. 
9 Acknowledgments 
We would like to thank SMI for their on-going 
commitment to this research. This work is sup-
ported by the Rural Digital Economy Research 
Hub (EPSRC EP/G066051/1). 
Reference 
Arts, K., Webster, G. ., Sharma, N. ., Melero, Y. ., 
Mellish, C., Lambin, X., & Van der Wal, R. 
(2013). Capturing mink and data. Interacting with a 
small and dispersed environmental initiative over 
the introduction of digital innovation Uploader. 
Case study for the online platform ?Framework for 
Responsible Research and Innovation in ICT.? Re-
trieved from http://responsible-
innovation.org.uk/torrii/resource-detail/1059 
Beirne, C., & Lambin, X. (2013). Understanding the 
Determinants of Volunteer Retention Through 
Capture-Recapture Analysis: Answering Social 
Science Questions Using a Wildlife Ecology 
Toolkit. Conservation Letters, 6(6), 391?401. 
doi:10.1111/conl.12023 
Brusilovsky, P. (2001). Adaptive Hypermedia. User 
Modeling and User-Adapted Interaction, 11(1-2), 
87?110. doi:10.1023/A:1011143116306 
Bryce, R., Oliver, M. K., Davies, L., Gray, H., Ur-
quhart, J., & Lambin, X. (2011). Turning back the 
tide of American mink invasion at an unprecedent-
ed scale through community participation and 
adaptive management. Biological conservation, 
144(1), 575?583. Retrieved from 
http://cat.inist.fr/?aModele=afficheN&cpsidt=2377
9637 
Duboue, P. A., & McKeown, K. R. (2003). Statistical 
acquisition of content selection rules for natural 
language generation. In Proceedings of the 2003 
conference on Empirical methods in natural lan-
guage processing - (Vol. 10, pp. 121?128). Morris-
town, NJ, USA: Association for Computational 
Linguistics. doi:10.3115/1119355.1119371 
Fischer, G. (2001). User Modeling in Human?
Computer Interaction. User Modeling and User-
Adapted Interaction, 11(1-2), 65?86. 
doi:10.1023/A:1011145532042 
Konstas, I., & Lapata, M. (2012). Concept-to-text 
generation via discriminative reranking, 369?378. 
Retrieved from 
http://dl.acm.org/citation.cfm?id=2390524.239057
6 
Levine, J., Cawsey, A., Mellish, C., Poynter, L., 
Reiter, E., Tyson, P., & Walker, J. (1991). IDAS: 
Combining hypertext and natural language genera-
tion. In Procs of the Third European Workshop on 
NLG (pp. 55?62). Innsbruck, Austria. 
Power, R., Scott, D., & Bouayad-Agha, N. (2003). 
Generating texts with style, 444?452. Retrieved 
from 
http://dl.acm.org/citation.cfm?id=1791562.179161
9 
Reiter, E., & Dale, R. (2000). Building Applied Natu-
ral Language Generation Systems. clt.mq.edu.au 
(Vol. 33.). Cambridge: Cambridge university press. 
Retrieved from 
http://clt.mq.edu.au/~rdale/publications/papers/199
7/jnle97.pdf 
Tintarev, N., & Masthoff, J. (2008). Adaptive Hyper-
media and Adaptive Web-Based Systems. (W. 
Nejdl, J. Kay, P. Pu, & E. Herder, Eds.) (Vol. 
5149, pp. 204?213). Berlin, Heidelberg: Springer 
Berlin Heidelberg. doi:10.1007/978-3-540-70987-9 
Tintarev, N., Melero, Y., Sripada, S., Tait, E., Van 
Der Wal, R., & Mellish, C. (2012). MinkApp: gen-
erating spatio-temporal summaries for nature con-
servation volunteers, 17?21. Retrieved from 
http://dl.acm.org/citation.cfm?id=2392712.239272
0 
Zancanaro, M., Kuflik, T., Boger, Z., Goren-Bar, D., 
& Goldwasser, D. (2007). Analyzing museum vis-
itors? behavior patterns. In C. Conati, K. McCoy, 
& G. Paliouras (Eds.), 11th International Confer-
ence on User Modeling (Vol. 4511, pp. 238?246). 
Berlin, Heidelberg: Springer Berlin Heidelberg. 
doi:10.1007/978-3-540-73078-1 
117
Proceedings of the 8th International Natural Language Generation Conference, pages 133?137,
Philadelphia, Pennsylvania, 19-21 June 2014. c?2014 Association for Computational Linguistics
Latent User Models for Online River Information Tailoring 
 Xiwu Han1, Somayajulu Sripada1, Kit (CJA) Macleod2, and Antonio A. R. Ioris3 Department of Computing Sciences, University of Aberdeen, UK1 James Hutton Institute, Aberdeen; University of Exeter, Exeter, UK2	 ?School of GeoSciences, University of Edinburgh, UK3 {xiwuhan,yaji.sripada}@abdn.ac.uk  kit.macleod@hutton.ac.uk a.ioris@ed.ac.uk
Abstract 
This paper explores Natural Language Genera-tion techniques for online river information tailoring. To solve the problem of unknown users, we propose ?latent models?, which relate typical visitors to river web pages, river data types, and river related activities. A hierarchy is used to integrate domain knowledge and la-tent user knowledge, and serves as the search space for content selection, which triggers us-er-oriented selection rules when they visit a page. Initial feedback received from user groups indicates that the latent models deserve further research efforts.  1 Introduction Within recent decades, access to online river in-formation has increased exponentially thanks to great progresses in data collection and storage technologies employed by hydrological organiza-tions worldwide (Dixon, 2010). Local residents nearby rivers and those engaged in river related activities are now much better informed and more engaged with data providers than decades ago. However, organizations such as SEPA (Scottish Environment Protection Agency), CEH (Centre for Ecology and Hydrology), EA (Envi-ronment Agency) in UK, and quite a few Cana-dian and Australian ones are working to improve the presentation of river information further. Many of these data providers, who are mostly government agencies, provide descriptive texts along with archived data of flow, level, flood and temperature along with their graphs and/or ta-bles. A typical example of linguistic description from the EA website is shown below:    The river level at Morwick is 0.65 me-tres. This measurement was recorded at 08:45 on 23/01/2013. The typical river 
level range for this location is between 0.27 metres and 2.60 metres. The highest river level recorded at this location is 6.32 metres and the river level reached 6.32 me-tres on 07/09/2008.1    The above descriptive text could vary to some extent according to different river users. For in-stance, it may provide information perceived as good news by farmers whilst other users e.g. ca-noeists or paddlers may interpret the information as bad news for their activity. Such tailored in-formation provision promotes communication efficiency between stakeholders and the relevant government offices (Macleod et al., 2012). We explored data-to-text techniques (Reiter, 2007) in promoting online river information provision. Our engagement activities with river stakehold-ers showed that there could be great difficulties in specifying user groups for online river infor-mation tailoring. First, the relations between do-main knowledge and user knowledge are difficult to be acquired due to domain sensitive challeng-es. Second, for online communication, the issue that users themselves sometimes are not sure about their tasks further hinders user modeling. This paper proposes an alternative approach of latent user models, instead of directly asking us-ers to indicate what they are interested in. 2 User Modeling Problem It has long been argued in NLG research that contents of generated texts should be oriented to users? tasks and existing knowledge. User mod-els are usually employed for the tailoring task. However, user models may not be easily ac-quired. Reiter et al (2003a) claimed that no NLG system actually used detailed user models with non-trivial numbers of users. Most commercial                                                 1 http://www.environment-agency.gov.uk/homeandleisure/ floods/riverlevels/120694.aspx?stationId=8143 
133
NLG systems would rather do with very limited user models, and examples are STOP (Reiter et al., 2003b), SUMTIME-MOUSAM (Sripada et al., 2002), and GIRL (Williams, 2002).     Recent research on user modeling falls into roughly three categories, i.e. explicit, implicit and hybrid approaches 2 . All approaches start with knowledge acquisition. Explicit models then define a finite number of user groups, and finally generate tailored texts for users to choose from, or choose to generate for a unique group at each time, e.g. (Molina, 2011 and 2012). Implicit models, e.g. (Mairesse and Walker, 2011), then construct a framework of human computer inter-action to learn about the values of a finite set of features, and finally generate tailored texts ac-cording to the intersection between domain knowledge and feature values. Hybrid models, e.g. (Bouayad-Agha et al, 2012) and (Dannels et al, 2012), specify both a finite set of user groups and a human computer interaction framework, and finally classify online users into defined groups for tailored generation. 3 Latent User Models Online river information tailoring involves a website, such as SEPA?s, which provides map based (or text based) searchable river infor-mation 3. The NLG task is to generate user-oriented texts while users are navigating the website. Both explicit and implicit user models can be employed for online river information tailoring. A finite set of user groups could be defined according to river-related activities, such as flooding, fishing, canoeing, etc. along with a set of features such as level trends, temperature ranges, etc. Then an interactive navigation mech-anism could ask a user to either choose a group or tailor his/her own parameters, and relevant texts can be generated thereafter.     Unfortunately, our engagement activities with stakeholders showed that it is almost impossible to define user models using mappings from river-related activities to river data features. Further-more, frequent users are reluctant to spend time on specifying their preferences before viewing the river information. For such an NLG task, the uncertainty comes not only from a large variety of river users and stakeholders, but also from the issue that users themselves sometimes are not                                                 2 Note the difference between NLG and HCI user models. The former tailor the output of NLG systems, while the later tailor the systems themselves. 3 http://sepa.org.uk/water/river_levels/river_level_data.aspx 
sure of what data features are associated with making decisions about their activities.    Our efforts on dealing with NLG domain knowledge and user models brought about the idea of extending domain knowledge to statisti-cally cover user knowledge, without explicitly defining user groups or implicitly modeling po-tential users. We argue that non-trivial number of uncertain users can be dynamically and statisti-cally modeled by integrating a module for web mining and Google analytics into the NLG pipe-line system. We regard these statistically estab-lished models as latent since they are hidden be-neath the domain knowledge, and the latent vari-able of typical users is linked to river data types and river related activities. 
 Figure 1. Domain Knowledge with Latent Models    The domain knowledge and latent user models are constructed as a whole in a hierarchical struc-ture, as in Figure 1. We technically maintain this hierarchy as an ontology based on existing ap-proaches e.g. (Bontcheva, 2005; Bouayad-Agha et al, 2012). The general part of the main frame was extracted from hydrology or environment websites, such as SEPA, CEH and EA, with the view that these websites were deliberately estab-lished hierarchically by manual work of domain experts in the fields of hydrology, ecology and/or geology. This part serves as the center of our domain knowledge, which starts with a root node and branches to river catchments, rivers, river stations and river data, while river data consists of water level, water flow, water temperature, etc. There are also some non-hierarchical rela-tions embedded, namely the tributary relation between rivers, the upriver relation between river stations, and the relationship between certain river data and river related activities. In addition 
134
to the time series on the status of the rivers, other information is integrated offline. Then, the do-main knowledge was extended to cover potential users? knowledge and online visiting behaviors. The extended information, or the latent user models, as denoted in italic fonts in Figure 1, includes three parts, i.e. the webpage visiting frequency, the relevance degrees between certain river data and river related activities, and the ranking of popularities of river-related activities for each river station.    Our extension process includes three stages, i.e. web mining, Google analytics, and engage-ment activities. At first, basic and rough infor-mation about river stations was statistically gath-ered by using free or trial version web mining tools, such as spiders and crawlers, and corpus analysis tools. For all combinations of elements respectively from each pair of columns in Table 1, we simply count the tokens of co-occurrence within an empirical window of 10 words. For the co-occurring tokens between a given river station and related activities, the top five tokens were selected by filtering according to one threshold on co-occurrence frequencies and another threshold on frequency differences between ad-jacent ranked types. For the co-occurring tokens between a given activity and river data type, rel-evant tokens were chosen by only one threshold on the co-occurrence frequencies. Finally, the co-occurring types of river stations and river data with high frequencies were used to fine-tune the previously acquired results, supposing that some river stations seldom or never provide some types of river data. 
River Stations Related Activities River Data Type Aberlour Aberuchill Aberuthven Abington Alford Allnabad Almondell Alness Ancrum Anie Apigill Arbroath ? 
Farming Fishing Canoeing Swimming Kayaking Rowing Boating Research Education Hiking Cycling ? ? 
Level Flow Temperature Width Rainfall Wind Pollution Birds Animals Fishes ? ?  Table 1. Basic Domain Knowledge for Extension     We further had the statistically acquired re-sults complemented and modified by Google analytics data for river websites and engagement activities with domain experts and users. Google 
analytics provided us with webpage visiting fre-quencies for each hydrological station, and con-tributed to the ranking of river-related activity for a given station. Knowledge gathered from engagement activities, such as semi-structured interviews and focus groups, was mainly used to confirm the statistically gathered information during the first two stages (as well as refine our overall understanding of data demands, water-related activities and perception of existing communication tools). For example, flood warn-ing information was moved up in the ranks since over 5 million people in England and Wales live and work in properties that are at risk of flooding from rivers or the sea4 (Marsh and Hannaford, 2007). Our present research is limited to rivers in Scotland, involving 107 river catchments, 233 rivers, and 339 river stations. The webpage visit-ing frequencies for these stations were gathered from Google analytics data for the website of SEPA5. The page visiting frequency for each riv-er station is represented by a time series with yearly periodicity, and each period includes 12 numeric elements calculated by dividing the number of monthly visiting times of the station by the total number of monthly visiting times of all river stations. 4 NLG for Online Tailoring Our NLG pipeline system takes numeric data of a given river station as input, and outputs a tai-lored description for that river station. The sys-tem analyzes data of water level, flow, and tem-perature as similar to time series analysis tasks presented in (Turner et al., 2006). Then, the ana-lyzed patterns are interpreted into symbolic con-ceptual representations, including vague expres-sions, which might facilitate users? understand-ing (van Deemter, 2010). SEPA defines normal ranges for river levels and we use these defini-tions in our computations to generate vague ex-pressions. For content selection, we define five sets: S = {s1, s2, ?} the set of stations; A = {a1, a2, ?} the set of activities for a given station; D = {d1, d2, ?}= {{d11, d12, ?}, {d21, d22, ?}, ?} the set of river data sets for a given station; AD = {a1d1, a1d2, ?, a2d1, ?} where aidj refers to in-formation from the interpretation of an activity ai under the condition of data dj; and SAD an over-view on one station. For a river station, using the domain knowledge hierarchy, which embeds la-                                                4 http://www.environment-agency.gov.uk/homeandleisure/ floods/default.aspx. 5 http://www.sepa.org.uk. 
135
tent user models implicitly (Figure 1), we select A ? D ? AD ? SAD as the initial contents. 
 Figure 2. Statistical Schemas    A schema-based approach was employed for document planning. Each schema at the high lev-el is made up of three components: Introduction, Events and Summary. Each of these components has its own substructure as shown in examples in Figure 4. With the estimated probabilistic distri-bution we generate schemas for a station based on its popular activities. We then tailor the text by randomly selecting from users? favorite vo-cabulary, which was acquired from online corpus for different river-related activities. Other words for structural purposes are dependent on certain schemas. Realization was performed using the simpleNLG library (Gatt and Reiter, 2009), and some generated examples are listed in Table 2. 
Schema (1) 
The Tyne at Nungate boasts its excellent salm-on catches. Now with medium steady water level and comparatively low water temperature, many people want to fish some salmons in pools between the rapids or experience whitewater rafting within them, which makes the periphery of Nungate a hot spot. Schema (2) 
The periphery of Tyne at Nungate poses a hot spot now, where many people are fishing or canoeing while appreciating the medium steady water level and comparatively low water tem-perature. No wonder Nungate can boast one of the best salmon catching places. Schema (3) 
The Tyne at Nungate boasts its excellent salm-on catches. Many people may now fish or canoe there thanks to the medium steady water level and comparatively low water temperature, mak-ing the periphery of Nungate a hot spot. Table 2. Some Tailored NLG Examples (Italic fonts denote the tailored lexical realization) 5 Initial Feedback and Conclusion This research is still underway and a thorough evaluation is still pending. We have received valuable feedback from small user groups. Sup-portive examples are: a. An overview about pop-ular river stations can help users? further explora-tion of information to a significant extent; b. A general comprehension for a given river station can be more easily built up by simply reading the generated descriptions, than by solely reading the data and its related graphics; c. Along with the graphics, the generated descriptions can improve the communication efficiency by a large degree. Examples recommending further improve-ment/focus include: a. Schemas filled in with acquired vocabulary sometimes endow the gen-erated document a syntactically and/or semanti-cally unexpected flavor; b. Established users de-mand more linguistic varieties than new users.    Present feedback implicates that latent user models deserve further research. Our future ef-forts will focus on a. extending the domain knowledge to cover all river stations, b. develop-ing generic methodology for acquiring latent user models for other online NLG tasks (e.g. generat-ing descriptions of Census data), and c. integrat-ing an automatic update of latent models. Acknowledgement This research is supported by an award from the RCUK DE programme: EP/G066051/1. The au-thors are also grateful to Dr. Rene van der Wal, Dr. Koen Arts, and the three anonymous review-ers for improving the quality of this paper. 
136
References  K. Bontcheva. 2005. Generating Tailored Textual Summaries from Ontologies. The Semantic Web: Research and Applications, Lecture Notes in Com-puter Science, Vol. 3532, pages 531-545. Springer-Verlag. N. Bouayad-Agha, G. Casamayor, Simon Mille, et al. 2012.  From Ontology to NL: Generation of Multi-lingual User-Oriented Environmental Reports. Natural Language Processing and Information Systems, Lecture Notes in Computer Science Vol. 7337, pages 216-221. Springer-Verlag. Dana Dannells, Mariana Damova, Ramona Enache and Milen Chechev. 2012. Multilingual Online Generation from Semantic Web Ontologies. WWW 2012 ? European Projects Track, pages 239-242. H. Dixon. 2010. Managing national hydrometric data: from data to information. Global Change: Facing Risks and Threats to Water Resources. Walling-ford, UK, IAHS Press, pages 451-458. A. Gatt and Ehud Reiter. 2009. Simplenlg: A Realiza-tion Engine for Practical Applications. Proceedings ENLG-2009, pages 90-93. K. Macleod, S. Sripada, A. Ioris, K. Arts and R. Van der Wal. 2012. Communicating River Level Data and Information to Stakeholders with Different In-terests: the Participative Development of an Inter-active Online Service. International Environmental Modeling and Software Society (iEMSs): Interna-tional Congress on Environmental Modeling and Software Managing Resources of a Limited Planet, Sixth Biennial Meeting, Leipzig, Germany. R. Seppelt, A.A. Voinov, S. Lange, D. Bankamp (Eds.) pages 33-40. Francois Mairesse and Marilyn A. Walker. 2011. Controlling User Perceptions of Linguistic Style: Trainable Generation of Personality Traits. Compu-tational Linguistics, Volume 37 Issue 3, September 2011, pages 455-488. T. J. Marsh and J. Hannaford. 2007. The summer 2007 floods in England and Wales ? a hydrological appraisal. Centre for Ecology & Hydrology, UK. M. Molina. 2012. Simulating Data Journalism to Communicate Hydrological Information from Sen-sor Networks. Proceedings of IBERAMIA, pages 722-731. M. Molina, A. Stent, and E. Parodi. 2011. Generating Automated News to Explain the Meaning of Sen-sor Data. In: Gama, J., Bradley, E., Hollm?n, J. (eds.) IDA 2011. LNCS, vol. 7014, pages 282-293. Springer, Heidelberg. Ehud Reiter, Somayajulu Sripada, and Sandra Wil-liams. 2003a. Acquiring and Using Limited User Models in NLG. In Proceedings of the 9th Europe-
an Workshop on Natural Language Generation, pages 13-14, Budapest, Hungary. Ehud Reiter, Roma Robertson, and Liesl Osman. 2003b. Lessons from a failure: Generating tailored smoking cessation letters. Artificial Intelligence, 144(1-2), pages 41-58.  Ehud Reiter. 2007. An Architecture for Data-to-Text Systems. Proceedings of ENLG-2007, pages 97-104.  S. Sripada, Ehud Reiter, Jim Hunter, and Jin Yu. 2002. Segmenting time series for weather fore- casting. Applications and Innovations in Intelli- gent Systems X, pages 105-118. Springer-Verlag. R. Turner, S. Sripada, E. Reiter and I. Davy. 2006. Generating Spatio-Temporal Descriptions in Pollen Forecasts. Proceedings of EACL06 poster session, pages 163-166. K. van Deemter. 2010. Vagueness Facilitates Search. Proceedings of the 2009 Amsterdam Colloquium, Springer Lecture Notes in Computer Science (LNCS). FoLLI LNAI 6042. Sandra Williams. 2002. Natural language generation of discourse connectives for different reading lev-els. In Proceedings of the 5th Annual CLUK Re-search Colloquium, Leeds, UK.  
137
