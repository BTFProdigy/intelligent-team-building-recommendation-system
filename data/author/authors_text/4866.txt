Proceedings of the COLING/ACL 2006 Interactive Presentation Sessions, pages 1?4,
Sydney, July 2006. c?2006 Association for Computational Linguistics
FAST ? An Automatic Generation System for Grammar Tests 
 
 
Chia-Yin Chen 
Inst. of Info. Systems & Applications 
National Tsing Hua University 
101, Kuangfu Road, 
Hsinchu, 300, Taiwan 
G936727@oz.nthu.edu.tw 
Hsien-Chin Liou 
Dep. of Foreign Lang. & Lit. 
National Tsing Hua University 
101, Kuangfu Road, 
Hsinchu, 300, Taiwan 
hcliu@mx.nthu.edu.tw 
Jason S. Chang 
Dep. of Computer Science 
National Tsing Hua University 
101, Kuangfu Road, 
Hsinchu, 300, Taiwan 
jschang@cs.nthu.edu.tw
 
  
 
Abstract 
This paper introduces a method for the 
semi-automatic generation of grammar 
test items by applying Natural Language 
Processing (NLP) techniques. Based on 
manually-designed patterns, sentences 
gathered from the Web are transformed 
into tests on grammaticality. The method 
involves representing test writing 
knowledge as test patterns, acquiring 
authentic sentences on the Web, and 
applying generation strategies to 
transform sentences into items. At 
runtime, sentences are converted into two 
types of TOEFL-style question: multiple-
choice and error detection. We also 
describe a prototype system FAST (Free 
Assessment of Structural Tests). 
Evaluation on a set of generated 
questions indicates that the proposed 
method performs satisfactory quality. 
Our methodology provides a promising 
approach and offers significant potential 
for computer assisted language learning 
and assessment. 
1 Introduction 
Language testing, aimed to assess learners? 
language ability, is an essential part of language 
teaching and learning. Among all kinds of tests, 
grammar test is commonly used in every 
educational assessment and is included in well-
established standardized tests like TOEFL (Test 
of English as Foreign Language). 
Larsen-Freeman (1997) defines grammar is 
made of three dimensions: form, meaning, and 
use (See Figure 1). Hence, the goal of a grammar  
test is to test learners to use grammar accurately, 
meaningfully, and appropriately.  Consider the 
possessive case of the personal noun in English. 
The possessive form comprises an apostrophe 
and the letter ?s?. For example, the possessive 
form of the personal noun ?Mary? is ?Mary?s?. 
The grammatical meaning of the possessive case 
can be (1) showing the ownership: ?Mary?s book 
is on the table.? (= a book that belongs to Mary); 
(2) indicating the relationship: ?Mary?s sister is 
a student.? (=the sister that Mary has).Therefore, 
a comprehensive grammar question needs to 
examine learners? grammatical knowledge from 
all three aspects (morphosyntax, semantics and 
pragmatics).  
 
 
 
 
 
 
 
 
Figure 1: Three Dimensions of Grammar 
(Larsen-Freeman, 1997) 
The most common way of testing grammar is 
the multiple-choice test (Kathleen and Kenji, 
1996). Multiple-choice test format on 
grammaticality consists of two kinds: one is the 
traditional multiple-choice test and the other is 
the error detection test. Figure 2 shows a typical 
example of traditional multiple-choice item. As 
for Figure 3, it shows a sample of error detection 
question. 
Traditional multiple-choice is composed of 
three components, where we define the sentence 
with a gap as the stem, the correct choice to the 
gap as the key and the other incorrect choices as 
the distractors. For instance, in Figure 2, the 
Form Meaning 
(appropriateness) 
(accuracy) 
ness) 
(meaningful- 
Use 
1
In the Great Smoky Mountains, one can see _____ 150 
different kinds of tress. 
(A) more than 
(B) as much as 
(C) up as 
(D) as many to 
Although maple trees are among the most colorful  
                                             (A)   
varieties in the fall, they lose its leaves 
                    (B)                     (C)   
sooner than oak trees.  
       (D) 
partially blanked sentence acts as the stem and 
the key ?more than? is accompanied by three 
distractors of ?as much as?, ?up as?, and ?as 
many to?. On the other hand, error detection item 
consists of a partially underlined sentence (stem) 
where one choice of the underlined part 
represents the error (key) and the other 
underlined parts act as distractors to distract test 
takers. In Figure 3, the stem is ?Although maple 
trees are among the most colorful varieties in the 
fall, they lose its leaves sooner than oak trees.? 
and ?its? is the key with distractors ?among?, ?in 
the fall?, and ?sooner than.? 
 
Grammar tests are widely used to assess 
learners? grammatical competence, however, it is 
costly to manually design these questions. In 
recent years, some attempts (Coniam, 1997; 
Mitkov and Ha, 2003; Liu et al, 2005) have been 
made on the automatic generation of language 
testing. Nevertheless, no attempt has been made 
to generate English grammar tests. Additionally, 
previous research merely focuses on generating 
questions of traditional multiple-choice task, no 
attempt has been made for the generation of error 
detection test types. 
In this paper, we present a novel approach to 
generate grammar tests of traditional multiple-
choice and error detection types. First, by 
analyzing syntactic structure of English 
sentences, we constitute a number of patterns for 
the development of structural tests. For example, 
a verb-related pattern requiring an infinitive as 
the complement (e.g., the verb ?tend?) can be 
formed from the sentence ?The weather tends to 
improve in May.? For each pattern, distractors 
are created for the completion of each grammar 
question. As in the case of foregoing sentence, 
wrong alternatives are constructed by changing 
the verb ?improve? into different forms: ?to 
improving?, ?improve?, and ?improving.? Then, 
we collect authentic sentences from the Web as 
the source of the tests. Finally, by applying 
different generation strategies, grammar tests in 
two test formats are produced. A complete 
grammar question is generated as shown in 
Figure 4. Intuitively, based on certain surface 
pattern (See Figure 5), computer is able to 
compose a grammar question presented in Figure 
4. We have implemented a prototype system 
FAST and the experiment results have shown that 
about 70 test patterns can be successfully written 
to convert authentic Web-based texts into 
grammar tests. 
 
 
 
  
 
 
 
* X/INFINITIVE * CLAUSE. 
 
* _______* CLAUSE. 
(A) X/INFINITIVE 
(B) X/to VBG 
(C) X/VBG 
(D) X/VB 
 
2 Related Work 
Since the mid 1980s, item generation for test 
development has been an area of active research. 
In our work, we address an aspect of CAIG 
(computer-assisted item generation) centering on 
the semi-automatic construction of grammar tests. 
Recently, NLP (Natural Language Processing) 
has been applied in CAIG to generate tests in 
multiple-choice format. Mitkov and Ha (2003) 
established a system which generates reading 
comprehension tests in a semi-automatic way by 
using an NLP-based approach to extract key 
concepts of sentences and obtain semantically 
alternative terms from WordNet. 
Coniam (1997) described a process to 
compose vocabulary test items relying on corpus 
word frequency data. Recently, Gao (2000) 
presented a system named AWETS that semi-
automatically constructs vocabulary tests based 
on word frequency and part-of-speech 
information. Most recently, Hoshino and 
Nakagawa (2005) established a real-time system 
which automatically generates vocabulary 
questions by utilizing machine learning 
techniques. Brown, Frishkoff, and Eskenazi 
(2005) also introduced a method on the 
automatic generation of 6 types of vocabulary 
questions by employing data from WordNet. 
I intend _______ you that we cannot approve your 
application. 
(A) to inform 
(B) to informing 
(C) informing 
(D) inform
Figure 4: An example of generated question.
Figure 5: An example of surface pattern. Figure 3: An example of error detection. 
Figure 2: An example of multiple-choice. 
2
Liu, Wang, Gao, and Huang (2005) proposed 
ways of the automatic composing of English 
cloze items by applying word sense 
disambiguation method to choose target words of 
certain sense and collocation-based approach to 
select distractors.  
Previous work emphasizes the automatic 
generation of reading comprehension, 
vocabulary, and cloze questions. In contrast, we 
present a system that allows grammar test writers 
to represent common patterns of test items and 
distractors. With these patterns, the system 
automatically gathers authentic sentences and 
generates grammar test items. 
3 The FAST System 
The question generation process of the FAST 
system includes manual design of test patterns 
(including construct pattern and distractor 
generation pattern), extracting sentences from the 
Web, and semi-automatic generation of test 
items by matching sentences against patterns. In 
the rest of this section, we will thoroughly 
describe the generation procedure.  
3.1 Question Generation Algorithm 
Input: P = common patterns for grammar test 
items, URL = a Web site for gathering sentences 
Output: T, a set of grammar test items g 
 
1. Crawl the site URL for webpages 
2. Clean up HTML tags. Get sentences S 
therein that are self-contained. 
3. Tag each word in S with part of speech (POS) 
and base phrase (or chunk). (See Figure 6 for 
the example of the tagging sentence ?A 
nuclear weapon is a weapon that derives its 
and or fusion.?) 
 
 
 
 
 
 
 
 
 
 
 
4. Match P against S to get a set of candidate 
sentences D. 
5. Convert each sentence d in D into a grammar 
test item g. 
3.2 Writing Test Patterns 
Grammar tests usually include a set of patterns 
covering different grammatical categories. These 
patterns are easily to conceptualize and to write 
down. In the first step of the creation process, we 
design test patterns. 
A construct pattern can be observed through 
analyzing sentences of similar structural features. 
Sentences ?My friends enjoy traveling by plane.? 
and ?I enjoy surfing on the Internet.? are 
analyzed as an illustration. Two sentences share 
identical syntactic structure {* enjoy X/Gerund 
*}, indicating the grammatical rule for the verb 
?enjoy? needing a gerund as the complement. 
Similar surface patterns can be found when 
replacing ?enjoy? by verbs such as ?admit? and 
?finish? (e.g., {* admit X/Gerund *} and {* 
finish X/Gerund *} ). These two generalize these 
surface patterns, we write a construct pattern {* 
VB VBG *} in terms of POS tags produced by a 
POS tagger. Thus, a construct pattern 
characterizing that some verbs require a gerund 
in the complement is contrived. 
Distractor generation pattern is dependent on 
each designed construct pattern and therefore 
needs to design separately. Distractors are 
usually composed of words in the construct 
pattern with some modifications: changing part 
of speech, adding, deleting, replacing, or 
reordering of words. By way of example, in the 
sentence ?Strauss finished writing two of his 
published compositions before his tenth 
birthday.?, ?writing? is the pivot word according 
to the construct pattern {* VBD VBG *}. 
Distractors for this question are: ?write?, 
?written?, and ?wrote?. Similar to the way for the 
construct pattern devise, we use POS tags to 
represent distractor generation pattern: {VB}, 
{VBN}, and {VBD}. We define a notation 
scheme for the distractor designing. The symbol 
$0 designates the changing of the pivot word in 
the construct pattern while $9 and $1 are the 
words proceeding and following the pivot word, 
respectively. Hence, distractors for the 
abovementioned question are {$0 VB}, {$0 
VBN}, and {$0 VBD}  
3.3   Web Crawl for Candidate Sentences 
As the second step, we extract authentic 
materials from the Web for the use of question 
stems. We collect a large number of sentences 
from websites containing texts of learned genres 
(e.g., textbook, encyclopedia).  
Lemmatization:  a nuclear weapon be a weapon that derive its 
energy from the nuclear reaction of fission 
and or fusion. 
POS:  a/at nuclear/jj weapon/nn be/bez a/at weapon/nn that/wps
          derive/vbz its/pp$ energy/nn from/in the/at nuclear/jj 
         reaction/nns of/in fission/nn  and/cc or/cc fusion/nn ./.  
Chunk:   a/B-NP nuclear/I-NP weapon/I-NP be/B-VP a/B-NP 
               weapon/I-NP that/B-NP derive/B-VP its/B-NP 
               energy/I-NP from/B-PP the/B-NP nuclear/I-NP 
reaction/I-NP of/B-PP fission/B-NP and/O or/B-UCP 
fusion/B-NP ./O  
Figure 6: Lemmatization, POS tagging and      
           chunking of a sentence. 
3
3.4 Test Strategy   
The generation strategies of multiple-choice and 
error detection questions are different. The 
generation strategy of traditional multiple-choice 
questions involves three steps. The first step is to 
empty words involved in the construct pattern. 
Then, according to the distractor generation 
pattern, three erroneous statements are produced. 
Finally, option identifiers (e.g., A, B, C, D) are 
randomly assigned to each alternative.  
The test strategy for error detection questions 
is involved with: (1) locating the target point, (2) 
replacing the construct by selecting wrong 
statements produced based on distractor 
generation pattern, (3) grouping words of same 
chunk type to phrase chunk (e.g., ?the/B-NP 
nickname/I-NP? becomes ?the nickname/NP?) 
and randomly choosing three phrase chunks to 
act as distractors, and (4) assigning options based 
on position order information.  
4 Experiment and Evaluation Results  
In the experiment, we first constructed test 
patterns by adapting a number of grammatical 
rules organized and classified in ?How to 
Prepare for the TOEFL?, a book written by 
Sharpe (2004). We designed 69 test patterns 
covering nine grammatical categories. Then, the 
system extracted articles from two websites, 
Wikipedia (an online encyclopedia) and VOA 
(Voice of American). Concerning about the 
readability issue (Dale-Chall, 1995) and the self-
contained characteristic of grammar question 
stems, we extracted the first sentence of each 
article and selected sentences based on the 
readability distribution of simulated TOEFL tests. 
Finally, the system matched the tagged sentences 
against the test patterns. With the assistance of 
the computer, 3,872 sentences are transformed 
into 25,906 traditional multiple-choice questions 
while 2,780 sentences are converted into 24,221 
error detection questions. 
A large amount of verb-related grammar 
questions were blindly evaluated by seven 
professor/students from the TESOL program. 
From a total of 1,359 multiple-choice questions, 
77% were regarded as ?worthy? (i.e., can be 
direct use or only needed minor revision) while 
80% among 1,908 error detection tasks were 
deemed to be ?worthy?. The evaluation results 
indicate a satisfactory performance of the   
proposed method.   
5 Conclusion 
We present a method for the semi-automatic 
generation of grammar tests in two test formats 
by using authentic materials from the Web. At 
runtime, a given sentence sharing classified 
construct patterns is generated into tests on 
grammaticality. Experimental results assess the 
facility and appropriateness of the introduced 
method and indicate that this novel approach 
does pave a new way of CAIG.  
References 
Coniam, D. (1997). A Preliminary Inquiry into 
Using Corpus Word Frequency Data in the 
Automatic Generation of English Cloze Tests. 
CALICO Journal, No 2-4, pp. 15- 33. 
Gao, Z.M. (2000). AWETS: An Automatic Web-
Based English Testing System. In Proceedings 
of the 8th Conference on Computers in 
Education/International Conference on 
Computer-Assisted Instruction ICCE/ICCAI, 
2000, Vol. 1, pp. 28-634. 
Hoshino, A. & Nakagawa, H. (2005). A Real-
Time Multiple-Choice Question Generation for 
Language Testing-A Preliminary Study-. In 
Proceedings of the Second Workshop on 
Building Educational Applications Using NLP, 
pp. 1-8, Ann Arbor, Michigan, 2005. 
Larsen-Freeman, D. (1997). Grammar and its 
teaching: Challenging the myths (ERIC Digest). 
Washington, DC: ERIC Clearinghouse on 
languages and Linguistics, Center for Applied 
Linguistics. Retrieved July 13, 2005, from 
http://www.vtaide.com/png/ERIC/Grammar.htm 
Liu, C.L., Wang, C.H., Gao, Z.M., & Huang, 
S.M. (2005). Applications of Lexical 
Information for Algorithmically Composing 
Multiple-Choice Cloze Items, In Proceedings of 
the Second Workshop on Building Educational 
Applications Using NLP, pp. 1-8, Ann Arbor, 
Michigan, 2005. 
Mitkov, R. & Ha, L.A. (2003). Computer-Aided 
Generation of Multiple-Choice Tests. In 
Proceedings of the HLT-NAACL 2003 
Workshop on Building Educational 
Applications Using Natural Language 
Processing, Edmonton, Canada, May, pp. 17 ? 
22. 
Sharpe, P.J. (2004). How to Prepare for the 
TOEFL. Barrons? Educational Series, Inc. 
Chall, J.S. & Dale, E. (1995). Readability 
Revisited: The New Dale-Chall Readability 
Formula. Cambridge, MA:Brookline Books. 
 
4
Proceedings of the COLING/ACL 2006 Interactive Presentation Sessions, pages 41?44,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Computational Analysis of Move Structures in Academic Abstracts 
Jien-Chen Wu1   Yu-Chia Chang1   Hsien-Chin Liou2   Jason S. Chang1 
CS1 and FLL2, National Tsing Hua Univ. 
{d928322,d948353}@oz.nthu.edu.tw, hcliu@mx.nthu.edu.tw, 
jason.jschang@gmail.com 
Abstract 
This paper introduces a method for 
computational analysis of move 
structures in abstracts of research articles. 
In our approach, sentences in a given 
abstract are analyzed and labeled with a 
specific move in light of various 
rhetorical functions. The method involves 
automatically gathering a large number 
of abstracts from the Web and building a 
language model of abstract moves. We 
also present a prototype concordancer, 
CARE, which exploits the move-tagged 
abstracts for digital learning. This system 
provides a promising approach to Web-
based computer-assisted academic 
writing. 
1 Introduction 
In recent years, with the rapid development of 
globalization, English for Academic Purposes 
has drawn researchers' attention and become the 
mainstream of English for Specific Purposes, 
particularly in the field of English of Academic 
Writing (EAW). EAW deals mainly with genres, 
including research articles (RAs), reviews, 
experimental reports, and other types of 
academic writing. RAs play the most important 
role of offering researchers the access to actively 
participating in the academic and discourse 
community and sharing academic research 
information with one another. 
Abstracts are constantly regarded as the first 
part of RAs and few scholarly RAs go without an 
abstract. ?A well-prepared abstract enables 
readers to identify the basic content of a 
document quickly and accurately.? (American 
National Standards Institute, 1979) Therefore, 
RAs' abstracts are equally important to writers 
and readers. 
Recent research on abstract requires manually 
analysis, which is time-consuming and labor-
intensive. Moreover, with the rapid development 
of science and technology, learners are 
increasingly engaged in self-paced learning in a 
digital environment. Our study, therefore, 
attempts to investigate ways of automatically 
analyzing the move structure of English RAs? 
abstracts and develops an online learning system, 
CARE (Concordancer for Academic wRiting in 
English). It is expected that the automatic 
analytical tool for move structures will facilitate 
non-native speakers (NNS) or novice writers to 
be aware of appropriate move structures and 
internalize relevant knowledge to improve their 
writing. 
2 Macrostructure of Information in 
RAs 
Swales (1990) presented a simple and succinct 
picture of the organizational pattern for a RA?
the IMRD structure (Introduction, Methods, 
Results, and Discussion). Additionally Swales 
(1981, 1990) introduced the theory of genre 
analysis of a RA and a four-move scheme, which 
was later refined as the "Create a Research 
Space" (CARS) model for analyzing a RA?s 
introduction section.  
Even though Swales seemed to have 
overlooked the abstract section, in which he did 
not propose any move analysis, he himself 
plainly realized ?abstracts continue to remain a 
neglected field among discourse analysts? 
(Swales, 1990, p. 181). Salager-Meyer (1992) 
also stated, ?Abstracts play such a pivotal role in 
any professional reading? (p. 94). Seemingly 
researchers have perceived this view, so research 
has been expanded to concentrate on the abstract 
in recent years. 
Anthony (2003) further pointed out, ?research 
has shown that the study of rhetorical 
organization or structure of texts is particularly 
useful in the technical reading and writing 
classroom? (p. 185). Therefore, he utilized 
computational means to create a system, Mover, 
which could offer move analysis to assist 
abstract writing and reading. 
3 CARE 
Our system focuses on automatically 
computational analysis of move structures (i.e. 
41
Background, Purpose, Method, Result, and 
Conclusion) in RA abstracts. In particular, we 
investigate the feasibility of using a few 
manually labeled data as seeds to train a Markov 
model and to automatically acquire move-
collocation relationships based on a large number 
of unlabeled data. These relationships are then 
used to analyze the rhetorical structure of 
abstracts. It is important that only a small 
number of manually labeled data are required 
while much of move tagging knowledge is 
learned from unlabeled data. We attempt to 
identify which rhetorical move is correspondent 
to a sentence in a given abstract by using features 
(e.g. collocations in the sentence). Our learning 
process is shown as follows: 
 
(1)Automatically collect abstracts from the Web for 
     training 
(2)Manually label each sentence in a small set of given  
     abstracts 
(3)Automatically extract collocations from all abstracts 
(4)Manually label one move for each distinct collocation 
(5)Automatically expand collocations indicative of each 
    move 
(6)Develop a hidden Markov model for move tagging 
Figure 1: Processes used to learn collocation 
classifiers 
3.1 Collecting Training Data 
In the first four processes, we collected data 
through a search engine to build the abstract 
corpus A. Three specialists in computer science 
tagged a small set of the qualified abstracts based 
on our coding scheme of moves. Meanwhile, we 
extracted the collocations (Jian et al, 2004) from 
the abstract corpus, and labeled these extracted 
collocations with the same coding scheme.  
3.2 Automatically Expanding Collocations 
for Moves 
To balance the distribution in the move-tagged 
collocation (MTC), we expand the collocation for 
certain moves in this stage. We use the one-
move-per-collocation constraint to bootstrap, 
which mainly hinges on the feature redundancy 
of the given data, a situation where there is often 
evidence to indicate that a given should be 
annotated with a certain move. That is, given one 
collocation ci is tagged with move mi, all 
sentences S containing collocation ci will be 
tagged with mi as well; meanwhile, the other 
collocations in S are thus all tagged with mi. For 
example: 
 
Step 1. The collocation ?paper address? 
extracted from corpus A is labeled with the ?P? 
move. Then we use it to label other untagged 
sentences US (e.g. Examples (1) through (2)) 
containing ?paper address? as ?P? in A. As a 
result, these US become tagged sentences TS 
with ?P? move. 
 
  (1)This paper addresses the state explosion problem in  
       automata based ltl model checking. //P// 
  (2)This paper addresses the problem of fitting mixture  
       densities to multivariate binned and truncated data. //P// 
 
Step 2. We then look for other features (e.g. the 
collocation, ?address problem?) that occur in TS 
of A to discover new evidences of a ?P? move 
(e.g. Examples (3) through (4)). 
 
  (3)This paper addresses the state explosion problem in  
       automata based ltl model checking. 
  (4)This paper addresses the problem of fitting mixture   
       densities to multivariate binned and truncated data. 
 
Step 3. Subsequently, the feature ?address 
problem? can be further exploited to tag 
sentences which realize the ?P? move but do not 
contain the collocation ?paper address?, thus 
gradually expanding the scope of the annotations 
to A. For example, in the second iteration, 
Example (5) and (6) can be automatically tagged 
as indicating the ?P? move. 
 
   (5)In this paper we address the problem of query  
       answering using views for non-recursive data log    
       queries embedded in a Description Logics  
       knowledge base. //P// 
  (6)We address the problem of learning robust  
       plans for robot navigation by observing  
       particular robot behaviors. //P// 
 
From these examples ((5) and (6)), we can 
extend to another feature ?we address?, which 
can be tagged as ?P? move as well. The 
bootstrapping processes can be repeated until no 
new feature with high enough frequency is found 
(a sample of collocation expanded list is shown 
in Table1).  
 
Type Collocation Move Count of 
Collocation 
with mj 
Total of 
Collocation 
Occurrences
NV we present P 3,441 3,668 
NV we show R 1,985 2,069 
NV we propose P 1,722 1,787 
NV we describe P 1,505 1,583 
? ? ? ? ? 
Table 1: The sample of the expanded collocation 
list 
42
3.3 Building a HMM for Move Tagging 
The move sequence probability P(ti+1? ti) is 
given as the following description: 
We are given a corpus of unlabeled abstracts A 
= {A1,?, AN}. We are also given a small labeled 
subset S = {L1,?, Lk} of A, where each abstract 
Li consists of a sequence of sentence and move 
{t1, t2,?, tk}. The moves ti take out of a value 
from a set of possible move M = {m1,m2,?,mn}. 
Then 11
( | )( | )
( )
i i
i i
i
N t tP t t
N t
+
+
? ?= ? ?? ?
 
According to the bi-gram move sequence 
score (shown in Table 2), we can see move 
sequences follow a certain schematic pattern. For 
instance, the ?B? move is usually directly 
followed by the ?P? move or ?B? move, but not 
by the ?M? move. Also rarely will a ?P? move 
occur before a ?B? move. Furthermore, an 
abstract seldom have a move sequence wherein 
?P? move directly followed by the ?R? move, 
which tends to be a bad move structure. In sum, 
the move progression generally follows the 
sequence of "B-P-M-R-C". 
 
Table 2: The score of bi-gram move sequence 
(Note that ?$? denotes the beginning or the 
ending of a given abstract.) 
 
Finally, we synchronize move sequence and 
one-move-per-collocation probabilities to train a 
language model to automatically learn the 
relationship between those extracted linguistic 
features based on a large number of unlabeled 
data. Meanwhile, we set some parameters of the 
proposed model, such as, the threshold of the 
number of collocation occurring in a given 
abstract, the weight of move sequence and 
collocation and smoothing. Based on these 
parameters, we implement the Hidden Markov 
Model (HMM). The algorithm is described as the 
following: 
1 1 1 1 1( ,...., ) ( ) ( | ) ( | ) ( | )n i i i ip s s p t p s t p t t p s t?= ?  
The moves ti take out of a value from a set of 
possible moves M={m1, m2, ?., mk} (The 
following parameters ?1 and ?2 will be 
determined based on some heuristics). 
( | )i i ip S t m=  
= ?1 if Si contains a collocation in MTCj 
 i j=  
= ?2 if Si contains a collocation in MTCj 
 but i j?  
= 
1
k
 if Si does not contain a collocation MTCj 
The optimal move sequence t* is 
1 2
1 2 1
, ,...,
( *, *,..., *) ( ,..., | ,..., )arg max
n
n n i n
t t t
t t t p s s t t=  
In summary, at the beginning of training time, 
we use a few human move-tagged sentences as 
seed data. Then, collocation-to-move and move-
to-move probabilities are employed to build the 
HMM. This probabilistic model derived at the 
training stage will be applied at run time. 
4 Evaluation 
In terms of the training data, we retrieved 
abstracts from the search engine, Citeseer; a 
corpus of 20,306 abstracts (95,960 sentences) 
was generated. Also 106 abstracts composed of 
709 sentences were manually move-tagged by 
four informants. Meanwhile, we extracted 72,708 
collocation types and manually tagged 317 
collocations with moves.  
At run time, 115 abstracts containing 684 
sentences were prepared to be the training data. 
We then used our proposed HMM to perform 
some experimentation with the different values 
of parameters: the frequency of collocation types, 
the number of sentences with collocation in each 
abstract, move sequence score and collocation 
score.  
4.1 Performance of CARE 
We investigated how well the HMM model 
performed the task of automatic move tagging 
under different values of parameters. The 
parameters involved included the weight of 
transitional probability function, the number of 
sentences in an abstract, the minimal number of 
instance for the applicable collocations. Figure 2 
indicates the best precision of 80.54% when 627 
sentences were qualified with the set of various 
Move ti Move ti+1 - log P (ti+1|ti) 
$ B 0.7802 
$ P 0.6131 
B B 0.9029 
B M 3.6109 
B P 0.5664 
C $ 0.0000 
M $ 4.4998 
M C 1.9349 
M M 0.7386 
M R 1.0033 
P M 0.4055 
P P 1.1431 
P R 4.2341 
R $ 0.9410 
R C 0.8232 
R R 1.7677 
43
parameters, including 0.7 as the weight of 
transitional probability function and a frequency 
threshold of 18 for a collocation to be applicable, 
and the minimally two sentences containing an 
applicable collocation. Although it is important 
to have many collocations, it is crucial that we 
set an appropriate frequency threshold of 
collocation so as not to include unreliable 
collocation and lower the precision rate. 
 
Figure2: The results of tagging performance with 
different setting of weight and threshold for 
applicable collocations (Note that C_T denotes 
the frequency threshold of collocation) 
5 System Interface 
The goal of the CARE System is to allow a 
learner to look for instances of sentences labeled 
with moves. For this purpose, the system is 
developed with three text boxes for learners to 
enter queries in English (as shown in Figure3.): 
? Single word query (i.e. directly input one 
word to query)  
? Multi-word query (i.e. enter the result 
show to find citations that contain the 
three words, ?the?, ?paper? and ?show? 
and all the derivatives) 
? Corpus selection (i.e. learners can focus on 
a corpus in a specific domain)  
Once a query is submitted, CARE displays the 
results in returned Web pages. Each result 
consists of a sentence with its move annotation. 
The words matching the query are highlighted. 
Figure 3: The sample of searching result with the 
phrase ?the result show? 
6 Conclusion 
In this paper, we have presented a method for 
computational analysis of move structures in 
RAs' abstracts and addressed its pedagogical 
applications. The method involves learning the 
inter-move relationships, and some labeling rules 
we proposed. We used a large number of 
abstracts automatically acquired from the Web 
for training, and exploited the HMM to tag 
sentences with the move of a given abstract. 
Evaluation shows that the proposed method 
outperforms previous work with higher precision. 
Using the processed result, we built a prototype 
concordance, CARE, enriched with words, 
phrases and moves. It is expected that NNS can 
benefit from such a system in learning how to 
write an abstract for a research article. 
References 
Anthony, L. and Lashkia, G. V. 2003. Mover: A 
machine learning tool to assist in the reading and 
writing of technical papers. IEEE Trans. Prof. 
Communication, 46:185-193.  
American National Standards Institute. 1979. 
American national standard for writing abstracts. 
ANSI Z39, 14-1979. New York: Author.  
Jian, J. Y., Chang, Y. C., and Chang, J. S. 2004. 
TANGO: Bilingual Collocational Concordancer, 
Post & demo in ACL 2004, Barcelona. 
Salager-Meyer, F. S. 1992. A text-type and move 
analysis study of verb tense and modality 
distribution in medical English abstracts. English 
for Specific Purposes, 11:93-113.  
Swales, J.M. 1981. Aspects of article introductions. 
Birmingham, UK: The University of Aston, 
Language Studies Unit.   
Swales, J.M. 1990. Genre analysis: English in 
Academic and Research Settings. Cambridge 
University Press.  
44
Proceedings of the Sixth Workshop on Innovative Use of NLP for Building Educational Applications, pages 96?104,
Portland, Oregon, 24 June 2011. c?2011 Association for Computational Linguistics
GRASP: Grammar- and Syntax-based Pattern-Finder in CALL 
 
 
Chung-Chi Huang*  Mei-Hua Chen* Shih-Ting Huang+  Hsien-Chin Liou**  Jason S. Chang+ 
  
* Institute of Information Systems and Applications, NTHU, HsinChu, Taiwan, R.O.C. 300 
+ Department of Computer Science, NTHU, HsinChu, Taiwan, R.O.C. 300 
**  Department of Foreign Languages and Literature, NTHU, HsinChu, Taiwan, R.O.C. 300 
{u901571,chen.meihua,koromiko1104,hsienchin,jason.jschang}gmail.com 
 
 
 
 
 
 
Abstract 
We introduce a method for learning to 
describe the attendant contexts of a given 
query for language learning. In our 
approach, we display phraseological 
information in the form of a summary of 
general patterns as well as lexical bundles 
anchored at the query. The method 
involves syntactical analyses and inverted 
file construction. At run-time, grammatical 
constructions and their lexical 
instantiations characterizing the usage of 
the given query are generated and 
displayed, aimed at improving learners? 
deep vocabulary knowledge. We present a 
prototype system, GRASP, that applies the 
proposed method for enhanced collocation 
learning. Preliminary experiments show 
that language learners benefit more from 
GRASP than conventional dictionary 
lookup. In addition, the information 
produced by GRASP is potentially useful 
information for automatic or manual 
editing process. 
1 Introduction 
Many learners submit word or phrase queries (e.g., 
?role?) to language learning sites on the Web to 
get usage information every day, and an increasing 
number of services on the Web specifically target 
such queries. Language learning tools such as 
concordancers typically accept single-word queries 
and respond with example sentences containing the 
words. There are also collocation reference tools 
such as Sketch Engine and TANGO that provide 
co-occurring words for the query word. Another 
collocation tool, JustTheWord further organizes 
and displays collocation clusters. 
Learners may want to submit phrase queries 
(fixed or rigid collocaions) to learn further how to 
use the phrase in context, or in other words, to 
acquire the knowledge on the attendant 
phraseology of the query. These queries could be 
answered more appropriately if the tool accepted 
long queries and returned a concise summary of 
their surrounding contexts. 
Consider the query ?play role?. The best 
responses for this query are probably not just 
example sentences, but rather the phraseological 
tendencies described grammatically or lexically. A 
good response of such a summary might contain  
patterns such as ?play Det Adj role? (as in ?play an 
important role?) and ?play ~ role in V-ing? (as in 
?play ~ role in shaping ??). Intuitively, by 
exploiting simple part-of-speech analysis, we can 
derive such patterns, inspired by the grammatical 
theory of Pattern Grammar 1  in order to provide 
more information on demand beyond what is given 
in a grammar book. 
We present a system, GRASP, that provide a 
usage summary of the contexts of the query in the 
form of patterns and frequent lexical bundles. Such 
rich information is expected to help learners and 
lexicographers grasp the essence of word usages. 
An example GRASP response for the query ?play 
                                                           
1
 Please refer to (Hunston and Francis, 2000). 
96
role? is shown in Figure 1. GRASP has retrieved 
the sentences containing the query in a reference 
corpus. GRASP constructs these query-to-sentence 
index in the preparation stage (Section 3). 
 
Figure 1. An example GRASP search for ?play role?. 
 
At run-time, GRASP starts with a search query 
(e.g., ?play role?) submitted by the user. GRASP 
then retrieves example sentences and generates a 
summary of representative contexts, using patterns 
(e.g., ?play ~ role in V-ing?) and lexical bundles 
(e.g., ?play ~ role in shaping. In our 
implementation, GRASP also returns the 
translations and the example sentences of the 
lexical instances, so the learner can use their 
knowledge of native language to enhance the 
learning process. 
2 Related Work 
Computer-assisted language learning (CALL) has 
been an area of active research. Recently, more and 
more research based on natural language 
processing techniques has been done to help 
language learners. In our work, we introduce a 
language learning environment, where summarized 
usage information are provided, including how 
function words and verb forms are used in 
combination with the query. These usage notes 
often help contrast the common sources of error in 
learners? writing (Nicholls, 1999). In our pilot 
teaching experiment, we found learners have 
problems using articles and prepositions correctly 
in sentence composition (as high as 80% of the 
articles and 60% of the prepositions were used 
incorrectly), and GRASP is exactly aimed at 
helping ESL or EFL learners in that area. 
Until recently, collocations and usage 
information are compiled mostly manually 
(Benson et al, 1986). With the accessibility to 
large-scale corpora and powerful computers, it has 
become common place to compile a list of 
collocations automatically (Smadja, 1993). In 
addition, there are many collocation checkers 
developed to help non-native language learners 
(Chang et al, 2008), or learners of English for 
academic purposes (Durrant, 2009). 
Recently, automatic generation of collocations 
for computational lexicography and online 
language learning has drawn much attention. 
Sketch Engine (Kilgarriff et al, 2004) summarizes 
a word?s grammatical and collocation behavior, 
while JustTheWord clusters the co-occurring 
words of single-word queries and TANGO (Jian et 
al., 2004) accommodates cross-lingual collocation 
searches. Moreover, Cheng et al (2006) describe 
how to retrieve mutually expected words using 
concgrams. In contrast, GRASP, going one step 
further, automatically computes and displays the 
information that reveals the regularities of the 
contexts of user queries in terms of grammar 
patterns. 
Recent work has been done on incorporating 
word class information into the analyses of 
phraseological tendencies. Stubbs (2004) 
introduces phrase-frames, which are based on 
lexical ngrams with variable slots, while Wible et 
al. (2010) describe a database called StringNet, 
with lexico-syntactic patterns. Their methods of 
using word class information are similar in spirit to 
our work. The main differences are that our 
patterns is anchored with query words directly and 
generalizes query?s contexts via parts-of-speech, 
and that we present the query?s usage summary in 
Search query: 
Mapping query words to (position, sentence) pairs: 
?play? occurs in (10,77), (4,90), (6,102), ?, and so on. 
?role? occurs in (7,90), (12,122), (6,167), ?, and so on. 
A. In-between pattern grammar: 
   Distance 3 (1624): 
play DT JJ role (1364): 
e.g., ?play an important role? (259), ?play a major role? (168), ? 
play DT VBG role (123): 
e.g., ?play a leading role? (75), ?play a supporting role? (5), ? 
play DT JJR role (40): 
e.g., ?play a greater role? (17), ?play a larger role? (8), ? 
   Distance 2 (480): 
play DT role (63): 
e.g., ?play a role? (197), ?play the role? (123), ? 
play JJ role (63): 
e.g., ?play important role? (15), ?play different role? (6), ? 
   Distance 1 (6): 
play role (6) 
B. Subsequent pattern grammar: 
play ~ role IN(in) DT (707): 
e.g., ?play ~ role in the? (520), ?play ~ role in this? (24), ? 
play ~ role IN(in) VBG (407): 
e.g., ?play ~ role in shaping? (22), ? 
play ~ role IN(in) NN (166): 
e.g., ?play ~ role in society? (7), ?play ~ role in relation? (5), ? 
C. Precedent pattern grammar: 
NN MD play ~ role (83): 
e.g., ?communication will play ~ role ? (2), ? 
JJ NNS play ~ role (69): 
e.g., ?voluntary groups play ~ role? (2), ? 
Type your search query, and push GRASP! 
97
terms of function words as well as content word 
form (e.g., ?play ~ role in V-ing?), as well as 
elastic lexical bundles (e.g., ?play ~ role in 
shaping?). Additionally, we also use semantic 
codes (e.g., PERSON) to provide more information 
in a way similar what is provided in learner 
dictionaries. 
3 The GRASP System 
3.1 Problem Statement 
We focus on constructing a usage summary likely 
to explain the contexts of a given linguistic search. 
The usage summary, consisting of the query?s 
predominant attendant phraseology ranging from 
pattern grammar to lexical phrases, is then returned 
as the output of the system. The returned summary, 
or a set of patterns pivoted with both content and 
function words, can be used for learners? benefits 
directly, or passed on to an error detection and 
correction system (e.g., (Tsao and Wible, 2009) 
and some modules in (Gamon et al, 2009) as rules. 
Therefore, our goal is to return a reasonable-sized 
set of lexical and grammatical patterns 
characterizing the contexts of the query. We now 
formally state the problem that we are addressing. 
Problem Statement: We are given a reference 
corpus C from a wide range of sources and a 
learner search query Q. Our goal is to construct a 
summary of word usages based on C that is likely 
to represent the lexical or grammatical preferences 
on Q?s contexts. For this, we transform the words 
in Q into sets of (word position, sentence record) 
pairs such that the context information, whether 
lexically- or grammatical-oriented, of the querying 
words is likely to be acquired efficiently. 
In the rest of this section, we describe our 
solution to this problem. First, we define a strategy 
for preprocessing our reference corpus (Section 
3.2). Then, we show how GRASP generates 
contextual patterns, comprising the usage summary, 
at run-time (Section 3.3). 
3.2 Corpus Preprocessing 
We attempt to find the word-to-sentence mappings 
and the syntactic counterparts of the L1 sentences 
expected to speed up run-time pattern generation. 
Our preprocessing procedure has two stages. 
Lemmatizing and PoS Tagging. In the first stage, 
we lemmatize each sentence in the reference 
corpus C and generate its most probable POS tag 
sequence. The goal of lemmatization is to reduce 
the impact of morphology on statistical analyses 
while that of POS tagging is to provide a way to 
grammatically describe and generalize the 
contexts/usages of a linguistic query. Actually, 
using POS tags is quite natural: they are often used 
for general description in grammar books, such as 
one?s (i.e., possessive pronoun) in the phrase 
?make up one?s mind?, oneself (i.e., reflexive 
pronoun) in ?enjoy oneself very much?, 
superlative_adjective in ?the most 
superlative_adjective?, NN (i.e., noun) and VB (i.e., 
base form of a verb) in ?insist/suggest/demand that 
NN VB? and so on. 
Constructing Inverted Files. In the second stage, 
we build up inverted files of the lemmas in C for 
quick run-time search. For each lemma, we record 
the sentences and positions in which it occurs. 
Additionally, its corresponding surface word and 
POS tag are kept for run-time pattern grammar 
generation. 
 
Figure 2. Generating pattern grammar and usage 
summary at run-time. 
procedure GRASPusageSummaryBuilding(query,proximity,N,C) 
(1)  queries=queryReformulation(query) 
(2)  GRASPresponses= ?  
for each query in queries 
(3)    interInvList=findInvertedFile(w1 in query) 
for each lemma wi in query except for w1 
(4)      InvList=findInvertedFile(wi) 
//AND operation on interInvList and InvList 
(5a)    newInterInvList= ? ; i=1; j=1 
(5b)    while i<=length(interInvList) and j<=lengh(InvList) 
(5c)       if interInvList[i].SentNo==InvList[j].SentNo 
(5d)         if withinProximity(interInvList[i]. 
wordPosi,InvList[j].wordPosi,proximity) 
(5e)   Insert(newInterInvList, interInvList[i],InvList[j]) 
else if interInvList[i].wordPosi<InvList[j].wordPosi 
(5f)   i++ 
else //interInvList[i].wordPosi>InvList[j].wordPosi 
(5g)   j++ 
else if interInvList[i].SentNo<InvList[j].SentNo 
(5h)          i++ 
else //interInvList[i].SentNo>InvList[j].SentNo 
(5i)           j++ 
(5j)     interInvList=newInterInvList 
//construction of GRASP usage summary for this query 
(6)    Usage= ?  
for each element in interInvList 
(7)       Usage+={PatternGrammarGeneration(query,element,C)} 
(8a)  Sort patterns and their instances in Usage in descending order 
of frequency 
(8b)  GRASPresponse=the N patterns and instances in Usage with 
highest frequency 
(9)    append GRASPresponse to GRASPresponses 
(10) return GRASPresponses 
98
3.3 Run-Time Usage Summary Construction 
Once the word-to-sentence mappings and syntactic 
analyses are obtained, GRASP generates the usage 
summary of a query using the procedure in Figure 
2. 
In Step (1) we reformulate the user query into 
new ones, queries, if necessary. The first type of 
query reformulation concerns the language used in 
query. If it is not in the same language as C, we 
translate query and append the translations to 
queries as if they were submitted by the user. The 
second concerns the length of the query. Since 
single words may be ambiguous in senses and 
contexts or grammar patterns are closely associated 
with words? meanings (Hunston and Francis, 2000), 
we transform single-word queries into their 
collocations, particularly focusing on one word 
sense (Yarowsky, 1995), as stepping stones to 
GRASP patterns. Notice that, in implementation, 
users may be allowed to choose their own 
interested translation or collocation of the query 
for usage learning. The prototypes for first-
language (i.e., Chinese) queries and English 
queries of any length are at A2 and B3 respectively. 
The goal of cross-lingual GRASP is to assist EFL 
users even when they do not know the words of 
their searches and to avoid incorrect queries 
largely because of miscollocation, misapplication, 
and misgeneralization. 
Afterwards, we initialize GRASPresponses to 
collect usage summaries for queries (Step (2)) and 
leverage inverted files to extract and generate each 
query?s syntax-based contexts. In Step (3) we prep 
interInvList for the intersected inverted files of the 
lemmas in query. For each lemma wi within, we 
first obtain its inverted file, InvList (Step (4)) and 
perform an AND operation on interInvList 
(intersected results from previous iteration) and 
InvList (Step (5a) to (5j)4), defined as follows. 
First, we enumerate the inverted lists (Step (5b)) 
after the initialization of their indices i and j and 
temporary resulting intersection newInterInvList 
(Step (5a)). Second, we incorporate a new instance 
of (position, sentence), based on interInvList[i] and 
InvList[j], into newInterInvList (Step (5e)) if the 
sentence records of the indexed list elements are 
the same (Step (5c)) and the distance between their 
                                                           
2
 http://140.114.214.80/theSite/bGRASP_v552/ 
3
 http://140.114.214.80/theSite/GRASP_v552/ 
4
 These steps only hold for sorted inverted files. 
words are within proximity (Step (5d)). Otherwise, 
i and j are moved accordingly. To accommodate 
the contexts of queries? positional variants (e.g., 
?role to play? and ?role ~ play by? for the query 
?play role?), Step (5d) considers the absolute 
distance. Finally, interInvList is set for the next 
AND iteration (Step (5j)). 
Once we obtain the sentences containing query, 
we construct its context summary as below. For 
each element, taking the form ([wordPosi(w1), ?, 
wordPosi(wn)], sentence record) denoting the 
positions of query?s lemmas in the sentence, we 
generate pattern grammar involving replacing 
words in the sentence with POS tags and words in 
wordPosi(wi) with lemmas, and extracting fixed-
window 5  segments surrounding query from the 
transformed sentence. The result is a set of 
grammatical patterns with counts. Their lexical 
realizations also retrieved and displayed. 
The procedure finally generates top N 
predominant syntactic patterns and their N most 
frequent lexical phrases as output (Step (8)). The 
usage summaries GRASP returns are aimed to 
accelerate EFL learners? language understanding 
and learning and lexicographers? word usage 
navigation. To acquire more semantic-oriented 
patterns, we further exploit WordNet and majority 
voting to categorize words, deriving the patterns 
like ?provide PERSON with.? 
4 Experimental Results 
GRASP was designed to generate usage 
summarization of a query for language learning. 
As such, GRASP will be evaluated over CALL. In 
this section, we first present the setting of GRASP 
(Section 4.1) and report the results of different 
consulting systems on language learning in Section 
4.2. 
4.1 Experimental Setting 
We used British National Corpus (BNC) as our 
underlying reference corpus C. It is a British 
English text collection. We exploited GENIA 
tagger to obtain the lemmas and POS tags of C?s 
sentences. After lemmatizing and syntactic 
analyses, all sentences in BNC were used to build 
up inverted files and used as examples for 
grammar pattern extraction. 
                                                           
5
 Inspired by (Gamon and Leacock, 2010). 
99
English (E) sentence with corresponding Chinese (C) translation answer to 1st blank  answer to 2nd blank 
C: ????????????? 
E: Environmental protection has ___ impact ___. 
a profound on the Earth 
C: ????????????? 
E: The real estate agent ___ record profit ___. 
made a on house selling 
C: ?????????????? 
E: They plan to release their new album in ___ future 
the near none 
C: ???????????? 
E: He waited for her for a long time in ___ attempt ___ again. 
an to see her 
 
4.2 Results of Constrained Experiments 
In our experiments, we showed GRASP6  to two 
classes of Chinese EFL (first-year) college students. 
32 and 86 students participated, and were trained 
to use GRASP and instructed to perform a sentence 
translation/composition task, made up of pretest 
and posttest. In (30-minute) pretest, participants 
were to complete 15 English sentences with 
Chinese translations as hints, while, in (20-minute) 
posttest, after spending 20 minutes familiarizing 
word usages of the test candidates from us by 
consulting traditional tools or GRASP, participants 
were also asked to complete the same English 
sentences. We refer to the experiments as 
constrained ones since the test items in pre- and 
post-test are the same except for their order. A 
more sophisticated testing environment, however, 
are to be designed. 
Each test item contains one to two blanks as 
shown in the above table. In the table, the first item 
is supposed to test learners? knowledge on the 
adjective and prepositional collocate of ?have 
impact? while the second test the verb collocate 
make, subsequent preposition on, and preceding 
article a of ?record profit?. On the other hand, the 
third tests the ability to produce the adjective 
enrichment of ?in future?, and the fourth the in-
between article a or possessive his and the 
following infinitive of ?in attempt?. Note that as 
existing collocation reference tools retrieve and 
display collocates, they typically ignore function 
words like articles and determiners, which happen 
to be closely related to frequent errors made by the 
learners (Nicholls, 1999), and fail to provide an 
overall picture of word usages. In contrast, GRASP 
attempts to show the overall picture with 
appropriate function words and word forms. 
We selected 20 collocations and phrases 7 
manually from 100 most frequent collocations in 
                                                           
6
 http://koromiko.cs.nthu.edu.tw/grasp/ 
7
 Include the 15 test items. 
BNC whose MI values exceed 2.2 and used them 
as the target for learning between the pretest and 
posttest. To evaluate GRASP, half of the 
participants were instructed to use GRASP for 
learning and the other half used traditional tools 
such as online dictionaries or machine translation 
systems (i.e., Google Translate and Yahoo! Babel 
Fish). We summarize the performance of our 
participants on pre- and post-test in Table 1 where 
GRASP denotes the experimental group and TRAD 
the control group. 
 
 class 1 class 2 combined 
 pretest posttest  pretest  posttest  pretest posttest 
GRASP 26.4 41.9 43.6 58.4 38.9 53.9 
TRAD 27.1 32.7 43.8 53.4 39.9 48.6 
Table 1. The performance (%) on pre- and post-test. 
 
We observe in Table 1 that (1) the partition of 
the classes was quite random (the difference 
between GRASP and TRAD was insignificant 
under pretest); (2) GRASP summaries of words? 
contexts were more helpful in language learning 
(across class 1, class 2 and combined). Specifically, 
under the column of the 1st class, GRASP helped to 
boost students? achievements by 15.5%, almost 
tripled (15.5 vs. 5.6) compared to the gain using 
TRAD; (3) the effectiveness of GRASP in language 
learning do not confine to students at a certain 
level. Encouragingly, both high- and low-
achieving students benefited from GRASP if we 
think of students in class 2 and those in class 1 as 
the high and the low respectively (due to the 
performance difference on pretests). 
We have analyzed some participants? answers 
and found that GRASP helped to reduce learners? 
article and preposition errors by 28% and 8%, 
comparing to much smaller error reduction rate 7% 
and 2% observed in TRAD group. Additionally, an 
experiment where Chinese EFL students were 
asked to perform the same task but using GRASP 
as well as GRASP with translation information8 
                                                           
8
 http://koromiko.cs.nthu.edu.tw/grasp/ch 
100
was conducted. We observed that with Chinese 
translation there was an additional 5% increase in 
students? test performance. This suggests to some 
extent learners still depend on their first languages 
in learning and first-language information may 
serve as another quick navigation index even when 
English GRASP is presented. 
Overall, we are modest to say that (in the 
constrained experiments) GRASP summarized 
general-to-specific usages, contexts, or phrase-
ologies of words are quite effective in assisting 
learners in collocation and phrase learning. 
5 Applying GRASP to Error Correction 
To demonstrate the viability of GRASP-retrieved 
lexicalized grammar patterns (e.g., ?play ~ role In 
V-ING? and ?look forward to V-ING?) in error 
detection and correction, we incorporate them into 
an extended Levenshtein algorithm (1966) to 
provide broad-coverage sentence-level grammat-
ical edits (involving substitution, deletion, and 
insertion) to inappropriate word usages in learner 
text. 
Previously, a number of interesting rule-based 
error detection/correction systems have been 
proposed for some specific error types such as 
article and preposition error (e.g., (Uria et al, 
2009), (Lee et al, 2009), and some modules in 
(Gamon et al, 2009)). Statistical approaches, 
supervised or unsupervised, to grammar checking 
have become the recent trend. For example, 
unsupervised systems of (Chodorow and Leacock, 
2000) and (Tsao and Wible, 2009) leverage word 
distributions in general and/or word-specific 
corpus for detecting erroneous usages while 
(Hermet et al, 2008) and (Gamon and Leacock, 
2010) use Web as a corpus. On the other hand, 
supervised models, typically treating error 
detection/correction as a classification problem, 
utilize the training of well-formed texts ((De Felice 
and Pulman, 2008) and (Tetreault et al, 2010)), 
learner texts, or both pairwisely (Brockett et al, 
2006). Moreover, (Sun et al, 2007) describes a 
way to construct a supervised error detection 
system trained on well-formed and learner texts 
neither pairwise nor error tagged. 
In contrast to the previous work in grammar 
checking, our pattern grammar rules are 
automatically inferred from a general corpus (as 
described in Section 3) and helpful for correcting 
errors resulting from the others (e.g., ?to close? in 
?play ~ role to close?), our pattern grammar 
lexicalizes on both content and function words and 
lexical items within may be contiguous (e.g., ?look 
forward to V-ING PRP?) or non-contiguous (e.g., 
?play ~ role In V-ING?), and, with word class 
(POS) information, error correction or grammatical 
suggestion is provided at sentence level. 
5.1 Error Correcting Process 
Figure 3 shows how we check grammaticality and 
provide suggestions for a given text with accurate 
spelling. 
 
 
Figure 3. Procedure of grammar suggestion/correction. 
 
In Step (1), we initiate a set Suggestions to 
collect grammar suggestions to the user text T 
according to a bank of patterns 
PatternGrammarBank, i.e., a collection of 
summaries of grammatical usages (e.g., ?play ~ 
role In V-ING?) of queries (e.g., ?play role?) 
submitted to GRASP. Since we focus on grammar 
checking at sentence level, T is heuristically split 
(Step (2)). 
For each sentence, we extract user-proposed 
word usages (Step (3)), that is, the user 
grammatical contexts of ngram and collocation 
sequences. Take for example the (ungrammatical) 
sentences and their corresponding POS sequences 
?he/PRP play/VBP an/DT important/JJ roles/NNS 
to/TO close/VB this/DT deals/NNS? and ?he/PRP 
looks/VBZ forward/RB to/TO hear/VB you/PRP?. 
Ngram contexts include ?he VBP DT?, ?play an JJ 
NNS?, ?this NNS? for the first sentence and ?look 
forward to VB PRP? and ?look forward to hear 
PRP? for the second. And collocation contexts for 
procedure GrammarChecking(T,PatternGrammarBank) 
(1) Suggestions=??//candidate suggestions 
(2) sentences=sentenceSplitting(T) 
for each sentence in sentences 
(3)   userProposedUsages=extractUsage(sentence) 
for each userUsage in userProposedUsages 
(4)     patGram=findPatternGrammar(userUsage.lexemes, 
PatternGrammarBank) 
(5)     minEditedCost=SystemMax; minEditedSug=?? 
for each pattern in patGram 
(6)        cost=extendedLevenshtein(userUsage,pattern) 
if cost<minEditedCost 
(7)            minEditedCost=cost; minEditedSug=pattern 
if minEditedCost>0 
(8)       append (userUsage,minEditedSug) to Suggestions 
(9) Return Suggestions 
101
the first sentence are ?play ~ role to VERB? and 
?close ~ deal .? 
For each userUsage in the sentence (e.g., ?play 
~ role TO VB? and ?look forward to hear PRP?), 
we first acquire the pattern grammar of its lexemes 
(e.g., ?play role? and ?look forward to hear?) such 
as ?play ~ role in V-ing? and ?look forward to 
hear from? in Step (4), and we compare the user-
proposed usage against the corresponding 
predominant, most likely more proper, ones (from 
Step (5) to (7)). We leverage an extended 
Levenshtein?s algorithm in Figure 4 for usage 
comparison, i.e. error detection and correction, 
after setting up minEditedCost and minEditedSug 
for the minimum-cost edit from alleged error usage 
into appropriate one (Step (5)). 
 
 
Figure 4. Extended Levenshtein algorithm for correction. 
 
In Step (1) of the algorithm in Figure 4 we 
allocate and initialize costArray to gather the 
dynamic programming based cost to transform 
userUsage into a specific pattern. Afterwards, the 
algorithm defines the cost of performing 
substitution (Step (2)), deletion (Step (3)) and 
insertion (Step (4)) at i-indexed userUsage and j-
indexed pattern. If the entries userUsage[i] and 
pattern[j] are equal literally (e.g., ?VB? and ?VB?) 
or grammatically (e.g., ?DT? and ?PRP$?9), no edit 
                                                           
9
 ONE?S denotes possessives. 
is needed, hence, no cost (Step (2a)). On the other 
hand, since learners tend to select wrong word 
form and preposition, we make less the cost of the 
substitution of the same word group, say from 
?VERB? to ?V-ing?, ?TO? to ?In? and ?In? to 
?IN(on)? (Step (2b)) compared to a total edit (Step 
(2c)). In addition to the conventional deletion and 
insertion (Step (3b) and (4b) respectively), we look 
ahead to the elements userUsage[i+1] and 
pattern[j+1] considering the fact that ?with or 
without preposition? and ?transitive or intransitive 
verb? often puzzles EFL learners (Step (3a) and 
(4a)). Only a small edit cost is applied if the next 
elements in userUsage and Pattern are ?equal?. In 
Step (6) the extended Levenshtein?s algorithm 
returns the minimum cost to edit userUsage based 
on pattern. 
Once we obtain the costs to transform the 
userUsage into its related frequent patterns, we 
propose the minimum-cost one as its grammatical 
suggestion (Step (8) in Figure 3), if its minimum 
edit cost is greater than zero. Otherwise, the usage 
is considered valid. At last, the gathered 
suggestions Suggestions to T are returned to users 
(Step (9)). Example edits to the user text ?he play 
an important roles to close this deals. he looks 
forward to hear you.? from our working prototype, 
EdIt10, is shown in Figure 5. Note that we exploit 
context checking of collocations to cover longer 
span than ngrams?, and longer ngrams like 
fourgrams and fivegrams to (more or less) help 
semantic checking (or word sense disambiguation). 
For example, ?hear? may be transitive or 
intransitive, but, in the context of ?look forward 
to?, there is strong tendency it is used intransitively 
and follows by ?from?, as EdIt would suggest (see 
Figure 5). 
There are two issues worth mentioning on the 
development of EdIt. First, grammar checkers 
typically have different modules examining 
different types of errors with different priority. In 
our unified framework, we set the priority of 
checking collocations? usages higher than that of 
ngrams?, set the priority of checking longer 
ngrams? usages higher than that of shorter, and we 
do not double check. Alternatively, one may first 
check usages of all sorts and employ majority 
voting to determine the grammaticality of a 
sentence. Second, we further incorporate
                                                           
10
 http://140.114.214.80/theSite/EdIt_demo2/ 
procedure extendedLevenshtein(userUsage,pattern) 
(1) allocate and initialize costArray 
for i in range(len(userUsage)) 
for j in range(len(pattern)) 
//substitution 
if equal(userUsage[i],pattern[j]) 
(2a)       substiCost=costArray[i-1,j-1]+0 
elseif sameWordGroup(userUsage[i],pattern[j]) 
(2b)       substiCost=costArray[i-1,j-1]+0.5 
else 
(2c)       substiCost=costArray[i-1,j-1]+1 
//deletion 
if equal(userUsage[i+1],pattern[j+1]) 
(3a)       delCost=costArray[i-1,j]+smallCost 
else 
(3b)       delCost=costArray[i-1,j]+1 
//insertion 
if equal(userUsage[i+1],pattern[j+1])  
(4a)        insCost=costArray[i,j-1]+smallCost 
else 
(4b)       insCost=costArray[i,j-1]+1 
(5)       costArray[i,j]=min(substiCost,delCost,insCost) 
(6) Return costArray[len(userUsage),len(pattern)] 
102
Erroneous sentence EdIt suggestion ESL Assistant suggestion 
Wrong word form 
? a sunny days ? a sunny NN a sunny day 
every days, I ? every NN every day 
I would said to ? would VB would say 
he play a ? he VBD none 
? should have tell the truth should have VBN should have to tell 
? look forward to see you look forward to VBG none 
? in an attempt to seeing you an attempt to VB none 
? be able to solved this problem able to VB none 
Wrong preposition 
he plays an important role to close ? play ~ role IN(in) none 
he has a vital effect at her. have ~ effect IN(on) effect on her 
it has an effect on reducing ? have ~ effect IN(of) VBG none 
? depend of the scholarship depend IN(on) depend on 
Confusion between intransitive and transitive verb 
he listens the music. missing ?to? after ?listens? missing ?to? after ?listens? 
it affects to his decision. unnecessary ?to? unnecessary ?to? 
I understand about the situation. unnecessary ?about? unnecessary ?about? 
we would like to discuss about this matter. unnecessary ?about? unnecessary ?about? 
Mixture 
she play an important roles to close this deals. she VBD; an JJ NN; 
play ~ role IN(in) VBG; this NN 
play an important role; 
close this deal 
I look forward to hear you. look forward to VBG; 
missing ?from? after ?hear? 
none 
Table 2. Three common score-related error types and their examples with suggestions from EdIt and ESL Assistant. 
 
 
Figure 5. Example EdIt responses to the ungrammatical. 
 
probabilities conditioned on word positions to 
weigh edit costs. For example, the conditional 
probability of ?VERB? being the immediate 
follower of ?look forward to? is virtually zero, but 
the probability of ?V-ing? is around 0.3. 
5.2 Preliminary Results in Error Correction 
We examined three common error types in learner 
text that are highly correlated with essay scores 
(Leacock and Chodorow, 2003; Burstein et al, 
2004), to evaluate EdIt, (see Table 2). In Table 2, 
the results of a state-of-the-art checker, ESL 
Assistant (www.eslassistant.com/), are shown for 
comparison, and information produced by both 
systems are underscored. As indicated, GRASP 
retrieves patterns which are potential useful if 
incorporated into an extension of Levenshtein?s 
algorithm to correct substitution, deletion, and 
insertion errors in learner. 
6 Summary 
We have introduced a new method for producing a 
general-to-specific usage summary of the contexts 
of a linguistic search query aimed at accelerating 
learners? grasp on word usages. We have 
implemented and evaluated the method as applied 
to collocation and phrase learning and grammar 
checking. In the preliminary evaluations we show 
that GRASP is more helpful than traditional 
language learning tools, and that the patterns and 
lexical bundles provided are promising in detecting 
and correcting common types of errors in learner 
writing. 
References 
Morton Benson, Evellyn Benson, and Robert Ilson. 
1986. The BBI Combinatory Dictionary of English: A 
Article: 
Related pattern grammar 
(a) of collocation sequences includes ?play ~ role 
IN(in) NN?, ?play ~ role IN(in) DT?, ?play ~ role 
IN(in) VBG? and so on. 
(b) of ngram sequences includes ?he VBD DT?, ?play 
an JJ NN?, ?this NN?, ?look forward to VBG PRP? 
and ?look forward to hear IN(from) PRP? and so on. 
Grammatical/Usage suggestion: 
For sentence 1: 
(a) use the VBD of ?play?, (b) use the NN of ?roles?, 
(c) use the preposition ?in? and VBG of ?close?, 
instead of ?to close?. (d) use the NN of ?deals? 
For sentence 2: 
(a) insert the preposition ?from? after ?hear?, (b) use 
the ?VBG? of ?hear? 
he play an important roles to close this deals. 
he looks forward to hear you. 
Type your article and push the buttom ?EdIt? ! 
103
guide to word combinations. Philadelphia: John 
Benjamins. 
Chris Brockett, William B. Dolan, and Michael Gamon. 
2006. Correcting ESL errors using phrasal SMT 
techniques. In Proceedings of the ACL, pages 249-
256. 
Jill Burstein, Martin Chodorow, and Claudia Leacock. 
2004. Automated essay evaluation: the criterion 
online writing service. AI Magazine, 25(3): 27-36. 
Yu-Chia Chang, Jason S. Chang, Hao-Jan Chen, and 
Hsien-Chin Liou. 2008. An automatic collocation 
writing assistant for Taiwanese EFL learners: a case 
of corpus-based NLP technology. CALL, 21(3): 283-
299. 
Winnie Cheng, Chris Greaves, and Martin Warren. 2006. 
From n-gram to skipgram to concgram. Corpus 
Linguistics, 11(4): 411-433. 
Martin Chodorow and Claudia Leacock. 2000. An 
unsupervised method for detecting grammatical 
errors. In Proceedings of the NAACL, pages 140-147. 
Rachele De Felice and Stephen G. Pulman. 2008. A 
classifer-based approach to preposition and 
determiner error correction in L2 English. In 
Proceedings of the COLING, pages 169-176. 
Philip Durrant. 2009. Investigating the viability of a 
collocation list for students of English for academic 
purposes. ESP, 28(3): 157-169. 
John R. Firth. 1957. Modes of meaning. In Papers in 
Linguistics. London: Oxford University Press, pages 
190-215. 
Michael Gamon, Claudia Leacock, Chris Brockett, 
William B. Dolan., Jianfeng Gao, Dmitriy Belenko, 
and Alexandre Klementiev. 2009. Using statistical 
techniques and web search to correct ESL errors. 
CALICO, 26(3): 491-511. 
Michael Gamon and Claudia Leacock. 2010. Search 
right and thou shalt find ? using web queries for 
learner error detection. In Proceedings of the NAACL. 
Matthieu Hermet, Alain Desilets, and Stan Szpakowicz. 
2008. Using the web as a linguistic resource to 
automatically correct lexico-syntatic errors. In 
Proceedings of the LREC, pages 874-878. 
Susan Hunston and Gill Francis. 2000. Pattern 
Grammar: A Corpus-Driven Approach to the Lexical 
Grammar of English. Amsterdam: John Benjamins. 
Jia-Yan Jian, Yu-Chia Chang, and Jason S. Chang. 2004. 
TANGO: Bilingual collocational concordancer. In 
ACL Poster. 
Adam Kilgarriff, Pavel Rychly, Pavel Smrz, and David 
Tugwell. 2004. The sketch engine. In Proceedings of 
the EURALEX, pages 105-116. 
Chong Min Lee, Soojeong Eom, and Markus Dickinson. 
2009. Toward analyzing Korean learner particles. In 
CALICO Workshop. 
Claudia Leacock and Martin Chodorow. 2003. 
Automated grammatical error detection. In M.D. 
Shermis and J.C. Burstein, editors, Automated Essay 
Scoring: A Cross-Disciplinary Perspective, pages 
195-207. 
Vladimir I. Levenshtein. 1966. Binary codes capable of 
correcting deletions, insertions and reversals. Soviet 
Physics Doklady, 10, page 707. 
Diane Nicholls. 1999. The Cambridge Learner Corpus ? 
error coding and analysis for writing dictionaries and 
other books for English Learners. 
John M. Sinclair. 1987. The nature of the evidence. In J. 
Sinclair (ed.) Looking Up. Collins: 150-159. 
Frank Smadja. 1993. Retrieving collocations from text: 
Xtract. Computational Linguistics, 19(1): 143-177. 
Michael Stubbs. 2004. At 
http://web.archive.org/web/20070828004603/http://www.u
ni-trier.de/uni/fb2/anglistik/Projekte/stubbs/icame-2004.htm. 
Guihua Sun, Xiaohua Liu, Gao Cong, Ming Zhou, 
Zhongyang Xiong, John Lee, and Chin-Yew Lin. 
2007. Detecting erroneous sentences using 
automatically mined sequential patterns. In 
Proceedings of the ACL, pages 81-88. 
Joel Tetreault, Jennifer Foster, and Martin Chodorow. 
2010. Using parse features for prepositions selection 
and error detection. In Proceedings of the ACL, pages 
353-358. 
Nai-Lung Tsao and David Wible. 2009. A method for 
unsupervised broad-coverage lexical error detection 
and correction. In NAACL Workshop, pages 51-54. 
Larraitz Uria, Bertol Arrieta, Arantza D. De Ilarraza, 
Montse Maritxalar, and Maite Oronoz. 2009. 
Determiner errors in Basque: analysis and automatic 
detection. Procesamiento del Lenguaje Natural, 
pages 41-48. 
David Wible and Nai-Lung Tsao. 2010. StringNet as a 
computational resource for discovering and 
investigating linguistic constructions. In NAACL 
Workshop, pages 25-31. 
David Yarowsky. 1995. Unsupervised word sense 
disambiguation rivaling supervised methods. In 
Proceedings of the ACL, pages 189-196. 
104
The 7th Workshop on the Innovative Use of NLP for Building Educational Applications, pages 80?85,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
 
 
PREFER: Using a Graph-Based Approach to Generate Paraphrases for 
Language Learning 
  
Mei-Hua Chen*, Shih-Ting Huang+, Chung-Chi Huang*, Hsien-Chin Liou**, Jason S. Chang+ 
  * Institute of Information Systems and Applications 
+ Department of Computer Science 
**  Department of Foreign Languages and Literature 
National Tsing Hua University  
HsinChu, Taiwan, R.O.C. 30013 
{chen.meihua,koromiko1104,u901571,hsienchin,jason.jschang}@gmail.com 
  
  
Abstract 
Paraphrasing is an important aspect of language 
competence; however, EFL learners have long 
had difficulty paraphrasing in their writing 
owing to their limited language proficiency. 
Therefore, automatic paraphrase suggestion 
systems can be useful for writers. In this paper, 
we present PREFER1, a paraphrase reference 
tool for helping language learners improve their 
writing skills. In this paper, we attempt to 
transform the paraphrase generation problem 
into a graphical problem in which the phrases 
are treated as nodes and translation similarities 
as edges. We adopt the PageRank algorithm to 
rank and filter the paraphrases generated by the 
pivot-based paraphrase generation method. We 
manually evaluate the performance of our 
method and assess the effectiveness of 
PREFER in language learning. The results 
show that our method successfully preserves 
both the semantic meaning and syntactic 
structure of the query phrase. Moreover, the 
students? writing performance improve most 
with the assistance of PREFER.  
1. Introduction 
Paraphrasing, or restating information using 
different words, is an essential part of productive 
language competence (Fuchs, 1980; Mel??uk, 1992; 
Martinot, 2003). However, EFL learners have 
difficulty paraphrasing in their writing partly 
                                                 
1 http://140.114.89.231/PREFER 
because of their insufficient lexical knowledge 
(Abasi et al 2006; Chandrasoma et al 2004). If 
they are provided with direct and substantial 
support while writing, they may be able to express 
their thoughts more fluently. Unfortunately, few 
paraphrase reference tools have been developed to 
provide instant assistance to learners in their 
writing process. In the light of the pressing need 
for paraphrase reference tools, we develop 
PREFER, a paraphrasing assistant system to help 
EFL learners vary their expression during writing.  
Over the past decade, paraphrasing techniques 
have played an important role in many areas of 
Natural Language Processing, such as machine 
translation, and question answering. However, very 
few studies have been conducted concerning the 
application of automatic paraphrase generation 
techniques in language learning and teaching.  
In this paper, we treat the paraphrase generation 
problem as a graph-related problem. We adopt the 
PageRank algorithm (Page et al, 1999) to generate 
paraphrases based on the assumption that a page 
with more incoming links is likely to receive a 
higher rank. Meanwhile, a page which is linked by 
a higher ranked page should transitively be ranked 
higher. We take advantage of transitivity of 
relevance to rank and filter the paraphrases 
generated by the pivot-based method (i.e., phrase 
are treated as paraphrases if they share the same 
translations) of Bannard and Callison-Burch 
(2005).  
The advantage of the pivot approach is that the 
generated paraphrases are exactly semantically 
equivalent to the query phrase. However, its 
80
  
quality of the paraphrases highly correlates with 
that of the techniques of bilingual alignment. To 
overcome such limitation, we use the PageRank 
algorithm to refine the generated paraphrases. In 
other words, we leverage the PageRank algorithm 
to find more relevant paraphrases that preserve 
both meaning and grammaticality for language 
learners. The results of a manual evaluation and a 
system assessment show that our approach and 
system perform well. 
2. Related Work 
A number of studies have investigated EFL leaners? 
paraphrase competence. For example, Campbell 
(1987) reveals that language proficiency 
significantly affects paraphrasing competence. 
McInnis (2009) reports that paraphrasing task is 
more difficult for L2 students than that for L1 
students. According to Milicevic (2011), L2 
learners propose less valid paraphrases than native 
speakers. These findings indicate that EFL students 
have problems in paraphrasing. In view of this, we 
develop PREFER, a paraphrase reference tool, for 
helping English learners with their writing. 
Paraphrase generation, on the other hand, has 
been an area of active research and the related 
work has been thoroughly surveyed in 
Androutsopoulos and Malakasiotis (2010) as well 
as in Madnani and Dorr (2010). In the rest of this 
section, we focus on reviewing the methods related 
to our work.  
One prominent approach to paraphrase 
generation is based on bilingual parallel corpora. 
For example, Bannard and Callison-Burch (2005) 
propose the pivot approach to generate phrasal 
paraphrases from an English-German parallel 
corpus. With the advantage of its parallel and 
bilingual natures of such a corpus, the output 
paraphrases do preserve semantic similarity. 
Callison-Burch (2008) further places syntactic 
constraints on generated paraphrases to improve 
the quality of the paraphrases. In this paper, we 
generate paraphrases adopting the pivot-based 
method proposed by Bannard and Callison-Burch 
(2005) in the first round. Then we use a 
graph-based approach to further ensure paraphrase 
candidates preserve both meaning and 
grammaticality. 
In a study more closely related to our work, 
Kok and Brockett (2010) take a graphical view of 
the pivot-based approach. They propose the Hitting 
Time Paraphrase algorithm (HTP) to measure 
similarities between phrases. The smaller the 
number of steps a random walker goes from one 
node to the other, the more likely these two nodes 
are paraphrases. The main difference between their 
work and ours lies in the definition of the graph. 
While they treat multilingual phrases as nodes, we 
treat only English phrases as nodes. Besides, we 
define the edges between nodes as semantic 
relation instead of bilingual alignment. 
In contrast to the previous work, we present a 
graph-based method for refining the paraphrases 
generated by the pivoting approach. Our goal is to 
consolidate the relation between paraphrases to 
provide learners with more and better paraphrases 
which are helpful in expanding their lexical 
knowledge. 
3. Graph-Based Paraphrase Generation 
In this section, we describe how we use the 
PageRank algorithm to rank and filter the 
paraphrases generated by the pivot-based method. 
3.1 Graph Construction 
We first exploit the pivot-based method proposed 
by Bannard and Callison-Burch (2005) to populate 
our graph G using of candidate paraphrases 
cP={             } from a bilingual parallel 
corpus B for a query phrase q. Each phrase in cP is 
also represented as a node in G. Note that the 
query phrase q is excluded from cP.  
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 1.  A simple graph G. Note that the cp1 and 
   
  
 will be linked iff    
  
 is the paraphrase of q 
and is also the paraphrase of      
 
q 
cP 
        
 
   
    
      
81
  
Graph G only contains the paraphrases cpi 
whose probabilities are higher than a certain 
threshold ?2  as nodes. In addition, each cpi is 
linked to the query phrase q with edge e which is 
weighted by the probability  (   | ). Furthermore, 
we establish the edges among the phrases in cP. 
An example graph is shown in Figure 1. By 
repeating the previous steps, for each phrase cp1, 
cp2,... in cP, we find their corresponding 
paraphrases,         
 
  
        and 
         
 
  
       ?., and discard the 
paraphrases that are not in cP. Once the phrases are 
linked with their paraphrases, the graph G is 
created.  
In this paper, we also place a constraint that a 
paraphrase of a phrase q must neither be a 
substring nor a superstring of q. These strings are 
usually aligned with the same foreign language 
phrase while they are not paraphrases at all. For 
example, ?play an important? and ?play an 
important role in? are excluded for ?play an 
important role?. This has the effect of reducing 
some of the noise generated by the pivot-based 
method.   
3.2 Graph-Based Paraphrase Generation 
We then refine the generated paraphrases adopting 
the PageRank algorithm proposed by Page et al 
(1999). Consider a graph consisting of a set of 
webpages on the Web V and a set of hyperlinks E. 
The PageRank algorithm assigns a value PR to 
each webpage as their importance measurement. 
The PR value of a certain page u is defined 
iteratively as the following equation: 
  ( )   ?
  ( )
 ( )
                   ( )
    
 
where Bu is a set of pages linked to u and L(.) 
denotes the number of outbound links from a page 
v.  
Intuitively, by using formula (1) iteratively, we 
are able to calculate the PR values for all nodes 
and thus extract relatively important paraphrases. 
However, the original PageRank algorithm does 
not take the weight of each edge into consideration. 
That is, the PageRank algorithm treats all links 
equally when distributing rank scores. Treating all 
links equally in paraphrase generation task might 
                                                 
2 We set ? to be 0.01. 
lose some linguistic properties. For this, we 
consider the importance of edges of the nodes and 
weight the edges based on the paraphrase 
probability in the pivot-based approach using 
 (      )  ? ( |  ) ( |  )
 
      ( )  
Formula (2) represents the probability that the 
phrase u is the paraphrase of the phrase v. f refers 
to shared translations of v and u. Then for each 
iteration of the PageRank calculation, we reassign 
the PR value for all u in V to be PR?(u) as:  
   ( )   ?
 (   )  ( )
 ( )
           ( )
    
 
Instead of treating all edges equally, formula (3) 
integrates the weights of inbound link and 
outbound link edges (see Section 4 for the 
performance differences with and without 
weighting edges). 
4. Results 
In this section, we first present our experimental 
setting. Then evaluation results are reported. 
4.1 Experimental Setting 
In this paper, word alignments were produced by 
Giza++ toolkit (Och and Ney, 2003) over a set of 
Danish-English section (containing 1,236,427 
sentences) of the Europarl corpus, version 2 
(Koehn, 2002).  
We compared our graph-based approach with a 
strong baseline, the pivot-based method with 
syntactic constraint (SBP) (Callison-Burch, 2008) 
utilizing the same Danish-English corpus. We also 
investigate the contribution of adding the edge 
weights to the PageRank algorithm by building 
two models, PR representing the method of the 
PageRank algorithm without weights and PRw 
representing the method of the weighted PageRank 
algorithm, for comparison.   
To assess the performance of our method, we 
conducted a manual evaluation. We asked an 
experienced English lecturer to randomly select 
100 most commonly used and meaningful phrases 
from 30 research articles in the discipline of 
Computer-Assisted Language Learning (CALL). A 
total of 88 unique phrases were used as our test set 
for evaluation excluding 12 phrases not existing in 
the Europarl corpus. For each phrase, we extracted 
82
  
the corresponding candidate paraphrases and chose 
top 5 for evaluation. Two raters, provided with a 
simplified scoring standard used by Callison-Burch 
(2008), manually evaluate the accuracy of the top 
ranked paraphrases of each phrase by score 0, 1 
and 2. It is worth noting that the raters were asked 
to score each paraphrase candidate by considering 
its appropriateness in various contexts. In this 
evaluation, we strictly deemed a paraphrase to be 
correct if and only if both raters scored 2.  The 
inter-annotator agreement was 0.63.   
The coverage was measured by the number of 
correct answers within top 5 candidates. The 
precision was measured by the number of correct 
answers within the returned answers. 
On the other hand, to assess the effectiveness of 
PREFER in language learning, we carried out an 
experiment with 55 Chinese-speaking EFL college 
freshmen, who had at least six years of formal 
instruction from junior to senior high schools and 
were estimated to be at the intermediate level 
regarding their overall English competence. The 
students were randomly divided into three groups. 
They were asked to paraphrase seven short 
paragraphs in the pre-test with no system support, 
and then paraphrase another seven short 
paragraphs in the post-test using three different 
tools: PREFER (P), LONGMAN Dictionary of 
Contemporary English Online (L), and 
Thesaurus.com (T). A total of 22 default phrases 
(http://140.114.75.22/share/examples.htm) were 
embedded in the paragraphs in the pre- and 
post-tests, targeted at comparing the quality and 
quantity of students? paraphrasing performance. 
Students were not restricted to paraphrase these 
embedded phrases. Instead, they were encouraged 
to replace any possible phrases or even restructure 
sentences. We had two experienced native-speaker 
TESL (Teaching English as a Second Language) 
lecturers to score the students? paraphrasing 
performance. 
4.2 Experimental Results 
4.2.1 Manual Evaluation 
As shown in Table 1, PRw achieved both good 
precision and coverage. Moreover, PR and PRw 
performed better than SBP in both coverage and 
precision. Also, the result that the performance of 
PRw is better than that of PR implies that PRw is 
able to generate more semantically and 
syntactically correct paraphrases. However, the 
precision of 0.19 indicates that there is still room to 
improve the paraphrase generation model.  
 
 
 
  PR PRw SBP 
Coverage 0.17 0.18 0.07 
Precision 0.17 0.19 0.10 
Table 1: The measurement of paraphrases. 
 
Additionally, Mean Reciprocal Rank (MRR) is 
also reported. Here, MRR is defined as a measure 
of how much effort needed to locate the first 
appropriate paraphrase for the given phrase in the 
ranked list of paraphrases. The MRR score of PRw 
(0.53) outperformed PR (0.51) and SBP (0.47). It 
demonstrated that the PRw model facilitates the 
high ranking of good paraphrases (i.e., paraphrases 
with meaning and grammaticality preserved would 
be ranked high).  
4.2.2. Evaluation on Language Learning  
The second evaluation is to assess the effectiveness 
of PREFER applied to CALL. We used a 
comparison method to measure the extent to which 
EFL learners achieved good performance in 
paraphrasing.  
 
Table 2. Comparison of paraphrasing performance 
among students using three different reference tools.  
 
As seen in the first row of Table 2, the students? 
writing performance improved most with the 
assistance of PREFER (i.e., group P), compared 
with group L and group T. We further analyzed 
and compared the number of the rephrased phrases 
and the correct paraphrases, and the rate of 
    P L T 
improvement of paraphrasing 
task 
38.2% -31.6% -6.2% 
all 
paraphrasable 
phrases 
rephrased 38.4% -23.2% 9.5% 
correct 53.3% -17.5% 4.6% 
correctness rate  7.9% 4.9% -3.1% 
22 default 
phrases 
rephrased 68% -16% 28% 
correct 100% -5% 31% 
correctness rate 13.6% 7.9% 1.5% 
83
  
correctness students achieved using different 
reference tools among our testing paraphrase 
candidates (see the middle and bottom panels of 
Table 2). Obviously, the students consulting 
PREFER achieved substantial paraphrasing 
improvement in all three aspects of both all and 
default phrases. But the other two groups seemed 
unable to manage well the paraphrasing task with 
traditional way of phrase information. This limited 
information seems insufficient to enable students 
to familiarize themselves with proper usages of 
phrases which might lead to improper 
paraphrasing. 
In short, PREFER outperformed the other two 
reference tools in assisting EFL learners in their 
paraphrasing task. 
5. Conclusion and Future Work   
In this paper, we treat the paraphrase generation 
problem as a graphical problem. We utilize the 
PageRank algorithm to rank and filter the 
paraphrases generated using the pivot-based 
method. The results show that our method 
significantly produces better paraphrases in both 
precision and coverage compared with the 
syntactically-constrained pivot method of 
Callison-Burch (2008). Additionally, PREFER 
does benefit learners? writing performance. 
 In order to conduct a more comprehensive 
evaluation, we plan to adapt the in-context 
evaluation metric introduced by Callison-Burch et. 
al (2008). A larger test set would be generated 
manually to evaluate the performance of our 
paraphrase system. In addition, we will implement 
various kinds of baseline systems such as Kok and 
Brockett (2010) and Chan et al (2011) to provide a 
more competitive comparison. 
Many avenues exist for future research and 
improvement. For example, we would like to 
extend paraphrasing consecutive n-gram phrases to 
inconsecutive ones such as ones with incomplete 
transitive verbs (e.g., ?provide someone with 
something?). Besides, we are interested in 
weighting edges using syntactic and semantic 
relation in our graph-based method to further 
improve the quality of generated paraphrases. 
 
 
 
References  
Ali R. Abasi, Nahal Akbari, and Barbara Graves. 2006. 
Discourse appropriation, construction of identities, 
and the complex issue of plagiarism: ESL students 
writing in graduate school. Journal of Second 
Language Writing, 15, 102?117. 
Ion Androutsopoulos and Prodromos Malakasiotis. 2010. 
A Survey of Paraphrasing and Textual Entailment 
Methods. Journal of Artificial Intelligence Research 
38, 135?187. 
Colin Bannard and Chris Callison-Burch. 2005. 
Paraphrasing with bilingual parallel corpora. In 
Proceedings of ACL, pp. 597-604. 
Chris Callison-Burch. 2008. Syntactic constraints on 
paraphrases extracted from parallel corpora. In 
Proceedings of EMNLP, pp. 196?205. 
Chris Callison-Burch, Trevor Cohn, and Mirella Lapata. 
2008. ParaMetric: an automatic evaluation metric for 
paraphrasing. In Proceedings of the 22nd 
International Conference on Computational 
Linguistics, pp. 97-104. 
Cherry Campbell. 1990. Writing with others? words: 
Using background reading text in academic 
compositions. In B. Kroll (Ed.), Second language 
writing: Research insights for the classroom (pp. 
211-30). Cambridge, UK: Cambridge University 
Press. 
Tsz Ping Chan, Chris Callison-Burch and Benjamin Van 
Durme. 2011. Reranking Bilingually Extracted 
Paraphrases Using Monolingual Distributional 
Similarity. In Proceedings of the GEMS 2011 
Workshop on GEometrical Models of Natural 
Language Semantics, pp. 33-42.  
Ranamukalage Chandrasoma, Celia Thompson, and 
Alastair Pennycook. 2004. Beyond plagiarism: 
Transgressive and nontransgressive intertextuality. 
Journal of Language, Identity, and Education, 3(3), 
171?194. 
Catherine Fuchs. 1980. Paraphrase et th?orie du 
langage. Contribition ? une histoire des theories 
linguistiques contemporaines et ? la construction 
d?une th?orie ?nonciative de la paraphraase. Th?se 
de doctorat. Paris: Universit? Paris VII. 
Philipp Koehn. 2002. Europarl: A multilingual corpus 
for evaluation of machine translation. Unpublished, 
http://www.isi.edu/~koehn/europarl/. 
Stanley Kok and Chris Brockett. 2010. Hitting the right 
paraphrases in good time. In Proceedings of 
NAACL/HLT, pp. 145-153. 
84
  
Nitin Madnani and Bonnie J. Dorr. 2010. Generating 
phrasal and sentential paraphrases: A survey of 
data-driven methods. Computational Linguistics, 
36(3):341?388. 
Claire Martinot. 2003. Pour une linguistique de 
l?acquisition. La reformulation: du concept descriptif 
au concept explicatif. Langage et Soci?t?, 2(104); 
147-151. 
Lara McInnis. 2009. Analyzing English L1 and L2 
Paraphrasing Strategies through Concurrent Verbal 
Report and Stimulated Recall Protocols. MA Thesis. 
Toronto: University of Toronto. 
Igor Mel??uk. 1992. Paraphrase et lexique: la th?orie 
Sens-Texte et le Dictionnaire explicatif et 
combinatoire. In: Mel??uk, I. et al, Dictionnaire 
explicatif et combinatoire du fran?ais contemporain. 
Recherches lexico-s?mantiques III. Montr?al: Presses 
de l?Universit? de Montr?al; 9-59. 
Jasmina Mili?evi? and Alexandra Tsedryk. 2011. 
Assessing and Improving Paraphrasing Competence 
in FSL. In Proceedings of the 5th International 
Conference on Meaning- Text Theory, pp. 175-184. 
Franz Josef Och and Hermann Ney. 2003. A systematic 
comparison of various statistical alignment models. 
Computational Linguistics, 29(1): 19-51. 
Lawrence Page, Sergey Brin, Rajeev Motwani, Terry 
Winograd. 1999. The PageRank Citation Ranking: 
Bringing Order to the Web. Technical Report. pp. 
1999-66, Stanford University InfoLab. 
 
85
