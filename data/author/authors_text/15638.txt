Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 778?788,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Training continuous space language models:
some practical issues
Le Hai Son and Alexandre Allauzen and Guillaume Wisniewski and Franc?ois Yvon
Univ. Paris-Sud, France and LIMSI/CNRS
BP 133, 91403 Orsay Cedex
Firstname.Lastname@limsi.fr
Abstract
Using multi-layer neural networks to esti-
mate the probabilities of word sequences is
a promising research area in statistical lan-
guage modeling, with applications in speech
recognition and statistical machine transla-
tion. However, training such models for large
vocabulary tasks is computationally challeng-
ing which does not scale easily to the huge
corpora that are nowadays available. In this
work, we study the performance and behav-
ior of two neural statistical language models
so as to highlight some important caveats of
the classical training algorithms. The induced
word embeddings for extreme cases are also
analysed, thus providing insight into the con-
vergence issues. A new initialization scheme
and new training techniques are then intro-
duced. These methods are shown to greatly re-
duce the training time and to significantly im-
prove performance, both in terms of perplexity
and on a large-scale translation task.
1 Introduction
Statistical language models play an important role in
many practical applications, such as machine trans-
lation and automatic speech recognition. Let V be
a finite vocabulary, statistical language models de-
fine distributions over sequences of words wL1 in V
?
usually factorized as:
P (wL1 ) = P (w1)
L?
l=1
P (wl|w
l?1
1 )
Modeling the joint distribution of several discrete
random variables (such as words in a sentence) is
difficult, especially in real-world Natural Language
Processing applications where V typically contains
dozens of thousands words.
Many approaches to this problem have been pro-
posed over the last decades, the most widely used
being back-off n-gram language models. n-gram
models rely on a Markovian assumption, and de-
spite this simplification, the maximum likelihood es-
timate (MLE) remains unreliable and tends to under-
estimate the probability of very rare n-grams, which
are hardly observed even in huge corpora. Con-
ventional smoothing techniques, such as Kneser-
Ney and Witten-Bell back-off schemes (see (Chen
and Goodman, 1996) for an empirical overview,
and (Teh, 2006) for a Bayesian interpretation), per-
form back-off on lower order distributions to pro-
vide an estimate for the probability of these unseen
events. n-gram language models rely on a discrete
space representation of the vocabulary, where each
word is associated with a discrete index. In this
model, the morphological, syntactic and semantic
relationships which structure the lexicon are com-
pletely ignored, which negatively impact the gen-
eralization performance of the model. Various ap-
proaches have proposed to overcome this limita-
tion, notably the use of word-classes (Brown et al,
1992; Niesler, 1997), of generalized back-off strate-
gies (Bilmes et al, 1997) or the explicit integration
of morphological information in the random-forest
model (Xu and Jelinek, 2004; Oparin et al, 2008).
One of the most successful alternative to date is to
use distributed word representations (Bengio et al,
2003), where distributionally similar words are rep-
resented as neighbors in a continuous space. This
778
turns n-grams distributions into smooth functions
of the word representations. These representations
and the associated probability estimates are jointly
computed in a multi-layer neural network architec-
ture. This approach has showed significant and
consistent improvements when applied to automatic
speech recognition (Schwenk, 2007; Emami and
Mangu, 2007; Kuo et al, 2010) and machine trans-
lation tasks (Schwenk et al, 2006). Hence, contin-
uous space language models are becoming increas-
ingly used. These successes have revitalized the re-
search on neuronal architectures for language mod-
els, and given rise to several new proposals (see, for
instance, (Mnih and Hinton, 2007; Mnih and Hinton,
2008; Collobert and Weston, 2008)). A major diffi-
culty with these approaches remains the complexity
of training, which does not scale well to the mas-
sive corpora that are nowadays available. Practical
solutions to this problem are discussed in (Schwenk,
2007), which introduces a number of optimization
and tricks to make training doable. Even then, train-
ing a neuronal language model typically takes days.
In this paper, we empirically study the conver-
gence behavior of two multi-layer neural networks
for statistical language modeling, comparing the
standard model of (Bengio et al, 2003) with the log-
bilinear (LBL) model of (Mnih and Hinton, 2007).
Our contributions are the following: we first pro-
pose a reformulation of Mnih and Hinton?s model,
which reveals its similarity with extant models, and
allows a direct and fair comparison with the stan-
dard model. For the standard model, these results
highlight the impact of parameter initialization. We
first investigate a re-initialization method which al-
lows to escape from the local extremum the standard
model converges to. While this method yields a sig-
nificative improvement, the underlying assumption
about the structure of the model does not meet the
requirement of very large-scale tasks. We therefore
introduce a different initialization strategy, called
one vector initialization. Experimental results show
that these novel training strategies drastically reduce
the total training time, while delivering significant
improvements both in terms of perplexity and in a
large-scale translation task.
The rest of this paper is organized as follows. We
first describe, in Section 2, the standard and the LBL
language models. By reformulating the latter, we
show that both models are very similar and empha-
size the remaining differences. Section 2.4 discusses
complexity issues and possible solutions to reduce
the training time. We then report, in Section 3, pre-
liminary experimental results that enlighten some
caveats of the standard approach. Based on these
observations, we introduce in Section 4 novel and
more efficient training schemes, yielding improved
performance and a reduced training time both on
small and large scale experiments.
2 Continuous space language models
Learning a language model amounts to estimate the
parameters of the discrete conditional distribution
over words given each possible history, where the
history corresponds to some function of the preced-
ing words. For an n-gram model, the history con-
tains the n ? 1 preceding words, and the model
parameters correspond to P (wl|w
l?1
l?n+1). Continu-
ous space language models aim at computing these
estimates based on a distributed representation of
words (Bengio et al, 2003), thereby reducing the
sparsity issues that plague conventional maximum
likelihood estimation. In this approach, each word
in the vocabulary is mapped into a real-valued vec-
tor and the conditional probability distributions are
then expressed as a (parameterized) smooth func-
tion of these feature vectors. The formalism of neu-
ral networks allows to express these two steps in a
well-known framework, where, crucially, the map-
ping and the model parameters can be learned in
conjunction. In the next paragraphs, we describe the
two continuous space language models considered
in our study and present the various issues associ-
ated with the training of such models, as well as their
most common remedies.
2.1 The standard model
In the following, we will consider words as indices
in a finite dictionary of size V ; depending on the
context, w will either refer to the word or to its in-
dex in the dictionary. A word w can also be repre-
sented by a 1-of-V coding vector v of RV in which
all elements are null except the wth. In the standard
approach of (Bengio et al, 2003), the feed-forward
network takes as input the n?1 word history and de-
livers an estimate of the probability P (wl|w
l?1
l?n+1)
779
as its output. It consists of three layers.
The first layer builds a continuous representation
of the history by mapping each word into its real-
valued representation. This mapping is defined by
RTv, where R ? RV?m is a projection matrix
and m is the dimension of the continuous projection
word space. The output of this layer is a vector i of
(n ? 1)m real numbers obtained by concatenating
the representations of the context words. The pro-
jection matrix R is shared along all positions in the
history vector and is learned automatically.
The second layer introduces a non-linear trans-
form, where the output layer activation values are
defined by h = tanh (Wihi + bih) , where i is the
input vector, Wih ? RH?(n?1)m and bih ? RH are
the parameters of this layer. The vector h ? RH can
be considered as an higher (more abstract) represen-
tation of the context than i.
The third layer is an output layer that estimates the
desired probability, thanks to the softmax function:
P (wl = k|w
l?1
l?n+1) =
exp(ok)
?
k? exp(ok?)
(1)
o = Whoh + bho, (2)
where Who ? RV?H and bho ? RV are respec-
tively the projection matrix and the bias term associ-
ated with this layer. The wth component in P corre-
sponds to the estimated probability of the wth word
of the vocabulary given the input history vector.
The standard model has two hyper-parameters
(the dimension of projection space m and the size of
hidden layer, H) that define the architecture of the
neural network and a set of free parameters ? that
need to be learned from data: the projection matrix
R, the weight matrix Wih, the bias vector bih, the
weight matrix Who and the bias vector bho.
In this model, the projection matrices R and Who
play similar roles as they define maps between the
vocabulary and the hidden representation. The fact
that R assigns similar representations to history
words w1 and w2 implies that these words can be
exchanged with little impact on the resulting prob-
ability distribution. Likewise, the similarity of two
lines in Who is an indication that the corresponding
words tend to have a similar behavior, i.e. tend to
have a similar probabilities of occurrence in all con-
texts. In the remainder, we will therefore refer to R
as the matrix representing the context space, and to
Who as the matrix for the prediction space.
2.2 The log-bilinear model
The work reported (Mnih and Hinton, 2007) de-
scribes another parameterization of the architecture
introduced in the previous section. This parameter-
ization is based on Factored Restricted Boltzmann
Machine. According to (Mnih and Hinton, 2007),
this model, termed the log-bilinear language model
(LBL), achieves, for large vocabulary tasks, bet-
ter results in terms of perplexity than the standard
model, even if the reasons beyond this improvement
remain unclear.
In this section, we will describe this model and
show how it relates to the standard model. The LBL
model estimates the n-gram parameters by:
P (wl|w
l?1
l?n+1) =
exp(?E(wl;w
l?1
l?n+1))
?
w exp(?E(w;w
l?1
l?n+1))
(3)
In this equation, E is an energy function defined as:
E(wl;w
l?1
1 ) = ?
(
l?1?
k=l?n+1
vk
TRCTk
)
RTvl
(4)
? brTRTvl ? bv
Tvl
= ?vTl R
(
l?1?
k=l?n+1
CkR
Tvk + br
)
? vTl bv (5)
where R is the projection matrix introduced above,
(vk)l?n+1?k?l?1 are the 1-of-V coding vectors for
the history words and vl is the coding vector for wl;
Ck ? Rm?m is a combination matrix and br and bv
denote bias vectors. All these parameters need to be
learned during training.
Equation (4) can be rewritten using the notations
introduced for the standard model. We then rename
br and bv respectively bih and bho. We also denote
i the concatenation of the (n ? 1) vectors RTvk;
likewise Wih denotes the H ? (n? 1)m matrix ob-
tained by concatenating row-wise the (n ? 1) ma-
trices Ck. With these new notations, equations (4)
780
and (3) can be rewritten as:
h = Wihi + bih
o = Rh + bho
P (wl = k|w
l?1
l?n+1) =
exp(ok)
?
k? exp(ok?)
This formulation allows to highlight the similarity of
the LBL model and the standard model. These two
models differ only by the activation function of their
hidden layer (linear for the LBL model and tangent
hyperbolic for the standard model) and by their def-
inition of the prediction space: for the LBL model,
the context space and the prediction space are the
same (R = Who, and thus H = m), while in the
standard model, the prediction space is defined in-
dependently from the context space. This restriction
drastically reduces the number of free parameters of
the LBL model.
It is finally noteworthy to outline the similarity
of this model with standard maximum entropy lan-
guage models (Lau et al, 1993; Rosenfeld, 1996).
Let x denote the binary vector formed by stacking
the (n-1) 1-of-V encodings of the history words;
then the conditional probability distributions esti-
mated in the model are proportional to expF (x),
where F is an affine transform of x. The main dif-
ference with MaxEnt language models are thus the
restricted form of the feature functions, which only
test one history word, and the particular representa-
tion of F , which is defined as:
F (x) = RWihR
?Tv + Rbih + bho
where, as before, R? is formed by concatenating
(n? 1) copies of the projection matrix R.
2.3 Training and inference
Training the two models introduced above can be
achieved by maximizing the log-likelihood L of the
parameters ?. This optimization is usually per-
formed by stochastic back-propagation as in (Ben-
gio et al, 2003). For all our experiments, the learn-
ing rate is fixed at 5?10?3. The learning weight de-
cay and the the weight decay (respectively 1? 10?9
and 0) seem to have a minor impact on the results.
Learning starts with a random initialization of the
parameters under the uniform distribution and con-
verges to a local maximum of the log-likelihood
function. Moreover, to prevent overfitting, an early
stopping strategy is adopted: after each epoch, train-
ing is stopped when the likelihood of a validation set
stops increasing.
2.4 Complexity issues
The main problem with neural language models is
their computational complexity. For the two mod-
els presented in this section, the number of floating
point operations needed to predict the label of a sin-
gle example is1:
((n? 1) ?m + 1)?H + (H + 1)? V (6)
where the first term of the sum corresponds to the
computation of the hidden layer and the second one
to the computation of the output layer. The projec-
tion of the context words amounts to select one row
of the projection matrix R, as the words are repre-
sented with a 1-of-V coding vector. We can there-
fore assume that the computation complexity of the
first layer is negligible. Most of the computation
time is thus spent in the output layer, which implies
that the computing time grows linearly with the vo-
cabulary size. Training these models for large scale
tasks is therefore challenging, and a number of tricks
have been introduced to make training and inference
tractable (Schwenk and Gauvain, 2002; Schwenk,
2007).
Short list A simple method to reduce the com-
plexity in inference and in learning is to reduce
the size of the output vocabulary (Schwenk, 2007):
rather than estimating the probability P (wl =
w|wl?1l?n+1) for all words in the vocabulary, we only
estimate it for the N most frequent words of the
training set (the so-called short-list). In this case,
two vocabularies need to be considered, correspond-
ing respectively to the context vocabulary Vc used to
define the history; and the prediction vocabulary Vp.
However, this method fails to deliver any probability
estimate for words outside of the prediction vocab-
ulary, meaning that a fall-back strategy needs to be
defined for those words. In practice, neural network
1Recall that learning requires to repeatedly predict the label
for all the examples in the training set.
781
language models are combined with a conventional
n-gram model as described in (Schwenk, 2007).
Batch mode and resampling Additional speed-
ups can be obtained by propagating several exam-
ples at once through the network (Bilmes et al,
1997). This ?batch mode? allows to factorize the
matrix operations and cut down both inference and
training time. In all our experiments, we used a
batch size of 64. Moreover, the training time is lin-
ear in the number of examples in the training data2.
Training on very large corpora, which, nowadays,
comprise billions of word tokens, cannot be per-
formed exhaustively and requires to adopt resam-
pling strategies, whereby, at each epoch, the system
is trained with only a small random subset of the
training data. This approach enables to effectively
estimate neural language models on very large cor-
pora; it has also been observed empirically that sam-
pling the training data can increase the generaliza-
tion performance (Schwenk, 2007).
3 A head-to-head comparison
In this section, we analyze a first experimental
study of the two neural network language models
introduced in Section 2 in order to better under-
stand the differences between these models espe-
cially in terms of the word representations they in-
duce. Based on this study, we will propose, in the
next section, improvements of both the speed and
the prediction capacity of the models. In all our ex-
periments, 4-gram language models are used.
3.1 Corpus
The data we use for training is a large monolingual
corpus, containing all the English texts in the par-
allel data of the Arabic to English NIST 2009 con-
strained task3. It consists of 176 millions word to-
kens with 532, 557 different word types as the size
of vocabulary. The perplexity is computed with re-
spect to the 2006 NIST test data, which is used here
as our development data.
2Equation (6) gives the complexity of inference for a single
example.
3http://www.itl.nist.gov/iad/mig/tests/
mt/2009/MT09_ConstrainedResources.pdf
3.2 Convergence study
In a first experiment, we trained the two models in
the same setting: we choose to consider a small
vocabulary comprising the 10, 000 most frequent
words. The same vocabulary is used to constrain
the words occurring in the history and the words
to be predicted. The size of hidden layer is set to
m = H = 200, the history contains the 3 preceding
words, we use a batch size of 64, a resampling rate
of 5% and no weight decay.
Figure 1 displays the perplexity convergence
curve measured on the development data for the
standard and the LBL models4. The convergence
perplexities after the combination with the standard
back-off model are also provided for all the mod-
els in table 2 (see section 4.3). We can observe
that the LBL model converges faster than the stan-
dard model: the latter needs 13 epochs to reach
the stopping criteria, while the former only needs
6 epochs. However, upon convergence, the stan-
dard model reaches a lower perplexity than the LBL
model.
0 2 4 6 8 10 12 14120
130
140
150
160
170
180
epochs
per
plex
ity
Perplexity
standardlog bilinear
Figure 1: Convergence rate of the standard and the LBL
models evaluated by the evolution of the perplexity on a
development set
As described in Section 2.2, the main difference
between the standard and the LBL model is the way
the context and the prediction spaces are defined: in
the standard model, the two spaces are distinct; in
4The use of a back-off 4-model estimated with the modified
Knesser-Ney smoothing on the same training data achieves a
perplexity of 141 on the development data.
782
the LBL model, they are bound to be the same. With
a smaller number of parameters, the LBL model can
not capture as many characteristics of the data as the
standard model, but it converges faster5. This differ-
ence in convergence can be explained by the scarcity
of the updates in the projection matrix R in the
standard model: during backpropagation, only those
weights that are associated with words in the history
are updated. By contrast, each training sample up-
dates all the weights in the prediction matrix Who.
3.3 An analysis of the continuous word space
To deepen our understanding, we propose to further
analyze the induced word embeddings by finding,
for some randomly selected words, the five nearest
neighbors (according to the Euclidian distance) in
the context space and in the prediction space of the
two models. Results are presented in Table 1.
If we look first at the standard model, the global
picture is that for frequent words (is, are, and, to
a lesser extend, have), both spaces seem to define
meaningful neighborhood, corresponding to seman-
tic and syntactic similarities; this is less true for rarer
words, where we see a greater discrepancy between
the context and prediction spaces. For instance, the
date 1947 seems to be randomly associated in the
context space, while the 5 nearest words in the pre-
diction space form a consistent set of dates. The
same trend is also observed for the word Castro. Our
interpretation is that for less frequent words, the pro-
jection vectors are hardly ever updated and remain
close to their original random initialization.
By contrast, the similarities in the (unique) pro-
jection space of the LBL remain consistent for all
frequency ranges, and are very similar to the predic-
tion space of the standard model. This seems to val-
idate our hypothesis that in the standard model, the
prediction space is learned much faster than the con-
text space and corroborates our interpretation of the
impact of the scarce updates of rare words. Another
possible explanation is that there is no clear relation
5We could increase the number of parameters of the LBL
model for a fairer comparison with the standard model. How-
ever, this would also increase the size of the vocabulary and
cause two new issues: on one hand, the time complexity would
drastically increase for the LBL model, and on the other hand,
both models would not be comparable in terms of perplexity as
their vocabulary would be different.
between the context space and the target function:
the context space is learned only indirectly by back-
propagation. As a result, due to the random initial-
ization of the parameters and to data sparsity, many
vectors of R might be blocked in some local max-
ima, meaning that similar vectors cannot be grouped
in a consistent way and that the induced similarity is
more ?loose?.
4 Improving the standard model
In Section 3.2, we observed that slightly better re-
sults can be obtained with the standard rather than
with the LBL model. The latter is however much
faster to train, and seems to induce better projection
matrices. Both effects can be attributed to the partic-
ular parameterization of this model, which uses the
same projection matrix both for the context and for
the prediction spaces. In this section, we propose
several new learning regimes that allowed us to im-
prove the standard model in terms of both speed and
prediction capacity. All these improvements rely on
the idea of sharing word representations. While this
idea is not new (see for instance (Collobert and We-
ston, 2008)), our analysis enables to better under-
stand its impact on the convergence rate. Finally, the
improvements we propose are evaluated on a real-
word machine translation task.
4.1 Improving performances with
re-initialization
The experiments reported in the previous section
suggest that it is possible to improve the perfor-
mances of the standard model by building a better
context space. Thus, we introduce a new learning
regime, called re-initialization which aims to im-
prove the context space by re-injecting the informa-
tion on word neighborhoods that emerges in the pre-
diction space. One possible implementation of this
idea is as follows:
1. train a standard model until convergence;
2. use the prediction space of this model to ini-
tialize the context space of a new model; the
prediction space is chosen randomly;
3. train this new model.
783
Table 1: The 5 closest words in the representation spaces of the standard and LBL language models.
word (frequency) model space 5 most closest words
is standard context was are were be been
900, 350 standard prediction was has would had will
LBL both was reveals proves are ON
are standard context were is was be been
478, 440 standard prediction were could will have can
LBL both were is was FOR ON
have standard context had has of also the
465, 417 standard prediction are were provide remain will
LBL both had has Have were embrace
meeting standard context meetings conference them 10 talks
150, 317 standard prediction undertaking seminar meetings gathering project
LBL both meetings summit gathering festival hearing
Imam standard context PCN rebellion 116. Cuba 49
787 standard prediction Castro Sen Nacional Al- Ross
LBL both Salah Khaled Al- Muhammad Khalid
1947 standard context 36 Mercosur definite 2002-2003 era
774 standard prediction 1965 1945 1968 1964 1975
LBL both 1965 1976 1964 1968 1975
Castro standard context exclusively 12. Boucher Zeng Kelly
768 standard prediction Singh Clark da Obasanjo Ross
LBL both Clark Singh Sabri Rafsanjani Sen
Figure 2: Evolution of the perplexity on a development
set for various initialization regimes.
The evolution of the perplexity with respect to train-
ing epochs for this new method is plotted on Fig-
ure 2, where we only represent the evolution of the
perplexity during the third training step. As can be
seen, at convergence, the perplexity the model esti-
mated with this technique is about 10% smaller than
the perplexity of the standard model.
This result can be explained by considering the re-
initialization as a form of annealing technique: re-
initializing the context space allows to escape from
the local extrema the standard model converges to.
The fact that the prediction space provides a good
initialization of the context space also confirms our
analysis that one difficulty with the standard model
is the estimation of the context space parameters.
4.2 Iterative re-initialization
The re-initialization policy introduced in the previ-
ous section significantly reduces the perplexity, at
the expense of a longer training time, as it requires
to successively train two models. As we now know
that the parameters of the prediction space are faster
to converge, we introduce a second training regime
called iterative re-initialization which aims to take
advantage of this property. We summarize this new
training regime as follows:
1. Train the model for one epoch.
2. Use the prediction space parameters to reini-
tialize the context space.
3. Iterate steps (1) and (2) until convergence.
784
Figure 3: Evolution of the perplexity on the training data
for various initialization regimes.
This regimes yields a model that is somewhat in-
between the standard and LBL models as it adds a
relationship between the two representation spaces,
which lacks in the former model. This relationship is
however not expressed through the tying of the cor-
responding parameters; instead we let the prediction
space guide the convergence of the context space.
As a consequence, we hope that it can achieve a con-
vergence speed as fast as the one of the LBL model
without degrading its prediction capacity.
The result plotted on Figure 2 shows that this in-
deed the case: using this training regime, we ob-
tained a perplexity similar to the one of the stan-
dard model, while at the same time reducing the
total training time by more than a half, which is
of great practical interest (each epoch lasts approxi-
mately 8 hours on a 3GHz Xeon processor).
Figure 3 displays the perplexity convergence
curve measured on the training data for the standard
learning regime as well as for the re-initialization
and iterative re-initialization. These results show
the same trend as for the perplexity measured on
the development data, and suggest a regularization
effect of the re-initialization schemes rather than al-
lowing the models to escape local optima.
4.3 One vector initialization
Principle The new training regimes introduced
above outperform the standard training regime both
in terms of perplexity and of training time. However,
exchanging information between the context and
prediction spaces is only possible when the same
vocabulary is used in both spaces. As discussed
in Section 2.4, this configuration is not realistic for
very large-scale tasks. This is because increasing the
number of predicted word types is much more com-
putationally demanding than increasing the number
of types in the context vocabulary. Thus, the former
vocabulary is typically order of magnitudes larger
than the latter, which means that the re-initialization
strategies can no longer be directly used.
It is nonetheless possible to continue drawing in-
spirations from the observations made in Section 3,
and, crucially, to question the random initialization
strategy. As discussed above, this strategy may ex-
plain why the neighborhoods in the induced con-
text space for the less frequent types were diffi-
cult to interpret. As a straightforward alternative,
we consider a different initialization strategy where
all the words in the context vocabulary are initially
projected onto the same (random) point in the con-
text space. The intuition is that it will be easier to
build meaningful neighborhoods, especially for rare
types, if all words are initially considered similar
and only diverge if there is sufficient evidence in the
training data to suggest that they should. This model
is termed the one vector initialization model.
Experimental evaluation To validate this ap-
proach, we compare the convergence of a standard
model trained (with the standard learning regime)
with the one vector initialization regime. The con-
text vocabulary is defined by the 532, 557 words oc-
curring in the training data and the prediction vo-
cabulary by the 10, 000 most frequent words6. All
other parameters are the same as in the previous
experiments. Based on the curves displayed on
Figure 4, we can observe that the model obtained
with the one vector initialization regime outperforms
the model trained with a completely random ini-
tialization. Moreover, the latter reaches conver-
gence in only 14 epochs, while the learning regime
we propose only needs 9 epochs. Convergence is
even faster than when we used the standard training
regime and a small context vocabulary.
6In this case, the distinction between the context and the pre-
diction vocabulary rules out the possibility of a relevant compar-
ison based on perplexity between the continuous space language
model and a standard back-off language model.
785
0 5 10 15100
110
120
130
140
150
160
170
180
epochs
perp
lexit
y
Perplexity
standardone vector initialization
Figure 4: Perplexity with all-10, 000, 200? 200 models
Table 2: Summary of the perplexity (PPX) results mea-
sured on the same development set with the different con-
tinuous space language models. For all of them, the prob-
abilities are combined with the back-off n-gram model
Vc size Model # epochs PPX
10000 log bilinear 6 239
standard 13 227
iterative reinit. 6 223
reinit. 11 211
all standard 14 276
one vector init. 9 260
To illustrate the impact of our initialization
scheme, we also used a principal component anal-
ysis to represent the induced word representations
in a two dimensional space. Figure 5 represents the
vectors associated with numbers7 in red, while all
other words are represented in blue. Two different
models are used: the standard model on the left, and
the one vector initialization model on the right. We
can observe that, for the standard model, most of
the red points are scattered all over a large portion
of the representation space. On the opposite, for
the one vector initialization model, points associated
with numbers are much more concentrated: this is
simply because all the points are originally identi-
cal, and the training aim to spread the point around
this starting point. We also created the closest word
list reported in Table 3, in a manner similar to Ta-
ble 1. Clearly, the new method seems to yield more
7Number are all the words consisting only of digits, with an
optional sign, point or comma such as: 1947; 0,001; -8,2.
(a) with the standard model (b) with the one vector initial-
ization model
Figure 5: Comparison of the word embedding in the con-
text space for numbers (red points).
meaningful neighborhoods in the context space.
It is finally noteworthy to mention that when used
with a small context vocabulary (as in the experi-
mental setting of Section 4.1) this initialization strat-
egy underperforms the standard initialization. This
is simply due to the much greater data sparsity in
the large context vocabulary experiments, where the
rarer word types are really rare (they typically occur
once or twice). By contrast, the rarer words in the
small vocabulary tasks occurred more than several
hundreds times in the training corpus, which was
more than sufficient to guide the model towards sat-
isfactory projection matrices. This finally suggests
that there still exists room for improvement if we
can find more efficient initialization strategies than
starting from one or several random points.
4.4 Statistical machine translation experiments
As a last experiment, we compare the various mod-
els on a large scale machine translation task. Sta-
tistical language models are key component of cur-
rent statistical machine translation systems (Koehn,
2010), where they both help disambiguate lexical
choices in the target language and influence the
choice of the right word ordering. The integration of
a neural network language model in such a system is
far from easy, given the computational cost of com-
puting word probabilities, a task that is performed
repeatedly during the search of the best translation.
We then had to resort to a two pass decoding ap-
proach: the first pass uses a conventional back-off
language model to produce a n-best list (the n most
likely translations and their associated scores); in the
second pass, the probability of the neural language
model is computed for each hypothesis and the n-
786
Table 3: The 5 closest words in the context space of the standard and one vector initialization language models
word (freq.) model 5 closest words
is standard was are were been remains
900, 350 1 vector init. was are be were been
conducted standard undertaken launched $270,900 Mufamadi 6.44-km-long
18, 388 1 vector init. pursued conducts commissioned initiated executed
Cambodian standard Shyorongi $3,192,700 Zairian depreciations teachers
2, 381 1 vector init. Danish Latvian Estonian Belarussian Bangladeshi
automatically standard MSSD Sarvodaya $676,603,059 Kissana 2,652,627
1, 528 1 vector init. routinely occasionally invariably inadvertently seldom
Tosevski standard $12.3 Action,3 Kassouma 3536 Applique
34 1 vector init. Shafei Garvalov Dostiev Bourloyannis-Vrailas Grandi
October-12 standard 39,572 anti-Hutu $12,852,200 non-contracting Party?s
8 1 vector init. March-26 April-11 October-1 June-30 August4
3727th standard Raqu Tatsei Ayatallah Mesyats Langlois
1 1 vector init. 4160th 3651st 3487th 3378th 3558th
best list is accordingly reordered to produce the final
translations.
The different language models discussed in this
article are evaluated on the Arabic to English
NIST 2009 constrained task. For the continuous
space language model, the training data consists
in the parallel corpus used to train the translation
model (previously described in section 3.1). The de-
velopment data is again the 2006 NIST test set and
the test data is the official 2008 NIST test set. Our
system is built using the open-source Moses toolkit
(Koehn et al, 2007) with default settings. To set
up our baseline results, we used an extensively op-
timized standard back-off 4-grams language model
using Kneser-Ney smoothing described in (Allauzen
et al, 2009). The weights used during the reranking
are tuned using the Minimum Error Rate Training
algorithm (Och, 2003). Performance is measured
based on the BLEU (Papineni et al, 2002) scores,
which are reported in Table 4.
Table 4: BLEU scores on the NIST MT08 test set with
different language models.
Vc size Model # epochs BLEU
all baseline - 37.8
10000 log bilinear 6 38.2
standard 13 38.3
iterative reinit. 6 38.4
reinit. 11 38.4
all standard 14 38.6
one vector init. 9 38.7
All the experimented neural language models
yield to a significant BLEU increase. The best re-
sult is obtained by the one vector initialization stan-
dard model which achieves a 0.9 BLEU improve-
ment. While this results is similar to the one ob-
tained with the standard model, the training time is
reduced here by a third.
5 Conclusion
In this work, we proposed three new methods
for training neural network language models and
showed their efficiency both in terms of computa-
tional complexity and generalization performance in
a real-word machine translation task. These meth-
ods rely on conclusions drawn from a careful study
of the convergence rate of two state-of-the-art mod-
els and are based on the idea of sharing the dis-
tributed word representations during training.
Our work highlights the impact of the initializa-
tion and the training scheme for neural network lan-
guage models. Both our experimental results and
our new training methods can be closely related to
the pre-training techniques introduced by (Hinton
and Salakhutdinov, 2006). Our future work will thus
aim at studying the connections between our empir-
ical observations and the deep learning framework.
Acknowledgments
This work was partly realized as part of the Quaero
Program, funded by OSEO, the French agency for
innovation.
787
References
Alexandre Allauzen, Josep Crego, Aure?lien Max, and
Franc?ois Yvon. 2009. LIMSI?s statistical transla-
tion systems for WMT?09. In Proceedings of the
Fourth Workshop on Statistical Machine Translation,
pages 100?104, Athens, Greece, March. Association
for Computational Linguistics.
Yoshua Bengio, Re?jean Ducharme, Pascal Vincent, and
Christian Janvin. 2003. A neural probabilistic lan-
guage model. JMLR, 3:1137?1155.
J. Bilmes, K. Asanovic, C. Chin, and J. Demmel. 1997.
Using phipac to speed error back-propagation learn-
ing. Acoustics, Speech, and Signal Processing, IEEE
International Conference on, 5:4153.
Peter F. Brown, Peter V. deSouza, Robert L. Mercer, Vin-
cent J. Della Pietra, and Jenifer C. Lai. 1992. Class-
based n-gram models of natural language. Comput.
Linguist., 18(4):467?479.
Stanley F. Chen and Joshua Goodman. 1996. An empiri-
cal study of smoothing techniques for language model-
ing. In Proc. ACL?96, pages 310?318, San Francisco.
Ronan Collobert and Jason Weston. 2008. A uni-
fied architecture for natural language processing: deep
neural networks with multitask learning. In Proc.
of ICML?08, pages 160?167, New York, NY, USA.
ACM.
Ahmed Emami and Lidia Mangu. 2007. Empirical study
of neural network language models for Arabic speech
recognition. In Proc. ASRU?07, pages 147?152, Ky-
oto. IEEE.
Geoffrey E. Hinton and Ruslan R. Salakhutdinov. 2006.
Reducing the dimensionality of data with neural net-
works. Science, 313(5786):504?507, July.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In Proc.
ACL?07, pages 177?180, Prague, Czech Republic.
Philipp Koehn. 2010. Statistical Machine Translation.
Cambridge University Press.
Hong-Kwang Kuo, Lidia Mangu, Ahmad Emami, and
Imed Zitouni. 2010. Morphological and syntactic fea-
tures for arabic speech recognition. In Proc. ICASSP
2010.
Raymond Lau, Ronald Rosenfeld, and Salim Roukos.
1993. Adaptive language modeling using the maxi-
mum entropy principle. In Proc HLT?93, pages 108?
113, Princeton, New Jersey.
Andriy Mnih and Geoffrey Hinton. 2007. Three new
graphical models for statistical language modelling. In
Proc. ICML ?07, pages 641?648, New York, NY, USA.
Andriy Mnih and Geoffrey E Hinton. 2008. A scalable
hierarchical distributed language model. In D. Koller,
D. Schuurmans, Y. Bengio, and L. Bottou, editors, Ad-
vances in Neural Information Processing Systems 21,
volume 21, pages 1081?1088.
Thomas R. Niesler. 1997. Category-based statistical
language models. Ph.D. thesis, University of Cam-
bridge.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proc. ACL?03, pages
160?167, Sapporo, Japan.
Ilya Oparin, Ondr?ej Glembek, Luka?s? Burget, and Jan
C?ernocky?. 2008. Morphological random forests for
language modeling of inflectional languages. In Proc.
SLT?08, pages 189?192.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic evalu-
ation of machine translation. In Proc. ACL?02, pages
311?318, Philadelphia.
Ronald Rosenfeld. 1996. A maximum entropy approach
to adaptive statistical language modeling. Computer,
Speech and Language, 10:187?228.
Holger Schwenk and Jean-Luc Gauvain. 2002. Connec-
tionist language modeling for large vocabulary contin-
uous speech recognition. In Proc. ICASSP, pages 765?
768, Orlando, FL.
Holger Schwenk, Daniel De?chelotte, and Jean-Luc Gau-
vain. 2006. Continuous space language models
for statistical machine translation. In Proc. COL-
ING/ACL?06, pages 723?730.
Holger Schwenk. 2007. Continuous space language
models. Comput. Speech Lang., 21(3):492?518.
Yeh W. Teh. 2006. A hierarchical Bayesian language
model based on Pitman-Yor processes. In Proc. of
ACL?06, pages 985?992, Sidney, Australia.
Peng Xu and Frederik Jelinek. 2004. Random forests in
language modeling. In Proceedings of EMNLP?2004,
pages 325?332, Barcelona, Spain.
788
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 933?943,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Assessing Phrase-Based Translation Models with Oracle Decoding
Guillaume Wisniewski and Alexandre Allauzen and Fran?cois Yvon
Univ. Paris Sud ; LIMSI?CNRS
91403 ORSAY CEDEX
France
{wisniews,allauzen,yvon}@limsi.fr
Abstract
Extant Statistical Machine Translation (SMT) sys-
tems are very complex softwares, which embed mul-
tiple layers of heuristics and embark very large num-
bers of numerical parameters. As a result, it is diffi-
cult to analyze output translations and there is a real
need for tools that could help developers to better
understand the various causes of errors.
In this study, we make a step in that direction and
present an attempt to evaluate the quality of the
phrase-based translation model. In order to identify
those translation errors that stem from deficiencies
in the phrase table (PT), we propose to compute the
oracle BLEU-4 score, that is the best score that a
system based on this PT can achieve on a reference
corpus. By casting the computation of the oracle
BLEU-1 as an Integer Linear Programming (ILP)
problem, we show that it is possible to efficiently
compute accurate lower-bounds of this score, and re-
port measures performed on several standard bench-
marks. Various other applications of these oracle de-
coding techniques are also reported and discussed.
1 Phrase-Based Machine Translation
1.1 Principle
A Phrase-Based Translation System (PBTS) consists of a
ruleset and a scoring function (Lopez, 2009). The ruleset,
represented in the phrase table, is a set of phrase1pairs
{(f, e)}, each pair expressing that the source phrase f
can be rewritten (translated) into a target phrase e. Trans-
lation hypotheses are generated by iteratively rewriting
portions of the source sentence as prescribed by the rule-
set, until each source word has been consumed by exactly
one rule. The order of target words in an hypothesis is
uniquely determined by the order in which the rewrite op-
eration are performed. The search space of the translation
model corresponds to the set of all possible sequences of
1Following the usage in statistical machine translation literature, we
use ?phrase? to denote a subsequence of consecutive words.
rules applications. The scoring function aims to rank all
possible translation hypotheses in such a way that the best
one has the highest score.
A PBTS is learned from a parallel corpus in two inde-
pendent steps. In a first step, the corpus is aligned at the
word level, by using alignment tools such as Giza++
(Och and Ney, 2003) and some symmetrisation heuris-
tics; phrases are then extracted by other heuristics (Koehn
et al, 2003) and assigned numerical weights. In the
second step, the parameters of the scoring function are
estimated, typically through Minimum Error Rate train-
ing (Och, 2003).
Translating a sentence amounts to finding the best scor-
ing translation hypothesis in the search space. Because
of the combinatorial nature of this problem, translation
has to rely on heuristic search techniques such as greedy
hill-climbing (Germann, 2003) or variants of best-first
search like multi-stack decoding (Koehn, 2004). More-
over, to reduce the overall complexity of decoding, the
search space is typically pruned using simple heuristics.
For instance, the state-of-the-art phrase-based decoder
Moses (Koehn et al, 2007) considers only a restricted
number of translations for each source sequence2 and en-
forces a distortion limit3 over which phrases can be re-
ordered. As a consequence, the best translation hypothe-
sis returned by the decoder is not always the one with the
highest score.
1.2 Typology of PBTS Errors
Analyzing the errors of a SMT system is not an easy task,
because of the number of models that are combined, the
size of these models, and the high complexity of the vari-
ous decision making processes. For a SMT system, three
different kinds of errors can be distinguished (Germann
et al, 2004; Auli et al, 2009): search errors, induction
errors and model errors. The former corresponds to cases
where the hypothesis with the best score is missed by
the search procedure, either because of the use of an ap-
2the ttl option of Moses, defaulting to 20.
3the dl option of Moses, whose default value is 7.
933
proximate search method or because of the restrictions of
the search space. Induction errors correspond to cases
where, given the model, the search space does not contain
the reference. Finally, model errors correspond to cases
where the hypothesis with the highest score is not the best
translation according to the evaluation metric.
Model errors encompass several types of errors that oc-
cur during learning (Bottou and Bousquet, 2008)4. Ap-
proximation errors are errors caused by the use of a re-
stricted and oversimplistic class of functions (here, finite-
state transducers to model the generation of hypotheses
and a linear scoring function to discriminate them) to
model the translation process. Estimation errors corre-
spond to the use of sub-optimal values for both the phrase
pairs weights and the parameters of the scoring function.
The reasons behind these errors are twofold: first, train-
ing only considers a finite sample of data; second, it re-
lies on error prone alignments. As a result, some ?good?
phrases are extracted with a small weight, or, in the limit,
are not extracted at all; and conversely that some ?poor?
phrases are inserted into the phrase table, sometimes with
a really optimistic score.
Sorting out and assessing the impact of these various
causes of errors is of primary interest for SMT system
developers: for lack of such diagnoses, it is difficult to
figure out which components of the system require the
most urgent attention. Diagnoses are however, given the
tight intertwining among the various component of a sys-
tem, very difficult to obtain: most evaluations are limited
to the computation of global scores and usually do not
imply any kind of failure analysis.
1.3 Contribution and organization
To systematically assess the impact of the multiple
heuristic decisions made during training and decoding,
we propose, following (Dreyer et al, 2007; Auli et al,
2009), to work out oracle scores, that is to evaluate the
best achievable performances of a PBTS. We aim at both
studying the expressive power of PBTS and at providing
tools for identifying and quantifying causes of failure.
Under standard metrics such as BLEU (Papineni et al,
2002), oracle scores are difficult (if not impossible) to
compute, but, by casting the computation of the oracle
unigram recall and precision as an Integer Linear Pro-
gramming (ILP) problem, we show that it is possible to
efficiently compute accurate lower-bounds of the oracle
BLEU-4 scores and report measurements performed on
several standard benchmarks.
The main contributions of this paper are twofold. We
first introduce an ILP program able to efficiently find
the best hypothesis a PBTS can achieve. This program
can be easily extended to test various improvements to
4We omit here optimization errors.
phrase-base systems or to evaluate the impact of differ-
ent parameter settings. Second, we present a number of
complementary results illustrating the usage of our or-
acle decoder for identifying and analyzing PBTS errors.
Our experimental results confirm the main conclusions of
(Turchi et al, 2008), showing that extant PBTs have the
potential to generate hypotheses having very high BLEU-
4 score and that their main bottleneck is their scoring
function.
The rest of this paper is organized as follows: in Sec-
tion 2, we introduce and formalize the oracle decoding
problem, and present a series of ILP problems of increas-
ing complexity designed so as to deliver accurate lower-
bounds of oracle score. This section closes with various
extensions allowing to model supplementary constraints,
most notably reordering constraints (Section 2.5). Our
experiments are reported in Section 3, where we first in-
troduce the training and test corpora, along with a de-
scription of our system building pipeline (Section 3.1).
We then discuss the baseline oracle BLEU scores (Sec-
tion 3.2), analyze the non-reachable parts of the reference
translations, and comment several complementary results
which allow to identify causes of failures. Section 4 dis-
cuss our approach and findings with respect to the exist-
ing literature on error analysis and oracle decoding. We
conclude and discuss further prospects in Section 5.
2 Oracle Decoder
2.1 The Oracle Decoding Problem
Definition To get some insights on the errors of phrase-
based systems and better understand their limits, we pro-
pose to consider the oracle decoding problem defined as
follows: given a source sentence, its reference transla-
tion5 and a phrase table, what is the ?best? translation
hypothesis a system can generate? As usual, the quality
of an hypothesis is evaluated by the similarity between
the reference and the hypothesis. Note that in the ora-
cle decoding problem, we are only assessing the ability
of PBT systems to generate good candidate translations,
irrespective of their ability to score them properly.
We believe that studying this problem is interesting for
various reasons. First, as described in Section 3.4, com-
paring the best hypothesis a system could have gener-
ated and the hypothesis it actually generates allows us to
carry on both quantitative and qualitative failure analysis.
The oracle decoding problem can also be used to assess
the expressive power of phrase-based systems (Auli et
al., 2009). Other applications include computing accept-
able pseudo-references for discriminative training (Till-
mann and Zhang, 2006; Liang et al, 2006; Arun and
5The oracle decoding problem can be extended to the case of multi-
ple references. For the sake of simplicity, we only describe the case of
a single reference.
934
Koehn, 2007) or combining machine translation systems
in a multi-source setting (Li and Khudanpur, 2009). We
have also used oracle decoding to identify erroneous or
difficult to translate references (Section 3.3).
Evaluation Measure To fully define the oracle de-
coding problem, a measure of the similarity between a
translation hypothesis and its reference translation has
to be chosen. The most obvious choice is the BLEU-4
score (Papineni et al, 2002) used in most machine trans-
lation evaluations.
However, using this metric in the oracle decoding
problem raises several issues. First, BLEU-4 is a met-
ric defined at the corpus level and is hard to interpret at
the sentence level. More importantly, BLEU-4 is not de-
composable6: as it relies on 4-grams statistics, the con-
tribution of each phrase pair to the global score depends
on the translation of the previous and following phrases
and can not be evaluated in isolation. Because of its non-
decomposability, maximizing BLEU-4 is hard; in partic-
ular, the phrase-level decomposability of the evaluation
metric is necessary in our approach.
To circumvent this difficulty, we propose to evaluate
the similarity between a translation hypothesis and a ref-
erence by the number of their common words. This
amounts to evaluating translation quality in terms of un-
igram precision and recall, which are highly correlated
with human judgements (Lavie et al, ). This measure
is closely related to the BLEU-1 evaluation metric and
the Meteor (Banerjee and Lavie, 2005) metric (when it is
evaluated without considering near-matches and the dis-
tortion penalty). We also believe that hypotheses that
maximize the unigram precision and recall at the sen-
tence level yield corpus level BLEU-4 scores close the
maximal achievable. Indeed, in the setting we will intro-
duce in the next section, BLEU-1 and BLEU-4 are highly
correlated: as all correct words of the hypothesis will be
compelled to be at their correct position, any hypothesis
with a high 1-gram precision is also bound to have a high
2-gram precision, etc.
2.2 Formalizing the Oracle Decoding Problem
The oracle decoding problem has already been consid-
ered in the case of word-based models, in which all trans-
lation units are bound to contain only one word. The
problem can then be solved by a bipartite graph matching
algorithm (Leusch et al, 2008): given a n?m binary ma-
trix describing possible translation links between source
words and target words7, this algorithm finds the subset
of links maximizing the number of words of the reference
that have been translated, while ensuring that each word
6Neither at the sentence (Chiang et al, 2008), nor at the phrase level.
7The (i, j) entry of the matrix is 1 if the ith word of the source can
be translated by the jth word of the reference, 0 otherwise.
is translated only once.
Generalizing this approach to phrase-based systems
amounts to solving the following problem: given a set
of possible translation links between potential phrases of
the source and of the target, find the subset of links so that
the unigram precision and recall are the highest possible.
The corresponding oracle hypothesis can then be easily
generated by selecting the target phrases that are aligned
with one source phrase, disregarding the others. In ad-
dition, to mimic the way OOVs are usually handled, we
match identical OOV tokens appearing both in the source
and target sentences. In this approach, the unigram pre-
cision is always one (every word generated in the oracle
hypothesis matches exactly one word in the reference).
As a consequence, to find the oracle hypothesis, we just
have to maximize the recall, that is the number of words
appearing both in the hypothesis and in the reference.
Considering phrases instead of isolated words has a
major impact on the computational complexity: in this
new setting, the optimal segmentations in phrases of both
the source and of the target have to be worked out in ad-
dition to links selection. Moreover, constraints have to
be taken into account so as to enforce a proper segmenta-
tion of the source and target sentences. These constraints
make it impossible to use the approach of (Leusch et al,
2008) and concur in making the oracle decoding prob-
lem for phrase-based models more complex than it is for
word-based models: it can be proven, using arguments
borrowed from (De Nero and Klein, 2008), that this prob-
lem is NP-hard even for the simple unigram precision
measure.
2.3 An Integer Program for Oracle Decoding
To solve the combinatorial problem introduced in the pre-
vious section, we propose to cast it into an Integer Lin-
ear Programming (ILP) problem, for which many generic
solvers exist. ILP has already been used in SMT to find
the optimal translation for word-based (Germann et al,
2001) and to study the complexity of learning phrase
alignments (De Nero and Klein, 2008) models. Follow-
ing the latter reference, we introduce the following vari-
ables: fi,j (resp. ek,l) is a binary indicator variable that
is true when the phrase contains all spans from between-
word position i to j (resp. k to l) of the source (resp.
target) sentence. We also introduce a binary variable, de-
noted ai,j,k,l, to describe a possible link between source
phrase fi,j and target phrase ek,l. These variables are
built from the entries of the phrase table according to se-
lection strategies introduced in Section 2.4. In the fol-
lowing, index variables are so that:
0 ? i < j ? n, in the source sentence and
0 ? k < l ? m, in the target sentence,
935
where n (resp. m) is the length of the source (resp. target)
sentence.
Solving the oracle decoding problem then amounts to
optimizing the following objective function:
max
i,j,k,l
?
i,j,k,l
ai,j,k,l ? (l ? k) , (1)
under the constraints:
?x ? J1,mK :
?
k,l s.t. k?x?l
ek,l ? 1 (2)
?y ? J1, nK :
?
i,j s.t. i?y?j
fi,j = 1 (3)
?k, l :
?
i,j
ai,j,k,l = fk,l (4)
?i, j :
?
k,l
ai,j,k,l = ei,j (5)
The objective function (1) corresponds to the number
of target words that are generated. The first set of con-
straints (2) ensures that each word in the reference e ap-
pears in no more than one phrase. Maximizing the objec-
tive under these constraints amounts to maximizing the
unigram recall. The second set of constraints (3) ensures
that each word in the source f is translated exactly once,
which guarantees that the search space of the ILP prob-
lem is the same as the search space of a phrase-based sys-
tem. Constraints (4) bind the fk,l and ai,j,k,l variables,
ensuring that whenever a link ai,j,k,l is active, the corre-
sponding phrase fk,l is also active. Constraints (5) play a
similar role for the reference.
The Relaxed Problem Even though it accurately
models the search space of a phrase-based decoder,
this programs is not really useful as is: due to out-of-
vocabulary words or missing entries in the phrase table,
the constraint that all source words should be translated
yields infeasible problems8. We propose to relax this
problem and allow some source words to remain untrans-
lated. This is done by replacing constraints (3) by:
?y ? J1, nK :
?
i,j s.t. i?y?j
fi,j ? 1
To better reflect the behavior of phrase-based decoders,
which attempt to translate all source words, we also need
to modify the objective function as follows:
?
i,j,k,l
ai,j,k,l ? (l ? k) +
?
i,j
fi,j ? (j ? i) (6)
The second term in this new objective ensures that opti-
mal solutions translate as many source words as possible.
8An ILP problem is said to be infeasible when every possible solu-
tion violates at least one constraint.
The Relaxed-Distortion Problem A last caveat
with the Relaxed optimization program is caused by
frequently occurring source tokens, such as function
words or punctuation signs, which can often align with
more than one target word. For lack of taking distor-
tion information into account in our objective function,
all these alignments are deemed equivalent, even if some
of them are clearly more satisfactory than others. This
situation is illustrated on Figure 1.
le chat et le chien
the cat and the dog
Figure 1: Equivalent alignments between ?le? and ?the?. The
dashed lines corresponds to a less interpretable solution.
To overcome this difficulty, we propose a last change
to the objective function:
?
i,j,k,l
ai,j,k,l ? (l ? k) +
?
i,j
fi,j ? (j ? i)
??
?
i,j,k,l
ai,j,k,l|k ? i| (7)
Compared to the objective function of the relaxed prob-
lem (6), we introduce here a supplementary penalty factor
which favors monotonous alignments. For each phrase
pair, the higher the difference between source and target
positions, the higher this penalty. If ? is small enough,
this extra term allows us to select, among all the opti-
mal alignments of the relaxed problem, the one with
the lowest distortion. In our experiments, we set ? to
min {n,m} to ensure that the penalty factor is always
smaller than the reward for aligning two single words.
2.4 Selecting Indicator Variables
In the approach introduced in the previous sections, the
oracle decoding problem is solved by selecting, among
a set of possible translation links, the ones that yield the
solution with the highest unigram recall.
We propose two strategies to build this set of possible
translation links. In the first one, denoted exact match,
an indicator ai,j,k,l is created if there is an entry (f, e) so
that f spans from word position i to j in the source and
e from word position k to l in the target. In this strat-
egy, the ILP program considers exactly the same ruleset
as conventional phrase-based decoders.
We also consider an alternative strategy, which could
help us to identify errors made during the phrase extrac-
tion process. In this strategy, denoted inside match, an
indicator ai,j,k,l is created when the following three cri-
teria are met: i) f spans from position i to j of the source;
ii) a substring of e, denoted e?, spans from position k to l
936
of the reference; iii) (f, e?) is not an entry of the phrase ta-
ble. The resulting set of indicator variables thus contains,
at least, all the variables used in the exact match strategy.
In addition, we license here the use of phrases containing
words that do not occur in the reference. In fact, using
such solutions can yield higher BLEU scores when the
reward for additional correct matches exceeds the cost
incurred by wrong predictions. These cases are symp-
toms of situations where the extraction heuristic failed to
extract potentially useful subphrases.
2.5 Oracle Decoding with Reordering Constraints
The ILP problem introduced in the previous section can
be extended in several ways to describe and test various
improvements to phrase-based systems or to evaluate the
impact of different parameter settings. This flexibility
mainly stems from the possibility offered by our frame-
work to express arbitrary constraints over variables. In
this section, we illustrate these possibilities by describing
how reordering constraints can easily be considered.
As a first example, the Moses decoder uses a distortion
limit to constrain the set of possible reorderings. This
constraint ?enforces (...) that the last word of a phrase
chosen for translation cannot be more than d9 words from
the leftmost untranslated word in the source? (Lopez,
2009) and is expressed as:
?aijkl, ai?j?k?l? s.t. k > k
?,
aijkl ? ai?j?k?l? ? |j ? i
? + 1| ? d,
The maximum distortion limit strategy (Lopez, 2009) is
also easily expressed and take the following form (assum-
ing this constraint is parameterized by d):
?l < m? 1,
ai,j,k,l?ai?,j?,l+1,l? ? |i
? ? j ? 1| < d
Implementing the ?local? or MJ-d (Kumar and Byrne,
2005) reordering strategy is also straightforward, and im-
plies using the following constraints:
?i, k,
?
?
?
?
?
?
?
i??i
ai?,j?,k?,l? ?
?
k??k
ai?,j?,k?,l?
?
?
?
?
?
?
? d
Similarly, It is possible to simulate decoding under the
so-called IBM(d) reordering constraints10 by considering
the following constraints:
?? ? m, max
i,k,l
j??
ai,j,k,l ? j ?
?
i,j,k,l
ai,j,k,l ? (j ? i) ? d
9This corresponds to the dl parameter of Moses
10Under IBM(d) constraints, the translation is done, phrase by phrase,
from the beginning of the sentence until the end and only one of the first
d untranslated phrase can be selected for translation.
In these constraints, the first factor corresponds to the
rightmost translated word of the source and the second
one to the number of translated source words. The con-
straints simply enforce that, at each step of the decoding,
there are no more than d source words that were skipped.
Note that the constraints introduced above are not all
linear in the problem variables; however they can eas-
ily be linearized using standard ILP techniques (Roth and
Yih, 2005).
3 Oracle Decoding for Failure Analysis
3.1 Experimental Setting
We propose to use our oracle decoder to study the ability
of a PBTS to translate from English to French and from
German to English. These two languages pairs present
different challenges: English to French translation is con-
sidered a relatively easy pair, notwithstanding the diffi-
culties of generating the right inflection marks in French.
Translating from German into English is more difficult,
notably due to the productivity of inflectional and com-
pounding processes in German, and also to significant
differences in word ordering between these languages.
Our experiments are based on the corpora distributed
for the WMT?09 constrained tasks (Callison-Burch et
al., 2009). All data are tokenized, cleaned and con-
verted to lowercase letters using the tools provided
by the organizers. We then used a standard training
pipeline to construct the translation model: the bitexts
were aligned using Giza++11, symmetrized using the
grow-diag-final-and heuristic; the phrase table
was extracted and scored using the tools distributed with
Moses.12 Finally, baseline systems were optimized using
WMT?08 test set as development using MERT. Note that,
for all these steps, we used the default value of the var-
ious parameters. The extracted phrase table is then used
to find the oracle alignment on the task test set. Recall
that oracle decoding do not use the scores estimated by
Moses in any way.
In the experiments reported below, two settings are
considered. In the first one, denoted NEWSCO, Moses
was trained only on a small data set taken from the News
Commentary corpus. Using a small sized corpus reduces
both training time and decoding time, which allows us to
quickly test different configurations of the decoder. In a
second setting, denoted EUROPARL, Moses was trained
on a larger corpora containing the entirety of the Europarl
Corpus, but no in-domain data, to provide results on more
realistic conditions. Statistics regarding the different cor-
pora used are reported in Table 1. These statistics are
computed on the lowercase cleaned corpora.
11http://www.fjoch.com/GIZA++.html
12http://statmt.org/moses
937
en ? fr de ? en
NEWSCO EUROPARL NEWSCO EUROPARL
#words 1, 023, 401 21, 616, 114 1, 530, 693 22, 898, 644
#sentences 51, 375 1, 050, 398 71, 691 1, 118, 399
#vocabulary 31, 416 78, 071 78, 140 242, 219
#phrase table 3, 061, 701 46, 003, 525 4, 133, 190 44, 402, 367
% OOV 5.3% 3.1% 8.0% 5.2%
Table 1: Statistics regarding the training corpora: number of words, number of sentences, vocabulary and phrase table size and
percentage of test words not appearing in the train set (OOV).
Finding the oracle alignment amounts to solving the
ILP problems introduced above. Even though ILP prob-
lems are NP-hard in general, there exist several off-the-
shelf ILP solvers able to efficiently find an optimal solu-
tion or decide that the problem is infeasible. In our exper-
iments, we used the free solver SCIP (Achterberg, 2007).
An optimal solution was found for all problems we con-
sidered. Decoding the 3, 027 sentences of WMT?09 test
set takes about 10 minutes (wall time) for the NEWSCO
setting, and several hours for the EUROPARL setting13.
3.2 Oracle BLEU Score
Table 2 reports, for all considered settings, the BLEU-4
scores14 achieved by our oracle decoder, as well as the
number of source words used to generate the oracle hy-
pothesis and the number of target words that are reach-
able. In these experiments, two objective functions were
considered: first, we only consider the objective function
corresponding to the relaxed problem defined by Eq. (6);
second, we introduced an extra term in the objective to
penalize distortion, as described by Eq. (7). Unless ex-
plicitly stated otherwise, we always used the exact match
strategy.
The main result in 2 is that, for the two language pairs
considered, the expressive power of PBTS is not the lim-
iting factor to achieve high translation performance. In
fact, for most sentences in the test set, excellent oracle
hypotheses, which contain a very high proportion of ref-
erence words, are found. This remains true even when the
phrase table is extracted from a small corpus. Given that
the best BLEU-4 scores achieved during the WMT?09
evaluation are about 28 for the English to French task
and 24 for the German to English task ((Callison-Burch
et al, 2009), Tables 26 and 25), these results strongly
suggest that the main bottleneck of current phrase-based
translation systems is their scoring function rather than
their expressive power. As we will discuss in Section 4,
similar conclusions were drawn by (Auli et al, 2009) and
(Turchi et al, 2008).
Several additional comments on these numbers are in
13All our experiments are run on a 8 cores computer, each core being
a 2.2GHz Intel Processor; the decoder is multi-threaded.
14These are computed on lowercase with the default tokenization.
order. Despite these very high BLEU scores, in most
cases, the reference is only partly translated. In the most
favorable case, for the English to French EUROPARL set-
ting, only 26% of the references could be fully gener-
ated15. These numbers are consistent with the results re-
ported in (Auli et al, 2009). Similarly, only about 31%
of the source sentences are completely translated by the
oracle decoder, which supports our choice to consider a
relaxed version of the ILP problem. Finally, Table 2 also
shows that introducing the distortion penalty does not af-
fect the oracle performance of the decoder.
Considering the inside match strategy improves the
performance of the oracle decoder: for instance, for the
English to French NEWSCO setting, oracle decoder with
the inside match strategy achieves a BLEU-4 score of
70.15 (a 2.5 points improvement over the baseline). To
achieve this score, 21.45% of the phrases used during de-
coding were phrases that are not considered by the exact
match strategy. Similar results can be observed for other
settings, which highlights the significance of one kind of
failure of the extraction heuristic: useful ?subphrases? of
actual phrase pairs are not always extracted.
The numbers in Table 2, no matter how good they may
look, should be considered with caution: they only imply
that, for most test sentences, all the information necessary
to produce a good translation is available in the phrase ta-
ble. However, the alignment decisions underlying these
oracle hypotheses are sometimes hard to justify, and one
has to accept that part of these good hypotheses transla-
tions are due to a series of lucky alignment errors. This
is illustrated on Figure 2, which displays one such lucky
oracle alignment based on the misalignment, during train-
ing, of the French preposition ?des? (of the) with the En-
glish noun ?stock?. Such lucky errors are naturally also
observed in the outputs of conventional decoders, even
though phrase table filtering heuristics probably makes
them somewhat more rare.
3.3 Analyzing Non-Reachable Parts of a Reference
Table 3 contains typical examples of sentence pairs that
could not be fully generated by our oracle decoder. They
15Similar numbers were obtained, albeit much more slowly, with the
--constraint option of Moses.
938
training set objective function % source translated % target generated 4-BLEU
en ? fr
NEWSCO
RELAXED 86.04% 84.74% 67.65
RELAXED-DISTORTION 85.99% 84.77% 67.77
EUROPARL
RELAXED 93.66% 93.06% 85.05
RELAXED-DISTORTION 93.65% 93.06% 85.08
de ? en
NEWSCO
RELAXED 82.57% 82.33% 64.60
RELAXED-DISTORTION 82.59% 82.30% 64.65
EUROPARL
RELAXED 90.34% 91.16% 81.77
RELAXED-DISTORTION 90.36% 91.12% 81.77
Table 2: Translation score of the ILP oracle decoder for the various settings described in Section 3.1
stock fall in asia
chute des actions en asie
Figure 2: Example of alignment obtained by our oracle decoder
illustrate the three main reasons which cause some parts
of the reference to remain unreachable:
? phrases are missing from the phrase table, either
because they do not occur in the training corpus
(OOVs) or because they failed to be extracted. In
Table 3, OOV errors are mainly due to past tense
forms translated into verbs conjugated in passe? sim-
ple (?rejeta?, ?rencontre`rent?, ?renoua?) a French
literary tense, mostly used in formal writings.
? obvious errors (misspelled words, misinterpretation
or mistranslation, ...) in the reference. The refer-
ence of the fifth example contains one such error:
the state name ?Nevada? is translated to ?n?e?vadiez?
(literally ?have not escaped?), yielding a very poor
reference sentence.
? parts of the reference have no translation equiva-
lence in the source. This can be either because ref-
erences are produced in ?context? and some pieces
of information are moved across sentence bound-
aries or because these references are non-literal. The
fourth example, which seems to be the translation of
a title, falls into this category: the French part con-
tains a reference to the context (?les SA? is referring
to the bacteria the text is talking about) which is not
in the source text. Non-literal translation are illus-
trated by the third example, where English ?Mon-
day? is translated into French ?la veille? (the day
before).
While the first kind of errors is inherent to the use of
a statistical approach, the last two kinds result from the
quality of the data used in the evaluation and directly im-
pact both training and evaluation of automatic translation
systems: if they should not distort too much comparisons
of MT systems, these errors prevent us from assessing
the ?global? quality of automatic translation and, if sim-
ilar errors are found in the train set, they make learning
harder as some probability mass is wasted to model them.
To provide a more quantitative analysis, we manually
looked at all the non-aligned parts of some WMT?09 ref-
erences and found that out of 800 references, more than
133 contain either an obvious translation error or can not
be achieved by a PBTS16. Note that, while identifying
these errors could be done in many ways, our oracle de-
coder makes it far easier.
3.4 Identifying Causes of Failure
By comparing the hypotheses found by the oracle de-
coder and the ones found by the phrase-based decoder,
causes of failure can be easily identified. In this section,
we will present several measures that allow us to identify
and quantify several causes of failure.
Errors Caused by Search Space Pruning Recall from
Section 1.1 that Moses uses several heuristics to prune the
search space. In particular, there is a distortion limit and
a limit on the number of target phrases considered for one
source phrase. In this paragraph, we evaluate the impact
of these two heuristics on translation quality.
Table 4 presents the average distortion computed on
the oracle hypotheses, as well as the percentage of
phrases used that have a distortion strictly greater than
6 (the default distortion limit of Moses). All these num-
bers are obtained by solving the RELAXED-DISTORTION
problem. Surprisingly enough, the average distortion of
oracle hypotheses is quite small, even for the German to
English task, and the distortion constraint seems to be vi-
olated only in a few cases. It also appears that the distor-
tion of the hypotheses generated in the NEWSCO setting
is significantly larger than in the EUROPARL setting. This
can be explained by the extra degrees of freedom in the
16Annotation at a finer level is an on-going effort; the annotated
corpus is available from http://www.limsi.fr/Individu/
wisniews/oracle decoding.
939
? ? On Monday the American House of Representatives rejected the plan to support the financial
system, into which up to 700 billion dollars (nearly 12 billion Czech crowns) was to be invested.
? Lundi, la chambre des repre?sentants ame?ricaine rejeta le projet de soutient du syste`me financier,
auquel elle aurait du? consacrer jusqu?a` 700 milliards de dollars (pre`s de 12 bilions de kc?).
? ? Representatives of the legislators met with American Finance Minister Henry Paulson Saturday
night in order to give the government fund a final form.
? Dans la nuit de samedi a` dimanche, des repre?sentants des le?gislateurs rencontre`rent le ministre
des finances ame?ricain Henry Paulson, afin de donner au fond du gouvernement une forme finale.
? ? The Prague Stock Market immediately continued its fall from Monday at the beginning of
Tuesday?s trading , when it dropped by nearly six percent.
? Mardi, de`s le de?but des e?changes, la bourse de prague renoua avec sa chute de la veille,
lorsqu?elle perdait presque six pour cent.
? ? Antibiotic Resistance
? Les SA re?sistent aux antibiotiques.
? ? According to Nevada Democratic senator Harry Reid, that is how that legislators are trying to
have Congress to reach a definitive agreement as early as on Sunday.
? D?apre`s le se?nateur de`mocrate n?e?vadiez Harry Reid, les le?gislateurs font de sorte que le Congre`s
aboutisse a` un accord de?finitif de`s dimanche.
Table 3: Output examples of our oracle decoder on the English to French task. Words in bold are non-aligned words and words in
italic are non-aligned out-of-vocabulary words. For clarity the examples have been detokenized and recased.
training set avg.
distortion
%phrases
with a dist.
> 6
en ? fr
NEWSCO 4.57 22.02%
EUROPARL 3.21 13.32%
de ? en
NEWSCO 5.16 25.37%
EUROPARL 3.81 17.21%
Table 4: Average distortion and percentage of phrases with a
distortion greater that Moses default distortion limit.
alignment decisions enabled by the use of larger training
corpora and phrase table.
To evaluate the impact of the second heuristic, we com-
puted the number of phrases discarded by Moses (be-
cause of the default ttl limit) but used in the oracle hy-
potheses. In the English to French NEWSCO setting,
they account for 34.11% of the total number of phrases
used in the oracle hypotheses. When the oracle decoder
is constrained to use the same phrase table as Moses, its
BLEU-4 score drops to 42.78. This shows that filtering
the phrase table prior to decoding discards many useful
phrase pairs and is seriously limiting the best achievable
performance, a conclusion shared with (Auli et al, 2009).
Search Errors Search errors can be identified by com-
paring the score of the best hypothesis found by Moses
and the score of the oracle hypothesis. If the score of the
oracle hypothesis is higher, then there has been a search
error; on the contrary, there has been an estimation error
when the score of the oracle hypothesis is lower than the
score of the best hypothesis found by Moses.
Based on the comparison of the score of Moses hy-
potheses and of oracle hypotheses for the English to
French NEWSCO setting, our preliminary conclusion is
that the number of search errors is quite limited: only
about 5% of the hypotheses of our oracle decoder are ac-
tually getting a better score than Moses solutions. Again,
this shows that the scoring function (model error) is
one of the main bottleneck of current PBTS. Compar-
ing these hypotheses is nonetheless quite revealing: while
Moses mostly selects phrase pairs with high translation
scores and generates monotonous alignments, our ILP de-
coder uses larger reorderings and less probable phrases
to achieve better solutions: on average, the reordering
score of oracle solutions is ?5.74, compared to ?76.78
for Moses outputs. Given the weight assigned through
MERT training to the distortion score, no wonder that
these hypotheses are severely penalized.
The Impact of Phrase Length The observed outputs
do not only depend on decisions made during the search,
but also on decisions made during training. One such
decision is the specification of maximal length for the
source and target phrases. In our framework, evaluating
the impact of this decision is simple: it suffices to change
the definition of indicator variables so as to consider only
alignments between phrases of a given length.
In the English-French NEWSCO setting, the most re-
strictive choice, when only alignments between single
words are authorized, yields an oracle BLEU-4 of 48.68;
however, authorizing phrases up to length 2 allows to
achieve an oracle value of 66.57, very close to the score
achieved when considering all extracted phrases (67.77).
940
This is corroborated with a further analysis of our ora-
cle alignments, which use phrases whose average source
length is 1.21 words (respectively 1.31 for target words).
If many studies have already acknowledged the predomi-
nance of ?small? phrases in actual translations, our oracle
scores suggest that, for this language pair, increasing the
phrase length limit beyond 2 or 3 might be a waste of
computational resources.
4 Related Work
To the best of our knowledge, there are only a few works
that try to study the expressive power of phrase-based ma-
chine translation systems or to provide tools for analyzing
potential causes of failure.
The approach described in (Auli et al, 2009) is very
similar to ours: in this study, the authors propose to find
and analyze the limits of machine translation systems by
studying the reference reachability. A reference is reach-
able for a given system if it can be exactly generated
by this system. Reference reachability is assessed using
Moses in forced decoding mode: during search, all hy-
potheses that deviate from the reference are simply dis-
carded. Even though the main goal of this study was to
compare the search space of phrase-based and hierarchi-
cal systems, it also provides some insights on the impact
of various search parameters in Moses, delivering con-
clusions that are consistent with our main results. As de-
scribed in Section 1.2, these authors also propose a typol-
ogy of the errors of a statistical translation systems, but
do not attempt to provide methods for identifying them.
The authors of (Turchi et al, 2008) study the learn-
ing capabilities of Moses by extensively analyzing learn-
ing curves representing the translation performances as a
function of the number of examples, and by corrupting
the model parameters. Even though their focus is more
on assessing the scoring function, they reach conclusions
similar to ours: the current bottleneck of translation per-
formances is not the representation power of the PBTS
but rather in their scoring functions.
Oracle decoding is useful to compute reachable
pseudo-references in the context of discriminative train-
ing. This is the main motivation of (Tillmann and Zhang,
2006), where the authors compute high BLEU hypothe-
ses by running a conventional decoder so as to maximize
a per-sentence approximation of BLEU-4, under a simple
(local) reordering model.
Oracle decoding has also been used to assess the
limitations induced by various reordering constraints in
(Dreyer et al, 2007). To this end, the authors propose
to use a beam-search based oracle decoder, which com-
putes lower bounds of the best achievable BLEU-4 us-
ing dynamic programming techniques over finite-state
(for so-called local and IBM constraints) or hierarchically
structured (for ITG constraints) sets of hypotheses. Even
though the numbers reported in this study are not directly
comparable with ours17, it seems that our decoder is not
only conceptually much simpler, but also achieves much
more optimistic lower-bounds of the oracle BLEU score.
The approach described in (Li and Khudanpur, 2009) em-
ploys a similar technique, which is to guide a heuristic
search in an hypergraph representing possible translation
hypotheses with n-gram counts matches, which amounts
to decoding with a n-gram model trained on the sole ref-
erence translation. Additional tricks are presented in this
article to speed-up decoding.
Computing oracle BLEU scores is also the subject of
(Zens and Ney, 2005; Leusch et al, 2008), yet with a
different emphasis. These studies are concerned with
finding the best hypotheses in a word graph or in a con-
sensus network, a problem that has various implications
for multi-pass decoding and/or system combination tech-
niques. The former reference describes an exponential
approximate algorithm, while the latter proves the NP-
completeness of this problem and discuss various heuris-
tic approaches. Our problem is somewhat more complex
and using their techniques would require us to built word
graphs containing all the translations induced by arbitrary
segmentations and permutations of the source sentence.
5 Conclusions
In this paper, we have presented a methodology for ana-
lyzing the errors of PBTS, based on the computation of
an approximation of the BLEU-4 oracle score. We have
shown that this approximation could be computed fairly
accurately and efficiently using Integer Linear Program-
ming techniques. Our main result is a confirmation of
the fact that extant PBTS systems are expressive enough
to achieve very high translation performance with respect
to conventional quality measurements. The main efforts
should therefore strive to improve on the way phrases and
hypotheses are scored during training. This gives further
support to attempts aimed at designing context-dependent
scoring functions as in (Stroppa et al, 2007; Gimpel and
Smith, 2008), or at attempts to perform discriminative
training of feature-rich models. (Bangalore et al, 2007).
We have shown that the examination of difficult-to-
translate sentences was an effective way to detect errors
or inconsistencies in the reference translations, making
our approach a potential aid for controlling the quality or
assessing the difficulty of test data. Our experiments have
also highlighted the impact of various parameters.
Various extensions of the baseline ILP program have
been suggested and/or evaluated. In particular, the ILP
formalism lends itself well to expressing various con-
straints that are typically used in conventional PBTS. In
17The best BLEU-4 oracle they achieve on Europarl German to En-
glish is approximately 48; but they considered a smaller version of the
training corpus and the WMT?06 test set.
941
our future work, we aim at using this ILP framework to
systematically assess various search configurations. We
plan to explore how replacing non-reachable references
with high-score pseudo-references can improve discrim-
inative training of PBTS. We are also concerned by de-
termining how tight is our approximation of the BLEU-
4 score is: to this end, we intend to compute the best
BLEU-4 score within the n-best solutions of the oracle
decoding problem.
Acknowledgments
Warm thanks to Houda Bouamor for helping us with the
annotation tool. This work has been partly financed by
OSEO, the French State Agency for Innovation, under
the Quaero program.
References
Tobias Achterberg. 2007. Constraint Integer Program-
ming. Ph.D. thesis, Technische Universita?t Berlin.
http://opus.kobv.de/tuberlin/volltexte/
2007/1611/.
Abhishek Arun and Philipp Koehn. 2007. Online learning
methods for discriminative training of phrase based statis-
tical machine translation. In Proc. of MT Summit XI, Copen-
hagen, Denmark.
Michael Auli, Adam Lopez, Hieu Hoang, and Philipp Koehn.
2009. A systematic analysis of translation model search
spaces. In Proc. of WMT, pages 224?232, Athens, Greece.
Satanjeev Banerjee and Alon Lavie. 2005. METEOR: An
automatic metric for MT evaluation with improved correla-
tion with human judgments. In Proc. of the ACL Workshop
on Intrinsic and Extrinsic Evaluation Measures for Machine
Translation and/or Summarization, pages 65?72, Ann Arbor,
Michigan.
Srinivas Bangalore, Patrick Haffner, and Stephan Kanthak.
2007. Statistical machine translation through global lexi-
cal selection and sentence reconstruction. In Proc. of ACL,
pages 152?159, Prague, Czech Republic.
Le?on Bottou and Olivier Bousquet. 2008. The tradeoffs of large
scale learning. In Proc. of NIPS, pages 161?168, Vancouver,
B.C., Canada.
Chris Callison-Burch, Philipp Koehn, Christof Monz, and Josh
Schroeder. 2009. Findings of the 2009 Workshop on Sta-
tistical Machine Translation. In Proc. of WMT, pages 1?28,
Athens, Greece.
David Chiang, Steve DeNeefe, Yee Seng Chan, and Hwee Tou
Ng. 2008. Decomposability of translation metrics for
improved evaluation and efficient algorithms. In Proc. of
ECML, pages 610?619, Honolulu, Hawaii.
John De Nero and Dan Klein. 2008. The complexity of phrase
alignment problems. In Proc. of ACL: HLT, Short Papers,
pages 25?28, Columbus, Ohio.
Markus Dreyer, Keith B. Hall, and Sanjeev P. Khudanpur. 2007.
Comparing reordering constraints for smt using efficient bleu
oracle computation. In NAACL-HLT/AMTA Workshop on
Syntax and Structure in Statistical Translation, pages 103?
110, Rochester, New York.
Ulrich Germann, Michael Jahr, Kevin Knight, Daniel Marcu,
and Kenji Yamada. 2001. Fast decoding and optimal decod-
ing for machine translation. In Proc. of ACL, pages 228?235,
Toulouse, France.
Ulrich Germann, Michael Jahr, Kevin Knight, Daniel Marcu,
and Kenji Yamada. 2004. Fast and optimal decoding for
machine translation. Artificial Intelligence, 154(1-2):127?
143.
Ulrich Germann. 2003. Greedy decoding for statistical ma-
chine translation in almost linear time. In Proc. of NAACL,
pages 1?8, Edmonton, Canada.
Kevin Gimpel and Noah A. Smith. 2008. Rich source-side
context for statistical machine translation. In Proc. of WMT,
pages 9?17, Columbus, Ohio.
Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Sta-
tistical phrase-based translation. In Proc. of NAACL, pages
48?54, Edmonton, Canada.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-
Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan,
Wade Shen, Christine Moran, Richard Zens, Chris Dyer, On-
drej Bojar, Alexandra Constantin, and Evan Herbst. 2007.
Moses: Open source toolkit for statistical machine transla-
tion. In Proc. of ACL, demonstration session.
Philipp Koehn. 2004. Pharaoh: A beam search decoder for
phrase-based statistical machine translation models. In Proc.
of AMTA, pages 115?124, Washington DC.
Shankar Kumar and William Byrne. 2005. Local phrase re-
ordering models for statistical machine translation. In Proc.
of HLT, pages 161?168, Vancouver, Canada.
Alon Lavie, Kenji Sagae, and Shyamsundar Jayaraman. The
significance of recall in automatic metrics for MT evaluation.
In In Proc. of AMTA, pages 134?143, Washington DC.
Gregor Leusch, Evgeny Matusov, and Hermann Ney. 2008.
Complexity of finding the BLEU-optimal hypothesis in a
confusion network. In Proc. of EMNLP, pages 839?847,
Honolulu, Hawaii.
Zhifei Li and Sanjeev Khudanpur. 2009. Efficient extraction
of oracle-best translations from hypergraphs. In Proc. of
NAACL, pages 9?12, Boulder, Colorado.
Percy Liang, Alexandre Bouchard-Co?te?, Dan Klein, and Ben
Taskar. 2006. An end-to-end discriminative approach to ma-
chine translation. In Proc. of ACL, pages 761?768, Sydney,
Australia.
Adam Lopez. 2009. Translation as weighted deduction. In
Proc. of EACL, pages 532?540, Athens, Greece.
Franz Josef Och and Hermann Ney. 2003. A systematic com-
parison of various statistical alignment models. Comput.
Linguist., 29(1):19?51.
Franz Josef Och. 2003. Minimum error rate training in statis-
tical machine translation. In Proc. of ACL, pages 160?167,
Sapporo, Japan.
Kishore Papineni, Salim Roukos, ToddWard, andWei-jing Zhu.
2002. Bleu: A method for automatic evaluation of machine
translation. Technical report, Philadelphia, Pennsylvania.
D. Roth and W. Yih. 2005. Integer linear programming infer-
ence for conditional random fields. In Proc. of ICML, pages
737?744, Bonn, Germany.
Nicolas Stroppa, Antal van den Bosch, and Andy Way. 2007.
Exploiting source similarity for smt using context-informed
942
features. In Andy Way and Barbara Gawronska, editors,
Proc. of TMI, pages 231?240, Sko?vde, Sweden.
Christoph Tillmann and Tong Zhang. 2006. A discriminative
global training algorithm for statistical mt. In Proc. of ACL,
pages 721?728, Sydney, Australia.
Marco Turchi, Tijl De Bie, and Nello Cristianini. 2008. Learn-
ing performance of a machine translation system: a statistical
and computational analysis. In Proc. of WMT, pages 35?43,
Columbus, Ohio.
Richard Zens and Hermann Ney. 2005. Word graphs for sta-
tistical machine translation. In Proc. of the ACL Workshop
on Building and Using Parallel Texts, pages 191?198, Ann
Arbor, Michigan.
943
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1779?1785,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Cross-Lingual Part-of-Speech Tagging through Ambiguous Learning
Guillaume Wisniewski Nicolas P?cheux Souhir Gahbiche-Braham Fran?ois Yvon
Universit? Paris Sud
LIMSI-CNRS
91 403 ORSAY CEDEX, France
{wisniews,pecheux,souhir,yvon}@limsi.fr
Abstract
When Part-of-Speech annotated data is
scarce, e.g. for under-resourced lan-
guages, one can turn to cross-lingual trans-
fer and crawled dictionaries to collect par-
tially supervised data. We cast this prob-
lem in the framework of ambiguous learn-
ing and show how to learn an accurate
history-based model. Experiments on ten
languages show significant improvements
over prior state of the art performance.
1 Introduction
In the past two decades, supervised Machine
Learning techniques have established new perfor-
mance standards for many NLP tasks. Their suc-
cess however crucially depends on the availability
of annotated in-domain data, a not so common sit-
uation. This means that for many application do-
mains and/or less-resourced languages, alternative
ML techniques need to be designed to accommo-
date unannotated or partially annotated data.
Several attempts have recently been made to
mitigate the lack of annotated corpora using par-
allel data pairing a (source) text in a resource-rich
language with its counterpart in a less-resourced
language. By transferring labels from the source
to the target, it becomes possible to obtain noisy,
yet useful, annotations that can be used to train a
model for the target language in a weakly super-
vised manner. This research trend was initiated
by Yarowsky et al. (2001), who consider the trans-
fer of POS and other syntactic information, and
further developed in (Hwa et al., 2005; Ganchev
et al., 2009) for syntactic dependencies, in (Pad?
and Lapata, 2009; Kozhevnikov and Titov, 2013;
van der Plas et al., 2014) for semantic role la-
beling and in (Kim et al., 2012) for named-entity
recognition, to name a few.
Assuming that labels can actually be projected
across languages, these techniques face the issue
of extending standard supervised techniques with
partial and/or uncertain labels in the presence of
alignment noise. In comparison to the early ap-
proach of Yarowsky et al. (2001) in which POS
are directly transferred, subject to heuristic fil-
tering rules, recent works consider the integra-
tion of softer constraints using expectation regu-
larization techniques (Wang and Manning, 2014),
the combination of alignment-based POS transfer
with additional information sources such as dic-
tionaries (Li et al., 2012; T?ckstr?m et al., 2013)
(Section 2), or even the simultaneous use of both
techniques (Ganchev and Das, 2013).
In this paper, we reproduce the weakly super-
vised setting of T?ckstr?m et al. (2013). By re-
casting this setting in the framework of ambiguous
learning (Bordes et al., 2010; Cour et al., 2011)
(Section 3), we propose an alternative learning
methodology and show that it improves the state of
the art performance on a large array of languages
(Section 4). Our analysis of the remaining errors
suggests that in cross-lingual settings, improve-
ments of error rates can have multiple causes and
should be looked at with great care (Section 4.2).
All tools and resources used in this study
are available at http://perso.limsi.fr/
wisniews/ambiguous.
2 Projecting Labels across Aligned
Corpora
Projecting POS information across languages re-
lies on a rather strong assumption that morpho-
syntactic categories in the source language can
be directly related to the categories in the tar-
get language, which might not always be war-
ranted (Evans and Levinson, 2009; Broschart,
2009). The universal reduced POS tagset pro-
posed by Petrov et al. (2012) defines an opera-
tional, albeit rather empirical, ground to perform
this mapping. It is made of the following 12 cat-
egories: NOUN (nouns), VERB (verbs), ADJ (ad-
1779
ar cs de el es fi fr id it sv
% of test covered tokens (type) 83.2 93.2 95.6 97.4 96.7 83.0 98.3 90.5 95.8 95.3
% of test correctly covered token (type) 72.9 94.2 93.7 92.9 93.8 93.6 92.1 89.6 93.6 94.1
avg. number of labels per token (type) 2.1 1.3 1.3 1.3 1.3 1.4 1.3 1.2 1.3 1.3
avg. number of labels per token (type+token) 1.7 1.1 1.1 1.1 1.1 1.2 1.2 1.1 1.1 1.1
% of aligned tokens 53.0 77.8 66.7 69.3 74.0 73.1 64.7 81.6 72.2 79.9
% of token const. violating type const. 2.5 16.0 15.8 21.4 16.9 14.3 16.1 19.3 17.5 13.6
% informative token const. 79.7 27.5 15.7 29.8 21.3 36.0 25.5 16.2 28.2 26.4
Table 1: Interplay between token and type constraints on our training parallel corpora. ?Informative?
token constraints correspond to tokens for which (a) a POS is actually transfered and (b) type constraints
do not disambiguate the label, but type+token constraints do.
jectives), ADV (adverbs), PRON (pronouns), DET
(determiners and articles), ADP (prepositions and
postpositions), NUM (numerals), CONJ (conjunc-
tions), PRT (particles), ?.? (punctuation marks)
and X (a catch-all for other categories). These
labels have been chosen for their stability across
languages and for their usefulness in various mul-
tilingual applications. In the rest of this work, all
annotations are mapped to this universal tagset.
Transfer-based methods have shown to be very
effective, even if projected labels only deliver a
noisy supervision, due to tagging (of the source
language) and other alignment errors (Yarowsky
et al., 2001). While this uncertainty can be ad-
dressed in several ways, recent works have pro-
posed to combine projected labels with monolin-
gual information in order to filter out invalid la-
bel sequences (Das and Petrov, 2011; T?ckstr?m
et al., 2013). In this work we follow T?ckstr?m et
al. (2013) and use two families of constraints:
Token constraints rely on word alignments to
project labels of source words to target words
through alignment links. Table 1 shows that, de-
pendening on the language, only 50?80% of the
target tokens would benefit from label transfer.
Type constraints rely on a tag dictionary to
define the set of possible tags for each word
type. Type constraints reduce the possible la-
bels for a given word and help filtering out cross-
lingual transfer errors (up to 20%, as shown in Ta-
ble 1). As in (T?ckstr?m et al., 2013), we con-
sider two different dictionaries. The first one is
extracted automatically from Wiktionary,
1
us-
ing the method of (Li et al., 2012). The second
tag dictionary is built by using for each word the
two most frequently projected POS labels from
the training data.
2
In contrast to T?ckstr?m et al.
1
http://www.wiktionary.org/
2
This heuristic is similar to the way T?ckstr?m et al.
(2013) we use the intersection
3
of the two type
constraints instead of their union. Table 1 shows
the precision and recall of the resulting constraints
on the test data.
These two information sources are merged ac-
cording to the rules of T?ckstr?m et al. (2013).
These rules assume that type constraints are more
reliable than token constraints and should take
precedence: by default, a given word is associated
to the set of possible tags licensed type constraints;
additionally, when a POS tag can be projected
through alignment and also satisfies the type con-
straints, then it is actually projected, thereby pro-
viding a full (yet noisy) supervision.
As shown in Table 1, token and type con-
straints complement each other effectively and
greatly reduce label ambiguity. However, the
transfer method sketched above associates each
target word with a set of possible labels, of which
only one is true. This situation is less favorable
than standard supervised learning in which one
unique gold label is available for each occurrence.
We describe in the following section how to learn
from this ambiguous supervision information.
3 Modeling Sequences under Ambiguous
Supervision
We use a history-based model (Black et al., 1992)
with a LaSO-like training method (Daum? and
Marcu, 2005). History-based models reduce struc-
tured prediction to a sequence of multi-class clas-
sification problems. The prediction of a complex
structure (here, a sequence of POS tags) is thus
modeled as a sequential decision problem: at each
(2013) filter the tag distribution with a threshold to build the
projected type constraints.
3
If the intersection is empty we use the constraints
from Wiktionary first, if also empty, the projected con-
straints then, and by default the whole tag set.
1780
position in the sequence, a multiclass classifier
is used to make a decision, using features that
describe both the input structure and the history
of past decisions (i.e. the partially annotated se-
quence).
Let x = (x
i
)
n
i=1
denote the observed sequence
and Y be the set of possible labels (in our case
the 12 universal POS tags). Inference consists in
predicting labels one after the other using, for in-
stance, a linear model:
y
?
i
= argmax
y?Y
?w|?(x, i, y, h
i
)? (1)
where ??|?? is the standard dot product operation,
y
?
i
the predicted label for position i, w the weight
vector, h
i
= y
?
1
, ..., y
?
i?1
the history of past de-
cisions and ? a joint feature map. Inference can
therefore be seen as a greedy search in the space
of the # {Y}
n
possible labelings of the input se-
quence. Trading off the global optimality of in-
ference for additional flexibility in the design of
features and long range dependencies between la-
bels has proved useful for many sequence labeling
tasks in NLP (Tsuruoka et al., 2011).
The training procedure, sketched in Algo-
rithm 1, consists in performing inference on each
input sentence and correcting the weight vector
each time a wrong decision is made. Impor-
tantly (Ross and Bagnell, 2010), the history used
during training has to be made of the previous pre-
dicted labels so that the training samples reflect the
fact that the history will be imperfectly known at
test time.
This reduction of sequence labeling to multi-
class classification allows us to learn a sequence
model in an ambiguous setting by building on the
theoretical results of Bordes et al. (2010) and Cour
et al. (2011). The decision about the correctness of
a prediction and the weight updates can be adapted
to the amount of supervision information that is
available.
Full Supervision In a fully supervised setting,
the correct label is known for each word token: a
decision is thus considered wrong when this gold
label is not predicted. In this case, a standard per-
ceptron update is performed:
w
t+1
? w
t
?? (x, i, y
?
i
, h
i
)+? (x, i, y?
i
, h
i
) (2)
where y
?
i
and y?
i
are the predicted and the gold la-
bel, respectively. This update is a stochastic gra-
dient step that increases the score of the gold label
while decreasing the score of the predicted label.
Ambiguous Supervision During training, each
observation i is now associated with a set of possi-
ble labels, denoted by
?
Y
i
. In this case, a decision is
considered wrong when the predicted label is not
in
?
Y
i
and the weight vector is updated as follows:
w
t+1
? w
t
?? (x, i, y
?
i
, h
i
)+
?
y?
i
?
?
Y
i
? (x, i, y?
i
, h
i
)
(3)
Compared to (2), this rule uniformly increases the
scores of all the labels in
?
Y
i
.
It can be shown (Bordes et al., 2010; Cour et
al., 2011), under mild assumptions (namely that
two labels never systematically co-occur in the
supervision information), that the update rule (3)
enables to learn a classifier in an ambiguous set-
ting, as if the gold labels were known. Intuitively,
as long as two labels are not systematically co-
occurring in
?
Y , updates will reinforce the correct
labels more often than the spurious ones; at the
end of training, the highest scoring label should
therefore be the correct one.
Algorithm 1 Training algorithm. In the ambigu-
ous setting,
?
Y
i
contains all possible labels; in the
supervised setting, it only contains the gold label.
w
0
? 0
for t ? J1, T K do
Randomly pick example x,
?
y
h? empty list
for i ? J1, nK do
y
?
i
= argmax
y?Y
?w
t
|?(x, i, y, h
i
)?
if y
?
i
/?
?
Y
i
then
w
t+1
? update(w
t
,x, i,
?
Y
i
, y
?
i
, h
i
)
end if
push(y
?
i
, h)
end for
end for
return
1
T
?
T
t=1
w
t
4 Empirical Study
Datasets Our approach is evaluated on 10 lan-
guages that present very different characteristics
and cover several language families.
4
In all our ex-
periments we use English as the source language.
Parallel sentences
5
are aligned with the standard
4
Resources considered in the related works are not freely
available, which prevents us from presenting a more complete
comparison.
5
All resources and features used in our experiments are
thoroughly documented in the supplementary material.
1781
ar cs de el es fi fr id it sv
HBAL 27.9 10.4 8.8 8.1 8.2 13.3 10.2 11.3 9.1 10.1
Partially observed CRF 33.9 11.6 12.2 10.9 10.7 12.9 11.6 16.3 10.4 11.6
HBSL ? 1.5 5.0 ? 2.4 5.9 3.5 4.8 2.8 3.8
HBAL + matched POS 24.1 7.6 8.0 7.3 7.4 12.2 7.4 9.8 8.3 8.8
(Ganchev and Das, 2013) 49.9 19.3 9.6 9.4 12.8 ? 12.5 ? 10.1 10.8
(T?ckstr?m et al., 2013) ? 18.9 9.5 10.5 10.9 ? 11.6 ? 10.2 11.1
(Li et al., 2012) ? ? 14.2 20.8 13.6 ? ? ? 13.5 13.9
Table 2: Error rate (in %) achieved by the method described in Sec. 3 trained in an ambiguous (HBAL)
or in a supervised setting (HBSL), a partially observed CRF and different state-of-the-art results.
MOSES pipeline, using the intersection heuristic
that only retains the most reliable alignment links.
The English side of the bitext is tagged using a
standard linear CRF trained on the Penn Treebank.
Tags are then transferred to the target language us-
ing the procedure described in Section 2. For each
language, we train a tagger using the method de-
scribed in Section 3 with T = 100 000 iterations
6
using a feature set similar to the one of Li et al.
(2012) and T?ckstr?m et al. (2013). The baseline
system is our reimplementation of the partially
observed CRF model of T?ckstr?m et al. (2013).
Evaluation is carried out on the test sets of tree-
banks for which manual gold tags are known. For
Czech and Greek, we use the CoNLL?07 Shared
Task on Dependency Parsing; for Arabic, the Ara-
bic Treebank; and otherwise the data of the Uni-
versal Dependency Treebank Project (McDonald
et al., 2013). Tagging performance is evaluated
with the standard error rate.
4.1 Results
Table 2 summarizes the performance achieved
by our method trained in the ambiguous setting
(HBAL) and by our re-implementation of the
partially supervised CRF baseline. As an upper
bound, we also report the score of our method
when trained in a supervised (HBSL) settings
considering the training part of the various tree-
banks, when it is available.
7
For the sake of com-
parison, we also list the best scores of previous
studies. Note, however, that a direct comparison
with these results is not completely fair as these
6
Preliminary experiments showed that increasing the
number of iterations T in Algorithm 1 has no significant
impact.
7
In this setting, HBSL implements an averaged percep-
tron, and achieves results that are similar to those obtained
with standard linear CRF.
systems were not trained and evaluated with the
same exact resources (corpora,
8
type constraints,
alignments, etc). Also note that the state-of-the-
art scores have been achieved by different models,
which have been selected based on their scores on
the test set and not on a validation set.
9
Experimental results show that HBAL signif-
icantly outperforms, on all considered languages
but one, the partially observed CRF that was
trained and tested in the same setting.
4.2 Discussion
The performance of our new method still falls
short of the performance of a fully supervised POS
tagger: for instance, in Spanish, full supervision
reduces the error rate by a factor of 4. A fine-
grained error analysis shows that many errors of
HBAL directly result from the fact that, contrary
to the fully supervised learner HBSL, our am-
biguous setting suffers from a train/test mismatch,
which has two main consequences. First, the train
and test sets do not follow exactly the same nor-
malization and tokenization conventions, which is
an obvious source of mistakes. Second, and more
importantly, many errors are caused by systematic
differences between the test tags and the super-
vised tags (i.e. the English side of the bitext and
Wiktionary). While some of these differences
are linguistically well-justified and reflect funda-
mental differences in the language structure and
usage, others seem to be merely due to arbitrary
annotation conventions.
For instance, in Greek, proper names are labeled
8
The test sets are only the same for Czech, Greek and
Swedish.
9
The partially observed CRF is the best model in (T?ck-
str?m et al., 2013) only for German (de), Greek (el) and
Swedish (sv), and uses only type constraints extracted from
Wiktionary.
1782
either as X (when they refer to a foreigner and are
not transliterated) or as NOUN (in all other cases),
while they are always labeled as NOUN in English.
In French and in Greek, contractions of a prepo-
sition and a determiner such as ????? (??? ???,
meaning ?to the?) or ?aux? (?? les? also meaning
?to the?) are labeled as ADP in the Universal De-
pendency Treebank but as DET in Wiktionary
and are usually aligned with a determiner in the
parallel corpora. In the Penn Treebank, quanti-
fiers like ?few? or ?little? are generally used in con-
junction with a determiner (?a few years?, ?a little
parable?, ...) and labeled as ADJ; the correspond-
ing Spanish constructions lack an article (?mucho
tempio?, ?pocos a?os?, ...) and the quantifiers are
therefore labeled as DET. Capturing such subtle
differences is hardly possible without prior knowl-
edge and specifically tailored features.
This annotation mismatch problem is all the
more important in settings like ours, that rely
on several, independently designed, information
sources, which follow contradictory annotation
conventions and for which the mapping to the uni-
versal tagset is actually error-prone (Zhang et al.,
2012). To illustrate this point, we ran three ad-
ditional experiments to assess the impact of the
train/test mismatch.
We first designed a control experiment in which
the type constraints were manually completed
with the gold labels of the most frequent errors of
HBAL. These errors generally concern function
words and can be assumed to result from system-
atic differences in the annotations rather than pre-
diction errors. For instance, for French the type
constraints for ?du?, ?des?, ?au? and ?aux? were cor-
rected from DET to ADP. The resulting model,
denoted ?HBAL + matched POS? in Table 2, sig-
nificantly outperforms HBAL, stressing the diver-
gence in the different annotation conventions.
Additionally, in order to approximate the am-
biguous setting train/test mismatch, we learn two
fully supervised Spanish taggers on the same train-
ing data as HBAL, using two different strategies
to obtain labeled data. We first use HBSL (which
was trained on the treebank) to automatically la-
bel the target side of the parallel corpus. In this
setting, the POS tagger is trained with data from
a different domain, but labeled with the same an-
notation scheme as a the test set. Learning with
this fully supervised data yields an error rate of
4.2% for Spanish, almost twice as much as HBSL,
bringing into light the impact of domain shift. We
then use a generic tagger, FREELING,
10
to label
the training data, this time with possible addi-
tional inconsistent annotations. The correspond-
ing error rate for Spanish was 6.1%, to be com-
pared with the 8.2% achieved by HBAL. The last
two control experiments show that many of the re-
maining labeling errors seem to be due to domain
and convention mismatches rather to the trans-
fer/ambiguous setting, as supervised models also
suffer from very similar conditions.
These observations show that the evaluation of
transfer-based methods suffer from several biases.
Their results must therefore be interpreted with
great care.
5 Conclusion
In this paper, we have presented a novel learning
methodology to learn from ambiguous supervision
information, and used it to train several POS tag-
gers. Using this method, we have been able to
achieve performance that surpasses the best re-
ported results, sometimes by a wide margin. Fur-
ther work will attempt to better analyse these re-
sults, which could be caused by several subtle
differences between HBAL and the baseline sys-
tem. Nonetheless, these experiments confirm that
cross-lingual projection of annotations have the
potential to help in building very efficient POS
taggers with very little monolingual supervision
data. Our analysis of these results also suggests
that, for this task, additional gains might be more
easily obtained by fixing systematic biases intro-
duced by conflicting mappings between tags or
by train/test domain mismatch than by designing
more sophisticated weakly supervised learners.
Acknowledgments
We wish to thank Thomas Lavergne and Alexan-
dre Allauzen for early feedback and for providing
us with the partially observed CRF implementa-
tion. We also thank the anonymous reviewers for
their helpful comments.
References
Ezra Black, Fred Jelinek, John Lafferty, David M.
Magerman, Robert Mercer, and Salim Roukos.
1992. Towards history-based grammars: Using
10
http://nlp.lsi.upc.edu/freeling/
1783
richer models for probabilistic parsing. In Proceed-
ings of the Workshop on Speech and Natural Lan-
guage, HLT ?91, pages 134?139, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Antoine Bordes, Nicolas Usunier, and Jason Weston.
2010. Label ranking under ambiguous supervision
for learning semantic correspondences. In ICML,
pages 103?110.
J?rgen Broschart. 2009. Why Tongan does it differ-
ently: Categorial distinctions in a language without
nouns and verbs. Linguistic Typology, 1:123?166,
10.
Timothee Cour, Ben Sapp, and Ben Taskar. 2011.
Learning from partial labels. Journal of Machine
Learning Research, 12:1501?1536, July.
Dipanjan Das and Slav Petrov. 2011. Unsupervised
part-of-speech tagging with bilingual graph-based
projections. In Proceedings of the 49th Annual
Meeting of the Association for Computational Lin-
guistics: Human Language Technologies - Volume
1, HLT ?11, pages 600?609, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Hal Daum?, III and Daniel Marcu. 2005. Learning
as search optimization: Approximate large margin
methods for structured prediction. In Proceedings
of the 22Nd International Conference on Machine
Learning, ICML ?05, pages 169?176, New York,
NY, USA. ACM.
Nicholas Evans and Stephen C. Levinson. 2009. The
myth of language universals: Language diversity
and its importance for cognitive science. Behavioral
and Brain Sciences, 32:429?448, 10.
Kuzman Ganchev and Dipanjan Das. 2013. Cross-
lingual discriminative learning of sequence models
with posterior regularization. In Proceedings of the
2013 Conference on Empirical Methods in Natu-
ral Language Processing, pages 1996?2006, Seattle,
Washington, USA, October. Association for Compu-
tational Linguistics.
Kuzman Ganchev, Jennifer Gillenwater, and Ben
Taskar. 2009. Dependency grammar induction
via bitext projection constraints. In Proceedings of
the Joint Conference of the 47th Annual Meeting of
the ACL and the 4th International Joint Conference
on Natural Language Processing of the AFNLP:
Volume 1 - Volume 1, ACL ?09, pages 369?377,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Rebecca Hwa, Philip Resnik, Amy Weinberg, Clara
Cabezas, and Okan Kolak. 2005. Bootstrapping
parsers via syntactic projection across parallel texts.
Nat. Lang. Eng., 11(3):311?325, September.
Sungchul Kim, Kristina Toutanova, and Hwanjo Yu.
2012. Multilingual named entity recognition using
parallel data and metadata from wikipedia. In Pro-
ceedings of the 50th Annual Meeting of the Associ-
ation for Computational Linguistics: Long Papers
- Volume 1, ACL ?12, pages 694?702, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
Mikhail Kozhevnikov and Ivan Titov. 2013. Cross-
lingual transfer of semantic role labeling models.
In Proceedings of the 51st Annual Meeting of the
Association for Computational Linguistics (Volume
1: Long Papers), pages 1190?1200, Sofia, Bulgaria,
August. Association for Computational Linguistics.
Shen Li, Jo?o V. Gra?a, and Ben Taskar. 2012. Wiki-ly
supervised part-of-speech tagging. In Proceedings
of the 2012 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning, EMNLP-CoNLL ?12,
pages 1389?1398, Stroudsburg, PA, USA. Associa-
tion for Computational Linguistics.
Ryan McDonald, Joakim Nivre, Yvonne Quirmbach-
Brundage, Yoav Goldberg, Dipanjan Das, Kuz-
man Ganchev, Keith Hall, Slav Petrov, Hao
Zhang, Oscar T?ckstr?m, Claudia Bedini, N?ria
Bertomeu Castell?, and Jungmee Lee. 2013. Uni-
versal dependency annotation for multilingual pars-
ing. In Proceedings of the 51st Annual Meeting of
the Association for Computational Linguistics (Vol-
ume 2: Short Papers), pages 92?97, Sofia, Bulgaria,
August. Association for Computational Linguistics.
Sebastian Pad? and Mirella Lapata. 2009. Cross-
lingual annotation projection of semantic roles. J.
Artif. Int. Res., 36(1):307?340, September.
Slav Petrov, Dipanjan Das, and Ryan McDonald. 2012.
A universal part-of-speech tagset. In Nicoletta Cal-
zolari (Conference Chair), Khalid Choukri, Thierry
Declerck, Mehmet U?gur Do?gan, Bente Maegaard,
Joseph Mariani, Jan Odijk, and Stelios Piperidis, ed-
itors, Proceedings of the Eight International Con-
ference on Language Resources and Evaluation
(LREC?12), Istanbul, Turkey, may. European Lan-
guage Resources Association (ELRA).
St?phane Ross and Drew Bagnell. 2010. Efficient re-
ductions for imitation learning. In AISTATS, pages
661?668.
Yoshimasa Tsuruoka, Yusuke Miyao, and Jun?ichi
Kazama. 2011. Learning with lookahead: Can
history-based models rival globally optimized mod-
els? In Proceedings of the Fifteenth Conference on
Computational Natural Language Learning, pages
238?246, Portland, Oregon, USA, June. Association
for Computational Linguistics.
Oscar T?ckstr?m, Dipanjan Das, Slav Petrov, Ryan
McDonald, and Joakim Nivre. 2013. Token and
type constraints for cross-lingual part-of-speech tag-
ging. Transactions of the Association for Computa-
tional Linguistics, 1:1?12.
1784
Lonneke van der Plas, Marianna Apidianaki, and Chen-
hua Chen. 2014. Global methods for cross-lingual
semantic role and predicate labelling. In Proceed-
ings of COLING 2014, the 25th International Con-
ference on Computational Linguistics: Technical
Papers, pages 1279?1290, Dublin, Ireland, August.
Dublin City University and Association for Compu-
tational Linguistics.
Mengqiu Wang and Christopher D. Manning. 2014.
Cross-lingual projected expectation regularization
for weakly supervised learning. Transactions of the
ACL, 2:55?66, February.
David Yarowsky, Grace Ngai, and Richard Wicen-
towski. 2001. Inducing multilingual text analy-
sis tools via robust projection across aligned cor-
pora. In Proceedings of the First International Con-
ference on Human Language Technology Research,
HLT ?01, pages 1?8, Stroudsburg, PA, USA. Asso-
ciation for Computational Linguistics.
Yuan Zhang, Roi Reichart, Regina Barzilay, and Amir
Globerson. 2012. Learning to map into a univer-
sal pos tagset. In Proceedings of the 2012 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning, EMNLP-CoNLL ?12, pages 1368?
1378, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
1785
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 120?129,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
Computing Lattice BLEU Oracle Scores for Machine Translation
Artem Sokolov Guillaume Wisniewski
LIMSI-CNRS & Univ. Paris Sud
BP-133, 91 403 Orsay, France
{firstname.lastname}@limsi.fr
Franc?ois Yvon
Abstract
The search space of Phrase-Based Statisti-
cal Machine Translation (PBSMT) systems
can be represented under the form of a di-
rected acyclic graph (lattice). The quality
of this search space can thus be evaluated
by computing the best achievable hypoth-
esis in the lattice, the so-called oracle hy-
pothesis. For common SMT metrics, this
problem is however NP-hard and can only
be solved using heuristics. In this work,
we present two new methods for efficiently
computing BLEU oracles on lattices: the
first one is based on a linear approximation
of the corpus BLEU score and is solved us-
ing the FST formalism; the second one re-
lies on integer linear programming formu-
lation and is solved directly and using the
Lagrangian relaxation framework. These
new decoders are positively evaluated and
compared with several alternatives from the
literature for three language pairs, using lat-
tices produced by two PBSMT systems.
1 Introduction
The search space of Phrase-Based Statistical Ma-
chine Translation (PBSMT) systems has the form
of a very large directed acyclic graph. In several
softwares, an approximation of this search space
can be outputted, either as a n-best list contain-
ing the n top hypotheses found by the decoder, or
as a phrase or word graph (lattice) which com-
pactly encodes those hypotheses that have sur-
vived search space pruning. Lattices usually con-
tain much more hypotheses than n-best lists and
better approximate the search space.
Exploring the PBSMT search space is one of
the few means to perform diagnostic analysis and
to better understand the behavior of the system
(Turchi et al 2008; Auli et al 2009). Useful
diagnostics are, for instance, provided by look-
ing at the best (oracle) hypotheses contained in
the search space, i.e, those hypotheses that have
the highest quality score with respect to one or
several references. Such oracle hypotheses can
be used for failure analysis and to better under-
stand the bottlenecks of existing translation sys-
tems (Wisniewski et al 2010). Indeed, the in-
ability to faithfully reproduce reference transla-
tions can have many causes, such as scantiness
of the translation table, insufficient expressiveness
of reordering models, inadequate scoring func-
tion, non-literal references, over-pruned lattices,
etc. Oracle decoding has several other applica-
tions: for instance, in (Liang et al 2006; Chi-
ang et al 2008) it is used as a work-around to
the problem of non-reachability of the reference
in discriminative training of MT systems. Lattice
reranking (Li and Khudanpur, 2009), a promising
way to improve MT systems, also relies on oracle
decoding to build the training data for a reranking
algorithm.
For sentence level metrics, finding oracle hy-
potheses in n-best lists is a simple issue; how-
ever, solving this problem on lattices proves much
more challenging, due to the number of embed-
ded hypotheses, which prevents the use of brute-
force approaches. When using BLEU, or rather
sentence-level approximations thereof, the prob-
lem is in fact known to be NP-hard (Leusch et
al., 2008). This complexity stems from the fact
that the contribution of a given edge to the total
modified n-gram precision can not be computed
without looking at all other edges on the path.
Similar (or worse) complexity result are expected
120
for other metrics such as METEOR (Banerjee and
Lavie, 2005) or TER (Snover et al 2006). The
exact computation of oracles under corpus level
metrics, such as BLEU, poses supplementary com-
binatorial problems that will not be addressed in
this work.
In this paper, we present two original methods
for finding approximate oracle hypotheses on lat-
tices. The first one is based on a linear approxima-
tion of the corpus BLEU, that was originally de-
signed for efficient Minimum Bayesian Risk de-
coding on lattices (Tromble et al 2008). The sec-
ond one, based on Integer Linear Programming, is
an extension to lattices of a recent work on failure
analysis for phrase-based decoders (Wisniewski
et al 2010). In this framework, we study two
decoding strategies: one based on a generic ILP
solver, and one, based on Lagrangian relaxation.
Our contribution is also experimental as we
compare the quality of the BLEU approxima-
tions and the time performance of these new ap-
proaches with several existing methods, for differ-
ent language pairs and using the lattice generation
capacities of two publicly-available state-of-the-
art phrase-based decoders: Moses1 and N-code2.
The rest of this paper is organized as follows.
In Section 2, we formally define the oracle decod-
ing task and recall the formalism of finite state
automata on semirings. We then describe (Sec-
tion 3) two existing approaches for solving this
task, before detailing our new proposals in sec-
tions 4 and 5. We then report evaluations of the
existing and new oracles on machine translation
tasks.
2 Preliminaries
2.1 Oracle Decoding Task
We assume that a phrase-based decoder is able
to produce, for each source sentence f , a lattice
Lf = ?Q,??, with # {Q} vertices (states) and
# {?} edges. Each edge carries a source phrase
fi, an associated output phrase ei as well as a fea-
ture vector h?i, the components of which encode
various compatibility measures between fi and ei.
We further assume that Lf is a word lattice,
meaning that each ei carries a single word3 and
1http://www.statmt.org/moses/
2http://ncode.limsi.fr/
3Converting a phrase lattice to a word lattice is a simple
matter of redistributing a compound input or output over a
that it contains a unique initial state q0 and a
unique final state qF . Let ?f denote the set of all
paths from q0 to qF in Lf . Each path pi ? ?f cor-
responds to a possible translation epi. The job of
a (conventional) decoder is to find the best path(s)
in Lf using scores that combine the edges? fea-
ture vectors with the parameters ?? learned during
tuning.
In oracle decoding, the decoder?s job is quite
different, as we assume that at least a reference
rf is provided to evaluate the quality of each indi-
vidual hypothesis. The decoder therefore aims at
finding the path pi? that generates the hypothesis
that best matches rf . For this task, only the output
labels ei will matter, the other informations can be
left aside.4
Oracle decoding assumes the definition of a
measure of the similarity between a reference
and a hypothesis. In this paper we will con-
sider sentence-level approximations of the popu-
lar BLEU score (Papineni et al 2002). BLEU is
formally defined for two parallel corpora, E =
{ej}Jj=1 and R = {rj}
J
j=1, each containing J
sentences as:
n-BLEU(E ,R) = BP ?
( n?
m=1
pm
)1/n
, (1)
where BP = min(1, e1?c1(R)/c1(E)) is the
brevity penalty and pm = cm(E ,R)/cm(E) are
clipped or modified m-gram precisions: cm(E) is
the total number of wordm-grams in E ; cm(E ,R)
accumulates over sentences the number of m-
grams in ej that also belong to rj . These counts
are clipped, meaning that a m-gram that appears
k times in E and l times in R, with k > l, is only
counted l times. As it is well known, BLEU per-
forms a compromise between precision, which is
directly appears in Equation (1), and recall, which
is indirectly taken into account via the brevity
penalty. In most cases, Equation (1) is computed
with n = 4 and we use BLEU as a synonym for
4-BLEU.
BLEU is defined for a pair of corpora, but, as an
oracle decoder is working at the sentence-level, it
should rely on an approximation of BLEU that can
linear chain of arcs.
4The algorithms described below can be straightfor-
wardly generalized to compute oracle hypotheses under
combined metrics mixing model scores and quality measures
(Chiang et al 2008), by weighting each edge with its model
score and by using these weights down the pipe.
121
evaluate the similarity between a single hypoth-
esis and its reference. This approximation intro-
duces a discrepancy as gathering sentences with
the highest (local) approximation may not result
in the highest possible (corpus-level) BLEU score.
Let BLEU? be such a sentence-level approximation
of BLEU. Then lattice oracle decoding is the task
of finding an optimal path pi?(f) among all paths
?f for a given f , and amounts to the following
optimization problem:
pi?(f) = arg max
pi??f
BLEU?(epi, rf ). (2)
2.2 Compromises of Oracle Decoding
As proved by Leusch et al(2008), even with
brevity penalty dropped, the problem of deciding
whether a confusion network contains a hypoth-
esis with clipped uni- and bigram precisions all
equal to 1.0 is NP-complete (and so is the asso-
ciated optimization problem of oracle decoding
for 2-BLEU). The case of more general word and
phrase lattices and 4-BLEU score is consequently
also NP-complete. This complexity stems from
chaining up of local unigram decisions that, due
to the clipping constraints, have non-local effect
on the bigram precision scores. It is consequently
necessary to keep a possibly exponential num-
ber of non-recombinable hypotheses (character-
ized by counts for each n-gram in the reference)
until very late states in the lattice.
These complexity results imply that any oracle
decoder has to waive either the form of the objec-
tive function, replacing BLEU with better-behaved
scoring functions, or the exactness of the solu-
tion, relying on approximate heuristic search al-
gorithms.
In Table 1, we summarize different compro-
mises that the existing (section 3), as well as
our novel (sections 4 and 5) oracle decoders,
have to make. The ?target? and ?target level?
columns specify the targeted score. None of
the decoders optimizes it directly: their objec-
tive function is rather the approximation of BLEU
given in the ?target replacement? column. Col-
umn ?search? details the accuracy of the target re-
placement optimization. Finally, columns ?clip-
ping? and ?brevity? indicate whether the corre-
sponding properties of BLEU score are considered
in the target substitute and in the search algorithm.
2.3 Finite State Acceptors
The implementations of the oracles described in
the first part of this work (sections 3 and 4) use the
common formalism of finite state acceptors (FSA)
over different semirings and are implemented us-
ing the generic OpenFST toolbox (Allauzen et al
2007).
A (?,?)-semiring K over a set K is a system
?K,?,?, 0?, 1??, where ?K,?, 0?? is a commutative
monoid with identity element 0?, and ?K,?, 1?? is
a monoid with identity element 1?. ? distributes
over ?, so that a ? (b ? c) = (a ? b) ? (a ? c)
and (b? c)? a = (b? a)? (c? a) and element
0? annihilates K (a? 0? = 0?? a = 0?).
Let A = (?, Q, I, F,E) be a weighted finite-
state acceptor with labels in ? and weights in K,
meaning that the transitions (q, ?, q?) in A carry a
weight w ? K. Formally, E is a mapping from
(Q ? ? ? Q) into K; likewise, initial I and fi-
nal weight F functions are mappings from Q into
K. We borrow the notations of Mohri (2009):
if ? = (q, a, q?) is a transition in domain(E),
p(?) = q (resp. n(?) = q?) denotes its origin
(resp. destination) state, w(?) = ? its label and
E(?) its weight. These notations extend to paths:
if pi is a path in A, p(pi) (resp. n(pi)) is its initial
(resp. ending) state and w(pi) is the label along
the path. A finite state transducer (FST) is an FSA
with output alphabet, so that each transition car-
ries a pair of input/output symbols.
As discussed in Sections 3 and 4, several oracle
decoding algorithms can be expressed as shortest-
path problems, provided a suitable definition of
the underlying acceptor and associated semiring.
In particular, quantities such as:
?
pi??(A)
E(pi), (3)
where the total weight of a successful path pi =
?1 . . . ?l in A is computed as:
E(pi) =I(p(?1))?
[
l?
i=1
E(?i)
]
? F (n(?l))
can be efficiently found by generic shortest dis-
tance algorithms over acyclic graphs (Mohri,
2002). For FSA-based implementations over
semirings where ? = max, the optimization
problem (2) is thus reduced to Equation (3), while
the oracle-specific details can be incorporated into
in the definition of ?.
122
oracle target target level target replacement search clipping brevity
ex
is
ti
ng LM-2g/4g 2/4-BLEU sentence P2(e; r) or P4(e; r) exact no no
PB 4-BLEU sentence partial log BLEU (4) appr. no no
PB` 4-BLEU sentence partial log BLEU (4) appr. no yes
th
is
pa
pe
r LB-2g/4g 2/4-BLEU corpus linear appr. lin BLEU (5) exact no yes
SP 1-BLEU sentence unigram count exact no yes
ILP 2-BLEU sentence uni/bi-gram counts (7) appr. yes yes
RLX 2-BLEU sentence uni/bi-gram counts (8) exact yes yes
Table 1: Recapitulative overview of oracle decoders.
3 Existing Algorithms
In this section, we describe our reimplementation
of two approximate search algorithms that have
been proposed in the literature to solve the oracle
decoding problem for BLEU. In addition to their
approximate nature, none of them accounts for the
fact that the count of each matching word has to
be clipped.
3.1 Language Model Oracle (LM)
The simplest approach we consider is introduced
in (Li and Khudanpur, 2009), where oracle decod-
ing is reduced to the problem of finding the most
likely hypothesis under a n-gram language model
trained with the sole reference translation.
Let us suppose we have a n-gram language
model that gives a probability P (en|e1 . . . en?1)
of word en given the n? 1 previous words.
The probability of a hypothesis e is then
Pn(e|r) =
?
i=1 P (ei+n|ei . . . ei+n?1). The lan-
guage model can conveniently be represented as a
FSA ALM , with each arc carrying a negative log-
probability weight and with additional ?-type fail-
ure transitions to accommodate for back-off arcs.
If we train, for each source sentence f , a sepa-
rate language model ALM (rf ) using only the ref-
erence rf , oracle decoding amounts to finding a
shortest (most probable) path in the weighted FSA
resulting from the composition L ?ALM (rf ) over
the (min,+)-semiring:
pi?LM (f) = ShortestPath(L ?ALM (rf )).
This approach replaces the optimization of n-
BLEU with a search for the most probable path
under a simplistic n-gram language model. One
may expect the most probable path to select fre-
quent n-gram from the reference, thus augment-
ing n-BLEU.
3.2 Partial BLEU Oracle (PB)
Another approach is put forward in (Dreyer et
al., 2007) and used in (Li and Khudanpur, 2009):
oracle translations are shortest paths in a lattice
L, where the weight of each path pi is the sen-
tence level log BLEU(pi) score of the correspond-
ing complete or partial hypothesis:
log BLEU(pi) =
1
4
?
m=1...4
log pm. (4)
Here, the brevity penalty is ignored and n-
gram precisions are offset to avoid null counts:
pm = (cm(epi, r) + 0.1)/(cm(epi) + 0.1).
This approach has been reimplemented using
the FST formalism by defining a suitable semir-
ing. Let each weight of the semiring keep a set
of tuples accumulated up to the current state of
the lattice. Each tuple contains three words of re-
cent history, a partial hypothesis as well as current
values of the length of the partial hypothesis, n-
gram counts (4 numbers) and the sentence-level
log BLEU score defined by Equation (4). In the
beginning each arc is initialized with a singleton
set containing one tuple with a single word as the
partial hypothesis. For the semiring operations we
define one common?-operation and two versions
of the ?-operation:
? L1 ?PB L2 ? appends a word on the edge of
L2 to L1?s hypotheses, shifts their recent histories
and updates n-gram counts, lengths, and current
score; ? L1 ?PB L2 ? merges all sets from L1
and L2 and recombinates those having the same
recent history; ? L1 ?PB` L2 ? merges all sets
from L1 and L2 and recombinates those having
the same recent history and the same hypothesis
length.
If several hypotheses have the same recent
history (and length in the case of ?PB`), re-
combination removes all of them, but the one
123
q?
0:0/01:1/0
(a) ?1
q?
00:/10 
:/10
0:0010
:0100:010
:10
(b) ?2
q?
0
0:/10
:/10
0:/10 00
0:/10
00:/10 
:/10
:010
0:0010
:010
0:00100:010
:10
:0010
0:00010
(c) ?3
Figure 1: Examples of the ?n automata for ? = {0, 1} and n = 1 . . . 3. Initial and final states are marked,
respectively, with bold and with double borders. Note that arcs between final states are weighted with 0, while in
reality they will have this weight only if the corresponding n-gram does not appear in the reference.
with the largest current BLEU score. Optimal
path is then found by launching the generic
ShortestDistance(L) algorithm over one of
the semirings above.
The (?PB`,?PB)-semiring, in which the
equal length requirement also implies equal
brevity penalties, is more conservative in recom-
bining hypotheses and should achieve final BLEU
that is least as good as that obtained with the
(?PB,?PB)-semiring5.
4 Linear BLEU Oracle (LB)
In this section, we propose a new oracle based on
the linear approximation of the corpus BLEU in-
troduced in (Tromble et al 2008). While this ap-
proximation was earlier used for Minimum Bayes
Risk decoding in lattices (Tromble et al 2008;
Blackwood et al 2010), we show here how it can
also be used to approximately compute an oracle
translation.
Given five real parameters ?0...4 and a word vo-
cabulary ?, Tromble et al(2008) showed that one
can approximate the corpus-BLEU with its first-
order (linear) Taylor expansion:
lin BLEU(pi) = ?0 |epi|+
4?
n=1
?n
?
u??n
cu(epi)?u(r),
(5)
where cu(e) is the number of times the n-gram
u appears in e, and ?u(r) is an indicator variable
testing the presence of u in r.
To exploit this approximation for oracle decod-
ing, we construct four weighted FSTs ?n con-
taining a (final) state for each possible (n ? 1)-
5See, however, experiments in Section 6.
gram, and all weighted transitions of the kind
(?n?11 , ?n : ?
n
1 /?n ? ??n1 (r), ?
n
2 ), where ?s are
in ?, input word sequence ?n?11 and output se-
quence ?n2 , are, respectively, the maximal prefix
and suffix of an n-gram ?n1 .
In supplement, we add auxiliary states corre-
sponding to m-grams (m < n ? 1), whose func-
tional purpose is to help reach one of the main
(n ? 1)-gram states. There are |?|
n?1?1
|?|?1 , n > 1,
such supplementary states and their transitions are
(?k1 , ?k+1 : ?
k+1
1 /0, ?
k+1
1 ), k = 1 . . . n?2. Apart
from these auxiliary states, the rest of the graph
(i.e., all final states) reproduces the structure of
the well-known de Bruijn graphB(?, n) (see Fig-
ure 1).
To actually compute the best hypothesis, we
first weight all arcs in the input FSA L with ?0 to
obtain ?0. This makes each word?s weight equal
in a hypothesis path, and the total weight of the
path in ?0 is proportional to the number of words
in it. Then, by sequentially composing ?0 with
other ?ns, we discount arcs whose output n-gram
corresponds to a matching n-gram. The amount
of discount is regulated by the ratio between ?n?s
for n > 0.
With all operations performed over the
(min,+)-semiring, the oracle translation is then
given by:
pi?LB = ShortestPath(?0??1??2??3??4).
We set parameters ?n as in (Tromble et al
2008): ?0 = 1, roughly corresponding to the
brevity penalty (each word in a hypothesis adds
up equally to the final path length) and ?n =
?(4p ? rn?1)?1, which are increasing discounts
124
 0 0.2
 0.4 0.6
 0.8 1
p
 0
 0.2
 0.4
 0.6
 0.8
 1
r
 22
 24
 26
 28
 30
 32
 34
 36
BLEU
 22
 24
 26
 28
 30
 32
 34
 36
Figure 2: Performance of the LB-4g oracle for differ-
ent combinations of p and r on WMT11 de2en task.
for matching n-grams. The values of p and r were
found by grid search with a 0.05 step value. A
typical result of the grid evaluation of the LB or-
acle for German to English WMT?11 task is dis-
played on Figure 2. The optimal values for the
other pairs of languages were roughly in the same
ballpark, with p ? 0.3 and r ? 0.2.
5 Oracles with n-gram Clipping
In this section, we describe two new oracle de-
coders that take n-gram clipping into account.
These oracles leverage on the well-known fact
that the shortest path problem, at the heart of
all the oracles described so far, can be reduced
straightforwardly to an Integer Linear Program-
ming (ILP) problem (Wolsey, 1998). Once oracle
decoding is formulated as an ILP problem, it is
relatively easy to introduce additional constraints,
for instance to enforce n-gram clipping. We will
first describe the optimization problem of oracle
decoding and then present several ways to effi-
ciently solve it.
5.1 Problem Description
Throughout this section, abusing the notations,
we will also think of an edge ?i as a binary vari-
able describing whether the edge is ?selected? or
not. The set {0, 1}#{?} of all possible edge as-
signments will be denoted by P . Note that ?, the
set of all paths in the lattice is a subset of P: by
enforcing some constraints on an assignment ? in
P , it can be guaranteed that it will represent a path
in the lattice. For the sake of presentation, we as-
sume that each edge ?i generates a single word
w(?i) and we focus first on finding the optimal
hypothesis with respect to the sentence approxi-
mation of the 1-BLEU score.
As 1-BLEU is decomposable, it is possible to
define, for every edge ?i, an associated reward, ?i
that describes the edge?s local contribution to the
hypothesis score. For instance, for the sentence
approximation of the 1-BLEU score, the rewards
are defined as:
?i =
{
?1 if w(?i) is in the reference,
??2 otherwise,
where ?1 and ?2 are two positive constants cho-
sen to maximize the corpus BLEU score6. Con-
stant ?1 (resp. ?2) is a reward (resp. a penalty)
for generating a word in the reference (resp. not in
the reference). The score of an assignment ? ? P
is then defined as: score(?) =
?#{?}
i=1 ?i ? ?i. This
score can be seen as a compromise between the
number of common words in the hypothesis and
the reference (accounting for recall) and the num-
ber of words of the hypothesis that do not appear
in the reference (accounting for precision).
As explained in Section 2.3, finding the or-
acle hypothesis amounts to solving the shortest
distance (or path) problem (3), which can be re-
formulated by a constrained optimization prob-
lem (Wolsey, 1998):
arg max
??P
#{?}?
i=1
?i ? ?i (6)
s.t.
?
????(qF )
? = 1,
?
???+(q0)
? = 1
?
???+(q)
? ?
?
????(q)
? = 0, q ? Q\{q0, qF }
where q0 (resp. qF ) is the initial (resp. final) state
of the lattice and ??(q) (resp. ?+(q)) denotes the
set of incoming (resp. outgoing) edges of state q.
These path constraints ensure that the solution of
the problem is a valid path in the lattice.
The optimization problem in Equation (6) can
be further extended to take clipping into account.
Let us introduce, for each word w, a variable ?w
that denotes the number of times w appears in the
hypothesis clipped to the number of times, it ap-
pears in the reference. Formally, ?w is defined by:
?w = min
?
?
?
?
???(w)
?, cw(r)
?
?
?
6We tried several combinations of ?1 and ?2 and kept
the one that had the highest corpus 4-BLEU score.
125
where ? (w) is the subset of edges generating w,
and
?
???(w) ? is the number of occurrences of
w in the solution and cw(r) is the number of oc-
currences of w in the reference r. Using the ?
variables, we define a ?clipped? approximation of
1-BLEU:
?1 ?
?
w
?w ??2 ?
?
?
#{?}?
i=1
?i ?
?
w
?w
?
?
Indeed, the clipped number of words in the hy-
pothesis that appear in the reference is given by
?
w ?w, and
?#{?}
i=1 ?i ?
?
w ?w corresponds to
the number of words in the hypothesis that do not
appear in the reference or that are surplus to the
clipped count.
Finally, the clipped lattice oracle is defined by
the following optimization problem:
arg max
??P,?w
(?1 + ?2) ?
?
w
?w ??2 ?
#{?}?
i=1
?i
(7)
s.t. ?w ? 0, ?w ? cw(r), ?w ?
?
???(w)
?
?
????(qF )
? = 1,
?
???+(q0)
? = 1
?
???+(q)
? ?
?
????(q)
? = 0, q ? Q \ {q0, qF }
where the first three sets of constraints are the lin-
earization of the definition of ?w, made possible
by the positivity of ?1 and ?2, and the last three
sets of constraints are the path constraints.
In our implementation we generalized this op-
timization problem to bigram lattices, in which
each edge is labeled by the bigram it generates.
Such bigram FSAs can be produced by compos-
ing the word lattice with ?2 from Section 4. In
this case, the reward of an edge will be defined as
a combination of the (clipped) number of unigram
matches and bigram matches, and solving the op-
timization problem yields a 2-BLEU optimal hy-
pothesis. The approach can be further generalized
to higher-order BLEU or other metrics, as long as
the reward of an edge can be computed locally.
The constrained optimization problem (7) can
be solved efficiently using off-the-shelf ILP
solvers7.
7In our experiments we used Gurobi (Optimization,
2010) a commercial ILP solver that offers free academic li-
cense.
5.2 Shortest Path Oracle (SP)
As a trivial special class of the above formula-
tion, we also define a Shortest Path Oracle (SP)
that solves the optimization problem in (6). As
no clipping constraints apply, it can be solved ef-
ficiently using the standard Bellman algorithm.
5.3 Oracle Decoding through Lagrangian
Relaxation (RLX)
In this section, we introduce another method to
solve problem (7) without relying on an exter-
nal ILP solver. Following (Rush et al 2010;
Chang and Collins, 2011), we propose an original
method for oracle decoding based on Lagrangian
relaxation. This method relies on the idea of re-
laxing the clipping constraints: starting from an
unconstrained problem, the counts clipping is en-
forced by incrementally strengthening the weight
of paths satisfying the constraints.
The oracle decoding problem with clipping
constraints amounts to solving:
arg min
???
?
#{?}?
i=1
?i ? ?i (8)
s.t.
?
???(w)
? ? cw(r), w ? r
where, by abusing the notations, r also denotes
the set of words in the reference. For sake of clar-
ity, the path constraints are incorporated into the
domain (the arg min runs over ? and not over P).
To solve this optimization problem we consider its
dual form and use Lagrangian relaxation to deal
with clipping constraints.
Let ? = {?w}w?r be positive Lagrange mul-
tipliers, one for each different word of the refer-
ence, then the Lagrangian of the problem (8) is:
L(?, ?) = ?
#{?}?
i=1
?i?i+
?
w?r
?w
?
?
?
???(w)
? ? cw(r)
?
?
The dual objective is L(?) = min? L(?, ?)
and the dual problem is: max?,?0 L(?). To
solve the latter, we first need to work out the dual
objective:
?? = arg min
???
L(?, ?)
= arg min
???
#{?}?
i=1
?i
(
?w(?i) ? ?i
)
126
where we assume that ?w(?i) is 0 when word
w(?i) is not in the reference. In the same way
as in Section 5.2, the solution of this problem can
be efficiently retrieved with a shortest path algo-
rithm.
It is possible to optimize L(?) by noticing that
it is a concave function. It can be shown (Chang
and Collins, 2011) that, at convergence, the clip-
ping constraints will be enforced in the optimal
solution. In this work, we chose to use a simple
gradient descent to solve the dual problem. A sub-
gradient of the dual objective is:
?L(?)
??w
=
?
???(w)???
? ? cw(r).
Each component of the gradient corresponds to
the difference between the number of times the
word w appears in the hypothesis and the num-
ber of times it appears in the reference. The algo-
rithm below sums up the optimization of task (8).
In the algorithm ?(t) corresponds to the step size
at the tth iteration. In our experiments we used a
constant step size of 0.1. Compared to the usual
gradient descent algorithm, there is an additional
projection step of ? on the positive orthant, which
enforces the constraint ?  0.
?w, ?(0)w ? 0
for t = 1? T do
??(t) = arg min?
?
i ?i ?
(
?w(?i) ? ?i
)
if all clipping constraints are enforced
then optimal solution found
else for w ? r do
nw ? n. of occurrences of w in ??(t)
?(t)w ? ?
(t)
w + ?(t) ? (nw ? cw(r))
?(t)w ? max(0, ?
(t)
w )
6 Experiments
For the proposed new oracles and the existing ap-
proaches, we compare the quality of oracle trans-
lations and the average time per sentence needed
to compute them8 on several datasets for 3 lan-
guage pairs, using lattices generated by two open-
source decoders: N-code and Moses9 (Figures 3
8Experiments were run in parallel on a server with 64G
of RAM and 2 Xeon CPUs with 4 cores at 2.3 GHz.
9As the ILP (and RLX) oracle were implemented in
Python, we pruned Moses lattices to accelerate task prepa-
ration for it.
decoder fr2en de2en en2de
te
st N-code 27.88 22.05 15.83
Moses 27.68 21.85 15.89
or
ac
le N-code 36.36 29.22 21.18
Moses 35.25 29.13 22.03
Table 2: Test BLEU scores and oracle scores on
100-best lists for the evaluated systems.
and 4). Systems were trained on the data provided
for the WMT?11 Evaluation task10, tuned on the
WMT?09 test data and evaluated on WMT?10 test
set11 to produce lattices. The BLEU test scores
and oracle scores on 100-best lists with the ap-
proximation (4) for N-code and Moses are given
in Table 2. It is not until considering 10,000-best
lists that n-best oracles achieve performance com-
parable to the (mediocre) SP oracle.
To make a fair comparison with the ILP and
RLX oracles which optimize 2-BLEU, we in-
cluded 2-BLEU versions of the LB and LM ora-
cles, identified below with the ?-2g? suffix. The
two versions of the PB oracle are respectively
denoted as PB and PB`, by the type of the ?-
operation they consider (Section 3.2). Parame-
ters p and r for the LB-4g oracle for N-code were
found with grid search and reused for Moses:
p = 0.25, r = 0.15 (fr2en); p = 0.175, r = 0.575
(en2de) and p = 0.35, r = 0.425 (de2en). Cor-
respondingly, for the LB-2g oracle: p = 0.3, r =
0.15; p = 0.3, r = 0.175 and p = 0.575, r = 0.1.
The proposed LB, ILP and RLX oracles were
the best performing oracles, with the ILP and
RLX oracles being considerably faster, suffering
only a negligible decrease in BLEU, compared to
the 4-BLEU-optimized LB oracle. We stopped
RLX oracle after 20 iterations, as letting it con-
verge had a small negative effect (?1 point of the
corpus BLEU), because of the sentence/corpus dis-
crepancy ushered by the BLEU score approxima-
tion.
Experiments showed consistently inferior per-
formance of the LM-oracle resulting from the op-
timization of the sentence probability rather than
BLEU. The PB oracle often performed compara-
bly to our new oracles, however, with sporadic
resource-consumption bursts, that are difficult to
10http://www.statmt.org/wmt2011
11All BLEU scores are reported using the multi-bleu.pl
script.
127
 25
 30
 35
 40
 45
 50
RLX ILP LB-4g LB-2g PB PBl SP LM-4g LM-2g  0
 1
 2
 3
 4
 5
 6
BLE
U
avg
. tim
e, s
BLEU
47.8
2
48.1
2
48.2
2
47.7
1
46.7
6
46.4
8
41.2
3
38.9
1
38.7
5
avg. time
(a) fr2en
 25
 30
 35
RLX ILP LB-4g LB-2g PB PBl SP LM-4g LM-2g  0
 0.5
 1
 1.5
BLE
U
avg
. tim
e, s
BLEU
34.7
9
34.7
0 35.4
9
35.0
9
34.8
5
34.7
6
30.7
8
29.5
3
29.5
3
avg. time
(b) de2en
 15
 20
 25
 30
RLX ILP LB-4g LB-2g PB PBl SP LM-4g LM-2g  0
 0.5
 1
BLE
U
avg
. tim
e, s
BLEU
24.7
5
24.6
6 25.3
4
24.8
5
24.7
8
24.7
3
22.1
9
20.7
8
20.7
4
avg. time
(c) en2de
Figure 3: Oracles performance for N-code lattices.
 25
 30
 35
 40
 45
 50
RLX ILP LB-4g LB-2g PB PBl SP LM-4g LM-2g  0
 1
 2
 3
BLE
U
avg
. tim
e, s
BLEU
43.8
2
44.0
8
44.4
4
43.8
2
43.4
2
43.2
0
41.0
3
36.3
4
36.2
5
avg. time
(a) fr2en
 25
 30
 35
RLX ILP LB-4g LB-2g PB PBl SP LM-4g LM-2g  0
 1
 2
 3
 4
BLE
U
avg
. tim
e, s
BLEU
36.4
3 36.9
1 37.7
3
36.5
2
36.7
5
36.6
2
30.5
2
29.5
1
29.4
5
avg. time
(b) de2en
 15
 20
 25
 30
RLX ILP LB-4g LB-2g PB PBl SP LM-4g LM-2g  0
 1
 2
 3
 4
 5
 6
 7
 8
 9
BLE
U
avg
. tim
e, s
BLEU
28.6
8
28.6
4 29.
94
28.9
4
28.7
6
28.6
5
26.4
8
21.2
9
21.2
3
avg. time
(c) en2de
Figure 4: Oracles performance for Moses lattices pruned with parameter -b 0.5.
avoid without more cursory hypotheses recom-
bination strategies and the induced effect on the
translations quality. The length-aware PB` oracle
has unexpectedly poorer scores compared to its
length-agnostic PB counterpart, while it should,
at least, stay even, as it takes the brevity penalty
into account. We attribute this fact to the com-
plex effect of clipping coupled with the lack of
control of the process of selecting one hypothe-
sis among several having the same BLEU score,
length and recent history. Anyhow, BLEU scores
of both of PB oracles are only marginally differ-
ent, so the PB`?s conservative policy of pruning
and, consequently, much heavier memory con-
sumption makes it an unwanted choice.
7 Conclusion
We proposed two methods for finding oracle
translations in lattices, based, respectively, on a
linear approximation to the corpus-level BLEU
and on integer linear programming techniques.
We also proposed a variant of the latter approach
based on Lagrangian relaxation that does not rely
on a third-party ILP solver. All these oracles have
superior performance to existing approaches, in
terms of the quality of the found translations, re-
source consumption and, for the LB-2g oracles,
in terms of speed. It is thus possible to use bet-
ter approximations of BLEU than was previously
done, taking the corpus-based nature of BLEU, or
clipping constrainst into account, delivering better
oracles without compromising speed.
Using 2-BLEU and 4-BLEU oracles yields com-
parable performance, which confirms the intuition
that hypotheses sharing many 2-grams, would
likely have many common 3- and 4-grams as well.
Taking into consideration the exceptional speed of
the LB-2g oracle, in practice one can safely opti-
mize for 2-BLEU instead of 4-BLEU, saving large
amounts of time for oracle decoding on long sen-
tences.
Overall, these experiments accentuate the
acuteness of scoring problems that plague modern
decoders: very good hypotheses exist for most in-
put sentences, but are poorly evaluated by a linear
combination of standard features functions. Even
though the tuning procedure can be held respon-
sible for part of the problem, the comparison be-
tween lattice and n-best oracles shows that the
beam search leaves good hypotheses out of the n-
best list until very high value of n, that are never
used in practice.
Acknowledgments
This work has been partially funded by OSEO un-
der the Quaero program.
128
References
Cyril Allauzen, Michael Riley, Johan Schalkwyk, Wo-
jciech Skut, and Mehryar Mohri. 2007. OpenFst:
A general and efficient weighted finite-state trans-
ducer library. In Proc. of the Int. Conf. on Imple-
mentation and Application of Automata, pages 11?
23.
Michael Auli, Adam Lopez, Hieu Hoang, and Philipp
Koehn. 2009. A systematic analysis of translation
model search spaces. In Proc. of WMT, pages 224?
232, Athens, Greece.
Satanjeev Banerjee and Alon Lavie. 2005. ME-
TEOR: An automatic metric for MT evaluation with
improved correlation with human judgments. In
Proc. of the ACL Workshop on Intrinsic and Extrin-
sic Evaluation Measures for Machine Translation,
pages 65?72, Ann Arbor, MI, USA.
Graeme Blackwood, Adria` de Gispert, and William
Byrne. 2010. Efficient path counting transducers
for minimum bayes-risk decoding of statistical ma-
chine translation lattices. In Proc. of the ACL 2010
Conference Short Papers, pages 27?32, Strouds-
burg, PA, USA.
Yin-Wen Chang and Michael Collins. 2011. Exact de-
coding of phrase-based translation models through
lagrangian relaxation. In Proc. of the 2011 Conf. on
EMNLP, pages 26?37, Edinburgh, UK.
David Chiang, Yuval Marton, and Philip Resnik.
2008. Online large-margin training of syntactic
and structural translation features. In Proc. of the
2008 Conf. on EMNLP, pages 224?233, Honolulu,
Hawaii.
Markus Dreyer, Keith B. Hall, and Sanjeev P. Khu-
danpur. 2007. Comparing reordering constraints
for SMT using efficient BLEU oracle computation.
In Proc. of the Workshop on Syntax and Structure
in Statistical Translation, pages 103?110, Morris-
town, NJ, USA.
Gregor Leusch, Evgeny Matusov, and Hermann Ney.
2008. Complexity of finding the BLEU-optimal hy-
pothesis in a confusion network. In Proc. of the
2008 Conf. on EMNLP, pages 839?847, Honolulu,
Hawaii.
Zhifei Li and Sanjeev Khudanpur. 2009. Efficient
extraction of oracle-best translations from hyper-
graphs. In Proc. of Human Language Technolo-
gies: The 2009 Annual Conf. of the North Ameri-
can Chapter of the ACL, Companion Volume: Short
Papers, pages 9?12, Morristown, NJ, USA.
Percy Liang, Alexandre Bouchard-Co?te?, Dan Klein,
and Ben Taskar. 2006. An end-to-end discrim-
inative approach to machine translation. In Proc.
of the 21st Int. Conf. on Computational Linguistics
and the 44th annual meeting of the ACL, pages 761?
768, Morristown, NJ, USA.
Mehryar Mohri. 2002. Semiring frameworks and al-
gorithms for shortest-distance problems. J. Autom.
Lang. Comb., 7:321?350.
Mehryar Mohri. 2009. Weighted automata algo-
rithms. In Manfred Droste, Werner Kuich, and
Heiko Vogler, editors, Handbook of Weighted Au-
tomata, chapter 6, pages 213?254.
Gurobi Optimization. 2010. Gurobi optimizer, April.
Version 3.0.
Kishore Papineni, Salim Roukos, Todd Ward, and
Wei-Jing Zhu. 2002. BLEU: a method for auto-
matic evaluation of machine translation. In Proc. of
the Annual Meeting of the ACL, pages 311?318.
Alexander M. Rush, David Sontag, Michael Collins,
and Tommi Jaakkola. 2010. On dual decomposi-
tion and linear programming relaxations for natural
language processing. In Proc. of the 2010 Conf. on
EMNLP, pages 1?11, Stroudsburg, PA, USA.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study
of translation edit rate with targeted human anno-
tation. In Proc. of the Conf. of the Association for
Machine Translation in the America (AMTA), pages
223?231.
Roy W. Tromble, Shankar Kumar, Franz Och, and
Wolfgang Macherey. 2008. Lattice minimum
bayes-risk decoding for statistical machine transla-
tion. In Proc. of the Conf. on EMNLP, pages 620?
629, Stroudsburg, PA, USA.
Marco Turchi, Tijl De Bie, and Nello Cristianini.
2008. Learning performance of a machine trans-
lation system: a statistical and computational anal-
ysis. In Proc. of WMT, pages 35?43, Columbus,
Ohio.
Guillaume Wisniewski, Alexandre Allauzen, and
Franc?ois Yvon. 2010. Assessing phrase-based
translation models with oracle decoding. In Proc.
of the 2010 Conf. on EMNLP, pages 933?943,
Stroudsburg, PA, USA.
L. Wolsey. 1998. Integer Programming. John Wiley
& Sons, Inc.
129
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 137?142,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
On the Predictability of Human Assessment: when Matrix Completion
Meets NLP Evaluation
Guillaume Wisniewski
Universite? Paris Sud
LIMSI?CNRS
Orsay, France
guillaume.wisniewski@limsi.fr
Abstract
This paper tackles the problem of collect-
ing reliable human assessments. We show
that knowing multiple scores for each ex-
ample instead of a single score results in
a more reliable estimation of a system
quality. To reduce the cost of collect-
ing these multiple ratings, we propose to
use matrix completion techniques to pre-
dict some scores knowing only scores of
other judges and some common ratings.
Even if prediction performance is pretty
low, decisions made using the predicted
score proved to be more reliable than de-
cision based on a single rating of each ex-
ample.
1 Introduction
Human assessment is often considered as the best,
if not the only, way to evaluate ?subjective? NLP
tasks like MT or speech generation. However,
human evaluations are doomed to be noisy and,
sometimes, even contradictory as they depend on
individual perception and understanding of the
score scale that annotators generally use in re-
markably different ways (Koehn and Monz, 2006).
Moreover, annotation is known to be a long and
frustrating process and annotator fatigue has been
identified as another source of noise (Pighin et al,
2012).
In addition to defining and enforcing stricter
guidelines, several solutions have been proposed
to reduce the annotation effort and produce more
reliable ratings. For instance, to limit the impact
of the score scale interpretation, in the WMT eval-
uation campaign (Callison-Burch et al, 2012), an-
notators are asked to rank translation hypotheses
from best to worst instead of providing absolute
scores (e.g. in terms of adequacy or fluency). Gen-
eralizing this approach, several works (Pighin et
al., 2012; Lopez, 2012) have defined novel annota-
tion protocols to reduce the number of judgments
that need to be collected. However, all these meth-
ods suffer from several limitations: first, they pro-
vide no interpretable information about the quality
of the system (only a relative comparison between
two systems is possible); second, (Koehn, 2012)
has recently shown that the ranking they induce is
not reliable.
In this work, we study an alternative approach
to the problem of collecting reliable human as-
sessments. Our basic assumption, motivated by
the success of ensemble methods, is that hav-
ing several judgments for each example, even if
they are noisy, will result in a more reliable de-
cision than having a single judgment. An evalu-
ation campaign should therefore aim at gathering
a score matrix, in which each example is rated by
all judges instead of having each judge rate only
a small subset of examples, thereby minimizing
redundancy. Obviously, the former approach re-
quires a large annotation effort and is, in practice,
not feasible. That is why, to reduce the number
of judgments that must be collected, we propose
to investigate the possibility of using matrix com-
pletion techniques to recover the entire score ma-
trix from a sample of its entries. The question
we try to answer is whether the missing scores of
one judge can be predicted knowing only scores of
other judges and some shared ratings.
The contributions of this paper are twofold: i)
we show how knowing the full score matrix in-
stead of a single score for each example provides a
more reliable estimation of a system quality (Sec-
tion 3); ii) we present preliminary experiments
137
showing that missing data techniques can be used
to recover the score matrix from a sample of its
entries despite the low inter-rater agreement (Sec-
tion 4).
2 Matrix Completion
The recovering of a matrix from a sampling of its
entries is a task of considerable interest (Cande`s
and Recht, 2012). It can be used, for instance, in
recommender systems: rows of the matrix repre-
sent users that are rating movies (columns of the
matrix); the resulting matrix is mostly unknown
(each user only rates a few movies) and the task
consists in completing the matrix so that movies
that any user is likely to like can be predicted.
Matrix completion generally relies on the low
rank hypothesis: because of hidden factors be-
tween the observations (the columns of the ma-
trix), the matrix has a low rank. For instance,
in recommender systems it is commonly believed
that only a few factors contribute to an individual?s
tastes. Formally, recovering a matrix M amounts
at solving:
minimize rank X
subject to Xij = Mij (i, j) ? ? (1)
whereX is the decision variable and ? is the set of
known entries. This optimization problem seeks
the simplest explanation fitting the observed data.
Solving the rank minimization problem has
been proved to be NP-hard (Chistov and
Grigor?ev, 1984). However several convex relax-
ations of this program have been proposed. In
this work, we will consider the relaxation of the
rank by the nuclear norm1 that can be efficiently
solved by semidefinite programming (Becker et
al., 2011). This relaxation enjoys many theoret-
ical guarantees with respect to the optimality of
its solution (under mild assumptions its solution is
also the solution of the original problem), the con-
ditions under which the matrix can be recovered
and the number of entries that must be sampled
to recover the original matrix. In our experiments
we used TFOCS,2 a free implementation of this
method.
1The nuclear norm of a matrix is the sum of its singular
values; the relation between rank an nuclear norm is similar
to the one between `0 and `1 norms.
2http://cvxr.com/tfocs/
3 Corpora
For our experiments we considered two publicly
available corpora in which multiple human ratings
(i.e. scores on an ordinal scale) were available.
The CECorpus The first corpus of human judg-
ments we have considered has been collected
for the WMT12 shared task on quality estima-
tion (Callison-Burch et al, 2012).3 The data set is
made of 2, 254 English sentences and their auto-
matic translations in Spanish predicted by a stan-
dard Moses system. Each sentence pair is accom-
panied by three estimates in the range 1 to 5 of
its translation quality expressed in terms of post-
editing effort. These human grades are in the range
1 to 5, the latter standing for a very good trans-
lation that hardly requires post-editing, while the
former identifies very poor automatic translations
that are not deemed to be worth the post-editing
effort.
As pointed out by the task organizers, despite
the special care that was taken to ensure the quality
of the data, the inter-raters agreement was much
lower than what is typically observed in NLP
tasks (Artstein and Poesio, 2008): the weighted
? ranged from 0.39 to 0.50 depending on the pair
of annotators considered4; the Fleiss coefficient (a
generalization of ? to multi-raters) was 0.25 and
the Kendall ?b correlation coefficient5 between
0.64 and 0.68, meaning that, on average, two raters
do not agree on the relative order of two transla-
tions almost two out of five times. In fact, as of-
ten observed for the sentence level human evalua-
tion of MT outputs, the different judges have used
the score scale differently: the second judge had
a clear tendency to give more ?medium? scores
than the others, and the variance of her scores
was low. Because theirs distributions are differ-
ent, standardizing the scores has only a very lim-
ited impact on the agreement.
If, as in many manual evaluations, each exam-
ple had been rated by a single judge chosen ran-
domly, the resulting scores would have been only
moderately correlated with the average of the three
scores which is, intuitively, a better estimate of the
?true? quality: the 95% confidence interval of the
3The corpus is available from http://www.statmt.
org/wmt12/quality-estimation-task.html
4The weighted ? is a generalization of the ? to ordinal
data; a linear weighting schema was used.
5Note that, in statistics, agreement is a stronger notion
than correlation, as the former compare the actual values.
138
?b between the averaged scores and the ?sampled?
score is 0.754?0.755.
TIDES The second corpus considered was col-
lected for the DARPA TIDES program: a team of
human judges provided multiple assessments of
adequacy and fluency for Arabic to English and
Chinese to English automatic translations.6 For
space reasons, only results on the Chinese to En-
glish fluency corpus will be presented; similar re-
sults were achieved on the other corpora.
In the considered corpus, 31 sets of automatic
translations, generated by three systems, have
been rated by two judges on a scale of 1 to 5. The
inter-rater agreement is very low: depending on
the pair of judges, the weighted ? is between -0.05
and 0.2, meaning that agreement occurs less of-
ten than predicted by chance alone. More impor-
tantly, if the ratings of a pair of judges were used
to decide which is the best system among two, the
two judges will disagree 36% of the time. This
?agreement? score is computed as follows: if mA,i
is the mean of the scores given to system A by
the i-th annotator, we say that there is no agree-
ment in a pairwise comparison if mA,i > mB,i
and mA,j < mB,j , i.e. if two judges rank two sys-
tems in a different order; the score is then the per-
centage of agreement when considering all pairs
of systems and judges.
Considering the full scoring matrix instead of
single scores has a large impact: if each example is
rated by a single judge (chosen randomly), the re-
sulting comparison between the two systems will
be different from the decision made by averaging
the two scores of the full score matrix in almost
20% of the comparisons.
4 Experimental Results
4.1 Testing the Low-Rank Hypothesis
Matrix completion relies on the hypothesis that
the matrix has a low rank. We first propose to
test this hypothesis on simulated data, using a
method similar to the one proposed in (Mathet
et al, 2012), to evaluate the impact of noise in
human judgments on the score matrix rank. Ar-
tificial ratings are generated as follows: a MT
system is producing n translations the quality of
which, qi, is estimated by a continuous value,
that represents, for instance, a hTER score. This
6These corpora are available from LDC under the refer-
ences ldc2003t17 and ldc2003t18
value is drawn from N (?, ?2). Based on this
?intrinsic? quality, two ratings, ai and bi, are
generated according to three strategies: in the
first, ai and bi are sampled from N (qi, ?); in
the second, ai ? N
(
qi + ?2 , ??2
) and bi ?
N
(
qi ? ?2 , ??2
) and in the third, ai ? N
(
qi, ??2
)
and the bi is drawn from a bimodal distribu-
tion 12
(
N
(
qi ? ?2 , ??2
)
+N
(
qi + ?2 , ??2
)) (with
??2 < ?2 ). ? describes the noise level.
Each of these strategies models a different kind
of noise that has been observed in different evalua-
tion campaigns (Koehn and Monz, 2006): the first
one describes random noise in the ratings; the sec-
ond a systematic difference in the annotators? in-
terpretation of the score scale and the third, the sit-
uation in which one annotator gives medium score
while the other one tend to commit more strongly
to whether she considered the translation good or
bad. Stacking all these judgments results in a n?2
score matrix. To test whether this matrix has a low
rank or not, we assess how close it is to its ap-
proximation by a rank 1 matrix. A well-known
result (Lawson and Hanson, 1974) states that the
Frobenius norm of the difference of these matri-
ces is equal to the 2nd singular value of the orig-
inal matrix; the quality of the approximation can
thus be estimated by ?, defined as the 2nd eigen-
value of the matrix normalized by its norm (Leon,
1994). Intuitively, the smaller ?, the better the ap-
proximation.
Figure 1 represents the impact of the noise level
on the condition number. As a baseline, we have
also represented ? for a random matrix. All values
are averaged over 100 simulations. As it could be
expected, ? is close to 0 for small noise level; but
even for moderate noise level, the second eigen-
value continue to be small, suggesting that the ma-
trix can still be approximated by a matrix of rank 1
without much loss of information. As a compari-
son, on average, ? = 0.08 for the CE score matrix,
in spite of the low inter-rater agreement.
4.2 Prediction Performance
We conducted several experiments to evaluate the
possibility to use matrix completion to recover a
score matrix. Experiments consist in choosing
randomly k% of the entries of a matrix; these en-
tries are considered unknown and predicted using
the method introduced in Section 2 denoted pred
in the following. In our experiments k varies from
10% to 40%. Note that, when, as in our exper-
139
0 0.1 0.2 0.3 0.4
0
0.1
0.2
0.3
0.4
random
?
?
1st strat.
2nd strat.
3rd strat.
Figure 1: Evolution of the condition number ?
with the noise level ? for the different strategies
(see text for details)
iments, only two judges are involved, k = 50%
would mean that each example is rated by a sin-
gle judge. Two simple methods for handling miss-
ing data are used as baselines: in the first one, de-
noted rand, missing scores are chosen randomly;
the second one, denoted mean, predicts for all the
missing scores of a judge the mean of her known
scores.
We propose to evaluate the quality of the recov-
ery, first by comparing the predicted score to their
true value and then by evaluating the decision that
will be made when considering the recovered ma-
trix instead of the full matrix.
Prediction Performance Comparing the com-
pleted matrix to the original score matrix can be
done in terms of Mean Absolute Error (MAE) de-
fined as 1N
?N
i=1 |yi ? y?i|where y?i is the predicted
value and yi the corresponding ?true? value; the
sum runs over all unknown values of the matrix.
Table 1 presents the results achieved by the dif-
ferent methods. All reported results are averaged
over 10 runs (i.e.: sampling of the score matrix
and prediction of the missing scores) and over all
pairs of judges. All tables also report the 95% con-
fidence interval. The MAE of the rand method is
almost constant, whatever the number of samples
is. Performance of the matrix completion tech-
nique is not so good: predicted scores are quite
different than true scores. In particular, perfor-
mance falls quickly when the number of missing
data increases. This observation is not surprising:
when 40% of the scores are missing, only a few
examples have more than a single score and many
have no score at all. In these conditions recovering
missing data pred mean
40% 0.78 ?6.21 ? 10?3 0.72 ?8.86 ? 10?3
30% 0.83 ?3.19 ? 10?3 0.80 ?5.42 ? 10?3
20% 0.88 ?2.49 ? 10?3 0.87 ?3.54 ? 10?3
10% 0.93 ?1.76 ? 10?3 0.92 ?1.51 ? 10?3
Table 2: Correlation between the rankings induced
by the recovered matrix and the original score ma-
trix for the CE corpus
the matrix is almost impossible. The performance
of the simple mean technique is, comparatively,
pretty good, especially when only a few entries
are known. However, the pred method always
outperform the rand method showing that there
are dependencies between the two ratings even if
statistical measures of agreement are low.
Impact on the Decision The negative results of
the previous paragraph only provide indirect mea-
sure of the recovery quality as it is not the value of
the score that is important but the decision that it
will support. That is why, we also evaluated ma-
trix recovery in a more task-oriented way by com-
paring the decision made when considering the re-
covered score matrix instead of the ?true? score
matrix.
For the CE corpus, a task-oriented evaluation
can be done by comparing the rankings induced
by the recovered matrix and by the original matrix
when examples are ordered according to their av-
eraged score. Such a ranking can be used by a MT
user to set a quality threshold granting her con-
trol over translation quality (Soricut and Echihabi,
2010). Table 2 shows the correlation between the
two rankings as evaluated by ?b. The two rankings
appear to be highly correlated, the matrix comple-
tion technique outperforming slightly the mean
baseline. More importantly, even when 40% of
the data are missing, the ranking induced by the
true scores is better correlated to the ranking in-
duced by the predicted scores than to the ranking
induced when each example is only rated once: as
reported in Section 3, the ?b is, in this case, 0.75.
For the TIDES corpus, we computed the num-
ber of pairs of judges for which the results of a
pairwise comparison between two systems is dif-
ferent when the systems are evaluated using the
predicted scores and the true scores. Results pre-
sented in Table 3 show that considering the pre-
dicted matrix is far better than having judges rate
140
QE TIDES
k pred mean rand pred mean rand
40% 1.14 ?2.9 ? 10?2 0.78 ?6.6 ? 10?3 1.45 ? ? ?
30% 0.94 ?2.9 ? 10?2 0.78 ?7.4 ? 10?3 1.44 0.95 ?2.7 ? 10?2 0.43 ?2.6 ? 10?2 1.37
20% 0.77 ?3.4 ? 10?2 0.78 ?1.0 ? 10?2 1.45 0.76 ?2.6 ? 10?2 0.41 ?2.5 ? 10?2 1.38
10% 0.65 ?2.1 ? 10?2 0.79 ?1.9 ? 10?2 1.47 0.48 ?3.0 ? 10?2 0.41 ?2.5 ? 10?2 1.36
Table 1: Completion performance as evaluated by the MAE for the three prediction methods and the
three corpora considered.
random samples of the examples: the number of
disagreement falls from 20% (Sect. 3) to less than
4%. While the mean method outperforms the
pred method, this result shows that, even in case
of low inter-rater agreement, there is still enough
information to predict the score of one annotator
knowing only the score of the others.
For the tasks considered, decisions based on a
recovered matrix are therefore more similar to de-
cisions made considering the full score matrix than
decisions based on a single rating of each example.
5 Conclusion
This paper proposed a new way of collecting reli-
able human assessment. We showed, on two cor-
pora, that knowing multiple scores for each exam-
ple instead of a single score results in a more reli-
able estimation of the quality of a NLP system. We
proposed to used matrix completion techniques
to reduce the annotation effort required to collect
these multiple ratings. Our experiments showed
that while scores predicted using these techniques
are pretty different from the true scores, decisions
considering them are more reliable than decisions
based on a single score.
Even if it can not predict scores accurately, we
believe that the connection between NLP evalua-
tion and matrix completion has many potential ap-
plications. For instance, it can be applied to iden-
tify errors made when collecting scores by com-
paring the predicted and actual scores.
6 Acknowledgments
This work was partly supported by ANR project
Trace (ANR-09-CORD-023). The author would
like to thank Franc?ois Yvon and Nicolas Pe?cheux
for their helpful questions and comments on the
various drafts of this work.
% missing data pred mean
30% 9.24% 3.53 %
20% 6.45% 2.10 %
10% 3.66% 1.20 %
Table 3: Disagreements in a pairwise comparison
of two systems of the TIDES corpus, when the
systems are evaluated using the predicted scores
and the true scores
References
Ron Artstein and Massimo Poesio. 2008. Inter-coder
agreement for computational linguistics. Comput.
Linguist., 34(4):555?596, December.
Stephen R. Becker, Emmanuel J. Cande`s, and
Michael C. Grant. 2011. Templates for convex cone
problems with applications to sparse signal recovery.
Math. Prog. Comput., 3(3):165?218.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Matt Post, Radu Soricut, and Lucia Specia. 2012.
Findings of the 2012 workshop on statistical ma-
chine translation. In Proc. of WMT, pages 10?51,
Montre?al, Canada, June. ACL.
Emmanuel Cande`s and Benjamin Recht. 2012. Exact
matrix completion via convex optimization. Com-
mun. ACM, 55(6):111?119, June.
A. Chistov and D. Grigor?ev. 1984. Complexity of
quantifier elimination in the theory of algebraically
closed fields. In M. Chytil and V. Koubek, editors,
Math. Found. of Comp. Science, volume 176, pages
17?31. Springer Berlin / Heidelberg.
Philipp Koehn and Christof Monz. 2006. Manual and
automatic evaluation of machine translation between
european languages. In Proc. WMT, pages 102?121,
New York City, June. ACL.
Philipp Koehn. 2012. Simulating human judgment in
machine translation evaluation campaigns. In Proc.
of IWSLT.
Charles L. Lawson and Richard J. Hanson. 1974. Solv-
ing Least Squares Problems. Prentice Hall.
141
Stephen J: Leon. 1994. Linear Algebra with Applica-
tions. Macmillan,.
Adam Lopez. 2012. Putting human assessments of
machine translation systems in order. In Proc. of
WMT, pages 1?9, Montre?al, Canada, June. ACL.
Yann Mathet, Antoine Widlcher, Kare?n Fort, Claire
Franc?ois, Olivier Galibert, Cyril Grouin, Juliette
Kahn, Sophie Rosset, and Pierre Zweigenbaum.
2012. Manual corpus annotation: Giving meaning
to the evaluation metrics. In Proceedings of COL-
ING 2012: Posters, pages 809?818, Mumbai, India,
December.
Daniele Pighin, Llu??s Formiga, and Llu??s Ma`rquez.
2012. A graph-based strategy to streamline trans-
lation quality assessments. In Proc. of AMTA.
Matthew Snover, Nitin Madnani, Bonnie Dorr, and
Richard Schwartz. 2009. Fluency, adequacy, or
HTER? Exploring different human judgments with
a tunable MT metric. In Proc. of WMT, pages 259?
268, Athens, Greece, March. ACL.
Radu Soricut and Abdessamad Echihabi. 2010.
Trustrank: Inducing trust in automatic translations
via ranking. In Proc. of ACL, pages 612?621, Upp-
sala, Sweden, July. ACL.
142
Proceedings of the 6th Workshop on Statistical Machine Translation, pages 309?315,
Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational Linguistics
LIMSI @ WMT11
Alexandre Allauzen
He?le`ne Bonneau-Maynard
Hai-Son Le
Aure?lien Max
Guillaume Wisniewski
Franc?ois Yvon
Univ. Paris-Sud and LIMSI-CNRS
B.P. 133, 91403 Orsay cedex, France
Gilles Adda
Josep M. Crego
Adrien Lardilleux
Thomas Lavergne
Artem Sokolov
LIMSI-CNRS
B.P. 133, 91403 Orsay cedex, France
Abstract
This paper describes LIMSI?s submissions to
the Sixth Workshop on Statistical Machine
Translation. We report results for the French-
English and German-English shared transla-
tion tasks in both directions. Our systems
use n-code, an open source Statistical Ma-
chine Translation system based on bilingual
n-grams. For the French-English task, we fo-
cussed on finding efficient ways to take ad-
vantage of the large and heterogeneous train-
ing parallel data. In particular, using a sim-
ple filtering strategy helped to improve both
processing time and translation quality. To
translate from English to French and Ger-
man, we also investigated the use of the
SOUL language model in Machine Trans-
lation and showed significant improvements
with a 10-gram SOUL model. We also briefly
report experiments with several alternatives to
the standard n-best MERT procedure, leading
to a significant speed-up.
1 Introduction
This paper describes LIMSI?s submissions to the
Sixth Workshop on Statistical Machine Translation,
where LIMSI participated in the French-English and
German-English tasks in both directions. For this
evaluation, we used n-code, our in-house Statistical
Machine Translation (SMT) system which is open-
source and based on bilingual n-grams.
This paper is organized as follows. Section 2 pro-
vides an overview of n-code, while the data pre-
processing and filtering steps are described in Sec-
tion 3. Given the large amount of parallel data avail-
able, we proposed a method to filter the French-
English GigaWord corpus (Section 3.2). As in our
previous participations, data cleaning and filtering
constitute a non-negligible part of our work. This
includes detecting and discarding sentences in other
languages; removing sentences which are also in-
cluded in the provided development sets, as well as
parts that are repeated (for the monolingual news
data, this can reduce the amount of data by a fac-
tor 3 or 4, depending on the language and the year);
normalizing the character set (non-utf8 characters
which are aberrant in context, or in the case of the
GigaWord corpus, a lot of non-printable and thus in-
visible control characters such as EOT (end of trans-
mission)1).
For target language modeling (Section 4), a stan-
dard back-off n-gram model is estimated and tuned
as described in Section 4.1. Moreover, we also in-
troduce in Section 4.2 the use of the SOUL lan-
guage model (LM) (Le et al, 2011) in SMT. Based
on neural networks, the SOUL LM can handle an
arbitrary large vocabulary and a high order marko-
vian assumption (up to 10-gram in this work). Fi-
nally, experimental results are reported in Section 5
both in terms of BLEU scores and translation edit
rates (TER) measured on the provided newstest2010
dataset.
2 System Overview
Our in-house n-code SMT system implements the
bilingual n-gram approach to Statistical Machine
Translation (Casacuberta and Vidal, 2004). Given a
1This kind of characters was used for Teletype up to the sev-
enties or early eighties.
309
source sentence sJ1, a translation hypothesis t?
I
1 is de-
fined as the sentence which maximizes a linear com-
bination of feature functions:
t?I1 = argmax
tI1
{
M
?
m=1
?mhm(sJ1, tI1)
}
(1)
where sJ1 and t
I
1 respectively denote the source and
the target sentences, and ?m is the weight associated
with the feature function hm. The translation fea-
ture is the log-score of the translation model based
on bilingual units called tuples. The probability as-
signed to a sentence pair by the translation model is
estimated by using the n-gram assumption:
p(sJ1, t
I
1) =
K
?
k=1
p((s, t)k|(s, t)k?1 . . .(s, t)k?n+1)
where s refers to a source symbol (t for target) and
(s, t)k to the kth tuple of the given bilingual sentence
pair. It is worth noticing that, since both languages
are linked up in tuples, the context information pro-
vided by this translation model is bilingual. In ad-
dition to the translation model, eleven feature func-
tions are combined: a target-language model (see
Section 4 for details); four lexicon models; two lex-
icalized reordering models (Tillmann, 2004) aim-
ing at predicting the orientation of the next transla-
tion unit; a ?weak? distance-based distortion model;
and finally a word-bonus model and a tuple-bonus
model which compensate for the system preference
for short translations. The four lexicon models are
similar to the ones used in a standard phrase-based
system: two scores correspond to the relative fre-
quencies of the tuples and two lexical weights are
estimated from the automatically generated word
alignments. The weights associated to feature func-
tions are optimally combined using a discriminative
training framework (Och, 2003) (Minimum Error
Rate Training (MERT), see details in Section 5.4),
using the provided newstest2009 data as develop-
ment set.
2.1 Training
Our translation model is estimated over a training
corpus composed of tuple sequences using classi-
cal smoothing techniques. Tuples are extracted from
a word-aligned corpus (using MGIZA++2 with de-
fault settings) in such a way that a unique segmenta-
tion of the bilingual corpus is achieved, allowing to
estimate the n-gram model. Figure 1 presents a sim-
ple example illustrating the unique tuple segmenta-
tion for a given word-aligned pair of sentences (top).
Figure 1: Tuple extraction from a sentence pair.
The resulting sequence of tuples (1) is further re-
fined to avoid NULL words in the source side of the
tuples (2). Once the whole bilingual training data is
segmented into tuples, n-gram language model prob-
abilities can be estimated. In this example, note that
the English source words perfect and translations
have been reordered in the final tuple segmentation,
while the French target words are kept in their orig-
inal order.
2.2 Inference
During decoding, source sentences are encoded
in the form of word lattices containing the most
promising reordering hypotheses, so as to reproduce
the word order modifications introduced during the
tuple extraction process. Hence, at decoding time,
only those encoded reordering hypotheses are trans-
lated. Reordering hypotheses are introduced using
a set of reordering rules automatically learned from
the word alignments.
In the previous example, the rule [perfect transla-
tions ; translations perfect] produces the swap of
the English words that is observed for the French
and English pair. Typically, part-of-speech (POS)
information is used to increase the generalization
power of such rules. Hence, rewriting rules are built
using POS rather than surface word forms. Refer
2http://geek.kyloo.net/software
310
to (Crego and Marin?o, 2007) for details on tuple ex-
traction and reordering rules.
3 Data Pre-processing and Selection
We used all the available parallel data allowed in
the constrained task to compute the word align-
ments, except for the French-English tasks where
the United Nation corpus was not used to train our
translation models. To train the target language
models, we also used all provided data and mono-
lingual corpora released by the LDC for French
and English. Moreover, all parallel corpora were
POS-tagged with the TreeTagger (Schmid, 1994).
For German, the fine-grained POS information used
for pre-processing was computed by the RFTag-
ger (Schmid and Laws, 2008).
3.1 Tokenization
We took advantage of our in-house text process-
ing tools for the tokenization and detokenization
steps (De?chelotte et al, 2008). Previous experi-
ments have demonstrated that better normalization
tools provide better BLEU scores (Papineni et al,
2002). Thus all systems are built in ?true-case.?
As German is morphologically more complex
than English, the default policy which consists in
treating each word form independently is plagued
with data sparsity, which poses a number of diffi-
culties both at training and decoding time. Thus,
to translate from German to English, the German
side was normalized using a specific pre-processing
scheme (described in (Allauzen et al, 2010)), which
aims at reducing the lexical redundancy and splitting
complex compounds.
Using the same pre-processing scheme to trans-
late from English to German would require to post-
process the output to undo the pre-processing. As in
our last year?s experiments (Allauzen et al, 2010),
this pre-processing step could be achieved with a
two-step decoding. However, by stacking two de-
coding steps, we may stack errors as well. Thus, for
this direction, we used the German tokenizer pro-
vided by the organizers.
3.2 Filtering the GigaWord Corpus
The available parallel data for English-French in-
cludes a large Web corpus, referred to as the Giga-
Word parallel corpus. This corpus is very noisy, and
contains large portions that are not useful for trans-
lating news text. The first filter aimed at detecting
foreign languages based on perplexity and lexical
coverage. Then, to select a subset of parallel sen-
tences, trigram LMs were trained for both French
and English languages on a subset of the available
News data: the French (resp. English) LM was used
to rank the French (resp. English) side of the cor-
pus, and only those sentences with perplexity above
a given threshold were selected. Finally, the two se-
lected sets were intersected. In the following exper-
iments, the threshold was set to the median or upper
quartile value of the perplexity. Therefore, half (or
75%) of this corpus was discarded.
4 Target Language Modeling
Neural networks, working on top of conventional
n-gram models, have been introduced in (Bengio
et al, 2003; Schwenk, 2007) as a potential means
to improve conventional n-gram language models
(LMs). However, probably the major bottleneck
with standard NNLMs is the computation of poste-
rior probabilities in the output layer. This layer must
contain one unit for each vocabulary word. Such a
design makes handling of large vocabularies, con-
sisting of hundreds thousand words, infeasible due
to a prohibitive growth in computation time. While
recent work proposed to estimate the n-gram dis-
tributions only for the most frequent words (short-
list) (Schwenk, 2007), we explored the use of the
SOUL (Structured OUtput Layer Neural Network)
language model for SMT in order to handle vocabu-
laries of arbitrary sizes.
Moreover, in our setting, increasing the order of
standard n-gram LM did not show any significant
improvement. This is mainly due to the data spar-
sity issue and to the drastic increase in the number of
parameters that need to be estimated. With NNLM
however, the increase in context length at the input
layer results in only a linear growth in complexity
in the worst case (Schwenk, 2007). Thus, training
longer-context neural network models is still feasi-
ble, and was found to be very effective in our system.
311
4.1 Standard n-gram Back-off Language
Models
To train our language models, we assumed that the
test set consisted in a selection of news texts dat-
ing from the end of 2010 to the beginning of 2011.
This assumption was based on what was done for
the 2010 evaluation. Thus, for each language, we
built a development corpus in order to optimize the
vocabulary and the target language model.
Development set and vocabulary In order to
cover different periods, two development sets were
used. The first one is newstest2008. This corpus is
two years older than the targeted time period; there-
fore, a second development corpus named dev2010-
2011 was collected by randomly sampling bunches
of 5 consecutive sentences from the provided news
data of 2010 and 2011.
To estimate such large LMs, a vocabulary
was first defined for each language by including
all tokens observed in the Europarl and News-
Commentary corpora. For French and English, this
vocabulary was then expanded with all words that
occur more than 5 times in the French-English Gi-
gaWord corpus, and with the most frequent proper
names taken from the monolingual news data of
2010 and 2011. As for German, since the amount
of training data was smaller, the vocabulary was ex-
panded with the most frequent words observed in the
monolingual news data of 2010 and 2011. This pro-
cedure resulted in a vocabulary containing around
500k words in each language.
Language model training All the training data al-
lowed in the constrained task were divided into sev-
eral sets based on dates or genres (resp. 9 and 7
sets for English and French). On each set, a stan-
dard 4-gram LM was estimated from the 500k words
vocabulary using absolute discounting interpolated
with lower order models (Kneser and Ney, 1995;
Chen and Goodman, 1998).
All LMs except the one trained on the news cor-
pora from 2010-2011 were first linearly interpolated.
The associated coefficients were estimated so as to
minimize the perplexity evaluated on dev2010-2011.
The resulting LM and the 2010-2011 LM were fi-
naly interpolated with newstest2008 as development
data. This procedure aims to avoid overestimating
the weight associated to the 2010-2011 LM.
4.2 The SOUL Model
We give here a brief overview of the SOUL LM;
refer to (Le et al, 2011) for the complete training
procedure. Following the classical work on dis-
tributed word representation (Brown et al, 1992),
we assume that the output vocabulary is structured
by a clustering tree, where each word belongs to
only one class and its associated sub-classes. If wi
denotes the i-th word in a sentence, the sequence
c1:D(wi) = c1, . . . ,cD encodes the path for the word
wi in the clustering tree, with D the depth of the tree,
cd(wi) a class or sub-class assigned to wi, and cD(wi)
the leaf associated with wi (the word itself). The
n-gram probability of wi given its history h can then
be estimated as follows using the chain rule:
P(wi|h) = P(c1(wi)|h)
D
?
d=2
P(cd(wi)|h,c1:d?1)
Figure 2 represents the architecture of the NNLM
to estimate this distribution, for a tree of depth
D = 3. The SOUL architecture is the same as for
the standard model up to the output layer. The
main difference lies in the output structure which in-
volves several layers with a softmax activation func-
tion. The first softmax layer (class layer) estimates
the class probability P(c1(wi)|h), while other out-
put sub-class layers estimate the sub-class proba-
bilities P(cd(wi)|h,c1:d?1). Finally, the word layers
estimate the word probabilities P(cD(wi)|h,c1:D?1).
Words in the short-list are a special case since each
of them represents its own class without any sub-
classes (D = 1 in this case).
5 Experimental Results
The experimental results are reported in terms of
BLEU and translation edit rate (TER) using the
newstest2010 corpus as evaluation set. These auto-
matic metrics are computed using the scripts pro-
vided by the NIST after a detokenization step.
5.1 English-French
Compared with last year evaluation, the amount of
available parallel data has drastically increased with
about 33M of sentence pairs. It is worth noticing
312
wi-1
w
i-2
w
i-3
R
R
R
W
ih
 shared context space
input layer
hidden layer:
tanh activation
word layers
sub-class 
layers
}
short list
Figure 2: Architecture of the Structured Output Layer
Neural Network language model.
that the provided corpora are not homogeneous, nei-
ther in terms of genre nor in terms of topics. Never-
theless, the most salient difference is the noise car-
ried by the GigaWord and the United Nation cor-
pora. The former is an automatically collected cor-
pus drawn from different websites, and while some
parts are indeed relevant to translate news texts, us-
ing the whole GigaWord corpus seems to be harm-
ful. The latter (United Nation) is obviously more
homogeneous, but clearly out of domain. As an il-
lustration, discarding the United Nation corpus im-
proves performance slightly.
Table 1 summarizes some of our attempts at deal-
ing with such a large amount of parallel data. As
stated above, translation models are trained with
the news-commentary, Europarl, and GigaWord cor-
pora. For this last data set, results show the reward of
sentence pair selection as described in Section 3.2.
Indeed, filtering out 75% of the corpus yields to
a significant BLEU improvement when translating
from English to French and of 1 point in the other
direction (line upper quartile in Table 1). More-
over, a larger selection (50% in the median line) still
increases the overall performance. This shows the
room left for improvement by a more accurate data
selection process such as a well optimized thresh-
old in our approach, or a more sophisticated filtering
strategy (see for example (Foster et al, 2010)).
Another issue when using such a large amount
System en2fr fr2en
BLEU TER BLEU TER
All 27.4 56.6 26.8 55.0
Upper quartile 27.8 56.3 28.4 53.8
Median 28.1 56.0 28.6 53.5
Table 1: English-French translation results in terms of
BLEU score and TER estimated on newstest2010 with
the NIST script. All means that the translation model is
trained on news-commentary, Europarl, and the whole
GigaWord. The rows upper quartile and median corre-
spond to the use of a filtered version of the GigaWord.
of data is the mismatch between the target vocab-
ulary derived from the translation model and that of
the LM. The translation model may generate words
which are unknown to the LM, and their probabili-
ties could be overestimated. To avoid this behaviour,
the probability of unknown words for the target LM
is penalized during the decoding step.
5.2 English-German
For this translation task, we compare the impact of
two different POS-taggers to process the German
part of the parallel data. The results are reported
in Table 2. Results show that to translate from En-
glish to German, the use of a fine-grained POS infor-
mation (RFTagger) leads to a slight improvement,
whereas it harms the source reordering model in the
other direction. It is worth noticing that to translate
from German to English, the RFTagger is always
used during the data pre-processing step, while a dif-
ferent POS tagger may be involved for the source
reordering model training.
System en2de de2en
BLEU TER BLEU TER
RFTagger 22.8 60.1 16.3 66.0
TreeTagger 23.1 59.4 16.2 66.0
Table 2: Translation results in terms of BLEU score
and translation edit rate (TER) estimated on newstest2010
with the NIST scoring script.
5.3 The SOUL Model
As mentioned in Section 4.2, the order of a con-
tinuous n-gram model such as the SOUL LM can
be raised without a prohibitive increase in complex-
ity. We summarize in Table 3 our experiments with
313
SOUL LMs of orders 4, 6, and 10. The SOUL LM
is introduced in the SMT pipeline by rescoring the
n-best list generated by the decoder, and the asso-
ciated weight is tuned with MERT. We observe for
the English-French task: a BLEU improvement of
0.3, as well as a similar trend in TER, when intro-
ducing a 4-gram SOUL LM; an additional BLEU
improvement of 0.3 when increasing the order from
4 to 6; and a less important gain with the 10-gram
SOUL LM. In the end, the use of a 10-gram SOUL
LM achieves a 0.7 BLEU improvement and a TER
decrease of 0.8. The results on the English-German
task show the same trend with a 0.5 BLEU point
improvement.
SOUL LM en2fr en2de
BLEU TER BLEU TER
without 28.1 56.0 16.3 66.0
4-gram 28.4 55.5 16.5 64.9
6-gram 28.7 55.3 16.7 64.9
10-gram 28.8 55.2 16.8 64.6
Table 3: Translation results from English to French and
English to German measured on newstest2010 using a
100-best rescoring with SOUL LMs of different orders.
5.4 Optimization Issues
Along with MIRA (Margin Infused Relaxed Al-
gorithm) (Watanabe et al, 2007), MERT is the
most widely used algorithm for system optimiza-
tion. However, standard MERT procedure is known
to suffer from instability of results and very slow
training cycle with approximate estimates of one de-
coding cycle for each training parameter. For this
year?s evaluation, we experimented with several al-
ternatives to the standard n-best MERT procedure,
namely, MERT on word lattices (Macherey et al,
2008) and two differentiable variants to the BLEU
objective function optimized during the MERT cy-
cle. We have recast the former in terms of a spe-
cific semiring and implemented it using a general-
purpose finite state automata framework (Sokolov
and Yvon, 2011). The last two approaches, hereafter
referred to as ZHN and BBN, replace the BLEU
objective function, with the usual BLEU score on
expected n-gram counts (Rosti et al, 2010) and
with an expected BLEU score for normal n-gram
counts (Zens et al, 2007), respectively. All expecta-
tions (of the n-gram counts in the first case and the
BLEU score in the second) are taken over all hy-
potheses from n-best lists for each source sentence.
Experiments with the alternative optimization
methods achieved virtually the same performance in
terms of BLEU score, but 2 to 4 times faster. Neither
approach, however, showed any consistent and sig-
nificant improvement for the majority of setups tried
(with the exception of the BBN approach, that had
almost always improved over n-best MERT, but for
the sole French to English translation direction). Ad-
ditional experiments with 9 complementary transla-
tion models as additional features were performed
with lattice-MERT, but neither showed any substan-
tial improvement. In the view of these rather incon-
clusive experiments, we chose to stick to the classi-
cal MERT for the submitted results.
6 Conclusion
In this paper, we described our submissions to
WMT?11 in the French-English and German-
English shared translation tasks, in both directions.
For this year?s participation, we only used n-code,
our open source Statistical Machine Translation sys-
tem based on bilingual n-grams. Our contributions
are threefold. First, we have shown that n-gram
based systems can achieve state-of-the-art perfor-
mance on large scale tasks in terms of automatic
metrics such as BLEU. Then, as already shown by
several sites in the past evaluations, there is a signifi-
cant reward for using data selection algorithms when
dealing with large heterogeneous data sources such
as the GigaWord. Finally, the use of a large vocab-
ulary continuous space language model such as the
SOUL model has enabled to achieve significant and
consistent improvements. For the upcoming evalua-
tion(s), we would like to suggest that the important
work of data cleaning and pre-processing could be
shared among all the participants instead of being
done independently several times by each site. Re-
ducing these differences could indeed help improve
the reliability of SMT systems evaluation.
Acknowledgment
This work was achieved as part of the Quaero Pro-
gramme, funded by OSEO, French State agency for
innovation.
314
References
Alexandre Allauzen, Josep M. Crego, I?lknur Durgar El-
Kahlout, and Francois Yvon. 2010. LIMSI?s statis-
tical translation systems for WMT?10. In Proc. of the
Joint Workshop on Statistical Machine Translation and
MetricsMATR, pages 54?59, Uppsala, Sweden.
Yoshua Bengio, Re?jean Ducharme, Pascal Vincent, and
Christian Janvin. 2003. A neural probabilistic lan-
guage model. JMLR, 3:1137?1155.
P.F. Brown, P.V. de Souza, R.L. Mercer, V.J. Della Pietra,
and J.C. Lai. 1992. Class-based n-gram models of nat-
ural language. Computational Linguistics, 18(4):467?
479.
Francesco Casacuberta and Enrique Vidal. 2004. Ma-
chine translation with inferred stochastic finite-state
transducers. Computational Linguistics, 30(3):205?
225.
Stanley F. Chen and Joshua T. Goodman. 1998. An
empirical study of smoothing techniques for language
modeling. Technical Report TR-10-98, Computer Sci-
ence Group, Harvard University.
Josep Maria Crego and Jose? Bernardo Marin?o. 2007. Im-
proving statistical MT by coupling reordering and de-
coding. Machine Translation, 20(3):199?215.
Daniel De?chelotte, Gilles Adda, Alexandre Allauzen,
Olivier Galibert, Jean-Luc Gauvain, He?le`ne Mey-
nard, and Franc?ois Yvon. 2008. LIMSI?s statisti-
cal translation systems for WMT?08. In Proc. of the
NAACL-HTL Statistical Machine Translation Work-
shop, Columbus, Ohio.
George Foster, Cyril Goutte, and Roland Kuhn. 2010.
Discriminative instance weighting for domain adapta-
tion in statistical machine translation. In Proceedings
of the 2010 Conference on Empirical Methods in Natu-
ral Language Processing, pages 451?459, Cambridge,
MA, October.
Reinhard Kneser and Herman Ney. 1995. Improved
backing-off for m-gram language modeling. In Pro-
ceedings of the International Conference on Acoustics,
Speech, and Signal Processing, ICASSP?95, pages
181?184, Detroit, MI.
Hai-Son Le, Ilya Oparin, Alexandre Allauzen, Jean-Luc
Gauvain, and Franc?ois Yvon. 2011. Structured output
layer neural network language model. In IEEE Inter-
national Conference on Acoustics, Speech and Signal
Processing (ICASSP 2011), Prague (Czech Republic),
22-27 May.
Wolfgang Macherey, Franz Josef Och, Ignacio Thayer,
and Jakob Uszkoreit. 2008. Lattice-based minimum
error rate training for statistical machine translation.
In Proc. of the Conf. on EMNLP, pages 725?734.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In ACL ?03: Proc. of
the 41st Annual Meeting on Association for Computa-
tional Linguistics, pages 160?167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic eval-
uation of machine translation. In ACL ?02: Proc. of
the 40th Annual Meeting on Association for Compu-
tational Linguistics, pages 311?318. Association for
Computational Linguistics.
Antti-Veikko I. Rosti, Bing Zhang, Spyros Matsoukas,
and Richard Schwartz. 2010. BBN system description
for wmt10 system combination task. In Proceedings of
the Joint Fifth Workshop on Statistical Machine Trans-
lation and MetricsMATR, WMT ?10, pages 321?326,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Helmut Schmid and Florian Laws. 2008. Estimation
of conditional probabilities with decision trees and an
application to fine-grained POS tagging. In Proceed-
ings of the 22nd International Conference on Com-
putational Linguistics (Coling 2008), pages 777?784,
Manchester, UK, August.
Helmut Schmid. 1994. Probabilistic part-of-speech tag-
ging using decision trees. In Proc. of International
Conference on New Methods in Language Processing,
pages 44?49, Manchester, UK.
Holger Schwenk. 2007. Continuous space language
models. Computer, Speech & Language, 21(3):492?
518.
Artem Sokolov and Franc?ois Yvon. 2011. Minimum er-
ror rate training semiring. In Proceedings of the 15th
Annual Conference of the European Association for
Machine Translation, EAMT?2011, May.
Christoph Tillmann. 2004. A unigram orientation model
for statistical machine translation. In Proceedings of
HLT-NAACL 2004, pages 101?104. Association for
Computational Linguistics.
Taro Watanabe, Jun Suzuki, Hajime Tsukada, and Hideki
Isozaki. 2007. Online large-margin training for sta-
tistical machine translation. In Proceedings of the
2007 Joint Conference on Empirical Methods in Nat-
ural Language Processing and Computational Natu-
ral Language Learning (EMNLP-CoNLL), pages 764?
773, Prague, Czech Republic.
Richard Zens, Sasa Hasan, and Hermann Ney. 2007.
A systematic comparison of training criteria for sta-
tistical machine translation. In Proceedings of the
2007 Joint Conference on Empirical Methods in Nat-
ural Language Processing and Computational Natu-
ral Language Learning (EMNLP-CoNLL), pages 524?
532.
315
Proceedings of the 7th Workshop on Statistical Machine Translation, pages 157?162,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
Non-Linear Models for Confidence Estimation
Yong Zhuang?
Zhejiang University
866 Yuhangtang Road
Hangzhou, China
yong.zhuang22@gmail.com
Guillaume Wisniewski and Franc?ois Yvon
Univ. Paris Sud and LIMSI?CNRS
rue John von Neumann
91403 Orsay CEDEX, France
{firstname.lastname}@limsi.fr
Abstract
This paper describes our work with the data
distributed for the WMT?12 Confidence Es-
timation shared task. Our contribution is
twofold: i) we first present an analysis of
the data which highlights the difficulty of
the task and motivates our approach; ii) we
show that using non-linear models, namely ran-
dom forests, with a simple and limited feature
set, succeeds in modeling the complex deci-
sions required to assess translation quality and
achieves results that are on a par with the sec-
ond best results of the shared task.
1 Introduction
Confidence estimation is the task of predicting the
quality of a system prediction without knowledge
of the expected output. It is an important step
in many Natural Language Processing applications
(Gandrabur et al, 2006). In Machine Translation
(MT), this task has recently gained interest (Blatz
et al, 2004; Specia et al, 2010b; Soricut and Echi-
habi, 2010; Bach et al, 2011). Indeed, professional
translators are more and more requested to post-edit
the outputs of a MT system rather than to produce
a translation from scratch. Knowing in advance the
segments they should focus on would be very help-
ful (Specia et al, 2010a). Confidence estimation is
also of great interest for developers of MT system, as
it provides them with a way to analyze the systems
output and to better understand the main causes of
errors.
Even if several studies have tackled the problem
of confidence estimation in machine translation, un-
til now, very few datasets were publicly available and
comparing the proposed methods was difficult, if not
impossible. To address this issue, WMT?12 orga-
nizers proposed a shared task aiming at predict the
?This work was conducted during an internship at LIMSI?
CNRS
quality of a translation and provided the associated
datasets, baselines and metrics.
This paper describes our work with the data of the
WMT?12 Confidence Estimation shared task. Our
contribution is twofold: i) we first present an analysis
of the provided data that will stress the difficulty of
the task and motivate the choice of our approach; ii)
we show how using non-linear models, namely ran-
dom forests, with a simple and limited features set
succeed in modeling the complex decisions require
to assess translation quality and achieve the second
best results of the shared task.
The rest of this paper is organized as follows: Sec-
tion 2 summarizes our analysis of the data; in Sec-
tion 3, we describe our learning method; our main
results are finally reported in Section 4.
2 Data Analysis
In this section, we quickly analyze the data dis-
tributed in the context of the WMT?12 Confidence
Estimation Shared Task in order to evaluate the diffi-
culty of the task and to find out what predictors shall
be used. We will first describe the datasets, then the
features usually considered in confidence estimation
tasks and finally summarize our analyses.
2.1 Datasets
The datasets used in our experiments were released
for the WMT?12 Quality Estimation Task. All the
data provided in this shared task are based on the
test set of WMT?09 and WMT?10 translation tasks.
The training set is made of 1, 832 English sen-
tences and their Spanish translations as computed by
a standard Moses system. Each sentence pair is ac-
companied by an estimate of its translation quality.
This score is the average of ordinal grades assigned
by three human evaluators. The human grades are in
the range 1 to 5, the latter standing for a very good
translation that hardly requires post-editing, while
the former stands for a bad translation that does
157
not deserve to be edited, meaning that the machine
output useless and that translation should better be
produced from scratch. The test contains 422 sen-
tence pairs, the quality of which has to be predicted.
The training set alo contains additional material,
namely two references (the reference originally given
by WMT and a human post-edited one), which will
allow us to better interpret our results. No references
were provided for the test set.
2.2 Features
Several works have studied the problem of confidence
estimation (Blatz et al, 2004; Specia et al, 2010b) or
related problems such as predicting readability (Ka-
nungo and Orr, 2009) or developing automated essay
scoring systems (Burstein et al, 1998). They all use
the same basic features:
IBM 1 score measures the quality of the ?associa-
tion? of the source and the target sentence using
bag-of-word translation models;
Language model score accounts for the ?flu-
ency?, ?grammaticality? and ?plausibility? of a
target sentence;
Simple surface features like the sentence length,
the number of out-of-vocabulary words or words
that are not aligned. These features are used to
account for the difficulty of the translation task.
More elaborated features, derived, for instance,
from parse trees or dependencies analysis have also
been used in past studies. However they are far more
expensive to compute and rely on the existence of ex-
ternal resources, which may be problematic for some
languages. That is why we only considered a re-
stricted number of basic features in this work1. An-
other reason for considering such a small set of fea-
tures is the relatively small size of the training set: in
our preliminary experiments, considering more fea-
tures, especially lexicalized features that would be of
great interest for failure analysis, always resulted in
overfitting.
2.3 Data Analysis
The distribution of the human scores on the training
set is displayed in Figure 1. Surprisingly enough,
the baseline translation system used to generate the
data seems to be pretty good: 73% of the sentences
have a score higher than 3 on a 1 to 5 scale. It
also appears that most scores are very close: more
than half of them are located around the mean. As
a consequence, it seems that distinguishing between
them will require to model subtle nuances.
1The complete list of features is given in Appendix A.
Figure 1: Distribution of the human scores on the train
set. (HS? stands for Human Scores)
Figure 2 plots the distribution of quality scores
as a function of the Spanish-to-English IBM 1 score
and of the probability of the target sentence. These
two scores were computed with the same models that
were used to train the MT systems that have gener-
ated the training data. It appears that even if the
examples are clustered by their quality, these clusters
overlap and the frontiers between them are fuzzy and
complex. Similar observations were made for others
features.
Figure 2: Quality scores as a function of the Spanish-to-
English IBM 1 score and of the probability of the target
sentence (HS? stands for Human Scores)
These observations prove that a predictor of the
translation quality has to capture complex interac-
tion patterns in the training data. Standard results
from machine learning show that such structures can
be described either by a linear model using a large
number of features or by a non-linear model using a
158
(potentially) smaller set of features. As only a small
number of training examples is available, we decided
to focus on non-linear models in this work.
3 Inferring quality scores
Predicting the quality scores can naturally be cast
as a standard regression task, as the reference scores
used in the evaluation are numerical (real) values.
Regression is the approach adopted in most works
on confidence estimation for MT (Albrecht and Hwa,
2007; Specia et al, 2010b). A simpler way to tackle
the problem would be to recast it as binary classi-
fication task aiming at distinguishing ?good? trans-
lations from ?bad? ones (Blatz et al, 2004; Quirk,
2004). It is also possible, as shown by (Soricut and
Echihabi, 2010), to use ranking approaches. How-
ever, because the shared task is evaluated by com-
paring the actual value of the predictions with the
human scores, using these last two frameworks is not
possible.
In our experiments, following the observations re-
ported in the previous section, we use two well-
known non-linear regression methods: polynomial
regression and random forests. We also consider lin-
ear regression as a baseline. We will now quickly
describe these three methods.
Linear regression (Hastie et al, 2003) is a simple
model in which the prediction is defined by a linear
combination of the feature vector x: y? = ?0 + x>?,
where ?0 and ? are the parameters to estimate.
These parameters are usually learned by minimiz-
ing the sum of squared deviations on the training
set, which is an easy optimization problem with a
close-form solution.
Polynomial regression (Hastie et al, 2003) is a
straightforward generalization of linear regression in
which the relationship between the features and the
label is modeled as a n-th order polynomial. By care-
fully extending the feature vector, the model can be
reduced to a linear regression model and trained in
the same way.
Random forest regressor (Breiman, 2001) is an en-
semble method that learns many regression trees and
predicts an aggregation of their result. In contrast
with standard decision tree, in which each node is
split using the best split among all features, in a ran-
dom forest the split is chosen randomly. In spite of
this simple and counter-intuitive learning strategy,
random forests have proven to be very good ?out-
of-the-box? learners and have achieved state-of-the-
art performance in many tasks, demonstrating both
their robustness to overfitting and their ability to
take into account complex interactions between fea-
tures.
In our experiments, we use the implementation
provided by scikit-learn (Pedregosa et al, 2011).
Hyper-parameters of the random forest (the num-
ber of trees and the stopping criterion) were chosen
by 10-fold cross-validation.
4 Experimental Setting
4.1 Features
In all our experiments, we considered a simple de-
scription of the translation hypotheses relying on
31 features. The complete list of features is given
in Appendix A. All these features have already been
used in works related to ours and are simple fea-
tures that can be easily computed using only a lim-
ited number of external resources.
A key finding in our preliminary experiments is
the need to re-scale the features by dividing their
value by the length of the corresponding sentence
(e.g. the language model score of a source sentence
will be divided by its length of the source sentence,
and the one of a target sentence will be done by its
length of the target sentence). This rescaling makes
features that depend on the sentence length (like the
LM score) comparable and results in a large improve-
ment of the performance of the associated feature.
4.2 Metrics
The two metrics used to evaluate prediction perfor-
mance are the standard metrics for regression: Mean
Absolute Error (MAE) and Root Mean Squared Er-
ror (RMSE) defined by:
MAE =
1
n
n?
i=1
|y?i ? yi|
RMSE =
?
?
?
? 1
n
n?
i=1
(y?i ? yi)
2
where n is the number of examples, yi and y?i the true
label and predicted label of the ith example. MAE
can be understood as the averaged error made in
predicting the quality of a translation. As it is easy
to interpret, we will use it to analyze our results.
RMSE scores are reported to facilitate comparison
with other submissions to the shared task.
All the reported scores have been computed using
the tools provided by the Quality Estimation task
organizers2.
2https://github.com/lspecia/QualityEstimation
159
4.3 Results
Table 1 details the results achieved by the different
methods introduced in the previous section. All of
them achieve similar performances: their MAE is be-
tween 0.64 and 0.66, which is a pretty good result as
the best reported MAE in the shared task is 0.61.
Our best model is the second-best when submissions
are ranked according to their MAE.
Even if their results are very close (significance of
the score differences will be investigated in the fol-
lowing subsection), all non-linear models outperform
a simple linear regression, which corroborates the ob-
servations made in Section 2.
For the polynomial regression, we tried different
polynomial orders in order to achieve an optimal
setting. Even if this method achieves the best re-
sults when the model is selected on the test set, it is
not usable in practice: when we tried to select the
polynomial degree by cross-validation, the regressors
systematically overfitted due to the reduction of the
number of examples. That is why random forests,
which do not suffer from overfitting and can learn
good predictor even when features outnumber exam-
ples, is our method of choice.
4.4 Interpretation
To get a better understanding of the task difficulty
and to make interpretation of the error rate easier,
we train another regressor using an ?oracle? feature:
the hTER score. It is clear that this feature can only
be computed on the training set and that considering
it does not make much sense in a ?real-life? scenario.
However, this feature is supposed to be highly rele-
vant to the quality prediction task and should there-
fore result in a ?large? reduction of the error rates.
Quantifying what ?large? means in this context will
allow us to analyze the results presented in Table 1.
Training a random forest with this additional fea-
ture on 1, 400 examples of the train set chosen ran-
domly reduces the MAE evaluated on the 432 re-
maining examples by 0.10 and the RMSE by 0.12.
This small reduction stresses how difficult the task
is. Comparatively, the 0.02 reduction achieved by
replacing a linear model with a non-linear model
should therefore be considered noteworthy. Further
investigations are required to find out whether the
difficulty of the task results from the way human
scores are collected (low inter-annotators agreement,
bias in the gathering of the collection, ...) or from
the impossibility to solve the task using only surface
features.
Another important question in the analysis of our
results concerns the usability of our approach: an
error of 0.6 seems large on a 1 to 5 scale and may
question the interest of our approach. To allow a fine-
grained analysis, we report the correlation between
the predicted score and the human score (Figure 3)
and the distribution of the absolute error (Figure 4).
These figures show that the actual error is often quite
small: for more than 45% of the examples, the error
is smaller than 0.5 and for 23% it is smaller than 0.2.
Figure 3 also shows that the correlation between our
predictions and the true labels is ?substantial? ac-
cording to the established guidelines of (Landis and
Koch, 1977) (the Pearson correlation coefficient is
greater than 0.6). The difference between the mean
of the two distributions is however quite large. Cen-
tering the predictions on the mean of the true label
may improves the MAE. This observation also sug-
gests that we should try to design evaluation metrics
that do not rely on the actual predicted values.
Figure 3: Correlation between our predictions and the
true label (HS? stands for Human Scores)
5 Conclusion
In this work, we have presented, a simple, yet effi-
cient, method to predict the quality of a translation.
Using simple features and a non-linear model, our
approach has achieved results close to the best sub-
mission to the Confidence Estimation shared task,
which supports the results of our analysis of the data.
In our future work, we aim at considering more fea-
tures, avoiding overfitting thanks to features selec-
tion methods.
Even if a fine-grained analysis of our results shows
the interest and usefulness of our approach, more re-
mains to be done to develop reliable confidence esti-
mation methods. Our results also highlight the need
to continue gathering high-quality resources to train
and investigate confidence estimation systems: even
when considering only very few features, our systems
160
Train Test
Methods parameters MAE RMSE MAE RMSE
linear regression ? 0.58 0.71 0.66 0.82
polynomial regression
n=2 0.55 0.68 0.64 0.79
n=3 0.54 0.67 0.64 0.79
n=4 0.54 0.67 0.65 0.85
random forest cross-validated 0.39 0.46 0.64 0.80
Table 1: Prediction performance achieved by different regressors
Figure 4: Distribution of the absolute error (|yi ? y?i|) of
our predictions
were prone to overfitting. Developing more elabo-
rated systems will therefore only be possible if more
training resource is available.
Acknowledgment
The authors would like to thank Nicolas Usunier for
helpful discussions about ranking and regression us-
ing random forest. This work was partially funded by
the French National Research Agency under project
ANR-CONTINT-TRACE.
References
Joshua Albrecht and Rebecca Hwa. 2007. Regression for
sentence-level mt evaluation with pseudo references. In
Proceedings of the 45th Annual Meeting of the Asso-
ciation of Computational Linguistics, pages 296?303,
Prague, Czech Republic, June. Association for Com-
putational Linguistics.
Nguyen Bach, Fei Huang, and Yaser Al-Onaizan. 2011.
Goodness: a method for measuring machine transla-
tion confidence. In Proceedings of the 49th Annual
Meeting of the Association for Computational Linguis-
tics: Human Language Technologies - Volume 1, HLT
?11, pages 211?219, Stroudsburg, PA, USA. Associa-
tion for Computational Linguistics.
John Blatz, Erin Fitzgerald, George Foster, Simona Gan-
drabur, Cyril Goutte, Alex Kulesza, Alberto Sanchis,
and Nicola Ueffing. 2004. Confidence estimation for
machine translation. In Proceedings of the 20th in-
ternational conference on Computational Linguistics,
COLING ?04, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Leo Breiman. 2001. Random forests. Mach. Learn.,
45(1):5?32, October.
Jill Burstein, Karen Kukich, Susanne Wolff, Chi Lu, Mar-
tin Chodorow, Lisa Braden-Harder, and Mary Dee
Harris. 1998. Automated scoring using a hybrid fea-
ture identification technique. In Proceedings of the
17th international conference on Computational lin-
guistics - Volume 1, COLING ?98, pages 206?210,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Simona Gandrabur, George Foster, and Guy Lapalme.
2006. Confidence estimation for nlp applications.
ACM Trans. Speech Lang. Process., 3(3):1?29, Octo-
ber.
T. Hastie, R. Tibshirani, and J. H. Friedman. 2003. The
Elements of Statistical Learning. Springer, July.
Tapas Kanungo and David Orr. 2009. Predicting the
readability of short web summaries. In Proceedings
of the Second ACM International Conference on Web
Search and Data Mining, WSDM ?09, pages 202?211,
New York, NY, USA. ACM.
R. J. Landis and G. G. Koch. 1977. The measurement of
observer agreement for categorical data. Biometrics,
33(1):159?174.
F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,
B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer,
R. Weiss, V. Dubourg, J. Vanderplas, A. Passos,
D. Cournapeau, M. Brucher, M. Perrot, and E. Duch-
esnay. 2011. Scikit-learn: Machine Learning in
Python . Journal of Machine Learning Research,
12:2825?2830.
Chris Quirk. 2004. Training a sentence-level machine
translation confidence metric. In Proceedings of the
4th International Conference on Language Resources
and Evaluation (LREC), pages 825?828.
Radu Soricut and Abdessamad Echihabi. 2010.
Trustrank: Inducing trust in automatic translations
via ranking. In Proceedings of the 48th Annual Meet-
ing of the Association for Computational Linguistics,
pages 612?621, Uppsala, Sweden, July. Association for
Computational Linguistics.
161
Lucia Specia, Nicola Cancedda, and Marc Dymetman.
2010a. A dataset for assessing machine translation
evaluation metrics. In 7th Conference on Interna-
tional Language Resources and Evaluation (LREC-
2010), pages 3375?3378, Valletta, Malta.
Lucia Specia, Dhwaj Raj, and Marco Turchi. 2010b. Ma-
chine translation evaluation versus quality estimation.
Machine Translation, 24(1):39?50, March.
A Features List
Here is the whole list of the 31 features we used in
our experiments (? has been used in the baseline of
the shared task organizer):
? ? Number of tokens in the source sentence
? ? Number of tokens in the target sentence
? ? Average token length in source sentence
? English-Spanish IBM 1 scores
? Spanish-English IBM 1 scores
? English-Spanish IBM 1 scores divided by the
length of source sentence
? English-Spanish IBM 1 scores divided by the
length of target sentence
? Spanish-English IBM 1 scores divided by the
length of source sentence
? Spanish-English IBM 1 scores divided by the
length of target sentence
? Number of out-of-vocabulary in source sentence
? Number of out-of-vocabulary in target sentence
? Out-of-vocabulary rates in source sentence
? Out-of-vocabulary rates in target sentence
? log10(LM probability of source sentence)
? log10(LM probability of target sentence)
? log10(LM probability of source sentence) divided
by the length of source sentence
? log10(LM probability of target sentence) divided
by the length of target sentence
? Ratio of functions words in source sentence
? Ratio of functions words in target sentence
? ? Number of occurrences of the target word
within the target hypothesis (averaged for all
words in the hypothesis - type/token ratio)
? ? Average number of translations per source
word in the sentence (as given by IBM 1 table
thresholded so that prob(t|s) > 0.2)
? ? Average number of translations per source
word in the sentence (as given by IBM 1 table
thresholded so that prob(t|s) > 0.01) weighted
by the inverse frequency of each word in the
source corpus
? ? Percentage of unigrams in quartile 1 of fre-
quency (lower frequency words) in a corpus of
the source language (SMT training corpus)
? ? Percentage of unigrams in quartile 4 of fre-
quency (higher frequency words) in a corpus of
the source sentence
? ? Percentage of bigrams in quartile 1 of fre-
quency of source words in a corpus of the source
language
? ? Percentage of bigrams in quartile 4 of fre-
quency of source words in a corpus of the source
language
? ? Percentage of trigrams in quartile 1 of fre-
quency of source words in a corpus of the source
language
? ? Percentage of trigrams in quartile 4 of fre-
quency of source words in a corpus of the source
language
? ? Percentage of unigrams in the source sentence
seen in a corpus (SMT training corpus)
? ? Number of punctuation marks in the source
sentence
? ? Number of punctuation marks in the target
sentence
162
Proceedings of the 7th Workshop on Statistical Machine Translation, pages 330?337,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
LIMSI @ WMT?12
Hai-Son Le1,2, Thomas Lavergne2, Alexandre Allauzen1,2,
Marianna Apidianaki2, Li Gong1,2, Aure?lien Max1,2,
Artem Sokolov2, Guillaume Wisniewski1,2, Franc?ois Yvon1,2
Univ. Paris-Sud1 and LIMSI-CNRS2
rue John von Neumann, 91403 Orsay cedex, France
{firstname.lastname}@limsi.fr
Abstract
This paper describes LIMSI?s submissions to
the shared translation task. We report results
for French-English and German-English in
both directions. Our submissions use n-code,
an open source system based on bilingual
n-grams. In this approach, both the transla-
tion and target language models are estimated
as conventional smoothed n-gram models; an
approach we extend here by estimating the
translation probabilities in a continuous space
using neural networks. Experimental results
show a significant and consistent BLEU im-
provement of approximately 1 point for all
conditions. We also report preliminary experi-
ments using an ?on-the-fly? translation model.
1 Introduction
This paper describes LIMSI?s submissions to the
shared translation task of the Seventh Workshop
on Statistical Machine Translation. LIMSI partic-
ipated in the French-English and German-English
tasks in both directions. For this evaluation, we
used n-code, an open source in-house Statistical
Machine Translation (SMT) system based on bilin-
gual n-grams1. The main novelty of this year?s
participation is the use, in a large scale system, of
the continuous space translation models described
in (Hai-Son et al, 2012). These models estimate the
n-gram probabilities of bilingual translation units
using neural networks. We also investigate an alter-
native approach where the translation probabilities
of a phrase based system are estimated ?on-the-fly?
1http://ncode.limsi.fr/
by sampling relevant examples, instead of consider-
ing the entire training set. Finally we also describe
the use in a rescoring step of several additional fea-
tures based on IBM1 models and word sense disam-
biguation information.
The rest of this paper is organized as follows. Sec-
tion 2 provides an overview of the baseline systems
built with n-code, including the standard transla-
tion model (TM). The continuous space translation
models are then described in Section 3. As in our
previous participations, several steps of data pre-
processing, cleaning and filtering are applied, and
their improvement took a non-negligible part of our
work. These steps are summarized in Section 5.
The last two sections report experimental results ob-
tained with the ?on-the-fly? system in Section 6 and
with n-code in Section 7.
2 System overview
n-code implements the bilingual n-gram approach
to SMT (Casacuberta and Vidal, 2004; Marin?o et al,
2006; Crego and Marin?o, 2006). In this framework,
translation is divided in two steps: a source reorder-
ing step and a (monotonic) translation step. Source
reordering is based on a set of learned rewrite rules
that non-deterministically reorder the input words.
Applying these rules result in a finite-state graph of
possible source reorderings, which is then searched
for the best possible candidate translation.
2.1 Features
Given a source sentence s of I words, the best trans-
lation hypothesis t? is defined as the sequence of J
words that maximizes a linear combination of fea-
330
ture functions:
t? = argmax
t,a
{
M?
m=1
?mhm(a, s, t)
}
(1)
where ?m is the weight associated with feature func-
tion hm and a denotes an alignment between source
and target phrases. Among the feature functions, the
peculiar form of the translation model constitute one
of the main difference between the n-gram approach
and standard phrase-based systems. This will be fur-
ther detailled in section 2.2 and 3.
In addition to the translation model, fourteen
feature functions are combined: a target-language
model (Section 5.3); four lexicon models; six lexi-
calized reordering models (Tillmann, 2004; Crego
et al, 2011) aiming at predicting the orientation of
the next translation unit; a ?weak? distance-based
distortion model; and finally a word-bonus model
and a tuple-bonus model which compensate for the
system preference for short translations. The four
lexicon models are similar to the ones used in stan-
dard phrase-based systems: two scores correspond
to the relative frequencies of the tuples and two lexi-
cal weights are estimated from the automatic word
alignments. The weights vector ? is learned us-
ing a discriminative training framework (Och, 2003)
(Minimum Error Rate Training (MERT)) using the
newstest2009 as development set and BLEU (Pap-
ineni et al, 2002) as the optimization criteria.
2.2 Standard n-gram translation models
n-gram translation models rely on a specific de-
composition of the joint probability of a sentence
pair P (s, t): a sentence pair is assumed to be
decomposed into a sequence of L bilingual units
called tuples defining a joint segmentation: (s, t) =
u1, ..., uL2. In the approach of (Marin?o et al, 2006),
this segmentation is a by-product of source reorder-
ing obtained by ?unfolding? initial word alignments.
In this framework, the basic translation units are
tuples, which are the analogous of phrase pairs and
represent a matching u = (s, t) between a source
s and a target t phrase (see Figure 1). Using the
n-gram assumption, the joint probability of a seg-
2From now on, (s, t) thus denotes an aligned sentence pair,
and we omit the alignment variable a in further developments.
mented sentence pair decomposes as:
P (s, t) =
L?
i=1
P (ui|ui?1, ..., ui?n+1) (2)
During the training phase (Marin?o et al, 2006), tu-
ples are extracted from a word-aligned corpus (us-
ing MGIZA++3 with default settings) in such a
way that a unique segmentation of the bilingual
corpus is achieved. A baseline n-gram translation
model is then estimated over a training corpus com-
posed of tuple sequences using modified Knesser-
Ney Smoothing (Chen and Goodman, 1998).
2.3 Inference
During decoding, source sentences are represented
in the form of word lattices containing the most
promising reordering hypotheses, so as to reproduce
the word order modifications introduced during the
tuple extraction process. Hence, only those reorder-
ing hypotheses are translated and they are intro-
duced using a set of reordering rules automatically
learned from the word alignments.
In the example in Figure 1, the rule [prix no-
bel de la paix ; nobel de la paix prix] repro-
duces the invertion of the French words that is ob-
served when translating from French into English.
Typically, part-of-speech (POS) information is used
to increase the generalization power of these rules.
Hence, rewrite rules are built using POS rather than
surface word forms (Crego and Marin?o, 2006).
3 SOUL translation models
A first issue with the model described by equa-
tion (2) is that the elementary units are bilingual
pairs. As a consequence, the underlying vocabulary,
hence the number of parameters, can be quite large,
even for small translation tasks. Due to data sparsity
issues, such model are bound to face severe estima-
tion problems. Another problem with (2) is that the
source and target sides play symmetric roles: yet,
in decoding, the source side is known and only the
target side must be predicted.
3.1 A word factored translation model
To overcome these issues, the n-gram probability in
equation (2) can be factored by decomposing tuples
3http://www.kyloo.net/software/doku.php
331
 s?
8
: ? 
 t
?
8
: to 
 s?
9
: recevoir 
 t
?
9
: receive 
 s?
10
: le 
 t
?
10
: the 
 s?
11
: nobel de la paix 
 t
?
11
: nobel peace 
 s?
12
: prix 
 t
?
12
: prize 
 u
8
  u
9
  u
10
  u
11
  u
12
 
S :   .... 
T :   .... 
? recevoir le prix nobel de la paixorg :   ....
....
....
Figure 1: Extract of a French-English sentence pair segmented into bilingual units. The original (org) French sentence
appears at the top of the figure, just above the reordered source s and target t. The pair (s, t) decomposes into a
sequence of L bilingual units (tuples) u1, ..., uL. Each tuple ui contains a source and a target phrase: si and ti.
in two parts (source and target), and by taking words
as the basic units of the n-gram TM. This may seem
to be a regression with respect to current state-of-
the-art SMT systems, as the shift from the word-
based model of (Brown et al, 1993) to the phrase-
based models of (Zens et al, 2002) is usually con-
sidered as a major breakthrough of the recent years.
Indeed, one important motivation for considering
phrases was to capture local context in translation
and reordering. It should however be emphasized
that the decomposition of phrases into words is only
re-introduced here as a way to mitigate the param-
eter estimation problems. Translation units are still
pairs of phrases, derived from a bilingual segmen-
tation in tuples synchronizing the source and target
n-gram streams. In fact, the estimation policy de-
scribed in section 4 will actually allow us to take into
account larger contexts than is possible with conven-
tional n-gram models.
Let ski denote the k
th word of source tuple si.
Considering the example of Figure 1, s111 denotes
the source word nobel, s411 the source word paix.
We finally denote hn?1(tki ) the sequence made of
the n? 1 words preceding tki in the target sentence:
in Figure 1, h3(t211) thus refers to the three words
context receive the nobel associated with t211 peace.
Using these notations, equation (2) is rewritten as:
P (a, s, t) =
L?
i=1
[ |ti|?
k=1
P
(
tki |h
n?1(tki ), h
n?1(s1i+1)
)
?
|si|?
k=1
P
(
ski |h
n?1(t1i ), h
n?1(ski )
)] (3)
This decomposition relies on the n-gram assump-
tion, this time at the word level. Therefore, this
model estimates the joint probability of a sentence
pair using two sliding windows of length n, one for
each language; however, the moves of these win-
dows remain synchronized by the tuple segmenta-
tion. Moreover, the context is not limited to the cur-
rent phrase, and continues to include words from ad-
jacent phrases. Using the example of Figure 1, the
contribution of the target phrase t11 = nobel, peace
to P (s, t) using a 3- gram model is:
P
(
nobel|[receive, the], [la, paix]
)
?P
(
peace|[the, nobel], [la, paix]
)
.
A benefit of this new formulation is that the vo-
cabularies involved only contain words, and are thus
much smaller that tuple vocabularies. These models
are thus less at risk to be plagued by data sparsity is-
sues. Moreover, the decomposition (3) now involves
two models: the first term represents a TM, the sec-
ond term is best viewed as a reordering model. In
this formulation, the TM only predicts the target
phrase, given its source and target contexts.
P (s, t) =
L?
i=1
[ |si|?
k=1
P
(
ski |h
n?1(ski ), h
n?1(t1i+1)
)
?
|ti|?
k=1
P
(
tki |h
n?1(s1i ), h
n?1(tki )
)] (4)
4 The principles of SOUL
In section 3.1, we defined a n-gram translation
model based on equations (3) and (4). A major diffi-
culty with such models is to reliably estimate their
parameters, the numbers of which grow exponen-
tially with the order of the model. This problem
is aggravated in natural language processing due to
332
the well-known data sparsity issue. In this work,
we take advantage of the recent proposal of (Le et
al., 2011). Using a specific neural network architec-
ture (the Structured OUtput Layer or SOUL model),
it becomes possible to handle large vocabulary lan-
guage modeling tasks. This approach was experi-
mented last year for target language models only and
is now extended to translation models. More details
about the SOUL architecture can be found in (Le et
al., 2011), while its extension to translation models
is more precisely described in (Hai-Son et al, 2012).
The integration of SOUL models for large SMT
tasks is carried out using a two-pass approach: the
first pass uses conventional back-off n-gram trans-
lation and language models to produce a k-best list
(the k most likely translations); in the second pass,
the probability of a m-gram SOUL model is com-
puted for each hypothesis and the k-best list is ac-
cordingly reordered. In all the following experi-
ments, we used a context size for SOUL of m = 10,
and used k = 300. The two decompositions of equa-
tions (3) and (4) are used by introducing 4 scores
during the rescoring step.
5 Corpora and data pre-processing
Concerning data pre-processing, we started from our
submissions from last year (Allauzen et al, 2011)
and mainly upgraded the corpora and the associated
language-dependent pre-processing routines.
5.1 Pre-processing
We used in-house text processing tools for the to-
kenization and detokenization steps (De?chelotte et
al., 2008). Previous experiments have demonstrated
that better normalization tools provide better BLEU
scores: all systems are thus built in ?true-case?.
Compared to last year, the pre-processing of utf-8
characters was significantly improved.
As German is morphologically more complex
than English, the default policy which consists in
treating each word form independently is plagued
with data sparsity, which severely impacts both
training (alignment) and decoding (due to unknown
forms). When translating from German into En-
glish, the German side is thus normalized using a
specific pre-processing scheme (described in (Al-
lauzen et al, 2010; Durgar El-Kahlout and Yvon,
2010)), which aims at reducing the lexical redun-
dancy by (i) normalizing the orthography, (ii) neu-
tralizing most inflections and (iii) splitting complex
compounds. All parallel corpora were POS-tagged
with the TreeTagger (Schmid, 1994); in addition, for
German, fine-grained POS labels were also needed
for pre-processing and were obtained using the RF-
Tagger (Schmid and Laws, 2008).
5.2 Bilingual corpora
As for last year?s evaluation, we used all the avail-
able parallel data for the German-English language
pair, while only a subpart of the French-English par-
allel data was selected. Word alignment models
were trained using all the data, whereas the transla-
tion models were estimated on a subpart of the par-
allel data: the UN corpus was discarded for this step
and about half of the French-English Giga corpus
was filtered based on a perplexity criterion as in (Al-
lauzen et al, 2011)).
For French-English, we mainly upgraded the
training material from last year by extracting the
new parts from the common data. The word
alignment models trained last year were then up-
dated by running a forced alignment 4 of the new
data. These new word-aligned data was added to
last year?s parallel corpus and constitute the train-
ing material for the translation models and feature
functions described in Section 2. Given the large
amount of available data, three different bilingual
n-gram models are estimated, one for each source of
data: News-Commentary, Europarl, and the French-
English Giga corpus. These models are then added
to the weighted mixture defined by equation (1). For
German-English, we simply used all the available
parallel data to train one single translation models.
5.3 Monolingual corpora and language models
For the monolingual training data, we also used the
same setup as last year. For German, all the train-
ing data allowed in the constrained task were di-
vided into several sets based on dates or genres:
News-Commentary, the news crawled from the Web
grouped by year, and Europarl. For each subset,
a standard 4-gram LM was estimated using inter-
polated Kneser-Ney smoothing (Kneser and Ney,
4The forced alignment step consists in an additional EM it-
eration.
333
1995; Chen and Goodman, 1998). The resulting
LMs are then linearly combined using interpolation
coefficients chosen so as to minimize the perplexity
of the development set. The German vocabulary is
created using all the words contained in the parallel
data and expanded to reach a total of 500k words by
including the most frequent words observed in the
monolingual News data for 2011.
For French and English, the same monolingual
corpora as last year were used5. We did not observe
any perplexity decrease in our attempts to include
the new data specifically provided for this year?s
evaluation. We therefore used the same language
models as in (Allauzen et al, 2011).
6 ?On-the-fly? system
We also developped an alternative approach imple-
menting ?on-the-fly? estimation of the parameter of
a standard phase-based model, using Moses (Koehn
et al, 2007) as the decoder. Implementing on-the-
fly estimation for n-code, while possible in the-
ory, is less appealing due to the computational cost
of estimating a smoothed language model. Given
an input source file, it is possible to compute only
those statistics which are required to translate the
phrases it contains. As in previous works on on-
the-fly model estimation for SMT (Callison-Burch
et al, 2005; Lopez, 2008), we compute a suffix
array for the source corpus. This further enables
to consider only a subset of translation examples,
which we select by deterministic random sampling,
meaning that the sample is chosen randomly with
respect to the full corpus but that the same sample
is always returned for a given value of sample size,
hereafter denoted N . In our experiments, we used
N = 1, 000 and computed from the sample and the
word alignments (we used the same tokenization and
word alignments as in all other submitted systems)
the same translation6 and lexical reordering models
as the standard training scripts of the Moses system.
Experiments were run on the data sets used for
WMT English-French machine translation evalua-
tion tasks, using the same corpora and optimization
5The fifth edition of the English Gigaword (LDC2011T07)
was not used.
6An approximation is used for p(f |e), and coherent transla-
tion estimation is used; see (Lopez, 2008).
procedure as in our other experiments. The only no-
table difference is our use of the Moses decoder in-
stead of the n-gram-based system. As shown in Ta-
ble 1, our on-the-fly system achieves a result (31.7
BLEU point) that is slightly worst than the n-code
baseline (32.0) and slightly better than the equiva-
lent Moses baseline (31.5), but does it much faster.
Model estimation for the test file is reduced to 2
hours and 50 minutes, with an additional overhead
for loading and writing files of one and a half hours,
compared to roughly 210 hours for our baseline sys-
tems under comparable hardware conditions.
7 Experimental results
7.1 n-code with SOUL
Table 1 summarizes the experimental results sub-
mitted to the shared translation for French-English
and German-English in both directions. The perfor-
mances are measured in terms of BLEU on new-
stest2011, last year?s test set, and this year?s test
set newstest2012. For the former, BLEU scores are
computed with the NIST script mteva-v13.pl, while
we provide for newstest2012 the results computed
by the organizers 7. The Baseline results are ob-
tained with standard n-gram models estimated with
back-off, both for the bilingual and monolingual tar-
get models. With standard n-gram estimates, the or-
der is limited to n = 4. For instance, the n-code
French-English baseline achieves a 0.5 BLEU point
improvement over a Moses system trained with the
same data setup in both directions.
From Table 1, it can be observed that adding
the SOUL models (translation models and target
language model) consistently improves the base-
line, with an increase of 1 BLEU point. Con-
trastive experiments show that the SOUL target LM
does not bring significant gain when added to the
SOUL translation models. For instance, a gain of
0.3 BLEU point is observed when translating from
French to English with the addition of the SOUL tar-
get LM. In the other translation directions, the differ-
ences are negligible.
7All results come from the official website: http://
matrix.statmt.org/matrix/.
334
Direction System BLEU
test2011 test2012?
en2fr Baseline 32.0 28.9
+ SOUL TM 33.4 29.9
on-the-fly 31.7 28.6
fr2en Baseline 30.2 30.4
+ SOUL TM 31.1 31.5
en2de Baseline 15.4 16.0
+ SOUL TM 16.6 17.0
de2en Baseline 21.8 22.9
+ SOUL TM 22.8 23.9
Table 1: Experimental results in terms of BLEU scores
measured on the newstest2011 and newstest2012. For
newstest2012, the scores are provided by the organizers.
7.2 Experiments with additional features
For this year?s evaluation, we also investigated sev-
eral additional features based on IBM1 models and
word sense disambiguation (WSD) information in
rescoring. As for the SOUL models, these features
are added after the n-best list generation step.
In previous work (Och et al, 2004; Hasan, 2011),
the IBM1 features (Brown et al, 1993) are found
helpful. As the IBM1 model is asymmetric, two
models are estimated, one in both directions. Con-
trary to the reported results, these additional features
do not yield significant improvements over the base-
line system. We assume that the difficulty is to add
information to an already extensively optimized sys-
tem. Moreover, the IBM1 models are estimated on
the same training corpora as the translation system,
a fact that may explain the redundancy of these ad-
ditional features.
In a separate series of experiments, we also add
WSD features calculated according to a variation of
the method proposed in (Apidianaki, 2009). For
each word of a subset of the input (source lan-
guage) vocabulary, a simple WSD classifier pro-
duces a probability distribution over a set of trans-
lations8. During reranking, each translation hypoth-
esis is scanned and the word translations that match
one of the proposed variant are rewarded using an
additional score. While this method had given some
8The difference with the method described in (Apidianaki,
2009) is that no sense clustering is performed, and each transla-
tion is represented by a separate weighted source feature vector
which is used for disambiguation
small gains on a smaller dataset (IWSLT?11), we did
not observe here any improvement over the base-
line system. Additional analysis hints that (i) most
of the proposed variants are already covered by the
translation model with high probabilities and (ii) that
these variants are seldom found in the reference sen-
tences. This means that, in the situation in which
only one reference is provided, the hypotheses with
a high score for the WSD feature are not adequately
rewarded with the actual references.
8 Conclusion
In this paper, we described our submissions to
WMT?12 in the French-English and German-
English shared translation tasks, in both directions.
As for our last year?s participation, our main sys-
tems are built with n-code, the open source Statis-
tical Machine Translation system based on bilingual
n-grams. Our contributions are threefold. First, we
have experimented a new kind of translation mod-
els, where the bilingual n-gram distribution are es-
timated in a continuous space with neural networks.
As shown in past evaluations with target language
model, there is a significant reward for using this
kind of models in a rescoring step. We observed that,
in general, the continuous space translation model
yields a slightly larger improvement than the target
translation model. However, their combination does
not result in an additional gain.
We also reported preliminary results with a sys-
tem ?on-the-fly?, where the training data are sam-
pled according to the data to be translated in order
to train contextually adapted system. While this sys-
tem achieves comparable performance to our base-
line system, it is worth noticing that its total train-
ing time is much smaller than a comparable Moses
system. Finally, we investigated several additional
features based on IBM1 models and word sense dis-
ambiguation information in rescoring. While these
methods have sometimes been reported to help im-
prove the results, we did not observe any improve-
ment here over the baseline system.
Acknowledgment
This work was partially funded by the French State
agency for innovation (OSEO) in the Quaero Pro-
gramme.
335
References
Alexandre Allauzen, Josep M. Crego, I?lknur Durgar El-
Kahlout, and Franc?ois Yvon. 2010. LIMSI?s statis-
tical translation systems for WMT?10. In Proc. of the
Joint Workshop on Statistical Machine Translation and
MetricsMATR, pages 54?59, Uppsala, Sweden.
Alexandre Allauzen, Gilles Adda, He?le`ne Bonneau-
Maynard, Josep M. Crego, Hai-Son Le, Aure?lien Max,
Adrien Lardilleux, Thomas Lavergne, Artem Sokolov,
Guillaume Wisniewski, and Franc?ois Yvon. 2011.
LIMSI @ WMT11. In Proceedings of the Sixth Work-
shop on Statistical Machine Translation, pages 309?
315, Edinburgh, Scotland, July. Association for Com-
putational Linguistics.
Marianna Apidianaki. 2009. Data-driven semantic anal-
ysis for multilingual WSD and lexical selection in
translation. In Proceedings of the 12th Conference of
the European Chapter of the ACL (EACL 2009), pages
77?85, Athens, Greece, March. Association for Com-
putational Linguistics.
Peter F. Brown, Vincent J. Della Pietra, Stephen A. Della
Pietra, and Robert L. Mercer. 1993. The mathemat-
ics of statistical machine translation: parameter esti-
mation. Comput. Linguist., 19(2):263?311.
Chris Callison-Burch, Colin Bannard, and Josh
Schroeder. 2005. Scaling phrase-based statisti-
cal machine translation to larger corpora and longer
phrases. In Proceedings of the 43rd Annual Meeting
of the Association for Computational Linguistics
(ACL?05), pages 255?262, Ann Arbor, Michigan,
June. Association for Computational Linguistics.
Francesco Casacuberta and Enrique Vidal. 2004. Ma-
chine translation with inferred stochastic finite-state
transducers. Computational Linguistics, 30(3):205?
225.
Stanley F. Chen and Joshua T. Goodman. 1998. An
empirical study of smoothing techniques for language
modeling. Technical Report TR-10-98, Computer Sci-
ence Group, Harvard Un iversity.
Josep M. Crego and Jose? B. Marin?o. 2006. Improving
statistical MT by coupling reordering and decoding.
Machine Translation, 20(3):199?215.
Josep M. Crego, Franc?ois Yvon, and Jose? B. Marin?o.
2011. N-code: an open-source Bilingual N-gram SMT
Toolkit. Prague Bulletin of Mathematical Linguistics,
96:49?58.
Ilknur Durgar El-Kahlout and Franc?ois Yvon. 2010. The
pay-offs of preprocessing for German-English Statis-
tical Machine Translation. In Marcello Federico, Ian
Lane, Michael Paul, and Franc?ois Yvon, editors, Pro-
ceedings of the seventh International Workshop on
Spoken Language Translation (IWSLT), pages 251?
258.
Daniel De?chelotte, Gilles Adda, Alexandre Allauzen,
Olivier Galibert, Jean-Luc Gauvain, He?le`ne May-
nard, and Franc?ois Yvon. 2008. LIMSI?s statisti-
cal translation systems for WMT?08. In Proc. of the
NAACL-HTL Statistical Machine Translation Work-
shop, Columbus, Ohio.
Hai-Son, Alexandre Allauzen, and Franc?ois Yvon. 2012.
Continuous space translation models with neural net-
works. In NAACL ?12: Proceedings of the 2012 Con-
ference of the North American Chapter of the Associ-
ation for Computational Linguistics on Human Lan-
guage Technology.
Sas?a Hasan. 2011. Triplet Lexicon Models for Statisti-
cal Machine Translation. Ph.D. thesis, RWTH Aachen
University.
Reinhard Kneser and Herman Ney. 1995. Improved
backing-off for m-gram language modeling. In Pro-
ceedings of the International Conference on Acoustics,
Speech, and Signal Processing, ICASSP?95, pages
181?184, Detroit, MI.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In Proceed-
ings of the 45th Annual Meeting of the Association for
Computational Linguistics Companion Volume Pro-
ceedings of the Demo and Poster Sessions, pages 177?
180, Prague, Czech Republic, June. Association for
Computational Linguistics.
Hai-Son Le, Ilya Oparin, Alexandre Allauzen, Jean-Luc
Gauvain, and Franc?ois Yvon. 2011. Structured output
layer neural network language model. In Proceedings
of ICASSP?11, pages 5524?5527.
Adam Lopez. 2008. Tera-scale translation models via
pattern matching. In Proceedings of the 22nd Interna-
tional Conference on Computational Linguistics (Col-
ing 2008), pages 505?512, Manchester, UK, August.
Coling 2008 Organizing Committee.
Jose? B. Marin?o, Rafael E. Banchs, Josep M. Crego, Adria`
de Gispert, Patrick Lambert, Jose? A.R. Fonollosa, and
Marta R. Costa-Jussa`. 2006. N-gram-based machine
translation. Computational Linguistics, 32(4):527?
549.
Franz Josef Och, Daniel Gildea, Sanjeev Khudanpur,
Anoop Sarkar, Kenji Yamada, Alex Fraser, Shankar
Kumar, Libin Shen, David Smith, Katherine Eng,
Viren Jain, Zhen Jin, and Dragomir Radev. 2004. A
smorgasbord of features for statistical machine trans-
lation. In Daniel Marcu Susan Dumais and Salim
Roukos, editors, HLT-NAACL 2004: Main Proceed-
ings, pages 161?168, Boston, Massachusetts, USA,
336
May 2 - May 7. Association for Computational Lin-
guistics.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In ACL ?03: Proc. of
the 41st Annual Meeting on Association for Computa-
tional Linguistics, pages 160?167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic eval-
uation of machine translation. In ACL ?02: Proc. of
the 40th Annual Meeting on Association for Compu-
tational Linguistics, pages 311?318. Association for
Computational Linguistics.
Helmut Schmid and Florian Laws. 2008. Estimation
of conditional probabilities with decision trees and an
application to fine-grained POS tagging. In Proceed-
ings of the 22nd International Conference on Com-
putational Linguistics (Coling 2008), pages 777?784,
Manchester, UK, August.
Helmut Schmid. 1994. Probabilistic part-of-speech tag-
ging using decision trees. In Proc. of International
Conference on New Methods in Language Processing,
pages 44?49, Manchester, UK.
Christoph Tillmann. 2004. A unigram orientation model
for statistical machine translation. In Proceedings of
HLT-NAACL 2004, pages 101?104. Association for
Computational Linguistics.
Richard Zens, Franz Josef Och, and Hermann Ney. 2002.
Phrase-based statistical machine translation. In KI
?02: Proceedings of the 25th Annual German Con-
ference on AI, pages 18?32, London, UK. Springer-
Verlag.
337
Proceedings of SSST-6, Sixth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 1?9,
Jeju, Republic of Korea, 12 July 2012. c?2012 Association for Computational Linguistics
WSD for n-best reranking and local language modeling in SMT
Marianna Apidianaki, Guillaume Wisniewski?, Artem Sokolov, Aure?lien Max?, Franc?ois Yvon?
LIMSI-CNRS
? Univ. Paris Sud
BP 133, F-91403, Orsay Cedex, France
firstname.lastname@limsi.fr
Abstract
We integrate semantic information at two
stages of the translation process of a state-of-
the-art SMT system. A Word Sense Disam-
biguation (WSD) classifier produces a proba-
bility distribution over the translation candi-
dates of source words which is exploited in
two ways. First, the probabilities serve to
rerank a list of n-best translations produced by
the system. Second, the WSD predictions are
used to build a supplementary language model
for each sentence, aimed to favor translations
that seem more adequate in this specific sen-
tential context. Both approaches lead to sig-
nificant improvements in translation perfor-
mance, highlighting the usefulness of source
side disambiguation for SMT.
1 Introduction
Word Sense Disambiguation (WSD) is the task of
identifying the sense of words in texts by reference
to some pre-existing sense inventory. The selec-
tion of the appropriate inventory and WSD method
strongly depends on the goal WSD intends to serve:
recent methods are increasingly oriented towards
the disambiguation needs of specific end applica-
tions, and explicitly aim at improving the overall
performance of complex Natural Language Process-
ing systems (Ide and Wilks, 2007; Carpuat and Wu,
2007). This task-oriented conception of WSD is
manifested in the area of multilingual semantic pro-
cessing: supervised methods, which were previously
shown to give the best results, are being abandoned
in favor of unsupervised ones that do not rely on pre-
annotated training data. Accordingly, pre-defined
semantic inventories, that usually served to provide
the lists of candidate word senses, are being replaced
by senses relevant to the considered applications and
directly identified from corpora by means of word
sense induction methods.
In a multilingual setting, the sense inventories
needed for disambiguation are generally built from
all possible translations of words or phrases in a par-
allel corpus (Carpuat and Wu, 2007; Chan et al,
2007), or by using more complex representations
of the semantics of translations (Apidianaki, 2009;
Mihalcea et al, 2010; Lefever and Hoste, 2010).
However, integrating this semantic knowledge into
Statistical Machine Translation (SMT) raises sev-
eral challenges: the way in which the predictions of
the WSD classifier have to be taken into account;
the type of context exploited for disambiguation;
the target words to be disambiguated (?all-words?
WSD vs. WSD restricted to target words satisfy-
ing specific criteria); the use of a single classifier
versus building separate classifiers for each source
word; the quantity and type of data used for training
the classifier (e.g., use of raw data or of more ab-
stract representations, such as lemmatization, allow-
ing to deal with sparseness issues), and many oth-
ers. Seemingly, the optimal way to take advantage
of WSD predictions remains an open issue.
In this work, we carry out a set of experiments
to investigate the impact of integrating the predic-
tions of a cross-lingual WSD classifier into an SMT
system, at two different stages of the translation pro-
cess. The first approach exploits the probability dis-
tribution built by the WSD classifier over the set of
translations of words found in the parallel corpus,
1
for reranking the translations in the n-best list gen-
erated by the SMT system. Words in the list that
match one of the proposed translations are boosted
and are thus more likely to appear in the final trans-
lation. Our results on the English-French IWSLT?11
task show substantial improvements in translation
quality. The second approach provides a tighter in-
tegration of the WSD classifier with the rest of the
system: using the WSD predictions, an additional
sentence specific language model is estimated and
used during decoding. These additional local mod-
els can be used as an external knowledge source to
reinforce translation hypotheses matching the pre-
diction of the WSD system.
In the rest of the paper, we present related work
on integrating semantic information into SMT (Sec-
tion 2). The WSD classifier used in the current study
is described in Section 3. We then present the two
approaches adopted for integrating the WSD out-
put into SMT (Section 4). Evaluation results are
presented in Section 5, before concluding and dis-
cussing some avenues for future work.
2 Related work
Word sense disambiguation systems generally work
at the word level: given an input word and its con-
text, they predict its (most likely) meaning. At
the same time, state-of-the-art translation systems
all consider groups of words (phrases, tuples, etc.)
rather than single words in the translation process.
This discrepancy between the units used in MT and
those used in WSD is one of the major difficul-
ties in integrating word predictions into the decoder.
This was, for instance, one of the reasons for the
somewhat disappointing results obtained by Carpuat
and Wu (2005) when the output of a WSD system
was directly incorporated into a Chinese-English
SMT system. Because of this difficulty, other cross-
lingual semantics works have considered only sim-
plified tasks, like blank-filling, without addressing
the integration of the WSD models in full-scale MT
systems (Vickrey et al, 2005; Specia, 2006).
Since the pioneering work of Carpuat and Wu
(2005), several more successful ways to take WSD
predictions into account have been proposed. For
instance, Carpuat and Wu (2007) proposed to gen-
eralize the WSD system so that it performs a fully
phrasal multiword disambiguation. However, given
that the number of phrases is far larger than the num-
ber of words, this approach suffers from sparsity
and computational problems, as it requires training
a classifier for each entry of the phrase table.
Chan et al (2007) introduced a way to modify the
rule weights of a hierarchical translation system to
reflect the predictions of their WSD system. While
their approach and ours are built on the same intu-
ition (an adaptation of a model to incorporate word
predictions) their work is specific to hierarchical
systems, while ours can be applied to any decoder
that uses a language model. Haque et al (2009) et
Haque et al (2010) introduce lexico-syntactic de-
scriptions in the form of supertags as source lan-
guage context-informed features in a phrase-based
SMT and a state-of-the-art hierarchical model, re-
spectively, and report significant gains in translation
quality.
Closer to our work, Mauser et al (2009) and Pa-
try and Langlais (2011) train a global lexicon model
that predicts the bag of output words from the bag
of input words. As no explicit alignment between
input and output words is used, words are chosen
based on the (global) input context. For each input
sentence, the decoder considers these word predic-
tions as an additional feature that it uses to define a
new model score which favors translation hypothe-
ses containing words predicted by the global lexicon
model. A difference between this approach and our
work is that instead of using a global lexicon model,
we disambiguate a subset of the words in the input
sentence by employing a WSD classifier that cre-
ates a probability distribution over the translations
of each word in its context.
The unsupervised cross-lingual WSD classifier
used in this work is similar to the one proposed in
Apidianaki (2009). The original classifier disam-
biguates new instances of words in context by se-
lecting the most appropriate cluster of translations
among a set of candidate clusters found in an auto-
matically built bilingual sense inventory. The sense
inventory exploited by the classifier is created by
a cross-lingual word sense induction (WSI) method
that reveals the senses of source words by grouping
their translations into clusters according to their se-
mantic proximity, revealed by a distributional sim-
ilarity calculation. The resulting clusters represent
2
the source words? candidate senses. This WSD
method gave good results in a word prediction task
but, similarly to the work of Vickrey et al (2005)
and of Specia (2006), the predictions are not inte-
grated into a complete MT system.
3 The WSD classifier
Our WSD classifier is a variation of the one intro-
duced in Apidianaki (2009). The main difference
is that here the classifier serves to discriminate be-
tween unclustered translations of a word and to as-
sign a probability to each translation for new in-
stances of the word in context. Each translation is
represented by a source language feature vector that
the classifier uses for disambiguation. All experi-
ments carried out in this study are for the English
(EN) - French (FR) language pair.
3.1 Source Language Feature Vectors
Preprocessing The information needed by the clas-
sifier is gathered from the EN-FR training data pro-
vided for the IWSLT?11 evaluation task.1 The
dataset consists of 107,268 parallel sentences, word-
aligned in both translation directions using GIZA++
(Och and Ney, 2003). We disambiguate EN words
found in the parallel corpus that satisfy the set of
criteria described below.
Two bilingual lexicons are built from the align-
ment results and filtered to eliminate spurious align-
ments. First, translation correspondences with a
probability lower than a threshold are discarded;2
then translations are filtered by part-of-speech
(PoS), keeping for each word only translations per-
taining to the same grammatical category;3 finally,
only intersecting alignments (i.e., correspondences
found in the lexicons of both directions) are retained.
Given that the lexicons contain word forms, the in-
tersection is calculated based on lemmatization in-
formation in order to perform a generalization over
the contents of the lexicons. For instance, if the EN
adjective regular is translated by habituelle (femi-
1http://www.iwslt2011.org/
2The translation probabilities between word tokens are
found in the translation table produced by GIZA++; the thresh-
old is set to 0.01.
3For this filtering, we employ a PoS and lemmatization lex-
icon built after tagging both parts of the training corpus with
TreeTagger (Schmid, 1994).
nine singular form of the adjective habituel) in the
EN-FR lexicon, but is found to translate habituel
(masculine singular form) in the other direction,
the EN-FR correspondence regular/habituelle is re-
tained (because the two variants of the adjective are
reduced to the same lemma).
All lexicon entries satisfying the above criteria are
retained and used for disambiguation. In these initial
experiments, we disambiguate English words having
less than 20 French translations in the lexicon. Each
French translation of an English word that appears
more than once in the training corpus4 is character-
ized by a weighted English feature vector built from
the training data.
Vector building The feature vectors corresponding
to the translations are built by exploiting information
from the source contexts (Apidianaki, 2008; Grefen-
stette, 1994). For each translation of an EN word w,
we extract the content words that co-occur with w
in the corresponding source sentences of the parallel
corpus (i.e. the content words that occur in the same
sentence as w whenever it is translated by this trans-
lation). The extracted source language words con-
stitute the features of the vector built for the transla-
tion.
For each translation Ti of w, let N be the number
of features retained from the corresponding source
context. Each feature Fj (1 ? j ? N) receives a to-
tal weight tw(Fj,Ti) defined as the product of the
feature?s global weight, gw(Fj), and its local weight
with that translation, lw(Fj,Ti):
tw(Fj,Ti) = gw(Fj) ? lw(Fj,Ti) (1)
The global weight of a feature Fj is a function of
the number Ni of translations (Ti?s) to which Fj is re-
lated, and of the probabilities (pi j) that Fj co-occurs
with instances of w translated by each of the Ti?s:
gw(Fj) = 1?
?Ti pi j log(pi j)
Ni
(2)
Each of the pi j?s is computed as the ratio between
the co-occurrence frequency of Fj with w when
translated as Ti, denoted as cooc frequency(Fj,Ti),
4We do not consider hapax translations because they often
correspond to alignment errors.
3
and the total number of features (N) seen with Ti:
pi j =
cooc frequency(Fj,Ti)
N
(3)
Finally, the local weight lw(Fj,Ti) between Fj and Ti
directly depends on their co-occurrence frequency:
lw(Fj,Ti) = log(cooc frequency(Fj,Ti)) (4)
3.2 Cross-Lingual WSD
The weighted feature vectors corresponding to the
different translations of an English word are used
for disambiguation.5 As noted in Section 3.1, we
disambiguate source words satisfying a set of crite-
ria. Disambiguation is performed by comparing the
vector associated with each translation to the new
context of the words in the input sentences from the
IWSLT?11 test set.
More precisely, the information contained in each
vector is exploited by the WSD classifier to produce
a probability distribution over the translations, for
each new instance of a word in context. We dis-
ambiguate word forms (not lemmas) in order to di-
rectly use the selected translations in the translated
texts. However, we should note that in some cases
this reduces the role of WSD to distinguishing be-
tween different forms of one word and no different
senses are involved. Using more abstract represen-
tations (corresponding to senses) is one of the per-
spectives of this work.
The classifier assigns a score to each transla-
tion by comparing information in the corresponding
source vector to information found in the new con-
text. Given that the vector features are lemmatized,
the new context is lemmatized as well and the lem-
mas of the content words are gathered in a bag of
words. The adequacy of each translation for a new
instance of a word is estimated by comparing the
translation?s vector with the bag of words built from
the new context. If common features are found be-
tween the new context and a translation vector, an
association score is calculated corresponding to the
mean of the weights of the common features rela-
tively to the translation (i.e. found in its vector). In
5The vectors are not used for clustering the translations as
in Apidianaki (2009) but all translations are considered as can-
didate senses.
Equation (5), (CFj)|CF |j=1 is the set of common fea-
tures between the translation vector Vi and the new
context C and tw is the weight of a CF with transla-
tion Ti (cf. formula (1)).
assoc score(Vi,C) =
?|CF |j=1 tw(CFj,Ti)
|CF| (5)
The scores assigned to the different translations of a
source word are normalized to sum up to one.
In this way, a subset of the words that occur in the
input sentences from the test set are annotated with
their translations and the associated scores (contex-
tual probabilities), as shown in the example in Fig-
ure 1.6 The WSD classifier makes predictions only
for the subset of the words found in the source part
of the parallel test set that were retained from the ini-
tial EN-FR lexicon after filtering. Table 1 presents
the total coverage of the WSD method as well as its
coverage for words of different PoS, with a focus
on content words. We report the number of disam-
biguated words for each content PoS (cf. third col-
umn) and the corresponding percentage, calculated
on the basis of the total number of words pertaining
to this PoS (cf. second column). We observe that
the coverage of the method on nouns and adjectives
is higher than the one on verbs. Given the rich ver-
bal morphology of French, several verbs have a very
high number of translations in the bilingual lexicon
(over 20) and are not handled during disambigua-
tion. The same applies to function words (articles,
prepositions, conjunctions, etc.) included in the ?all
PoS? category.
4 Integrating Semantics into SMT
In this section, we present two ways to integrate
WSD predictions into an SMT decoder. The first
one (Section 4.1) is a simple method based on n-
best reranking. This method, already proposed in
the literature (Specia et al, 2008), allows us to eas-
ily evaluate the impact of WSD predictions on au-
tomatic translation quality. The second one (Sec-
tion 4.2) builds on the idea, introduced in (Crego et
al., 2010), of using an additional language model to
6Some source words are tagged with only one translation
(e.g. stones {pierres(1.000)}) because their other translations
in the lexicon occurred only once in the training corpus and,
consequently, were not considered.
4
PoS # of words # of WSD predictions %
Nouns 5535 3472 62.72
Verbs 5336 1269 23.78
Adjs 1787 1249 69.89
Advs 2224 1098 49.37
all content PoS 14882 7088 47.62
all PoS 27596 8463 30.66
Table 1: Coverage of the WSD method
you know, one of the intense {intenses(0.305), forte(0.306), intense(0.389)} pleasures of
travel {transport(0.334), voyage(0.332), voyager(0.334)} and one of the delights of ethnographic
research {recherche(0.225), research(0.167), e?tudes(0.218), recherches(0.222), e?tude(0.167)} is
the opportunity {possibilite?(0.187), chance(0.185), opportunite?s(0.199), occasion(0.222), opportu-
nite?(0.207)} to live amongst those who have not forgotten {oubli(0.401), oublie?s(0.279), ou-
blie?e(0.321)} the old {ancien(0.079), a?ge(0.089), anciennes(0.072), a?ge?es(0.100), a?ge?s(0.063), an-
cienne(0.072), vieille(0.093), ans(0.088), vieux(0.086), vieil(0.078), anciens(0.081), vieilles(0.099)}
ways {fac?ons(0.162), manie`res(0.140), moyens(0.161), aspects(0.113), fac?on(0.139), moyen(0.124),
manie`re(0.161)} , who still feel their past {passe?e(0.269), autrefois(0.350), passe?(0.381)} in the
wind {e?olienne(0.305), vent(0.392), e?oliennes(0.304)} , touch {touchent(0.236), touchez(0.235),
touche(0.235), toucher(0.293)} it in stones {pierres(1.000)} polished by rain {pluie(1.000)} ,
taste {gou?t(0.500), gou?ter(0.500)} it in the bitter {amer(0.360), ame`re(0.280), amertume(0.360)}
leaves {feuilles(0.500), feuillages(0.500)} of plants {usines(0.239), centrales(0.207), plantes(0.347),
ve?ge?taux(0.207)}.
Figure 1: Input sentence with WSD information
directly integrate the prediction of the WSD system
into the decoder.
4.1 N-best List Reranking
A simple way to influence translation hypotheses se-
lection with WSD information is to use the WSD
probabilities of translation variants to produce an ad-
ditional feature appended to the n-best list after its
generation. The feature value should reflect the de-
gree to which a particular hypothesis includes pro-
posed WSD variants for the respective words. Re-
running the standard MERT optimization procedure
on the augmented features gives a new set of model
weights, that are used to rescore the n-best list.
We propose the following method of features con-
struction. Given the phrase alignment information
between a source sentence and a hypothesis, we ver-
ify if one or more of the proposed WSD variants for
the source word occur in the corresponding phrase of
the translation hypothesis. If this is the case, the cor-
responding probabilities are additively accumulated
for the current hypothesis. At the end, two features
are appended to each hypothesis in the n-best list:
the total score accumulated for the hypothesis and
the same score normalized by the number of words
in the hypothesis.
Two MERT initialization schemes were consid-
ered: (1) all model weights are initialized to zero,
and (2) all the weights of ?standard? features are ini-
tialized to the values found by MERT and the new
WSD features to zero.
4.2 Local Language Models
We propose to adapt the approach introduced in
Crego et al (2010) as an alternative way to inte-
grate the WSD predictions within the decoder: for
each sentence to be translated, an additional lan-
guage model (LM) is estimated and taken into ac-
count during decoding. As this additional ?local?
model depends on the source sentence, it can be
used as an external source of knowledge to reinforce
translation hypotheses complying with criteria pre-
dicted from the whole source sentence. For instance,
the unigram probabilities of the additional LM can
be derived from the (word) predictions of a WSD
system, bigram probabilities from the prediction of
phrases and so on and so forth. Although this ap-
proach was suggested in (Crego et al, 2010), this
5
is, to the best of our knowledge, the first time it is
experimentally validated.
In practice, the predictions of the WSD system
described in Section 3 can be integrated by defining,
for each sentence, an additional unigram language
model as follows:
? each translation predicted by the WSD classi-
fier can be generated by the language model
with the probability estimated by the WSD
classifier; no information about the source
word that has been disambiguated is consid-
ered;
? the probability of unknown words is set to a
small arbitrary constant.
Even if most of the words composing the transla-
tion hypothesis are considered as unknown words,
hypotheses that contain the words predicted by the
WSD system still have a higher LM score and are
therefore preferred. Note that even if we only use
unigram language models in our experiments, as
senses are predicted at the word level, our approach
is able to handle disambiguation of phrases as well.
This approach has two main advantages over ex-
isting ways to integrate WSD predictions in an SMT
system. First, no hard decisions are made: errors
of the WSD can be ?corrected? by the translation.
Second, sense disambiguation at the word level is
naturally and automatically propagated at the phrase
level: the additional LM is influencing all phrase
pairs using one of the predicted words.
Compared to the reranking approach introduced
in the previous section, this method results in a
tighter integration with the decoder. In particu-
lar, the WSD predictions are applied before search-
space pruning and are therefore expected to have a
more important role.
5 Evaluation
5.1 Experimental Setting
In all our experiments, we considered the TED-
talk English to French data set provided by the
IWSLT?11 evaluation campaign, a collection of pub-
lic speeches on a variety of topics. We used the
Moses decoder (Koehn et al, 2007).
The TED-talk corpus is a small data set made
of a monolingual corpus (111,431 sentences) used
to estimate a 4-gram language model with KN-
smoothing, and a bilingual corpus (107,268 sen-
tences) used to extract the phrase table. All data
are tokenized, cleaned and converted to lowercase
letters using the tools provided by the WMT orga-
nizers.7 We then use a standard training pipeline to
construct the translation model: the bitext is aligned
using GIZA++, symmetrized using the grow-diag-
final-and heuristic; the phrase table is extracted and
scored using the tools distributed with Moses. Fi-
nally, systems are optimized using MERT on the
934 sentences of the dev-2010 set. All evalua-
tions are performed on the 1,664 sentences of the
test-2010 set.
5.2 Baseline
In addition to the models introduced in Section 4,
we considered two other supplementary models as
baselines. The first one uses the IBM 1 model esti-
mated during the SMT system training as a simple
WSD system: for each source sentence, a unigram
additional language model is defined by taking, for
each source, the 20 best translations according to the
IBM 1 model and their probability. Model 1 has
been shown to be one of the best performing fea-
tures to be added to an SMT system in a reranking
step (Och et al, 2004) and can be seen as a naive
WSD classifier.
To test the validity of our approach, we repli-
cate the ?oracle? experiments of Crego et al (2010)
and estimate the best gain our method can achieve.
These experiments consist in using the reference to
train a local n-gram language model (with n in the
range 1 to 3) which amounts, in the local language
model method of Section 4.2, to assuming that the
WSD system correctly predicted a single translation
for each source word.
5.3 Results
Table 2 reports the results of our experiments. It
appears that, for the considered task, sense disam-
biguation improves translation performance: n-best
rescoring results in a 0.37 BLEU improvement and
using an additional language model brings about an
improvement of up to a 0.88 BLEU. In both cases,
MERT assigns a large weight to the additional fea-
7http://statmt.org/wmt08/scripts.tgz
6
method BLEU METEOR
baseline ? 29.63 53.78
rescoring WSD (zero init) 30.00 54.26WSD (reinit) 29.58 53.96
additional LM
oracle 3-gram 43.56 64.64
oracle 2-gram 39.36 62.92
oracle 1-gram 42.92 69.39
IBM 1 30.18 54.36
WSD 30.51 54.38
Table 2: Evaluation results on the TED-talk task of our two methods to integrate WSD predictions.
PoS baseline WSD
Nouns 67.57 69.06
Verbs 45.97 47.76
Adjectives 51.79 53.94
Adverbs 52.17 56.25
Table 3: Contrastive lexical evaluation: % of words correctly translated within each PoS class
tures during tuning. When rescoring n-best, an im-
provement is observed only when the weights are
initialized to zero and not to the weights resulting
from the previous optimization, maybe because of
the difficulty to exit the local minimum MERT had
found earlier.
As expected, integrating the WSD predictions
with an additional language model results in a larger
improvement than simple rescoring, which shows
the importance of applying this new source of in-
formation early in the translation pipeline, before
search space pruning. Also note that the system us-
ing the IBM 1 predictions is outperformed by the
system using the WSD classifier introduced in Sec-
tion 3, showing the quality of its predictions.
Oracle experiments stress the high potential of
the method introduced in (Crego et al, 2010) as a
way to integrate external sources of knowledge: all
three conditions result in large improvements over
the baseline and the proposed methods. It must,
however, be noted that contrary to the WSD method
introduced in Section 3, these oracle experiments
rely on sense predictions for all source words and
not only content words. Surprisingly enough, pre-
dicting phrases instead of words results only in a
small improvement. Additional experiments are re-
quired to explain why 2-gram oracle achieved such
a low performance.
5.4 Contrastive lexical evaluation
All the measures used for evaluating the impact
of WSD information on translation show improve-
ments, as discussed in the previous section. We
complement these results with another measure of
translation performance, proposed by Max et al
(2010), which allows for a more fine-grained con-
trastive evaluation of the translations produced by
different systems. The method permits to compare
the results produced by the systems on different
word classes and to take into account the source
words that were actually translated. We focus this
evaluation on the classes of content words (nouns,
adjectives, verbs and adverbs) on which WSD had
an important coverage. Our aim is, first, to ex-
plore how these words are handled by a WSD-
informed SMT system (the system using the lo-
cal language models) compared to the baseline sys-
tem that does not exploit any semantic informa-
tion; and, second, to investigate whether their dis-
ambiguation influences the translation of surround-
ing non-disambiguated words.
Table 3 reports the percentage of words cor-
rectly translated by the semantically-informed sys-
tem within each content word class: consistent gains
in translation quality are observed for all parts-of-
speech compared to the baseline, and the best results
are obtained for nouns.
7
baseline WSD
w?2 w?1 w+1 w+2 w?2 w?1 w+1 w+2
Nouns 64.01 68.69 75.17 64.6 65.47 70.46 76.3 66.6
Verbs 68.67 67.58 63 62.19 69.98 68.89 64.85 64.25
Adjectives 63.1 64.39 64.28 66.55 64.09 65.65 64.76 69.33
Adverbs 70.8 69.44 68.67 66.38 71 71.21 70 67.22
Table 4: Impact of WSD prediction on the surrounding words
Table 4 shows how the words surrounding a dis-
ambiguated word w (noun, verb, adjective or adverb)
in the text are handled by the two systems. More
precisely, we look at the translation of words in the
immediate context of w, i.e. at positions w?2, w?1,
w+1 and w+2. The left column reports the percent-
age of correct translations produced by the baseline
system (without disambiguation) for words in these
positions; the right column shows the positive im-
pact that the disambiguation of a word has on the
translation of its neighbors. Note that this time we
look at disambiguated words and their context with-
out evaluating the correctness of the WSD predic-
tions. Nevertheless, even in this case, consistent
gains are observed when WSD information is ex-
ploited. For instance, when a noun is disambiguated,
70.46% and 76.3% of the immediately preceding
(w?1) and following (w+1) words, respectively, are
correctly translated, versus 68.69% and 75.17% of
correct translations produced by the baseline system.
6 Conclusion and future work
The preliminary results presented in this paper on
integrating cross-lingual WSD into a state-of-the-
art SMT system are encouraging. Both adopted ap-
proaches (n-best rescoring and local language mod-
eling) benefit from the predictions of the proposed
cross-lingual WSD classifier. The contrastive eval-
uation results further show that WSD improves not
only the translation of disambiguated words, but also
the translation of neighboring words in the input
texts.
We consider various ways for extending this
work. First, future experiments will involve the use
of more abstract representations of senses than indi-
vidual translations, by applying a cross-lingual word
sense induction method to the training corpus prior
to disambiguation. We will also experiment with
disambiguation at the level of lemmas, to reduce
sparseness issues, and with different ways for han-
dling lemmatized predictions by the SMT systems.
Furthermore, we intend to extend the coverage of the
WSD method by exploring other filtering methods
for cleaning the alignment lexicons, and by address-
ing the disambiguation of words of all PoS.
Acknowledgments
This work was partly funded by the European Union
under the FP7 project META-NET (T4ME), Con-
tract No. 249119, and by OSEO, the French agency
for innovation, as part of the Quaero Program.
References
Marianna Apidianaki. 2008. Translation-oriented Word
Sense Induction Based on Parallel Corpora. In Pro-
ceedings of the Sixth International Conference on Lan-
guage Resources and Evaluation (LREC?08), Mar-
rakech, Morocco.
Marianna Apidianaki. 2009. Data-driven Semantic
Analysis for Multilingual WSD and Lexical Selection
in Translation. In Proceedings of the 12th Confer-
ence of the European Chapter of the Association for
Computational Linguistics (EACL-09), pages 77?85,
Athens, Greece.
Marine Carpuat and Dekai Wu. 2005. Word Sense Dis-
ambiguation vs. Statistical Machine Translation. In
Proceedings of the 43rd Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL?05), pages
387?394, Ann Arbor, Michigan.
Marine Carpuat and Dekai Wu. 2007. Improving Sta-
tistical Machine Translation using Word Sense Disam-
biguation. In Proceedings of the Joint EMNLP-CoNLL
Conference, pages 61?72, Prague, Czech Republic.
Yee Seng Chan, Hwee Tou Ng, and David Chiang. 2007.
Word Sense Disambiguation Improves Statistical Ma-
chine Translation. In Proceedings of the 45th Annual
Meeting of the Association for Computational Linguis-
tics (ACL-07), pages 33?40, Prague, Czech Republic.
8
Josep Maria Crego, Aure?lien Max, and Franc?ois Yvon.
2010. Local lexical adaptation in Machine Transla-
tion through triangulation: SMT helping SMT. In
Proceedings of the 23rd International Conference on
Computational Linguistics (Coling 2010), pages 232?
240, Beijing, China.
Gregory Grefenstette. 1994. Explorations in Automatic
Thesaurus Discovery. Kluwer Academic Publishers,
Norwell, MA.
Rejwanual Haque, Sudip Naskar, Yanjun Ma, and Andy
Way. 2009. Using supertags as source language con-
text in SMT. In Proceedings of the 13th Annual Meet-
ing of the European Association for Machine Transla-
tion (EAMT 2009), pages 234?241, Barcelona, Spain.
Rejwanul Haque, Sudip Kumar Naskar, Antal Van Den
Bosch, and Andy Way. 2010. Supertags as source lan-
guage context in hierarchical phrase-based SMT. In
Proceedings of AMTA 2010: The Ninth Conference of
the Association for Machine Translation in the Ameri-
cas, pages 210?219, Denver, CO.
N. Ide and Y. Wilks. 2007. Making Sense About Sense.
In E. Agirre and P. Edmonds, editors, Word Sense Dis-
ambiguation, Algorithms and Applications, pages 47?
73. Springer.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In Proceed-
ings of the 45th Annual ACL Meeting, Companion Vol-
ume Proceedings of the Demo and Poster Sessions,
pages 177?180, Prague, Czech Republic.
Els Lefever and Veronique Hoste. 2010. SemEval-2010
Task 3: Cross-lingual Word Sense Disambiguation.
In Proceedings of the 5th International Workshop on
Semantic Evaluations (SemEval-2), ACL 2010, pages
15?20, Uppsala, Sweden.
Arne Mauser, Sas?a Hasan, and Hermann Ney. 2009. Ex-
tending statistical machine translation with discrimi-
native and trigger-based lexicon models. In Proceed-
ings of the Conference on Empirical Methods in Nat-
ural Language Processing (EMNLP), pages 210?217,
Singapore, August.
Aure?lien Max, Josep Maria Crego, and Franc?ois Yvon.
2010. Contrastive Lexical Evaluation of Machine
Translation. In Proceedings of the Seventh Interna-
tional Conference on Language Resources and Evalu-
ation (LREC?10), Valletta, Malta.
Rada Mihalcea, Ravi Sinha, and Diana McCarthy. 2010.
SemEval-2010 Task 2: Cross-Lingual Lexical Sub-
stitution. In Proceedings of the 5th International
Workshop on Semantic Evaluations (SemEval-2), ACL
2010, pages 9?14, Uppsala, Sweden.
Franz Josef Och and Hermann Ney. 2003. A Systematic
Comparison of Various Statistical Alignment Models.
Computational Linguistics, 29(1):19?51.
Franz Josef Och, Daniel Gildea, Sanjeev Khudanpur,
Anoop Sarkar, Kenji Yamada, Alex Fraser, Shankar
Kumar, Libin Shen, David Smith, Katherine Eng,
Viren Jain, Zhen Jin, and Dragomir Radev. 2004. A
smorgasbord of features for statistical machine transla-
tion. In Proceedings of HLT-NAACL 2004, pages 161?
168, Boston, Massachusetts, USA.
Alexandre Patry and Philippe Langlais. 2011. Going be-
yond word cooccurrences in global lexical selection
for statistical machine translation using a multilayer
perceptron. In Proceedings of 5th International Joint
Conference on Natural Language Processing, pages
658?666, Chiang Mai, Thailand, November.
Helmut Schmid. 1994. Probabilistic Part-of-Speech Tag-
ging Using Decision Trees. In Proceedings of the In-
ternational Conference on New Methods in Language
Processing, pages 44?49, Manchester, UK.
Lucia Specia, Baskaran Sankaran, and Maria Das
Grac?as Volpe Nunes. 2008. n-Best Reranking for the
Efficient Integration of Word Sense Disambiguation
and Statistical Machine Translation. In Proceedings of
the 9th international conference on Computational lin-
guistics and intelligent text processing, CICLing?08,
pages 399?410, Berlin, Heidelberg. Springer-Verlag.
Lucia Specia. 2006. A Hybrid Relational Approach for
WSD - First Results. In Proceedings of the COL-
ING/ACL 2006 Student Research Workshop, pages 55?
60, Sydney, Australia.
David Vickrey, Luke Biewald, Marc Teyssier, and
Daphne Koller. 2005. Word-Sense Disambiguation
for Machine Translation. In Proceedings of the Joint
Conference on Human Language Technology / Empir-
ical Methods in Natural Language Processing (HLT-
EMNLP), pages 771?778, Vancouver, Canada.
9
Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 398?404,
Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational Linguistics
LIMSI Submission for the WMT?13 Quality Estimation Task: an
Experiment with n-gram Posteriors
Anil Kumar Singh
LIMSI
Orsay, France
anil@limsi.fr
Guillaume Wisniewski
Universite? Paris Sud
LIMSI
Orsay, France
wisniews@limsi.fr
Franc?ois Yvon
Universite? Paris Sud
LIMSI
Orsay, France
yvon@limsi.fr
Abstract
This paper describes the machine learning
algorithm and the features used by LIMSI
for the Quality Estimation Shared Task.
Our submission mainly aims at evaluating
the usefulness for quality estimation of n-
gram posterior probabilities that quantify
the probability for a given n-gram to be
part of the system output.
1 Introduction
The dissemination of statistical machine transla-
tion (SMT) systems in the professional translation
industry is still limited by the lack of reliability of
SMT outputs, the quality of which varies to a great
extent. In this context, a critical piece of informa-
tion would be for MT systems to assess their out-
put translations with automatically derived quality
measures. This problem is the focus of a shared
task, the aim of which is to predict the quality
of a translation without knowing any human ref-
erence(s).
To the best of our knowledge, all approaches
so far have tackled quality estimation as a super-
vised learning problem (He et al, 2010; Soricut
and Echihabi, 2010; Specia et al, 2010; Specia,
2011). A wide variety of features have been pro-
posed, most of which can be described as loosely
?linguistic? features that describe the source sen-
tence, the target sentence and the association be-
tween them (Callison-Burch et al, 2012). Sur-
prisingly enough, information used by the decoder
to choose the best translation in the search space,
such as its internal scores, have hardly been con-
sidered and never proved to be useful. Indeed, it is
well-known that these scores are hard to interpret
and to compare across hypotheses. Furthermore,
mapping scores of a linear classifier (such as the
scores estimated by MERT) into consistent prob-
abilities is a difficult task (Platt, 2000; Lin et al,
2007).
This work aims at assessing whether informa-
tion extracted from the decoder search space can
help to predict the quality of a translation. Rather
than using directly the decoder score, we propose
to consider a finer level of information, the n-gram
posterior probabilities that quantifies the probabil-
ity for a given n-gram to be part of the system
output. These probabilities can be directly inter-
preted as the confidence the system has for a given
n-gram to be part of the translation. As they are
directly derived from the number of hypotheses in
the search space that contains this n-gram, these
probabilities might be more reliable than the ones
estimated from the decoder scores.
We first quickly review, in Section 2, the n-gram
posteriors introduced by (Gispert et al, 2013) and
explain how they can be used in the QE task; we
then describe, in Section 3 the different systems
that have developed for our participation in the
WMT?13 shared task on Quality Estimation and
assess their performance in Section 4.
2 n-gram Posterior Probabilities in SMT
Our contribution to the WMT?13 shared task on
quality estimation relies on n-gram posteriors. For
the sake of completeness, we will quickly formal-
ize this notion and summarize the method pro-
posed by (Gispert et al, 2013) to efficiently com-
pute them. We will then describe preliminary ex-
periments to assess their usefulness for predicting
the quality of a translation hypothesis.
2.1 Computing n-gram Posteriors
For a given source sentence F , the n-gram pos-
terior probabilities quantifies the probability for
a given n-gram to be part of the system output.
Their computation relies on all the hypotheses
considered by a SMT system during decoding: in-
tuitively, the more hypotheses a n-gram appears
in, the more confident the system is that this n-
gram is part of the ?correct? translation, and the
398
higher its posterior probability is. Formally, the
posterior of a given n-gram u is defined as:
P (u|E) =
?
(A,E)?E
?u(E) ? P (E,A|F )
where the sum runs over the translation hypothe-
ses contained in the search space E (generally rep-
resented as a lattice); ?u(E) has the value 1 if u
occurs in the translation hypothesis E and 0 oth-
erwise and P (E,A|F ) is the probability that the
source sentence F is translated by the hypothesis
E using a derivation A. Following (Gispert et al,
2013), this probability is estimated by applying a
soft-max function to the score of the decoder:
P (A,E|F ) = exp (??H(E,A, F ))?
(A?,E?)?E exp (H(E?, A?, F ))
where the decoder score H(E,A, F ) is typically
a linear combination of a handful of features, the
weights of which are estimated by MERT (Och,
2003).
n-gram posteriors therefore aggregate two
pieces of information: first, the number of paths in
the lattice (i.e. the number of translation hypothe-
ses of the search path) the n-gram appears in; sec-
ond, the decoder scores of these paths that can be
roughly interpreted as a quality of the path.
Computing P (u|E) requires to enumerate all n-
gram contained in E and to count the number of
paths in which this n-gram appears at least once.
An efficient method to perform this computation
in a single traversal of the lattice is described
in (Gispert et al, 2013). This algorithm has been
reimplemented1 to generate the posteriors used in
this work.
2.2 Analysis of n-gram Posteriors
Figure 1 represents the distribution of n-gram pos-
teriors on the training set of the task 1-1. This dis-
tribution is similar to the ones observed for task 1-
3 and for higher n-gram orders. It appears that, the
distribution is quite irregular and has two modes.
The minor modes corresponds to n-grams that ap-
pear in almost every translation hypotheses and
have posterior probability close to 1. Further anal-
yses show that these n-grams are mainly made of
stop words and of out-of-vocabulary words. The
major mode corresponds to very small n-gram
posteriors (less than 10?1) that the system has only
1Our implementation can be downloaded from http://
perso.limsi.fr/Individu/wisniews/.
a very small confidence in producing. The num-
ber of n-grams that have such a small posterior
suggests that most n-grams occur only in a small
number of paths.
0 5 10 15 20 250
.00
0.0
5
0.1
0
0.1
5
0.2
0
? logP (u|E)
De
nsi
ty
Figure 1: Distribution of the unigram posteriors
observed on the training set of the task 1-1
Using n-gram posteriors to predict the quality
of translation raises a representation issue: the
number of n-grams contained in a sentence varies
with the sentence length (and hence with the num-
ber of posteriors) but this information needs to be
represented in a fixed-length vector describing the
sentence. Similarly to what is usually done in the
quality estimation task, we chose to represent pos-
teriors probability by their histogram: for a given
n-gram order, each posterior is mapped to a bin;
each bin is then represented by a feature equal to
the number of n-gram posteriors it contains. To
account for the irregular distribution of posteriors,
bin breaks are chosen on the training set so as to
ensure that each bin contains the same number of
examples. In our experiments, we considered a
partition of the training data into 20 bins.
3 Systems Description
LIMSI has participated to the tasks 1-1 (predic-
tion of the hTER) and 1-3 (prediction of the post-
edition time). Similar features and learning algo-
rithms have been considered for the two tasks. We
will first quickly describe them before discussing
the specific development made for task 1-3.
399
3.1 Features
In addition to the features described in the previ-
ous section, 176 ?standard? features for quality es-
timation have been considered. The full list of fea-
tures we have considered is given in (Wisniewski
et al, 2013) and the features set can be down-
loaded from our website.2 These features can be
classified into four broad categories:
? Association Features: Measures of the qual-
ity of the ?association? between the source
and the target sentences like, for instance,
features derived from the IBM model 1
scores;
? Fluency Features: Measures of the ?fluency?
or the ?grammaticality? of the target sentence
such as features based on language model
scores;
? Surface Features: Surface features extracted
mainly from the source sentence such as
the number of words, the number of out-
of-vocabulary words or words that are not
aligned;
? Syntactic Features: some simple syntactic
features like the number of nouns, modifiers,
verbs, function words, WH-words, number
words, etc., in a sentence;
These features sets differ, in several ways, from
the baseline feature set provided by the shared task
organizers. First, in addition to features derived
from a language model, it also includes several
features based on large span continuous space lan-
guage models (Le et al, 2011). Such language
models have already proved their efficiency both
for the translation task (Le et al, 2012) and the
quality estimation task (Wisniewski et al, 2013).
Second, each feature was expanded into two ?nor-
malized forms? in which their value was divided
either by the source length or the target length
and, when relevant, into a ?ratio form? in which
the feature value computed on the target sentence
is divided by its value computed in the source sen-
tence. At the end, when all possible feature expan-
sions are considered, each example is described by
395 features.
2http://perso.limsi.fr/Individu/
wisniews/
3.2 Learning Methods
The main focus of this work is to study the rel-
evance of features for quality estimation; there-
fore, only very standard learning methods were
used in our work. For this year submission
both random forests (Breiman, 2001) and elas-
tic net regression (Zou and Hastie, 2005) have
been used. The capacity of random forests to take
into account complex interactions between fea-
tures has proved to be a key element in the re-
sults achieved in our experiments with last year
campaign datasets (Zhuang et al, 2012). As we
are considering a larger features set this year and
the number of examples is comparatively quite
small, we also considered elastic regression, a lin-
ear model trained with L1 and L2 priors as regu-
larizers, hoping that training a sparse model would
reduce the risk of overfitting.
In this study, we have used the implementation
provided by scikit-learn (Pedregosa et al,
2011). As detailed in Section 4.1, cross-validation
has been used to choose the hyper-parameters of
all regressors, namely the number of estimators,
the maximal depth of a tree and the minimum
number of examples in a leaf for the random
forests and the importance of the L1 and the L2
regularizers for the elastic net regressor.
3.3 System for Task 1-3
Like task 1-1, task 1-3 is a regression task that
aims at predicting the time needed to post-edit a
translation hypothesis. From a machine learning
point of view, this task differs from task 1-1 in
three aspects. First, the distributed training set
is much smaller: it is made of only 803 exam-
ples, which increases the risk of overfitting. Sec-
ond, contrary to hTER scores, post-edition time is
not normalized and the label of this task can take
any positive value. Finally and most importantly,
as shown in Figure 2, the label distributions es-
timated on the training set has a long tail which
indicates the presence of several outliers: in the
worse case, it took more than 18 minutes to cor-
rect a single sentence made of 35 words! Such
a long post-edition time most certainly indicates
that the corrector has been distracted when post-
editing the sentence rather than a true difficulty in
the post-edition.
These outliers have a large impact on training
and on testing, as their contributions to both MAE
400
0 200 400 600 800 1000 12000.
00
0
0.0
02
0.0
04
0.0
06
0.0
08
Post-edition time (s)
De
nsi
ty
Figure 2: Kernel density estimate of the post-
edition time distribution used as label in task 1-3.
and MSE,3 directly depends on label values and
can therefore be very large in the case of outliers.
For instance, a simple ridge regression with the
baseline features provided by the shared task or-
ganizer achieves a MAE of 42.641 ? 2.126 on
the test set. When all the examples having a la-
bel higher than 300 are removed from the training
set, the MAE drops to 41.843? 4.134. When out-
liers are removed from both the training and the
test sets, the MAE further drops to 32.803?1.673.
These observations indicate that special care must
be taken when collecting the data and that, maybe,
post-edition times should be clipped to provide a
more reliable estimation of the predictor perfor-
mance.
In the following (and in our submission) only
examples for which the post-edition time was less
than 300 seconds were considered.
4 Results
4.1 Experimental Setup
We have tested different combinations of features
and learning methods using a standard metric for
regression: Mean Absolute Error (MAE) defined
by:
MAE = 1n
n?
i=1
|y?i ? yi|
3The two standard loss functions used to train and evalu-
ate a regressor
where n is the number of examples, yi and y?i
the true label and predicted label of the ith exam-
ple. MAE can be understood as the averaged error
made in predicting the quality of a translation.
Performance of both task 1-1 and task 1-34 was
also evaluated by the Spearman rank correlation
coefficient ? that assesses how well the relation-
ship between two variables can be described using
a monotonic function. While the value of the cor-
relation coefficient is harder to interpret as it not
directly related to the value to predict, it can be
used to compare the performance achieved when
predicting different measures of the post-editing
effort. Indeed, several sentence-level (or docu-
ment level) annotation types can be used to reflect
translation quality (Specia, 2011), such as the time
needed to post-edit a translation hypothesis, the
hTER, or qualitative judgments as it was the case
for the shared task of WMT 2012. Comparing di-
rectly these different settings is complicated, since
each of them requires to optimize a different loss,
and even if the losses are the same, their actual
values will depend on the actual annotation to be
predicted (refer again to the discussion in (Specia,
2011, p5)). Using a metric that relies on the pre-
dicted rank of the example rather than the actual
value predicted allows us to directly compare the
performance achieved on the two tasks.
As the labels for the different tasks were not re-
leased before the evaluation, all the reported re-
sults are obtained on an ?internal? test set, made of
20% of the data released by the shared task or-
ganizers as ?training? data. The remaining data
were used to train the regressor in a 10 folds cross-
validation setting. In order to get reliable estimate
of our methods performances, we used bootstrap
resampling (Efron and Tibshirani, 1993) to com-
pute confidence intervals of the different scores:
10 random splits of the data into a training and
sets were generated; a regressor was then trained
and tested for each of these splits and the resulting
confidence intervals at 95% computed.
4.2 Results
Table 1and Table 2 contain the results achieved by
our different conditions. We used, as a baseline,
the set of 17 features released by the shared task
organizers.
It appears that the differences in MAE between
4The Spearman ? was an official metric only for task 1-
1. For reasons explained in this paragraph, we also used it to
evaluate our results for task 1-3.
401
the different configurations are always very small
and hardly significant. However, the variation of
the Spearman ? are much larger and the difference
observed are practically significant when the inter-
pretation scale of (Landis and Koch, 1977) is used.
We will therefore mainly consider ? in our discus-
sion.
For the two tasks 1-1 and 1-3, the features we
have designed allow us to significantly improve
prediction performance in comparison to the base-
line. For instance, for task 1-1, the correlation
is almost doubled when the features described in
Section 3.1 are used. As expected, random forests
are overfitting and did not manage to outperform
a simple linear classifier. That is why we only
used the elastic net method for our official submis-
sion. Including posterior probabilities in the fea-
ture set did not improve performance much (ex-
cept when only the baseline features are consid-
ered) and sometimes even hurt performance. This
might be caused by an overfitting problem, the
training set becoming too small when new features
are added. We are conducting further experiments
to explain this paradoxical observation.
Another interesting observation that can be
made looking at the results of Table 1 and Ta-
ble 2 is that the prediction of the post-edition time
seems to be easier than the prediction of the hTER:
using the same classifiers and the same features,
the performance for the former task is always far
better than the performance for the latter.
5 Conclusion
In this paper, we described our submission to the
WMT?13 shared task on quality estimation. We
have explored the use of posteriors probability,
hoping that information about the search space
could help in predicting the quality of a transla-
tion. Even if features derived from posterior prob-
abilities have shown to have only a very limited
impact, we managed to significantly improve the
baseline with a standard learning method and sim-
ple features. Further experiments are required to
understand the reasons of this failure.
Our results also highlight the need to continue
gathering high-quality resources to train and in-
vestigate quality estimation systems: even when
considering few features, our systems were prone
to overfitting. Developing more elaborated sys-
tems will therefore only be possible if more train-
ing resource is available. Our experiments also
stress that both the choice of the quality measure
(i.e. the quantity to predict) and of the evaluation
metrics for quality estimation are still open prob-
lems.
6 Acknowledgments
This work was partly supported by ANR
projects Trace (ANR-09-CORD-023) and Tran-
sread (ANR-12-CORD-0015).
References
Leo Breiman. 2001. Random forests. Mach. Learn.,
45(1):5?32, October.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Matt Post, Radu Soricut, and Lucia Specia. 2012.
Findings of the 2012 workshop on statistical ma-
chine translation. In Proceedings of the Seventh
Workshop on Statistical Machine Translation, pages
10?51, Montre?al, Canada, June. Association for
Computational Linguistics.
B. Efron and R. Tibshirani. 1993. An Introduction
to the Bootstrap. Chapman and Hall/CRC Mono-
graphs on Statistics and Applied Probability Series.
Chapman & Hall.
Adria` Gispert, Graeme Blackwood, Gonzalo Iglesias,
and William Byrne. 2013. N-gram posterior prob-
ability confidence measures for statistical machine
translation: an empirical study. Machine Transla-
tion, 27(2):85?114.
Yifan He, Yanjun Ma, Josef van Genabith, and Andy
Way. 2010. Bridging smt and tm with translation
recommendation. In Proceedings of the 48th An-
nual Meeting of the Association for Computational
Linguistics, pages 622?630, Uppsala, Sweden, July.
Association for Computational Linguistics.
R. J. Landis and G. G. Koch. 1977. The measurement
of observer agreement for categorical data. Biomet-
rics, 33(1):159?174.
Hai Son Le, Ilya Oparin, Alexandre Allauzen, Jean-
Luc Gauvain, and Franc?ois Yvon. 2011. Structured
Output Layer Neural Network Language Model.
In Proceedings of IEEE International Conference
on Acoustic, Speech and Signal Processing, pages
5524?5527, Prague, Czech Republic.
Hai-Son Le, Thomas Lavergne, Alexandre Al-
lauzen, Marianna Apidianaki, Li Gong, Aure?lien
Max, Artem Sokolov, Guillaume Wisniewski, and
Franc?ois Yvon. 2012. Limsi @ wmt12. In
Proceedings of the Seventh Workshop on Statisti-
cal Machine Translation, pages 330?337, Montre?al,
Canada, June. Association for Computational Lin-
guistics.
402
MAE ?
train test train test
Baseline Features
RandomForest 0.109? 0.013 0.130? 0.004 0.405? 0.008 0.314? 0.016
Elastic 0.127? 0.001 0.129? 0.003 0.336? 0.004 0.319? 0.015
?Linguistic? Features
RandomForest 0.082? 0.019 0.118? 0.003 0.689? 0.003 0.625? 0.009
Elastic 0.107? 0.004 0.115? 0.003 0.705? 0.009 0.660? 0.009
?Linguistic? Features + posteriors
RandomForest 0.088? 0.017 0.116? 0.003 0.694? 0.003 0.615? 0.014
Elastic 0.105? 0.006 0.114? 0.002 0.699? 0.007 0.662? 0.011
Table 1: Results for the task 1-1
MAE ?
train test train test
Baseline Features
RandomForest 25.145? 3.745 33.279? 1.687 0.669? 0.007 0.639? 0.017
Elastic 32.776? 0.795 33.702? 2.328 0.678? 0.006 0.657? 0.018
Baseline Features + Posteriors
RandomForest 33.707? 0.309 35.646? 0.889 0.674? 0.004 0.637? 0.017
Elastic 31.487? 0.261 32.922? 0.789 0.698? 0.004 0.681? 0.016
?Linguistic? Features
RandomForest 25.236? 4.400 33.017? 1.582 0.735? 0.007 0.666? 0.023
Elastic 28.706? 1.273 31.630? 1.612 0.760? 0.006 0.701? 0.017
?Linguistic? Features + Posteriors
RandomForest 22.951? 3.903 33.013? 1.514 0.741? 0.003 0.695? 0.013
Elastic 28.911? 1.020 31.865? 1.636 0.761? 0.008 0.710? 0.017
Table 2: Results for the task 1-3
403
Hsuan-Tien Lin, Chih-Jen Lin, and Ruby C. Weng.
2007. A note on platt?s probabilistic outputs for
support vector machines. Mach. Learn., 68(3):267?
276, October.
Franz Josef Och. 2003. Minimum error rate train-
ing in statistical machine translation. In Proceed-
ings of the 41st Annual Meeting of the Association
for Computational Linguistics, pages 160?167, Sap-
poro, Japan.
F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,
B. Thirion, O. Grisel, M. Blondel, P. Pretten-
hofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Pas-
sos, D. Cournapeau, M. Brucher, M. Perrot, and
E. Duchesnay. 2011. Scikit-learn: Machine Learn-
ing in Python . Journal of Machine Learning Re-
search, 12:2825?2830.
John C. Platt, 2000. Probabilities for SV Machines,
pages 61?74. MIT Press.
Radu Soricut and Abdessamad Echihabi. 2010.
Trustrank: Inducing trust in automatic translations
via ranking. In Proceedings of the 48th Annual
Meeting of the Association for Computational Lin-
guistics, pages 612?621, Uppsala, Sweden, July. As-
sociation for Computational Linguistics.
Lucia Specia, Dhwaj Raj, and Marco Turchi. 2010.
Machine translation evaluation versus quality esti-
mation. Machine Translation, 24(1):39?50, March.
Lucia Specia. 2011. Exploiting objective annotations
for measuring translation post-editing effort. In Pro-
ceedings of the 15th conference of EAMT, pages 73?
80, Leuven, Belgium.
Guillaume Wisniewski, Anil Kumar Singh, and
Franc?ois Yvon. 2013. Quality estimation for ma-
chine translation: Some lessons learned. Machine
Translation. accepted for publication.
Yong Zhuang, Guillaume Wisniewski, and Franc?ois
Yvon. 2012. Non-linear models for confidence es-
timation. In Proceedings of the Seventh Workshop
on Statistical Machine Translation, pages 157?162,
Montre?al, Canada, June. Association for Computa-
tional Linguistics.
Hui Zou and Trevor Hastie. 2005. Regularization and
variable selection via the elastic net. Journal of the
Royal Statistical Society, Series B, 67:301?320.
404
Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 348?354,
Baltimore, Maryland USA, June 26?27, 2014.
c?2014 Association for Computational Linguistics
LIMSI Submission for WMT?14 QE Task
Guillaume Wisniewski and Nicolas P
?
echeux and Alexandre Allauzen and Franc?ois Yvon
Universit?e Paris Sud and LIMSI-CNRS
91 403 ORSAY CEDEX, France
{wisniews, pecheux, allauzen, yvon}@limsi.fr
Abstract
This paper describes LIMSI participation
to the WMT?14 Shared Task on Qual-
ity Estimation; we took part to the word-
level quality estimation task for English
to Spanish translations. Our system re-
lies on a random forest classifier, an en-
semble method that has been shown to
be very competitive for this kind of task,
when only a few dense and continuous fea-
tures are used. Notably, only 16 features
are used in our experiments. These fea-
tures describe, on the one hand, the qual-
ity of the association between the source
sentence and each target word and, on the
other hand, the fluency of the hypothe-
sis. Since the evaluation criterion is the
f
1
measure, a specific tuning strategy is
proposed to select the optimal values for
the hyper-parameters. Overall, our system
achieves a 0.67 f
1
score on a randomly ex-
tracted test set.
1 Introduction
This paper describes LIMSI submission to the
WMT?14 Shared Task on Quality Estimation. We
participated in the word-level quality estimation
task (Task 2) for the English to Spanish direction.
This task consists in predicting, for each word in
a translation hypothesis, whether this word should
be post-edited or should rather be kept unchanged.
Predicting translation quality at the word level
raises several interesting challenges. First, this is
a (relatively) new task and the best way to for-
mulate and evaluate it has still to be established.
Second, as most works on quality estimation have
only considered prediction at the sentence level, it
is not clear yet which features are really effective
to predict quality at the word and a set of base-
line features has still to be found. Finally, sev-
eral characteristic of the task (the limited number
of training examples, the unbalanced classes, etc.)
makes the use of ?traditional? machine learning al-
gorithms difficult. This papers describes how we
addressed this different issues for our participation
to the WMT?14 Shared Task.
The rest of this paper is organized as follows.
Section 2 gives an overview of the shared task data
that will justify some of the design decisions we
made. Section 3 describes the different features
we have considered and Section 4, the learning
methods used to estimate the classifiers parame-
ters. Finally the results of our models are pre-
sented and analyzed in Section 5.
2 World-Level Quality Estimation
WMT?14 shared task on quality estimation num-
ber 2 consists in predicting, for each word of a
translation hypothesis, whether this word should
be post-edited (denoted by the BAD label) or
should be kept unchanged (denoted by the OK la-
bel). The shared task organizers provide a bilin-
gual dataset from English to Spanish
1
made of
translations produced by three different MT sys-
tems and by one human translator; these transla-
tions have then been annotated with word-level la-
bels by professional translators. No additional in-
formation about the systems used, the derivation
of the translation (such as the lattices or the align-
ment between the source and the best translation
hypothesis) or the tokenization applied to identify
words is provided.
The distributions of the two labels for the dif-
ferent systems is displayed in Table 1. As it
could be expected, the class are, overall, unbal-
anced and the systems are of very different qual-
ity: the proportion of BAD and OK labels highly
depends on the system used to produce the transla-
tion hypotheses. However, as our preliminary ex-
periments have shown, the number of examples is
1
We did not consider the other language pairs.
348
too small to train a different confidence estimation
system for each system.
The distribution of the number of BAD labels
per sentence is very skewed: on average, one word
out of three (precisely 35.04%) in a sentence is la-
beled as BAD but the median of the distribution of
the ratio of word labeled BAD in a sentence is 20%
and its standard deviation is pretty high (34.75%).
Several sentences have all their words labeled as
either OK or BAD, which is quite surprising as the
sentences of the corpus for Task 2 have been se-
lected because there were ?near miss translations?
that is to say translations that should have con-
tained no more that 2 or 3 errors.
Another interesting finding is that the propor-
tion of word to post-edit is the same across the
different parts-of-speech (see Table 2).
2
Table 1: Number of examples and distribution of
labels for the different systems on the training set
System #sent. #words % OK % BAD
1 791 19,456 75.48 24.52
2 621 14,620 59.11 40.89
3 454 11,012 59.76 40.24
4 90 2,296 36.85 63.15
Total 1,956 47,384 64.90 35.10
Table 2: Distribution of labels according to the
POS on the training set
POS % in train % BAD
NOUN 23.81 35.02
ADP 15.06 35.48
DET 14.90 32.88
VERB 14.64 41.26
PUNCT 10.92 27.26
ADJ 6.61 35.68
CONJ 5.04 30.77
PRON 4.58 43.15
ADV 4.39 36.56
As the classes are unbalanced, prediction per-
formance will be evaluated in terms of precision,
recall and f
1
score computed on the BAD label.
More precisely, if the number of true positive (i.e.
2
We used FreeLing (http:nlp.lsi.upc.edu/
freeling/) to predict the POS tags of the translation
hypotheses and, for the sake of clarity, mapped the 71 tags
used by FreeLing to the 11 universal POS tags of Petrov et
al. (2012).
BAD word predicted as BAD), false positive (OK
word predicted as BAD) and false negative (BAD
word predicted as OK) are denoted tp
BAD
, fp
BAD
and fn
BAD
, respectively, the quality of a confidence
estimation system is evaluated by the three follow-
ing metrics:
p
BAD
=
tp
BAD
tp
BAD
+ fp
BAD
(1)
r
BAD
=
tp
BAD
tp
BAD
+ fn
BAD
(2)
f
1
=
2 ? p
BAD
? r
BAD
p
BAD
+ r
BAD
(3)
3 Features
In our experiments, we used 16 features to de-
scribe a given target word t
i
in a translation hy-
pothesis t = (t
j
)
m
j=1
. To avoid sparsity issues we
decided not to include any lexicalized information
such as the word or the previous word identities.
As the translation hypotheses were generated by
different MT systems, no white-box features (such
as word alignment or model scores) are consid-
ered. Our features can be organized in two broad
categories:
Association Features These features measure
the quality of the ?association? between the source
sentence and a target word: they characterize the
probability for a target word to appear in a transla-
tion of the source sentence. Two kinds of associa-
tion features can be distinguished.
The first one is derived from the lexicalized
probabilities p(t|s) that estimate the probability
that a source word s is translated by the target
word t
j
. These probabilities are aggregated using
an arithmetic mean:
p(t
j
|s) =
1
n
n
?
i=1
p(t
j
|s
i
) (4)
where s = (s
i
)
n
i=1
is the source sentence (with an
extra NULL token). We assume that p(t
j
|s
i
) = 0 if
the words t
j
and s
i
have never been aligned in the
train set and also consider the geometric mean of
the lexicalized probabilities, their maximum value
(i.e. max
s?s
p(t
j
|s)) as well as a binary feature
that fires when the target word t
j
is not in the lex-
icalized probabilities table.
The second kind of association features relies
on pseudo-references, that is to say, translations
of the source sentence produced by an indepen-
dent MT system. Many works have considered
349
pseudo-references to design new MT metrics (Al-
brecht and Hwa, 2007; Albrecht and Hwa, 2008)
or for confidence estimation (Soricut and Echi-
habi, 2010; Soricut and Narsale, 2012) but, to the
best of our knowledge, this is the first time that
they are used to predict confidence at the word
level.
Pseudo-references are used to define 3 binary
features which fire if the target word is in the
pseudo-reference, in a 2-gram shared between the
pseudo-reference and the translation hypothesis or
in a common 3-gram, respectively. The lattices
representing the search space considered to gen-
erate these pseudo-references also allow us to es-
timate the posterior probability of a target word
that quantifies the probability that it is part of the
system output (Gispert et al., 2013). Posteriors ag-
gregate two pieces of information for each word in
the final hypothesis: first, all the paths in the lat-
tice (i.e. the number of translation hypotheses in
the search space) where the word appears in are
considered; second, the decoder scores of these
paths are accumulated in order to derive a confi-
dence measure at the word level. In our experi-
ments, we considered pseudo-references and lat-
tices produced by the n-gram based system de-
veloped by our team for last year WMT evalu-
ation campaign (Allauzen et al., 2013), that has
achieved very good performance.
Fluency Features These features measure the
?fluency? of the target sentence and are based on
different language models: a ?traditional? 4-gram
language model estimated on WMT monolingual
and bilingual data (the language model used by
our system to generate the pseudo-references); a
continuous-space 10-gram language model esti-
mated with SOUL (Le et al., 2011) (also used by
our MT system) and a 4-gram language model
based on Part-of-Speech sequences. The latter
model was estimated on the Spanish side of the
bilingual data provided in the translation shared
task in 2013. These data were POS-tagged with
FreeLing (Padr?o and Stanilovsky, 2012).
All these language models have been used to de-
fine two different features :
? the probability of the word of interest p(t
j
|h)
where h = t
j?1
, ..., t
j?n+1
is the history
made of the n? 1 previous words or POS
? the ratio between the probability of
the sentence and the ?best? probabil-
ity that can be achieved if the target
word is replaced by any other word (i.e.
max
v?V
p(t
1
, ..., t
j?1
, v, t
j+1
, ..., t
m
) where
the max runs over all the words of the
vocabulary).
There is also a feature that describes the back-off
behavior of the conventional language model: its
value is the size of the largest n-gram of the trans-
lation hypothesis that can be estimated by the lan-
guage model without relying on back-off probabil-
ities.
Finally, there is a feature describing, for each
word that appears more than once in the train set,
the probability that this word is labeled BAD. This
probability is simply estimated by the ratio be-
tween the number of times this word is labeled
BAD and the number of occurrences of this word.
It must be noted that most of the features we
consider rely on models that are part of a ?clas-
sic? MT system. However their use for predicting
translation quality at the word-level is not straight-
forward, as they need to be applied to sentences
with a given unknown tokenization. Matching the
tokenization used to estimate the model to the one
used for collecting the annotations is a tedious and
error-prone process and some of the prediction er-
rors most probably result from mismatches in tok-
enization.
4 Learning Methods
4.1 Classifiers
Predicting whether a word in a translation hypoth-
esis should be post-edited or not can naturally be
framed as a binary classification task. Based on
our experiments in previous campaigns (Singh et
al., 2013; Zhuang et al., 2012), we considered ran-
dom forest in all our experiments.
3
Random forest (Breiman, 2001) is an ensem-
ble method that learns many classification trees
and predicts an aggregation of their result (for in-
stance by majority voting). In contrast with stan-
dard decision trees, in which each node is split
using the best split among all features, in a ran-
dom forest the split is chosen randomly. In spite
of this simple and counter-intuitive learning strat-
egy, random forests have proven to be very good
?out-of-the-box? learners. Random forests have
achieved very good performance in many similar
3
we have used the implementation provided by
scikit-learn (Pedregosa et al., 2011).
350
tasks (Chapelle and Chang, 2011), in which only
a few dense and continuous features are available,
possibly because of their ability to take into ac-
count complex interactions between features and
to automatically partition the continuous features
value into a discrete set of intervals that achieves
the best classification performance.
As a baseline, we consider logistic regres-
sion (Hastie et al., 2003), a simple linear model
where the parameters are estimated by maximiz-
ing the likelihood of the training set.
These two classifiers do not produce only a class
decision but yield an instance probability that rep-
resents the degree to which an instance is a mem-
ber of a class. As detailed in the next section,
thresholding this probability will allow us to di-
rectly optimize the f
1
score used to evaluate pre-
diction performance.
4.2 Optimizing the f
1
Score
As explained in Section 2, quality prediction will
be evaluated in terms of f
1
score. The learn-
ing methods we consider can not, as most learn-
ing method, directly optimize the f
1
measure dur-
ing training, since this metric does not decompose
over the examples. It is however possible to take
advantage of the fact that they actually estimate a
probability to find the largest f
1
score on the train-
ing set.
Indeed these probabilities are used with a
threshold (usually 0.5) to produce a discrete (bi-
nary) decision: if the probability is above the
threshold, the classifier produces a positive out-
put, and otherwise, a negative one. Each thresh-
old value produces a different trade-off between
true positives and false positives and consequently
between recall and precision: as the the threshold
becomes lower and lower, more and more exam-
ple are assigned to the positive class and recall in-
crease at the expense of precision.
Based on these observations, we propose the
following three-step method to optimize the f
1
score on the training set:
1. the classifier is first trained using the ?stan-
dard? learning procedure that optimizes either
the 0/1 loss (for random forest) or the likeli-
hood (for the logistic regression);
2. all the possible trade-offs between recall
and precision are enumerated by varying
the threshold; exploiting the monotonicity of
thresholded classifications,
4
this enumeration
can be efficiently done in O (n ? log n) and
results in at most n threshold values, where n
is the size of the training set (Fawcett, 2003);
3. all the f
1
scores achieved for the different
thresholds found in the previous step are eval-
uated; there are strong theoretical guaran-
tees that the optimal f
1
score that can be
achieved on the training set is one of these
values (Boyd and Vandenberghe, 2004).
Figure 1 shows how f
1
score varies with the deci-
sion threshold and allows to assess the difference
between the optimal value of the threshold and its
default value (0.5).
Figure 1: Evolution of the f
1
score with respect to
the threshold used to transform probabilities into
binary decisions
5 Experiments
The features and learning strategies described in
the two previous sections were evaluated on the
English to Spanish datasets. As no official devel-
opment set was provided by the shared task orga-
nizers, we randomly sampled 200 sentences from
the training set and use them as a test set through-
out the rest of this article. Preliminary experiments
show that the choice of this test has a very low im-
pact on the classification performance. The dif-
ferent hyper-parameters of the training algorithm
4
Any instance that is classified positive with respect to a
given threshold will be classified positive for all lower thresh-
olds as well.
351
Table 3: Prediction performance for the two learn-
ing strategies considered
Classifier thres. r
BAD
p
BAD
f
1
Random forest 0.43 0.64 0.69 0.67
Logistic regression 0.27 0.51 0.72 0.59
were chosen by maximizing classification perfor-
mance (as evaluated by the f
1
score) estimated on
150 sentences of the training set kept apart as a
validation set.
Results for the different learning algorithms
considered are presented in Table 3. Random for-
est clearly outperforms a simple logistic regres-
sion, which shows the importance of using non-
linear decision functions, a conclusion at pair with
our previous results (Zhuang et al., 2012; Singh et
al., 2013).
The overall performance, with a f
1
measure of
0.67, is pretty low and in our opinion, not good
enough to consider using such a quality estimation
system in a computer-assisted post-edition con-
text. However, as shown in Table 4, the prediction
performance highly depends on the POS category
of the words: it is quite good for ?plain? words
(like verb and nouns) but much worse for other
categories.
There are two possible explanations for this
observation: predicting the correctness of some
morpho-syntaxic categories may be intrinsically
harder (e.g. for punctuation the choice of which
can be highly controversial) or depend on infor-
mation that is not currently available to our sys-
tem. In particular, we do not consider any in-
formation about the structure of the sentence and
about the labels of the context, which may explain
why our system does not perform well in predict-
ing the labels of determiners and conjunctions. In
both cases, this result brings us to moderate our
previous conclusions: as a wrong punctuation sign
has not the same impact on translation quality as a
wrong verb, our system might, regardless of its f
1
score, be able to provide useful information about
the quality of a translation. This also suggests that
we should look for a more ?task-oriented? metric.
Finally, Figure 2 displays the importance of the
different features used in our system. Random
forests deliver a quantification of the importance
of a feature with respect to the predictability of the
target variable. This quantification is derived from
Table 4: Prediction performance for each POS tag
System f
1
VERB 0.73
PRON 0.72
ADJ 0.70
NOUN 0.69
ADV 0.69
overall 0.67
DET 0.62
ADP 0.61
CONJ 0.57
PUNCT 0.56
the position of a feature in a decision tree: fea-
tures used in the top nodes of the trees, which con-
tribute to the final prediction decision of a larger
fraction of the input samples, play a more impor-
tant role than features used near the leaves of the
tree. It appears that, as for our previous experi-
ments (Wisniewski et al., 2013), the most relevant
feature for predicting translation quality is the fea-
ture derived from the SOUL language model, even
if other fluency features seem to also play an im-
portant role. Surprisingly enough, features related
to the pseudo-reference do not seem to be useful.
Further experiments are needed to explain the rea-
sons of this observation.
6 Conclusion
In this paper we described the system submitted
for Task 2 of WMT?14 Shared Task on Quality
Estimation. Our system relies on a binary clas-
sifier and consider only a few dense and contin-
uous features. While the overall performance is
pretty low, a fine-grained analysis of the errors of
our system shows that it can predict the quality of
plain words pretty accurately which indicates that
a more ?task-oriented? evaluation may be needed.
Acknowledgments
This work was partly supported by ANR project
Transread (ANR-12-CORD-0015). Warm thanks
to Quoc Khanh Do for his help for training a SOUL
model for Spanish.
References
Joshua Albrecht and Rebecca Hwa. 2007. Regression
for sentence-level mt evaluation with pseudo refer-
352
0 0.02 0.04 0.06 0.08 0.1 0.12 0.14
notInIBM1Table
wordNotInSearchSpace
pseudoRefCommon3gram
pseudoRefCommon1gram
pseudoRefCommon2gram
maxMatchingNGramSize
geomIBM1
diffMaxLM
bestAl
wordPosterior
arithIBM1
posLM
tradiLM
maxLMScore
priorProba
soulLM
Feature Importance
F
e
a
t
u
r
e
Figure 2: Features considered by our system sorted by their relevance for predicting translation errors
ences. In Proceedings of the 45th Annual Meeting of
the Association of Computational Linguistics, pages
296?303, Prague, Czech Republic, June. ACL.
Joshua Albrecht and Rebecca Hwa. 2008. The role
of pseudo references in MT evaluation. In Proceed-
ings of the Third Workshop on Statistical Machine
Translation, pages 187?190, Columbus, Ohio, June.
ACL.
Alexandre Allauzen, Nicolas P?echeux, Quoc Khanh
Do, Marco Dinarelli, Thomas Lavergne, Aur?elien
Max, Hai-Son Le, and Franc?ois Yvon. 2013. LIMSI
@ WMT13. In Proceedings of the Eighth Work-
shop on Statistical Machine Translation, pages 62?
69, Sofia, Bulgaria, August. ACL.
Stephen Boyd and Lieven Vandenberghe. 2004. Con-
vex Optimization. Cambridge University Press, New
York, NY, USA.
Leo Breiman. 2001. Random forests. Mach. Learn.,
45(1):5?32, October.
Olivier Chapelle and Yi Chang. 2011. Yahoo! learn-
ing to rank challenge overview. In Olivier Chapelle,
Yi Chang, and Tie-Yan Liu, editors, Yahoo! Learn-
ing to Rank Challenge, volume 14 of JMLR Pro-
ceedings, pages 1?24. JMLR.org.
Tom Fawcett. 2003. ROC Graphs: Notes and Practical
Considerations for Researchers. Technical Report
HPL-2003-4, HP Laboratories, Palo Alto.
Adri`a Gispert, Graeme Blackwood, Gonzalo Iglesias,
and William Byrne. 2013. N-gram posterior prob-
ability confidence measures for statistical machine
translation: an empirical study. Machine Transla-
tion, 27(2):85?114.
Trevor Hastie, Robert Tibshirani, and Jerome H. Fried-
man. 2003. The Elements of Statistical Learning.
Springer, July.
Hai-Son Le, Ilya Oparin, Alexandre Allauzen, Jean-
Luc Gauvain, and Franc?ois Yvon. 2011. Structured
output layer neural network language model. In
Acoustics, Speech and Signal Processing (ICASSP),
2011 IEEE International Conference on, pages
5524?5527. IEEE.
Llu??s Padr?o and Evgeny Stanilovsky. 2012. Freeling
3.0: Towards wider multilinguality. In Proceedings
of the Language Resources and Evaluation Confer-
ence (LREC 2012), Istanbul, Turkey, May. ELRA.
F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,
B. Thirion, O. Grisel, M. Blondel, P. Pretten-
hofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Pas-
sos, D. Cournapeau, M. Brucher, M. Perrot, and
E. Duchesnay. 2011. Scikit-learn: Machine learn-
ing in Python. Journal of Machine Learning Re-
search, 12:2825?2830.
Slav Petrov, Dipanjan Das, and Ryan McDonald. 2012.
A universal part-of-speech tagset. In Proceed-
ings of the Eight International Conference on Lan-
guage Resources and Evaluation (LREC?12), Istan-
bul, Turkey, may. European Language Resources
Association (ELRA).
Anil Kumar Singh, Guillaume Wisniewski, and
Franc?ois Yvon. 2013. LIMSI submission for
the WMT?13 quality estimation task: an experi-
ment with n-gram posteriors. In Proceedings of the
Eighth Workshop on Statistical Machine Transla-
tion, pages 398?404, Sofia, Bulgaria, August. ACL.
Radu Soricut and Abdessamad Echihabi. 2010.
Trustrank: Inducing trust in automatic translations
353
via ranking. In Proceedings of the 48th Annual
Meeting of the Association for Computational Lin-
guistics, pages 612?621, Uppsala, Sweden, July.
ACL.
Radu Soricut and Sushant Narsale. 2012. Combining
quality prediction and system selection for improved
automatic translation output. In Proceedings of the
Seventh Workshop on Statistical Machine Transla-
tion, pages 163?170, Montr?eal, Canada, June. ACL.
Guillaume Wisniewski, Anil Kumar Singh, and
Franc?ois Yvon. 2013. Quality estimation for ma-
chine translation: Some lessons learned. Machine
Translation, 27(3).
Yong Zhuang, Guillaume Wisniewski, and Franc?ois
Yvon. 2012. Non-linear models for confidence es-
timation. In Proceedings of the Seventh Workshop
on Statistical Machine Translation, pages 157?162,
Montr?eal, Canada, June. ACL.
354
