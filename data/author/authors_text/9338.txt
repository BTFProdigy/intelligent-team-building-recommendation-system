A Hybrid Chinese Language Model based on a Combination of 
Ontology with Statistical Method 
Dequan Zheng, Tiejun Zhao, Sheng Li and Hao Yu 
MOE-MS Key Laboratory of Natural Language Proceessing and Speech 
Harbin Institute of Technology 
Harbin, China, 150001 
{dqzheng, tjzhao, lisheng, yu}@mtlab.hit.edu.cn 
  
Abstract 
In this paper, we present a hybrid Chi-
nese language model based on a com-
bination of ontology with statistical 
method. In this study, we determined 
the structure of such a Chinese lan-
guage model. This structure is firstly 
comprised of an ontology description 
framework for Chinese words and a 
representation of Chinese lingual on-
tology knowledge. Subsequently, a 
Chinese lingual ontology knowledge 
bank is automatically acquired by de-
termining, for each word, its co-
occurrence with semantic, pragmatics, 
and syntactic information from the 
training corpus and the usage of Chi-
nese words will be gotten from lingual 
ontology knowledge bank for a actual 
document. To evaluate the performance 
of this language model, we completed 
two groups of experiments on texts re-
ordering for Chinese information re-
trieval and texts similarity computing. 
Compared with previous works, the 
proposed method improved the preci-
sion of nature language processing. 
1 Introduction 
Language modeling is a description of natural 
language and a good language model can help to 
improve the performance of the natural language 
processing. 
Traditional statistical language model 
(SLM) is fundamental to many natural language 
applications like automatic speech recognitionP[1]P, 
statistical machine translationP[2]P, and information 
retrievalP[3]P. Different statistical models have 
been proposed in the past, but n-gram models (in 
particular, bi-gram and tri-gram models) still 
dominate SLM research. After that, other ap-
proaches were put forward, such as the 
combination of statistical-based approach and 
rule-based approachP[4,5]P, self-adaptive language 
modelsP[6]P, topic-based model P[7]P and cache-based 
model P[8]P. But when the models are applied, the 
crucial disadvantages are that they can?t repre-
sent and process the semantic information of a 
natural language, so they can?t adapt well to the 
environment with changeful topics. 
Ontology was recognized as a conceptual 
modeling tool, which can descript an informa-
tion system in the semantic level and knowledge 
level. After it was first introduced in the field of 
Artificial IntelligenceP[9]P, it was closed combined 
with natural language processing and are widely 
applied in many field such as knowledge engi-
neering, digital library, information retrieval, 
semantic Web, and etc.  
In this paper, combining with the character-
istic of ontology and statistical method, we pre-
sent a hybrid Chinese language model. In this 
study, we determined the structure of Chinese 
language model and evaluate its performance 
with two groups of experiments on texts reorder-
ing for Chinese information retrieval and texts 
similarity computing. 
The rest of this paper is organized as fol-
lows. In section 2, we describe the Chinese lan-
guage model. In section 3, we evaluate the 
language model by several experiments about 
natural language processing. In section 4, we 
present the conclusion and some future work. 
2 The language model description 
Traditional SLM is make use to estimate the 
likelihood (or probability) of a word string, in 
13
this study, we determined the structure of Chi-
nese language model, first, we gave the ontology 
description framework of Chinese word and the 
representation of Chinese lingual ontology 
knowledge, and then, automatically acquired the 
usage of a word with its co-occurrence of con-
text in using semantic, pragmatics, syntactic, etc 
from the corpus to act as Chinese lingual ontol-
ogy knowledge bank. In actual document, the 
usage of lingual knowledge will be gotten from 
lingual ontology knowledge bank. 
2.1   Ontology description framework 
Traditional ontology mainly emphasizes the 
interrelations between essential concept, domain 
ontology is a public concept set of this do-
main P[10]P. We make use of this to present Chinese 
lingual ontology knowledge bank. 
In practical application, ontology can be 
figured in many waysP[11]P, natural languages, 
frameworks, semantic webs, logical languages, 
etc. Presently, popular models, such as Ontolin-
gua, CycL and Loom, are all based on logical 
language. Though logical language has a strong 
expression, its deduction is very difficult to lin-
gual knowledge. Semantic web and natural lan-
guage are non-formal, which have disadvantages 
in grammar and expression. 
For a Chinese word, we provided a frame-
work structure that can be understood by com-
puter combined with WordNet, HowNet and 
Chinese Thesaurus. This framework includes a 
Chinese word in concept, part of speech (POS), 
semantic, synonyms, English translation. Fig-
ure1 shows the ontology description framework 
of a Chinese word. 
 
 
 
 
 
 
 
Fig. 1. Ontology description framework 
2.2   Lingual ontology knowledge representation 
A word is the basic factor that composes the 
natural language, to acquire lingual ontology 
knowledge, we need to know POS, means and 
semantic of a word in a sentence. For example, 
for a Chinese sentence, the POS, means and 
Semantic label of ??? in HowNet are shown in 
table 1. For the Chinese sentence ??????
??????, after words segmented, POS tag-
ging and semantic tagging, we get a characteris-
tic string. They are shown in table 2. 
Table 1. the usage of ??? in Chinese sentence 
Chinese Sentence POS Means Semantic Num 
??? Verb Weave 525(weave|?? ) 
?? ? Verb Buy 348(buy|? ) 
Table 2. Segmentation, POS and Semantic tagging 
Items Results (???? acts as keyword) 
Chinese sentence ?????????? 
Words segmenta-
tion 
??  ??  ?  ??  ??  ? 
POS tagging ?? nd/ ?? Keyword/? vg/ ?? nd/ ??
vg/ ?wj/ 
Semantic label 
tagging 
?? nd/021243 ?? Keyword/070366?  
vg/017545 ?? nd/021243 ?? vg/092317 ?
wj/-1 
Characteristic string nd/021243 ?? Keyword/070366  vg/017545 
nd/021243 vg/092317 
Explanation of 
Semantic label 
021243 represents ????, 070366 represents 
???, 092317 represents ? ?? ?, ?-1? repre-
sents not to be defined or exist this semantic in 
HowNet. 
 
In order to use and express easily, we gave 
a description for ontology knowledge of every 
Chinese word, which learned from corpus, to be 
shown as expression 1. All of them composed 
the Chinese lingual ontology knowledge bank. 
( ) ( ) ( )???????? == UU
n
r
rrr
m
l
lll CLPOSSemCLPOSSemontologyKeyWord
11
,,,,,,,,
 
Where, KeyWord(ontology) is the ontology 
description of a Chinese word, ( )iii CLPOSSem ,,,  is 
the left co-occurrence knowledge of a Chinese 
word got from its context and ( )iii CLPOSSem ,,,  is 
the right co-occurrence knowledge. Symbol 
?? ? represents the aggregate of all the co-
occurrence with the KeyWord. ( )iii CLPOSSem ,,,  denotes the multi-grams 
from context of a Chinese word, which is com-
posed of semantic information SemBi B, part of 
speech POS Bi B, the position L from the word 
KeyWord to its co-occurrence, the average dis-
tance lC  from the word to its left (or right) i-th 
word. 
( )( )LPOSSemKeyword ii ,,,  denotes a seman-
tic relation pair between the keyword and its co-
occurrence in current context. 
The multi-grams of a Chinese word in con-
text, including the co-occurrence and their posi-
tion will act as the composition of lingual 
ontology knowledge too. In figure 2, the charac-
teristic string WB1 B, WB2 B, ?, WBi B represents POS and 
semantic label, Keyword is keyword itself, l or r 
Keywords  <?>
Concept             <?> 
Part of Speech  <?> 
Ontology   Semantic            <?> 
                    Synonym           <?> 
E-translation     <?> 
14
is the position of word that is left or right co-
occurrence with keyword. 
 
Fig. 2. Co-occurrence and the position information 
2.3   Lingual ontology knowledge acquisition 
According to the course that human being ac-
quires and accumulates knowledge, we propose 
a measurable description for Chinese lingual 
ontology knowledge through automatically 
learning typical corpus. In this approach, we will 
acquire the usage of a Chinese word in semantic, 
pragmatic and syntactic in all documents. We 
combine with the multi-grams in context includ-
ing its co-occurrence, POS, semantic, synonym, 
position. In practical application, we will proc-
ess every Chinese keyword that has the same 
grammar expression, semantic representation 
and syntactic structure with Chinese lingual on-
tology knowledge bank. 
2.3.1   Algorithm of automatic acquisition 
Step 1: corpus pre-processing.  
For any Chinese document DBi B in the docu-
ment set {D}, we treat the sentence that includes 
keyword as a processing unit. First, we have a 
Chinese word segmentation, POS tagging, Se-
mantic label tagging based on HowNet, and then, 
confirm a word to act as the keyword for acquir-
ing its co-occurrence knowledge. We wipe off 
the word that can do little contribution to the 
lingual ontology knowledge, such as preposition, 
conjunction, auxiliary word and etc. 
Step 2: Unify the keyword. 
Making use of the ontology description of 
Chinese word, we make the synonym into uni-
form one. 
Step 3: Calculate the co-occurrence distance. 
In our proposal, first, we treat the sentence 
that includes keyword as a processing unit and 
make POS tagging, semantic label tagging, then, 
we get Characteristic string. We take the key-
word as the center, define the left and right dis-
tance factor B Bl B and B Br B to be shown at formula 1. 
ml
B
??
???
??
?
=
2
11
2
11               
nr
B
??
???
??
?
=
2
11
2
11     (1) 
Where, m and n represent the left and right 
number of word that centered with the keyword. 
In this way, we try to get the language intuition, 
in a word, if the co-occurrence is nearer to the 
keyword, we will get more the co-occurrence 
distant. Final, we respectively get the left-side 
and right-side co-occurrence distant from key-
word to its co-occurrence to be shown as for-
mula 2. 
l
i
li BC
1
2
1 ???
???
?=  (i=1,?,m) 
r
j
rj BC
1
2
1 ???
???
?=  (j=1,?,n)       (2) 
Step4: Calculate the average co-occurrence 
distance. 
For a keyword, in the current sentence of 
document DBi,B we regard the keyword and its co-
occurrence (SemBi B, POS Bi B, L) as semantic relation 
pair, and CBjB is their co-occurrence distance. We 
calculate the average of CBjB that appear in corpus 
and act as the average co-occurrence distance 
lC  
between the keyword and its co-occurrence 
(SemBi B, POS Bi B, L). 
When all of documents are learned, all of 
keyword and their co-occurrence information ( )iii CLPOSSem ,,,  compose the Chinese lingual 
ontology knowledge bank. 
Step 5: Rebuild the index. 
In order to improve the processing speed, 
for acquired lingual ontology knowledge bank, 
we first build an index according to Chinese 
word, and then, we respectively make a sorting 
according to the semantic label SemBi B for every 
Chinese word. 
2.3.2 Lingual ontology knowledge application 
In practical application, we will respectively get 
different evaluation of a document from the lin-
gual ontology knowledge bank. For the natural 
language processing, e.g. documents similarity 
computing, text re-ranking for information re-
trieval, information filtering, the general proc-
essing is as follow. 
Step 1: Pre-processing and unify the key-
word. 
The processing is the same as Step 1 and 
Step 2 in section 2.3.1. 
Step 2: Fetch the average co-occurrence 
distance from lingual ontology knowledge bank. 
We regard a sentence including keyword in 
document D as a processing unit. First, we make 
POS tagging, semantic label tagging and get 
Characteristic string, and then, for every key-
word, if it has the same semantic relation pair as 
lingual ontology knowledge bank, i.e. the key-
word and its co-occurrence (SemBi B, POS Bi B, L) in 
practical document is the same one as lingual 
15
ontology knowledge bank, we add up all the 
average co-occurrence distance 
lC  from Chinese 
lingual ontology knowledge bank acquired in 
section 2.3.1. 
Step 3: Get the evaluation value of a docu-
ment. 
Repeat Step 2 until all keywords be proc-
essed and the accumulation of the average co-
occurrence distance 
lC will act as the evaluation 
value of current document. 
3 Evaluation of language model 
We completed two groups of experiments on 
text re-ranking for information retrieval, text 
similarity computing to verify the performance 
of lingual ontology knowledge. 
3.1   Texts reordering 
Information retrieval is used to retrieve relevant 
documents from a large document set for a user 
query, where the user query can be a simple de-
scription by natural. As a general rule, users 
hope more to acquire relevant information from 
the top ranking documents, so they concern 
more on the precision of top ranking documents 
than the recall. 
We use the Chinese document set CIRB011 
(132,173 documents) and CIRB020 (249,508 
documents) from NTCIR3 CLIR dataset and 
select 36 topics from 50 search topics (see 
http://research.nii.ac.jp/ntcir-ws3/work-en.html 
for more information) to evaluate our method. 
We use the same method to retrieve documents 
mentioned by Yang LingpengP[12]P, i.e. we use 
vector space model to retrieve documents, use 
cosine to calculate the similarity between docu-
ment and user query. We respectively use bi-
grams and words as indexing unitsP[13,14]P, the av-
erage precision of top N ranking documents acts 
as the normal results. In this paper, we used a 
Chinese dictionary that contains about 85,000 
items to segment Chinese document and query. 
To measure the effectiveness of informa-
tion retrieval, we use the same two kinds of 
relevant measures: relax-relevant and rigid-
relevantP[14,15]P. A document is rigid-relevant if it?s 
highly relevant or relevant with user query, and 
a document is relax-relevant if it is high relevant 
or relevant or partially relevant with user query. 
We also use PreAt10 and PreAt100 to represent 
the precision of top 10 ranking documents and 
top 100 ranking documents. 
3.1.1   Strategy of texts reordering 
First, we get some keywords to every topic by 
query description. For example, 
Title: ????? (The birth of a cloned 
calf) 
Description: ????????????
?????????????? (Find Arti-
cles relating to the birth of cloned calves using 
the technique called somatic cell nuclear transfer) 
We extract ???, ???, ??, ???
?? as feature word in this topic. 
Second, acquire lingual ontology knowl-
edge every topic by their feature words. In this 
proposal, we arrange 300 Chinese texts of this 
topic as learning corpus to get lingual ontology 
knowledge bank. 
Third, get the evaluation value of every text 
about this topic, i.e. respectively add up all the 
average co-occurrence distance lC  to the same 
semantic relation pairs in every text from lingual 
ontology knowledge bank.  
If a text has several keywords, repeat step3 
to acquire every evaluation value to these key-
words, and then, add up each evaluation value to 
act as the text evaluation value. 
Final, we reorder the initial retrieval texts 
according to the every text evaluation value of 
every topic. 
3.1.2   Experimental results and analysis 
We calculate the evaluation value of every text 
in each topic to reorder the initial relevant 
documents. 
Table 3 lists the normal results and our re-
sults based on bi-gram indexing, our results are 
acquired based on Chinese lingual ontology 
knowledge to enhance the effectiveness. 
PreAt10 is the average precision of 36 topics in 
precision of top 10 ranking documents, while 
PreAt100 is top 100 ranking documents. 
Table 4 lists the normal results and our re-
sults based on word indexing. Ratio displays an 
increase ratio of our result compared with nor-
mal result. 
Table 3. Precision (bi-gram as indexing unit) 
Items Normal Our method Ratio 
PreAt10 (Relax) 0.3704 0.4389 18.49% 
PreAt100 (Relax) 0.1941 0.2239 15.35% 
PreA10 (Rigid) 0.2625 0.3083 17.45% 
PreAt100 (Rigid) 0.1312 0.1478 12.65% 
16
Table 4. Precision (word as indexing unit) 
Items Normal Our method Ratio 
PreAt10 (Relax) 0.3829 0.4481 17.03% 
PreAt100 (Relax) 0.2022 0.2306 14.05% 
PreAt10 (Rigid) 0.2745 0.3169 15.45% 
PreAt100 (Rigid) 0.1405 0.1573 11.96% 
In table 3, it is shown that compared with 
bi-grams as indexing units, our method respec-
tively increases 18.49% in relax relevant meas-
ure and 17.45% in rigid in PreAt10. In PreAt100 
level, our method respectively increases 15.35% 
in relax relevant and 12.65% in rigid relevant 
measure. Figure 3 displays the PreAt10 values 
of each topic in relax relevant measure based on 
bi-gram indexing where one denotes the preci-
sion enhanced with our method, another denotes 
the normal precision. It is shown the precision of 
each topic is all improved by using our method. 
 
Fig. 3. PreAt10 of all topics in relax judgment 
In table 4, using words as indexing units, 
our method respectively increases 17.03% in 
relax relevant measure and 15.45% in rigid in 
PreAt10. In PreAt100 level, our method respec-
tively increases 14.05% in relax relevant meas-
ure and 11.96% in rigid. 
In our experiments, compared with the two 
Chinese indexing units: bi-gram and words, our 
method increases the average precision of all 
queries in top 10 and top 100 measure levels for 
about 17.1% and 13.5%. What lies behind our 
method is that for each topic, we manually select 
some Chinese corpus to acquire the lingual on-
tology knowledge, and can help us to focus on 
relevant documents. Our experiment also shows 
improper extract and corpus may decrease the 
precision of top documents. So our method de-
pends on right keywords in texts, queries and the 
corpus. 
3.2   Text similarity computing 
Text similarity is a measure for the matching 
degree between two or more texts, the more high 
the similarity degree is, the more the meaning of 
text expressing is closer, vice versa. Some pro-
posal methods include Vector Space Model P[16]P, 
Ontology-based P[17]P, Distributional Semantics 
model P[18]P. 
3.2.1   Strategy of similarity computation 
First, for two Chinese texts DBiB and DBjB, we re-
spectively extract k same feature words, if the 
same feature words in the two texts is less than k, 
we don?t compare their similarity. 
Second, acquire lingual ontology knowl-
edge every text by their feature words. 
Third, get the evaluation value of every text, 
i.e. respectively add up all the average co-
occurrence distance 
lC  to the same semantic 
relation pairs in two texts. 
Final, compute the similarity ratio of every 
two text DBi B and DBj B. The similarity ratio equals to 
the ratio of the similarity evaluation value of 
text DBi B and DBj B, if the ratio is in the threshold ?, 
then we think that text DBi B is similar to text DBj B. 
3.2.2   Experimental results and analysis 
We download four classes of text for testing 
from Sina, Yahoo, Sohu and Tom, which in-
clude 71 current affairs news, 68 sports news, 69 
IT news, 74 education news. 
For the test of current affairs texts, accord-
ing to the strategy of similarity computation, we 
choose five words as feature word. They are ??
?, ??, ??, ??, ???. In the texts, the 
word ???, ??? are all replaced by word ??
?? and other classes are similar. The testing 
result is shown in table 5.  
Table 5. Testing results for text similarity 
0.95<?<1.05 0.85<?<1.15 Items 
Precision Recall FB1 B-measure Precision Recall FB1 B-measure 
Current affairs news 97.14% 97.14% 97.14% 94.60% 100% 97.23% 
Sports News 88.57% 91.18% 89.86% 84.62% 97.06% 90.41% 
IT news 93.75% 96.77% 95.24% 91.18% 100% 95.39% 
Education news 94.74% 97.30% 96.00% 90.24 100% 94.87% 
General results 93.57% 95.62% 94.58% 90.07% 99.27% 94.42% 
 
17
We analyzed all the experimental results to 
find that the results for current affairs texts are 
the best, while the sports texts are lower than 
others. We think it is mainly because some 
sports terms are unprofessional for the lower 
sports texts recognition, such as ????, ??, 
???. Other feature words are more fixed and 
more concentrated. 
4 Conclusion 
In this paper, we presented a hybrid Chinese 
language model based on a combination of on-
tology with statistical method. We discuss the 
modeling and evaluate its performance. In the 
test about texts reordering, our experiences show 
that our method can increase the performance of 
Chinese information retrieval about 17.1% and 
13.5% at top 10 and top 100 documents measure 
level. In another test about texts similarity com-
puting, F1-measure is above 95%. 
On the other hand, in the current disposal 
of our information processing, we only make 
use of some characteristics ontology and use 
some co-occurrence information, such as seman-
tics, POS, context, position, distance, and etc. 
For the further research and experiment, we will 
be on the following: (1) Research on the charac-
teristics of relations between semantics and 
combine with some mature natural language 
processing techniques. (2) Research traditional 
ontology representation to keep up with interna-
tional stand. (3) Apply our key techniques to 
English information retrieval and cross-lingual 
information retrieval systems and study a 
general approach. 
References 
1. Jelinek, F. 1990. Self-organized language model-
ing for speech recognition. In Readings in Speech 
Recognition,A. Waibel and K. F. Lee, eds. Mor-
gan-Kaufmann, San Mateo, CA,1990, 450-506. 
2.  Brown, P., Pietra, S. D., Pietra, V. D., and Mercer, 
R. 1993. The mathematics of statistical machine-
translation: Parameter estimation. Computational 
Linguistics 19, 2 (1993), 269-311. 
3.  Croft, W. B. and Lafferty, J. (EDS.) 2003. Lan-
guage Modeling for Information Retrieval. Kluwer 
Academic,Amsterdam. 
4. Wang Xiaolong, Wang Kaizhu. 1994. Speech in-
put by sentence, Chinese Journal of Computers, 
17(2): 96-103 
5.  Zhou Ming, Huang Changning, Zhang Min, Bai 
Shuanhu, and Wu Sheng. 1994. A Chinese parsing 
model based on corpus, rules and statistics, Com-
puter research and development, 31(2):40-49 
6. R DeMori, M Federico. 1999. Language model 
adaptation. In: Keith Pointing ed. Computational 
Models of Speech Pattern Processing. NATO ASI 
Series. Berlin: Springer Verlag, 102-111 
7. R Kuhn , R D Mori. 1990. A cache-based natural 
language model for speech reproduction. IEEE 
Trans on Pattern Analysis and Machine Intelli-
gence, PAM2-12(6), 570-583 
8.  Daniel Gildea, Thomas Hofmannl. 1999. Topic-
based language models using EM1. In : Proceed-
ing of the 6th European Conf on Speech Commu-
nication and Technology, Budapest, Hungary: 
ESCA, 2167-2170 
9.  Neches R., Fikes R., Finin T., Gruber T., Patil R., 
Senator T., and Swartout W. R.. 1991. Enabling 
Technology for Knowledge Sharing. AI Magazine, 
12(3) :16~36 
10. Gruber, T. R. 1993. Toward principles for the 
design of ontologies used for knowledge sharing. 
International Workshop on Formal Ontology, Pa-
dova, Italy 
11. Uschold M. 1996. Building Ontologies-Towards 
A Unified Methodology. In expert systems 96 
12. Yang Lingpeng, Ji Donghong, TangLi. 2004. 
Document Re-ranking Based on Automatically 
Acquired Key Terms in Chinese Information Re-
trieval. In Proceedings of the COLING'2004, pp. 
480-486 
13. Kwok, K.L. 1997. Comparing Representation in 
Chinese Information Retrieval. In Proceeding of 
the ACM SIGIR-97, pp. 34-4 
14. Nie, J.Y., Gao, J., Zhang, J., Zhou, M. 2000. On 
the Use of Words and N-grams for Chinese Infor-
mation Retrieval. In Proceedings of the IRAL-
2000, pp. 141-148 
15. Robertson, S.E. and Walker, S. 2001. Microsoft 
Cambridge at TREC-9: Filtering track: In Pro-
ceeding of the TREC 2000, pages 361-369 
16. Salton, G., Buckley, C. Term weighting ap-
proaches in automatic text retrieval. Information 
Processing and Management, 1988, 24(5), 
pp.513?523 
17. Vladimir Oleshchuk, Asle Pedersen. Ontology 
Based Semantic Similarity Comparison of Docu-
ments, 14th International Workshop on Database 
and Expert Systems Applications, September, 
2003, pp.735-738 
18. Besancon, R., Rajman, M., Chappelier, J. C. Tex-
tual similarities based on a distributional approach, 
Tenth International Workshop on Database and 
Expert Systems Applications, 1-3 Sept. 1999, 
pp.180-184 
 
18
Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 213?216,
Suntec, Singapore, 4 August 2009. c?2009 ACL and AFNLP
Chinese Term Extraction Using Different Types of Relevance 
 
 
Yuhang Yang1, Tiejun Zhao1, Qin Lu2, Dequan Zheng1 and Hao Yu1
1School of Computer Science and Technology,  
Harbin Institute of Technology, Harbin 150001, China 
{yhyang,tjzhao,dqzheng,yu}@mtlab.hit.edu.cn 
2Department of Computing,  
The Hong Kong Polytechnic University, Hong Kong, China 
csluqin@comp.polyu.edu.hk 
 
  
 
Abstract 
This paper presents a new term extraction ap-
proach using relevance between term candi-
dates calculated by a link analysis based 
method. Different types of relevance are used 
separately or jointly for term verification. The 
proposed approach requires no prior domain 
knowledge and no adaptation for new domains. 
Consequently, the method can be used in any 
domain corpus and it is especially useful for 
resource-limited domains. Evaluations con-
ducted on two different domains for Chinese 
term extraction show significant improve-
ments over existing techniques and also verify 
the efficiency and relative domain independent 
nature of the approach. 
1 Introduction 
Terms are the lexical units to represent the most 
fundamental knowledge of a domain. Term ex-
traction is an essential task in domain knowledge 
acquisition which can be used for lexicon update, 
domain ontology construction, etc. Term extrac-
tion involves two steps. The first step extracts 
candidates by unithood calculation to qualify a 
string as a valid term. The second step verifies 
them through termhood measures (Kageura and 
Umino, 1996) to validate their domain specificity.  
Many previous studies are conducted on term 
candidate extraction. Other tasks such as named 
entity recognition, meaningful word extraction 
and unknown word detection, use techniques 
similar to that for term candidate extraction. But, 
their focuses are not on domain specificity. This 
study focuses on the verification of candidates by 
termhood calculation.  
Relevance between term candidates and docu-
ments is the most popular feature used for term 
verification such as TF-IDF (Salton and McGill, 
1983; Frank, 1999) and Inter-Domain Entropy 
(Chang, 2005), which are all based on the hy-
pothesis that ?if a candidate occurs frequently in 
a few documents of a domain, it is likely a term?. 
Limited distribution information of term candi-
dates in different documents often limits the abil-
ity of such algorithms to distinguish terms from 
non-terms. There are also attempts to use prior 
domain specific knowledge and annotated cor-
pora for term verification. TV_ConSem (Ji and 
Lu, 2007) calculates the percentage of context 
words in a domain lexicon using both frequency 
information and semantic information. However, 
this technique requires a domain lexicon whose 
size and quality have great impact on the per-
formance of the algorithm. Some supervised 
learning approaches have been applied to pro-
tein/gene name recognition (Zhou et al, 2005) 
and Chinese new word identification (Li et al, 
2004) using SVM classifiers (Vapnik, 1995) 
which also require large domain corpora and an-
notations. The latest work by Yang (2008) ap-
plied the relevance between term candidates and 
sentences by using the link analysis approach 
based on the HITS algorithm to achieve better 
performance. 
In this work, a new feature on the relevance 
between different term candidates is integrated 
with other features to validate their domain 
specificity. The relevance between candidate 
terms may be useful to identify domain specific 
terms based on two assumptions. First, terms are 
more likely to occur with other terms in order to 
express domain information. Second, term can-
didates extracted from domain corpora are likely 
213
to be domain specific. Previous work by (e.g. Ji 
and Lu, 2007) uses similar information by com-
paring the context to an existing large domain 
lexicon. In this study, the relevance between 
term candidates are iteratively calculated by 
graphs using link analysis algorithm to avoid the 
dependency on prior domain knowledge.  
The rest of the paper is organized as follows. 
Section 2 describes the proposed algorithms. 
Section 3 explains the experiments and the per-
formance evaluation. Section 4 concludes and 
presents the future plans. 
2 Methodology 
This study assumes the availability of term can-
didates since the focus is on term verification by 
termhood calculation. Three types of relevance 
are first calculated including (1) the term candi-
date relevance, CC; (2) the candidate to sentence 
relevance, CS; and the candidates to document 
relevance, CD. Terms are then verified by using 
different types of relevance. 
2.1 Relevance between Term Candidates 
Based on the assumptions that term candidates 
are likely to be used together in order to repre-
sent a particular domain concept, relevance of 
term candidates can be represented by graphs in 
a domain corpus. In this study, CC is defined as 
their co-occurrence in the same sentence of the 
domain corpus. For each document, a graph of 
term candidates is first constructed. In the graph, 
a node is a term candidate. If two term candi-
dates TC1 and TC2 occur in the same sentence, 
two directional links between TC1 to TC2 are 
given to indicate their mutually related. Candi-
dates with overlapped substrings are not removed 
which means long terms can be linked to their 
components if the components are also candi-
dates.  
After graph construction, the term candidate 
relevance, CC, is then iteratively calculated using 
the PageRank algorithm (Page et al 1998) origi-
nally proposed for information retrieval. PageR-
ank assumes that the more a node is connected to 
other nodes, it is more likely to be a salient node. 
The algorithm assigns the significance score to 
each node according to the number of nodes link-
ing to it as well as the significance of the nodes. 
The PageRank calculation PR of a node A is 
shown as follows:  
)
)(
)(
...
)(
)(
)(
)(
()1()(
2
2
1
1
t
t
BC
BPR
BC
BPR
BC
BPR
ddAPR ++++?=
(1) 
where B1, B2,?, Bt are all nodes linked to node A; 
C(Bi) is the number of outgoing links from node 
Bi; d is the factor to avoid loop trap in the 
graphic structure. d is set to 0.85 as suggested in 
(Page et al, 1998). Initially, all PR weights are 
set to 1. The weight score of each node are ob-
tained by (1), iteratively. The significance of 
each term candidate in the domain specific cor-
pus is then derived based on the significance of 
other candidates it co-occurred with. The CC 
weight of term candidate TCi is given by its PR 
value after k iterations, a parameter to be deter-
mined experimentally. 
2.2 Relevance between Term Candidates 
and Sentences 
A domain specific term is more likely to be con-
tained in domain relevant sentences. Relevance 
between term candidate and sentences, referred 
to as CS, is calculated using the TV_HITS (Term 
Verification ? HITS) algorithm proposed in 
(Yang et al, 2008) based on  Hyperlink-Induced 
Topic Search (HITS) algorithm (Kleinberg, 
1997). In TV_HITS, a good hub in the domain 
corpus is a sentence that contains many good 
authorities; a good authority is a term candidate 
that is contained in many good hubs.  
In TV_HITS, a node p can either be a sentence 
or a term candidate. If a term candidate TC is 
contained in a sentence Sen of the domain corpus, 
there is a directional link from Sen to TC. 
TV_HITS then makes use of the relationship be-
tween candidates and sentences via an iterative 
process to update CS weight for each TC.  
Let VA(w(p1)A, w(p2)A,?, w(pn)A) denote the 
authority vector and VH(w(p1)H, w(p2)H,?, w(pn)H) 
denote the hub vector. VA and VH are initialized 
to (1, 1,?, 1). Given weights VA and VH with a 
directional link p?q, w(q)A and w(p)H are up-
dated by using the I operation(an in-pointer to a 
node) and the O operation(an out-pointer to a 
node) shown as follows. The CS weight of term 
candidate TCi is given by its w(q)A value after 
iteration. 
I operation:          (2) ?
??
=
Eqp
HA w(p)w(q)
O operation:         (3) ?
??
=
Eqp
AH w(q)w(p)
2.3 Relevance between Term Candidates 
and Documents 
The relevance between term candidates and 
documents is used in many term extraction algo-
214
rithms. The relevance is measured by the TF-IDF 
value according to the following equations: 
)IDF(TC)TF(TC)TFIDF(TC iii ?=      (4) 
)
)(
log()(
i
i TCDF
D
TCIDF =             (5) 
where TF(TCi) is the number of times term can-
didate TCi occurs in the domain corpus, DF(TCi) 
is the number of documents in which TCi occurs 
at least once, |D| is the total number of docu-
ments in the corpus, IDF(TCi) is the inverse 
document frequency which can be calculated 
from the document frequency. 
2.4 Combination of Relevance 
To evaluate the effective of the different types of 
relevance, they are combined in different ways in 
the evaluation. Term candidates are then ranked 
according to the corresponding termhood values 
Th(TC) and the top ranked candidates are con-
sidered terms.  
For each document Dj in the domain corpus 
where a term candidate TCi occurs, there is CCij 
weight and a CSij weight. When features CC and 
CS are used separately, termhood ThCC(TCi) and 
ThCS(TCi) are calculated by averaging CCij and 
CSij, respectively. Termhood of different combi-
nations are given in formula (6) to (9). R(TCi) 
denotes the ranking position of TCi.  
)(TCR)(TCR
)(TCTh
iCSiCC
iCSCC
11 +=+    (6) 
)log()()(
Cj
ijiCDCC DF
D
CCTCTh ?=+     (7) 
)log()()(
Cj
ijiCDCS DF
D
CSTCTh ?=+     (8) 
)(TCR)(TCR
TCTh
iCDCSiCDCC
iCDCSCC
++
++ += 11)( (9) 
3 Performance Evaluation 
3.1 Data Preparation 
To evaluate the performance of the proposed 
relevance measures for Chinese in different do-
mains, experiments are conducted on two sepa-
rate domain corpora CorpusIT and CorpusLegal., 
respectively. CorpusIT includes academic papers 
of 6.64M in size from Chinese IT journals be-
tween 1998 and 2000. CorpusLegal includes the 
complete set of official Chinese constitutional 
law articles and Economics/Finance law articles 
of 1.04M in size (http://www.law-lib.com/).  
For comparison to previous work, all term 
candidates are extracted from the same domain 
corpora using the delimiter based algorithm 
TCE_DI (Term Candidate Extraction ? Delimiter 
Identification) which is efficient according to 
(Yang et al, 2008). In TCE_DI, term delimiters 
are identified first. Words between delimiters are 
then taken as term candidates. 
The performances are evaluated in terms of 
precision (P), recall (R) and F-value (F). Since 
the corpora are relatively large, sampling is used 
for evaluation based on fixed interval of 1 in 
each 10 ranked results. The verification of all the 
sampled data is carried out manually by two ex-
perts independently. To evaluate the recall, a set 
of correct terms which are manually verified 
from the extracted terms by different methods is 
constructed as the standard answer. The answer 
set is certainly not complete. But it is useful as a 
performance indication for comparison since it is 
fair to all algorithms. 
3.2 Evaluation on Term Extraction 
For comparison, three reference algorithms are 
used in the evaluation. The first algorithm is 
TV_LinkA which takes CS and CD into consid-
eration and performs well (Yang et al, 2008). 
The second one is a supervised learning ap-
proach based on a SVM classifier, SVMlight 
(Joachims, 1999). Internal and external features 
are used by SVMlight. The third algorithm is the 
popular used TF-IDF algorithm. All the refer-
ence algorithms require no training except 
SVMlight. Two training sets containing thousands 
of positive and negative examples from IT do-
main and legal domain are constructed for the 
SVM classifier. The training and testing sets are 
not overlapped. 
Table 1 and Table 2 show the performance of 
the proposed algorithms using different features 
for IT domain and legal domain, respectively. 
The algorithm using CD alone is the same as the 
TF-IDF algorithm. The algorithm using CS and 
CD is the TV_LinkA algorithm.  
Algorithms Precision 
(%) 
Recall 
(%) 
F-value 
(%) 
SVM 63.6 49.5 55.6 
CC 47.1 36.5 41.2 
CS 65.6 51 57.4 
CD(TF-IDF) 64.8 50.4 56.7 
CC+CS 80.4 62.5 70.3 
CC+CD 49 38.1 42.9 
CS+CD 
(TV_LinkA) 
75.4 58.6 66 
CC+CS+CD 82.8 64.4 72.4 
Table 1. Performance on IT Domain 
215
 Algorithms Precision 
(%) 
Recall 
(%) 
F-value 
(%) 
SVM 60.1 54.2 57.3 
CC 45.2 40.3 42.6 
CS 70.5 40.1 51.1 
CD(TF-IDF) 59.4 52.9 56 
CC+CS 64.2 49.9 56.1 
CC+CD 48.4 43.1 45.6 
CS+CD 
(TV_LinkA) 
67.4 60.1 63.5 
CC+CS+CD 70.2 62.6 66.2 
Table 2. Performance on Legal Domain 
Table 1 and Table 2 show that the proposed 
algorithms achieve similar performance on both 
domains. The proposed algorithm using all three 
features (CC+CS+CD) performs the best. The 
results confirm that the proposed approach are 
quite stable across domains and the relevance 
between candidates are efficient for improving 
performance of term extraction in different do-
mains. The algorithm using CC only does not 
achieve good performance. Neither does CC+CS. 
The main reason is that the term candidates used 
in the experiments are extracted using the 
TCE_DI algorithm which can extract candidates 
with low statistical significance. TCE_DI pro-
vides a better compromise between recall and 
precision. CC alone is vulnerable to noisy candi-
dates since it relies on the relevance between 
candidates themselves. However, as an addi-
tional feature to the combined use of CS and CD 
(TV_LinkA), improvement of over 10% on F-
value is obtained for the IT domain, and 5% for 
the legal domain. This is because the noise data 
are eliminated by CS and CD, and CC help to 
identify additional terms that may not be statisti-
cally significant.  
4 Conclusion and Future Work 
In conclusion, this paper exploits the relevance 
between term candidates as an additional feature 
for term extraction approach. The proposed ap-
proach requires no prior domain knowledge and 
no adaptation for new domains. Experiments for 
term extraction are conducted on IT domain and 
legal domain, respectively. Evaluations indicate 
that the proposed algorithm using different types 
of relevance achieves the best performance in 
both domains without training.  
In this work, only co-occurrence in a sentence 
is used as the relevance between term candidates. 
Other features such as syntactic relations can 
also be exploited. The performance may be fur-
ther improved by using more efficient combina-
tion strategies. It would also be interesting to 
apply this approach to other languages such as 
English. 
Acknowledgement: The project is partially sup-
ported by the Hong Kong Polytechnic University 
(PolyU CRG G-U297) 
References 
Chang Jing-Shin. 2005. Domain Specific Word Ex-
traction from Hierarchical Web Documents: A 
First Step toward Building Lexicon Trees from 
Web Corpora. In Proc of the 4th SIGHAN Work-
shop on Chinese Language Learning: 64-71. 
Eibe Frank, Gordon. W. Paynter, Ian H. Witten, Carl 
Gutwin, and Craig G. Nevill-Manning. 1999. Do-
main-specific Keyphrase Extraction. In Proc.of 
16th Int. Joint Conf. on AI,  IJCAI-99: 668-673. 
Joachims T. 2000. Estimating the Generalization Per-
formance of a SVM Efficiently. In Proc. of the Int 
Conf. on Machine Learning, Morgan Kaufman, 
2000. 
Kageura K., and B. Umino. 1996. Methods of auto-
matic term recognition: a review. Term 3(2):259-
289. 
Kleinberg J. 1997. Authoritative sources in a hyper-
linked environment. In Proc. of the 9th ACM-SIAM 
Symposium on Discrete Algorithms: 668-677. New 
Orleans, America, January 1997. 
Ji Luning, and Qin Lu. 2007. Chinese Term Extrac-
tion Using Window-Based Contextual Information. 
In Proc. of CICLing 2007, LNCS 4394: 62 ? 74. 
Li Hongqiao, Chang-Ning Huang, Jianfeng Gao, and 
Xiaozhong Fan. The Use of SVM for Chinese New 
Word Identification. In Proc. of the 1st Int.Joint 
Conf. on NLP (IJCNLP2004): 723-732. Hainan Is-
land, China, March 2004. 
Salton, G., and McGill, M.J. (1983). Introduction to 
Modern Information Retrieval. McGraw-Hill. 
S. Brin, L. Page. The anatomy of a large-scale hyper-
textual web search engine. The 7th Int. World Wide 
Web Conf, Brisbane, Australia, April 1998, 107-
117. 
Vladimir N. Vapnik. 1995. The Nature of Statistical 
Learning Theory. Springer, 1995. 
Yang Yuhang, Qin Lu, Tiejun Zhao. (2008). Chinese 
Term Extraction Using Minimal Resources. The 
22nd Int. Conf. on Computational Linguistics (Col-
ing 2008). Manchester, Aug., 2008, 1033-1040. 
Zhou GD, Shen D, Zhang J, Su J, and Tan SH. 2005. 
Recognition of Protein/Gene Names from Text us-
ing an Ensemble of Classifiers. BMC Bioinformat-
ics 2005, 6(Suppl 1):S7. 
216
Proceedings of the 2010 Named Entities Workshop, ACL 2010, pages 102?109,
Uppsala, Sweden, 16 July 2010. c?2010 Association for Computational Linguistics
Using Deep Belief Nets for Chinese Named Entity Categorization 
Yu Chen1, You Ouyang2, Wenjie Li2, Dequan Zheng1, Tiejun Zhao1 
1School of Computer Science and Technology, Harbin Institute of Technology, China 
{chenyu, dqzheng, tjzhao}@mtlab.hit.edu.cn 
2Department of Computing, The Hong Kong Polytechnic University, Hong Kong 
{csyouyang, cswjli}@comp.polyu.edu.hk
Abstract 
Identifying named entities is essential in 
understanding plain texts. Moreover, the 
categories of the named entities are indicative 
of their roles in the texts. In this paper, we 
propose a novel approach, Deep Belief Nets 
(DBN), for the Chinese entity mention 
categorization problem. DBN has very strong 
representation power and it is able to 
elaborately self-train for discovering 
complicated feature combinations. The 
experiments conducted on the Automatic 
Context Extraction (ACE) 2004 data set 
demonstrate the effectiveness of DBN. It 
outperforms the state-of-the-art learning 
models such as SVM or BP neural network. 
1 Introduction 
Named entities (NE) are defined as the names of 
existing objects, such as persons, organizations 
and etc. Identifying NEs in plain texts provides 
structured information for semantic analysis. 
Hence the named entity recognition (NER) task 
is a fundamental task for a wide variety of 
natural language processing applications, such as 
question answering, information retrieval and etc. 
In a text, an entity may either be referred to by a 
common noun, a noun phrase, or a pronoun. 
Each reference of the entity is called a mention. 
NER indeed requires the systems to identify 
these entity mentions from plain texts. The task 
can be decomposed into two sub-tasks, i.e., the 
identification of the entities in the text and the 
classification of the entities into a set of pre-
defined categories. In the study of this paper, we 
focus on the second sub-task and assume that the 
boundaries of all the entity mentions to be 
categorized are already correctly identified. 
In early times, NER systems are mainly based 
on handcrafted rule-based approaches. Although 
rule-based approaches achieved reasonably good 
results, they have some obvious flaws. First, they 
require exhausted handcraft work to construct a 
proper and complete rule set, which partially 
expressing the meaning of entity. Moreover, 
once the interest of task is transferred to a 
different domain or language, rules have to be 
revised or even rewritten. The discovered rules 
are indeed heavily dependent on the task 
interests and the particular corpus. Finally, the 
manually-formatted rules are usually incomplete 
and their qualities are not guaranteed. 
Recently, more attentions are switched to the 
applications of machine learning models with 
statistic information. In this camp, entity 
categorization is typically cast as a multi-class 
classification process, where the named entities 
are represented by feature vectors. Usually, the 
vectors are abstracted by some lexical and 
syntactic features instead of semantic feature. 
Many learning models, such as Support Vector 
Machine (SVM) and Neural Network (NN), are 
then used to classify the entities by their feature 
vectors. 
Entity categorization in Chinese attracted less 
attention when compared to English or other 
western languages. This is mainly because the 
unique characteristics of Chinese. One of the 
most common problems is the lack of boundary 
information in Chinese texts. For this problem, 
character-based methods are reported to be a 
possible substitution of word-based methods. As 
to character-based methods, it is important to 
study the implicit combination of characters.  
In our study, we explore the use of Deep 
Belief Net (DBN) in character-based entity 
categorization. DBN is a neural network model 
which is developed under the deep learning 
architecture. It is claimed to be able to 
automatically learn a deep hierarchy of the input 
features with increasing levels of abstraction for 
the complex problem. In our problem, DBN is 
used to automatically discover the complicated 
composite effects of the characters to the NE 
categories from the input data. With DBN, we 
need not to manually construct the character 
combination features for expressing the semantic 
relationship among characters in entities. 
Moreover, the deep structure of DBN enables the 
possibility of discovering very sophisticated 
102
combinations of the characters, which may even 
be hard to discover by human. 
The rest of this paper is organized as follow. 
Section 2 reviews the related work on name 
entity categorization. Section 3 introduces the 
methodology of the proposed approach. Section 
4 provides the experimental results. Finally, 
section 5 concludes the whole paper. 
2 Related work 
Over the past decades, NER has evolved from 
simple rule-based approaches to adapted self-
training machine learning approaches. 
As early rule-based approaches, MacDonald 
(1993) utilized local context, which implicate 
internal and external evidence, to aid on 
categorization. Wacholder (1997) employed an 
aggregation of classification method to capture 
internal rules. Both used hand-written rules and 
knowledge bases. Later, Collins (1999) adopted 
the AdaBoost algorithm to find a weighted 
combination of simple classifiers. They reported 
that the combination of simple classifiers can 
yield some powerful systems with much better 
performances. As a matter of fact, these methods 
all need manual studies on the construction of the 
rule set or the simple classifiers. 
Machine learning models attract more 
attentions recently. Usually, they train 
classification models based on context features. 
Various lexical and syntactic features are 
considered, such as N-grams, Part-Of-Speech 
(POS), and etc. Zhou and Su (2002) integrated 
four different kinds of features, which convey 
different semantic information, for a 
classification model based on the Hidden 
Markov Model (HMM). Koen (2006) built a 
classifier with the Conditional Random Field 
(CRF) model to classify noun phrases in a text 
with the WordNet SynSet. Isozaki and Kazawa 
(2002) studied the use of SVM instead. 
There were fewer studies in Chinese entity 
categorization. Guo and Jiang (2005) applied 
Robust Risk Minimization to classify the named 
entities. The features include seven traditional 
lexical features and two external-NE-hints based 
features. An important result they reported is that 
character-based features can be as good as word-
based features since they avoid the Chinese word 
segmentation errors. In (Jing et al, 2003), it was 
further reported that pure character-based models 
can even outperform word-based models with 
character combination features.  
Deep Belief Net is introduced in (Hinton et al, 
2006). According to their definition, DBN is a 
deep neural network that consists of one or more 
Restricted Boltzmann Machine (RBM) layers 
and a Back Propagation (BP) layer. This multi-
layer structure leads to a strong representation 
power of DBN. Moreover, DBN is quite efficient 
by using RBM to implement the middle layers, 
since RBM can be learned very quickly by the 
Contrastive Divergence (CD) approach. 
Therefore, we believe that DBN is very suitable 
for the character-level Chinese entity mention 
categorization approach. It can be used to solve 
the multi-class categorization problem with just 
simple binary features as the input. 
3 Deep Belief Network for Chinese 
Entity Categorization 
3.1 Problem Formalization 
An Entity mention categorization is a process of 
classifying the entity mentions into different 
categories. In this paper, we assume that the 
entity mentions are already correctly detected 
from the texts. Moreover, an entity mention 
should belong to one and only one predefined 
category. Formally, the categorization function 
of the name entities is 
( ( ))if V e C?            (1) 
where 
ie  is an entity mention from all the 
mention set E, ( )iV e  is the binary feature 
vector of 
ie , C={C1, C2, ?, CM} is the pre-
defined categories. Now the question is to find a 
classification function : Df R C?  which maps 
the feature vector V(ei) of an entity mention to its 
category. Generally, this classification function 
is learned from training data consisting of entity 
mentions with labeled categories. The learned 
function is then used to predict the category of 
new entity mentions by their feature vectors. 
3.2 Character-based Features 
As mentioned in the introduction, we intend to 
use character-level features for the purpose of 
avoiding the impact of the Chinese word 
segmentation errors. Denote the character 
dictionary as D={d1, d2, ?, dN}. To an e, it?s 
feature vector is V(e)={ v1, v2, ?, vN }. Each unit 
vi can be valued as Equation 2. 
??
??
?
?
??
      0
    1
ed
edv
i
i
i
          (2) 
103
For example, there is an entity mention ??
? ?Clinton?. So its feature vector is a vector 
with the same length as the character dictionary, 
in which all the dimensions are 0 except the three 
dimensions standing for ?, ?, and ?. The 
representation is clearly illustrated in Figure 1 
below. Since our objective is to test the 
effectiveness of DBN for this task. Therefore, we 
do not involve any other feature. 
 
Fig. 1. Generating the character-level features 
Characters compose the named entity and 
express its meaning. As a matter of fact, the 
composite effect of the characters to the 
mention category is quite complicated. For 
example, ?? ?Mr. Li? and ?? ?Laos? both 
have character ?, but ?? ?Mr. Li? indicates 
a person but ?? ?Laos? indicates a country. 
These are totally different NEs. Another 
example is ????? ?Capital of Paraguay? 
and ??? ?Asuncion?. They are two entity 
mentions point to the same entity despite that 
the two entities do not have any common 
characters. In such case, independent character 
features are not sufficient to determine the 
categories of the entity mentions. So we should 
also introduce some features which are able to 
represent the combinational effects of the 
characters. However, such kind of features is 
very hard to discover. Meanwhile, a complete 
set of combinations is nearly impossible to be 
found manually due to the exponential number 
of all the possible combinations. As in our 
study, we adopt DBN to automatically find the 
character combinations.  
3.3 Deep Belief Nets 
Deep Belief Network (DBN) is a complicated 
model which combines a set of simple models 
that are sequentially connected (Ackley, 1985). 
This deep architecture can be viewed as multiple 
layers. In DBN, upper layers are supposed to 
represent more ?abstract? concepts that explain 
the input data whereas lower layers extract ?low-
level features? from the data. DBN often consists 
of many layers, including multiple Restricted 
Boltzmann Machine (RBM) layers and a Back 
Propagation (BP) layer.  
 
Fig. 2.  The structure of a DBN. 
As illustrated in Figure 2, when DBN receives 
a feature vector, the feature vector is processed 
from the bottom to the top through several RBM 
layers in order to get the weights in each RBM 
layer, maintaining as many features as possible 
when they are transferred to the next layer. RBM 
deals with feature vectors only and omits the la-
bel information. It is unsupervised. In addition, 
each RBM layer learns its parameters indepen-
dently. This makes the parameters optimal for 
the relevant RBM layer but not optimal for the 
whole model. To solve this problem, there is a 
supervised BP layer on top of the model which 
fine-tunes the whole model in the learning 
process and generates the output in the inference 
process. After the processing of all these layers, 
the final feature vector consists of some sophisti-
cated features, which reflect the structured in-
formation among the original features. With this 
new feature vector, the classification perfor-
mance is better than directly using the original 
feature vector. 
None of the RBM is capable of guaranteeing 
that all the information conveyed to the output is 
accurate or important enough. However the 
learned information produced by preceding RBM 
layer will be continuously refined through the 
next RBM layer to weaken the wrong or insigni-
ficant information in the input. Each layer can 
detect feature in the relevant spaces. Multiple 
layers help to detect more features in different 
spaces. Lower layers could support object detec-
tion by spotting low-level features indicative of 
object parts. Conversely, information about ob-
jects in the higher layers could resolve lower-
level ambiguities. The units in the final layer 
share more information from the data. This in-
creases the representation power of the whole 
model. It is certain that more layers mean more 
computation time. 
104
DBN has some attractive features which make 
it very suitable for our problem. 
1) The unsupervised process can detect the 
structures in the input and automatically ob-
tain better feature vectors for classification. 
2) The supervised BP layer can modify the 
whole network by back-propagation to im-
prove both the feature vectors and the classi-
fication results. 
3) The generative model makes it easy to in-
terpret the distributed representations in the 
deep hidden layers. 
4) This is a fast learning algorithm that can 
find a fairly good set of parameters quickly 
and can ensure the efficiency of DBN. 
3.3.1 Restricted Boltzmann Machine (RBM) 
In this section, we will introduce RBM, which is 
the core component of DBN. RBM is Boltzmann 
Machine with no connection within the same 
layer. An RBM is constructed with one visible 
layer and one hidden layer. Each visible unit in 
the visible layer V  is an observed variable iv  
while each hidden unit in the hidden layer H  is 
a hidden variable 
jh
. Its joint distribution is 
( , ) exp( ( , )) T T Th Wv b x c hp v h E v h e ? ?? ? ? (3) 
In RBM, the parameters that need to be esti-
mated are ( , , )W b c? ?  and 2( , ) {0,1}v h ? . 
To learn RBM, the optimum parameters are 
obtained by maximizing the above probability on 
the training data (Hinton, 1999). However, the 
probability is indeed very difficult in practical 
calculation. A traditional way is to find the gra-
dient between the initial parameters and the re-
spect parameters. By modifying the previous pa-
rameters with the gradient, the expected parame-
ters can gradually approximate the target para-
meters as 
0
( 1) ( ) ( )
W
P vW W W ?
? ? ?? ?? ? ?
 (4) 
where ?  is a parameter controlling the leaning 
rate. It determines the speed of W converging to 
the target. 
Traditionally, the Markov chain Monte Carlo 
method (MCMC) is used to calculate this kind of 
gradient. 
0 0log ( , )p v h h v h vw ? ?
? ? ??       
(5) 
where log ( , )p v h  is the log probability of the 
data. 
0 0h v
 denotes the multiplication of the av-
erage over the data states and its relevant sample 
in hidden unit. 
h v? ?
 denotes the multiplication 
of the average over the model states in visible 
unit and its relevant sample in hidden unit. 
However, MCMC requires estimating an ex-
ponential number of terms. Therefore, it typically 
takes a long time to converge to 
h v? ?
. Hinton 
(2002) introduced an alternative algorithm, i.e., 
the contrastive divergence (CD) algorithm, as a 
substitution. It is reported that CD can train the 
model much more efficiently than MCMC. To 
estimate the distribution ( )p x , CD considers a 
series of distributions { ( )np x } which indicate the 
distributions in n steps. It approximates the gap 
of two different Kullback-Leiler divergences 
(Kullback, 1987) as 
0( || ) ( || )n nCD KL p p KL p p? ?? ?     (6) 
Maximizing the log probability of the data is 
exactly the same as minimizing the Kullback?
Leibler divergence between the distribution of 
the data 
0p  and the equilibrium distribution p?  
defined by the model. In each step, the gap is 
approximately minimized so that we can obtain 
the final distribution which has the smallest 
Kullback-Leiler divergence with the fantasy dis-
tribution.  
After n steps, the gradient can be estimated 
and used in Equation 4 to adjust the weights of 
RBM. In our experiments, we set n to be 1. It 
means that in each step of gradient calculation, 
the estimate of the gradient is used to adjust the 
weight of RBM. In this case, the estimate of the 
gradient is just the gap between the products of 
the visual layer and the hidden layer, i.e., 
0 0 1 1log ( , )p v h h v h vW
? ? ??
 (7) 
Figure 3 below illustrates the process of learning 
RBM with CD-based gradient estimation. 
 
105
Fig. 3.  Learning RBM with CD-based gradient 
estimation 
3.3.2 Back-propagation (BP) 
The RBM layers provide an unsupervised analy-
sis on the structures of data set. They automati-
cally detect sophisticated feature vectors. The 
last layer in DBN is the BP layer. It takes the 
output from the last RBM layer and applies it in 
the final supervised learning process. In DBN, 
not only is the supervised BP layer used to gen-
erate the final categories, but it is also used to 
fine-tune the whole network. Specifically speak-
ing, when the parameters in BP layer are 
changed during its iterating process, the changes 
are passed to the other RBM layers in a top-to-
bottom sequence. 
The BP algorithm has a feed-forward step and 
a back-propagation step. In the feed-forward step, 
the input values are propagated to obtain the out-
put values. In the back-propagation step, the out-
put values are compared to the real category la-
bels and used them to modify the parameters of 
the model. We consider the weight
ijw  
which 
indicates the edge pointing from the i-th node in 
one RBM layer to the j-th node in its upper layer. 
The computation in feed-forward is 
i ijo w , 
where 
io  is the stored output for the unit i. In 
the back-propagation step, we compute the error 
E in the upper layers and also the gradient with 
respect to this error, i.e., 
i ijE o w? ?
. Then the 
weight
ijw  
will be adjusted by the gradient des-
cent. 
ij i i j
i ij
Ew o oo w? ? ?
?? ? ? ? ??
 (8) 
where ??  is used to control the length of the 
moving step. 
3.3.3 DBN-based Entity Mention Categori-
zation 
For each entity mention, it is represented by the 
character feature vector as introduced in section 
3.2 and then fed to DBN. The training procedure 
can be divided into two phases. The first phase is 
the parameter estimation process of the RBMs on 
all the inputted feature vectors. When a feature 
vector is fed to DBN, the first RBM layer is 
adjusted automatically according to this vector. 
After the first RBM layer is ready, its output 
becomes the input of the second RBM layer. The 
weights of the second RBM layer are also 
adjusted. The similar procedure is carried out on 
all the RBM layers. Then DBN will operates in 
the second phase, the back-propagation 
algorithm. The labeled categories of the entity 
mention are used to tune the parameters of the 
BP layer. Moreover, the changes of the BP layer 
are also fed back to the RBM layers. The 
procedure will iterate until the terminating 
condition is met. It can be a fixed number of 
iterations or a pre-given precision threshold. 
Once the weights of all the layers in DBN are 
obtained, the estimated model could be used to 
prediction. 
 
Fig. 4.  The mention categorization process 
of DBN 
Figure 4 illustrates the classification process of 
DBN. In prediction, for an entity mention e, we 
first calculate its feature vector V(e) and used as 
the input of DBN. V(e) is passed through all the 
layers to get the outputs for all RBM layers and 
last back-propagation layer. In the ith RBM layer, 
the dimensions in the input vector Vinput_i(e) are 
combined to yield the dimensions of the next 
feature vector Voutput_i(e) as input of the next layer. 
After the feature vector V(e) goes through all the 
RBM layers, it is indeed transformed to another 
feature vector V?(e) which consists of 
complicated combinations of the original 
character features and contains rich structured 
information between the characters. This feature 
vector is then fed into the BP layer to get the 
final category c(e). 
4 Experiments 
4.1 Experiment Setup 
In our experiment, we use the ACE 2004 corpus 
to evaluate our approach. The objective of this 
study is that the correctly detected Chinese entity 
mentions categorization using DBN from the text 
and figure out the suitability of DBN on this task. 
Moreover, an entity mention should belong to 
one and only one category. 
106
According to the guideline of the ACE04 task, 
there are five categories for consideration in total, 
i.e., Person, Organization, Geo-political entity, 
Location, and Facility. Moreover, each entity 
mention is expressed in two forms, i.e., the head 
and the extent. For example, ??????? 
?President Clinton of USA? is the extent of an 
entity mention and ???  ?Clinton? is the 
corresponding head. The two phrases both point 
to a named entity whose name is Clinton and he 
is the president of USA.  Here we make the 
?breakdown? strategy mentioned in Li et al 
(2007) that only the entity head is considered to 
generate the feature vector, considering that the 
information from the entity head refines the 
name entity. Although the entity extent includes 
more information, it also brings many noises 
which may make the learning process much 
more difficult. 
   In our experiments, we test the machine 
learning models under a 4-flod cross-validation. 
All entity mentions are divided into four parts 
randomly where three parts are used for training 
and one for test. In total, 7746 mentions are used 
for training and 2482 mentions are used for 
testing at each round. Precision is chosen as the 
evaluation criterion, calculated by the proportion 
of the number of correctly categorized instances 
and the number of total instances. Since all the 
instances should be classified, the recall value is 
equal to the precision value. 
4.2 Evaluation on Named Entity categoriza-
tion 
First of all, we provide some statistics of the data 
set. The distribution of entity mentions in each 
category is given in table 1. The size of the 
character dictionary in the corpus is 1185, so 
does the dimension of each feature vector. 
Type Quantity 
Person 4197 
Organization 1783 
Geo-political entity 287 
Location 3263 
Facility 399 
Table 1.  Number of entity mentions in each 
category 
In the first experiment, we compare the 
performance of DBN with some popular 
classification algorithms, including Support 
Vector Machine (labeled by SVM) and a 
traditional BP neutral network (labeled by NN 
(BP)). To implement the models, we use the 
LibSVM toolkit1 for SVM and the neural neutral 
network toolbox in Matlab2 for BP. The DBN in 
this experiment includes two RBM layers and 
one BP layer. Results of the first experiment are 
given in Table 2.  
Learning Model Precision 
DBN 91.45% 
SVM 90.29% 
NN(BP) 87.23% 
Table 2.  Performances of the systems with 
different classification models 
In this experiment, the DBN has three RBM 
layers and one BP layer. And the numbers of 
units in each RBM layer are 900, 600 and 300 
respectively. NN (BP) has the same structure as 
DBN. As for SVM, we choose the linear kernel 
with the penalty parameter C=1 and set the other 
parameters as default after comparing different 
kernels and parameters. 
In the results, DBN achieved better 
performance than both SVM and BP neural 
network. This clearly proved the advantages of 
DBN. The deep architecture of DBN yields 
stronger representation power which makes it 
able to detect more complicated and efficient 
features, thus better performance is achieved.  
In the second experiment, we intend to 
examine the performance of DBN with different 
number of RBM layers, from one RBM layer 
plus one BP layer to three RBM layers plus one 
BP layer. The amount of the units in the first 
RBM layer is set 900 and the amount in the 
second RBM layer is 600, if the second layer 
exists. As for the third RBM layers, the amount 
of units is set to 300. 
Construction of Neural Network Precision 
Three RBMs and One BP 91.45% 
Two RBMs and One BP 91.42% 
One RBM and one BP 91.05% 
Table 3.  Performance of DBNs with different 
number s of RBM layers 
Results in Table 3 show that the performance 
tends to be better when more RBM layers are 
incorporated. More RBM layers do enhance the 
representation power of DBN. However, it is 
also noted that the improvement is not significant 
from two layers to three layers. The reason may 
                                                 
1 available at http://www.csie.ntu.edu.tw/~cjlin/libsvm/ 
2 available at 
http://www.mathworks.com/access/helpdesk/help/toolbox
/nnet/backprop.html 
107
be that two-RBM DBN already has enough 
representation power for modeling this data set 
and thus one more RBM layer brings 
insignificant improvement. It is also mentioned 
in Hinton (2006) that more than three RBM 
layers are indeed not necessary. Another 
important result in Table 3 is that the DBN with 
One RBM and one BP performs much better than 
the neutral network with only BP in Table 1. 
This clearly showed the effectiveness of feature 
combination by the RBM layer again. 
As to the amount of units in each RBM layer, 
it is manually fixed in upper experiments. This 
number certainly affects the representation 
power of an RBM layer, consequently the 
representation power of the whole DBN. In this 
set of experiment, we intend to study the 
effectiveness of the unit size to the performance 
of DBN. A series of DBNs with only one RBM 
layer and different unit numbers for this RBM 
layer is evaluated. The results are provided in 
Table 4 below. 
Construction of Neural Network Precision 
one RBM(300 units) + one BP 90.61% 
one RBM(600 units) + one BP 90.69% 
one RBM(900 units) + one BP 91.05% 
one RBM(1200 units) + one BP 90.98% 
one RBM(1500 units) + one BP 90.61% 
one RBM(1800 units) + one BP 90.57% 
Table 4.  Performance of One-RBM DBNs 
with different number of units 
Based on the results, we can see that the 
performance is quite stable with different unit 
numbers. But the numbers that are closer to the 
original feature size seem to be some better. This 
could suggest that we should not decrease or 
increase the dimension of the vector feature too 
much when casting the vector transformation by 
RBM layers. 
Finally, we show the results of the individual 
categories. For each category, the Precision-
Recall-F values are provided in table 5, in which 
the F-measure is calculated by 
2*Precision*Recall-measure= Precision+RecallF
    (9) 
Type P R F 
Person 91.26% 96.26% 93.70% 
Organization 89.86% 89.04% 89.45% 
Location 77.58% 59.21% 76.17% 
Geo-political 
entity 
93.60% 91.89% 92.74% 
Facility 77.43% 63.72% 69.91% 
Table 5.  Performances of the system on each 
category 
5 Conclusions 
In this paper we presented our recent work on 
applying a novel machine learning model, the 
Deep Belief Nets, on Chinese entity mention 
categorization. It is demonstrated that DBN is 
very suitable for character-level mention 
categorization approaches due to its strong 
representation power and the ability on 
discovering complicated feature combinations. 
We conducted a series of experiments to prove 
the benefits of DBN. Experimental results 
clearly showed the advantages of DBN that it 
obtained better performance than existing 
approaches such as SVM and traditional BP 
neutral network. 
References  
David Ackley, Geoffrey Hinton, and Terrence 
Sejnowski. 1985. A learning algorithm for 
Boltzmann machines. Cognitive Science. 9. 
David MacDonald. 1993. Internal and external 
evidence in the identification and semantic 
categorization of proper names. Corpus 
Processing for Lexical Acquisition, MIT Press, 61-
76. 
Geoffrey Hinton. 1999. Products of experts. In 
Proceedings of the Ninth International. 
Conference on Artificial Neural Networks 
(ICANN). Vol. 1, 1?6. 
Geoffrey Hinton. 2002. Training products of experts 
by minimizing contrastive divergence. Neural 
Computation, 14, 1771?1800. 
Geoffrey Hinton, Simon Osindero, and Yee-Whey 
Teh. 2006. A fast learning algorithm for deep 
belief nets. Neural Computation. 18, 1527?1554 . 
GuoDong Zhou and Jian Su. 2002. Named entity 
recognition using an hmm-based chunk tagger. In 
proceedings of ACL. 473-480. 
Hideki Isozaki and Hideto Kazawa. 2002. Efficient 
support vector classifiers for named entity 
recognition. In proceedings of IJCNLP. 1-7. 
Honglei Guo, Jianmin Jiang, Guang Hu and Tong 
Zhang. 2005. Chinese named entity recognition 
based on multilevel linguistics features. In 
pr ceedings of IJCNLP. 90-99. 
Jing, Hongyan, Radu Florian, Xiaoqiang Luo, Tong 
Zhang and Abraham Ittycheriah. 2003. How to get 
a Chinese name (entity): Segmentation and 
combination issues. In proceedings of EMNLP. 
200-207. 
Koen Deschacht and Marie-Francine Moens. 2006, 
Efficient Hierarchical Entity Classifier Using 
Conditional Random Field. In Proceedings of the 
108
2nd Workshop on Ontology Learning and 
Population. 33-40. 
Michael Collins and Yoram Singer. 1999. 
Unsupervised models for named entity 
classification. In Proceedings of EMNLP'99. 
Nina Wacholder, Yael Ravin and Misook Choi. 1997. 
Disambiguation of Proper Names in Text. In 
Proceedings of the Fifth Conference on Applied 
Natural Language Processing. 
Solomon Kullback. 1987. Letter to the Editor: The 
Kullback-Leibler distance. The American 
Statistician 41 (4): 340?341. 
Wenjie Li and Donglei Qian. 2007. Detecting, 
Categorizing and Clustering Entity Mentions in 
Chinese Text, in Proceedings of the 30th Annual 
International ACM SIGIR Conference (SIGIR?07). 
647-654. 
Yoshua Bengio and Yann LeCun. 2007. Scaling 
learning algorithms towards ai. Large-Scale Ker-
nel Machines. MIT Press. 
 
109
Exploring Deep Belief Network for Chinese Relation Extraction 
Yu Chen1, Wenjie Li2, Yan Liu2, Dequan Zheng1, Tiejun Zhao1 
1School of Computer Science and Technology, Harbin Institute of Technology, China 
{chenyu, dqzheng, tjzhao}@mtlab.hit.edu.cn 
2Department of Computing, The Hong Kong Polytechnic University, Hong Kong 
{cswjli, csyliu}@comp.polyu.edu.hk 
Abstract 
Relation extraction is a fundamental 
task in information extraction that 
identifies the semantic relationships 
between two entities in the text. In this 
paper, a novel model based on Deep 
Belief Network (DBN) is first 
presented to detect and classify the 
relations among Chinese entities. The 
experiments conducted on the 
Automatic Content Extraction (ACE) 
2004 dataset demonstrate that the 
proposed approach is effective in 
handling high dimensional feature 
space including character N-grams, 
entity types and the position 
information. It outperforms the state-
of-the-art learning models such as 
SVM or BP neutral network. 
1 Introduction 
Information Extraction (IE) is to automatically 
pull out the structured information required by 
the users from a large volume of plain text. It 
normally includes three sequential tasks, i.e., 
entity extraction, relation extraction and event 
extraction. In this paper, we limit our focus on 
relation extraction.  
In early time, pattern-based approaches were 
the main focus of most research studies in 
relation extraction. Although pattern-based 
approaches achieved reasonably good results, 
they have some obvious flaws. It requires 
expensive handcraft work to assemble patterns 
and not all relations can be identified by a set 
of reliable patterns (Willy Yap, 2009). Also, 
once the interest of task is transferred to a 
different domain or a different language, 
patterns have to be revised or even rewritten. 
That is to say, the discovered patterns are 
heavily dependent on the task in a specific 
domain or on a particular corpus. 
Naturally, a vast amount of work was spent 
on feature-based machine learning approaches 
in later years. In this camp, relation extraction 
is typically cast as a classification problem, 
where the most important issue is to train a 
model to scale and measure the similarity of 
features reflecting relation instances. The 
entity semantic information expressing relation 
was often formulated as the lexical and 
syntactic features, which are identical to a 
certain linear vector in high dimensions. Many 
learning models are capable of self-training 
and classifying these vectors according to 
similarity, such as Support Vector Machine 
(SVM) and Neural Network (NN).  
Recently, kernel-based approaches have 
been developing rapidly. These approaches 
involved kernels of structure representations, 
like parse tree or dependency tree, in similarity 
calculation. In fact, feature-based approaches 
can be viewed as the special and simplified 
kinds of kernel-based approaches. They used 
dot-product as the kernel function and did not 
range over the intricate structure information 
(Ji, et al 2009). 
Relation extraction in Chinese received 
quite limited attention as compared to English 
and other western languages. The main reason 
is the unique characteristic of Chinese, such as 
more flexible grammar, lack of boundary 
information and morphological variations etc 
(Sun and Dong, 2009). Especially, the existing 
Chinese syntactic analysis tools at current 
stage are not yet reliable to capture the 
valuable structured information. It is urgent to 
develop approaches that are in particular 
suitable for Chinese relation extraction. 
In this paper, we explore the use of Deep 
Belief Network (DBN), a new feature-based 
machine learning model for Chinese relation 
extraction. It is a neural network model 
developed under the deep learning architecture 
that is claimed by Hinton (2006) to be able to 
automatically learn a deep hierarchy of 
features with increasing levels of abstraction 
for the complex problems like natural language 
processing (NLP). It avoids assembling 
patterns that express the semantic relation 
information and meanwhile it succeeds to 
produce accurate model that is not confined to 
the parsing results.  
The rest of this paper is structured in the 
following manner. Section 2 reviews the 
previous work on relation extraction. Section 3 
presents task definition, briefly introduces the 
DBN model and the feature construction. 
Section 4 provides the experimental results. 
Finally, Section 5 concludes the paper.  
2 Related Work 
Over the past decades, relation extraction had 
come to a significant progress from simple 
pattern-based approaches to adapted self-
training machine learning approaches. 
Brin (1998) used Dual Iterative Pattern 
Relation Expansion, a bootstrapping-based 
system, to find the largest common substrings 
as patterns. It had the ability of searching 
patterns automatically and was good for large 
quantity of uniform contexts. Chen (2006) 
proposed graph algorithm called label 
propagation, which transferred the pattern 
similarity to probability of propagating the 
label information from any vertex to its nearby 
vertices. The label matrix indicated the relation 
type. 
Feature-based approaches utilized the linear 
vector of carefully chosen lexical and syntactic 
features derived from different levels of text 
analysis and ranging from part-of-speech (POS) 
tagging to full parsing and dependency parsing 
(Zhang 2009). Jing and Zhai (2007) defined a 
unified graphic representation of features that 
served as a general framework in order to 
systematically explore the information at 
diverse levels in three subspaces and finally 
estimated the effectiveness of these features. 
They reported that the basic unit feature was 
generally sufficient to achieve state-of-art 
performance. Meanwhile, over-inclusion 
complex features were harmful. 
Kernel-based approaches utilize kernel 
functions on structures between two entities, 
such as sequences and trees, to measure the 
similarity between two relation instances. 
Zelenok (2003) applied parsing tree kernel 
function to distinguish whether there was an 
existing relationship between two entities. 
However, they limited their task on Person-
affiliation and organization-location.  
The previous work mainly concentrated on 
relation extraction in English. Relatively, less 
attention was drawn on Chinese relation 
extraction. However, its importance is being 
gradually recognized. For instance, Zhang et al 
(2008) combined position information, entity 
type and context features in a feature-based 
approach and Che (2005) introduced the edit 
distance kernel over the original Chinese string 
representation.  
DBN is a new feature-based approach for 
NLP tasks. According to the work by Hinton 
(2006), DBN consisted of several layers 
including multiple Restricted Boltzmann 
Machine (RBM) layers and a Back 
Propagation (BP) layer. It was reported to 
perform very well in many classification 
problems (Ackley, 1985), which is from the 
origin of its ability to scale gracefully and be 
computationally tractable when applied to high 
dimensional feature vectors. Furthermore, to 
against the combinations of feature were 
intricate, it detected invariant representations 
from local translations of the input by deep 
architecture.  
3 Deep Belief Network for Chinese 
Relation Extraction 
3.1 Task Definition 
Relation extraction, promoted by the 
Automatic Content Extraction (ACE) program, 
is a task of finding predefined semantic 
relations between pairs of entities from the 
texts. According to the ACE program, an entity 
is an object or a set of objects in the world 
while a relation is an explicitly or implicitly 
stated relationship between entities. The task 
can be formalized as:  
1 2( , , )e e s r?       (1) 
where 
1e  and 2e  are the two entities in a 
sentence s  under concern and r  is the relation 
between them. We call the triple 
1 2( , , )e e s  the 
relation candidate. According to the ACE 2004 
guideline 1 , five relation types are defined. 
They are: 
Role: it represents an affiliation between a 
Person entity and an Organization, Facility, 
or GPE (a Geo-political entity) entities. 
Part: it represents the part-whole relationship 
between Organization, Facility and GPE 
entities. 
At: it represents that a Person, Organization, 
GPE, or Facility entity is location at a 
Location entities. 
Near: it represents the fact that a Person, 
Organization, GPE or Facility entity is near 
(but not necessarily ?At?) a Location or 
GPE entities. 
Social: it represents personal and professional 
affiliations between Person entities. 
3.2 Deep Belief Networks (DBN) 
DBN often consists of several layers, 
including multiple RBM layers and a BP layer. 
As illustrated in Figure 1, each RBM layer 
learns its parameters independently and 
unsupervisedly. RBM makes the parameters 
optimal for the relevant RBM layer and detect 
complicated features, but not optimal for the 
whole model. There is a supervised BP layer 
on top of the model which fine-tunes the whole 
model in the learning process and generates the 
output in the inference process. RBM keeps 
information as more as possible when it 
transfers vectors to next layer. It makes 
networks to avoid local optimum. RBM is also 
adopted to ensure the efficiency of the DBN 
model. 
 
Fig. 1.  The structure of a DBN. 
                                                 
1 available at http://www.nist.gov/speech/tests/ace/. 
Deep architecture of DBN represents many 
functions compactly. It is expressible by 
integrating different levels of simple functions 
(Y. Bengio and Y. LeCun). Upper layers are 
supposed to represent more ?abstract? concepts 
that explain the input data whereas lower 
layers extract ?low-level features? from the 
data. In addition, none of the RBM guarantees 
that all the information conveyed to the output 
is accurate or important enough. The learned 
information produced by preceding RBM layer 
will be continuously refined through the next 
RBM layer to weaken the wrong or 
insignificant information in the input. Multiple 
layers filter valuable features. The units in the 
final layer share more information from the 
data. This increases the representation power 
of the whole model. The final feature vectors 
used for classification consist of sophisticated 
features which reflect the structured 
information, promote better classification 
performance than direct original feature vector.  
3.3 Restricted Boltzmann Machine (RBM) 
In this section, we will introduce RBM, which 
is the core component of DBN. RBM is 
Boltzmann Machine with no connection within 
the same layer. An RBM is constructed with 
one visible layer and one hidden layer. Each 
visible unit in the visible layer V  is an 
observed variable 
iv  while each hidden unit in 
the hidden layer H  is a hidden variable 
jh
. Its 
joint distribution is 
( , ) exp( ( , )) T T Th Wv b x c hp v h E v h e ? ?? ? ? (2) 
In RBM, 2( , ) {0,1}v h ? and ( , , )W b c? ? are 
the parameters that need to be estimated?W  
is the weight tying visible layer and hidden 
layer. b is the bias of units v and c is the bias of 
units h. 
To learn RBM, the optimum parameters are 
obtained by maximizing the joint distribution 
( , )p v h  on the training data (Hinton, 1999). A 
traditional way is to find the gradient between 
the initial parameters and the expected 
parameters. By modifying the previous 
parameters with the gradient, the expected 
parameters can gradually approximate the 
target parameters as 
0
( 1) ( ) log ( )
W
P vW W W ?
? ? ?? ?? ? ?
 (3) 
where ?  is a parameter controlling the leaning 
rate. It determines the speed of W converging 
to the target. 
Traditionally, the Monte Carlo Markov 
chain (MCMC) is used to calculate this kind of 
gradient. 
0 0log ( , )p v h h v h vw ? ?
? ? ??       
(4) 
where log ( , )p v h  is the log probability of the 
data. 
0 0h v
 denotes the multiplication of the 
average over the data states and its relevant 
sample in hidden unit. 
h v? ?
 denotes the 
multiplication of the average over the model 
states in visible units and its relevant sample in 
hidden units. 
  
Fig. 2.  Learning RBM with CD-based 
gradient estimation 
However, MCMC requires estimating an 
exponential number of terms. Therefore, it 
typically takes a long time to converge to 
h v? ?
. Hinton (2002) introduced an alternative 
algorithm, i.e., the contrastive divergence (CD) 
algorithm, as a substitution. It is reported that 
CD can train the model much more efficiently 
than MCMC. To estimate the distribution ( )p x , 
CD considers a series of distributions { ( )np x } 
which indicate the distributions in n steps. It 
approximates the gap of two different 
Kullback-Leiler divergences as 
0( || ) ( || )n nCD KL p p KL p p? ?? ?     (5) 
Maximizing the log probability of the data is 
exactly the same as minimizing the Kullback?
Leibler divergence between the distribution of 
the data 
0p  and the equilibrium distribution 
p?  defined by the model.  
In our experiments, we set n to be 1. It 
means that in each step of gradient calculation, 
the estimate of the gradient is used to adjust 
the weight of RBM as Equation 6.  
0 0 1 1log ( , )p v h h v h vW
? ? ??
 (6) 
Figure 2 below illustrates the process of 
learning RBM with CD-based gradient 
estimation. 
3.4 Back-Propagation (BP) 
The RBM layers provide an unsupervised 
analysis on the structures of data set. They 
automatically detect sophisticated feature 
vectors. The last layer in DBN is the BP layer. 
It takes the output from the last RBM layer and 
applies it in the final supervised learning 
process. In DBN, not only is the supervised BP 
layer used to generate the final categories, but 
it is also used to fine-tune the whole network. 
Specifically speaking, when the BP layer is 
changed during its iterating process, the 
changes are passed to the other RBM layers in 
a top-to-bottom sequence. 
3.5 The Feature Set 
DBN is able to detect high level hidden 
features from lexical, syntactic and/or position 
characteristic. As mentioned in related work, 
over-inclusion complex features are harmful. 
We therefore involve only three kinds of low 
level features in this study. They are described 
below. 
3.5.1 Character-based Features 
Since Chinese text is written without word 
boundaries, the word-level features are limited 
by the efficiency of word segmentation results. 
In the paper presented by H. Jing (2003) and 
some others, they observed that pure character-
based models can even outperform word-based 
models. Li et al?s (2008) work relying on 
character-based features also achieved 
significant performance in relation extraction. 
We denote the character dictionary as D={d1, 
d2, ?, dN}. In our experiment, N is 1500. To 
an e, it?s character-based feature vector is 
V(e)={ v1, v2, ?, vN }. Each unit vi can be 
valued as Equation 8. 
??
??
?
?
??
      0
    1
ed
edv
i
i
i
          (7) 
3.5.2 Entity Type Features 
According to the ACE 2004 guideline, there 
are five entity types in total, including Person, 
Organization, GPE, Location, and Facility. We 
recognize and classify the relation between the 
recognized entities. The entities in ACE 2004 
corpus were labeled with these five types. 
Type features are distinctive for classification. 
For example, the entities of Location cannot 
appear in the Role relation.  
3.5.3 Relative Position Features 
We define three types of position features 
which depict the relative structures between 
the two entities, including Nested, Adjacent 
and Separated. For each relation candidate 
triple 
1 2( , , )e e s , let .starte  and .ende  denote 
the starting and end positions of e  in a 
document. Table 1 summarizes the conditions 
for each type, where }2,1{, ?ji  and ji ? .  
Type Condition 
Nested ( .start, .end) ( .start, .end)i i j je e e e?
 
Adjacent .end= .start-1i je e
 
Separated ( .start< .start)&( .end+1< .start)i j i je e e e
 
Table 1. The internal postion structure features 
between two named entities 
We combine the character-based features of 
two entities, their type information and 
position information as the feature vector of 
relation candidate.  
3.6 Order of Entity Pair 
A relation is basically an order pair. For 
example, ?Bank of China in Hong Kong? 
conveys the ACE-style relation ?At? between 
two entities ?Bank of China (Organization)? 
and ?Hong Kong (Location)?. We can say that 
Bank of China can be found in Hong Kong, 
but not vice verse. The identified relation is 
said to be correct only when both its type and 
the order of the entity pair are correct. We 
don?t explicitly incorporate such order 
restriction as an individual feature but use the 
specified rules to sort the two entities in a 
relation once the relation type is recognized. 
As for those symmetric relation types, the 
order needs not to be concerned. Either order is 
considered correct in the ACE standard. As for 
those asymmetric relation types, we simply 
select the first (in adjacent and separated 
structure) or outer (in nested structures) as the 
first entity. In most cases, this treatment leads 
to the correct order. We also make use of 
entity types to verify (and rectify if necessary) 
this default order. For example, considering 
?At? is a relation between a Person, 
Organization, GPE, or Facility entity and a 
Location entity, the Location entity must be 
placed after the Person, Organization, GPE, or 
Facility entity in a relation. 
4 Experiments and Evaluations 
4.1 Experiment Setup 
The experiments are conducted on the ACE 
2004 Chinese relation extraction dataset, 
which consists of 221 documents selected from 
broadcast news and newswire reports. There 
are 2620 relation instances and 11800 pairs of 
entities have no relationship in the dataset. The 
size of the feature space is 3017.  
We examine the proposed DBN model 
using 4-fold cross-validation. The performance 
is measured by precision, recall, and F-
measure. 
2*Precision*Recall-measure= Precision+RecallF
    (8) 
In the following experiments, we plan to test 
the effectiveness of the DBN model in three 
ways: 
Detection Only: For each relation candidate, 
we only recognize whether there is a certain 
relationship between the two entities, no 
matter what type of relation they hold.  
Detection and Classification in Sequence: 
For each relation candidate, when it is 
detected to be an instance of relation, it 
proceeds to detect the type of the relation 
the two entities hold. 
Detection and Classification in Combination: 
We define N+1 relation label, N for relation 
types defined by ACE and one for NULL 
indicating there is no relationship between 
the two entities. In this way, the processes 
of detection and classification are combined. 
We will compare DBN with a well-known 
Support Vector Machine model (labeled as 
SVM in the tables) and a traditional BP neutral 
network model (labeled as NN (BP only)). 
Among them, SVM has been successfully 
applied in many classification applications. We 
use the LibSVM toolkit 2  to implement the 
SVM model. 
4.2 Evaluation on Detection Only 
We first evaluate relation detection, where 
only two output classes are concerned, i.e. 
NULL (which means no relation recognized) 
and RELATION. The parameters used in DBN, 
SVM and NN (BP only) are tuned 
experimentally and the results with the best 
parameter settings are presented in Table 2. In 
each of our experiments, we test many 
parameters of SVM and chose the best set of 
that to show below. 
Regarding the structure of DBN, we 
experiment with different combinations of unit 
numbers in the RBM layers. Finally we choose 
DBN with three RBM layers and one BP layer. 
And the numbers of units in each RBM layer 
are 2400, 1800 and 1200 respectively, which is 
the best size of each layer in our experiment. 
Our empirical results showed that the numbers 
of units in adjoining layers should not decrease 
the dimension of feature vector too much when 
casting the vector transformation. NN has the 
same structure as DBN. As for SVM, we 
choose the linear kernel with the penalty 
parameter C=0.3, which is the best penalty 
coefficient, and set the other parameters as 
default after comparing different kernels and 
parameter values.  
Model Precision Recall F-measure 
DBN 67.8% 70.58% 69.16% 
SVM 73.06% 52.42% 61.04% 
NN (BP 
only) 
51.51% 61.77% 56.18% 
Table 2. Performances of DBN, SVM and NN 
models for detection only 
As showed in Table 2, with their best 
parameter settings, DBN performs much better 
                                                 
2 http://www.csie.ntu.edu.tw/~cjlin/libsvm/  
than both SVM and NN (BP only) in terms of 
F-measure. It tells that DBN is quite good in 
this binary classification task. Since RBM is a 
fast approach to approximate global optimum 
of networks, its advantage over NN (BP only) 
is clearly demonstrated in their results.  
4.3 Evaluation on Detection and 
Classification in Sequence 
In the next experiment, we go one step further. 
If a relation is detected, we classified it into 
one of the 5 pre-defined relation types. For 
relation type classification, DBN and NN (BP 
only) have the same structures as they are in 
the first experiment. We adopt SVM linear 
kernel again and set C to 0.09 and other 
parameters as default. The overall performance 
of detection and classification of three models 
are illustrated in Table 3 below. DBN again is 
more effective than SVM and NN. 
Model Precision Recall F-measure 
DBN 63.67% 59% 61.25% 
SVM 67.78% 47.43% 55.81% 
NN  61% 45.62% 52.2% 
Table 3. Performances of DBN and other 
classification models for detection and 
classification in sequence 
4.4 Evaluation on Detection and 
Classification in Combination 
In the third experiment, we unify relation 
detection and relation type classification into 
one classification task. All the candidates are 
directly classified into one of the 6 classes, 
including 5 relation types and a NULL class. 
Parameter settings of the three models in this 
experiment are identical to those in the second 
experiment, except that C in SVM is set to 0.1. 
Model Precision Recall F-measure 
DBN 65.8% 59.15% 62.3% 
SVM 75.25% 44.07% 55.59% 
NN (BP 
only) 
63.2% 45.7% 53.05% 
Table 4. Performances of DBN, SVM and NN 
models for detection and classification in 
combination 
As demonstrated, DBN outperforms both 
SVM and NN (BP only) in all these three 
experiments consistently. In this regard, the 
advantages of DBN over the other two models 
are apparent. RBM approximates expected 
parameters rapidly and the deep DBN 
architecture yields stronger representativeness 
of complicated, efficient features.  
Comparing the results of the second and the 
third experiments, SVM perform better 
(although not quite significantly) when 
detection and classification are in sequence 
than in combination. This finding is consistent 
with our previous work (to be added later). It 
can possibly be that preceding detection helps 
to deal with the severe unbalance problem, i.e. 
there are much more relation candidates that 
don?t hold pre-defined relations. However, 
DBN obtaining the opposite result cause by 
that the amount of examples we have is not 
sufficient for DBN to self-train itself well for 
type classification. We will further exam this 
issue in our feature work. 
4.5 Evaluation on DBN Structure 
Next, we compare the performance of DBN 
with different structures by changing the 
number of RBM layers. All the candidates are 
directly classified into 6 types in this 
experiment.  
DBN  Precision Recall F-measure 
3 RBMs + 
BP 
65.8% 59.15% 62.3% 
2 RBMs + 
BP 
65.22% 57.1% 60.09% 
1 RBM + 
BP 
64.35% 55.5% 59.6% 
Table 5. Performance RBM with different 
layers 
The results provided in Table 5 show that 
the performance can be improved when more 
RBM layers are incorporated. Multiple RBM 
layers enhance representation power. Since it 
was reported by Hinton (2006) that three RBM 
layer is enough to detect the complex features 
and more RBM layer are of less help, we do 
not try to go beyond the three layers in this 
experiment. Note that the improvement is more 
obvious from two layers to three layers than 
from one layer to two layers. 
4.6 Error Analysis 
Finally, we provide the test results for 
individual relation types in Table 6. We can 
see that the proposed model performs better on 
?Role? and ?Part? relations. When taking a 
closer look at their relation instance 
distributions, the instances of these two types 
comprise over 63% percents of all the relation 
instances in the dataset. Clearly their better 
results benefit from the amount of training data. 
It further implies that if we have more training 
data, we should be able to train a more 
powerful DBN. The same characteristic is also 
observed in Table 7 which shows the 
distributions of the identified relations against 
the gold standard.  However, the sizes of ?At? 
relation instances and ?Role? relation instances 
are similar, its result is much worse. We 
believe it is from the origin of that the position 
feature is not distinctive for ?At? relation, as 
shown in Table 8. ?Near? and ?Social? are two 
symmetric relation types. Ideally, they should 
have better results. But due to quite small 
number of training examples, you can see that 
they are actually the types with the worst F-
measure. 
Type Precision Recall F-measure 
Role 65.19% 69.2% 67.14% 
Part 67.86% 71.43% 69.59% 
At 51.15% 60% 55.22% 
Near 15.38% 33.33% 20.05% 
Social 25% 35.71% 29.41% 
Table 6. Performance of DBN for each 
relation type 
 R P A N S Null 
Role (R) 191 1 5 0 0 96 
Part (P) 1 95 12 0 0 32 
At (A) 4 8 111 2 1 91 
Near (N) 0 1 0 2 0 10 
Social (S) 1 0 0 0 5 14 
Table 7. Distribution of the identified relations 
Type Adjacent  Separated Nested  
Role 7 63 223 
Part 1 17 122 
At 21 98 98 
Near 0 8 5 
Social 10 10 10 
           Identified 
Standard 
Table 8.  Statistic of position feature 
The main mistakes observed in Table 7 are 
wrongly classifying a ?Part? relation as a ?At? 
relations. We further inspect these 12 mistakes 
and find that it is indeed difficult to distinct the 
two types for the given entity pairs. Here is a 
typical example: entity 1: ?????  (the 
Democratic Party of the United States, defined 
as an organization entity), entity 2: ?? (the 
United States, defined as a GPE entity). 
Therefore, the major problem we have to face 
is how to effectively recall more relations. 
Given the limited training resources, it is 
needed to well explore the appropriate external 
knowledge or the Web resources. 
5 Conclusions 
In this paper we present our recent work on 
applying a novel machine learning model, 
namely Deep Belief Network, to Chinese 
relation extraction. DBN is demonstrated to 
be effective for Chinese relation extraction 
because of its strong representativeness. We 
conduct a series of experiments to prove the 
benefits of DBN. Experimental results clearly 
show the strength of DBN which obtains 
better performance than other existing models 
such as SVM and the traditional BP neutral 
network. In the future, we will explore if it is 
possible to incorporate the appropriate 
external knowledge in order to recall more 
relation instances, given the limited training 
resource. 
References 
Ackley D., Hinton G. and Sejnowski T. 1985. A 
learning algorithm for Boltzmann machines, 
Cognitive Science, 9. 
Brin Sergey. 1998. Extracting patterns and relations 
from world wide web, In Proceedings of 
WebDB Workshop at 6th International 
Conference on Extending Database 
Technology (WebDB?98), 172-183. 
Che W.X. Improved-Edit-Distance Kernel for 
Chinese Relation Extraction, In Dale, R.,Wong, 
K.-F., Su, J., Kwong, O.Y. (eds.) IJCNLP 
2005.LNCS(LNAI). vol. 2651. 
H. Jing, R. Florian, X. Luo, T. Zhang, A. 
Ittycheriah. 2003. How to get a Chinese name 
(entity): Segmentation and combination issues. 
In proceedings of EMNLP. 200-207. 
Hinton, G.. 1999. Products of experts. In 
Proceedings of the Ninth International. 
Conference on Artificial Neural Networks 
(ICANN). Vol. 1, 1?6. 
Hinton, G. E. 2002. Training products of experts by 
minimizing contrastive divergence, Neural 
Computation, 14(8), 1711?1800. 
Hinton G. E., Osindero S. and Teh Y. 2006. A fast 
learning algorithm for deep belief nets, Neural 
Computation, 18. 1527?1554. 
Ji Zhang, You Ouyang, Wenjie Li and Yuexian 
Hou. 2009. A Novel Composite Kernel 
Approach to Chinese Entity Relation 
Extraction. in Proceedings of the 22nd 
International Conference on the Computer 
Processing of Oriental Languages, Hong Kong, 
pp240-251. 
Ji Zhang, You Ouyang, Wenjie Li, and Yuexian 
Hou. 2009. Proceedings of the 22nd 
International Conference on Computer 
Processing of Oriental Languages. 236-247.  
Jiang J. and Zhai C. 2007. A Systematic 
Exploration of the Feature Space for Relation 
Extraction, In Proceedings of NAACL/HLT, 
113?120. 
Jinxiu Chen, Donghong Ji, Chew L., Tan and 
Zhengyu Niu. 2006. Relation extraction using 
label propagation based semi-supervised 
learning, In Proceedings of ACL?06, 129?136. 
Li W.J., Zhang P., Wei F.R., Hou Y.X. and Lu, Q. 
2008. A Novel Feature-based Approach to 
Chinese Entity Relation Extraction, In 
Proceeding of ACL 2008 (Companion Volume), 
89?92 
Sun Xia and Dong Lehong, 2009. Feature-based 
Approach to Chinese Term Relation Extraction. 
International Conference on Signal Processing 
Systems. 
Willy Yap and Timothy Baldwin. 2009. 
Experiments on Pattern-based Relation 
Learning. Proceeding of the 18th ACM 
conference on Information and knowledge 
management. 1657-1660.  
Y. Bengio and Y. LeCun. 2007. Scaling learning 
algorithms towards ai. Large-Scale Kernel 
Machines. MIT Press. 
Zelenko D. Aone C and Richardella A. 2003. 
Kernel Methods for Relation Extraction, 
Journal of Machine Learning Research 
2003(2), 1083?1106. 
Zhang P., Li W.J., Wei F.R., Lu Q. and Hou Y.X. 
2008. Exploiting the Role of Position Feature 
in Chinese Relation Extraction, In Proceedings 
of the 6th International Conference on 
Language Resources and Evaluation (LREC). 
 
