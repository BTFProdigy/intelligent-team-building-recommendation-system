Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 1223?1233, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
First-order vs. higher-order modification in distributional semantics
Gemma Boleda
Linguistics Department
University of Texas at Austin
gemma.boleda@utcompling.com
Eva Maria Vecchi
Center for Mind/Brain Sciences
University of Trento
evamaria.vecchi@unitn.it
Miquel Cornudella and Louise McNally
Departament de Traduccio? i Cie`ncies del Llenguatge
Universitat Pompeu Fabra
miquel.cornudellagaya@gmail.com, louise.mcnally@upf.edu
Abstract
Adjectival modification, particularly by ex-
pressions that have been treated as higher-
order modifiers in the formal semantics tradi-
tion, raises interesting challenges for semantic
composition in distributional semantic mod-
els. We contrast three types of adjectival mod-
ifiers ? intersectively used color terms (as in
white towel, clearly first-order), subsectively
used color terms (white wine, which have been
modeled as both first- and higher-order), and
intensional adjectives (former bassist, clearly
higher-order) ? and test the ability of different
composition strategies to model their behav-
ior. In addition to opening up a new empir-
ical domain for research on distributional se-
mantics, our observations concerning the at-
tested vectors for the different types of adjec-
tives, the nouns they modify, and the resulting
noun phrases yield insights into modification
that have been little evident in the formal se-
mantics literature to date.
1 Introduction
One of the most appealing aspects of so-called dis-
tributional semantic models (see Turney and Pan-
tel (2010) for a recent overview) is that they af-
ford some hope for a non-trivial, computationally
tractable treatment of the context dependence of lex-
ical meaning that might also approximate in inter-
esting ways the psychological representation of that
meaning (Andrews et al 2009). However, in or-
der to have a complete theory of natural language
meaning, these models must be supplied with or
connected to a compositional semantics; otherwise,
we will have no account of the recursive potential
that natural language affords for the construction of
novel complex contents.
In the last 4-5 years, researchers have begun
to introduce compositional operations on distribu-
tional semantic representations, for instance to com-
bine verbs with their arguments or adjectives with
nouns (Erk and Pado?, 2008; Mitchell and Lapata,
2010; Baroni and Zamparelli, 2010; Grefenstette
and Sadrzadeh, 2011; Socher et al 2011)1. Al-
though the proposed operations have shown vary-
ing degrees of success in a number of tasks such as
detecting phrase similarity and paraphrasing, it re-
mains unclear to what extent they can account for
the full range of meaning composition phenomena
found in natural language. Higher-order modifica-
tion (that is, modification that cannot obviously be
modeled as property intersection, in contrast to first-
order modification, which can) presents one such
challenge, as we will detail in the next section.
The goal of this paper is twofold. First, we exam-
ine how the properties of different types of adjecti-
val modifiers, both in isolation and in combination
with nouns, are represented in distributional mod-
els. We take as a case study three groups of adjec-
tives: 1) color terms used to ascribe true color prop-
erties (referred to here as intersective color terms),
as prototypical representative of first-order modi-
fiers; 2) color terms used to ascribe properties other
than simple color (here, subsective color terms), as
representatives of expressions that could in principle
1In a complementary direction, Garrette et al(2011) con-
nect distributional representations of lexical semantics to logic-
based compositional semantics.
1223
be given a well-motivated first-order or higher-order
analysis; and 3) intensional adjectives (e.g. former),
as representative of modifiers that arguably require a
higher-order analysis. Formal semantic models tend
to group the second and third groups together, de-
spite the existence of some natural language data
that questions this grouping. However, our results
show that all three types of modifiers behave differ-
ently from each other, suggesting that their semantic
treatment needs to be differentiated.
Second, we test how five different composition
functions that have been proposed in recent literature
fare in predicting the attested properties of nominals
modified by each type of adjective. The model by
Baroni and Zamparelli (2010) emerges as a suitable
model of adjectival composition, while multiplica-
tion and addition shed mixed results.
The paper is structured as follows. Section 2 pro-
vides the necessary background on the semantics of
adjectival modification. Section 3 presents the meth-
ods used in our study. Section 4 describes the char-
acteristics of the different types of adjectival modifi-
cation, and Section 5, the results of the composition
operations. The paper concludes with a general dis-
cussion of the results and prospects for future work.
2 The semantics of adjectival modification
Accounting for inference in language is an impor-
tant concern of semantic theory. Perhaps for this rea-
son, within the formal semantics tradition the most
influential classification of adjectives is based on
the inferences they license (see (Parsons, 1970) and
(Kamp, 1975) for early discussion). We very briefly
review this classification here.
First, so called intersective adjectives, such as (the
literally used) white in white dress, yield the infer-
ence that both the property contributed by the ad-
jective and that contributed by the noun hold of the
individual described; in other words, a white dress
is white and is a dress. The semantics for such mod-
ifiers is easily characterized in terms of the intersec-
tion of two first-order properties, that is, properties
that can be ascribed to individuals.
On the other extreme, intensional adjectives, such
as former or alleged in former/alleged criminal, do
not license the inference that either of the properties
holds of the individual to which the modified nom-
inal is ascribed. Indeed, such adjectives cannot be
used as predicates at all:
(1) ??The criminal was former/alleged.
The infelicity of (1) is generally attributed to the
fact that these adjectives do not describe individu-
als directly but rather effect more complex opera-
tions on the meaning of the modified noun. It is for
this reason that these adjectives can be considered
higher-order modifiers: they behave as properties of
properties. Though rather abstract, the higher-order
analysis is straightforwardly implementable in for-
mal semantic models and captures a range of lin-
guistic facts successfully.
Finally, subsective adjectives such as (the non-
literally-used) white in white wine, consitute an in-
termediate case: they license the inference that the
property denoted by the noun holds of the indi-
vidual being described, but not the property con-
tributed by the adjective. That is, white wine is
not white but rather a color that we would proba-
bly call some shade of yellow. This use of color
terms, in general, is distinguished primarily by the
fact that color serves as a proxy for another prop-
erty that is related to color (e.g. type of grape),
though the color in question may or may not match
the color identified by the adjective on the intersec-
tive use (see (Ga?rdenfors, 2000) and (Kennedy and
McNally, 2010) for discussion and analysis). The
effect of the adjective, rather than to identify a value
for an incidental COLOR attribute of an object, is of-
ten to characterize a subclass of the class described
by the noun (white wine is a kind of wine, brown
rice a kind of rice, etc.).
This use of color terms can be modeled by prop-
erty intersection in formal semantic models only if
the term is previously disambiguated or allowed to
depend on context for its precise denotation. How-
ever, it is easily modeled if the adjective denotes a
(higher-order) function from properties (e.g. that de-
noted by wine) to properties (that denoted by white
wine), since the output of the function denoted by
the color term can be made to depend on the input it
receives from the noun meaning. Nonetheless, there
is ample evidence in natural language that a first-
order analysis of the subsective color terms would
be preferable, as they share more features with pred-
1224
icative adjectives such as happy than they do with
adjectives such as former.
The trio of intersective color terms, subsective
color terms, and intensional adjectives provides fer-
tile ground for exploring the different composition
functions that have been proposed for distributional
semantic representations. Most of these functions
start from the assumption that composition takes
pairs of vectors (e.g. a verb vector and a noun vec-
tor) and returns another vector (e.g. a vector for
the verb with the noun as its complement), usually
by some version of vector addition or multiplication
(Erk and Pado?, 2008; Mitchell and Lapata, 2010;
Grefenstette and Sadrzadeh, 2011). Such func-
tions, insofar as they yield representations which
strengthen distributional features shared by the com-
ponent vectors, would be expected to model inter-
sective modification.
Consider the example of white dress. We might
expect the vector for dress to include non-zero fre-
quencies for words such as wedding and funeral.
The vector for white, on the other hand, is likely
to have higher frequencies for wedding than for fu-
neral, at least in corpora obtained from the U.S. and
the U.K. Combining the two vectors with an addi-
tive or multiplicative operation should rightly yield
a vector for white dress which assigns a higher fre-
quency to wedding than to funeral.
Additive and multiplicative functions might also
be expected to handle subsective modification with
some success because these operations provide a
natural account for how polysemy is resolved in
meaning composition. Thus, the vector that results
from adding or multiplying the vector for white with
that for dress should differ in crucial features from
the one that results from combining the same vector
for white with that for wine. For example, depend-
ing on the details of the algorithm used, we should
find the frequencies of words such as snow or milky
weakened and words like straw or yellow strength-
ened in combination with wine, insofar as the former
words are less likely than the latter to occur in con-
texts where white describes wine than in those where
it describes dresses. In contrast, it is not immedi-
ately obvious how these operations would fare with
intensional adjectives such as former. In particular,
it is not clear what specific distributional features of
the adjective would capture the effect that the ad-
jective has on the meaning of the resulting modified
nominal.
Interestingly, recent approaches to the semantic
composition of adjectives with nouns such as Baroni
and Zamparelli (2010) and Guevara (2010) draw on
the classical analysis of adjectives within the Mon-
tagovian tradition of formal semantic theory (Mon-
tague, 1974), on which they are treated as higher or-
der predicates, and model adjectives as matrices of
weights that are applied to noun vectors. On such
models, the distributional properties of observed oc-
currences of adjective-noun pairs are used to induce
the effect of adjectives on nouns. Insofar as it is
grounded in the intuition that adjective meanings
should be modeled as mappings from noun mean-
ings to adjective-noun meanings, the matrix anal-
ysis might be expected to perform better than ad-
ditive or multiplicative models for adjective-noun
combinations when there is evidence that the adjec-
tive denotes only a higher-order property. There is
also no a priori reason to think that it would fare
more poorly at modeling the intersective and subsec-
tive adjectives than would additive or multiplicative
analyses, given its generality.
In this paper, we present the first studies that we
know of that explore these expectations.
3 Method
We built a semantic space and tested the composi-
tion functions as specified in what follows.
3.1 Semantic space
The semantic space we used for our experiments
consists of a matrix where each row vector repre-
sents an adjective, noun or adjective-noun phrase
(henceforth, AN). We first introduce the source cor-
pus, then the vocabulary that we represent in the
space, and finally the procedure to build the vectors
representing the vocabulary items from corpus data.
3.1.1 Source corpus
Our source corpus is the concatenation of the
ukWaC corpus2, a mid-2009 dump of the English
Wikipedia3 and the British National Corpus4. The
corpus is tokenized, POS-tagged and lemmatized
2http://wacky.sslmit.unibo.it/
3http://en.wikipedia.org
4http://www.natcorp.ox.ac.uk/
1225
with TreeTagger (Schmid, 1995) and contains about
2.8 billion tokens. We extracted all statistics at the
lemma level, ignoring inflectional information.
3.1.2 Vocabulary
The core vocabulary of the semantic space con-
sists of the 8K most frequent nouns and the 4K most
frequent adjectives from the corpus. By crossing the
set of 700 most frequent adjectives (reduced to 663
after removing questionable items like above, less
and very) and the 4K most frequent nouns and se-
lecting those ANs that occured at least 100 times
in the corpus, we obtained a set of 179K ANs that
we added to the semantic space, for a total of 191K
rows. These ANs were used for training the linear
models as well as for providing a basis for the anal-
ysis of the results.
3.1.3 Semantic space parameters
The dimensions (columns) of our semantic space
are the top 10K most frequent content words in the
corpus (nouns, adjectives, verbs and adverbs), ex-
cluding the 300 most frequent words of all parts of
speech.
For each word or AN, we collected raw co-
occurrence counts by recording their sentence-
internal co-occurrence with each of words in the di-
mensions. The counts were then transformed into
Local Mutual Information (LMI) scores, an associ-
ation measure that closely approximates the com-
monly used Log-Likelihood Ratio but is simpler to
compute (Evert, 2005). Specifically, given a row el-
ement r, a column element c and a counting function
C(r, c), then
LMI = C(r, c) ? log
C(r, c)C(?, ?)
C(r, ?)C(?, c)
(1)
where C(r, c) is how many times r cooccurs with
c, C(r, ?) is the total count of r, C(?, c) is the to-
tal count of c, and C(?, ?) is the cumulative co-
occurrence count of any r with any c.
The dimensionality of the space was reduced us-
ing Singular Value Decomposition (SVD), as in La-
tent Semantic Analysis and related distributional
semantic methods (Landauer and Dumais, 1997;
Rapp, 2003; Schu?tze, 1997). Both LMI and SVD
were used for the core vocabulary, and the AN vec-
tors were computed based on the values for the
core vocabulary. All of the results discussed in the
article are based on the SVD-reduced space, be-
cause it yielded consistently better results, except for
those involving multiplicative composition, which
was carried out on the non-reduced model because
SVD reduction introduces negative values for the la-
tent dimensions used for the reduced space.
Some of the parameters of the space and com-
position functions were set based on performance
on independent word similarity and AN similarity
tasks (Rubenstein and Goodenough, 1965; Mitchell
and Lapata, 2010). In addition to LMI, we tested
the performance using log-transformed frequencies
and found very poor performance in the aforemen-
tioned tasks. The number of latent dimensions for
the SVD-reduced space was set at 300 after testing
the performance using 300, 600 and 900 latent di-
mensions.
In the discussion, we use the cosine of two vectors
as a measure of similarity. This is the most common
choice in related work, as it has shown to be robust
across different tasks and settings, though other op-
tions (in particular, measures that are not symmetric
or do not normalize) could be explored (Widdows,
2004).
3.2 Composition models
The experiments described below were carried out
using five compositional methods that have been ex-
plored in recent studies of compositionality in dis-
tributional semantic spaces (Mitchell and Lapata,
2010; Guevara, 2010; Baroni and Zamparelli, 2010).
For each function, we define p as the composition
of the adjective vector, u, and the noun vector, v,
a nomenclature that follows Mitchell and Lapata
(2010).
Additive (add) AN vectors were obtained by
summing the corresponding adjective and noun vec-
tors. We also explored the effects of the additive
model with normalized component adjective and
noun vectors (addn).
p = u + v (2)
Multiplicative (mult) AN vectors were obtained
by component-wise multiplication of the adjective
and noun vectors in the non-reduced semantic space.
p = u v (3)
1226
Dilation (dl) AN vectors were obtained by calcu-
lating the dot products of u?u and u?v and stretching
v by a factor ? (in our case, 16.7) in the direction of
u (Clark et al 2008; Mitchell and Lapata, 2010).
The effect of this operation is to ?stretch? the head
vector v (noun, in our case) in the direction of the
modifying vector u (adjective).
p = (u ? u)v + (?? 1)(u ? v) (4)
The factor ? was selected based on the optimal pa-
rameters presented in Mitchell and Lapata (2010).
We tested both reported values (16.7 and 2.2) and
found ? = 16.7 to perform better in terms of rank of
observed equivalent (see Section 5).
The preceding functions produce an AN vector
from the component A and N vectors. The remain-
ing two functions do not use the vector for the ad-
jective, but learn a matrix representation for it. The
composed AN vector is obtained by multiplying the
matrix by the noun vector. The general equation for
the two functions is the following, where B is a ma-
trix of weights that is multiplied by the noun vector
v to produce the AN vector p.
p = Bv (5)
In the linear map (lim) approach proposed by
Guevara (2010), one single matrix B is learnt that
represents all adjectives. An AN vector is obtained
by multiplying the weight matrix by the concate-
nation of the adjective and noun vectors, so that
each dimension of the generated AN vector is a lin-
ear combination of dimensions of the correspond-
ing adjective and noun vectors. In our implementa-
tion, B is an 300 x 300 weight matrix representing
an adjective, and v is a 300-dimension noun vec-
tor. Following Guevara (2010), we estimate the co-
efficients of the equation using (multivariate) partial
least squares regression (PLSR) as implemented in
the R pls package (Mevik and Wehrens, 2007), set-
ting the latent dimension parameter of PLSR to 300.
This value was chosen after testing values 100, 200
and 300 on the AN similarity tasks (Mitchell and
Lapata, 2010). Coefficient matrix estimation is per-
formed by feeding PLSR a set of input-output exam-
ples, where the input is given by concatenated ad-
jective and noun vectors, and the output is the vector
of the corresponding AN directly extracted from our
semantic space. The matrix is estimated using a ran-
dom sample of 2.5K adjective-noun-AN tuples.5
In the adjective-specific linear map (alm) model,
proposed by Baroni and Zamparelli (2010), a dif-
ferent matrix B is learnt for each adjective. The
weights of each of the rows of the weight matrix
are the coefficients of a linear equation predicting
the values of one of the dimensions of the normal-
ized AN vector as a linear combination of the di-
mensions of the normalized component noun. The
linear equation coefficients are estimated again us-
ing PLSR, and in the present implementation we use
ridge regression generalized cross-validation (GCV)
to automatically choose the optimal ridge parameter
for each adjective (Golub et al 1979). This pro-
cedure drastically outperforms setting a fixed num-
ber of dimensions. The model is trained on all N-
AN vector pairs available in the semantic space for
each adjective, and range from 100 to over 1K items
across the adjectives we tested.
3.3 Datasets
We built two datasets of adjective-noun phrases for
the present research, one with color terms and one
with intensional adjectives.6
Color terms. This dataset is populated with a ran-
domly selected set of adjective-noun pairs from the
space presented above. From the 11 colors in the ba-
sic set proposed by Berlin and Kay (1969), we cover
7 (black, blue, brown, green, red, white, and yel-
low), since the remaining (grey, orange, pink, and
purple) are not in the 700 most frequent set of ad-
jectives in the corpora used. From an original set
of 412 ANs, 43 were manually removed because of
suspected parsing errors (e.g. white photograph, for
black and white photograph) or because the head
noun was semantically transparent (white variety).
The remaining 369 ANs were tagged independently
by the second and fourth authors of this paper, both
native English speaker linguists, as intersective (e.g.
white towel), subsective (e.g. white wine), or id-
iomatic, i.e. compositionally non-transparent (e.g.
black hole). They were allowed the assignment of at
52.5K ANs is the upper bound of the software package used.
6Available at http://dl.dropbox.com/u/513347/
resources/data-emnlp2012.zip. See Bruni et al(to
appear) for an analysis of the color term dataset from a multi-
modal perspective.
1227
most two labels in case of polysemy, for instance for
black staff for the person vs. physical object senses
of the noun or yellow skin for the race vs. literally
painted interpretations of the AN. In this paper, only
the first label (most frequent interpretation, accord-
ing to the judges) has been used. The ? coefficient of
the annotation on the three categories (first interpre-
tation only) was 0.87 (conf. int. 0.82-0.92, according
to Fleiss et al(1969)), observed agreement 0.96.7
There were too few instances of idioms (17) for a
quantitative analysis of the sort presented here, so
these are collapsed with the subsective class in what
follows.8 The dataset as used here consists of 239
intersective and 130 subsective ANs.
Intensional adjectives. The intensional dataset
contains all ANs in the semantic space with a pre-
selected list of 10 intensional adjectives, manually
pruned by one of the authors of the paper to elimi-
nate erroneous examples and to ensure that the ad-
jective was being intensionally used. Examples of
the ANs eliminated on these grounds include past
twelve (cp. accepted past president), former girl
(probably former girl friend or similar), false rumor
(which is a real rumor that is false, vs. e.g. false
floor, which is not a real floor), or theoretical work
(which is real work related to a theory, vs. e.g. theo-
retical speed, which is a speed that should have been
reached in theory). Other AN pairs were excluded
on the grounds that the noun was excessively vague
(e.g. past one) or because the AN formed a fixed
expression (e.g. former USSR). The final dataset
contained 1,200 ANs, distributed as follows: former
(300 examples), possible (244), future (243), poten-
tial (183), past (87), false (44), apparent (39), arti-
ficial (36), likely (18), theoretical (6).9
Table 1 contains examples of each type of AN we
are considering.
7Code for the computation of inter-annotator agreement by
Stefan Evert, available at http://www.collocations.
de/temp/kappa_example.zip.
8An alternative would have been to exclude idiomatic ANs
from the analysis.
9Alleged, one of the most prototypical intensional adjectives,
is not considered here because it was not among the 700 most
frequent adjectives in the space. We will consider it in future
work.
Intersective Subsective Intensional
white towel white wine artificial leg
black sack black athlete former bassist
green coat green politics likely suspect
red disc red ant possible delay
blue square blue state theoretical limit
Table 1: Example ANs in the datasets.
4 Observed vectors
We began by exploring the empirically observed
vectors for the adjectives (A), nouns (N), and
adjective-noun phrases (AN) in the datasets, as they
are represented in the semantic space. Note that
we are working with the AN vectors directly har-
vested from the corpora (that is, based on the co-
occurrence of, say, the phrase white towel with each
of the 10K words in the space dimensions), with-
out doing any composition. AN vectors obtained by
composition will be examined in the following sec-
tion. Though observed AN vectors should not be
regarded as a gold standard in the sense of, for in-
stance, Machine Learning approaches, because they
are typically sparse10 and thus the vectors of their
component adjective and noun will be richer, they
are still useful for exploration and as a compari-
son point for the composition operations (Baroni and
Lenci, 2010; Guevara, 2010).
Figure 1 shows the distribution of the cosines be-
tween A, N, and AN vectors with intensional adjec-
tives (I, white box), intersective uses of color terms
(IE, lighter gray box), and subsective uses of color
terms (S, darker gray box).
In general, the similarity of the A and N vectors is
quite low (cosine < 0.2, left graph of Figure 1), and
much lower than the similarities between both the
AN and A vectors and the AN and N vectors. This
is not surprising, given that adjectives and nouns de-
scribe rather different sorts of things.
We find significant differences between the three
types of adjectives in the similarity between AN and
A vectors (middle graph of Figure 1). The adjec-
tive and adjective-noun phrase vectors are nearer for
10The frequency of the adjectives in the datasets range from
3.5K to 3.7M, with a median frequency of 109,114. The nouns
range from 4.9K to 2.5M, with a median frequency of 148,459.
While the frequency of the ANs range from 100 to 18.5K, with
a median frequency of 239.
1228
ll
l
ll
l
l
l
ll
lll
l
l
l
ll
l
ll
l
l
ll
l
l
l
l
I L N
0.0
0.2
0.4
0.6
0.8
1.0
cos(A,N)
l
I L N0
.0
0.2
0.4
0.6
0.8
1.0
cos(AN,A)
I L N0
.0
0.2
0.4
0.6
0.8
1.0
cos(AN,N)
Figure 1: Cosine distance distribution in the different types of AN. We report the cosines between the component
adjective and noun vectors (cos(A,N)), between the observed AN and adjective vectors (cos(AN,A)), and between the
observed AN and noun vectors (cos(AN,N)). Each chart contains three boxplots with the distribution of the cosine
scores (y-axis) for the intensional (I), intersective (IE), and subsective (S) types of ANs. The boxplots represent the
value distribution of the cosine between two vectors. The horizontal lines in the rectangles represent the first quartile,
median, and third quartile. Larger rectangles correspond to a more spread distribution, and their (a)symmetry mirrors
the (a)symmetry of the distribution. The lines above and below the rectangle stretch to the minimum and maximum
values, at most 1.5 times the length of the rectangle. Values outside this range (outliers) are represented as points.
intersective uses than for subsective uses of color
terms, a pattern that parallels the difference in the
distance between component A and N vectors. Since
intersective uses correspond to the prototypical use
of color terms (a white dress is the color white, while
white wine is not), the greater similarity for the in-
tersective cases is unsurprising ? it suggests that in
the case of subsective adjectival modifiers, the noun
?pulls? the AN further away from the adjective than
happens with the cases of intersective modification.
This is compatible with the intuition (manifest in the
formal semantics tradition in the treatment of sub-
sective adjectives as higher-order rather than first-
order, intersective modifiers) that the adjective?s ef-
fect on the AN in cases of subsective modification
depends heavily on the interpretation of the noun
with which the adjective combines, whereas that is
less the case when the adjective is used intersec-
tively.
As for intensional adjectives, the middle graph
shows that their AN vectors are quite distant from
the corresponding A vectors, in sharp contrast to
what we find with both intersective and subsective
color terms. We hypothesize that the results for the
intensional adjectives are due to the fact that they
cannot plausibly be modeled as first order attributes
(i.e. being potential or apparent is not a property
in the same sense that being white or yellow is) and
thus typically do not restrict the nominal description
per se, but rather provide information about whether
or when the nominal description applies. The re-
sult is that intensional adjectives should be even
weaker than subsectively used adjectives, in com-
parison with the nouns with which they combine, in
their ability to ?pull? the AN vector in their direc-
tion. Note, incidentally, that an alternative expla-
nation, namely that the effect mentioned could be
due to the fact that most nouns in the intensional
dataset are abstract and that adjectives modifying
abstract nouns might tend to be further away from
their nouns altogether, is ruled out by the compari-
son between the A and N vectors: the A-N cosines
of the intensional and intersective ANs are similar.
We thus conclude that here we see an effect of the
type of modification involved.
An examination of the average distances among
1229
the nearest neighbors of the intensional and of the
color adjectives in the distributional space supports
our hypothesized account of their contrasting be-
haviors. We predict that the nearest neighbors are
more dispersed for adjectives that cannot be mod-
eled as first-order properties (i.e., intensional adjec-
tives), than for those that can (here, the color terms).
We find that the average cosine distance among the
nearest ten neighbors of the intensional adjectives is
0.74 with a standard deviation of 0.13, which is sig-
nificantly lower (t-test, p<0.001) than the average
similarity among the nearest neighbors of the color
adjectives, 0.96 with astandard deviation of 0.04.
Finally, with respect to the distances between the
adjective-noun and head noun vectors (right graph
of Figure 1), there is no significant difference for the
intersective vs. subsective color terms. This can be
explained by the fact that both kinds of modifiers
are subsective, that is, the fact that a white dress is a
dress and that white wine is wine.
In contrast, intensional ANs are closer to their
component Ns than are color ANs (the difference
is qualitatively quite small, but significant even for
the intersective vs. intensional ANs according to a
t-test, p-value = 0.015). This effect, the inverse of
what we find with the AN-A vectors, can similarly
be explained by the fact that intensional adjectives
do not restrict the descriptive content of the noun
they modify, in contrast to both the intersective and
subsective color ANs. Restriction of the nominal
description may lead to significantly restricted dis-
tributions (e.g. the phrase red button may appear
in distinctively different contexts than does button;
similarly for green politics and politics), while we
do not expect the contexts in which former bassist
and bassist appear to diverge in a qualitatively dif-
ferent way because the basic nominal descriptions
are identical, though further research will be neces-
sary to confirm these explanations.
Finally, note that, contrary to predictions from
some approaches in formal semantics, subsective
color ANs and intensional ANs do not pattern to-
gether: subsective ANs are closer to their compo-
nent As, and intensional ANs closer to their compo-
nent Ns. This unexpected behavior underscores the
fact highlighted in the previous paragraph: that the
distributional properties of modified expressions are
more sensitive to whether the modification restricts
the nominal description than to whether the modifier
is intersective in the strictest sense of term.
We now discuss the extent to which the different
composition functions account for these patterns.
5 Composed vectors
Since intersective modification is the point of com-
parison for both subsective and intensional modifi-
cation, we first discuss the composed vectors for the
intersective vs. subsective uses of color terms, and
then turn to intersective vs. intensional modification.
5.1 Intersective and subsective modification
with color terms
To adequately model the differences between inter-
sective and subsective modification observed in the
previous section, a successful composition function
should yield a significantly smaller distance between
the adjective and AN vectors for intersectively used
adjectives, whereas it should yield no significant dif-
ference for the distances between the noun and AN
vectors.
Table 2 provides a summary of the results with
the observed data (obs) and the composition func-
tions discussed in Section 3.2. The median rank of
observed equivalent (ROE) is provided as a general
measure of the quality of the composition function.
It is computed by finding the cosine between the
composed AN vectors and all rows in the semantic
space and then determining the rank in which the ob-
served ANs are found.11 The remaining columns re-
port the differences in standardized (z-score) cosines
between the vector built with each of the composi-
tion functions and the observed AN, A, and N vec-
tors. A positive value means that the cosines for
intersective uses are higher, while a negative value
means that the cosines for subsective uses are higher.
The first row (obs) contains a numerical summary
of the tendencies for observed ANs explained in the
previous section. This is the behavior that we expect
to model.
Two composition functions come close to mod-
eling the observed behavior: alm and mult, though
alm is better in terms of ROE, consistent with the
11The ROE is provided as a general guide; however, recall
that the ROE was taken into account to tune the ? parameter in
the dilation model, and that the ANs of the color dataset were
included when training the matrices for the alm model.
1230
model ROE ?:AN ?:A ?:N
obs - - .54 ??? .10
add 286 .40 ??? .14 .15
addn 11 .40 ??? .65 ??? .65 ???
mult 111 .40 ??? .74 ??? .29 ?
dl 298 .63 ??? .85 ??? -.66 ???
lim 1,940 .46 ??? .20 .38 ??
alm 1 .16 .52 ??? .27 ?
Table 2: Intersective vs. subsective uses of color terms.
The first column reports the rank of the observed equiva-
lent (ROE), the rest report the differences (?) betwen the
intersective and subsective uses of color terms when com-
paring the composed AN with the observed vectors for:
AN, adjective (A), noun (N). See text for details. Signifi-
cances according to a t-test: *** for p< 0.001, **< 0.01,
* < 0.05.
results reported in Baroni and Zamparelli (2010).
In both cases, we find that these functions yield
higher similarities for AN-A for the intersective than
for the subsective uses of color terms, and a very
slight (though still mildly significant) difference for
the distance to the head noun. The addn function
performs very good in terms of ROE (median 11).
This suggests that, for adjectival modification, pro-
viding a vector that is in the middle of the two
component vectors (which is what normalized ad-
dition does) is a reasonable approximation of the
observed vectors. However, precisely because the
resulting vector is in the middle of the two com-
ponent vectors, this function cannot account for the
asymmetries in the distances found in the observed
data. The non-normalized version also cannot ac-
count for these effects because the adjective vec-
tor, being much longer (as color terms are very fre-
quent), totally dominates the AN, which results in
no difference across uses when comparing to the ad-
jective or to the noun.
The dilation model shows a strange pattern, as it
yields a strongly significant negative difference in
the AN-N distance. The lim function exhibits the op-
posite pattern as predicted, yielding no difference for
the AN-A similarities and a difference for the AN-
N similarities. A possible explanation for the AN-
A results is that lim learns from such a broad range
of AN pairs that the impact of the distance between
intersective vs. subsective uses of color terms from
their component adjectives is dampened. Moreover,
lim is by far the worst function in terms of ROE.
All composition functions except for alm find in-
tersective uses easier to model. This is shown in the
positive values in column ?:AN, which mean that
the similarity between observed and composed AN
vectors is greater for intersective than for subsective
ANs. This is consistent with expectations. The sub-
sective uses are specific to the nouns with which the
color terms combine, and the exact interpretation of
the adjective varies across those nouns. In contrast,
the interpretation associated with intersective use is
consistent across a larger variety of nouns, and in
that sense should be predominantly reflected in the
adjective?s vector. The exception in this respect is
the alm function, since the weights for each adjec-
tive matrix are estimated in relation to the noun vec-
tors with which the adjective combines, on the one
hand, and the related observed AN vectors, on the
other; thus, the basic lexical representation of the
adjective is inherently reflective of the distributions
of the ANs in which it appears in a way that is not
the case for the adjective representations used in the
other composition models. And indeed, alm is the
only function that shows no difference in difficulty
(distance) between the predicted and observed AN
vectors for intersective vs. subsective ANs.
Both mult and alm seem to account for the ob-
served patterns in color terms. However, an exam-
ination of the nearest neighbors of the composed
ANs suggest that alm captures the semantics of ad-
jective composition in this case to a larger extent
than mult. For instance, the NN for blue square (in-
tersective) are the following according to mul: blue,
red, official colour, traditional colour, blue num-
ber, yellow; while alm yields the following: blue
square, red square, blue circle, blue triangle, blue
pattern, yellow circle. Similarly, for green poli-
tics (subsective) mul yields: pleasant land, green
business, green politics, green issue, green strategy,
green product, while alm yields green politics, green
movement, political agenda, environmental move-
ment, progressive government, political initiative.
5.2 Intensional modification
Table 3 contains the results of the composition func-
tions comparing the behavior of intersective color
ANs and intensional ANs. The tendencies in the
ROE are as in Table 2, so we will not comment on
1231
model ROE ?:AN ?:A ?:N
obs - - 1.39 ??? -.27 ???
add 198 .66 ??? .71 ??? -.81 ???
addn 40 .93 ??? .20 ? .20 ?
mult 110 .58 ??? 1.09 ??? -.25 ???
dl 354 .97 ??? -.27 ?? .47 ???
lim 7,943 .27 ??? .65 ??? -.47 ???
alm 1 .81 ??? 1.43 ??? -.59 ???
Table 3: Intersective vs. intensional ANs. Information as
in Table 2.
them further (note the very poor performance of lim,
though). As noted above, we expect more difficulty
in modeling intensional modification vs. other kinds
of modification, and this is verified in the results
(cf. the positive values in second column). The dif-
ference with the results in the previous subsection
is that in this case the alm function does present a
higher difficulty in modeling intensional ANs, un-
like with the color terms. This points to a qualitative
difference between subsective and intensional adjec-
tives that could be evidence for a first-order analysis
of subsective color terms.
A good composition function should provide a
large positive difference when comparing the AN
to the A, and a small negative difference (because
the effect is very small in the observed data) when
comparing the AN to the N. The functions that best
match the observed data are again alm and mult.
Add and lim show the predicted pattern, but to a
much lesser degree (cf. smaller differences in col-
umn ?:A). Dl yields the exact opposite effect and
addn, though good in terms of ROE, is subject to
the problems discussed in the previous section.
Again, alm seems to be capturing relevant seman-
tic aspects of composition with intensional adjec-
tives. For instance, the nearest neighbors of artificial
leg according to alm are artificial leg, artificial limb,
artificial joint, artificial hip, scar, small wound.
6 Discussion and conclusions
The present research provides some evidence for
treating adjectives as matrices or functions, rather
than vectors, although simple operations on vectors
such as addition (for its excellent approximation to
observed vectors) and multiplication (for its ability
to reproduce the observed trends in the data) still ac-
count for some aspects of adjectival modification.
The dilation model, in contrast, is not suitable for
adjectival modification.
Our results also show that alm performs better
than lim, but it is worth observing that it does so
at the expense of modeling each adjective as a com-
pletely different function. We consider lim very at-
tractive in principle because it generalizes across ad-
jectives and is thus more parsimonious. Part of the
poor results on lim were due to limitations of our
implementation, as we trained the matrices on only
2.5K ANs, while our semantic space contains more
than 170K ANs. However, the linguistic literature
and the present results suggest that it might be use-
ful to try a compromise between alm and lim, train-
ing one matrix for each subclass of adjectives under
analysis.
Beyond the new data it offers regarding the com-
parative ability of the different composition func-
tions to account for different kinds of adjectival
modification, the study presented here underscores
the complexity of modification as a semantic phe-
nomenon. The role of adjectival modifiers as restric-
tors of descriptive content is reflected differently in
distributional data than is their role in providing in-
formation about whether or when a description ap-
plies to some individual. Formal semantic models,
thanks to their abstractness, are able to handle these
two roles with little difficulty, but also with limited
insight. Distributional models, in contrast, offer the
promise of greater insight into each of these roles,
but face serious challenges in handling both of them
in a unified manner.
Acknowledgments
This research was funded by the Spanish MICINN
(FFI2010-09464-E, FFI2010-15006, TIN2009-
14715-C04-04), the Catalan AGAUR (2010BP-
A00070), and the EU (PASCAL2; FP7-ICT-
216886). Eva Maria Vecchi was partially funded
by ERC Starting Grant 283554. We thank Marco
Baroni, Roberto Zamparelli, and six anonymous
reviewers for valuable feedback, and Yao-zhong
Zhang for the code for the alm function.
1232
References
Mark Andrews, Gabriella Vigliocco, and David Vinson.
2009. Integrating experiential and distributional data
to learn semantic represenations. Psychological Re-
view, 116(3):463?498.
Marco Baroni and Alessandro Lenci. 2010. Dis-
tributional Memory: A general framework for
corpus-based semantics. Computational Linguistics,
36(4):673?721.
Marco Baroni and Roberto Zamparelli. 2010. Nouns
are vectors, adjectives are matrices: Representing
adjective-noun constructions in semantic space. In
Proceedings of EMNLP, pages 1183?1193, Boston,
MA.
Brent Berlin and Paul Kay. 1969. Basic Color Terms:
Their Universality an Evolution. University of Cali-
fornia Press, Berkeley and Los Angeles, CA.
E. Bruni, G. Boleda, M. Baroni, and N. K. Tran. to ap-
pear. Distributional semantics in technicolor. In Pro-
ceedings of ACL 2012.
Stephen Clark, Bob Coecke, and Mehrnoosh Sadrzadeh.
2008. A compositional distributional model of mean-
ing. In Proceedings of the AAAI Spring Symposium on
Quantum Interaction, pages 52?55, Stanford, CA.
Katrin Erk and Sebastian Pado?. 2008. A structured vec-
tor space model for word meaning in context. In Pro-
ceedings of EMNLP, pages 897?906, Honolulu, HI,
USA.
Stefan Evert. 2005. The Statistics of Word Cooccur-
rences. Dissertation, Stuttgart University.
Joseph L. Fleiss, Jacob Cohen, and B. S. Everitt. 1969.
Large sample standard errors of kappa and weighted
kappa. Psychological Bulletin, 72(5):323?327.
Peter Ga?rdenfors. 2000. Conceptual Spaces: The Geom-
etry of Thought. MIT Press, Cambridge, MA.
Dan Garrette, Katrin Erk, and Raymond Mooney. 2011.
Integrating logical representations with probabilistic
information using markov logic. In Proceedings of
IWCS 2011.
G.H. Golub, M. Heath, and G. Wahba. 1979. General-
ized cross-validation as a method for choosing a good
ridge parameter. Technometrics, pages 215?223.
Edward Grefenstette and Mehrnoosh Sadrzadeh. 2011.
Experimenting with transitive verbs in a discocat. In
Proceedings of the GEMS 2011 Workshop on GEomet-
rical Models of Natural Language Semantics.
Emiliano Guevara. 2010. A regression model of
adjective-noun compositionality in distributional se-
mantics. In Proceedings of the ACL GEMS Workshop,
pages 33?37, Uppsala, Sweden.
H. Kamp. 1975. Two theories about adjectives. Formal
semantics of natural language, pages 123?155.
Christopher Kennedy and Louise McNally. 2010. Color,
context, and compositionality. Synthese, 174:79?98.
Thomas Landauer and Susan Dumais. 1997. A solu-
tion to Plato?s problem: The latent semantic analysis
theory of acquisition, induction, and representation of
knowledge. Psychological Review, 104(2):211?240.
Bjo?rn-Helge Mevik and Ron Wehrens. 2007. The
pls package: Principal component and partial least
squares regression in R. Journal of Statistical Soft-
ware, 18(2). Published online: http://www.
jstatsoft.org/v18/i02/.
Jeff Mitchell and Mirella Lapata. 2010. Composition in
distributional models of semantics. Cognitive Science,
34(8):1388?1429.
Richard Montague. 1974. Formal philosophy: Selected
Papers of Richard Montague. Yale University Press,
New Haven.
Terence Parsons. 1970. Some problems concerning the
logic of grammatical modifiers. Synthese, 21:320?
334.
Reinhard Rapp. 2003. Word sense discovery based on
sense descriptor dissimilarity. In Proceedings of the
9th MT Summit, pages 315?322, New Orleans, LA,
USA.
Herbert Rubenstein and John Goodenough. 1965. Con-
textual correlates of synonymy. Communications of
the ACM, 8(10):627?633.
Helmut Schmid. 1995. Improvements in part-of-speech
tagging with an application to German. In Proceed-
ings of the EACL-SIGDAT Workshop, Dublin, Ireland.
Hinrich Schu?tze. 1997. Ambiguity Resolution in Natural
Language Learning. CSLI, Stanford, CA.
R. Socher, J. Pennington, E.H. Huang, A.Y. Ng, and C.D.
Manning. 2011. Semi-supervised recursive autoen-
coders for predicting sentiment distributions. In Pro-
ceedings of the Conference on Empirical Methods in
Natural Language Processing, pages 151?161, Edin-
burgh, UK.
Peter Turney and Patrick Pantel. 2010. From frequency
to meaning: Vector space models of semantics. Jour-
nal of Artificial Intelligence Research, 37:141?188.
Dominic Widdows. 2004. The Geometry of Meaning.
CSLI Publications, Stanford, CA.
1233
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 141?151,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Studying the recursive behaviour of adjectival modification
with compositional distributional semantics
Eva Maria Vecchi and Roberto Zamparelli and Marco Baroni
Center for Mind/Brain Sciences (University of Trento, Italy)
(evamaria.vecchi|roberto.zamparelli|marco.baroni)@unitn.it
Abstract
In this study, we use compositional distribu-
tional semantic methods to investigate restric-
tions in adjective ordering. Specifically, we
focus on properties distinguishing Adjective-
Adjective-Noun phrases in which there is flex-
ibility in the adjective ordering from those
bound to a rigid order. We explore a number
of measures extracted from the distributional
representation of AAN phrases which may in-
dicate a word order restriction. We find that
we are able to distinguish the relevant classes
and the correct order based primarily on the
degree of modification of the adjectives. Our
results offer fresh insight into the semantic
properties that determine adjective ordering,
building a bridge between syntax and distri-
butional semantics.
1 Introduction
A prominent approach for representing the meaning
of a word in Natural Language Processing (NLP) is
to treat it as a numerical vector that codes the pat-
tern of co-occurrence of that word with other ex-
pressions in a large corpus of language (Sahlgren,
2006; Turney and Pantel, 2010). This approach to
semantics (sometimes called distributional seman-
tics) scales well to large lexicons and does not re-
quire words to be manually disambiguated (Schu?tze,
1997). Until recently, however, this method had
been almost exclusively limited to the level of sin-
gle content words (nouns, adjectives, verbs), and had
not directly addressed the problem of composition-
ality (Frege, 1892; Montague, 1970; Partee, 2004),
the crucial property of natural language which al-
lows speakers to derive the meaning of a complex
linguistic constituent from the meaning of its imme-
diate syntactic subconstituents.
Several recent proposals have strived to ex-
tend distributional semantics with a component that
also generates vectors for complex linguistic con-
stituents, using compositional operations in the vec-
tor space (Baroni and Zamparelli, 2010; Guevara,
2010; Mitchell and Lapata, 2010; Grefenstette and
Sadrzadeh, 2011; Socher et al, 2012). All of
these approaches construct distributional represen-
tations for novel phrases starting from the corpus-
derived vectors for their lexical constituents and
exploiting the geometric quality of the representa-
tion. Such methods are able to capture complex se-
mantic information of adjective-noun (AN) phrases,
such as characterizing modification (Boleda et al,
2012; Boleda et al, 2013), and can detect seman-
tic deviance in novel phrases (Vecchi et al, 2011).
Furthermore, these methods are naturally recursive:
they can derive a representation not only for, e.g.,
red car, but also for new red car, fast new red car,
etc. This aspect is appealing since trying to extract
meaningful representations for all recursive phrases
directly from a corpus will result in a problem of
sparsity, since most large phrases will never occur in
any finite sample.
Once we start seriously looking into recursive
modification, however, the issue of modifier order-
ing restrictions naturally arises. Such restrictions
have often been discussed in the theoretical linguis-
tic literature (Sproat and Shih, 1990; Crisma, 1991;
Scott, 2002), and have become one of the key in-
141
gredients of the ?cartographic? approach to syntax
(Cinque, 2002). In this paradigm, the ordering is
derived by assigning semantically different classes
of modifiers to the specifiers of distinct functional
projections, whose sequence is hard-wired. While
it is accepted that in different languages movement
can lead to a principled rearrangement of the linear
order of the modifiers (Cinque, 2010; Steddy and
Samek-Lodovici, 2011), one key assumption of the
cartographic literature is that exactly one intonation-
ally unmarked order for stacked adjectives should
be possible in languages like English. The possi-
bility of alternative orders, when discussed at all,
is attributed to the presence of idioms (high Amer-
ican building, but American high officer), to asyn-
detic conjunctive meanings (e.g. new creative idea
parsed as [new & creative] idea, rather than [new
[creative idea]]), or to semantic category ambiguity
for any adjective which appears in different orders
(see Cinque (2004) for discussion).
In this study, we show that the existence of both
rigid and flexible order cases is robustly attested at
least for adjectival modification, and that flexible or-
dering is unlikely to reduce to idioms, coordination
or ambiguity. Moreover, we show that at least for
some recursively constructed adjective-adjective-
noun phrases (AANs) we can extract meaning-
ful representations from the corpus, approximating
them reasonably well by means of compositional
distributional semantic models, and that the seman-
tic information contained in these models character-
izes which AA will have rigid order (as with rapid
social change vs. *social rapid change), or flexible
order (e.g. total estimated population vs. estimated
total population). In the former case, we find that
the same distributional semantic cues discriminate
between correct and wrong orders.
To achieve these goals, we consider various
properties of the distributional representation of
AANs (both corpus-extracted and compositionally-
derived), and explore their correlation with restric-
tions in adjective ordering. We conclude that mea-
sures that quantify the degree to which the modifiers
have an impact on the distributional meaning of the
AAN can be good predictors of ordering restrictions
in AANs.
2 Materials and methods
2.1 Semantic space
Our initial step was to construct a semantic space for
our experiments, consisting of a matrix where each
row represents the meaning of an adjective, noun,
AN or AAN as a distributional vector, each column
a semantic dimension of meaning. We first introduce
the source corpus, then the vocabulary of words and
phrases that we represent in the space, and finally the
procedure adopted to build the vectors representing
the vocabulary items from corpus statistics, and ob-
tain the semantic space matrix. We work here with a
traditional, window-based semantic space, since our
focus is on the effect of different composition meth-
ods given a common semantic space. In addition,
Blacoe and Lapata (2012) found that a vanilla space
of this sort performed best in their composition ex-
periments, when compared to a syntax-aware space
and to neural language model vectors such as those
used for composition by Socher et al (2011).
Source corpus We use as our source corpus the
concatenation of the Web-derived ukWaC corpus, a
mid-2009 dump of the English Wikipedia and the
British National Corpus1. The corpus has been tok-
enized, POS-tagged and lemmatized with the Tree-
Tagger (Schmid, 1995), and it contains about 2.8 bil-
lion tokens. We extract all statistics at the lemma
level, meaning that we consider only the canonical
form of each word ignoring inflectional information,
such as pluralization and verb inflection.
Semantic space vocabulary The words/phrases
in the semantic space must of course include the
items that we need for our experiments (adjectives,
nouns, ANs and AANs used for model training, as
input to composition and for evaluation). Therefore,
we first populate our semantic space with a core vo-
cabulary containing the 8K most frequent nouns and
the 4K most frequent adjectives from the corpus.
The ANs included in the semantic space are com-
posed of adjectives with very high frequency in the
corpus so that they are generally able to combine
with many classes of nouns. They are composed
of the 700 most frequent adjectives and 4K most
frequent nouns in the corpus, which were manually
1http://wacky.sslmit.unibo.it, http://en.
wikipedia.org, http://www.natcorp.ox.ac.uk
142
controlled for problematic cases ? excluding adjec-
tives such as above, less, or very, and nouns such
as cant, mph, or yours ? often due to tagging errors.
We generated the set of ANs by crossing the filtered
663 adjectives and 3,910 nouns. We include those
ANs that occur at least 100 times in the corpus in
our vocabulary, which amounted to a total of 128K
ANs.
Finally, we created a set of AAN phrases com-
posed of the adjectives and nouns used to gener-
ate the ANs. Additional preprocessing of the gen-
erated AxAyNs includes: (i) control that both AxN
and AyN are attested in the corpus; (ii) discard any
AxAyN in which AxN or AyN are among the top
200 most frequent ANs in the source corpus (as in
this case, order will be affected by the fact that such
phrases are almost certainly highly lexicalized); and
(iii) discard AANs seen as part of a conjunction in
the source corpus (i.e., where the two adjectives ap-
pear separated by comma, and, or or; this addresses
the objection that a flexible order AAN might be a
hidden A(&)A conjunction: we would expect that
such a conjunction should also appear overtly else-
where). The set of AANs thus generated is then di-
vided into two types of adjective ordering:
1. Flexible Order (FO): phrases where both or-
ders, AxAyN and AyAxN, are attested (f>10
in both orders).
2. Rigid Order (RO): phrases with one order,
AxAyN, attested (20<f<200)2 and AyAxN
unattested.
All AANs that did not meet either condition were
excluded from our semantic space vocabulary. The
preserved set resulted in 1,438 AANs: 621 flexible
order and 817 rigid order. Note that there are almost
as many flexible as rigid order cases; this speaks
against the idea that free order is a marginal phe-
nomenon, due to occasional ambiguities that reas-
sign the adjective to a different semantic class. The
existence of freely ordered stacked adjectives is a ro-
bust phenomenon, which needs to be addressed.
2The upper threshold was included as an additional filter
against potential multiword expressions. Of course, the bound-
ary between phrases that are at least partially compositional and
those that are fully lexicalized is not sharp, and we leave it to
further work to explore the interplay between the semantic fac-
tors we study here and patterns of lexicalization.
Model ? M&L
CORP 0.41 0.43
W.ADD 0.41 0.44
F.ADD 0.40 ?
MULT 0.33 0.46
LFM 0.40 ?
Table 1: Correlation scores (Spearman?s ?, all signif-
icant at p<0.001) between cosines of corpus-extracted
or model-generated AN vectors and phrase similarity rat-
ings collected in Mitchell and Lapata (2010), as well as
best reported results from Mitchell & Lapata (M&L).
Semantic vector construction For each of
the items in our vocabulary, we first build 10K-
dimensional vectors by recording the item?s
sentence-internal co-occurrence with the top 10K
most frequent content lemmas (nouns, adjectives,
verbs or adverbs) in the corpus. We built a rank
of these co-occurrence counts, and excluded as
stop words from the dimensions any element of
any POS whose rank was from 0 to 300. The raw
co-occurrence counts were then transformed into
(positive) Pointwise Mutual Information (pPMI)
scores (Church and Hanks, 1990). Next, we reduce
the full co-occurrence matrix to 300 dimensions
applying the Non-negative Matrix Factorization
(NMF) operation (Lin, 2007). We did not tune the
semantic vector construction parameters, since we
found them to work best in a number of independent
earlier experiments.
Corpus-extracted vectors (corp) were computed
for the ANs and for the flexible order and attested
rigid order AANs, and then mapped onto the 300-
dimension NMF-reduced semantic space. As a san-
ity check, the first row of Table 1 reports the corre-
lation between the AN phrase similarity ratings col-
lected in Mitchell and Lapata (2010) and the cosines
of corpus-extracted vectors in our space, for the
same ANs. For the AAN vectors, which are sparser,
we used human judgements to build a reliable sub-
set to serve as our gold standard, as detailed in Sec-
tion 2.4.
2.2 Composition models
We focus on four composition functions proposed
in recent literature with high performance in a num-
ber of semantic tasks. We first consider meth-
ods proposed by Mitchell and Lapata (2010) in
143
which the model-generated vectors are simply ob-
tained through component-wise operations on the
constituent vectors. Given input vectors ~u and ~v, the
multiplicative model (MULT) computes a composed
vector by component-wise multiplication () of the
constituent vectors, where the i-th component of the
composed vector is given by pi = uivi.3 Given an
AxAyN phrase, this model extends naturally to the
recursive setting of this experiment, as seen in Equa-
tion (1).
~p = ~ax  ~ay  ~n (1)
This composition method is order-insensitive, the
formula above corresponding to the representation
of both AxAyN and AyAxN.
In the weighted additive model (W.ADD), we ob-
tain the composed vector as a weighted sum of the
two component vectors: ~p = ?~u+ ?~v, where ? and
? are scalars. Again, we can easily apply this func-
tion recursively, as in Equation (2).
~p = ?~ax + ?(?~ay + ?~n) = ?~ax + ??~ay + ?
2~n
(2)
We also consider the full extension of the addi-
tive model (F.ADD), presented in Guevara (2010)
and Zanzotto et al (2010), such that the component
vectors are pre-multiplied by weight matrices before
being added: ~p = W1~u + W2~v. Similarly to the
W.ADD model, Equation (3) describes how we apply
this function recursively.
~p = W1~ax + W2(W1~ay + W2~n) (3)
= W1~ax + W2W1~ay + W22~n
Finally, we consider the lexical function model
(LFM), first introduced in Baroni and Zamparelli
(2010), in which attributive adjectives are treated as
functions from noun meanings to noun meanings.
This is a standard approach in Montague semantics
(Thomason, 1974), except noun meanings here are
distributional vectors, not denotations, and adjec-
tives are (linear) functions learned from a large cor-
pus. In this model, predicted vectors are generated
3We conjecture that the different performance of our multi-
plicative model and M&L?s (cf. Table 1) is due to the fact that
we use log-transformed pPMI scores, making their multiplica-
tive model more akin to our additive approach.
by multiplying a function matrix U with a compo-
nent vector: ~p = U~v. Given a weight matrix, A, for
each adjective in the phrase, we apply the functions
in sequence recursively as shown in Equation (4).
~p = Ax(Ay~n) (4)
Composition model estimation Parameters for
W.ADD, F.ADD and LFM were estimated following
the strategy proposed by Guevara (2010) and Ba-
roni and Zamparelli (2010), recently extended to all
composition models by Dinu et al (2013b). Specif-
ically, we learn parameter values that optimize the
mapping from the noun to the AN as seen in ex-
amples of corpus-extracted N-AN vector pairs, us-
ing least-squares methods. All parameter estima-
tions and phrase compositions were implemented
using the DISSECT toolkit4 (Dinu et al, 2013a),
with a training set of 74,767 corpus-extracted N-
AN vector pairs, ranging from 100 to over 1K items
across the 663 adjectives. Importantly, while below
we report experimental results on capturing various
properties of recursive AAN constructions, no AAN
was seen during training, which was based entirely
on mapping from N to AN. Table 1 reports the re-
sults attained by our model implementations on the
Mitchell and Lapata AN similarity data set.
2.3 Measures of adjective ordering
Our general goal is to determine which
linguistically-motivated factors distinguish the
two types of adjective ordering. We hypothesize
that in cases of flexible order, the two adjectives
will have a similarly strong effect on the noun, thus
transforming the meaning of the noun equivalently
in the direction of both adjectives and component
ANs. For example, in the phrase creative new idea,
the idea is both new and creative, so we would
expect a similar impact of modification by both
adjectives.
On the other hand, we predict that in rigid order
cases, one adjective, the one closer to the noun, will
dominate the meaning of the phrase, distorting the
meaning of the noun by a significant amount. For
example, the phrase different architectural style in-
tuitively describes an architectural style that is dif-
4http://clic.cimec.unitn.it/composes/
toolkit
144
ferent, rather than a style that is to the same extent
architectural and different.
We consider a number of measures that could cap-
ture our intuitions and quantify this difference, ex-
ploring the distance relationship between the AAN
vectors and each of the AAN subparts. First, we
examine how the similarity of an AAN to its com-
ponent adjectives affects the ordering, using the co-
sine between the AxAyN vector and each of the
component A vectors as an expression of similarity
(we abbreviate this as cosAx and cosAy for the first
and second adjective, respectively).5 Our hypothe-
sis predicts that flexible order AANs should remain
similarly close to both component As, while rigid
order AANs should remain systematically closer to
their Ay than to their Ax.
Next, we consider the similarity between the
AxAyN vector and its component N vector (cosN ).
This measure is aimed at verifying if the degree to
which the meaning of the head noun is distorted
could be a property that distinguishes the two types
of adjective ordering. Again, vectors for flexible or-
der AANs should remain closer to their component
nouns in the semantic space, while rigid order AANs
should distort the meaning of the head noun more
notably.
We also inspect how the similarity of the AAN
to its component AN vectors affects the type of ad-
jective ordering (cosAxN and cosAyN ). Consid-
ering the examples above, we predict that the flex-
ible order AAN creative new idea will share many
properties with both creative idea and new idea, as
represented in our semantic space, while rigid or-
der AANs, like different architectural style, should
remain quite similar to the AyN, i.e., architectural
style, and relatively distant from the AxN, i.e., dif-
ferent style.
Finally, we consider a measure that does not ex-
ploit distributional semantic representations, namely
the difference in PMI between AxN and AyN
(?PMI). Based on our hypothesis described for the
other measures, we expect the association in the cor-
pus of AyN to be much greater than AxN for rigid
order AANs, resulting in a large negative ?PMI val-
ues. While flexible order AANs should have similar
5In the case of LFM, we compare the similarity of the AAN
with the AN centroids for each adjective, since the model does
not make use of A vectors (Baroni and Zamparelli, 2010).
association strengths for both AxN and AyN, thus
we expect ?PMI to be closer to 0 than for rigid or-
der AANs.
2.4 Gold standard
To our knowledge, this is the first study to use
distributional representations of recursive modifi-
cation; therefore we must first determine if the
composed AAN vector representations are seman-
tically coherent objects. Thus, for vector analysis,
a gold standard of 320 corpus-extracted AAN vec-
tors were selected and their quality was established
by inspecting their nearest neighbors. In order to
create the gold standard, we ran a crowdsourcing
experiment on CrowdFlower6 (Callison-Burch and
Dredze, 2010; Munro et al, 2010), as follows.
First, we gathered a randomly selected set of 600
corpus-extracted AANs, containing 300 flexible or-
der and 300 attested rigid order AANs. We then
extracted the top 3 nearest neighbors to the corpus-
extracted AAN vectors as represented in the seman-
tic space7. Each AAN was then presented with each
of the nearest neighbors, and participants were asked
to judge ?how strongly related are the two phrases??
on a scale of 1-7. The rationale was that if we
obtained a good distributional representation of the
AAN, its nearest neighbors should be closely related
words and phrases. Each pair was judged 10 times,
and we calculated a relatedness score for the AAN
by taking the average of the 30 judgments (10 for
each of the three neighbors).
The final set for the gold standard contains the 320
AANs (152 flexible order and 168 attested rigid or-
der) which had a relatedness score over the median-
split (3.9). Table 2 shows examples of gold stan-
dard AANs and their nearest neighbors. As these
example indicate, the gold standard AANs reside in
semantic neighborhoods that are populated by in-
tuitively strongly related expressions, which makes
them a sensible target for the compositional models
to approximate.
We also find that the neighbors for the AANs rep-
resent an interesting variety of types of semantic
6http://www.crowdflower.com
7The top 3 neighbors included adjectives, nouns, ANs and
AANs. The preference for ANs and AANs, as seen in Table 2,
is likely a result of the dominance of those elements in the se-
mantic space (c.f. Section 2.1).
145
medieval old town contemp. political issue
fascinating town cultural topic
impressive cathedral contemporary debate
medieval street contemporary politics
rural poor people British naval power
poor rural people naval war
rural infrastructure British navy
rural people naval power
friendly helpful staff last live performance
near hotel final gig
helpful staff live dvd
quick service live release
creative new idea rapid social change
innovative effort social conflict
creative design social transition
dynamic part cultural consequence
national daily newspaper new regional government
national newspaper regional government
major newspaper local reform
daily newspaper regional council
daily national newspaper fresh organic vegetable
national daily newspaper organic vegetable
well-known journalist organic fruit
weekly column organic product
Table 2: Examples of the nearest neighbors of the gold
standard, both flexible order (left column) and rigid order
(right column) AANs.
similarity. For example, the nearest neighbors to the
corpus-extracted vectors for medieval old town and
rapid social change include phrases which describe
quite complex associations, cf. Table 2. In addition,
we find that the nearest neighbors for flexible order
AAN vectors are not necessarily the same for both
adjective orders, as seen in the difference in neigh-
bors of national daily newspaper and daily national
newspaper. We can expect that the change in or-
der, when acceptable and frequent, does not neces-
sarily yield synonymous phrases, and that corpus-
extracted vector representations capture subtle dif-
ferences in meaning.
3 Results
3.1 Quality of model-generated AAN vectors
Our nearest neighbor analysis suggests that the
corpus-extracted AAN vectors in the gold standard
are meaningful, semantically coherent objects. We
can thus assess the quality of AANs recursively gen-
erated by composition models by how closely they
Gold FO RO
W.ADD 0.565 0.572 0.558
F.ADD 0.618 0.622 0.614
MULT 0.424 0.468 0.384
LFM 0.655 0.675 0.637
Table 3: Mean cosine similarities between the corpus-
extracted and model-generated gold AAN vectors. All
pairwise differences between models are significant ac-
cording to Bonferroni-corrected paired t-tests (p<0.001).
For MULT and LFM, the difference between mean flexible
order (FO) and rigid order (RO) cosines is also signifi-
cant.
approximate these vectors. We find that the perfor-
mances of most composition models in approximat-
ing the vectors for the gold AANs is quite satisfac-
tory (cf. Table 3). To put this evaluation into per-
spective, note that 99% of the simulated distribu-
tion of pairwise cosines of corpus-extracted AANs
is below the mean cosine of the worst-performing
model (MULT), that is, a cosine of 0.424 is very sig-
nificantly above what is expected by chance for two
random corpus-extracted AAN vectors. Also, ob-
serve that the two more parameter-rich models are
better than W.ADD, and that LFM also significantly
outperforms F.ADD.
Further, the results show that the models are able
to approximate flexible order AAN vectors better
than rigid order AANs, significantly so for LFM and
MULT. This result is quite interesting because it sug-
gests that flexible order AANs express a more lit-
eral (or intersective) modification by both adjectives,
which is what we would expect to be better captured
by compositional models. Clearly, a more complex
modification process is occurring in the case of rigid
order AANs, as we predicted to be the case.
3.2 Distinguishing flexible vs. rigid order
In the results reported below, we test how both our
baseline ?PMI measure and the distance from the
AAN and its component parts changes depending on
the type of adjective ordering to which the AAN be-
longs. From this point forward, we only use gold
standard items, where we are sure of the quality of
the corpus-extracted vectors. The first block of Ta-
ble 4 reports the t-normalized difference between
flexible order and rigid order mean cosines for the
corpus-extracted vectors.
146
Measure t sig.
CORP
cosAx 2.478
cosAy -4.348 * RO>FO
cosN 4.656 * FO>RO
cosAxN 5.913 * FO>RO
cosAyN 1.970
W.ADD
cosAx 4.805 * FO>RO
cosAy -1.109
cosN 1.140
cosAxN 1.059
cosAyN 0.584
F.ADD
cosAx 2.050
cosAy -1.451
cosN 4.493 * FO>RO
cosAxN -0.445
cosAyN 2.300
MULT
cosAx 3.830 * FO>RO
cosAy -0.503
cosN 5.090 * FO>RO
cosAxN 4.435 * FO>RO
cosAyN 3.900 * FO>RO
LFM
cosAx -1.649
cosAy -1.272
cosN 5.539 * FO>RO
cosAxN 3.336 * FO>RO
cosAyN 4.215 * FO>RO
?PMI 8.701 * FO>RO
Table 4: Flexible vs. Rigid Order AANs. t-normalized
differences between flexible order (FO) and rigid order
(FO) mean cosines (or mean ?PMI values) for corpus-
extracted and model-generated vectors. For significant
differences (p<0.05 after Bonferroni correction), the last
column reports whether mean cosine (or ?PMI) is larger
for flexible order (FO) or rigid order (RO) class.
These results show, in accordance with our con-
siderations in Section 2.3 above: (i) flexible or-
der AxAyNs are closer to AxN and the component
N than rigid order AxAyNs, and (ii) rigid order
AxAyNs are closer to their Ay (flexible order AANs
are also closer to Ax but the effect does not reach
significance).8 The results imply that the degree of
modification of the Ay on the noun is a significant
indicator of the type of ordering present.
8As an aside, the fact that mean cosines are significantly
larger for the flexible order class in two cases but for the rigid or-
der class in another addresses the concern, raised by a reviewer,
that the words and phrases in one of the two classes might sys-
tematically inhabit denser regions of the space than those of the
other class, thus distorting results based on comparing mean
cosines.
In particular, rigid order AxAyNs are heavily
modified by Ay, distorting the meaning of the head
noun in the direction of the closest adjective quite
drastically, and only undergoing a slight modifica-
tion when the Ax is added. In other words, in rigid
order phrases, for example rapid social change, the
AyN expresses a single concept (probably a ?kind?,
in the terminology of formal semantics), strongly re-
lated to social, social change, which is then mod-
ified by the Ax. Thus, the change is not both so-
cial and rapid, rather, the social change is rapid. On
the other hand, flexible order AANs maintain the se-
mantic value of the head noun while being modi-
fied only slightly by both adjectives, almost equiv-
alently. For example, in the phrase friendly help-
ful staff, one is saying that the staff is both friendly
and helpful. Most importantly, the corpus-extracted
distributional representations are able to model this
phenomenon inherently and can significantly distin-
guish the two adjective orders.
The results of the composition models (cf. Ta-
ble 4) show that for all models at least some prop-
erties do distinguish flexible and rigid order AANs,
although only MULT and LFM capture the two prop-
erties that show the largest effect for the corpus-
extracted vectors, namely the asymmetry in similar-
ity to the noun and the AxN (flexible order AANs
being more similar to both).
It is worth remarking that MULT approximated the
patterns observed in the corpus vectors quite well,
despite producing order-insensitive representations
of recursive structures. For flexible order AANs, or-
der is indeed only slightly affecting the meaning, so
it stands to reason that MULT has no problems mod-
eling this class. For rigid order AANs, where we
consider here the attested-order only, evidently the
order-insensitive MULT representation is sufficient
to capture their relations to their constituents.
Finally, we see that the ?PMI measure is the best
at distinguishing between the two classes of AAN
ordering. This confirms our hypothesis that a lot has
to do with how integrated Ay and N are. While it
is somewhat disappointing that ?PMI outperforms
all distributional semantic cues, note that this mea-
sure conflates semantic and lexical factors, as the
high PMI of AyN in at least some rigid order AANs
might be also a cue of the fact that the latter bigram
is a lexicalized phrase (as discussed in footnote 2, it
147
is unlikely that our filtering strategies sifted out all
multiword expressions). Moreover, ?PMI does not
produce a semantic representation of the phrase (see
how composed distributional vectors approximate of
high quality AAN vectors in Table 3). Finally, this
measure will not scale up to cases where the ANs
are not attested, whereas measures based on compo-
sition only need corpus-harvested representations of
adjectives and nouns.
3.3 Properties of the correct adjective order
Having shown that flexible order and rigid order
AANs are significantly distinguished by various
properties, we proceed now to test whether those
same properties also allow us to distinguish between
correct (corpus-attested) and wrong (unattested) ad-
jective ordering in rigid AANs (recall that we are
working with cases where the attested-order occurs
more than 20 times in the corpus, and both adjec-
tives modify the nouns at least 10 times, so we are
confident that there is a true asymmetry).
We expect that the fundamental property that dis-
tinguishes the orders is again found in the degree
of modification of both component adjectives. We
predict that the single concept created by the AyN
in attested-order rigid AANs, such as legal status
in formal legal status, is an effect of the modifica-
tion strength of the Ay on the head noun, and when
seen in the incorrect ordering, i.e., ?legal formal sta-
tus, the strong modification of legal will still domi-
nate the meaning of the AAN. Composition models
should be able to capture this effect based on the dis-
tance from both the component adjectives and ANs.
Clearly, we cannot run these analyses on corpus-
extracted vectors since the unattested order, by def-
inition, is not seen in our corpus, and therefore we
cannot collect co-occurrence statistics for the AAN
phrase. Thus, we test our measures of adjective or-
dering on the model-generated AAN vectors, for all
gold rigid order AANs in both orders.
We also consider the ?PMI measure which was
so effective in distinguishing flexible vs. rigid or-
der AANs. We expect that the greater association
with AyN for attested-order AANs will again lead
to large, negative differences in PMI scores, while
the expectation that unattested-order AANs will be
highly associated with their AxN will correspond to
large, positive differences in PMI.
Measure t sig.
W.ADD
cosAx -7.840 * U>A
cosAy 7.924 * A>U
cosN 2.394
cosAxN -5.462 * U>A
cosAyN 3.627 * A>U
F.ADD
cosAx -8.418 * U>A
cosAy 6.534 * A>U
cosN -1.927
cosAxN -3.583 * U>A
cosAyN -2.185
MULT
cosAx -5.100 * U>A
cosAy 5.100 * A>U
cosN 0.000
cosAxN -0.598
cosAyN 0.598
LFM
cosAx -7.498 * U>A
cosAy 7.227 * A>U
cosN -2.172
cosAxN -5.792 * U>A
cosAyN 0.774
?PMI -11.448 * U>A
Table 5: Attested- vs. unattested-order rigid order
AANs. t-normalized mean paired cosine (or ?PMI) dif-
ferences between attested (A) and unattested (U) AANs
with their components. For significant differences (paired
t-test p<0.05 after Bonferroni correction), last column
reports whether cosines (or ?PMI) are on average larger
for A or U.
Across all composition models, we find that the
distance between the model-generated AAN and its
component adjectives, Ax and Ay, are significant in-
dicators of attested vs. unattested adjective ordering
(cf. Table 5). Specifically, we find that rigid order
AANs in the correct order are closest to their Ay,
while we can detect the unattested order when the
rigid order AAN is closer to its Ax. This finding
is quite interesting, since it shows that the order in
which the composition functions are applied does
not alter the fact that the modification of one ad-
jective in rigid order AANs (the Ay in the case of
attested-order rigid order AANs) is much stronger
than the other. Unlike the measures that differenti-
ated flexible and rigid order AANs, here we see that
the distance from the component N is not an indi-
cator of the correct adjective ordering (trivially so
for MULT, where attested and unattested AANs are
identical).
Next, we find that for W.ADD, F.ADD and LFM,
148
the distance from the component AxN is a strong
indicator of attested- vs. unattested-order rigid order
AANs. Specifically, attested-order AANs are further
from their AxN than unattested-order AANs. This
finding is in line with our predictions and follows
the findings of the impact of the distance from the
component adjectives.
?PMI, as seen in the ability to distinguish flexi-
ble vs. rigid order AANs, is the strongest indicator
of correct vs wrong adjective ordering. This mea-
sure confirms that the association of one adjective
(the Ay in attested-order AANs) with the head noun
is indeed the most significant factor distinguishing
these two classes. However, as we mentioned be-
fore, this measure has its limitations and is likely not
to be entirely sufficient for future steps in modeling
recursive modification.
4 Conclusion
While AN constructions have been extensively stud-
ied within the framework of compositional distri-
butional semantics (Baroni and Zamparelli, 2010;
Boleda et al, 2012; Boleda et al, 2013; Guevara,
2010; Mitchell and Lapata, 2010; Turney, 2012;
Vecchi et al, 2011), for the first time, we extended
the investigation to recursively built AAN phrases.
First, we showed that composition functions ap-
plied recursively can approximate corpus-extracted
AAN vectors that we know to be of high semantic
quality.
Next, we looked at some properties of the same
high-quality corpus-extracted AAN vectors, finding
that the distinction between ?flexible? AANs, where
the adjective order can be flipped, and ?rigid? ones,
where the order is fixed, is reflected in distributional
cues. These results all derive from the intuition that
the most embedded adjective in a rigid AAN has a
very strong effect on the distributional semantic rep-
resentation of the AAN. Most compositional models
were able to capture at least some of the same cues
that emerged in the analysis of the corpus-extracted
vectors.
Finally, similar cues were also shown to distin-
guish (compositional) representations of rigid AANs
in the ?correct? (corpus-attested) and ?wrong?
(unattested) orders, again pointing to the degree to
which the (attested-order) closest adjective affects
the overall AAN meaning as an important factor.
Comparing the composition functions, we find
that the linguistically motivated LFM approach has
the most consistent performance across all our tests.
This model significantly outperformed all others in
approximating high-quality corpus-extracted AAN
vectors, it provided the closest approximation to the
corpus-observed patterns when distinguishing flexi-
ble and rigid AANs, and it was one of the models
with the strongest cues distinguishing attested and
unattested orders of rigid AANs.
From an applied point of view, a natural next step
would be to use the cues we proposed as features to
train a classifier to predict the preferred order of ad-
jectives, to be tested also in cases where neither or-
der is found in the corpus, so direct corpus evidence
cannot help. For a full account of adjectival order-
ing, non-semantic factors should also be taken into
account. As shown by the effectiveness in our ex-
periments of PMI, which is a classic measure used
to harvest idioms and other multiword expressions
(Church and Hanks, 1990), ordering is affected by
arbitrary lexicalization patterns. Metrical effects are
also likely to play a role, like they do in the well-
studied case of ?binomials? such as salt and pep-
per (Benor and Levy, 2006; Copestake and Herbe-
lot, 2011). In a pilot study, we found that indeed
word length (roughly quantified by number of let-
ters) is a significant factor in predicting adjective
ordering (the shorter adjective being more likely to
occur first), but its effect is not nearly as strong as
that of the semantic measures we considered here.
In our future work, we would like to develop an or-
der model that exploits semantic, metrical and lexi-
calization features jointly for maximal classification
accuracy.
Adjectival ordering information could be useful
in parsing: in English, it could tell whether an
AANN sequence should be parsed as A[[AN]N]
or A[A[NN]]; in languages with pre- and post-
N adjectives, like Italian or Spanish, it could tell
whether ANA sequences should be parsed as A[NA]
or [AN]A. The ability to detect ordering restric-
tions could also help Natural Language Generation
tasks (Malouf, 2000), especially for the generation
of unattested combinations of As and Ns.
From a theoretical point of view, we would like to
extend our analysis to adjective coordination (what?s
149
the difference between new and creative idea and
new creative idea?). Additionally, we could go more
granular, looking at whether compositional models
can help us to understand why certain classes of ad-
jectives are more likely to precede or follow others
(why is size more likely to take scope over color,
so that big red car sounds more natural than red big
car?) or studying the behaviour of specific adjectives
(can our approach capture the fact that strong alco-
holic drink is preferable to alcoholic strong drink
because strong pertains to the alcoholic properties
of the drink?).
In the meantime, we hope that the results we re-
ported here provide convincing evidence of the use-
fulness of compositional distributional semantics in
tackling topics, such as recursive adjectival modifi-
cation, that have been of traditional interest to theo-
retical linguists from a new perspective.
Acknowledgments
We would like to thank the anonymous reviewers,
Fabio Massimo Zanzotto, Yao-Zhong Zhang and the
members of the COMPOSES team. This research
was supported by the ERC 2011 Starting Indepen-
dent Research Grant n. 283554 (COMPOSES).
References
Marco Baroni and Roberto Zamparelli. 2010. Nouns
are vectors, adjectives are matrices: Representing
adjective-noun constructions in semantic space. In
Proceedings of EMNLP, pages 1183?1193, Boston,
MA.
Sarah Bunin Benor and Roger Levy. 2006. The chicken
or the egg? A probabilistic analysis of english binomi-
als. Language, pages 233?278.
William Blacoe and Mirella Lapata. 2012. A comparison
of vector-based representations for semantic composi-
tion. In Proceedings of the 2012 Joint Conference on
EMNLP and CoNLL, pages 546?556, Jeju Island, Ko-
rea.
Gemma Boleda, Eva Maria Vecchi, Miquel Cornudella,
and Louise McNally. 2012. First-order vs. higher-
order modification in distributional semantics. In Pro-
ceedings of the 2012 Joint Conference on EMNLP and
CoNLL, pages 1223?1233, Jeju Island, Korea.
Gemma Boleda, Marco Baroni, Louise McNally, and
Nghia Pham. 2013. Intensionality was only alleged:
On adjective-noun composition in distributional se-
mantics. In Proceedings of IWCS, pages 35?46, Pots-
dam, Germany.
Chris Callison-Burch and Mark Dredze. 2010. Creating
speech and language data with amazon?s mechanical
turk. In Proceedings of the NAACL HLT 2010 Work-
shop on Creating Speech and Language Data with
Amazon?s Mechanical Turk, pages 1?12, Los Angeles,
CA.
Kenneth Church and Peter Hanks. 1990. Word asso-
ciation norms, mutual information, and lexicography.
Computational Linguistics, 16(1):22?29.
Guglielmo Cinque, editor. 2002. Functional Structure in
DP and IP - The Carthography of Syntactic Structures,
volume 1. Oxford University Press.
Guglielmo Cinque. 2004. Issues in adverbial syntax.
Lingua, 114:683?710.
Guglielmo Cinque. 2010. The syntax of adjectives: a
comparative study. MIT Press.
Ann Copestake and Aure?lie Herbelot. 2011. Exciting
and interesting: issues in the generation of binomials.
In Proceedings of the UCNLG+ Eval: Language Gen-
eration and Evaluation Workshop, pages 45?53, Edin-
burgh, UK.
Paola Crisma. 1991. Functional categories inside the
noun phrase: A study on the distribution of nominal
modifiers. ?Tesi di Laurea?, University of Venice.
Georgiana Dinu, Nghia The Pham, and Marco Baroni.
2013a. DISSECT: DIStributional SEmantics Compo-
sition Toolkit. In Proceedings of the System Demon-
strations of ACL 2013, East Stroudsburg, PA.
Georgiana Dinu, Nghia The Pham, and Marco Baroni.
2013b. General estimation and evaluation of compo-
sitional distributional semantic models. In Proceed-
ings of the ACL 2013 Workshop on Continuous Vec-
tor Space Models and their Compositionality (CVSC
2013), East Stroudsburg, PA.
Gottlob Frege. 1892. U?ber sinn und bedeutung.
Zeitschrift fuer Philosophie un philosophische Kritik,
100.
Edward Grefenstette and Mehrnoosh Sadrzadeh. 2011.
Experimental support for a categorical compositional
distributional model of meaning. In Proceedings of
EMNLP, Edinburgh, UK.
Emiliano Guevara. 2010. A regression model of
adjective-noun compositionality in distributional se-
mantics. In Proceedings of the ACL GEMS Workshop,
pages 33?37, Uppsala, Sweden.
Chih-Jen Lin. 2007. Projected gradient methods for
Nonnegative Matrix Factorization. Neural Computa-
tion, 19(10):2756?2779.
Robert Malouf. 2000. The order of prenominal adjec-
tives in natural language generation. In Proceedings
of ACL, pages 85?92, East Stroudsburg, PA.
150
Jeff Mitchell and Mirella Lapata. 2010. Composition in
distributional models of semantics. Cognitive Science,
34(8):1388?1429.
Richard Montague. 1970. Universal Grammar. Theoria,
36:373?398.
Robert Munro, Steven Bethard, Victor Kuperman,
Vicky Tzuyin Lai, Robin Melnick, Christopher Potts,
Tyler Schnoebelen, and Harry Tily. 2010. Crowd-
sourcing and language studies: the new generation of
linguistic data. In Proceedings of the NAACL HLT
2010 Workshop on Creating Speech and Language
Data with Amazon?s Mechanical Turk, pages 122?130,
Los Angeles, CA.
Barbara Partee. 2004. Compositionality. In Compo-
sitionality in Formal Semantics: Selected Papers by
Barbara H. Partee. Blackwell, Oxford.
Magnus Sahlgren. 2006. The Word-Space Model. Dis-
sertation, Stockholm University.
Helmut Schmid. 1995. Improvements in part-of-speech
tagging with an application to German. In Proceed-
ings of the EACL-SIGDAT Workshop, Dublin, Ireland.
Hinrich Schu?tze. 1997. Ambiguity Resolution in Natural
Language Learning. CSLI, Stanford, CA.
Gary-John Scott. 2002. Stacked adjectival modification
and the structure of nominal phrases. In Guglielmo
Cinque, editor, Functional Structure in DP and IP. The
Carthography of Syntactic Structures, volume 1. Ox-
ford University Press.
Richard Socher, E.H. Huang, J. Pennington, Andrew Y.
Ng, and C.D. Manning. 2011. Dynamic pooling and
unfolding recursive autoencoders for paraphrase de-
tection. Advances in Neural Information Processing
Systems, 24:801?809.
Richard Socher, Brody Huval, Christopher D. Manning,
and Andrew Y. Ng. 2012. Semantic compositionality
through recursive matrix-vector spaces. In Proceed-
ings of EMNLP, pages 1201?1211, Edinburgh, UK.
Richard Sproat and Chilin Shih. 1990. The cross-
linguistics distribution of adjective ordering restric-
tions. In C. Georgopoulos and Ishihara R., editors,
Interdisciplinary approaches to language: essays in
honor of Yuki Kuroda, pages 565?593. Kluver, Dor-
drecht.
Sam Steddy and Vieri Samek-Lodovici. 2011. On the
ungrammaticality of remnant movement in the deriva-
tion of greenberg?s universal 20. Linguistic Inquiry,
42(3):445?469.
Richmond H. Thomason, editor. 1974. Formal Philoso-
phy: Selected Papers of Richard Montague. Yale Uni-
versity Press, New York.
Peter Turney and Patrick Pantel. 2010. From frequency
to meaning: Vector space models of semantics. Jour-
nal of Artificial Intelligence Research, 37:141?188.
Peter Turney. 2012. Domain and function: A dual-space
model of semantic relations and compositions. Jour-
nal of Artificial Intelligence Research, 44:533?585.
Eva Maria Vecchi, Marco Baroni, and Roberto Zampar-
elli. 2011. (Linear) maps of the impossible: Cap-
turing semantic anomalies in distributional space. In
Proceedings of the ACL Workshop on Distributional
Semantics and Compositionality, pages 1?9, Portland,
OR.
Fabio Zanzotto, Ioannis Korkontzelos, Francesca Faluc-
chi, and Suresh Manandhar. 2010. Estimating linear
models for compositional distributional semantics. In
Proceedings of COLING, pages 1263?1271, Beijing,
China.
151
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1908?1913,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Fish transporters and miracle homes:
How compositional distributional semantics can help NP parsing
Angeliki Lazaridou, Eva Maria Vecchi and Marco Baroni
Center for Mind/Brain Sciences
University of Trento, Italy
first.last@unitn.it
Abstract
In this work, we argue that measures that have
been shown to quantify the degree of semantic
plausibility of phrases, as obtained from their
compositionally-derived distributional seman-
tic representations, can resolve syntactic am-
biguities. We exploit this idea to choose the
correct parsing of NPs (e.g., (live fish) trans-
porter rather than live (fish transporter)). We
show that our plausibility cues outperform
a strong baseline and significantly improve
performance when used in combination with
state-of-the-art features.
1 Introduction
Live fish transporter: A transporter of live fish
or rather a fish transporter that is not dead?
While our intuition, based on the meaning of this
phrase, prefers the former interpretation, the Stan-
ford parser, which lacks semantic features, incor-
rectly predicts the latter as the correct parse.1 The
correct syntactic parsing of sentences is clearly
steered by semantic information (as formal syn-
tacticians have pointed out at least since Fillmore
(1968)), and consequently the semantic plausibil-
ity of alternative parses can provide crucial evidence
about their validity.
An emerging line of parsing research capitalizes
on the advances of compositional distributional se-
mantics (Baroni and Zamparelli, 2010; Guevara,
2010; Mitchell and Lapata, 2010; Socher et al,
2012). Information related to compositionally-
derived distributional representations of phrases is
1http://nlp.stanford.edu:8080/parser/
index.jsp
integrated at various stages of the parsing process
to improve overall performance.2 We are aware of
two very recent studies exploiting the semantic in-
formation provided by distributional models to re-
solve syntactic ambiguity: Socher et al (2013) and
Le et al (2013).
Socher et al (2013) present a recursive neural net-
work architecture which jointly learns semantic rep-
resentations and syntactic categories of phrases. By
annotating syntactic categories with their distribu-
tional representation, the method emulates lexical-
ized approaches (Collins, 2003) and captures sim-
ilarity more flexibly than solutions based on hard
clustering (Klein and Manning, 2003; Petrov et al,
2006). Thus, their approach mainly aims at improv-
ing parsing by capturing a richer, data-driven cate-
gorial structure.
On the other hand, Le et al (2013) work with the
output of the parser. Their hypothesis is that parses
that lead to less semantically plausible interpreta-
tions will be penalized by a reranker that looks at
the composed semantic representation of the parse.
Their method achieves an improvement of 0.2% in
F-score. However, as the authors also remark, be-
cause of their experimental setup, they cannot con-
clude that the improvement is truly due to the se-
mantic composition component, a crucial issue that
is deferred to further investigation.
This work aims at corroborating the hypothesis
that the semantic plausibility of a phrase can in-
deed determine its correct parsing. We develop a
system based on simple and intuitive measures, ex-
2Distributional representations approximate word and
phrase meaning by vectors that record the contexts in which
they are likely to appear in corpora; for a review see, e.g., Tur-
ney and Pantel (2010).
1908
Type of NP # Example
A (N N) 1296 local phone company
(A N) N 343 crude oil sector
N (N N) 164 miracle home run
(N N) N 424 blood pressure medicine
Total 2227 -
Table 1: NP dataset
tracted from the compositional distributional repre-
sentations of phrases, that have been shown to corre-
late with semantic plausibility (Vecchi et al, 2011).
We develop a controlled experimental setup, fo-
cusing on a single syntactic category, that is, noun
phrases (NP), where our task can be formalized as
(left or right) bracketing. Unlike previous work,
we compare our compositional semantic component
against features based on n-gram statistics, which
can arguably also capture some semantic informa-
tion in terms of frequent occurrences of meaningful
phrases. Inspired by previous literature demonstrat-
ing the power of metrics based on Pointwise Mu-
tual Information (PMI) in NP bracketing (Nakov and
Hearst, 2005; Pitler et al, 2010; Vadas and Curran,
2011), we test an approach exploiting PMI features,
and show that plausibility features relying on com-
posed representations can significantly boost accu-
racy over PMI.
2 Setup
Noun phrase dataset To construct our dataset,
we used the Penn TreeBank (Marcus et al, 1993),
which we enriched with the annotation provided by
Vadas and Curran (2007a), since the original tree-
bank does not distinguish different structures inside
the NPs and always marks them as right bracketed,
e.g., local (phone company) but also blood (pressure
medicine). We focus on NPs formed by three ele-
ments, where the first can be an adjective (A) or a
noun (N), the other two are nouns. Table 1 summa-
rizes the characteristics of the dataset.3
Distributional semantic space As our source cor-
pus we use the concatenation of ukWaC, the English
Wikipedia (2009 dump) and the BNC, with a total of
3The dataset is available from: http://clic.cimec.
unitn.it/composes
about 2.8 billion tokens.4 We collect co-occurrence
statistics for the top 8K Ns and 4K As, plus any
other word from our NP dataset that was below this
rank. Our context elements are composed of the top
10K content words (adjectives, adverbs, nouns and
verbs). We use a standard bag-of-words approach,
counting within-sentence collocates for every target
word. We apply (non-negative) Pointwise Mutual
Information as weighting scheme and dimensional-
ity reduction using Non-negative Matrix Factoriza-
tion, setting the number of reduced-space dimen-
sions to 300.5
Composition functions We experiment with vari-
ous composition functions, chosen among those sen-
sitive to internal structure (Baroni and Zamparelli,
2010; Guevara, 2010; Mitchell and Lapata, 2010),
namely dilation (dil), weighted additive (wadd), lex-
ical function (lexfunc) and full additive (fulladd).6
For model implementation and (unsupervised) es-
timation, we rely on the freely available DISSECT
toolkit (Dinu et al, 2013).7 For all methods, vectors
were normalized before composing, both in training
and in generation. Table 2 presents a summary de-
scription of the composition methods we used.
Following previous literature (Mitchell and Lap-
ata, 2010), and the general intuition that adjectival
modification is quite a different process from noun
combination (Gagne? and Spalding, 2009; McNally,
2013), we learn different parameters for noun-noun
(NN) and adjective-noun (AN) phrases. As an ex-
ample of the learned parameters, for the wadd model
the ratio of parameters w1 and w2 is 1:2 for ANs,
whereas for NNs it is almost 1:1, confirming the in-
tuition that a non-head noun plays a stronger role in
composition than an adjective modifier.
4http://wacky.sslmit.unibo.it, http://en.
wikipedia.org, http://www.natcorp.ox.ac.uk
5For tuning the parameters of the semantic space, we com-
puted the correlation of cosines produced with a variety of pa-
rameter settings (SVD/NMF/no reduction, PMI/Local MI/raw
counts/log transform, 150 to 300 dimensions in steps of 50) with
the word pair similarity ratings in the MEN dataset: http:
//clic.cimec.unitn.it/?elia.bruni/MEN
6We do not consider the popular multiplicative model, as it
produces identical representations for NPs irrespective of their
internal structure.
7http://clic.cimec.unitn.it/composes/
toolkit/
1909
Model Composition function Parameters
wadd w1~u+ w2~v w1, w2
dil ||~u||22~v + (?? 1)?~u,~v?~u ?
fulladd W1~u+W2~v W1,W2 ? Rm?m
lexfunc Au~v Au ? Rm?m
Table 2: Composition functions of inputs (u, v).
Recursive composition In this study we also ex-
periment with recursive composition; to the best
of our knowledge, this is the first time that these
composition functions have been explicitly used in
this manner. For example, given the left brack-
eted NP (blood pressure) medicine, we want to
obtain its compositional semantic representation,
???????????????????
blood pressure medicine. First, basic composition
is applied, in which
????
blood and ???????pressure are com-
bined with one of the composition functions. Fol-
lowing that, we apply recursive composition; the
output of basic composition, i.e.,
???????????
blood pressure,
is fed to the function again to be composed with the
representation of
???????
medicine.
The latter step is straightforward for all com-
position functions except lexfunc applied to left-
bracketed NPs, where the first step should return a
matrix representing the left constituent (blood pres-
sure in the running example). To cope with this nui-
sance, we apply the lexfunc method to basic compo-
sition only, while recursive representations are de-
rived by summing (e.g.,
???????????
blood pressure is obtained
by multiplying the blood matrix by the pressure vec-
tor, and it is then summed to
???????
medicine).
3 Experiments
Semantic plausibility measures We use mea-
sures of semantic plausibility computed on com-
posed semantic representations introduced by Vec-
chi et al (2011). The rationale is that the correct
(wrong) bracketing will lead to semantically more
(less) plausible phrases. Thus, a measure able to dis-
criminate semantically plausible from implausible
phrases should also indicate the most likely parse.
Considering, for example, the alternative parses of
miracle home run, we observe that home run is
a more semantically plausible phrase than miracle
home. Furthermore, we might often refer to a base-
ball player?s miracle home run, but we doubt that
even a miracle home can run! Given the com-
posed representation of an AN (or NN), Vecchi et
al. (2011) define the following measures:
? Density, quantified as the average cosine of a
phrase with its (top 10) nearest neighbors, cap-
tures the intuition that a deviant phrase should
be isolated in the semantic space.
? Cosine of phrase and head N aims to capture
the fact that the meaning of a deviant AN (or
NN) will tend to diverge from the meaning of
the head noun.
? Vector length should capture anomalous vec-
tors.
Since length, as already observed by Vecchi et al,
is strongly affected by independent factors such as
input vector normalization and the estimation pro-
cedure, we introduce entropy as a measure of vec-
tor quality. The intuition is that meaningless vec-
tors, whose dimensions contain mostly noise, should
have high entropy.
NP Parsing as Classification Parsing NPs con-
sisting of three elements can be treated as bi-
nary classification; given blood pressure medicine,
we predict whether it is left- ((blood pres-
sure) medicine) or right-bracketed (blood (pressure
medicine)).
We conduct experiments using an SVM with Ra-
dial Basis Function kernel as implemented in the
scikit-learn toolkit.8 Our dataset is split into 10 folds
in which the ratio between the two classes is kept
constant. We tune the SVM complexity parameter
C on the first fold and we report accuracy results on
the remaining nine folds after cross-validation.
Features Given a composition function f , we de-
fine the following feature sets, illustrated with the
usual blood pressure medicine example, which are
used to build different classifiers:
? fbasic consists of the semantic plausibility
measures described above computed for the
two-word phrases resulting from alternative
bracketings, i.e., 3 measures for each bracket-
ing, evaluated on blood pressure and pressure
medicine respectively, for a total of 6 features.
? frec contains 6 features computed on the vec-
tors resulting from the recursive compositions
8http://scikit-learn.org/
1910
Features Accuracy
right 65.6
pos 77.3
lexfuncbasic 74.6
lexfuncrec 74.0
lexfuncplausibility 76.2
waddbasic 75.9
waddrec 78.2
waddplausibility 78.7
pmi 81.2
pmi+lexfuncplausibility 82.9
pmi+waddplausibility 85.6
Table 3: Evaluation of feature sets from Section 3
(blood pressure) medicine and blood (pressure
medicine).
? fplausibility concatenates fbasic and frec.
? pmi contains the PMI scores extracted from
our corpus for blood pressure and pressure
medicine.9
? pmi + fplausibility concatenates pmi and
fplausibility.
Baseline Model Given the skewed bracketing dis-
tribution in our dataset, we implement the following
majority baselines: a) right classifies all phrases
as right-bracketed; b) pos classifies NNN as left-
bracketed (Lauer, 1995), ANN as right-bracketed.
4 Results and Discussion
Table 3 omits results for dil and fulladd since they
were outperformed by the right baseline. That
wadd- and lexfunc-based plausibility features per-
form well above this baseline is encouraging, since
it represents the typical default behaviour of parsers
for NPs, although note that these features perform
comparably to the pos baseline, which would be
quite simple to embed in a parser (for English, at
least). For both models, using both basic and recur-
sive features leads to a boost in performance over
basic features alone. Note that recursive features
(frec) achieve at least equal or better performance
than basic ones (fbasic). We expect indeed that in
many cases the asymmetry in plausibility will be
9Several approaches to computing PMI for these purposes
have been proposed in the literature including the dependency
model (Lauer, 1995) and the adjacency model (Marcus, 1980).
We implement the latter since it has been shown to perform
better (Vadas and Curran, 2007b) on NPs extracted from Penn
TreeBank.
sharper when considering the whole NP rather than
its sub-parts; a pressure medicine is still a conceiv-
able concept, but blood (pressure medicine) makes
no sense whatsoever. Finally, wadd outperforms
both the more informative baseline pos and lexfunc.
The difference between wadd and lexfunc is signif-
icant (p < 0.05)10 only when they are trained with
recursive composition features, probably due to our
suboptimal adaptation of the latter to recursive com-
position (see Section 2).
The pmi approach outperforms the best
plausibility-based feature set waddplausibility.
However, the two make only a small proportion of
common errors (29% of the total waddplausibility
errors, 32% for pmi), suggesting that they are com-
plementary. Indeed the pmi + waddplausibility
combination significantly outperforms pmi alone
(p < 0.001), indicating that plausibility features
can improve NP bracketing on top of the pow-
erful PMI-based approach. The same effect can
also be observed in the combination of pmi +
lexfuncplausibility, which again significantly
outperforms pmi alone (p < 0.05). This behaviour
further suggests that the different types of errors are
not a result of the parameters or type of composi-
tion applied, but rather highlights fundamental dif-
ferences in the kind of information that PMI and
composition models are able to capture.
One hypothesis is that compositional models are
more robust for low-frequency NPs, for which
PMI estimates will be less accurate; results on
those low-frequency trigrams only (20% of the NP
dataset, operationalized as those consisting of bi-
grams with frequency ? 100) revealed indeed that
waddplausibility performed 8.1% better in terms
of accuracy than pmi.
5 Conclusion
Our pilot study showed that semantic plausibility,
as measured on compositional distributional repre-
sentations, can improve syntactic parsing of NPs.
Our results further suggest that state-of-the-art PMI
features and the ones extracted from compositional
representations are complementary, and thus, when
combined, can lead to significantly better results.
Besides paving the way to a more general integration
10Significance values are based on t-tests.
1911
of compositional distributional semantics in syntac-
tic parsing, the proposed methodology provides a
new way to evaluate composition functions.
The relatively simple-minded wadd approach out-
performed more complex models such as lexfunc.
We plan to experiment next with more linguistically
motivated ways to adapt the latter to recursive com-
position, including hybrid methods where ANs and
NNs are treated differently. We would also like
to consider more sophisticated semantic plausibility
measures (e.g., supervised ones), and apply them to
other ambiguous syntactic constructions.
6 Acknowledgments
We thank Georgiana Dinu and German Kruszewski
for helpful discussions and the reviewers for use-
ful feedback. This research was supported by the
ERC 2011 Starting Independent Research Grant
n. 283554 (COMPOSES).
References
Marco Baroni and Roberto Zamparelli. 2010. Nouns
are vectors, adjectives are matrices: Representing
adjective-noun constructions in semantic space. In
Proceedings of EMNLP, pages 1183?1193, Boston,
MA.
Michael Collins. 2003. Head-driven statistical models
for natural language parsing. Computational linguis-
tics, 29(4):589?637.
Georgiana Dinu, Nghia The Pham, and Marco Baroni.
2013. DISSECT: DIStributional SEmantics Composi-
tion Toolkit. In Proceedings of the System Demonstra-
tions of ACL 2013, Sofia, Bulgaria.
Charles Fillmore. 1968. The case for case. In Emmon
Bach and Robert Harms, editors, Universals in Lin-
guistic Theory, pages 1?89. Holt, Rinehart and Win-
ston, New York.
Christina Gagne? and Thomas Spalding. 2009. Con-
stituent integration during the processing of compound
words: Does it involve the use of relational structures?
Journal of Memory and Language, 60:20?35.
Emiliano Guevara. 2010. A regression model of
adjective-noun compositionality in distributional se-
mantics. In Proceedings of GEMS, pages 33?37, Up-
psala, Sweden.
Dan Klein and Christopher D Manning. 2003. Accurate
unlexicalized parsing. In Proceedings of ACL, pages
423?430. Association for Computational Linguistics.
Mark Lauer. 1995. Corpus statistics meet the noun com-
pound: Some empirical results. In Proceedings of
the Annual Meeting on Association for Computational
Linguistics, pages 47?54, Cambridge, MA.
Phong Le, Willem Zuidema, and Remko Scha. 2013.
Learning from errors: Using vector-based composi-
tional semantics for parse reranking. In Proceedings of
the ACL 2013 Workshop on Continuous Vector Space
Models and their Compositionality, Sofia, Bulgaria.
Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beat-
rice Santorini. 1993. Building a large annotated cor-
pus of English: The Penn Treebank. Computational
linguistics, 19(2):313?330.
Mitchell P Marcus. 1980. Theory of syntactic recogni-
tion for natural languages. MIT press.
Louise McNally. 2013. Modification. In Maria Aloni
and Paul Dekker, editors, Cambridge Handbook of
Semantics. Cambridge University Press, Cambridge,
UK. In press.
Jeff Mitchell and Mirella Lapata. 2010. Composition in
distributional models of semantics. Cognitive Science,
34(8):1388?1429.
Preslav Nakov and Marti Hearst. 2005. Search en-
gine statistics beyond the n-gram: Application to noun
compound bracketing. In Proceedings of CoNLL,
pages 17?24, Stroudsburg, PA, USA.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and inter-
pretable tree annotation. In Proceedings of COLING-
ACL, pages 433?440, Stroudsburg, PA, USA.
Emily Pitler, Shane Bergsma, Dekang Lin, and Kenneth
Church. 2010. Using web-scale n-grams to improve
base NP parsing performance. In Proceedings of the
COLING, pages 886?894, Beijing, China.
Richard Socher, Brody Huval, Christopher Manning,
and Andrew Ng. 2012. Semantic compositionality
through recursive matrix-vector spaces. In Proceed-
ings of EMNLP, pages 1201?1211, Jeju Island, Korea.
Richard Socher, John Bauer, Christopher D. Manning,
and Andrew Y. Ng. 2013. Parsing with compositional
vector grammars. In Proceedings of ACL, Sofia, Bul-
garia.
Peter Turney and Patrick Pantel. 2010. From frequency
to meaning: Vector space models of semantics. Jour-
nal of Artificial Intelligence Research, 37:141?188.
David Vadas and James Curran. 2007a. Adding noun
phrase structure to the Penn Treebank. In Proceedings
of ACL, pages 240?247, Prague, Czech Republic.
David Vadas and James R Curran. 2007b. Large-scale
supervised models for noun phrase bracketing. In Pro-
ceedings of the PACLING, pages 104?112.
David Vadas and James R. Curran. 2011. Parsing
noun phrases in the penn treebank. Comput. Linguist.,
37(4):753?809.
1912
Eva Maria Vecchi, Marco Baroni, and Roberto Zampar-
elli. 2011. (Linear) maps of the impossible: Cap-
turing semantic anomalies in distributional space. In
Proceedings of the ACL Workshop on Distributional
Semantics and Compositionality, pages 1?9, Portland,
OR.
1913
Proceedings of the Workshop on Distributional Semantics and Compositionality (DiSCo?2011), pages 1?9,
Portland, Oregon, 24 June 2011. c?2011 Association for Computational Linguistics
(Linear) Maps of the Impossible:
Capturing semantic anomalies in distributional space
Eva Maria Vecchi and Marco Baroni and Roberto Zamparelli
Center for Mind/Brain Sciences, University of Trento
Rovereto (TN), Italy
{evamaria.vecchi-1,marco.baroni,roberto.zamparelli}@unitn.it
Abstract
In this paper, we present a first attempt to
characterize the semantic deviance of com-
posite expressions in distributional seman-
tics. Specifically, we look for properties of
adjective-noun combinations within a vector-
based semantic space that might cue their lack
of meaning. We evaluate four different com-
positionality models shown to have various
levels of success in representing the mean-
ing of AN pairs: the simple additive and
multiplicative models of Mitchell and Lap-
ata (2008), and the linear-map-based models
of Guevara (2010) and Baroni and Zamparelli
(2010). For each model, we generate com-
posite vectors for a set of AN combinations
unattested in the source corpus and which
have been deemed either acceptable or seman-
tically deviant. We then compute measures
that might cue semantic anomaly, and com-
pare each model?s results for the two classes of
ANs. Our study shows that simple, unsuper-
vised cues can indeed significantly tell unat-
tested but acceptable ANs apart from impos-
sible, or deviant, ANs, and that the simple ad-
ditive and multiplicative models are the most
effective in this task.
1 Introduction
Statistical approaches to describe, represent and un-
derstand natural language have been criticized as
failing to account for linguistic ?creativity?, a prop-
erty which has been accredited to the compositional
nature of natural language. Specifically, criticisms
against statistical methods were based on the ar-
gument that a corpus cannot significantly sample a
natural language because natural language is infi-
nite (Chomsky, 1957). This cricticism also applies
to distributional semantic models that build seman-
tic representations of words or phrases in terms of
vectors recording their distributional co-occurrence
patterns in a corpus (Turney and Pantel, 2010), but
have no obvious way to generalize to word combi-
nations that have not been observed in the corpus.
To address this problem, there have been several re-
cent attempts to incorporate into distributional se-
mantic models a component that generates vectors
for unseen linguistic structures by compositional op-
erations in the vector space (Baroni and Zamparelli,
2010; Guevara, 2010; Mitchell and Lapata, 2010).
The ability to work with unattested data leads to
the question of why a linguistic expression might
not be attested in even an extremely large and well-
balanced corpus. Its absence might be motivated
by a number of factors: pure chance, the fact that
the expression is ungrammatical, uses a rare struc-
ture, describes false facts, or, finally, is nonsensi-
cal. One criticism from generative linguists is pre-
cisely that statistical methods could not distinguish
between these various possibilities.
The difficulty of solving this problem can be il-
lustrated by the difference in semantics between the
adjective-noun pairs in (1a) and (1b):
(1) a. blue rose
b. residential steak
Although it may be the case that you have never ac-
1
tually seen a blue rose, the concept is not inconceiv-
able. On the other hand, the concept of a residen-
tial steak is rather unimaginable, and intuitively its
absence in a corpus is motivated by more than just
chance or data sparseness.
The present paper is a first attempt to use com-
positionality and distributional measures to distin-
guish nonsensical, or semantically deviant, linguis-
tic expression from other types of unattested struc-
tures. The task of distinguishing between unattested
but acceptable and unattested but semantically de-
viant linguistic expressions is not only a way to ad-
dress the criticism about the meaning of ?unattest-
edness?, but also a task that could have a large im-
pact on the (computational) linguistic community as
a whole (see Section 2.1).
Our specific goal is to automatically detect se-
mantic deviance in attributive Adjective-Noun (AN)
expressions, using a small number of simple cues in
the vectorial representation of an AN as it is gener-
ated from the distributional vectors of its component
A and N by four compositional models found in the
literature. The choice of AN as our testbed is moti-
vated by two facts: first of all, ANs are common,
small constituents containing no functional mate-
rial, and secondly, ANs have already been studied in
compositional distributional semantics (Baroni and
Zamparelli, 2010; Guevara, 2010; Mitchell and La-
pata, 2010).
It is important to note that in this research we talk
about ?semantically deviant? expressions, but we do
not exclude the possibility that such expressions are
interpreted as metaphors, via a chain of associations.
In fact, distributional measures are desirable models
to account for this, since they naturally lead to a gra-
dient notion of semantic anomaly.
The rest of this paper is structured as follows.
Section 2 discusses relevant earlier work, introduc-
ing the literature on semantic deviance as well as
compositional methods in distributional semantics.
Section 3 presents some hypotheses about cues of
semantic deviance in distributional space. Our ex-
perimental setup and procedure are detailed in Sec-
tion 4, whereas the experiments? results are pre-
sented and analyzed in Section 5. We conclude by
summarizing and proposing future directions in Sec-
tion 6.
2 Related work
2.1 Semantic deviance
As far as we know, we are the first to try to model
semantic deviance using distributional methods, but
the issue of when a complex linguistic expression is
semantically deviant has been addressed since the
1950?s in various areas of linguistics. In compu-
tational linguistics, the possibility of detecting se-
mantic deviance has been seen as a prerequisite to
access metaphorical/non-literal semantic interpreta-
tions (Fass and Wilks, 1983; Zhou et al, 2007). In
psycholinguistics, it has been part of a wide debate
on the point at which context can make us perceive a
?literal? vs. a ?figurative? meaning (Giora, 2002). In
theoretical generative linguistics, the issue was orig-
inally part of a discussion on the boundaries between
syntax and semantics. Cases like Chomsky?s clas-
sic ?Colorless green ideas sleep furiously? can actu-
ally be regarded as violations of very fine-grained
syntactic selectional restrictions on the arguments
of verbs or modifiers, on the model of *much com-
puter (arguably a failure of much to combine with a
noun +COUNT). By 1977, even Chomsky doubted
that speakers could in general have intuitions about
whether ill-formedness was syntactic or semantic
(Chomsky, 1977, p. 4). The spirit of the selectional
approach persists in Asher (2011), who proposes a
detailed system of semantic types plus a theory of
type coercion, designed to account for the shift in
meaning seen in, e.g., (2) (lunch as food or as an
event).
(2) Lunch was delicious but took forever.
A practical problem with this approach is that a
full handmade specification of the features that de-
termine semantic compatibility is a very expensive
and time-consuming enterprise, and it should be
done consistently across the whole content lexicon.
Moreover, it is unclear how to model the intuition
that naval fraction, musical North or institutional
acid sound odd, in the absence of very particular
contexts, while (2) sounds quite natural. Whatever
the nature of coercion, we do not want it to run so
smoothly that any combination of A and N (or V and
its arguments) becomes meaningful and completely
acceptable.
2
2.2 Distributional approaches to meaning
composition
Although the issue of how to compose meaning has
attracted interest since the early days of distribu-
tional semantics (Landauer and Dumais, 1997), re-
cently a very general framework for modeling com-
positionality has been proposed by Mitchell and La-
pata (Mitchell and Lapata, 2008; Mitchell and La-
pata, 2009; Mitchell and Lapata, 2010). Given two
vectors u and v, they identify two general classes of
composition models, (linear) additive models:
p = Au + Bv (1)
where A and B are weight matrices, and multiplica-
tive models:
p = Cuv
where C is a weight tensor projecting the uv ten-
sor product onto the space of p. Mitchell and La-
pata derive two simplified models from these gen-
eral forms: The simplified additive model given by
p = ?u + ?v, and a simplified multiplicative ap-
proach that reduces to component-wise multiplica-
tion, where the i-th component of the composed vec-
tor is given by: pi = uivi. Mitchell and Lapata
evaluate the simplified models on a wide range of
tasks ranging from paraphrasing to statistical lan-
guage modeling to predicting similarity intuitions.
Both simple models fare quite well across tasks
and alternative semantic representations, also when
compared to more complex methods derived from
the equations above. Given their overall simplic-
ity, good performance and the fact that they have
also been extensively tested in other studies (Baroni
and Zamparelli, 2010; Erk and Pado?, 2008; Guevara,
2010; Kintsch, 2001; Landauer and Dumais, 1997),
we re-implement here both the simplified additive
and simplified multiplicative methods (we do not,
however, attempt to tune the weights of the additive
model, although we do apply a scalar normalization
constant to the adjective and noun vectors).
Mitchell and Lapata (as well as earlier re-
searchers) do not exploit corpus evidence about
the p vectors that result from composition, despite
the fact that it is straightforward (at least for short
constructions) to extract direct distributional evi-
dence about the composite items from the corpus
(just collect co-occurrence information for the com-
posite item from windows around the contexts in
which it occurs). The main innovation of Guevara
(2010), who focuses on adjective-noun combina-
tions (AN), is to use the co-occurrence vectors of
corpus-observed ANs to train a supervised compo-
sition model. Guevara, whose approach we also re-
implement here, adopts the full additive composi-
tion form from Equation (1) and he estimates the
A and B weights (concatenated into a single ma-
trix, that acts as a linear map from the space of con-
catenated adjective and noun vectors onto the AN
vector space) using partial least squares regression.
The training data are pairs of adjective-noun vec-
tor concatenations, as input, and corpus-derived AN
vectors, as output. Guevara compares his model
to the simplified additive and multiplicative models
of Mitchell and Lapata. Corpus-observed ANs are
nearer, in the space of observed and predicted test
set ANs, to the ANs generated by his model than
to those from the alternative approaches. The addi-
tive model, on the other hand, is best in terms of
shared neighbor count between observed and pre-
dicted ANs.
The final approach we re-implement is the one
proposed by Baroni and Zamparelli (2010), who
treat attributive adjectives as functions from noun
meanings to noun meanings. This is a standard ap-
proach in Montague semantics (Thomason, 1974),
except noun meanings here are distributional vec-
tors, not denotations, and adjectives are (linear)
functions learned from a large corpus. Unlike in
Guevara?s approach, a separate matrix is generated
for each adjective using only examples of ANs con-
taining that adjective, and no adjective vector is
used: the adjective is represented entirely by the ma-
trix mapping nouns to ANs. In terms of Mitchell
and Lapata?s general framework, this approach de-
rives from the additive form in Equation (1) with the
matrix multiplying the adjective vector (say, A) set
to 0, the other matrix (B) representing the adjective
at hand, and v a noun vector. Baroni and Zamparelli
(2010) show that their model significantly outper-
forms other vector composition methods, including
addition, multiplication and Guevara?s approach, in
the task of approximating the correct vectors for pre-
viously unseen (but corpus-attested) ANs. Simple
addition emerges as the second best model.
3
See Section 4.3 below for details on our re-
implementations. Note that they follow very closely
the procedure of Baroni and Zamparelli (2010), in-
cluding choices of source corpus and parameter val-
ues, so that we expect their results on the quality of
the various models in predicting ANs to also hold
for our re-implementations.
3 Simple indices of semantic deviance
We consider here a few simple, unsupervised mea-
sures to help us distinguish the representation that a
distributional composition model generates for a se-
mantically anomalous AN from the one it generates
for a semantically acceptable AN. In both cases, we
assume that the AN is not already part of the model
semantic space, just like you can distinguish be-
tween parliamentary tomato (odd) and marble iPad
(OK), although you probably never heard either ex-
pression.
We hypothesize that, since the values in the di-
mensions of a semantic space are a distributional
proxy to the meaning of an expression, a mean-
ingless expression should in general have low val-
ues across the semantic space dimensions. For ex-
ample, a parliamentary tomato, no longer being a
vegetable but being an unlikely parliamentary event,
might have low values on both dimensions char-
acterizing vegetables and dimensions characterizing
events. Thus, our first simple measure of seman-
tic anomaly is the length of the model-generated
AN. We hypothesize that anomalous AN vectors are
shorter than acceptable ANs.
Second, if deviant composition destroys or ran-
domizes the meaning of a noun, as a side effect we
might expect the resulting AN to be more distant, in
the semantic space, from the component noun. Al-
though even a marble iPad might have lost some es-
sential properties of iPads (it could for example be
an iPad statue you cannot use as a tablet), to the ex-
tent that we can make sense of it, it must retain at
least some characteristics of iPads (at the very least,
it will be shaped like an iPad). On the other hand, we
cannot imagine what a parliamentary tomato should
be, and thus cannot attribute even a subset of the reg-
ular tomato properties to it. We thus hypothesize that
model-generated vectors of deviant ANs will form
a wider angle (equivalently, will have a lower co-
sine) with the corresponding N vectors than accept-
able ANs.
Finally, if an AN makes no sense, its model-
generated vector should not have many neighbours
in the semantic space, since our semantic space is
populated by nouns, adjectives and ANs that are
commonly encountered in the corpus, and should
thus be meaningful. We expect deviant ANs to
be ?semantically isolated?, a notion that we opera-
tionalize in terms of a (neighborhood) density mea-
sure, namely the average cosine with the (top 10)
nearest neighbours. We hypothesize that model-
generated vectors of deviant ANs will have lower
density than model-generated acceptable ANs.
4 Experimental setup
4.1 Semantic space
Our initial step was to construct a semantic space for
our experiments, consisting of a matrix where each
row vector represents an adjective, noun or AN. We
first introduce the source corpus, then the vocabulary
of words and ANs that we represent in the space,
and finally the procedure adopted to build the vec-
tors representing the vocabulary items from corpus
statistics, in order to obtain the semantic space ma-
trix. We work here with a ?vanilla? semantic space
(essentially, we follow the steps of Baroni and Zam-
parelli (2010)), since our focus is on the effect of
different composition methods given a common se-
mantic space. We leave it to further work to study
how choices in semantic space construction affect
composition operations.
4.1.1 Source corpus
We use as our source corpus the concate-
nation of the Web-derived ukWaC corpus
(http://wacky.sslmit.unibo.it/),
a mid-2009 dump of the English Wikipedia
(http://en.wikipedia.org) and the British
National Corpus (http://www.natcorp.
ox.ac.uk/). The corpus has been tokenized,
POS-tagged and lemmatized with the TreeTagger
(Schmid, 1995), and it contains about 2.8 billion
tokens. We extract all statistics at the lemma level,
ignoring inflectional information.
4
4.1.2 Semantic space vocabulary
The words/ANs in the semantic space must of
course include the items that we need for our exper-
iments (adjectives, nouns and ANs used for model
training and as input to composition). Moreover, in
order to study the behaviour of the test items we are
interested in (that is, model-generated AN vectors)
within a large and less ad-hoc space, we also include
many more adjectives, nouns and ANs in our vocab-
ulary not directly relevant to our experimental ma-
nipulations.
We populate our semantic space with the 8K most
frequent nouns and 4K most frequent adjectives
from the corpus (excluding, in both cases, the top
50 most frequent elements). We extended this vo-
cabulary to include two sets of ANs (33K ANs cu-
mulatively), for a total of 45K vocabulary items in
the semantic space.
To create the ANs needed to run and evaluate the
experiments described below, we focused on a set
of adjectives which are very frequent in the corpus
so that they will be in general able to combine with
wide classes of nouns, making the unattested cases
more interesting, but not so frequent as to have such
a general meaning that would permit a free combi-
nation with nearly any noun. The ANs were there-
fore generated by crossing a selected set of 200 very
frequent adjectives (adjectives attested in the corpus
at least 47K times, and at most 740K) and the set
of the 8K nouns in our semantic space vocabulary,
producing a set of 4.92M generated ANs.
The first set of ANs included in the semantic
space vocabulary is a randomly sampled set of 30K
ANs from the generated set which are attested in
the corpus at least 200 times (to avoid noise and fo-
cus on ANs for which we can extract reasonably ro-
bust distributional data). We also extracted any unat-
tested ANs from the set of generated set (about 3.5M
unattested ANs), putting them aside to later assem-
ble our evaluation material, described in Section 4.2.
To add further variety to the semantic space, we
included a less controlled second set of 3K ANs ran-
domly picked among those that are attested and are
formed by the combination of any of the 4K adjec-
tives and 8K nouns in the vocabulary.
4.1.3 Semantic space construction
For each of the items in our vocabulary, we first
build 10K-dimensional vectors by recording their
sentence-internal co-occurrence with the top 10K
most frequent content words (nouns, adjectives or
verbs) in the corpus. The raw co-occurrence counts
are then transformed into Local Mutual Information
scores (Local Mutual Information is an association
measure that closely approximates the commonly
used Log-Likelihood Ratio while being simpler to
compute (Baroni and Lenci, 2010; Evert, 2005)).
Next, we reduce the full co-occurrence matrix
applying the Singular Value Decomposition (SVD)
operation, like in LSA and related distributional
semantic methods (Landauer and Dumais, 1997;
Rapp, 2003; Schu?tze, 1997). The original 45K-by-
10K-dimensional matrix is reduced in this way to a
45K-by-300 matrix, where vocabulary items are rep-
resented by their coordinates in the space spanned
by the first 300 right singular vectors of the SVD
solution. This step is motivated by the fact that we
will estimate linear models to predict the values of
each dimension of an AN from the dimensions of the
components. We thus prefer to work in a smaller and
denser space. As a sanity check, we verify that we
obtain state-of-the-art-range results on various se-
mantic tasks using this reduced semantic space (not
reported here for space reason).
4.2 Evaluation materials
Our goal is to study what happens when composi-
tional methods are used to construct a distributional
representation for ANs that are semantically deviant,
compared to the AN representations they generate
for ANs they have not encountered before, but that
are semantically acceptable.
In order to assemble these lists, we started from
the set of 3.5M unattested ANs described in Sec-
tion 4.1.2 above, focusing on 30 randomly chosen
adjectives. For each of these, we randomly picked
100 ANs for manual inspection (3K ANs in total).
Two authors went through this list, marking those
ANs that they found semantically highly anomalous,
no matter how much effort one would put in con-
structing metaphorical or context-dependent inter-
pretations, as well as those they found completely
acceptable (so, rating was on a 3-way scale: deviant,
5
intermediate, acceptable). The rating exercise re-
sulted in rather low agreement (Cohen?s ?=0.32),
but we reasoned that those relatively few cases (456
over 3K) where both judges agreed the AN was odd
should indeed be odd, and similarly for the even
rarer cases in which they agreed an AN was com-
pletely acceptable (334 over 3K). We thus used the
agreed deviant and acceptable ANs as test data.
Of 30 adjectives, 5 were discarded for either tech-
nical reasons or for having less than 5 agreed de-
viant or acceptable ANs. This left us with a de-
viant AN test set comprising of 413 ANs, on av-
erage 16 for each of the 25 remaining adjectives.
Some examples of ANs in this set are: academic
bladder, blind pronunciation, parliamentary potato
and sharp glue. The acceptable (but unattested) AN
test set contains 280 ANs, on average 11 for each of
the 25 studied adjectives. Examples of ANs in this
set include: vulnerable gunman, huge joystick, aca-
demic crusade and blind cook. The evaluation sets
can be downloaded from http://www.vecchi.
com/eva/resources.html.
There is no significant difference between the
length of the vectors of the component nouns in the
acceptable vs. deviant AN sets (two-tailed Welch?s t
test; t=?0.25; p>0.8). This is important, since at
least one of the potential cues to deviance we con-
sider (AN vector length) is length-dependent, and
we do not want a trivial result that can simply be
explained by systematic differences in the length of
the input vectors.
4.3 Composition methods
As discussed in Section 2.2, the experiment was car-
ried out across four compositional methods.
Additive AN vectors (add method) are simply
obtained by summing the corresponding adjective
and noun vectors after normalizing them. Multi-
plicative vectors (mult method) were obtained by
component-wise multiplication of the adjective and
noun vectors, also after normalization. Confirm-
ing the results of Baroni and Zamparelli (2010),
non-normalized versions of add and mult were also
tested, but did not produce significant results (in
the case of multiplication, normalization amounts to
multiplying the composite vector by a scalar, so it
only affects the length-dependent vector length mea-
sure). It is important to note that, as reported in
Baroni and Zamparelli (2010), the mult method can
be expected to perform better in the original, non-
reduced semantic space because the SVD dimen-
sions can have negative values, leading to counter-
intuitive results with component-wise multiplication
(multiplying large opposite-sign values results in
large negative values instead of being cancelled out).
The tests of Section 5, however, are each run in the
SVD-reduced space to remain consistent across all
models. We leave it to future work to explore the
effect on the performance of using the non-reduced
space for the models for which this option is com-
putationally viable.
In the linear map (lm) approach proposed by
Guevara (2010), a composite AN vector is obtained
by multiplying a weight matrix by the concatenation
of the adjective and noun vectors, so that each di-
mension of the generated AN vector is a linear com-
bination of dimensions of the corresponding adjec-
tive and noun vectors. That is, the 600 weights in
each of the 300 rows of the weight matrix are the
coefficients of a linear equation predicting the val-
ues of a single dimension in the AN vector as a lin-
ear combination (weighted sum) of the 300 adjective
and 300 noun dimensions. Following Guevara, we
estimate the coefficients of the equation using (mul-
tivariate) partial least squares regression (PLSR) as
implemented in the R pls package (Mevik and
Wehrens, 2007), with the latent dimension param-
eter of PLSR set to 50, the same value used by Ba-
roni and Zamparelli (2010). Coefficient matrix es-
timation is performed by feeding the PLSR a set
of input-output examples, where the input is given
by concatenated adjective and noun vectors, and the
output is the vector of the corresponding AN directly
extracted from our semantic space (i.e., the AN vec-
tors used in training are not model-generated, but
directly derived from corpus evidence about their
distribution). The matrix is estimated using a ran-
dom sample of 2K adjective-noun-AN tuples where
the AN belongs to the set of 30K frequently attested
ANs in our vocabulary.
Finally, in the adjective-specific linear map
(alm) method of Baroni and Zamparelli (2010), an
AN is generated by multiplying an adjective weight
matrix with a noun vector. The weights of each of
the 300 rows of the weight matrix are the coefficients
of a linear equation predicting the values of one of
6
the dimensions of the AN vector as a linear com-
bination of the 300 dimensions of the component
noun. The linear equation coefficients are estimated
separately for each of the 25 tested adjectives from
the attested noun-AN pairs containing that adjective
(observed adjective vectors are not used), again us-
ing PLSR with the same parameter as above. For
each adjective, the training N-AN vector pairs cho-
sen are those available in the semantic space for each
test set adjective, and range from 100 to more than
500 items across the 25 adjectives.
4.4 Experimental procedure
Using each composition method, we generate com-
posite vectors for all the ANs in the two (acceptable
and deviant) evaluation sets (see Section 4.2 above).
We then compute the measures that might cue se-
mantic deviance discussed in Section 3 above, and
compare their values between the two AN sets. In
order to smooth out adjective-specific effects, we z-
normalize the values of each measure across all the
ANs sharing an adjective before computing global
statistics (i.e., the values for all ANs sharing an ad-
jective from the two sets are transformed by sub-
tracting their mean and dividing by their variance).
We then compare the two sets, for each composition
method and deviance cue, by means of two-tailed
Welch?s t tests. We report the estimated t score,
that is, the standardized difference between the mean
acceptable and deviant AN values, with the corre-
sponding significance level. For all our cues, we
predict t to be significantly larger than 0: Accept-
able AN vectors should be longer than deviant ones,
they should be nearer ? that is, have a higher cosine
with ? the component N vectors and their neighbour-
hood should be denser ? that is, the average cosines
with their top neighbours should be higher than the
ones of deviant ANs with their top neighbours.
5 Results
The results of our experiments are summarized in
Table 1. We see that add and mult provide signif-
icant results in the expected direction for 2 over 3
cues, only failing the cosine test. With the lm model,
acceptable and deviant ANs are indistinguishable
across the board, whereas alm captures the distinc-
tion in terms of density.
LENGTH COSINE DENSITY
method t sig. t sig. t sig.
add 7.89 * 0.31 2.63 *
mult 3.16 * -0.56 2.68 *
lm 0.16 0.55 -0.23
alm 0.48 1.37 3.12 *
Table 1: t scores for difference between acceptable and
deviant ANs with respect to 3 cues of deviance: length
of the AN vector, cosine of the AN vector with the com-
ponent noun vector and density, measured as the average
cosine of an AN vector with its nearest 10 neighbours in
semantic space. For all significant results, p<0.01.
The high scores in the vector length analyses of
both the addition and the multiplication models are
an indication that semantically acceptable ANs tend
to be composed of similar adjectives and nouns, i.e.,
those which occur in similar contexts and we can as-
sume are likely to belong to the same domain, which
sounds plausible.
In Baroni and Zamparelli (2010), the alm model
performed far better than add and mult in approxi-
mating the correct vectors for unseen ANs, while on
this (in a sense, more metalinguistic) task add and
mult work better, while alm is successful only in the
more sophisticated measure of neighbor density.
The lack of significant results for the cosine mea-
sure is disappointing, but not entirely surprising. A
large angle between N and AN might be a feature of
impossible ANs common to various types of pos-
sible ANs: idioms (a red herring is probably far
from herring in semantic space), non-subsective ad-
jectives (stone lion vs. lion; fake butterfly vs. but-
terfly), plus some metaphorical constructions (aca-
demic crusade vs. crusade?one of several ANs
judged acceptable in our study, which can only be
taken as metaphors). Recall, finally, that the vector
for the base N collapses together all the meanings
of an ambiguous N. The adjective might have a dis-
ambiguating effect which would increase the cosine
distance.
To gain a better understanding of the neighbor-
hood density test we performed a detailed analysis
of the nearest neighbors of the AN vectors generated
by the three models in which the difference in neigh-
bor distance was significant across deviant and ac-
ceptable ANs: alm, multiplication and addition. For
7
each of the ANs, we looked at the top 10 semantic-
space neighbors generated by each of the three mod-
els, focusing on two aspects: whether the neighbor
was a single A or N, rather than AN, and whether
the neighbor contained the same A or N as the AN
is was the neighbor of (as in blind regatta / blind
athlete or biological derivative / partial derivative).
The results are summarized in Table 2.
method status A N A1= N1=
only only A2 N2
add
accept 11.9 8.7 14.6 2.4
deviant 12.5 6.8 14.6 2.3
mult
accept 6.9 8.0 0.7 0.1
deviant 2.7 7.3 0.5 0.1
alm
accept 4.9 17.7 7.0 0.0
deviant 7.1 19.6 6.2 0.0
Table 2: Percentage distributions of various properties of
the top 10 neighbours of ANs in the acceptable (2800)
and deviant (4130) sets for add, mult and alm. The last
two columns express whether the neighbor contains the
same Adjective or Noun as the target AN.
In terms of the properties we measured, neighbor
distributions are quite similar across acceptable and
deviant ANs. One interesting finding is that the sys-
tem is quite ?adjective-driven?: particularly for the
additive model (where we can imagine that some Ns
with low dimensional values do not shift much the
adjective position in the multidimensional space),
less so in the alm method, and not at all for mult. To
put the third and forth columns in context, the subset
of the semantic space used to generate the SVD from
which the neighbors are drawn contained 2.69% ad-
jectives, 5.24% nouns and 92.07% ANs. With re-
spect to the last two columns, it is interesting to ob-
serve that matching As are frequent for deviant ANs
even in alm, a model which has never seen A-vectors
during training. Further qualitative evaluations show
that in many deviant AN cases the similarity is be-
tween the A in the target AN and the N of the neigh-
bor (e.g. academic bladder / honorary lectureship),
while the opposite effect seems to be much harder to
find.
6 Conclusion and future work
The main aim of this paper was to propose a new
challenge to the computational distributional seman-
tics community, namely that of characterizing what
happens, distributionally, when composition leads
to semantically anomalous composite expressions.
The hope is, on the one hand, to bring further sup-
port to the distributional approach by showing that it
can be both productive and constrained; and on the
other, to provide a more general characterization of
the somewhat elusive notion of semantic deviance ?
a notion that the field of formal semantics acknowl-
edges but might lack the right tools to model.
Our results are very preliminary, but also very en-
couraging, suggesting that simple unsupervised cues
can significantly tell unattested but acceptable ANs
apart from impossible, or at least deviant, ones. Al-
though, somewhat disappointingly, the model that
has been shown in a previous study (Baroni and
Zamparelli, 2010) to be the best at capturing the se-
mantics of well-formed ANs turns out to be worse
than simple addition and multiplication.
Future avenues of research must include, first of
all, an exploration on the effect on each model when
tested in the non-reduced space where computation-
ally possible, or using different dimensionality re-
duction methods. A preliminary study demonstrates
an enhanced performance of the mult method in the
full space.
Second, we hope to provide a larger benchmark
of acceptable and deviant ANs, beyond the few hun-
dreds we used here, and sampling a larger typology
of ANs across frequency ranges and adjective and
noun classes. To this extent, we are implementing
a crowd-sourcing study to collect human judgments
from a large pool of speakers on a much larger set of
ANs unattested in the corpus. Averaging over mul-
tiple judgments, we will also be able to characterize
semantic deviance as a gradient property, probably
more accurately.
Next, the range of cues we used was quite limited,
and we intend to extend the range to include more
sophisticated methods such as 1) combining multi-
ple cues in a single score; 2) training a supervised
classifier from labeled acceptable and deviant ANs,
and studying the most distinctive features discov-
ered by the classifier; 3) trying more complex unsu-
pervised techniques, such as using graph-theoretical
methods to characterize the semantic neighborhood
of ANs beyond our simple density measure.
Finally, we are currently not attempting a typol-
8
ogy of deviant ANs. We do not distinguish cases
such as parliamentary tomato, where the adjective
does not apply to the conceptual semantic type of
the noun (or at least, where it is completely undeter-
mined which relation could bridge the two objects),
from oxymorons such as dry water, or vacuously
redundant ANs (liquid water) and so on. We real-
ize that, at a more advanced stage of the analysis,
some of these categories might need to be explicitly
distinguished (for example, liquid water is odd but
perfectly meaningful), leading to a multi-way task.
Similarly, among acceptable ANs, there are spe-
cial classes of expressions, such as idiomatic con-
structions, metaphors or other rhetorical figures, that
might be particularly difficult to distinguish from
deviant ANs. Again, more cogent tasks involving
such well-formed but non-literal constructions (be-
yond the examples that ended up by chance in our
acceptable set) are left to future work.
Acknowledgments
We thank Raffaella Bernardi, Gemma Boleda,
Louise McNally and the anonymous reviewers for
their advice and comments.
References
Nicholas Asher. 2011. Lexical Meaning in Context: A
Web of Words. Cambridge University Press.
Marco Baroni and Alessandro Lenci. 2010. Dis-
tributional Memory: A general framework for
corpus-based semantics. Computational Linguistics,
36(4):673?721.
Marco Baroni and Roberto Zamparelli. 2010. Nouns
are vectors, adjectives are matrices: Representing
adjective-noun constructions in semantic space. In
Proceedings of EMNLP, pages 1183?1193, Boston,
MA.
Noam Chomsky. 1957. Syntactic Structures. Mouton.
Noam Chomsky. 1977. Essays on Form and Interpreta-
tion. North Holland, New York.
Katrin Erk and Sebastian Pado?. 2008. A structured vec-
tor space model for word meaning in context. In Pro-
ceedings of EMNLP, pages 897?906, Honolulu, HI,
USA.
Stefan Evert. 2005. The Statistics of Word Cooccur-
rences. Dissertation, Stuttgart University.
Dan Fass and Yorick Wilks. 1983. Preference seman-
tics, ill-formedness, and metaphor. Computational
Linguistics, 9:178?187.
Rachel Giora. 2002. Literal vs. figurative language: Dif-
ferent or equal? Journal of Pragmatics, 34:487?506.
Emiliano Guevara. 2010. A regression model of
adjective-noun compositionality in distributional se-
mantics. In Proceedings of the ACL GEMS Workshop,
pages 33?37, Uppsala, Sweden.
Walter Kintsch. 2001. Predication. Cognitive Science,
25(2):173?202.
Thomas Landauer and Susan Dumais. 1997. A solu-
tion to Plato?s problem: The latent semantic analysis
theory of acquisition, induction, and representation of
knowledge. Psychological Review, 104(2):211?240.
Bjo?rn-Helge Mevik and Ron Wehrens. 2007. The
pls package: Principal component and partial least
squares regression in R. Journal of Statistical Soft-
ware, 18(2). Published online: http://www.
jstatsoft.org/v18/i02/.
Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. In Proceedings of
ACL, pages 236?244, Columbus, OH, USA.
Jeff Mitchell and Mirella Lapata. 2009. Language mod-
els based on semantic composition. In Proceedings of
EMNLP, pages 430?439, Singapore.
Jeff Mitchell and Mirella Lapata. 2010. Composition in
distributional models of semantics. Cognitive Science.
Reinhard Rapp. 2003. Word sense discovery based on
sense descriptor dissimilarity. In Proceedings of the
9th MT Summit, pages 315?322, New Orleans, LA,
USA.
Helmut Schmid. 1995. Improvements in part-of-speech
tagging with an application to German. In Proceed-
ings of the EACL-SIGDAT Workshop, Dublin, Ireland.
Hinrich Schu?tze. 1997. Ambiguity Resolution in Natural
Language Learning. CSLI, Stanford, CA.
Richmond H Thomason, editor. 1974. Formal Philoso-
phy: Selected Papers of Richard Montague. Yale Uni-
versity Press, New York.
Peter Turney and Patrick Pantel. 2010. From frequency
to meaning: Vector space models of semantics. Jour-
nal of Artificial Intelligence Research, 37:141?188.
Chang-Le Zhou, Yun Yang, and Xiao-Xi Huang. 2007.
Computational mechanisms for metaphor in lan-
guages: a survey. Journal of Computer Science and
Technology, 22:308?319.
9
