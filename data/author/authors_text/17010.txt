Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1508?1516,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
A joint model of word segmentation and phonological variation for
English word-final /t/-deletion
Benjamin Bo?rschinger1,3 and Mark Johnson1 and Katherine Demuth2
(1) Department of Computing, Macquarie University
(2) Department of Linguistics, Macquarie University
(3) Department of Computational Linguistics, Heidelberg University
{benjamin.borschinger, mark.johnson, katherine.demuth}@mq.edu.au
Abstract
Word-final /t/-deletion refers to a common
phenomenon in spoken English where
words such as /wEst/ ?west? are pro-
nounced as [wEs] ?wes? in certain con-
texts. Phonological variation like this is
common in naturally occurring speech.
Current computational models of unsu-
pervised word segmentation usually as-
sume idealized input that is devoid of
these kinds of variation. We extend a
non-parametric model of word segmenta-
tion by adding phonological rules that map
from underlying forms to surface forms
to produce a mathematically well-defined
joint model as a first step towards han-
dling variation and segmentation in a sin-
gle model. We analyse how our model
handles /t/-deletion on a large corpus of
transcribed speech, and show that the joint
model can perform word segmentation and
recover underlying /t/s. We find that Bi-
gram dependencies are important for per-
forming well on real data and for learning
appropriate deletion probabilities for dif-
ferent contexts.1
1 Introduction
Computational models of word segmentation try
to solve one of the first problems language learn-
ers have to face: breaking an unsegmented stream
of sound segments into individual words. Cur-
rently, most such models assume that the input
consists of sequences of phonemes with no pro-
nunciation variation across different occurrences
of the same word type. In this paper we describe
1The implementation of our model as well as
scripts to prepare the data will be made available at
http://web.science.mq.edu.au/~bborschi.
We can?t release our version of the Buckeye Corpus (Pitt et
al., 2007) directly because of licensing issues.
an extension of the Bayesian models of Gold-
water et al (2009) that incorporates phonologi-
cal rules to ?explain away? surface variation. As
a concrete example, we focus on word-final /t/-
deletion in English, although our approach is not
limited to this case. We choose /t/-deletion be-
cause it is a very common and well-studied phe-
nomenon (see Coetzee (2004, Chapter 5) for a
review) and segmental deletion is an interesting
test-case for our architecture. Recent work has
found that /t/-deletion (among other things) is in-
deed common in child-directed speech (CDS) and,
importantly, that its distribution is similar to that in
adult-directed speech (ADS) (Dilley et al, to ap-
pear). This justifies our using ADS to evaluate our
model, as discussed below.
Our experiments are consistent with long-
standing and recent findings in linguistics, in par-
ticular that /t/-deletion heavily depends on the im-
mediate context and that models ignoring context
work poorly on real data. We also examine how
well our models identify the probability of /t/-
deletion in different contexts. We find that models
that capture bigram dependencies between under-
lying forms provide considerably more accurate
estimates of those probabilities than correspond-
ing unigram or ?bag of words? models of underly-
ing forms.
In section 2 we discuss related work on han-
dling variation in computational models and on /t/-
deletion. Section 3 describes our computational
model and section 4 discusses its performance for
recovering deleted /t/s. We look at both a sit-
uation where word boundaries are pre-specified
and only inference for underlying forms has to
be performed; and the problem of jointly finding
the word boundaries and recovering deleted un-
derlying /t/s. Section 5 discusses our findings, and
section 6 concludes with directions for further re-
search.
1508
2 Background and related work
The work of Elsner et al (2012) is most closely
related to our goal of building a model that han-
dles variation. They propose a pipe-line archi-
tecture involving two separate generative models,
one for word-segmentation and one for phonolog-
ical variation. They model the mapping to sur-
face forms using a probabilistic finite-state trans-
ducer. This allows their architecture to handle
virtually arbitrary pronunciation variation. How-
ever, as they point out, combining the segmenta-
tion and the variation model into one joint model
is not straight-forward and usual inference proce-
dures are infeasible, which requires the use of sev-
eral heuristics. We pursue an alternative research
strategy here, starting with a single well-studied
example of phonological variation. This permits
us to develop a joint generative model for both
word segmentation and variation which we plan to
extend to handle more phenomena in future work.
An earlier work that is close to the spirit of our
approach is Naradowsky and Goldwater (2009),
who learn spelling rules jointly with a simple
stem-suffix model of English verb morphology.
Their model, however, doesn?t naturally extend to
the segmentation of entire utterances.
/t/-deletion has received a lot of attention within
linguistics, and we point the interested reader to
Coetzee (2004, Chapter 5) for a thorough review.
Briefly, the phenomenon is as follows: word-final
instances of /t/ may undergo deletion in natural
speech, such that /wEst/ ?west? is actually pro-
nounced as [wEs] ?wes?.2 While the frequency of
this phenomenon varies across social and dialectal
groups, within groups it has been found to be ro-
bust, and the probability of deletion depends on
its phonological context: a /t/ is more likely to
be dropped when followed by a consonant than
a vowel or a pause, and it is more likely to be
dropped when following a consonant than a vowel
as well. We point out two recent publications that
are of direct relevance to our research. Dilley et al
(to appear) study word-final variation in stop con-
sonants in CDS, the kind of input we ideally would
like to evaluate our models on. They find that ?in-
fants largely experience statistical distributions of
non-canonical consonantal pronunciation variants
[including deletion] that mirror those experienced
by adults.? This both directly establishes the need
2Following the convention in phonology, we give under-
lying forms within ?/. . . /? and surface forms within ?[. . . ]?.
for computational models to handle this dimension
of variation, and justifies our choice of using ADS
for evaluation, as mentioned above.
Coetzee and Kawahara (2013) provide a com-
putational study of (among other things) /t/-
deletion within the framework of Harmonic Gram-
mar. They do not aim for a joint model that also
handles word segmentation, however, and rather
than training their model on an actual corpus, they
evaluate on constructed lists of examples, mimick-
ing frequencies of real data. Overall, our findings
agree with theirs, in particular that capturing the
probability of deletion in different contexts does
not automatically result in good performance for
recovering individual deleted /t/s. We will come
back to this point in our discussion at the end of
the paper.
3 The computational model
Our models build on the Unigram and the Bigram
model introduced in Goldwater et al (2009). Fig-
ure 1 shows the graphical model for our joint Bi-
gram model (the Unigram case is trivially recov-
ered by generating the Ui,js directly from L rather
than from LUi,j?1). Figure 2 gives the mathemati-
cal description of the graphical model and Table 1
provides a key to the variables of our model.
The model generates a latent sequence of un-
derlying word-tokens U1, . . . , Un. Each word to-
ken is itself a non-empty sequence of segments or
phonemes, and each Uj corresponds to an under-
lying word form, prior to the application of any
phonological rule. This generative process is re-
peated for each utterance i, leading to multiple
utterances of the form Ui,1, . . . , Ui,ni where ni is
the number of words in the ith utterance, and Ui,j
is the jth word in the ith utterance. Each utter-
ance is padded by an observed utterance bound-
ary symbol $ to the left and to the right, hence
Ui,0 = Ui,ni+1 = $.3 Each Ui,j+1 is generated
conditionally on its predecessor Ui,j from LUi,j ,
as shown in the first row of the lower plate in Fig-
ure 1. Each Lw is a distribution over the pos-
sible words that can follow a token of w and L
is a global distribution over possible words, used
as back-off for all Lw. Just as in Goldwater et
al. (2009), L is drawn from a Dirichlet Process
(DP) with base distribution B and concentration
3Each utterance terminates as soon as a $ is generated,
thus determining the number of words ni in the ith utterance.
See Goldwater et al (2009) for discussion.
1509
Figure 1: The graphical model for our joint
model of word-final /t/-deletion and Bigram
word segmentation. The corresponding math-
ematical description is given in Figure 2. The
generative process mimics the intuitively plau-
sible idea of generating underlying forms from
some kind of syntactic model (here, a Bi-
gram language model) and then mapping the
underlying form to an observed surface-form
through the application of a phonological rule
component, here represented by the collection
of rule probabilities ?c.
L |?, ?0 ?DP (?0, B(? | ?))
Lw |L,?1 ?DP (?1, L)
?c |? ?Beta(1, 1)
Ui,0 = $
Si,0 = $
Ui,j+1 |Ui,j , LUi,j ?LUi,j
Si,j |Ui,j , Ui,j+1,? =PR(? | Ui,j , Ui,j+1)
Wi |Si,1, . . . , Si,ni = CAT(Si,0, . . . , Si,ni)
Figure 2: Mathematical description of our joint
Bigram model. The lexical generator B(? | ?)
is specified in Figure 3 and PR is explained in
the text below. CAT stands for concatenation
without word-boundaries, ni refers to the num-
ber of words in utterance i.
Variable Explanation
B base distribution over possible words
L back-off distribution over words
Lw distribution over words following w
Ui,j underlying form, a word
Si,j surface realization of Ui,j , a word
?c /t/-deletion probability in context c
Wi observed segments for ith utterance
Table 1: Key for the variables in Figure 1 and
Figure 2. See Figure 3 for the definition of B.
parameter ?0, and the word type specific distri-
butions Lw are drawn from a DP (L,?1), result-
ing in a hierarchical DP model (Teh et al, 2006).
The base distribution B functions as a lexical gen-
erator, defining a prior distribution over possible
words. In principle, B can incorporate arbitrary
prior knowledge about possible words, for exam-
ple syllable structure (cf. Johnson (2008)). In-
spired by Norris et al (1997), we use a simpler
possible word constraint that only rules out se-
quences that lack a vowel (see Figure 3). While
this is clearly a simplification it is a plausible as-
sumption for English data.
Instead of generating the observed sequence of
segments W directly by concatenating the under-
lying forms as in Goldwater et al (2009), we
map each Ui,j to a corresponding surface-form
Si,j by a probabilistic rule component PR. The
values over which the Si,j range are determined
by the available phonological processes. In the
model we study here, the phonological processes
only include a rule for deleting word-final /t/s
but in principle, PR can be used to encode a
wide variety of phonological rules. Here, Si,j ?
{Ui,j ,DELF(Ui,j)} if Ui,j ends in a /t/, and Si,j =
Ui,j otherwise, where DELF(u) refers to the same
word as u except that it lacks u?s final segment.
We look at three kinds of contexts on which a
rule?s probability of applying depends:
1. a uniform context that applies to every word-
final position
2. a right context that also considers the follow-
ing segment
3. a left-right context that additionally takes the
preceeding segment into account
For each possible context c there is a prob-
ability ?c which stands for the probability of
the rule applying in this context. Writing
1510
? ?Dir(?0.01, . . . , 0.01?)
B(w = x1:n | ?) =
{ [?ni=1 ?xi ]?#
Z if V(w)
0.0 if ?V(w)
Figure 3: Lexical generator with possible word-
constraint for words in ?+, ? being the alphabet
of available phonemes. x1:n is a sequence of ele-
ments of ? of length n. ? is a probability vector
of length |?| + 1 drawn from a sparse Dirichlet
prior, giving the probability for each phoneme and
the special word-boundary symbol #. The pred-
icate V holds of all sequences containing at least
one vowel. Z is a normalization constant that ad-
justs for the mass assigned to the empty and non-
possible words.
contexts in the notation familiar from genera-
tive phonology (Chomsky and Halle, 1968), our
model can be seen as implementing the fol-
lowing rules under the different assumptions:4
uniform /t/ ? ? / ]word
right /t/ ? ? / ]word ?
left-right /t/ ? ? / ? ]word ?
We let ? range over V(owel), C(onsonant) and $
(utterance-boundary), and ? over V and C. We
define a function CONT that maps a pair of ad-
jacent underlying forms Ui,j , Ui,j+1 to the con-
text of the final segment of Ui,j . For example,
CONT(/wEst/,/@v/) returns ?C ]word V? in the
left-right setting, or simply ? ]word? in the uni-
form setting. CONT returns a special NOT con-
text if Ui,j doesn?t end in a /t/. We stipulate that
?NOT = 0.0. Then we can define PR as follows:
PR(DELFINAL(u) | u, r)) = ?CONT(u,r)
PR(u | u, r) = 1? ?CONT(u,r)
Depending on the context setting used, our
model includes one (uniform), three (right) or six
(left-right) /t/-deletion probabilities ?c. We place a
uniform Beta prior on each of those so as to learn
their values in the LEARN-? experiments below.
Finally, the observed unsegmented utterances
Wi are generated by concatenating all Si,j using
the function CAT.
We briefly comment on the central intuition
of this model, i.e. why it can infer underlying
4For right there are three and for left-right six different
rules, one for every instantiation of the context-template.
from surface forms. Bayesian word segmentation
models try to compactly represent the observed
data in terms of a small set of units (word types)
and a short analysis (a small number of word
tokens). Phonological rules such as /t/-deletion
can ?explain away? an observed surface type such
as [wEs]] in terms of the underlying type /wEst/
which is independently needed for surface tokens
of [wEst]. Thus, the /t/? ? rule makes possi-
ble a smaller lexicon for a given number of sur-
face tokens. Obviously, human learners have ac-
cess to additional cues, such as the meaning of
words, knowledge of phonological similarity be-
tween segments and so forth. One of the advan-
tages of an explicitly defined generative model
such as ours is that it is straight-forward to grad-
ually extend it by adding more cues, as we point
out in the discussion.
3.1 Inference
Just as for the Goldwater et al (2009) segmen-
tation models, exact inference is infeasible for
our joint model. We extend the collapsed Gibbs
breakpoint-sampler described in Goldwater et al
(2009) to perform inference for our extended mod-
els. We refer the reader to their paper for addi-
tional details such as how to calculate the Bigram
probabilities in Figure 4. Here we focus on the
required changes to the sampler so as to perform
inference under our richer model. We consider the
case of a single surface string W , so we drop the
i-index in the following discussion.
Knowing W , the problem is to recover the un-
derlying forms U1, . . . , Un and the surface forms
S1, . . . , Sn for unknown n. A major insight in
Goldwater?s work is that rather than sampling over
the latent variables in the model directly (the num-
ber of which we don?t even know), we can instead
perform Gibbs sampling over a set of boundary
variables b1, . . . , b|W |?1 that jointly determine the
values for our variables of interest where |W | is
the length of the surface string W . For our model,
each bj ? {0, 1, t}, where bj = 0 indicates ab-
sence of a word boundary, bj = 1 indicates pres-
ence of a boundary and bj = t indicates pres-
ence of a boundary with a preceeding underlying
/t/. The relation between the bj and the S1, . . . , Sn
and U1, . . . , Un is illustrated in Figure 5. The re-
quired sampling equations are given in Figure 4.
1511
P (bj = 0 | b?j) ? P (w12,u | wl,u, b?j)? Pr(w12,s | w12,u, wr,u)? P (wr,u | w12,u, b?j ? ?wl,u, w12,u?) (1)
P (bj = t | b?j) ? P (w1,t | wl,u, b?j)? Pr(w1,s | w1,t, w2,u)? P (w2,u | w1,t, b?j ? ?wl,u, w1,t?)
? Pr(w2,s | w2,u, wr,u)? P (wr,u | w2,u, b?j ? ?wl,u, w1,t? ? ?w1,t, w2,u?) (2)
P (bj = 1 | b?j) ? P (w1,s | wl,u, b?j)? Pr(w1,s | w1,s, w2,u)? P (w2,u | w1,s, b?j ? ?wl,u, w1,s?)
? Pr(w2,s | w2,u, wr,u)? P (wr,u | w2,u, b?j ? ?wl,u, w1,s? ? ?w1,s, w2,u?) (3)
Figure 4: Sampling equations for our Gibbs sampler, see figure 5 for illustration. bj = 0 corresponds
to no boundary at this position, bj = t to a boundary with a preceeding underlying /t/ and bj = 1 to a
boundary with no additional underlying /t/. We use b?j for the statistics determined by all but the jth
position and b?j ? ?r, l? for these statistics plus an additional count of the bigram ?r, l?. P (w | l, b)
refers to the bigram probability of ?l, w? given the the statistics b; we refer the reader to Goldwater et
al. (2009) for the details of calculating these bigram probabilities and details about the required statistics
for the collapsed sampler. PR is defined in the text.
1 10 t 1I h      i  i       t $
underlyingsurfaceboundariesobserved I h i i t $
I h      i       t  i       t $
Figure 5: The relation between the observed se-
quence of segments (bottom), the boundary vari-
ables b1, . . . , b|W |?1 the Gibbs sampler operates
over (in squares), the latent sequence of sur-
face forms and the latent sequence of underly-
ing forms. When sampling a new value for
b3 = t, the different word-variables in fig-
ure 4 are: w12,u=w12,s=hiit, w1,t=hit and w1,s=hi,
w2,u=w2,s=it, wl,u=I, wr,u=$. Note that we need
a boundary variable at the end of the utterance as
there might be an underlying /t/ at this position as
well. The final boundary variable is set to 1, not t,
because the /t/ in it is observed.
4 Experiments
4.1 The data
We are interested in how well our model han-
dles /t/-deletion in real data. Ideally, we?d eval-
uate it on CDS but as of now, we know of no
available large enough corpus of accurately hand-
transcribed CDS. Instead, we used the Buckeye
Corpus (Pitt et al, 2007) for our experiments,
a large ADS corpus of interviews with English
speakers that have been transcribed with relatively
fine phonetic detail, with /t/-deletion among the
things manually annotated. Pointing to the re-
cent work by Dilley et al (to appear) we want
to emphasize that the statistical distribution of /t/-
deletion has been found to be similar for ADS and
orthographic I don?t intend to
transcript /aI R oU n I n t E n d @/
idealized /aI d oU n t I n t E n d t U/
t-drop /aI d oU n I n t E n d t U/
Figure 6: An example fragment from the Buckeye-
corpus in orthographic form, the fine transcript
available in the Buckeye corpus, a fully idealized
pronunciation with canonical dictionary pronunci-
ations and our version of the data with dropped
/t/s.
CDS, at least for read speech.
We automatically derived a corpus of 285,792
word tokens across 48,795 utterances from the
Buckeye Corpus by collecting utterances across all
interviews and heuristically splitting utterances at
speaker-turn changes and indicated silences. The
Buckeye corpus lists for each word token a man-
ually transcribed pronunciation in context as well
as its canonical pronunciation as given in a pro-
nouncing dictionary. As input to our model, we
use the canonical pronunciation unless the pronun-
ciation in context indicates that the final /t/ has
been deleted in which case we also delete the final
/t/ of the canonical pronunciation Figure 6 shows
an example from the Buckeye Corpus, indicating
how the original data, a fully idealized version
and our derived input that takes into account /t/-
deletions looks like.
Overall, /t/-deletion is a quite frequent phe-
nomenon with roughly 29% of all underlying /t/s
being dropped. The probabilities become more
peaked when looking at finer context; see Table 3
for the empirical distribution of /t/-dropping for
the six different contexts of the left-right setting.
1512
4.2 Recovering deleted /t/s, given word
boundaries
In this set of experiments we are interested in how
well our model recovers /t/s when it is provided
with the gold word boundaries. This allows us
to investigate the strength of the statistical sig-
nal for the deletion rule without confounding it
with the word segmentation performance, and to
see how the different contextual settings uniform,
right and left-right handle the data. Concretely,
for the example in Figure 6 this means that we tell
the model that there are boundaries between /aI/,
/doUn/, /IntEnd/, /tu/ and /liv/ but we don?t tell it
whether or not these words end in an underlying
/t/. Even in this simple example, there are 5 possi-
ble positions for the model to posit an underlying
/t/. We evaluate the model in terms of F-score, the
harmonic mean of recall (the fraction of underly-
ing /t/s the model correctly recovered) and preci-
sion (the fraction of underlying /t/s the model pre-
dicted that were correct).
In these experiments, we ran a total of 2500 it-
erations with a burnin of 2000. We collect sam-
ples with a lag of 10 for the last 500 iterations and
perform maximum marginal decoding over these
samples (Johnson and Goldwater, 2009), as well
as running two chains so as to get an idea of the
variance.5
We are also interested in how well the model
can infer the rule probabilities from the data, that
is, whether it can learn values for the different ?c
parameters. We compare two settings, one where
we perform inference for these parameters assum-
ing a uniform Beta prior on each ?c (LEARN-?)
and one where we provide the model with the em-
pirical probabilities for each ?c as estimated off
the gold-data (GOLD-?), e.g., for the uniform con-
dition 0.29. The results are shown in Table 2.
Best performance for both the Unigram and
the Bigram model in the GOLD-? condition is
achieved under the left-right setting, in line with
the standard analyses of /t/-deletion as primarily
being determined by the preceding and the follow-
ing context. For the LEARN-? condition, the Bi-
gram model still performs best in the left-right set-
ting but the Unigram model?s performance drops
5As manually setting the hyper-parameters for the DPs in
our model proved to be complicated and may be objected to
on principled grounds, we perform inference for them under
a vague gamma prior, as suggested by Teh et al (2006) and
Johnson and Goldwater (2009), using our own implementa-
tion of a slice-sampler (Neal, 2003).
uniform right left-right
Unigram LEARN-? 56.52 39.28 23.59GOLD-? 62.08 60.80 66.15
Bigram LEARN-? 60.85 62.98 77.76GOLD-? 69.06 69.98 73.45
Table 2: F-score of recovered /t/s with known
word boundaries on real data for the three differ-
ent context settings, averaged over two runs (all
standard errors below 2%). Note how the Uni-
gram model always suffers in the LEARN-? condi-
tion whereas the Bigram model?s performance is
actually best for LEARN-? in the left-right setting.
C C C V C $ V C V V V $
empirical 0.62 0.42 0.36 0.23 0.15 0.07
Unigram 0.41 0.33 0.17 0.07 0.05 0.00
Bigram 0.70 0.58 0.43 0.17 0.13 0.06
Table 3: Inferred rule-probabilities for different
contexts in the left-right setting from one of the
runs. ?C C? stands for the context where the
deleted /t/ is preceded and followed by a conso-
nant, ?V $? stands for the context where it is pre-
ceded by a vowel and followed by the utterance
boundary. Note how the Unigram model severely
under-estimates and the Bigram model slightly
over-estimates the probabilities.
in all settings and is now worst in the left-right and
best in the uniform setting.
In fact, comparing the inferred probabilities
to the ?ground truth? indicates that the Bigram
model estimates the true probabilities more ac-
curately than the Unigram model, as illustrated
in Table 3 for the left-right setting. The Bi-
gram model somewhat overestimates the probabil-
ity for all post-consonantal contexts but the Uni-
gram model severely underestimates the probabil-
ity of /t/-deletion across all contexts.
4.3 Artificial data experiments
To test our Gibbs sampling inference procedure,
we ran it on artificial data generated according to
the model itself. If our inference procedure fails
to recover the underlying /t/s accurately in this set-
ting, we should not expect it to work well on actual
data. We generated our artificial data as follows.
We transformed the sequence of canonical pronun-
ciations in the Buckeye corpus (which we take to
be underlying forms here) by randomly deleting
final /t/s using empirical probabilities as shown in
Table 3 to generate a sequence of artificial sur-
face forms that serve as input to our models. We
1513
uniform right left-right
Unigram LEARN-? 94.35 23.55 (+) 63.06GOLD-? 94.45 94.20 91.83
Bigram LEARN-? 92.72 91.64 88.48GOLD-? 92.88 92.33 89.32
Table 4: F-score of /t/-recovery with known word
boundaries on artificial data, each condition tested
on data that corresponds to the assumption, aver-
aged over two runs (standard errors less than 2%
except (+) = 3.68%)).
Unigram Bigram
LEARN-? 33.58 55.64
GOLD-? 55.92 57.62
Table 5: /t/-recovery F-scores when performing
joint word segmention in the left-right setting, av-
eraged over two runs (standard errors less than
2%). See Table 6 for the corresponding segmenta-
tion F-scores.
did this for all three context settings, always es-
timating the deletion probability for each context
from the gold-standard. The results of these exper-
iments are given in table 4. Interestingly, perfor-
mance on these artificial data is considerably bet-
ter than on the real data. In particular the Bigram
model is able to get consistently high F-scores for
both the LEARN-? and the GOLD-? setting. For
the Unigram model, we again observe the severe
drop in the LEARN-? setting for the right and left-
right settings although it does remarkably well in
the uniform setting, and performs well across all
settings in the GOLD-? condition. We take this to
show that our inference algorithm is in fact work-
ing as expected.
4.4 Segmentation experiments
Finally, we are also interested to learn how well
we can do word segmentation and underlying /t/-
recovery jointly. Again, we look at both the
LEARN-? and GOLD-? conditions but focus on the
left-right setting as this worked best in the exper-
iments above. For these experiments, we perform
simulated annealing throughout the initial 2000 it-
erations, gradually cooling the temperature from
5 to 1, following the observation by Goldwater
et al (2009) that without annealing, the Bigram
model gets stuck in sub-optimal parts of the solu-
tion space early on. During the annealing stage,
we prevent the model from performing inference
for underlying /t/s so that the annealing stage can
be seen as an elaborate initialisation scheme, and
we perform joint inference for the remaining 500
iterations, evaluating on the last sample and av-
eraging over two runs. As neither the Unigram
nor the Bigram model performs ?perfect? word
segmentation, we expect to see a degradation in
/t/-recovery performance and this is what we find
indeed. To give an impression of the impact of
/t/-deletion, we also report numbers for running
only the segmentation model on the Buckeye data
with no deleted /t/s and on the data with deleted
/t/s. The /t/-recovery scores are given in Table 5
and segmentation scores in Table 6. Again the
Unigram model?s /t/-recovery score degrades dra-
matically in the LEARN-? condition. Looking at
the segmentation performance this isn?t too sur-
prising: the Unigram model?s poorer token F-
score, the standard measure of segmentation per-
formance on a word token level, suggests that it
misses many more boundaries than the Bigram
model to begin with and, consequently, can?t re-
cover any potential underlying /t/s at these bound-
aries. Also note that in the GOLD-? condition, our
joint Bigram model performs almost as well on
data with /t/-deletions as the word segmentation
model on data that includes no variation at all.
The generally worse performance of handling
variation as measured by /t/-recovery F-score
when performing joint segmentation is consistent
with the finding of Elsner et al (2012) who report
considerable performance drops for their phono-
logical learner when working with induced bound-
aries (note, however, that their model does not per-
form joint inference, rather the induced boundaries
are given to their phonological learner as ground-
truth).
5 Discussion
There are two interesting findings from our exper-
iments. First of all, we find a much larger differ-
ence between the Unigram and the Bigram model
in the LEARN-? condition than in the GOLD-? con-
dition. We suggest that this is due to the Unigram
model?s lack of dependencies between underlying
forms, depriving it of an important source of ev-
idence. Bigram dependencies provide additional
evidence for underlying /t/ that are deleted on the
surface, and because the Bigram model identifies
these underlying /t/ more accurately, it can also es-
timate the /t/ deletion probability more accurately.
1514
Unigram Bigram
LEARN-? 54.53 72.55 (2.3%)
GOLD-? 54.51 73.18
NO-? 54.61 70.12
NO-VAR 54.12 73.99
Table 6: Word segmentation F-scores for the /t/-
recovery F-scores in Table 5 averaged over two
runs (standard errors less than 2% unless given).
NO-? are scores for running just the word segmen-
tation model with no /t/-deletion rule on the data
that includes /t/-deletion, NO-VAR for running just
the word segmentation model on the data with no
/t/-deletions.
For example, /t/ dropping in ?don?t you? yields
surface forms ?don you?. Because the word bi-
gram probability P (you | don?t) is high, the bi-
gram model prefers to analyse surface ?don? as
underlying ?don?t?. The Unigram model does not
have access to word bigram information so the
underlying forms it posits are less accurate (as
shown in Table 2), and hence the estimate of the
/t/-deletion probability is also less accurate. When
the probabilities of deletion are pre-specified the
Unigram model performs better but still consider-
ably worse than the Bigram model when the word
boundaries are known, suggesting the importance
of non-phonological contextual effects that the Bi-
gram model but not the Unigram model can cap-
ture. This suggests that for example word pre-
dictability in context might be an important factor
contributing to /t/-deletion.
The other striking finding is the considerable
drop in performance between running on natural-
istic and artificially created data. This suggests
that the natural distribution of /t/-deletion is much
more complex than can be captured by statistics
over the phonological contexts we examined. Fol-
lowing Guy (1991), a finer-grained distinction for
the preceeding segments might address this prob-
lem.
Yet another suggestion comes from the recent
work in Coetzee and Kawahara (2013) who claim
that ?[a] model that accounts perfectly for the
overall rate of application of some variable pro-
cess therefore does not necessarily account very
well for the actual application of the process to in-
dividual words.? They argue that in particular the
extremely high deletion rates typical of high fre-
quency items aren?t accurately captured when the
deletion probability is estimated across all types.
A look at the error patterns of our model on a sam-
ple from the Bigram model in the LEARN-? setting
on the naturalistic data suggests that this is in fact a
problem. For example, the word ?just? has an ex-
tremely high rate of deletion with 17462442 = 0.71%.While many tokens of ?jus? are ?explained away?
through predicting underlying /t/s, the (literally)
extra-ordinary frequency of ?jus?-tokens lets our
model still posit it as an underlying form, although
with a much dampened frequency (of the 1746 sur-
face tokens, 1081 are analysed as being realiza-
tions of an underlying ?just?).
The /t/-recovery performance drop when per-
forming joint word segmentation isn?t surprising
as even the Bigram model doesn?t deliver a very
high-quality segmentation to begin with, leading
to both sparsity (through missed word-boundaries)
and potential noise (through misplaced word-
boundaries). Using a more realistic generative
process for the underlying forms, for example an
Adaptor Grammar (Johnson et al, 2007), could
address this shortcoming in future work without
changing the overall architecture of the model al-
though novel inference algorithms might be re-
quired.
6 Conclusion and outlook
We presented a joint model for word segmentation
and the learning of phonological rule probabili-
ties from a corpus of transcribed speech. We find
that our Bigram model reaches 77% /t/-recovery
F-score when run with knowledge of true word-
boundaries and when it can make use of both the
preceeding and the following phonological con-
text, and that unlike the Unigram model it is able
to learn the probability of /t/-deletion in different
contexts. When performing joint word segmen-
tation on the Buckeye corpus, our Bigram model
reaches around above 55% F-score for recovering
deleted /t/s with a word segmentation F-score of
around 72% which is 2% better than running a Bi-
gram model that does not model /t/-deletion.
We identified additional factors that might help
handling /t/-deletion and similar phenomena. A
major advantage of our generative model is the
ease and transparency with which its assump-
tions can be modified and extended. For fu-
ture work we plan to incorporate into our model
richer phonological contexts, item- and frequency-
specific probabilities and more direct use of word
1515
predictability. We also plan to extend our model
to handle additional phenomena, an obvious can-
didate being /d/-deletion.
Also, the two-level architecture we present is
not limited to the mapping being defined in terms
of rules rather than constraints in the spirit of Op-
timality Theory (Prince and Smolensky, 2004); we
plan to explore this alternative path as well in fu-
ture work.
To conclude, we presented a model that pro-
vides a clean framework to test the usefulness of
different factors for word segmentation and han-
dling phonological variation in a controlled man-
ner.
Acknowledgements
We thank the anonymous reviewers for their
valuable comments. This research was sup-
ported under Australian Research Council?s Dis-
covery Projects funding scheme (project numbers
DP110102506 and DP110102593).
References
Noam Chomsky and Morris Halle. 1968. The Sound
Pattern of English. Haper & Row, New York.
Andries W. Coetzee and Shigeto Kawahara. 2013. Fre-
quency biases in phonological variation. Natural
Language and Linguisic Theory, 31:47?89.
Andries W. Coetzee. 2004. What it Means to be a
Loser: Non-Optimal Candidates in Optimality The-
ory. Ph.D. thesis, University of Massachusetts ,
Amherst.
Laura Dilley, Amanda Millett, J. Devin McAuley, and
Tonya R. Bergeson. to appear. Phonetic variation
in consonants in infant-directed and adult-directed
speech: The case of regressive place assimilation
in word-final alveolar stops. Journal of Child Lan-
guage.
Micha Elsner, Sharon Goldwater, and Jacob Eisenstein.
2012. Bootstrapping a unified model of lexical and
phonetic acquisition. In Proceedings of the 50th An-
nual Meeting of the Association for Computational
Linguistics, pages 184?193, Jeju Island, Korea. As-
sociation for Computational Linguistics.
Sharon Goldwater, Thomas L. Griffiths, and Mark
Johnson. 2009. A Bayesian framework for word
segmentation: Exploring the effects of context.
Cognition, 112(1):21?54.
Gregory R. Guy. 1991. Contextual conditioning in
variable lexical phonology. Language Variation and
Change, 3(2):223?39.
Mark Johnson and Sharon Goldwater. 2009. Im-
proving nonparameteric Bayesian inference: exper-
iments on unsupervised word segmentation with
adaptor grammars. In Proceedings of Human Lan-
guage Technologies: The 2009 Annual Conference
of the North American Chapter of the Associa-
tion for Computational Linguistics, pages 317?325,
Boulder, Colorado, June. Association for Computa-
tional Linguistics.
Mark Johnson, Thomas L. Griffiths, and Sharon Gold-
water. 2007. Adaptor Grammars: A framework for
specifying compositional nonparametric Bayesian
models. In B. Scho?lkopf, J. Platt, and T. Hoffman,
editors, Advances in Neural Information Processing
Systems 19, pages 641?648. MIT Press, Cambridge,
MA.
Mark Johnson. 2008. Using Adaptor Grammars to
identify synergies in the unsupervised acquisition of
linguistic structure. In Proceedings of the 46th An-
nual Meeting of the Association of Computational
Linguistics, pages 398?406, Columbus, Ohio. Asso-
ciation for Computational Linguistics.
Jason Naradowsky and Sharon Goldwater. 2009. Im-
proving morphology induction by learning spelling
rules. In Proceedings of the 21st international jont
conference on Artifical intelligence, pages 1531?
1536, San Francisco, CA, USA. Morgan Kaufmann
Publishers Inc.
Radford M. Neal. 2003. Slice sampling. Annals of
Statistics, 31:705?767.
Dennis Norris, James M. Mcqueen, Anne Cutler, and
Sally Butterfield. 1997. The possible-word con-
straint in the segmentation of continuous speech.
Cognitive Psychology, 34(3):191 ? 243.
Mark A. Pitt, Laura Dilley, Keith Johnson, Scott Kies-
ling, William Raymond, Elizabeth Hume, and Eric
Fosler-Lussier. 2007. Buckeye corpus of conversa-
tional speech.
Alan Prince and Paul Smolensky. 2004. Optimality
Theory: Constraint Interaction in Generative Gram-
mar. Blackwell.
Yee Whye Teh, Michael Jordan, Matthew Beal, and
David Blei. 2006. Hierarchical Dirichlet pro-
cesses. Journal of the American Statistical Associa-
tion, 101:1566?1581.
1516
Proceedings of the Workshop on Cognitive Modeling and Computational Linguistics, pages 1?10,
Sofia, Bulgaria, August 8, 2013. c?2013 Association for Computational Linguistics
Whyisenglishsoeasytosegment?
Abdellah Fourtassi1, Benjamin Bo?rschinger2,3
Mark Johnson3 and Emmanuel Dupoux1
(1) Laboratoire de Sciences Cognitives et Psycholinguistique, ENS/EHESS/CNRS, Paris
(2) Department of Computing, Macquarie University
(3) Department of Computational Linguistics, Heidelberg University
{abdellah.fourtassi, emmanuel.dupoux}@gmail.com , {benjamin.borschinger, mark.johnson}@mq.edu.au
?
Abstract
Cross-linguistic studies on unsupervised
word segmentation have consistently
shown that English is easier to segment
than other languages. In this paper, we
propose an explanation of this finding
based on the notion of segmentation
ambiguity. We show that English has a
very low segmentation ambiguity com-
pared to Japanese and that this difference
correlates with the segmentation perfor-
mance in a unigram model. We suggest
that segmentation ambiguity is linked
to a trade-off between syllable structure
complexity and word length distribution.
1 Introduction
During the course of language acquisition, in-
fants must learn to segment words from continu-
ous speech. Experimental studies show that they
start doing so from around 7.5 months of age
(Jusczyk and Aslin, 1995). Further studies indi-
cate that infants are sensitive to a number of word
boundary cues, like prosody (Jusczyk et al, 1999;
Mattys et al, 1999), transition probabilities (Saf-
fran et al, 1996; Pelucchi et al, 2009), phonotac-
tics (Mattys et al, 2001), coarticulation (Johnson
and Jusczyk, 2001) and combine these cues with
different weights (Weiss et al, 2010).
Computational models of word segmentation
have played a major role in assessing the relevance
and reliability of different statistical cues present
in the speech input. Some of these models focus
mainly on boundary detection, and assess differ-
ent strategies to identify them (Christiansen et al,
1998; Xanthos, 2004; Swingley, 2005; Daland and
Pierrehumbert, 2011). Other models, sometimes
called lexicon-building algorithms, learn the lexi-
con and the segmentation at the same time and use
knowledge about the extracted lexicon to segment
novel utterances. State-of-the-art lexicon-building
segmentation algorithms are typically reported to
yield better performance than word boundary de-
tection algorithms (Brent, 1999; Venkataraman,
2001; Batchelder, 2002; Goldwater, 2007; John-
son, 2008b; Fleck, 2008; Blanchard et al, 2010).
As seen in Table 1, however, the performance
varies considerably across languages with English
winning by a high margin. This raises a general-
izability issue for NLP applications, but also for
the modeling of language acquisition since, obvi-
ously, it is not the case that in some languages,
infants fail to acquire an adult lexicon. Are these
performance differences only due to the fact that
the algorithms might be optimized for English? Or
do they also reflect some intrinsic linguistic differ-
ences between languages?
Lang. F-score Model Reference
English 0.89 AG Johnson (2009)
Chinese 0.77 AG Johnson (2010)
Spanish 0.58 DP Bigram Fleck (2008)
Arabic 0.56 WordEnds Fleck (2008)
Sesotho 0.55 AG Johnson (2008)
Japanese 0.55 BootLex Batchelder (2002)
French 0.54 NGS-u Boruta (2011)
Table 1: State-of-the-art unsupervised segmentation scores
for eight languages.
The aim of the present work is to understand
why English usually scores better than other lan-
guages, as far as unsupervised segmentation is
concerned. As a comparison point, we chose
Japanese because it is among the languages that
have given the poorest word segmentation scores.
In fact, Boruta et al (2011) found an F-score
around 0.41 using both Brent (1999)?s MBDP-1
and Venkataraman (2001)?s NGS-u models, and
Batchelder (2002) found an F-score that goes
from 0.40 to 0.55 depending on the corpus used.
Japanese also differs typologically from English
along several phonological dimensions such as
1
number of syllabic types, phonotactic constraints
and rhythmic structure. Although most lexicon-
building segmentation algorithms do not attempt
to model these dimensions, they still might be rel-
evant to speech segmentation and help explain the
performance difference.
The structure of the paper is as follows. First,
we present the class of lexical-building segmen-
tation algorithm that we use in this paper (Adap-
tor Grammar), and our English and Japanese cor-
pora. We then present data replicating the basic
finding that segmentation performance is better for
English than for Japanese. We then explore the hy-
pothesis that this finding is due to an intrinsic dif-
ference in segmentation ambiguity in the two lan-
guages, and suggest that the source of this differ-
ence rests in the structure of the phonological lexi-
con in the two languages. Finally, we use these in-
sights to try and reduce the gap between Japanese
and English segmentation through a modification
of the Unigram model where multiple linguistic
levels are learned jointly.
2 Computational Framework and
Corpora
2.1 Adaptor Grammar
In this study, we use the Adaptor Grammar frame-
work (Johnson et al, 2007) to test different mod-
els of word segmentation on English and Japanese
Corpora. This framework makes it possible to
express a class of hierarchical non-parametric
Bayesian models using an extension of probabilis-
tic context-free grammars called Adaptor Gram-
mar (AG). It allows one to easily define models
that incorporate different assumptions about lin-
guistic structure and is therefore a useful practical
tool for exploring different hypotheses about word
segmentation (Johnson, 2008b; Johnson, 2008a;
Johnson et al, 2010; Bo?rschinger et al, 2012).
For mathematical details and a description of
the inference procedure for AGs, we refer the
reader to Johnson et al (2007). Briefly, AG uses
the non-parametric Pitman-Yor-Process (Pitman
and Yor, 1997) which, as in Minimum Descrip-
tion lengths models, finds a compact representa-
tion of the input by re-using frequent structures
(here, words).
2.2 Corpora
In the present study, we used both Child Di-
rected Speech (CDS) and Adult Directed Speech
(ADS) corpora. English CDS was derived from
the Bernstein-Ratner corpus (Bernstein-Ratner,
1987), which consists in transcribed verbal inter-
action of parents with nine children between 1
and 2 years of age. We used the 9,790 utter-
ances that were phonemically transcribed by Brent
and Cartwright (1996). Japanese CDS consists in
the first 10, 000 utterances of the Hamasaki cor-
pus (Hamasaki, 2002). It provides a phonemic
transcript of spontaneous speech to a single child
collected from when the child was 2 up to when
it was 3.5 years old. Both CDS corpora are avail-
able from the CHILDES database (MacWhinney,
2000).
As for English ADS, we used the first 10,000
utterances of the Buckeye Speech Corpus (Pitt et
al., 2007) which consists in spontaneous conver-
sations with 40 speakers in American English. To
make it comparable to the other corpora in this
paper, we only used the idealized phonemic tran-
scription. Finally, for Japanese ADS, we used
the first 10,000 utterances of a phonemic tran-
scription of the Corpus of Spontaneous Japanese
(Maekawa et al, 2000). It consists of recorded
spontaneous conversations, or public speeches in
different fields ranging from engineering to hu-
manities. For each corpus, we present elementary
statistics in Table 2.
3 Unsupervised segmentation with the
Unigram Model
3.1 Setup
In this experiment we used the Adaptor Gram-
mar framework to implement a Unigram model of
word segmentation (Johnson et al, 2007). This
model has been shown to be equivalent to the orig-
inal MBDP-1 segmentation model (see Goldwater
(2007)). The model is defined as:
?
Utterance?Word+
Word? Phoneme+
?
In the AG framework, an underlined non-
terminal indicates that this non-terminal is
adapted, i.e. that the AG will cache (and learn
probabilities for) entire sub-trees rooted in this
non-terminal. Here, Word is the only unit that the
model effectively learns, and there are no depen-
dencies between the words to be learned. This
grammar states that an utterance must be analyzed
in terms of one or more Words, where a Word is a
2
Corpus Child Directed Speech Adult Directed Speech
? English Japanese English Japanese
Tokens
? Utterances 9, 790 10, 000 10, 000 10, 000
? Words 33, 399 27, 362 57, 185 87, 156
? Phonemes 95, 809 108, 427 183, 196 289, 264
Types
? Words 1, 321 2, 389 3, 708 4, 206
? Phonemes 50 30 44 25
Average Lengths
? Words per utterance 3.41 2.74 5.72 8.72
? Phonemes per utterance 9.79 10.84 18.32 28.93
? Phonemes per word 2.87 3.96 3.20 3.32
Table 2 : Characteristics of phonemically transcribed corpora
sequence of Phonemes.
We ran the model twice on each corpus for
2,000 iterations with hyper-parameter sampling
and we collected samples throughout the process,
following the methodology of Johnson and Gold-
water (2009)1. For evaluation, we performed their
Minimum Bayes Risk decoding using the col-
lected samples to get a single score.
3.2 Evaluation
For the evaluation, we used the same measures as
Brent (1999), Venkataraman (2001) and Goldwa-
ter (2007), namely token Precision (P), Recall (R)
and F-score (F). Precision is defined as the num-
ber of correct word tokens found out of all tokens
posited. Recall is the number of correct word to-
kens found out of all tokens in the gold standard.
The F-score is defined as the harmonic mean of
Precision and Recall , F = 2?P?RP+R .
We will refer to these scores as the segmentation
scores. In addition, we define similar measures for
word boundaries and word types in the lexicon.
3.3 Results and discussion
The results are shown in Table 3. As expected,
the model yields substantially better scores in En-
glish than Japanese, for both CDS and ADS. In
addition, we found that in both languages, ADS
yields slightly worse results than CDS. This is to
be expected because ADS uses between 60% and
300% longer utterances than CDS, and as a result
presents the learner with a more difficult segmen-
tation problem. Moreover, ADS includes between
1We used incremental initialization
70% and 280% more word types than CDS, mak-
ing it a more difficult lexical learning problem.
Note, however, that despite these large differences
in corpus statistics, the difference in segmentation
performance between ADS and CDS are small
compared to the differences between Japanese and
English.
An error analysis on English data shows that
most errors come from the Unigrammodel mistak-
ing high frequency collocations for single words
(see also Goldwater (2007)). This leads to an
under-segmentation of chunks like ?a boy? or ?is
it? 2. Yet, the model also tends to break off fre-
quent morphological affixes, especially ?-ing? and
?-s? , leading to an over-segmentation of words
like ?talk ing? or ?black s?.
Similarly, Japanese data shows both over-
and under-segmentation errors. However, over-
segmentation is more severe than for English, as
it does not only affect affixes, but surfaces as
breaking apart multi-syllabic words. In addition,
Japanese segmentation faces another kind of er-
ror which acts across word boundaries. For exam-
ple, ?ni kashite? is segmented as ?nika shite? and
?nurete inakatta? as ?nure tei na katta?. This leads
to an output lexicon that, on the one hand, allows
for a more compact analysis of the corpus than
the true lexicon: the number of word types drops
from 2,389 to 1,463 in CDS and from 4,206 to
2,372 in ADS although the average token length ?
and consequently, overall number of tokens ? does
not change as dramatically, dropping from 3.96 to
2For ease of presentation, we use orthography to present
examples although all experiments are run on phonemic tran-
scripts.
3
? Child Directed Speech Adult Directed Speech
? English Japanese English Japanese
? F P R F P R F P R F P R
Segmentation 0.77 0.76 0.77 0.55 0.51 0.61 0.69 0.66 0.73 0.50 0.48 0.52
Boundaries 0.87 0.87 0.88 0.72 0.63 0.83 0.86 0.81 0.91 0.76 0.74 0.79
Lexicon 0.62 0.65 0.59 0.33 0.43 0.26 0.41 0.48 0.36 0.30 0.42 0.23
Table 3 : Word segmentation scores of the Unigram model
3.31 for CDS and from 3.32 to 3.12 in ADS. On
the other hand, however, most of the output lex-
icon items are not valid Japanese words and this
leads to the bad lexicon F-scores. This, in turn,
leads to the bad overall segmentation performance.
In brief, we have shown that, across two dif-
ferent corpora, English yields consistently better
segmentation results than Japanese for the Uni-
gram model. This confirms and extends the results
of Boruta et al (2011) and Batchelder (2002). It
strongly suggests that the difference is neither due
to a specific choice of model nor to particularities
of the corpora, but reflects a fundamental property
of these two languages.
In the following section, we introduce the no-
tion of segmentation ambiguity, it to English and
Japanese data, and show that it correlates with seg-
mentation performance.
4 Intrinsic Segmentation Ambiguity
Lexicon-based segmentation algorithms like
MBDP-1, NGS-u and the AG Unigram model
learn the lexicon and the segmentation at the
same time. This makes it difficult, in case of
poor performance, to see whether the problem
comes from the intrinsic segmentability of the
language or from the quality of the extracted
lexicon. Our claim is that Japanese is intrinsically
more difficult to segment than English, even when
a good lexicon is already assumed. We explore
this hypothesis by studying segmentation alone,
assuming a perfect (Gold) lexicon.
4.1 Segmentation ambiguity
Without any information, a string of N phonemes
could be segmented in 2N?1 ways. When a lexi-
con is provided, the set of possible segmentations
is reduced to a smaller number. To illustrate this,
suppose we have to segment the input utterance:
/ay s k r iy m/ 3, and that the lexicon contains the
following words : /ay/ (I), /s k r iy m/ (scream),
/ay s/ (ice), /k r iy m/ (cream). Only two segmen-
tations are possible : /ay skriym/ (I scream) and
/ays kriym/ (ice cream).
We are interested in the ambiguity generated by
the different possible parses that result from such a
supervised segmentation. In order to quantify this
idea in general, we define a Normalized Segmenta-
tion Entropy. To do this, we need to assign a prob-
ability to every possible segmentation. To this end,
we use a unigram model where the probability of a
lexical item is its normalized frequency in the cor-
pus and the probability of a parse is the product
of the probabilities of its terms. In order to obtain
a measure that does not depend on the utterance
length, we normalize by the number of possible
boundaries in the utterance. So for an utterance of
length N , the Normalized Segmentation Entropy
(NSE) is computed using Shannon formula (Shan-
non, 1948) as follows:
?
NSE = ?
?
i Pilog2(Pi)/(N ? 1)
?
where Pi is the probability of the parse i .
For CDS data we found Normalized Segmen-
tation Entropies of 0.0021 bits for English and
0.0156 bits for Japanese. In ADS data we
found similar results with 0.0032 bits for English
and 0.0275 bits for Japanese. This means that
Japanese needs between 7 and 8 times more bits
than English to encode segmentation information.
This is a very large difference, which is of the
same magnitude in CDS and ADS. These differ-
ences clearly show that intrinsically, Japanese is
more ambiguous than English with regards to seg-
mentation.
One can refine this analysis by distinguishing
two sources of ambiguity: ambiguity across word
boundaries, as in ?ice cream / [ay s] [k r iy m]?
3We use ARPABET notation to represent phonemic input.
4
Figure 1 : Correlation between Normalized Segmentation Entropy (in bits) and the segmentation F-score for CDS (left) and
ADS (Right)
vs ?I scream / [ay] [s k r iy m]?. And ambigu-
ity within the lexicon, that occurs when a lexical
item is composed of two or more sub-words (like
in ?Butterfly?).
Since we are mainly investigating lexicon-
building models, it is important to measure the am-
biguity within the lexicon itself, in the ideal case
where this lexicon is perfect. To this end, we com-
puted the average number of segmentations for a
lexicon item. For example, the word ?butterfly?
has two possible segmentations : the original word
?butterfly? and a segmentation comprising the two
sub-words : ?butter? and ?fly?. For English to-
kens, we found an average of 1.039 in CDS and
1.057 in ADS. For Japanese tokens, we found an
average of 1.811 in CDS and 1.978 in ADS. En-
glish?s averages are close to 1, indicating that it
doesn?t exhibit lexicon ambiguity. Japanese, how-
ever, has averages close to 2 which means that lex-
ical ambiguity is quite systematic in both CDS and
ADS.
4.2 Segmentation ambiguity and supervised
segmentation
The intrinsic ambiguity in Japanese only shows
that a given sentence has multiple possible seg-
mentations. What remains to be demonstrated is
that these multiple segmentations result in system-
atic segmentation errors. To do this we propose
a supervised segmentation algorithm that enumer-
ates all possible segmentations of an utterance
based on the gold lexicon, and selects the segmen-
tation with the highest probability. In CDS data,
this algorithm yields a segmentation F-score equal
to 0.99 for English and 0.95 for Japanese. In ADS
we find an F-score of 0.96 for English and 0.93 for
Japanese. These results show that lexical informa-
tion alone plus word frequency eliminates almost
all segmentation errors in English, especially for
CDS. As for Japanese, even if the scores remain
impressively high, the lexicon alone is not suffi-
cient to eliminate all the errors. In other words,
even with a gold lexicon, English remains easier
to segment than Japanese.
To quantify the link between segmentation en-
tropy and segmentation errors, we binned the sen-
tences of our corpus in 10 bins according to the
Normalized Segmentation Entropy, and correlate
this with the average segmentation F-score for
each bin. As shown Figure 1, we found significant
correlations: (R = ?0.86, p < 0.001) for CDS
and (R = ?0.93, p < 0.001) for ADS, showing
that segmentation ambiguity has a strong effect
even on supervised segmentation scores. The cor-
relation within language was also significant but
only in the Japanese data : R = ?0.70 for CDS
and R = ?0.62 for ADS.
?
Next, we explore one possible reason for this
structural difference between Japanese and En-
glish, especially at the level of the lexicon.
4.3 Syllable structure and lexical
composition of Japanese and English
One of the most salient differences between En-
glish and Japanese phonology concerns their syl-
lable structure. This is illustrated in Figure 2
(above), where we plotted the frequency of the dif-
ferent syllabic structures of monosyllabic tokens
in English and Japanese CDS. The statistics show
that English has a very rich syllabic composition
where a diversity of consonant clusters is allowed,
whereas Japanese syllable structure is quite simple
and mostly composed of the default CV type. This
difference is bound to have an effect on the struc-
ture of the lexicon. Indeed, Japanese has to use
5
Figure 2 : Trade-off between the complexity of syllable structure (above) and the word token length in terms of syllables
(below) for English and Japanese CDS.
multisyllabic words in order to achieve a large size
lexicon, whereas, in principle, English could use
mostly monosyllables. In Figure 2 (below) we dis-
play the distribution of word length as measured
in syllables in the two languages for the CDS cor-
pora. The English data is indeed mostly composed
of mono-syllabic words whereas the Japanese one
is made of words of more varied lengths. Overall,
we have documented a trade-off between the di-
versity of syllable structure on the one hand, and
the diversity of word lengths on the other (see Ta-
ble 4 for a summary of this tradeoff expressed in
terms of entropy).
? CDS ADS
? Eng. Jap. Eng. Jap.
Syllable types 2.40 1.38 2.58 1.03
Token lengths 0.62 2.04 0.99 1.69
Table 4 : Entropies of syllable types and token lengths in
terms of syllables (in bits)
We suggest that this trade-off is responsible for
the difference in the lexicon ambiguity across the
two languages. Specifically, the combination of
a small number of syllable types and, as a conse-
quence, the tendency for multi-syllabic word types
in Japanese makes it likely that a long word will
be composed of smaller ones. This cannot happen
very often in English, since most words are mono-
syllabic, and words smaller than a syllable are not
allowed.
5 Improving Japanese unsupervised
segmentation
We showed in the previous section that ambigu-
ity impacts segmentation even with a gold lexicon,
mainly because the lexicon itself could be ambigu-
ous. In an unsupervised segmentation setting, the
problem is worse because ambiguity within and
across word boundaries leads to a bad lexicon,
which in turn results in more segmentation errors.
In this section, we explore the possibility of miti-
gating some of these negative consequences.
In section 3, we saw that when the Unigram
model tries to learn Japanese words, it produces an
output lexicon composed of both over- and under-
segmented words in addition to words that re-
sult from a segmentation across word boundaries.
One way to address this is by learning multiple
kinds of units jointly, rather than just words; in-
deed, previous work has shown that richer mod-
els with multiple levels improve segmentation for
English (Johnson, 2008a; Johnson and Goldwater,
2009).
5.1 Two dependency levels
As a first step, we will allow the model to not
just learn words but to also memorize sequences of
words. Johnson (2008a) introduced these units as
?collocations? but we choose to use the more neu-
tral notion of level for reasons that become clear
shortly. Concretely, the grammar is:
6
? CDS ADS
? English Japanese English Japanese
? F P R F P R F P R F P R
Level 1
? Segmentation 0.81 0.77 0.86 0.42 0.33 0.55 0.70 0.63 0.78 0.42 0.35 0.50
? Boundaries 0.91 0.84 0.98 0.63 0.47 0.96 0.86 0.76 0.98 0.73 0.61 0.90
? Lexicon 0.64 0.79 0.54 0.18 0.55 0.10 0.36 0.56 0.26 0.15 0.68 0.08
Level 2
? Segmentation 0.33 0.45 0.26 0.59 0.65 0.53 0.50 0.60 0.43 0.45 0.54 0.38
? Boundaries 0.56 0.98 0.40 0.71 0.87 0.60 0.76 0.95 0.64 0.73 0.92 0.60
? Lexicon 0.36 0.25 0.59 0.47 0.44 0.49 0.46 0.38 0.56 0.43 0.37 0.50
Table 5 : Word segmentation scores of the two levels model
?
Utterance? level2+
level2? level1+
level1? Phoneme+
?
We run this model under the same conditions
as the Unigram model but evaluate two different
situations. The model has no inductive bias that
would force it to equate level1 with words, rather
than level2. Consequently, we evaluate the seg-
mentation that is the result of taking there to be a
boundary between every level1 constituent (Level
1 in Table 5) and between every level2 constituent
(Level 2 in Table 5 ). From these results , we see
that English data has better scores when the lower
level represents the Word unit and when the higher
level captures regularities above the word. How-
ever, Japanese data is best segmented when the
higher level is the Word unit and the lower level
captures sub-word regularities.
Level 1 generally tends to over-segment utter-
ances as can be seen by comparing the Boundary
Recall and Precision scores (Goldwater, 2007). In
fact when the Recall is much higher than the Pre-
cision, we can say that the model has a tendency
to over-segment. Conversely, we see that Level 2
tends to under-segment utterances as the Bound-
ary Precision is higher than the Recall.
Over-segmentation at Level 1 seems to benefit
English since it counteracts the tendency of the
Unigram model to cluster high frequency colloca-
tions. As far as segmentation is concerned, this
effect seems to outweigh the negative effect of
breaking words apart (especially in CDS), as En-
glish words are mostly monosyllabic.
For Japanese, under-segmentation at Level 2
seems to be slightly less harmful than over-
segmentation at Level 1, as it prevents, to some
extent, multi-syllabic words to be split. However,
the scores are not very different from the ones we
had with the Unigram model and slightly worse
for the ADS. What seems to be missing is an inter-
mediate level where over- and under-segmentation
would counteract one another.
5.2 Three dependency levels
We add a third dependency level to our model as
follows :
?
Utterance? level3+
level3? level2+
level2? level1+
level1? Phoneme+
?
As with the previous model, we test each of the
three levels as the word unit, the results are shown
in Table 6.
Except for English CDS, all the corpora
have their best scores with this intermediate
level. Level 1 tends to over-segment Japanese
utterances into syllables and English utterances
into morphemes. Level 3, however, tends to
highly under-segment both languages. English
CDS seems to be already under-segmented at
Level 2, very likely caused by the large number
of word collocations like ?is-it? and ?what-is?,
an observation also made by Bo?rschinger et al
(2012) using different English CDS corpora.
English ADS is quantitatively more sensitive to
over-segmentation than CDS mainly because it
has a richer morphological structure and relatively
longer words in terms of syllables (Table 4).
7
? CDS ADS
? English Japanese English Japanese
? F P R F P R F P R F P R
Level 1
? Segmentation 0.79 0.74 0.85 0.27 0.20 0.41 0.35 0.28 0.48 0.37 0.30 0.47
? Boundaries 0.89 0.81 0.99 0.56 0.39 0.99 0.68 0.52 0.99 0.70 0.57 0.93
? Lexicon 0.58 0.76 0.46 0.10 0.47 0.05 0.13 0.39 0.07 0.10 0.70 0.05
Level 2
? Segmentation 0.49 0.60 0.42 0.70 0.70 0.70 0.77 0.76 0.79 0.60 0.65 0.55
? Boundaries 0.71 0.97 0.56 0.81 0.82 0.81 0.90 0.88 0.92 0.81 0.90 0.74
? Lexicon 0.51 0.41 0.64 0.53 0.59 0.47 0.58 0.69 0.50 0.51 0.57 0.46
Level 3
? Segmentation 0.18 0.31 0.12 0.39 0.53 0.30 0.43 0.55 0.36 0.28 0.42 0.21
? Boundaries 0.26 0.99 0.15 0.46 0.93 0.31 0.71 0.98 0.55 0.59 0.96 0.43
? Lexicon 0.17 0.10 0.38 0.32 0.25 0.41 0.37 0.28 0.51 0.27 0.20 0.42
Table 6 : Word segmentation scores of the three levels model
6 Conclusion
In this paper we identified a property of lan-
guage, segmentation ambiguity, which we quan-
tified through Normalized Segmentation Entropy.
We showed that this quantity predicts performance
in a supervised segmentation task.
With this tool we found that English was in-
trinsically less ambiguous than Japanese, account-
ing for the systematic difference found in this pa-
per. More generally, we suspect that Segmentation
Ambiguity would, to some extent, explain much
of the difference observed across languages (Ta-
ble 1). Further work needs to be carried out to test
the robustness of this hypothesis on a larger scale.
We showed that allowing the system to learn
at multiple levels of structure generally improves
performance, and compensates partially for the
negative effect of segmentation ambiguity on un-
supervised segmentation (where a bad lexicon am-
plifies the effect of segmentation ambiguity). Yet,
we end up with a situation where the best level of
structure may not be the same across corpora or
languages, which raises the question as to how to
determine which level is the correct lexical level,
i.e., the level that can sustain successful grammat-
ical and semantic learning. Further research is
needed to answer this question.
Generally speaking, ambiguity is a challenge in
many speech and language processing tasks: for
example part-of-speech tagging and word sense
disambiguation tackle lexical ambiguity, proba-
bilistic parsing deals with syntactic ambiguity and
speech act interpretation deals with pragmatic am-
biguities. However, to our knowledge, ambiguity
has rarely been considered as a serious problem in
word segmentation tasks.
As we have shown, the lexicon-based approach
does not completely solve the segmentation am-
biguity problem since the lexicon itself could be
more or less ambiguous depending on the lan-
guage. Evidently, however, infants in all lan-
guages manage to overcome this ambiguity. It has
to be the case, therefore, that they solve this prob-
lem through the use of alternative strategies, for
instance by relying on sub-lexical cues (see Jarosz
and Johnson (2013)) or by incorporating semantic
or syntactic constraints (Johnson et al, 2010). It
remains a major challenge to integrate these strate-
gies within a common model that can learn with
comparable performance across typologically dis-
tinct languages.
Acknowledgements
The research leading to these results has received funding
from the European Research Council (FP/2007-2013) / ERC
Grant Agreement n. ERC-2011-AdG-295810 BOOTPHON,
from the Agence Nationale pour la Recherche (ANR-2010-
BLAN-1901-1 BOOTLANG, ANR-11-0001-02 PSL* and
ANR-10-LABX-0087) and the Fondation de France. This
research was also supported under the Australian Research
Council?s Discovery Projects funding scheme (project num-
bers DP110102506 and DP110102593).
8
References
Eleanor Olds Batchelder. 2002. Bootstrapping the lex-
icon: A computational model of infant speech seg-
mentation. Cognition, 83(2):167?206.
N. Bernstein-Ratner. 1987. The phonology of parent-
child speech. In K. Nelson and A. van Kleeck,
editors, Children?s Language, volume 6. Erlbaum,
Hillsdale, NJ.
Daniel Blanchard, Jeffrey Heinz, and Roberta
Golinkoff. 2010. Modeling the contribution of
phonotactic cues to the problem of word segmenta-
tion. Journal of Child Language, 37(3):487?511.
Benjamin Bo?rschinger, Katherine Demuth, and Mark
Johnson. 2012. Studying the effect of input size
for Bayesian word segmentation on the Providence
corpus. In Proceedings of the 24th International
Conference on Computational Linguistics (Coling
2012), pages 325?340, Mumbai, India. Coling 2012
Organizing Committee.
Luc Boruta, Sharon Peperkamp, Beno??t Crabbe?, and
Emmanuel Dupoux. 2011. Testing the robustness
of online word segmentation: Effects of linguistic
diversity and phonetic variation. In Proceedings of
the 2nd Workshop on Cognitive Modeling and Com-
putational Linguistics, pages 1?9, Portland, Oregon,
USA, June. Association for Computational Linguis-
tics.
M. Brent and T. Cartwright. 1996. Distributional regu-
larity and phonotactic constraints are useful for seg-
mentation. Cognition, 61:93?125.
M. Brent. 1999. An efficient, probabilistically sound
algorithm for segmentation and word discovery.
Machine Learning, 34:71?105.
Morten H Christiansen, Joseph Allen, and Mark S Sei-
denberg. 1998. Learning to segment speech using
multiple cues: A connectionist model. Language
and cognitive processes, 13(2-3):221?268.
Robert Daland and Janet B Pierrehumbert. 2011.
Learning diphone-based segmentation. Cognitive
Science, 35(1):119?155.
Margaret M. Fleck. 2008. Lexicalized phonotac-
tic word segmentation. In Proceedings of ACL-08:
HLT, pages 130?138, Columbus, Ohio, June. Asso-
ciation for Computational Linguistics.
Sharon Goldwater. 2007. Nonparametric Bayesian
Models of Lexical Acquisition. Ph.D. thesis, Brown
University.
Naomi Hamasaki. 2002. The timing shift of two-year-
olds responses to caretakers yes/no questions. In
Studies in language sciences (2)Papers from the 2nd
Annual Conference of the Japanese Society for Lan-
guage Sciences, pages 193?206.
Gaja Jarosz and J Alex Johnson. 2013. The richness
of distributional cues to word boundaries in speech
to young children. Language Learning and Devel-
opment, (ahead-of-print):1?36.
Mark Johnson and Katherine Demuth. 2010. Unsuper-
vised phonemic Chinese word segmentation using
Adaptor Grammars. In Proceedings of the 23rd In-
ternational Conference on Computational Linguis-
tics (Coling 2010), pages 528?536, Beijing, China,
August. Coling 2010 Organizing Committee.
Mark Johnson and Sharon Goldwater. 2009. Im-
proving nonparameteric Bayesian inference: exper-
iments on unsupervised word segmentation with
adaptor grammars. In Proceedings of Human Lan-
guage Technologies: The 2009 Annual Conference
of the North American Chapter of the Associa-
tion for Computational Linguistics, pages 317?325,
Boulder, Colorado, June. Association for Computa-
tional Linguistics.
Elizabeth K. Johnson and Peter W. Jusczyk. 2001.
Word segmentation by 8-month-olds: When speech
cues count more than statistics. Journal of Memory
and Language, 44:1?20.
Mark Johnson, Thomas Griffiths, and Sharon Gold-
water. 2007. Bayesian inference for PCFGs via
Markov chain Monte Carlo. In Human Language
Technologies 2007: The Conference of the North
American Chapter of the Association for Computa-
tional Linguistics; Proceedings of the Main Confer-
ence, pages 139?146, Rochester, New York. Associ-
ation for Computational Linguistics.
Mark Johnson, Katherine Demuth, Michael Frank, and
Bevan Jones. 2010. Synergies in learning words
and their referents. In J. Lafferty, C. K. I. Williams,
J. Shawe-Taylor, R.S. Zemel, and A. Culotta, ed-
itors, Advances in Neural Information Processing
Systems 23, pages 1018?1026.
Mark Johnson. 2008a. Unsupervised word segmen-
tation for Sesotho using Adaptor Grammars. In
Proceedings of the Tenth Meeting of ACL Special
Interest Group on Computational Morphology and
Phonology, pages 20?27, Columbus, Ohio, June.
Association for Computational Linguistics.
Mark Johnson. 2008b. Using Adaptor Grammars to
identify synergies in the unsupervised acquisition of
linguistic structure. In Proceedings of the 46th An-
nual Meeting of the Association of Computational
Linguistics, pages 398?406, Columbus, Ohio. Asso-
ciation for Computational Linguistics.
Peter W Jusczyk and Richard N Aslin. 1995. Infants
detection of the sound patterns of words in fluent
speech. Cognitive psychology, 29(1):1?23.
Peter W. Jusczyk, E. A. Hohne, and A. Bauman.
1999. Infants? sensitivity to allophonic cues for
word segmentation. Perception and Psychophysics,
61:1465?1476.
9
Brian MacWhinney. 2000. The CHILDES Project:
Tools for Analyzing Talk. Transcription, format and
programs, volume 1. Lawrence Erlbaum.
Kikuo Maekawa, Hanae Koiso, Sadaoki Furui, and Hi-
toshi Isahara. 2000. Spontaneous speech corpus of
japanese. In proc. LREC, volume 2, pages 947?952.
Sven L Mattys, Peter W Jusczyk, Paul A Luce, James L
Morgan, et al 1999. Phonotactic and prosodic ef-
fects on word segmentation in infants. Cognitive
psychology, 38(4):465?494.
Sven L Mattys, Peter W Jusczyk, et al 2001. Do
infants segment words or recurring contiguous pat-
terns? Journal of experimental psychology, human
perception and performance, 27(3):644?655.
Bruna Pelucchi, Jessica F Hay, and Jenny R Saffran.
2009. Learning in reverse: Eight-month-old infants
track backward transitional probabilities. Cognition,
113(2):244?247.
J. Pitman and M. Yor. 1997. The two-parameter
Poisson-Dirichlet distribution derived from a stable
subordinator. Annals of Probability, 25:855?900.
M. A. Pitt, L. Dilley, K. Johnson, S. Kiesling, W. Ray-
mond, E. Hume, and Fosler-Lussier. 2007. Buckeye
corpus of conversational speech.
J. Saffran, R. Aslin, and E. Newport. 1996. Sta-
tistical learning by 8-month-old infants. Science,
274:1926?1928.
Claude Shannon. 1948. A mathematical theory of
communication. Bell System Technical Journal,
27(3):379?423.
Daniel Swingley. 2005. Statistical clustering and the
contents of the infant vocabulary. Cognitive Psy-
chology, 50:86?132.
A. Venkataraman. 2001. A statistical model for word
discovery in transcribed speech. Computational
Linguistics, 27(3):351?372.
Daniel J Weiss, Chip Gerfen, and Aaron D Mitchel.
2010. Colliding cues in word segmentation: the
role of cue strength and general cognitive processes.
Language and Cognitive Processes, 25(3):402?422.
Aris Xanthos. 2004. Combining utterance-boundary
and predictability approaches to speech segmenta-
tion. In First Workshop on Psycho-computational
Models of Human Language Acquisition, page 93.
10
