Proceedings of the ACL-08: HLT Workshop on Parsing German (PaGe-08), pages 1?8,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Lexcalised Parsing of German V2
Yo Sato
Department of Computer Science
Queen Mary, University of London
Mile End Road, London E1 4NS, U.K.
Abstract
This paper presents a method and implemen-
tation of parsing German V2 word order by
means of constraints that reside in lexical
heads. It first describes the design of the
underlying parsing engine: the head-corner
chart parsing that incorporates a procedure
that dynamically enforces word order con-
straints. While the parser could potentially
generate all the permutations of terminal sym-
bols, constraint checking is conducted locally
in an efficient manner. The paper then shows
how this parser can adequately cover a variety
of V2 word order patterns with sets of lexi-
cally encoded constraints, including non-local
preposing of an embedded argument or an ad-
verbial.
1 Introduction
This paper presents a method of parsing V2 word
order manifested in a variety of German matrix sen-
tences in a lexicalised and locality-respecting man-
ner: lexicalised, as the V2 pattern is licensed ulti-
mately encoded in verbs, in the form of constraints
that hold amongst its arguments and itself; locality-
respecting, because (a) no constraint that operates on
constituents from different subcategorisation frames
is invoked and (b) the matrix verb and the prever-
bal constituent, however ?distant? its origin is, are
ordered in the same projection via the slash-based
mechanism.
The underlying grammar is loosely linearisation-
based, in the sense that word order is dissoci-
ated from the syntactic structure in a discontinuity-
allowing manner, as presented in Sato (2008). The
main benefit of a linearisation approach is that syn-
tactic constituency becomes independent (to a de-
gree) of its surface realisation and hence discour-
ages constituency manipulation for the sake of word
order. In line of this spirit I will largely adopt the
simple constituency construal that faithfully corre-
spond to its semantics. However, I distance myself
from the more or less standard version of linearisa-
tion grammar where potentially non-local LP con-
ditions are permitted (Reape, 1993) or word order
patterns are imposed at the clause level (as in ?topo-
logical field? model of Kathol (2000)).
The crux of the proposal consists in employing
a head-corner parsing in which the set of word or-
der constraints are incorporated into a VP?s lexical
head (i.e. common or auxiliary verb). For a V2 pro-
jection, its head verb contains the constraints to the
effect that only one of its arguments can be fronted
immediately before the verb itself. To enable this,
potential discontinuity and obligatory adjacency in
part of a phrase is included in the repertoire of word
order constraints in addition to the standard LP (lin-
ear precedence) constraints.
2 The data
The V2 constructions to be dealt with in this paper
are as follows (I will use as an example the tertiary
verb gebengive or its past participle gegebengiven
throughout):
1. The ?basic? case where dependency between
the preverbal constituent and the matrix verb is
strictly local, e.g:
1
Ein Buch geben die Eltern dem Sohn.
a book give the parents the son
?A book the parents give the son?
2. The case where an argument of the lower verb
is fronted across the higher auxiliary verb:
Ein Buch haben die Eltern dem Sohn gegeben.
a book have the parents the son given
?A book the parents have given the son?
3. The long-distance dependency case:
Ein Buch, sagt ein Freund, dass er glaubt, dass die
Eltern dem Sohn geben.
?A book, a friend says that he thinks that the parents
give the son?
4. Adjunct fronting
Heimlich haben die Eltern dem Sohn ein Buch gegeben.
secretly have the parents the son a book given
?Secretly the parents have given the son a book.?
5. Partial VP fronting
Ein Buch dem Sohn gegeben haben die Eltern.
Ein Buch gegeben haben die Eltern dem Sohn.
As stated, our approach adopts a linearisation ap-
proach in which constituency does not determine the
surface realisation, which is handled instead by word
order conditions encoded in lexical heads. My con-
tention here is not so much plausibility as a grammar
as neutrality to particular phrase structures, which
linearisation promotes. Therefore I take a rather
simplified position to use an entirely uniform phrase
structure for the verb-argument structure for com-
mon verbs, namely the flat construal where all the
arguments as well as the head project onto a clause
(?VP?) as mutual sisters, although I hasten to add
our constraint enforcement could equally apply to
configurational analyses. In fact we take an auxil-
iary verb to subcategorise for a clause rather than
the complex verb analysis, and adopt the traditional
binary iteration analysis for adjunct-head phrases, to
see how our parser fares with configurational analy-
ses.
I sum up the assumed constituency of the above
examples graphically as trees (though this has little
impact on word order):
(1) Clause(=VP)      
V
geben

NP
die Eltern
DD
NP
dem Sohn
aaa
NP
ein Buch
(2)&(5)     
Aux
haben
PPP
Clause(((( hhhh
-e E. -em S. ein Buch gegeben
(3) ((((((
NP
ein Freund

V
sagt
bb
CP!!
C
dass
HH
Clause
NP
er

V
glaubt
aa
CP""
C
dass
```
C`lause    ````
-e E. -em S. ein Buch geben
(4) 
Aux
haben
PPP
Clause    
Adv
heimlich
PPP
Clause(((( hhhh
-e E. -em S. ein Buch gegeben
3 The parser
3.1 Core design
The design of the parser employed here can be
called constrained free word order parsing. First,
it allows for completely free word order at default.
The core algorithm for the parse engine is what
Reape (1991) presents as a generalised permutation-
complete parser, which in turn is based on the pre-
ceding proposal of Johnson (1985). Details apart,
while using context-free production rules (no multi-
ple left-hand side non-terminal symbols), this algo-
rithm only checks for the presence of all the right-
hand side constituents, wherever in the string they
occur, potentially discontinuously,1 effectively li-
censing all the permutations of the given terminal
symbols (e.g. 3! = 6 permutations for the string
consisting of ring, up and John including up John
ring etc.). This ?directionless? parsing is rendered
possible by Johnson?s ?bitvector? representation of
partial string coverage. In the above up John ring
string, the coverage of the ring and up combina-
1More precisely, it searches for non-overlapping combina-
tions, excluding the same word being counted more than once
or more than one word counting towards the same rule in the
same search path.
2
tion, which materially constitutes a complex verb,
is represented as [1,0,1]. This is then then merged
with the bitvector of John, [0,1,0] into [1,1,1]. Sec-
ond, however, this rather promiscuous (and expen-
sive) parsing is dynamically restricted by word or-
der constraints that obtain in individual languages.
With sufficient constraints applied during the parse,
the above combinations with ring, up and John are
restricted to ring up John and ring John up.
I do not claim for originality in this basic design.
Daniels (2005) for example describes an implemen-
tation of an algorithm that falls precisely in such
style of parsing.2 The main points of the proposal
lie in lexicalisation and localisation, which contrast
with the general trend to introduce phrasal and non-
local constraint processing for German processing,
of which Daniels? work is an example. All the word
order constraints are stored in lexicon, more specifi-
cally in lexical heads.
To adapt this design to a practical lexically driven
parsing, the author implemented a rendering of
head-corner chart parsing. It is head-corner in the
sense described e.g. in van Noord (1991), where
the parsing of a production rule always starts from
its head. This is necessary for our design because
the parser first retrieves the word order information
from the head. Furthermore, it requires the words
to be processed first by preterminal rules since with-
out processing lexical heads the whole recognition
process does not come off the ground. Therefore, a
chart parsing algorithm that invokes lexical initiali-
sation is utilised (as described in Gazdar & Mellish
(1989) rather than the classical top-down parsing of
Earley (1970)).
3.2 Constraint checking and propagation
Since no non-local word order constraints are intro-
duced in our parsing, they can be fully enforced at
each application of a production rule. More specif-
ically, the checking of constraint compliance is car-
ried out at the completer operation of chart pars-
ing.3 The data structure of an edge is suitably mod-
ified. In addition to the dotted production rule, it
needs to carry the constraint set relevant to the corre-
2A foregoing implementation by Mu?ller (2004) also em-
ploys bitvector-based linearisation approach.
3The equivalent operation is called the ?fundamental rule? in
Gazdar & Mellish (1989).
sponding production rule, retrievable from the head,
which is always processed first in our head-corner
algorithm.4 Also, as we are adopting the bitvector
representation of coverage, an edge contains its cor-
responding bitvector. The completer operation in-
volves merger of two bitvectors, so the check can be
conducted at this stage:
Completer in constrained parsing
Let A and B be symbols, ?, ? and ? be arbi-
trary strings, V1 and V2 be bitvectors and V m
be their merge, then:
If the chart contains an active edge ?V1, A? ?
? B ?? and a passive edge ?V2, B? ? ? ?, run
the CHECK-ORDER procedure. If it succeeds,
add edge ?V m, A? ?B ? ?? to the chart if V1
and V2 are mergeable. If it fails, do nothing.
The CHECK-ORDER procedure consists in a bit-
wise comparison of bitvectors. It picks out the
bitvectors of the categories in question and checks
the compliance of the newly found category with re-
spect to the relevant constraints. If for example A, B
and C had been found at [0,1,0,0,0], [0,0,1,0,1] and
[1,0,0,1,0] respectively, this would validate A ? B
but not A ? C. Thus the edges for string combina-
tions that violate the word order constraints would
not be created, eliminating wasteful search paths.
As we will shortly see, the constraint type that
checks continuity of a phrase is also introduced.
Therefore the phrase (dis)continuity can also be as-
certained locally, which is a major advantage over a
parsing that relies largely on concatenation. Thus,
the cost of constraint checking remains very small
despite the capability of processing discontinuity.5
Note however that by locality is meant subcat-
egorisation locality (or ?selection? locality as de-
scribed in Sag (2007)): whatever is in the same
subcategorisation frame of a lexical head is consid-
ered local. Depending on the adopted analysis, con-
stituents ?local? in this sense may of course occur
in different trees. Constraints on such ?non-local?
?in the tree sense but not in the subcategorisation
sense? constituents are still enforceable in the im-
plemented parser. The unused constraints at a node,
4This retrieval of word order information is carried out at the
predictor stage of chart parsing.
5It is worth mentioning that the bitvector checking is con-
ducted over the whole string, the effect of applied constraints
will be never lost.
3
for example some constraint applicable to the verb
and its subject at the VP node in the configurational
(subjectless-VP) analysis, is made to propagate up
to the upper node. Thus it is no problem to enforce
a constraint over ?different trees?, as long as it is ap-
plied to ?local? constituents in our sense.6
4 Possible constraints and subtyping
It is crucial, if the computational properties of the
parser is to be transparent in constrained free word
order parsing, to identify the kind of word order con-
straints admitted into lexical heads. We will remain
relatively conservative, in introducing only two op-
erators for constraint encoding. We first invoke the
binary LP operator (?) in a conventional sense: the
whole (or, equivalently, right-periphery) of a string
for category A needs to precede the whole (or left-
periphery) of a string for category B to satisfy A ?
B (I will use the shorthand A ? (B,C) to express
(A ? B) ? (A ? C). Crucially, the contiguity op-
erator () is added. It takes a set of constituents as its
operand and requires the constituents in it to be con-
tiguous, regardless of their order. Thus, {A,B,C}
encodes the requirement for A, B and C as a whole
forming a contiguous string. For example, the string
I ring John up does not satisfy {ring, up} but does
satisfy {ring, John, up}.
Also important is how to succinctly generalise
on the word order patterns now encoded in lexical
items, as one would certainly want to avoid a te-
dious task of writing them all individually, if they
allow for broader classification. For example the En-
glish transitive verb generally follows its subject ar-
gument and precedes its object argument, and one
would naturally want to lump these verbs under one
umbrella. For such a cluster of lexical heads, we will
introduce a word order (sub)type. More pertinently,
the German verbs may be classified into v1-verb, v2-
verb and vf-verb according to the positions of their
arguments in their projection. We will also allow
multiple inheritance that becomes standard in the
typed feature system (cf. Pollard and Sag (1987)).
6See Sato (2006) for details.
5 Constraints for V2
5.1 General setup
To enforce the V2 word order pattern lexically, I pro-
pose to use a combination of two word order sub-
types: dislocating-verb (disl-v) and matrix-v2-verb
(mtrx-v2-v). The former type represents a verb one
of whose arguments is to be ?dislocated?. A verb of
this type can thus be characterised as ?contributing?
the dislocated (preverbal) element. The latter, on the
other hand, is the type that is projected onto a ma-
trix sentence. This type should be constrained such
that one dislocated constituent must ?and only one
may? precede and be adjacent to the verb itself. It
may be characterised as a verb that provides a locus
?immediately before itself? of, or ?receives? the
dislocated element.
Dislocation is handled by a constraint percola-
tion mechanism. I assume the dislocated constituent
is pushed into a storage that then participates in a
slash style percolation, although the storage content
would still need to be ordered by lexicalised con-
straints rather than by the percolation mechanism it-
self, as they are the sole resource for word order.7
Thus the checking as regards the dislocated con-
stituent is conducted at each projection in the per-
colation path, hence locally, while the percolation
mechanism gives some ?global? control over disloca-
tion. Not just the positioning of the dislocated con-
stituent at the left-periphery of the whole sentence,
but the assurance of a global singularity restriction
of dislocation ?not just one constituent per clause
in multiple embeddings? becomes thus possible.
Let args be the set of the arguments of a disl-v,
disl be that of the dislocated one and situ be that of
the remaining arguments, i.e. disl ? args where
|disl| = 1 and situ = {x|x ? args ? x /? disl}.
Then the type disl-v can be characterised as having
the following constraint:
disl-v: disl ? situ (disl ? dislst)
Simply put, this says that the arguments are divided
into two parts, the dislocated and in-situ parts, the
former of which precedes the latter. We assume, as
7The adopted mechanism is close to Penn (1999), though
he invokes potentially non-local topology-based constraints and
removes the filler and gapped head entirely.
4
in the standard treatment, there is only one dislo-
cated constituent, until we consider the VP fronting.
The notation with an arrow on the right indicates this
singleton set is pushed into the storage that is prop-
agated upwards.
The mtrx-v2-v type is then characterised as fol-
lows:
mtrx-v2-v: dislst ? verb, {dislst, verb}
This simply says the dislocated constituent (stored
in a lower node and percolated) immediately pre-
cedes the matrix verb. (For the following presen-
tation, the storage-related notations will be omitted
and implicitly assumed unless necessary. Also, the
set variables disl and args will be used with the same
meaning.)
Thus the combination of the two types gives, for
example where args = {A,B,C}, disl = {A} and
the matrix verb is V , the following constraint set:
{A ? (B,C), A ? V, {A, V }}
which essentially says that the dislocated A immedi-
ately precedes the matrix verb V and precedes (not
necessarily immediately) the in-situ B and C.
5.2 Local case
To begin with, let us see a case where dependency
between the preverbal constituent and the matrix
verb is strictly local, taking (1) as an example. Note
first that there are six possible variants:
(1)
a. Die Eltern geben dem Sohn ein Buch.
b. Die Eltern geben ein Buch dem Sohn.
c. Dem Sohn geben die Eltern ein Buch.
d. Dem Sohn geben ein Buch die Eltern.
e. Ein Buch geben die Eltern dem Sohn.
f. Ein Buch geben dem Sohn die Eltern.
In this case, geben is both a matrix (argument-
receiving) and dislocating (argument-contributing)
verb. This means that the two subtypes should be
overloaded. Let us call this overloaded sub-species
disl-mtrx-v2-v: which is given the following specifi-
cation:
disl-mtrx-v2-v:
disl ? situ, disl ? verb, {disl, verb}
To adapt this type to our verb, geben, where we rep-
resent its arguments as sNP (subject NP), ioNP (in-
direct object NP) and doNP (direct object NP), we
obtain, for the case where sNP is preposed:
{sNP ? (ioNP, doNP),
sNP ? geben, (sNP, geben)}
where the constraints on the first line is inher-
ited from disloc-v while those on the second from
matrix-v2-v. This corresponds to the sentences (a)
and (b) above. The followings are the cases where
ioNP and doNP are preposed, corresponding to (c,d)
and (e,f), respectively.
{ioNP ? (sNP, doNP), ioNP ? geben, (ioNP, geben)}
{doNP ? (sNP, ioNP), doNP ? geben, (doNP, geben)}
These possible sets are enforced in the manner of
exclusive disjunction, that is, only one of the above
three sets actually obtains. This does not mean, how-
ever, each set must be explicitly stated in the verb
and processed blindly. Only the abstract form of
the constraint, as described under the type specifi-
cation above, is written in the lexicon. During pars-
ing, then, one of the sets, as dynamically found to
match the input string, is computed and applied. In
the subsequent discussion, therefore, only the direct-
object fronting case is considered as a representative
example for each construction.
5.3 Argument fronting across auxiliary
We now consider the cases where the dependency is
not local, starting with an auxiliary-involving case.
The dependency between an auxiliary and an ar-
gument of its lower verb is, according to the Aux-
Clause construal adopted here, is not local. We can
however succinctly specify such non-local V2 ren-
derings as a case where the above two types are in-
stantiated separately in two verbs. The example is
reproduced below:
(2) Ein Buch haben die Eltern dem Sohn gegeben.
The argument-contributing gegebengiven is, as
before, assigned the disl-v type, but is further sub-
typed and inherits the constraints also from vf-v (v-
final verb), reflecting the fact that it occurs head-
finally.
gegeben (type disl-vf-v):
{doNP ? (sNP, ioNP),
5
(sNP, doNP, ioNP) ? gegeben}
The dislocated doNP climbs up the tree ((2) in
Section 2) in the storage, which is then subject to
the constraints of matrix haben at the top node. This
argument-receiving auxiliary haben is, as before,
given the mtrx-v2-v status.8.
haben (type mtrx-v2-v):
{doNPst ? haben, (doNPst, haben)}
Thus the dislocated ein Buch is duly placed at the
left-periphery in a manner that forbids intervention
between itself and the matrix verb.
5.4 Long-Distance Dependency
Having dealt with an argument fronting of the auxil-
iary construction as a non-local case, we could now
extend the same treatment to long-distance depen-
dency. Our example is:
(3) Ein Buch, sagt ein Freund, dass er glaubt, dass
die Eltern dem Sohn geben.
(?A book, a friend says that he thinks that the
parents give the son?)
In fact, it suffices to endow exactly the same type
as gegeben, i.e. disl-vf-v, to the occurrence of geben
in a subordinate clause.9
geben (in subord. clause, type disl-vf-v):
{doNP ? (sNP, ioNP),
(sNP, doNP, ioNP) ? geben}
This ensures that the dislocated argument goes
progressively up towards the top node. To prevent
this argument from being ?dropped? the half way
through, however, the non-matrix CP-taking verbs
?in the middle? that should be bypassed, in our case
glaubt, needs to possess the constraint that pushes
the dislocated element to the left of itself:
glaubt (in subord. clause, type ?middle-v?):10
{doNPst ? glaubt}
8More precisely this also involves haben? VP(gapped)
9This means that, given the identical morphological form,
gegeben is type-ambiguous between the matrix and subordinate
occurrences. This does not add too much to parsing complexity,
however, as this ?ambiguity? is quickly resolved when one of its
argument is encountered.
10The constraints applicable to the usual finite verb is omit-
ted, i.e. sNP ? glaubt and glaubt ? CP(gapped).
Finally, a mtrx-v2-v, in our case sagt, takes care of
placing the dislocated constituent immediately be-
fore itself.
sagt (type mtrx-v2-v):11
{doNPst ? sagt, (doNPst, sagt)}
5.5 Adjunct fronting
I declared at the beginning to use the traditional bi-
nary adjunction analysis for adjunct-head phrases.12
In order to achieve this, I first propose a fundamental
conceptual shift, given the iterability and optionality
of adjuncts. In the traditional concept of adjunct-
head phrases, it is the adjunct that selects for the
head it modifies rather than the other way round.
Also semantically, the adjunct is considered the ?se-
mantic head? that works as a functor. In light of
this background, it is not implausible to take the
adjunct as the ?parsing head? equipped with word
order constraints. In fact, the opposite option ?
equipping the syntactic head with its relative word
order with adjuncts? is not as feasible in our lexi-
cal head-corner parsing. The iterability of adjuncts
means that the head would have to be equipped with
an infinite number of adjuncts as its ?arguments?,
which would lead to various uninstantiation prob-
lems. Therefore, I swap the statuses and treat, in
terms of parsing, the adjunct as a functor with word
order constraints incorporated relative to its modi-
fiee.
Thus, the word order constraints are now given
to the lexical adjuncts also. I will take as an ex-
ample adverbs.13 Adverbs are now the potential lo-
cus of word order patterns relative to its modifiee
(clause/VP), but are not given any specific constraint
in German generally, because one can appear either
after or inside a clause. Our focus is solely on the
possibility of putting one before the clause it modi-
fies, when it is subject to the V2 constraint. This is
handled simply by saying, for such a type, which we
call disl-adverb, it dislocates itself, in the manner of
11Likewise: sagt ? CP(gapped) omitted.
12That is against the temptation for a constituency change
that renders adjuncts sisters on par with arguments (cf. Bouma
et al(2001)), in which case V2 would simply fall out from the
foregoing word order types.
13The same treatment can be extended to prepositional ad-
juncts (remember the unused constraints will percolate up to
the maximal projection).
6
?head movement? which is widely used in German
syntax (Kiss and Wesche, 1991; Netter, 1992).
disl-adverb: adv (adv? dislst)
This specification ensures the adverb itself goes
onto the extraction path, to be placed at the left-
periphery, triggered by the mtrx-v2-v type. The sin-
gularity of the adverbials at the prerverbal position
is ensured by means of percolation storage control.
6 Verbal Fronting
Our last challenge concerns fronting of verb or ver-
bal projections. From the preceding discussion, an
option that suggests itself is to treat the verb fronting
as the case of verb dislocating itself. I will in-
deed propose a strategy along this line, but this av-
enue proves more difficult due to complications spe-
cific to verb-related fronting. Firstly, generally such
fronting is limited to the environment of a lower VP
governed by a higher verb such as an auxiliary, as
can be seen from the following contrast:
(4)
a. Gegeben haben die Eltern dem Sohn ein Buch.
b. *Geben, sagt ein Freund, dass die Eltern dem Sohn ein
Buch.
Second, the type we used for gegeben in Section
5.3, namely disl-vf-v, clearly does not work, as the
verb does not occur phrase-finally (but in fact ini-
tially) relative to its sisters in (4a). Some relaxation
of LP constraints seem to be in order.
Thirdly, German displays a variety of ways to
front part of a VP:
(5)
Gegeben haben die Eltern dem Sohn ein Buch.
Dem Sohn gegeben haben die Eltern ein Buch.
Ein Buch gegeben haben die Eltern dem Sohn.
Dem Sohn ein Buch gegeben haben die Eltern.
This raises the question of whether this fits in the V2
pattern at all, coupled with the ongoing debate on the
status of the preverbal string. Quite apart from the
theoretical debate, however, how best to adequately
generate these patterns is an acute parsing issue. We
are assuming the flat clause=VP anaylsis, so relax-
ing the singularity condition seems unavoidable.
Fourthly, to make the matter worse, allowing mul-
tiple frontings and dropping LP requirements does
not solve the problem, as ordering of the preverbal
constituents is constrained, as shown in the follow-
ing data:
(6)
*Gegeben dem Sohn haben die Eltern ein Buch.
*Dem Sohn gegeben ein Buch haben die Eltern.
It is a great challenge for any syntactician to pro-
vide a unified account for such complex behaviour,
and I confine myself here to offering the ?solution?
sets of constraints that adequately generate the de-
sired string. What I offer is this: allowing multiple
dislocations only for the verbal fronting cases via a
new word order subtype, while retaining the verb-
final LP conditions for these dislocated constituents.
For this new type we first relax the singularity
condition for dislocation. To allow multiple dislo-
cations, it would suffice to drop the |disl| = 1 condi-
tion, but an unrestricted application of disl ? args
would lead to overgeneration, due to two further
constraints applicable: (1) not all arguments can and
(2) the subject argument cannot be fronted along
with the verb (as in (a) and (b) below, respectively):
(7)
a. *Die Eltern dem Sohn ein Buch gegeben haben.
b. *Die Eltern gegeben haben dem Sohn ein Buch.
*Die Eltern ein Buch gegeben haben dem Sohn.
Therefore we add the conditions to exlude the above,
along with the the verb-final constraint applicable
the dislocated constituents to exclude (6). Let us call
this type frontable-v. The constraint specification is
as follows:
gegeben (frontable-v):
disl = {gegeben} ? ptargs, ptargs ? gegeben
where ptargs ? args and sNP /? ptargs
The proposed constraint set might strike as rather
ad hoc. It would clearly be better to treat both the
fronted and non-fronted occurrences of gegeben as
sharing some common word order type, and what is
meant by ?applying the constraints amongst the dis-
located constituents? needs to be fleshed out. Thus
this may not be an elegant solution, but nevertheless
is an generatively adequate solution. More impor-
tantly it serves as a good example for the flexibility
7
and adaptability of constrained free word order pars-
ing, because it handles a rather complex word order
pattern in a way neutral to grammatical construal,
i.e. without invoking constituency manipulation.
7 Concluding Remarks
I conclude this paper by responding to a natural ob-
jection: why would one have to go through this con-
voluted route of lexical word order control, when
the ?natural? way to constrain V2 ?or V1 and VF,
for that matter? would be to have some ?global?
patterns pertinent to clause types? My responses
are primarily engineering-oriented. First, lexicalised
encoding gives the parser, through locality restric-
tion, a certain control over computational complex-
ity, as the search space for constraint enforcement is
restricted.14 However this not an entirely unique, if
more amenable, feature to lexicalised parsing, as one
could impose such a control in non-lexicalised pars-
ing. The advantage truly unique to lexicalising word
order lies in rendering the parser and grammar in-
dependent of surface realisation and hence re-usable
across languages. In short, it promotes modularity.
As we have seen, though the parser needs to con-
form to a certain strategy, the word order component
is fairly independent, as a separate procedure which
can be modified if for example more types of word
order operators are needed. The grammar could also
be kept more compact and cross-linguistically appli-
cable, because word order is abstracted away from
constituency. Therefore, paradoxically, an advan-
tage of lexicalising German parsing is to enable the
same parser/grammar to be used in other languages
too, even if it is not naturally suited to the language.
References
Gosse Bouma, Robert Malouf, and Ivan Sag. 2001. Sat-
isfying constraints on extraction and adjunction. Nat-
ural Language and Linguistic Theory, 19(1).
Mike Daniels. 2005. Generalized ID/LP Grammar.
Ph.D. thesis, Ohio State University.
Jay Earley. 1970. An efficient context free parsing algo-
rithm. Communications of ACM, 13:94?102.
Gerald Gazdar and Chris Mellish. 1989. Natural Lan-
guage Processing in Prolog. Addison Wesley.
14For a complexity analysis of such grammar, see Sato (2008)
and Suhre (1999).
Mark Johnson. 1985. Parsing with discontinuous con-
stituents. In Proceedings of the 23rd Annual Meeting
of the ACL, pages 127?132.
Andreas Kathol. 2000. Linear Syntax. OUP.
Tibor Kiss and B Wesche. 1991. Verb order and head
movement. In O Herzog, editor, Text Understanding
in LILOG, pages 216?40. Springer.
Stefan Mu?ller. 2004. Continuous or discontinuous con-
stituents? a comparison between syntactic analyses for
constituent order and their processing systems. Re-
search on Language and Computation 2(2).
Klaus Netter. 1992. On non-head non-movement. An
HPSG treatment of finite verb position in German.
In G. Go?rz, editor, Proceedings of KONVENS 92.
Springer.
Gerald Penn. 1999. Linearization and Wh-extraction in
HPSG: Evidence from Serbo-Croatian. In R. Borsely
and A. Przepiorkowski, editors, Slavic in HPSG.
CSLI.
Carl Pollard and Ivan Sag. 1987. Information-Based
Syntax and Semantics. CSLI.
Mike Reape. 1991. Parsing bounded discontinuous con-
stituents: Generalisation of some common algorithms.
DIANA Report, Edinburgh University.
Mike Reape. 1993. A Formal Theory of Word Order.
Ph.D. thesis, Edinburgh University.
Ivan Sag. 2007. Remarks on locality. In Stefan Mu?ller,
editor, Proceedings of HPSG07. CSLI.
Yo Sato. 2006. A proposed lexicalised linearisation
grammar: a monostratal alternative. In Stefan Mu?ller,
editor, Proceedings of HPSG06. CSLI.
Yo Sato. 2008. Implementing Head-Driven Linearisa-
tion Grammar. Ph.D. thesis, King?s College London.
Oliver Suhre. 1999. Computational Aspects of a Gram-
mar Formalism for Languages with Freer Word Order.
Diplomarbeit, Eberhard-Karls-Universita?t Tu?bingen.
Gertjan van Noord. 1991. Head corner parsing for dis-
continuous constituency. In Proceedings of the 29th
annual meeting on ACL, pages 114?121.
8
Proceedings of the 8th International Conference on Computational Semantics, pages 128?139,
Tilburg, January 2009. c?2009 International Conference on Computational Semantics
Dialogue Modelling and the Remit
of Core Grammar
Eleni Gregoromichelaki
?
, Yo Sato
?
, Ruth Kempson
?
Andrew Gargett
?
, Christine Howes
?
?
King?s College London,
?
Queen Mary University of London
1 Introduction
In confronting the challenge of providing formal models of dialogue, with
its plethora of fragments and rich variation in modes of context-dependent
construal, it might seem that linguists face two types of methodological
choice: either (a) conversation employs dialogue-specific mechanisms, for
which a grammar specific to such activity must be constructed; or (b) vari-
ation arises due to independent parsing/production systems which invoke
a process-neutral grammar. However, as dialogue research continues to de-
velop, there are intermediate possibilities, and in this paper we discuss the
approach developed within Dynamic Syntax (DS, Kempson et al 2001,
Cann et al 2005), a grammar framework within which, not only the parser,
but indeed ?syntax? itself are just a single mechanism allowing the pro-
gressive construction of semantic representations in context. Here we take
as a case study the set of phenomena classifiable as clarifications, reformu-
lations, fragment requests and corrections accompanied by extensions, and
argue that though these may seem to be uniquely constitutive of dialogue,
they are grounded in the mechanisms of apposition equivalently usable in
monologue for presenting reformulations, extensions, self-corrections etc.
2 Background
The data we focus on are non-repetitive fragment forms of acknowledge-
ments, clarifications and corrections (henceforth, A female, B male):
(1) A: Bob left.
B: (Yeah,) the accounts guy.
(2)
A: They X-rayed me, and took a urine sample, took a blood sample.
Er, the doctor
B: Chorlton?
A: Chorlton, mhm, he examined me, erm, he, he said now they
were on about a slight [shadow] on my heart.
128
(3) A: Bob left.
B: Rob?
A: (No,) (Bob,) the accounts guy.
Even though in the literature the fragments in (2)-(3) might be characterised
as illustrating distinct construction-types, in our view, they all illustrate how
speakers and hearers may contribute, in some sense to be made precise, to
the joint enterprise of establishing some shared communicative content, in
what might be loosely called split utterances. Even (1), an acknowledgement,
can be seen this way upon analysis: B?s addition is similar in structure to
an afterthought extension that might have been added by A herself to A?s
fully sentential utterance. It can be seen in (2) that such joint construction
of content can proceed incrementally: the clarification request in the form of
a reformulation is provided by B and resolved by A within the construction
of a single proposition. In (3) the fragment reply can be taken to involve
correction, in the sense that, according to the DS analysis of B?s fragment
question, he has provided content construable as equivalent to that derived
by processing Rob left? (see Kempson et al (2007)). Nevertheless such
corrections can also incorporate extensions in the above sense, enabling a
single conjoined propositional content to be derived in a single step.
It might seem that such illustration of diversity of fragment usage is am-
ple evidence of the need for conversation-specific rules. Indeed, Ferna?ndez
(2006) presents a thorough taxonomy, as well as detailed formal modelling
of Non-Sentential Utterances (NSUs), referring to contributions such as (1)
as repeated acknowledgements involving reformulation. Ferna?ndez models
such constructions via type-specific ?accommodation rules? which make a
constituent of the antecedent utterance ?topical?. The semantic effect of
acknowledgement is then derived by applying an appropriately defined ut-
terance type for such fragments to the newly constructed context. A distinct
form of contextual accommodation is employed to model so-called helpful
rejection fragments, as in (3) (without the reformulation), whereby a wh-
question is accommodated in the context by abstracting over the content
of one of the sub-constituents of the previous utterance. The content of
the correction is derived by applying this wh-question in the context to the
content of the fragment (see also Schlangen (2003) for another classification
and analysis).
In contrast, the alternative explored here is whether phenomena such
as (1)-(2), both of which are non-repetitive next-speaker contributions, can
be handled uniformly using the mechanisms for structure-building made
available in the core grammar, without recourse to construction-specific ex-
tensions of that grammar and contextual accommodation rules. This is
because, in our view, the range of interpretations these fragments receive
in actual dialogue seem to form continua with no well-defined boundaries
and mixing of functions (see also comments in Schlangen (2003)). Thus we
129
propose that the grammar itself simply provides mechanisms for process-
ing/integrating such fragments in the current structure while their precise
contribution to the interaction can be calculated by pragmatic inferencing
if needed (as in e.g. Schlangen (2003)) or, as seems most often to be the
case, be left underspecified without disruption to the dialogue.
One bonus of the stance taken here is the promise it offers for elucidating
the grammar-parser contribution to the disambiguation task. Part of the
challenge of modelling dialogue is the apparent multiplicity of interpretive
and structural options opened up during processing by the recurrent, of-
ten overlapping fragments as seen in (2) above. Thus, it might seem that
the rich array of elliptical fragments available in dialogue adds to its com-
plexity. However, an alternative point of view is to see such phenomena as
providing a window on how interlocutors exploit the incrementality afforded
by the grammar. The reliance of fragments on context for interpretation,
when employed incrementally, enables the hearer to immediately respond to
a previous utterance at any relevant point, in a constrained manner, with-
out ?recovering? a propositional unit. Three features of the Dynamic Syntax
model of dialogue (Purver et al (2006)), presented below, provide the flex-
ible control required for such processing: (a) word-by-word incrementality
(b) interaction with contextually provided information at every step of the
construction process (c) tight coordination of parsing and production.
3 Dynamic Syntax: A Sketch
Dynamic Syntax (DS ) is a parsing-based framework, involving strictly se-
quential interpretation of linguistic strings. The model is implemented via
goal-directed growth of tree structures and their annotations formalised us-
ing LOFT (Blackburn and Meyer-Viol (1994)), with modal operators ???, ???
to define concepts of mother and daughter, and their iterated counterparts,
??
?
?, ??
?
?, to define the notions be dominated by and dominate. Under-
specification and update are core aspects of the grammar itself and involve
strictly monotonic information growth for any dimension of tree structures
and annotations. Underspecification is employed at all levels of tree rela-
tions (mother, daughter etc.), as well as formulae and type values, each
having an associated requirement that drives the goal-directed process of
update. For example, an underspecified subject node of a tree may have a
requirement expressed in DS with the node annotation ?Ty(e), for which
the only legitimate updates are logical expressions of type entity (Ty(e));
but requirements may also take a modal form, e.g. ????Ty(e ? t), a con-
straint that the mother node be annotated with a formula of predicate type.
Requirements are essential to the dynamics informing the DS account: all
requirements must be satisfied if the construction process is to lead to a
successful outcome.
130
Semantic structure is built from lexical and general computational ac-
tions. Computational actions govern general tree-constructional processes,
such as introducing and updating nodes, as well as compiling interpretation
for all non-terminal nodes in the tree. Construction of only weakly spec-
ified tree relations (unfixed nodes) can also be induced, characterised only
as dominance by some current node, with subsequent update required. In-
dividual lexical items also provide procedures for building structure in the
form of lexical actions, inducing both nodes and annotations. Thus partial
trees grow incrementally, driven by procedures associated with particular
words as they are encountered, with a pointer, ?, recording the parser?s
progress (unlike van Leusen and Muskens (2003), partial trees are part of
the model and, unlike in other frameworks, incrementality is word-by-word
rather than sentence-by-sentence).
Complete individual trees are taken to correspond to predicate-argument
structures (with an event term associated with tense, suppressed in this
paper). The epsilon calculus (see e.g. Meyer-Viol (1995)) provides the se-
mantic representation language. Complex structures are obtained via a gen-
eral tree-adjunction operation licensing the construction of so-called linked
trees, hosting information that is eventually transferred onto the tree from
which the link is made (Kempson et al2001). Structures projected as such
paired trees range over restrictive relatives, nonrestrictive relatives, condi-
tionals, topic structures and appositions as here. As the semantic represen-
tations employ the epsilon calculus, eventual compound epsilon terms (e.g.
?, x, P (x)) are constructed incrementally through link-adjunction:
(4) A consultant, a friend of Jo?s, is retiring
Ty(t),Retire
?
((?, x, Consultant
?
(x) ? Friend
?
(Jo
?
)(x)))
Ty(e), (?, x, Consultant
?
(x) ? Friend
?
(Jo
?
)(x)) Ty(e? t), Retire
?
Ty(e), (?, x, Friend
?
(Jo
?
)(x))
Ty(cn), (x,Friend
?
(Jo
?
)(x))
x Friend
?
(Jo
?
)
Jo
?
Friend
?
Ty(cn? e), ?P.?, P
Underspecification of content as well as structure are central to facilitat-
ing successful linguistic interaction, our primary concern here. Pronouns,
the prototypical case, contribute a place-holding metavariable, noted as e.g.
U, plus an associated requirement for update by an appropriate term value:
??x.Fo(x). Equally, definite NPs contribute place-holders plus a constraint
providing a restriction/?presupposition? on the kind of entity picked out,
e.g., the man contributes the annotation U
Man
?
(U)
, T y(e). The subscript
specification is shorthand for a transition to a linked tree whose root node is
131
annotated with a formula Man
?
(U)
1
. The update of metavariables can be
accomplished if the context contains an appropriate term for substitution:
context involves storage of parse states, i.e., storing of partial tree, word se-
quence to date, plus the actions used in building up the partial tree (Purver
et al2006).
Scope dependencies between constructed terms or the index of evalua-
tion (e.g. S) are defined on completed propositional formulae, relative to
incrementally collected scope constraints (of the form x < y for constructed
terms containing variables x and y respectively). Constraints reflect on-line
processing considerations modulo over-riding lexical stipulations. For ex-
ample, proper names contribute as iota terms, i.e, epsilon terms reflecting
uniqueness in the context, ?, x,Bob
?
(x), and these project a scope depen-
dency solely on the index of evaluation reflecting their widest scope property
(cf Kamp and Reyle 1994). The structure projected from A?s utterance in
(1) is thus (5) (note that trees are the result of processing words but do
not encode the structure of strings, word order etc., only semantic content
derived in interaction with context, thus are the equivalents of DRSs):
(5)
S < x Ty(t),Leave
?
((?, x,Bob
?
(x)))
Ty(e), (?, x,Bob
?
(x)) Ty(e? t), Leave
?
The scope evaluation rule reflects the predicate-logic/epsilon-calculus equiv-
alence ?xF (x) ? F (?, x, F (x)) so evaluated terms eventually reflect their
containing structure. Hence, evaluation of (5) yields:
(6) Ty(t), Leave
?
(?, x,Bob
?
(x) ? Leave
?
(x))
A major aspect of the DS dialogue model is that both generation and
parsing are goal-directed and incremental, and hence are governed by es-
sentially the same mechanism. Under this model, a human hearer-parser
builds a succession of partial parse trees based on what (s)he has heard
thus far. Importantly, however, unlike the conventional bottom-up parsing,
the DS model assumes a strong predictive element in parsing: a hearer is
assumed to entertain some goal to be reached eventually at any stage of
parsing. In (1), for example, as soon as the hearer encounters Bob, an un-
derspecified propositional tree is constructed, as in the first simplified and
schematised tree in Figure 1. Then the tree ?grows? monotonically, i.e. such
that at each word input, it is ?updated? to an ?incremented? tree that is
subsumed by the original tree, as depicted in the same Figure. This can be
described as a process of specifying the relevant nodes towards a complete
tree. This predictive element in DS allows a speaker-generator to be mod-
elled as doing exactly the same, i.e. going through monotonically updated
partial trees, the only difference being that (s)he also has available a more
1
These linked structures are suppressed in all diagrams.
132
fully specified goal tree representing what (s)he wishes to say, corresponding
to the rightmost tree in the Figure (with ?0? in the ?generation? row at the
bottom indicating it is entertained before utterance). Each licensed step in
generation, i.e. the utterance of a word, is governed by whatever step is
licensed by the parsing formalism, constrained via a required subsumption
relation of the goal tree. By updating their growing ?parse? tree relative
to the goal tree, speakers are licensed to produce the associated natural
language string.
Parsing: 1 2 3
?Ty(t)
q
q
q
q
q
q
q
M
M
M
M
M
M
M
Ty(e),
Bob
?
,?
?Ty(e ? t)
7?
?Ty(t)
q
q
q
q
q
q
q
M
M
M
M
M
M
M
Ty(e),
Bob?
Ty(e ? t),
Leave
?
,?
7?
Ty(t),
Leave?(Bob?),?
q
q
q
q
q
q
q
M
M
M
M
M
M
M
Ty(e),
Bob?
Ty(e ? t),
Leave?
Generation: 1 2 0,3
Figure 1: Parallel parsing and generation in DS
This architecture allows for a dialogue model in which generation (what
a speaker does) and parsing (what a hearer does) function in parallel. The
speaker goes through partial trees subsuming a specified goal tree, while
the hearer attempts to ?mirror? the same series of partial trees, albeit not
knowing what the content of the unspecified nodes will be. For the dialogues
in (1)-(3), therefore, B as the hearer will have the partial representation of
what he has successfully parsed, required also for generation. This provides
him with the ability at any stage to become the speaker, interrupting to
ask for clarification, reformulating, or providing a correction, as and when
necessary. As we shall see, B?s parse tree reveals where need of clarifica-
tion or miscommunication occurs, as it will be at that node from which a
sub-routine extending it takes place
2
. According to our model of dialogue,
repeating or extending a constituent of A?s utterance by B is licensed only
if B, the hearer of A turned now a speaker, entertains currently a goal tree
that matches or extends the parse tree of what he has heard in a monotonic
fashion, although he only utters the relevant subpart of A?s utterance. In-
deed, this update is what B is seeking to clarify, correct or acknowledge. In
DS, B can reuse the already constructed (partial) parse tree in his context,
rather than having to rebuild an entire propositional tree or subtree
3
.
2
The account extends the implementation reported in Purver et al (2006)
3
Given the DS concept of linked trees projecting propositional content, we anticipate
that this mechanism will be extendable to fragment construal involving inference (see e.g.
Schlangen (2003), Schlangen and Lascarides (2003))
133
4 NSU fragments in Dynamic Syntax
4.1 Non-repetitive Acknowledgement
From a DS perspective, phenomena like reformulations as in (1), or exten-
sions to what one understands of the other speaker?s utterance, (2), can
be handled with exactly the same mechanisms as the sentence-internal phe-
nomenon independently identifiable as apposition, as in (4), and equally
usable by a single individual as a means of incrementally reformulating, cor-
recting or extending what they have just uttered. The update rule for such
structures, applicable to all terms, takes the two type e terms so formed and
yields a new term whose compound content is a combination of both.
We now have the basis for analysing extensions potentially functioning
as acknowledgements which build on what has been previously said as a way
of confirming the previous utterance. Recall (1), (2). There are two ways for
fragments which reformulate an interlocutor A?s utterance to occur: (a) as
interruptions of A?s utterance with immediate confirmation of identification
of the individual concerned, see (2); (b) as confirmations/extensions of A?s
utterance after the whole of her utterance has been integrated, see (1). Both
are modelled by DS as incremental additions.
Turning to (1), B?s response (Yeah,) the accounts guy constitutes a re-
formulation of A?s utterance and an extension of A?s referring expression,
yielding a similar content as that of an appositive expression Bob, the ac-
counts guy in this case jointly constructed. B?s reformulation/extension
counts in effect as an acknowledgement in virtue of signalling successful
processing of A?s utterance without objection raised. Thus there is no need
for a separate grammatical mechanism to process these structures. In DS
terms, after processing A?s utterance, B?s context consists of the following
tree:
(7) B?s Context for producing ?Yeah?
Ty(t), Leave
?
(?, x,Bob
?
(x) ? Leave
?
(x)),?
(?, x,Bob
?
(x)) Leave
?
B, as a speaker, can now re-use this representation as point of departure
for generating the expression the accounts guy. In this case his goal tree,
the message to be expressed, will now be annotated with a composite term
made up from both the term recovered from parsing A?s utterance and the
new addition. This requires attaching a linked tree to the correct node
and an appropriate update of the context tree (for reasons of space, the
exact structure of the linked tree is condensed below, with subscripting as
shorthand):
134
(8) B?s goal tree:
Ty(t), Leave
?
((?, x,Bob
?
(x) ?Acc.guy
?
(x))) ? Leave
?
(x))
(?, x,Bob
?
(x) ?Acc.guy
?
(x))
Leave
?
(?, x,Bob
?
(x))
Acc.guy
?
(x)
In order to license generation of the expression the accounts guy, B now
needs to verify that processing these words in the context provided by the
tree in (7) will produce a tree that matches this goal tree in (8). To achieve
this, starting from (7), a series of simulated ?parse? trees are generated
which indeed result in the requisite matching. Steps include shifting the
pointer to the appropriate node, projection of a linked tree from that node
and test-processing the words the accounts guy, each step checking against
the goal tree that a subsumption relation between the current ?parse? tree
and the goal tree is always maintained:
(9) B?s parse tree licensing production of the accounts guy: link adjunction
Ty(t), Leave
?
(?, x,Bob
?
(x) ? Leave
?
(x))
(?, x,Bob
?
(x)) Leave
?
U
Acc.guy
?
(U)
,?
The only way to update this representation relative to both the restriction
on the metavariable and monotonicity of growth on any one node in a tree
involves replacing the metavariable with (?, x,Bob
?
(x)), as this is commen-
surate with an extension of the term annotating the node from which the
link transition was constructed:
(10) Updating B?s parse tree licensing production of the accounts guy
Ty(t), Leave
?
(?, x,Bob
?
(x) ?Acc.guy
?
(x) ? Leave
?
(x)),?
(?, x,Bob
?
(x) ?Acc.guy
?
(x)) Leave
?
(?, x,Bob
?
(x))
Acc.guy
?
(x)
Finally, the information is passed up to the top node of the main tree,
completing the parse tree to match B?s goal tree, (8), thus licensing the
utterance of the expression the accounts guy.
4.2 Non-repetitive Clarification
In the acknowledgement case above, the proposition relative to which the
linked structure is built is completed (with an already extended epsilon
term); but the same mechanism can be used when the interlocutor needs
clarification, prior to any such completion of the tree. In (2), B again, as
the speaker, takes as his goal tree a tree annotated with an expansion of the
term constructed from parsing A?s utterance but nevertheless picking out
the same individual. Using the very same mechanism as in (1) of building a
linked structure, B, interrupting A, provides a distinct expression, the name
Chorlton, this time before he has completed the parse tree for A?s utterance.
All that has been achieved at this point is the definite?s contribution of a
meta-variable with the restriction that the individual picked out must be a
doctor:
135
(11) A/B?s parse tree for Chorlton:
?Ty(t)
U
Doctor
?
(U)
,?
?Ty(e ? t)
(?, x, Chorlton
?
(x))
As in the acknowledgement case, but this time at the node initiating the
link transition, the only possible value to provide for the metavariable U
compatible both with its restriction and the monotoniticity constraint is
the composite term (?, x,Doctor
?
(x) ? Chorlton
?
(x)). The mechanism of
constructing paired structures involving type e terms across linked trees
is identical to that employed in B?s utterance in (1), though to a rather
different effect at this intermediate stage in the interpretation process. This
extension of the term is confirmed by A, this time replicating the composite
term which processing B?s utterance has led to. The eventual effect of the
process of inducing linked structures to be annotated by coreferential type e
terms may thus vary across monologue and different dialogue applications,
yielding different interpretations, but the mechanism is the same.
4.3 Correction
It might be argued nonetheless that correction is intrinsically a dialogue
phenomenon. Consider (3), for example. One of the possible interpretations
of (3), according to the DS analysis, is that B has offered the equivalent of the
content derived by processing Rob left?. That is, let?s assume here that B has
misheard and requests confirmation of what he has perceived A as saying.
A in turn rejects B?s understanding of her utterance and provides more
information. Presuming rejection as simple disagreement (i.e. the utterance
has been understood, but judged as incorrect), in DS terms, this means that
A has in mind a goal tree that licensed what she had produced, which is
distinct from the one derived by processing B?s clarification. As shown in
Kempson et al (2007), this means that A has been unable to process B?s
clarification request as an extension of her own context. Instead, she has to
parse the clarification by exploiting the potential for introducing an initially
structurally underspecified tree-node to accommodate the contribution of
the word Rob. Subsequently, by re-running the actions stored in context
previously by processing her own utterance of the word left, she is able to
complete the integration of the fragment in a new propositional structure.
Now, in order for A to produce the following correction, what is required
is for A to establish as the current most recent representation in context her
original goal tree. This can be monotonically achieved by recovering and
copying this original goal tree to serve as the current most immediate con-
text
4
. An option available to A at this point is to introduce, in addition
or exclusively, a reformulation of her original utterance in order to facilitate
4
Mistaken representations must be maintained in the context as they can provide an-
tecedents for subsequent anaphoric expressions.
136
identification of the named individual which proved problematic for B previ-
ously. She can answer B?s utterance of Rob? with (No,) Bob, the accounts
guy, as in (3) or simply with (No,) the accounts guy. Both are licensed
by the DS parsing mechanism without more ado. For both, the goal tree
will be as follows and it will always be the point of reference for checking
the subsumption relation relative to the simulated parsing steps described
further below:
(12) A?s goal tree
Ty(t), Leave
?
((?, x,Bob
?
(x) ?Acc.guy
?
(x)))
(?, x,Bob
?
(x) ?Acc.guy
?
(x)) Leave
?
(?, x,Bob
?
(x))
Acc.guy
?
(x)
Under these circumstances, given the DS grammar-as-parser perspective,
several strategies are now available for the licensing of generation of the
fragment. A is licensed to repeat the name Bob by locally extending the
node in the context tree where the representation of the individual referred to
is located by using the rule of late*adjunction, a process which involves
building a node of type e from a dominating node of that type (illustrated in
Kempson et al 2007). An alternative way of licensing repetition of the word
Bob is to employ one of the strategies generally available for the parsing of
long distance dependencies i.e. constructing initial tree nodes as unfixed
(*adjunction). We show here how the latter strategy can be exploited to
license the production of the fragment by A.
(13) Parsing simulation licensing generation of Bob, the accounts guy
Step 1: *Adjunction Step 2: LINK-Adjuction + testing the accounts guy
?Ty(t)
(?, x,Bob
?
(x)),?
?Ty(t)
(?, x,Bob
?
(x)),?
U
Acc.guy
?
(U)
The only way to develop the constructed tree at Step 2 commensurate with
the goal tree (12) is to identify the value of U as (?, x,Bob
?
(x)), so this
is what is entered at the newly constructed linked tree, duly leading to
extension of the term originally given as annotating the unfixed node as
(?, x,Bob
?
(x) ? Acc.guy
?
(x)). The structure
5
derived by processing such an
extension is exactly that of (1) above (compare goal tree in (12) above
and tree in (8)). Now, as mentioned before, context, as defined in DS,
keeps track not only of tree representations and words but also of actions
contributed by the words and utilised in building up the tree representations.
Here, according to DS, production of the correction in (3) is licensed to be
5
Again note that DS trees represent derived content rather than structure over natural
language strings.
137
fragmental only because the original actions for parsing/producing the word
left are available in the context and can be recalled to complete the structure
initiated by processing/producing the name Bob. Now these stored actions
can be retrieved to develop the tree further:
(14) Parsing simulation licensing generation of Bob, the accounts guy
Step 3: test-processing stored actions for left
?Ty(t)
(?, x,Bob
?
(x) ?Acc.guy
?
(x)) ?Ty(e),? Leave?
(?, x,Bob
?
(x))
Acc.guy
?
(?,x,Bob
?
(x))
With this partial tree being commensurate with the goal tree, all actions that
follow are general computational processes for completing the tree: unifying
the unfixed node to determine the subject argument, applying the subject
to the predicate, evaluating the quantified terms. Nothing specific to this
structure is needed. Indeed, all these mechanisms are equally applicable by
an individual speaker, perhaps more familiar as right dislocation phenomena,
but equally available incrementally:
(15) Bob left, (Bob) the accounts guy.
5 Conclusion
As these fragments and their construal have demonstrated, despite serving
distinct functions in dialogue, the mechanisms which make such diversity
possible are general strategies for tree growth. In all cases, the advantage
which use of fragments provides is a ?least effort? means of re-employing
previous content/structure/actions which constitute the minimal local con-
text. As modelled in DS, it is more economical to reuse information from
this local context rather than constructing representations afresh (via costly
processes of lexical retrieval, choice of alternative parsing strategies, etc.).
A further quandary in dialogue construal is that, despite such avenues
for economising their efforts, interlocutors are nevertheless faced with an
increasing set of interpretive options at any point during the construction
of representations. One strategy available to hearers is to delay a disam-
biguating move until further input potentially resolves the uncertainty. How-
ever, as further input is processed and parsing/interpretive options increase
rapidly, the human processor struggles. The incremental definition of the
DS formalism allows for the modelling of an alternative strategy available
to hearers: at any (sub-sentential) point they could opt to intervene imme-
diately, and make a direct appeal to the speaker for more information as
illustrated by the clause-medial fragment interruption (2). It seems clear
that the grammar should allow the resources for modelling this behaviour
without any complications.
138
The phenomena examined here are also cases where speakers? and hear-
ers? representations, despite attempts at coordination, may nevertheless sep-
arate sufficiently for them to have to seek ?repair? (see especially (3)). In
the model presented here, the dynamics of interaction allow fully incremen-
tal generation and integration of fragmental utterances so that interlocutors
can be taken to constantly provide optimal evidence of each other?s represen-
tations with necessary adjuncts being able to be incrementally introduced.
But such mechanisms apply equally within an individual utterance, with self-
correction, extension, elaboration, repetition etc. The effect is that all the de-
vices which seem so characteristic of dialogue involve mechanisms invariably
available within an individual?s core grammar. This suggests a new inverse
methodology: it is the challenge of modelling dialogue that can be used as
a point of departure for modelling grammars for individual speakers, rather
than the other, more familiar, way round (see also Ginzburg (forthcmg)).
This reversibility is, notably, straightforwardly available to grammar for-
malisms in which the incremental dynamics of information growth is the
core structural concept because emergent dialogue structure crucially ex-
hibits and interpretively relies on such incrementality.
Acknowledgements
This work was supported by grants ESRC RES-062-23-0962 and Leverhulme F07-
04OU. We are grateful for comments to: Robin Cooper, Alex Davies, Arash Eshghi,
Jonathan Ginzburg, Pat Healey, Greg Mills. Normal disclaimers apply.
References
Patrick Blackburn and Wilfried Meyer-Viol. Linguistics, logic and finite trees.
Bulletin of the IGPL, 2:3?31, 1994.
Raquel Ferna?ndez. Non-Sentential Utterances in Dialogue: Classification, Resolu-
tion and Use. PhD thesis, King?s College London, University of London, 2006.
Jonathan Ginzburg. Semantics for Conversation. CSLI, forthcmg.
Ruth Kempson, Andrew Gargett, and Eleni Gregoromichelaki. Clarification re-
quests: An incremental account. In Proceedings of the 11th Workshop on the
Semantics and Pragmatics of Dialogue (DECALOG), 2007.
Wilfried Meyer-Viol. Instantial Logic. PhD thesis, University of Utrecht, 1995.
Matthew Purver, Ronnie Cann, and Ruth Kempson. Grammars as parsers: Meeting
the dialogue challenge. Research on Language and Computation, 4(2-3):289?326,
2006.
David Schlangen. A Coherence-Based Approach to the Interpretation of Non-
Sentential Utterances in Dialogue. PhD thesis, University of Edinburgh, 2003.
David Schlangen and Alex Lascarides. The interpretation of non-sentential utter-
ances in dialogue. In Proceedings of the 4th SIGdial Workshop on Discourse and
Dialogue, pages 62?71, Sapporo, Japan, July 2003. Association for Computa-
tional Linguistics.
Noor van Leusen and Reinhard Muskens. Construction by description in discourse
representation. In J. Peregrin, editor, Meaning: The Dynamic Turn, chapter 12,
pages 33?65. 2003.
139
Lexicalising Word Order Constraints for
Implemented Linearisation Grammar
Yo Sato
Department of Computer Science
King?s College London
yo.sato@kcl.ac.uk
Abstract
This paper presents a way in which a lex-
icalised HPSG grammar can handle word
order constraints in a computational pars-
ing system, without invoking an additional
layer of representation for word order,
such as Reape?s Word Order Domain. The
key proposal is to incorporate into lexi-
cal heads the WOC (Word Order Con-
straints) feature, which is used to constrain
the word order of its projection. We also
overview our parsing algorithm.
1 Introduction
It is a while since the linearisation technique was
introduced into HPSG by Reape (1993; 1994) as
a way to overcome the inadequacy of the con-
ventional phrase structure rule based grammars in
handling ?freer? word order of languages such as
German and Japanese. In parallel in computa-
tional linguistics, it has long been proposed that
more flexible parsing techniques may be required
to adequately handle such languages, but hitherto
a practical system using linearisation has eluded
large-scale implementation. There are at least two
obstacles: its higher computational cost accom-
panied with non-CFG algorithms it requires, and
the difficulty to state word order information suc-
cinctly in a grammar that works well with a non-
CFG parsing engine.
In a recent development, the ?cost? issue has
been tackled by Daniels and Meurers (2004), who
propose to narrow down on search space while us-
ing a non-CFG algorithm. The underlying princi-
ple is to give priority to the full generative capac-
ity, let the parser overgenerate at default but re-
strict generation for efficiency thereafter. While
sharing this principle, I will attempt to further
streamline the computation of linearisation, focus-
ing mainly on the issue of grammar formalism.
Specifically, I would like to show that the lex-
icalisation of word order constraints is possible
with some conservative modifications to the stan-
dard HPSG (Pollard and Sag, 1987; Pollard and
Sag, 1994). This will have the benefit of making
the representation of linearisation grammar sim-
pler and more parsing friendly than Reape?s influ-
ential Word Order Domain theory.
In what follows, after justifying the need for
non-CFG parsing and reviewing Reape?s theory, I
will propose to introduce into HPSG the Word Or-
der Constraint (WOC) feature for lexical heads. I
will then describe the parsing algorithm that refers
to this feature to constrain the search for efficiency.
1.1 Limitation of CFG Parsing
One of the main obstacles for CFG parsing is
the discontinuity in natural languages caused by
?interleaving? of elements from different phrases
(Shieber, 1985). Although there are well-known
syntactic techniques to enhance CFG as in GPSG
(Gazdar et al, 1985), there remain constructions
that show ?genuine? discontinuity of the kind that
cannot be properly dealt with by CFG.
Such ?difficult? discontinuity typically occurs
when it is combined with scrambling ? another
symptomatic phenomenon of free word order lan-
guages ? of a verb?s complements. The follow-
ing is an example from German, where scrambling
and discontinuity co-occur in what is called ?inco-
herent? object control verb construction.
(1) Ich glaube, dass der Fritz dem Frank
I believe Comp Fritz(Nom) Frank(Dat)
das Buch zu lesen erlaubt.
the book(Acc) to read allow
?I think that Fritz allows Frank to read the book?
23
(1?) Ich glaube, dass der Fritz [das Buch] dem Frank
[zu lesen] erlaubt
Ich glaube, dass dem Frank [das Buch] der Fritz
[zu lesen] erlaubt
Ich glaube, dass [das Buch] dem Frank der Fritz
[zu lesen] erlaubt
...
Here (1) is in the ?canonical? word order while the
examples in (1?) are its scrambled variants. In
the traditional ?bi-clausal? analysis according to
which the object control verb subcategorises for
a zu-infinitival VP complement as well as nomi-
nal complements, this embedded VP, das Buch zu
lesen, becomes discontinuous in the latter exam-
ples (in square brackets).
One CFG response is to use ?mono-clausal?
analysis or argument composition(Hinrichs and
Nakazawa, 1990), according to which the higher
verb and lower verb (in the above example er-
lauben and zu lesen) are combined to form a sin-
gle verbal complex, which in turn subcategorises
for nominal complements (das Buch, der Fritz and
dem Frank). Under this treatment both the ver-
bal complex and the sequence of complements are
rendered continuous, rendering all the above ex-
amples CFG-parseable.
However, this does not quite save the CFG
parseability, in the face of the fact that you could
extrapose the lower V + NP, as in the following.
(2) Ich glaube, dass der Fritz dem Frank [erlaubt], das
Buch [zu lesen].
Now we have a discontinuity of ?verbal complex?
instead of complements (the now discontinuous
verbal complex is marked with square brackets).
Thus either way, some discontinuity is inevitable.
Such discontinuity is by no means a marginal
phenomenon limited to German. Parallel phenom-
ena are observed in the object control verbs in
Korean and Japanese ((Sato, 2004) for examples).
These languages also show a variety of ?genuine?
discontinuity of other sorts, which do not lend
itself to a straightforward CFG parsing (Yatabe,
1996). The CFG-recalcitrant constructions exist in
abundance, pointing to an acute need for non-CFG
parsing.
1.2 Reape?s Word Order Domain
The most influential proposal to accommodate
such discontinuity/scrambling in HPSG is Reape?s
Word Order Domain, or DOM, a feature that con-
stitutes an additional layer separate from the dom-
inance structure of phrases (Reape, 1993; Reape,
1994). DOM encodes the phonologically realised
(?linearised?) list of signs: the daughter signs of a
?
?
?
?
?
?
?
?
?
?
?
?
phrase
DOM
?
1 ? 2 ? 3 ?...? n
?
HD-DTR
?
[
phrase
DOM 1
UNIONED +
]
?
NHD-DTRs
?
[
phrase
DOM 2
UNIONED +
]
,
[
phrase
DOM 3
UNIONED +
]
...
[
phrase
DOM n
UNIONED +
]
?
?
?
?
?
?
?
?
?
?
?
?
?
Figure 1: Word Order Domain
phrase in the HD-DTR and NHD-DTRS features
are linearly ordered as in Figure 1.
The feature UNIONED in the daughters indi-
cates whether discontinuity amongst their con-
stituents is allowed. Computationally, the positive
(?+?) value of the feature dictates (the DOMs of)
the daughters to be sequence unioned (represented
by the operator ?) into the mother DOM: details
apart, this operation essentially merges two lists in
a way that allows interleaving of their elements.
In Reape?s theory, LP constraints come from
an entirely different source. There is nothing as
yet that blocks, for instance, the ungrammatical
zu lesen das Buch VP sequence. The relevant
constraint, i.e. COMPS?ZU-INF-V in German, is
stated in the LP component of the theory. Thus
with the interaction of the UNIONED feature and
LP statements, the grammar rules out the unac-
ceptable sequences while endorsing grammatical
ones such as the examples in (1?).
One important aspect of Reape?s theory is that
DOM is a list of whole signs rather than of any
part of them such as PHON. This is necessi-
tated by the fact that in order to determine how
DOM should be constructed, the daughters? inter-
nal structure need to be referred to, above all, the
UNIONED feature. In other words, the internal
features of the daughters must be accessible.
While this is a powerful system that overcomes
the inadequacies of phrase-structure rules, some
may feel this is a rather heavy-handed way to
solve the problems. Above all, much information
is repeated, as all the signs are effectively stated
twice, once in the phrase structure and again in
DOM. Also, the fact that discontinuity and lin-
ear precedence are handled by two distinct mecha-
nisms seems somewhat questionable, as these two
factors are computationally closely related. These
properties are not entirely attractive features for a
computational grammar.
24
2 Lexicalising Word Order Constraints
2.1 Overview
Our theoretical goal is, in a nutshell, to achieve
what Reape does, namely handling discontinuity
and linear precedence, in a simpler, more lexical-
ist manner. My central proposal consists in incor-
porating the Word Order Constraint (WOC) fea-
ture into the lexical heads, rather than positing an
additional tier for linearisation. Some new sub-
features will also be introduced.
The value of the WOC feature is a set of word-
order related constraints. It may contain any re-
lational constraint the grammar writer may want
with the proviso of its formalisability, but for the
current proposal, I include two subfeatures ADJ
(adjacency) and LP, both of which, being binary
relations, are represented as a set of ordered pairs,
the members of which must either be the head it-
self or its sisters. Figure 2 illustrates what such
feature structure looks like with an English verb
provide, as in provide him with a book.
We will discuss the new PHON subfeatures in
the next section ? for now it would suffice to con-
sider them to constitute the standard PHON list ?
so let us focus on WOC here. The WOC feature of
this verb says, for its projection (VP), three con-
straints have to be observed. Firstly, the ADJ sub-
feature says that the indirect object NP has to be
in the adjacent position to the verb (?provide yes-
terday him with a book? is not allowed). Secondly,
the first two elements of the LP value encode a
head-initial constraint for English VPs, namely
that a head verb has to be preceded by its com-
plements. Lastly, the last pair in the same set says
the indirect object must precede the with-PP (?pro-
vide with a book him? is not allowed). Notice that
this specification leaves room for some disconti-
nuity, as there is no ADJ requirement between the
indirect NP and with-PP. Hence, provide him yes-
terday with a book is allowed.
The key idea here is that since the complements
of a lexical head are available in its COMPS fea-
ture, it should be possible to state the relative lin-
ear order which holds between the head and a
complement, as well as between complements, in-
side the feature structure of the head.
Admittedly word order would naturally be con-
sidered to reside in a phrase, string of words.
It might be argued, on the ground that a head?s
COMPS feature simply consists of the categories
it selects for in exclusion of the PHON feature,
that with this architecture one would inevitably
encounter the ?accessibility? problem discussed in
v
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
verb
PHON
?
?
phon-wd
CONSTITUENTS
{
provide
}
CONSTRAINTS{}
?
?
COMPS
?
np
[
np
case Acc
]
,
pp
[
pp
pform with
]
?
WOC
?
?
?
?
woc
ADJ
{
?
v , np
?
}
LP
{
?
v , np
?
,
?
v , pp
?
,
?
np
,
pp
?
}
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
Figure 2: Example of lexical head with WOC fea-
ture
Section 1.2: in order to ensure the enforceability
of word order constraints, an access must be se-
cured to the values of the internal features includ-
ing the PHON values. However, this problem can
be overcome, as we will see, if due arrangements
are in place.
The main benefit of this mechanism is that it
paves way to an entirely lexicon-based rule spec-
ification, so that, on one hand, duplication of in-
formation between lexical specification and phrase
structure rules can be reduced and on the other, a
wide variety of lexical properties can be flexibly
handled. If the word order constraints, which have
been regarded as the bastion of rule-based gram-
mars, is shown to be lexically handled, it is one
significant step further to a fully lexicalist gram-
mar.
2.2 New Head-Argument Schema
What is crucial for this WOC-incorporated gram-
mar is how the required word order constraints
stated in WOC are passed on and enforced in its
projection. I attempt to formalise this in the form
of Head-Argument Schema, by modifying Head-
Complement Schema of Pollard and Sag (1994).
There are two key revisions: an enriched PHON
feature that contains word order constraints and
percolation of these constraints emanating from
the WOC feature in the head.
The revised Schema is shown in Figure 3. For
simplicity only the LP subfeature is dealt with,
since the ADJ subfeature would work exactly the
same way. The set notations attached underneath
states the restriction on the value of WOC, namely
that all the signs that appear in the constraint
pairs must be ?relevant?, i.e. must also appear as
daughters (included in ?DtrSet?, the set of the head
daughter and non-head daughters). Naturally, they
also cannot be the same signs (x6=y).
Let me discuss some auxiliary modifications
25
??
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
head-arg-phrase
PHON
?
?
?
?
?
phon
CONSTITS
?
{
{
ph
}
,
pa1
,...,
pai
,...,
paj
,...
pan
}
CONSTRTS | LP
?
{
{
...,
?
pai
,
paj
?
,...
}
, ca1 ,..., cai ,... caj ,..., can
}
?
?
?
?
?
ARGS??
HD-DTR hd
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
word
PHN
[
CONSTITS
{
ph
}
CONSTRS{}
]
ARGS args
?
a1
?
?
sign
PHN
[
CONSTITS pa1
CONSTRS ca1
]
?
?,..., ai
?
?
sign
PHN
[
CONSTITS pai
CONSTRS cai
]
?
?,
...,
aj
?
?
sign
PHN
[
CONSTITS paj
CONSTRS caj
]
?
?,..., an
[
sign
PHN
[
CONSTITS pan
CONSTRS can
]
]
?
WOC | LP wocs
{
...,
?
ai , aj
?
,...
}
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
NHD-DTRs args
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
where wocs ? {?x,y?|x6=y, x,y?DtrSet}
DtrSet = {hd}? args
Figure 3: Head-Argument Schema with WOC feature
first. Firstly, we change the feature name from
COMPS to ARGS because we assume a non-
configurational flat structure, as is commonly the
case with linearisation grammar. Another change
I propose is to make ARGS a list of underspeci-
fied signs instead of SYNSEMs as standardly as-
sumed (Pollard and Sag, 1994). In fact, this is a
position taken in an older version of HPSG (Pol-
lard and Sag, 1987) but rejected on the ground of
the locality of subcategorisation. The main reason
for this reversal is to facilitate the ?accessibility?
we discussed earlier. As unification and percola-
tion of the PHON information is involved in the
Schema, it is much more straightforward to for-
mulate with signs. Though the change may not
be quite defensible solely on this ground,1 there is
reason to leave the locality principle as an option
for languages of which it holds rather than hard-
wire it into the Schema, since some authors raise
doubt as for the universal applicability of the lo-
cality principle e.g. (Meurers, 1999).
Turning to a more substantial modification, our
new PHON feature consists of two subfeatures,
CONSTITUENTS (or CONSTITS) and CON-
STRAINTS (or CONSTRS). The former encodes
the set that comprises the phonology of words of
which the string consists. Put simply, it is the un-
1Another potential problem is cyclicity, since the sign-
valued ARGS feature contains the WOC feature, which could
contain the head itself. This has to be fixed for the systems
that do not allow cyclicity.
ordered version of the standard PHON list. The
CONSTRAINTS feature represents the concata-
native constraints applicable to the string. Thus,
the PHON feature overall represents the legitimate
word order patterns in an underspecified way, i.e.
any of the possible string combinations that obey
the constraints. Let me illustrate with a VP ex-
ample, say, consisting of meet, often and Tom, for
which we assume that the following word order
patterns are acceptable,
?meet, Tom, often?, ?often, meet, Tom?
but not the followings:
?meet, often, Tom?, ?Tom, often, meet?,
?Tom, meet, often?, ?often, Tom, meet?.
This situation can be captured by the following
feature specification for PHON, which encodes
any of the acceptable strings above in an under-
specified way.
?
?
?
?
?
?
?
?
PHON
?
?
?
?
?
?
?
?
CONSTITS
{
often, Tom, meet
}
CONSTRS
?
?
?
?
?
ADJ
{
?
{
meet
}
,
{
Tom
}
?
}
LP
{
?
{
meet
}
,
{
Tom
}
?
}
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
The key point is that now the computation of
word order can be done based on the information
inside the PHON feature, though indeed the CON-
STR values have to come from outside ? the word
order crucially depends on SYNSEM-related val-
ues of the daughter signs.
26
Let us now go back to the Schema in Figure 3
and see how to determine the CONSTR values to
enter the PHON feature. This is achieved by look-
ing up the WOC constraints in the head (let?s call
this Step 1) and pushing the relevant constraints
into the PHON feature of its mother, according to
the type of constraints (Step 2).
For readability Figure 3 only states explicitly
a special case ? where one LP constraint holds
of two of the arguments ? but the reader is
asked to interpret ai and aj in the head daughter?s
WOC|LP to represent any two signs chosen from
the ?DTRS? list (including the head, hd). 2 The
structure sharing of ai and aj between WOC|LP
and ARGS indicates that the LP constraint applies
to these two arguments in this order, i.e. ai?aj.
Thus through unification, it is determined which
constraints apply to which pairs of daughter signs
inside the head. This corresponds to Step 1.
Now, only for these WOC-applicable daughter
signs, the PHON|CONSTIITS values are paired up
for each constraint (in this case ?pai, paj?) and
pushed into the mother?s PHON|CONSTRS fea-
ture. This corresponds to Step 2.
Notice also that the CONSTRAINTS subfeature
is cumulatively inherited. All the non-head daugh-
ters? CONSTR values (ca1,...,can) ? the word or-
der constraints applicable to each of these daugh-
ters ? are also passed up, collecting effectively
all the CONSTR values of its daughters and de-
scendants. This means the information concern-
ing word order, as tied to particular string pairs, is
never lost and passed up all the way through. Thus
the WOC constraints can be enforced at any point
where both members of the string pair in question
are instantiated.
2.3 A Worked Example
Let us now go through an example of applying
the Schema, again with the German subordinate
clause, das Buch der Fritz dem Frank zu lesen er-
laubt (and other acceptable variants). Our goal is
to enforce the ADJ and LP constraints in a flexible
enough way, allowing the acceptable sequences
such as those we saw in Section 1.2.1. while
blocking the constraint-violating instances.
The instantiated Schema is shown in Figure 4.
Let us start with a rather deeply embedded level,
the embedded verb zu-lesen, marked v2, found in-
side vp (the last and largest NHD-DTR) as its HD-
2For the generality of the number of ARGS elements,
which should be taken to be any number including zero, the
recursive definition as detailed in (Richter and Sailer, 1995)
can be adopted.
DTR, which I suppose to be one lexical item for
simplicity. This is one of the lexical heads from
which the WOC constraints emanate. Find, in
this item?s WOC, a general LP constraint for zu-
Infinitiv VPs, COMPS?V, namely np3?v2. Then
the PHON|CONSTITS values of these signs are
searched for and found in the daughters, namely
pnp3 and pv2. These values are paired up and
passed into the CONSTRS|LP value of its mother
VP. Notice also that into this value the NHD-
DTRs? CONSTR|LP values, in this case only
lpnp3 ({das}?{Buch}), are also unioned, consti-
tuting lpvp: we are here witnessing the cumula-
tive inheritance of constraints explained earlier.
Turn attention now to the percolation of ADJ sub-
feature: no ADJ requirement is found between
das Buch and zu-lesen (v2?s WOC|ADJ is empty),
though ADJ is required one node below, between
das and Buch (np3?s PHN|CONSTR|ADJ). Thus
no new ADJ pair is added to the mother VP?s
PHON|CONSTR feature.
Exactly the same process is repeated for the
projection of erlauben (v1), where its WOC
again contains only LP requirements. With the
PHON|CONSTITS values of the relevant signs
found and paired up ({Fritz,der}?{erlaubt} and
{Frank,dem}?{erlaubt}), they are pushed into its
mother?s PHON|CONSTRS|LP value, which is
also unioned with the PHON|CONSTRS values of
the NHD-DTRS. Notice this time that there is no
LP requirement between the zu-Infinitiv VP, das
Buch zu-lesen, and the higher verb, erlaubt. This
is intended to allow for extraposition.3
The eventual effect of the cumulative constraint
inheritance can be more clearly seen in the sub-
AVM underneath, which shows the PHON part of
the whole feature structure with its values instan-
tiated. After a succession of applications of the
Head-Argument Schema, we now have a pool of
WOCs sufficient to block unwanted word order
patterns while endorsing legitimate ones. The rep-
resentation of the PHON feature being underspec-
ified, it corresponds to any of the appropriately
constrained order patterns. der Fritz dem Frank
zu lesen das Buch erlaubt would be ruled out by
the violation of the last LP constraint, der Fritz er-
laubt dem Frank das Buch zu lesen by the second,
and so on.
The reader might be led to think, because of
3The lack of this LP requirement also entails some
marginally acceptable instances, such as der Fritz dem Frank
das Buch erlaubt zu lesen, considered ungrammatical by
many. These instances can be blocked, however, by intro-
ducing more complex WOCs. See Sato (forthcoming a).
27
??
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
subordinate-clause
PHON
?
?
?
CONSTITS pv1 ? pnp1 ? pnp2 ? pvp
CONSTRS
[
ADJ adnp1 ? adnp2 ? adnp3
LP
{
?
pnp1
,
pv1
?
,
?
pnp2
,
pv1
?
}
? lpnp1 ? lpnp2 ? lpvp
]
?
?
?
ARGS??
HD-DTR v1
?
?
?
?
?
?
?
verb
PHON | CONSTITS pv1
{
erlaubt
}
ARGS
?
np1
,
np2
,
vp
?
WOC
[
ADJ{}
LP
{
?
np1
, v1
?
,
?
np2
, v1
?
}
]
?
?
?
?
?
?
?
NHD-DTRs
?
np1
?
?
?
?
?
?
?
?
?
?
?
np
PHON
?
?
?
?
?
?
?
CONSTITS pnp1
{
Fritz, der
}
CONSTRS
?
?
?
?
ADJ adnp1
{
?
{
Fritz
}
,
{
der
}
?
}
LP lpnp1
{
?
{
der
}
,
{
Fritz
}
?
}
?
?
?
?
?
?
?
?
?
?
?
SYNSEM | ... | CASE Nom
?
?
?
?
?
?
?
?
?
?
?
,
np2
?
?
?
?
?
?
?
?
?
?
?
np
PHON
?
?
?
?
?
?
?
CONSTITS pnp1
{
Frank, dem
}
CONSTRS
?
?
?
?
ADJ adnp2
{
?
{
Frank
}
,
{
der
}
?
}
LP lpnp2
{
?
{
der
}
,
{
Frank
}
?
}
?
?
?
?
?
?
?
?
?
?
?
SYNSEM | ... | CASE Dat
?
?
?
?
?
?
?
?
?
?
?
,
vp
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
vp
PHON
?
?
?
CONSTITS pvp : pv2 ? pnp3
CONSTRS
[
ADJ adnp3
LP lpvp
{
?
pnp3
,
pv2
?
}
? lpnp3
]
?
?
?
ARGS??
HD-DTR v2
?
?
?
?
?
?
?
v
PHON | CONSTITS pv2
{
zu-lesen
}
ARGS
?
np3
?
WOC
[
ADJ{}
LP
{
?
np3
, v2
?
}
]
?
?
?
?
?
?
?
NHD-DTRS
?
np3
?
?
?
?
?
?
?
?
?
?
?
np
PHON
?
?
?
?
?
?
?
CONSTITS pnp3
{
Buch,das
}
CONSTRS
?
?
?
?
ADJ adnp3
{
?
{
Buch
}
,
{
das
}
?
}
LP lpnp3
{
?
{
das
}
,
{
Buch
}
?
}
?
?
?
?
?
?
?
?
?
?
?
SYNSEM | ... | CASE Acc
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
Instantiated PHON part of the above:
PHON
?
?
?
?
?
?
?
?
?
?
CONSTITS
{
erlaubt, Fritz, der, Frank, dem, zu-lesen, Buch, das
}
CONSTRS
?
?
?
?
?
?
?
ADJ
{
?
{
Fritz
}
,
{
der
}
?
,
?
{
Frank
}
,
{
dem
}
?
,
?
{
Buch
}
,
{
das
}
?
}
LP
?
?
?
?
?
?
{
Fritz,der
}
,
{
erlaubt
}
?
,
?
{
Frank,dem
}
,
{
erlaubt
}
?
,
?
{
der
}
,
{
Fritz
}
?
,
?
{
dem
}
,
{
Frank
}
?
,
?
{
das
}
,
{
Buch
}
?
,
?
{
Buch,das
}
,
{
zu-lesen
}
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
Figure 4: An application of Head-Argument Schema
28
the monotonic inheritance of constraints, that the
WOC compliance cannot be checked until the
stage of final projection. While this is generally
true for freer word order languages considering
various scenarios such as bottom-up generation,
one can conduct the WOC check immediately after
the instantiation of relevant categories in parsing,
the fact we can exploit in our implementation, as
we will now see.
3 Constrained Free Word Order Parsing
3.1 Algorithm
In this section our parsing algorithm that works
with the lexicalised linearisation grammar out-
lined above is briefly overviewed.4 It expands on
two existing ideas: bitmasks for non-CFG parsing
and dynamic constraint application.
Bitmasks are used to indicate the positions of
a parsed words, wherever they have been found.
Reape (1991) presents a non-CFG tabular parsing
algorithm using them, for ?permutation complete?
language, which accepts all the permutations and
discontinuous realisations of words. To take for
an example a simple English NP that comprises
the, thick and book, this parser accepts not only
their 3! permutations but discontinuous realisa-
tions thereof in a longer string, such as [book, -,
the, -, thick] (?-? indicates the positions of con-
stituents from other phrases).
Clearly, the problem here is overgeneration and
(in)efficiency. In the current form the worst-
case complexity will be exponential (O (n!?2n), n =
length of string). In response, Daniels and Meur-
ers (2004) propose to restrict search space dur-
ing the parse with two additional bitmasks, pos-
itive and negative masks, which encode the bits
that must be and must not be occupied, respec-
tively, based on what has been found thus far and
the relevant word order constraints. For example,
given the constraints that Det precedes Nom and
Det must be adjacent to Nom and supposing the
parser has found Det in the third position of a five
word string like above, the negative mask [ x, x,
the, -, -] is created, where x indicates the position
that cannot be occupied by Nom, as well as the
positive mask [ * , das, *, -], where * indicates the
positions that must be occupied by Nom. Thus,
you can stop the parser from searching the posi-
tions the categories yet to be found cannot occupy,
or force it to search only the positions they have to
occupy.
4For full details see Sato (forthcoming b).
A remaining important job is to how to state the
constraints themselves in a grammar that works
with this architecture, and Daniels and Meurers?
answer is a rather traditional one: stating them in
phrase structure rules as LP attachments. They
modify HPSG rather extensively in a way simi-
lar to GPSG, in what they call ?Generalised ID/LP
Grammar?. However, as we have been arguing,
this is not an inevitable move. It is possible to keep
the general contour of the standard HPSG largely
intact.
The way our parser interacts with the grammar
is fundamentally different. We take full advan-
tage of the information that now resides in lexi-
cal heads. Firstly, rules are dynamically generated
from the subcategorisation information (ARGS
feature) in the head. Secondly, the constraints
are picked up from the WOC feature when lexical
heads are encountered and carried in edges, elimi-
nating the need for positive/negative masks. When
an active edge is about to embrace the next cate-
gory, these constraints are checked and enforced,
limiting the search space thereby.
After the lexicon lookup, the parser generates
rules from the found lexical head and forms lexi-
cal edges. It is also at this stage that the WOC is
picked up and pushed into the edge, along with the
rule generated:
?Mum? Hd-Dtr ? Nhd1 Nhd2...Nhdn; WOCs?
where WOCs is the set of ADJ and LP constraints
picked up, if any. This edge now tries to find the
rest ? non-head daughters. The following is the
representation of an edge when the parsing pro-
ceeds to the stage where some non-head daughter,
in this representation Dtri, has been parsed, and
Dtrj is to be searched for.
?Mum? Dtr1 Dtr2...Dtri? Dtrj...Dtrn; WOCs?
When Dtrj is found, the parser does not immedi-
ately move the dot. At this point the WOC com-
pliance check with the relevant WOC constraint ?
the one(s) involving Dtri and Dtrj ? is conducted
on these two daughters. The compliance check is
a simple list operation. It picks the bitmasks of
the two daughters in question and checks whether
the occupied positions of one daughter precede/are
adjacent to those of the other.
The failure of this check would prevent the dot
move from taking place. Thus, edges that violate
the word order constraints would not be created,
thereby preventing wasteful search. This is the
same feature as Daniels and Meurers?, and there-
fore the efficiency in terms of the number of edges
is identical. The main difference is that we use
29
the information inside the feature structure with-
out having media like positive/negative masks.
3.2 Implementation
I have implemented the algorithm in Prolog and
coded the HPSG feature structure in the way de-
scribed using ProFIT (Erbach, 1995). It is a head-
corner, bottom-up chart parser, roughly based on
Gazdar and Mellish (1989). The main modifi-
cation consists of introducing bitmasks and the
word order checking procedure described above.
I created small grammars for Japanese and Ger-
man and put them to the parser, to confirm that
linearisation-heavy constructions such as object
control construction can be successfully parsed,
with the WOC constraints enforced.
4 Future Tasks
What we have seen is an outline of my initial pro-
posal and there are numerous tasks yet to be tack-
led. First of all, now that the constraints are writ-
ten in individual lexical items, we are in need of
appropriate typing in terms of word order con-
straints, in order to be able to state succinctly gen-
eral constraints such as the head-final/initial con-
straint. In other words, it is crucial to devise an
appropriate type hierarchy.
Another potential problem concerns the gen-
erality of our theoretical framework. I have fo-
cused on the Head-Argument structure in this pa-
per, but if the present theory were to be of gen-
eral use, non-argument constructions, such as the
Head-Modifier structure, must be accounted for.
Also, the cases where the head of a phrase is itself
a phrase may pose a challenge, if such a phrasal
head were to determine the word order of its pro-
jection. Since it is desirable for computational
transparency not to use emergent constraints, I will
attempt to get al the word order constraints ul-
timately propagated and monotonically inherited
from the lexical level. Though some word order
constraints may turn out to have to be written into
the phrasal head directly, I am confident that the
majority, if not all, of the constraints can be stated
in the lexicon. These issues are tackled in a sepa-
rate paper (Sato, forthcoming a).
In terms of efficiency, more study has to be re-
quired to identify the exact complexity of my algo-
rithm. Also, with a view to using it for a practical
system, an evaluation of the efficiency on the ac-
tual machine will be crucial.
References
M. Daniels and D. Meurers. 2004. GIDLP: A gram-
mar format for linearization-based HPSG. In Pro-
ceedings of the HPSG04 Conference.
G. Erbach. 1995. ProFIT: Prolog with features, in-
heritance and templates. Proceedings of the Seventh
Conference of the European Association for Compu-
tational Linguistics.
G. Gazdar and C. Mellish. 1989. Natural Language
Processing in Prolog. Addison Wesley.
G. Gazdar, E. Klein, G. Pullum, and I. Sag. 1985. Gen-
eralized Phrase Structure Grammar. Harvard UP.
E. Hinrichs and T. Nakazawa. 1990. Subcategorization
and VP structure in German. In S. Hughes et al,
editor, Proceedings of the Third Symposium on Ger-
manic Linguistics.
D. Meurers. 1999. Raising Spirits (and assigning them
case). Groninger Arbeiten zur Germanistischen Lin-
guistik, Groningen Univ.
C. Pollard and I. Sag. 1987. Information-Based Syntax
and Semantics. CSLI.
C. Pollard and I. Sag. 1994. Head-Driven Phrase
Structure Grammar. CSLI.
M. Reape. 1991. Parsing bounded discontinuous
constituents: Generalisation of some common algo-
rithms. DIANA Report, Edinburgh Univ.
M. Reape. 1993. A Formal Theory of Word Order.
Ph.D. thesis, Edinburgh University.
M. Reape. 1994. Domain union and word order vari-
ation in German. In J. Nerbonne et al, editor, Ger-
man in Head-Driven Phrase Structure Grammar.
F. Richter and M. Sailer. 1995. Remarks on lineariza-
tion. Magisterarbeit, T?bingen Univ.
Y. Sato. 2004. Discontinuous constituency and non-
CFG parsing. http://www.dcs.kcl.ac.uk/pg/satoyo.
Y. Sato. forthcoming a. Two alternatives for lexicalist
linearisation grammar: Locality Principle revisited.
Y. Sato. forthcoming b. Constrained free word order
parsing for lexicalist grammar.
S. Shieber. 1985. Evidence against the context free-
ness of natural languages. Linguistics and Philoso-
phy, 8(3):333?43.
S. Yatabe. 1996. Long-distance scrambling via partial
compaction. In M. Koizumi et al, editor, Formal
Approaches to Japanese Linguistics 2. MIT Press,
Cambridge, Mass.
30
Proceedings of EACL 2009 Workshop on Semantic Representation of Spoken Language - SRSL 2009, pages 74?81,
Athens, Greece, 30 March 2009. c?2009 Association for Computational Linguistics
Incrementality, Speaker-Hearer Switching
and the Disambiguation Challenge
Ruth Kempson, Eleni Gregoromichelaki
King?s College London
{ruth.kempson, eleni.gregor}@kcl.ac.uk
Yo Sato
University of Hertfordshire
y.sato@herts.ac.uk
Abstract
Taking so-called split utterances as our
point of departure, we argue that a new
perspective on the major challenge of dis-
ambiguation becomes available, given a
framework in which both parsing and gen-
eration incrementally involve the same
mechanisms for constructing trees reflect-
ing interpretation (Dynamic Syntax: (Cann
et al, 2005; Kempson et al, 2001)). With
all dependencies, syntactic, semantic and
pragmatic, defined in terms of incremental
progressive tree growth, the phenomenon
of speaker/hearer role-switch emerges as
an immediate consequence, with the po-
tential for clarification, acknowledgement,
correction, all available incrementally at
any sub-sentential point in the interpreta-
tion process. Accordingly, at all interme-
diate points where interpretation of an ut-
terance subpart is not fully determined for
the hearer in context, uncertainty can be
resolved immediately by suitable clarifica-
tion/correction/repair/extension as an ex-
change between interlocutors. The result
is a major check on the combinatorial ex-
plosion of alternative structures and inter-
pretations at each choice point, and the ba-
sis for a model of how interpretation in
context can be established without either
party having to make assumptions about
what information they and their interlocu-
tor share in resolving ambiguities.
1 Introduction
A major characteristic of dialogue is effortless
switching between the roles of hearer and speaker.
Dialogue participants seamlessly shift between
parsing and generation bi-directionally across any
syntactic dependency, without any indication of
there being any problem associated with such
shifts (examples from Howes et al (in prep)):
(1) Conversation from A and B, to C:
A: We?re going
B: to Bristol, where Jo lives.
(2) A smelling smoke comes into the kitchen:
A: Have you burnt
B the buns. Very thoroughly.
A: But did you burn
B: Myself? No. Luckily.
(3) A: Are you left or
B: Right-handed.
Furthermore, in no case is there any guarantee that
the way the shared utterance evolves is what ei-
ther party had in mind to say at the outset, indeed
obviously not, as otherwise the exchange risks be-
ing otiose. This flexibility provides a vehicle for
ongoing clarification, acknowledgement, correc-
tions, repairs etc. ((6)-(7) from (Mills, 2007)):
(4) A: I?m seeing Bill.
B: The builder?
A: Yeah, who lives with Monica.
(5) A: I saw Don
B: John?
A: Don, the guy from Bristol.
(6) A: I?m on the second switch
B: Switch?
A: Yeah, the grey thing
(7) A: I?m on the second row third on the left.
B: What?
A: on the left
The fragmental utterances that constitute such in-
cremental, joint contributions have been analysed
as falling into discrete structural types according
to their function, in all cases resolved to propo-
sitional types by combining with appropriate ab-
stractions from context (Ferna?ndez, 2006; Purver,
2004). However, any such fragment and their
resolution may occur as mid-turn interruptions,
well before any emergent propositional structure
is completed:
74
(8) A: They X-rayed me, and took a urine
sample, took a blood sample.
Er, the doctor ...
B: Chorlton?
A: Chorlton, mhm, he examined me, erm,
he, he said now they were on about a slight
[shadow] on my heart. [BNC: KPY
1005-1008]
The advantage of such ongoing, incremental, joint
conversational contributions is the effective nar-
rowing down of the search space out of which
hearers select (a) interpretations to yield some
commonly shared understanding, e.g. choice
of referents for NPs, and, (b) restricted struc-
tural frames which allow (grammatical) context-
dependent fragment resolution, i.e. exact speci-
fications of what contextually available structures
resolve elliptical elements. This seems to pro-
vide an answer as to why such fragments are so
frequent and undemanding elements of dialogue,
forming the basis for the observed coordination
between participants: successive resolution at sub-
sentential stages yields a progressively jointly es-
tablished common ground, that can thereafter be
taken as a secure, albeit individual, basis for filter-
ing out interpretations inconsistent with such con-
firmed knowledge-base (see (Poesio and Rieser,
2008; Ginzburg, forthcmg) etc). All such dialogue
phenomena, illustrated in (1)-(8), jointly and in-
crementally achieved, we address with the general
term split utterances.
However, such exchanges are hard to model
within orthodox grammatical frameworks, given
that usually it is the sentence/proposition that is
taken as the unit of syntactic/semantic analysis;
and they have not been addressed in detail within
such frameworks, being set aside as deviant, given
that such grammars in principle do not specify
a concept of grammaticality that relies on a de-
scription of the context of occurrence of a certain
structure (however, see Poesio and Rieser (2008)
for German completions). In so far as fragment
utterances are now being addressed, the pressure
of compatibility with sentence-based grammars
is at least partly responsible for analyses of e.g.
clarificatory-request fragments as sentential in na-
ture (Ginzburg and Cooper, 2004). But such anal-
yses fail to provide a basis for incrementally re-
solved clarification requests such as the interrup-
tion in (8) where no sentential basis is yet avail-
able over which to define the required abstraction
of contextually provided content.
In the psycholinguistic literature, on the other
hand, there is broad agreement that incrementality
is a crucial feature of parsing with semantic inter-
pretation taking place as early as possible at the
sub-sentential level (see e.g. (Sturt and Crocker,
1996)). Nonetheless, this does not, in and of it-
self, provide a basis for explaining the ease and
frequency of split utterances in dialogue: the inter-
active coordination between the parsing and pro-
duction activities, one feeding the other, remains
as a challenge.
In NLP modelling, parsing and generation algo-
rithms are generally dissociated from the descrip-
tion of linguistic entities and rules, i.e. the gram-
mar formalisms, which are considered either to be
independent of processing (?process-neutral?) or
to require some additional generation- or parsing-
specific mechanisms to be incorporated. However,
this point of view creates obstacles for a success-
ful account of data as in (1)-(8). Modelling those
would require that, for the current speaker, the ini-
tiated generation mechanism has to be displaced
mid-production without the propositional genera-
tion task having been completed. Then the parsing
mechanism, despite being independent of, indeed
in some sense the reverse of, the generation com-
ponent, has to take over mid-sentence as though, in
some sense there had been parsing involved up to
the point of switchover. Conversely, for the hearer-
turned-speaker, it would be necessary to somehow
connect their parse with what they are now about
to produce in order to compose the meaning of the
combined sentence. Moreover, in both directions
of switch, as (2) shows, this is not a phenomenon
of both interlocutors intending to say the same
sentence: as (3) shows, even the function of the
utterance (e.g. question/answer) can alter in the
switch of roles and such fragments can play two
roles (e.g. question/completion) at the same time
(e.g. (2)). Hence the grammatical integration of
such joint contributions must be flexible enough
to allow such switches which means that such
fragment resolutions must occur before the com-
putation of intentions at the pragmatic level. So
the ability of language users to successfully pro-
cess such utterances, even at sub-sentential levels,
means that modelling their grammar requires fine-
grained grammaticality definitions able to char-
acterise and integrate sub-sentential fragments in
turns jointly constructed by speaker and hearer.
75
This can be achieved straightforwardly if fea-
tures like incrementality and context-dependent
processing are built into the grammar architecture
itself. The modelling of split utterances then be-
comes straightforward as each successive process-
ing step exploits solely the grammatical apparatus
to succeed or fail. Such a view notably does not in-
voke high-level decisions about speaker/hearer in-
tentions as part of the mechanism itself. That this
is the right view to take is enhanced by the fact that
as all of (1)-(8) show, neither party in such role-
exchanges can definitively know in advance what
will emerge as the eventual joint proposition. If,
to the contrary, generation decisions are modelled
as involving intentions for whole utterances, there
will be no the basis for modelling how such in-
complete strings can be integrated in suitable con-
texts, with joint propositional structures emerging
before such joint intentions have been established.
An additional puzzle, equally related to both
the challenges of disambiguation and the status
of modelling speaker?s intentions as part of the
mechanism whereby utterance interpretation takes
place, is the common occurrence of hearers NOT
being constrained by any check on consistency
with speaker intentions in determining a putative
interpretation, failing to make use of well estab-
lished shared knowledge:
(9) A: I?m going to cook salmon, as John?s
coming.
B: What? John?s a vegetarian.
A: Not my brother. John Smith.
(10) A: Why don?t you have cheese and noodles?
B: Beef? You KNOW I?m a vegetarian
Such examples are problematic for any account
that proposes that interpretation mechanisms for
utterance understanding solely depend on selec-
tion of interpretations which either the speaker
could have intended (Sperber and Wilson, 1986;
Carston, 2002), or ones which are compati-
ble with checking consistency with the com-
mon ground/plans established between speaker
and hearer (Poesio and Rieser, 2008; Ginzburg,
forthcmg), mutual knowledge, etc. (Clark, 1996;
Brennan and Clark, 1996). To the contrary, the
data in (9)-(10) tend to show that the full range
of interpretations computable by the grammar has
in principle to be available at all choice points for
construal, without any filter based on plausibility
measures, thus leaving the disambiguation chal-
lenge still unresolved.
In this paper we show how with speaker and
hearer in principle using the same mechanisms for
construal, equally incrementally applied, such dis-
ambiguation issues can be resolved in a timely
manner which in turn reduces the multiplication
of structural/interpretive options. As we shall see,
what connects our diverse examples, and indeed
underpins the smooth shift in the joint endeav-
our of conversation, lies in incremental, context-
dependent processing and bidirectionality, essen-
tial ingredients of the Dynamic Syntax (Cann et al,
2005) dialogue model.
2 Incrementality in Dynamic Syntax
Dynamic Syntax (DS) is a procedure-oriented
framework, involving incremental processing, i.e.
strictly sequential, word-by-word interpretation of
linguistic strings. The notion of incrementality
in DS is closely related to another of its features,
the goal-directedness of BOTH parsing and gener-
ation. At each stage of processing, structural pre-
dictions are triggered that could fulfill the goals
compatible with the input, in an underspecified
manner. For example, when a proper name like
Bob is encountered sentence-initially in English,
a semantic predicate node is predicted to follow
(?Ty(e ? t)), amongst other possibilities.
By way of introducing the reader to the DS
devices, let us look at some formal details with
an example, Bob saw Mary. The ?complete? se-
mantic representation tree resulting after the com-
plete processing of this sentence is shown in Fig-
ure 2 below. A DS tree is formally encoded with
the tree logic LOFT (Blackburn and Meyer-Viol
(1994)), we omit these details here) and is gen-
erally binary configurational, with annotations at
every node. Important annotations here, see the
(simplified) tree below, are those which represent
semantic formulae along with their type informa-
tion (e.g. ?Ty(x)?) based on a combination of the
epsilon and lambda calculi1.
Such complete trees are constructed, starting
from a radically underspecified annotation, the ax-
iom, the leftmost minimal tree in Figure 2, and
going through monotonic updates of partial, or
structurally underspecified, trees. The outline of
this process is illustrated schematically in Figure
2. Crucial for expressing the goal-directedness
are requirements, i.e. unrealised but expected
1These are the adopted semantic representation languages
in DS but the computational formalism is compatible with
other semantic-representation formats
76
0?Ty(t),
?
7?
1
?Ty(t)
?Ty(e),? ?Ty(e? t)
7?
2
?Ty(t)
Ty(e),Bob? ?Ty(e? t),?
7?
3
?Ty(t)
Ty(e),
Bob? ?Ty(e? t)
?Ty(e),
?
Ty(e? (e? t)),
See?
7?
0(gen)/4
Ty(t),?
See?(Mary?)(Bob?)
Ty(e),
Bob?
Ty(e? t),
See?(Mary?)
Ty(e),
Mary?
Ty(e? (e? t)),
See?
Figure 2: Monotonic tree growth in DS
Ty(t),
See?(Mary?)(Bob?)
Ty(e),
Bob?
Ty(e? t),
See?(Mary?)
Ty(e),
Mary?
Ty(e? (e? t)),
See?
Figure 1: A DS complete tree
node/tree specifications, indicated by ??? in front
of annotations. The axiom says that a proposition
(of type t, Ty(t)) is expected to be constructed.
Furthermore, the pointer, notated with ??? indi-
cates the ?current? node in processing, namely the
one to be processed next, and governs word order.
Updates are carried out by means of applying
actions, which are divided into two types. Compu-
tational actions govern general tree-constructional
processes, such as moving the pointer, introducing
and updating nodes, as well as compiling interpre-
tation for all non-terminal nodes in the tree. In our
example, the update of (1) to (2) is executed via
computational actions specific to English, expand-
ing the axiom to the subject and predicate nodes,
requiring the former to be processed next by the
position of the ?. Construction of only weakly
specified tree relations (unfixed nodes) can also be
induced, characterised only as dominance by some
current node, with subsequent update required. In-
dividual lexical items also provide procedures for
building structure in the form of lexical actions,
inducing both nodes and annotations. For exam-
ple, in the update from (2) to (3), the set of lexical
actions for the word see is applied, yielding the
predicate subtree and its annotations. Thus partial
trees grow incrementally, driven by procedures as-
sociated with particular words as they are encoun-
tered.
Requirements embody structural predictions as
mentioned earlier. Thus unlike the conven-
tional bottom-up parsing,2 the DS model takes
the parser/generator to entertain some predicted
goal(s) to be reached eventually at any stage of
processing, and this is precisely what makes the
formalism incremental. This is the characteri-
sation of incrementality adopted by some psy-
cholinguists under the appellation of connected-
ness (Sturt and Crocker, 1996; Costa et al, 2002):
an encountered word always gets ?connected? to a
larger, predicted, tree.
Individual DS trees consist of predicates and
their arguments. Complex structures are obtained
via a general tree-adjunction operation licensing
the construction of so-called LINKed trees, pairs
of trees where sharing of information occurs. In
its simplest form this mechanism is the same one
which provides the potential for compiling in-
2The examples in (1)-(8) also suggest the implausibility
of purely bottom-up or head-driven parsing being adopted di-
rectly, because such strategies involve waiting until all the
daughters are gathered before moving on to their projection.
In fact, the parsing strategy adopted by DS is somewhat sim-
ilar to mixed parsing strategies like the left-corner or Earley
algorithm to a degree. These parsing strategic issues are more
fully discussed in Sato (forthcmg).
77
A consultant, a friend of Jo?s, is retiring: Ty(t), Retire?((?, x, Consultant?(x) ? Friend?(Jo?)(x)))
Ty(e), (?, x, Consultant?(x) ? Friend?(Jo?)(x)) Ty(e? t), Retire?
Ty(e), (?, x, Friend?(Jo?)(x))
Ty(cn), (x, Friend?(Jo?)(x))
x Friend?(Jo?)
Jo? Friend?
Ty(cn? e), ?P.?, P
Figure 3: Apposition in DS
terpretation for apposition constructions as can
be seen in Figure (3)3. The assumption in the
construction of such LINKed structures is that at
any arbitrary stage of development, some type-
complete subtree may constitute the context for
the subsequent parsing of the following string as
an adjunct structure candidate for incorporation
into the primary tree, hence the obligatory sharing
of information in the resulting semantic represen-
tation.
More generally, context in DS is defined as the
storage of parse states, i.e., the storing of par-
tial tree, word sequence parsed to date, plus the
actions used in building up the partial tree. For-
mally, a parse state P is defined as a set of triples
?T, W, A?, where: T is a (possibly partial) tree;
W is the associated sequence of words; A is the
associated sequence of lexical and computational
actions. At any point in the parsing process, the
context C for a particular partial tree T in the set
P can be taken to consist of: a set of triples P ? =
{. . . , ?Ti, Wi, Ai?, . . .} resulting from the previ-
ous sentence(s); and the triple ?T, W, A? itself,
the subtree currently being processed. Anaphora
and ellipsis construal generally involve re-use of
formulae, structures, and actions from the set C.
Grammaticality of a string of words is then de-
fined relative to its context C, a string being well-
formed iff there is a mapping from string onto
completed tree with no outstanding requirements
given the monotonic processing of that string rela-
tive to context. All fragments illustrated above are
processed by means of either extending the current
3Epsilon terms, like ?, x, Consultant?(x), stand for wit-
nesses of existentially quantified formulae in the epsilon cal-
culus and represent the semantic content of indefinites in DS.
Defined relative to the equivalence ?(?, x, ?(x)) = ?x?(x),
their defining property is their reflection of their contain-
ing environment, and accordingly they are particularly well-
suited to expressing the growth of terms secured by such ap-
positional devices.
tree, or constructing LINKed structures and trans-
fer of information among them so that one tree
provides the context for another, and are licensed
as wellformed relative to that context. In particu-
lar, fragments like the doctor in (8) are licensed by
the grammar because they occur at a stage in pro-
cessing at which the context contains an appropri-
ate structure within which they can be integrated.
The definite NP is taken as an anaphoric device,
relying on a substitution process from the context
of the partial tree to which the node it decorates is
LINKed to achieve the appropriate construal and
tree-update:
(11) The?parse? tree licensing production of the
doctor: LINK adjunction
?Ty(t)
Chorlton? ?Ty(e? t)
(Doctor?(Chorlton?)),?
3 Bidirectionality in DS
Crucially, for our current concern, this architec-
ture allows a dialogue model in which generation
and parsing function in parallel, following exactly
the same procedure in the same order. See Fig (2)
for a (simplified) display of the transitions manip-
ulated by a parse of Bob saw Mary, as each word
is processed and integrated to reach the complete
tree. Generation of this utterance from a complete
tree follows precisely the same actions and trees
from left to right, although the complete tree is
available from the start (this is why the complete
tree is marked ?0? for generation): in this case the
eventual message is known by the speaker, though
of course not by the hearer. What generation in-
volves in addition to the parse steps is reference
78
to this complete tree to check whether each pu-
tative step is consistent with it in order not to be
deviated from the legitimate course of action, that
is, a subsumption check. The trees (1-3) are li-
censed because each of these subsumes (4). Each
time then the generator applies a lexical action, it
is licensed to produce the word that carries that ac-
tion under successful subsumption check: at Step
(3), for example, the generator processes the lex-
ical action which results in the annotation ?See??,
and upon success and subsumption of (4) license
to generate the word see at that point ensues.
For split utterances, two more assumptions are
pertinent. On the one hand, speakers may have
initially only a partial structure to convey: this is
unproblematic, as all that is required by the for-
malism is monotonicity of tree growth, the check
being one of subsumption which can be carried
out on partial trees as well. On the other hand,
the utterance plan may change, even within a sin-
gle speaker. Extensions and clarifications in DS
can be straightforwardly generated by appending
a LINKed structure projecting the added material
to be conveyed (preserving the monotonicity con-
straint)4.
(12) I?m going home, with my brother, maybe
with his wife.
Such a model under which the speaker and
hearer essentially follow the same sets of actions,
updating incrementally their semantic representa-
tions, allows the hearer to ?mirror? the same series
of partial trees, albeit not knowing in advance what
the content of the unspecified nodes will be.
4 Parser/generator implementation
The process-integral nature of DS emphasised
thus far lends itself to the straightforward imple-
mentation of a parsing/generating system, since
the ?actions? defined in the grammar directly pro-
vide a major part of its implementation. By now it
should also be clear that the DS formalism is fully
bi-directional, not only in the sense that the same
grammar can be used for generation and parsing,
but also because the two sets of activities, conven-
tionally treated as ?reverse? processes, are mod-
elled to run in parallel. Therefore, not only can the
same sets of actions be used for both processes,
4Revisions however will involve shifting to a previous
partial tree as the newly selected context: I?m going home,
to my brother, sorry my mother.
but also a large part of the parsing and generation
algorithms can be shared.
This design architecture and a prototype im-
plementation are outlined in (Purver and Otsuka,
2003), and the effort is under way to scale up the
DS parsing/generating system incorporating the
results in (Gargett et al, 2008; Gregoromichelaki
et al, to appear).5 The parser starts from the axiom
(step 0 in Fig.2), which ?predicts? a proposition to
be built, and follows the applicable actions, lexi-
cal or general, to develop a complete tree. Now,
as has been described in this paper, the genera-
tor follows exactly the same steps: the axiom is
developed through successive updates into a com-
plete tree. The only material difference from ?
or rather in addition to? parsing is the complete
tree (Step 0(gen)/4), given from the very start of
the generation task, which is then referred to at
each tree update for subsumption check. The main
point is that despite the obvious difference in their
purposes ?outputting a string from a meaning ver-
sus outputting a meaning from a string? parsing
and generation indeed share the direction of pro-
cessing in DS. Moreover, as no intervening level
of syntactic structure over the string is ever com-
puted, the parsing/generation tasks are more effi-
ciently incremental in that semantic interpretation
is directly imposed at each stage of lexical integra-
tion, irrespective of whether some given partially
developed constituent is complete.
To clarify, see the pseudocode in the Prolog
format below, which is a close analogue of the
implemented function that both does parsing and
generation of a word (context manipulation is
ignored here for reasons of space). The plus
and minus signs attached to a variable indicate it
must/needn?t be instantiated, respectively. In ef-
fect, the former corresponds to the input, the latter
to the output.
(13) parse gen word(
+OldMeaning,?Word,?NewMeaning):-
apply lexical actions(+OldMeaning, ?Word,
+LexActions, ?IntermediateMeaning ),
apply computational actions(
+IntermediateMeaning, +CompActions,
?NewMeaning )
OldMeaning is an obligatory input item, which
corresponds to the semantic structure con-
structed so far (which might be just structural
tree information initially before any lexical
5The preliminary results are described in (Sato,
forthcmg).
79
input has been processed thus advocating a
strong predictive element even compared to
(Sturt and Crocker, 1996). Now notice that
the other two variables ?corresponding to the
word and the new (post-word) meaning? may
function either as the input or output. More
precisely, this is intended to be a shorthand
for either (+OldMeaning,+Word,?NewMeaning)
i.e. Word as input and NewMeaning as out-
put, or (+OldMeaning,?Word,+NewMeaning), i.e.
NewMeaning as input and Word as output, to repeat,
the former corresponding to parsing and the latter
to generation.
In either case, the same set of two sub-
procedures, the two kinds of actions described in
(13), are applied sequentially to process the input
to produce the output. These procedures corre-
spond to an incremental ?update? from one par-
tial tree to another, through a word. The whole
function is then recursively applied to exhaust the
words in the string, from left to right, either in
parsing or generation. Thus there is no differ-
ence between the two in the order of procedures
to be applied, or words to be processed. Thus it is
a mere switch of input/output that shifts between
parsing and generation.6
4.1 Split utterances in Dynamic Syntax
Split utterances follow as an immediate conse-
quence of these assumptions. For the dialogues in
(1)-(8), therefore, while A reaches a partial tree of
what she has uttered through successive updates
as described above, B as the hearer, will follow
the same updates to reach the same representation
of what he has heard. This provides him with the
ability at any stage to become the speaker, inter-
rupting to continue A?s utterance, repair, ask for
clarification, reformulate, or provide a correction,
as and when necessary7. According to our model
of dialogue, repeating or extending a constituent
of A?s utterance by B is licensed only if B, the
hearer turned now speaker, entertains a message
6Thus the parsing procedure is dictated by the grammar to
a large extent, but importantly, not completely. More specif-
ically, the grammar formalism specifies the state paths them-
selves, but not how the paths should be searched. The DS ac-
tions are defined in conditional terms, i.e. what to do as and
when a certain condition holds. If a number of actions can be
applied at some point during a parse, i.e. locally ambiguity
is encountered, then it is up to a particular implementation
of the parser to decide which should be traversed first. The
current implementation includes suggestions of search strate-
gies.
7The account extends the implementation reported in
(Purver et al, 2006)
to be conveyed that matches or extends the parse
tree of what he has heard in a monotonic fashion.
In DS, this message is a semantic representation
in tree format and its presence allows B to only ut-
ter the relevant subpart of A?s intended utterance.
Indeed, this update is what B is seeking to clarify,
extend or acknowledge. In DS, B can reuse the
already constructed (partial) parse tree in his con-
text, rather than having to rebuild an entire propo-
sitional tree or subtree.
The fact that the parsing formalism integrates
a strong element of predictivity, i.e. the parser
is always one step ahead from the lexical in-
put, allows a straightforward switch from pars-
ing to generation thus resulting in an explana-
tion of the facility with which split utterances oc-
cur (even without explicit reasoning processes).
Moreover, on the one hand, because of incremen-
tality, the issue of interpretation-selection can be
faced at any point in the process, with correc-
tions/acknowledgements etc. able to be provided
at any point; this results in the potential exponen-
tial explosion of interpretations being kept firmly
in check. And, structurally, such fragments can
be integrated in the current partial tree represen-
tation only (given the position of the pointer) so
there is no structural ambiguity multiplication. On
the other hand, for any one of these intermedi-
ate check points, bidirectionality entails that con-
sistency checking remains internal to the individ-
ual interlocutors? system, the fact of their mir-
roring each other resulting at their being at the
same point of tree growth. This is sufficient to en-
sure that any inconsistency with their own parse
recognised by one party as grounds for correc-
tion/repair can be processed AS a correction/repair
by the other party without requiring any additional
metarepresentation of their interlocutors? informa-
tion state (at least for these purposes). This allows
the possibility of building up apparently complex
assumptions of shared content, without any neces-
sity of constructing hypotheses of what is enter-
tained by the other, since all context-based selec-
tions are based on the context of the interlocutor
themselves. This, in its turn, opens up the possi-
bility of hearers constructing interpretations based
on selections made that transparently violate what
is knowledge shared by both parties, for no pre-
sumption of common ground is essential as input
to the interpretation process (see, e.g. (9)-(10)).
80
5 Conclusion
It is notable that, from this perspective, no pre-
sumption of common ground or hypothesis as to
what the speaker could have intended is necessary
to determine how the hearer selects interpretation.
All that is required is a concept of system-internal
consistency checking, the potential for clarifica-
tion in cases of uncertainty, and reliance at such
points on disambiguation/correction/repair by the
other party. The advantage of such a proposal, we
suggest, is the provision of a fully mechanistic ac-
count for disambiguation (cf. (Pickering and Gar-
rod, 2004)). The consequence of such an analysis
is that language use is essentially interactive (see
also (Ginzburg, forthcmg; Clark, 1996)): the only
constraint as to whether some hypothesised in-
terpretation assigned by either party is confirmed
turns on whether it is acknowledged or corrected
(see also (Healey, 2008)).
Acknowledgements
This work was supported by grants ESRC RES-062-23-0962,
the EU ITALK project (FP7-214668) and Leverhulme F07-
04OU. We are grateful for comments to: Robin Cooper, Alex
Davies, Arash Eshghi, Jonathan Ginzburg, Pat Healey, Greg
James Mills. Normal disclaimers apply.
References
Patrick Blackburn and Wilfried Meyer-Viol. 1994.
Linguistics, logic and finite trees. Bulletin of the
IGPL, 2:3?31.
Susan E. Brennan and Herbert H. Clark. 1996. Con-
ceptual pacts and lexical choice in conversation.
Journal of Experimental Psychology: Learning,
Memory and Cognition, 22:482?1493.
Ronnie Cann, Ruth Kempson, and Lutz Marten. 2005.
The Dynamics of Language. Elsevier, Oxford.
Robyn Carston. 2002. Thoughts and Utterances: The
Pragmatics of Explicit Communication. Blackwell.
Herbert H. Clark. 1996. Using Language. Cambridge
University Press.
Fabrizio Costa, Paolo Frasconi, Vincenzo Lombardo,
Patrick Sturt, and Giovanni Soda. 2002. Enhanc-
ing first-pass attachment prediction. In ECAI 2002:
508-512.
Raquel Ferna?ndez. 2006. Non-Sentential Utterances
in Dialogue: Classification, Resolution and Use.
Ph.D. thesis, King?s College London, University of
London.
Andrew Gargett, Eleni Gregoromichelaki, Chris
Howes, and Yo Sato. 2008. Dialogue-grammar cor-
respondence in dynamic syntax. In Proceedings of
the 12th SEMDIAL (LONDIAL).
Jonathan Ginzburg and Robin Cooper. 2004. Clarifi-
cation, ellipsis, and the nature of contextual updates
in dialogue. Linguistics and Philosophy, 27(3):297?
365.
Jonathan Ginzburg. forthcmg. Semantics for Conver-
sation. CSLI.
Eleni Gregoromichelaki, Yo Sato, Ruth Kempson, An-
drew Gargett, and Christine Howes. to appear. Dia-
logue modelling and the remit of core grammar. In
Proceedings of IWCS 2009.
Patrick Healey. 2008. Interactive misalignment: The
role of repair in the development of group sub-
languages. In R. Cooper and R. Kempson, editors,
Language in Flux. College Publications.
Christine Howes, Patrick G. T. Healey, and Gregory
Mills. in prep. a: An experimental investigation
into. . . b: . . . split utterances.
Ruth Kempson, Wilfried Meyer-Viol, and Dov Gabbay.
2001. Dynamic Syntax: The Flow of Language Un-
derstanding. Blackwell.
Gregory J. Mills. 2007. Semantic co-ordination in di-
alogue: the role of direct interaction. Ph.D. thesis,
Queen Mary University of London.
Martin Pickering and Simon Garrod. 2004. Toward
a mechanistic psychology of dialogue. Behavioral
and Brain Sciences.
Massimo Poesio and Hannes Rieser. 2008. Comple-
tions, coordination, and alignment in dialogue. Ms.
Matthew Purver and Masayuki Otsuka. 2003. Incre-
mental generation by incremental parsing: Tactical
generation in Dynamic Syntax. In Proceedings of
the 9th European Workshop in Natural Language
Generation (ENLG), pages 79?86.
Matthew Purver, Ronnie Cann, and Ruth Kempson.
2006. Grammars as parsers: Meeting the dialogue
challenge. Research on Language and Computa-
tion, 4(2-3):289?326.
Matthew Purver. 2004. The Theory and Use of Clari-
fication Requests in Dialogue. Ph.D. thesis, Univer-
sity of London, forthcoming.
Yo Sato. forthcmg. Local ambiguity, search strate-
gies and parsing in dynamic syntax. In Eleni Gre-
goromichelaki and Ruth Kempson, editors, Dynamic
Syntax: Collected Papers. CSLI.
Dan Sperber and Deirdre Wilson. 1986. Relevance:
Communication and Cognition. Blackwell.
Patrick Sturt and Matthew Crocker. 1996. Monotonic
syntactic processing: a cross-linguistic study of at-
tachment and reanalysis. Language and Cognitive
Processes, 11:448?494.
81
Coling 2008: Proceedings of the workshop on Cross-Framework and Cross-Domain Parser Evaluation, pages 29?35
Manchester, August 2008
Parser Evaluation across Frameworks without Format Conversion
Wai Lok Tam
Interfaculty Initiative in
Information Studies
University of Tokyo
7-3-1 Hongo Bunkyo-ku
Tokyo 113-0033 Japan
Yo Sato
Dept of Computer Science
Queen Mary
University of London
Mile End Road
London E1 4NS, U.K.
Yusuke Miyao
Dept of Computer Science
University of Tokyo
7-3-1 Hongo Bunkyo-ku
Tokyo 113-0033 Japan
Jun-ichi Tsujii
Abstract
In the area of parser evaluation, formats
like GR and SD which are based on
dependencies, the simplest representation
of syntactic information, are proposed as
framework-independent metrics for parser
evaluation. The assumption behind these
proposals is that the simplicity of depen-
dencies would make conversion from syn-
tactic structures and semantic representa-
tions used in other formalisms to GR/SD a
easy job. But (Miyao et al, 2007) reports
that even conversion between these two
formats is not easy at all. Not to mention
that the 80% success rate of conversion
is not meaningful for parsers that boast
90% accuracy. In this paper, we make
an attempt at evaluation across frame-
works without format conversion. This
is achieved by generating a list of names
of phenomena with each parse. These
names of phenomena are matched against
the phenomena given in the gold stan-
dard. The number of matches found is used
for evaluating the parser that produces the
parses. The evaluation method is more ef-
fective than evaluation methods which in-
volve format conversion because the gen-
eration of names of phenomena from the
output of a parser loaded is done by a rec-
ognizer that has a 100% success rate of
recognizing a phenomenon illustrated by a
sentence. The success rate is made pos-
sible by the reuse of native codes: codes
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
used for writing the parser and rules of the
grammar loaded into the parser.
1 Introduction
The traditional evaluation method for a deep parser
is to test it against a list of sentences, each of which
is paired with a yes or no. The parser is evaluated
on the number of grammatical sentences it accepts
and that of ungrammatical sentences it rules out.
A problem with this approach to evaluation is that
it neither penalizes a parser for getting an analy-
sis wrong for a sentence nor rewards it for getting
it right. What prevents the NLP community from
working out a universally applicable reward and
penalty scheme is the absence of a gold standard
that can be used across frameworks. The correct-
ness of an analysis produced by a parser can only
be judged by matching it to the analysis produced
by linguists in syntactic structures and semantic
representations created specifically for the frame-
work on which the grammar is based. A match or
a mismatch between analyses produced by differ-
ent parsers based on different frameworks does not
lend itself for a meaningful comparison that leads
to a fair evaluation of the parsers. To evaluate two
parsers across frameworks, two kinds of methods
suggest themselves:
1. Converting an analysis given in a certain for-
mat native to one framework to another na-
tive to a differernt framework (e.g. converting
from a CCG (Steedman, 2000) derivation tree
to an HPSG (Pollard and Sag, 1994) phrase
structure tree with AVM)
2. Converting analyses given in different
framework-specific formats to some simpler
format proposed as a framework-independent
evaluation schema (e.g. converting from
29
HPSG phrase structure tree with AVM to GR
(Briscoe et al, 2006))
However, the feasibility of either solution is
questionable. Even conversion between two eval-
uation schemata which make use of the simplest
representation of syntactic information in the form
of dependencies is reported to be problematic by
(Miyao et al, 2007).
In this paper, therefore, we propose a different
method of parser evaluation that makes no attempt
at any conversion of syntactic structures and se-
mantic representations. We remove the need for
such conversion by abstracting away from com-
parison of syntactic structures and semantic rep-
resentations. The basic idea is to generate a list
of names of phenomena with each parse. These
names of phenomena are matched against the phe-
nomena given in the gold standard for the same
sentence. The number of matches found is used
for evaluating the parser that produces the parse.
2 Research Problem
Grammar formalisms differ in many aspects. In
syntax, they differ in POS label assignment, phrase
structure (if any), syntactic head assignment (if
any) and so on, while in semantics, they differ
from each other in semantic head assignment, role
assignment, number of arguments taken by pred-
icates, etc. Finding a common denominator be-
tween grammar formalisms in full and complex
representation of syntactic information and seman-
tic information has been generally considered by
the NLP community to be an unrealistic task, al-
though some serious attempts have been made re-
cently to offer simpler representation of syntactic
information (Briscoe et al, 2006; de Marneffe et
al., 2006).
Briscoe et al(2006)?s Grammatical Rela-
tion (GR) scheme is proposed as a framework-
independent metric for parsing accuracy. The
promise of GR lies actually in its dependence on
a framework that makes use of simple representa-
tion of syntactic information. The assumption be-
hind the usefulness of GR for evaluating the out-
put of parsers is that most conflicts between gram-
mar formalisms would be removed by discarding
less useful information carried by complex syn-
tactic or semantic representations used in gram-
mar formalisms during conversion to GRs. But
is this assumption true? The answer is not clear.
A GR represents syntactic information in the form
of a binary relation between a token assigned as
the head of the relation and other tokens assigned
as its dependents. Notice however that grammar
frameworks considerably disagree in the way they
assign heads and non-heads. This would raise the
doubt that, no matter how much information is re-
moved, there could still remain disagreements be-
tween grammar formalisms in what is left.
The simplicity of GR, or other dependency-
based metrics, may give the impression that con-
version from a more complex representation into
it is easier than conversion between two complex
representations. In other words, GRs or a sim-
ilar dependency relation looks like a promising
candidate for lingua franca of grammar frame-
works. However the experiment results given by
Miyao et al(2007) show that even conversion into
GRs of predicate-argument structures, which is not
much more complex than GRs, is not a trivial task.
Miyao et al(2007) manage to convert 80% of the
predicate-argument structures outputted by their
deep parser, ENJU, to GRs correctly. However the
parser, with an over 90% accuracy, is too good for
the 80% conversion rate. The lesson here is that
simplicity of a representation is a different thing
from simplicity in converting into that representa-
tion.
3 Outline of our Solution
The problem of finding a common denominator for
grammar formalisms and the problem of conver-
sion to a common denominator may be best ad-
dressed by evaluating parsers without making any
attempt to find a common denominator or conduct
any conversion. Let us describe briefly in this sec-
tion how such evaluation can be realised.
3.1 Creating the Gold Standard
The first step of our evaluation method is to con-
struct or find a number of sentences and get an an-
notator to mark each sentence for the phenomena
illustrated by each sentence. After annotating all
the sentences in a test suite, we get a list of pairs,
whose first element is a sentence ID and second is
again a list, one of the corresponding phenomena.
This list of pairs is our gold standard. To illustrate,
suppose we only get sentence 1 and sentence 2 in
our test suite.
(1) John gives a flower to Mary
(2) John gives Mary a flower
30
Sentence 1 is assigned the phenomena: proper
noun, unshifted ditransitive, preposition. Sentence
2 is assigned the phenomena: proper noun, dative-
shifted ditransitive. Our gold standard is thus the
following list of pairs:
?1, ?proper noun, unshifted ditransitive, preposition? ?,
?2, ?proper noun,dative-shifted ditransitive? ?
3.2 Phenomena Recognition
The second step of our evaluation method requires
a small program that recognises what phenomena
are illustrated by an input sentence taken from the
test suite based on the output resulted from pars-
ing the sentence. The recogniser provides a set
of conditions that assign names of phenomena to
an output, based on which the output is matched
with some framework-specific regular expressions.
It looks for hints like the rule being applied at a
node, the POS label being assigned to a node, the
phrase structure and the role assigned to a refer-
ence marker. The names of phenomena assigned
to a sentence are stored in a list. The list of phe-
nomena forms a pair with the ID of the sentence,
and running the recogniser on multiple outputs ob-
tained by batch parsing (with the parser to be eval-
uated) will produce a list of such pairs, in exactly
the same format as our gold standard. Let us illus-
trate this with a parser that:
1. assigns a monotransitive verb analysis to
?give? and an adjunct analysis to ?to Mary? in
1
2. assigns a ditransitive verb analysis to ?give? in
2
The list of pairs we obtain from running the
recogniser on the results produced by batch pars-
ing the test suite with the parser to be evaluated is
the following:
?1,?proper noun,monotransitive,preposition,adjunct??,
?2, ?proper noun,dative-shifted ditransitive? ?
3.3 Performance Measure Calculation
Comparing the two list of pairs generated from the
previous steps, we can calculate the precision and
recall of a parser using the following formulae:
Precision = (
n
?
i=1
| R
i
?A
i
|
| R
i
|
)? n (1)
Recall = (
n
?
i=1
| R
i
?A
i
|
| A
i
|
)? n (2)
where list R
i
is the list generated by the recogniser
for sentence i, list A
i
is the list produced by anno-
tators for sentence i, and n the number of sentences
in the test suite.
In our example, the parser that does a good job
with dative-shifted ditransitives but does a poor job
with unshifted ditranstives would have a precision
of:
(
2
4
+
2
2
)? 2 = 0.75
and a recall of:
(
2
3
+
2
2
)? 2 = 0.83
4 Refining our Solution
In order for the precision and recall given above to
be a fair measure, it is necessary for both the recog-
niser and the annotators to produce an exhaustive
list of the phenomena illustrated by a sentence.
But we foresee that annotation errors are likely
to be a problem of exhaustive annotation, as is re-
ported in Miyao et al(2007) for the gold standard
described in Briscoe et al(2006). Exhaustive an-
notation procedures require annotators to repeat-
edly parse a sentence in search for a number of
phenomena, which is not the way language is nor-
mally processed by humans. Forcing annotators to
do this, particularly for a long and complex sen-
tence, is a probable reason for the annotation er-
rors in the gold standard described in (Briscoe et
al., 2006).
To avoid the same problem in our creation of a
gold standard, we propose to allow non-exhaustive
annotation. In fact, our proposal is to limit the
number of phenomena assigned to a sentence to
one. This decision on which phenomenon to be as-
signed is made, when the test suite is constructed,
for each of the sentences contained in it. Follow-
ing the traditional approach, we include every sen-
tence in the test suite, along with the core phe-
nomenon we intend to test it on (Lehmann and
Oepen, 1996). Thus, Sentence 1 would be as-
signed the phenomenon of unshifted ditransitive.
Sentence 2 would be assigned the phenomenon of
31
dative-shifted ditransitive. This revision of anno-
tation policy removes the need for exhaustive an-
notation. Instead, annotators are given a new task.
They are asked to assign to each sentence the most
common error that a parser is likely to make. Thus
Sentence 1 would be assigned adjunct for such an
error. Sentence 2 would be assigned the error of
noun-noun compound. Note that these errors are
also names of phenomena.
This change in annotation policy calls for a
change in the calculation of precision and recall.
We leave the recogniser as it is, i.e. to produce an
exhaustive list of phenomena, since it is far beyond
our remit to render it intelligent enough to select a
single, intended, phenomenon. Therefore, an in-
correctly low precision would result from a mis-
match between the exhaustive list generated by the
recogniser and the singleton list produced by an-
notators for a sentence. For example, suppose we
only have sentence 2 in our test suite and the parser
correctly analyses the sentence. Our recogniser as-
signs two phenomena (proper noun, dative-shifted
ditransitive) to this sentence as before. This would
result in a precision of 0.5.
Thus we need to revise our definition of preci-
sion, but before we give our new definition, let us
define a truth function t:
t(A ? B) =
{
1 A ? B
0 A ?B = ?
t(A ?B = ?) =
{
0 A ?B 6= ?
1 A ?B = ?
Now, our new definition of precision and recall
is as follows:
Precision (3)
=
(
?
n
i=1
t(R
i
?AP
i
)+t(R
i
?AN
i
=?)
2
)
n
Recall (4)
=
(
?
n
i=1
|R
i
?AP
i
|
|AP
i
|
)
n
where list AP
i
is the list of phenomena produced
by annotators for sentence i, and list AN
i
is the list
of errors produced by annotators for sentence i.
While the change in the definition of recall is
trivial, the new definition of precision requires
some explanation. The exhaustive list of phenom-
ena generated by our recogniser for each sentence
is taken as a combination of two answers to two
questions on the two lists produced by annotators
for each sentence. The correct answer to the ques-
tion on the one-item-list of phenomenon produced
by annotators for a sentence is a superset-subset re-
lation between the list generated by our recogniser
and the one-item-list of phenomenon produced by
annotators. The correct answer to the question on
the one-item-list of error produced by annotators
for a sentence is the non-existence of any common
member between the list generated by our recog-
niser and the one-item-list of error produced by an-
notators.
To illustrate, let us try a parser that does a good
job with dative-shifted ditransitives but does a poor
job with unshifted ditranstives on both 2 and 1.
The precision of such a parser would be:
(
0
2
+
2
2
)? 2 = 0.5
and its recall would be:
(
0
1
+
1
1
)? 2 = 0.5
5 Experiment
For this abstract, we evaluate ENJU (Miyao,
2006), a released deep parser based on the HPSG
formalism and a parser based on the Dynamic Syn-
tax formalism (Kempson et al, 2001) under devel-
opment against the gold standard given in table 1.
The precision and recall of the two parsers
(ENJU and DSPD, which stands for ?Dynamic
Syntax Parser under Development?) are given in
table 3:
The experiment that we report here is intended
to be an experiment with the evaluation method de-
scribed in the last section, rather than a very seri-
ous attempt to evaluate the two parsers in question.
The sentences in table 1 are carefully selected to
include both sentences that illustrate core phenom-
ena and sentences that illustrate rarer but more in-
teresting (to linguists) phenomena. But there are
too few of them. In fact, the most important num-
ber that we have obtained from our experiment is
the 100% success rate in recognizing the phenom-
ena given in table 1.
32
ID Phenomenon Error
1 unshifted ditransi-
tive
adjunct
2 dative-shifted di-
transitive
noun-noun com-
pound
3 passive adjunct
4 nominal gerund verb that takes
verbal comple-
ment
5 verbal gerund imperative
6 preposition particle
7 particle preposition
8 adjective with ex-
trapolated senten-
tial complement
relative clause
9 inversion question
10 raising control
Figure 1: Gold Standard for Parser Evaluation
ID Sentence
1 John gives a flower to Mary
2 John give Mary a flower
3 John is dumped by Mary
4 Your walking me pleases me
5 Abandoning children increased
6 He talks to Mary
7 John makes up the story
8 It is obvious that John is a fool
9 Hardly does anyone know Mary
10 John continues to please Mary
Figure 2: Sentences Used in the Gold Standard
Measure ENJU DSPD
Precision 0.8 0.7
Recall 0.7 0.5
Figure 3: Performance of Two Parsers
6 Discussion
6.1 Recognition Rate
The 100% success rate is not as surprising as it
may look. We made use of two recognisers, one
for each parser. Each of them is written by the
one of us who is somehow involved in the devel-
opment of the parser whose output is being recog-
nised and familiar with the formalism on which the
output is based. This is a clear advantage to for-
mat conversion used in other evaluation methods,
which is usually done by someone familiar with ei-
ther the source or the target of conversion, but not
both, as such a recogniser only requires knowledge
of one formalism and one parser. For someone
who is involved in the development of the gram-
mar and of the parser that runs it, it is straight-
forward to write a recogniser that can make use
of the code built into the parser or rules included
in the grammar. We can imagine that the 100%
recognition rate would drop a little if we needed
to recognise a large number of sentences but were
not allowed sufficient time to write detailed regular
expressions. Even in such a situation, we are con-
fident that the success rate of recognition would be
higher than the conversion method.
Note that the effectiveness of our evaluation
method depends on the success rate of recognition
to the same extent that the conversion method em-
ployed in Briscoe et al (2006) and de Marneff et
al. (2006) depends on the conversion rate. Given
the high success rate of recognition, we argue that
our evaluation method is more effective than any
evaluation method which makes use of a format
claimed to be framework independent and involves
conversion of output based on a different formal-
ism to the proposed format.
6.2 Strictness of Recognition and Precision
There are some precautions regarding the use of
our evaluation method. The redefined precision 4
is affected by the strictness of the recogniser. To
illustrate, let us take Sentence 8 in Table 1 as an
example. ENJU provides the correct phrase struc-
ture analysis using the desired rules for this sen-
tence but makes some mistakes in assigning roles
to the adjective and the copular verb. The recog-
niser we write for ENJU is very strict and refuses
to assign the phenomenon ?adjective with extrap-
olated sentential complement? based on the output
given by ENJU. So ENJU gets 0 point for its an-
swer to the question on the singleton list of phe-
33
nomenon in the gold standard. But it gets 1 point
for its answer to the question on the singleton list
of error in the gold standard because it does not
go to the other extreme: a relative clause analysis,
yielding a 0.5 precision. In this case, this value is
fair for ENJU, which produces a partially correct
analysis. However, a parser that does not accept
the sentence at all, a parser that fails to produce
any output or one that erroneously produces an un-
expected phenomenon would get the same result:
for Sentence 8, such a parser would still get a pre-
cision of 0.5, simply because its output does not
show that it assigns a relative clause analysis.
We can however rectify this situation. For the
lack of parse output, we can add an exception
clause to make the parser automatically get a 0 pre-
cision (for that sentence). Parsers that make unex-
pected mistakes are more problematic. An obvi-
ous solution to deal with these parsers is to come
up with an exhaustive list of mistakes but this is an
unrealistic task. For the moment, a temporary but
realistic solution would be to expand the list of er-
rors assigned to each sentence in the gold standard
and ask annotators to make more intelligent guess
of the mistakes that can be made by parsers by con-
sidering factors such as similarities in phrase struc-
tures or the sharing of sub-trees.
6.3 Combining Evaluation Methods
For all measures, some distortion is unavoidable
when applied to exceptional cases. This is true for
the classical precision and recall, and our redefined
precision and recall is no exception. In the case of
the classical precision and recall, the distortion is
countered by the inverse relation between them so
that even if one is distorted, we can tell from the
other that how well (poorly) the object of evalua-
tion performs. Our redefined precision and recall
works pretty much the same way.
What motivates us to derive measures so closely
related to the classical precision and recall is the
ease to combine the redefined precision and recall
obtained from our evaluation method with the clas-
sical precision and recall obtained from other eval-
uation methods, so as to obtain a full picture of
the performance of the object of evaluation. For
example, our redefined precision and recall figures
given in Table 3 (or figures obtained from running
the same experiment on a larger test set) for ENJU
can be combined with the precision and recall fig-
ures given in Miyao et al (2006) for ENJU, which
is based on a evaluation method that compares its
predicate-argument structures those given in Penn
Treebank. Here the precision and recall figures are
calculated by assigning an equal weight to every
sentence in Section 23 of Penn Treebank. This
means that different weights are assigned to dif-
ferent phenomena depending on their frequency in
the Penn Treebank. Such assignment of weights
may not be desirable for linguists or developers
of NLP systems who are targeting a corpus with a
very different distribution of phenomena from this
particular section of the Penn Treebank. For exam-
ple, a linguist may wish to assign an equal weight
across phenomena or more weights to ?interesting?
phenomena. A developer of a question-answering
system may wish to give more weights to question-
related phenomena than other phenomena of less
interest which are nevertheless attested more fre-
quently in the Penn Treebank.
In sum, the classical precision and recall fig-
ures calculated by assigning equal weight to ev-
ery sentence could be considered skewed from the
perspective of phenomena, whereas our redefined
precision and recall figures may be seen as skewed
from the frequency perspective. Frequency is rela-
tive to domains: less common phenomena in some
domains could occur more often in others. Our re-
defined precision and recall are not only useful for
those who want a performance measure skewed the
way they want, but also useful for those who want
a performance measure as ?unskewed? as possible.
This may be obtained by combining our redefined
precision and recall with the classical precision
and recall yielded from other evaluation methods.
7 Conclusion
We have presented a parser evaluation method
that addresses the problem of conversion between
frameworks by totally removing the need for that
kind of conversion. We do some conversion but
it is a different sort. We convert the output of a
parser to a list of names of phenomena by drawing
only on the framework that the parser is based on.
It may be inevitable for some loss or inaccuracy
to occur during this kind of intra-framework con-
version if we try our method on a much larger test
set with a much larger variety of longer sentences.
But we are confident that the loss would still be
far less than any inter-framework conversion work
done in other proposals of cross-framework evalu-
ation methods. What we believe to be a more prob-
34
lematic area is the annotation methods we have
suggested. At the time we write this paper based
on a small-scale experiment, we get slightly bet-
ter result by asking our annotator to give one phe-
nomenon and one common mistake for each sen-
tence. This may be attributed to the fact that he
is a member of the NLP community and hence he
gets the knowledge to identify the core phenom-
ena we want to test and the common error that
parsers tend to make. If we expand our test set
and includes longer sentences, annotators would
make more mistakes whether they attempt exhaus-
tive annotation or non-exhaustive annotation. It
is difficult to tell whether exhaustive annotation
or non-exhaustive annotation would be better for
large scale experiments. As future work, we intend
to try our evaluation method on more test data to
determine which one is better and find ways to im-
prove the one we believe to be better for large scale
evaluation.
References
Briscoe, Ted, John Carroll, and Rebecca Watson. 2006.
The second release of the RASP system. In Proceed-
ings of COLING/ACL 2006.
de Marneffe, Marie-Catherine, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
Proceedings of LREC 2006.
Kempson, Ruth, Wilfried Meyer-Viol, and Dov Gab-
bay. 2001. Dynamic Syntax: The Flow of Language
Understanding. Blackwell.
Lehmann, Sabine and Stephan Oepen. 1996. TSNLP
test suites for natural language processing. In Pro-
ceedings of COLING 1996.
Miyao, Yusuke, Kenji Sagae, and Junichi Tsujii. 2007.
Towards framework-independent evaluation of deep
linguistic parsers. In Proceedings of GEAF 2007.
Miyao, Yusuke. 2006. From Linguistic Theory to Syn-
tactic Analysis: Corpus-Oriented Grammar Devel-
opment and Feature Forest Model. Ph.D. thesis, Uni-
versity of Tokyo.
Pollard, Carl and Ivan A. Sag. 1994. Head-Driven
Phrase Structure Grammar. University of Chicago
Press and CSLI Publications.
Steedman, Mark. 2000. Syntactic Process. MIT Press.
35
