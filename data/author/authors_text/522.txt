Proceedings of the ACL Workshop on Feature Engineering for Machine Learning in NLP, pages 17?23,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Temporal Feature Modification for Retrospective Categorization
Robert Liebscher and Richard K. Belew
Department of Cognitive Science
University of California, San Diego
{rliebsch|rik}@cogsci.ucsd.edu
Abstract
We show that the intelligent use of one small
piece of contextual information?a document?s
publication date?can improve the performance
of classifiers trained on a text categorization
task. We focus on academic research docu-
ments, where the date of publication undoubt-
edly has an effect on an author?s choice of
words. To exploit this contextual feature, we
propose the technique of temporal feature mod-
ification, which takes various sources of lexi-
cal change into account, including changes in
term frequency, associative strength between
terms and categories, and dynamic categoriza-
tion systems. We present results of classifi-
cation experiments using both full text papers
and abstracts of conference proceedings, show-
ing improved classification accuracy across the
whole collection, with performance increases
of greater than 40% when temporal features
are exploited. The technique is fast, classifier-
independent, and works well even when mak-
ing only a few modifications.
1 Introduction
As they are normally conceived, many tasks relevant to
Computational Linguistics (CL), such as text categoriza-
tion, clustering, and information retrieval, ignore the con-
text in which a document was written, focusing instead on
the lexical content of the document. Numerous improve-
ments have been made in such tasks when context is con-
sidered, for example the hyperlink or citation structure of
a document collection (Cohn and Hofmann, 2001; Getoor
et al, 2001). In this paper, we aim to show that the intel-
ligent use of another dimension of context?a document?s
publication date?can improve the performance of classi-
fiers trained on a text categorization task.
Traditional publications, such as academic papers and
patents, have histories that span centuries. The World
Wide Web is no longer a new frontier; over a decade of its
contents have been archived (Kahle, 2005); Usenet and
other electronic discussion boards have been around for
several decades. These forums continue to increase their
publication rates and show no signs of slowing. A cur-
sory glance at any one of them at two different points in
time can reveal widely varying content.
For a concrete example, we can ask, ?What is Compu-
tational Linguistics about?? Some topics, such as ma-
chine translation, lie at the heart of the discipline and
will always be of interest. Others are ephemeral or have
reached theoretical upper bounds on performance. It is
thus more appropriate to ask what CL is about at some
point in time. Consider Table 1, which lists the top five
unigrams that best distinguished the field at different six-
year periods, as derived from the odds ratio measure (see
Section 3.2) over the full text of the ACL proceedings.
1979-84 1985-90 1991-96 1997-02
system phrase discourse word
natural plan tree corpus
language structure algorithm training
knowledge logical unification model
database interpret plan data
Table 1: ACL?s most characteristic terms for four time
periods.
While these changes are interesting in their own right
for an historical linguist, we aim to show that they can
also be exploited for practical purposes. We focus on a
fairly homogeneous set of academic research documents,
where the time of publication undoubtedly has an effect
both on an author?s choice of words and on a field?s defi-
nition of underlying topical categories. A document must
say something novel while building upon what has al-
ready been said. This dynamic generates a landscape
of changing research language, where authors and dis-
ciplines constantly influence and alter the course of one
another.
17
1.1 Motivations
Text Categorization (TC) systems are typically used to
classify a stream of documents soon after they are pro-
duced, based upon a set of historical training data. It is
common for some TC applications, such as topic tracking
(see Section 5.2), to downweight older features, or the
feature vectors of entire documents, while placing more
emphasis on features that have recently shown increased
importance through changes in frequency and discrimi-
native ability.
Our task, which we call retrospective categorization,
uses historical data in both the training and test sets. It is
retrospective from the viewpoint of a current user brows-
ing through previous writings that are categorized with
respect to a ?modern? interpretation. Our approach is mo-
tivated by three observations concerning lexical change
over time, and our task is to modify features so that a text
classifier can account for all three.
First, lexical changes can take place within a category.
The text collections used in our experiments are from var-
ious conference proceedings of the Association of Com-
puting Machinery, which uses a hierarchical classifica-
tion system consisting of over 500 labels (see Section
2). As was suggested by the example of Table 1, even if
classification labels remain constant over time, the terms
that best characterize them can change to reflect evolv-
ing ?meanings?. We can expect that many of the terms
most closely associated with a category like Computa-
tional Linguistics cannot be captured properly without
explicitly addressing their temporal context.
Second, lexical changes can occur between categories.
A term that is significant to one category can suddenly
or gradually become of interest to another category. This
is especially applicable in news corpora (see examples
in Section 3), but also applies to academic research doc-
uments. Terminological ?migrations? between topics in
computer science, and across all of science, are common.
Third, any coherent document collection on a par-
ticular topic is sufficiently dynamic that, over time, its
categorization system must be updated to reflect the
changes in the world on which its texts are based. Al-
though Computational Linguistics predates Artificial In-
telligence (Kay, 2003), many now consider the former a
subset of the latter. Within CL, technological and theo-
retical developments have continually altered the labels
ascribed to particular works.
In the ACM?s hierarchical Computing Classification
System (see Section 2.1), several types of transforma-
tions are seen in the updates it received in 1983, 1987,
1991, and 1998.1 In bifurcations, categories can be split
apart. With collapses, categories that were formerly more
fine-grained, but now do not receive much attention, can
1http://acm.org/class/
be combined. Finally, entirely new categories can be in-
serted into the hierarchy.
2 Data
To make our experiments tractable and easily repeatable
for different parameter combinations, we chose to train
and test on two subsets of the ACM corpus. One subset
consists of collections of abstracts from several different
ACM conferences. The other includes the full text col-
lection of documents from one conference.
2.1 The ACM hierarchy
All classifications were performed with respect to the
ACM?s Computing Classification System, 1998 version.
This, the most recent version of the ACM-CCS, is a hi-
erarchic classification scheme that potentially presents a
wide range of hierarchic classification issues. Because
the work reported here is focused on temporal aspects of
text classification, we have adopted a strategy that effec-
tively ?flattens? the hierarchy. We interpret a document
which has a primary2 category at a narrow, low level in
the hierarchy (e.g., H.3.3.CLUSTERING) as also classi-
fied at all broader, higher-level categories leading to the
root (H, H.3, H.3.3). With this construction, the most
refined categories will have fewer example documents,
while broader categories will have more.
For each of the corpora considered, a threshold of 50
documents was set to guarantee a sufficient number of in-
stances to train a classifier. Narrower branches of the full
ACM-CCS tree were truncated if they contained insuf-
ficient numbers of examples, and these documents were
associated with their parent nodes. For example, if H.3.3
contained 20 documents and H.3.4 contained 30, these
would be ?collapsed? into the H.3 category.
All of our corpora carry publication timestamp infor-
mation involving time scales on the order of one to three
decades. The field of computer science, not surprisingly,
has been especially fortunate in that most of its pub-
lications have been recorded electronically. While ob-
viously skewed relative to scientific and academic pub-
lishing more generally, we nevertheless find significant
?micro-cultural? variation among the different special in-
terest groups.
2.2 SIGIR full text
We have processed the annual proceedings of the Associ-
ation for Computing Machinery?s Special Interest Group
in Information Retrieval (SIGIR) conference from its in-
ception in 1978 to 2002. The collection contains over
1,000 documents, most of which are 6-10 page papers,
though some are keynote addresses and 2-3 page poster
2Many ACM documents also are classified with additional
?other? categories, but these were not used.
18
Corpus Vocab size No. docs No. cats
SIGIR 16104 520 17
SIGCHI 4524 1910 20
SIGPLAN 6744 3123 22
DAC 6311 2707 20
Table 2: Corpus features
Unlabeled Expected
Proceedings 18.97% 7.73%
Periodicals 19.08% 11.54%
No. docs 24,567 8,703
Table 3: Missing classification labels in ACM
summaries. Every document is tagged with its year of
publication. Unfortunately, only about half of the SIGIR
documents bear category labels. The majority of these
omissions fall within the 1978-1987 range, leaving us the
remaining 15 years to work with.
2.3 Conference abstracts
We collected nearly 8,000 abstracts from the Special In-
terest Group in Programming Languages (SIGPLAN),
the Special Interest Group in Computer-Human Interac-
tion (SIGCHI) and the Design Automation Conference
(DAC). Characteristics of these collections, and of the SI-
GIR texts, are shown in Table 2.
2.4 Missing labels in ACM
We derive the statistics below from the corpus of all doc-
uments published by the ACM between 1960 and 2003.
The arguments can be applied to any corpus which has
categorized documents, but for which there are classifi-
cation gaps in the record.
The first column of Table 3 shows that nearly one fifth
of all ACM documents, from both conference proceed-
ings and periodicals, do not possess category labels. We
define a document?s label as ?expected? when more than
half of the other documents in its publication (one confer-
ence proceeding or one issue of a periodical) are labeled,
and if there are more than ten total. The second column
lists the percentage of documents where we expected a
label but did not find one.
3 Methods
Text categorization (TC) is the problem of assigning doc-
uments to one or more pre-defined categories. As Section
1 demonstrated, the terms which best characterize a cate-
gory can change through time, so it is not unreasonable to
assume that intelligent use of temporal context will prove
useful in TC.
Imagine the example of sorting several decades of
articles from the Los Angeles Times into the cate-
gories ENTERTAINMENT, BUSINESS, SPORTS, POL-
ITICS, and WEATHER. Suppose we come across the
term schwarzenegger in a training document. In the
1970s, during his career as a professional bodybuilder,
Arnold Schwarzenegger?s name would be a strong indica-
tor of a SPORTS document. During his film career in the
1980s-1990s, his name would be most likely to appear in
an ENTERTAINMENT document. After 2003, at the out-
set of his term as California?s governor, the POLITICS
and BUSINESS categories would be the most likely can-
didates. We refer to schwarzenegger as a temporally
perturbed term, because its distribution across categories
varies greatly with time.
Documents containing temporally perturbed terms
hold valuable information, but this is lost in a statistical
analysis based purely on the average distribution of terms
across categories, irrespective of temporal context. This
information can be recovered with a technique we call
temporal feature modification (TFM). We first outline a
formal model for its use.
3.1 A term generator framework
One obvious way to introduce temporal information into
the categorization task is to simply provide the year of
publication as a new lexical feature. Preliminary exper-
iments (not reported here) showed that this method had
virtually no effect on classification performance. When
the date features were ?emphasized? with higher frequen-
cies, classification performance declined.
Instead, we proceed from the perspective of a simpli-
fied language generator model (e.g. (Blei et al, 2003)).
We imagine that the first step in the production of a doc-
ument involves an author choosing a category C. Each
term k (word, bigram, phrase, etc.) is accorded a unique
generator G
 
that determines the distribution of k across
categories, and therefore its likelihood to appear in cat-
egory C. The model assumes that all authors share the
same generator for each term, and that the generators do
not change over time. We are particularly interested in
identifying temporally perturbed lexical generators that
violate this assumption.
External events at time t can perturb the generator of k,
causing Pr(C|k  ) to be different relative to the background
Pr(C|k) computed over the entire corpus. If the perturba-
tion is significant, we want to separate the instances of k
at time t from all other instances.
Returning to our earlier example, we would treat
a generic, atemporal occurrence of schwarzeneg-
ger and the pseudo-term ?schwarzenegger+2003?
as though they were actually different terms, because they
were produced by two different generators. We hypoth-
esize that separating the analysis of the two can improve
19
our estimates of the true Pr(C|k), both in 2003 and in
other years.
3.2 TFM Procedure
The generator model motivates a procedure we outline
below for flagging certain lexemes with explicit temporal
information that distinguish them so as to contrast them
with those generated by the underlying atemporal alter-
natives. This procedure makes use of the (log) odds ratio
for feature selection:
	
ff