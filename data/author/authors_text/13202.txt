Morphological Rule Induction for Terminology Acquisition 
Bdatr ice  Dai l le  
IRIN, 2, rue de la Itoussinib, re. BP 92208, 44322 Nmll;es Cedex 3 France 
daille(@irin.univ-nantes.fl 
Abst ract  
We 1)res(;111; the identiti(:ation in corl)Ol"a of 
th:elM1 relatio11M adjectives (RAdj) such as 
gazc'uz (gaseous) which is derived from the noun 
gaz (.(/as). RAdj at)t)earillg in nonlinal phras('s 
are int('resting tbr ternlinology acquisition 1)e- 
cause they 11ollt a llalning flnl(:tion. The (leriw> 
tio11M rules emt)loyed to (:omt)ute the nora1 front 
which has been deriv('(t he RAdj are a(xluired 
s('nli-mltonmti(:ally fronl n t~Gge(t and ~l leln- 
matize(t (:orl)ora. q'hesc rules are then integr;tt- 
ell into ~t t('~rmer whi('h i(h',ntifies I{A(lj tlmnks 
to their 1)roi)el"ty of being paraphrasal)h; l)y 
a prepositionM phrase. RA(tj and comt)ound 
nouns which inchlde a I{A(tj m'e 1;111;11 (tuanti- 
fled, their linguistic precision is lneasured and 
their iutbrmative status is e.vahl;Lted thnnks to 
~ thesaurus of the. dOUlMn. 
1 In t roduct ion  
litelM\[ying relationM adje(:tives (l{.Adj) such a.s 
malarial, ~md 11Ol111 phrases in whirl1 they hi)- 
pear su(:h as malarial mosq'uitoc.s, could be iu- 
teresting in several tiehts (if NLP, such as ternli- 
nology acquisition, tot)i(: detection, updating of 
thesauri, tm(:mlS(; they hold ~t 11~mlillg flln(:tiou 
acknowledged t)y linguists: (Levi, 1978), (M61is- 
Pudluhl, 1991), (;to. The us(', of RAdj is par- 
ticularly ti'e(luent in scieutiti(: tields (Monceaux, 
1993). P~m~(loxically~ ternlinology acquisition 
systems uc\]l as TEI{MINO (David ~md Plant(': 
11,)90), LEXTER (Bourigault, 1992), TERMS 
(.hlsteson and Katz, 1995), have not 1)een con- 
(:erned with RAdj. Even (I1)ekwe-Sanjua, 1998) 
in her study of tern1 wMatiOllS for idelltit)illg 
research tot)its fi'onl texts does 11ot; take into 
account derivatio1111 WMmltS. Our (:ou(:ern is: 
1. 31) idelltit~y 1101111 phrases in wlli(:ll relation- 
al adje(:tives nt)l)ear, as well as the prel)o- 
sitiollM I)llrases l)y which they could 1)c 
t)ar;~l)llrase(t. We will see through anotll- 
(;1" source 1)resented iu section 2 that this 
t)l"Ot)erl~y of parai)hrase (;tin be used to i- 
dellti(y these adjectives. 
2. To check the naming character of these ad- 
je('tives and to evahlate the 11;ruling (:lmra(:- 
ter of the, noun 1)hras(;s in which they ;11)- 
l)em'. 
Moreover, i(hmtitlying both the a(tje(:tive ~ll(l 
the t)ret)ositional phrase is useflll ill th(; tMd of 
ternlinology a(:(luisition for t)eribrlning accurate 
tel'n1 llornlalization l)y grout)ing synonynL tbt'lnS 
referring to an uuique coneet)t such as p~vduit 
laitier (dairy prod'uct,) :re(l, pro&tit a'u lair (prod- 
'uc/; 'with, milk), pTvd,uit de lair (p'~vd',,ct of milh), 
p~vd'uit issv, du lair (p~wd,uct, made o.f milk), (;t(:. 
3\]) (:m'ry out this i(tentitic;ttion, we use shal- 
low t)m'Sillg (Almey, 1991), and then, tbr m()r- 
i)hologi(:al processing, a dymmli(: nlethod wllMl 
t~lkes ~s input n (:orl)us \]M)eled with t)m't-ol "- 
sl)eech and lellUUa l;;lgs. ~J)lle lnorl)hologicM 
rules m'e l)uilt selui-autonlaticMly ti'oln the (:or- 
l)US. 
I\]1 this stu(ty, we tirst defiue, and give some lin- 
guistic 1)roperties of RAdj. We then l)resent he 
method to build morphological rules mM how to 
integrate then into a ternl extractor. \?e qUall- 
tit~y the resullis ot)tailled fl'Oln a te(:hnical eorl)us 
in the tield of agriculture \[AGII,IC\] and evaluate 
their linguistic mid int'or111~tive precision. 
2 L ingu is t i c  p roper t ies  of  re lat ional  
ad ject ives  
Ac(:ording to linguistic and gralnlnaticM tradi- 
tion, there are two nlain categories aUlOllg adjec- 
tives: el)ithetic slM1 as important (,sign'~ificant) 
and relatio11M adjectives uch as laitier Malty). 
The tirst ones cannot \]l~ve an ~gentive interl)re- 
215 
ration in contrast to the second: tile adjective 
laiticr (dairy) within the uoun phrase pr'oduc- 
lion laiti~re (dairy production) is an argument 
to the predicative noun production (production) 
and this is not the case fbr the adjective impof 
tant (significant) within the phrase production 
importante (significant production). Relation- 
al adjectives (RAdj) possess the following well- 
known linguistic properties: 
? they are either denonfinal adjectives - -  
morphologically derived from a noun 
thanks to suttix--, or adjectives having a 
noun usage such as mathdmatique (math- 
cmatical/mathcmatics). For the former, 
not all the adjective-tbrming sufiqxes lead 
to relational adjectives. The following suf- 
tixes are considered by (Dubois, 1962) as 
appropriate:-ain, -air'e, -al, -el, -estr'c, 
ien,-icr',-il(e),-in,-ique. However, (Guy- 
on, 1993) remarks that a suffix, even 
the most appropriate, is never necessary 
nor sufficient. Several adjectives carry- 
ing a favorable suffix are not relationah 
this is the case with the adjectives ending 
with -iquc (-ic), which characterize chem- 
istry and which are not derived from a 
noun, such as ddsox!/ribonucldique (deoryri- 
bonucleic), dodecanoiquc Modecanoic), etc. 
Other suffixes inappropriate are sometimes 
used such as the suffixes -d and -e'a:~:: car- 
bone &,'bon) -* car'bon~J (~.a,'bo,,,,eeo'a,~), 
c,,,,ce," #a,,,cer9 + ca,~c~r'e'a:~ &ancc','o'~,~), 
etc. 
? they own tile possibility, in special condi- 
tions, of replacing tile attributiw'~ use of 
a corresponding prepositional phrase. The 
preposition employed, as well as tile pres- 
ence or not of a deternfiner, depends on the 
head noun of the noun phrase: 
aciditd sanguine (blood acidity) ~_ aciditd 
du sang (acidity of the blood) 
conqugtc spatiale (space conquest) ~_ con- 
qu~tc de l'espace (conquest of space) 
ddbit horairc (hourly rate) ~- ddbit par' 
heure (rate per" h, our) 
cxpdrimentations animales (animal experi- 
mentation) ~ cxpdrimcntations sur lea an- 
imaux (experimentation on animals) 
? and several other properties uch the im- 
possibility of a predicative position, the ill- 
compatibility with a degree modification, 
etc. 
3 Morphological Rule I nduct ion  
~lb identify RAdj trough a term extractor, we 
use their paraphrastic property which inchldes 
the morphological property, the morl)hological 
property being insufficient alone. We need rules 
to recover the lemma of the noun fl'om which the 
lemma of the RAdj has been derived. 
These rules tbllow the tbllowing schemata: 
r~ = \[-S +M \]{exceptions} where: 
S is the relational suffix to be deleted from the 
end of an adjective. The result of this dele- 
tion is the stem R; 
M is the mutative segment o be concatenated 
to R in order to tbrm a noun; 
except ions  list the adjectives that should not 
be submitted to this rule. 
For example, the rule \[-d -l-e \]{agd} says that 
if there is an adjective which ends with d, we 
should strip this ending from it and append tile 
string c to tile stem except if this a4jective be- 
longs to tile list of exceptions, namely agd. 
We extract these mort)hological rules Kom 
the corpora following the method presented in 
(Mikheev, 1997) with the difl'erenee that we 
don't limit the length of the mutative segmen- 
t. The relational suffixes are known, only the 
nmtative segments have to be guessed. For tlm 
lemma of an adjective ending with a relational 
suffix in the corpus Adji, we strip this suffix of 
Adji and store the resulting stem ill R. Then, 
wc try to segment this stein R to each noun 
Nounj at)pearing in the corpus. If the subtrac- 
tion result in all non-empty string, the system 
creates a morphological rule where tile muta- 
tive segment is tile result of the subtraction of 
R to Nounj. We thus obtained couples (Ad.ii, 
Nounj) associated to a morphological rule. For 
example: (gazeux, gaz) \[-cux +""\]. 
This schemata doesn't take into account stem 
alternants uch as: 
el6 alphabe t/ aph, abd t-ique 
~/~ hygi~ ne/hygidn-ique 
e/ i  polle n/polli n-ique 
x /c  th, orux / thorac-ique 
216 
In order to h~mdle this alh)mort)hy, we, use the 
Lcvenshtein's weighted distance (l ,cvenshtcin, 
1.966) which determines the min imum numl)e,r of 
insertions or deletions of characters to transfor- 
m one word into another. (Wagner and Fisher, 
1974) presents  n re(:nrsive ~dgorithm to (:ah:ulate 
this dist~mcc. 
? ! 
&.s t ('w ~,i, 'w~ ,j ) = 
min(di.st (w~ ,i-~, ""~ ,j) + q, 
? ~ di,~t(wi,i, 
with w~,m 1)eing the substring t)egimfing nt tlm 
1l I'h' C\]I}II'}I, CtCI" }~ll(1 tinishing after tim mth char- 
acl;(;r of the word w, 
d is@c,y )  = 1 i . f : c - -y  
= 0 i f  :~: ? y 
and 
q cost; of the, inserl;ion/de, h',tion of one, character 
p cost of" t;he sul)stitution of one (:h;~racter |)y 
~mothcr. 
Generally, a subst i tut ion is (:onsidcr(~,d as a dch~- 
lion fi)llowed 1)y ;m insertion, thus I ) -- 2(1? Wc 
apply this alg()rithm to e,a(:h stem 1{, ()l)tahm(t 
;d'te, r the (h~letion of tim r(~,lational suffix, that  
had not; 1)c(m found ~s a stem ()f n l lOll l l . \]~llt,, 
we add the constraint hat l/. ~m(1 the n(mn must 
share the same, two first; characters, i.e. the sul)- 
string comput(:d t)cgin at character 3. We only 
rel;~fin cout)les comi)oscd of ml ~uljectivc and a 
noun with it Levenshtcin's w(;ightcd e(tual (;o 3 
(i.e. one sul)stitutiol~ + one insertion) . From 
the, se tout)los, wc dcdu(:c new rel;~tional suffix- 
cs to l)c ~ulded to list; of ~dlowc, d sullixes. More, 
1)re('iscly, we (:onsidcr theft such suffixes are, al- 
lomorphic w~rbmts of the relation suffixes. Wc 
also add new mort)hok)gic;d ruh',s. For cxam- 
ple, for the couple (hygi&t,c, hygidniquc,), we add 
the suffix -~niquc which is conside, red as an al- 
lomorph of the sutfix -iquc, mid creatc tim rule: 
\[-&t, ique +&~,e\]. However, this method doesn't  
rc,~ricve, RAdj lmilt from non ~mtonomous t)ascs 
of such 
nor from Lat in noun 1)ases such as ph'r(,./patc/r 
(fathen'/patcr), vill@urb (tov,,,/,~rb). 
We check m~mmflly the rules ot)tained and 
ll,elational Number of Number 
Suffix allomorphs of rules 
-al 3 5 
-airc 4 8 
-d 2 2 
-d  1 2 
-or 1 2 
-cu:c 1 3 
-ion 1 2 
-i~:r 1 2 
-if 2 6 
- in 1 2 
-iquc 8 18 
-isle 1 1 
-cite 1 1 
Total 25 54 
Figm:c l: Numl)er of varimlts mid rules 1)y rel;L- 
t iomd suffix 
added to the list; of cxccptions thc wrong (lcriva- 
lions obtain(',(l. %d)\]c I prescnt, s tim 1mini)or of 
rules r(',t~dn(xt nn(t the mnubcr  of v~riants fl)r 
(~(:11 suffix. 
4 Term Ext ractor  
First, we present the tcrm e, xtr~mtor ('hosen 
the, n, the modif ications perfi)nn to enable the 
al)l)li('ation of the dcriw~tional rules. 
4.1  I n i t ia l  Term Ext ractor  
ACAB\ ]T  (\])ailh~, 1996), the term cxtra(:tor used 
ti)r this (!xt)(',rim(mt; eases I;he task ()f t;he, t;ernli- 
no\]ogist l)y proposing, \['or ;~ given (:orl)uS , a, list 
of (:mldi(l~tc terms ranked, from the most rei)- 
rcscnl;ativc of the domain to the lc:~sl; using a 
st~tistical score. Can(lid~tte terms whi(:h are cx- 
tr;tctcd fl:om the corlms t)elong to a Sl)CCiM type 
of cooc(:m:rcnces: 
? the cooc(:urrcn(:c is oriented and follows the 
l incar ordcr of the text;; 
? it; is ('Oml)OS(,xl of two lexi(:al milts whi('h (lo 
not l)elong to the, (:lass of functional words 
such as prcl)ositions, articles, etc.; 
? it m~tchcs one of the morphosyntact ic  pat- 
tcrns of wh~Lt wc will (:all "l)~se terms",  or 
one of their t)ossible vm'iations. 
The l)atterns for base \[;CI'IlIS arc: 
Noun1 Ad j  cmballagc biod@radablc 
(biodcqradabh: packag(;) 
Noun1 Noun2 ions calcium 
217 
Noun l  (Prep (Det) )  Noun2 ions calcium 
(calcium ion) pTvtdine de poissons (fish 
protein), ehimioprophylaxie a'u r'~fa~n, pine 
(riJhmpicin chemoprophylazis) 
Noun1 5_ V in f  v iandes  ~t 9riller (grill meat) 
These base structures are not frozen structures 
and do accept several variations. Those which 
are taken into account are: 
1. Inflexional and Internal morphosyntactic 
variants: 
? graphic and orthographic variants 
which gather together predictable in- 
flexional variants: conservation de 
p~vduit (product preservation), conser- 
vations de p'rvduit (product preserva- 
tions), or not: conservation dc prod'ait- 
s (products preservation) and ease d i f  
ferences. 
? variations of the preposition: eh, w- 
matographie en colonne (column 
chrwnatography), chromatographic sur 
colonne (chrvmatograph, y on col'area); 
? optional character of the preposition 
and of the z~rticle: fixation azote (hi- 
trogen fization), fixation d'azote (fiz- 
ation of nitrogen), fi.~:ation de l~azote 
(fization of the nitrogen); 
2. Intermfl modification variants: insertion in- 
side the base-term structure of a modifi- 
er such as the adjective inside the Noun1 
(Prep (Det)) Nom~2 structure: lair de bre- 
bis (goat's milk), lait cru de brebis (milk 
straigh, t .from the goat); 
3. Coordinational w~riants: coordination of 
base term structures: alimentation hu- 
maine (human diet), alimentation animale 
et hnmaine (human and animal diet); 
4. Predicative variants: the predicative role of 
the adjective: peetinc mdthylgc (mcthylate 
pectin), cos pectines ont m6thyldes (these 
pectins are metylated). 
The corpus is tagged and lemmatized. The pro- 
gram scans the corpus, counts and extracts col- 
locations whose syntax characterizes base-terms 
or one of their variants. This is done with shal- 
low parsing using local grammars based on reg- 
ular expressions (Basili et al, 1993). These 
grammars use the morphosyntactie information 
associated with the words of the corpus by the 
tagger. The different occurrences are grouped 
as pairs formed by lemmas of the candidate ter- 
m and sorted following an association measure 
which takes into account the frequence of the 
COOCCtlrrOllCeS. 
4.2 Term Ext rac tor  mod i f i ca t ions  
The identilication of relational adjective takes 
place afl;er extraction of the occurrences of the 
candidate terms and their syntactic variation- 
s. The algorithm below resmnes the successive 
steps tbr identifying relational adjectives: 
1. Examine ach candidate of Noun Adj struc- 
ture; 
2. Apply a transtbrmational rule in order 
to generate all the possible corresponding 
base nouns. We added morphosyntactie 
constraints for some suffixes, such as tbr 
the suffix -er, that the identitied adjective 
is not a past-participle; 
3. Search the set of candidate terms tbr a pair 
formed with Nomtl (identical between a 
Noun1 (Prep (l)?t)) Nou,~2 and a Noun1 
Adj structures) and Noun2 generated from 
step 2. 
4. If step 3 succeeds, group the two base struc- 
tures mlcter a new candidate term. Take 
out all the Noun Adj structures owing this 
adjective from the set; of Noun Adj candi- 
dates and rename them as a Nomt RAdj 
structure. 
I11 Step 2, morl)hoh)gical rules generate one or 
several nouns tbr a given adjective. We gener- 
ate a notllt for each relational suffix class. A 
class of suffixes includes the allomorphic vari- 
ants. This overgeneration method used in in- 
forlnation retrieval by (aacquemin and Tzouk- 
ermann, 1999) gives low noise because the base 
noun must not only be an attested for in the 
corpus, but must also appear as an extension of 
a head noun. For exanti)le, with the adjective 
ioniqne (ionic), we generate both ionic ('ionia) 
and ion (ion), but only ion (ion) is an attested 
tbrm; with the adjective gazeux (gaseous), the 
noun forms gaz #as) and gaze #auze); are gen- 
erated and the two of them are attested; but, 
the adjective gazeux (gaseous) appears with the 
218 
Nmnber  of oc(:urrences 1 > 2 Total  
1)ase slir~l(:l;ures 
Nora1 Prep (\])et) Nora2 17 232 5 949 23 181 
Nora Adj 12 344 4 778 17 122 
Nora h Vinf 203 16 219 
'.FoCal 29 912 10 895 40 807 
Figure 2: Quant i tat ive (bfl;a on 1)nse, stru(:tures 
llOllll dchange (ezch, ange) whi(:h is t)aral)hrased 
in the tort)us t)y dchangc de gaz (.qa.s ezchange) 
and not by ~.changc de gaze (gauze exehanftc). 
I,i)r adjectives with a mmn fimction, as for ex- 
ample pwbldmc technique (te.ehnical pTvblem) 
and Frobl&nc de tech.nique~ (pwbh:m of tech- 
7~,ics), we tl;tve ac(:el)ted th~tt ~t (:;m(ti(l~te term 
(:ouhl share several base stru(:tur('.s: on(; ()f type 
Nounl (Prep (l)et)) No,m2 and ;mother of type. 
N(mnl Adj. No comtmtalfion is n(;('.(lcd to see 
that  Noun2 as Noun2 and Adj shin'(; the s;une 
1CIlSIlI~L 
5 Resu l ts  and Eva luat ion  
Ore: corI)us, (:alled \[AGRIC\], is made up of 7 272 
aJ)str;tcts (/130000 wor(ls) fronl th'en(:h texts 
in tlm ~tgri(:ulture (tomnil~ mM extra(:te(t from 
PASCAL. We used 1;t5(; Brill t)a.rt-ofSt)ee(:h Tag- 
ger (Brill, 1992) trained for l,?en(:h by (Le(:olntc~ 
and Pm'out)ek, 1996)) and the lelmnatizer (h> 
veh)ped t)y F. Na.mer (\[Ibussaint et M., 1998). 
5.1 Quant i ta t ive  resu l t s  
q_~d)le 2 resmnes the mmfl)er of l)ase stru(:tures 
extr;mted from \[AGRIC\] corlms. \]q:om these 
t)ase structures, 395 groul)ings were identitied. 
The linked presence of noun l)hrases of which 
the extension is fultilled either 1)y a rebttional 
adjective, or l)e a l)rel)ositional phrase the nmn- 
ber is rare - -a  l itt le bit more than 1. % of the 
tol;al of occurrence, s- . B15t, these groupings al- 
low us to extract from the 5mmerous hal);,x - -  
more than 70 % of l;he totM of occurrences 
candidates which, we presu5ne, will t)e, highly 
denonfinative and to increase the numt)er of oc- 
currences of a candidate term. The mmfl)er 
of relational adjectives which h~ve l)een identi- 
fied is 129: agTvnomique (agTvnomical), alimen- 
tai,'c, (fl, od), araeh, idier (groundn,,d), aromatiq'ac 
(arow, atie), etc. 
5.2 L ingu is t i c  P rec i s ion  
We chc(:k(;d tim linguistic accuracy of the 395 
structural  wu'iations which group ~ Noun1 Prep 
(Det) N(mn2 structure ~md a Nounl  RAd- 
j structure. Reported errors COlmern 3 inco f  
re('t groupings due to 1;15('. homograi)hy , and 
the non homonymy, of the adjective ;tn(l the 
noun: fin gh, in (A@/(,',,d (Nou@), ,:o,a'ra,> 
t (ordi,,,ary(Adj)/e'm're.nt(Nov, n)), potentiel (po- 
tential). This lead us to a linguisti(" i)rc(:i- 
sion of more than 99 % in the identitication 
of relational adjectives. As ~ matter  of com- 
1)arison, (Ja(:quenfin, 1999) obtained a pr(:(:i- 
s ion of 69,6 % for the Nora5 to Adj morl)hO- 
synl, tmti(: wtriations (:M(:ulat(',d according to the 
morl)hologi(:M fimfilies l)roduced 1)y ~ sl;enl- 
ruing algorithm al)l)lied to the MUI;.I)F, XT lex- 
i(:;d datM)ase (MUIT.13'3XT, 1998) on the StLllle 
French corpus \[AGRIC\]. 
5.3 In fo rmat ive  P rec i s ion  
The thes~mrus (AGI/,()V()C, 1998) is ~ taxono- 
my of M)out 15 000 terms ;~ssocbtted with syn- 
onyms in n SGML fi)rm;~t, which leads to 25 964 
(tiff('xent terms. AGROVOC is used for indexing 
with (l~tta tittillg ;tgri(:ultural retriev;tl syst('.lliS 
and indexing syst(mlS. \~e lna(le two ('Oml)~tr- 
is(ms with AGI/OVOC: we tirst (:h(;(:k('A whetllcr 
thc.se RA(tjl~. were re.ally t)~rt of terms of it ml(t 
se(:oll(l, we colnt)~re(t the c~mdi(t,~te rlllS ex- 
tracted with a I/.A(lj with its terms. We ('onsi(t- 
or |;hat the t)resence of the I/,A(tj in AGR,()VOC 
(:ontirms its informative character, mM th}tt the 
l)resen(:e of a (:an(li(late t(;rm ~ttests its termi- 
nological wtlue. 
5.3.1 Re la t iona l  ad jec t ives  a lone  
Fronl the 124 correct RAdj,  68 appear insid- 
e terms of the thesaurus in epithetic 1)osition, 
and 15 only under their noun tbrm in an exten- 
sion position, for exmnple arach, idier (ground- 
n'at) does not appear but arach, ide is used in an 
extension position. Moreover, among the 124 
adjectives, 73 appear in AGROVOC under their 
noun term as mfitenns. The adjectives which 
are not l>resent ill the thesaurus in an extension 
t>osition tamer either their adje(:tiwfl or n<mn 
form are 11 in mmflmr. So 93% of them m'e 
indeed highly inf'ormtLtive. 
219 
5.3.2 Cand idate  terms w i th  a re lat ional  
ad ject ive  
Pour 9 AdjR belonging to AGROVOC, we com- 
pute the tbllowing indexes: 
TA tile number of terms in AGROVOC in 
which tile relational adjective appears in an 
epithetic position, i.e. the terms of Noun 
RAdj structure. Fox" example TA=15 tbr 
the adjective cellulairc (eellular) because it 
appears in 15 terms of AGROVOC such 
as di./~renciation cellulairc (cellular differ'- 
enciation), division cclIulaire (cellular divi- 
sion). 
TN the number of terms in AGROVOC in 
which the noun from which has 1)een de- 
rived the relational adjective appears in- 
side ~ prepositional phrase, i.e. the terms 
of Nounl  Prep (Det) Nounl~Adj structure. 
For example TN=4 tbr the noun eellulc 
(cell) because it appears in 4 terms of A- 
GROVOC such as banque de ccllulcs (cell 
bank), c'alt'a,'e de ecUules (e~tlt~u'e of cells). 
C A the number of candidate terms of Noun 
RAdj structure. For example, CA=61 for 
the adjective celluIaire (cellular) because it 
appears in 61candidate terms such as acidc 
cellulaire (cellular acid), activitd cell'alaire 
(cclluhtr activity), agr@at cell'ulaire (ccll'a- 
la'r aggregate). 
C N the munber of candidate terms of Noun1 
Prep (Det) NounltAd j structure.  For exam- 
ple CN=58 tbr the noun eellule (cell) be- 
cause it appears in 58 candidate terms such 
as ADN de cellule &ell DNA), addition de 
cellules (cell addition). 
Then, tbr each candidate term of CA and CN, 
we checked tbr their presence in AGROVOC. 
Tile only matches that we have accepted are 
exact matches. With this comparison, we ob- 
tained the following indexes: 
a the number of candidate terms of Noun RAdj 
structure tbund in AGR.OVOC under the 
Noun RAdj structure. 
b the number of candidate terms of Noun RAdj 
structure tbund in AGROVOC muler the 
Nounl Prep (Det) NounlIAdj structure. 
Noun RAdj N1 Prep (Det) NIIA4i 
Precision 0,34 {},{}4 
Recall 0,46 O, 14 
Figure 3: Averages of precisions and recalls 
c the number of candidate l;erms of Nounl 
Prep (Det) Nounl~Adj structure found in A- 
GROVOC under the Noun RAdj structure. 
d the number of candidate terms of Nounl. 
Prep (Det) Noun~Adj structure found in 
AGROVOC under the Noun1 Prep (Det) 
NounRAdj structure. 
These indexes allow us to compute precision 
P and recall R for each Noun RAdj structure 
and each Noun1 Prep (Det) Noun~Adj structure 
with the help of the fbllowing tbrmula: 
((,, + b) 
I'No~,~A~j -- C~ (1) 
+ d) (2) 
aNounPrep(Del.)Nounl~A,lj -- CN 
(a + t,) (3) ~NounRAdj  -- TA 
(c +d) 
l~,Nounl)rep(Det)Nounl?A4i -- TAr (4) 
The averages of precision and recall for the t- 
wo structures are summarized in table 3. This 
comparison of the average of precision comput- 
ed shows that candidate terms with a Noun 
RAdj structure are 10 times more likely to be 
terms than their eqniwflent in Nounl Prep (De- 
t) Nounl~.Adj. The analysis of the average of re- 
call is also impressive: it is generally difficult to 
obtain a recall sut)erior to 25 % when comparing 
candidate terms extracted from a corpus and 
a thesaurus of the same domain (Daille et el., 
1998). The average of recalls obtained thanks 
to the identification of RAdj shows that nearly 
half of the terms lmilt with the defined RAdj are 
identified. These good wflues of precision and 
recall have been obtained on linguistic criteria 
only without taking into account frequency. 
6 Conc lus ion  
Tile method proposed in this study to acquire 
morphological rules fl:om corpora in order to re- 
cover derivational term variations trough a ter- 
m extractor and identi(y relational adjectives 
220 
shows an excellent I)recision. We h~v(; Mso 
proved that noun l)hrases including a l l,Ad.i arc 
fitr more infornlativ(; l;hmt their equivMent in 
Nounl Pre 1) (Det) Nounlbb/j stru(;ture. \?c still 
h~we to write the program whose task will t)e to 
merge, new mort)hologicM rules ttcquire, d Kom 
another (:orlms with t\]le existing Olle, S. 
I~eferences  
S. A1)n(;y. 1991. l~&rsing with (:hunks. In 
R. Berwi('k mid C. Tcnny, extitors, Principh;- 
Base Parsing, I)agcs 257 278. Kluwer Aca(h;- 
too(: Pul)lishers. 
AGR()VOC, 1998. A GI~OVOG'- M'altiling'aal 
Agricult'mul Th, c,.s'a'aru.s', l?ood and Agricul- 
tural ()rganiz~tion of the United N;~tions. 
httl)://www.f~u/.org. 
l{.ol)crto Basili, Mm:b~ 'l.bresa l)azienza, mM 
l)aob~ Velar(li. 1993. Acquisition of Selc(:tiolF 
al PaA, terns in Sul)lmlgu~gcs. Math, in('. 7;ran- 
lation,, 8:175 201. 
l-)idier Bom:igmflt. :1992. Surface grmmnnti(:a.1 
anMysis for the extr~u:tion of t(:rminoh)gi(:M 
noun t)hrases. In COLING~'92, pages 977 
981, Nantes, Frmme. 
F, ric Brill. 1992. A siml)h'~ ruh',-1)ased par|; of 
st)eech t~gg(;r. In ANLP'g2, pages 152 155, 
Trcnl;o, mar(:h. 
Bd~d;ri(:(,' l)Mlle, Eri(: Ga.ussier, ;m(l .le, ml-Mm'(: 
LanK& 1998. An (',wduati()n ()f statisti(:al 
s(;or(~s fOl' Wolxl ass()(:inti()n. In .lonathan 
(finzt)urg, Zm'al) Kha.si(tashvili, C:u'l Vogel, 
&;;m-,\]a(:(tues Ldvy, ~md Era'i(: Va.llduvi, ed- 
itors, 77~,e 7'blisi Symposium on l,ogic , Lan- 
g'uafle and Computation: ,~clccl,('d Papers, 
pnges 1177 188. CSLI Publications. 
Bdatrice \])Mlle. 1996. Study ;rod imt)l(',menta- 
tion of ('onfl)in(;(l techni(tue, s for ;mt()nl~ti(: ex- 
traction ()f terminology. In Judith l~. l(bwan- 
s and Philil) Rcsnik, (;ditors, The, Bala'aci'nfl 
Act - Combining Symbolic and Statistical Ap- 
proach, es to Language , (:hal)ter 3, t)~ges 28 49. 
MIrl? \]?tess. 
Sot)hie David and 17. Plante,. 1990. L(; 1)rogi- 
(:iel tcrmino : l)e, la ndc(;ssit;d (l'mie, ml~lyse 
morphosyntaxique pour le ddt)ouillement ter- 
minologique, des textes. In lCO, volume 2. 
,l. Dul)ois. 1962. Etude s'ar ht ddrivation suf- 
.fixale (',',, F'ra',,~:ai.~' 'm, odcrne ~:l, co'nicmi)orain. 
Lm:oussc, Paris. 
Anne, Guyon. 199"1. Lt's adjeet'(fs r('Jalion',,t',ls 
arguments de noms pre~dieat@. Ph.D. thesis, 
Univea'sitd Paris 7. 
Fidelin l\])ekwe-Snltjun. 1998. Ternfinologi(:al 
variation, a mean of identitlying research tot)- 
its from texts. In COLING-ACL'98, vol- 
rune 1, t)t~g(;s 564 570, MontrM, Canada. 
Christian ,la(:quemin mM Evelyne Tzoukerman- 
n. \]999. Np1 tbr term variant extra('tion: 
Syn(;rgy between mort)hoh)gy, lexicon ~md 
synt~x. In T. StrzMkowski, editor, Nat, u- 
ral Language Processing and IT~:formation Re- 
trieval. Kluwer, Boston, MA. 
Christian .hu:(tuenlin. 1!199. Syntagmati(: nnd 
l)m'~Migmati(: l{.el)resentation f Term V~ria- 
tion. In A6'1)'99, University of Marylnnd. 
,l. Justeson ;rod S. K;tl;z. 1995. Technical ter- 
minology: Some linguistic l)roperties mM ml 
Mgorithm for id(mtitic~tion in text. \]ill ,lour- 
'hal fff Li',,g'H, isbh: Enflinecri'n,9, volum(; \]. 
.\]os(;l;t(', Le('omtc ~11(t Patri(:l{ 1)nr()ul)e,k. 1996. 
l,e (:at(goris(',ur (t'(;ri(: t)rill, raise (',n (mlvr(', (le 
la version (;ntr:md(; n l'imdt'. ~lb, t:hlfical tel)Oft ,
CNllS-INAIAL 
V.I. l~e,v(msht(;in. 1966. Binary (:ode, s cat)al)le of 
(:orr(;('ting deletions, insertions mM l"eversa\]s. 
Soy. \])h, ys.-Dokl., 10(8):707 710. 
Judith Levi. 1978. 7'he .syntaz and the seman- 
tics of complez 'nominals. A('adenfi(: Press, 
I~on(lon. 
A. Mdlis-1)u(:hulu. 1!)91. Les adj(;('tit~ 
ddnomina.ux : (h;s ~utje(:titls ie "r(,J~ttion". 
\],c:riq.uc, 10:33 60. 
An(h'ei Mikhe(',v. 19!)7. Autonl~Lti(: rule, iu(tu(:- 
tion for unknown-word guessing. Comp'ata- 
l, ionaI Linguistics, 23(3):405 423. 
Mine Moncemlx. 1993. La .formation des 'sore- 
s composds de str'act'are NOM ADJECTI?.  
Thb, s(; (le do(:tornt en linguisl;ique thdorique, 
et formcllc, Universitd de Mm:nc 1~ Valid(;. 
MULTEXT, 1998. \]~M)or~toire Pa.role et Ira.n- 
gag(;, httl):/ /www.ll)l.univ-aix.fr. 
Ymmi(:k Toussaint, l.'imnetta Nalner, Bdatrice 
l)aille, Christian ,\]a~c(tuentin , .\](;all l{oymd:d, 
mM Nal)il llIathout. 1998. Une api)roche 
linguistique et stntistique 1)ore: l'mmlyse de, 
l'informntion (',n corpus. In TALN'98, pages 
182 191, Pro'is. 
R.A. W~tgn(;r mid M.,J. Fisher. 1974. The 
string-l;o-sl:ring corre,(:tion l)rol)le, m. ,Journal 
of th, c Association .for Computing Machinery, 
21 (1):168 173. 
221 
French-English Terminology Extraction from
Comparable Corpora
Be?atrice Daille and Emmanuel Morin
University of Nantes, LINA - FRE CNRS 2729,
2, rue de la Houssinie`re - BP 92208, 44322 Nantes Cedex 3, France
{beatrice.daille, emmanuel.morin}@univ-nantes.fr
Abstract. This article presents a method of extracting bilingual lexica
composed of single-word terms (SWTs) and multi-word terms (MWTs)
from comparable corpora of a technical domain. First, this method ex-
tracts MWTs in each language, and then uses statistical methods to
align single words and MWTs by exploiting the term contexts. After ex-
plaining the difficulties involved in aligning MWTs and specifying our
approach, we show the adopted process for bilingual terminology ex-
traction and the resources used in our experiments. Finally, we evaluate
our approach and demonstrate its significance, particularly in relation to
non-compositional MWT alignment.
1 Introduction
Traditional research into the automatic compilation of bilingual dictionaries from
corpora exploits parallel texts, i.e. a text and its translation [17]. From sentence-
to-sentence aligned corpora, symbolic [2], statistical [11], or combined [7] tech-
niques are used for word and expression alignments.
The use of parallel corpora raises two problems:
? as a parallel corpus is a pair of translated texts, the vocabulary appearing
in the translated text is highly influenced by the source text, especially for
technical domains;
? such corpora are difficult to obtain for paired languages not involving
English.
New methods try to exploit comparable corpora: texts that are of the same text
type and on the same subject without a source text-target text relationship. The
main studies concentrate on finding in such corpora translation candidates for
one-item words. For example, the French SWT manteau is translated in English
by mantle in the domain of forestry, shield in the domain of marine activities,
and by coat in the domain of clothing. The method is based on lexical context
analysis and relies on the simple observation that a word and its translation tend
to appear in the same lexical contexts. Thus, for our three possible translations
of manteau, three different lexical contexts are encountered which are expressed
below by English lexical units:
R. Dale et al (Eds.): IJCNLP 2005, LNAI 3651, pp. 707?718, 2005.
c
? Springer-Verlag Berlin Heidelberg 2005
708 B. Daille and E. Morin
? manteau/mantle : vegetation, forest, wood. . .
? manteau/shield : boat, sea, shipbuilding. . .
? manteau/coat : cloth, cold, wear. . .
These contexts can be represented by vectors, and each vector element represents
a word which occurs within the window of the word to be translated. Translation
is obtained by comparing the source context vector to each translation candidate
vector after having translated each element of the source vector with a general
dictionary. This method is known as the ?direct context-vector approach?. Using
this method, [10] extracts English-Chinese one-item candidate translations from
two years of English and Chinese newspaper articles by matching the context
vector with 76% precision on the first 20 candidates. From English-German
newspaper corpora of 85 million words, [14] improves the precision to 89% on the
first one-item 10 candidates using the same techniques. [4] obtain 50% precision
on the first one-item 10 candidates from a French/English corpus of 1.2 million
words. [1] adapted this approach to deal with many-to-many word translations.
In extracting English-Chinese nominal phrases belonging to general domains
from the web, they obtain a precision of 91% on the first 3 candidates.
Some improvements have been proposed by [9] to avoid the insufficient cov-
erage of bilingual dictionary and thus not to get context vectors with too many
elements that are not translated. This method is called ?similarity-vector ap-
proach?: it associates to the word to be translated the context vectors of the
nearest lexical units that are in the bilingual dictionary. With this method, they
obtain for one-item French-English words 43% and 51% precision on the ten and
twenty first candidates applied on a medical corpus of 100 000 words (respec-
tively 44% and 57% with the direct method) and 79% and 84% precision on the
ten and twenty first candidates applied on a social science corpus of 8 millions
words (respectively 35% and 42% with the direct method).
If the results obtained in the field of bilingual lexicon extraction from compa-
rable corpora are promising, they only cover either bilingual single words from
general or specialised corpora, or bilingual nominal phrases from general corpora.
Our goal is to find translation for multi-word terms (MWTs) from specialised
comparable corpora.
If MWTs are more representative of domain specialities than single-word
terms (SWTs), pinpointing their translations poses specific problems:
? SWTs and MWTs are not always translated by a term of the same length. For
example, the French MWT peuplement forestier (2 content words) is trans-
lated into English as the SWT crop and the French term essence d?ombre (2
content words) as shade tolerant species (3 content words). This well-known
problem, referred to as ?fertility?, is seldom taken into account in bilingual
lexicon extraction, a word-to-word assumption being generally adopted.
? When a MWT is translated into a MWT of the same length, the target
sequence is not typically composed of the translation of its parts [13]. For
example, the French term plantation e?nerge?tique is translated into English as
fuel plantation where fuel is not the translation of e?nerge?tique. This property
is referred to as ?non-compositionality?.
French-English Terminology Extraction from Comparable Corpora 709
? A MWT could appear in texts under different forms reflecting either syn-
tactic, morphological or semantic variations [12],[5]. Term variations should
be taken into account in the translation process. For example, the French
sequences ame?nagement de la fore?t and ame?nagement forestier refer to the
same MWT and are both translated into the same English term: forest man-
agement.
We propose tackling these three problems, fertility, non-compositionality, and
variations, by using both linguistic and statistical methods. First, MWTs are
identified in both the source and target language using a monolingual term
extraction program. Second, a statistical alignment algorithm is used to link
MWTs in the source language to single words and MWTs in the target language.
Our alignment algorithm extracts the words and MWT contexts and proposes
translations by comparing source and target words and MWT contexts.
2 Extraction Process
We present in this section the bilingual extraction process which is composed of
two steps:
1. Identification in source and target languages of MWTs and their variations;
2. Alignment of theses MWTs using a method close to the ?similarity-vector
approach?.
2.1 MWT Identification
MWTs are extracted using a terminology extraction program available for French
and English: ACABIT 1. This program is open source and one of its character-
istics is to take into account variants of MWTs (graphical, inflectional, syntac-
tic, and morphosyntactic)[6]. It does not need any external linguistic resources
and is domain-independent. ACABIT applies on a corpus with the following
pre-processing:
? tokenisation and sentence segmentation;
? part-of-speech and lemma tagging.
First, ACABIT carries out shallow parsing: it scans the corpus, counts and
extracts strings whose tag sequences characterise patterns of MWTs or one of
their variants. The different occurrences referring to a MWT or one of its variants
are grouped and constitute an unique candidate MWT. Thus the candidate
MWT produit forestier ?forest product? appears under the following forms:
1 http://www.sciences.univ-nantes.fr/info/perso/permanents/daille/ and LINUX
Mandrake release
710 B. Daille and E. Morin
? base form: produit forestier ;
? graphical variant: produit fo-restier, pro-duit forestier ;
? inflexional variant: produits forestiers ;
? syntatic variant: modification: produit non forestier, produit alimentaire
forestier, produit fini d?origine forestie?re, produit ligneux non forestier ;
? syntactic variant: coordination: produit halieutique et forestier, produit
agricole ou forestier, le produit et le service forestier.
The MWT candidates produit de la fore?t, produit agroforestier, non-produit agro-
forestier, and sous-produit forestier, sous-produit de la fore?t have also been iden-
tified.
Second, ACABIT performs semantic grouping thanks to the following
operations:
Merging of two MWTs. Two MWT candidates are merged if they are syn-
onymic variants obtained by derivation or conversion. Such variants in-
clude a relational adjective: either a denominal adjective, i.e. morphologi-
cally derived from a noun thanks to a suffix, such as fore?t/forestier ?forest?,
or an adjective having a noun usage such as mathe?matique ?mathemati-
cal/mathematics?.
Dissociation of some MWT variants. Syntactical variants that induce se-
mantic discrepancies are retrieved from the set of the candidate variants
and new MWT candidates are created. Modification variants with the in-
sertion of an adverb of negation denoting an antonymy link such as produit
non forestier ?non forest product? and produit forestier ?forest product?, or
insertion of a relational adjectives denoting an hyperonymy link such as pro-
duit alimentaire forestier ?food forest product? with produit forestier ?forest
product? [6].
Grouping of MWTs. All MWT candidates linked by derivational morphol-
ogy or by variations inducing semantic variations are clustered. For exam-
ple, the following MWT candidates constitutes a cluster of MWTs: produit
forestier/produit de la fore?t, produit non forestier, non-produit agroforestier,
produit agroforestier, sous-produit forestier/sous-produit de la fore?t, produit
alimentaire forestier andproduit forestier.
In the following steps, we do not consider a unique sequence reflecting a
candidate MWT but a set of sequences. We consider only term variants that
are grouped under a unique MWT. This grouping of term variations could be
interpreted as a terminology normalisation in the same way as lemmatisation at
the morphological level.
2.2 MWT Alignment
The goal of this step, which adapts the similarity vector-based approach defined
for single words by [9] to MWTs, is to align source MWTs with target single
words, SWTs or MWTs. From now on, we will refer to lexical units as words,
SWTs or MWTs.
French-English Terminology Extraction from Comparable Corpora 711
Context Vectors. First, we collect all the lexical units in the context of each
lexical unit i and count their occurrence frequency in a window of n sentences
around i. For each lexical unit i of the source and the target language, we obtain a
context vector vi which gathers the set of co-occurrence units j associated with
the number of times that j and i occur together occij . We normalise context
vectors using an association score such as Mutual Information or Log-likelihood.
(cf. equations 1 and 2 and table 1). In order to reduce the arity of context vectors,
we keep only the co-occurrences with the highest association scores.
Table 1. Contingency table
j ?j
i a = occ(i, j) b = occ(i, ?j)
?i c = occ(?i, j) d = occ(?i, ?j)
MI(i, j) = log
a
(a + b)(a + c)
(1)
?(i, j) = a log(a) + b log(b) + c log(c) + d log(d)
+(a + b + c + d) log(a + b + c + d) ? (a + b) log(a + b)
?(a + c) log(a + c) ? (b + d) log(b + d) ? (c + d) log(c + d)
(2)
Similarity Vectors. For each lexical unit k to be translated, we identify the
lexical units which the context vectors are similar to vk thanks to a vector
distance measure such as Cosine [15] or Jaccard [16] (cf. equations 3 and 4).
From now, we call ?similarity vector? of the unit k a vector that contains all the
lexical units which the context vectors are similar to vk. To each unit l of the
similarity vector vk, we associate a similarity score similvkvl between vl and vk.
In order to reduce the arity of similarity vectors, we keep only the lexical units
with the highest similarity scores. Up to now, similarity vectors have only been
built for the source language.
similvkvl =
?
t assoc
l
t assoc
k
t
?
?
t assoc
l
t
2
assockt
2
(3)
similvkvl =
?
t min(assoc
l
t, assoc
k
t )
?
t assoc
l
t
2
+
?
t assoc
k
t
2 ?
?
t assoc
l
t assoc
k
t
(4)
Translation of the Similarity Vectors. Using a bilingual dictionary, we
translate the lexical units of the similarity vector and identify their context
vectors in the target language. Figure 1 illustrates this translation process.
Depending the nature of the lexical unit, two different treatments are
carried out:
712 B. Daille and E. Morin
TRANSLATION
SOURCE LANGUAGE TARGET LANGUAGE
close vector
close vector
close vector
close vector
close vector
Candidate translations
average vector
average context vectorsimilarity vectors
context vectorscontext vector of the lexical unit to be translated
MWT to be translated
context vector of the candidate translations
Fig. 1. Transfer procedure of similarity vectors from source to target language
Translation of a SWT. If the bilingual dictionary provides several transla-
tions for a word belonging to the similarity vector, we generate as many
target context vectors as possible translations. Then, we calculate the union
of these vectors to obtain only one target context vector.
Translation of a MWT. If the translation of the parts of the MWT are found
in the bilingual dictionary, we generate as many target context vectors as
translated combinations identified by ACABIT and calculate their union.
When it is not possible to translate all the parts of a MWT, or when the
translated combinations are not identified by ACABIT, the MWT is not
taken into account in the translation process.
Finding the MWT Translations. We calculate the barycentre of all the
target context vectors obtained in the preceding step in order to propose a
target average vector. The candidate translations of a lexical unit are the tar-
get lexical units closest to the target average vector according to vector
distance.
3 Resources Presentation
We present in this section the different resources used for our experiments:
3.1 Comparable Corpus
Our comparable corpus has been built from the Unasylva electronic international
journal published by FAO2 and representing 4 million words. This journal deals
2 http://www.fao.org/forestry/foris/webview/forestry2/
French-English Terminology Extraction from Comparable Corpora 713
with forests and forest industries and is available in English, French and Spanish.
In order to constitute a comparable corpus, we only select texts which are not
the translation of each other.
3.2 Bilingual Dictionary
Our bilingual dictionary has been built from lexical resources on the Web. It
contains 22,300 French single words belonging to the general language with an
average of 1.6 translation per entry.
3.3 Reference Bilingual Terminology
The evaluation of our bilingual terminology extraction method has been done
from a reference bilingual terminology. This reference list has been built from
three different terminological resources:
1. a bilingual glossary of the terminology of silviculture3. It contains 700 terms
of which 70% are MWTs.
2. the Eurosilvasur multilingual lexicon4. It contains 2,800 terms of which 66%
are MWTs.
3. the multilingual AGROVOC thesaurus5. It contains 15,000 index terms of
which 47% are MWTs.
These three terminological resources are complementary, the glossary being the
most specialised, the thesaurus the least. From these resources, we automatically
select 300 terms with the constraint that each French term should appear at least
5 times in our corpus. These terms are divided into three sub-lists:
? [list 1] 100 French SWTs of which the translation is an English SWT. Of
course, this translation is not given by our bilingual dictionary.
? [list 2] 100 French MWTs of which the translation could be an English SWT
or a MWT. In the case of MWTs, the translation could not be obtained by
the translation of the MWT?s parts.
? [list 3] 100 MWT of which the translation is an English MWT. The transla-
tion of these MWTs is obtained by the translation of their parts.
This reference list contains a majority of terms with low frequency (cf.
Table 2). Two main reasons explain this fact: on the one hand, the different
resources which have been used to build this reference list are either specific or
generic; on the other hand, our corpus covers several domains linked to forestry
and does not constitute a highly specialised resource.
3 http://nfdp.ccfm.org/silviterm/silvi f/silvitermintrof.htm
4 http://www.eurosilvasur.net/francais/lexique.php
5 http://www.fao.org/agrovoc/
714 B. Daille and E. Morin
Table 2. Frequency in the corpus of the French terms belonging to the reference list
# occ. < 50 ? 100 ? 1 000 > 1 000
[list 1] 50 21 18 11
[list 2] 54 21 25 0
[list 3] 51 18 29 2
4 Evaluation
We present now the evaluation of the bilingual terminology extraction. We have
to deal with 55 013 SWTs and MWTs, but only 7 352 SWTs and 6 769 MWTs
appear both in the reference bilingual terminology and in the corpus.
4.1 Parameter Estimation
Several parameters appear in the extraction process presented in Section 2. The
most interesting results have been obtained with the following values:
? Size of the context window is 3 sentences around the lexical unit to be
translated;
? Context vectors are built only with one-item words to increase representa-
tivity. For example, the context vector of the French term de?bardage ?hauling?
includes the MWT tracteur a` chenille ?crawler tractor? which is more dis-
criminating than its parts, tracteur or chenille. But including MWTs into
context vectors increases the vectorial space dimension and reduces the rep-
resentativity of the terms appearing both in the corpus and the reference
bilingual terminology. The term de?bardage ?hauling? has a frequency of 544
as a SWT and only a frequency of 144 as part of a MWT as it appears in
several MWTs. The context vector size are limited to the first 100 values of
the Log-likelihood association score.
? Similarity vectors are the first 30 values of Cosine distance measure.
? Finding translations is done with Cosine distance measure.
4.2 Result Analysis
Table 3 gives the results obtained with our experiments. For each sublist, we
give the number of translations found (NBtrans), and the average and standard
deviation position for the translations in the ranked list of candidate translations
(AV Gpos, STDDEVpos).
We note that translations of MWTs belonging to [list 3] which are composi-
tionally translated are well-identified and often appear in the first 20 candidate
translations. The translations belonging to [lists 1 and 2 ] are not always found
and, when they are, they seldom appear in the first 20 candidate translations.
The examination of the candidate translations of a MWT regardless of the
list to which it belongs shows that they share the same semantic field (cf. table 5).
French-English Terminology Extraction from Comparable Corpora 715
Table 3. Bilingual terminology extraction results
NBtrans AV Gpos STDDEVpos
|list 1] 56 32.9 23,7
[list 2] 63 30.7 26,7
[list 3] 89 3.8 7,9
Table 4. Bilingual MWT extraction with parameter combination
NBtrans AV Gpos STDDEVpos Top 10 Top 20
|list 1] 59 16.2 15.9 41 51
[list 2] 63 14.8 22.3 45 55
[list 3] 89 2.4 3.7 87 88
Table 5. Exemples of candidate translations obtained for 3 terms belonging to [list 2]
degre? de humidite? gaz a` effet de serre papeterie
(# occ. 41) (# occ. 33) (# occ. 178)
humidity carbon newsprint
saturation carbon cycle paper production
aridity atmosphere raw material
evaporation greenhouse gas mill
saturation deficit greenhouse pulp mill
rate of evaporation global carbon raw
atmospheric humidity atmospheric carbon manufacture
water vapor emission paper mill
joint sink manufacturing
dry carbon dioxide capacity
hot fossil fuel printing
rainy fossil paper manufacture
temperature carbon pool factory
moisture control mitigate paperboard
meyer global warming fiberboard
party climate change bagasse
atmospheric atmospheric paper-making
dryness dioxide board
monsoon sequestration material supply
joint meeting quantity of carbon paper pulp
As noted above, our results differ widely according the chosen parameter values.
Because of time constraints, we cannot evaluate all the possible values of all the
different parameters, but manual examination of the candidate translations for
a few different configurations shows:
716 B. Daille and E. Morin
? Some good translations obtained for one parameter configuration are not
found for another, and, inversely, some terms which are not translated in
the first configuration could be correctly translated by another. So, it is
difficult to choose the best configuration, especially for [lists 1 and 2].
? More precisely, for a given term, the first candidate translations are different
for different configurations. For example, for the French MWT pa?te a` papier
(paper pulp), the first 50 candidate translations of 20 different configurations
have only 30 items in common.
? The right translation appears in different positions for different configura-
tions.
In order to identify more correct translations, we decided to take into account
the different results proposed by different configurations by fusing the first 20
candidate translations proposed by each configuration. The different configura-
tions concern the size of the context and similarity vectors, and the association
and similarity measures. The results obtained and presented in Table 4 show a
slight improvement in the position of the correct translations among the set of
candidate translations.
The results for [list 3] are still very satisfactory. The results for [list 1] improve,
but remain a little below the results obtained by [8] who obtained 43% and 51%
for the first 10 and 20 candidates respectively for a 100,000-word medical corpus,
and 79% and 84% for a multi-domain 8 million word corpus.
4.3 Comment
In a general way, it is difficult to compare our experiments to previous ones
[3],[8] as the corpora are different. Indeed, our comparable corpus covers several
domains belonging to forestry, and does not constitute a very specialised re-
source on the contrary of the medical corpus of [3] built thanks to the key words
?symptoms, pathological status?. Moreover, half of the terms of the reference
bilingual terminological database have a frequency of less than 50 occurrences in
the corpus that lead to non-discriminating context vectors. [8] use for their ex-
periments a social sciences corpora of 8 millions words and a reference bilingual
terminological database of 180 words with high frequencies in the corpus: from
100 to 1000. Our automatic evaluation is also more constrained than manual
evaluation. For example, our reference list gives haulage road as the transla-
tion of piste de de?bardage. In our candidate translation list, haulage road is not
present. We find an acceptable translation, skid trail, in the first 20 candidates,
but this is never considered valid by our automatic evaluation.
Our results for MWTs are better than those for single words. The method seems
promising, especially for MWTs for which translation is not compositional.
5 Conclusion
In this paper, we proposed and evaluated a combined method for bilingual MWT
extraction from comparable corpora which takes into account three main char-
acteristics of MWT translation: fertility, non-compositionality, and variation
French-English Terminology Extraction from Comparable Corpora 717
clustering. We first extracted monolingually MWTs and clustered synonymic
variants. Secondly, we aligned them using a statistical method adapted from
similarity-vector approach for single words which exploits the context of these
MWTs. This combined approach for MWTs gives satisfactory results compared
to those for single word. It also allows us to obtain non compositional translations
of MWTs. Our further works will concentrate on the interaction parameters, the
combining of the source-to-target and target-to-source alignment results, and
the handling of non-synonymic term variations.
Acknowledgements
We are particularly grateful to Samuel Dufour-Kowalski, who undertook the
computer programs. This work has also benefited from his comments.
References
1. Cao, Y., Li, H.: Base Noun Phrase Translation Using Web Data and the EM
Algorithm. In: Proceeding of the 19th International Conference on Computational
Linguistics (COLING?02), Tapei, Taiwan (2002) 127?133
2. Carl, M., Langlais, P.: An intelligent Terminology Database as a pre-processor
for Statistical Machine Translation. In Chien, L.F., Daille, B., Kageura, L., Nak-
agawa, H., eds.: Proceeding of the COLING 2002 2nd International Workshop on
Computational Terminology (COMPUTERM?02), Tapei, Taiwan (2002) 15?21
3. Chiao, Y.C.: Extraction lexicale bilingue a` partir de textes me?dicaux comparables :
application a` la recherche d?information translangue. PhD thesis, Universite? Pierre
et Marie Curie, Paris VI (2004)
4. Chiao, Y.C., Zweigenbaum, P.: Looking for candidate translational equivalents in
specialized, comparable corpora. In: Proceedings of the 19th International Confer-
ence on Computational Linguistics (COLING?02), Tapei, Taiwan (2002) 1208?1212
5. Daille, B.:. Conceptual Structuring through Term Variations. In Bond, F.,
Korhonen, A., MacCarthy, D., Villacicencio A., eds.: Proceedings of the ACL
2003 Workshop on Multiword Expressions: Analysis, Acquisition and Treatment
(2003) 9?16
6. Daille, B.: Terminology Mining. In Pazienza, M., ed.: Information Extraction in
the Web Era. Springer (2003) 29?44
7. Daille, B., Gaussier, E., Lange?, J.-M..: Towards Automatic Extraction of Monolin-
gual and Bilingual Terminology. Proceedings of the 15th International Conference
on Computational Linguistics (COLING?94) 1 (1994) 515?521
8. De?jean, H., Sadat, F., Gaussier, E.: An approach based on multilingual thesauri
and model combination for bilingual lexicon extraction. In: Proceedings of the
19th International Conference on Computational Linguistics (COLING?02). (2002)
218?224
9. De?jean, H., Gaussier, E.: Une nouvelle approche a` l?extraction de lexiques bilingues
a` partir de corpus comparables. Lexicometrica, Alignement lexical dans les corpus
multilingues (2002) 1?22
10. Fung, P.: A Statistical View on Bilingual Lexicon Extraction: From Parallel Cor-
pora to Non-parallel Corpora. In Farwell, D., Gerber, L., Hovy, E., eds.: Pro-
ceedings of the 3rd Conference of the Association for Machine Translation in the
Americas (AMTA?98), Springer (1998) 1?16
718 B. Daille and E. Morin
11. Gaussier, E., Lange?, J.M.: Mode`les statistiques pour l?extraction de lexiques
bilingues. Traitement Automatique des Langues (TAL) 36 (1995) 133?155
12. Jacquemin, C.: Spotting and Discovering Terms through Natural Language Pro-
cessing. Cambridge: MIT Press (2001)
13. Melamed, I.D.: Empirical Methods for Exploiting Parallel Texts. MIT Press (2001)
14. Rapp, R.: Automatic Identification of Word Translations from Unrelated English
and German Corpora. In: Proceedings of the 37th Annual Meeting of the Associ-
ation for Computational Linguistics (ACL?99). (1999) 519?526
15. Salton, G., Lesk, M.E.: Computer Evaluation of Indexing and Text Processing.
Journal of the Association for Computational Machinery 15 (1968) 8?36
16. Tanimoto, T.T.: An elementary mathematical theory of classification. Technical
report, IBM Research (1958)
17. Veronis, J., ed.: Parallel Text Processing. Kluwer Academic Publishers (2000)
An Effective Compositional Model for Lexical Alignment
Be?atrice Daille Emmanuel Morin
Universit?e de Nantes, LINA - FRE CNRS 2729
2, rue de la Houssiniere, BP 92208
F-44322 Nantes cedex 03
 
beatrice.daille,emmanuel.morin  @univ-nantes.fr
Abstract
The automatic compilation of bilingual dic-
tionaries from comparable corpora has been
successful for single-word terms (SWTs),
but remains disappointing for multi-word
terms (MWTs). One of the main problems is
the insufficient coverage of the bilingual dic-
tionary. Using the compositional translation
method improved the results, but still shows
some limits for MWTs of different syntac-
tic structures. In this paper, we propose to
bridge the gap between syntactic structures
through morphological links. The results
show a significant improvement in the com-
positional translation of MWTs that demon-
strate the efficiency of the morphologically
based-method for lexical alignment.
1 Introduction
Current research in the automatic compilation of
bilingual dictionaries from corpora uses of compara-
ble corpora. Comparable corpora gather texts shar-
ing common features (domain, topic, genre, dis-
course) without having a source text-target text re-
lationship. They are considered by human transla-
tors more trustworthy than parallel corpora (Bowker
and Pearson, 2002). Moreover, they are available for
any written languages and not only for pairs of lan-
guages involving English. The compilation of spe-
cialized dictionaries should take into account multi-
word terms (MWTs) that are more precise and spe-
cific to a particular scientific domain than single-
word terms (SWTs). The standard approach is based
on lexical context analysis and relies on the simple
observation that a SWT or a MWT and its trans-
lation tend to appear in the same lexical contexts.
Correct results are obtained for SWTs with an ac-
curacy of about 80% for the top 10-20 proposed
candidates using large comparable corpora (Fung,
1998; Rapp, 1999; Chiao and Zweigenbaum, 2002)
or 60% using small comparable corpora (De?jean
and Gaussier, 2002). In comparison, the results ob-
tained for MWTs are disappointing. For instance,
(Morin et al, 2007) have achieved 30% and 42%
precision for the top 10 and top 20 candidates in a
0.84 million-word French-Japanese corpus. These
results could be explained by the low frequency of
MWTs compared to SWTs, by the lack of paral-
lelism between the source and the target MWT ex-
traction systems, and by the low performance of the
alignment program. For SWTs, the process is in
two steps: looking in a dictionary, and if no direct
translation is available, starting the contextual anal-
ysis. Looking in the dictionary gives low results for
MWTs: 1% compared to 30% for French and 20%
for Japanese SWTs (Morin and Daille, 2006). To ex-
tend the coverage of the bilingual dictionary, an in-
termediate step is added between looking in the dic-
tionary and the contextual analysis that will propose
several translation candidates to compare with the
target MWTs. These candidate translations are ob-
tained thanks to a compositional translation method
(Melamed, 1997; Grefenstette, 1999). This method
reveals some limits when MWTs in the source and
the target languages do not share the same syntactic
patterns.
In this paper, we put forward an extended compo-
95
sitional method that bridges the gap between MWTs
of different syntactic structures through morpho-
logical links. We experiment within this method
of French-Japanese lexical alignment, using multi-
lingual terminology mining chain made up of two
terminology extraction systems; one in each lan-
guage, and an alignment program. The term extrac-
tion systems are publicly available and both extract
MWTs. The alignment program makes use of the
direct context-vector approach (Fung, 1998; Rapp,
1999). The results show an improvement of 33% in
the translation of MWTs that demonstrate the effi-
ciency of the morphologically based-method for lex-
ical alignment.
2 Multilingual terminology mining chain
Taking a comparable corpora as input, the multi-
lingual terminology mining chain outputs a list of
single- and multi-word candidate terms along with
their candidate translations (see Figure 1). This
chain performs a contextual analysis that adapts the
direct context-vector approach (Rapp, 1995; Fung
and McKeown, 1997) for SWTs to MWTs. It con-
sists of the following five steps:
1. For each language, the documents are cleaned,
tokenized, tagged and lemmatized. For French,
Brill?s POS tagger1 and the FLEM lemmatiser2
are used, and for Japanese, ChaSen3. We then
extract the MWTs and their variations using
the ACABIT terminology extraction system avail-
able for French4 (Daille, 2003), English and
Japanese5 (Takeuchi et al, 2004). (From now
on, we will refer to lexical units as words,
SWTs or MWTs).
2. We collect all the lexical units in the context of
each lexical unit  and count their occurrence
frequency in a window of  words around  .
For each lexical unit  of the source and the
target languages, we obtain a context vector
1http://www.atilf.fr/winbrill/
2http://www.univ-nancy2.fr/pers/namer/
3http://chasen-legacy.sourceforge.jp/
4http://www.sciences.univ-nantes.fr/
info/perso/permanents/daille/ and release for
Mandriva Linux.
5http://cl.cs.okayama-u.ac.jp/rsc/
jacabit/
 which gathers the set of co-occurrence units
 associated with the number of times that 
and  occur together 	
	

 . In order to iden-
tify specific words in the lexical context and
to reduce word-frequency effects, we normal-
ize context vectors using an association score
such as Mutual Information (Fano, 1961) or
Log-likelihood (Dunning, 1993).
3. Using a bilingual dictionary, we translate the
lexical units of the source context vector. If the
bilingual dictionary provides several transla-
tions for a lexical unit, we consider all of them
but weigh the different translations by their fre-
quency in the target language.
4. For a lexical unit to be translated, we com-
pute the similarity between the translated con-
text vector and all target vectors through vector
distance measures such as Cosine (Salton and
Lesk, 1968) or Jaccard (Tanimoto, 1958).
5. The candidate translations of a lexical unit are
the target lexical units closest to the translated
context vector according to vector distance.
In this approach, the translation of the lexical units
of the context vectors (step 3 of the previous ap-
proach), which depends on the coverage of the bilin-
gual dictionary vis-a`-vis the corpus, is the most im-
portant step: the greater the number of elements
translated in the context vector, the more discrim-
inating the context vector in selecting translations
in the target language. Since the lexical units re-
fer to SWTs and MWTs, the dictionary must con-
tain many entries which occur in the corpus. For
SWTs, combining a general bilingual dictionary
with a specialized bilingual dictionary or a multi-
lingual thesaurus to translate context vectors ensures
that much of their elements will be translated (Chiao
and Zweigenbaum, 2002; De?jean et al, 2002). For a
MWT to be translated, steps 3 to 5 could be avoided
thanks to a compositional method that will propose
several translation candidates to directly compare
with the target MWTs identified in step 1. More-
over, the compositional method is useful in step 3
to compensate for the bilingual dictionary when the
multi-word units of the context vector are not di-
rectly translated.
96
dictionary
bilingual
Japanese documents French documents
terminology
extraction
terminology
extraction
lexical context
extraction
lexical context
extraction
process
translated
terms to be
translations
candidate
haversting
lexical alignment
The Web
documents
Figure 1: Architecture of the multilingual terminology mining chain
3 Default compositional method
In order to increase the coverage of the dictionary for
MWTs that could not be directly translated, we gen-
erated possible translations by using a default com-
positional method (Melamed, 1997; Grefenstette,
1999).
For each element of the MWT found in the bilin-
gual dictionary, we generated all the translated com-
binations identified by the terminology extraction
system. For example, for the French MWT fatigue
chronique (chronic fatigue), there are four Japanese
translations for fatigue (fatigue) ?  ,  ,  ,
 ? and two translations for chronique (chronic)
? ffProceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 664?671,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Bilingual Terminology Mining ? Using Brain, not brawn comparable
corpora
E. Morin, B. Daille
Universit? de Nantes
LINA FRE CNRS 2729
2, rue de la Houssini?re
BP 92208
F-44322 Nantes Cedex 03
{morin-e,daille-b}@
univ-nantes.fr
K. Takeuchi
Okayama University
3-1-1, Tsushimanaka
Okayama-shi, Okayama,
700-8530, Japan
koichi@
cl.it.okayama-u.ac.jp
K. Kageura
Graduate School of Education
The University of Tokyo
7-3-1 Hongo, Bunkyo-ku,
Tokyo, 113-0033, Japan
kyo@p.u-tokyo.ac.jp
Abstract
Current research in text mining favours the
quantity of texts over their quality. But for
bilingual terminology mining, and for many
language pairs, large comparable corpora
are not available. More importantly, as terms
are defined vis-?-vis a specific domain with
a restricted register, it is expected that the
quality rather than the quantity of the corpus
matters more in terminology mining. Our
hypothesis, therefore, is that the quality of
the corpus is more important than the quan-
tity and ensures the quality of the acquired
terminological resources. We show how im-
portant the type of discourse is as a charac-
teristic of the comparable corpus.
1 Introduction
Two main approaches exist for compiling corpora:
?Big is beautiful? or ?Insecurity in large collec-
tions?. Text mining research commonly adopts the
first approach and favors data quantity over qual-
ity. This is normally justified on the one hand by
the need for large amounts of data in order to make
use of statistic or stochastic methods (Manning and
Sch?tze, 1999), and on the other by the lack of oper-
ational methods to automatize the building of a cor-
pus answering to selected criteria, such as domain,
register, media, style or discourse.
For lexical alignment from comparable corpora,
good results on single words can be obtained from
large corpora ? several millions words ? the accu-
racy of proposed translation is about 80% for the top
10-20 candidates (Fung, 1998; Rapp, 1999; Chiao
and Zweigenbaum, 2002). (Cao and Li, 2002) have
achieved 91% accuracy for the top three candidates
using the Web as a comparable corpus. But for spe-
cific domains, and many pairs of languages, such
huge corpora are not available. More importantly,
as terms are defined vis-?-vis a specific domain with
a restricted register, it is expected that the quality
rather than the quantity of the corpus matters more in
terminology mining. For terminology mining, there-
fore, our hypothesis is that the quality of the corpora
is more important than the quantity and that this en-
sures the quality of the acquired terminological re-
sources.
Comparable corpora are ?sets of texts in different
languages, that are not translations of each other?
(Bowker and Pearson, 2002, p. 93). The term com-
parable is used to indicate that these texts share
some characteristics or features: topic, period, me-
dia, author, register (Biber, 1994), discourse... This
corpus comparability is discussed by lexical align-
ment researchers but never demonstrated: it is of-
ten reduced to a specific domain, such as the med-
ical (Chiao and Zweigenbaum, 2002) or financial
domains (Fung, 1998), or to a register, such as
newspaper articles (Fung, 1998). For terminology
664
mining, the comparability of the corpus should be
based on the domain or the sub-domaine, but also
on the type of discourse. Indeed, discourse acts
semantically upon the lexical units. For a defined
topic, some terms are specific to one discourse or
another. For example, for French, within the sub-
domain of obesity in the domain of medicine, we
find the term exc?s de poids (overweight) only in-
side texts sharing a popular science discourse, and
the synonym exc?s pond?ral (overweight) only in
scientific discourse. In order to evaluate how impor-
tant the discourse criterion is for building bilingual
terminological lists, we carried out experiments on
French-Japanese comparable corpora in the domain
of medicine, more precisely on the topic of diabetes
and nutrition, using texts collected from the Web and
manually selected and classified into two discourse
categories: one contains only scientific documents
and the other contains both scientific and popular
science documents.
We used a state-of-the-art multilingual terminol-
ogy mining chain composed of two term extraction
programs, one in each language, and an alignment
program. The term extraction programs are pub-
licly available and both extract multi-word terms
that are more precise and specific to a particular sci-
entific domain than single word terms. The align-
ment program makes use of the direct context-vector
approach (Fung, 1998; Peters and Picchi, 1998;
Rapp, 1999) slightly modified to handle both single-
and multi-word terms. We evaluated the candidate
translations of multi-word terms using a reference
list compiled from publicly available resources. We
found that taking discourse type into account re-
sulted in candidate translations of a better quality
even when the corpus size is reduced by half. Thus,
even using a state-of-the-art alignment method well-
known as data greedy, we reached the conclusion
that the quantity of data is not sufficient to obtain
a terminological list of high quality and that a real
comparability of corpora is required.
2 Multilingual terminology mining chain
Taking as input a comparable corpora, the multilin-
gual terminology chain outputs a list of single- and
multi-word candidate terms along with their candi-
date translations. Its architecture is summarized in
Figure 1 and comprises term extraction and align-
ment programs.
2.1 Term extraction programs
The terminology extraction programs are avail-
able for both French1 (Daille, 2003) and Japanese2
(Takeuchi et al, 2004). The terminological units
that are extracted are multi-word terms whose syn-
tactic patterns correspond either to a canonical or a
variation structure. The patterns are expressed us-
ing part-of-speech tags: for French, Brill?s POS tag-
ger3 and the FLEM lemmatiser4 are utilised, and for
Japanese, CHASEN5. For French, the main patterns
are N N, N Prep N et N Adj and for Japanese, N N,
N Suff, Adj N and Pref N. The variants handled are
morphological for both languages, syntactical only
for French, and compounding only for Japanese. We
consider as a morphological variant a morphological
modification of one of the components of the base
form, as a syntactical variant the insertion of another
word into the components of the base form, and as
a compounding variant the agglutination of another
word to one of the components of the base form. For
example, in French, the candidate MWT s?cr?tion
d?insuline (insulin secretion) appears in the follow-
ing forms:
  base form of N Prep N pattern: s?cr?tion
d?insuline (insulin secretion);
  inflexional variant: s?cr?tions d?insuline (in-
sulin secretions);
  syntactic variant (insertion inside the base
form of a modifier): s?cr?tion pancr?atique
d?insuline (pancreatic insulin secretion);
  syntactic variant (expansion coordination of
base form): secr?tion de peptide et d?insuline
(insulin and peptide secretion).
The MWT candidates secr?tion insulinique (insulin
secretion) and hypers?cr?tion insulinique (insulin
1http://www.sciences.univ-nantes.fr/
info/perso/permanents/daille/ and release
LINUX.
2http://research.nii.ac.jp/~koichi/
study/hotal/
3http://www.atilf.fr/winbrill/
4http://www.univ-nancy2.fr/pers/namer/
5http://chasen.org/$\sim$taku/software/
mecab/
665
WEB
dictionary
bilingual
Japanese documents French documents
terminology
extraction
terminology
extraction
lexical context
extraction
lexical context
extraction
process
translated
terms to be
translations
candidate
haversting
documents
lexical alignment
Figure 1: Architecture of the multilingual terminology mining chain
hypersecretion) have also been identified and lead
together with s?cr?tion d?insuline (insulin secretion)
to a cluster of semantically linked MWTs.
In Japanese, the MWT
 
. 
	
6 (in-
sulin secretion) appears in the following forms:
  base form of NN pattern:   /N  . 
	 /N  (insulin secretion);
  compounding variant (agglutination of a
word at the end of the base form):  

/N  . 	 /N  .  /N  (insulin secretion
ability)
At present, the Japanese term extraction program
does not cluster terms.
2.2 Term alignment
The lexical alignment program adapts the direct
context-vector approach proposed by (Fung, 1998)
for single-word terms (SWTs) to multi-word terms
(MWTs). It aligns source MWTs with target single
6For all Japanese examples, we explicitly segment the com-
pound into its component parts through the use of the ?.? sym-
bol.
words, SWTs or MWTs. From now on, we will refer
to lexical units as words, SWTs or MWTs.
2.2.1 Implementation of the direct
context-vector method
Our implementation of the direct context-vector
method consists of the following 4 steps:
1. We collect all the lexical units in the context of
each lexical unit  and count their occurrence
frequency in a window of  words around  .
For each lexical unit  of the source and the
target language, we obtain a context vector Conceptual Structuring through Term Variations
Be?atrice Daille
IRIN
University of Nantes
France
daille@irin.univ-nantes.fr
Abstract
Term extraction systems are now an inte-
gral part of the compiling of specialized
dictionaries and updating of term banks.
In this paper, we present a term detection
approach that discovers, structures, and
infers conceptual relationships between
terms for French. Conceptual relation-
ships are deduced from specific types of
term variations, morphological and syn-
tagmatic, and are expressed through lexi-
cal functions. The linguistic precision of
the conceptual structuring through mor-
phological variations is of 95 %.
1 Introduction
Term extraction systems are now an integral part
of the compiling of specialized dictionaries and up-
dating of term banks. Several tools exist either for
extracting or structuring terminology (see (Cabre?
et al, 2001) for a review of the current systems).
Systems for identifying conceptual relationships are
generally based on external evidence: (Corte`s and
Cabre?, 2002) define a catalogue of linguistic mark-
ers to detect conceptual relationship such as simi-
larity or inclusion relationships. Similarity is de-
tected by the prototype linguistic expression: to be
similar to. Other systems relies on internal evi-
dence. (Morin and Jacquemin, 1999; Hamon and
Nazarenko, 2001) structure complex terms (multi-
word terms) with the help of lexical databases or
general dictionaries. The relationships handled are
limited to synonymy and hyperonymy. (Grabar and
Zweigenbaum, 2000) identify morphological fami-
lies of word forms applied on a medical thesaurus
with a precision of 92 % such as (arthrite ?arthri-
tis?, arthrose ?arthrosis?, arthropathie ?arthropa-
thy?,     ). They also make deductions on their con-
ceptual relationship with other lexical units and their
contribution to the overall knowledge organization
of a specialized field, namely medicine. The con-
ceptual relationships identified are synonymy, refer-
ence and hyperonymy.
In this paper, we present a term detection approach
that discovers, structures, and infers conceptual rela-
tionships between terms for French. Conceptual re-
lationships are deduced from specific types of term
variations, morphological and syntagmatic, and are
expressed in terms of lexical functions (Wanner,
1996). Term variations have already proved their re-
liability in information retrieval: (Jacquemin, 2001)
considers different types of terminological variants
including syntactic variations to perform accurate
text indexing. In the remaining sections, we present
some conceptual systems, provide a linguistic typol-
ogy of term variations and describe the two steps of
our approach. In the final part, we give results and
briefly discuss them.
2 Conceptual systems
Terms are generally classified using partitive and
generic relationships to be presented in a thesaural
structure. But other relationships exist, the so-called
complex relationships (Sager, 1990, pages 34-35)
which are domain and application dependent. Ex-
amples of such complex relationships are:
FALLOUT is caused by NUCLEAR EXPLOSION
COAL-MINE is a place for COAL-MINING
Studying term formation, (Kageura, 2002) intro-
duces intra-term relationships dealing with complex
terms and defining the role of the determinant with
respect to the head noun. In computer program, pro-
gram is the head noun and computer the determi-
nant. As program is intended for computer, a ?des-
tination? intra-relationship occurs between program
and computer. (L?Homme, 2002) used lexical func-
tions to represent various types of relationships via
an unique formalism for a computerized version of a
dictionary. These relationships are of several types:
  paradigmatic such as the generic (Gener) or
antonymy (Anti) relationships:
Gener(retail sale) = sale,
Anti(retail sale) = wholesaling;
  derivational such as nominalizations (S  , V  ):
S  (to program) = programmer,
V  (to program) = programming;
  syntagmatic (Bon, Real  ):
Bon(software) = performing,
Real  (programme) = to run [DET  ].
These conceptual relationships are assigned manu-
ally to terms or sets of terms. We propose to au-
tomatically assign conceptual relationships to com-
plex terms through their variations.
3 Concept identification through Term
variations
For complex terms identification, it is necessary
to first define syntactic structures which are poten-
tially lexicalisable. These complex sequences are
so-called ?base-terms?.
3.1 Base terms and their linguistic Variations
For French, the syntactic structures or patterns of
base-terms are:
Noun1 Adj emballage biode?gradable (biodegrad-
able package)
Noun1 (Prep (Det)) Noun2 ions calcium (calcium
ion) prote?ine de poissons (fish protein), chimio-
prophylaxie au rifampine (rifampicin chemo-
prophylaxis)
Noun1 a` Vinf viandes a` griller (grill meat)
These base structures are not frozen structures and
accept variations. Terminological variation in texts
is now a well-known phenomenon estimated from
15 % to 35 %, depending on the domain reflecting by
the texts and the different kinds of variants handled.
For acquisition, it is essential to identify extensively
all the concepts represented by terms in textual data.
Thus, only term variants which can preserve the
base-term semantics and thus refer to the same con-
cept are taken into account in a first step. Two se-
quences such as histamine pre?sente dans le vin (his-
tamine which is present in wine) et histamine du vin
(histamine of the wine) refer to the same term his-
tamine du vin (wine histamine); but, the sequences
produit a` surgeler (product to be frozen) and pro-
duit surgele? (frozen product) refer to two different
terms linked by an aspectual relationship.
We present now a linguistic typology of base-term
variations for French:
Graphical case differences and presence of a op-
tional hyphen inside the Noun1 Noun2 structure.
Inflexional orthographic variants gathering to-
gether inflexional variants that are predictable such
as conservations de produit (product preservations)
or unpredictable such as conservation de produits
(product preservation).
Shallow syntactic The shallow syntactic varia-
tions modify the function words of the base-terms.
There are three kinds of internal syntactic variations:
is-1 variations of the preposition: chromatographie
en colonne (column chromatography)  chro-
matographie sur colonne (chromatography on
column);
is-2 optional character of the preposition and of the
article: fixation azote (nitrogen fixation)  fix-
ation d?azote (fixation of nitrogen)  fixation
de l?azote (fixation of the nitrogen);
is-3 predicative variants: the predicative role of the
adjective: pectine me?thyle?e (methylate pectin)
 ces pectines sont me?thyle?es (these pectins
are methylated).
Syntactic The shallow syntactic variations modify
the internal structure of the base-terms:
S-1 Internal modification variants: insertion inside
the base-term structure of
  a modifier such as the adjective inside the
Noun1 Prep Noun2 structure: lait de bre-
bis (goat?s milk), lait cru de brebis (milk
straight from the goat);
  a nominal specifier inside the Noun Adj.
These specifiers belongs to a closed list
of nouns such as type, origine, couleur
(colour): prote?ine ve?ge?tale ?vegetable
protein?  prote?ine d?origine ve?ge?tale
?protein of vegetable origin?.
S-2 Coordinational variants: head or expansion co-
ordination of base term structures and enumer-
ation:
analyse de particules ?particule analysis? 
analyse et le tri de particules ?particle sort and
analysis?
alimentation humaine ?human feeding?  ali-
mentation animale et humaine ?human and an-
imal feeding?.
Morphosyntactic The Morphosyntactic varia-
tions modify the internal structure of the base-terms
and its components are liable to morphological
modification (including derivation).
M-1 Morphology : the preposition inside a can-
didate term of Noun1 Prep Noun2 structure
is equivalent to a prefix applying on Noun2:
pourrissement apre`s re?colte (rot after harvest)
 pourrissement post-re?colte (post-harvesting
rot) ;
M-2 Derivational morphology: a derivational vari-
ation that keeps the synonymy of the base
term implies a relational adjective: acidite? du
sang (acidity of the blood)  acidite? sanguine
(blood acidity). This morphosyntactic variation
could be associated with a syntactic variation:
the sequence: alimentation destine?e a` l?homme
et a` l?animal ?food destined to man and to ani-
mal? is a variation of the base-term: alimenta-
tion animale ?animal food?.
Two other types of variation could have been in-
cluded in this typology: paradigmatic and anaphori-
cal variations. The first one relies on the substitution
principle of distributional linguistics (Harris, 1968).
One or two words of the base-term could be sub-
stituted by one of their synonyms without modify-
ing the syntactic structure (Hamon and Nazarenko,
2001). The second one gathers elliptical anaphora
and acronyms.
3.2 Variations reflecting conceptual
relationships
All these variations are those which could preserve
synonymy with the base term. They can, of course,
include semantic discrepancies and can refer either
to two base terms or to a base term and a con-
ceptually linked term. Thus, two different prepo-
sitions lead to two base terms: transmission par
satellite (satellite transmission)   transmission en-
tre satellites (transmission between satellites) and
internal modification (see variation S-1a) refers to
a overcomposed term: huile essentielle de sapin (fir
essence) is a hyponym of huile essentielle (essence)
and not a variation of huile de sapin (fir oil).
We propose to identify the conceptual relation-
ships betwen base terms through syntactic or mor-
phological clues. We use standard lexical functions
to express the conceptual relationships. When there
does not exist a lexical function to label a conceptual
relationship, we introduce a new lexical function (i.e
a non standard one). Standard lexical function are
written in lower-case, non-standard in upper-case.
Syntactic The internal modification of the base
structures mainly implies two types of semantic re-
lationships:
  Hyperonymy: if it is a relational adjective
that modifies the base term of N1 Adj or
N1 Prep (Det) N2 structure, an hyperonymic
relationship occurs between the base term
and the modified one. The lexical function
that captures hyperonymic relationships is the
function Spec introduced by (Grimes, 1990):
Spec (contraction isome?trique ?isomet-
ric contraction?) = contraction musculaire
isome?trique ?isometric muscular contraction?
Spec (agent bacte?rien ?bacterial agent?) =
agent infectieux bacte?rien ?bacterial infectious
agent?
  Antonymy: if it is an adverb of negation that
modifies the base term of N1 Adj structure,
an antonymic relationship occurs between the
base term and the modified one. This relation-
ship of opposition is described with the func-
tion Anti:
Anti(levure floculante ?flocculating yeast?)=
levure non floculante ?non-flocculating yeast?
Morphosyntactic Semantic distinctions appear
with base terms that are morphologically related to
other base terms. Two base-terms     and  


 


are considered as morphologically-related if one of
the three following constraints are satisfied:
i.    and  


are head nouns and are identical.
  and  


are expansions and are semantically
related by the use of an affix;
ii.    and  


are head nouns and are semantically
related by the use of an affix.   and  


are
expansions and are identical;
iii.    and  


are head nouns,   and  


are ex-
pansions, either    and  


are identical and   
and  


are semantically related by the use of
a suffix such as preserved food/food preserva-
tion;
Some affixes that have been studied for French
by (Corbin, 1987) provide clues to character-
ize the semantic link occurring between two
morphologically-related candidate terms.
  Antonymy: the prefixes ir, de?, non(-) applying
either on the head or expansion element on a
base term whatever is its structure characterize
an antonymic relationship. Examples are:
Anti (solubilisation micellaire ?micellar
solubilization?) = insolubilisation micellaire
?micellar insolubilisation?
Anti (phe?nol polyme?rise? ?polymerized phe-
nol?) = phe?nol non-polyme?rise? ?unpolymerized
phenol?
  Set of: the suffixes age, ade applying on the
head noun of base term attest of a ?set of? rela-
tionship expressed with the function Mult:
Mult (plume de canard ?duck feather?) =
plumage de canards ?duck feather? The two
base-terms share the same pattern.
  Result: A ?result? relationship is expressed
with the function N 	
 applying on nouns. This
relation is induced either by:
? the suffixes age, ade, erie applying on the
head noun of base terms:
N 	
 (plumage de canards ?duck feather?)
= plume de canard ?duck feather?
N 	
 (filetage du saumon ?salmon fillet-
ing?) = filet de saumon ?salmon fillet?;
The two base-terms share the same pat-
tern.
? or by the suffixes age, ade, erie, ment,
tion, ure associated with an inversion. We
distinguish two cases:
 if this morphological link involves a
N Adj structure, the function N 	
 ap-
plies:
N 	
 (conservation des aliments ?food
preservation?) = aliment conserve?
?preserved food?;
 if the morphological link involves a
N a` Vinf structure, we face a non-
standard function where the term of N
a` Vinf structure expresses the state be-
fore the process. Thus, we introduce
the new function N  :
N  (conservation des aliments ?food
preservation?) = aliment a` conserver
?food to preserve?;
  Actor: the suffixe eur applying on the head
noun of a base term builds its actant expressed
with the function S  : S  (transport routier
?road transport? = transporteur routier ?road
haulier?. The two base-terms share the same
pattern.
Other semantic relationships involving two base-
terms with the same pattern are induced by prefixes.
For those, we have to introduce new functions as:
 
?again? relationship with the prefixes re, re?:
AGAIN(este?rification enzymatique ?enzymatic
esterification?) = re?este?rification enzymatique
?enzymatic reesterification?;
 
?before? relationship with the prefixe pre?(-):
BEFORE(traitement enzymatique ?enzymatic
treatment?) = pre?traitement enzymatique ?en-
zymatic pretreatment?.
4 Automatic discovery and structuring
4.1 Linguistic structuring
The term extractor program takes as input a tagged
and lemmatized corpus. The programme imple-
ments shallow parsing and morphological conflat-
ing. First, it scans the corpus, counts and extracts
strings whose syntax characterizes base-terms or
one of their variants. This collecting step uses lo-
cal grammars based on regular expressions (Abney,
1997). These grammars use the morphosyntactic
information associated with the words of the cor-
pus by the tagger. The different occurrences re-
ferring to a base term or one of its variants are
grouped as a pair formed by lemmas of the can-
didate base term. Second, morphological analysis
is performed to confluate synomymic derivational
variants of base terms such as acidite? du sang (acid-
ity of the blood)  acidite? sanguine (blood acid-
ity). Stripping-recoding morphological rules adopt
the following rule schemata:
 
 	

where:
S is the relational suffix to be deleted from the end
of an adjective. The result of this deletion is the
stem R;
M is the mutative segment to be concatenated to R
in order to form a noun.
For example, the rule [ -e? +e ] says that if there is
an adjective which ends with e?, we should strip this
ending from it and append the string e to the stem.
The algorithm below resumes the successive steps
for identifying relational adjectives:
1. Examine each candidate of Noun Adj structure;
2. Apply a transformational rule in order to gener-
ate all the possible corresponding base nouns.
3. Search the set of candidate terms for a
pair formed with Noun1 (identical between a
Noun1 (Prep (Det)) Noun2 and a Noun1 Adj
structures) and Noun2 generated from step 2.
4. If step 3 succeeds, group the two base struc-
tures under an unique candidate term.
In Step 2, morphological rules generate one or sev-
eral nouns for a given adjective. We generate a noun
for each relational suffix class. A class of suffixes in-
cludes the allomorphic variants. This overgeneration
method used in information retrieval by (Jacquemin,
2001) gives low noise because the base noun must
not only be an attested form in the corpus, but must
also appear as an extension of a head noun.
At the end of the linguistic processing, the term
extractor proposes as output:
1. a list of pilot terms ranked from the most repre-
sentative of the corpus to the least thanks to the
Loglikelihood coefficient introduced by (Dun-
ning, 1993).
2. for each pilot term, a XML structure is pro-
vided which gathers all the base structures and
the variations encountered.
An example of such data is given in figure in Table 1.
4.2 Conceptual structuring
The conceptual structuring takes as input the data
provided by the first step. First, we present the
methodology employed to exploit variables of base
terms. We then demonstrate the labelling of concep-
tual links through morphological analysis.
4.2.1 Treatment of modification variants
In the previous step, a first list of relational ad-
jectives has been established thanks to their para-
phrasic property. (Daille, 2001) demonstrated that
candidate terms of N Adj structure where Adj is re-
lational hold a more important naming potential than
for the synonym form in N1 Prep N2. The absence
of paraphrases, the non-paraphrasability, or a com-
plex paraphrasability or a large derivational distance
between the adjective and the noun do not allow ex-
haustive identification. We extend this list by ex-
ploiting the coordination variations of N Adj base
terms. Indeed, a relational adjective holds the prop-
erty to coordinate only with other relational adjec-
tives. To summarise:
1. From industrie de l?alimentation ?food indus-
try? and industrie alimentaire ?food industry?,
Sorted list of candidate terms
Score Pilot term Index
785 acide gras 926
722 mise au point 344
629 acide aminer 394
559 matie`re grasse 2002
512 re?sultat obtenir 155
472 chromatographie gazeuse 1374
469 bacte?rie lactique 118
           
XML structure associated to chromatographie gazeuse
 CAND ident=1374 freq=103 
 NPN freq=9   BASE   TERM  chromatographie du gaz  /TERM 
 TERM  chromatographie gaz  /TERM 
 TERM  chromatographie de gaz  /TERM 
 /BASE 
 /NPN 
 NPNA freq=80   BASE   TERM  chromatographie en phase gazeuze  /TERM 
 /BASE 
 MODIF   TERM  chromatographie capillaire en phase gazeuze  /TERM 
 /MODIF 
 /NPNA 
 NA freq=14   BASE   TERM  chromatographie gazeuze  /TERM 
 /BASE 
 /NA 
 /CAND 
Table 1: Output of the first step
we deduce that alimentaire is a relational ad-
jective;
2. From the coordinational variant produit agri-
cole et almentaire ?farm and food product?,
we deduce that agricole is a relational adjec-
tive.
This classic learning algorithm that is normally
bound by the number of adjectives in the corpus con-
verges in five steps. It allows to extend the set of re-
lational adjectives from 143 to 239. The following
are some examples of acquired relational adjectives:
Relational Number of Coordinated relational
adjective iterations adjectives
gazeux 1 ( microbien solide liquide
organoleptique )
ferme 2 ( e?lastique )
productif 1 ( )
global 2 ( micro-e?conomique
spe?cifique local )
peroxydasique 1 ( polyphe?noloxydasique
lipoxyge?nasique
catalasique)
hydrodynamique 3 ( thermique )
Using this extended list of relational adjectives,
we automatically check all the modification variants
of collected base-terms:
  if a relational adjective is present, we infer
an hyperonymy link between the variant and
the base term as for contraction isome?trique
?isometric contraction? and contraction mus-
culaire isome?trique ?isometric muscular con-
traction?, but not for organisation ordonne?e
des mole?cules ?ordered molecule organization?
that remains a syntactic variation of organisa-
tion mole?culaire ?molecule organization?;
  if an adverb of negation is present, we infer an
antonymy link between the variant and the base
term as for brunissement non enzymatique ?non
enzymatic browning? and brunissement enzy-
matique ?enzymatic browning?.
4.2.2 Morphological conflating
To identify the conceptual relationships denoted
by derivational links, we perform a morphological
analysis using the same method as in section 4.1:
we wrote stripping-recoding morphological rules for
each conceptual relationship, we apply the overgen-
eration method and the filtering based on the pres-
ence or not of the generated base term candidates. In
order to browse the list of candidate terms, we apply
to each candidate terms successively all the possible
derivations.
The output of the conceptual structuring program
is a list of candidate terms ranked, each of them
representing a set of conceptually linked candidate
terms. An example of such structure is given in Ta-
ble 2.
5 Results and Evaluation
We apply our program on a technical corpus in the
field of agriculture which consists of 2,702 scientific
abstracts for a total of 427,482 tokens and an average
size of a record of 316 tokens.
Table 3 gives the results of the collecting phase and
Table 4 shows the percentages of the different types
of variations for candidate terms appearing at least
two times and the number of synonymic conflations.
This conflating has a linguistic precision of 99 %.
Number of occurrences 1   2 Total
base structures
Nom1 Prep (Det) Nom2 17 232 5 949 23 181
Nom Adj 12 344 4 778 17 122
Nom a` Vinf 203 16 219
Total 29 912 10 895 40 807
Table 3: Number of candidate base terms
Syntactic variation
Coor + Modif Coor Modif
61 (0,5 %) 458 (4 %) 1651 (15,1 %)
19,2 %
Morphological variation
N1 (Prep (Det)) N2 / N1 AdjR
with AdjR derived from N2
343
Table 4: Number of base-term variations
Conceptual Link Syntactic Morphological
Spec 731
Anti 183 106
N  132
MICRO 59
AGAIN 36
BEFORE 29
Mult 23
INTER 20
         
Total 914 558
Table 5: Number of major conceptual links
Table 5 gives the number of the most frequent auto-
matically identified conceptual relationships.
Concerning morphological links, we note that two
non-standard functions that have not been presented
yet obtain a consequent representativity: MICRO
induces from the suffixe micro(-): MICRO(film per-
fore? ?perforated film?) = film micro-perfore? ?pin-
hole film?; INTER infers from the suffixe inter ex-
pressing a reprocivity relationship: INTER(e?chelle
nationale ?national scale?) = e?chelle internationale
?international scale?. The average precision of
the morphological links is 95 %. The wrong
links are 75 % due to the prefixes re, re? refering
to the function AGAIN. Examples of false drop
are: action/re?action ?reaction? in several candidate
base-terms: action/re?action enzimatique ?enzimatic
action/reaction?, action/re?action acide ?acide ac-
tion/reaction?,     , production/reproduction, solu-
tion/re?solution ?resolution?, etc.
6 Conclusion
Links between complex terms can be used to as-
sist terminolographers to handle extracted termino-
logical data more conveniently, since several related
concepts are clustered. This conceptual structuring
relies on term variation and we have stressed the cru-
cial part of this handling. The method can be eas-
ily adapted to other romance languages and English
for which it suffices to define patterns for base-terms
and their variations and to list appropriate affixes re-
flecting semantic relationships.
References
Steven Abney. 1997. Part-of-Speech Tagging and Partial
Parsing. In Steve Young and Gerrit Bloothooft, edi-
tors, Corpus-Based Methods in Language and Speech
Processing, volume 2, chapter 4. Kluwer Academic
Publishers.
M. Teresa Cabre?, Rosa Estopa` Bagot, and Jordi Vivaldi
Platresi. 2001. Automatic term detection: A re-
view of current systems. In Didier Bourigault, Chris-
tian Jacquemin, and Marie-Claude L?Homme, editors,
Recent Advances in Computational Terminology, vol-
ume 2 of Natural Language Processing, pages 53?88.
John Benjamins.
Danielle Corbin. 1987. Morphologie de?rivationnelle
et structuration du lexique. Max Niemeyer Verlag
Tu?bingen.
XML structure
 SETCAND ident=1613 
 LINK type=spec ident1=1643 ident2=15789 
 LINK type=anti ident1=1643 ident2=25128 
 LINK type=N  ident1=1643 ident2=16667 
 CAND ident=1643 freq=2 
 NPN freq=2   BASE   TERM  phosphorylation de la case?ine  /TERM 
 /BASE 
 /NPN 
 /CAND 
 CAND ident=15789 freq=1 
 NPN freq=1   MODIF   TERM  phosphorylation chimique de la case?ine  /TERM 
 /MODIF 
 /NPN 
 /CAND 
 CAND ident=1667 freq=1 
 NA freq=3   BASE   TERM  case?ine phosphoryle?e  /TERM 
 /BASE 
 /NA 
 /CAND 
 CAND ident=25128 freq=2 
 NPN freq=1   BASE   TERM  de?phosphorylation de la case?ine  /TERM 
 /BASE 
  NPN 
 /CAND 
 /SETCAND 
Table 2: Example of conceptual structuring
Judit Feliu Corte`s and M. Teresa Cabre?. 2002. Concep-
tual relations in specializes texts: new typology and an
extraction system proposal. In Proceeding of the 6th
International Conference of Terminology and Knowl-
egde Engineering (TKE?02).
Be?atrice Daille. 2001. Qualitative terminology extrac-
tion. In Didier Bourigault, Christian Jacquemin, and
Marie-Claude L?Homme, editors, Recent Advances
in Computational Terminology, volume 2 of Natu-
ral Language Processing, pages 149?166. John Ben-
jamins.
Ted Dunning. 1993. Accurate methods for the statistics
of surprise and coincidence. Computational Linguis-
tics, 19(1):61?74.
Natalia Grabar and Pierre Zweigenbaum. 2000. Au-
tomatic acquisition of domain-specific morphological
resources from thesauri. In In Proceedings of RIAO
2000: Content-Based Multimedia Information Access,
pages 765?784, Paris, France.
J. Grimes. 1990. Inverse lexical functions. In J. Steele,
editor, Meaning-Text Theory: Linguistics, Lexicogra-
phy and Implications, pages 350?364. Ottawa Univer-
sity Press, Ottawa.
Thierry Hamon and Adeline Nazarenko. 2001. Detection
of synonymy link between terms: Experiment and re-
sults. In Didier Bourigault, Christian Jacquemin, and
Marie-Claude L?Homme, editors, Recent Advances
in Computational Terminology, volume 2 of Natu-
ral Language Processing, pages 185?208. John Ben-
jamins.
Zelig S. Harris. 1968. Mathematical Structures of Lan-
guage. Wiley, New York.
C. Jacquemin. 2001. Spotting and Discovering Terms
through Natural Language Processing. Cambridge:
MIT Press.
Kyo Kageura. 2002. The Dynamics of Terminology:
A Theoretico-Descriptive Study of Term Formation
and Terminological Growth, volume 5 of Terminology
and Lexicography Research and Practice. John Ben-
jamins.
M.-C. L?Homme. 2002. Fonctions lexicales pour
repre?senter les relations smantiques entre termes.
Traitement automatique des langues (TAL), 43(1):19?
42.
Emmanuel Morin and Christian Jacquemin. 1999. Pro-
jecting corpus-based semantic links on a thesaurus. In
Proceedings of the 37th Annual Meeting of the Associ-
ation for Computational Linguistics (ACL?99), pages
389?396.
J.C. Sager. 1990. A Practical Course in Terminology
Processing. John Benjamins.
Leo Wanner, editor. 1996. Lexical Functions in Lexicog-
raphy and Natural Language Processing. John Ben-
jamins, Amsterdam/Philadelphia.
 
	
	Proceedings of the 2nd Workshop on Building and Using Comparable Corpora, ACL-IJCNLP 2009, pages 55?63,
Suntec, Singapore, 6 August 2009. c?2009 ACL and AFNLP
Compilation of Specialized Comparable Corpora in French and Japanese
Lorraine Goeuriot, Emmanuel Morin and B?atrice Daille
LINA - Universit? de Nantes
France
firstname.lastname@univ-nantes.fr
Abstract
We present in this paper the development
of a specialized comparable corpora com-
pilation tool, for which quality would be
close to a manually compiled corpus. The
comparability is based on three levels: do-
main, topic and type of discourse. Domain
and topic can be filtered with the keywords
used through web search. But the detec-
tion of the type of discourse needs a wide
linguistic analysis. The first step of our
work is to automate the detection of the
type of discourse that can be found in a
scientific domain (science and popular sci-
ence) in French and Japanese languages.
First, a contrastive stylistic analysis of the
two types of discourse is done on both lan-
guages. This analysis leads to the creation
of a reusable, generic and robust typology.
Machine learning algorithms are then ap-
plied to the typology, using shallow pars-
ing. We obtain good results, with an av-
erage precision of 80% and an average re-
call of 70% that demonstrate the efficiency
of this typology. This classification tool
is then inserted in a corpus compilation
tool which is a text collection treatment
chain realized through IBM UIMA system.
Starting from two specialized web docu-
ments collection in French and Japanese,
this tool creates the corresponding corpus.
1 Introduction
Comparable corpora are sets of texts in differ-
ent languages, that are not translations, but share
some characteristics (Bowker and Pearson, 2002).
They represent useful resources from which are
extracted multilingual terminologies (D?jean et
al., 2002) or multilingual lexicons (Fung and Yee,
1998). Comparable corpora are also used in
contrastive multilingual studies framework (Peters
and Picchi, 1997), they constitute a precious re-
source for translators (Laviosa, 1998) and teachers
(Zanettin, 1998), as they provide a way to observe
languages in use.
Their compilation is easier than parallel corpora
compilation, because translated resources are rare
and there is a lack of resources when the languages
involved do not include English. Furthermore, the
amount of multilingual documents available on the
Web ensures the possibility of automatically com-
piling them. Nevertheless, this task can not be
summarized to a simple collection of documents
sharing vocabulary. It is necessary to respect the
common characteristics of texts in corpora, es-
tablished before the compilation, according to the
corpus finality (McEnery and Xiao, 2007). Many
works are about compilation of corpora from the
Web (Baroni and Kilgarriff, 2006) but none, in our
knowledge, focuses on compilation of compara-
ble corpora, which has to satisfy many constraints.
We fix three comparability levels: domain, topic
and type of discourse. Our goal is to automate
recognition of these comparability levels in docu-
ments, in order to include them into a corpus. We
work on Web documents on specialized scientific
domains in French and Japanese languages. As
document topics can be filtered with keywords in
the Web search (Chakrabarti et al, 1999), we fo-
cus in this paper on automatic recognition of types
of discourse that can be found in scientific docu-
ments: science and popular science. This classi-
fication tool is then inserted in a specialized com-
parable corpora compilation tool, which is devel-
opped through the Unstructured Information Man-
55
agement Architecture (UIMA) (Ferrucci and Lally,
2004).
This paper is structured as follows. After an in-
troduction of related works in section 2, stylistic
analysis of our corpus will be presented in sec-
tion 3. This analysis will lead to the creation of
a typology of scientific and popular science dis-
course type in specifialized domains. The appli-
cation of learning algorithms to the typology will
be described in section 4, and the results will be
presented in section 5. We will show that our ty-
pology, based on linguistically motivated features,
can characterize science and popular science dis-
courses in French and Japanese documents, and
that the use of our three comparablility levels can
improve corpora comparability. Finally, we de-
scribe the development of the corpus compilation
tool.
2 Background
?A comparable corpus can be defined as a corpus
containing components that are collected using
the same sampling frame and similar balance and
representativeness? (McEnery and Xiao, 2007, p.
20). Comparability is ensured using character-
istics which can refer to the text creation con-
text (period, author...), or to the text itself (topic,
genre...). The choice of the common characteris-
tics, which define the content of corpora, affects
the degree of comparability, notion used to quan-
tify how two corpora can be comparable. The
choice of these characteristics depends on the fi-
nality of the corpus. Among papers on comparable
corpora, we distinguish two types of works, which
induces different choices:
? General language works, where texts of cor-
pora usually share a domain and a period.
Fung and Yee (1998) used a corpus composed
of newspaper in English and Chinese on a
specific period to extract words translations,
using IR and NLP methods. Rapp (1999)
used a English / German corpus, composed of
documents coming from newspapers as well
as scientific papers to study alignment meth-
ods and bilingual lexicon extraction from
non-parallel corpora (which can be consid-
ered as comparable);
? Specialized language works, where choice of
criteria is various. D?jean et al (2002) used a
corpus composed of scientific abstracts from
Medline, a medical portal, in English and
German. Thus they used documents sharing a
domain and a genre to extract bilingual termi-
nology. Chiao (2002) used a corpus of docu-
ments of medical domain on a specific topic
to work on the extraction of specialized ter-
minologies.
In general language works, documents of compa-
rable corpora often share characteristics like do-
main or topic. As they are usually extracted from
newspapers, it is important to limit them to a cer-
tain period to guarantee their comparability.
In specialized corpora, first levels of compara-
bility can be achieved with the domain and the
topic. Moreover, several communicative settings
appear in specialized language (Bowker and Pear-
son, 2002): expert-expert, expert-initiate, relative
expert to the uninitiated, teacher-pupil. Malrieu
and Rastier (2002) specify several levels of tex-
tual classification, each of which corresponding to
a certain granularity. The first level is discourse,
defined as a set of utterances from a enunciator
characterized by a global topical unit (Ducrot and
Todorov, 1972). The second level is genre, de-
fined as text categories distinguished by matured
speakers. For example, to literary discourse corre-
spond several genres: drama, poetry, prose. . . In-
spired by these communicative settings and tex-
tual categories, we choose to distinguish two com-
municative settings or type of discourse in spe-
cialized domains: science (texts written by ex-
perts to experts) and popular science (texts written
to non-experts, by experts, semi-experts or non-
experts). This comparability level, the type of dis-
course, reflects the context of production or usage
of the documents, and guarantees a lexical homo-
geneity in corpora (Bowker and Pearson, 2002, p.
27). Furthermore, Morin et al (2007) proved that
comparable corpora sharing a topic and a type of
discourse are well adapted for multilingual termi-
nologies extraction.
Our goal is to create a tool to compile compa-
rable corpora in French and Japanese which docu-
ments are extracted from the Web. We investigate
automatic categorization of documents according
to their type of discourse. This categorization is
based on a typology of elements characterizing
these types of discourse. To this end, we carry
out a stylistic and contrastive analysis (Karlgren,
1998). This analysis aims to highlight linguis-
tically motivated features through several dimen-
56
sions (structural, modal and lexical), whose com-
bination characterizes scientific or popular science
discourse. A specialized comparable corpus can
be compiled from a single type of discourse docu-
ment collection through several steps. Last part of
this paper focuses on the automation of these steps
using the IBM Unstructured Information Manage-
ment Architecture (UIMA).
3 Analysis of Types of Discourse
The recognition of types of discourse is based
on a stylistic analysis adapted from a deductive
and contrastive method, which purpose is to raise
discriminant and linguistically motivated features
characterizing these two types of discourse. Main
difficulty here is to find relevant features which fit
every language involved. These features, gathered
in a typology, will be used to adapt machine learn-
ing algorithms to compilation of corpora. This
typology thus needs to be robust, generic and
reusable in other languages and domains. Gener-
icity is ensured by a broad typology composed of
features covering a wide range of documents char-
acteristics, while robustness is guaranteed with
operational (computable) features and treatment
adaptable to Web documents as well as texts.
Sinclair (1996) distinguishes two levels of anal-
ysis in his report on text typologies: external level,
characterizing the context of creation of the docu-
ment; and internal level, corresponding to linguis-
tic characteristics of document. Because our cor-
pora are composed of documents extracted from
the Web, we consider external level features as
all the features related to the creation of docu-
ments and their structure (non-linguistic features)
and call them structural features. Stylistic analy-
sis raises several granularity levels among linguis-
tic characteristics of the texts. We thus distinguish
two levels in the internal dimension. Firstly, in
order to distinguish between scientific and pop-
ular science documents, we need to consider the
speaker in his speech: the modality. Secondly, sci-
entific discourse can be characterized by vocabu-
lary, word length and other lexical features. There-
fore our typology is based on three analysis levels:
structural, modal and lexical.
3.1 Structural Dimension
When documents are extracted from the Web, the
structure and the context of creation of the doc-
uments should be considered. In the framework
Feature French Japanese
URL pattern ?
Document?s format ? ?
Meta tags ? ?
Title tag ? ?
Pages layout ? ?
Pages background ? ?
Images ? ?
Links ? ?
Paragraphs ? ?
Item lists ? ?
Number of sentences ? ?
Typography ? ?
Document?s length ? ?
Table 1: Structural dimension features
of Web documents classification, several elements
bring useful information: pictures, videos and
other multimedia contents (Asirvatham and Ravi,
2001); meta-information, title and HTML struc-
ture (Riboni, 2002). While those information are
not often used in comparable corpora, they can be
used to classify them. Table 1 shows structural
features.
3.2 Modal Dimension
The degree of specialization required by the recip-
ient or reader is characterized by the relation built
in the utterance between the speaker or author and
the recipient or reader1. The tone and linguistic
elements in texts define this relation. The modal-
isation is an interpretation of the author?s attitude
toward the content of his/her assertion. Modali-
sation is characterized by many textual markers:
verbs, adverbs, politeness forms, etc. Presence of
the speaker and his position towards his speech
are quite different in scientific and popular science
discourse. Thus we think modalisation markers
can be relevant. For example, the speaker directly
speaks to the reader in some popular science doc-
uments: ?By eating well, you?ll also help to pre-
vent diabetes problems that can occur later in life,
like heart disease?. Whereas a scientific document
would have a neutral tone: ?Obesity plays a cen-
tral role in the insulin resistance syndrome, which
includes hyperinsulinemia, [. . . ] and an increased
risk of atherosclerotic cardiovascular disease?.
Most of the modal theories are language de-
pendent, and use description phenomena that are
specific to each language. Conversely, the theory
exposed in (Charaudeau, 1992) is rather indepen-
1Since we work on a scientific domain, we will consider
the speaker as the author of texts, and the recipient as the
reader.
57
dent of the language and operational for French
and Japanese (Ishimaru, 2006). According to Cha-
raudeau (1992, p.572), modalisation clarifies the
position of the speaker with respect to his reader,
to himself and to his speech. Modalisation is com-
posed of locutive acts, particular positions of the
author in his speech, and each locutive act is char-
acterized by modalities. We kept in his theory two
locutive acts involving the author:
Allocutive act: the author gets the reader in-
volved in the speech (ex.: ?You have to do
this.?);
Elocutive act: the author is involved in his own
speech, he reveals his position regarding his
speech (ex.: ?I would like to do this.?).
Each of these acts are then divided into several
modalities. These modalities are presented in ta-
ble 2 with English examples. Some of the modali-
ties are not used in a language or another, because
they are not frequent or too ambiguous.
3.3 Lexical Dimension
Biber (1988) uses lexical information to observe
variations between texts, especially between gen-
res and types of texts. Karlgren (1998) also use
lexical information to characterize text genres, and
use them to observe stylistic variations among
texts. Thus, we assume that lexical information
is relevant in the distinction between science and
popular science discourse. Firstly, because a spe-
cialized vocabulary is a principal characteristic of
specialized domain texts (Bowker and Pearson,
2002, p. 26). Secondly, because scientific docu-
ments contain more complex lexical units, nomi-
nal compounds or nominal sentences than popular
science documents (Sager, 1990).
Table 3 presents the lexical dimension features.
Note that these features show a higher language
dependency than other dimension features.
4 Automatic Classification by Type of
Discourse
The process of documents classification can be di-
vided into three steps: document indexing, classi-
fier learning and classifier evaluation (Sebastiani,
2002). Document indexing consists in building
a compact representation of documents that can
be interpreted by a classifier. In our case, each
document di is represented as a vector of fea-
tures weight: ~di = {w1i, . . . , wni} where n is the
Feature French Japanese
Specialized vocabulary ? ?
Numerals ? ?
Units of measurement ? ?
Words length ?
Bibliography ? ?
Bibliographic quotes ? ?
Punctuation ? ?
Sentences end ?
Brackets ? ?
Other alphabets (latin, ?
hiragana, katakana)
Symbols ?
Table 3: Lexical dimension features
Dimension Method
Structural Pattern matching
Modal Lexical and lexico-syntactic patterns
Lexical Lexical patterns
Table 4: Markers detection methods
number of features of the typology and wij is the
weight of the jth feature in the ith document. Each
feature weight is normalized, dividing the weight
by the total. Documents indexing is characterized
by our typology (section 3) and features imple-
mentation.
4.1 Features Implementation
In order to get a fast classification system, we priv-
ileged for the implementation of our typology fea-
tures shallow parsing such as lexical markers and
lexico-syntactic patterns (method for each dimen-
sion is detailed in table 4).
Structural Features We used 12 structural fea-
tures introduced in section 3.1. Most of these fea-
tures are achieved through pattern matching. For
example, URL patterns can determine is the docu-
ment belongs to websites such as hospital (http:
//www.chu-***.fr) or universities websites
(http://www.univ-***.fr), etc. As for
paragraphs, images, links, etc., one simple search
of HTML tags was made.
Modal Features Locutor presence markers in
a text can be implicit or ambiguous. We fo-
cused here on simple markers of his presence in
order to avoid noise in our results (high preci-
sion but weak recall). Thus we don?t recognize
all modal markers in a text but those recognized
are correct. There are pronouns which are spe-
cific to the speech act: for instance, for the eloc-
utive act, the French pronouns je (I) and nous
(we), and the Japanese pronouns? (I),?? (we)
58
Feature Example French Japanese
Allocutive modality
Allocutive personal pronouns You ?
Injunction modality Don?t do this ? ?
Authorization modality You can do this ?
Judgement modality Congratulations for doing it! ?
Suggestion modality You should do this ? ?
Interrogation modality When do you arrive? ? ?
Interjection modality How are you, Sir? ?
Request modality Please, do this ? ?
Elocutive modality
Elocutive personal I, we ? ?
Noticing modality We notice that he left ? ?
Knowledge modality I know that he left ? ?
Opinion modality I think he left ? ?
Will modality I would like him to leave ? ?
Promise modality I promise to be here ? ?
Declaration modality I affirm he left ?
Appreciation modality I like this ?
Commitment modality We have to do this ?
Possibility modality I can inform them ?
Table 2: Modal dimension features
and ?? (we). The modalities are also com-
puted with lexical markers. For example, the
modality of knowledge can be detected in French
with verbs like savoir, conna?tre (know), and in
Japanese with the verb ?? (know), with po-
lite form ?????? and with neutral form
?????.
Lexical Features Some of our lexical criteria
are specific to the scientific documents, like bib-
liographies and bibliographic quotations, special-
ized vocabulary or the measurement units. To
measure the terminological density (proportion of
specialized vocabulary in the text) in French, we
evaluate terms with stems of Greek-Latin (Namer
and Baud, 2007) and suffix characters of rela-
tional adjectives that are particularly frequent in
scientific domains (Daille, 2000). We listed about
50 stems such as inter-, auto- or nano-, and the
10 relational suffixes such such as -ique or -al.
For Japanese, we listed prefix characteristics of
names of disease or symptoms (??? (congen-
ital), ???(hereditary), etc.). These stems can
be found in both type of discourse, but not in the
same proportions. Specialized terms are used in
both type of discourse in different ways. For ex-
ample, the term ?ovarectomie? (ovarectomy) can
be frequent in a scientific document and used once
in a popular science documents to explain it and
then replaced by ?ablation des ovaires? (ovary ab-
lation). Sentences end are specific ending particles
used in japanese, for example the particle? is of-
ten used at the end of an interrogative sentence.
4.2 Learning Algorithms
Classifier learning is a process which observes fea-
tures weight of documents classified in a class
c or c and determine characteristics that a new
document should have to be classified in one of
these two classes 2. Given a document indexing,
there are some well-known algorithms that can
achieve this process (neural network, Bayes clas-
sifiers, SVM, etc.) of which Sebastiani (2002) car-
ried out a research about the assemblage and com-
parison. Applied to a Reuters newswires corpus,
these techniques showed variable performances in
the usage level of supervised or unsupervised ap-
proaches, of the size of the corpus, of the number
of categories, etc. We decided to use SVMlight
(Joachims, 2002) and C4.5 (Quinlan, 1993), since
both of them seem to be the most appropriate to
our data (small corpora, binary classification, less
than 100 features).
5 Experiments
In this section, we describe the two comparable
corpora used and present the two experiments car-
ried out with each of them. The first compara-
ble corpus is used to train the classifier in order
to learn a classification model based on our typol-
ogy (i.e. training task). The second comparable
corpus is used to evaluate the impact of the clas-
sification model when applied on new documents
(i.e. evaluation task).
2This is the binary case. See (Sebastiani, 2002) for other
cases.
59
5.1 Comparable Corpora
The corpora used in our experiments are both
composed of French and Japanese documents har-
vested from the Web. The documents were taken
from the medical domain, within the topic of di-
abetes and nutrition for training task, and breast
cancer for the evaluation task. Document harvest-
ing was carried out with a domain-based search
and a manual selection. Documents topic is fil-
tered using keywords reflecting the specialized
domain: for example alimentation, diab?te and
ob?sit? 3 for French part and ??? and ?? 4
for the Japanese part of the training task corpus.
Those keywords are directly related to the topic or
they can be synonyms (found on thesaurus) or se-
mantically linked terms (found in Web documents
collected). Then the documents were manually se-
lected by native speakers of each language who are
not domain specialists, and classified with respect
to their type of discourse: science (SC) or pop-
ular science (PS). Manual classification is based
on the following heuristics, to decide their type of
discourse:
? A scientific document is written by special-
ists to specialists.
? We distinguish two levels of popular science:
texts written by specialists for the general
public and texts written by the general pub-
lic for the general public. Without distinction
of these last two levels, we privileged doc-
uments written by specialists, assuming that
they may be richer in content and vocabulary
(for example advices from a doctor would be
richer and longer than forum discussions).
Our manual classification is based on the two
previous heuristics, and endorsed by several em-
pirical elements: website?s origin, vocabulary
used, etc. The classification of ambiguous docu-
ments has been validated by linguists. A few doc-
uments for which it was difficult to decide on the
type of discourse, such as those written by peo-
ple whose specialist status was not clear, were not
retained.
We thus created two comparable corpora:
? [DIAB_CP] related to the topic of diabetes
and nutrition and used to train the classifier.
3nutrition, diabetes, and obesity
4diabetes and overweight
? [BC_CP] related to the topic of breast cancer
and used to evaluate the effectiveness of the
classifier.
Table 5 shows the main features of each compa-
rable corpora: the number of documents, and the
number of words5 for each language and each type
of discourse.
# docs # words
[DIAB_CP]
FR SC 65 425,781PS 183 267,885
JP SC 119 234,857PS 419 572,430
[BC_CP]
FR SC 50 443,741PS 42 71,980
JP SC 48 211,122PS 51 123,277
Table 5: Basic data on each comparable corpora
5.2 Results
We present in this section two classification tasks:
? the first one consists in training and test-
ing classifiers with [DIAB_CP], using N-fold
cross validation method that consists in divid-
ing the corpus into n sub-samples of the same
size (we fix N = 5). Results are for 5 parti-
tioning on average;
? the second one consists in testing on [BC_CP]
the best classifier learned on [DIAB_CP], in
order to evaluate its impact on new docu-
ments.
Tables 6 and 7 show results of these two tasks.
On both table we present precision and recall
metrics with the two learning systems used. On
table 6, we can see that the results concerning
the French documents are quite satisfactory alto-
gether, with a recall on average of 87%, and a pre-
cision on average of 90% as for the classifier C4.5
(more than 215 documents are well classified from
248 French documents of [DIAB_CP]). The re-
sults of the classification in Japanese are also good
with the classifier C.4.5. More than 90% of doc-
uments are correctly classified, and the precision
reaches on average 80%. Some of the lower results
can be explained, especially in Japanese by the
high range of document genres in the corpus (re-
search papers, newspapers, scientific magazines,
recipes, job offers, forum discussions. . . ).
5For Japanese, the number of words is the number of oc-
currences recognized by ChaSen (Matsumoto et al, 1999)
60
French Japanese
Prec. Rec. Prec. Rec.
SC 1.00 0.36 0.70 0.65
svm
l
PS 0.80 1,00 0.72 0.80
SC 0.89 0.80 0.76 0.96
c4
.5
PS 0.91 0.94 0.95 0.99
Table 6: Precision and recall for each language,
each classifier, on [DIAB_CP]
Table 7 shows results on [BC_CP]. In general,
we note a decrease of the results with [BC_CP],
although results are still satisfactory. French doc-
uments are well classified whatever the classifier
is, with a precision higher than 75% and a recall
higher than 75%, which represent more than 70
well classified documents on 92. Japanese docu-
ments are well classified too, with 76% precision
and 77% recall on average, with 23 documents
wrong classified on 99. This classification model
is effective when it is applied to a different medi-
cal topic. This classification model seems efficient
to recognize scientific discourse from popular sci-
ence one in French and Japanese documents on a
particular topic.
French Japanese
Prec. Rec. Prec. Rec.
SC 0.92 0.53 0.90 0.61
svm
l
PS 0.64 0.95 0.66 0.98
SC 0.70 0.92 0.76 0.70
c4
.5
PS 0.87 0.56 0.75 0.80
Table 7: Precision and recall for each language,
each classifier, on [BC_CP]
6 Comparable Corpora Compilation
Tool
Compilation of a corpus, whatever type it is, is
composed of several steps.
1. Corpus Specifications: they must be defined
by the creator or user of the corpus. It in-
cludes decisions on its type, languages in-
volved, resources from which are extracted
documents, its size, etc. In the case of spe-
cialized comparable corpora, specifications
concern languages involved, size, resources
and documents domain, theme and type of
discourse. This step depends on the applica-
tive goals of the corpus and has to be done
carefully.
2. Documents Selection and Collection:
according to the resource, size and other
corpus criteria chosen during the first step,
documents are collected.
3. Documents Normalization and Annotation:
cleaning and linguistic treatments are applied
to documents in order to convert them into
raw texts and annotated texts.
4. Corpus Documentation: compilation of a
corpus that can be used in a durable way
must include this step. Documentation
of the corpus includes information about
the compilation (creator, date, method,
resources, etc.) and information about the
corpus documents. Text Encoding Initiative
(TEI) standard has been created in order to
conserve in an uniformed way this kind of
information in a corpus 6.
A corpus quality highly depends on the first two
steps. Moreover, these steps are directly linked to
the creator use of the corpus. The first step must
be realized by the user to create an relevant corpus.
Although second step can be computerizable (Ro-
gelio Nazar and Cabr?, 2008), we choose to keep
it manual in order to guarantee corpus quality. We
decided to work on a system which realizes the
last steps, i.e. normalization, annotation and docu-
mentation, starting from a collection of documents
selected by a user.
Our tool has been developed on Unstructured
Information Management Architecture (UIMA)
that has been created by IBM Research Divi-
sion (Ferrucci and Lally, 2004). Unstructured
data (texts, images, etc.) collections can be eas-
ily treated on this platform and many libraries are
available. Our tool starts with a web documents or
texts collection and is composed of several com-
ponents realizing each part of the creation of the
corpus:
1. the collection is loaded and documents are
converted to texts (with conversion tools
from pdf or html to text mainly);
2. all texts are cleaned and normalized (noise
from the conversion is cleaned, all texts are
converted into the same encoding, etc.);
6http://www.tei-c.org/index.xml
61
3. a pre-syntactic treatment is applied on texts
(segmentation mainly) to prepare them for
the following step;
4. morphologic and morpho-syntactic tagging
tools are applied on the texts (Brill tagger
(Brill, 1994) and Flemm lemmer (Namer,
2000) for French texts, Chasen (Matsumoto
et al, 1999) for Japanese);
5. texts are classified according to their type
of discourse: we use here the most efficient
SVMlight classifier. In fact, two corpus are
created, on for each type of discourse, then
the user can choose one of them. A vecto-
rial representation of each document is com-
puted, then these vectors are classified with
the classifier selected.
6. documentation is produced for the corpus, a
certain amount of information are included
and they can be easily completed by the user.
In reality, this tool is more a compilation assis-
tant than a compilator. It facilitates the compila-
tion task: the user is in charge of the most im-
portant part of the compilation, but the technical
part (treatment of each document) is realized by
the system. This guarantee a high quality in the
corpus.
7 Conclusion
This article has described a first attempt of com-
piling smart comparable corpora. The quality is
close to a manually collected corpus, and the high
degree of comparability is guaranteed by a com-
mon domain and topic, but also by a same type of
discourse. In order to detect automatically some of
the comparability levels, we carried out a stylistic
and contrastive analysis and elaborated a typology
for the characterization of scientific and popular
science types of discourse on the Web. This typol-
ogy is based on three aspects of Web documents:
the structural aspect, the modal aspect and lexi-
cal aspect. From the modality part, this distinction
is operational even on linguistically distant lan-
guages, as we proved by the validation on French
and Japanese. Our typology, implemented using
SVMlight and C4.5 learning algorithms brought
satisfactory results of classification, not only on
the training corpus but also on an evaluation cor-
pus, since we obtained a precision on average of
80% and a recall of 70%. This classifier has then
been included into a tool to assist specialized com-
parable corpora compilation. Starting from a Web
documents collection selected by the user, this
tool realizes cleaning, normalization and linguis-
tic treatment of each document and ?physically?
creates the corpus.
This tool is a first attempt and can be improved.
In a first time, we would like to assist the selection
and collection of documents, which could be real-
ized through the tool. Moreover, we would like to
investigate needs of comparable corpora users in
order to adapt our tool. Finally, others languages
could be added to the system, which represents a
quite time-consuming task: a classifier would have
to be created so all the linguistic analysis and clas-
sification tasks would have to be done again for
other languages.
Acknowledgement
This research program has been funded by the
French National Research Agency (ANR) through
the C-mantic project (ANR-07-MDCO-002-01)
2008-2010. We thank Yukie Nakao for the
japanese corpus and linguistic resources.
References
Arul Prakash Asirvatham and Kranthi Kumar Ravi.
2001. Web page classification based on document
structure. IEEE National Convention.
Marco Baroni and Adam Kilgarriff. 2006. Large
linguistically-processed web corpora for multiple
languages. In EACL?06, pages 87?90. The Associa-
tion for Computer Linguistics.
Douglas Biber. 1988. Variation across Speech and
Writing. Cambridge University Press.
Lynne Bowker and Jennifer Pearson. 2002. Working
with Specialized Language: A Practical Guide to
Using Corpora. London/New York, Routeledge.
Eric Brill. 1994. Some advances in transformation-
based part of speech tagging. In Proceedings of the
12th National Conference on Artificial Intelligence
(AAAI?94), pages 722?727, Seattle, WA, USA.
Soumen Chakrabarti, Martin van den Berg, and Byron
Dom. 1999. Focused crawling: a new approach
to topic-specific Web resource discovery. Computer
Networks (Amsterdam, Netherlands: 1999), 31(11?
16):1623?1640.
Patrick Charaudeau. 1992. Grammaire du sens et de
l?expression. Hachette.
62
Yun-Chuang Chiao and Pierre Zweigenbaum. 2002.
Looking for candidate translational equivalents in
specialized, comparable corpora. In COLING?02,
pages 1208?1212, Tapei, Taiwan.
B?atrice Daille. 2000. Morphological rule induction
for terminology acquisition. In COLING?00, pages
215?221, Sarrbrucken, Germany.
Herv? D?jean, ?ric Gaussier, and Fatia Sadat. 2002.
An approach based on multilingual thesauri and
model combination for bilingual lexicon extraction.
In COLING?02.
Oswald Ducrot and Tzvetan Todorov. 1972. Diction-
naire encyclop?dique des sciences du langage. ?di-
tions du Seuil.
David Ferrucci and Adam Lally. 2004. Uima: An
architectural approach to unstructured information
processing in the corporate research environment.
Natural Language Engineering, 10:327?348.
Pascale Fung and Lo Yuen Yee. 1998. An ir approach
for translating new words from nonparallel, com-
parable texts. In Christian Boitet and Pete White-
lock, editors, COLING?98, volume 1, pages 414?
420, Montreal, Quebec, Canada.
Kumiko Ishimaru. 2006. Comparative study on the
discourse of advertisement in France and Japan:
beauty products. Ph.D. thesis, Osaka University,
Japan.
Thorsten Joachims. 2002. Learning to Classify Text
using Support Vector Machines. Kluwer Academic
Publishers.
Jussi Karlgren, 1998. Natural Language Information
Retrieval, chapter Stylistic Experiments in Informa-
tion Retrieval. Tomek, Kluwer.
Sarah Laviosa. 1998. Corpus-based approaches to
contrastive linguistics and translation studies. Meta,
43(4):474?479.
Denise Malrieu and Francois Rastier. 2002. Genres et
variations morphosyntaxiques. Traitement Automa-
tique des Langues (TAL), 42(2):548?577.
Yuji Matsumoto, Akira Kitauchi, Tatsuo Yamashita,
and Yoshitaka Hirano. 1999. Japanese Morpho-
logical Analysis System ChaSen 2.0 Users Manual.
Technical report, Nara Institute of Science and Tech-
nology (NAIST).
Anthony McEnery and Zhonghua Xiao. 2007. Par-
allel and comparable corpora: What is happening?
In Gunilla Anderman and Margaret Rogers, editors,
Incorporating Corpora: The Linguist and the Trans-
lator. Clevedon: Multilingual Matters.
Fiammetta Namer and Robert Baud. 2007. Defin-
ing and relating biomedical terms: Towards a cross-
language morphosemantics-based system. Interna-
tional Journal of Medical Informatics, 76(2-3):226?
233.
Fiametta Namer. 2000. Flemm : Un analyseur flexion-
nel du fran?ais ? base de r?gles. Traitement Automa-
tique des Langues (TAL), 41(2):523?548.
Carol Peters and Eugenio Picchi. 1997. Using lin-
guistic tools and resources in cross-language re-
trieval. In David Hull and Doug Oard, editors,
Cross-Language Text and Speech Retrieval. Papers
from the 1997 AAAI Spring Symposium, Technical
Report SS-97-05, pages 179?188.
J. Ross Quinlan. 1993. C4.5: Programs for Machine
Learning. Morgan Kaufmann Publishers, San Fran-
cisco, CA, USA.
Reinhard Rapp. 1999. Automatic Identification of
Word Translations from Unrelated English and Ger-
man Corpora. In ACL?99, pages 519?526, College
Park, Maryland, USA.
Daniele Riboni. 2002. Feature selection for web
page classification. In Hassan Shafazand and A Min
Tjoa, editors, Proceedings of the 1st EurAsian Con-
ference on Advances in Information and Communi-
cation Technology (EURASIA-ICT), pages 473?478,
Shiraz, Iran. Springer.
Jorge Vivaldi Rogelio Nazar and Teresa Cabr?. 2008.
A suite to compile and analyze an lsp corpus. In
Nicoletta Calzolari, Khalid Choukri, Bente Mae-
gaard, Joseph Mariani, Jan Odjik, Stelios Piperidis,
and Daniel Tapias, editors, Proceedings of the Sixth
International Language Resources and Evaluation
(LREC?08), Marrakech, Morocco, may. European
Language Resources Association (ELRA).
J. C. Sager. 1990. A Pratical Course in Terminology
Processing. John Benjamins, Amsterdam.
Fabrizio Sebastiani. 2002. Machine learning in au-
tomated text categorization. ACM Computing Sur-
veys, 34(1):1?47.
John Sinclair. 1996. Preliminary recommendations on
text typology. Technical report, EAGLES (Expert
Advisory Group on Language Engineering Stan-
dards).
Federico Zanettin. 1998. Bilingual comparable
corpora and the training of translators. Meta,
43(4):616?630.
63
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 178?181,
Uppsala, Sweden, 15-16 July 2010.
c?2010 Association for Computational Linguistics
UNPMC: Na??ve Approach to Extract Keyphrases from Scientific Articles
Jungyeul Park
LINA,
Universit?e de Nantes
Nantes, France
jungyeul.park
@univ-nantes.fr
Jong Gun Lee
LIP6-CNRS,
UPMC (Paris 6)
Paris, France
jonggun.lee
@lip6.fr
B
?
eatrice Daille
LINA,
Universit?e de Nantes
Nantes, France
beatrice.daille
@univ-nantes.fr
Abstract
We describe our method for extracting
keyphrases from scientific articles which
we participate in the shared task of
SemEval-2 Evaluation Exercise. Even
though general-purpose term extractors
along with linguistically-motivated analy-
sis allow us to extract elaborated morpho-
syntactic variation forms of terms, a na??ve
statistic approach proposed in this paper
is very simple and quite efficient for ex-
tracting keyphrases especially from well-
structured scientific articles. Based on
the characteristics of keyphrases with sec-
tion information, we obtain 18.34% for
f-measure using top 15 candidates. We
also show further improvement without
any complications and we discuss this at
the end of the paper.
1 Introduction
1
Key phrases are a set of words to capture the main
topic of the document. Since key phrases con-
tain the substance of the document, they are used
in the large spectrum of areas; from applications
which explicitly use key phrases such as automatic
indexing, documents classification and search en-
gine optimization in information retrieval, to ap-
plications which implicitly use key phrases such as
summarization and question-answering systems.
During the last decade, many previous works have
dealt with the various methods for automatically
extracting key phrases (e.g., Frank et al, 1999;
Barker and Corrnacchia, 2000; Turney, 2003;
Medelyan and Witten, 2006; Nguyen and Kan,
2007; Wan and Xiao, 2008).
1
UNPMC means the collaborative team from Laboratoire
d?Informatique de Nantes Atlantique of the Universit?e de
Nantes and Laboratoire d?Informatique de Paris 6 of the Uni-
versit?e Pierre et Marie Curie.
The task of extracting key phrases would be
considered as a subtask of extracting terminology
if key phrases are a kind of terms. Typical ap-
proaches for automatically extracting terms use
linguistic preprocessing which involves morpho-
syntactic analysis such as part-of-speech tagging
and phrase chunking, and statistical postprocess-
ing such as log likelihood which compares the
term frequencies in a document against their ex-
pected frequencies derived in a bigger text. Be-
sides, extracting terms prefers syntactically plau-
sible noun phrases (NPs) which are mainly multi-
words terms. Kim and Kan (2009) report that most
of key phrases are often simple words than less of-
ten compound words
2
.
The task for extracting key phrases tend to in-
clude analyzing the document structure. Espe-
cially, extracting key phrases from well-structured
scientific articles should consider cross-section in-
formation (Nguyen and Kan, 2007). This informa-
tion has been explored to assess the suitability of
features during learning in Kim and Kan (2009).
Extracting key phrases, however, is more than to
extracting terminology or analyzing the document
structure. While terms are words which appear in
specific contexts and analyse concept structures in
domains of human activity, key phrases are words
that capture the key idea of documents. In addi-
tion, while terms usually occur in the given doc-
ument more often than we would expect to occur,
key phrases do not necessarily occur frequently or
key phrases do not occur at all in the document.
Consequently, the task for extracting key phrases
should not be considered as the subtask of extract-
ing terminology and we are not able to directly ap-
ply general-purpose term extractors to extract key
phrases.
In this paper, we describe our method for ?Au-
tomatic Keyphrase Extraction from Scientific Ar-
2
In training data, only 23.4% of keyphrases, however, are
single words.
178
ticles?, the shared task of SemEval-2 Evalua-
tion Exercise which we participated in. Al-
though term extractors along with linguistically-
motivated analysis allow us to extract even elab-
orated morpho-syntactic variation forms of terms,
the na??ve statistic approach proposed in this pa-
per is very simple and quite efficient for extracting
keyphrases especially from well-structured scien-
tific articles. In a nutshell, our method is based
on empirical rules without any linguistically-
motivated preprocessing. Empirical rules are ob-
tained from the analysis of the characteristics of
keyphrases by observing training data.
The remaining of this paper is organized as fol-
lows: Section 2 explains the characteristics of
keyphrases in scientific articles. Section 3 and 4
detail our na??ve statistic approach and experiment,
respectively. We conclude this paper and discuss a
further improvement in Section 6.
2 Characteristics of Keyphrases in
Scientific Articles
In this section, we investigate the characteristics of
keyphrases in training data. Table 1 shows statis-
tics of training data. In Table 1, D-author means
the keyphrases assigned by authors, D-reader the
keyphrases assigned by readers, and D-combined
the combined keyphrases assigned by both of au-
thors and readers.
# of papers (p) # of key phrases (k) k / p
D-author 144 563 3.91
D-reader 144 1,865 12.95
D-combined 144 2,265 15.73
Table 1: Statistics of training data
2.1 Word length of keyphrases
We measure the distribution of word length of key
phrases in training data and present it in Figure 1.
Over half of key phrases are two-word key phrases
in both author- and reader-assigned key phrases.
Differently with Kim and Kan (2009) which they
reported that most of key phrases are often sim-
ple words than less often compound words, only
29.7% and 17.7% of key phrases are one-word key
phrases. There are also more than four-word key
phrases which hold 4.3% and 7.2% of author and
reader assigned key phrases, respectively.
2.2 Occurrences of keyphrases
In which section do keyphrases occur frequently?
To answer this question, we count the number of
length=1(29.7%)
length=2(51.3%) length=3(14.7%)
length=4+(4.3%)
(a) D-author
length=1(17.7%)length=2(53.2%)
length=3(21.8%)
length=4+(7.2%)
(b) D-reader
Figure 1: Word length of keyphrases in training
data
occurrences of keyphrases of each section. Due
to the variation of the naming of the section,
we divide sections into title and abstract, intro-
duction, conclusion, and the rest including refer-
ences. Table 2 and 3 show the number of occur-
rences and the accumulative number of unique oc-
currences of keyphrases in each section, respec-
tively. We also show the accumulative number
of words in each section in Table 4. Including
the rest sections exponentially diminishes the ra-
tio of the number of gold keyphrases to the number
of candidate keyphrases. Note that m words pro-
duce
?
n?1
i=0
(m ? i) candidate keyphrases for up
to n-word keyphrases by supposing that candidate
keyphrases are simple n-word terms.
Note also that both author- and reader-assigned
keyphrases hold only 75.49% and 89.44%, re-
spectively. Even some keyphrases are different
with surface forms in the document and our na??ve
method with no linguistic intervention is not able
to recognize them. For example, one of reader-
assigned keyphrases distributed real-time embed-
ded system for C-41 actually appears as distributed
real-time and embedded (DRE) systems.
D-author D-reader
Title and Abstract 277 802
Introduction 215 491
Conclusion 313 982
Other 387 1,210
Table 2: Number of occurrences of keyphrases in
each section
D-author D-reader
Total 563 (100.0%) 1,865 (100.0%)
Title and Abstract 277 (49.20%) 802 (43.00%)
?+? Introduction 317 (56.30%) 937 (50.24%)
?+? Conclusion 367 (65.19%) 1,311 (70.29%)
?+? Other 425 (75.49%) 1,668 (89.44%)
Table 3: Accumulative number of unique occur-
rences of keyphrases in each section
179
# words (W) # gold (G) G/W
Title and Abstract 28435 802 0.0282
?+? Introduction 72729 937 0.0128
?+? Conclusion 178473 1311 0.0073
?+? Other 948007 1668 0.0018
Table 4: Number of words in training data and
gold data (D-reader)
2.3 Coincidence of keyphrases
Figure 2 shows the coincidence of keyphrases
3
.
Almost half of keyphrases (58.44% and 45.74%
for author- and reader-assigned keyphrases, re-
spectively) occur coincidentally in keysections
and the rest sections. Keysections hold 65.19%
and 70.29% of keyphrases and the rest sections
besides keysections hold 68.74% and 64.88% of
whole keyphrases. Note that the rest sections oc-
cupy over 70% of the document on the average.
(a) D-author (b) D-reader
Figure 2: Coincidence of keyphrases
3 Methodology
From training data, we observe and decide the fol-
lowings:
? More than four-word keyphrases hold only
4.3% and 7.2% of whole keyphrases. We
decide that our approach limits the word
length as three for extracting keyphrases.
Thus we extract only up to three-word
keyphrases. This choice might lead the per-
formance degradation of our method because
we explicitly exclude more than four-word
keyphrases.
? Keysections hold 65.19% and 70.29% of
keyphrases. We decide that our approach
limits keysections from which we extract
keyphrases. Including the rest sections may
3
We denote title and abstract as A, introduction as I, con-
clusion as C, and the rest sections including references as
Other.
improve recall, but probably diminish preci-
sion since the rest sections occupy over 70%
of the document.
? Almost half of keyphrases occur coinciden-
tally in keysections and the rest sections. We
decide that our approach limits coincident
keyphrases in both of them. This decision is
made empirically and improve precision.
The following procedure explains and details
our approach for extracting keyphrases.
? Extract up to three-word terms from keysec-
tions as candidate keyphrases.
? Filter them out if they contain one or more of
stop words or non-content-containing words
(see Table 5 for non-content-containing
words).
? Count the number of occurrences of extracted
terms from each keysection.
? Check the coincidence whether candidate
keyphrases occurs in more than two keysec-
tions. If so, we assign weight.
? Calculate a score for candidate keyphrases
and list them by order of the score.
4 Experiment results
This section shows the experiment results with
training and test data.
4.1 Training data
To optimize our results, we use various thresholds
for the number of n-word keyphrases and weight.
We try to find the (i : j : k) pattern which
means i one-word, j two-word, and K three-
word keyphrases to produce the best results. We
also try to find the threshold for weight d to cal-
culate the score as follows: if keyphrases ap-
pear in more than two keysections, score =
d ? # of total occurences, otherwise score =
# of total occurences. Table 6 shows our best
results for training data where (i : j : k) = (3 :
9 : 3) and d = 2. Empirically, we found these
thresholds from training data by iterating several
possibilities
4
.
4.2 Test data
Table 7 shows our test data results published by
organizers of the shared task of SemEval-2 Evalu-
ation Exercise.
4
These thresholds will be more examined in future work.
180
Type Examples
Noun section, abstract, introduction, conclusion, reference, future work, figure, paper, result, laboratory, university
Verb present, how, introduce, become, improve, find, help, improve, consider, call, yield, allow, give, assume
Adverb always, formally, necessarily, successfully, previously, usually,mainly, final, essentially, ultinately, commonly,
severely, significantly, dramatically, clearly, still, well, who, whose, whom, which, whether, therefore,
Other POSs that, this, those, these, many, several, more, over, less, behind, above, below, each, few, different, under,
both, within, through, prior, various, better, following, between, possible, via, before,even, such, if, new,
show, important, simple, good, tranditional, current, varying, necessary, previous, clear
Table 5: Example of (heuristically obtained) non-content-containing terms
AUTHOR.STEM.FINAL
# Gold: 559 Match Precision Recall F-score
Top 05 43 5.97% 7.69% 6.72%
Top 10 101 7.01% 18.07% 10.10%
Top 15 139 6.44% 24.87% 10.23%
READER.STEM.FINAL
# Gold: 1824 Match Precision Recall F-score
Top 05 118 16.39% 6.47% 9.28%
Top 10 249 17.29% 13.65% 15.26%
Top 15 361 16.71% 19.79% 18.12%
COMBINED.STEM.FINAL
# Gold: 2223 Match Precision Recall F-score
Top 05 143 19.86% 6.43% 9.71%
Top 10 309 21.46% 13.90% 16.87%
Top 15 441 20.42% 19.84% 20.13%
Table 6: Training data results
READER.STEM.FINAL
# Gold: 1204 Precision Recall Fscore
Top 05 13.80% 5.73% 8.10%
Top 10 15.10% 12.54% 13.70%
Top 15 14.47% 18.02% 16.05%
COMBINED.STEM.FINAL
# Gold: 1466 Precision Recall Fscore
Top 05 18.00% 6.14% 9.16%
Top 10 19.00% 12.96% 15.41%
Top 15 18.13% 18.55% 18.34%
Table 7: Test data results
5 Conclusion and Discussion
In this paper, we described our simple method
for extracting keyphrases from scientific arti-
cles which we participate in the shared task of
SemEval-2 Evaluation Exercise. The na??ve ap-
proach was proposed. This approach turned
out very simple and quite efficient for extracting
keyphrases from well-structured scientific articles.
Based on learning the distribution of keyphrases
with section information, we obtain 18.34% for f-
measure using top 15 candidates.
Our na??ve approach still has much room for
improvement. For example, we are able to im-
prove the result for same test data up to 20.71%
and 25.55% for f-measure using top 15 candidates
simply by adding the rest sections and normaliz-
ing the number of occurrences of terms from each
section
5
.
5
The result is not improved only by adding the rest sec-
tions.
Moreover, our n-word terms based extraction
can be benefited by linguistic preprocessing such
as normalizing surface forms. Handcrafted regu-
lar expression rules along with part-of-speech tag-
ging and phrase chunking would be also intro-
duced to improve candidate selection. We have
not explored thoroughly feature engineering, nei-
ther. For example, more fine-grained section infor-
mation and weight re-assignment might help filter
out irrelevant candidates. We leave these possibil-
ities for future work.
References
Ken Barker and Nadia Cornacchia. 2000. Using noun phrase
heads to extract document keyphrases. In Proceedings
of the 13th Biennial Conference of the Canadian Soci-
ety on Computational Studies of Intelligence: Advances
in Artificial Intelligence, pages 40-52. May 14-17, 2000.
Montr?eal, Quebec, Canada.
Eibe Frank , Gordon W. Paynter , Ian H. Witten , Carl Gutwin,
and Craig G. Nevill-Manning. 1999. Domain-Specific
Keyphrase Extraction. In Proceedings of the 16th Inter-
national Joint Conference on Artificial Intelligence, pages
668-673. July 31-August 6, 1999. Stockholm, Sweden.
Su Nam Kim and Min-Yen Kan. 2009. Re-examining Auto-
matic Keyphrase Extraction Approaches in Scientific Ar-
ticles. In Proceedings of the Workshop on Multiword Ex-
pressions: Identification, Interpretation, Disambiguation
and Applications (MWE 2009), ACL-IJCNLP 2009, pages
9-12. August 6, 2009. Singapore.
Olena Medelyan and Ian H. Witten. 2006. Thesaurus based
automatic keyphrase indexing. In Proceedings of the
6th ACM/IEEE-CS joint conference on Digital libraries,
pages 296-297. June 11-15, 2006. Chapel Hill, NC, USA.
Thuy Dung Nguyen and Min-Yen Kan. 2007. Key phrase
Extraction in Scientific Publications. Asian Digital Li-
braries. Looking Back 10 Years and Forging New Fron-
tiers, pages 317-326. Springer Berlin, Heidelberg.
Peter D. Turney. 2003. Coherent keyphrase extraction via
Web mining. In Proceedings of the 18th International
Joint Conference on Artificial Intelligence, pages 434-
439. August 9-15, 2003. Acapulco, Mexico.
Xiaojun Wan and Jianguo Xiao. 2008. CollabRank: towards
a collaborative approach to single-document keyphrase
extraction. In Proceedings of the 22nd International
Conference on Computational Linguistics (Coling 2008),
pages 969-976. 18-22 August, 2008. Manchester, UK.
181
