Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: System Demonstrations,
pages 76?80, Dublin, Ireland, August 23-29 2014.
CRAB 2.0: A text mining tool for supporting literature review in chemical
cancer risk assessment
Yufan Guo
1
, Diarmuid
?
O S
?
eaghdha
1
, Ilona Silins
2
, Lin Sun
1
,
Johan H
?
ogberg
2
, Ulla Stenius
2
, Anna Korhonen
1
1
Computer Laboratory, University of Cambridge, UK
2
Institute of Environmental Medicine, Karolinska Institutet, Stockholm, Sweden
Abstract
Chemical cancer risk assessment is a literature-dependent task which could greatly benefit from
text mining support. In this paper we describe CRAB ? the first publicly available tool for
supporting the risk assessment workflow. CRAB, currently at version 2.0, facilitates the gathering
of relevant literature via PubMed queries as well as semantic classification, statistical analysis and
efficient study of the literature. The tool is freely available as an in-browser application.
1 Introduction
Biomedical text mining addresses the great need to access information in the growing body of literature
in biomedical sciences. Prior research has produced useful tools for supporting practical tasks such as
literature curation and development of semantic databases, among others (Chapman and Cohen, 2009;
Harmston et al., 2010; Simpson and Demner-Fushman, 2012; McDonald and Kelly, 2012). In this paper
we describe a tool we have built to aid literature exploration for the task of chemical risk assessment
(CRA). The need for assessment of chemical hazards, exposures and their corresponding health risks is
growing, as many countries have tightened up their chemical safety rules. CRA work requires thorough
review of available scientific data for each chemical under inspection, much of which can be found in
scientific literature (EPA, 2005). Since the scientific data is highly varied and well-studied chemicals
may have tens of thousands of publications (e.g. to date PubMed contains 23,665 articles mentioning
phenobarbital), the task can be extremely time consuming when conducted via conventional means
(Korhonen et al., 2009). As a result, there is interest among the CRA community in text mining tools that
can aid and streamline the literature review process.
We have developed CRAB, an online system that supports the entire process of literature review for
cancer risk assessors. It is the first and only NLP system that serves this need. CRAB contains three main
components:
1. Literature search with PubMed integration
2. Semantic classification of abstracts with summary visualisation
3. Literature browsing with markup of information structure
These components are described further in Section 2 below. Version 2.0 of CRAB is freely available as an
in-browser application; see Section 4 for access information.
2 System description
2.1 Literature search
The first step for the user is to retrieve a collection of scientific articles relevant to their need, e.g., all
articles with abstracts that contain the name of a given chemical. The CRAB 2.0 search page (Figure
1) allows the user to directly query the MEDLINE database of biomedical abstracts. The search query
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer
are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/
76
Figure 1: The CRAB 2.0 search interface
(a) Taxonomy view
gene
ral(wo
rds re
lated 
to RA
)
hum
an s
tudy
/epid
emio
logy
tumo
r rela
ted
mo
rpho
logic
al ef
fect 
on ti
ssue
/orga
n
bioc
hem
ical/c
ellbio
logic
al ef
fects
biom
arke
rs
polym
orph
ism
anim
al st
udy
stud
y len
gth
2?ye
ar ca
ncer
 bioa
ssay
shor
t and
  me
dium tumo
rs
pren
eopl
astic
 lesio
ns
mo
rpho
logic
al ef
fect 
on ti
ssue
/orga
n
bioc
hem
ical/c
ellbio
logic
al ef
fects
biom
arke
r
type
 of a
nima
l
gene
ticall
y mo
dified
 anim
als
cell 
expe
rime
nts 
bioc
hem
ical/c
ellbio
logic
al ef
fects
subc
ellula
r sys
tems
 
stud
y on
 mic
roorg
anism
s
rev
iew 
artic
le, s
umm
ary
Scientific Evidence
% ab
strac
ts
0
20
40
60
80
100
(b) Histogram view
Figure 2: The CRAB 2.0 classification component
is sent, and the results received, using the E-Utilities web service provided by the National Center
for Biotechnology Information.
1
This query interface supports PubMed Advanced Search, facilitating
complex Boolean queries.
2.2 Semantic classification
The document collection returned by the PubMed web service is passed in XML format to a semantic
classifier that annotates each abstract with 42 binary labels indicating the presence/absence of concepts
relevant to CRA. These concepts are organised hierarchically in two main taxonomies: (1) kinds of
scientific evidence used for CRA (e.g., human studies, animal studies, cell experiments, biochemical/cell
biological effects); (2) the carcinogenic modes of action indicated by the evidence (e.g., genotoxic,
nongenotoxic/indirect genotoxic, cell death, inflammation, angiogenesis). The underlying classifier is a
support vector machine (SVM) trained on a dataset of 3,078 manually annotated abstracts. Features used
by the SVM include lexical n-grams, character n-grams and MeSH concepts. For more details on the
concept taxonomies, training corpus and classifier see Korhonen et al. (2012).
1
http://www.ncbi.nlm.nih.gov/books/NBK25501/
77
Figure 3: The CRAB 2.0 information structure component
Once each abstract in the retrieved collection has been classified, the user is presented with a summary
of counts for each concept (Figure 2a). In a user study, risk assessors found this summary very useful for
obtaining a broad overview of the literature, identifying groups of chemicals with similar toxicological
profiles and identifying data gaps (Korhonen et al., 2012). The user can also request a histogram
visualisation (Figure 2b), which is produced through a call to the statistical software R.
2
2.3 Literature browsing
The risk assessment workflow involves close reading of relevant abstracts to identify specific information
about methods, experimental details, results and conclusions. While it is not feasible to automate this
process, we have shown that automatic markup and visualisation of abstracts? information structure
can accelerate it considerably (Guo et al., 2011). The model of information structure incorporated in
CRAB 2.0 is based on argumentative zoning (Teufel and Moens, 2002; Mizuta et al., 2006; Teufel, 2010),
whereby the text of a scientific abstract (or article) is segmented into blocks of sentences that carry a
specific rhetorical function and combine to communicate the argument the authors wish to convey to
the reader. The markup scheme used in our system labels each sentence with one of seven categories:
background, objective, method, result, conclusion, related work and future work (Guo et al., 2010). The
CRAB system incorporates preprocessing (lemmatisation, POS tagging, parsing) with the C&C toolkit
3
and information structure markup with an SVM classifier that labels sentences according to a combination
of lexical, syntactic and discourse features (Guo et al., 2011). The classifier has been trained on an
annotated dataset of 1,000 CRA abstracts (Guo et al., 2010).
The automatic information structure markup is used to support browsing of the set of abstracts assigned
a label of interest by the semantic classifier; e.g., the user can inspect all abstracts labelled genotoxic
(Figure 3). Each information structure category is highlighted in a different colour and the user can select
a single category to focus on. To our knowledge, CRAB 2.0 is the first publicly available online tool that
provides information structure analysis of biomedical literature.
3 Evaluation
Intrinsic cross-validation evaluations of the semantic taxonomy classifier and information structure
classifier show high performance: 0.78 macro-averaged F-score (Korhonen et al., 2012) and 0.88 accuracy
(Guo et al., 2011), respectively. Furthermore, user-based evaluation in the context of real-life CRA has
2
http://www.r-project.org/
3
http://svn.ask.it.usyd.edu.au/trac/candc
78
produced promising results. (Korhonen et al., 2012) showed that the concept distributions produced by
our classifier confirmed known properties of chemicals without human input. Guo et al. (2011) found that
integrating information structure visualisation in abstract browsing helped risk assessors to find relevant
information in abstracts 7-8% more quickly.
4 Use
CRAB 2.0 is freely available as an in-browser application at http://omotesando-e.cl.cam.
ac.uk/CRAB/request.html. New users can register an id and password to allow them to store
and retrieve data from previous sessions. Alternatively, they can use an anonymous guest account (id
guest@coling, password guest@coling).
5 Conclusion
We have presented Version 2.0 of CRAB, the first NLP tool for supporting the workflow of literature
review for cancer risk assessment. CRAB meets a real, specialised need and is already being used to
improve the efficiency of CRA work. Although currently focused on cancer, CRAB can be easily adapted
to other health risks provided with the appropriate taxonomy and annotated data for machine learning. In
the future, the tool can be developed further in various ways, e.g. to support submissions in other formats
than PubMed XML; to take into account journal impact factors, number of citations and cross references
to better organize the literature; and to offer enriched statistical analysis of classified literature.
Acknowledgements
This work was supported by the Royal Society, Vinnova and the Swedish Research Council.
References
Wendy W. Chapman and K. Bretonnel Cohen. 2009. Current issues in biomedical text mining and natural language
processing. Journal of Biomedical Informatics, 42(5):757?759.
EPA. 2005. Guidelines for carcinogen risk assessment. US Environmental Protection Agency.
Yufan Guo, Anna Korhonen, Maria Liakata, Ilona Silins, Lin Sun, and Ulla Stenius. 2010. Identifying the informa-
tion structure of scientific abstracts: An investigation of three different schemes. In Proceedings of BioNLP-10,
Uppsala, Sweden.
Yufan Guo, Anna Korhonen, Ilona Silins, and Ulla Stenius. 2011. Weakly supervised learning of information
structure of scientific abstracts: Is it accurate enough to benefit real-world tasks in biomedicine? Bioinformatics,
27(22):3179?3185.
Nathan Harmston, Wendy Filsell, and Michael P.H. Stumpf. 2010. What the papers say: Text mining for genomics
and systems biology. Human Genomics, 5(1):17?29.
Anna Korhonen, Ilona Silins, Lin Sun, and Ulla Stenius. 2009. The first step in the development of text min-
ing technology for cancer risk assessment: Identifying and organizing scientific evidence in risk assessment
literature. BMC Bioinformatics, 10:303.
Anna Korhonen, Diarmuid
?
O S?eaghdha, Ilona Silins, Lin Sun, Johan H?ogberg, and Ulla Stenius. 2012. Text
mining for literature review and knowledge discovery in cancer risk assessment and research. PLoS ONE,
7(4):e33427.
Diane McDonald and Ursula Kelly. 2012. The value and benefit of text mining to UK further and higher education.
Report 811, JISC.
Yoko Mizuta, Anna Korhonen, Tony Mullen, and Nigel Collier. 2006. Zone analysis in biology articles as a basis
for information extraction. International Journal of Medical Informatics, 75(6):468?487.
Matthew S. Simpson and Dina Demner-Fushman. 2012. Biomedical text mining: A survey of recent progress. In
Charu C. Aggarwal and ChengXiang Zhai, editors, Mining Text Data. Springer.
79
Simone Teufel and Marc Moens. 2002. Summarizing scientific articles: Experiments with relevance and rhetorical
status. Computational Linguistics, 28(4):409?445.
Simone Teufel. 2010. The Structure of Scientific Articles: Applications to Citation Indexing and Summarization.
CSLI Publications, Stanford, CA.
80
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 273?283,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
A Weakly-supervised Approach to Argumentative Zoning of Scientific
Documents
Yufan Guo
Computer Laboratory
University of Cambridge, UK
yg244@cam.ac.uk
Anna Korhonen
Computer Laboratory
University of Cambridge, UK
alk23@cam.ac.uk
Thierry Poibeau
LaTTiCe, UMR8094
CNRS & ENS, France
thierry.poibeau@ens.fr
Abstract
Argumentative Zoning (AZ) ? analysis of the
argumentative structure of a scientific paper ?
has proved useful for a number of informa-
tion access tasks. Current approaches to AZ
rely on supervised machine learning (ML).
Requiring large amounts of annotated data,
these approaches are expensive to develop and
port to different domains and tasks. A poten-
tial solution to this problem is to use weakly-
supervised ML instead. We investigate the
performance of four weakly-supervised clas-
sifiers on scientific abstract data annotated for
multiple AZ classes. Our best classifier based
on the combination of active learning and self-
training outperforms our best supervised clas-
sifier, yielding a high accuracy of 81% when
using just 10% of the labeled data. This re-
sult suggests that weakly-supervised learning
could be employed to improve the practical
applicability and portability of AZ across dif-
ferent information access tasks.
1 Introduction
Many practical tasks require accessing specific types
of information in scientific literature. For example,
a reader of scientific literature may be looking for
information about the objective of the study in ques-
tion, the methods used in the study, the results ob-
tained, or the conclusions drawn by authors. Sim-
ilarly, many Natural Language Processing (NLP)
tasks focus on the extraction of specific types of in-
formation in documents only.
To date, a number of approaches have been pro-
posed for sentence-based classification of scien-
tific literature according to categories of information
structure (or discourse, rhetorical, argumentative or
conceptual structure, depending on the framework
in question). Some of these classify sentences ac-
cording to typical section names seen in scientific
documents (Lin et al, 2006; Hirohata et al, 2008),
while others are based e.g. on argumentative zones
(Teufel and Moens, 2002; Mizuta et al, 2006; Teufel
et al, 2009), qualitative dimensions (Shatkay et al,
2008) or conceptual structure (Liakata et al, 2010)
of documents.
The best of current approaches have yielded
promising results and proved useful for information
retrieval, information extraction and summarization
tasks (Teufel and Moens, 2002; Mizuta et al, 2006;
Tbahriti et al, 2006; Ruch et al, 2007). How-
ever, relying on fully supervised machine learning
(ML) and a large body of annotated data, existing
approaches are expensive to develop and port to dif-
ferent scientific domains and tasks.
A potential solution to this bottleneck is to de-
velop techniques based on weakly-supervised ML.
Relying on a small amount of labeled data and
a large pool of unlabeled data, weakly-supervised
techniques (e.g. semi-supervision, active learning,
co/tri-training, self-training) aim to keep the advan-
tages of fully supervised approaches. They have
been applied to a wide range of NLP tasks, includ-
ing named-entity recognition, question answering,
information extraction, text classification and many
others (Abney, 2008), yielding performance levels
similar or equivalent to those of fully supervised
techniques.
To the best of our knowledge, such techniques
273
have not yet been applied to the analysis of infor-
mation structure of scientific documents by afore-
mentioned approaches. Recent experiments have
demonstrated the usefulness of weakly-supervised
learning for classifying discourse relations in scien-
tific texts, e.g. (Hernault et al, 2011). However, fo-
cusing on local (rather than global) structure of doc-
uments and being much more fine-grained in nature,
this related task differs from ours considerably.
In this paper, we investigate the potential of
weakly-supervised learning for Argumentative Zon-
ing (AZ) of scientific abstracts. AZ is an approach to
information structure which provides an analysis of
the rhetorical progression of the scientific argument
in a document (Teufel and Moens, 2002). It has
been used to analyze scientific texts in various disci-
plines ? including computational linguistics (Teufel
and Moens, 2002), law, (Hachey and Grover, 2006),
biology (Mizuta et al, 2006) and chemistry (Teufel
et al, 2009) ? and has proved useful for NLP tasks
such as summarization (Teufel and Moens, 2002).
Although the basic scheme is said to be discipline-
independent (Teufel et al, 2009), its application to
different domains has resulted in various modifica-
tions and laborious annotation exercises. This sug-
gests that a weakly-supervised approach would be
more practical than a fully supervised one for the
real-world application of AZ.
Taking two supervised classifiers as a comparison
point ? Support Vector Machines (SVM) and Con-
ditional Random Fields (CRF) ? we investigate the
performance of four weakly-supervised classifiers
on the AZ task: two based on semi-supervised learn-
ing (transductive SVM and semi-supervised CRF)
and two on active learning (Active SVM alone and
in combination with self-training).
The results are promising. Our best weakly-
supervised classifier (Active SVM with self-
training) outperforms the best supervised classifier
(SVM), yielding high accuracy of 81% when using
just 10% of the labeled data. When using just one
third of the labeled data, it performs equally well as
a fully supervised SVM which uses 100% of the la-
beled data. Our investigation suggests that weakly-
supervised learning could be employed to improve
the practical applicability and portability of AZ to
different information access tasks.
2 Data
We used in our experiments the recent dataset of
(Guo et al, 2010). Guo et al (2010) provide a cor-
pus of 1000 biomedical abstracts (consisting of 7985
sentences and 225785 words) annotated according
to three schemes of information structure ? those
based on section names (Hirohata et al, 2008), AZ
(Mizuta et al, 2006) and Core Scientific Concepts
(CoreSC) (Liakata et al, 2010). We focus here on
AZ only, because it subsumes all the categories of
the simple section name -based scheme, and accord-
ing to the inter-annotator agreement and ML experi-
ments reported by Guo et al (2010) it performs bet-
ter on this data than the fairly fine-grained CoreSC
scheme.
AZ is a scheme which provides an analysis of
the rhetorical progression of the scientific argument,
following the knowledge claims made by authors.
(Teufel and Moens, 2002) introduced AZ and ap-
plied it first to computational linguistics papers.
(Hachey and Grover, 2006) applied the scheme later
to legal texts and (Mizuta et al, 2006) modified it for
biology papers. More recently, (Teufel et al, 2009)
introduced a refined version of AZ and applied it to
chemistry papers.
The biomedical dataset of (Guo et al, 2010) has
been annotated according to the version of AZ de-
veloped for biology papers (Mizuta et al, 2006)
(with only minor modifications concerning zone
names). Seven categories of this scheme (out of the
10 possible) actually appear in abstracts and in the
resulting corpus. These are shown and explained
in Table 1. For example, the Method zone (METH)
is for sentences which describe a way of doing re-
search, esp. according to a defined and regular
plan; a special form of procedure or characteristic
set of procedures employed in a field of study as a
mode of investigation and inquiry.
An example of a biomedical abstract annotated
according to AZ is shown in Figure 1, with different
zones highlighted in different colors. For example,
the RES zone is highlighted in lemon green.
Table 2 shows the distribution of sentences per
scheme category in the corpus: Results (RES) is
by far the most frequent zone (accounting for 40%
of the corpus), while Background (BKG), Objective
(OBJ), Method (METH) and Conclusion (CON) cover
274
Table 1: Categories of AZ appearing in the corpus of (Guo et al, 2010)
Category Abbr. Definition
Background BKG The circumstances pertaining to the current work, situation, or its causes, history, etc.
Objective OBJ A thing aimed at or sought, a target or goal
Method METH A way of doing research, esp. according to a defined and regular plan; a special form
of procedure or characteristic set of procedures employed in a field of study as a mode
of investigation and inquiry
Result RES The effect, consequence, issue or outcome of an experiment; the quantity, formula,
etc. obtained by calculation
Conclusion CON A judgment or statement arrived at by any reasoning process; an inference, deduction,
induction; a proposition deduced by reasoning from other propositions; the result of
a discussion, or examination of a question, final determination, decision, resolution,
final arrangement or agreement
Related work REL A comparison between the current work and the related work
Future work FUT The work that needs to be done in the future
Figure 1: An example of an annotated abstract
B
ut
ad
ie
ne
(B
D
)m
et
ab
ol
is
m
sh
ow
s
ge
nd
er
,s
pe
ci
es
an
d
co
nc
en
tr
at
io
n
de
pe
nd
en
cy
,m
ak
in
g
th
e
ex
tr
ap
ol
at
io
n
of
an
im
al
re
su
lts
to
hu
m
an
s
co
m
pl
ex
.B
D
is
m
et
ab
ol
iz
ed
m
ai
nl
y
by
cy
to
ch
ro
m
e
P4
50
2E
1
to
th
re
e
ep
ox
id
es
,1
,2
-e
po
xy
-3
-b
ut
en
e
(E
B
),
1,
2;
3,
4-
di
ep
ox
yb
ut
an
e
(D
E
B
)a
nd
1,
2-
ep
ox
y-
bu
ta
ne
di
ol
(E
B
-d
io
l).
Fo
ra
cc
ur
at
e
ris
k
as
se
ss
m
en
ti
ti
s
im
po
rt
an
tt
o
el
uc
id
at
e
sp
ec
ie
s
di
ff
er
en
ce
s
in
th
e
in
te
rn
al
fo
rm
at
io
n
of
th
e
in
di
vi
du
al
ep
ox
id
es
in
or
de
rt
o
as
si
gn
th
e
re
la
tiv
e
ris
ks
as
so
ci
at
ed
w
ith
th
ei
rd
iff
er
en
tm
ut
ag
en
ic
po
te
nc
ie
s.
A
na
ly
si
s
of
N
-t
er
m
in
al
gl
ob
in
ad
du
ct
s
is
a
co
m
m
on
ap
pr
oa
ch
fo
rm
on
ito
rin
g
th
e
in
te
rn
al
fo
rm
at
io
n
of
B
D
de
riv
ed
ep
ox
id
es
.O
ur
lo
ng
te
rm
st
ra
te
gy
is
to
de
ve
lo
p
an
L
C
-M
S/
M
S
m
et
ho
d
fo
rs
im
ul
ta
ne
ou
s
de
te
ct
io
n
of
al
lt
hr
ee
B
D
he
m
og
lo
bi
n
ad
du
ct
s.
Th
is
ap
pr
oa
ch
is
m
od
el
ed
af
te
rt
he
re
ce
nt
ly
re
po
rt
ed
im
m
un
oa
ff
in
ity
L
C
-M
S/
M
S
m
et
ho
d
fo
rt
he
cy
cl
ic
N
,N
-(
2,
3-
di
hy
dr
ox
y-
1,
4-
bu
ta
dy
il)
-v
al
in
e
(p
yr
-V
al
,d
er
iv
ed
fr
om
D
E
B
).
W
e
re
po
rt
he
re
in
th
e
an
al
ys
is
of
th
e
E
B
-d
er
iv
ed
2-
hy
dr
ox
yl
-3
-b
ut
en
yl
-v
al
in
e
pe
pt
id
e
(H
B
-V
al
).
Th
e
pr
oc
ed
ur
e
ut
ili
ze
s
tr
yp
si
n
hy
dr
ol
ys
is
of
gl
ob
in
an
d
im
m
un
oa
ff
in
ity
(I
A
)p
ur
ifi
ca
tio
n
of
al
ky
la
te
d
he
pt
ap
ep
tid
es
.Q
ua
nt
ita
tio
n
is
ba
se
d
on
L
C
-M
S/
M
S
m
on
ito
rin
g
of
th
e
tr
an
si
tio
n
fr
om
th
e
si
ng
ly
ch
ar
ge
d
m
ol
ec
ul
ar
io
n
of
H
B
-V
al
(1
-7
)t
o
th
e
a(
1)
fr
ag
m
en
t.
H
um
an
H
B
-V
al
(1
-1
1)
w
as
sy
nt
he
si
ze
d
an
d
us
ed
fo
ra
nt
ib
od
y
pr
od
uc
tio
n.
A
s
in
te
rn
al
st
an
da
rd
,t
he
la
be
le
d
ra
t-
[(
13
)C
(5
)(
15
)N
]-
V
al
(1
-1
1)
w
as
pr
ep
ar
ed
th
ro
ug
h
di
re
ct
al
ky
la
tio
n
of
th
e
co
rr
es
po
nd
in
g
pe
pt
id
e
w
ith
E
B
.S
ta
nd
ar
ds
w
er
e
ch
ar
ac
te
riz
ed
an
d
qu
an
tif
ie
d
by
L
C
-M
S/
M
S
an
d
L
C
-U
V
.T
he
m
et
ho
d
w
as
va
lid
at
ed
w
ith
di
ff
er
en
ta
m
ou
nt
s
of
hu
m
an
H
B
-V
al
st
an
da
rd
.T
he
re
co
ve
ry
w
as
>7
5%
an
d
co
ef
fic
ie
nt
of
va
ria
tio
n
<2
5%
.T
he
L
O
Q
w
as
se
tt
o
10
0
fm
ol
/in
je
ct
io
n.
Fo
ra
pr
oo
fo
fp
rin
ci
pa
le
xp
er
im
en
t,
gl
ob
in
sa
m
pl
es
fr
om
m
al
e
an
d
fe
m
al
e
ra
ts
ex
po
se
d
to
10
00
pp
m
B
D
fo
r9
0
da
ys
w
er
e
an
al
yz
ed
.T
he
am
ou
nt
s
of
H
B
-V
al
pr
es
en
tw
er
e
26
8.
2+
/-
56
an
d
35
0+
/-
70
pm
ol
/g
(m
ea
n+
/-
S.
D
.)
fo
rm
al
es
an
d
fe
m
al
es
,r
es
pe
ct
iv
el
y.
N
o
H
B
-V
al
w
as
de
te
ct
ed
in
co
nt
ro
ls
.T
he
se
da
ta
ar
e
m
uc
h
lo
w
er
co
m
pa
re
d
to
pr
ev
io
us
ly
re
po
rt
ed
va
lu
es
m
ea
su
re
d
by
G
C
-M
S/
M
S.
Th
e
di
ff
er
en
ce
m
ay
be
du
e
hi
gh
er
sp
ec
ifi
ci
ty
of
th
e
L
C
-M
S/
M
S
m
et
ho
d
to
th
e
N
-t
er
m
in
al
pe
pt
id
e
fr
om
th
e
al
ph
a-
ch
ai
n
ve
rs
us
de
riv
at
iz
at
io
n
of
bo
th
al
ph
a-
an
d
be
ta
-c
ha
in
by
E
dm
an
de
gr
ad
at
io
n,
an
d
po
ss
ib
le
in
st
ab
ili
ty
of
H
B
-V
al
ad
du
ct
s
du
rin
g
lo
ng
te
rm
st
or
ag
e
(a
bo
ut
10
ye
ar
s)
be
tw
ee
n
th
e
an
al
ys
es
.T
he
se
di
ff
er
en
ce
s
w
ill
be
re
so
lv
ed
by
ex
am
in
in
g
re
ce
nt
ly
co
lle
ct
ed
sa
m
pl
es
,u
si
ng
th
e
sa
m
e
in
te
rn
al
st
an
da
rd
fo
rp
ar
al
le
la
na
ly
si
s
by
G
C
-M
S/
M
S
an
d
L
C
-M
S/
M
S.
B
as
ed
on
ou
re
xp
er
ie
nc
e
w
ith
py
r-
V
al
ad
du
ct
as
sa
y
w
e
an
tic
ip
at
e
th
at
th
is
as
sa
y
w
ill
be
su
ita
bl
e
fo
r
ev
al
ua
tio
n
of
H
B
-V
al
in
m
ul
tip
le
sp
ec
ie
s.
Ba
ck
gr
ou
nd
O
bj
ec
tiv
e
M
et
ho
d
Re
su
lt
Co
nc
lu
si
on
Re
la
te
d 
w
or
k
Fu
tu
re
 w
or
k
Table 2: Distribution of sentences in the AZ-annotated
corpus
BKG OBJ METH RES CON REL FUT
Word 36828 23493 41544 89538 30752 2456 1174
Sentence 1429 674 1473 3185 1082 95 47
Sentence 18% 8% 18% 40% 14% 1% 1%
8-18% of the corpus each. Two categories are very
low in frequency, only covering 1% of the corpus
each: Related work (REL) and Future work (FUT).
Guo et al (2010) report the inter-annotator agree-
ment between their three annotators: one linguist,
one computational linguist and one domain expert.
According to Cohen?s kappa (Cohen, 1960) the
agreement is relatively high: ? = 0.85.
3 Automatic identification of AZ
3.1 Features and feature extraction
Guo et al (2010) used a variety of features in
their fully supervised ML experiments on different
schemes of information structure. Since their fea-
ture types cover the best performing feature types in
earlier works e.g. (Teufel and Moens, 2002; Lin et
al., 2006; Mullen et al, 2005; Hirohata et al, 2008;
Merity et al, 2009) we re-implemented and used
them in our experiment1. However, being aware
of the fact that some of these features may not be
optimal for weakly-supervised learning (i.e. when
learning from smaller data), we evaluate their per-
formance and suitability for the task later in sec-
tion 4.3.
? Location. Zones tend to appear in typical po-
sitions in abstracts. Each abstract was there-
1The only exception is the history feature which was left out
because it cannot be applied to all of our methods
275
fore divided into ten parts (1-10, measured by
the number of words), and the location was de-
fined by the parts where the sentence begins
and ends.
? Word. All the words in the corpus.
? Bi-gram. Any combination of two adjacent
words in the corpus.
? Verb. All the verbs in the corpus.
? Verb Class. 60 verb classes appearing in
biomedical journal articles.
? Part-of-Speech ? POS. The POS tag of each
verb in the corpus.
? Grammatical Relation ? GR. Subject (nc-
subj), direct object (dobj), indirect object (iobj)
and second object (obj2) relations in the cor-
pus. e.g. (ncsubj observed 14 difference 5
obj). The value of this feature equals 1 if it
occurs in a particular sentence (and 0 if not).
? Subj and Obj. The subjects and objects ap-
pearing with any verbs in the corpus (extracted
from above GRs).
? Voice. The voice of verbs (active or passive) in
the corpus.
These features were extracted from the corpus us-
ing a number of tools. A tokenizer was used to detect
the boundaries of sentences and to separate punctu-
ation from adjacent words e.g. in complex biomed-
ical terms such as 2-amino-3,8-diethylimidazo[4,5-
f]quinoxaline. The C&C tools (Curran et al, 2007)
trained on biomedical literature were employed for
POS tagging, lemmatization and parsing. The
lemma output was used for creating Word, Bi-gram
and Verb features. The GR output was used for cre-
ating the GR, Subj, Obj and Voice features. The
?obj? marker in a subject relation indicates passive
voice (e.g. (ncsubj observed 14 difference 5 obj)).
The verb classes were acquired automatically from
the corpus using the unsupervised spectral cluster-
ing method of (Sun and Korhonen, 2009). To con-
trol the number of features we lemmatized the lexi-
cal items for all the features, and removed the words
and GRs with fewer than 2 occurrences and bi-grams
with fewer than 5 occurrences.
3.2 Machine learning methods
Support Vector Machines (SVM) and Conditional
Random Fields (CRF) have proved the best perform-
ing fully supervised methods in most recent works
on information structure, e.g. (Teufel and Moens,
2002; Mullen et al, 2005; Hirohata et al, 2008; Guo
et al, 2010). We therefore implemented these meth-
ods as well as weakly supervised variations of them:
active SVM with and without self-training, transduc-
tive SVM and semi-supervised CRF.
3.2.1 Supervised methods
SVM constructs hyperplanes in a multidimen-
sional space to separate data points of different
classes. Good separation is achieved by the hyper-
plane that has the largest distance from the nearest
data points of any class. The hyperplane has the
form w ? x ? b = 0, where w is its normal vec-
tor. We want to maximize the distance from the hy-
perplane to the data points, or the distance between
two parallel hyperplanes each of which separates the
data. The parallel hyperplanes can be written as:
w ? x ? b = 1 and w ? x ? b = ?1, and the dis-
tance between them is 2|w| . The problem reduces to:
Minimize |w| (in w, b)
Subject to
w ? x? b ? 1 for x of one class,
w ? x? b ? ?1 for x of the other,
which can be solved by using the SMO algorithm
(Platt, 1999b). We used Weka software (Hall et al,
2009) (employing its linear kernel) for SVM experi-
ments.
CRF is an undirected graphical model which de-
fines a probability distribution over the hidden states
(e.g. label sequences) given the observations. The
probability of a label sequence y given an observa-
tion sequence x can be written as:
p(y|x, ?) = 1Z(x)exp(
?
j ?jFj(y, x)),
where Fj(y, x) is a real-valued feature function of
the states and the observations; ?j is the weight of
Fj , and Z(x) is a normalization factor. The ? pa-
rameters can be learned using the L-BFGS algorithm
(Nocedal, 1980). We used Mallet software (McCal-
lum, 2002) for CRF experiments.
3.2.2 Weakly-supervised methods
Active SVM (ASVM) starts with a small amount of
labeled data, and iteratively chooses a proportion of
276
unlabeled data for which SVM has less confidence
to be labeled (the labels can be restored from the
original corpus) and used in the next round of learn-
ing, i.e. active learning. Query strategies based on
the structure of SVM are frequently employed (Tong
and Koller, 2001; Novak et al, 2006). For exam-
ple, it is often assumed that the data points close to
the separating hyperplane are those that the SVM is
uncertain about. Unlike these methods, our learn-
ing algorithm compares the posterior probabilities
of the best estimate given each unlabeled instance,
and queries those with the lowest probabilities for
the next round of learning. The probabilities can be
obtained by fitting a Sigmoid after the standard SVM
(Platt, 1999a), and combined using a pairwise cou-
pling algorithm (Hastie and Tibshirani, 1998) in the
multi-class case. We used the SVM linear kernel in
Weka for classification, and the -M flag in Weka for
calculating the posterior probabilities.
Active SVM with self-training (ASSVM) is an ex-
tension of ASVM where each round of training has
two steps: (i) training on the labeled, and testing
on the unlabeled data, and querying; (ii) training on
both labeled and unlabeled/machine-labeled data by
using the estimates from step (i). The idea of ASSVM
is to make the best use of the labeled data, and to
make the most use of the unlabeled data.
Transductive SVM (TSVM) is an extension of
SVM which takes advantage of both labeled and un-
labeled data (Vapnik, 1998). Similar to SVM, the
problem is defined as:
Minimize |w| (in w, b, y(u))
Subject to
y(l)(w ? x(l) ? b) ? 1,
y(u)(w ? x(u) ? b) ? 1 ,
y(u) ? {?1, 1},
where x(u) is unlabeled data and y(u) the estimate
of its label. The problem can be solved by using
the CCCP algorithm (Collobert et al, 2006). We
used UniverSVM software (Sinz, 2011) for TSVM
experiments.
Semi-supervised CRF (SSCRF) can be imple-
mented with entropy regularization (ER). It ex-
tends the objective function on Labeled data?
L log p(y(l)|x(l), ?) with an additional term?
U
?
Y p(y|x(u), ?) log p(y|x(u), ?) to minimize
the conditional entropy of the model?s predictions on
Unlabeled data (Jiao et al, 2006; Mann and Mccal-
lum, 2007). We used Mallet software (McCallum,
2002) for SSCRF experiments.
4 Experimental evaluation
4.1 Evaluation methods
We evaluated the ML results in terms of accuracy,
precision, recall, and F-measure against manual AZ
annotations in the corpus:
acc = no. of correctly classified sentencestotal no. ofsentences in the corpus
p = no. of sentences correctly identified as Classitotal no. of sentences identified as Classi
r = no. of sentences correctly identified as Classitotal no. of sentences in Classi
f = 2?p?rp+r
We used 10-fold cross validation for all the meth-
ods to avoid the possible bias introduced by rely-
ing on any particular split of the data. More specif-
ically, the data was randomly assigned to ten folds
of roughly the same size. Each fold was used once
as test data and the remaining nine folds as training
data. The results were then averaged.
Following (Dietterich, 1998), we used McNe-
mar?s test (McNemar, 1947) to measure the statisti-
cal significance between the results of different ML
methods. The chosen significance level was .05.
4.2 Results
Table 3 shows the results for the four weakly-
supervised and two supervised methods when 10%
of the training data (i.e. ?700 sentences) has been
labeled. We can see that ASSVM is the best perform-
ing method with an accuracy of 81% and the macro
Table 3: Results when using 10% of the labeled data
Acc. F-score
MF BKG OBJ METH RES CON REL FUT
SVM .77 .74 .84 .68 .71 .82 .64 - -
CRF .70 .65 .75 .46 .48 .78 .76 - -
ASVM .80 .75 .88 .56 .68 .87 .78 .33
ASSVM .81 .76 .86 .56 .76 .88 .76 - -
TSVM .76 .73 .84 .61 .71 .79 .71 - -
SSCRF .73 .67 .76 .48 .52 .81 .78 - -
MF: Macro F-score of the five high frequency categories:
BKG, OBJ, METH, RES, CON.
277
Figure 2: Learning curve for different methods when using 0-100% of the labeled data
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0% 10% 20% 30% 40% 50% 60% 70% 80% 90% 100%
Labeled
A
cc
u
ra
cy
SVM CRF ASVM ASSVM TSVM SSCRF
Figure 3: Area under learning curves at different intervals
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
SVM CRF ASVM ASSVM TSVM SSCRF
Ar
ea
(0,10%] [10%,20%] [20%,40%] [40%,100%]
F-score of .76 (the macro F-score is calculated for
the 5 scheme categories which are found by all the
methods). ASVM performs nearly as well, with an
accuracy of 80% and F-score of .75. Both methods
outperform supervised SVM with a statistically sig-
nificant difference (p < .001).
TSVM is the lowest performing SVM-based
method. Yielding an accuracy of 76% and F-score
of .73 its performance is lower than that of the super-
vised SVM. However, it does outperform both CRF-
based methods. SSCRF performs better than CRF
with 3% higher accuracy and .02 higher F-score.
The difference in accuracy is statistically significant
(p < .001).
Only one method (ASVM) identifies six out of the
seven possible categories. Other methods identify
five categories. The 1-2 missing categories are very
low in frequency (accounting for 1% of the corpus
data each, see table 2). Looking at the results for
other categories, they seem to reflect the amount of
corpus data available for each category (Table 2),
with RES (Results) being the highest and OBJ (Ob-
jective) the lowest performing category with most
methods. Interestingly, the only method that per-
forms relatively well on OBJ is the supervised SVM.
The best method ASSVM outperforms other meth-
ods most clearly on METH (Method) category. Al-
though METH is a high frequency category (account-
ing for 18% of the corpus data) other methods tend
to confuse it with OBJ, presumably because a single
sentence may contain elements of both (e.g. scien-
tists may describe some of their method when de-
scribing the objective of the study).
Figure 2 shows the learning curve of different
methods (in terms of accuracy) when the percentage
of the labeled data (in the training set) ranges from 0
to 100%. ASSVM outperforms other methods, reach-
ing its best performance of 88% accuracy when us-
ing ?40% of the labeled data. Indeed when using
33% of the labeled data, it performs already equally
well as fully-supervised SVM using 100% of the la-
beled data. The advantage of ASSVM over ASVM
(the second best method) is clear especially when
20-40% of the labeled data is used. SVM and TSVM
tend to perform quite similarly with each other when
more than 25% of the labeled data is used, but when
less data is available, SVM performs better. Look-
ing at the CRF-based methods, SSCRF outperforms
CRF in particular when 10-25% of the labeled data
is used. However, neither of them reaches the per-
formance level of SVM-based methods.
Figure 3 shows the area under the learning curves
(by the trapezoidal rule) at different intervals, which
gives a reasonable approximation to the overall per-
formance of different methods. The area under
ASSVM is the largest at each of the four intervals,
with a value of .08 at (0,10%], .07 at [10%,20%],
278
.20 at [20%, 40%] and .50 at [40%,100%]. The dif-
ference between supervised and weakly-supervised
methods is more significant at (0, 20%] than at
[20%,100%].
4.3 Further analysis of the features
As explained in section 3.1, we employed in our
experiments a collection of features which had per-
formed well in previous supervised AZ experiments.
We conducted further analysis to investigate which
of these features are the most (and the least) useful
for weakly-supervised learning. We took our best
performing method ASSVM and conducted leave-
one-out analysis of the features with 10% of the la-
beled data. The results are shown in Table 4.
Table 4: Leaving one feature out results for ASSVM when
using 10% of the labeled data
Acc. F-score
MF BKG OBJ METH RES CON REL FUT
Location .73 .67 .67 .55 .62 .85 .65 - -
Word .80 .78 .87 .70 .74 .85 .72 - -
Bigram .81 .75 .83 .57 .71 .87 .78 .33 -
Verb .81 .79 .84 .77 .73 .87 .75 - -
VC .79 .75 .86 .62 .72 .84 .70 - -
POS .74 .70 .66 .65 .66 .82 .73 - -
GR .79 .75 .83 .67 .69 .84 .72 - -
Subj .80 .76 .87 .65 .73 .85 .72 - -
Obj .80 .78 .84 .75 .70 .85 .75 - -
Voice .78 .75 .88 .70 .71 .83 .62 - -
? .81 .76 .86 .56 .76 .88 .76 - -
MF: Macro F-score of the five high frequency categories:
BKG, OBJ, METH, RES, CON.
?: Employing all the features.
We can see that the Location feature is by far the
most useful feature for ASSVM. The performance
drops 8% in accuracy and .09 in F-score in the ab-
sence of this feature. Location is particularly im-
portant for BKG (which nearly always appears in the
same location: in the beginning of an abstract) and is
highly useful for METH and CON as well. Removing
POS has almost equally strong effect, in particular
on BKG and METH, suggesting that verb tense is par-
ticularly useful for distinguishing these categories.
Also Voice, Verb class and GR contribute to gen-
eral performance, especially to accuracy. Voice is
particularly important for CON, which differs from
other categories in the sense that it is marked by fre-
quent usage of active voice. Verb class is helpful for
METH, RES and CON while GR is helpful for all high
frequency categories.
Among the least helpful features are those which
suffer from sparse data problems, including e.g.
Word, Bi-gram, and Verb. They perform particularly
badly when applied to low frequency zones. How-
ever, this is not the case when using fully-supervised
methods (i.e. 100% of the labeled data), suggest-
ing that a good performance in fully supervised ex-
periments does not necessarily translate into a good
performance in weakly-supervised experiments, and
that careful feature analysis and selection is impor-
tant when aiming to optimize the performance when
learning from sparse data.
5 Discussion
In our experiments, the majority of weakly-
supervised methods outperformed their correspond-
ing supervised methods when using just 10% of
the labeled data. The SVM-based methods per-
formed better than the CRF-based ones (regardless of
whether they were weakly or fully supervised). Guo
et al (2010) made a similar discovery when com-
paring fully supervised versions of SVM and CRF.
Our best performing weakly-supervised methods
were those based on active learning. Making a good
use of both labeled and unlabeled data, active learn-
ing combined with self-training (ASSVM) proved to
be the most useful method. Given 10% of the la-
beled data, ASSVM obtained an accuracy of 81% and
F-score of .76, outperforming the best supervised
method SVM with a statistically significant differ-
ence. It reached its top performance (88% accuracy)
when using 40% of the labeled data, and performed
equally well as fully supervised SVM (i.e. 100% of
the labeled data) when using just one third of the la-
beled data.
This result is in line with the results of many
other text classification works where active learn-
ing (alone or in combination with other techniques
such as self-training) has proved similarly useful,
e.g. (Lewis and Gale, 1994; Tong and Koller, 2002;
Brinker, 2006; Novak et al, 2006; Esuli and Sebas-
tiani, 2009; Yang et al, 2009).
While active learning iteratively explores the
unknown aspects of the unlabeled data, semi-
supervised learning attempts to make the best use
279
of what it already knows about the data. In our ex-
periments, semi-supervised methods (TSVM and SS-
CRF) did not perform equally well as active learning
? TSVM even produced a lower accuracy than SVM
with the same amount of labeled data ? although
these methods have gained success in related works.
We therefore looked into related works using
TSVM, e.g. (Chapelle and Zien, 2005), and discov-
ered that our dataset is much higher in dimensional-
ity than those employed in many other works. High
dimensional data is more sensitive, and therefore
fine-tuning with unlabeled data may cause a big de-
viation. We also looked into related works using
SSCRF, in particular the work of (Jiao et al, 2006)
who used the same SSCRF as the one we used in our
experiments. Jiao et al (2006) employed a much
larger data set than we did ? one including 5448 la-
beled instances (in 3 classes) and 5210-25145 unla-
beled instances. Given more labeled and unlabeled
data per class we might be able to obtain better per-
formance using SSCRF also on our task. However,
given the high cost of obtaining labeled data meth-
ods not needing it are preferable.
6 Conclusions and future work
Our experiments show that weakly-supervised
learning can be used to identify AZ in scientific
documents with good accuracy when only a limited
amount of labeled data is available. This is helpful
thinking of the real-world application and porting of
the approach to different tasks and domains. To the
best of our knowledge, no previous work has been
done on weakly-supervised learning of information
structure according to schemes of the type we have
focused on (Teufel and Moens, 2002; Mizuta et al,
2006; Lin et al, 2006; Hirohata et al, 2008; Shatkay
et al, 2008; Liakata et al, 2010).
Recently, some work has been done on the related
task of classification of discourse relations in sci-
entific texts: (Hernault et al, 2011) used structural
learning (Ando and Zhang, 2005) for this task. They
obtained 30-60% accuracy on the RST Discourse
Treebank (including 41 relation types) when using
100-10000 labeled and 100000 unlabeled instances.
The accuracy was 20-60% when using the labeled
data only. However, although related, the task of
discourse relation classification differs substantially
from our task in that it focuses on local discourse re-
lations while our task focuses on the global structure
of the scientific document.
In the future, we plan to improve and extend this
work in several directions. First, the approach to
active learning could be improved in various ways.
The query strategy we employed (uncertainty sam-
pling) is a relatively straightforward method which
only considers the best estimate for each unlabeled
instance, disregarding other estimates that may con-
tain useful information. In the future, we plan to
experiment with more sophisticated strategies, e.g.
the margin sampling algorithm by (Scheffer et al,
2001) and the query-by-committee (QBC) algorithm
by (Seung et al, 1992). In addition, there are al-
gorithms designed for reducing the redundancy in
queries which may be worth investigating (Hoi et al,
2006).
Also, (Hoi et al, 2006) shows that Logistic Re-
gression (LR) outperforms SVM when used with ac-
tive learning, yielding higher F-score on the Reuters-
21578 data set (binary classification, 10,788 docu-
ments in total, 100 of them labeled). It would be
interesting to explore whether supervised methods
other than SVM are optimal for active learning when
applied to our task.
Secondly, we plan to investigate other semi-
supervised methods, for example, the Expectation-
Maximization (EM) algorithm. (Lanquillon, 2000)
has shown that EM SVM performs better than super-
vised and transductive SVM on a text classification
task when applied to the dataset of 20 Newsgroups
(20 classes, 4000 documents for testing, 10000 un-
labeled ones), yielding up to ?10% higher accu-
racy when 200-5000 labeled documents are used for
training.
In addition, other combinations of weakly-
supervised methods might be worth looking into,
such as EM+active learning (McCallum and Nigam,
1998) and co-training+EM+active learning (Muslea
et al, 2002), which have proved promising in related
text classification works.
Besides looking for optimal ML strategies, we
plan to look for optimal features for the task. Our
feature analysis showed that not all the features
which had proved promising in fully supervised ex-
periments were equally promising when applied to
weakly-supervised learning from smaller data. We
280
plan to look into ways of reducing the sparse data
problem in features, e.g. by classifying not only
verbs but also other word classes into semantically-
motivated categories.
One the key motivations for developing a weakly-
supervised approach is to facilitate easy porting of
schemes such as AZ to new tasks and domains. Re-
cent research shows that active learning in a target
domain can leverage information from a different
but related (source) domain (Rai et al, 2010). Mak-
ing use of existing annotated datasets in biology,
chemistry, computational linguistics and law (Teufel
and Moens, 2002; Mizuta et al, 2006; Hachey
and Grover, 2006; Teufel et al, 2009) we will ex-
plore optimal ways of combining weakly-supervised
learning with domain-adaptation.
The work presented in this paper has focused on
the abstracts annotated according to the AZ scheme.
In the future, we plan to investigate the usefulness
of weakly-supervised learning for identifying other
schemes of information structure, e.g. (Lin et al,
2006; Hirohata et al, 2008; Shatkay et al, 2008;
Liakata et al, 2010), and not only in scientific ab-
stracts but also in full journal papers which typically
exemplify a larger set of scheme categories.
Finally, an important avenue of future research
is to evaluate the usefulness of weakly-supervised
identification of information structure for NLP tasks
such as summarization and information extraction
(Tbahriti et al, 2006; Ruch et al, 2007), and for
practical tasks such as manual review of scientific
papers for research purposes (Guo et al, 2010).
Acknowledgments
The work reported in this paper was funded by the
Royal Society (UK). YG was funded by the Cam-
bridge International Scholarship.
References
Steven Abney. 2008. Semi-supervised learning for com-
putational linguistics. Chapman & Hall / CRC.
Rie Kubota Ando and Tong Zhang. 2005. A framework
for learning predictive structures from multiple tasks
and unlabeled data. J. Mach. Learn. Res., 6:1817?
1853.
Klaus Brinker. 2006. On active learning in multi-label
classification. In From Data and Information Analysis
to Knowledge Engineering, pages 206?213.
Olivier Chapelle and Alexander Zien. 2005. Semi-
supervised classification by low density separation.
J. Cohen. 1960. A coefficient of agreement for nominal
scales. Educational and Psychological Measurement,
20(1):37?46.
Ronan Collobert, Fabian Sinz, Jason Weston, and Le?on
Bottou. 2006. Trading convexity for scalability. In
Proceedings of the 23rd international conference on
Machine learning.
J. R. Curran, S. Clark, and J. Bos. 2007. Linguistically
motivated large-scale nlp with c&c and boxer. In Pro-
ceedings of the ACL 2007 Demonstrations Session.
Thomas G. Dietterich. 1998. Approximate statistical
tests for comparing supervised classification learning
algorithms. Neural Comput., 10:1895?1923.
Andrea Esuli and Fabrizio Sebastiani. 2009. Active
learning strategies for multi-label text classification.
In Proceedings of the 31th European Conference on
IR Research on Advances in Information Retrieval.
Yufan Guo, Anna Korhonen, Maria Liakata, Ilona Silins
Karolinska, Lin Sun, and Ulla Stenius. 2010. Identi-
fying the information structure of scientific abstracts:
an investigation of three different schemes. In Pro-
ceedings of the 2010 Workshop on Biomedical Natural
Language Processing.
Ben Hachey and Claire Grover. 2006. Extractive sum-
marisation of legal texts. Artif. Intell. Law, 14:305?
345.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The weka data mining software: an update.
SIGKDD Explor. Newsl., 11:10?18.
T. Hastie and R. Tibshirani. 1998. Classification by pair-
wise coupling. Advances in Neural Information Pro-
cessing Systems, 10.
Hugo Hernault, Danushka Bollegala, and Mitsuru
Ishizuka. 2011. Semi-supervised discourse relation
classification with structural learning. In CICLing (1).
K. Hirohata, N. Okazaki, S. Ananiadou, and M. Ishizuka.
2008. Identifying sections in scientific abstracts us-
ing conditional random fields. In Proceedings of 3rd
International Joint Conference on Natural Language
Processing.
Steven C. H. Hoi, Rong Jin, and Michael R. Lyu. 2006.
Large-scale text categorization by batch mode active
learning. In Proceedings of the 15th international con-
ference on World Wide Web.
F. Jiao, S. Wang, C. Lee, R. Greiner, and D. Schuur-
mans. 2006. Semi-supervised conditional random
fields for improved sequence segmentation and label-
ing. In COLING/ACL.
Carsten Lanquillon. 2000. Learning from labeled and
unlabeled documents: A comparative study on semi-
supervised text classification. In Proceedings of the
281
4th European Conference on Principles of Data Min-
ing and Knowledge Discovery.
David D. Lewis and William A. Gale. 1994. A sequential
algorithm for training text classifiers. In Proceedings
of the 17th annual international ACM SIGIR confer-
ence on Research and development in information re-
trieval.
M. Liakata, S. Teufel, A. Siddharthan, and C. Batche-
lor. 2010. Corpora for the conceptualisation and zon-
ing of scientific papers. In Proceedings of the Seventh
conference on International Language Resources and
Evaluation (LREC?10).
J. Lin, D. Karakos, D. Demner-Fushman, and S. Khu-
danpur. 2006. Generative content models for struc-
tural analysis of medical abstracts. In Proceedings of
BioNLP-06.
G. S. Mann and A. Mccallum. 2007. Efficient compu-
tation of entropy gradient for semi-supervised condi-
tional random fields. In HLT-NAACL.
Andrew McCallum and Kamal Nigam. 1998. Employ-
ing em and pool-based active learning for text classi-
fication. In Proceedings of the Fifteenth International
Conference on Machine Learning.
A. K. McCallum. 2002. Mallet: A machine learning for
language toolkit. http://mallet.cs.umass.edu.
Quinn McNemar. 1947. Note on the Sampling Error
of the Difference Between Correlated Proportions or
Percentages. Psychometrika, 12(2):153?157.
S. Merity, T. Murphy, and J. R. Curran. 2009. Accurate
argumentative zoning with maximum entropy models.
In Proceedings of the 2009 Workshop on Text and Ci-
tation Analysis for Scholarly Digital Libraries.
Y. Mizuta, A. Korhonen, T. Mullen, and N. Collier. 2006.
Zone analysis in biology articles as a basis for in-
formation extraction. International Journal of Med-
ical Informatics on Natural Language Processing in
Biomedicine and Its Applications, 75(6):468?487.
T. Mullen, Y. Mizuta, and N. Collier. 2005. A base-
line feature set for learning rhetorical zones using full
articles in the biomedical domain. Natural language
processing and text mining, 7(1):52?58.
Ion Muslea, Steven Minton, and Craig A. Knoblock.
2002. Active + semi-supervised learning = robust
multi-view learning. In Proceedings of the Nineteenth
International Conference on Machine Learning.
Jorge Nocedal. 1980. Updating Quasi-Newton Matrices
with Limited Storage. Mathematics of Computation,
35(151):773?782.
Bla Novak, Dunja Mladeni, and Marko Grobelnik. 2006.
Text classification with active learning. In From Data
and Information Analysis to Knowledge Engineering,
pages 398?405.
J. C. Platt. 1999a. Probabilistic outputs for support vec-
tor machines and comparisons to regularized likeli-
hood methods. Advances in Large Margin Classiers,
pages 61?74.
John C. Platt. 1999b. Using analytic qp and sparseness
to speed training of support vector machines. In Pro-
ceedings of the 1998 conference on Advances in neural
information processing systems II.
Piyush Rai, Avishek Saha, Hal Daume?, III, and Suresh
Venkatasubramanian. 2010. Domain adaptation
meets active learning. In Proceedings of the NAACL
HLT 2010 Workshop on Active Learning for Natural
Language Processing.
P. Ruch, C. Boyer, C. Chichester, I. Tbahriti, A. Geiss-
buhler, P. Fabry, J. Gobeill, V. Pillet, D. Rebholz-
Schuhmann, C. Lovis, and A. L. Veuthey. 2007. Using
argumentation to extract key sentences from biomedi-
cal abstracts. Int J Med Inform, 76(2-3):195?200.
Tobias Scheffer, Christian Decomain, and Stefan Wro-
bel. 2001. Active hidden markov models for informa-
tion extraction. In Proceedings of the 4th International
Conference on Advances in Intelligent Data Analysis.
H. S. Seung, M. Opper, and H. Sompolinsky. 1992.
Query by committee. In Proceedings of the fifth an-
nual workshop on Computational learning theory.
H. Shatkay, F. Pan, A. Rzhetsky, and W. J. Wilbur. 2008.
Multi-dimensional classification of biomedical text:
Toward automated, practical provision of high-utility
text to diverse users. Bioinformatics, 24(18):2086?
2093.
F. Sinz, 2011. UniverSVM Support Vector Ma-
chine with Large Scale CCCP Functionality.
http://www.kyb.mpg.de/bs/people/fabee/universvm.html.
L. Sun and A. Korhonen. 2009. Improving verb cluster-
ing with automatically acquired selectional preference.
In Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing.
I. Tbahriti, C. Chichester, Frederique Lisacek, and
P. Ruch. 2006. Using argumentation to retrieve
articles with similar citations. Int J Med Inform,
75(6):488?495.
S. Teufel and M. Moens. 2002. Summarizing scien-
tific articles: Experiments with relevance and rhetor-
ical status. Computational Linguistics, 28:409?445.
S. Teufel, A. Siddharthan, and C. Batchelor. 2009. To-
wards domain-independent argumentative zoning: Ev-
idence from chemistry and computational linguistics.
In Proceedings of EMNLP.
S. Tong and D. Koller. 2001. Support vector machine
active learning with applications to text classification.
Journal of Machine Learning Research, 2:45?66.
Simon Tong and Daphne Koller. 2002. Support vector
machine active learning with applications to text clas-
sification. J. Mach. Learn. Res., 2:45?66.
282
V. N. Vapnik. 1998. Statistical learning theory. Wiley,
New York.
Bishan Yang, Jian-Tao Sun, Tengjiao Wang, and Zheng
Chen. 2009. Effective multi-label active learning for
text classification. In Proceedings of the 15th ACM
SIGKDD international conference on Knowledge dis-
covery and data mining.
283
Proceedings of NAACL-HLT 2013, pages 928?937,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Improved Information Structure Analysis of Scientific Documents Through
Discourse and Lexical Constraints
Yufan Guo
University of Cambridge, UK
yg244@cam.ac.uk
Roi Reichart
University of Cambridge, UK
rr439@cam.ac.uk
Anna Korhonen
University of Cambridge, UK
alk23@cam.ac.uk
Abstract
Inferring the information structure of scien-
tific documents is useful for many down-
stream applications. Existing feature-based
machine learning approaches to this task re-
quire substantial training data and suffer from
limited performance. Our idea is to guide
feature-based models with declarative domain
knowledge encoded as posterior distribution
constraints. We explore a rich set of discourse
and lexical constraints which we incorporate
through the Generalized Expectation (GE) cri-
terion. Our constrained model improves the
performance of existing fully and lightly su-
pervised models. Even a fully unsupervised
version of this model outperforms lightly su-
pervised feature-based models, showing that
our approach can be useful even when no la-
beled data is available.
1 Introduction
Techniques that enable automatic analysis of the in-
formation structure of scientific articles can help sci-
entists identify information of interest in the grow-
ing volume of scientific literature. For example,
classification of sentences according to argumenta-
tive zones (AZ) ? an information structure scheme
that is applicable across scientific domains (Teufel
et al, 2009) ? can support information retrieval, in-
formation extraction and summarization (Teufel and
Moens, 2002; Tbahriti et al, 2006; Ruch et al,
2007; Liakata et al, 2012; Contractor et al, 2012).
Previous work on sentence-based classification of
scientific literature according to categories of infor-
mation structure has mostly used feature-based ma-
chine learning, such as Support Vector Machines
(SVM) and Conditional Random Fields (CRF) (e.g.
(Teufel and Moens, 2002; Lin et al, 2006; Hiro-
hata et al, 2008; Shatkay et al, 2008; Guo et al,
2010; Liakata et al, 2012)). Unfortunately, the per-
formance of these methods is rather limited, as indi-
cated e.g. by the relatively low numbers reported by
Liakata et al (2012) in biochemistry and chemistry
with per-class F-scores ranging from .18 to .76.
We propose a novel approach to this task in which
traditional feature-based models are augmented with
explicit declarative expert and domain knowledge,
and apply it to sentence-based AZ. We explore two
sources of declarative knowledge for our task - dis-
course and lexical. One way to utilize discourse
knowledge is to guide the model predictions by en-
coding a desired predicted class (i.e. information
category) distribution in a given position in the doc-
ument. Consider, for example, sentence (1) from the
first paragraph of the Discussion section in a paper:
(1) In time, this will prove to be most suitable for
detailed analysis of the role of these hormones in
mammary cancer development.
Although the future tense and cue phrases such as
?in time? can indicate that authors are discussing fu-
ture work (i.e. the ?Future work? class in the AZ
scheme), in this case they refer to their own contri-
bution (i.e. the ?Conclusion? class in AZ). As most
authors discuss their own contribution in the begin-
ning of the Discussion section and future directions
in the end, encoding the desired class distribution as
a function of the position in this section can guide
the model to the right decision.
Likewise, lexical knowledge can guide the model
928
through predicted class distributions for sentences
that contain specific vocabulary. Consider, for ex-
ample, sentence (2):
(2) The values calculated for lungs include the
presumed DNA adduct of BA and might thus be
slightly overestimated.
The verb ?calculated? usually indicates the
?Method? class, but, when accompanied by the
modal verb ?might?, it is more likely to imply that
authors are interpreting their own results (i.e. the
?Conclusion? class in AZ). This can be explicitly
encoded in the model through a target distribution
for sentences containing certain modal verbs.
Recent work has shown that explicit declaration
of domain and expert knowledge can be highly use-
ful for structured NLP tasks such as parsing, POS
tagging and information extraction (Chang et al,
2007; Mann and McCallum, 2008; Ganchev et al,
2010). These works have encoded expert knowledge
through constraints, with different frameworks dif-
fering in the type of constraints and the inference
and learning algorithms used. We build on the Gen-
eralized Expectation (GE) framework (Mann and
McCallum, 2007) which encodes expert knowledge
through a preference (i.e. soft) constraints for pa-
rameter settings for which the predicted label distri-
bution matches a target distribution.
In order to integrate domain knowledge with a
features-based model, we develop a simple taxon-
omy of constraints (i.e. desired class distributions)
and employ a top-down classification algorithm on
top of a Maximum Entropy Model augmented with
GE constraints. This algorithm enables us to break
the multi-class prediction into a pipeline of consecu-
tive, simpler predictions which can be better assisted
by the encoded knowledge.
We experiment in the biological domain with the
eight-category AZ scheme (Table 1) adapted from
(Mizuta et al, 2006) and described in (Contractor
et al, 2012). The results show that our constrained
model substantially outperforms a baseline uncon-
strained Maximum Entropy Model. While this type
of constrained models have previously improved
the feature-based model performance mostly in the
weakly supervised and domain adaptation scenarios
(e.g. (Mann and McCallum, 2007; Mann and Mc-
Callum, 2008; Ganchev et al, 2010)), we demon-
strate substantial gains both when the Maximum En-
Table 1: The AZ categories included in the categorization
scheme of this paper.
Zone Definition
Background (BKG) the background of the study
Problem (PROB) the research problem
Method (METH) the methods used
Result (RES) the results achieved
Conclusion (CON) the authors? conclusions
Connection (CN) work consistent with the current work
Difference (DIFF) work inconsistent with the current work
Future work (FUT) the potential future direction of the research
tropy Model is fully trained and when its training
data is sparse. This demonstrates the importance of
expert knowledge for our task and supports our mod-
eling decision that combines feature-based methods
with domain knowledge encoded via constraints.
2 Previous work
Information structure analysis The information
structure of scientific documents (e.g. journal ar-
ticles, abstracts, essays) can be analyzed in terms
of patterns of topics, functions or relations observed
in multi-sentence scientific text. Computational ap-
proaches have mainly focused on analysis based
on argumentative zones (Teufel and Moens, 2002;
Mizuta et al, 2006; Hachey and Grover, 2006;
Teufel et al, 2009), discourse structure (Burstein et
al., 2003; Webber et al, 2011), qualitative dimen-
sions (Shatkay et al, 2008), scientific claims (Blake,
2009), scientific concepts (Liakata et al, 2010) and
information status (Markert et al, 2012).
Most existing methods for analyzing scientific
text according to information structure use full
supervision in the form of thousands of manu-
ally annotated sentences (Teufel and Moens, 2002;
Burstein et al, 2003; Mizuta et al, 2006; Shatkay
et al, 2008; Guo et al, 2010; Liakata et al, 2012;
Markert et al, 2012). Because manual annotation is
prohibitively expensive, approaches based on light
supervision are now emerging for the task, including
those based on active learning and self-training (Guo
et al, 2011) and unsupervised methods (Varga et al,
2012; Reichart and Korhonen, 2012). Unfortunately,
these approaches do not reach the performance level
of fully supervised models, let alne exceed it. Our
novel method addresses this problem.
Declarative knowledge and constraints Previ-
ous work has shown that incorporating declara-
tive constraints into feature-based machine learning
929
models works well in many NLP tasks (Chang et
al., 2007; Mann and McCallum, 2008; Druck et al,
2008; Bellare et al, 2009; Ganchev et al, 2010).
Such constraints can be used in a semi-supervised or
unsupervised fashion. For example, (Mann and Mc-
Callum, 2008) shows that using CRF in conjunction
with auxiliary constraints on unlabeled data signif-
icantly outperforms traditional CRF in information
extraction, and (Druck et al, 2008) shows that using
declarative constraints alone for unsupervised learn-
ing achieves good results in text classification. We
show that declarative constraints can be highly use-
ful for the identification of information structure of
scientific documents. In contrast with most previous
works, we show that such constraints can improve
the performance of a fully supervised model. The
constraints are particularly helpful for identifying
low-frequency information categories, but still yield
high performance on high-frequency categories.
3 Maximum-Entropy Estimation and
Generalized Expectation (GE)
In this section we describe the Generalized Expecta-
tion method for declarative knowledge encoding.
Maximum Entropy (ME) The idea of General-
ized Expectation (Dud??k, 2007; Mann and McCal-
lum, 2008; Druck et al, 2008) stems from the prin-
ciple of maximum entropy (Jaynes, 1957; Pietra and
Pietra, 1993) which raises the following constrained
optimization problem:
max
p
H(?)
subject to Ep[f(?)] = Ep?[f(?)]
p(?) ? 0
?
p(?) = 1, (1)
where p?(?) is the empirical distribution, p(?) is a
probability distribution in the model and H(?) is the
corresponding information entropy, f(?) is a collec-
tion of feature functions, and Ep[f(?)] and Ep?[f(?)]
are the expectations of f with respect to p(?) and
p?(?). An example of p(?) could be a conditional
probability distribution p(y|x), and H(?) could be
a conditional entropy H(y|x). The optimal p(y|x)
will take on an exponential form:
p?(y|x) =
1
Z?
exp(? ? f(x, y)), (2)
where ? is the Lagrange multipliers in the corre-
sponding unconstrained objective function, and Z?
is the partition function. The dual problem be-
comes maximizing the conditional log-likelihood of
labeled data L (Berger et al, 1996):
max
?
?
(xi,yi)?L
log(p?(yi|xi)), (3)
which is usually known as a Log-linear or Maximum
Entropy Model (MaxEnt).
ME with Generalized Expectation The objec-
tive function and the constraints on expectations in
(1) can be generalized to:
max
?
?
?
x
p?(x)D(p?(y|x)||p0(y|x))
? g(Ep?(x)[Ep?(y|x)[f(x, y)|x]]), (4)
where D(p?||p0) is the divergence from p? to a base
distribution p0, and g(?) is a constraint/penalty func-
tion that takes empirical evidence Ep?(x,y)[f(x, y)] as
a reference point (Pietra and Pietra, 1993; Chen et
al., 2000; Dud??k, 2007). Note that a special case of
this is MaxEnt where p0 is set to be a uniform distri-
bution, D(?) to be the KL divergence, and g(?) to be
an equality constraint.
The constraint g(?) can be set in a relaxed manner:
?
k
1
2?2k
(Ep?(x)[Ep?(y|x)[fk(x, y)|x]]? Ep?(x,y)[fk(x, y)])
2,
which is the logarithm of a Gaussian distribution
centered at the reference values with a diagonal co-
variance matrix (Pietra and Pietra, 1993), and the
dual problem will become a regularized MaxEnt
with a Gaussian prior (?k = 0, ?2k =
1
?2k
) over the
parameters:
max
?
?
(xi,yi)?L
log(p?(yi|xi))?
?
k
?2k
2?2k
(5)
Such a model can be further extended to include
expert knowledge or auxiliary constraints on unla-
beled data U (Mann and McCallum, 2008; Druck et
al., 2008; Bellare et al, 2009):
max
?
?
(xi,yi)?L
log(p?(yi|xi))?
?
k
?2k
2?2k
? ?g?(Ep?(y|x)[f
?(x, y)]) (6)
where f?(?) is a collection of auxiliary feature func-
tions on U , g?(?) is a constraint function that takes
expert/declarative knowledge Ep?(y|x)[f
?(x, y)] as a
reference point, and ? is the weight of the auxiliary
GE term.
930
The auxiliary constraint g?(?) can take on many
forms and the one we used in this work is an L2
penalty function (Dud??k, 2007). We trained the
model with L-BFGS (Nocedal, 1980) in supervised,
semi-supervised and unsupervised fashions on la-
beled and/or unlabeled data, using the Mallet soft-
ware (McCallum, 2002).
4 Incorporating Expert Knowledge into
GE constraints
We defined the auxiliary feature functions ? the ex-
pert knowledge on unlabeled data as1:
f?k (x, y) = 1(xk,yk)(x, y),
such that Ep?(y|x)[fk(x, y)] = p
?(yk|xk), (7)
where 1(xk,yk)(x, y) is an indicator function, and
p?(yk|xk) is a conditional probability specified in
the form of
p?(yk|xk) ? [ak, bk] (8)
by experts. In particular, we took
p?(yk|xk) =
?
?
?
ak if p?(yk|xk) < a
bk if p?(yk|xk) > b
p?(yk|xk) if a ? p?(yk|xk) ? b
(9)
as the reference point when calculating g?(?).
We defined two types of constraints: those based
on discourse properties such as the location of a sen-
tence in a particular section or paragraph, and those
based on lexical properties such as citations, refer-
ences to tables and figures, word lists, tenses, and
so on. Note that the word lists actually contain both
lexical and semantic information.
To make an efficient use of the declarative knowl-
edge we build a taxonomy of information structure
categories centered around the distinction between
categories that describe the authors? OWN work and
those that describe OTHER work (see Section 5). In
practice, our model labels every sentence with an
AZ category augmented by one of the two cate-
gories, OWN or OTHER. In evaluation we consider
only the standard AZ categories which are part of
the annotation scheme of (Contractor et al, 2012).
1Accordingly, Ep?(y|x)[fk(x, y)] = p?(yk|xk)
Table 2: Discourse and lexical constraints for identifying infor-
mation categories at different levels of the information structure
taxonomy.
(a) OWN / OTHER
OWN Discourse
(1) Target(last part of paragraph) = 1
(2) Target(last part of section) = 1
Lexical
(3) Target(tables/figures) ? 1
(4) ?x ? {w|w?we} Target(x) = 1
? ?y ? {w|w?previous} Target(y) = 0
(5) ?x ? {w|w?thus} Target(x) = 1
OTHER Lexical
(6) Target(cite) = 1
(7) Target(cite) > 1
(8) Backward(cite) = 1
? ?x ? {w|w?in addition} Target(x) = 1
(b) PROB / METH / RES / CON / FUT
PROB Discourse
(1) Target(last part in section) = 1
Lexical
(2) ?x ? {w|w?aim} Target(x) = 1
(3) ?x ? {w|w?question} Target(x) = 1
(4) ?x ? {w|w?investigate} Target(x) = 1
METH Lexical
(5) ?x ? {w|w?{use,method}} Target(x) = 1
RES Lexical
(6) Target(tables/figures) ? 1
(7) ?x ? {w|w?observe} Target(x) = 1
CON Lexical
(8) Target(cite) ? 1
(9) ?x ? {w|w?conclude} Target(x) = 1
(10) ?x ? {w|w?{suggest, thus, because, likely}}
Target(x) = 1
FUT Discourse
(11) Target(first part in section) = 1
(12) Target(last part in section) = 1
? ?x ? {w|w?{will,need,future}} Target(x) = 1
Lexical
(13) ?x {w|w?will,future} Target(x) = 1
(14) Target(present continuous tense) = 1
(c) BKG / CN / DIFF
BKG Discourse
(1) Target(first part in paragraph) = 1
(2) Target(first part in section) = 1
Lexical
(3) ?x ? {w|w?we} Target(x) = 1
? ?y ? {w|w?previous} Target(y) = 0
(4) Forward(cite) = 1
? ?x ? {w|w?{consistent,inconsistent,than}}
(Target(x) = 0 ? Forward(x) = 0)
CN Lexical
(5) ?x ? {w|w?consistent} Target(x) = 1
(6) ?x ? {w|w?consistent} Forward(x) = 1
DIFF Lexical
(7) ?x ? {w|w?inconsistent} Target(x) = 1
(8) ?x ? {w|w?inconsistent} Forward(x) = 1
(9) ?x ? {w|w?{inconsistent,than,however}}
Forward(x) = 1 ? ?y ? {w|w?we} Forward(y) = 1
? ?z ? {w|w?previous}} Forward(z) = 0
931
Table 3: The lexical sets used as properties in the constraints.
Cue Synonyms
we our, present study
previous previously, recent, recently
thus therefore
aim objective, goal, purpose
question hypothesis, ?
investigate explore, study, test, examine, evaluate, assess, deter-
mine, characterize, analyze, report, present
use employ
method algorithm, assay
observe see, find, show
conclude conclusion, summarize, summary
suggest illustrate, demonstrate, imply, indicate, confirm, re-
flect, support, prove, reveal
because result from, attribute to
likely probable, probably, possible, possibly, may, could
need remain
future further
consistent match, agree, support, in line, in agreement, similar,
same, analogous
inconsistent conflicting, conflict, contrast, contrary, differ, differ-
ent, difference
than compare
however other hand, although, though, but
The constraints in Table 2(a) refer to the top level
of this taxonomy: distinction between the authors?
own work and the work of others, and the constraints
in Tables 2(b)-(c) refer to the bottom level of the tax-
onomy: distinction between AZ categories related to
the authors? own work (Table 2(b)) and other?s work
(Table 2(c)).
The first and second columns in each table refer
to the y and x variables in Equation (8), respectively.
The functions Target(?), Forward(?) and Backward(?)
refer to the property value for the target, next and
preceding sentence, respectively. If their value is 1
then the property holds for the respective sentence,
if it is 0, the property does not hold. In some cases
the value of such functions can be greater than 1,
meaning that the property appears multiple times in
the sentence. Terms of the form {w|w?{wi}} refer
to any word/bi-grams that have the same sense aswi,
where the actual word set we use with every example
word in Table 2 is described in Table 3.
For example, take constraints (1) and (4) in Table
2(a). The former is a standard discourse constraint
that refers to the probability that the target sentence
describes the authors? own work given that it appears
in the last of the ten parts in the paragraph. The lat-
ter is a standard lexical constraint that refers to the
probability that a sentence presents other people?s
work given that it contains any words in {we, our,
present study} and that it doesn?t contain any words
Figure 1: The constraint taxonomy for top-down modeling.
INFO [Table 2(a)]
OWN [Table 2(b)]
PROB METH RES CON FUT
OTHER [Table 2(c)]
BKG CN DIFF
in {previous, previously, recent, recently}. Our con-
straint set further includes constraints that combine
both types of information. For example, constraint
(12) in Table 2(b) refers to the probability that a sen-
tence discusses future work given that it appears in
the last of the ten parts of the section (discourse) and
that it contains at least one word in {will, future, fur-
ther, need, remain} (lexical).
5 Top-Down Model
An interesting property of our task and domain is
that the available expert knowledge does not directly
support the distinctions between AZ categories, but
it does provide valuable indirect guidance. For ex-
ample, the number of citations in a sentence is only
useful for separating the authors? work from other
people?s work, but not for further fine grained dis-
tinctions between zone categories. Moreover, those
constraints that are useful for making fine grained
distinctions between AZ categories are usually use-
ful only for a particular subset of the categories only.
For example, all the constraints in Table 2(b) are
conditioned on the assumption that the sentence de-
scribes the authors? own work.
To make the best use of the domain knowledge,
we developed a simple constraint taxonomy, and ap-
ply a top-down classification approach which uti-
lizes it. The taxonomy is presented in Figure 1. For
classification we trained three MaxEnt models aug-
mented with GE constraints: one for distinguishing
between OWN and OTHER2, one for distinguishing
between the AZ categories under the OWN auxiliary
category and one for distinguishing between the AZ
categories under the OTHER auxiliary category. At
test time we first apply the first classifier and based
on its prediction we apply either the classifier that
distinguishes between OWN categories or the one
that distinguishes between OTHER categories.
2For the training of this model, each training data AZ cate-
gory is mapped to its respective auxiliary class.
932
6 Experiments
Data We used the full paper corpus used by Contrac-
tor et al (2012) which contains 8171 sentences from
50 biomedical journal articles. The corpus is anno-
tated according to the AZ scheme described in Table
1. AZ describes the logical structure, scientific argu-
mentation and intellectual attribution of a scientific
paper. It was originally introduced by Teufel and
Moens (2002) and applied to computational linguis-
tics papers, and later adapted to other domains such
as biology (Mizuta et al, 2006) ? which we used in
this work ? and chemistry (Teufel et al, 2009).
Table 4 shows the AZ class distribution in full ar-
ticles as well as in individual sections. Since section
names vary across scientific articles, we grouped
similar sections before calculating the statistics (e.g.
Discussion and Conclusions sections were grouped
under Discussion). We can see that although there is
a major category in each section (e.g. CON in Dis-
cussion), up to 36.5% of the sentences in each sec-
tion still belong to other categories.
Features We extracted the following features
from each sentence and used them in the feature-
based classifiers: (1) Discourse features: location in
the article/section/paragraph. For this feature each
text batch was divided to ten equal size parts and the
corresponding feature value identifies the relevant
part; (2) Lexical features: number of citations and
references to tables and figures (0, 1, or more), word,
bi-gram, verb, and verb class (obtained by spectral
clustering (Sun and Korhonen, 2009)); (3) Syntac-
tic features: tense and voice (POS tags of main and
auxiliary verbs), grammatical relation, subject and
object. The lexical and the syntactic features were
extracted for the represented sentence as well as for
its surrounding sentences. We used the C&C POS
tagger and parser (Curran et al, 2007) for extract-
ing the lexical and the syntactic features. Note that
all the information encoded into our constraints is
also encoded in the features and is thus available to
the feature-based model. This enables us to properly
evaluate the impact of our modeling decision which
augments a feature-based model with constraints.
Baselines We compared our model against four
baselines, two with full supervision: Support Vec-
tor Machines (SVM) and Maximum Entropy Mod-
els (MaxEnt), and two with light supervision: Trans-
Table 4: Class distribution (shown in percentages) in articles
and their individual sections in the AZ-annotated corpus.
BKG PROB METH RES CON CN DIFF FUT
Article 16.9 2.8 34.8 17.9 22.3 4.3 0.8 0.2
Introduction 74.8 13.2 5.4 0.6 5.9 0.1 - -
Methods 0.5 0.2 97.5 1.4 0.2 0.2 0.1 -
Results 4.0 2.1 11.7 68.9 12.1 1.1 0.1 -
Discussion 16.9 1.1 0.7 1.5 63.5 13.3 2.4 0.7
Table 5: Performance of baselines on the Discussion section.
BKG PROB METH RES CON CN DIFF FUT
Full supervision
SVM .56 0 0 0 .84 .35 0 0
MaxEnt .55 .08 0 0 .84 .38 0 0
Light supervision with 150 labeled sentence
SVM .26 0 0 0 .80 .05 0 0
TSVM .25 .04 .04 .03 .33 14 .06 .02
MaxEnt .25 0 0 0 .80 .10 0 0
MaxEnt+ER .23 0 0 0 .80 .07 0 0
ductive SVM (TSVM) and semi-supervised Max-
Ent based on Entropy Regularization (ER) (Vapnik,
1998; Jiao et al, 2006). SVM and MaxEnt have
proved successful in information structure analysis
(e.g. (Merity et al, 2009; Guo et al, 2011)) but,
to the best of our knowledge, their semi-supervised
versions have not been used for AZ of full articles.
Parameter tuning The boundaries of the ref-
erence probabilities (ak and bk in Equation (8))
were defined and optimized on the development data
which consists of one third of the corpus. We con-
sidered six types of boundaries: Fairly High for
1, High for [0.9,1), Medium High for [0.5,0.9),
Medium Low for [0.1,0.5), Low for [0,0.1), and
Fairly Low for 0.
Evaluation We evaluated the precision, recall and
F-score for each category, using a standard ten-fold
cross-validation scheme. The models were tested on
each of the ten folds and trained on the rest of them,
and the results were averaged across the ten folds.
7 Results
We report results at two levels of granularity. We
first provide detailed results for the Discussion sec-
tion which should be, as is clearly evident from Ta-
ble 4, the most difficult section for AZ prediction as
only 63.5% of its sentences take its most dominant
class (CON). As we show below, this is also where
our constrained model is most effective. We then
show the advantages of our model for other sections.
Results for the Discussion section To get a bet-
933
Table 6: Discussion section performance of MaxEnt, MaxEnt+GE and a MaxEnt+GE model that does not include our top-down
classification scheme. Results are presented for different amounts of labeled training data. The MaxEnt+GE (Top-down) model
outperforms the MaxEnt in 44 out of 48 cases, and MaxEnt+GE (Flat) in 39 out of 48 cases.
MaxEnt MaxEnt + GE (Top-down) MaxEnt + GE (Flat)
50 100 150 500 1000 Full 50 100 150 500 1000 Full 50 100 150 500 1000 Full
BKG .10 .26 .25 .44 .48 .55 .49 .49 .48 .52 .55 .57 .35 .37 .37 .46 .51 .53
PROB 0 0 0 0 0 0 .38 .16 .29 .13 .30 .41 .38 .23 .19 .39 .38 .33
METH 0 0 0 0 0 0 .17 .22 .37 .35 .50 .39 .16 .17 .21 .24 .32 .29
RES 0 0 0 0 0 0 .18 .24 .58 0 0 .46 .13 .05 .21 .31 .25 .34
CON .79 .80 .80 .83 .83 .84 .77 .78 .82 .83 .84 .84 .63 .66 .68 .74 .78 .78
CN .02 .04 .10 .24 .34 .38 .29 .31 .33 .35 .40 .39 .21 .21 .24 .26 .30 .32
DIFF 0 0 0 0 0 0 .26 .25 .25 .19 .24 .21 .14 .16 .15 .14 .18 .17
FUT 0 0 0 0 0 0 .35 .38 .31 .25 .35 .31 .36 .36 .39 .33 .25 .37
Figure 2: Performance of the MaxEnt and MaxEnt+GE models on the Introduction (left), Methods (middle) and Results (right)
sections. The MaxEnt+GE model is superior.
0
0.2
0.4
0.6
0.8
1
BKG PROB METH RES CON CN DIFF FUT
F-s
co
re
MaxEnt MaxEnt+GE
 
 
 
 
0
0.2
0.4
0.6
0.8
1
BKG PROB METH RES CON CN DIFF FUT
F-s
co
re
MaxEnt MaxEnt+GE
 
 
 
0
0.2
0.4
0.6
0.8
1
BKG PROB METH RES CON CN DIFF FUT
F-s
co
re
MaxEnt MaxEnt+GE
 
0
0.2
0.4
0.6
0.8
1
BKG PROB METH RES CON CN DIFF FUT
F-s
co
re
MaxEnt MaxEnt+GE
 Table 7: Discussion section performance of the MaxEnt, Max-
Ent+GE and unsupervised GE models when the former two are
trained with 150 labeled sentences. Unsupervised GE outper-
forms the standard MaxEnt model for all categories except for
CON ? the major c tegory of the section. The result pattern for
the other sections are very similar.
MaxEnt MaxEnt + GE Unsup GE
P R F P R F P R F
BKG .38 .19 .25 .49 .48 .48 .49 .44 .46
PROB 0 0 0 .38 .23 .29 .28 .38 .32
METH 0 0 0 .29 .50 .37 .08 .56 .14
RES 0 0 0 .68 .51 .58 .08 .51 .14
CON .69 .96 .80 .81 .84 .82 .74 .69 .71
CN .35 .06 .10 .39 .29 .33 .40 .13 .20
DIFF 0 0 0 .21 .30 .25 .12 .13 .12
FUT 0 0 0 .24 .44 .31 .26 .61 .36
ter understanding of the nature of the challenge we
face, Table 5 shows the F-scores of fully- and semi-
supervised SVM and MaxEnt on the Discussion sec-
tion. The dominant zone category CON, which ac-
counts for 63.5% of the section sentences, has the
highest F-scores for all methods and scenarios. Most
of the methods also identify the second and the third
most frequent zones BKG and CN, but with relatively
lower F-scores. Other low-frequency categories can
hardly be identified by any of the methods regardless
of the amount of labeled data available for training.
Note that the compared models perform quite sim-
ilarly. We therefore use the MaxEnt model, which
Table 8: Analysis of the impact of the different constraint types
for the lightly supervised and the fully supervised cases. Results
are presented for the Discussion section. Using only the lexical
constraints is generally preferable in the fully supervised case.
Combining the different constraint types is preferable for the
lightly supervised case.
Discourse Lexical Discourse+Lexical
150 Full 150 Full 150 Full
BKG .29 .55 .46 .58 .48 .57
PROB 0 0 .37 .40 .29 .41
METH 0 .11 .29 .35 .37 .39
RES 0 .06 .32 .47 .58 .46
CON .81 .84 .80 .84 .82 .84
CN .12 .34 .35 .42 .33 .39
DIFF 0 0 .21 .21 .25 .21
FUT 0 0 0 .29 .31 .31
is most naturally augmented with GE constraints, as
the baseline unconstrained model.
When adding the GE constraints we observe a
substantial performance gain, in both the fully and
the lightly supervised cases, especially for the low-
frequency categories. Table 6 presents the F-scores
of MaxEnt with and without GE constraints (?Max-
Ent+GE (Top-down)? and ?MaxEnt?) in the light
and full supervision scenarios. Incorporating GE
into MaxEnt results in a substantial F-score im-
provement for all AZ categories except for the ma-
jor category CON for which the performance is kept
very similar. In total, MaxEnt+GE (Top-down) is
934
better in 44 out of the 48 cases presented in the table.
Importantly, the constrained model provides sub-
stantial improvements for both the relatively high-
frequency classes (BKG and CN which together label
30.2% of the sentences) and for the low-frequency
classes (which together label 6.4% of the sentences).
The table also clearly demonstrates the impact of
our tree-based top-down classification scheme, by
comparing the Top-down version of MaxEnt+GE
to the standard ?Flat? version. In 39 out of 48
cases, the Top-down model performs better. In some
cases, especially for high-frequency categories and
when the amount of training data increases, un-
constrained MaxEnt even outperforms the flat Max-
Ent+GE model. The results presented in the rest of
the paper for the MaxEnt+GE model therefore refer
to its Top-down version.
All sections We next turn to the performance of
our model on the three other sections. Our exper-
iments show that augmenting the MaxEnt model
with domain knowledge constraints improves per-
formance for all the categories (either low or high
frequency), except the major section category, and
keep the performance for the latter on the same level.
Figure 2 demonstrates this pattern for the lightly su-
pervised case with 150 training sentences but the
same pattern applies to all other amounts of training
data, including the fully supervised case. Naturally,
we cannot demonstrate all these cases due to space
limitations. The result patterns are very similar to
those presented above for the Discussion section.
Unsupervised GE We next explore the quality of
the domain knowledge constraints when used in iso-
lation from a feature-based model. The objective
function of this model is identical to Equation (6)
except that the first (likelihood) term is omitted. Our
experiments reveal that this unsupervised GE model
outperforms standard MaxEnt for all the categories
except the major category of the section, when up
to 150 training sentences are used. Table 7 demon-
strates this for the Discussion section. This pattern
holds for the other scientific article sections. Even
when more than 150 labeled sentences are used, the
unsupervised model better detects the low frequency
categories (i.e. those that label less than 10% of
the sentences) for all sections. These results provide
strong evidence for the usefulness of our constraints
even when they are used with no labeled data.
Model component analysis We finally analyze
the impact of the different types of constraints on
the performance of our model. Table 8 presents the
Discussion section performance of the constrained
model with only one or the full set of constraints.
Interestingly, when the feature-based model is fully
trained the application of the lexical constraints
alone results in a very similar performance to the
application of the full set of lexical and discourse
constraints. It is only in the lightly supervised case
where the full set of constraints is required and re-
sults in the best performing model.
8 Conclusions and Future Work
We have explored the application of posterior dis-
course and lexical constraints for the analysis of the
information structure of scientific documents. Our
results are strong. Our constrained model outper-
forms standard feature-based models by a large mar-
gin in both the fully and the lightly supervised cases.
Even an unsupervised model based on these con-
straints provides substantial gains over feature-based
models for most AZ categories.
We provide a detailed analysis of the results
which reveals a number of interesting properties of
our model which may be useful for future research.
First, the constrained model significantly outper-
forms its unconstrained counterpart for low-medium
frequency categories while keeping the performance
on the major section category very similar to that of
the baseline model. Improved modeling of the major
category is one direction for future research. Sec-
ond, our full constraint set is most beneficial in the
lightly supervised case while the lexical constraints
alone yield equally good performance in the fully
supervised case. This calls for better understand-
ing of the role of discourse constraints for our task
as well as for the design of additional constraints
that can enhance the model performance either in
combination with the existing constraints or when
separately applied to the task. Finally, we demon-
strated that our top-down tree classification scheme
provides a substantial portion of our model?s impact.
A clear direction for future research is the design of
more fine-grained constraint taxonomies which can
enable efficient usage of other constraint types and
can result in further improvements in performance.
935
References
Kedar Bellare, Gregory Druck, and Andrew McCallum.
2009. Alternating projections for learning with expec-
tation constraints. In Proceedings of the Twenty-Fifth
Conference on Uncertainty in Artificial Intelligence,
UAI ?09, pages 43?50, Arlington, Virginia, United
States. AUAI Press.
Adam L. Berger, Vincent J. Della Pietra, and Stephen
A. Della Pietra. 1996. A maximum entropy approach
to natural language processing. Comput. Linguist.,
22(1):39?71.
Catherine Blake. 2009. Beyond genes, proteins, and
abstracts: Identifying scientific claims from full-text
biomedical articles. J Biomed Inform, 43(2):173?89.
Jill Burstein, Daniel Marcu, and Kevin Knight. 2003.
Finding the write stuff: Automatic identification of
discourse structure in student essays. IEEE Intelligent
Systems, 18(1):32?39.
M.W. Chang, L. Ratinovc, and D. Roth. 2007. Guiding
semi-supervision with constraint-driven learning. In
ACL.
Stanley F. Chen, Ronald Rosenfeld, and Associate Mem-
ber. 2000. A survey of smoothing techniques for me
models. IEEE Transactions on Speech and Audio Pro-
cessing, 8:37?50.
Danish Contractor, Yufan Guo, and Anna Korhonen.
2012. Using argumentative zones for extractive sum-
marization of scientific articles. In COLING.
J. R. Curran, S. Clark, and J. Bos. 2007. Linguisti-
cally motivated large-scale nlp with c&c and boxer. In
Proceedings of the ACL 2007 Demonstrations Session,
pages 33?36.
Gregory Druck, Gideon Mann, and Andrew McCallum.
2008. Learning from labeled features using gener-
alized expectation criteria. In Proceedings of the
31st annual international ACM SIGIR conference on
Research and development in information retrieval,
pages 595?602.
Miroslav Dud??k. 2007. Maximum entropy density
estimation and modeling geographic distributions of
species. Ph.D. thesis.
K. Ganchev, J. Graca, J. Gillenwater, and B. Taskar.
2010. Posterior regularization for structured latent
variable models. Journal of Machine Learning Re-
search.
Yufan Guo, Anna Korhonen, Maria Liakata, Ilona Silins
Karolinska, Lin Sun, and Ulla Stenius. 2010. Identi-
fying the information structure of scientific abstracts:
an investigation of three different schemes. In Pro-
ceedings of BioNLP, pages 99?107.
Yufan Guo, Anna Korhonen, and Thierry Poibeau. 2011.
A weakly-supervised approach to argumentative zon-
ing of scientific documents. In Proceedings of the
2011 Conference on Empirical Methods in Natural
Language Processing, pages 273?283.
Ben Hachey and Claire Grover. 2006. Extractive sum-
marisation of legal texts. Artif. Intell. Law, 14:305?
345.
K. Hirohata, N. Okazaki, S. Ananiadou, and M. Ishizuka.
2008. Identifying sections in scientific abstracts us-
ing conditional random fields. In Proceedings of 3rd
International Joint Conference on Natural Language
Processing, pages 381?388.
E. T. Jaynes. 1957. Information Theory and Statistical
Mechanics. Physical Review Online Archive (Prola),
106(4):620?630.
F. Jiao, S. Wang, C. Lee, R. Greiner, and D. Schuur-
mans. 2006. Semi-supervised conditional random
fields for improved sequence segmentation and label-
ing. In COLING/ACL, pages 209?216.
M. Liakata, S. Teufel, A. Siddharthan, and C. Batchelor.
2010. Corpora for the conceptualisation and zoning of
scientific papers. In Proceedings of LREC?10.
Maria Liakata, Shyamasree Saha, Simon Dobnik, Colin
Batchelor, and Dietrich Rebholz-Schuhmann. 2012.
Automatic recognition of conceptualisation zones in
scientific articles and two life science applications.
Bioinformatics, 28:991?1000.
J. Lin, D. Karakos, D. Demner-Fushman, and S. Khu-
danpur. 2006. Generative content models for struc-
tural analysis of medical abstracts. In Proceedings of
BioNLP-06, pages 65?72.
G. Mann and A. McCallum. 2007. Simple, robust, scal-
able semi-supervised learning via expectation regular-
ization. In ICML.
G. Mann and A. McCallum. 2008. Generalized expec-
tation criteria for semi-supervised learning of condi-
tional random fields. In ACL.
Katja Markert, Yufang Hou, and Michael Strube. 2012.
Collective classification for fine-grained information
status. In Proceedings of ACL 2012, pages 795?804.
A. K. McCallum. 2002. Mallet: A machine learning for
language toolkit. http://mallet.cs.umass.edu.
S. Merity, T. Murphy, and J. R. Curran. 2009. Accurate
argumentative zoning with maximum entropy models.
In Proceedings of the 2009 Workshop on Text and Ci-
tation Analysis for Scholarly Digital Libraries, pages
19?26.
Y. Mizuta, A. Korhonen, T. Mullen, and N. Collier. 2006.
Zone analysis in biology articles as a basis for in-
formation extraction. International Journal of Med-
ical Informatics on Natural Language Processing in
Biomedicine and Its Applications, 75(6):468?487.
Jorge Nocedal. 1980. Updating Quasi-Newton Matrices
with Limited Storage. Mathematics of Computation,
35(151):773?782.
936
S. Della Pietra and V. Della Pietra. 1993. Statistical mod-
eling by me. Technical report, IBM.
Roi Reichart and Anna Korhonen. 2012. Document and
corpus level inference for unsupervised and transduc-
tive learning of information structure of scientic docu-
ments. In Proceedings of COLING 2012.
P. Ruch, C. Boyer, C. Chichester, I. Tbahriti, A. Geiss-
buhler, P. Fabry, J. Gobeill, V. Pillet, D. Rebholz-
Schuhmann, C. Lovis, and A. L. Veuthey. 2007. Using
argumentation to extract key sentences from biomedi-
cal abstracts. Int J Med Inform, 76(2-3):195?200.
H. Shatkay, F. Pan, A. Rzhetsky, and W. J. Wilbur. 2008.
Multi-dimensional classification of biomedical text:
Toward automated, practical provision of high-utility
text to diverse users. Bioinformatics, 24(18):2086?
2093.
L. Sun and A. Korhonen. 2009. Improving verb cluster-
ing with automatically acquired selectional preference.
In Proceedings of EMNLP, pages 638?647.
I. Tbahriti, C. Chichester, Frederique Lisacek, and
P. Ruch. 2006. Using argumentation to retrieve
articles with similar citations. Int J Med Inform,
75(6):488?495.
S. Teufel and M. Moens. 2002. Summarizing scien-
tific articles: Experiments with relevance and rhetor-
ical status. Computational Linguistics, 28:409?445.
S. Teufel, A. Siddharthan, and C. Batchelor. 2009. To-
wards discipline-independent argumentative zoning:
Evidence from chemistry and computational linguis-
tics. In EMNLP.
V. N. Vapnik. 1998. Statistical learning theory. Wiley,
New York.
Andrea Varga, Daniel Preotiuc-Pietro, and Fabio
Ciravegna. 2012. Unsupervised document zone iden-
tification using probabilistic graphical models. In
Proceedings of the Eight International Conference on
Language Resources and Evaluation (LREC?12).
B. Webber, M. Egg, and V. Kordoni. 2011. Discourse
structure and language technology. Natural Language
Engineering, 18:437?490.
937
Proceedings of the 2010 Workshop on Biomedical Natural Language Processing, ACL 2010, pages 99?107,
Uppsala, Sweden, 15 July 2010. c?2010 Association for Computational Linguistics
Identifying the Information Structure of Scientific Abstracts: An
Investigation of Three Different Schemes
Yufan Guo
University of Cambridge, UK
yg244@cam.ac.uk
Anna Korhonen
University of Cambridge, UK
alk23@cam.ac.uk
Maria Liakata
Aberystwyth University, UK
mal@aber.ac.uk
Ilona Silins
Karolinska Institutet, SWEDEN
Ilona.Silins@ki.se
Lin Sun
University of Cambridge, UK
ls418@cam.ac.uk
Ulla Stenius
Karolinska Institutet, SWEDEN
Ulla.Stenius@ki.se
Abstract
Many practical tasks require accessing
specific types of information in scientific
literature; e.g. information about the ob-
jective, methods, results or conclusions of
the study in question. Several schemes
have been developed to characterize such
information in full journal papers. Yet
many tasks focus on abstracts instead. We
take three schemes of different type and
granularity (those based on section names,
argumentative zones and conceptual struc-
ture of documents) and investigate their
applicability to biomedical abstracts. We
show that even for the finest-grained of
these schemes, the majority of categories
appear in abstracts and can be identified
relatively reliably using machine learning.
We discuss the impact of our results and
the need for subsequent task-based evalu-
ation of the schemes.
1 Introduction
Scientific abstracts tend to be very similar in terms
of their information structure. For example, many
abstracts provide some background information
before defining the precise objective of the study,
and the conclusions are typically preceded by the
description of the results obtained.
Many readers of scientific abstracts are inter-
ested in specific types of information only, e.g.
the general background of the study, the methods
used in the study, or the results obtained. Accord-
ingly, many text mining tasks focus on the ex-
traction of information from certain parts of ab-
stracts only. Therefore classification of abstracts
(or full articles) according to the categories of in-
formation structure can support both the manual
study of scientific literature as well as its auto-
matic analysis, e.g. information extraction, sum-
marization and information retrieval (Teufel and
Moens, 2002; Mizuta et al, 2005; Tbahriti et al,
2006; Ruch et al, 2007).
To date, a number of different schemes and
techniques have been proposed for sentence-based
classification of scientific literature according to
information structure, e.g. (Teufel and Moens,
2002; Mizuta et al, 2005; Lin et al, 2006; Hi-
rohata et al, 2008; Teufel et al, 2009; Shatkay
et al, 2008; Liakata et al, 2010). Some of the
schemes are coarse-grained and merely classify
sentences according to typical section names seen
in scientific documents (Lin et al, 2006; Hirohata
et al, 2008). Others are finer-grained and based
e.g. on argumentative zones (Teufel and Moens,
2002; Mizuta et al, 2005; Teufel et al, 2009),
qualitative dimensions (Shatkay et al, 2008) or
conceptual structure (Liakata et al, 2010) of doc-
uments.
The majority of such schemes have been de-
veloped for full scientific journal articles which
are richer in information and also considered to
be more in need of the definition of information
structure (Lin, 2009). However, many practical
tasks currently focus on abstracts. As a distilled
summary of key information in full articles, ab-
stracts may exhibit an entirely different distribu-
tion of scheme categories than full articles. For
tasks involving abstracts, it would be useful to
know which schemes are applicable to abstracts
and which can be automatically identified in them
with reasonable accuracy.
In this paper, we will compare the applicabil-
ity of three different schemes ? those based on
section names, argumentative zones and concep-
tual structure of documents ? to a collection of
biomedical abstracts used for cancer risk assess-
ment (CRA). CRA is an example of a real-world
task which could greatly benefit from knowledge
about the information structure of abstracts since
cancer risk assessors look for a variety of infor-
mation in them ranging from specific methods to
99
results concerning different chemicals (Korhonen
et al, 2009). We report work on the annotation
of CRA abstracts according to each scheme and
investigate the schemes in terms of their distri-
bution, mutual overlap, and the success of iden-
tifying them automatically using machine learn-
ing. Our investigation provides an initial idea of
the practical usefulness of the schemes for tasks
involving abstracts. We discuss the impact of our
results and the further task-based evaluation which
we intend to conduct in the context of CRA.
2 The three schemes
We investigate three different schemes ? those
based on Section Names (S1), Argumentative
Zones (S2) and Core Scientific Concepts (S3):
S1: The first scheme differs from the others in the
sense that it is actually designed for abstracts. It
is based on section names found in some scientific
abstracts. We use the 4-way classification from
(Hirohata et al, 2008) where abstracts are divided
into objective, method, results and conclusions.
Table 1 provides a short description of each cate-
gory for this and other schemes (see also this table
for any category abbreviations used in this paper).
S2: The second scheme is based on Argumenta-
tive Zoning (AZ) of documents. The idea of AZ
is to follow the knowledge claims made by au-
thors. Teufel and Moens (2002) introduced AZ
and applied it to computational linguistics papers.
Mizuta et al (2005) modified the scheme for biol-
ogy papers. More recently, Teufel et al (2009) in-
troduced a refined version of AZ and applied it to
chemistry papers. As these schemes are too fine-
grained for abstracts (some of the categories do
not appear in abstracts at all), we adopt a reduced
version of AZ which integrates seven categories
from (Teufel and Moens, 2002) and (Mizuta et al,
2005) - those which actually appear in abstracts.
S3: The third scheme is concept-driven and
ontology-motivated (Liakata et al, 2010). It treats
scientific papers as humanly-readable representa-
tions of scientific investigations and seeks to re-
trieve the structure of the investigation from the
paper as generic high-level Core Scientific Con-
cepts (CoreSC). The CoreSC is a 3-layer annota-
tion scheme but we only consider the first layer
in the current work. The second layer pertains to
properties of the categories (e.g. ?advantage? vs.
?disadvantage? of METH, ?new? vs. ?old? METH
or OBJT). Such level of granularity is rare in ab-
stracts. The 3rd layer involves coreference iden-
tification between the same instances of each cat-
egory, which is also not of concern in abstracts.
With eleven categories, S3 is the most fine-grained
of our schemes. CoreSC has been previously ap-
plied to chemistry papers (Liakata et al, 2010,
2009).
3 Data: cancer risk assessment abstracts
We used as our data the corpus of CRA ab-
stracts described in (Korhonen et al, 2009) which
contains MedLine abstracts from different sub-
domains of biomedicine. The abstracts were se-
lected so that they provide rich information about
various scientific data (human, animal and cellu-
lar) used for CRA. We selected 1000 abstracts (in
random) from this corpus. The resulting data in-
cludes 7,985 sentences and 225,785 words in total.
4 Annotation of abstracts
Annotation guidelines. We used the guidelines of
Liakata for S3 (Liakata and Soldatova, 2008), and
developed the guidelines for S1 and S2 (15 pages
each). The guidelines define the unit (a sentence)
and the categories of annotation and provide ad-
vice for conflict resolution (e.g. which categories
to prefer when two or several are possible within
the same sentence), as well as examples of anno-
tated abstracts.
Annotation tool. We modified the annotation tool
of Korhonen et al (2009) so that it could be used to
annotate abstracts according to the schemes. This
tool was originally developed for the annotation of
CRA abstracts according to the scientific evidence
they contain. The tool works as a Firefox plug-in.
Figure 1 shows an example of an abstract anno-
tated according to the three schemes.
Description of annotation. Using the guidelines
and the tool, the CRA corpus was annotated ac-
cording to each of the schemes. The annotation
proceeded scheme by scheme, independently, so
that annotations of one scheme were not based on
any of the other two. One annotator (a computa-
tional linguist) annotated all the abstracts accord-
ing to the three schemes, starting from the coarse-
grained S1, then proceeding to S2 and finally to
the finest-grained S3. It took 45, 50 and 90 hours
in total for S1, S2 and S3, respectively.
The resulting corpus. Table 2 shows the distri-
bution of sentences per scheme category in the re-
sulting corpus.
100
Table 1: The Three Schemes
S1 Objective OBJ The background and the aim of the research
Method METH The way to achieve the goal
Result RES The principle findings
Conclusion CON Analysis, discussion and the main conclusions
S2 Background BKG The circumstances pertaining to the current work, situation, or its causes, history, etc.
Objective OBJ A thing aimed at or sought, a target or goal
Method METH A way of doing research, esp. according to a defined and regular plan; a special form of proce-
dure or characteristic set of procedures employed in a field of study as a mode of investigation
and inquiry
Result RES The effect, consequence, issue or outcome of an experiment; the quantity, formula, etc. obtained
by calculation
Conclusion CON A judgment or statement arrived at by any reasoning process; an inference, deduction, induc-
tion; a proposition deduced by reasoning from other propositions; the result of a discussion,
or examination of a question, final determination, decision, resolution, final arrangement or
agreement
Related work REL A comparison between the current work and the related work
Future work FUT The work that needs to be done in the future
S3 Hypothesis HYP A statement that has not been yet confirmed rather than a factual statement
Motivation MOT The reason for carrying out the investigation
Background BKG Description of generally accepted background knowledge and previous work
Goal GOAL The target state of the investigation where intended discoveries are made
Object OBJT An entity which is a product or main theme of the investigation
Experiment EXP Experiment details
Model MOD A statement about a theoretical model or framework
Method METH The means by which the authors seek to achieve a goal of the investigation
Observation OBS The data/phenomena recorded within an investigation
Result RES Factual statements about the outputs of an investigation
Conclusion CON Statements inferred from observations and results, relating to research hypothesis
Inter-annotator agreement. We measured the
inter-annotator agreement on 300 abstracts (i.e. a
third of the corpus) using three annotators (one lin-
guist, one expert in CRA, and the computational
linguist who annotated all the corpus). Accord-
ing to Cohen?s Kappa (Cohen, 1960), the inter-
annotator agreement for S1, S2, and S3 was ? =
0.84, ? = 0.85, and ? = 0.50, respectively. Ac-
cording to (Landis and Koch, 1977), the agree-
ment 0.81-1.00 is perfect and 0.41-0.60 is mod-
erate. Our results indicate that S1 and S2 are
the easiest schemes for the annotators and S3 the
most challenging. This is not surprising as S3 is
the scheme with the finest granularity. Its reliable
identification may require a longer period of train-
ing and possibly improved guidelines. Moreover,
previous annotation efforts using S3 have used do-
main experts for annotation (Liakata et al, 2009,
2010). In our case the domain expert and the lin-
guist agreed the most on S3 (? = 0.60). For S1
and S2 the best agreement was between the lin-
guist and the computational linguist (? = 0.87 and
? = 0.88, respectively).
Table 2: Distribution of sentences in the scheme-
annotated CRA corpus
S1 OBJ METH RES CON
61483 39163 89575 35564 Words
2145 1396 3203 1241 Sentences
27% 17% 40% 16% Sentences
S2 BKG OBJ METH RES CON REL FUT
36828 23493 41544 89538 30752 2456 1174 Words
1429 674 1473 3185 1082 95 47 Sentences
18% 8% 18% 40% 14% 1% 1% Sentences
S3 HYP MOT BKG GOAL OBJT EXP MOD METH OBS RES CON
2676 4277 28028 10612 15894 22444 1157 17982 17402 75951 29362 Words
99 172 1088 294 474 805 41 637 744 2582 1049 Sentences
1% 2% 14% 4% 6% 10% 1% 8% 9% 32% 13% Sentences
5 Comparison of the schemes in terms of
annotations
The three schemes we have used to annotate ab-
stracts were developed independently and have
separate guidelines. Thus, even though they seem
to have some categories in common (e.g. METH,
RES, CON) this does not necessarily guarantee that
the latter cover the same information across all
three schemes. We therefore wanted to investigate
the relation between the schemes and the extent of
overlap or complementarity between them.
We used the annotations obtained with each
scheme to create three contingency matrices for
pairwise comparison. We calculated the chi-
squared Pearson statistic, the chi-squared like-
101
Figure 1: An example of an abstract annotated ac-
cording to the three schemes
S1
S2
S3
lihood ratio, the contingency coefficient and
Cramer?s V (Table 3)1, all of which showed a def-
inite correlation between rows and columns for the
pairwise comparison of all three schemes.
However, none of the above measures give an
indication of the differential association between
schemes, i.e. whether it goes both directions and
to what extent. For this reason we calculated the
Goodman-Kruskal lambda L statistic (Siegel and
Castellan, 1988), which gives us the reduction in
error for predicting the categories of one annota-
tion scheme, if we know the categories assigned
according to the other. When using the categories
of S1 as the independent variables, we obtained a
lambda of over 0.72 which suggests a 72% reduc-
tion in error in predicting S2 categories and 47% in
1These are association measures for r x c tables. We used
the implementation in the vcd package of R (http://www.r-
project.org/).
predicting S3 categories. With S2 categories being
the independent variables, we obtained a reduction
in error of 88% when predicting S1 and 55% when
predicting S3 categories. The lower lambdas for
predicting S3 are hardly surprising as S3 has 11
categories as opposed to 4 and 7 for S1 and S2 re-
spectively. S3 on the other hand has strong predic-
tive power in predicting the categories of S1 and
S2 with lambdas of 0.86 and 0.84 respectively. In
terms of association, S1 and S2 seem to be more
strongly associated, followed by S1 and S3 and
then S2 and S3.
We were then interested in the correspondence
between the actual categories of the three schemes,
which is visualized in Figure 2. Looking at the
categories of S1, OBJ maps mostly to BKG and OBJ
in S2 (with a small percentage in METH and REL).
S1 OBJ maps to BKG, GOAL, HYP, MOT and OBJT
in S3 (with a small percentage in METH and MOD).
S1 METH maps to METH in S2 (with a small per-
centage in S2 OBJ) while it maps to EXP, METH
and MOD in S3 (with a small percentage in GOAL
and OBJT). S1 RES covers S2 RES and 40% REL,
whereas in S3 it covers RES, OBS and 20% MOD.
S1 CON covers S2 CON, FUT, 45% REL and a small
percentage of RES. In terms of the S2 vs S3 com-
parison, S2 BKG maps to S3 BKG, HYP, MOT and a
small percentage of OBJT and MOD. S2 CON maps
to S3 CON, with a small percentage in RES, OBS
and HYP. S2 FUT maps entirely to S3 CON. S2
METH maps to S3 METH, EXP, MOD, 20% OBJT
and a small percentage of GOAL. S2 OBJ maps
to S3 GOAL and OBJT, with 15% HYP, MOD and
MOT and a small percentage in METH. S2 REL
spans across S3 CON, RES, MOT and OBJT, albeit
in very small percentages. Finally, S2 RES maps to
S3 RES and OBS, with 25% in MOD and small per-
centages in METH, CON, OBJT. Thus, it appears
that each category in S1 maps to a couple of cate-
gories in S2 and several in S3, which in turn seem
to elaborate on the S2 categories.
Based on the above analysis of the categories,
it is reasonable to assume a subsumption relation
between the categories of the type S1 > S2 >
S3, with REL cutting across several of the S3 cat-
egories and FUT branching off S3 CON. This is
an interesting and exciting outcome given that the
three different schemes have such a different ori-
gin.
102
Table 3: Association measures between schemes S1, S2, S3
S1 vs S2 S1 vs S3 S2 vs S3
X2 df P X2 df P X2 df P
Likelihood Ratio 5577.1 18 0 5363.6 30 0 6293.4 60 0
Pearson 6613.0 18 0 6371.0 30 0 8554.7 60 0
Contingency Coeff 0.842 0.837 0.871
Cramer?s V 0.901 0.885 0.725
Figure 2: Pairwise interpretation of categories of
one scheme in terms of the categories of the other.
6 Automatic identification of information
structure
6.1 Features
The first step in automatic identification of infor-
mation structure is feature extraction. We chose
a number of general purpose features suitable for
all the three schemes. With the exception of our
novel verb class feature, the features are similar to
those employed in related works, e.g. (Teufel and
Moens, 2002; Mullen et al, 2005; Hirohata et al,
2008):
History. There are typical patterns in the infor-
mation structure, e.g. RES tends to be followed
by CON rather than by BKG. Therefore, we used
the category assigned to the previous sentence as
a feature.
Location. Categories tend to appear in typical po-
sitions in a document, e.g. BKG occurs often in the
beginning and CON at the end of the abstract. We
divided each abstract into ten equal parts (1-10),
measured by the number of words, and defined the
location (of a sentence) feature by the parts where
the sentence begins and ends.
Word. Like many text classification tasks, we em-
ployed all the words in the corpus as features.
Bi-gram. We considered each bi-gram (combina-
tion of two word features) as a feature.
Verb. Verbs are central to the meaning of sen-
tences, and can vary from one category to another.
For example, experiment is frequent in METH and
conclude in CON. Previous works have used the
matrix verb of each sentence as a feature. Because
the matrix verb is not the only meaningful verb,
we used all the verbs instead.
Verb Class. Because individual verbs can result in
sparse data problems, we also experimented with a
novel feature: verb class (e.g. the class of EXPERI-
MENT verbs for verbs such as measure and inject).
We obtained 60 classes by clustering verbs appear-
ing in full cancer risk assessment articles using the
approach of Sun and Korhonen (2009).
POS. Tense tends to vary from one category to an-
other, e.g. past is common in RES and past partici-
103
ple in CON. We used the part-of-speech (POS) tag
of each verb assigned by the C&C tagger (Curran
et al, 2007) as a feature.
GR. Structural information about heads and de-
pendents has proved useful in text classification.
We used grammatical relations (GRs) returned by
the C&C parser as features. They consist of a
named relation, a head and a dependent, and pos-
sibly extra parameters depending on the relation
involved, e.g. (dobj investigate mouse). We cre-
ated features for each subject (ncsubj), direct ob-
ject (dobj), indirect object (iobj) and second object
(obj2) relation in the corpus.
Subj and Obj. As some GR features may suf-
fer from data sparsity, we collected all the subjects
and objects (appearing with any verbs) from GRs
and used them as features.
Voice. There may be a correspondence between
the active and passive voice and categories (e.g.
passive is frequent in METH). We therefore used
voice as a feature.
6.2 Methods
We used Naive Bayes (NB) and Support Vector
Machines (SVM) for classification. NB is a sim-
ple and fast method while SVM has yielded high
performance in many text classification tasks.
NB applies Bayes? rule and Maximum Like-
lihood estimation with strong independence as-
sumptions. It aims to select the class c with maxi-
mum probability given the feature set F :
argmaxc P (c|F )=argmaxc
P (c)?P (F |c)
P (F )
=argmaxc P (c)?P (F |c)
=argmaxc P (c)?
?
f?F P (f |c)
SVM constructs hyperplanes in a multidimen-
sional space that separates data points of different
classes. Good separation is achieved by the hyper-
plane that has the largest distance from the nearest
data points of any class. The hyperplane has the
form w ? x? b = 0, where w is the normal vector
to the hyperplane. We want to maximize the dis-
tance from the hyperplane to the data points, or the
distance between two parallel hyperplanes each of
which separates the data. The parallel hyperplanes
can be written as:
w?x?b = 1 andw?x?b = ?1, and the distance
between the two is 2|w| . The problem reduces to:
Minimize |w|
Subject to w ? xi ? b ? 1 for xi of one class,
and w ? xi ? b ? ?1 for xi of the other.
7 Experimental evaluation
7.1 Preprocessing
We developed a tokenizer to detect the bound-
aries of sentences and to perform basic tokenisa-
tion, such as separating punctuation from adjacent
words e.g. in tricky biomedical terms such as 2-
amino-3,8-diethylimidazo[4,5-f]quinoxaline. We
used the C&C tools (Curran et al, 2007) for POS
tagging, lemmatization and parsing. The lemma
output was used for extracting Word, Bi-gram and
Verb features. The parser produced GRs for each
sentence from which we extracted the GR, Subj,
Obj and Voice features. We only considered the
GRs relating to verbs. The ?obj? marker in a sub-
ject relation indicates a verb in passive voice (e.g.
(ncsubj observed 14 difference 5 obj)). To control
the number of features we removed the words and
GRs with fewer than 2 occurrences and bi-grams
with fewer than 5 occurrences, and lemmatized the
lexical items for all the features.
7.2 Evaluation methods
We used Weka (Witten, 2008) for the classifica-
tion, employing its NB and SVM linear kernel. The
results were measured in terms of accuracy (the
percentage of correctly classified sentences), pre-
cision, recall, and F-Measure. We used 10-fold
cross validation to avoid the possible bias intro-
duced by relying on any one particular split of the
data. The data were randomly divided into ten
parts of approximately the same size. Each indi-
vidual part was retained as test data and the re-
maining nine parts were used as training data. The
process was repeated ten times with each part used
once as the test data. The resulting ten estimates
were then combined to give a final score. We
compare our classifiers against a baseline method
based on random sampling of category labels from
training data and their assignment to sentences on
the basis of their observed distribution.
7.3 Results
Table 4 shows F-measure results when using each
individual feature alone, and Table 5 when using
all the features but the individual feature in ques-
tion. In these two tables, we only report the results
for SVM which performed considerably better than
NB. Although we have results for most scheme
categories, the results for some are missing due to
the lack of sufficient training data (see Table 2), or
due to a small feature set (e.g. History alone).
104
Table 4: F-Measure results when using each in-
dividual feature alone
a b c d e f g h i j k
S1 OBJ .39 .83 .71 .69 .52 .45 .45 .45 .54 .39 -
METH - .47 .81 .74 .63 .49 - .46 .03 .42 .51
RES - .76 .85 .86 .76 .70 .72 .69 .70 .68 .54
CON - .72 .70 .65 .63 .53 .49 .57 .68 .20 -
S2 BKG .26 .73 .69 .67 .45 .38 .56 .33 .33 .29 -
OBJ - .13 .72 .68 .54 .63 - .49 .48 .20 -
METH - .50 .81 .72 .64 .47 - .47 .03 .42 .51
RES - .76 .85 .87 .76 .72 .72 .70 .69 .68 .54
CON - .70 .73 .71 .62 .51 .40 .61 .67 .23 -
REL - - - - - - - - - - -
FUT - - - - - - - - - - -
S3 HYP - - - - .67 - - - - - -
MOT .18 .57 .70 .49 .39 .13 .36 .33 .30 .40 -
BKG - - .54 .40 .21 - - .11 .06 .06 -
GOAL - - .53 .33 .22 - .19 .31 - .25 -
OBJT - - .73 .63 .60 .10 - .26 .32 - -
EXP - .22 .63 .46 .33 .30 - .31 .07 .44 .25
MOD - - - - - - - - - - -
METH - - .82 .61 .39 .39 - .50 - .37 -
OBS - .59 .75 .71 .63 .56 .56 .54 .48 .52 .47
RES - - .87 .73 .41 .34 - .38 .24 .35 -
CON - .74 .68 .65 .65 .50 .48 .49 .55 .21 -
a-k: History, Location, Word, Bi-gram, Verb, Verb Class, POS, GR,
Subj, Obj, Voice
Looking at individual features alone, Word,
Bi-gram and Verb perform the best for all the
schemes, and History and Voice perform the worst.
In fact History performs very well on the training
data, but for the test data we can only use esti-
mates rather than the actual labels. The Voice fea-
ture works only for RES and METH for S1 and S2,
and for OBS for S3. This feature is probably only
meaningful for some of the categories.
When using all but one of the features, S1 and
S2 suffer the most from the absence of Location,
while S3 from the absence of Word/POS. Verb
Class on its own performs worse than Verb, how-
ever when combined with other features it per-
forms better: leave-Verb-out outperforms leave-
Verb Class-out.
After comparing the various combinations of
features, we found that the best selection of fea-
tures was all but the Verb for all the schemes. Ta-
ble 6 shows the results for the baseline (BL), and
the best results for NB and SVM. NB and SVM per-
form clearly better than BL for all the schemes.
The results for SVM are the best. NB yields the
highest performance with S1. Being sensitive to
sparse data, it does not perform equally well on S2
and S3 which have a higher number of categories,
some of which are low in frequency (see Table 2).
For S1, SVM finds all the four scheme categories
with the accuracy of 89%. F-measure is 90 for
OBJ, RES and CON and 81 for METH. For S2,
the classifier finds six of the seven categories, with
the accuracy of 90% and the average F-measure of
Table 5: F-Measure results using all the features and
all but one of the features
ALL A B C D E F G H I J K
S1 OBJ .90 .89 .87 .92 .90 .90 .91 .91 .91 .92 .91 .88
METH .80 .81 .80 .80 .79 .81 .79 .80 .80 .80 .81 .81
RES .88 .90 .88 .90 .88 .90 .88 .88 .88 .89 .89 .90
CON .86 .85 .82 .87 .88 .90 .90 .88 .89 .88 .88 .90
S2 BKG .91 .94 .90 .90 .93 .94 .94 .91 .93 .94 .92 .94
OBJ .72 .78 .84 .78 .83 .88 .84 .81 .83 .84 .78 .83
METH .81 .83 .80 .81 .80 .85 .80 .78 .81 .81 .82 .83
RES .88 .90 .88 .89 .88 .91 .89 .89 .90 .90 .90 .89
CON .84 .83 .77 .83 .86 .88 .86 .87 .88 .89 .88 .81
REL - - - - - - - - - - - -
FUT - 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0
S3 HYP - - - - - - - - - - - -
MOT .82 .84 .80 .76 .82 .82 .83 .78 .83 .83 .83 .83
BKG .59 .60 .60 .54 .67 .62 .62 .59 .61 .61 .62 .61
GOAL .62 .67 .67 .62 .71 .62 .67 .43 .67 .67 .67 .62
OBJT .88 .85 .83 .74 .83 .85 .83 .74 .83 .83 .83 .85
EXP .72 .68 .72 .53 .65 .70 .72 .73 .74 .74 .72 .68
MOD - - - - - - - - - - - -
METH .87 .86 .87 .66 .85 .89 .87 .88 .86 .86 .87 .86
OBS .82 .81 .84 .72 .80 .82 .81 .80 .82 .82 .81 .81
RES .87 .87 .88 .74 .87 .86 .87 .86 .87 .87 .87 .88
CON .88 .88 .82 .88 .83 .87 .87 .84 .87 .88 .87 .86
A-K: History, Location, Word, Bi-gram, Verb, Verb Class, POS, GR, Subj,
Obj, Voice
We have 1.0 for FUT in S2 probably because the size of the training data is
just right, and the model doesn?t overfit the data. We make this assumption
because we have 1.0 for almost all the categories in the training data, but only
for FUT on the test data.
Table 6: Baseline and best NB and SVM results
Acc. F-Measure
S1 OBJ METH RES CON
BL .29 .23 .23 .39 .18
NB .82 .85 .75 .85 .71
SVM .89 .90 .81 .90 .90
Acc. F-Measure
S2 BKG OBJ METH RES CON REL FUT
BL .25 .13 .08 .22 .40 .13 - -
NB .76 .79 .25 .70 .83 .66 - -
SVM .90 .94 .88 .85 .91 .88 - 1.0
Acc. F-Measure
S3 HYP MOT BKG GOAL OBJT EXP MOD METH OBS RES CON
BL .15 - .10 .06 .04 .06 .11 - .13 .24 .15 .17
NB .53 - .56 - - - .30 - .32 .61 .59 .62
SVM .81 - .82 .62 .62 .85 .70 - .89 .82 .86 .87
91 for the six categories. As with S2, METH has
the lowest performance (at 85 F-measure). The
one missing category (REL) appears in our abstract
data with very low frequency (see Table 2).
For S3, SVM uncovers as many as nine of the
11 categories with accuracy of 81%. Six cate-
gories perform well, with F-measure higher than
80. EXP, BKG and GOAL have F-measure of 70,
62 and 62, respectively. Like the missing cate-
gories HYP and MOD, GOAL is very low in fre-
quency. The lower performance of the higher fre-
quency EXP and BKG is probably due to low pre-
cision in distinguishing between EXP and METH,
and BKG and other categories, respectively.
105
8 Discussion and conclusions
The results from our corpus annotation (see Ta-
ble 2) show that for the coarse-grained S1, all the
four categories appear frequently in biomedical
abstracts (this is not surprising because S1 was ac-
tually designed for abstracts). All of them can be
identified using machine learning. For S2 and S3,
the majority of categories appear in abstracts with
high enough frequency that we can conclude that
also these two schemes are applicable to abstracts.
For S2 we identified six categories using machine
learning, and for S3 as many as nine, indicating
that automatic identification of the schemes in ab-
stracts is realistic.
Our analysis in section 5 showed that there is
a subsumption relation between the categories of
the schemes. S2 and S3 provide finer-grained in-
formation about the information structure of ab-
stracts than S1, even with their 2-3 low frequency
(or missing) categories. They can be useful for
practical tasks requiring such information. For ex-
ample, considering S3, there may be tasks where
one needs to distinguish between EXP, MOD and
METH, between HYP, MOT and GOAL, or between
OBS and RES.
Ultimately, the optimal scheme will depend on
the level of detail required by the application at
hand. Therefore, in the future, we plan to conduct
task-based evaluation of the schemes in the con-
text of CRA and to evaluate the usefulness of S1-
S3 for tasks cancer risk assessors perform on ab-
stracts (Korhonen et al, 2009). Now that we have
annotated the CRA corpus for S1-S3 and have a
machine learning approach available, we are in an
excellent position to conduct this evaluation.
A key question for real-world tasks is the level
of machine learning performance required. We
plan to investigate this in the context of our task-
based evaluation. Although we employed fairly
standard text classification methodology in our ex-
periments, we obtained high performance for S1
and S2. Due to the higher number of categories
(and less training data for each of them), the over-
all performance was not equally impressive for S3
(although still quite high at 81% accuracy).
Hirohata et al (2008) have showed that the
amount of training data can have a big impact
on our task. They used c. 50,000 Medline ab-
stracts annotated (by the authors of the Medline
abstracts) as training data for S1. When using a
small set of standard text classification features
and Conditional Random Fields (CRF) (Lafferty
et al, 2001) for classification, they obtained 95.5%
per-sentence accuracy on 1000 abstracts. How-
ever, when only 1000 abstracts were used for train-
ing the accuracy was considerably worse; their re-
ported per-abstract accuracy dropped from 68.8%
to less than 50%. Although it would be difficult to
obtain similarly huge training data for S2 and S3,
this result suggests that one key to improved per-
formance is larger training data, and this is what
we plan to explore especially for S3.
In addition we plan to improve our method. We
showed that our schemes partly overlap and that
similar features and methods tend to perform the
best / worst for each of the schemes. It is therefore
unlikely that considerable scheme specific tuning
will be necessary. However, we plan to develop
our features further and to make better use of the
sequential nature of information structure. Cur-
rently this is only represented as the History fea-
ture, which provides a narrow window view to the
category of the previous sentence. Also we plan to
compare SVM against methods such as CRF and
Maximum Entropy which have proved successful
in recent related works (Hirohata et al, 2008; Mer-
ity et al, 2009). The resulting models will be eval-
uated both directly and in the context of CRA to
provide an indication of their practical usefulness
for real-world tasks.
Acknowledgments
The work reported in this paper was funded by the
Royal Society (UK), the Swedish Research Coun-
cil, FAS (Sweden), and JISC (UK) which is fund-
ing the SAPIENT Automation project. YG was
funded by the Cambridge International Scholar-
ship.
106
References
J. Cohen. 1960. A coefficient of agreement for nom-
inal scales. Educational and Psychological Mea-
surement, 20:37?46.
J. R. Curran, S. Clark, and J. Bos. 2007. Linguistically
motivated large-scale nlp with c&c and boxer. In
Proceedings of the ACL 2007 Demonstrations Ses-
sion, pages 33?36.
K. Hirohata, N. Okazaki, S. Ananiadou, and
M. Ishizuka. 2008. Identifying sections in scien-
tific abstracts using conditional random fields. In
Proc. of 3rd International Joint Conference on Nat-
ural Language Processing.
A. Korhonen, L. Sun, I. Silins, and U. Stenius. 2009.
The first step in the development of text mining tech-
nology for cancer risk assessment: Identifying and
organizing scientific evidence in risk assessment lit-
erature. BMC Bioinformatics, 10:303.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditionl random fields: Probabilistic models for seg-
menting and labeling sequence data. In Proc. of
ICML.
J. R. Landis and G. G. Koch. 1977. The measurement
of observer agreement for categorical data. Biomet-
rics, 33:159?174.
M. Liakata and L.N. Soldatova. 2008. Guide-
lines for the annotation of general scientific con-
cepts. Aberystwyth University, JISC Project Report
http://ie-repository.jisc.ac.uk/88/.
M. Liakata, Claire Q, and L.N. Soldatova. 2009. Se-
mantic annotation of papers: Interface & enrichment
tool (sapient). In Proceedings of BioNLP-09, pages
193?200, Boulder, Colorado.
M. Liakata, S. Teufel, A. Siddharthan, and C. Batch-
elor. 2010. Corpora for the conceptualisation and
zoning of scientific papers. To appear in the 7th In-
ternational Conference on Language Resources and
Evaluation.
J. Lin, D. Karakos, D. Demner-Fushman, and S. Khu-
danpur. 2006. Generative content models for struc-
tural analysis of medical abstracts. In Proceedings
of BioNLP-06, pages 65?72, New York, USA.
J. Lin. 2009. Is searching full text more effective than
searching abstracts? BMC Bioinformatics, 10:46.
S. Merity, T. Murphy, and J. R. Curran. 2009. Ac-
curate argumentative zoning with maximum entropy
models. In Proceedings of the 2009 Workshop
on Text and Citation Analysis for Scholarly Digital
Libraries, pages 19?26. Association for Computa-
tional Linguistics.
Y. Mizuta, A. Korhonen, T. Mullen, and N. Collier.
2005. Zone analysis in biology articles as a basis
for information extraction. International Journal of
Medical Informatics on Natural Language Process-
ing in Biomedicine and Its Applications.
T. Mullen, Y. Mizuta, and N. Collier. 2005. A baseline
feature set for learning rhetorical zones using full ar-
ticles in the biomedical domain. Natural language
processing and text mining, 7:52?58.
P. Ruch, C. Boyer, C. Chichester, I. Tbahriti, A. Geiss-
buhler, P. Fabry, J. Gobeill, V. Pillet, D. Rebholz-
Schuhmann, C. Lovis, and A. L. Veuthey. 2007.
Using argumentation to extract key sentences from
biomedical abstracts. Int J Med Inform, 76:195?
200.
H. Shatkay, F. Pan, A. Rzhetsky, and W. J. Wilbur.
2008. Multi-dimensional classification of biomed-
ical text: Toward automated, practical provision of
high-utility text to diverse users. Bioinformatics,
18:2086?2093.
S. Siegel and N. J. Jr. Castellan. 1988. Nonparamet-
ric Statistics for the Behavioral Sciences. McGraw-
Hill, Berkeley, CA, 2nd edition.
L. Sun and A. Korhonen. 2009. Improving verb clus-
tering with automatically acquired selectional pref-
erence. In Proceedings of the Conference on Empir-
ical Methods in Natural Language Processing.
I. Tbahriti, C. Chichester, Frederique Lisacek, and
P. Ruch. 2006. Using argumentation to retrieve
articles with similar citations. Int J Med Inform,
75:488?495.
S. Teufel and M. Moens. 2002. Summarizing scientific
articles: Experiments with relevance and rhetorical
status. Computational Linguistics, 28:409?445.
S. Teufel, A. Siddharthan, and C. Batchelor. 2009. To-
wards domain-independent argumentative zoning:
Evidence from chemistry and computational linguis-
tics. In Proc. of EMNLP.
I. H. Witten, 2008. Data mining: practical machine
learning tools and techniques with Java Implemen-
tations. http://www.cs.waikato.ac.nz/ml/weka/.
107
Proceedings of the 8th Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities (LaTeCH) @ EACL 2014, pages 71?79,
Gothenburg, Sweden, April 26 2014.
c?2014 Association for Computational Linguistics
Social and Semantic Diversity:
Socio-semantic Representation of a Scientific Corpus
Elisa Omodei
LATTICE and ISC-PIF
CNRS & ENS & U. Sorbonne Nouvelle
1 rue Mauriece Arnoux
92120 Montrouge France
elisa.omodei@ens.fr
Yufan Guo
University of Washington
Computer Science
Engineering
Box 352350 Seattle, WA 98195-2350
yufanguo@cs.washington.edu
Jean-Philippe Cointet
INRA Sens and ISC-PIF
Cit?e Descartes, 5 boulevard Descartes
77454 Marne-la-Vall?ee Cedex France
75013 Paris France
jphcoi@yahoo.fr
Thierry Poibeau
LATTICE
CNRS & ENS & U. Sorbonne Nouvelle
1 rue Mauriece Arnoux
92120 Montrouge France
thierry.poibeau@ens.fr
Abstract
We propose a new method to extract key-
words from texts and categorize these
keywords according to their informational
value, derived from the analysis of the ar-
gumentative goal of the sentences they ap-
pear in. The method is applied to the ACL
Anthology corpus, containing papers on
the computational linguistic domain pub-
lished between 1980 and 2008. We show
that our approach allows to highlight inter-
esting facts concerning the evolution of the
topics and methods used in computational
linguistics.
1 Introduction
Big data makes it possible to observe in vivo the
dynamics of a large number of different domains.
It is particularly the case in the scientific field,
where researchers produce a prolific literature but
also other kinds of data like numbers, figures, im-
ages and so on. For a number of domains, large
scientific archives are now available over several
decades.
This is for example the case for computational
linguistics. The ACL Anthology contains more
than 24,500 papers, for the most part in PDF for-
mat. The oldest ones date back to 1965 (first edi-
tion of the COLING conference) but it is mostly
after 1980 that data are available in large volumes
so that they can be exploited in evolution studies.
The volume of data increases over time, which
means there is a wide diversity in the number of
papers available depending on the given period of
time. There are similar archives for different do-
mains like, e.g. physics (the APS database pro-
vided by the American Physical Society) or the
bio-medical domain (with Medline).
These scientific archives have already given
birth to a large number of different pieces of work.
Collaboration networks have for example been au-
tomatically extracted so as to study the topology
of the domain (Girvan and Newman, 2002) or
its morphogenesis (Guimera et al., 2005). Ref-
erencing has also been the subject of numerous
studies on inter-citation (Garfield, 1972) and co-
citation (Small, 1973). Other variables can be
taken into account like the nationality of the au-
thors, the projects they are involved in or the re-
search institutions they belong to, but it is the anal-
ysis of the textual content (mostly titles, abstracts
and keywords provided with the papers) that have
attracted the most part of the research in the area
since the seminal work of Callon (Callon et al.,
1986; Callon et al., 1991).
In this paper, our goal is to investigate the evo-
lution of the field of computational linguistics,
which means that text will play a crucial role. Tex-
tual analysis is then mixed with the study of indi-
vidual trajectories in the semantic space: our goal
is to propose possible avenues for the study of the
dynamics of innovation in the computational lin-
71
guistics domain.
The ACL Anthology has been the subject of
several studies in 2012, for the 50 years of the
ACL. More specifically, a workshop called ?Re-
discovering 50 Years of Discoveries? was orga-
nized to examine 50 years of research in NLP
(but, for the reasons given above, the workshop
mostly focused on the evolution of the domain
since 1980). This workshop was also an oppor-
tunity to study a large scientific collection with re-
cent NLP techniques and see how these techniques
can be applied to study the dynamics of a scientific
domain.
The analysis of this kind of data is generally
based on the extraction of key information (au-
thors, keywords) and the discovery of their rela-
tionships. The data can be represented as a graph,
therefore graph algorithmics can be used to study
the topology and the evolution of the graph of col-
laborations or the graph of linked authors. It is
thus possible to observe the evolution of the do-
main, check some hypotheses or common assump-
tions about this evolution and provide a strong em-
pirical basis to epistemology studies.
The paper ?Towards a computational History of
the ACL: 1980-2008? is very relevant from this
point of view (Anderson et al., 2012). The au-
thors try to determine the evolution of the main
sub-domains of research within NLP since 1980
and they obtain very interesting results. For ex-
ample, they show the influence of the American
evaluation campaigns on the domain: when a US
agency sponsored a sub-domain of NLP, one can
observe a quick concentration effect since a wide
number of research groups suddenly concentrated
their efforts on the topic; when no evaluation cam-
paign was organized, research was much more
widespread across the different sub-domains of
NLP. Even if this is partially predictable, it was
not obvious to be able to show this in a collection
of papers as large as the ACL Anthology.
Our study has been profoundly influenced by
the study by Anderson et al. However, our goal
here is to characterize automatically the keywords
based on the information they carry. We will thus
combine keyword extraction with text zoning so
as to categorize the keywords depending on their
context of use.
The rest of the paper is organized as follows.
We first present an analysis of the structure of ab-
stracts so as to better characterize their content by
mixing keyword extraction with text zoning. We
show how these techniques can be applied to the
ACL Anthology in order to examine specific facts,
more specifically concerning the evolution of the
techniques used in the computational linguistics
domain.
2 A Text Zoning Analysis of the ACL
Anthology
The study of the evolution of topics in large cor-
pora is usually done through keyword extraction.
This is also our goal, but we would like to be able
to better characterize these keywords and make a
difference, for example, between keywords refer-
ring to concepts and keywords referring to meth-
ods. Hence, the context of these keywords seems
highly important. Consequently, we propose to
use Text Zoning that can provide an accurate char-
acterization of the argumentative goal of each sen-
tence in a scientific abstract.
2.1 Previous work
The first important contributions in text zoning are
probably the experiments by S. Teufel who pro-
posed to categorize sentences in scientific papers
(and more specifically, in the NLP domain) ac-
cording to different categories (Teufel, 1999) like
BKG: General scientific background, AIM: State-
ments of the particular aim of the current paper or
CTR: Contrastive or comparative statements about
other work. This task is called Rhetorical zoning
or Argumentative zoning since the goal is to iden-
tify the rhetoric or argumentative role of each sen-
tence of the text.
The initial work of Teufel was based on the
manual annotation of 80 papers representing the
different areas of NLP (the corpus was made of
papers published within the ACL conferences or
Computational Linguistics). A classifier was then
trained on this manually annotated corpus. The
author reported interesting results despite ?a 20%
diference between [the] system and human perfor-
mance? (Teufel and Moens, 2002). The learning
method used a Naive Bayesian model since more
sophisticated methods tested by the author did not
obtain better results. Teufel in subsequent publica-
tions showed that the technique can be used to pro-
duce high quality summaries (Teufel and Moens,
2002) or precisely characterize the different cita-
tions in a paper (Ritchie et al., 2008).
The seminal work of Teufel has since then given
72
rise to different kinds of works, on the one hand
to refine the annotation method, and on the other
hand to check its applicability to different scien-
tific domains. Concerning the first point, research
has focused on the identification of relevant fea-
tures for classification, on the evaluation of dif-
ferent learning algorithms for the task and more
importantly on the reduction of the volume of text
to be annotated. Concerning the second point, it
is mostly the biological and bio-medical domains
that have attracted attention, since scientists in
these domains often have to access the literature
?vertically? (i.e. experts may need to have access
to all the methods and protocols that have been
used in a specific domain) (Mizuta et al., 2006;
Tbahriti et al., 2006).
Guo has since developed a similar trend of re-
search to extend the initial work of Teufel (Guo
et al., 2011; Guo et al., 2013): she has tested a
large list of features to analyze the zones, evalu-
ated different learning algorithms for the task and
proposed new methods to decrease the number of
texts to be annotated. The features used for learn-
ing are of three categories: i) positional (location
of the sentence inside the paper), ii) lexical (words,
classes of words, bigrams, etc. are taken into con-
sideration) and iii) syntactic (the different syntac-
tic relations as well as the class of words appear-
ing in subject or object positions are taken into ac-
count). The analysis is thus based on more fea-
tures than in Teufel?s initial work and requires a
parser.
2.2 Application to the ACL Anthology corpus
In our experiment, we only used the abstracts of
the papers. Our hypothesis is that abstracts con-
tain enough information and are redundant enough
to study the evolution of the domain. Taking into
consideration the full text would probably give too
many details and thus introduce noise in the anal-
ysis.
The annotation scheme includes five different
categories, which are the following: OBJEC-
TIVE (objectives of the paper), METHOD (meth-
ods used in the paper), RESULTS (main results),
CONCLUSION (conclusion of the paper), BACK-
GROUND (general context), as in (Reichart and
Korhonen, 2012). These categories are also close
to those of (Mizuta et al., 2006; Guo et al., 2011;
Guo et al., 2013) and have been adapted to ab-
stracts (as opposed to full text
1
). It seems relevant
to take into consideration an annotation scheme
that has already been used by various authors so
that the results are easy to compare to others.
Around one hundred abstracts from the ACL
Anthology have then been manually annotated us-
ing this scheme (?500 sentences; ACL abstracts
are generally quite short since most of them are
related to conference papers). The selection of the
abstracts has been done using stratified sampling
over time and journals, so as to obtain a represen-
tative corpus (papers must be related to different
periods of time and different sub-areas of the do-
main). The annotation has been done according
to the annotation guideline defined by Y. Guo, es-
pecially for long sentences when more than one
category could be applied (preferences are defined
to solve complex cases
2
).
The algorithm defined by (Guo et al., 2011) is
then adapted to our corpus. The analysis is based
on positional, lexical and syntactic features, as ex-
plained above. No domain specific information
was added, which makes the whole process easy
to reproduce. As for parsing, we used the C&C
parser (James Curran and Stephen Clark and Johan
Bos, 2007). All the implementation details can be
found in (Guo et al., 2011), especially concerning
annotation and the learning algorithm. As a result,
each sentence is associated with a tag correspond-
ing to one of the zones defined in the annotation
scheme.
2.3 Results and Discussion
In order to evaluate the text zoning task, a num-
ber of abstracts were chosen randomly (?300 sen-
tences that do not overlap with the training set).
CONCLUSION represented less than 3% of the
sentences and was then dropped for the rest of
the analysis. The four remaining zones are un-
equaly represented: 18.05 % of the sentences re-
fer to BACKGROUND, 14.35% to OBJECTIVE,
14.81 % to RESULT and 52.77 % to METHOD.
Just by looking at these numbers, one can see how
1
The categories used in (Teufel, 1999) were not relevant
since this model focused on full text papers, with a special
emphasis on the novelty of the author?s work and the attitude
towards other people?s work, which is not the case here.
2
The task is to assign the sentence only a single category.
The choice of the category should be made according to the
following priority list: Conclusion > Objective > Result >
Method> Background. The only exception is that when 75%
or more of the sentence belongs to a less preferred category,
then that category will be assigned to the sentence.
73
Table 1: Result of the text zoning analysis (preci-
sion)
Category Precision
Objective 83,87 %
Background 81,25 %
Method 71,05 %
Results 82,05 %
Figure 1: An abstract annotated with text zoning
information. Categories are indicated in bold face.
Most of errors in Korean morphological analysis and
POS ( Part-of-Speech ) tagging are caused by unknown
morphemes . BACKGROUND
This paper presents a generalized unknown morpheme
handling method with POSTAG(POStech TAGger )
which is a statistical/rule based hybrid POS tagging
system . OBJECTIVE
The generalized unknown morpheme guessing is based
on a combination of a morpheme pattern dictionary
which encodes general lexical patterns of Korean
morphemes with a posteriori syllable tri-gram estimation
. METHOD
The syllable tri-grams help to calculate lexical proba-
bilities of the unknown morphemes and are utilized to
search the best tagging result . METHOD
In our scheme , we can guess the POS?s of unknown
morphemes regardless of their numbers and positions
in an eojeol , which was not possible before in Korean
tagging systems . RESULTS
In a series of experiments using three different domain
corpora , we can achieve 97% tagging accuracy regard-
less of many unknown morphemes in test corpora .
RESULTS
methodological issues are important for the do-
main.
We then calculate for each of the categories, the
percentage of sentences that received the right la-
bel, which allows us to calculate precision. The
results are given in table 1.
These results are similar to the state of the art
(Guo et al., 2011), which is positive taking into
consideration the small number of sentences an-
notated for training. The diversity of the features
used makes it easy to transfer the technique from
one domain to the other without any heavy anno-
tation phase. Results are slightly worse for the
METHOD category, probably because this cate-
gory is more diverse and thus more difficult to rec-
ognize. The fact that NLP terms can refer either to
objectives or to methods also contributes render-
ing the recognition of this category more difficult.
Figure 1 shows an abstract annotated by the text
zoning module (the paper is (Lee et al., 2002): it
has been chosen randomly between those contain-
ing the different types of zones). One category
is associated with each sentence but this is some-
times problematic: for example the fact that a hy-
brid method is used is mentioned in a sentence that
is globally tagged as OBJECTIVE by the system.
However, sentences tagged as METHOD contain
relevant keywords like lexical pattern or tri-gram
estimation, which makes it possible to infer that
the approach is hybrid. One can also spot some
problems with digitization, which are typical of
this corpus: the ACL Anthology contains automat-
ically converted files to PDF, which means texts
are not perfect and may contain some digitization
errors.
3 Contribution to the Study of the
Evolution ACL Anthology
As said above, we are largely inspired by (Ander-
son et al., 2012). We think the ACL Anthology
is typical since it contains papers spanning over
more than 30 years: it is thus interesting to use it
as a way to study the main evolutions of the com-
putational linguistics domain. The method can of
course also be applied to other scientific corpora.
3.1 Keyword extraction and characterization
The first step consists in identifying the main key-
words of the domain. We then want to more pre-
cisely categorize these keywords so as to identify
the ones specifically referring to methods for ex-
ample. From this perspective, keywords appear-
ing in the METHOD sections are thus particularly
interesting for us. However, one major problem is
that there is no clear-cut difference between goals
and methods in NLP since most systems are made
of different layers and require various NLP tech-
niques. For example, a semantic analyzer may use
a part-of-speech tagger and a parser, which means
NLP tools can appear as part of the method.
Keyword extraction aims at automatically ex-
tracting relevant keywords from a collection of
texts. A popular approach consists in first extract-
ing typical sequences of tags that are then filtered
according to specific criteria (these criteria can in-
clude the use of external resources but they are
more generally based on scores mixing frequency
and specificity (Bourigault and Jacquemin, 1999;
Frantzi and Ananiadou, 2000)). In this study, we
voluntarily used a minimal approach for keyword
extraction and filtering since we want to keep most
74
Table 2: Most specific keywords found in the METHOD sections.
Methods
Category Method N-grams
Machine learning
Bayesian methods baesyan
Vector Space model space model, vector space, cosine
Genetic algorithms genetic algorithms
HMM hidden markov models, markov model
CRF conditional random fields
SVM support vector machines
MaxEnt maximum entropy model, maximum entropy approach, maximum entropy
Clustering clustering algorithm, clustering method, word clusters, classification problem
Speech & Mach. Trans.
Language models large-vocabulary, n-gram language model, Viterbi
Parallel Corpora parallel corpus, bilingual corpus, phrase pairs, source and target languages, sentence pairs, word pairs,
source sentence
Alignment phrase alignment, alignment algorithm, alignment models, ibm model, phrase translation, translation
candidates, sentence alignment
NLP Methods
POS tagging part-of-speech tagger, part-of-speech tags
Morphology two-level morphology, morphological analyzer, morphological rules
FST finite-state transducers, regular expressions, state automata, rule-based approach
Syntax syntactic categories, syntactic patterns, extraction patterns
Dependency parsing dependency parser, dependency graphs, prague dependency, dependency treebank, derivation trees, parse
trees
Parsing grammar rules, parser output, parsing process, parsed sentences, transfer rules
Semantics logical forms, inference rules, generative lexicon, lexical rules, lexico-syntactic, predicate argument
Applications
IE and IR entity recognition, answer candidates, temporal information, web search, query expansion, google, user
queries, keywords, query terms, term recognition
Discourse generation component, dialogue acts, centering theory, lexical chains, resolution algorithm, generation
process, discourse model, lexical choice
Segmentation machine transliteration, phonological rules, segmentation algorithm, word boundaries
Words and Resource
Lexical knowledge bases lexical knowledge base, semantic network, machine readable dictionaries, eurowordnet, lexical entries,
dictionary entries, lexical units, representation structures, lookup
Word similarity word associations, mutual information, semantic relationships, word similarity, semantic similarity,
semeval-2007, word co-occurrence, synonymy
Corpora brown corpus, dialogue corpus, annotation scheme, tagged corpus
Evaluation Evaluation score, gold standard, evaluation measures, estimation method
Calculation & complexity Software tool development, polynomial time, software tools, series of experiments, system architecture, runtime,
programming language
Constraints relaxation, constraint satisfaction, semantic constraints
of the information for the subsequent text zoning
phase. We thus used NLTK for part-of-speech tag-
ging and from this result extracted the most com-
mon noun phrases. We used a pre-defined set
of grammatical patterns to extract noun phrases
defined as sequences of simple sequences (e.g.
adjectives + nouns, ?phrase pairs?, ?dependency
graph?, etc.) possibly connected to other such pat-
terns through propositions to form longer phrases
(e.g. ?series of experiments?). Only the noun
phrases appearing in more than 10 papers are kept
for subsequent processing.
Candidate keywords are then ranked per zone,
according to their specificity (the zone they are
the most specific of) . Specificity corresponds to
the Kolmogorov-Smirnov test that quantifies a dis-
tance between the empirical distribution functions
of two samples. The test is calculated as follows:
D = max
x
|S
N
1
(x)? S
N
2
(x)| (1)
where S
N
1
(x) et S
N
2
(x) are the empirical distri-
bution function of the two samples (that corre-
spond in our case to the number of occurrences
of the keyword in a given zone, and to the total
number of occurrences of all the keywords in the
same zone, respectively) (Press et al., 2007). A
high value of D for a given keyword means that it
is highly specific of the considered zone. At the
opposite, a low value means that the keyword is
spread over the different zones and not really spe-
cific of any zone.
The first keywords of each category are then
categorized by an expert of the domain. For the
METHOD category, we obtain Table 2. Logically,
given our approach, the table does not contain all
the keywords relevant for the computational lin-
guistics domain, but it contains the mots specific
ones according to the above approach. One should
thus not be surprised not to see all the keywords
used in the domain.
3.2 Evolution of methods over time
The automatic analysis of the corpus allows us to
track the main evolutions of the field over time.
During the last 30 years, the methods used have
changed to a large extent, the most notable fact be-
ing probably the generalization of machine learn-
ing methods since the late 1990s. This is outlined
by the fact that papers in the domain nowadays
nearly always include a section that describes an
experiment and some results.
To confirm this hypothesis, we observe the rel-
ative frequency of sentences tagged as RESULTS
in the papers over time. In the figure 3, we see that
the curve increases almost linearly from the early
1980s until the late 2000s.
75
1980 1982 1984 1986 1988 1990 1992 1994 1996 1998 2000 2002 2004 2006 20080
0.10.2
0.30.4
0.50.6
0.70.8
0.91
NLP Methods
SemanticsParsingDependency parsingSyntaxFSTMorphologyPOS tagging
Year
Relative
 Freque
ncy
1980 1982 1984 1986 1988 1990 1992 1994 1996 1998 2000 2002 2004 2006 20080
0.10.2
0.30.4
0.50.6
0.70.8
0.91
Applications
SegmentationDiscourseIE and IR
Year
Relative
 Freque
ncy
1980 1982 1984 1986 1988 1990 1992 1994 1996 1998 2000 2002 2004 2006 20080
0.10.2
0.30.4
0.50.6
0.70.8
0.91
Machine Learning
ClusteringMaxEntSVMCRFHMMGenetic algorithmsVector Space modelBayesian methods
Year
Relative
 Freque
ncy
1980 1982 1984 1986 1988 1990 1992 1994 1996 1998 2000 2002 2004 2006 20080
0.10.2
0.30.4
0.50.6
0.70.8
0.91
Speech & machine translation specific
AlignmentParallel CorporaLanguage models
Year
Relative
 Freque
ncy
1980 1982 1984 1986 1988 1990 1992 1994 1996 1998 2000 2002 2004 2006 20080
0.10.2
0.30.4
0.50.6
0.70.8
0.91
Resources
CorporaWord similarityLexical knowledge bases
Year
Relative
 Freque
ncy
1980 1982 1984 1986 1988 1990 1992 1994 1996 1998 2000 2002 2004 2006 20080
0.10.2
0.30.4
0.50.6
0.70.8
0.91
Calculation & Complexity
ConstraintsSoftware
Year
Relative
 Freque
ncy
Figure 2: Evolution of the relative frequency of the different groups of methods over time.
It is also possible to make more fine-grained ob-
servations, for example to follow over time the dif-
ferent kinds of methods under consideration. The
results are shown in figure 2. Rule based methods
and manually crafted resources are used all over
the period, while machine learning based meth-
ods are more and more successful after the late
1990s. This is not surprising since we know that
machine learning is now highly popular within the
field. However, symbolic methods are still used,
sometimes in conjunction with learning methods.
The two kinds of methods are thus more comple-
mentary than antagonistic.
One could observe details that should be
checked through a more thorough study. We ob-
serve for example the success of dependency pars-
ing in the end of the 1980s (probably due to the
success of the Tree Adjoining Grammars at the
time) and the new popularity of this area of re-
search in the early 2000s (dependency parsing has
been the subject of several evaluation campaigns
in the 2000s, see for example for the CONLL
shared tasks from 2006 to 2009).
Different machine learning methods have been
popular over time but each of them continues to be
used after a first wave corresponding to their ini-
tial success. Hidden Markov Models and n-grams
are highly popular in the 1990s, probably thanks
to the experiments made by Jelinek and his col-
leagues, which will open the field of statistical ma-
chine translation (Brown et al., 1990). SVM and
CRF have had a more recent success as everybody
knows.
We are also interested in the distribution of
these methods between papers and authors. Fig-
ure 4 shows the average number of keywords
1980 1982 1984 1986 1988 1990 1992 1994 1996 1998 2000 2002 2004 2006 20080
0.05
0.1
0.15
0.2
0.25
Results
Year
Relative
 Freque
ncy
Figure 3: Evolution of the relative frequency of
sentences tagged as RESULTS in the abstracts of
the papers
appearing in the METHOD section of the papers
over time. We see that this number regularly in-
creases, especially during the 1980s, showing pos-
sibly a gradually increasing complexity of the sys-
tems under consideration.
Lastly, figure 5 shows the number of authors
who are specialists of one or several methods.
Most of the authors just mention one method in
their papers and, logically, the curves decrease,
which means that there are few authors who are
really specialists of many methods. This result
should be confirmed by a larger scale study tak-
ing into account a larger number of keywords but
the trend seems however interesting.
3.3 The dynamics of the authors in the
method space
One could say that the results we have reported in
the previous section are not new but rather confirm
some already well known facts. Our method al-
lows to go one step further and try to answer more
76
Figure 4: Evolution of the number of keywords
related to methods over time.
1 9 8 0 2 4 6 .5
5R1
5R9
5R8
5R0
5R2
5R4
esultYariltrvs Fqalncr2rvyvnYaesultYariltrvs Fqalncr4rvyvnYaesultYariltrvs Fqalncr6rvyvnYaesultYariltrvs Fqalncr.rvyvnYaesultYariltrvs Fqalncr?rvyvnYaesultYariltrvs Fqalncr15rvyvnYaesultYariltrvs Fqalncr11rvyvnYaesultYariltrvs Fqalncr19rvyvnYaesultYariltrvs Fqalncr18rvyvnYa
?s? nYrt?r?nultca
?YtvtYu
qt?rt?ry
sultYa
Figure 5: Proportion of authors specialized in
a given number of methods (i.e. mentioning
frequently the name of the method in the ab-
stracts), for different categories of researchers.
challenging questions. How are new methods in-
troduced in the field? Are they mainly brought
by young researchers or is it mainly confirmed re-
searchers who develop new techniques (or import
them from related fields)? Are NLP experts spe-
cialized in one field or in a wide variety of differ-
ent fields?
These questions are of course quite complex.
Each individual has his own expertise and his
own history but we think that automatic meth-
ods can provide some interesting trends over time.
For example, (Anderson et al., 2012) show that
evaluation campaigns have played a central role
at certain periods of time, which does not mean
of course that there was no independent research
outside these campaigns at the time. Our goal
is thus to exhibit some tendencies that could be
interpreted or even make it possible to compare
the evolution of the computational linguistics field
with other fields. Out tools provide some hypothe-
ses that must of course be confirmed by further ob-
servations and analysis. We do not claim that they
provide an exact and accurate view of the domain.
Gene
tic alg
orithm
s HMM
Morp
holog
y
Corpo
ra SVM
Clust
ering
POS 
taggin
g
Baye
sian m
ethod
s
MaxE
nt CRF
Align
ment
Langu
age m
odels
Vecto
r Spa
ce m
odel
00.1
0.20.3
0.40.5
0.60.7
0.80.9
1
Fraction of pionners that are new to the field
Fraction of authors that enter the field in those years
Figure 6: For each ?new method?, number of ?pi-
oneers? not having published any paper before
(compared to the total number of new authors dur-
ing the same period of time).
For this study we only take into account authors
who have published at least 5 papers in the ACL
Anthology, in order to take into consideration au-
thors who have contributed to the domain during a
period of time relevant for the study. We consider
as ?pioneers? the authors of the first 25% of pa-
pers in which a keyword referring to a method is
introduced (for example, the first papers where the
keywords support vector machine or SVM appear).
We then calculate, among this set of authors, the
ones who can be considered as new authors, which
means people who have not published before in
the field. Since there are every year a large number
of new authors (who use standard techniques) we
compare the ratio of new authors using new tech-
niques with the number of authors using already
known techniques over the considered period. Re-
sults are visible in figure 6.
Results are variable depending on the method
under consideration but some of them seem inter-
esting. Papers with the keyword Hidden Markov
Model in the 1990s seem to be largely written
by new comers, probably by researchers having
tested this method in related fields before (and
we know that it was the case of Jelinek?s team
who was largely involved in speech processing, a
domain not so well represented in the ACL An-
thology before the 1990s. Of course, Jelinek and
colleague were confirmed and even highly estab-
lished researchers already at the beginning of the
1990s). We observe a similar patten for genetic
algorithms but the number of authors is too lim-
ited to say if the trend is really meaningful. SVM
also seem to have been popularized by new com-
ers but it is not the case of language models or of
the vector space model. A more thorough study is
of course needed to confirm and better understand
77
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  0.2  0.4  0.6  0.8  1
C
u
m
u
l
a
t
i
v
e
 
D
i
s
t
r
i
b
u
t
i
o
n
 
F
u
n
c
t
i
o
n
Fraction of total production of author already published
Figure 7: Distribution function of the number of
papers already published by ?pioneers? when they
have published their paper on the new method,
compared to the total production of their career.
these results.
We then do a similar experiment to try to de-
termine when, during their career, researchers use
new methods. Practically, we examine at what
point of their career the authors who are character-
ized as ?pioneers? in our study (what refers to the
first authors using a new method) have published
the papers containing new methods (for example,
if an author is one of the first who employed the
keyword SVM, has he done this at the beginning
of his career or later on?). The result is visible in
figure 7 and shows that 60% of pioneers had pub-
lished less than a third of their scientific produc-
tion when they use the new method. We thus ob-
serve a similar set of authors between the pioneers
and researchers having published so far in related
but nevertheless different communities. To con-
firm this result, it would be useful to study other
domains and other corpora (in computer science,
linguistics, cognitive sciences) so as to get a better
picture of the domain, but the task is then highly
challenging.
One may want then to observe the diversity of
methods employed in the domain, especially by
the set of people called ?pioneers? in our study.
Figure 8 shows in blue the number of methods
detected for the pioneers and in red the number of
methods used by all the authors.
We see that pioneers, when taking into consid-
eration the whole set of papers in the ACL An-
thology, are using a larger number of methods.
They are over represented among authors using 3
methods and more. This group of people also con-
tribute to a larger number of sub-areas in the do-
mains compared to the set of other authors.
1 2 3 4 5 6 7 80
0.05
0.1
0.15
0.2
0.25
0.3
0.35
Pioneers proportionTotal authors proportion
Number of methods per author
Propo
rtion 
of aut
hors
Figure 8: Proportion of ?pioneers? experts in a
given number of methods compared to all the other
authors in the corpus.
4 Conclusion
We have presented in this paper an analysis of the
ACL Anthology corpus. Our analysis is based on
the identification of keywords which are catego-
rized according to their informational status. Cate-
gorization is done according to a Text Zoning anal-
ysis of the papers? abstracts, which provides very
relevant information for the study. We have shown
that coupling keyword extraction with Text Zon-
ing makes it possible to observe fine grained facts
in the dynamics of a scientific domain.
These tools only give pieces of information that
should be confirmed by subsequent studies. It
is necessary to go back to the texts themselves,
consult domain experts and probably the larger
context to be able to get a really accurate pic-
ture of the evolution of a scientific domain. This
multi-disciplinary research means that to collabo-
rate with people from other fields is needed, espe-
cially with the history of science and epistemol-
ogy. However, the platforms and the techniques
we have described in this paper are now available
and can be re-used for other kinds of studies, mak-
ing it possible to reproduce similar experiments
across different domains.
References
Ashton Anderson, Dan Jurafsky, and Daniel A. McFar-
land. 2012. Towards a computational history of the
acl: 1980-2008. In Proceedings of the ACL-2012
Special Workshop on Rediscovering 50 Years of Dis-
coveries, pages 13?21, Jeju Island, Core. Associa-
tion for Computational Linguistics.
Didier Bourigault and Christian Jacquemin. 1999.
Term extraction + term clustering: An integrated
platform for computer-aided terminology. In Pro-
ceedings of the Ninth Conference on European
78
Chapter of the Association for Computational Lin-
guistics, EACL ?99, pages 15?22.
Peter F. Brown, John Cocke, Stephen A. Della Pietra,
Vincent J. Della Pietra, Fredrick Jelinek, John D.
Lafferty, Robert L. Mercer, and Paul S. Roossin.
1990. A statistical approach to machine translation.
Computational Linguistics, 16(2):79?85.
Michel Callon, John Law, and Arie Rip. 1986.
Mapping the dynamics of science and technology.
McMillan, London.
Michel Callon, Jean-Pierre Courtial, and Franc?oise
Laville. 1991. Co-word analysis as a tool for de-
scribing the network of interaction between basic
and technological research: The case of polymer
chemistry. Scientometrics, 22(1):155?205.
Katarina Frantzi and Sophia Ananiadou. 2000. Au-
tomatic recognition of multi-word terms:. the C-
value/NC-value method. International Journal on
Digital Libraries, 3(2):115?130.
Eugene Garfield. 1972. Citation Analysis as a Tool in
Journal Evaluation. Science, 178(4060):471?479.
Michelle Girvan and Mark E J Newman. 2002. Com-
munity structure in social and biological networks.
Proceedings of the National Academy of Sciences of
the United States of America, 99:7821?7826.
Roger Guimera, Brian Uzzi, Jarrett Spiro, and Luis
A. Nunes Amaral. 2005. Team Assembly Mech-
anisms Determine Collaboration Network Structure
and Team Performance. Science, 308(5722):697?
702.
Yufan Guo, Anna Korhonen, and Thierry Poibeau.
2011. A weakly-supervised approach to argumenta-
tive zoning of scientific documents. In Proceedings
of the 2011 Conference on Empirical Methods in
Natural Language Processing, pages 273?283, Ed-
inburgh.
Yufan Guo, Roi Reichart, and Anna Korhonen. 2013.
Improved information structure analysis of scien-
tific documents through discourse and lexical con-
straints. In Proceedings of Human Language Tech-
nologies: Conference of the North American Chap-
ter of the Association of Computational Linguistics
(HLT-NAACL), pages 928?937.
James Curran and Stephen Clark and Johan Bos.
2007. Linguistically Motivated Large-Scale NLP
with C&C and Boxer. In Proceedings of the 45th
Meeting of the Association for Computation Linguis-
tics (ACL), pages 33?36.
Gary Geunbae Lee, Jong-Hyeok Lee, and Jeong-
won Cha. 2002. Syllable-pattern-based unknown-
morpheme segmentation and estimation for hybrid
part-of-speech tagging of korean. Computational
Linguistics, 28(1):53?70.
Yoko Mizuta, Anna Korhonen, Tony Mullen, and Nigel
Collier. 2006. Zone analysis in biology articles as a
basis for information extraction. International Jour-
nal of Medical Informatics, 75(6):468?487.
William H. Press, Saul A. Teukolsky, William T. Vet-
terling, and Brian P. Flannery. 2007. Numerical
Recipes 3rd Edition: The Art of Scientific Comput-
ing. Cambridge University Press, New York, NY,
USA, 3 edition.
Roi Reichart and Anna Korhonen. 2012. Docu-
ment and corpus level inference for unsupervised
and transductive learning of information structure of
scientific documents. In Proceedings of COLING
(Posters), pages 995?1006, Mumbai.
Anna Ritchie, Stephen Robertson, and Simone Teufel.
2008. Comparing citation contexts for information
retrieval. In Proeedings of the 17th Conference on
Information and Knowledge Management (CIKM),
pages 213?222, Napa Valley.
Henry G Small. 1973. Co-citation in the scientific lit-
erature: A new measure of the relationship between
two documents. Journal of American Society for In-
formation Science, 24(4):265?269.
Imad Tbahriti, Christine Chichester, Fr?ed?erique
Lisacek, and Patrick Ruch. 2006. Using argumen-
tation to retrieve articles with similar citations: An
inquiry into improving related articles search in the
medline digital library. I. J. Medical Informatics,
75(6):488?495.
Simone Teufel and Marc Moens. 2002. Summariz-
ing scientific articles: Experiments with relevance
and rhetorical status. Computational Linguistics,
28(4):409?445.
Simone Teufel. 1999. Argumentative Zoning: Infor-
mation Extraction from Scientific Articles. Univer-
sity of Edinburgh.
79
