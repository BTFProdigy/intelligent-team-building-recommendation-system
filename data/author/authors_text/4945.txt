Competence and Performance Grammar
in Incremental Processing
Vincenzo Lombardo
Dipartimento di Informatica
Universita` di Torino
c.so Svizzera, 185
10149, Torino, Italy
vincenzo@di.unito.it
Alessandro Mazzei
Dipartimento di Informatica
Universita` di Torino
c.so Svizzera, 185
10149, Torino, Italy
mazzei@di.unito.it
Patrick Sturt
Department of Psychology
University of Glasgow
58 Hillhead Street
Glasgow, G12 8QB, UK
patrick@psy.gla.ac.uk
Abstract
The goal of this paper is to explore some conse-
quences of the dichotomy between competence
and performance from the point of view of in-
crementality. We introduce a TAG?based for-
malism that encodes a strong notion of incre-
mentality directly into the operations of the
formal system. A left-associative operation is
used to build a lexicon of extended elementary
trees. Extended elementary trees allow deriva-
tions in which a single fully connected struc-
ture is mantained through the course of a left-
to-right word-by-word derivation. In the paper,
we describe the consequences of this view for
semantic interpretation, and we also evaluate
some of the computational consequences of en-
larging the lexicon in this way.
1 Introduction
Incremental processing can be achieved with a
combination of grammar formalism and deriva-
tion/parsing strategy. In this paper we explore
some of the computational consequences of de-
riving the incremental character of the human
language processor from the competence gram-
mar. In the following paragraphs, we assume
that incremental processing proceeds through a
sequence of processing steps. Each step consists
of a configuration of partial syntactic structures
(possibly connected into only one structure) and
a configuration of semantic structures (again,
possibly connected into one single expression).
These semantic structures result from the ap-
plication of the semantic interpreter to the syn-
tactic structures in the same processing step.
Depending on the semantic rules, some syntac-
tic structures may not be interpretable?that
is, some processing steps do not involve an up-
dating of the semantic representation. In the
view we present here, competence grammar is
responsible for the definition of both the set of
well-formed sentences of the language and the
set of possible partial structures that are yielded
by the derivation process. According to this
view, the performance component is responsible
for other aspects of language processing, includ-
ing ambiguity handling and error handling. The
latter issues are not addressed in this paper.
In the psycholinguistic and computational lit-
erature, many models for incremental process-
ing have been discussed. These models can be
characterized in terms of the location of the bor-
der between competence and performance. In
particular, we discuss the relative responsibility
of the competence and performance components
on three key areas of syntactic processing: a)
the space of well-formed partial syntactic struc-
tures; b) the space of the possible configurations
of partial syntactic structures at each process-
ing step; c) the sub-space of partial structures
that can actually be interpreted.
The definition of well-formedness is almost
universally assigned to the competence compo-
nent, whether in a direct implementation of the
grammar formalism (cf. the Type Transparency
hypothesis (Berwick and Weinberg, 1984)) or a
compiled version of the competence grammar
(e.g. LR parsing (Shieber and Johnson, 1993)).
The space of the possible configurations of
partial structures refers to those partial syntac-
tic structures that are built and stored during
parsing or derivation. Different algorithms re-
sult in different possibilities for the configura-
tions of partial structures that the parser builds.
For example, a bottom?up algorithm will never
build a partial structure with non?terminal leaf
nodes. The standard approach is to assign this
responsibility to the parsing algorithm, whether
the grammar is based on standard context-free
formalisms (Roark, 2001), on generative syntac-
tic theories based on a context-free backbone
(Crocker, 1992), or on categorial approaches,
like e.g. Combinatory Categorial Grammar
(CCG ? (Steedman, 2000)). A different method
is to assign this responsibility to the compe-
tence component. In this case the space of
possible configurations of partial structures is
constrained by the grammatical derivation pro-
cess itself, and the parsing algorithm needs to
be aligned with these requirements. This ap-
proach is exemplified by the works of Kempson
et al (2000) and Phillips (2003), who argue
that many problems in theoretical syntax, like
the definition of constituency, can be solved by
extending this responsability to the competence
grammar.
This issue of constituency is also relevant in
the third key area, which is the definition of the
space of interpretable structures. The assign-
ment of responsibility with respect to current
approaches usually depends on the implementa-
tion of the incremental technique. Approaches
based on a coupling of syntactic and seman-
tic rules in the competence grammar (Steed-
man, 2000; Kempson et al, 2000) adhere to the
so-called Strict Competence Hypothesis (Steed-
man, 2000), which constrains the interpreter to
deal only with grammatical constituents, so the
responsibility for deciding the interpretable par-
tial structures is assigned to competence1. In
contrast, approaches that are based on com-
petence grammars that do not include seman-
tic rules, like CFG, implement semantic inter-
preters that mimic such semantic rules (Stabler,
1991), and so they assign the responsibility for
deciding the interpretable partial structures to
performance.
In this paper we explore the empirical con-
sequences of building a realistic grammar when
the formalism constrains all these three areas,
as is the case with Kempson et al (2000)
and Phillips (2003). The work relies upon the
Dynamic Version of Lexicalized Tree Adjoin-
ing Grammar (DV?TAG), introduced in (Lom-
bardo and Sturt, 2002b), a formalism that en-
codes a dynamic grammar (cf. (Milward, 1994))
in LTAG terms (Joshi and Schabes, 1997). The
consequence of encoding a dynamic grammar
is that the configurations of partial structures
discussed above are limited to fully connected
structures, that is no disconnected structures
are allowed in a configuration. In particular,
the paper focuses on the problem of building a
realistic DV?TAG grammar through a conver-
sion from an LTAG, in order to maintain the
1Notice that these approaches may, however, differ in
the time-course with which semantic rules are applied
in the interpreter, and this issue depends directly on
the space of configurations of partial structures discussed
above
linguistic significance of elementary trees while
extending them to allow the full connectivity.
2 Dynamic Version of Tree
Adjoining Grammar
This section reviews the major aspects of the
Dynamic Version of Tree Adjoining Grammar
(DV?TAG), with special reference to similari-
ties and differences with respect to LTAG.
Dynamic grammars define well-formedness in
terms of states and transitions between states.
They allow a natural formulation of incremental
processing, where each word wi defines a tran-
sition from Statei?1, also called the left context,
to Statei (Milward, 1994). The states can be
defined as partial syntactic or semantic struc-
tures that are ?updated? as each word is recog-
nized; roughly speaking, two adjacent states can
be thought of as two parse trees before and af-
ter the attachment of a word, respectively. The
derivation process proceeds from left to right
by extending a fully connected left context to
include the next input word.
Like an LTAG (Joshi and Schabes, 1997), a
Dynamic Version of Tree Adjoining Grammar
(DV?TAG) consists of a set of elementary trees,
divided into initial trees and auxiliary trees,
and attachment operations for combining them.
Lexicalization is expressed through the associ-
ation of a lexical anchor with each elementary
tree. The anchor defines the semantic content of
the elementary tree: the whole elementary tree
can be seen as an extended projection of the an-
chor (Frank, 2000). LTAG is said to define an
extended domain of locality ?unlike context-free
grammars, which use rules that describe one?
branch deep fragments of trees, TAG elemen-
tary trees can describe larger structures (e.g. a
verb, its maximal S node and subject NP node).
In figures 1(a) and 2(a) we can see the ele-
mentary trees for a derivation of the sentence
Bill often pleases Sue for LTAG and DV?TAG
respectively. Auxiliary trees in DV?TAG are
split into left auxiliary trees, where the lexical
anchor is on the left of the foot node, and right
auxiliary trees, where the lexical anchor is on
the right of the foot node. The tree anchored
by often in fig. 2(a) is a left auxiliary tree.
Non-terminal nodes have a distinguished
head daughter, which provides the lexical head
of the mother node: unlike in LTAG, each node
in the elementary trees is augmented with a fea-
ture indicating the lexical head that projects
the node. This feature is needed for the no-
Bill often pleases Sue. 
NNP 
Sue 
NP 
ADV
often
ADVP 
VP 
VP* 
NP 
NNP 
Sue 
S
V
pleases
NP 
VP NNP 
Bill 
ADV
often
ADVP 
VP 
V
pleases 
NP$ 
NP$ 
S
VP 
adjunction
Bill
pleases
Sueoften
(a)
(b)
(c)
substitutionNNP 
Bill 
NP 
substitution
 
Bill
 
pleases

often
 
Sue
Figure 1: The LTAG derivation of the sentence
Bill often pleases Sue.
NNP 
Sue 
NP(Sue)
ADV
often
ADVP(often)
VP(_j)
VP*(_j)
NP(Sue)
NNP 
Sue 
S(pleases)
V
pleases
NP(Bill)
VP(pleases)NNP 
Bill 
ADV
often
ADVP(often)
VP(pleases)
V(_i) NP$(_k)NNP 
Bill 
NP(Bill)
S(_i)
VP( i)
pleases 
likes 
eats 
plays 
?
1. adjunction 
from the left 
2. shift 
Bill
pleases
Sueoften
(a)
(b)
(c)
3. substitution 
Figure 2: The DVTAG derivation of the sen-
tence Bill often pleases Sue.
tion of derivation?dependency tree (see below).
If several unheaded nodes share the same lexical
head, they are all co-indexed with a head vari-
able (e.g. i in the elementary tree anchored by
Bill in figure 2(a)); the head variable is a vari-
able in logic terms: i will be unified with the
constant (?lexical head?) pleases.
In both LTAG and DV?TAG the lexical an-
chor does not necessarily provide the head fea-
ture of the root of the elementary tree. This is
trivially true for auxiliary trees (e.g. the tree
anchored often in figure 1(a) and figure 2(a)).
However, in DV?TAG this can also occur with
initial trees (e.g. the tree anchored by Bill in
figure 2(a)), because initial trees can include
not only the head projection of the anchor, but
also other higher projections that are required
to account for the full connectedness of the par-
tial parse tree. The elementary tree anchored
by Bill is linguistically motivated up to the NP
projection; the rest of the structure depends on
connectivity. These extra nodes are called pre-
dicted nodes. A predicted preterminal node is
referred by a set of lexical items. In the sec-
tion 3 we illustrate a method for building such
extended elementary trees.
The derivation process in LTAG and DV?
TAG builds a derived tree by combining the ele-
mentary trees via some operations that are illus-
trated below. DV?TAG implements the incre-
mental process by constraining the derivation
process to be a series of steps in which an ele-
mentary tree is combined with the partial tree
spanning the left fragment of the sentence. The
result of a step is an updated partial structure.
Specifically, at the processing step i, the ele-
mentary tree anchored by the i-th word in the
sentence is combined with the partial structure
spanning the words from 1 to i ? 1 positions;
the result is a partial structure spanning the
words from 1 to i. In contrast, LTAG does
not pose any order constraint on the deriva-
tion process, and the combinatorial operations
are defined over pairs of elementary trees. In
DV?TAG the derivation process starts from an
elementary tree anchored by the first word in
the sentence and that does not require any at-
tachment that would introduce lexical material
on the left of the anchor (such as in the case
that a Substitution node is on the left of the
anchor). This elementary tree becomes the first
left context that has to be combined with some
elementary tree on the right.
Since in DV?TAG we always combine a left
context with an elementary tree, the number
of attachment operations increases from two
in LTAG to six in DV?TAG. Three operations
(substitution, adjunction from the left and ad-
junction from the right) are called forward op-
erations because they insert the current elemen-
tary tree into the left context; two other oper-
ations (inverse substitution and inverse adjunc-
tion) are called inverse operations because they
insert the left context into the current elemen-
tary tree; the sixth operation (shift) does not
involve any insertion of new structural material.
The first operation in DV?TAG is the stan-
dard LTAG substitution, where some elemen-
tary tree replaces a substitution node in another
tree structure (see fig. 2(a)).
Standard LTAG adjunction is split into two
operations: adjunction from the left and ad-
junction from the right. The type of adjunction
depends on the position of the lexical material
introduced by the auxiliary tree with respect
to the material currently dominated by the ad-
joined node (which is in the left context). In
figure 2(a) we have an adjunction from the left
in the case of the left auxiliary tree anchored by
often.
Inverse operations account for the insertion
of the left context into the elementary tree. In
the case of inverse substitution the left context
replaces a substitution node in the elementary
tree; in the case of inverse adjunction, the left
context acts like an auxiliary tree, and the el-
ementary tree is split because of the adjoining
of the left context at some node. In (Lombardo
and Sturt, 2002b) there is shown the importance
of the latter operation to obtain the correct de-
pendencies for cross-serial Dutch dependencies
in DV?TAG.
Finally, the shift operation either scans a lex-
ical item which has been already introduced in
the structure or derives a lexical item from some
predicted preterminal node.
It is important to notice that, during the
derivation process, not all the nodes in the left
context and the elementary tree are accessible
for performing some operation: given the i? 1-
th word in the sentence we can compute a set
of accessible nodes in the left context (the right
fringe); also, given the lexical anchor of the el-
ementary tree, that in the derivation process
matches the i-th word in the sentence, we can
compute a set of accessible nodes in the elemen-
tary tree (the left fringe).
At the end of the derivation process the left
context structure spans the whole sentence, and
is called the derived tree: in the figures 1(c) and
2(c) there are the derived trees for Bill often
pleases Sue in LTAG and DV?TAG respectively.
A key device in LTAG is the derivation tree
(fig. 1(b)). The derivation tree represents the
history of the derivation of the sentence: it de-
scribes the substitutions and the adjoinings that
occur in a sentence derivation through a tree
structure. The nodes of the derivation tree are
identifiers of the elementary trees, and one edge
represents the operation that combines two ele-
mentary trees. Given an edge, the mother node
identifies the elementary tree where the elemen-
tary tree identified by the daughter node is sub-
stituted in or adjoined to, respectively. The
derivation tree provides a factorized representa-
tion of the derived tree. Since each elementary
is anchored by a lexical item, the derivation tree
also describes the syntactic dependencies in the
sentence in the terms of a dependency?style rep-
resentation (Rambow and Joshi, 1999) (Dras et
al., 2003).
The notion of derivation tree is not ade-
quate for DV?TAG, since the elementary trees
contain unheaded predicted nodes. For exam-
ple, the elementary tree anchored by Bill ac-
tually involves two anchors, Bill and pleases,
even if the latter anchor remains unspecified
until it is scanned/derived in the linear or-
der. We introduce a new word?based structure
that represents syntactic dependencies, namely
a derivation-dependency tree.
A derivation-dependency tree is a head-based
version of the derivation tree. Each node in an
elementary tree is augmented with the lexical
head that projects that node. The derivation-
dependency tree contains one node per lexi-
cal head, and a lexical head dominates another
when the corresponding projections in the de-
rived tree stand in a dominance relation. Each
elementary tree can contain only one overtly
marked lexical head, that represents the seman-
tic unit, but the presence of predicted nodes
in the partial derived tree corresponds to pre-
dicted heads in the derivation-dependency tree.
In figure 3 is depicted the evolution of the
derivation?dependency tree for the sentence Bill
often pleases Sue.
The DV?TAG derivation process requires the
full connectivity of the left context at all times.
The extended domain of locality provided by
LTAG elementary trees appears to be a desir-
able feature for implementing full connectivity.
However, each new word in a string has to be
connected with the preceding left context, and
there is no a priori limit on the amount of struc-
ture that may intervene between that word and
the preceding context. For example, in a DV?
TAG derivation of John said that tasty apples
     	


    	
 Dependency and relational structure in treebank annotation
Cristina BOSCO, Vincenzo LOMBARDO
Dipartimento di Informatica, Universita` di Torino
Corso Svizzera 185
10149 Torino,
Italia,
bosco,vincenzo@di.unito.it
Abstract
Among the variety of proposals currently mak-
ing the dependency perspective on grammar
more concrete, there are several treebanks
whose annotation exploits some form of Rela-
tional Structure that we can consider a general-
ization of the fundamental idea of dependency
at various degrees and with reference to differ-
ent types of linguistic knowledge.
The paper describes the Relational Structure as
the common underlying representation of tree-
banks which is motivated by both theoretical
and task-dependent considerations. Then it
presents a system for the annotation of the Rela-
tional Structure in treebanks, called Augmented
Relational Structure, which allows for a system-
atic annotation of various components of lin-
guistic knowledge crucial in several tasks. Fi-
nally, it shows a dependency-based annotation
for an Italian treebank, i.e. the Turin Univer-
sity Treebank, that implements the Augmented
Relational Structure.
1 Introduction
Different treebanks use different annotation
schemes which make explicit two distinct but
interrelated aspects of the structure of the sen-
tence, i.e. the function of the syntactic units
and their organization according to a part-whole
paradigm. The first aspect refers to a form of
Relational Structure (RS), the second refers to
its constituent or Phrase Structure (PS). The
major difference between the two structures is
that the RS allows for several types of rela-
tions to link the syntactic units, whilst the PS
involves a single relation ?part-of?. The RS
can be seen as a generalization of the depen-
dency syntax with the syntactic units instanti-
ated to individual words in the dependency tree
(Mel?c?uk, 1988). As described in many theo-
retical linguistic frameworks, the RS provides a
useful interface between syntax and a seman-
tic or conceptual representation of predicate-
argument structure. For example, Lexical Func-
tional Grammar (LFG) (Bresnan, 1982) collo-
cates relations at the interface between lexicon
and syntax, Relational Grammar (RG) (Perl-
mutter, 1983) provides a description of the sen-
tence structure exclusively based on relations
and syntactic units not structured beyond the
string level.
This paper investigates how the notion of RS
has been applied in the annotation of tree-
banks, in terms of syntactic units and types
of relations, and presents a system for the
definition of the RS that encompasses several
uses in treebank schemata and can be viewed
as a common underlying representation. The
system, called Augmented Relational Struc-
ture (ARS) allows for an explicit representa-
tion of the three major components of linguistic
structures, i.e. morpho-syntactic, functional-
syntactic and semantic. Then the paper shows
how a dependency-based annotation can de-
scend on ARS, and describes the ARS-based an-
notation of a dependency treebank for Italian,
the Turin University Treebank (TUT), which is
the first available treebank for Italian, with a
few quantitative results.
The paper is organized as follows. The next
section investigates both the annotation of RS
in treebanks and the major motivations for the
use of RS from language-specific issues and NLP
tasks implementation; then we present the ARS
system; finally, we show the dependency anno-
tation of the TUT corpus.
2 Annotation of the Relational
Structure
In practice, all the existing treebank schemata
implement some form of relational structure.
Annotation schemata range from pure (depen-
dency) RS-based approaches to RS-PS combi-
nations (Abeille?, 2003).
Some treebanks consider the relational infor-
mation as the exclusive basis of the annotation.
The Prague Dependency Treebank ((Hajic?ova?
and Ceplova?, 2000), (Bo?hmova? et al, 2003))
implements a three level annotation scheme
where both the analytical (surface syntactic)
and tectogrammatical level (deep syntactic and
topic-focus articulation) are dependency-based;
the English Dependency Treebank (Rambow
et al, 2002) implements a dependency-based
mono-stratal analysis which encompasses sur-
face and deep syntax and directly represents the
predicate-argument structure. Other projects
adopt mixed formalisms where the sentence is
split in syntactic subunits (phrases), but linked
by functional or semantic relations, e.g. the Ne-
gra Treebank for German ((Brants et al, 2003),
(Skut et al, 1998)), the Alpino Treebank for
Dutch (van der Beek et al, 2002), and the Lingo
Redwood Treebank for English (Oepen et al,
2002). Also in the Penn Treebank ((Marcus
et al, 1993), (Marcus et al, 1994)) a limited
set of relations is placed over the constituency-
based annotation in order to make explicit the
(morpho-syntactic or semantic) roles that the
constituents play.
The choice of a RS-based annotation schema
can depend on theoretical linguistic motivations
(a RS-based schema allows for an explicit, fine-
grained representation of several linguistic phe-
nomena), task-dependent motivations (the RS-
based schema represents the linguistic informa-
tion involved in the task(s) at hand), language-
dependent motivations (the relational structure
is traditionally considered as the most adequate
representation of the object language).
Theoretical motivations for exploiting repre-
sentations based on forms of RS was developed
in the several RS-based theoretical linguistic
frameworks (e.g. Lexical Functional Grammar,
Relaional Grammar and dependency grammar),
which allow for capturing information involved
at various level (e.g. syntactic and semantic)
in linguistic structures, and grammatical for-
malisms have been proposed with the aim to
capture the linguistic knowledge represented in
these frameworks. Since the most immediate
way to build wide-coverage grammars is to
extract them directly from linguistic data (i.e.
from treebanks), the type of annotation used in
the data is a factor of primary importance, i.e.
a RS-based annotation allows for the extraction
of a more descriptive grammar1.
1See (Mazzei and Lombardo, 2004a) and (Mazzei and
Lombardo, 2004b) for experiments of LTAG extraction
from TUT.
Task-dependent motivations rely on how the
annotation of the RS can facilitate some
processing aspects of NLP applications. The
explicit representation of predicative structures
allowed by the RS can be a powerful source
of disambiguation. In fact, a large amount of
ambiguity (such as coordination, Noun-Noun
compounds and relative clause attachment) can
be resolved using such a kind of information,
and relations can provide a useful interface
between syntax and semantics. (Hindle and
Rooth, 1991) had shown the use of dependency
in Prepositional Phrase disambiguation, and
the experimental results reported in (Hock-
enmaier, 2003) demonstrate that a language
model which encodes a rich notion of predicate
argument structure (e.g. including long-range
relations arising through coordination) can
significantly improve the parsing performances.
Moreover, the notion of predicate argument
structure has been advocated as useful in
a number of different large-scale language-
processing tasks, and the RS is a convenient
intermediate representation in several applica-
tions (see (Bosco, 2004) for a survey on this
topic). For instance, in Information Extraction
relations allows for recognizing different guises
in which an event can appear regardless of the
several different syntactic patterns that can be
used to specify it (Palmer et al, 2001)2. In
Question Answering, systems usually use forms
of relation-based structured representations of
the input texts (i.e. questions and answers)
and try to match those representations (see
e.g. (Litkowski, 1999), (Buchholz, 2002)).
Also the in-depth understanding of the text,
necessary in Machine Translation task, requires
the use of relation-based representations where
an accurate predicate argument structure is a
critical factor (Han et al, 2000)3.
Language-dependent motivations rely on the
fact that the dependency-based formalisms has
been traditionally considered as the most ade-
quate for the representation of free word order
languages. With respect to constituency-based
2Various approaches to IE (Collins and Miller, 1997)
address this issue by using relational representations,
that is forms of ?concept nodes? which specifies a trig-
ger word (usually a Verb) and also forms of mapping
between the syntactic and the semantic relations of the
trigger.
3The system presented in (Han et al, 2000) gener-
ates the dependency trees of the source language (Ko-
rean) sentences, then directly maps them to the trans-
lated (English) sentences.
formalisms, free word order languages involves a
large amount of discontinuous constituents (i.e.
constituents whose parts are not contiguous in
the linear order of the sentence). In practice, a
constituency-based representation was adopted
for languages with rather fixed word order
patterns, like English (Penn Treebank), while
a dependency representation for languages
which allow variable degrees of word order
freedom, such as Czech (see Prague Depen-
dency Treebank) or Italian (as we will see later,
TUT). Nevertheless, in principle, since the
representation of a discontinuous constituent X
can be addressed in various ways (e.g. by in-
troducing lexically empty elements co-indexed
with the moved parts of X ), the presence to
a certain extent of word order freedom does
not necessarily mean that a language has to be
necessarily annotated according to a relation-
based format rather than a constituency-based
one. Moreover, free word order languages can
present difficulties for dependency-based as
well as for constituency-based frameworks (e.g.
non-projective structures). The development
of dependency-based treebanks for English (see
English Dependency Treebank) together with
the inclusion of relations in constituency-based
treebanks (see Penn Treebank) too, confirms
the strongly prevailing relevance of motivations
beyond the language-dependent ones.
The types of knowledge that many applica-
tions actually need are RS-based representa-
tions where predicate argument structure and
the associated morphological and syntactic in-
formation can operate as an interface to a
semantic-conceptual representation. All these
types of knowledge have in common the fact
that they can be described according to the de-
pendency paradigm, rather than according to
the constituency paradigm. The many applica-
tions (in particular those referring to the Penn
Treebank) which use heuristics-based transla-
tion schemes from the phrase structure to lexical
dependency (?head percolation tables?) (Ram-
bow et al, 2002) show that the access to com-
prehensive and accurate extended dependency-
based representations has to be currently con-
sidered as a critical issue for the development of
robust and accurate NLP technologies.
Now we define our proposal for the representa-
tion of the RS in treebank annotation.
3 The Augmented Relational
Structure
A RS consists of syntactic units linked by re-
lations. An Augmented Relational Structure
(ARS) organizes and systematizes the informa-
tion usually associated in existing annotations
to the RS, and includes not only syntactic ,
but also linguistic information that can be rep-
resented according to a dependency paradigm
and that is proximate to semantics and un-
derlies syntax and morphology. Therefore the
eats
John the apple 
rel2rel1
Figure 1: A simple RS.
ARS expresses the relations and syntactic units
in terms of multiple components. We describe
the ARS as a dag where each relation is a
feature structure including three components.
Each component of the ARS-relations is use-
morph
fsynt
sem
v1
v2
v3
Figure 2: An ARS relation.
ful in marking both similarities and differences
among the behavior of units linked by the de-
pendency relations.
The morpho-syntactic component of the ARS
describes morpho-syntactic features of the
words involved in relations, such as their gram-
matical category. This component is useful for
making explicit the morpho-syntactic variants
of predicative structures. Instances of this kind
of variants often occur in intransitive, tran-
sitive and di-transitive predicative structures,
e.g. esplosione-esplodere (explosion - to ex-
plode) are respectively nominal and verbal vari-
ants of the intransitive structure ?something ex-
plodes?. By referring to the TUT, we can evalu-
ate the frequency of this phenomenon: in 1,500
sentences 944 Verbs occur (for a total of 4169
occurrences) and around the 30% of them are
present in the nominal variant too4.
The functional-syntactic component identifies
the subcategorized elements, that is it keeps
apart arguments and modifiers in the pred-
icative structures. Moreover, this component
makes explicit the similarity of a same predica-
tive structure when it occurs in the sentence in
different morpho-syntactic variants. In fact, the
functional-syntactic components involved, e.g.,
in the transitive predicative structure ?some-
one declares something?, are the same in both
the nominal (dichiarazione [di innocenza]
OBJ
[di John]
SUBJ
- John?s declaration of innocence)
and verbal realization ([John]
SUBJ
dichiara [la
sua innocenza]
OBJ
- John declares his inno-
cence) of such a predication, i.e. SUBJ and
OBJ. The distinction between arguments and
modifiers has been considered as quite problem-
atic in the practice of the annotation, even if rel-
evant from the applicative and theoretical point
of view5, and is not systematically annotated in
the Penn Treebank (only in clear cases, the use
of semantic roles allows for the annotation of
argument and modifier) and in the Negra Tree-
bank. This distinction is instead usually marked
in dependency representations, e.g. in the En-
glish Dependency Treebank and in the Prague
Dependency Treebank.
The semantic component of the ARS-relations
specifies the role of words in the syntax-
semantics interface and discriminates among
different kinds of modifiers and oblique com-
plements. We can identify at least three levels
of generality: verb-specific roles (e.g. Runner,
Killer, Bearer); thematic roles (e.g. Agent, In-
strument, Experiencer, Theme, Patient); gener-
alized roles (e.g. Actor and Undergoer). The
use of specific roles can cause the loss of useful
generalizations, whilst too generic roles do not
describe with accuracy the data. An example of
annotation of semantic roles is the tectogram-
matical layer of the Prague Dependency Tree-
bank.
ARS features a mono-stratal approach. By fol-
lowing this strategy, the annotation process can
be easier, and the result is a direct represen-
tation of a complete predicate argument struc-
ture, that is a RS where all the information
(morpho-syntactic, functional-syntactic and se-
mantic) are immediately available. An alterna-
tive approach has been followed by the Prague
4This statistics does not take into consideration the
possible polysemic nature of words involved.
5See, for instance, in LFG and RG.
Dependency Treebank, which is featured by
a three levels annotation. This case shows
that the major difference between the syntactic
(analytic) and the semantic (tectogrammatical)
layer consists in the inclusion of empty nodes
for recovering forms of deletion ((Bo?hmova et
al., 1999), (Hajic?ova? and Ceplova?, 2000)). But
this kind of information does not necessarily re-
quires a separated semantic layer and can be
annotated as well in a mono-stratal represen-
tation, like the English Dependency Treebank
(Rambow et al, 2002) does.
The tripartite structure of the relations in ARS
guarantees that different components can be ac-
cessed separately and analyzed independently
(like in (Montemagni et al, 2003) or in (Ram-
bow et al, 2002)). Furthermore, the ARS al-
lows for forms of annotation of relations where
not all the features are specified too. In fact, the
ARS-relations which specify only a part of com-
ponents allow for the description of syntactic
grammatical relations which do not correspond
with any semantic relation, either because they
have a void semantic content or because they
have a different structure from any possible cor-
responding semantic relation (i.e. there is no
semantic relation linking the same ARS-units
linked by the syntactic one). Typical relations
void of semantic content can link the parts of ex-
pressions not compositionally interpretable (id-
ioms), for instance together with with in to-
gether with. While a classic example of a non-
isomorphic syntactic and semantic structure is
one which involves the meaning of quantifiers:
a determiner within a NP extends its scope be-
yond the semantic concept that results from the
interpretation of the NP. Another example is
the coordination where the semantic and syn-
tactic structure are often considered as non iso-
morphic in several forms of representation.
The ARS-relations including values for both
functional-syntactic and semantic components
may be used in the representation of grammat-
ical relations which participate into argument
structures and the so-called oblique cases (see
Fillmore and (Hudson, 1990)), i.e. where the
semantic structures are completely isomorphic
to the syntactic structures. For example, a
locative adjunct like in the garden in John was
eating fish in the garden is represented at the
syntactic level as a Prepositional Phrase play-
ing the syntactic function locative in the Verb
Phrase (in the Penn Treebank it could be an-
notated as a PP-LOC); the semantic concept
corresponding to the garden plays the semantic
role LOCATION in the ?eating? event stated
by the sentence.
4 TUT: a dependency-based
treebank for Italian
The TUT is the first available tree-
bank of Italian (freely downloadable at
http://www.di.unito.it/?tutreeb/). The cur-
rent release of TUT includes 1,500 sentences
corresponding to 38,653 tokens (33,868 words
and 4,785 punctuation marks). The average
sentence length is of 22,57 words and 3,2
punctuation marks.
In this section, we concentrate on the major
features of TUT annotation schema, i.e. how
the ARS system can describe a dependency
structure.
4.1 A dependency-based schema
In Italian the order of words is fixed in non
verbal phrases, but verbal arguments and mod-
ifiers can be freely distributed without affect-
ing the semantic interpretation of the sentence.
A study on a set of sentences of TUT shows
that the basic word order for Italian is Subject-
Verb-Complement (SVC), as known in litera-
ture (Renzi, 1988), (Stock, 1989), but in more
than a quarter of declarative sentences it is vi-
olated (see the following table6). Although the
Permutations Occurrences
S V C 74,26%
V C S 11,38%
S C V 7,98%
C S V 3,23%
V S C 2,29%
C V S 0,77%
Table 1: Italian word order
SVC order is well over the others, the fact that
all the other orders are represented quantita-
tively confirms the assumption of partial con-
figurationality intuitively set in the literature.
The partial configurationality of Italian can be
considered as a language-dependent motivation
for the choice of a dependency-based annota-
tion for an Italian treebank. The schema is
similar to that of the Prague Dependency Tree-
bank analytical-syntactic level with which TUT
6The data reported in the table refer to 1,200 anno-
tated sentences where 1,092 verbal predicate argument
structures involving Subject and at least one other Com-
plement occur.
shares the following basic design principles typ-
ical of the dependency paradigm:
 the sentence is represented by a tree where
each node represents a word and each
edge represents a dependency labelled by a
grammatical relation which involves a head
and a dependent,
 each single word and punctuation mark
is represented by a single node, the so-
called amalgamated words, which are words
composed by lexical units that can occur
both in compounds and alone, e.g. Verbs
with clitic suffixes (amarti (to love-you) or
Prepositions with Article (dal (from-the)),
are split in more lexical units7,
 since the constituent structure of the sen-
tence is implicit in dependency trees, no
phrases are annotated8.
If the partial configurationality makes the
dependency-based annotation more adequate
for Italian, other features of this language
should be well represented by exploiting a
Negra-like format where the syntactic units are
phrases rather than single words. For instance,
in Italian, Nouns are in most cases introduced
by the Article: the relation between Noun and
Determiner is not very relevant in a dependency
perspective, while it contributes to the defini-
tion of a clearer notion of NP in Italian than
in languages poorer of Determiners like, e.g.,
Czech. The major motivation of a dependency-
based schema is therefore theoretical and, in
particular, to make explicit in the treebank an-
notation a variety of structures typical of the
object language.
Moreover, in order to make explicit in cases
of deletion and ellipsis the predicate argument
structure, we annotate in the TUT null ele-
ments. These elements allow for the annota-
tion of a variety of phenomena: from the ?equi?
deletion which affects the subject of infinitive
Verb depending on a tensed Verb (e.g. John(1)
vuole T(1) andare a casa - John(1) want to
T(1) go home), to the various forms of gap-
ping that can affect parts of the structure of
the sentence (e.g. John va(1) a casa e Mario
T(1) al cinema - John goes(1) home and Mario
7Referring to the current TUT corpus, we see that
around 7,7% words are amalgamated.
8If phrase structure is needed for a particular appli-
cation, it is possible to automatically derive it from the
dependency structure along with the surface word order.
dichiarava
In Sudja 
VERB,PREP
RMOD
TIME
il
VERB,NOUN
SUBJ
AGENT 
VERB,DET+DEF
OBJ
THEME
quei la 
PREP,DET+DEF
ARG 
#
fallimento
NOUN,DET+DEF
APPOSITION 
DENOM
DET+DEF,NOUN
ARG 
#
giorni zingara 
DET+DEF,NOUN
ARG 
#
DET+DEF,NOUN
ARG 
#
Figure 3: The TUT representation of In quei
giorni Sudja la zingara dichiarava il fallimento
(In those days Sudja the gipsy declared the
bankruptcy).
T(1) to the cinema), to the pro-dropped sub-
jects typical of Italian (as well as of Portuguese
and Spanish), i.e. the subject of tensed Verbs
which are not lexically realized in the sentence
(e.g. T Va a casa - T goes home). For phenom-
ena such as equi and gapping TUT implements
co-indexed traces, while it implements non co-
indexed traces for phenomena such as the pro-
drop subject.
4.2 An ARS-based schema
In TUT the dependency relations form the
skeleton of the trees and the ARS tripartite fea-
ture structures which are associated to these re-
lations resolve the interface between the mor-
phology, syntax and semantics. The ARS al-
lows for some form of annotation also of rela-
tions where only parts of the features are spec-
ified. In TUT this has been useful for under-
specifying relations both in automatic analysis
of the treebank (i.e. we can automatically ex-
clude the analysis of a specific component of
the relations) and in the annotation process (i.e.
when the annotator is not confident of a specific
component of a relation, he/she can leave such
a component void).
In the figure 3 we see a TUT tree.
All the relations annotated in the tree in-
clude the morpho-syntactic component, formed
by the morphological categories of the words in-
volved in the relation separated by a comma,
e.g. VERB,PREP for the relation linking the
root of the sentence with its leftmost child
(In). Some relation involves a morpho-syntactic
component where morphological categories are
composed by more elements, e.g. DET+DEF
(in DET+DEF,NOUN) for the relation linking
quei with giorni. The elements of the morpho-
syntactic component of TUT includes, in fact,
10 ?primary? tags that represent morphologi-
cal categories of words (e.g. DET for Deter-
miner, NOUN for Noun, and VERB for Verb),
and that can be augmented with 20 ?secondary?
tags (specific of the primary tags) which further
describe them by showing specific features, e.g.
DEF which specifies the definiteness of the De-
terminer or INF which specifies infiniteness of
Verb. Valid values of the elements involved in
TUT morpho-syntactic tags are 40.
By using the values of the functional-syntactic
component, TUT distinguishes among a variety
of dependency relations. In figure 3 we see the
distinction between argument, e.g. the relation
SUBJ linking the argument Sudja with the ver-
bal root of the sentence dichiarava, and the rela-
tion RMOD which represents a restrictive mod-
ifier and links the verbal root dichiarava with
in quei giorni. The dependents of Prepositions
and determiners are annotated as argument too,
according to arguments presented in (Hudson,
1990). Another distinction which is exploited
in the annotation of the sentence is that be-
tween restrictive modifier (i.e. RMOD which
links dichiarava with in quei giorni) and AP-
POSITION (i.e. non restrictive modifier linking
Sudja with la zingara), which are modifiers that
restrict the meaning of the head. Beyond these
basic distinctions, TUT schema draws other dis-
tinctions among the functional-syntactic rela-
tions and includes a large set of tags for a total
of 55 items, which are compounds of 27 pri-
mary and 23 secondary tags. These tags are
organized in a hierarchy (Bosco, 2004) accord-
ing to their different degree of specification. In
the hierarchy of relations, Arguments (ARG) in-
clude Subject (SUBJ), Object (OBJ), Indirect
Object (INDOBJ), Indirect Complement (IN-
DCOMPL), Predicative Complements (of the
Subject (PREDCOMPL+SUBJ) and of the Ob-
ject (PREDCOMPL+OBJ)). The direct conse-
quence of its hierarchical organization is the
availability of another mechanisms of under-
specification in the annotation or in the anal-
ysis of annotated data. In fact, by referring to
the hierarchy we can both annotate and analyze
relations at various degrees of specificity.
The semantic component discriminates among
different kinds of modifiers and oblique com-
plements. The approach pursued in TUT has
been to annotate very specific semantic roles
only when they are immediately and neatly
distinguishable. For instance, by referring to
the prepositional dependents introduced by da9
(from/by), we find the following six different
values for the semantic component:
- REASONCAUSE, e.g., in gli investitori sono
impazziti dalle prospettive di guadagno (the in-
vestors are crazy because of the perspectives of
gain)
- SOURCE, e.g., in traggono benefici dalla
bonifica ([they] gain benefit from the drainage)
- AGENT, e.g., l?iniziativa e` appoggiata dagli
USA (the venture is supported by USA)
- TIME, e.g., dal ?91 sono stati stanziati 450
miliardi (since ?91 has been allocated 450 bil-
lions)
- THEME, e.g., cio` distingue l?Albania dallo
Zaire (that distinguishes the Albany from
Zaire)
- LOC, which can be further specialized in
LOC+FROM, e.g., in da qui e` partito l?assalto
(from here started the attack), LOC+IN, e.g., in
quello che succedeva dall?altra parte del mondo
(what happened in the other side of the world),
LOC+METAPH, e.g., in l?Albania e` passata dal
lancio dei sassi alle mitragliatrici (the Albany
has passed from the stone-throwing to the ma-
chineguns).
In figure 3 the semantic component has been
annotated only in four relations, which respec-
tively represent the temporal modifier In quei
giorni of the main Verb dichiarava, the appo-
sition la zingara of the Noun Sudja, and the
arguments of the Verb, i.e. the subject Sudja la
zingara which plays the semantic role AGENT
and the object il fallimento which plays the se-
mantic role THEME. In the other relations in-
volved in this sentence a value for the semantic
component cannot be identified10, e.g. the ar-
gument of a Preposition or Determiner cannot
be semantically specified as in the case of the
verbal arguments.
5 Conclusions
The paper analyzes the annotation of the RS in
the existing treebanks by referring to a notion
of RS which is a generalization of dependency.
9In 1,500 TUT sentences we find 591 occurrences of
this Preposition.
10In figure 3, we marked the semantic component of
these cases with .
According to this notion, the RS includes types
of linguistic knowledge which are different, but
which have in common that they can be repre-
sented by a dependency paradigm rather than
to a constituency-based one.
The paper identifies two major motivations
for exploiting RS in treebank annotation:
language-dependent motivations that have de-
termined the use of dependency for the repre-
sentation of treebanks of free word order lan-
guages, and task-dependent motivations that
have determined a wider use of relations in tree-
banks.
In the second part of the paper, we show a sys-
tem for the annotation of RS, i.e. the ARS,
and how the ARS can be used for the an-
notation of a dependency-based treebank, the
TUT whose schema augments classical depen-
dency (functional-syntactic) relations with mor-
phological and semantic knowledge according to
the above mentioned notion of RS.
References
A. Abeille?, editor. 2003. Building and using
syntactically annotated corpora. Kluwer, Dor-
drecht.
A. Bo?hmova, J. Panevova?, and P. Sgall. 1999.
Syntactic tagging: procedure for the transi-
tion from analytic to the tectogrammatical
treestructures. In Proc. of 2nd Workshop on
Text, speech and dialog, pages 34?38.
A. Bo?hmova?, J. Hajic?, E. Hajic?ova?, and
B. Hladka?. 2003. The Prague Dependency
Treebank: A three level annotation scenario.
In Abeille? (Abeille?, 2003), pages 103?127.
C. Bosco. 2004. A grammatical rela-
tion system for treebank annotation.
Ph.D. thesis, University of Torino.
http://www.di.unito.it/?bosco/.
T. Brants, W. Skut, and H. Uszkoreit. 2003.
Syntactic annotation of a German newspaper
corpus. In Abeille? (Abeille?, 2003), pages 73?
87.
J. Bresnan, editor. 1982. The mental represen-
tation of grammatical relations. MIT Press,
Cambridge.
S. Buchholz. 2002. Using grammatical rela-
tions, answer frequencies and the World Wide
Web for TREC Question Answering. In Proc.
of TREC 2001, pages 502?509.
M. Collins and S. Miller. 1997. Semantic tag-
ging using a probabilistic context free gram-
mar. In Proc. of 6th Workshop on Very Large
Corpora.
E. Hajic?ova? and M. Ceplova?. 2000. Deletions
and their reconstruction in tectogrammatical
syntactic tagging of very large corpora. In
Porc. of COLING 2000, pages 278?284.
C. Han, B. Lavoie, M. Palmer, O. Rambow,
R. Kittredge, T. Korelsky, N. Kim, and
M. Kim. 2000. Handling structural diver-
gences and recovering dropped arguments in a
Korean/English machine translation system.
In Proc. of AMTA 2000, pages 40?54.
D. Hindle and M. Rooth. 1991. Structural am-
biguity and lexical relations. In Proc. of ACL
91, pages 229?236.
J. Hockenmaier. 2003. Parsing with generative
models of predicate-argument structure. In
Proc. of ACL 2003.
R. Hudson. 1990. English Word Grammar.
Basil Blackwell, Oxford and Cambridge.
K.C. Litkowski. 1999. Question-answering us-
ing semantic relation triples. In Proc. of
TREC-8, pages 349?356.
M. Marcus, B. Santorini, and M.A.
Marcinkiewicz. 1993. Building a large
annotated corpus of English: The Penn
Treebank. Computational Linguistics,
19:313?330.
M. Marcus, G. Kim, M.A. Marcinkiewicz,
R. MacIntyre, A. Bies, M. Ferguson, K. Katz,
and B. Schasberger. 1994. The Penn Tree-
bank: Annotating predicate argument struc-
ture. In Proc. of HLT-94.
A. Mazzei and V. Lombardo. 2004a. Building a
large grammar for Italian. In Proc. of LREC
2004, pages 51?54.
A. Mazzei and V. Lombardo. 2004b. A com-
parative analysis of extracted grammars. In
Proc. of ECAI 2004.
I.A. Mel?c?uk. 1988. Dependency Syntax: theory
and practice. SUNY, Albany.
S. Montemagni, F. Barsotti, M. Battista,
and N. Calzolari. 2003. Building the Ital-
ian syntactic-semantic treebank. In Abeille?
(Abeille?, 2003), pages 189?210.
S. Oepen, K. Toutanova, S. Shieber, C.D. Man-
ning, D. Flickinger, and T. Brants. 2002. The
LinGO Redwoods treebank: motivation and
prliminary applications. In Proc. of COLING
2002, pages 1253?1257.
M. Palmer, J. Rosenzweig, and S. Cotton. 2001.
Automatic predicate argument analysis of the
Penn Treebank. In Proc. of HLT 2001.
D.M. Perlmutter. 1983. Studies in Relational
Grammar 1. University of Chicago Press.
O. Rambow, C. Creswell, R. Szekely, H. Taber,
and M. Walker. 2002. A dependency tree-
bank for English. In Proc. of LREC 2002,
pages 857?863.
L. Renzi, editor. 1988. Grande grammatica
italiana di consultazione, vol. I. Il Mulino,
Bologna.
W. Skut, T. Brants, B. Krenn, and H. Uszkor-
eit. 1998. A linguistically interpreted corpus
of German in newspaper texts. In Proc. of
LREC 98, pages 705?713.
O. Stock. 1989. Parsing with flexibility, dy-
namic strategies, and idioms in mind. Com-
putational Linguistics, 15(1):1?17.
L. van der Beek, G. Bouma, R. Malouf, and
G. van der Noord. 2002. The Alpino depen-
dency treebank. In Proc. of CLIN 2001.
