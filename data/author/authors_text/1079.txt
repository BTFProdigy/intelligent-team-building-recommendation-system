 Non-Verbal Cues for Discourse Structure 
Justine Cassell?, Yukiko I. Nakano?, Timothy W. Bickmore?,  
Candace L. Sidner?, and Charles Rich? 
 
?MIT Media Laboratory 
20 Ames Street 
Cambridge, MA 02139 
{justine, yukiko, bickmore}@media.mit.edu 
 
?Mitsubishi Electric Research Laboratories  
201 Broadway 
Cambridge, MA 02139 
{sidner, rich}@merl.com
Abstract 
This paper addresses the issue of 
designing embodied conversational 
agents that exhibit appropriate posture 
shifts during dialogues with human 
users.  Previous research has noted the 
importance of hand gestures, eye gaze 
and head nods in conversations 
between embodied agents and humans. 
We present an analysis of human 
monologues and dialogues that 
suggests that postural shifts can be 
predicted as a function of discourse 
state in monologues, and discourse and 
conversation state in dialogues. On the 
basis of these findings, we have 
implemented an embodied 
conversational agent that uses 
Collagen in such a way as to generate 
postural shifts.  
1. Introduction 
This paper provides empirical support for the 
relationship between posture shifts and 
discourse structure, and then derives an 
algorithm for generating posture shifts in an 
animated embodied conversational agent from 
discourse states produced by the middleware 
architecture known as Collagen [18].  Other 
nonverbal behaviors have been shown to be 
correlated with the underlying conversational 
structure and information structure of discourse.  
For example, gaze shifts towards the listener 
correlate with a shift in conversational turn 
(from the conversational participants? 
perspective, they can be seen as a signal that the 
floor is available).  Gestures correlate with 
rhematic content in accompanying language 
(from the conversational participants? 
perspective, these behaviors can be seen as a 
signal that accompanying speech is of high 
interest).  A better understanding of the role of 
nonverbal behaviors in conveying discourse 
structures enables improvements in the 
naturalness of embodied dialogue systems, such 
as embodied conversational agents, as well as 
contributing to algorithms for recognizing 
discourse structure in speech-understanding 
systems.   Previous work, however, has not 
addressed major body shifts during discourse, 
nor has it addressed the nonverbal correlates of 
topic shifts. 
2. Background 
Only recently have computational linguists 
begun to examine the association of nonverbal 
behaviors and language.  In this section we 
review research by non-computational linguists 
and discuss how this research has been 
employed to formulate algorithms for natural 
language generation or understanding. 
About three-quarters of all clauses in descriptive 
discourse are accompanied by gestures [17], and 
within those clauses, the most effortful part of 
gestures tends to co-occur with or just before the 
phonologically most prominent syllable of the 
accompanying speech [13]. It has been shown 
that when speech is ambiguous or in a speech 
situation with some noise, listeners rely on 
gestural cues [22] (and, the higher the noise-to-
signal ratio, the more facilitation by gesture). 
Even when gestural content overlaps with 
speech (reported to be the case in roughly 50% 
of utterances, for descriptive discourse), gesture 
often emphasizes information that is also 
focused pragmatically by mechanisms like 
prosody in speech.  In fact, the semantic and 
pragmatic compatibility in the gesture-speech 
relationship recalls the interaction of words and 
graphics in multimodal presentations [11]. 
On the basis of results such as these, several 
researchers have built animated embodied 
conversational agents that ally synthesized 
speech with animated hand gestures.  For 
example, Lester et al [15] generate deictic 
gestures and choose referring expressions as a 
function of the potential ambiguity and 
proximity of objects referred to.  Rickel and 
Johnson [19]'s pedagogical agent produces a 
deictic gesture at the beginning of explanations 
about objects. Andr? et al [1] generate pointing 
gestures as a sub-action of the rhetorical action 
of labeling, in turn a sub-action of elaborating.   
Cassell and Stone [3] generate either speech, 
gesture, or a combination of the two, as a 
function of the information structure status and 
surprise value of the discourse entity. 
Head and eye movement has also been examined 
in the context of discourse and conversation.   
Looking away from one?s interlocutor has been 
correlated with the beginning of turns.  From the 
speaker?s point of view, this look away may 
prevent an overload of visual and linguistic 
information. On the other hand, during the 
execution phase of an utterance, speakers look 
more often at listeners. Head nods and eyebrow 
raises are correlated with emphasized linguistic 
items ? such as words accompanied by pitch 
accents [7].  Some eye movements occur 
primarily at the ends of utterances and at 
grammatical boundaries, and appear to function 
as synchronization signals. That is, one may 
request a response from a listener by looking at 
the listener, and suppress the listener?s response 
by looking away.  Likewise, in order to offer the 
floor, a speaker may gaze at the listener at the 
end of the utterance. When the listener wants the 
floor, s/he may look at and slightly up at the 
speaker [10].  It should be noted that turn taking 
only partially accounts for eye gaze behavior in 
discourse. A better explanation for gaze 
behavior integrates turn taking with the 
information structure of the propositional 
content of an utterance [5]. Specifically, the 
beginning of themes are frequently accompanied 
by a look-away from the hearer, and the 
beginning of rhemes are frequently accompanied 
by a look-toward the hearer. When these 
categories are co-temporaneous with turn 
construction, then they are strongly predictive of 
gaze behavior.  
Results such as these have led researchers to 
generate eye gaze and head movements in 
animated embodied conversational agents.  
Takeuchi and Nagao, for example, [21] generate 
gaze and head nod behaviors in a ?talking head.?  
Cassell et al [2] generate eye gaze and head 
nods as a function of turn taking behavior, head 
turns just before an utterance, and eyebrow 
raises as a function of emphasis.   
To our knowledge, research on posture shifts 
and other gross body movements, has not been 
used in the design or implementation of 
computational systems.  In fact, although a 
number of conversational analysts and 
ethnomethodologists have described posture 
shifts in conversation, their studies have been 
qualitative in nature, and difficult to reformulate 
as the basis of algorithms for the generation of 
language and posture.  Nevertheless, researchers 
in the non-computational fields have discussed 
posture shifts extensively.  Kendon [13] reports 
a hierarchy in the organization of movement 
such that the smaller limbs such as the fingers 
and hands engage in more frequent movements, 
while the trunk and lower limbs change 
relatively rarely.   
A number of researchers have noted that 
changes in physical distance during interaction 
seem to accompany changes in the topic or in 
the social relationship between speakers.  For 
example Condon and Osgton [9] have suggested 
that in a speaking individual the changes in 
these more slowly changing body parts occur at 
the boundaries of the larger units in the flow of 
speech.  Scheflen (1973) also reports that 
posture shifts and other general body 
movements appear to mark the points of change 
between one major unit of communicative 
activity and another.   Blom & Gumperz (1972) 
identify posture changes and changes in the 
spatial relationship between two speakers as 
indicators of what they term "situational  shifts" 
-- momentary changes in the mutual rights and 
obligations between  speakers accompanied by 
shifts in language style. Erickson (1975) 
concludes that proxemic shifts seem to be 
markers of 'important' segments. In his analysis 
of college counseling interviews, they occurred 
more frequently than any other coded indicator 
of segment changes, and were therefore the best 
predictor of new segments in the data.  
Unfortunately, in none of these studies are 
statistics provided, and their analyses rely on 
intuitive definitions of discourse segment or 
?major shift?.  For this reason, we carried out 
our own empirical study. 
3. Empirical Study 
Videotaped ?pseudo-monologues? and dialogues 
were used as the basis for the current study.  In 
?pseudo-monologues,? subjects were asked to 
describe each of the rooms in their home, then 
give directions between four pairs of locations 
they knew well (e.g., home and the grocery 
store). The experimenter acted as a listener, only 
providing backchannel feedback (head nods, 
smiles and paraverbals such as "uh-huh").  For 
dialogues, two subjects were asked to generate 
an idea for a class project that they would both 
like to work on, including: 1) what they would 
work on; 2) where they would work on it 
(including facilities, etc.), and 3) when they 
would work on it. Subjects stood in both 
conditions and were told to perform their tasks 
in 5-10 minutes.  The pseudo-monologue 
condition (pseudo- because there was in fact an 
interlocutor, although he gave backchannel 
feedback only and never took the turn) allowed 
us to investigate the relationship between 
discourse structure and posture shift 
independent of turn structure.  The two tasks 
were constructed to allow us to identify exactly 
where discourse segment boundaries would be 
placed.  
The video data was transcribed and coded for 
three features: discourse segment boundaries, 
turn boundaries, and posture shifts. A discourse 
segment is taken to be an aggregation of 
utterances and sub-segments that convey the 
discourse segment purpose, which is an 
intention that leads to the segment initiation 
[12].   In this study we chose initially to look at 
high-level discourse segmentation phenomena 
rather than those discourse segments embedded 
deeper in the discourse.  Thus, the time points at 
which the assigned task topics were started 
served as segmentation points.  Turn boundaries 
were coded (for dialogues only) as the point in 
time in which the start or end of an utterance co-
occurred with a change in speaker, but excluding 
backchannel feedback. Turn overlaps were 
coded as open-floor time. We defined a posture 
shift as a motion or a position shift for a part of 
the human body, excluding hands and eyes 
(which we have dealt with in other work).  
Posture shifts were coded with start and end 
time of occurrence (duration), body part in play 
(for this paper we divided the body at the 
waistline and compared upper body vs. lower 
body shifts), and an estimated energy level of 
the posture shift. Energy level was normalized 
for each subject by taking the largest posture 
shift observed for each subject as 100% and 
coding all other posture shift energies relative to 
the 100% case.  Posture shifts that occurred as 
part of gesture or were clearly intentionally 
generated (e.g., turning one's body while giving 
directions) were not coded.  
4. Results 
Data from seven monologues and five dialogues 
were transcribed, and then coded and analyzed 
independently by two raters. A total of 70.5 
minutes of data was analyzed (42.5 minutes of 
dialogue and 29.2 minutes of monologue). A 
total of 67 discourse segments were identified 
(25 in the dialogues and 42 in the monologues), 
which constituted 407 turns in the dialogue data.  
We used the instructions given to subjects 
concerning the topics to discuss as segmentation 
boundaries.  In future research, we will address 
the smaller discourse segmentation.  For posture 
shift coding, raters coded all posture shifts 
independently, and then calculated reliability on 
the transcripts of one monologue (5.2 minutes) 
and both speakers from one dialogue (8.5 
minutes).   Agreement on the presence of an 
upper body or lower body posture shift in a 
particular location (taking location to be a 1-
second window that contains all of or a part of a 
posture shift) for these three speakers was 89% 
(kappa = .64).  For interrater reliability of the 
coding of energy level, a Spearman?s rho 
revealed a correlation coefficient of .48 (p<.01).  
4.1 Analysis 
Posture shifts occurred regularly throughout the 
data (an average of 15 per speaker in both 
pseudo-monologues and dialogues). This, 
together with the fact that the majority of time 
was spent within discourse segments and within 
turns (rather than between segments), led us to 
normalize our posture shift data for comparison 
purposes. For relatively brief intervals (inter-
discourse-segment and inter-turn) normalization 
by number of inter-segment occurrences was 
sufficient (ps/int), however, for long intervals 
(intra-discourse segment and intra-turn) we 
needed to normalize by time to obtain 
meaningful comparisons. For this normalization 
metric we looked at posture-shifts-per-second 
(ps/s).  This gave us a mean average of .06 
posture shifts/second (ps/s) in the monologues 
(SD=.07), and .07 posture shifts/second in the 
dialogues (SD=.08). 
 
Table 4.1.1. Posture WRT Discourse Segments 
Our initial analysis compared posture shifts 
made by the current speaker within discourse 
segments (intra-dseg) to those produced at the 
boundaries of discourse segments (inter-dseg). It 
can be seen (in Table 4.1.1) that posture shifts 
occur an order of magnitude more frequently at 
discourse segment boundaries than within 
discourse segments in both monologues and 
dialogues. Posture shifts also tend to be more 
energetic at discourse segment boundaries 
(F(1,251)=10.4; p<0.001). 
Table 4.1.2 Posture Shifts WRT Turns 
 ps/s ps/int energy 
inter-turn 0.140 0.268 0.742 
intra-turn 0.022  0.738 
Initially, we classified data as being inter- or 
intra-turn. Table 4.1.2 shows that turn structure 
does have an influence on posture shifts; 
subjects were five times more likely to exhibit a 
shift at a boundary than within a turn. 
 
Table 4.1.3 Posture by Discourse and Turn Breakdown 
 ps/s ps/int 
inter-dseg/start-turn 0.562 0.542 
inter-dseg/mid-turn 0.000 0.000 
inter-dseg/end-turn 0.130 0.125 
intra-dseg/start-turn 0.067 0.135 
intra-dseg/mid-turn 0.041  
intra-dseg/end-turn 0.053 0.107 
An interaction exists between turns and 
discourse segments such that discourse segment 
boundaries are ten times more likely to co-occur 
with turn changes than within turns. Both turn 
and discourse structure exhibit an influence on 
posture shifts, with discourse having the most 
predictive value. Starting a turn while starting a 
new discourse segment is marked with a posture 
shift roughly 10 times more often than when 
starting a turn while staying within discourse 
segment.  We noticed, however, that posture 
shifts appeared to congregate at the beginnings 
or ends of turn boundaries, and so our 
subsequent analyses examined start-turns, mid-
turns and end-turns. It is clear from these results 
that posture is indeed correlated with discourse 
state, such that speakers generate a posture shift 
when initiating a new discourse segment, which 
is often at the boundary between turns. 
In addition to looking at the occurrence and 
energy of posture shifts we also analyzed the 
distributions of upper vs. lower body shifts and 
the duration of posture shifts.  Speaker upper 
body shifts were found to be used more 
frequently at the start of turns (48%) than at the 
middle of turns (36%) or end of turns (18%) 
(F(2,147)=5.39; p<0.005), with no significant 
 Monologues Dialogues 
 ps/s ps/int energy ps/s ps/int energy 
inter-
dseg 
0.340 0.837 0.832 0.332 0.533 0.844 
intra-
dseg 
0.039 0.701 0.053  0.723 
dependence on discourse structure. Finally, 
speaker posture shift duration was found to 
change significantly as a function of both turn 
and discourse structure (see Figure 4.1.3). At the 
start of turns, posture shift duration is 
approximately the same whether a new topic is 
introduced or not (2.5 seconds). However, when 
ending a turn, speakers move significantly 
longer (7.0 seconds) when finishing a topic than 
when the topic is continued by the other 
interlocutor (2.7 seconds) (F(1,148)=17.9; 
p<0.001). 
Figure 4.1.1 Posture Shift Duration by DSeg and Turn 
 
5. System  
In the following sections we discuss how the 
results of the empirical study were integrated 
along with Collagen into our existent embodied 
conversational agent, Rea. 
5.1 System Architecture 
Rea is an embodied conversational agent that 
interacts with a user in the real estate agent 
domain [2]. The system architecture of Rea is 
shown in Figure 5.1. Rea takes input from a 
microphone and two cameras in order to sense 
the user?s speech and gesture. The UM 
interprets and integrates this multimodal input 
and outputs a unified semantic representation. 
The Understanding Module then sends the 
output to Collagen as the Dialogue Manager. 
Collagen, as further discussed below, maintains 
the state of the dialogue as shared between Rea 
and a user. The Reaction Module decides Rea?s 
next action based on the discourse state 
maintained by Collagen. It also assigns 
information structure to output utterances so that 
gestures can be appropriately generated.  The 
semantic representation of the action, including 
verbal and non-verbal behaviors, is sent to the 
Generation Module which generates surface 
linguistic expressions and gestures, including a 
set of instructions to achieve synchronization 
between animation and speech. These 
instructions are executed by a 3D animation 
renderer and a text-to-speech system. Table 5.1 
shows the associations between discourse and 
conversational state that Rea is currently able to 
handle. In other work we have discussed how 
Rea deals with the association between 
information structure and gesture [6]. In the 
following sections, we focus on Rea?s 
generation of posture shifts. 
 
Table 5.1:
 Discourse functions & non-verbal 
behavior cues 
Discourse 
level info. 
Functions non-verbal 
behavior cues 
Discourse 
structure 
new segment Posture_shift 
turn giving eye_gaze & 
(stop_gesturing  
hand_gesture) 
turn keeping (look_away  
keep_gesture) 
Conversation 
structure 
turn taking eye_gaze & 
posture_shift 
Information 
structure 
emphasize 
information 
eye_gaze & 
beat_and 
other_hand_gsts 
 
 
DSEG 
mid 
end 
intra inter 
8
7
6
5
4
3
2
1
start 
Understanding 
Module
Dialogue 
Manager
(Collagen)
Reaction 
Module (RM)
Animation 
Renderer 
Text to 
Speech
Speech 
Recognition
Vision 
Processing
Microphone Camera
Animation Speech 
Generation Module
Sentence 
Realizer
Gesture  
Component
Figure5.1: System architecture 
5.2 The Collagen dialogue manager 
CollagenTM is JAVA middleware for building 
COLLAborative interface AGENts to work with 
users on interface applications.  Collagen is 
designed with the capability to participate in 
collaboration and conversation, based on [12], 
[16].  Collagen updates the focus stack and 
recipe tree using a combination of the discourse 
interpretation algorithm of [16] and plan 
recognition algorithms of [14].  It takes as input 
user and system utterances and interface actions, 
and accesses a library of recipes describing 
actions in the domain.  After updating the 
discourse state, Collagen makes three resources 
available to the interface agent: focus of 
attention (using the focus stack), segmented 
interaction history (of completed segments) and 
an agenda of next possible actions created from 
the focus stack and recipe tree.  
 
5.3 Output Generation 
The Reaction Module works as a content 
planner in the Rea architecture, and also plays 
the role of an interface agent in Collagen. It has 
access to the discourse state and the agenda 
using APIs provided by Collagen. Based on the 
results reported above, we describe here how 
Rea plans her next nonverbal actions using the 
resources that Collagen maintains.  
The empirical study revealed that posture shifts 
are distributed with respect to discourse segment 
and turn boundaries, and that the form of a 
posture shift differs according to these co-
determinants. Therefore, generation of posture 
shifts in Rea is determined according to these 
two factors, with Collagen contributing 
information about current discourse state.  
5.3.1 Discourse structure information 
Any posture shift that occurs between the end of 
one discourse segment and the beginning of the 
next is defined as an inter-discourse segment 
posture shift. In order to elaborate different 
generation rules for inter- vs. intra-discourse 
segments, Rea judges (D1) whether the next 
utterance starts a new topic, or contributes to the 
current discourse purpose, (D2) whether the 
next utterance is expected to finish a segment. 
First, (D1) is calculated by referring to the focus 
stack and agenda. In planning a next action, Rea 
accesses the goal agenda in Collagen and gets 
the content of her next utterance. She also 
accesses the focus stack and gets the current 
discourse purpose that is shared between her and 
the user. By comparing the current purpose and 
the purpose of her next utterance, Rea can judge 
whether the her next utterance contributes to the 
current discourse purpose or not. For example, if 
the current discourse purpose is to find a house 
to show the user (FindHouse), and the next 
utterance that Rea plans to say is as follows, 
(1) (Ask.What (agent Propose.What (user FindHouse 
<city ?>)))  
Rea says: "What kind of transportation access do you 
need?" 
then Rea uses Collagen APIs to compare the 
current discourse purpose (FindHouse) to the 
purpose of utterance (1). The purpose of this 
utterance is to ask the value of the transportation 
parameter of FindHouse. Thus, Rea judges that 
this utterance contributes to the current 
discourse purpose, and continues the same 
discourse segment (D1 = continue).  On the 
other hand, if Rea?s next utterance is about 
showing a house,  
(2) (Propose.Should (agent ShowHouse (joint 
123ElmStreet))   
Rea says: "Let's look at 123 Elm Street." 
then this utterance does not directly contribute 
to the current discourse purpose because it does 
not ask a parameter of FindHouse, and it 
introduces a new discourse purpose ShowHouse. 
In this case, Rea judges that there is a discourse 
segment boundary between the previous 
utterance and the next one (D1 = topic change).  
In order to calculate (D2), Rea looks at the plan 
tree in Collagen, and judges whether the next 
utterance addresses the last goal in the current 
discourse purpose. If it is the case, Rea expects 
to finish the current discourse segment by the 
next utterance (D1 = finish topic).  As for 
conversational structure, Rea needs to know; 
(T1) whether Rea is taking a new turn with the 
next utterance, or keeping her current turn for 
the next utterance, (T2) whether Rea?s next 
utterance requires that the user respond.  
First, (T1) is judged by referring to the dialogue 
history1. The dialogue history stores both system 
utterances and user utterances that occurred in 
the dialogue. In the history, each utterance is 
stored as a logical form based on an artificial 
discourse language [20]. As shown above in 
utterance (1), the first argument of the action 
indicates the speaker of the utterance; in this 
example, it is ?agent?. The turn boundary can be 
estimated by comparing the speaker of the 
previous utterance with the speaker of the next 
utterance. If the speaker of the previous 
utterance is not Rea, there is a turn boundary 
before the next utterance (T1 = take turn). If the 
speaker of the previous utterance is Rea, that 
means that Rea will keep the same turn for the 
next utterance (T1 = keep turn).  
Second, (T2) is judged by looking at the type of 
Rea?s next utterance. For example, when Rea 
asks a question, as in utterance (1), Rea expects 
the user to answer the question. In this case, Rea 
must convey to the user that the system gives up 
the turn (T2 = give up turn).  
5.3.2 Deciding and selecting a posture shift 
Combining information about discourse 
structure (D1, D2) and conversation structure 
(T1, T2), the system decides on posture shifts 
                                                                 
1
 We currently maintain a dialogue history in Rea even 
though Collagen has one as well. This is in order to store 
and manipulate the information to generate hand gestures 
and assign intonational accents. This information will be 
integrated into Collagen in the near future. 
for the beginning of the utterance and the end of 
the utterance. Rea decides to do or not to do a 
posture shift by calling a probabilistic function 
that looks up the probabilities in Table 5.3.1.  
A posture shift for the beginning of the utterance 
is decided based on the combination of (D1) and 
(T1). For example, if the combined factors 
match Case (a), the system decides to generate a 
posture shift with 54% probability for the 
beginning of the utterance.  Note that in Case 
(d), that is, Rea keeps the turn without changing 
a topic, we cannot calculate a per interval 
posture shift rate. Instead, we use a posture shift 
rate normalized for time. This rate is used in the 
GenerationModule, which calculates the 
utterance duration and generates a posture shift 
during the utterance based on this posture shift 
rate.   On the other hand, ending posture shifts 
are decided based on the combination of (D2) 
and (T2).  
For example, if the combined factors match 
Case (e), the system decides to generate a 
posture shift with 0.04% probability for the 
ending of the utterance. When Rea does decide 
to activate a posture shift, she then needs to 
choose which posture shift to perform. Our 
empirical data indicates that the energy level of 
the posture shift differs depending on whether 
there is a discourse segment boundary or not. 
Moreover the duration of a posture shift differs 
depending on the place in a turn: start-, mid-, or 
end-turn. 
Posture shift selection Place of a 
posture shift Case 
Discourse 
structure 
information 
Conversation 
structure 
information 
Posture shift 
decision 
probability energy duration body part 
a 
topic 
change  take turn 0.54/int high default 
upper & 
lower 
b topic 
change keep turn 0 - - - 
c continue take turn 0.13/int low default upper or lower 
beginning of 
the utterance 
d 
D1  
continue 
T1 
keep turn 0.14/sec low short lower 
e 
finish 
topic give turn 0.04/int high long lower End of the 
utterance 
f 
D2 
continue 
T2 
give turn 0.11/int low default lower 
Table 5.3.1:Posture Decision Probabilities for Dialogue 
Based on these results, we define posture shift 
selection rules for energy, duration, and body 
part. The correspondence with discourse 
information is shown in Table 5.3.1.  For 
example, in Case (a), the system selects a 
posture shift with high energy, using both upper 
and lower body. After deciding whether or not 
Rea should shift posture and (if so) choosing a 
kind of posture shift, Rea sends a command to 
the Generation Module to generate a specific 
kind of posture shift within a specific time 
duration. 
Posture shift 
selection 
 
Ca
se 
Discourse 
structure 
information 
Posture 
shift 
decision 
probability energy 
g change topic 0.84/int high 
h 
D1 
continue 0.04/sec low 
 
Posture shifts for pseudo-monologues can be 
decided using the same mechanism as that for 
dialogue, but omitting conversation structure 
information.   The probabilities are given in 
table Table 5.3.2. For example, if Rea changes 
the topic with her next utterance, a posture shift 
is generated 84% of the time with high-energy 
motion. In other cases, the system randomly 
generates low-energy posture shifts 0.04 times 
per second.  
 
6. Example 
Figure 6.1 shows a dialogue between Rea and 
the user, and shows how Rea decides to generate 
posture shifts. This dialogue consists of two 
major segments: finding a house (dialogue), and 
showing a house (pseudo-monologue). Based on 
this task structure, we defined plan recipes for 
Collagen. The first shared discourse purpose 
[goal: HaveConversation] is introduced by the 
user before the example. Then, in utterance (1), 
the user introduces the main part of the 
conversation [goal: FindHouse].  
The next goal in the agenda, [goal: 
IdentifyPreferredCity], should be 
accomplished to identify a parameter value for 
[goal: FindHouse]. This goal directly 
contributes to the current purpose, [goal: 
FindHouse].  This case is judged to be a turn 
boundary within a discourse segment (Case (c)), 
and Rea decides to generate a posture shift at the 
beginning of the utterance with 13% probability. 
If Rea decides to shift posture she selects a low 
energy posture shift using either upper or lower 
body. In addition to a posture shift at the 
beginning of the utterance, Rea may also choose 
to generate a posture shift to end the turn. As 
utterance (2) expects the user to take the turn, 
and continue to work on the same discourse 
purpose, this is Case (f). Thus, the system 
generates an end utterance posture shift 11% of 
the time. If generated, a low energy  posture 
shift is chosen. If a beginning and/or ending 
posture shifts are generated, they are sent to the 
GM, which calculates the schedule of these 
multimodal events and generates them.  
In utterance (25), Rea introduces a new 
discourse purpose [goal : ShowHouse]. Rea, 
using a default rule, decides to take the initiative 
on this goal.  At this point, Rea accesses the 
discourse state and confirms that a new goal is 
about to start.  Rea judges this case as a 
discourse segment boundary and also a turn 
boundary (Case (a)). Based on this information, 
Rea selects a high energy posture shift.  An 
example of Rea?s high energy posture shift is 
shown on the right in Figure 5.2. 
As a subdialogue of showing a house, in a 
discourse purpose [goal : DiscussFeature], Rea 
keeps the turn and continues to describe the 
house. We handle this type of interaction as a 
pseudo-monologue. Therefore, we can use table 
Table 5.3.2 for deciding on posture shifts here. 
In utterance (27), Rea starts the discussion about 
the house, and takes the initiative. This is judged 
as Case (g), and a high energy body motion is 
generated 84% of the time. 
Table 5.3.2: Posture Decision Probabilities: Monologue 
  
7. Conclusion and Further work 
We have demonstrated a clear relationship 
between nonverbal behavior and discourse state, 
and shown how this finding can be incorporated 
into the generation of language and nonverbal 
behaviors for an embodied conversational agent. 
Speakers produce posture shifts at 53% of 
discourse segment boundaries, more frequently 
than they produce those shifts discourse 
segment-internally, and with more motion 
energy.  Furthermore, there is a relationship 
between discourse structure and conversational 
structure such that when speakers initiate a new 
segment at the same time as starting a turn (the 
most frequent case by far), they are more likely 
to produce a posture shift; while when they end 
a discourse segment and a turn at the same time, 
their posture shifts last longer than when these 
categories do not co-occur. 
Although this paper reports results from a 
limited number of monologues and dialogues, 
the findings are promising.  In addition, they 
point the way to a number of future directions, 
both within the study of posture and discourse, 
and more generally within the study of non-
verbal behaviors in computational linguistics. 
 
Figure 6.2: Rea demonstrating a low and high energy 
posture shift 
First, given the relationship between 
conversational and information structure in [5], 
a natural next step is to examine the three-way 
relationship between discourse state, 
conversational structure (turns), and information 
structure (theme/rheme).  For the moment, we 
have demonstrated that posture shifts may signal 
boundaries of units; do they also signal the 
information content of units? Next, we need to 
look at finer segmentations of the discourse, to 
see whether larger and smaller discourse 
segments are distinguished through non-verbal 
means.  Third, the question of listener posture is 
an important one.  We found that a number of 
posture shifts were produced by the participant 
who was not speaking.  More than half of these 
shifts were produced at the same time as a 
speaker shift, suggesting a kind of mirroring.  In 
order to interpret these data, however, a more 
sensitive notion of turn structure is required, as 
one must be ready to define when exactly 
speakers and listeners shift roles. Also, of 
course, evaluation of the importance of such 
nonverbal behaviors to user interaction is 
essential.  In a user study of our earlier Gandalf 
system [4], users rated the agent's language 
skills significantly higher under test conditions 
in which Gandalf deployed conversational 
behaviors (gaze, head movement and limited 
gesture) than when these behaviors were 
disabled.  Such an evaluation is also necessary 
for the Rea-posture system.  But, more 
generally, we need to test whether generating 
posture shifts of this sort actually serves as a 
signal to listeners, for example to initiative 
 
[Finding a house] < dialogue> 
  
(1) 
  
U: I?m looking for a house. 
  
(2) 
  
R:  (c)   Where do you want to live? (f)  (3) 
  
U: I like Boston. 
  
(4) 
  
R:  (c) (d)  What kind of transportation  
access do you need? (f)   
(5) 
  
U: I need T access. 
  
 ?.  
(23) 
  
R:  (c) (d)  How much storage space do  
you need?  (f)  
(24) 
  
U: I need to have a storage place in the  
basement. 
  
(25) 
  
R:  (a) (d) 
  
Let?s look at 123 Elm Street. (f) 
  
(26) 
  
U: OK. 
  
[Discuss a feature of the house] 
  
(27) 
  
R:  (g)  Let's discuss a feature of this place. 
  
(28) 
  
R:  (h)  Notice the hardw ood flooring in the  
living room. 
  
(29) 
  
R:  (h)  Notice the jacuzzi. 
  
(30) 
  
R:  (h) Notice the remodeled kitchen 
  
[Showing a house] <Pseudo-monologue> 
  Figure 6.1: Example dialogue 
structure in task and dialogue [8]. These 
evaluations form part of our future research 
plans. 
8. Acknowledgements 
This research was supported by MERL, France 
Telecom, AT&T, and the other generous sponsors of 
the MIT Media Lab.  Thanks to the other members of 
the Gesture and Narrative Language Group, in 
particular Ian Gouldstone and Hannes Vilhj?lmsson. 
9. REFERENCES  
[1] Andre, E., Rist, T., & Muller, J., Employing AI 
methods to control the behavior of animated 
interface agents, Applied Artificial Intelligence, 
vol. 13, pp. 415-448, 1999. 
[2] Cassell, J., Bickmore, T., Billinghurst, M., 
Campbell, L., Chang, K., Vilhjalmsson, H., & 
Yan, H., Embodiment in Conversational 
Interfaces: Rea, Proc. of CHI 99, Pittsburgh, PA, 
ACM, 1999. 
[3] Cassell, J., Stone, M., & Yan, H., Coordination 
and context-dependence in the generation of 
embodied conversation, Proc. INLG 2000, 
Mitzpe Ramon, Israel, 2000. 
[4] Cassell, J. and Thorisson, K. R., The Power of a 
Nod and a Glance: Envelope vs. Emotional 
Feedback in Animated Conversational Agents, 
Applied Art. Intell., vol. 13, pp. 519-538, 1999. 
[5] Cassell, J., Torres, O., & Prevost, S., Turn 
Taking vs. Discourse Structure: How Best to 
Model Multimodal Conversation., in Machine 
Conversations, Y. Wilks, Ed. The Hague: 
Kluwer, 1999, pp. 143-154. 
[6] Cassell, J., Vilhj?lmsson, H., & Bickmore, T., 
BEAT: The Behavior Expression Animation 
Toolkit, Proc. of SIGGRAPH, ACM Press, 
2001. 
[7] Chovil, N., Discourse-Oriented Facial Displays 
in Conversation, Research on Language and 
Social Interaction, vol. 25, pp. 163-194, 1992. 
[8] Chu-Carroll, J. & Brown, M., Initiative in 
Collaborative Interactions - Its Cues and Effects, 
Proc. of AAAI Spring 1997 Symp. on 
Computational Models of Mixed Initiative, 
1997. 
[9] Condon, W. S. & Osgton, W. D., Speech and 
body motion synchrony of the speaker-hearer, in 
The perception of language, D. Horton & J. 
Jenkins, Eds. NY: Academic Press, 1971, pp. 
150-184. 
[10] Duncan, S., On the structure of speaker-auditor 
interaction during speaking turns, Language in 
Society, vol. 3, pp. 161-180, 1974. 
[11] Green, N., Carenini, G., Kerpedjiev, S., & Roth, 
S, A Media-Independent Content Language for 
Integrated Text and Graphics Generation, Proc. 
of Workshop on Content Visualization and 
Intermedia Representations at COLING and 
ACL '98, 1998. 
[12] Grosz, B. & Sidner, C., Attention, Intentions, 
and the Structure of Discourse, Computational 
Linguistics, vol. 12, pp. 175-204, 1986. 
[13] Kendon, A., Some Relationships between Body 
Motion and Speech, in Studies in Dyadic 
Communication, A. W. Siegman and B. Pope, 
Eds. Elmsford, NY: Pergamon Press, 1972, pp. 
177-210. 
[14] Lesh, N., Rich, C., & Sidner, C., Using Plan 
Recognition in Human-Computer Collaboration, 
Proc. of the Conference on User Modelling, 
Banff, Canada, NY: Springer Wien, 1999. 
[15] Lester, J., Towns, S., Callaway, C., Voerman, J., 
& FitzGerald, P., Deictic and Emotive 
Communication in Animated Pedagogical 
Agents, in Embodied Conversational Agents, J. 
Cassell, J. Sullivan, et. al, Eds. Cambridge: MIT 
Press, 2000. 
[16] Lochbaum, K., A Collaborative Planning Model 
of Intentional Structure, Computational 
Linguistics, vol. 24, pp. 525-572, 1998. 
[17] McNeill, D., Hand and Mind: What Gestures 
Reveal about Thought. Chicago, IL/London, 
UK: The University of Chicago Press, 1992. 
[18] Rich, C. & Sidner, C. L., COLLAGEN: A 
Collaboration Manager for Software Interface 
Agents, User Modeling and User-Adapted 
Interaction, vol. 8, pp. 315-350, 1998. 
 [19] Rickel, J. & Johnson, W. L., Task-Oriented 
Collaboration with Embodied Agents in Virtual 
Worlds, in Embodied Conversational Agents, J. 
Cassell, Ed. Cambridge, MA: MIT Press, 2000. 
[20] Sidner, C., An Artificial Discourse Language for 
Collaborative Negotiation, Proc. of 12th Intnl. 
Conf. on Artificial Intelligence (AAAI), Seattle, 
WA, MIT Press, 1994. 
[21] Takeuchi, A. & Nagao, K., Communicative 
facial displays as a new conversational modality, 
Proc. of InterCHI '93, Amsterdam, NL, ACM, 
1993. 
[22] Thompson, L. and Massaro, D., Evaluation and 
Integration of Speech and Pointing Gestures 
during Referential Understanding, Journal of 
Experimental Child Psychology, vol. 42, pp. 
144-168, 1986. 
Towards a Model of Face-to-Face Grounding 
Yukiko I. Nakano?/??   Gabe Reinstein?   Tom Stocky?   Justine Cassell? 
?MIT Media Laboratory 
E15-315 
20 Ames Street 
Cambridge, MA 02139 USA 
{yukiko, gabe, tstocky, justine}@media.mit.edu 
 
??Research Institute of Science and 
Technology for Society (RISTEX) 
2-5-1 Atago Minato-ku, 
Tokyo 105-6218, Japan 
nakano@kc.t.u-tokyo.ac.jp
Abstract 
We investigate the verbal and nonverbal 
means for grounding, and propose a design 
for embodied conversational agents that re-
lies on both kinds of signals to establish 
common ground in human-computer inter-
action. We analyzed eye gaze, head nods 
and attentional focus in the context of a di-
rection-giving task. The distribution of 
nonverbal behaviors differed depending on 
the type of dialogue move being grounded, 
and the overall pattern reflected a monitor-
ing of lack of negative feedback. Based on 
these results, we present an ECA that uses 
verbal and nonverbal grounding acts to up-
date dialogue state. 
1 Introduction 
An essential part of conversation is to ensure that 
the other participants share an understanding of 
what has been said, and what is meant.  The proc-
ess of ensuring that understanding ? adding what 
has been said to the common ground ? is called 
grounding [1]. In face-to-face interaction, nonver-
bal signals as well as verbal participate in the 
grounding process, to indicate that an utterance is 
grounded, or that further work is needed to ground. 
Figure 1 shows an example of human face-to-face 
conversation. Even though no verbal feedback is 
provided, the speaker (S) continues to add to the 
directions. Intriguingly, the listener gives no ex-
plicit nonverbal feedback ? no nods or gaze to-
wards S. S, however, is clearly monitoring the 
listener?s behavior, as we see by the fact that S 
looks at her twice (continuous lines above the 
words). In fact, our analyses show that maintaining 
focus of attention on the task (dash-dot lines un-
derneath the words) is the listener?s public signal 
of understanding S?s utterance sufficiently for the 
task at hand.  Because S is manifestly attending to 
this signal, the signal allows the two jointly to rec-
ognize S?s contribution as grounded. This paper 
provides empirical support for an essential role for 
nonverbal behaviors in grounding, motivating an 
architecture for an embodied conversational agent 
that can establish common ground using eye gaze, 
head nods, and attentional focus.   
Although grounding has received significant at-
tention in the literature, previous work has not ad-
dressed the following questions: (1) what 
predictive factors account for how people use non-
verbal signals to ground information, (2) how can a 
model of the face-to-face grounding process be 
used to adapt dialogue management to face-to-face 
conversation with an embodied conversational 
agent. This paper addresses these issues, with the 
goal of contributing to the literature on discourse 
phenomena, and of building more advanced con-
versational humanoids that can engage in human 
conversational protocols.  
In the next section, we discuss relevant previous 
work, report results from our own empirical study 
and, based on our analysis of conversational data, 
propose a model of grounding using both verbal 
and nonverbal information, and present our im-
plementation of that model into an embodied con-
versational agent. As a preliminary evaluation, we 
compare a user interacting with the embodied con-
versational agent with and without grounding.  
Figure 1: Human face-to-face conversation 
[580] S: Go    to    the    fourth    floor,
[590] S: hang    a    left,
[600] S: hang    another    left. 
look at map gaze at listener
gaze at listener
look at map
look at map
look at map
look at map
speaker?s behavior
listener?s behavior
2 Related Work 
Conversation can be seen as a collaborative activ-
ity to accomplish information-sharing and to pur-
sue joint goals and tasks.  Under this view, 
agreeing on what has been said, and what is meant, 
is crucial to conversation.  The part of what has 
been said that the interlocutors understand to be 
mutually shared is called the common ground, and 
the process of establishing parts of the conversa-
tion as shared is called grounding [1]. As [2] point 
out, participants in a conversation attempt to 
minimize the effort expended in grounding.  Thus, 
interlocutors do not always convey all the informa-
tion at their disposal; sometimes it takes less effort 
to produce an incomplete utterance that can be re-
paired if needs be. 
[3] has proposed a computational approach to 
grounding where the status of contributions as 
provisional or shared is part of the dialogue 
system?s representation of the ?information state? 
of the conversation. Conversational actions can 
trigger updates that register provisional 
information as shared. These actions achieve 
grounding.  Acknowledgment acts are directly as-
sociated with grounding updates while other utter-
ances effect grounding updates indirectly, because 
they proceed with the task in a way that presup-
poses that prior utterances are uncontroversial. 
[4], on the other hand, suggest that actions in 
conversation give probabilistic evidence of under-
standing, which is represented on a par with other 
uncertainties in the dialogue system (e.g., speech 
recognizer unreliability).  The dialogue manager 
assumes that content is grounded as long as it 
judges the risk of misunderstanding as acceptable. 
[1, 5] mention that eye gaze is the most basic 
form of positive evidence that the addressee is at-
tending to the speaker, and that head nods have a 
similar function to verbal acknowledgements. They 
suggest that nonverbal behaviors mainly contribute 
to lower levels of grounding, to signify that inter-
locutors have access to each other?s communica-
tive actions, and are attending.  With a similar goal 
of broadening the notion of communicative action 
beyond the spoken word, [6] examine other kinds 
of multimodal grounding behaviors, such as post-
ing information on a whiteboard.  Although these 
and other researchers have suggested that nonver-
bal behaviors undoubtedly play a role in grounding, 
previous literature does not characterize their pre-
cise role with respect to dialogue state.  
On the other hand, a number of studies on these 
particular nonverbal behaviors do exist. An early 
study, [7], reported that conversation involves eye 
gaze about 60% of the time. Speakers look up at 
grammatical pauses for feedback on how utter-
ances are being received, and also look at the task. 
Listeners look at speakers to follow their direction 
of gaze. In fact, [8] claimed speakers will pause 
and restart until they obtain the listener?s gaze.  [9] 
found that during conversational difficulties, mu-
tual gaze was held longer at turn boundaries. 
Previous work on embodied conversational 
agents (ECAs) has demonstrated that it is possible 
to implement face-to-face conversational protocols 
in human-computer interaction, and that correct 
relationships among verbal and nonverbal signals 
enhances the naturalness and effectiveness of em-
bodied dialogue systems [10], [11]. [12] reported 
that users felt the agent to be more helpful, lifelike, 
and smooth in its interaction style when it demon-
strated nonverbal conversational behaviors.  
3 Empirical Study 
In order to get an empirical basis for modeling 
face-to-face grounding, and implementing an ECA, 
we analyzed conversational data in two conditions. 
3.1 Experiment Design 
Based on previous direction-giving tasks, students 
from two different universities gave directions to 
campus locations to one another.  Each pair had a 
conversation in a (1) Face-to-face condition 
(F2F): where two subjects sat with a map drawn 
by the direction-giver sitting between them, and in 
a (2) Shared Reference condition (SR): where an 
L-shaped screen between the subjects let them 
share a map drawn by the direction-giver, but not 
to see the other?s face or body. 
Interactions between the subjects were video-
recorded from four different angles, and combined 
by a video mixer into synchronized video clips.  
3.2 Data Coding 
10 experiment sessions resulted in 10 dialogues per 
condition (20 in total), transcribed as follows. 
Coding verbal behaviors: As grounding oc-
curs within a turn, which consists of consecutive 
utterances by a speaker, following [13] we token-
ized a turn into utterance units (UU), correspond-
ing to a single intonational phrase [14]. Each UU 
was categorized using the DAMSL coding scheme 
[15]. In the statistical analysis, we concentrated on 
the following four categories with regular occur-
rence in our data: Acknowledgement, Answer, In-
formation request (Info-req), and Assertion.  
Coding nonverbal behaviors: Based on previ-
ous studies, four types of behaviors were coded: 
Gaze At Partner (gP): Looking at the partner?s 
eyes, eye region, or face. 
Gaze At Map (gM): Looking at the map 
Gaze Elsewhere (gE): Looking away elsewhere 
Head nod (Nod): Head moves up and down in a 
single continuous movement on a vertical axis, 
but eyes do not go above the horizontal axis. 
By combining Gaze and Nod, six complex catego-
ries (ex. gP with nod, gP without nod, etc) are gen-
erated.  In what follows, however, we analyze only 
categories with more than 10 instances. In order to 
analyze dyadic behavior, 16 combinations of the 
nonverbal behaviors are defined, as shown in Table 
1. Thus, gP/gM stands for a combination of 
speaker gaze at partner and listener gaze at map. 
Results 
We examine differences between the F2F and SR 
conditions, correlate verbal and nonverbal behav-
iors within those conditions, and finally look at 
correlations between speaker and listener behavior. 
Basic Statistics: The analyzed corpus consists 
of 1088 UUs for F2F, and 1145 UUs for SR. The 
mean length of conversations in F2F is 3.24 min-
utes, and in SR is 3.78 minutes  (t(7)=-1.667 p<.07 
(one-tail)). The mean length of utterances in F2F 
(5.26 words per UU) is significantly longer than in 
SR (4.43 words per UU) (t(7)=3.389 p< .01 (one-
tail)). For the nonverbal behaviors, the number of 
shifts between the statuses in Table 1 was com-
pared (eg. NV status shifts from gP/gP to gM/gM 
is counted as one shift). There were 887 NV status 
shifts for F2F, and 425 shifts for SR. The number 
of NV status shifts in SR is less than half of that in 
F2F (t(7)=3.377 p< .01 (one-tail)).  
These results indicate that visual access to the 
interlocutor?s body affects the conversation, sug-
gesting that these nonverbal behaviors are used as 
communicative signals. In SR, where the mean 
length of UU is shorter, speakers present informa-
tion in smaller chunks than in F2F, leading to more 
chunks and a slightly longer conversation. In F2F, 
on the other hand, conversational participants con-
vey more information in each UU. 
Correlation between verbal and nonverbal 
behaviors: We analyzed NV status shifts with re-
spect to the type of verbal communicative action 
and the experimental condition (F2F/SR). To look 
at the continuity of NV status, we also analyzed the 
amount of time spent in each NV status. For gaze, 
transition and time spent gave similar results; since 
head nods are so brief, however, we discuss the 
data in terms of transitions. Table 2 shows the most 
frequent target NV status (shift to these statuses from 
others) for each speech act type in F2F. Numbers in 
parentheses indicates the proportion to the total num-
ber of transitions.  
<Acknowledgement> Within an UU, the 
dyad?s NV status most frequently shifts to 
gMwN/gM (eg. speaker utters ?OK? while nodding, 
and listener looks at the map). At pauses, a shift to 
gMgM is most frequent. The same results were 
found in SR where the listener could not see the 
speaker?s nod. These findings suggest that Ac-
knowledgement is likely to be accompanied by a 
head nod, and this behavior may function intro-
spectively, as well as communicatively. 
<Answer> In F2F, the most frequent shift 
within a UU is to gP/gP. This suggests that speak-
ers and listeners rely on mutual gaze (gP/gP) to 
ensure an answer is grounded, whereas they cannot 
use this strategy in SR. In addition, we found that 
Table 1: NV statuses 
Listener?s behavior Combinations of 
NVs gP gM gMwN gE 
gP gP/gP gP/gM gP/gMwN gP/gE 
gM gM/gP gM/gM gM/gMwN gM/gE 
gMwN gMwN/gP gMwN/gM gMwN/gMwN gMwN/gE
 
Speaker?s 
behavior 
gE gE/gP gE/gM gE/gMwN gE/gE 
 
Shift to  
within UU pause 
Acknowledgement gMwN/gM (0.495) gM/gM (0.888) 
Answer gP/gP (0.436) gM/gM (0.667) 
Info-req gP/gM (0.38) gP/gP (0.5) 
Assertion gP/gM (0.317) gM/gM (0.418) 
 Table 2: Salient transitions 
speakers frequently look away at the beginning of 
an answer, as they plan their reply [7]. 
<Info-req> In F2F, the most frequent shift 
within a UU is to gP/gM, while at pauses between 
UUs shift to gP/gP is the most frequent. This sug-
gests that speakers obtain mutual gaze after asking 
a question to ensure that the question is clear, be-
fore the turn is transferred to the listener to reply. 
In SR, however, rarely is there any NV status shift, 
and participants continue looking at the map. 
<Assertion> In both conditions, listeners look 
at the map most of the time, and sometimes nod. 
However, speakers? nonverbal behavior is very 
different across conditions. In SR, speakers either 
look at the map or elsewhere. By contrast, in F2F, 
they frequently look at the listener, so that a shift 
to gP/gM is the most frequent within an UU. This 
suggests that, in F2F, speakers check whether the 
listener is paying attention to the referent men-
tioned in the Assertion. This implies that not only 
listener?s gazing at the speaker, but also paying 
attention to a referent works as positive evidence 
of understanding in F2F. 
In summary, it is already known that eye gaze 
can signal a turn-taking request [16], but turn-
taking cannot account for all our results. Gaze di-
rection changes within as well as between UUs, 
and the usage of these nonverbal behaviors differs 
depending on the type of conversational action. 
Note that subjects rarely demonstrated communica-
tion failures, implying that these nonverbal behaviors 
represent positive evidence of grounding. 
Correlation between speaker and listener 
behavior: Thus far we have demonstrated a differ-
ence in distribution among nonverbal behaviors, 
with respect to conversational action, and visibility 
of interlocutor.  But, to uncover the function of 
these nonverbal signals, we must examine how 
listener?s nonverbal behavior affects the speaker?s 
following action. Thus, we looked at two consecu-
tive Assertion UUs by a direction-giver, and ana-
lyzed the relationship between the NV status of the 
first UU and the direction-giving strategy in the 
second UU. The giver?s second UU is classified as 
go-ahead if it gives the next leg of the directions, 
or as elaboration if it gives additional information 
about the first UU, as in the following example: 
[U1]S: And then, you?ll go  
 down this little corridor. 
[U2]S: It?s not very long. 
Results are shown in Figure 2. When the listener 
begins to gaze at the speaker somewhere within an 
UU, and maintains gaze until the pause after the 
UU, the speaker?s next UU is an elaboration of the 
previous UU 73% of the time. On the other hand, 
when the listener keeps looking at the map during 
an UU, only 30% of the next UU is an elaboration 
(z = 3.678, p<.01). Moreover, when a listener 
keeps looking at the speaker, the speaker?s next 
UU is go-ahead only 27% of the time. In contrast, 
when a listener keeps looking at the map, the 
speaker?s next UU is go-ahead 52% of the time (z 
= -2.049, p<.05)1. These results suggest that speak-
ers interpret listeners? continuous gaze as evidence 
of not-understanding, and they therefore add more 
information about the previous UU. Similar find-
ings were reported for a map task by [17] who 
suggested that, at times of communicative diffi-
culty, interlocutors are more likely to utilize all the 
channels available to them. In terms of floor man-
agement, gazing at the partner is a signal of giving 
up a turn, and here this indicates that listeners are 
trying to elicit more information from the speaker.  
In addition, listeners? continuous attention to the 
map is interpreted as evidence of understanding, 
and speakers go ahead to the next leg of the direc-
tion2. 
3.3 A Model of Face-to-Face Grounding 
Analyzing spoken dialogues, [18] reported that 
grounding behavior is more likely to occur at an 
                                                          
1 The percentage for map does not sum to 100% because some 
of the UUs are cue phrases or tag questions which are part of 
the next leg of the direction, but do not convey content. 
2 We also analyzed two consecutive Answer UUs from a giver, 
and found that when the listener looks at the speaker at a 
pause, the speaker elaborates the Answer 78% of the time. 
When the listener looks at the speaker during the UU and at 
the map after the UU (positive evidence), the speaker  elabo-
rates only 17% of the time. 
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
gaze map
elaboration
go-ahead
Figure 2: Relationship between receiver?s NV and 
giver?s next verbal behavior 
intonational boundary, which we use to identify 
UUs. This implies that multiple grounding behav-
iors can occur within a turn if it consists of multi-
ple UUs. However, in previous models, 
information is grounded only when a listener re-
turns verbal feedback, and acknowledgement 
marks the smallest scope of grounding.  If we ap-
ply this model to the example in Figure 1, none of 
the UU have been grounded because the listener 
has not returned any spoken grounding clues. 
In contrast, our results suggest that considering 
the role of nonverbal behavior, especially eye-gaze, 
allows a more fine-grained model of grounding, 
employing the UU as a unit of grounding.  
Our results also suggest that speakers are ac-
tively monitoring positive evidence of understand-
ing, and also the absence of negative evidence of 
understanding (that is, signs of miscommunication).  
When listeners continue to gaze at the task, speak-
ers continue on to the next leg of directions.   
Because of the incremental nature of grounding, 
we implement nonverbal grounding functionality 
into an embodied conversational agent using a 
process model that describes steps for a system to 
judge whether a user understands system contribu-
tion: (1) Preparing for the next UU: according to 
the speech act type of the next UU, nonverbal posi-
tive or negative evidence that the agent expects to 
receive are specified. (2) Monitoring: monitors and 
checks the user?s nonverbal status and signals dur-
ing the UU. After speaking, the agent continues 
monitoring until s/he gets enough evidence of un-
derstanding or not-understanding represented by 
user?s nonverbal status and signals.(3) Judging: 
once the agent gets enough evidence, s/he tries to 
judge groundedness as soon as possible. According 
to some previous studies, length of pause between 
UUs is in between 0.4 to 1 sec [18, 19]. Thus, time 
out for judgment is 1 sec after the end of the UU. If 
the agent does not have evidence then, the UU re-
mains ungrounded.  
This model is based on the information state 
approach [3], with update rules that revise the state 
of the conversation based on the inputs the system 
receives.  In our case, however, the inputs are sam-
pled continuously, include the nonverbal state, and 
only some require updates.  Other inputs indicate 
that the last utterance is still pending, and allow the 
agent to wait further.  In particular, task attention 
over an interval following the utterance triggers 
grounding.  Gaze in the interval means that the 
contribution stays provisional, and triggers an ob-
ligation to elaborate.  Likewise, if the system 
times-out without recognizing any user feedback, 
the segment remains ungrounded.  This process 
allows the system to keep talking across multiple 
utterance units without getting verbal feedback 
from the user. From the user?s perspective, explicit 
acknowledgement is not necessary, and minimal 
cost is involved in eliciting elaboration. 
4 Face-to-face Grounding with ECAs 
Based on our empirical results, we propose a dia-
logue manager that can handle nonverbal input to 
the grounding process, and we implement the 
mechanism in an embodied conversational agent.  
4.1 System 
MACK is an interactive public information ECA 
kiosk.  His current knowledgebase concerns the 
activities of the MIT Media Lab; he can answer 
questions about the lab?s research groups, projects, 
and demos, and give directions to each. 
On the input side, MACK recognizes three mo-
dalities: (1) speech, using IBM?s ViaVoice, (2) pen 
gesture via a paper map atop a table with an em-
bedded Wacom tablet, and (3) head nod and eye 
gaze via a stereo-camera-based 6-degree-of-
freedom head-pose tracker (based on [20]).  These 
inputs operate as parallel threads, allowing the Un-
derstanding Module (UM) to interpret the multiple 
modalities both individually and in combination. 
MACK produces multimodal output as well: (1) 
speech synthesis using the Microsoft Whistler 
Text-to-Speech (TTS) API, (2) a graphical figure 
with synchronized hand and arm gestures, and 
head and eye movements, and (3) LCD projector 
highlighting on the paper map, allowing MACK to 
reference it. 
The system architecture is shown in Figure 3.  
The UM interprets the input modalities and con-
verts them to dialogue moves which it then passes 
on to the Dialogue Manager (DM).  The DM con-
sists of two primary sub-modules, the Response 
Planner, which determines MACK?s next action(s) 
and creates a sequence of utterance units, and the 
Grounding Module (GrM), which updates the Dis-
course Model and decides when the Response 
Planner?s next UU should be passed on to the Gen-
eration module (GM).  The GM converts the UU 
into speech, gesture, and projector output, sending 
these synchronized modalities to the TTS engine, 
Animation Module (AM), and Projector Module.  
The Discourse Model maintains information 
about the state and history of the discourse.  This 
includes a list of grounded beliefs and ungrounded 
UUs; a history of previous UUs with timing infor-
mation; a history of nonverbal information (di-
vided into gaze states and head nods) organized by 
timestamp; and information about the state of the 
dialogue, such as the current UU under considera-
tion, and when it started and ended. 
4.2 Nonverbal Inputs 
Eye gaze and head nod inputs are recognized by a 
head tracker, which calculates rotations and trans-
lations in three dimensions based on visual and 
depth information taken from two cameras [20].  
The calculated head pose is translated into ?look at 
MACK,? ?look at map,? or ?look elsewhere.? The 
rotation of the head is translated into head nods, 
using a modified version of [21].  Head nod and 
eye gaze events are timestamped and logged within 
the nonverbal component of the Discourse History.  
The Grounding Module can thus look up the ap-
propriate nonverbal information to judge a UU. 
4.3 The Dialogue Manager 
In a kiosk ECA, the system needs to ensure that the 
user understands the information provided by the 
agent.  For this reason, we concentrated on imple-
menting a grounding mechanism for Assertion, 
when the agent gives the user directions, and An 
swer, when the agent answers the user?s questions 
Generating the Response 
The first job of the DM is to plan the response to a 
user?s query.  When a user asks for directions, the 
DM receives an event from the UM stating this 
intention.  The Response Planner in the DM, rec-
ognizing the user?s direction-request, calculates the 
directions, broken up into segments.  These seg-
ments are added to the DM?s Agenda, the stack of 
UUs to be processed. 
At this point, the GrM sends the first UU (a di-
rection segment) on the Agenda to the GM to be 
processed.  The GM converts the UU into speech 
and animation commands.  For MACK?s own non-
verbal grounding acts, the GM determines 
MACK?s gaze behavior according to the type of 
UU. For example, when MACK generates a direc-
tion segment (an Assertion), 66% of the time he 
keeps looking at the map.  When elaborating a 
previous UU, 47% of the time he gazes at the user.  
When the GM begins to process the UU, it logs 
the start time in the Discourse Model, and when it 
finishes processing (as it sends the final command 
to the animation module), it logs the end time.  The 
GrM waits for this speech and animation to end 
(by polling the Discourse Model until the end time 
is available), at which point it retrieves the timing 
data for the UU, in the form of timestamps for the 
UU start and finish.  This timing data is used to 
look up the nonverbal behavior co-occurring with 
the utterance in order to judge whether or not the 
UU was grounded. 
Judgment of grounding  
When MACK finishes uttering a UU, the Ground-
ing Module judges whether or not the UU is 
grounded, based on the user?s verbal and nonverbal 
behaviors during and after the UU.   
Using verbal evidence:  If the user returns an 
acknowledgement, such as ?OK?, the GrM judges 
the UU grounded.  If the user explicitly reports 
failure in perceiving MACK?s speech (ex. 
?what??), or not-understanding (ex. ?I don?t un-
derstand?), the UU remains ungrounded.  Note 
that, for the moment, verbal evidence is considered 
stronger than nonverbal evidence. 
Using nonverbal evidence:  The GrM looks up 
the nonverbal behavior occurring during the utter-
ance, and compares it to the model shown in Table 
3.  For each type of speech act, this model specifies 
the nonverbal behaviors that signal positive or ex-
plicit negative evidence.  First, the GrM compares 
the within-UU nonverbal behavior to the model.  
Then, it looks at the first nonverbal behavior oc-
curring during the pause after the UU.  If these two 
behaviors (?within? and ?pause?) match a pattern 
that signals positive evidence, the UU is grounded.  
If they match a pattern for negative evidence, the 
UU is not grounded.  If no pattern has yet been 
Figure 3: MACK system architecture
matched, the GrM waits for a tenth of a second and 
checks again.  If the required behavior has oc-
curred during this time, the UU is judged.  If not, 
the GrM continues looping in this manner until the 
UU is either grounded or ungrounded explicitly, or 
a 1 second threshold has been reached.  If the 
threshold is reached without a decision, the GrM 
times out and judges the UU ungrounded. 
Updating the Dialogue State 
 After judging grounding, the GrM updates the 
Discourse Model. The Discourse State maintained 
in the Discourse Model is similar to TRINDI kit 
[3], except that we store nonverbal information.  
There are three key fields: (1) a list of grounded 
UUs, (2) a list of pending (ungrounded) UUs, and 
(3) the current UU. If the current UU is judged 
grounded, its belief is added to (1).  If ungrounded, 
the UU is stored in (2). If an UU has subsequent 
contributions such as elaboration, these are stored 
in a single discourse unit, and grounded together 
when the last UU is grounded.  
Determining the Next Action 
After judging the UU?s grounding, the GrM de-
cides what MACK does next. (1) MACK can con-
tinue giving the directions as normal, by sending 
on the next segment in the Agenda to the GM.  As 
shown in Table 3, this happens 70% of the time 
when the UU is grounded, and only 27% of the 
time when it is not grounded.  Note, this happens 
100% of the time if verbal acknowledgement (e.g. 
?Uh huh?) is received for the UU.  
(2) MACK can elaborate on the most recent 
stage of the directions.  Elaborations are generated 
73% of the time when an Assertion is judged un-
grounded, and 78% of the time for an ungrounded 
Answer.  MACK elaborates by describing the most 
recent landmark in more detail.  For example, if 
the directions were ?Go down the hall and make a 
right at the door,? he might elaborate by saying 
?The big blue door.?  In this case, the GrM asks 
the Response Planner (RP) to provide an elabora-
tion for the current UU; the RP generates this 
elaboration (looking up the landmark in the data-
base) and adds it to the front of the Agenda; and 
the GrM sends this new UU on to the GM. 
Finally, if the user gives MACK explicit verbal 
evience of not understanding, MACK will simply 
repeat the last thing he said, by sending the UU 
back to the GM. 
4.4 Example 
Figure 4 shows an example of a user's interaction 
with MACK.  The user asks MACK for directions, 
and MACK replies using speech and pointing (us-
ing a projector) to the shared map. 
When the GrM sends the first segment in the 
Agenda to the GM,the starting time of the UU is 
noted and it is sent to the AM to be spoken and 
animated. During this time, the user?s nonverbal 
signals are logged in the Discourse Model. When 
the UU has finished, the GrM evaluates the log of 
the UU and of the very beginning of the pause (by 
waiting a tenth of a second and then checking the 
nonverbal history). In this case, MACK noted that 
the user looked at the map during the UU, and con-
tinued to do so just afterwards. This pattern 
matches the model for Assertion. The UU is 
judged as grounded, and the grounded belief is 
added to the Discourse Model. 
MACK then utters the second segment as be-
fore, but this time the GrM, finds that the user was 
looking up at MACK during most of the UU as 
well as after it, which signals that the UU is not 
grounded.  Therefore, the RP generates an elabora-
tion (line 4).  This utterance is judged to be 
Table 3: Grounding Model for MACK 
Target UU 
Type 
Evidence 
Type 
NV  
Pattern 
Judgment of 
ground Suggested next action
positive within: map 
pause: map /nod 
grounded 
go-ahead: 0.7 
elaboration: 0.30 Assertion 
negative within: gaze 
pause: gaze 
ungrounded 
go-ahead: 0.27 
elaboration:0.73 
positive within: gaze 
pause: map 
grounded 
go-ahead: 0.83 
elaboration: 0.17 Answer 
negative pause: gaze ungrounded 
go-ahead: 0.22 
elaboration: 0.78 
 
[1] U: How do I get to Room 309?
[2] M: To get to Room 309, go to that door and make a right. 
[3] M: Walk down the hall and make a left at the door
[4] M: It?s the glass door with red couches right outside.
[5] M: And that?s Room 309.
look at map
look at map
look at map
look at map
gaze at MACK
nod
Figure 4: Example of user (U) interacting with 
MACK (M).  User gives negative evidence of 
grounding in [3], so MACK elaborates [4].  
grounded both because the user continues looking 
at the map, and because the user nods, and so the 
final stage of the directions is spoken.  This is also 
grounded, leaving MACK ready for a new inquiry. 
5 Preliminary Evaluation 
Although we have shown an empirical basis for 
our implementation, it is important to ensure both 
that human users interact with MACK as we ex-
pect, and that their interaction is more effective 
than without nonverbal grounding.  The issue of 
effectiveness merits a full-scale study and thus we 
have chosen to concentrate here on whether 
MACK elicits the same behaviors from users as 
does interaction with other humans. 
Two subjects were therefore assigned to one of the 
following two conditions, both of which were run 
as Wizard of Oz (that is, ?speech recognition? was 
carried out by an experimenter): 
(a) MACK-with-grounding: MACK recognized 
user?s nonverbal signals for grounding, and dis-
played his nonverbal signals as a speaker. 
(b) MACK-without-grounding: MACK paid no 
attention to the user?s nonverbal behavior, and did 
not display nonverbal signals as a speaker. He gave 
the directions in one single turn. 
Subjects were instructed to ask for directions to 
two places, and were told that they would have to 
lead the experimenters to those locations to test 
their comprehension. We analyzed the second di-
rection-giving interaction, after subjects became 
accustomed to the system.  
Results: In neither condition, did users return ver-
bal feedback during MACK?s direction giving. As 
shown in Table 4, in MACK-with-grounding 7 
nonverbal status transitions were observed during 
his direction giving, which consisted of 5 Assertion 
UUs, one of them an elaboration. The transition 
patterns between MACK and the user when 
MACK used nonverbal grounding are strikingly 
similar to those in our empirical study of human-
to-human communication. There were three transi-
tions to gM/gM (both look at the map), which is a 
normal status in map task conversation, and two 
transitions to gP/gM (MACK looks at the user, and 
the user looks at the map), which is the most fre-
quent transition in Assertion as reported in Section 
3. Moreover, in MACK?s third UU, the user began 
looking at MACK at the middle of the UU and 
kept looking at him after the UU ended. This be-
havior successfully elicited MACK?s elaboration 
in the next UU.  
On the other hand, in the MACK-without-
grounding condition, the user never looked at 
MACK, and nodded only once, early on. As shown 
in Table 4, only three transitions were observed 
(shift to gMgM at the beginning of the interaction, 
shift to gMgMwN, then back to gMgM).  
While a larger scale evaluation with quantita-
tive data is one of the most important issues for 
future work, the results of this preliminary study 
strongly support our model, and show MACK?s 
potential for interacting with a human user using 
human-human conversational protocols. 
6 Discussion and Future Work 
We have reported how people use nonverbal sig-
nals in the process of grounding. We found that 
nonverbal signals that are recognized as positive 
evidence of understanding are different depending 
on the type of speech act. We also found that main-
taining gaze on the speaker is interpreted as evi-
dence of not-understanding, evoking an additional 
explanation from the speaker. Based on these em-
pirical results, we proposed a model of nonverbal 
grounding and implemented it in an embodied 
conversational agent.  
One of the most important future directions is 
to establish a more comprehensive model of face-
to-face grounding. Our study focused on eye gaze 
Figure 5: MACK with user 
Table 4: Preliminary evaluation 
 with-grounding w/o-grounding 
num of UUs 5 4
gMgM 3 2
gPgM 2 0
gMgP 1 0
gPgP 1 0
gMgMwN 0 1
Shift to
total 7 3
 
and head nods, which directly contribute to 
grounding. It is also important to analyze other 
types of nonverbal behaviors and investigate how 
they interact with eye gaze and head nods to 
achieve common ground, as well as contradictions 
between verbal and nonverbal evidence (eg. an 
interlocutor says, ?OK?, but looks at the partner).  
Finally, the implementation proposed here is a 
simple one, and it is clear that a more sophisticated 
dialogue management strategy is warranted, and 
will allow us to deal with back-grounding, and 
other aspects of miscommunication. For example, 
it would be useful to distinguish different levels of 
miscommunication: a sound that may or may not 
be speech, an out-of-grammar utterance, or an ut-
terance whose meaning is ambiguous.  In order to 
deal with such uncertainty in grounding, incorpo-
rating a probabilistic approach [4] into our model 
of face-to-face grounding is an elegant possibility.  
Acknowledgement 
Thanks to Candy Sidner, Matthew Stone, and 3 
anonymous reviewers for comments that improved 
the paper. Thanks to Prof. Nishida at Univ. of To-
kyo for his support of the research. 
References 
1.Clark, H.H. and E.F. Schaefer, Contributing to dis-
course. Cognitive Science, 1989. 13,: p. 259-294. 
2.Clark, H.H. and D. Wilkes-Gibbs, Referring as a col-
laborative process. Cognition, 1986. 22: p. 1-39. 
3.Matheson, C., M. Poesio, and D. Traum. Modelling 
Grounding and Discourse Obligations Using Update 
Rules. in 1st Annual Meeting of the North American 
Association for Computational Linguistics 
(NAACL2000). 2000. 
4.Paek, T. and E. Horvitz, Uncertainty, Utility, and 
Misunderstanding, in Working Papers of the AAAI Fall 
Symposium on Psychological Models of Communication 
in Collaborative Systems, S.E. Brennan, A. Giboin, and 
D. Traum, Editors. 1999, AAAI: Menlo Park, California. 
p. 85-92. 
5.Clark, H.H., Using Language. 1996, Cambridge: 
Cambridge University Press. 
6.Traum, D.R. and P. Dillenbourg. Miscommunication 
in Multimodal Collaboration. in AAAI Workshop on 
Detecting, Repairing, and Preventing Human-Machine 
Miscommunication. 1996. Portland, OR. 
7.Argyle, M. and M. Cook, Gaze and Mutual Gaze. 
1976, Cambridge: Cambridge University Press. 
8.Goodwin, C., Achieving Mutual Orientation at Turn 
Beginning, in Conversational Organization: Interaction 
between speakers and hearers. 1981, Academic Press: 
New York. p. 55-89. 
9.Novick, D.G., B. Hansen, and K. Ward. Coordinating 
turn-taking with gaze. in ICSLP-96. 1996. Philadelphia, 
PA. 
10.Cassell, J., et al More Than Just a Pretty Face: Af-
fordances of Embodiment. in IUI 2000. 2000. New Or-
leans, Louisiana. 
11.Traum, D. and J. Rickel. Embodied Agents for Multi-
party Dialogue in Immersive Virtual Worlds. in 
Autonomous Agents and Multi-Agent Systems. 2002. 
12.Cassell, J. and K.R. Thorisson, The Power of a Nod 
and a Glance: Envelope vs. Emotional Feedback in 
Animated Conversational Agents. Applied Artificial 
Intelligence, 1999. 13: p. 519-538. 
13.Nakatani, C. and D. Traum, Coding discourse struc-
ture in dialogue (version 1.0). 1999, University of 
Maryland. 
14.Pierrehumbert, J.B., The phonology and phonetics of 
english intonation. 1980, Massachusetts Institute of 
Technology. 
15.Allen, J. and M. Core, Draft of DMSL: Dialogue Act 
Markup in Several Layers. 1997, 
http://www.cs.rochester.edu/research/cisd/resources/da
msl/RevisedManual/RevisedManual.html. 
16.Duncan, S., On the structure of speaker-auditor in-
teraction during speaking turns. Language in Society, 
1974. 3: p. 161-180. 
17.Boyle, E., A. Anderson, and A. Newlands, The Ef-
fects of Visibility in a Cooperative Problem Solving 
Task. Language and Speech, 1994. 37(1): p. 1-20. 
18.Traum, D. and P. Heeman. Utterance Units and 
Grounding in Spoken Dialogue. in ICSLP. 1996. 
19.Nakajima, S.y. and J.F. Allen. Prosody as a cue for 
discourse structure. in ICSLP. 1992. 
20.Morency, L.P., A. Rahimi, and T. Darrell. A View-
Based Appearance Model for 6 DOF Tracking," Pro-
ceed-ings of. in IEEE conference on Computer Vision 
and Pattern Recognition. 2003. Madison, Wisconsin. 
21.Kapoor, A. and R.W. Picard. A Real-Time Head Nod 
and Shake Detector. in Workshop on Perceptive User 
Interfaces. 2001. Orlando FL. 
 
Coordination and context-dependence in the generation of embodied 
conversation 
Justine Cassell* 
*Media Laboratory 
MIT 
E15-315 
20 Ames, Cambridge MA 
{justine, yanhao}@media.mit, edu 
Matthew Stone t Hao Yan* 
tDepartment of Computer Science & 
Center for Cognitive Science 
Rutgers University 
110 Frelinghuysen, Piscataway NJ 08854-8019 
mdstone@cs, rutgers, edu 
Abstract 
We describe the generation of communicative ac- 
tions in an implemented embodied conversational 
agent. Our agent plans each utterance so that mul- 
tiple communicative goals may be realized oppor- 
tunistically by a composite action including not only 
speech but also coverbal gesture that fits the con- 
text and the ongoing speech in ways representative 
of natural human conversation. We accomplish this 
by reasoning from a grammar which describes ges- 
ture declaratively in terms of its discourse function, 
semantics and synchrony with speech. 
1 Introduction 
When we are face-to-face with another human, no 
matter what our language, cultural background, or 
age, we virtually all use our faces and hands as an in- 
tegral part of our dialogue with others. Research on 
embod ied  conversat iona l  agents  aims to imbue in- 
teractive dialogue systems with the same nonverbal 
skills and behaviors (Cassell, 2000a). 
There is good reason to think that nonverbal be- 
havior will play an important role in evoking from 
users the kinds of communicative dialogue behav- 
iors they use with other humans, and thus allow 
them to use the computer with the same kind of ef- 
ficiency and smoothness that characterizes their di- 
alogues with other people. For example, (Cassell 
and Th6risson, 1999) show that humans are more 
likely to consider computers lifelike, and to rate their 
language skills more highly, when those computers 
display not only speech but appropriate nonverbal 
communicative behavior. This argument akes on 
particular importance given that users repeat hem- 
selves needlessly, mistake when it is their turn to 
speak, and so forth when interacting with voice di- 
alogue systems (Oviatt, 1995): tn -life; noisy situa- 
tions like these provoke the non-verbal modalities to 
come into play (Rogers, 1978). 
In this paper, we describe the generation of com- 
municative actions in an implemented embodied 
conversational gent. Our generation framework 
adopts a goal-directed view of generation and casts 
knowledge about communicative action in the form 
of a grammar that specifies how forms combine, 
what interpretive ffects they impart and in what 
contexts they are appropriate (Appelt, 1985; Moore, 
1994; Dale, 1992; Stone and Doran, 1997). We ex- 
pand this framework to take into account findings, 
by ourselves and others, on the relationship between 
spontaneous coverbal hand gestures and speech. In 
particular, our agent plans each utterance so that 
multiple communicative goals may be realized op- 
portunistically by a composite action including not 
only speech but also coverbal gesture. By describing 
gesture declaratively in terms of its discourse func- 
tion, semantics and synchrony with speech, we en- 
sure that coverbal gesture fits the context and the on- 
going speech in ways representative of natural hu- 
man conversation. The result is a streamlined imple- 
mentation that instantiates important theoretical in- 
sights into the relationship between speech and ges- 
ture in human-human conversation. 
2 Explor ing the relat ionship between 
-speech and gesture 
To generate mbodied communicative action re- 
quires an architecture for embodied conversation; 
ours is provided by the agent REA ("Real Estate 
Agent"), a computer-generated humanoid that has 
an articulated graphical body, can sense the user 
passively through cameras and audio input, and 
supports communicative actions realized in speech 
with intonation, facial display, and animated ges- 
ture. REA currently offers the reasoning and dis- 
play capabilities to act as a real estate agent showing 
..... ~users'the--features"o~ vm i-o-wsmodels"of howsesthat ~ 
appear on-screen behind her. We use existing fea- 
tures of kEA here as a resem'ch platform for imple- 
171 
menting models of the relationship between speech 
and spontaneous hand gestures during conversation. 
For more details about the functionality of REA see 
(Cassell, 2000a). 
Evidence from many sources uggests that this re- 
.lationship is aclose one..About three,quarters of al! 
clauses in narrative discourse are accompanied by 
gestures of one kind or another (McNeill, 1992), and 
within those clauses, the most effortful part of ges- 
tures tends to co-occur with or just before the phono- 
logically most prominent syllable of the accompany- 
ing speech (Kendon, 1974). 
Of course, communication is still possible with- 
out gesture. But it has been shown that when speech 
is ambiguous (Thompson and Massaro, 1986) or in 
a speech situation with some noise (Rogers, 1978), 
listeners do rely on gestural cues (and, the higher the 
noise-to-signal ratio, the more facilitation by ges- 
ture). Similarly, Cassell et al (1999) established that 
listeners rely on information conveyed only in ges- 
ture as they try to comprehend a story. 
Most interesting in terms of building interactive 
dialogue systems i the semantic and pragmatic rela- 
tionship between gesture and speech. The two chan- 
nels do not always manifest he same information, 
but what they convey is virtually always compati- 
ble. Semantically, speech and gesture give a con- 
sistent view of an overall situation. For example, 
gesture may depict he way in which an action was 
carried out when this aspect of meaning is not de- 
picted in speech. Pragmatically, speech and ges- 
ture mark information about his meaning as advanc- 
ing the purposes of the conversation i a consistent 
way. Indeed, gesture often emphasizes information 
that is also focused pragmatically by mechanisms 
like prosody in speech (Cassell, 2000b). The seman- 
tic and pragmatic ompatibility seen in the gesture- 
speech relationship recalls the interaction of words 
and graphics in multimodal presentations (Feiner 
and McKeown, 1991; Green et al, 1998; Wahlster 
et al, 1991 ). In fact, some suggest (McNeill, 1992), 
that gesture and speech arise together f om an under- 
lying representation that has both visual and linguis- 
tic aspects, and so the relationship between gesture 
and speech is essential to the production of meaning 
and to its comprehension. 
This theoretical perspective on speech and gesture 
involves two key claims with computational import: 
that gesture and speech ref lectacommon concep- 
tual source; and that the content and form of a ges- 
ture is tuned to the communicative context and the 
172 
actor's communicative intentions. We believe that 
these characteristics of the use of gesture are uni- 
versal, and see the key contribution of this work as 
providing ageneral framework for building dialogue 
systems in accord with them. However, a concrete 
!mplementationrequires " more thanJustgeneralities 
behind its operation; we also need an understanding 
of the precise ways gesture and speech are used to- 
gether in a particular task and setting. 
To this end, we collected a sample of real-estate 
descriptions in line with what REA might be asked 
to provide. To elicit each description, we asked one 
subject o study a video and floor plan of a partic- 
ular house, and then to describe the house to a sec- 
ond subject (who did not know the house and had not 
seen the video). During the conversation, the video 
and floor plan were not available to either subject; 
the listener was free to interrupt and ask questions. 
The collected conversations were transcribed, 
yielding 328 utterances and 134 referential gestures, 
and coded to describe the general communicative 
goals of the speaker and the kinds of semantic fea- 
tures realized in speech and gesture. 
Analysis of the data revealed that for roughly 
50% of the gesture-accompanied utterances, gestu- 
ral content was redundant with speech; for the other 
50% gesture contributed content hat was different, 
but complementary, to that contributed by speech. 
In addition, the relationship between content of ges- 
ture, content of speech and general communicative 
functions in house descriptions could be captured by 
a small number or rules; these rules are informed by 
and accord with our two key claims about speech 
and gesture. For example, one rule describes di- 
alogue contributions whose general function was 
what we call presentation, to advance the descrip- 
tion of the house by introducing a single new ob- 
ject.. These contributions tended to be made up of 
a sentence that asserted the existence of an object 
of some type, accompanied by a non-redundant ges- 
? ture that elaborated theshape or location of  the ob- 
ject. Our approach casts this extended escription of 
a new entity, mediated by two compatible modali- 
ties, as the speaker's expression of one overall func- 
tion of presentation. 
( I ) is a representative example. 
(1) It has \[a nice garden\]. (right hand, held flat, 
traces a circle, indicating location of the 
garden sunounding the house) 
Six rules account for 60% of the gestures in the 
Figure 1" Interacting with REA 
transcriptions (recall) and apply with an accuracy of 
96% (precision). These patterns provide a concrete 
specification for the main communicative strategies 
and communicative r sources required for REA. m 
full discussion of the experimental methods and 
analysis, and the resulting rules, can be found in 
(Yan, 2000). 
3 F raming  the generat ion problem 
In REA, requests for the generation of speech and 
gesture are formulated within the dialogue manage- 
ment module. REA'S utterances reflect a coordina- 
tion of multiple kinds of processing in the dialogue 
manager- the system recognizes that it has the floor, 
derives the appropriate communicative context for 
a response and an appropriate set of communicative 
goals, triggers the generation process, and realizes 
the resulting speech and gesture. The dialogue man- 
ager is only one component in a multithreaded ar- 
chitecture that carries out hardwired reactions to in- 
put as well as deliberative processing. The diver- 
sity is required in order to exhibit appropriate inter- 
actional and propositional conversational behaviors 
at a range of time scales, from tracking the user's 
movements with gaze and providing nods and other 
feedback as the user speaks, to participating in rou- 
tine exchanges and generating principled responses 
to user's queries. See (Cassell, 2000a) for descrip- 
tion and motivation of the architecture, aswell as the 
conversational functions and behaviors it supports. 
REA'S design and capabilities reflect our research 
focus on allying conversational content with conver- 
sation management, and allying nonverbal modali- 
ties with speech: how can anembodiedagent use'all 
its communicative modalities to contribute new con- 
tent when needed (propositional function), to signal 
the state of the dialogue, and to regulate the over- 
all process of conversation (interactional function)? 
Within this focus, REA's talk is firmly delimited. 
REA'S utterances take a question-answer fo mat, in 
which the user asks about (and REA describes) a
single house .at.a. time. REA'S .sentences ,ate short; 
generally, they contribute just a few new semantic 
features about particular ooms or features of the 
house (in speech and gesture), and flesh this contri- 
bution out with a handful of meaningful e ements (in 
speech and gesture) that ground the contribution i
shared context of the conversation. 
Despite the apparent simplicity, the dialogue 
manager must contribute a wealth of information 
about he domain and the conversation torepresent 
the communicative context. This detail is needed for 
REA tO achieve atheoretically-motivated realization 
of the common patterns of speech and gesture we ob- 
served in human conversation. For example, a vari- 
ety of changing features determine whether marked 
forms in speech and gesture are appropriate in the 
context. REA'S dialogue manager t acks the chang- 
ing status of such features as: 
e Attentionalprominence, r presented (as usual 
in natural language generation) by setting up a 
context set for each entity (Dale, 1992). Our 
model of prominence is a simple local one sim- 
ilar to (Strube, 1998). 
o Cognitive status, including whether an entity is 
hearer-old or hearer-new (Prince, 1992), and 
whether an entity is in-focus or not (Gundel 
et al, 1993). We can assume that houses and 
their rooms are hearer-new until REA describes 
them; and that just those entities mentioned in
the prior sentence are in-focus. 
Information structure, including the open 
propositions or, following (Steedman, 1991 ), 
themes, which describe the salient questions 
currently at issue in the discourse (Prince, 
1986). In REA'S dialogue, open questions are 
always general questions about some entity 
raised by a recent urn; although in principle 
such an open question ought o be formalized 
as theme(XP.Pe), REA can use the simpler 
theme(e). 
In fact, both speech and gesture depend on the same 
? " kinds of'feamresi;-andaccessthem in the same way; " 
this specification of the dialogue state crosscuts dis- 
tinctions of communicative modality. 
173 
Another component of context is provided by a 
domain knowledge base, consisting of facts explic- 
itly labeled with the kind of information they repre- 
sent. This defines the common ground in the con- 
versation in terms of sources of information that 
lation of goals and tightly fits the context specified 
by the dialogue manager. 
4 Generation and linguistic representation 
speaker and hearer share. Modeling the discourse as We model REA'S communicative actions as com- 
a shared source of information means that new ~e "'-':''~'lmsed~?f:a~c~rHeeti?n'?f'at?mie'etementsiqnclndiiag 
both lexical items in speech and clusters of seman- mantic features REA imparts are added to the com- 
mon ground as the dialogue proceeds. Following re- 
sults from (Kelly et al, 1999) which show that infor- 
mation from both speech and gesture is used to pro- 
vide context for ongoing talk, our common ground 
may be updated by both speech and gesture. 
The structured omain knowledge also provides 
a resource for specifying communicative strategies. 
Recall that REA'S communicative strategies are for- 
mulated in terms of functions which are common 
in naturally-occurring dialogues (such as "presenta- 
tion") and which lead to distinctive bundles of con- 
tent in gesture and speech. The knowledge base's 
kinds of information provide a mechanism for spec- 
ifying and reasoning about such functions. The 
knowledge base is structured to describe the rela- 
tionship between the system's private information 
and the questions of interest hat that information 
can be used to settle. Once the user's words have 
been interpreted, a layer of production rules con- 
structs obligations for response (Traum and Allen, 
1994); then, a second layer plans to meet hese obli- 
gations by deciding to present a specified kind of 
information about a specified object. This deter- 
mines some concrete communicative goals--facts 
of this kind that a contribution to dialogue could 
make. Both speech and gesture can access the 
whole structured database inrealizing these concrete 
communicative goals. For example, a variety of 
facts that bear on where a residence is--which city, 
which neighborhood or, if appropriate, where in a 
building--all provide the same kind of information, 
and would therefore fit the obligation to specify the 
location of a residence. Or, to implement the rule 
for presentation described in connection with ( 1 ), we 
can associate an obligation of presentation with a 
cluster of facts describing an object's type, its loca- 
tion in a house, and its size, shape or quality. 
The communicative context and concrete com- 
municative goals provide a common source for gen- 
erating speech and gesture in REA. The utterance 
generation problem ,in REa,.then, is to construct a- 
complex communicative action, made up of speech 
and coverbal gesture, that achieves a given constel- 
tic features expressed as gestures; ince we assume 
that any such item usually conveys a specific piece 
of content, we refer to these elements generally as 
lexicalized escriptors. The generation task in REA 
thus involves selecting a number of such lexical- 
ized descriptors and organizing them into a gram- 
matical whole that manifests the right semantic and 
pragmatic coordination between speech and gesture. 
The information conveyed must be enough that the 
hearer can identify the entity in each domain ref- 
erence from among its context set. Moreover, the 
descriptors must provide a source which allows the 
hearer to recover any needed new domain proposi- 
tion, either explicitly or by inference. 
We use the SPUD generator ("Sentence Planning 
Using Description") introduced in (Stone and Do- 
ran, 1997) to carry out this task for REA. SPUD 
builds the utterance lement-by-element; at each 
stage of construction, SPUD'S representation f the 
current, incomplete utterance specifies its syntax, 
semantics, interpretation a d fit to context. This rep- 
resentation both allows SPUD to determine which 
lexicalized escriptors are available at each stage to 
extend the utterance, and to assess the progress to- 
wards its communicative goals which each exten- 
sion would bring about. At each stage, then, SPUD 
selects the available option that offers the best im- 
mediate advance toward completing the utterance 
successfully. (We have developed a suite of guide- 
lines for the design of syntactic structures, eman- 
tic and pragmatic representations, and the interface 
between them so that SPUD'S greedy search, which 
is necessary for real-time performance, succeeds in 
finding concise and effective Utterances described 
by the grammar (Stone et al, 2000).) 
As part of the development of REA, we have con- 
structed a new inventory of lexicalized escriptors. 
REA'S descriptors consist of entries that contribute 
to coverbal gestures, as well as revised entries for 
spoken words that allow for their coordination with 
gesture under appropriate discourse conditions. The 
:-organization f'these ntries assures'that--rasing the 
same mechanism as with speech--REA'S gestures 
draw on the single available conceptual representa- 
174 
tion and that both REA'S  gesture and the relation- 
ship between gesture and speech-vary as a function 
of pragmatic context in the same way as natural ges- 
tures and speech do. More abstractly, these entries 
enable SPUD to realize the concrete goals tied to 
common communicative functions with same dis- 
tribution of speech and gestiire bbse~ed:iffn/lttl'ral- 
conversations. 
To explain how these entries work, we need to 
consider SPUD's representation of lexicalized de- 
scriptors in more detail. Each entry is specified 
in three parts. The first part--the syntax of the 
elemenv--sets out what words or other actions the 
element contributes to its utterance. The syn- 
tax is a hierarchical structure, formalized using 
Feature-Based Lexicalized Tree Adjoining Gram- 
mar (LTAG) (Joshi et al, 1975; Schabes, 1990). 
Syntactic structures are also associated with referen- 
tial indices that specify the entities in the discourse 
that the entry refers to. For the entry to apply at a 
particular stage, its syntactic structure must combine 
by LTAG operations with the syntax of the ongoing 
utterance. 
REA'S syntactic entries combine typical phrase- 
structure analyses of linguistic constructions with 
annotations that describe the occurrence of gestures 
in coordination with linguistic phrases. Our device 
for this is a construction SYNC which pairs a descrip- 
tion of a gesture G with the syntactic structure of a 
spoken constituent c:
SYNC 
(2) G C 
The temporal interpretation of (2) mirrors the rules 
for surface synchrony between speech and gesture 
presented in (Cassell et al, 1994). That is, the 
preparatory phase of gesture G is set to begin before 
the time constituent c begins; the stroke of gesture 
G (the most effortful part) co-occurs with the most 
phonologically prominent syllable in c; and, except 
in cases of coarticulation between successive ges- 
tures, by the time the constituent c is complete, the 
speaker must be relaxing and bringing the hands out 
of gesture space (while the generator specifies yn- 
chrony as described, in practice the synchronization 
of synthesized speech with graphics is an ongoing 
challenge in the REA~projeet).-Jn. sum; 'the produc- 
tion of gesture G is synchronized with the produc- 
tion of speech c. (Our representation f synchrony 
175 
in a single tree conveniently allows modules dowrL- 
stream to describe mbodied communicative actions 
as marked-up text.) 
The syntactic description of the gesture itself in- 
dicates the choices the generator must make to pro- 
duce a gesture, but does not analyze a ,gesture lit- 
er~i|y--~is '~/ hier~chy :i~f ~+p~a~e  m~=~fi~s~-'~f~?:"= .... 
stead, these choices specify independent semantic 
features which we can associate with aspects of a 
gesture (such as handshape and trajectory through 
space). Our current grammar does not undertake the 
final step of associating semantic features to choice 
of particular handshapes and movements, orgesture 
morphology; we reserve this problem for later in 
the research program. We allow gesture to accom- 
pany alternative constituents by introducing alterna- 
tive syntactic entries; these entries take on different 
pragmatic requirements (as described below) to cap- 
ture their respective discourse functions. 
So much for syntax. The second part--the seman- 
tics of the element--is a formula that specifies the 
content hat the element carries. Before the entry 
can be used, SPUD must establish that the semantics 
holds of the entities the entry describes. If the se- 
mantics already follows from the common ground, 
SPUD assumes that the hearer can use it to help iden- 
tify the entities described. If the semantics i merely 
part of the system's private knowledge, SPUD treats 
it as new information for the hearer. 
Finally, the third part--the pragmatics of the 
element--is also a formula that SPUD looks to prove 
before using the entry. Unlike the semantics, how- 
ever, the pragmatics does not achieve specific com- 
municative goals like identifying referents. Instead, 
the pragmatics establishes a general fit between the 
entry and the context. 
The entry schematized in (3) illustrates these three 
components; the entry also suggests how these com- 
ponents can define coordinated actions of speech 
and gesture that respond coherently to the context. 
(3) a syntax: s 
NP VP 
NP:o V SYNC 
/have/  G:x I NP:x 
b semantics: have(o,x) 
c 'pragmaties:"heardr-n-ew(x) A'theme{O) . . . . .  
(3) describes the use of have to introduce a new fea- 
ture of (a house) o. The feature, indicated through- 
out the entry by the variable x,.is realized as the ob- 
ject NP of the verb have, but x can also form the ba- 
sis of a gesture G coordinated with the noun phrase 
(as indicated by the SYNC constituent). The entry as- 
serts that o has x. 
(3) is a presentational Construction; in other 
words, it coordinates non-redundant paired speech 
and gesture in the same way as demonstrated by our 
house description data. To represent this constraint 
on its use, the entry carries two pragmatic require- 
ments: first, x must be new to the hearer; moreover, 
o must link up with the open question in the dis- 
course that the sentence responds to. 
The pragmatic onditions of (3) help support 
our theory of the discourse function of gesture and 
speech. A similar kind of sentence could be used 
to address other open questions in the discourse-- 
for example, to answer which house has a garden? 
This would not be a presentational function, and 
(3) would be infelicitous here. In that case, gesture 
would naturally coordinate with and elaborate on the 
answering information--in this case the house. So 
the different information structure would activate a
different entry, where the gesture would coordinate 
with the subject and describe o. 
Meanwhile, alternative ntries like (4a) and 
(4b)---two entries that both convey (4c) and that 
both could combine with (3) by LTAG operations-- 
underlie our claim that our implementation allows 
gesture and speech to draw on a single conceptual 
source and fulfill similar communicative intentions. 
(4) a syntax: G:x 
circular-trajectory RS:x l
b syntax: NP 
NP.:x VP 
V NP:p j 
I 
surrounding 
c semantics: urround(x.p) 
(4a) provides astructure that could substitute for the 
G node in (3) to produce semantically and pragmat- 
ically coordinated speech and gesture. (4a) speci- 
fies a right hand gestnre:in.wlhieh.~the hand. traces 
out a circular trajectory; a further decision must de- 
termine the correct handshape (node RS, as a func- 
176 
tion of the entity x that the gesture describes). We 
pair (4a) with the semantics in (4c), and thereby 
model that the gesture indicates that one object, x, 
surrounds another, p. Since p cannot be further de- 
scribed, p must be identified by an additional pre- 
supposition ofthe gesture which.picks up~a reference 
frame from the sliared context. 
Similarly, (4b) describes how we could modify 
the vP introduced by (3) (using the LTAG operation 
of adjunction), to produce an utterance such as It 
has a garden surrounding it. By pairing (4b) with 
the same semantics (4c), we ensure that SPUD will 
treat he communicative contribution of the alterna- 
tive constructions of (4) in a parallel fashion. Both 
are triggered by accessing background knowledge 
and both are recognized as directly communicating 
specified facts. 
5 Solving the generat ion problem 
We now sketch how entries uch as these combine 
together to account for REA'S utterances. Our exam- 
ple is the dialogue in (5): 
(5) a User: Tell me more about he house. 
b REA: It has \[a nice garden\]. (right hand, held 
fiat, traces a circle) 
REA's response indicates both that the house has a 
nice garden and that it surrounds the house. 
As we have seen, (5b) represents a common pat- 
tern of description; this particular example is moti- 
vated by an exchange two human subjects had in our 
study, cf. (1). (5b) represents a solution to a gen- 
eration problem that arises as follows within REA'S 
overall architecture. The user's directive is inter- 
preted and classified as a directive requiring adelib- 
erative response. The dialogue manager recognizes 
an obligation to respond to the directive, and con- 
cludes that to fulfill the function of presenting the 
garden would discharge this obligation. The presen- 
tational function grounds out in the communicative 
goal to convey a collection of facts about he garden 
(type, quality, location relative to the house). Along 
with these goals, the dialogue manager supplies its 
communicative context, which represents he cen- 
trality of the house in attentional prominence, cog- 
nitive status and information structure. 
In producing (5b) in response to this NLG prob- 
lem, SPUD both calculates the applicability of and 
. ~determines a preference,for-theqexiOatized descrip- 
tors involved. Initially, (3) is applicable; the system 
knows the house has the garden, and represents he 
garden as new and the house as questioned. The en- 
try can be selected over potential alternatives based 
on its interpretation--it achieves a communicative 
goal, refers to a prominent entity, and makes a rel- 
atively specific connection to facts in the context. 
and what its role might be. Likewise, we need a 
model of the communicative effects of spontaneous 
coverbal gesture--one that allows us to reason at- 
urally about he multiple goals speakers have in pro- 
ducing each utterance. 
_Similarly, in the .second, stage, SPUD evaluates .and 
selects (4a) because it Communicates a needed fact 7 
in a way that helps flesh out a concise, balanced 
communicative act by supplying a gesture that by 
using (3) SPUD has already realized belongs here. 
Choices of remaining elements--the words garden 
and nice, the semantic features to represent the gar- 
den in the gesture--proceed similarly. Thus SPUD 
arrives at the response in (5b) just by reasoning from 
the declarative specification ofthe meaning and con- 
text of communicative actions. 
6 Re la ted  Work  
The interpretation of speech and gesture has been 
investigated since the pioneering work of (Bolt, 
1980) on deictic gesture; recent work includes 
(Koons et al, 1993; Bolt and Herranz, 1992). Sys- 
tems have also attempted generation of gesture in 
conjunction with speech. Lester et al (1998) gener- 
ate deictic gestures and choose referring expressions 
as a function of the potential ambiguity of objects re- 
ferred to, and their proximity to the animated agent. 
Rickel and Johnson (1999)'s pedagogical gent pro- 
duces a deictic gesture at the beginning of explana- 
tions about objects in the virtual world. Andr6 et 
al. (1999) generate pointing estures as a sub-action 
of the rhetorical action of labeling, in turn a sub- 
action of elaborating. 
Missing from these prior systems, however, is a 
representation f communicative action that treats 
the different modalities on a par. Such representa- 
tions have been explored in research on combining 
linguistic and graphical interaction. For example, 
multimodal managers have been described to allo- 
cate an underlying content representation for gen- 
eration of text and graphics (Wahlster et al, 1991; 
Green et al, 1998). Meanwhile, (Johnston et al, 
1997; Johnston, 1998) describe a formalism for 
tightly-coupled interpretation which uses a gram- 
mar and semantic onstraints oanalyze input from 
speech and pen. While many insights from these 
formalisms are relevant in embodied conversation, 
spontaneous gesture requires adistinct analysis with 
different emphasis:For example;-we need some no- 
tion of discourse pragmatics that would allow us to 
predict where gesture occurs with respect to speech, 
Conclusion . . . . .  
Research on the robustness of human conversation 
suggests that a dialogue agent capable of acting 
as a conversational partner would provide for effi- 
cient and natural collaborative dialogue. But human 
conversational partners display gestures that derive 
from the same underlying conceptual source as their 
speech, and which relate appropriately to their com- 
municative intent. In this paper, we have summa- 
rized the evidence for this view of human conver- 
sation, and shown how it informs the generation 
of communicative action in our artificial embodied 
conversational agent, REA. REA has a working im- 
plementation, which includes the modules described 
in this paper, and can engage in a variety of interac- 
tions including that in (5). Experiments are under- 
way to investigate the extent o which REA 'S conver- 
sational capacities share the strengths of the human 
capacities they are modeled on. 
Acknowledgments  
The research reported here was supported by NSF (award 
IIS-9618939), Deutsche Telekom, AT&T, and the other 
generous sponsors of the MIT Media Lab, and a postdoc- 
toral fellowship from RUCCS. Hannes Vilhj~ilmsson as- 
sisted with the implementation of REA'S discourse man- 
ager. We thank Nancy Green. James Lester, Jeff Rickel, 
Candy Sidner, and anonymous reviewers for comments 
on this and earlier drafts. 
References 
Elisabeth Andr6, Thomas Rist, and Jochen Mailer. 1999. 
Employing AI methods to control the behavior of ani- 
mated interface agents. AppliedArtificial Intelligence, 
13:415-448. 
Douglas Appelt. 1985. Planning English Sentences. 
Cambridge University Press, Canlbridge England. 
R. A. Bolt and E. Herranz. 1992. Two-handed gesture in 
multi-modal natural dialog. In UIST92: Fifth Ammal 
Symposium on User Interface Software and Technol- 
ogy. 
R. A. Bolt. 1980. Put-that-there: voice and gesture at the 
graphics interface. Computer Graphics. 14(3):262- 
270. 
J. Cassell and K. Th6risson. 1999. The power of a nod 
and a glance: Envelope vs. emotional feedback in an- 
imatefd conversational agents. AppliedArt(ficial Intel- 
ligence, 13(3). 
177 
Justine Casseil, Catherine Pelachaud, Norm Badler, 
Mark Steedman, Brett Achorn, Tripp Becket, Brett 
Douville, Scott Prevost, and Matthew Stone. 1994. 
Animated conversation: Rule-based generation of fa- 
cial expression, gesture and spoken intonation for mul- 
tiple conversational gents. In SIGGRAPH, pages 
Patrick FitzGerald. 1998. Deictic and emotive com- 
munication i  animated pedagogical gents. In Work- 
shop on Embodied Conversational Characters. 
David McNeill. 1992. Hand and Mind: What Gestures 
Reveal about Thought. University of Chicago Press, 
Chicago. 
413-420. 
J. Cassell, D. McNeill, and K. E. McCullough. 1999. 
Speech-gesture mismatches: evidence for one under- 
lying representation f linguistic and nonlinguistic n- 
formation. Pragmatics and Cognition, 6(2). 
Justine Cassell. 2000a. Embodied conversational inter- 
face agents. Communications of the ACM, 43(4):70- 
78. 
Justine Cassell. 2000b. Nudge nudge wink wink: Ele- 
ments of face-to-face conversation for embodied con- 
versational gents. In J. Cassell, J. Sullivan, S. Pre- 
vost, and E. Churchill, editors, Embodied Conversa- 
tional Agents, pages 1-28. MIT Press, Cambridge, 
MA. 
Robert Dale. 1992. Generating Referring Expressions: 
Constructing Descriptions ina Domain of Objects and 
Processes. MIT Press, Cambridge MA. 
S. Feiner and K. McKeown. 1991. Automating the gen- 
eration of coordinated multimedia explanations. IEEE 
Computer, 24(10): 33-41. 
Nancy Green, Giuseppe Carenini, Stephan Kerpedjiev, 
Steven Roth, and Johanna Moore. 1998. A media- 
independent content language for integrated text and 
graphics generation. In CVIR '98- Workshop on Con- 
tent Visualization and Intermedia Representations. 
Jeanette K. Gundel, Nancy Hedberg, and Ron Zacharski. 
1993. Cognitive status and the form of referring ex- 
pressions in discourse. Language, 69(2):274--307. 
M. Johnston, P. R. Cohen, D. McGee, J. Pittman, S. L. 
Oviatt, and I. Smith. 1997. Unification-based multi- 
modal integration. In ACL/EACL 97: Proceedings of 
the Annual Meeting of the Assocation for Computa- 
tional Linguistics. 
Michael Johnston. 1998. Unification-based naultimodal 
parsing. In COLING/ACL. 
Aravind K. Joshi, L. Levy, and M. Takahashi. 1975. Tree 
adjunct grammars. Journal of the Computer and Sys- 
tem Sciences, 10:136--- 163. 
S. D. Kelly, J. D. Barr, R. B. Church, and K. Lynch. 1999. 
Offering a hand to pragmatic understanding: The role 
of speech and gesture in comprehension a d memory. 
Journal of Memory and Language, 40:577-592. 
A. Kendon. 1974. Movement coordination i social in- 
teraction: somem examples described. In S. Weitz, ed- 
itor, Nonverbal Communication. Oxford, New York. 
D. B. Koons. C. J. Sparrell, and K. R. Th6risson. 1993. 
Integrating simultaneous input from speech, gaze and 
hand gestures. In M. T. Maybtiry,'-editor~-lntel/igent 
Multi-media Interfaces. MIT Press, Cambridge. 
James Lester, Stuart Towns. Charles Calloway, and 
. . . . . . . .  - ~ .....  :.~,, ~Joharma:Moore; ~,~994,,P, articipating in Explanatory Di- 
alogues. MIT Press, Cambridge MA. 
S. L. Oviatt. 1995. Predicting spoken language disflu- 
encies during human-computer interaction. Computer 
Speech and Language, 9( l ): 19-35. 
Ellen Prince. 1986. On the syntactic marking of pre- 
supposed open propositions. In Proceedings of the 
22nd Annual Meeting of the Chicago Linguistic Soci- 
ety, pages 208-222, Chicago. CLS. 
Ellen F. Prince. 1992. The ZPG letter: Subjects, definite- 
ness and information status. In William C. Mann and 
Sandra A. Thompson, editors, Discourse Description: 
Diverse Analyses of a Fund-raising Text, pages 295- 
325. John Benjamins, Philadelphia. 
Jeff Rickel and W. Lewis Johnson. 1999. Animated 
agents for procedural training in virtual reality: Per- 
ception, cognition and motor control. Applied Artifi- 
cial Intelligence, 13:343-382. 
W. T. Rogers. 1978. The contribution of kinesic illus- 
trators towards the comprehension f verbal behavior 
within utterances. Human Communication Research, 
5:54--62. 
Yves Schabes. 1990. Mathematical nd Computational 
Aspects of Lexicalized Grammars. Ph.D. thesis, Com- 
puter Science Department, University of Pennsylva- 
nia. 
Mark Steedman. 1991. Structure and intonation. Lan- 
guage, 67:260-296. 
Matthew Stone and Christine Doran. 1997. Sentence 
planning as description using tree-adjoining grammar. 
In Proceedings of ACL, pages 198-205. 
Matthew Stone, Tonia Bleam, Christine Doran, and 
Martha Palmer. 2000. Lexicalized grammar and the 
description of motion events. In TAG+: Workshop on 
Tree-Adjoining Grammar and Related Forntalisms. 
Michael Strube. 1998. Never look back: An alternative 
to centering. In Proceedings of COLING-ACL. 
L. A. Thompson and D. W. Massaro. 1986. Evaluation 
and integration of speech and pointing gestures dur- 
ing referential understanding. Journal of Experimen- 
tal Child Psychology, 42:144-168. 
David R. Traum and James F. Allen. 1994. Discourse 
obligations in dialogue processing. In ACL, pages 1- 
8. 
W. Wahlster, E. Andr6, W. Graf, and T. Rist. 1991. 
Designing illustrated texts. In Proceedings of EACL, 
pages 8-14. 
Hao Yan.~ 20,00. Paired speech~and gesture generation i
embodied conversational agents~ Master's thesis, Me- 
dia Lab, MIT. 
178 
Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 301?304,
Suntec, Singapore, 4 August 2009.
c
?2009 ACL and AFNLP
Validating the web-based evaluation of NLG systems
Alexander Koller
Saarland U.
koller@mmci.uni-saarland.de
Kristina Striegnitz
Union College
striegnk@union.edu
Donna Byron
Northeastern U.
dbyron@ccs.neu.edu
Justine Cassell
Northwestern U.
justine@northwestern.edu
Robert Dale
Macquarie U.
Robert.Dale@mq.edu.au
Sara Dalzel-Job
U. of Edinburgh
S.Dalzel-Job@sms.ed.ac.uk
Jon Oberlander
U. of Edinburgh
Johanna Moore
U. of Edinburgh
{J.Oberlander|J.Moore}@ed.ac.uk
Abstract
The GIVE Challenge is a recent shared
task in which NLG systems are evaluated
over the Internet. In this paper, we validate
this novel NLG evaluation methodology by
comparing the Internet-based results with
results we collected in a lab experiment.
We find that the results delivered by both
methods are consistent, but the Internet-
based approach offers the statistical power
necessary for more fine-grained evaluations
and is cheaper to carry out.
1 Introduction
Recently, there has been an increased interest in
evaluating and comparing natural language gener-
ation (NLG) systems on shared tasks (Belz, 2009;
Dale and White, 2007; Gatt et al, 2008). However,
this is a notoriously hard problem (Scott and Moore,
2007): Task-based evaluations with human experi-
mental subjects are time-consuming and expensive,
and corpus-based evaluations of NLG systems are
problematic because a mismatch between human-
generated output and system-generated output does
not necessarily mean that the system?s output is
inferior (Belz and Gatt, 2008). This lack of evalua-
tion methods which are both effective and efficient
is a serious obstacle to progress in NLG research.
The GIVE Challenge (Byron et al, 2009) is a
recent shared task which takes a third approach to
NLG evaluation: By connecting NLG systems to
experimental subjects over the Internet, it achieves
a true task-based evaluation at a much lower cost.
Indeed, the first GIVE Challenge acquired data
from over 1100 experimental subjects online. How-
ever, it still remains to be shown that the results
that can be obtained in this way are in fact com-
parable to more established task-based evaluation
efforts, which are based on a carefully selected sub-
ject pool and carried out in a controlled laboratory
environment. By accepting connections from arbi-
trary subjects over the Internet, the evaluator gives
up control over the subjects? behavior, level of lan-
guage proficiency, cooperativeness, etc.; there is
also an issue of whether demographic factors such
as gender might skew the results.
In this paper, we provide the missing link by
repeating the GIVE evaluation in a laboratory en-
vironment and comparing the results. It turns out
that where the two experiments both find a signif-
icant difference between two NLG systems with
respect to a given evaluation measure, they always
agree. However, the Internet-based experiment
finds considerably more such differences, perhaps
because of the higher number of experimental sub-
jects (n = 374 vs. n = 91), and offers other oppor-
tunities for more fine-grained analysis as well. We
take this as an empirical validation of the Internet-
based evaluation of GIVE, and propose that it can
be applied to NLG more generally. Our findings
are in line with studies from psychology that indi-
cate that the results of web-based experiments are
typically consistent with the results of traditional
experiments (Gosling et al, 2004). Nevertheless,
we do find and discuss some effects of the uncon-
trolled subject pool that should be addressed in
future Internet-based NLG challenges.
2 The GIVE Challenge
In the GIVE scenario (Byron et al, 2009), users
try to solve a treasure hunt in a virtual 3D world
that they have not seen before. The computer has
complete information about the virtual world. The
challenge for the NLG system is to generate, in real
time, natural-language instructions that will guide
the users to the successful completion of their task.
From the perspective of the users, GIVE con-
sists in playing a 3D game which they start from
a website. The game displays a virtual world and
allows the user to move around in the world and
manipulate objects; it also displays the generated
301
instructions. The first room in each game is a tuto-
rial room in which users learn how to interact with
the system; they then enter one of three evaluation
worlds, where instructions for solving the treasure
hunt are generated by an NLG system. Players
can either finish a game successfully, lose it by
triggering an alarm, or cancel the game at any time.
When a user starts the game, they are randomly
connected to one of the three worlds and one of the
NLG systems. The GIVE-1 Challenge evaluated
five NLG systems, which we abbreviate as A, M,
T, U, and W below. A running GIVE NLG system
has access to the current state of the world and to
an automatically computed plan that tells it what
actions the user should perform to solve the task. It
is notified whenever the user performs some action,
and can generate an instruction and send it to the
client for display at any time.
3 The experiments
The web experiment. For the GIVE-1 challenge,
1143 valid games were collected over the Internet
over the course of three months. These were dis-
tributed over three evaluation worlds (World 1: 374,
World 2: 369, World 3: 400). A game was consid-
ered valid if the game client didn?t crash, the game
wasn?t marked as a test run by the developers, and
the player completed the tutorial.
Of these games, 80% were played by males and
10% by females (the remaining 10% of the partic-
ipants did not specify their gender). The players
were widely distributed over countries: 37% con-
nected from IP addresses in the US, 33% from
Germany, and 17% from China; the rest connected
from 45 further countries. About 34% of the par-
ticipants self-reported as native English speakers,
and 62% specified a language proficiency level of
at least ?expert? (3 on a 5-point scale).
The lab experiment. We repeated the GIVE-1
evaluation in a traditional laboratory setting with
91 participants recruited from a college campus.
In the lab, each participant played the GIVE game
once with each of the five NLG systems. To avoid
learning effects, we only used the first game run
from each subject in the comparison with the web
experiment; as a consequence, subjects were dis-
tributed evenly over the NLG systems. To accom-
modate for the much lower number of participants,
the laboratory experiment only used a single game
world ? World 1, which was known from the online
version to be the easiest world.
Among this group of subjects, 93% self-rated
their English proficiency as ?expert? or better; 81%
were native speakers. In contrast to the online ex-
periment, 31% of participants were male and 65%
were female (4% did not specify their gender).
Results: Objective measures. The GIVE soft-
ware automatically recorded data for five objec-
tive measures: the percentage of successfully com-
pleted games and, for the successfully completed
games, the number of instructions generated by
the NLG system, of actions performed by the user
(such as pushing buttons), of steps taken by the
user (i.e., actions plus movements), and the task
completion time (in seconds).
Fig. 1 shows the results for the objective mea-
sures collected in both experiments. To make the
results comparable, the table for the Internet ex-
periment only includes data for World 1. The task
success rate is only evaluated on games that were
completed successfully or lost, not cancelled, as
laboratory subjects were asked not to cancel. This
brings the number of Internet subjects to 322 for
the success rate, and to 227 (only successful games)
for the other measures.
Task success is the percentage of successfully
completed games; the other measures are reported
as means. The chart assigns systems to groups A
through C or D for each evaluation measure. Sys-
tems in group A are better than systems in group
B, and so on; if two systems have no letter in com-
mon, the difference between them is significant
with p < 0.05. Significance was tested using a ?
2
-
test for task success and ANOVAs for instructions,
steps, actions, and seconds. These were followed
by post hoc tests (pairwise ?
2
and Tukey) to com-
pare the NLG systems pairwise.
Results: Subjective measures. Users were
asked to fill in a questionnaire collecting subjec-
tive ratings of various aspects of the instructions.
For example, users were asked to rate the overall
quality of the direction giving system (on a 7-point
scale), the choice of words and the referring ex-
pressions (on 5-point scales), and they were asked
whether they thought the instructions came at the
right time. Overall, there were twelve subjective
measures (see (Byron et al, 2009)), of which we
only present four typical ones for space reasons.
For each question, the user could choose not to
answer. On the Internet, subjects made consider-
able use of this option: for instance, 32% of users
302
Objective Measures Subjective Measures
task
success
instructions steps actions seconds overall
choice
of words
referring
expressions
timing
A 91% A 83.4 B 99.8 A 9.4 A 123.9 A 4.7 A 4.7 A 4.7 A 81% A
M 76% B 68.1 A 145.1 B 10.0 AB 195.4 BC 3.8 AB 3.8 B 4.0 B 70% ABC
T 85% AB 97.8 C 142.1 B 9.7 AB 174.4 B 4.4 B 4.4 AB 4.3 AB 73% AB
U 93% AB 99.8 C 142.6 B 10.3 B 194.0 BC 4.0 B 4.0 B 4.0 B 51% C
W 24% C 159.7 D 256.0 C 9.6 AB 234.1 C 3.8 AB 3.8 B 4.2 AB 50% BC
A 100% A 78.2 AB 93.4 A 9.9 A 143.9 A 5.7 A 4.7 A 4.8 A 92% A B
M 95% A 66.3 A 141.8 B 10.5 A 211.8 B 5.4 A 3.8 B 4.3 A 95% A B
T 93% A 107.2 CD 134.6 B 9.6 A 205.6 B 4.9 A 4.5 A B 4.4 A 64% A B
U 100% A 88.8 BC 128.8 B 9.8 A 195.1 AB 5.7 A 4.7 A 4.3 A 100% A
W 17% B 134.5 D 213.5 C 10.0 A 252.5 B 5.0 A 4.5 A B 4.0 A 100% B
Figure 1: Objective and selected subjective measures on the web (top) and in the lab (bottom).
didn?t fill in the ?overall evaluation? field of the
questionnaire. In the laboratory experiment, the
subjects were asked to fill in the complete question-
naire and the response rate is close to 100%.
The results for the four selected subjective mea-
sures are summarized in Fig. 1 in the same way as
the objective measures. Also as above, the table
is based only on successfully completed games in
World 1. We will justify this latter choice below.
4 Discussion
The primary question that interests us in a compar-
ative evaluation is which NLG systems performed
significantly better or worse on any given evalua-
tion measure. In the experiments above, we find
that of the 170 possible significant differences (=
17 measures ? 10 pairs of NLG systems), the labo-
ratory experiment only found six that the Internet-
based experiment didn?t find. Conversely, there
are 26 significant differences that only the Internet-
based experiment found. But even more impor-
tantly, all pairwise rankings are consistent across
the two evaluations: Where both systems found a
significant difference between two systems, they al-
ways ranked them in the same order. We conclude
that the Internet experiment provides significance
judgments that are comparable to, and in fact more
precise than, the laboratory experiment.
Nevertheless, there are important differences be-
tween the laboratory and Internet-based results. For
instance, the success rates in the laboratory tend
to be higher, but so are the completion times. We
believe that these differences can be attributed to
the demographic characteristics of the participants.
To substantiate this claim, we looked in some detail
at differences in gender, language proficiency, and
questionnaire response rates.
First, the gender distribution differed greatly be-
Web
games reported mean
success 227 = 61% 93% 4.9
lost 92 = 24% 48% 3.4
cancelled 55 = 15% 16% 3.3
Lab
# games reported mean
success 73 = 80% 100% 5.4
lost 18 = 20% 94% 3.3
cancelled 0 ? ?
Figure 2: Skewed results for ?overall evaluation?.
tween the Internet experiment (10% female) and
the laboratory experiment (65% female). This is
relevant because gender had a significant effect
on task completion time (women took longer) and
on six subjective measures including ?overall eval-
uation? in the laboratory. We speculate that the
difference in task completion time may be related
to well-known gender differences in processing
navigation instructions (Moffat et al, 1998).
Second, the two experiments collected data from
subjects of different language proficiencies. While
93% of the participants in the laboratory experi-
ment self-rated their English proficiency as ?expert?
or better, only 62% of the Internet participants did.
This partially explains the lower task success rates
on the Internet, as Internet subjects with English
proficiencies of 3?5 performed significantly better
on ?task success? than the group with proficiencies
1?2. If we only look at the results of high-English-
proficiency subjects on the Internet, the success
rates for all NLG systems except W rise to at least
86%, and are thus close to the laboratory results.
Finally, the Internet data are skewed by the ten-
dency of unsuccessful participants to not fill in the
questionnaire. Fig. 2 summarizes some data about
the ?overall evaluation? question. Users who didn?t
complete the task successfully tended to judge the
303
systems much lower than successful users, but at
the same time tended not to answer the question
at all. This skew causes the mean subjective judg-
ments across all Internet subjects to be artificially
high. To avoid differences between the laboratory
and the Internet experiment due to this skew, Fig. 1
includes only judgments from successful games.
In summary, we find that while the two experi-
ments made consistent significance judgments, and
the Internet-based evaluation methodology thus
produces meaningful results, the absolute values
they find for the individual evaluation measures
differ due to the demographic characteristics of the
participants in the two studies. This could be taken
as a possible deficit of the Internet-based evalua-
tion. However, we believe that the opposite is true.
In many ways, an online user is in a much more
natural communicative situation than a laboratory
subject who is being discouraged from cancelling
a frustrating task. In addition, every experiment ?
whether in the laboratory or on the Internet ? suf-
fers from some skew in the subject population due
to sampling bias; for instance, one could argue that
an evaluation that is based almost exclusively on na-
tive speakers in universities leads to overly benign
judgments about the quality of NLG systems.
One advantage of the Internet-based approach
to data collection over the laboratory-based one is
that, due to the sheer number of subjects, we can de-
tect such skews and deal with them appropriately.
For instance, we might decide that we are only
interested in the results from proficient English
speakers and ignore the rest of the data; but we
retain the option to run the analysis over all partici-
pants, and to analyze how much each system relies
on the user?s language proficiency. The amount
of data also means that we can obtain much more
fine-grained comparisons between NLG systems.
For instance, the second and third evaluation world
specifically exercised an NLG system?s abilities to
generate referring expressions and navigation in-
structions, respectively, and there were significant
differences in the performance of some systems
across different worlds. Such data, which is highly
valuable for pinpointing specific weaknesses of a
system, would have been prohibitively costly and
time-consuming to collect with laboratory subjects.
5 Conclusion
In this paper, we have argued that carrying out task-
based evaluations of NLG systems over the Internet
is a valid alternative to more traditional laboratory-
based evaluations. Specifically, we have shown
that an Internet-based evaluation of systems in the
GIVE Challenge finds consistent significant differ-
ences as a lab-based evaluation. While the Internet-
based evaluation suffers from certain skews caused
by the lack of control over the subject pool, it does
find more differences than the lab-based evaluation
because much more data is available. The increased
amount of data also makes it possible to compare
the quality of NLG systems across different evalua-
tion worlds and users? language proficiency levels.
We believe that this type of evaluation effort
can be applied to other NLG and dialogue tasks
beyond GIVE. Nevertheless, our results also show
that an Internet-based evaluation risks certain kinds
of skew in the data. It is an interesting question for
the future how this skew can be reduced.
References
A. Belz and A. Gatt. 2008. Intrinsic vs. extrinsic eval-
uation measures for referring expression generation.
In Proceedings of ACL-08:HLT, Short Papers, pages
197?200, Columbus, Ohio.
A. Belz. 2009. That?s nice ... what can you do with it?
Computational Linguistics, 35(1):111?118.
D. Byron, A. Koller, K. Striegnitz, J. Cassell, R. Dale,
J. Moore, and J. Oberlander. 2009. Report on the
First NLG Challenge on Generating Instructions in
Virtual Environments (GIVE). In Proceedings of the
12th European Workshop on Natural Language Gen-
eration (Special session on Generation Challenges).
R. Dale and M. White, editors. 2007. Proceedings
of the NSF/SIGGEN Workshop for Shared Tasks and
Comparative Evaluation in NLG, Arlington, VA.
A. Gatt, A. Belz, and E. Kow. 2008. The TUNA
challenge 2008: Overview and evaluation results.
In Proceedings of the 5th International Natural
Language Generation Conference (INLG?08), pages
198?206.
S. D. Gosling, S. Vazire, S. Srivastava, and O. P. John.
2004. Should we trust Web-based studies? A com-
parative analysis of six preconceptions about Inter-
net questionnaires. American Psychologist, 59:93?
104.
S. Moffat, E. Hampson, and M. Hatzipantelis. 1998.
Navigation in a ?virtual? maze: Sex differences and
correlation with psychometric measures of spatial
ability in humans. Evolution and Human Behavior,
19(2):73?87.
D. Scott and J. Moore. 2007. An NLG evaluation com-
petition? Eight reasons to be cautious. In (Dale and
White, 2007).
304
Dialogue Systems that can Handle Face-to-Face Joint                        
Reference to Actions in Space 
 
 Justine Cassell 
Dept. of Communication Studies 
Northwestern University 
Evanston, IL 60208 
justine@northwestern.edu
 
 
Abstract 
This talk introduces new research that works towards an 
overarching model of natural face-to-face conversation 
about spatially-located actions in the world, and then 
uses that model to implement a trustworthy embodied  
conversational agent to guide users' ongoing, real-world 
activities away  from the desktop. 
 
Past research has demonstrated that the relationship 
between verbal and nonverbal behavior exists at the 
level of intonational phrases,  conversational turns, dis-
course units, and the negotiation of reference to  objects 
and actions, that mental representations of shared space  
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
are structured in such a way as to allow participants in a 
dialogue to draw on  them, that dialogue systems must 
be based on models of coordination and collaboration, 
and that users are willing to engage in persistent, natu-
ral, trusting conversation with embodied conversational 
systems.  In this talk, these diverse strands of research 
are brought together in the service of a single underly-
ing modality-independent model of action and language, 
non-verbal behaviors and words, production and com-
prehension that can lead to a physically-located, spa-
tially-aware, collaborative embodied conversational 
agent. 
 
 
Proceedings of the Second ACL Workshop on Effective Tools and Methodologies for Teaching NLP and CL, pages 9?14,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Teaching Dialogue to Interdisciplinary Teams through Toolkits
Justine Cassell
Technology and Social Behavior
Northwestern University
justine@northwestern.edu
Matthew Stone
Computer Science and Cognitive Science
Rutgers University
matthew.stone@rutgers.edu
Abstract
We present some lessons we have learned
from using software infrastructure to
support coursework in natural language
dialogue and embodied conversational
agents. We have a new appreciation
for the differences between coursework
and research infrastructure?supporting
teaching may be harder, because students
require a broader spectrum of implemen-
tation, a faster learning curve and the abil-
ity to explore mistaken ideas as well as
promising ones. We outline the collabo-
rative discussion and effort we think is re-
quired to create better teaching infrastruc-
ture in the future.
1 Introduction
Hands-on interaction with dialogue systems is a nec-
essary component of a course on computational lin-
guistics and natural language technology. And yet, it
is clearly impracticable to have students in a quarter-
long or semester-long course build a dialogue sys-
tem from scratch. For this reason, instructors of
these courses have experimented with various op-
tions to allow students to view the code of a work-
ing dialogue system, tweak code, or build their own
application using a dialogue system toolkit. Some
popular options include the NLTK (Loper and Bird,
2002), CSLU (Cole, 1999), Trindi (Larsson and
Traum, 2000) and Regulus (Rayner et al, 2003)
toolkits. However, each of these options has turned
out to have disadvantages. Some of the toolkits re-
quire too much knowledge of linguistics for the av-
erage computer science student, and vice-versa, oth-
ers require too much programming for the average
linguist. What is needed is an extensible dialogue
toolkit that allows easy application building for be-
ginning students, and more sophisticated access to,
and tweakability of, the models of discourse for ad-
vanced students.
In addition, as computational linguists become in-
creasingly interested in the role of non-verbal be-
havior in discourse and dialogue, more of us would
like to give our students exposure to models of the
interaction between language and nonverbal behav-
iors such as eye gaze, head nods and hand gestures.
However, the available dialogue system toolkits ei-
ther have no graphical body or if they do have (part
of) a body?as in the case of the CSLU toolkit?the
toolkit does not allow the implementation of alterna-
tive models of body?language interaction.
We feel, therefore, that there is a need for a
toolkit that allows the beginning graduate student?
who may have some computer science or some lin-
guistics background, but not both?to implement a
working embodied dialogue system, as a way to ex-
periment with models of discourse, dialogue, collab-
orative conversation and the interaction between ver-
bal and nonverbal behavior in conversation. We be-
lieve the community as a whole must be engaged in
the design, implementation and fielding of this kind
of educational software. In this paper, we survey
the experience that has led us to these conclusions
and frame the broader discussion we hope the TNLP
workshop will help to further.
9
2 Our Courses
Our perspective in this paper draws on more than
fifteen course offerings at the graduate level in dis-
course and dialogue over the years. Justine Cassell?s
course Theories and Technologies of Human Com-
munication is documented on the web here:
http://www.soc.northwestern.edu/justine/discourse
Matthew Stone?s courses Natural Language Pro-
cessing and Meaning Machines1 are documented
here:
http://www.cs.rutgers.edu/?mdstone/class/533-spring-03/
http://www.cs.rutgers.edu/?mdstone/class/672
These courses are similar in perspective. All ad-
dress an extremely diverse and interdisciplinary au-
dience of students from computer science, linguis-
tics, cognitive science, information science, commu-
nication, and education. The typical student is a first
or second-year PhD student with a serious interest in
doing a dissertation on human-computer communi-
cation or in enriching their dissertation research with
results from the theory or practice of discourse and
dialogue. All are project courses, but no program-
ming is required; projects may involve evaluation of
existing implementations or the prospective design
of new implementations based on ongoing empir-
ical research. Nevertheless, the courses retain the
dual goals that students should not only understand
discourse and the theory of pragmatics, but should
also understand how the theory is implemented, ei-
ther well enough to talk intelligently about the im-
plementation or, if they are computer scientists, to
actually carry it out.
As befits our dual goals, our courses all involve
a mix of instruction in human-human dialogue and
human-computer dialogue. For example, Cassell be-
gins her course with a homework where students
collect, transcribe and analyze their own recordings
of face-to-face conversation. Students are asked to
discuss what constitutes a sufficient record of dis-
course, and to speculate on what the most challeng-
ing processing issues would be to allow a computer
to replace one of the participants. Computer sci-
entists definitely have difficulty with this aspect of
1The catchy title is the inspiration of Deb Roy at MIT.
the course?only fair, since they are at the advan-
tage when it comes to implementation. But com-
puter scientists see the value in the exercise: even
if they do not believe that interfaces should be de-
signed to act like people, they still recognize that
well-designed interactive systems must be ready to
handle the kinds of behaviors people actually carry
out. And hands-on experience convinces them that
behavior in human conversation is both rich and sur-
prising. The computer scientists agree?after turn-
ing in impoverished and uninformed ?analyses? of
their discourse for a brutal critique?that they will
never look at conversation the same way again.
Our experience suggests that we should be try-
ing to give students outside computer science the
same kind of eye-opening hands-on experience with
technology. For example, we have found that lin-
guists are just as challenged and excited by the dis-
cipline of technology as computer scientists are by
the discipline of empirical observations. Linguists
in our classes typically report that successful en-
gagement with technology ?exposes a lot of de-
tails that were missing from my theoretical under-
standing that I never would have considered with-
out working through the code?. Nothing is better at
bringing out the assumptions you bring to an anal-
ysis of human-human conversation than the thought
experiment of replacing one of the participants by
something that has to struggle consciously to un-
derstand it?a space alien, perhaps, or, more real-
istically, an AI system. We are frustrated that no
succinct assignment, comparable to our transcrip-
tion homework, yet exists that can reliably deliver
this insight to students outside computer science.
3 Framing the Problem
Our courses are not typical NLP classes. Our treat-
ment of parsing is marginal, and for the most part
we ignore the mainstays of statistical language pro-
cessing courses: the low-level technology such as
finite-state methods; the specific language process-
ing challenges for machine learning methods; and
?applied? subproblems like named entity extraction,
or phrase chunking. Our focus is almost exclu-
sively on high-level and interactional issues, such
as the structure of discourse and dialogue, informa-
tion structure, intentions, turn-taking, collaboration,
10
reference and clarification. Context is central, and
under that umbrella we explicitly discuss both the
perceptual environment in which conversation takes
place and the non-verbal actions that contribute to
the management of conversation and participants?
real-world collaborations.
Our unusual focus means that we can not readily
take advantage of software toolkits such as NLTK
(Loper and Bird, 2002) or Regulus (Rayner et al,
2003). These toolkits are great at helping students
implement and visualize the fundamentals of natu-
ral language processing?lexicon, morphology, syn-
tax. They make it easy to experiment with machine
learning or with specific models for a small scale,
short course assignment in a specific NLP module.
You can think of this as a ?horizontal? approach, al-
lowing students to systematically develop a compre-
hensive approach to a single processing task. But
what we need is a ?vertical? approach, which allows
students to follow a specific choice about the rep-
resentation of communicative behaviors or commu-
nicative functions all the way through an end-to-end
dialogue system. We have not succeeded in concep-
tualizing how a carefully modularized toolkit would
support this kind of student experience.
Still, we have not met with success with alterna-
tive approaches, either. As we describe in Section
3.1, our own research systems may allow the kinds
of experiments we want students to carry out. But
they demand too much expertise of students for a
one-semester course. In fact, as we describe in Sec-
tion 3.2, even broad research systems that come with
specific support for students to carry out a range of
tasks may not enable the specific directions that re-
ally turn students on to the challenge of discourse
and dialogue. However, our experience with im-
plementing dedicated modules for teaching, as de-
scribed in Section 3.3, is that the lack of synergy
with ongoing research can result in impoverished
tools that fail to engage students. We don?t have the
tools we want?but our experience argues that we
think the tools we really want will be developed only
through a collaborative effort shared across multiple
sites and broadly engaged with a range of research
issues as well as with pedagogical challenges.
3.1 Difficulties with REA and BEAT
Cassell has experimented with the use of her re-
search platforms REA (Cassell et al, 1999) and
BEAT (Cassell et al, 2001) for course projects in
discourse and dialogue. REA is an embodied con-
versational agent that interacts with a user in a real
estate agent domain. It includes an end-to-end dia-
logue architecture; it supports speech input, stereo
vision input, conversational process including pres-
ence and turn-taking, content planning, the context-
sensitive generation of communicative action and
the animated realization of multimodal communica-
tive actions. BEAT (the behavior expression anima-
tion toolkit), on the other hand, is a module that fits
into animation systems. It marks up text to describe
appropriate synchronized nonverbal behaviors and
speech to realize on a humanoid talking character.
In teaching dialogue at MIT, Cassell invited stu-
dents to adapt her existing REA and BEAT system
to explore aspects of the theory and practice of dis-
course and dialogue. This led to a range of interest-
ing projects. For example, students were able to ex-
plore hypothetical differences among characters?
from virtual ?Italians? with profuse gesture, to vir-
tual children whose marked use of a large gesture
space contrasted with typical adults, to characters
who showed new and interesting behavior such as
the repeated foot-tap of frustrated condescension.
However, we think we can serve students much bet-
ter. Many of these projects were accomplished only
with substantial help from the instructor and TAs,
who were already extremely familiar with the over-
all system. Students did not have time to learn how
to make these changes entirely on their own.
The foot-tapping agent is a good example of this.
To add foot-tapping is a paradigmatic ?vertical?
modification. It requires adding suitable context to
the discourse state to represent uncooperative user
behavior; it requires extending the process for gener-
ating communicative actions to detect this new state
and schedule an appropriate behavioral response;
and then it requires extending the animation plat-
form to be able to show this behavior. BEAT makes
the second step easy?as it should be?even for lin-
guistics students. To handle the first and third steps,
you would hope that an interdisciplinary team con-
taining a communication student and a computer sci-
11
ence student would be able to bring the expertise to
design the new dialogue state and the new animated
behavior. But that wasn?t exactly true. In order to
add the behavior to REA, students needed not only
background in the relevant technology?like what a
computer scientist would learn in a general human
animation class. To add the behavior, students also
needed to know how this technology was realized
in our particular research platform. This proved too
much for one semester.
We think this is a general problem with new re-
search systems. For example, we think many of the
same issues would arise in asking students to build a
dialogue system on top of the Trindi toolkit in a one
semester course.
3.2 Difficulties with the CSLU toolkit
In Fall 2004, Cassell experimented with using the
CSLU dialogue toolkit (Cole, 1999) as a resource
for class projects. This is a broad toolkit to support
research and teaching in spoken language technol-
ogy. A particular strength of the toolkit is its sup-
port for the design of finite-state dialogue models.
Even students outside computer science appreciated
the toolkit?s drag-and-drop interface for scripting di-
alogue flow. For example, with this interface, you
can add a repair sequence to a dialogue flow in one
easy step. However, the indirection the toolkit places
between students and the actual constructs of dia-
logue theory can by quite challenging. For example,
the finite-state architecture of the CSLU toolkit al-
lows students to look at floor management and at di-
alogue initiative only indirectly: specific transition
networks encode specific strategies for taking turns
or managing problem solving by scheduling specific
communicative functions and behaviors.
The way we see it, the CSLU toolkit is more heav-
ily geared towards the rapid construction of particu-
lar kinds of research prototypes than we would like
in a teaching toolkit. Its dialogue models provide an
instructive perspective on actions in discourse, one
that nicely complements the perspective of DAMSL
(Core and Allen, 1997) in seeing utterances as the
combined realization of a specific, constrained range
of communicative functions. But we would like to
be able to explore a range of other metaphors for
organizing the information in dialogue. We would
like students to be able to realize models of face-to-
face dialogue (Cassell et al, 2000), the information-
state approach to domain-independent practical di-
alogue (Larsson and Traum, 2000), or approaches
that emphasize the grounding of conversation in the
specifics of a particular ongoing collaboration (Rich
et al, 2001). The integration of a talking head into
the CSLU toolkit epitomizes these limitations with
the platform. The toolkit allows for the automatic
realization of text with an animated spoken deliv-
ery, but does not expose the model to programmers,
making it impossible for programmers adapt or con-
trol the behavior of the face and head.
We think this is a general problem with platforms
that are primarily designed to streamline a particular
research methodology. For example, we think many
of the same issues would arise in asking students to
build a multimodal behavior realization system on
top of a general-purpose speech synthesis platform
like Festival (Black and Taylor, 1997).
3.3 Difficulties with TAGLET
At this point, the right solution might seem to be
to devise resources explicitly for teaching. In fact,
Stone advocated more or less this at the 2002 TNLP
workshop (2002). There, Stone motivated the poten-
tial role for a simple lexicalized formalism for nat-
ural language syntax, semantics and pragmatics in
a broad NLP class whose emphasis is to introduce
topics of current research.
The system, TAGLET, is a context-free tree-
rewriting formalism, defined by the usual comple-
mentation operation and the simplest imaginable
modification operation. This formalism may in fact
be a good way to present computational linguistics
to technically-minded cognitive science students?
those rare students who come with interest and ex-
perience in the science of language as well as a solid
ability to program. By implementing a strong com-
petence TAGLET parser and generator students si-
multaneously get experience with central computer
science ideas?data structures, unification, recur-
sion and abstraction?and develop an effective start-
ing point for their own subsequent projects.
However, in retrospect, TAGLET does not serve
to introduce students outside computer science to the
distinctive insights that come from a computational
approach to language use. For one thing, to reach
a broad audience, it is a mistake to focus on repre-
12
sentations that programmers can easily build at the
expense of representations that other students can
easily understand. These other students need visu-
alization; they need to be able to see what the sys-
tem computes and how it computes it. Moreover,
these other students can tolerate substantial com-
plexity in the underlying algorithms if the system
can be understood clearly and mechanistically in ab-
stract terms. You wouldn?t ask a computer scientist
to implement a parser for full tree-adjoining gram-
mar but that doesn?t change the fact that it?s still a
perfectly natural, and comprehensible, algorithmic
abstraction for characterizing linguistic structure.
Another set of representations and algorithms
might avoid some of these problems. But a new
approach could not avoid another problem that we
think applies generally to platforms that are de-
signed exclusively for teaching: there is no synergy
with ongoing research efforts. Rich resources are so
crucial to any computational treatment of dialogue:
annotated corpora, wide-coverage grammars, plan-
recognizers, context models, and the rest. We can?t
afford to start from scratch. We have found this con-
cretely in our work. What got linguists involved in
the computational exploration of dialogue semantics
at Rutgers was not the special teaching resources
Stone created. It was hooking students up with the
systems that were being actively developed in ongo-
ing research (DeVault et al, 2005). These research
efforts made it practical to provide students with the
visualizations, task and context models, and interac-
tive architecture they needed to explore substantive
issues in dialogue semantics. Whatever we do will
have to closely connect teaching and our ongoing re-
search.
4 Looking ahead
Our experience teaching dialogue to interdisci-
plinary teams through toolkits has been humbling.
We have a new appreciation for the differences
between coursework and research infrastructure?
supporting teaching may be harder, because stu-
dents require a broader spectrum of implementa-
tion, a faster learning curve and the ability to ex-
plore mistaken ideas as well as promising ones.
But we increasingly think the community can and
should come together to foster more broadly useful
resources for teaching.
We have reframed our ongoing activities so that
we can find new synergies between research and
teaching. For example, we are currently working
to expand the repertoire of animated action in our
freely-available talking head RUTH (DeCarlo et al,
2004). In our next release, we expect to make dif-
ferent kinds of resources available than in the initial
release. Originally, we distributed only the model
we created. The next version will again provide that
model, along with a broader and more useful inven-
tory of facial expressions for it, but we also want
the new RUTH to be more easily extensible than the
last one. To do that, we have ported our model to a
general-purpose animation environment (Alias Re-
search?s Maya) and created software tools that can
output edited models into the collection of files that
RUTH needs to run. This helps achieve our ob-
jective of quickly-learned extensibility. We expect
that students with a background in human anima-
tion will bring experience with Maya to a dialogue
course. (Anyway, learning Maya is much more gen-
eral than learning RUTH!) Computer science stu-
dents will thus find it easier to assist a team of com-
munication and linguistics students in adding new
expressions to an animated character.
Creating such resources to span a general system
for face-to-face dialogue would be an enormous un-
dertaking. It could happen only with broad input
from those who teach discourse and dialogue, as we
do, through a mix of theory and practice. We hope
the TNLP workshop will spark this kind of process.
We close with the questions we?d like to consider
further. What kinds of classes on dialogue and dis-
course pragmatics are currently being offered? What
kinds of audiences do others reach, what goals do
they bring, and what do they teach them? What are
the scientific and technological principles that oth-
ers would use toolkits to teach and illustrate? In
short, what would your dialogue toolkit make possi-
ble? And how can we work together to realize both
our visions?
5 Acknowledgments
Thanks to Doug DeCarlo, NSF HLC 0308121.
13
References
Alan Black and Paul Taylor. 1997. Festi-
val speech synthesis system. Technical Report
HCRC/TR-83, Human Communication Research Cen-
ter. http://www.cstr.ed.ac.uk/projects/festival/.
J. Cassell, T. Bickmore, M. Billinghurst, L. Campbell,
K. Chang, H. Vilhja?lmsson, and H. Yan. 1999. Em-
bodiment in conversational characters: Rea. In CHI
99, pages 520?527.
Justine Cassell, Tim Bickmore, Lee Campbell, Hannes
Vilhjalmsson, and Hao Yan. 2000. Human conver-
sation as a system framework. In J. Cassell, J. Sul-
livan, S. Prevost, and E. Churchill, editors, Embod-
ied Conversational Agents, pages 29?63. MIT Press,
Cambridge, MA.
Justine Cassell, Hannes Vilhja?lmsson, and Tim Bick-
more. 2001. BEAT: the behavioral expression ani-
mation toolkit. In SIGGRAPH, pages 477?486.
Ron Cole. 1999. Tools for research and ed-
ucation in speech science. In Proceedings of
the International Conference of Phonetic Sciences.
http://cslu.cse.ogi.edu/toolkit/.
Mark G. Core and James F. Allen. 1997. Cod-
ing dialogs with the DAMSL annotation scheme.
In Working Notes of AAAI Fall Symposium on
Communicative Action in Humans and Machines.
http://www.cs.rochester.edu/research/cisd/resources/damsl/.
Douglas DeCarlo, Corey Revilla, Matthew Stone, and
Jennifer Venditti. 2004. Specifying and animating fa-
cial signals for discourse in embodied conversational
agents. Journal of Visualization and Computer Ani-
mation. http://www.cs.rutgers.edu/?village/ruth/.
David DeVault, Anubha Kothari, Natalia Kariaeva,
Iris Oved, and Matthew Stone. 2005. An
information-state approach to collaborative ref-
erence. In ACL Proceedings Companion Vol-
ume (interactive poster and demonstration track).
http://www.cs.rutgers.edu/?mdstone/pointers/collabref.html.
Staffan Larsson and David Traum. 2000. In-
formation state and dialogue management in
the TRINDI dialogue move engine toolkit.
Natural Language Engineering, 6:323?340.
http://www.ling.gu.se/projekt/trindi/.
Edward Loper and Steven Bird. 2002. NLTK: the natu-
ral language toolkit. In Proceedings of the ACL Work-
shop on Effective Tools and Methodologies for Teach-
ing Natural Language Processing and Computational
Linguistics. http://nltk.sourceforge.net.
Manny Rayner, Beth Ann Hockey, and John Dowd-
ing. 2003. An open source environment for com-
piling typed unification grammars into speech recog-
nisers. In Proceedings of the 10th Conference of the
European Chapter of the Association for Computa-
tion Linguistics (interactive poster and demo track).
http://sourceforge.net/projects/regulus.
C. Rich, C. L. Sidner, and N. Lesh. 2001. COL-
LAGEN: applying collaborative discourse theory to
human-computer interaction. AI Magazine, 22:15?25.
Matthew Stone. 2002. Lexicalized grammar 101.
In ACL Workshop on Effective Tools and Method-
ologies for Teaching NLP and CL, pages 76?83.
http://www.cs.rutgers.edu/?mdstone/class/taglet/.
14
Proceedings of the Analyzing Conversations in Text and Speech (ACTS) Workshop at HLT-NAACL 2006, pages 15?22,
New York City, New York, June 2006. c?2006 Association for Computational Linguistics
Computational Measures for Language Similarity across Time 
 in Online Communities 
 
David Huffaker   Joseph Jorgensen    Francisco Iacobelli    Paul Tepper   Justine Cassell  
Northwestern University 
{d-huffaker, josephj, f-iacobelli, ptepper, justine}@northwestern.edu 
 
 
Abstract 
This paper examines language similarity 
in messages over time in an online com-
munity of adolescents from around the 
world using three computational meas-
ures: Spearman?s Correlation Coefficient, 
Zipping and Latent Semantic Analysis.  
Results suggest that the participants? lan-
guage diverges over a six-week period, 
and that divergence is not mediated by 
demographic variables such as leadership 
status or gender.  This divergence may 
represent the introduction of more unique 
words over time, and is influenced by a 
continual change in subtopics over time, 
as well as community-wide historical 
events that introduce new vocabulary at 
later time periods. Our results highlight 
both the possibilities and shortcomings of 
using document similarity measures to as-
sess convergence in language use. 
1 Introduction 
While document similarity has been a concern in 
computational linguistics for some time, less atten-
tion has been paid to change in similarity across 
time.  And yet, while historical linguists have long 
addressed the issue of divergence or convergence 
among language groups over long periods of time, 
there has also been increasing interest in conver-
gence (also referred to as entrainment, speech ac-
commodation, or alignment) in other areas of 
Linguistics, with the realization that we have little 
understanding of change in very short periods of 
time, such as months, in a particular conversational 
setting, between two people, or in a large group. 
The Internet provides an ideal opportunity to ex-
amine questions of this sort since all texts perse-
vere for later analysis, and the diversity in kinds of 
online communities ensures that the influence of 
social behavior on language can be examined. Yet 
there has been very little work on language similar-
ity in online communities.  
In this paper we compare the use of three sepa-
rate tools to measure document or message similar-
ity in a large data set from an online community of 
over 3,000 participants from 140 different coun-
tries.  Based on a review of related work on corpus 
similarity measures and document comparison 
techniques (Section 2.2), we chose Spearman?s 
Correlation Coefficient, a comparison algorithm 
that utilizes GZIP (which we will refer to as ?Zip-
ping?) and Latent Semantic Analysis. These three 
tools have all been shown effective for document 
comparison or corpus similarity, but never to our 
knowledge have any of them been used for docu-
ment similarity over time, nor have they been 
compared to one another. Even though each of 
these tools is quite different in what it specifically 
measures and how it is used, and each has been 
used by quite different communities of researchers, 
they are all fairly well-understood (Section 4).  
2 Related Work  
In the next sections, we review literature on lan-
guage similarity or convergence.  We also review 
literature on the three computational tools, Spear-
man?s Correlation Coefficient (SCC), Zipping, and 
Latent Semantic Analysis (LSA). 
2.1 Language Similarity in Computer-
mediated Communication 
In dyadic settings, speakers often converge to one 
another?s speech styles, not only matching the 
choice of referring expressions or other words, but 
also structural dimensions such as syntax, sound 
characteristics such as accent, prosody, or phonol-
15
ogy, or even non-verbal behaviors such as gesture 
(Brennan & Clark, 1996; Street & Giles, 1982). 
Some scholars suggest that this convergence or 
entrainment is based on a conscious need to ac-
commodate to one?s conversational partner, or as a 
strategy to maximize communication effectiveness 
(Street & Giles, 1982). Others suggest that the 
alignment is an automatic response, in which 
echoic aspects of speech, gesture and facial expres-
sions are unconscious reactions (Garrod & Ander-
son, 1987; Lakin, Jefferies, Cheng, & Chartrand, 
2003).  In short, conversational partners tend to 
accommodate to each other by imitating or match-
ing the semantic, syntactic and phonological char-
acteristics of their partners (Brennan & Clark, 
1996; Garrod & Pickering, 2004). 
Many studies have concentrated on dyadic inter-
actions, but large-scale communities also demon-
strate language similarity or convergence. In fact, 
speech communities have a strong influence in cre-
ating and maintaining language patterns, including 
word choice or phonological characteristics 
(Labov, 2001). Language use often plays an impor-
tant role in constituting a group or community 
identity (Eckert, 2003).  For example, language 
?norms? in a speech community often result in the 
conformity of new members in terms of accent or 
lexical choice (Milroy, 1980).  This effect has been 
quite clear among non-native speakers, who 
quickly pick up the vernacular and speech patterns 
of their new situation (Chambers, 2001), but the 
opposite is also true, with native speakers picking 
up speech patterns from non-native speakers (Auer 
& Hinskens, 2005) 
Linguistic innovation is particularly salient on 
the Internet, where words and linguistic patterns 
have been manipulated or reconstructed by indi-
viduals and quickly adopted by a critical mass of 
users (Crystal, 2001).  Niederhoffer & Pennebaker 
(2002) found that users of instant messenger tend 
to match each other?s linguistic styles.  A study of 
language socialization in a bilingual chat room 
suggests that participants developed particular lin-
guistic patterns and both native and non-native 
speakers were influenced by the other (Lam, 
2004).  Similar language socialization has been 
found in ethnographic research of large-scale 
online communities as well, in which various ex-
pressions are created and shared by group mem-
bers (Baym, 2000; Cherny, 1999). 
Other research not only confirms the creation of 
new linguistic patterns online, and subsequent 
adoption by users, but suggests that the strength of 
the social ties between participants influences how 
patterns are spread and adopted (Paolillo, 2001).  
However, little research has been devoted to how 
language changes over longer periods of time in 
these online communities. 
2.2 Computational Measures of Language 
Similarity 
The unit of analysis in online communities is the 
(e-mail or chat) message.  Therefore, measuring 
entrainment in online communities relies on as-
sessing whether or not similarity between the mes-
sages of each participant increases over time. Most 
techniques for measuring document similarity rely 
on the analysis of word frequencies and their co-
occurrence in two or more corpora (Kilgarriff, 
2001), so we start with these techniques. 
Spearman?s Rank Correlation Coefficient (SCC) 
is particularly useful because it is easy to compute 
and not dependent on text size. Unlike some other 
statistical approaches (e.g. chi-square), SCC has 
been shown effective on determining similarity 
between corpora of varying sizes, therefore SCC 
will serve as a baseline for comparison in this pa-
per (Kilgarriff, 2001). 
More recently, researchers have experimented 
with data compression algorithms as a measure of 
document complexity and similarity.  This tech-
nique uses compression ratios as an approximation 
of a document?s information entropy (Baronchelli, 
Caglioti, & Loreto, 2005; Benedetto, Caglioti, & 
Loreto, 2002).  Standard Zipping algorithms have 
demonstrated effectiveness in a variety of docu-
ment comparison and classification tasks. Behr et 
al. (2003) found that a document and its translation 
into another language compressed to approxi-
mately the same size. They suggest that this could 
be used as an automatic measure for testing ma-
chine translation quality. Kaltchenko (2004) argues 
that using compression algorithms to compute rela-
tive entropy is more relevant than using distances 
based on Kolmogorov complexity. Lastly, Ben-
detto et al (2002) present some basic findings us-
ing GZIP for authorship attribution, determining 
the language of a document, and building a tree of 
language families from a text written in different 
languages.  Although Zipping may be a conten-
16
tious technique, these results present intriguing 
reasons to continue exploration of its applications. 
Latent Semantic Analysis is another technique 
used for measuring document similarity.  LSA em-
ploys a vector-based model to capture the seman-
tics of words by applying Singular Value 
Decomposition on a term-document matrix 
(Landauer, Foltz, & Laham, 1998).  LSA has been 
successfully applied to tasks such as measuring 
semantic similarity among corpora of texts 
(Coccaro & Jurafsky, 1998), measuring cohesion 
(Foltz, Kintsch, & Landauer, 1998 ), assessing cor-
rectness of answers in tutoring systems (Wiemer-
Hastings & Graesser, 2000) and dialogue act clas-
sification (Serafin & Di Eugenio, 2004). 
To our knowledge, statistical measures like 
SCC, Zipping compression algorithms, or LSA 
have never been used to measure similarity of mes-
sages over time, nor have they been applied to 
online communities. However, it is not obvious 
how we would verify their performance, and given 
the nature of the task ? similarity in over 15,000 e-
mail messages ? it is impossible to compare the 
computational methods to hand-coding. As a pre-
liminary approach, we therefore decided to apply 
all three methods in turn to the messages in an 
online community to examine change in linguistic 
similarity over time, and to compare their results. 
Through the combination of lexical, phrasal and 
semantic similarity metrics, we hope to gain in-
sight into the questions of whether entrainment 
occurs in online communities, and of what compu-
tational measures can be used to measure it. 
2.3 The Junior Summit  
The Junior Summit launched in 1998 as a closed 
online community for young people to discuss how 
to use technology to make the world better.  3000 
children ages 10 to 16 participated in 1000 teams 
(some as individuals and some with friends).  Par-
ticipants came from 139 different countries, and 
could choose to write in any of 5 languages.  After 
2 weeks online, the young people divided into 20 
topic groups of their own choosing.  Each of these 
topic groups functioned as a smaller community 
within the community of the Junior Summit; after 
another 6 weeks, each topic group elected 5 dele-
gates to come to the US for an in-person forum.  
The dataset from the Junior Summit comprises 
more than 40,000 e-mail messages; however, in the 
current paper we look at only a sub-set of these 
data ? messages written in English during the 6-
week topic group period.  For complete details, 
please refer to Cassell & Tversky (2005).  
3 The Current Study 
In this paper, we examine entrainment among 419 
of the 1000 user groups (the ones who wrote in 
English) and among the 15366 messages they 
wrote over a six-week period (with participants 
divided into 20 topic groups, with an average of 
20.95 English writers per group).  We ask whether 
the young people?s language converges over time 
in an online community. Is similarity between the 
texts that are produced by the young people greater 
between adjacent weeks than between the less 
proximally-related weeks? Furthermore, what 
computational tools can effectively measure trends 
in similarity over time? 
3.1 Hypotheses 
In order to address these questions, we chose to 
examine change in similarity scores along two di-
mensions: (1) at the level of the individual; and (2) 
across the group as a whole. More specifically, we 
examine similarity between all pairs of individuals 
in a given topic group over time.  We also com-
pared similarity across the entire group at different 
time periods.  
As depicted below, we first look at pairwise 
comparisons between the messages of participants 
in a particular topic group within a given time pe-
riod, Tk (one week). For every pair of participants 
in a group, we calculated the similarity between 
two documents, each comprising all messages for a 
participant in the pair.  Then we averaged the 
scores computed for all topic groups within a time 
period Tk and produced PTk, the average, pairwise 
similarity score for Tk.  Our first hypothesis is that 
the average, pairwise similarity will increase over 
time, such that: 
 
PT1 < PT2 < PT3 < PT4 < PT5 < PT6 
 
For our second set of tests, we compared all 
messages from a single time period to all messages 
of a previous time period within a single topic 
group. Our hypothesis was that temporal proximity 
would correlate with mean similarity, such that the 
messages of two adjacent time periods would ex-
hibit more similarity than those of more distant 
17
time periods.  In order to examine this, we perform 
two individual hypothesis tests, where Mk is the 
document containing all the messages produced in 
time period Tk, and S(X,Y) is the similarity score 
for the two documents X and Y. 
 
a) S(Mk, Mk-1) > S(Mk, Mk-2) 
b) S(Mk, Mk-1) > S(Mk, M1) 
 
Finally, we posit that SCC, Zipping and LSA 
will yield similar results for these tests. 
4 Method  
To prepare the data, we wrote a script to remove 
the parts of messages that could interfere with 
computing their similarity, in particular quoted 
messages and binary attachments, which are com-
mon in a corpus of email-like messages.  We also 
removed punctuation and special characters. 
4.1 Spearman?s Correlation Coefficient 
SCC is calculated as in Kilgarriff (2001). First, we 
compile a list of the common words between the 
two documents. The statistic can be calculated on 
the n most common words, or on all common 
words (i.e. n = total number of common words). 
We applied the latter approach, using all the words 
in common for each document pair. For each docu-
ment, the n common words are ranked by fre-
quency, with the lowest frequency word ranked 1 
and the highest ranked n. For each common word, 
d is the difference in rank orders for the word in 
each document. SCC a normalized sum of the 
squared differences:  
 
The sum is taken over the n most frequent common 
words.  In the case of ties in rank, where more than 
one word in a document occurs with the same fre-
quency, the average of the ranks is assigned to the 
tying words. (For example, if words w1, w2 and w3 
are ranked 5th, 6th and 7th then all three words 
would be assigned the same rank of 5 6 73
+ + = 6). 
4.2 Zipping 
When compressing a document, the resulting com-
pression ratio provides an estimate of the docu-
ment's entropy. Many compression algorithms 
generate a dictionary of sequences based on fre-
quency that is used to compress the document. 
Likewise, one can leverage this technique to de-
termine the similarity between two documents by 
assessing how optimal the dictionary generated 
when compressing one document is when applied 
to another document. We used GZIP for compres-
sion, which employs a combination of the LZ77 
algorithm and Huffman coding.  We based our ap-
proach on the algorithm used by (Benedetto, 
Caglioti, & Loreto, 2002), where the cross-entropy 
per character is defined as:  
Here, A and B are documents; A B+  is docu-
ment B appended to document A; zip(A) is the 
zipped document; and length(A) is the length of the 
document. It is important to note that the test 
document (B) needs to be small enough that it 
doesn't cause the dictionary to adapt to the ap-
pended piece. (Benedetto, Caglioti, & Loreto, 
2002) refer to this threshold as the crossover 
length. The more similar the appended portion is, 
the more it will compress, and vice versa.  We ex-
tended the basic algorithm to handle the extremely 
varied document sizes found in our data. Our algo-
rithm does two one-way comparisons and returns 
the mean score. Each one-way comparison be-
tween two documents, A and B, is computed by 
splitting B into 300 character chunks. Then for 
each chunk, we calculated the cross entropy per 
character when appending the chunk onto A. Each 
one-way comparison returns the mean calculation 
for every chunk.  
We fine-tuned the window size with a small, 
hand-built corpus of news articles.  The differences 
are slightly more pronounced with larger window 
sizes, but that trend starts to taper off between 
window sizes of 300 and 500 characters.  In the 
end we chose 300 as our window size, because it 
provided sufficient contrast and yet still gave a few 
samples from even the smallest documents in our 
primary corpus. 
4.3 Latent Semantic Analysis (LSA) 
For a third approach, we used LSA to analyze the 
semantic similarity between messages across dif-
ferent periods of time. We explored three imple-
length(zip( )) length(zip( ))
length( ) 
A B A
B
+ ?
2
2
6
1
( 1)
d
n n
? = ? ?
?
18
mentations of LSA: (a) the traditional algorithm 
described by Foltz et al(1998 )  with one semantic 
space per topic group, (b) the same algorithm but 
with one semantic space for all topic groups and 
(c) an implementation based on Word Space 
(Schutze, 1993) called Infomap. All three were 
tested with several settings such as variations in the 
number of dimensions and levels of control for 
stop words, and all three demonstrated similar re-
sults.  For this paper, we present the Infomap re-
sults due to its wide acceptance among scholars as 
a successful implementation of LSA.  
To account for nuances of the lexicon used in 
the Junior Summit data, we built a semantic space 
from a subset of this data comprised of 7000 small 
messages (under one kb) and 100 dimensions with-
out removing stop words. We then built vectors for 
each document and compared them using cosine 
similarity (Landauer, Foltz, & Laham, 1998).  
5 Results 
The tools we employ approach document similarity 
quite differently; we therefore compare findings as 
a way of triangulating on the nature of entrainment 
in the Junior Summit online community.  
5.1 Pairwise Comparisons over Time 
First, we hypothesized that messages between in-
dividuals in a given topic group would demonstrate 
more similarity over time.  Our findings did not 
support this claim; in fact, they show the opposite.  
All three tests show slight convergence between 
time period one and two, some variation, and then 
divergence between time periods four, five and six. 
Spearman?s Correlation Coefficient demon-
strates a steady decline in similarity.  As shown in 
Figure 1, the differences between time periods 
were all significant, F(5,1375) = 21.475, p<.001, 
where N=1381 (N represents user pairs across all 
six time periods).  
Zipping also shows a significant difference be-
tween each time period, F(5,1190) = 39.027, p<.001, 
N=1196, demonstrating a similar decline in simi-
larity, although not as unwavering.  See Figure 2. 
LSA demonstrates the same divergent trend over 
time, F(5,1410) = 27.139, p<.001, N=1416, with a 
slight spike at T4 and T5.  While the dip at time 3 is 
more pronounced than SCC and Zipping, it is still 
consistent with the overall findings of the other 
measures. See Figure 3. 
 
0.495
0.505
0.515
0.525
0.535
0.545
0.555
0.565
0.575
T1 T2 T3 T4 T5 T6
Time Period  
Figure 1. Spearman's Correlation Coefficient Simi-
larity Scores for all Pairwise comparisons, T1 ?   T6 
 
0.535
0.54
0.545
0.55
0.555
0.56
0.565
0.57
0.575
0.58
T1 T2 T3 T4 T5 T6
Time Period  
Figure 2. Zipping Similarity Scores for all Pairwise 
comparisons, T1 ? T6 
 
0.82
0.83
0.84
0.85
0.86
0.87
0.88
0.89
0.9
0.91
T1 T2 T3 T4 T5 T6
Time Period  
Figure 3. LSA Similarity Scores for all Pairwise 
comparisons, T1 ? T6. 
 
Because of these surprising findings, we exam-
ined the influence of demographic variables, such 
as leadership (those chosen as delegates from each 
topic group to the in-person forum), gender, and 
the particular topic groups the individuals were a 
part of. We divided delegate pairs into (a) pairs 
where both individuals are delegates; (b) pairs 
where both individuals are non-delegates; and (c) 
mixed pairs of delegates and non-delegates.  Simi-
larly, gender pairs were divided into same-sex 
(e.g., male-male, female-female) and mixed-sex 
19
pairs.  For topic groups, we re-ran our analyses on 
each of the 20 topic groups separately. 
Overall, both leaders and gender pairs demon-
strate the same divergent trends as the group as a 
whole.  However, not all tests showed significant 
differences when comparing these pairs. 
For instance, Spearman?s Correlation Coeffi-
cient found a significant difference in similarity 
between three groups, where F(2,273) = 6.804, 
p<.001, n=276, such that delegate-delegate pairs 
demonstrate higher similarity scores than non-
delegate pairs and mixed pairs.  LSA found the 
same result, F(2,280) = 11.122,  p<.001 n=283. By 
contrast, Zipping did not find this to be the case, 
where F(2,226) = 2.568, p=.079, n=229. 
In terms of the potential effect of gender on 
similarity scores, Zipping showed a significant dif-
ference between the three groups, F(2,236) = 3.546, 
p<.05, n=239, such that female-female pairs and 
mixed-sex pairs demonstrate more similarity than 
male-male pairs. LSA found the same relationship, 
F(2,280) = 4.79, p<.005 n=283.  By contrast, Spear-
man?s Correlation Coefficient does not show a sig-
nificant between-groups difference, F(2,273) = .699, 
p=.498, n=276.  
In terms of differences among the topic groups, 
we did indeed find differences such that some topic 
groups demonstrated the fairly linear slope with 
decreasingly similarity shown above, while others 
demonstrated dips and rises resulting in a level of 
similarity at T6 quite similar to T1.  There is no 
neat way to statistically measure the differences in 
these slopes, but it does indicate that future analy-
ses need to take topic group into account.  
In sum, we did not find leadership or gender to 
mediate language similarity in this community.  
Topic group, on the other hand, did play a role, 
however no topic groups showed increasing simi-
larity across time. 
5.2 Similarity and Temporal Proximity 
Our second hypothesis concerned the gradual 
change of language over time such that temporal 
proximity of time periods would correlate with 
mean similarity.  In other words, we expect that 
messages in close time periods (e.g., adjacent 
weeks) should be more similar than messages from 
more distant time periods.  In order to examine 
this, we performed two individual tests, in which 
our predictions can be described as follows: (a) the 
similarity between texts in one time period and 
texts in the neighboring time period is greater than 
texts in one time period, and texts that came two 
periods previously, S(Mk, Mk-1) > S(Mk, Mk-2); and 
(b) the similarity between texts in one time period 
and texts in the neighboring time period is greater 
than the similarity between texts in one time pe-
riod, and texts in the very first time period, S(Mk, 
Mk-1) > S(Mk, M1). 
As shown in Table 1, SCC and Zipping tests 
confirm these hypotheses, while none of the LSA 
tests revealed significant differences.  
 
Table 1. Temporal Proximity Similarities SCC, 
Zipping, and LSA, n=20 topic groups 
 S(Mk,Mk-1)  
> S(Mk ,Mk-2) 
S(Mk,Mk-1)  
> S(Mk ,M1) 
S(Mk,Mk-2)  
> S(Mk ,M1) 
SCC .665 > .653? .665 > .639? .653 > .639? 
ZIP .628 > .608? .628 > .605? .608 > .605? 
LSA 9.74 > .971 9.74 > .971 .97166 < .97168 
Note: *p<.05, ?p<.01, ?p<.001, ?p = .0525, one-tailed 
6 Discussion 
This work presents several novel contributions 
to the analysis of text-based messages in online 
communities. Using three separate tools, Spear-
man?s Correlation Coefficient, Zipping and Latent 
Semantic Analysis measures, we found that across 
time, members of an online community diverge in 
the language they use.  More specifically, a com-
parison of the words contributed by any pair of 
users in a particular topic group shows increasing 
dissimilarity over the six-week period. 
This finding seems counter-intuitive given work 
in linguistics and psychology, which shows that 
dyads and communities converge, entrain and echo 
each other?s lexical choices and communication 
styles.  Similarly, our own temporal proximity re-
sults appear to indicate convergence, since closer 
time periods are more similar than more distant 
ones.  Finally, previous hand-coding of these data 
revealed convergence, for example between boys 
and girls on the use of emotion words, between 
older and younger children on talk about the future 
(Cassell & Tversky, 2005).  So we ask, why do our 
tools demonstrate this divergent trend? 
We believe that one answer comes from the fact 
that, while the young people may be discussing a 
more restricted range of topics, they are contribut-
ing a wider variety of vocabulary.  In order to ex-
amine whether indeed there were more unique 
20
words over time, we first simply manually com-
pared the frequency of words over time and found 
that, on the contrary, there are consistently fewer 
unique words by T6, which suggests convergence.  
However, there are also fewer and fewer total 
words by the end of the forum.  This is due to the 
number of participants who left the forum after 
they were not elected to go to Boston.  If we divide 
the unique words by the total words, we find that 
the ratio of unique words consistently increases 
over time (see Figure 4).  It is likely that this ratio 
contributes to our results of divergence. 
0.03
0.035
0.04
0.045
0.05
0.055
0.06
0.065
0.07
0.075
1 2 3 4 5 6
Time Period  
Figure 4. Ratio of Unique to Total Words, T1 ? T6 
 
In order to further examine the role of increasing 
vocabulary in the Junior Summit as a whole, we 
also created several control groups comprised of 
random pairs of users (i.e., users that had never 
written to each other), and measured their pairwise 
similarity across time.  The results were similar to 
the experimental groups, demonstrating a slope 
with roughly the same shape.  This argues for con-
vergence and divergence being affected by some-
thing at a broader, community-level such as an 
increase in vocabulary.   
This result is interesting for an additional rea-
son.  Some users ? perhaps particularly non-native 
speakers or younger adolescents, may be learning 
new vocabulary from other speakers, which they 
begin to introduce at later time periods.  An in-
creasingly diversified vocabulary could conceiva-
bly result in differences in word frequency among 
speakers.  This leads us to some key questions: to 
what extent does the language of individuals 
change over time? Is individual language influ-
enced by the language of the community?  This is 
heart of entrainment. 
In conclusion, we have shown that SCC, Zip-
ping and LSA can be used to assess message simi-
larity over time, although they may be somewhat 
blunt instruments for our purposes. In addition, 
while Zipping is somewhat contentious and not as 
widely-accepted as SCC or LSA is, we found that 
the three tools provide very similar results. This is 
particularly interesting given that, while all three 
methods take into account word or word-sequence 
frequencies, LSA is designed to also take into ac-
count aspects of semantics beyond the surface 
level of lexical form.  
All in all, these tools not only contribute to ways 
of measuring similarity across documents, but can 
be utilized in measuring smaller texts, such as 
online messages or emails.  Most importantly, 
these tools remind us how complex and dynamic 
everyday language really is, and how much this 
complexity must be taken into account when build-
ing computational tools for the analysis of text and 
conversation. 
6.1 Future Directions 
In future work, we intend to find ways to compare 
the results obtained from different topic groups and 
also to examine differences among individual us-
ers, including re-running our analyses after remov-
ing outliers.  We also hope to explore the interplay 
between individuals and the community and 
changes in language similarity.  In other words, 
can we find those individuals who may be acquir-
ing new vocabulary? Are there ?language leaders? 
responsible for language change online?   
We also plan to analyze words in terms of their 
local contexts, to see if this changes over time and 
how it impacts our results.  Furthermore, we intend 
to go beyond word frequency to classify topic 
changes over time to get a better understanding of 
the dynamics of the groups (Kaufmann, 1999). 
Finally, as we have done in the past with our 
analyses of this dataset, we would like to perform a 
percentage of hand-coded, human content analysis 
to check reliability of these statistical methods. 
Acknowledgements 
Thanks to members of the Articulab, Stefan Kauf-
mann, Stefan Wuchty, Will Thompson, Debbie 
Zutty and Lauren Olson for invaluable input. This 
research was in part supported by a generous grant 
from the Kellogg Foundation. 
References  
Auer, P., & Hinskens, F. (2005). The role of interper-
sonal accommodation in a theory of language 
change. In P. Auer, F. Hinskens & P. Kerswill 
21
(Eds.), Dialect change: The convergence and 
divergence of dialects in European languages 
(pp. 335-357). Cambridge, MA: Cambridge 
University Press. 
Baronchelli, A., Caglioti, E., & Loreto, V. (2005). Arti-
ficial sequences and complexity measures. 
Journal of Statistical Mechanics: Theory and 
Experiment, P04002, 1-26. 
Baym, N. K. (2000). Tune in, log on: Soaps, fandom, 
and online community. New York: Sage Publi-
cations. 
Benedetto, D., Caglioti, E., & Loreto, V. (2002). Lan-
guage trees and zipping. Physical Review Let-
ters, 88(4), 1-4. 
Brennan, S. E., & Clark, H. H. (1996). Conceptual pacts 
and lexical choice in conversation. Journal of 
Experimental Psychology: Learning, Memory, 
and Cognition, 22(6), 1482-1493. 
Cassell, J., & Tversky, D. (2005). The language of 
online intercultural community formation. 
Journal of Computer-Mediated Communica-
tion, 10(2), Article 2. 
Chambers, J. K. (2001). Dynamics of dialect conver-
gence. Journal of Sociolinguistics, 6(1), 117-
130. 
Cherny, L. (1999). Conversation and Community: Chat 
in a Virtual World. Stanford: Center for the 
Study of Language and Information. 
Coccaro, N., & Jurafsky, D. (1998, November 1998). 
Towards better integration of semantic predic-
tors in statistical language modeling. Paper 
presented at the International Conference on 
Spoken Language Processing (ICSLP-98), 
Sidney, Australia. 
Crystal, D. (2001). Language and the Internet. New 
York: Cambridge University Press. 
Eckert, P. (2003). Language and adolescent peer groups. 
Journal of Language and Social Psychology, 
22(1), 112-118. 
Foltz, P. W., Kintsch, W., & Landauer, T. K. (1998 ). 
The measurement of textual Coherence with 
Latent Semantic Analysis. Discourse Proc-
esses, 25, 285-307. 
Garrod, S., & Anderson, A. (1987). Saying what you 
mean in dialogue: A study in conceptual and 
semantic coordination. Cognition, 27, 181-218. 
Garrod, S., & Pickering, M. J. (2004). Why is conversa-
tion so easy? Trends in Cognitive Sciences, 
8(1), 8-11. 
Kalthchenko, A. (2004, May 2-5, 2004). Algorithms for 
estimation of information distance with appli-
cation to bioinformatics and linguistics. Paper 
presented at the Canadian Conference on Elec-
trical and Computer Engineering (CCECE 
2004), Niagara Falls, Ontario, Canada. 
Kaufmann, S. (1999). Cohesion and collocation: Using 
context vectors in text segmentation. Paper pre-
sented at the 37th Annual Meeting of the Asso-
ciation for Computational Linguistics, College 
Park, MD. 
Kilgarriff, A. (2001). Comparing corpora. International 
Journal of Corpus Linguistics, 6(1), 97-133. 
Labov, W. (2001). Principles of linguistic change (Vol. 
2: Social Factors). Oxford: Blackwell Publish-
ers. 
Lakin, J. L., Jefferies, V. E., Cheng, C. M., & Char-
trand, T. L. (2003). The chameleon effect as 
social glue: Evidence for the evolutionary sig-
nificance of nonconscious mimicry. Journal of 
Nonverbal Behavior, 27(3), 145-162. 
Lam, W. S. E. (2004). Second language socialization in 
a bilingual chat room: Global and local consid-
erations. Language Learning & Technology, 
8(3), 44-65. 
Landauer, T. K., Foltz, P. W., & Laham, D. (1998). In-
troduction to latent semantic analysis. Dis-
course Processes, 25, 259-284. 
Milroy, L. (1980). Language and social networks. Ox-
ford: Blackwell Publishers. 
Niederhoffer, K. G., & Pennebaker, J. W. (2002). Lin-
guistic style matching in social interaction. 
Journal of Language and Social Psychology, 
21(4), 337-360. 
Paolillo, J. (2001). Language variation on internet relay 
chat: A social network approach. Journal of 
Sociolinguistics, 5(2), 180-213. 
Schutze, H. (1993). Word space. In S. J. Hanson, J. D. 
Cowan & C. L. Giles (Eds.), Advances in Neu-
ral Information Processing Systems 5. San 
Mateo, CA: Morgan Kaufmann Publishers. 
Serafin, R., & Di Eugenio, B. (2004, July 21-26, 2004). 
FLSA: Extending latent semantic analysis with 
features for dialogue act classification. Paper 
presented at the 42nd Annual Meeting for the 
Association of Computational Linguistics 
(ACL04), Barcelona, Spain. 
Street, R. L., & Giles, H. (1982). Speech accommoda-
tion theory. In M. E. Roloff & C. R. Berger 
(Eds.), Social cognition and communication 
(pp. 193-226). London: Sage Publications. 
Wiemer-Hastings, P., & Graesser, A. C. (2000). Select-
a-Kibitzer: A computer tool that gives mean-
ingful feedback on student compositions. 
Interactive Learning Environments, 8(2), 149?
169. 
 
 
 
22
Proceedings of the Workshop on Embodied Language Processing, pages 41?50,
Prague, Czech Republic, June 28, 2007. c?2007 Association for Computational Linguistics
Coordination in Conversation and Rapport 
Justine Cassell, Alastair J. Gill and Paul A. Tepper 
Center for Technology & Social Behavior 
Northwestern University 
2240 Campus Drive, Evanston, IL 60208 
{justine, alastair, ptepper}@northwestern.edu 
 
Abstract 
We investigate the role of increasing 
friendship in dialogue, and propose a first 
step towards a computational model of the 
role of long-term relationships in language 
use between humans and embodied conver-
sational agents. Data came from a study of 
friends and strangers, who either could or 
could not see one another, and who were 
asked to give directions to one-another, 
three subsequent times. Analysis focused 
on differences in the use of dialogue acts 
and non-verbal behaviors, as well as co-
occurrences of dialogue acts, eye gaze and 
head nods, and found a pattern of verbal 
and nonverbal behavior that differentiates 
the dialogue of friends from that of strang-
ers, and differentiates early acquaintances 
from those who have worked together be-
fore. Based on these results, we present a 
model of deepening rapport which would 
enable an ECA to begin to model patterns 
of human relationships. 
1 Introduction 
 What characterizes the language of people who 
have known one another for a long time? In the US 
one thinks of groups of friends, leaning in towards 
one another, laughing, telling jokes at one an-
other?s expense, and interrupting one another in 
their eagerness to contribute to the conversation.  
The details may differ from culture to culture, but 
the fact of differences between groups of friends 
and groups of strangers are probably universal. 
Which characteristics, if any, reliably differentiates 
friends and strangers? Which can make a new 
friend feel welcome? An old friend feel appreci-
ated? Advances in natural language are ensuring 
that embodied conversational agents (ECAs) are 
increasingly scintillating, emotionally and socially 
expressive, and personality-rich. However, for the 
most part, those same ECAs demonstrate amnesia, 
beginning every conversation with a user as if it is 
their first, and never getting past the stage of intro-
ductory remarks. 
As the field of ECAs matures, and these systems 
are found on an increasing number of platforms, 
for an increasing number of applications, we feel 
that it is time to ensure that ECAs be able to en-
gage in deepening relationships that make their 
collaboration with humans productive and satisfy-
ing over long periods of time. To this end, in this 
paper we examine the verbal and nonverbal corre-
lates of friendship in an empirical study, and then 
take first steps towards a model of deepening 
friendship and rapport in ECAs. The current study 
is a part of a larger research program into linguistic 
and social coordination devices from the utterance 
level to the relationship level ? how they work in 
humans, how they can be modeled in virtual hu-
mans, and how virtual humans can be used to teach 
people who wish to learn these skills.  
2 Background & Theory 
As people become closer, their conversational 
style changes. They may raise more topics in the 
course of a conversation, refer more to themselves 
as a single unit than as two people, and be more 
responsive to one another?s talk (Cassell & Tver-
sky, 2005; Hornstein, 1982). They also are likely 
to sustain eye contact longer, smile more, and lean 
more towards one another (Grahe & Bernieri, 
1999; Richmond & McCroskey, 1995). In addition, 
friends appear to have fewer difficulties with lexi-
cal search, perhaps because they can rely on 
greater shared knowledge, and are more likely to 
talk at the same time, and to negotiate turn-taking 
in a less rigid manner, both through gaze and ges-
41
ture (Welji & Duncan, 2005). Tickle-Degnen & 
Rosenthal (1990) propose a model of deepening 
rapport over time based on the relationship among 
three components: positivity, mutual attention and 
coordination. As shown in Figure 1, as friendship 
deepens, the importance of positivity decreases, 
while the importance of coordination increases. 
Attention to the conversational partner, however, is 
hypothesized to remain constant. That is, strangers 
are more likely to be polite and uniformly positive 
in their talk, but also more likely to be awkward 
and badly coordinated with their interlocutors.   
As a relationship progresses and impressions 
have been formed and accepted, disagreement be-
comes acceptable and important. This may entail 
an increase in face-threatening issues and behav-
iors (cf. Brown & Levinson, 1987) accompanied 
by a decrease in the need to mediate these threats. 
At this stage in the relationship, coordination be-
comes highly important, so that the conversation 
will be less awkward and there is less likelihood of 
misunderstanding. Attention to one another, how-
ever, does not change. Tickle-Degnen & Rosenthal 
point out that these features are as likely to be ex-
pressed nonverbally (through smiles, nods, and 
posture shifts, for example) as verbally.  
One criticism of Tickle-Degnen & Rosenthal, 
and similar work, is that positive feelings for, and 
knowledge about, the other person are not distin-
guished (Cappella, 1990). That is, what might be 
perceived as lack of rapport could actually be a 
lack of familiarity with a partner?s behavioral cues 
for indicating misunderstanding or requesting in-
formation.  
This conflation may come from the fact that the 
word rapport is used both to refer to the phenome-
non of instant responsiveness (?we just clicked?) 
and that of deepening interdependence over time. 
ECA research has been divided between a focus on 
instant rapport (Gratch et al, 2006; Maatman, 
Gratch, & Marsella, 2005) and a focus on estab-
lishing and maintaining relationships over time 
(Bickmore & Picard, 2005; Cassell & Bickmore, 
2002; Stronks, Nijholt, van der Vet, & Heylen, 
2002). Perhaps due to difficulties with analyzing 
dyadic interdependent processes, and modeling 
them in computational systems, much of the work 
in both traditions still takes a signaling approach, 
whereby particular signals (such as nodding or 
small talk) demonstrate the responsiveness, extro-
version, or rapport-readiness of the agent, but are 
decontextualized from the actions of the dyad 
(Duncan, 1990). Although this approach is well 
paired to current technological constraints, it may 
not adequately account for the contingency of in-
terpersonal interaction and conversation. In addi-
tion, in none of these previous studies was there a 
focus on how verbal and nonverbal devices actu-
ally change over the course of a relationship, and 
how those devices are interdependent between 
speaker and listener. An instant rapport approach is 
useful for building systems that are initially attrac-
tive to users; but a system that signals increasing 
familiarity and intimacy through its linguistic and 
nonverbal behaviors may encourage users to stay 
with the system over a longer period of time. 
In the current work, we concentrate how dis-
course and nonverbal behavior changes over time, 
and across the dyad, as this perspective allows us 
to highlight the similarities between interpersonal 
coordination and knowledge coordination of the 
kind that has been studied in both conversational 
analysis and psycholinguistics.  
Work on conversational analysis demonstrates 
the importance of knowledge coordination compo-
nents such as turn-taking and adjacency pairs 
(e.g.Goodwin, 1981; Schegloff & Sacks, 1973). 
Inspired by this approach, work by Clark and col-
laborators on grounding and conversation as joint 
action has made demonstrated coordination and 
cooperation as defining characteristics of conversa-
tion (Clark, 1996; Clark & Brennan, 1991; Clark & 
Wilkes-Gibbs, 1986). This work has in turn, re-
ceived a significant amount of attention in compu-
tational linguistics, specifically in the study of dia-
logue (Matheson, Poesio, & Traum, 2000; Nakano, 
Reinstein, Stocky, & Cassell, 2003; Traum, 1994; 
Traum & Dillenbourg, 1998). To develop a model 
of nonverbal grounding, Nakano et al (2003) stud-
Early Late
Time of Interaction
Im
p
o
rt
a
n
ce
 f
o
r 
R
a
p
p
o
rt
Mutual Attention
Positivity
Coordination
Figure 1. Three component model of rapport 
(from Tickle-Degen & Rosenthal, 1990). 
42
ied people giving directions with respect to a map 
placed in between them. In that study, we observed 
that when a direction-receiver looked up from the 
map while the direction-giver was still giving di-
rections, the giver would initiate grounding behav-
ior such as a repeat or a rephrase.  
The literature reviewed above leads us to believe 
that there is an integral relationship between social 
and knowledge coordination. In this paper, we at-
tempt to draw conclusions about the changes in 
social and linguistic coordination over the short- 
and long-term in a way that illuminates that poten-
tial relationship, and that is also computationally 
viable. In order to do this, we replicate the task we 
used in our earlier grounding study (Nakano et al, 
2003); that is we use a direction-giving task, where 
half the subjects can see one another, and half are 
divided by a screen. Here, however, half of the 
subjects in each visibility condition are friends and 
half are strangers. And to study the potential de-
velopment of rapport across the experimental pe-
riod, each pair performs three subsequent direc-
tion-giving tasks.  
In the next section, we discuss the experimental 
procedure further. In section 4, we introduce first 
steps towards a new computational model of rap-
port that incorporates conversational coordination 
and grounding, based on our empirical findings. 
3 The Experiment 
3.1 Method 
Participants We collected eight task-based con-
versations (N = 16): in each dyad, one participant 
was accompanied by the experimenter and fol-
lowed a specific route from one place in the rococo 
university building where the experiment was run 
to another place in the building. S/he gave the 
other participant directions on how to reach that 
location, without the use of maps or other props. 
The direction-receiver (Receiver) was instructed to 
ask the direction-giver (Giver) as many questions 
as needed to understand the directions. After the 
conversation, the Receiver had to find the location. 
During recruitment the Giver was always selected 
as someone familiar with the building, while the 
Receiver was unfamiliar. All subjects were under-
graduate students, and were motivated by surprise 
gifts hidden at the target location. 
Design. We manipulated long-term rapport, visi-
bility, and subsequent route in a 2 ? 2 ? 3 design. 
We operationalized long-term rapport as a binary, 
between-subjects variable, with conditions Friends 
(self-reported as friends for at least one year) and 
Strangers. To study the effect of non-verbal behav-
ior, we manipulated visibility as a second between-
subject variable. To do this, half of the participants 
could see each other, and half were separated by a 
dividing panel. To study the effect of acquaintance 
across the experimental period, each dyad com-
pleted the task three consecutive times, going to 
three different locations. 
Data Coding All dyads were videotaped using a 
six-camera array, capturing the participants? body 
movements from the front, side, and above, along 
with close-up views of their faces. From each 
dyad, we made time-aligned transcriptions (using 
Praat). Non-verbal behavior was coded using An-
vil. From the transcripts, the following 9 DAMSL 
Dialogue Acts (Core & Allen, 1997) were coded: 
Acknowledgments, Answers, Assert, Completion, 
Influence, Information Request, Reassert, Repeat-
Rephrase, and Signal Non Understanding. Non-
verbal behavior in giver and receiver was coded 
using the following categories, based on Nakano, 
et al (2003): 
? Look at Speaker ? looking at the speaker?s 
eyes, eye region or face. 
? Look at Hearer ? looking at the hearer?s eyes, 
eye region or face. 
? Head nod [speaker or hearer] ? Head moves 
up and down in a single continuous movement 
on a vertical axis, but eyes do not go above 
the horizontal axis. 
3.2 Results 
We first provide basic statistics on the experimen-
tal manipulations and then examine the role of 
friendship and visibility on verbal and non-verbal 
behavior. 
Basic Statistics: Overall, we find that Friend dy-
ads use a significantly greater number of turns per 
minute than Strangers (t(6)= 2.45, p<.05, two tail), 
however, there is no difference in the mean num-
ber of seconds it took for dyads to complete the 
task. This lack of significance may have been due 
to variance among the dyads, since the mean 
length was 847 seconds for friends and 1049 for 
43
strangers. Given the instructional nature of the 
task, this means that Friends were more likely to 
intervene in the direction-giving than were Strang-
ers, even though ? for most of the dyads ? friends 
appear to take less time to finish. No difference 
was found in turns per minute for Visible and Non-
visible dyads; nor is there a difference in length in 
seconds. For routes, there is no difference in turns 
per minute, however for the length of the route in 
seconds there is a difference (F(2,21)=10.66; 
p<.006) such that the mean length of Route 1 is 
165 seconds; Route 2 is 395 seconds; Route 3 is 
387. For this reason, all statistics below are nor-
malized as a function of the length of that dyad?s 
data in seconds, and graphs are plotted to show 
least squares mean. 
Verbal and Nonverbal behavior: We examine 
the relationship between friendship and visibility 
of both Giver and Receiver across the three route 
tasks. Each of the DAMSL dialogue act variables 
and Non-verbal behavior variables was entered as 
the dependent variable in building mixed method  
models using the JMP statistical package (Version 
6, SAS Institute Inc., Cary, NC, 1989-2005); 
Speaker (direction-giver or receiver), Visibility, 
Friendship and Route were entered as predictor 
variables; experimental dialogue number was also 
entered as a source of random variance. We report 
the results in Tables 1 and 2 (for DAMSL and 
Non-verbal behavior variables respectively). 
Verbal Behavior: In terms of overall variance ex-
plained, we find that Acknowledgments is best 
accounted for by the model (Adjusted R Square of 
0.91), whilst Completion is least well accounted 
for (Adjusted R2 of 0.06).  
Turning first to main effects, for Visibility, 
Visible-Givers use Acknowledgements, Assert, 
Influence, and Reassert dialogue acts more fre-
quently than Non-visible Givers (post-hoc t tests at 
p <.05) 
Visible-Receivers use Acknowledgement, Re-
peat-rephrase, Signal Non Understanding these 
features more frequently than Non-visible Receiv-
ers. (post-hoc t tests at p <.05) 
For Friendship, no differences were found for 
production of DAMSL acts by givers. For receiv-
ers, receiver-strangers use more acknowledge-
ments than receiver friends.  
0
5
10
15
20
25
30
Nonvisible Visible
L
e
a
st
 S
q
 M
e
a
n
s 
fo
r 
N
o
rm
A
ck
n
o
w
le
d
g
m
e
n
t
Friends Strangers  
Figure 2: Giver Acknowledgment by condition 
 
0
2
4
6
8
10
12
14
16
Nonvisible Visible
L
e
a
st
 S
q
 M
e
a
n
s 
fo
r 
N
o
rm
A
ck
n
o
w
le
d
g
e
m
e
n
t
Friends Strangers  
Figure 3: Receiver Acknowledgment by condition 
 
These main effects are mediated by an interac-
tion between Visibility?Friendship for Acknowl-
edgements. Here, as shown in Figure 2 and 3 we 
see that in the nonvisibility condition, there is no 
difference in the use of acknowledgements per 
second between friends and strangers; on the other 
hand, strangers use more acknowledgements in the 
visible condition (p<.05). A very similar interac-
tion was found for Signal Non Understanding (at 
the trend level of p<.08). 
Route is only a main effect predictor of Signal 
Non Understanding as used by receivers, who pro-
duce it significantly more frequently during the 
third route task than the first. Since signaling one?s 
lack of understanding is potentially face-
DV  Source DF DF Total F Ratio 
ACK  V*F 1 20 10.64** 
COMP  V*F 1 4 9.78* 
 V*F 1 20 3.31? SNU 
 Rte 2 20 3.38* 
Note: ?p<0.08; *p < 0.05; **p < 0.01;  
Table 1: Verbal behavior 
 
Abbreviation: ACK=Acknowledgment; COMP=Completion; 
SNU=Signal Non Understanding; Sources abbreviated as: F 
= Friendship; V = Visibility; Rte = Route 
44
threatening, this result may indicate that both 
friends and strangers become more comfortable 
with one another by the third route. 
Nonverbal Behavior: Variance explained by the 
non-verbal models is the greatest for Look At 
Speaker (Adjusted R2 0.83) and least for Speaker 
Nod (0.38). With respect to the main effects result-
ing from the analysis of the non-verbal behaviors, 
we find the following.  
Visibility: Givers nod more in the visible condi-
tion when the receiver is speaking than they do in 
the Non-visible condition.  
Route: For both givers and receivers, there is an 
increase in use of Look At Speaker and Look At 
Hearer, over time; in both cases significantly 
greater instances of these variables occurred during 
Route task 2 and 3, compared to Route 1. Once 
again, these results may indicate increasing coor-
dination in conversational behavior for both 
Friends and Strangers. 
In fact, in the case of head nods, we note an in-
teresting pattern of coordination between speaker 
and hearer head-nods across the routes that differs 
for friends and strangers. For friends, both Re-
ceiver and Giver head nods in response to Receiver 
talk reduce in frequency between the first and sec-
ond routes (Giver t(8)=-2.36; p<0.05; Receiver 
t(8)=-2.28; p<0.05). For strangers, no such ac-
commodation over time occurs. Conversely, for 
friends when the Giver is speaking, both giver and 
receiver head nods increase over the three routes 
(significant only for Receiver t(8)=2.38; p<0.05). 
For strangers, however, head nods decrease (Giver 
t(8)=-2.80; p<0.05, Receiver t(8)=3.92; p<0.01). 
This means that speaker and hearer are increas-
ingly coordinated across the routes, particularly 
when they are friends. 
Interaction of verbal and nonverbal behavior  
So far we have concentrated on how individual 
verbal and nonverbal behaviors differ across con-
ditions. However, this does not take account of the 
interactive nature of the task and the focus of this 
paper. We therefore examine how specific respon-
sive nonverbal behaviors (looking at 
speaker/hearer and head nods) co-occur before, 
during, or after the DAMSL variables. Examina-
tion of the residuals of chi square analysis was 
used to identify co-occurrence of DAMSL dia-
logue acts with nonverbal behavior for each 
Speaker (Giver or Receiver) and condition 
(Friend/Stranger, Visible /Nonvisible). Significant 
over-use or underuse of these verbal/nonverbal co-
occurrences was then compared using the log-
likelihood statistic (Rayson, 2003) to dialogues in 
the other conditions (e.g., Giver-Friend-Visible 
with Giver-Friend-Nonvisible, and Giver-Stranger-
Visible for Head-nods, and just Friends with 
Strangers for the Gaze data). This technique, which 
we used in our earlier grounding experiment 
(Nakano et al, 2003) allows us insight into the 
probable causality of the behaviors of speaker and 
hearer, across verbal and nonverbal behavior.  
When direction-givers are speaking 
Head-nods. Givers did not nod significantly more 
or less frequently across Friends/Strangers condi-
tions when they were speaking. 
Gaze. More than in friendship dialogues, when 
strangers are speaking, and the direction-giver is 
acknowledging, the direction-receiver is likely to 
look at the Giver (G2=17.14; p<0.0001). 
More than in friendship dialogues, in Stranger 
dialogues, both before and after the direction-giver 
asserts something, the Receiver is likely to look at 
the Giver (G2=5.09; p<0.05, and G2=4.16, p<0.05, 
respectively). 
More than in friendship dialogues, both before 
and during the Giver?s use of Repeat-Rephrase 
utterances, the Receiver is likely to look at the 
Giver (G2=35.02; p<0.0001, and G2=60.74; 
p<0.0001, respectively). 
More than in friendship dialogues, both before 
and during the Giver?s use of Info-Request dia-
logue acts, the Receiver is likely to look at the 
Giver (G2=39.01; p<0.0001, and G2=9.60; p<0.01, 
respectively).  
This means that right after a direction receiver 
looks at the direction-giver, the giver produces an 
Assertion, a Repeat-Rephrase, or an Information 
DV  Source DF DF Total F Ratio 
Look At  
Speaker 
 Rte 2 10 18.03*** 
Hearer 
Nod 
 SPKR*F*Rte 2 20 5.14* 
Speaker  
Nod 
 SPKR*F*Rte 2 20 4.21* 
*p<0.05;**p<0.01; ***p<0.001 
Table 2. Non Verbal Behaviors.  
Sources abbreviated as: SPKR = Speaker; F = Friend-
ship; V = Visibility; Rte = Route,  
45
Request. As with Nakano et al, the stranger?s gaze 
towards the direction-giver can be seen as a signal 
of non-understanding and, in these contexts, it 
evokes one of these three grounding responses 
from the direction-giver.  
For friends, on the other hand, gaze towards the 
speaker evokes the next segment of the directions, 
and is therefore functioning as a signal of under-
standing. That is, more than in stranger dialogues, 
both before and during the Giver?s use of Influence 
dialogue acts (utterances such as ?turn right?), Re-
ceivers are more to look at the Giver (G2=4.77; 
p<0.05, and G2=31.92; p<0.0001, respectively). 
When direction-receivers are speaking 
Head-nods. As shown in Figure 4, Strangers used 
more head nods than Friends during their use of 
Acknowledgment dialogue acts in the visible con-
dition (G2 = 10.48, p<.01), however they do not 
differ from friends in the nonvisible condition (G2 
= 0.01, ns).  
89.52
70
41.46
71.43
0
10
20
30
40
50
60
70
80
90
100
Visible NonvisibleR
el
at
iv
e 
F
re
q
u
en
cy
 o
f R
ec
ei
ve
r N
o
d
s 
D
u
ri
n
g
 T
h
ei
r 
U
se
 o
f A
ck
n
o
w
le
d
g
m
en
t
Strangers Friends  
Figure 4: Receiver nods during Acknowledgment 
 
0
2.5
23.08
0
0
5
10
15
20
25
30
Visible Nonvisible
R
el
at
iv
e 
F
re
q
u
en
cy
 o
f R
ec
ei
ve
r 
N
o
d
s 
D
u
ri
n
g
 T
h
ei
r 
u
se
 o
f I
n
fo
-R
eq
u
es
t
Strangers Friends  
Figure 5: Receiver nods during Info Request 
Conversely, as shown in Figure 5, when receiv-
ers are making an Info-Request in the visible con-
dition (G2 =14.13, p<.001), Friends nod much more 
often than Strangers; but do not differ from Strang-
ers in the nonvisible condition (G2 = 1.44, ns). 
Once again, here the friends are marking their un-
derstanding, by nodding, even while they request 
further information. 
Gaze. Before the Receiver?s use of Acknowl-
edgment dialogue acts in Stranger dialogues, the 
Giver is more likely to look at the Receiver 
(G2=10.79; p<0.01). After the Receiver has used an 
Acknowledgment in a Stranger dialogue, s/he is 
more likely to look at the Giver a (G2=14.79; 
p<0.001). This means that among strangers the 
giver and receiver are likely to engage in mutual 
gaze around the acknowledgement dialogue act. 
During and after a Repeat-Rephrase dialogue act 
in Friends dialogues, the Receiver is more likely to 
look at the Giver (G2=10.37; p<0.01 and G2=6.72; 
p<0.01, respectively). Before the Receiver uses a 
Repeat-Rephrase dialogue act in a Friends dia-
logues, the Giver is more likely to look at the Re-
ceiver (G2=9.08; p<0.01). This means that among 
friends, giver and receiver engage in mutual gaze 
around the repeat and rephrase dialogue act. 
3.3 Discussion 
Our analysis of verbal and non-verbal behaviors 
reveals consistent differences across the short term, 
comparing subsequent direction-giving tasks, and 
across the long term, comparing strangers to 
friends.  
Strangers ? Knowledge coordination 
With respect to the co-ordination of verbal and 
nonverbal behavior, it is apparent that, among 
strangers, the Receiver?s use of Acknowledgments 
is strongly associated with characteristic gaze pat-
terns of signaling non-understanding. In these 
Stranger dyads, the Giver looks at the Receiver to 
signal the need for feedback. The Receiver then 
nods to emphasize comprehension while uttering 
the Acknowledgment (e.g., ?okay?), and then looks 
back at the Giver. This pattern is very specific to 
Strangers, and in the case of the Receiver?s use of 
head nod, specific to the visible condition. Simi-
larly, among strangers, when the Giver acknowl-
edges the receiver?s correct understanding, the Re-
ceiver looks at the Giver. This pattern of gaze re-
quest, and grounding response, happens repeatedly 
and often (Acknowledgements being used more 
frequently by strangers), ensuring coordination 
among strangers, but at the cost of frequent explicit 
requests. Of course, since Acknowledgements are 
generally backchannel utterances, used to indicate 
46
mutual understanding, one explanation for the 
higher frequency of acknowledgements, head nods 
and eye gaze by strangers, especially in the earlier 
tasks, is over-generation aimed at showing atten-
tion. Although over-generation could achieve these 
goals, it can also result in creating a false impres-
sion of mutual understanding and it is notable that 
these behaviors decrease over time. 
We also find that in the Strangers condition, Re-
ceivers are more likely to look at the Giver before 
and during the Giver?s use of Repeat-Rephrase 
(i.e., repeating back to the Receiver some earlier 
information), and also before and during the 
Giver?s use of Info-Request acts (that is the Giver 
asking the Receiver a question such as ?do you get 
that??). 
From the frequent and repeated use of Acknowl-
edgments and gaze (implying something like 
?okay? are you sure you?re okay? really??), to 
the Receiver?s gaze-anticipation of the Giver?s Re-
peat-Rephrase and Info-Request, we infer a much 
more effortful interaction for Strangers, and one 
that, in fact, for most dyads, takes longer.  
In line with Welji and Duncan (2005), we found 
evidence that the task may demand additional cog-
nitive resources for Strangers, with the Receiver in 
the Strangers dialogues breaking gaze at the Giver 
to apparently consult some internal representation 
of the space just described by the Givers Assert 
(e.g., ?you?ll find some blue couches?), before re-
turning attention, and gaze, once again to the 
Giver. 
We also note a greater use overall of Acknowl-
edgment and Completions by Strangers and in 
visible situations; Receivers in the visible situa-
tions also use more Signal Non Understanding. 
Taken together, these findings indicate that coordi-
nation and achieving mutual understanding is more 
effortful for Strangers: Friends use fewer dialogue 
acts such as Acknowledgment, Completion, and 
Signal Non Understanding, indicating that there is 
less need to negotiate understanding, and that they 
are more likely to have some kind of shared repre-
sentation. Because of this, the Friends dialogues 
and task performance would appear to be more 
efficient, with less grounding required and less mu-
tual gaze around their use of Acknowledgments, 
Info-Requests and Repeat-Rephrase.  
The fact that Friends are better able to calibrate 
the task than Strangers is also demonstrated by the 
results found for Route. Both Friend and Stranger 
dyads increase their gaze towards one another from 
Route 1 to Route 2. But Friends shift the way they 
use head nods over the course of the three routes. 
They begin in Route 1 by producing them in con-
junction with Receiver talk (acknowledgment, re-
quest for further information, repeating directions 
back). However, by Route 2, the friends are nod-
ding when the direction-giver speaks, marking that 
they don?t need further information but have un-
derstood on the first try. On the contrary, Strangers 
continue to nod just as much with receiver talk, 
and decrease their nods with giver talk; perhaps 
since by Route 2, it is clear that Strangers don?t 
understand on the first try. 
Tickle-Degnen and Rosenthal (1990) predict 
greater coordination as a relationship progresses. 
We found better coordination, but that was re-
vealed, paradoxically, through fewer coordination 
devices and fewer dialogue acts in each turn, both 
comparing from Route 1 to Route 3, and compar-
ing Strangers to Friends.  
Friends ? Positivity 
In the Friends dialogues, we find a notable col-
location of non-verbal behavior and the Receiver?s 
use of Repeat-Rephrase utterance (i.e., repeating 
the Giver?s utterance back to ensure correct inter-
pretation). This is in contrast to the findings for 
Stranger dyads which found nonverbal behaviors 
found in conjunction with the Giver?s reactive use 
of Repeat-Rephrase ? i.e., the Giver?s questioning 
of the Receiver?s understanding ? perhaps after a 
breakdown in mutual understanding. In the Friend 
dyads, it is the Receiver who proactively checks 
correct understanding of the Giver?s utterance be-
fore the interaction continues.  
Tickle-Degnen & Rosenthal predict a reduction 
in the importance of positivity as rapport increases 
over time in a relationship. We found some evi-
dence to support this, since such questioning of the 
Giver in itself may be viewed as face-threatening 
behavior. However, in the Friends dialogues, this 
Repeat-Rephrase appears anticipated ? or sanc-
tioned ? by the Giver who looks at the Receiver 
prior to the utterance. Further, during and after the 
Receivers? use of the Repeat-Rephrase utterance, 
they also look at the Giver, which again would be 
expected to be viewed as a threat to face.  
Similarly, the Receiver gazes at the Giver before 
and during the Giver?s use of Influence dialogue 
acts (explicit commands, such as ?turn left?). Such 
47
direct gaze, along with a reduced number of medi-
ating dialogue acts such as Acknowledgments, ap-
pears to indicate that Friends dialogues are less 
concerned with avoiding face-threatening behav-
ior, and as such would appear less concerned with 
maintaining positivity during the interaction.  
Note that, almost paradoxically, Friends demon-
strate their increased ability to coordinate their in-
teraction through a diminished use of explicit co-
ordination devices. This speeds up the interaction, 
and reduces the number of overall dialogue acts. 
And, finally, differences between Friends and 
Strangers are vastly diminished when the interlocu-
tors cannot see one another. This leads us to be-
lieve that nonverbal behaviors in addition to gaze 
and head nods may be playing a role in how 
Friends coordinate with one another; an advantage 
which is taken away when they can only hear one 
another?s voices.  
4 Towards a Computational Model 
In the short-term context of conversation, mainte-
nance of mutual attention and incremental coordi-
nation of beliefs are requisites for grounding and 
turn-taking. In prior computational systems, 
grounding has been achieved by marking the status 
of conversational contributions as provisional (un-
grounded) or shared (grounded). Conversational 
actions by either the user or the system can trigger 
updates that change provisional information to 
shared. Acknowledgements, for example, are ex-
plicit ways of achieving grounding, but moving on 
to the next stage of the task is equally effective, as 
it presupposes that prior utterances have been 
taken up (Traum, 1994). In a model such as this, 
grounding occurs at the turn level. In order to han-
dle the multimodal phenomena that participate in 
grounding in face-to-face conversation, as Nakano 
et al (Nakano et al, 2003) have shown, a model of 
knowledge coordination needs to have more fre-
quently updated access to potential grounding 
events. In that implementation, we continuously 
polled for inputs, so as to capture the updates in 
grounding that occur between typical linguistic 
segments. We believe that the focus on time and 
process that allowed us to look at events of a 
smaller granularity in our earlier work on nonver-
bal grounding behavior will also allow us to extend 
up to events of a larger granularity, such as stages 
in a relationship. That is, we believe that the results 
described in earlier sections of this paper can be 
taken into account in a computational system by 
maintaining a model of the state of shared and pri-
vate information across several interactions (sev-
eral years, if possible). In this way, the shared his-
tory of two interlocutors (the user and the system) 
can be translated into patterns of linguistic behav-
ior, such as reduced use of acknowledgements, and 
reduced positivity, with increased interruption and 
information requests. This is similar to Cheng, 
Cavedon & Dale (2004)?s approach to direction-
giving. In this approach, the system maintains a 
history of places it has given directions to before. 
Using this task history, it is able to generate shorter 
directions at later stages in the dialogue. In our im-
plementation, however, the very style of the inter-
action is modified by the shared history of the user 
and the system. In the sense that we are modifying 
the linguistic style of the dialogue based on psy-
chological attributes, our approach is similar to 
work by Mairesse & Walker (2007) and Isard et al 
(2006). In both cases, a broad set of natural lan-
guage generation parameters is employed to gener-
ate language that differs along a personality di-
mension, based on a number of previous empirical 
studies. In the current approach, however, the fea-
tures that are modified derive from the interde-
Figure 6. Proposed architecture for modeling coordination within and across conversation 
48
pendence of the system with a particular user.  
Some of the features that are present in the con-
versations of friends, such as interjections and 
completion of one another?s utterances, are still 
beyond current computational abilities, as they 
would require online, real-time processing and un-
derstanding of utterances with incremental plan-
ning and generation of responses. We are inter-
ested in pursuing this feature of the system as dia-
logue technologies improve. 
5 Conclusion 
In this paper, we have compared direction-
giving between friends and strangers, and within 
these two groups we have compared three subse-
quent direction-giving episodes. In order to deter-
mine the effect played specifically by nonverbal 
behavior in short- and long-term rapport, half of 
our participants could see one another, while the 
other half were divided by a screen. Our experi-
mental and analytic methodology drew from both 
the social psychological, conversational analysis, 
and conversation as joint action traditions. Conse-
quently, our results were able to demonstrate the 
ways in which the verbal and nonverbal devices 
that index rapport relate to the role those same de-
vices play in knowledge coordination. Based on 
this commonality, we proposed a computationally 
viable model of deepening friendship within and 
across subsequent tasks that extends our previous 
work on grounding in face-to-face interaction. The 
work we have presented here therefore differs sub-
stantially from previous work on rapport and rela-
tionship building in embodied conversational 
agents. We did not start out with a definition of 
rapport but instead investigated those behaviors 
that characterize dyads who have self-identified as 
friends or strangers. And rather than looking at 
rapid assessment of rapport (the feeling of ?click-
ing?) we looked at the long-term version: acquiring 
a sense of mutual interdependence. Finally, rather 
than looking at how to get ECAs to engage users 
into establishing a relationship, or into letting 
down their guard, we examined those behaviors 
that characterize the dyadic interaction at each 
stage.  
All of these topics, however, are clearly inter-
related, and future research will benefit from tak-
ing a greater number of them into account in both 
data analysis, and the implementation of ECAs. 
Future research in our own lab will also have to be 
more explicit about how to implement the compu-
tational model that we have started to lay out here. 
Additional subjects in a similar experiment will no 
doubt facilitate that task. 
As we increasingly understand better how con-
versation changes when people come to know one 
another, we expect to apply these results to our 
ongoing research on virtual peers that can teach 
children with autism how to sustain interpersonal 
relationships (Tartaro & Cassell, 2006) and to our 
work on building the survey interviewers of the 
future, who can both engage their survey-takers 
and keep them honest (Cassell & Miller, in press). 
More generally, however, we hope to increasingly 
implement ECAs who will stick around for the 
long haul. 
Acknowledgments 
Thanks to Kristina Striegnitz , Will Thompson, Tara 
Latta and Nate Cantelmo for their help, and Darren 
Gergle for his superior statistical knowledge. We are 
grateful to Motorola for funding that supported some of 
the research reported here. 
References 
Bickmore, T., & Picard, R. (2005). Establishing and 
Maintaining Long-Term Human-Computer 
Relationships. ACM Transactions on Computer 
Human Interaction (ToCHI), 12(2), 293-327. 
Brown, P., & Levinson, S. (1987). Politeness: Some 
Universals in Language Usage. Studies in 
International Sociolinguistics. New York: 
Cambridge University Press. 
Cappella, J. N. (1990). On Defining Conversational 
Coordination and Rapport. Psychological Inquiry, 
1(4), 303-305. 
Cassell, J., & Bickmore, T. (2002). Negotiated 
Collusion: Modeling Social Language and its 
Relationship Effects in Intelligent Agents. User 
Modeling and Adaptive Interfaces, 12, 1-44. 
Cassell, J., & Miller, P. (in press). Is it Self-
Administration if the Computer Gives you 
Encouraging Looks? In F. G. Conrad & M. F. 
Schober (Eds.), Envisioning the Survey Interview of 
the Future. New York: John Wiley & Sons. 
Cassell, J., & Tversky, D. (2005). The Language of 
Online Intercultural Community Formation. Journal 
of Computer-Mediated Communication, 10(2), 
article 2. 
Cheng, H., Cavedon, L., & Dale, R. (2004, 28th-29th 
August). Generating Navigation Information Based 
49
on the Driver's Route Knowledge. Paper presented 
at the COLING 2004 Workshop on Robust and 
Adaptive Information Processing for Mobile Speech 
Interfaces. Geneva,Switzerland. 
Clark, H. H. (1996). Using Language. Cambridge: 
Cambridge University Press. 
Clark, H. H., & Brennan, S. E. (1991). Grounding in 
communication. In L. B. Resnick, J. M. Levine & S. 
D. Teasley (Eds.), Perspectives on socially shared 
cognition (pp. 127-149). Washington DC: American 
Psychological Association. 
Clark, H. H., & Wilkes-Gibbs, D. (1986). Referring as a 
collaborative process. Cognition, 22, 1-39. 
Core, M., & Allen, J. (1997). Coding Dialogue with the 
DAMSL Annotation Scheme. Proceedings of the 
AAAI Fall Symposium on Communicative Action in 
Humans and Machines. Boston, MA. 
Duncan, S., Jr. (1990). Measuring Rapport. 
Psychological Inquiry, 1(4), 310-312. 
Goodwin, C. (1981). Conversational Organization: 
Interaction between speakers and hearers. New 
York: Academic Press. 
Grahe, J. E., & Bernieri, F. J. (1999, Win). The 
importance of nonverbal cues in judging rapport. 
Journal of Nonverbal Behavior, 23(4), 253-269. 
http://www.springeronline.com  
Gratch, J., Okhmatovskaia, A., Lamothe, F., Marsella, 
S., Morales, M., van der Werf, R. J., et al (2006). 
Virtual Rapport. Proceedings of the 5th 
International Conference on Interactive Virtual 
Agents (IVA). Marina del Rey, CA. 
Hornstein, G. A. (1982). Variations in conversational 
style as a function of the degree of intimacy between 
members of a dyad. Unpublished Doctoral, Clark 
University. 
Isard, A., Brockmann, C., & Oberlander, J. (2006). 
Individuality and alignment in generated dialogues. 
Proceedings of the 4th International Natural 
Language Generation Conference (pp. 22-29). 
Sydney, Australia. 
Maatman, M., Gratch, J., & Marsella, S. (2005). Natural 
Behavior of a Listening Agent. Paper presented at 
the 5th International Conference on Interactive 
Virtual Agents (IVA). Kos, Greece. 
Mairesse, F., & Walker, M. (2007). PERSONAGE: 
Personality generation for dialogue. Proceedings of 
the 45th Annual Meeting of the Association for 
Computational Linguistics (ACL). Prague. 
Matheson, C., Poesio, M., & Traum, D. (2000). 
Modelling Grounding and Discourse Obligations 
Using Update Rules. Proceedings of the 1st Annual 
Meeting of the North American Association for 
Computational Linguistics (NAACL2000). Seattle, 
WA. 
Nakano, Y. I., Reinstein, G., Stocky, T., & Cassell, J. 
(2003, July 7-12). Towards a Model of Face-to-Face 
Grounding. Proceedings of the Annual Meeting of 
the Association for Computational Linguistics (p. 
553?561). Sapporo, Japan: Association for 
Computational Linguistics  
Rayson, P. (2003). Matrix: A statistical method and 
software tool for linguistic analysis through corpus 
comparison. Unpublished doctoral thesis, Lancaster 
University, Lancaster. 
Richmond, V. P., & McCroskey, J. C. (1995). 
Immediacy. In Nonverbal Behavior in Interpersonal 
Relations (pp. 195-217). Boston: Allyn & Bacon. 
Schegloff, E. A., & Sacks, H. (1973). Opening up 
closings. Semiotica, 8, 289-327. 
Stronks, B., Nijholt, A., van der Vet, P., & Heylen, D. 
(2002). Designing for friendship: Becoming friends 
with your ECA. Proceedings of the Embodied 
conversational agents - let's specify and evaluate 
them! (pp. 91-97). Bologna, Italy: ACM Press. 
Tartaro, A., & Cassell, J. (2006, August 28 - September 
1). Authorable virtual peers for autism spectrum 
disorders. Proceedings of the Combined workshop 
on Language-Enabled Educational Technology and 
Development and Evaluation for Robust Spoken 
Dialogue Systems at the 17th European Conference 
on Artificial Intellegence. Riva Del Garda, Italy. 
Tickle-Degnen, L., & Rosenthal, R. (1990). The nature 
of rapport and its nonverbal correlates. 
Psychological Inquiry, 1(4), 285-293. 
Traum, D. R. (1994). A Computational Theory of 
Grounding in Natural Language Conversation. 
University of Rochester, Rochester, NY. 
Traum, D. R., & Dillenbourg, P. (1998). Towards a 
Normative Model of Grounding in Collaboration. 
Proceedings of the ESSLLI-98 workshop on Mutual 
Knowledge, Common Ground and Public 
Information. Saarbrucken, Germany. 
Welji, H., & Duncan, S. (2005). Collaboration and 
Narration: The role of shared knowledge in the 
speech and gesture production of friends and 
strangers. Paper presented at the International 
Society of Gesture Studies Conference. Lyon, France. 
50
Proceedings of the 9th SIGdial Workshop on Discourse and Dialogue, pages 37?45,
Columbus, June 2008. c?2008 Association for Computational Linguistics
Reactive Redundancy and Listener Comprehension in Direction-Giving 
 
Rachel E. Baker Alastair J. Gill, Justine Cassell 
Department of Linguistics Center for Technology and Social Behavior 
Northwestern University Northwestern University 
Evanston, IL 60208 Evanston, IL 60208 
r-baker2@northwestern.edu {alastair,justine}@northwestern.edu 
 
 
Abstract 
We explore the role of redundancy, both in 
anticipation of and in response to listener 
confusion, in task-oriented dialogue. We 
find that direction-givers provide redundant 
utterances in response to both verbal and 
non-verbal signals of listener confusion. 
We also examine the effects of prior ac-
quaintance and visibility upon redundancy. 
As expected, givers use more redundant ut-
terances overall, and more redundant utter-
ances in response to listener questions, 
when communicating with strangers. We 
discuss our findings in relation to theories 
of redundancy, the balance of speaker and 
listener effort, and potential applications. 
1 Introduction 
Our everyday conversations represent a carefully 
negotiated balance between the perceived needs of 
the speaker and the listener. These opposing forces 
affect every aspect of language from phonetics to 
pragmatics. A careful balance between these two 
forces allows speakers to produce language that is 
both efficient and effective at communicating a 
message (Lindblom, 1990; Horn, 1993). Of course, 
the same balance is not appropriate for every situa-
tion. When accuracy is critical to the message, or 
when the speaker perceives the listener to have 
difficulty understanding, the speaker is more likely 
to prioritize clarity over efficiency, resulting in 
more explicit communication. In contrast, during 
casual conversation or when speed is a factor, the 
speaker may choose a more reduced, efficient, 
communication style (Lindblom, 1990; Horton and 
Keysar, 1996). A number of scholars have pointed 
out that speakers seem to use the information 
available to themselves rather than that available to 
the listener to guide certain linguistic decisions, 
such as clarity of pronunciation and choice of syn-
tactic structure (Bard et al, 2000; Branigan et al, 
2003). However, these studies examine utterance 
form, while our study examines content, which is 
more influenced by audience design (Branigan et 
al., 2003). In every utterance, a speaker either re-
duces the likelihood of listener misunderstanding 
by being more explicit, or reduces their own effort 
by providing a minimal amount of information. 
Regardless of whether speakers pro-actively moni-
tor the information needs of listeners, they do need 
to respond when listeners say or do something to 
indicate confusion. Developing a better under-
standing of the factors that affect how and when 
speakers respond to signs of listener confusion is 
important at both theoretical and applied levels: 
first, it can better explain the variation in discourse 
strategies used in different communicative situa-
tions; second, it can help in the design of dialogue 
systems (Kopp et al, 2008; Theune et al, 2007). 
In this study, we examine what types of listener 
behavior increase the likelihood that a speaker will 
produce a redundant utterance. We also examine 
how communicative context affects the amount 
redundancy a speaker produces overall (Walker, 
1992, 1996) and a speaker?s use of redundancy in 
response to listener confusion. In contrast to pre-
vious work, we study reactive redundancy, or re-
dundancy produced in response to signs of listener 
confusion. We investigate two factors that may 
influence a speaker?s tendency to produce redun-
dant utterances and to respond to listener confusion 
with redundancy: the relationship between the in-
terlocutors and their visual contact.  
In the following section, we review relevant li-
terature and present our hypotheses; we then de-
scribe the direction-giving experiment which we 
used to examine redundancy in task-oriented di-
alogue, and present our results; we discuss our re-
sults in light of the literature and conclude by 
noting potential applications and future work.  
37
2 Related Work and Predictions 
2.1 Redundancy 
Grice?s (1975) second Maxim of Quantity: ?Do not 
make your contribution more informative than is 
required? has led to the general impression that 
redundancy (providing discourse-old information) 
is avoided in language (Stalnaker, 1978), with this 
mirrored by work in natural language generation 
(Dalianis, 1999). However, Walker (1992, 1996) 
points out that such conclusions relating to redun-
dancy are often based on flawed assumptions. For 
example, they assume that agents have unlimited 
working memory and the ability to automatically 
generate all the inferences entailed by every utter-
ance, that utterance production should be mini-
mized, and that assertions by Agent A are accepted 
by default by Agent B (Walker, 1996: 183).  
In fact, redundancy can serve many desirable 
purposes in communication. Redundancy has been 
shown to increase text cohesion and readability 
(Horning, 1991) as well as provide evidence of 
understanding and grounding, make a proposition 
salient, and make inferences explicit (Walker, 
1996). A computer simulation of a cooperative task 
dialogue between two agents suggested that the use 
of certain types of redundant utterances improved 
the performance of the pair (Walker, 1996). 
Fussell and Krauss (1989a) point out that there 
are two methods that speakers can use to tailor 
their message for the listener. The first method in-
volves predicting what information it is necessary 
to communicate, using knowledge of the listener?s 
interests and background. The second method in-
volves modifying the message in response to lis-
tener feedback. Walker?s model only captures the 
use of redundancy in the service of the first me-
thod. We will refer to this type of redundancy as 
proactive redundancy, whereby a speaker provides 
redundant information without waiting for the lis-
tener to express a need for it. The advantages of 
providing redundant information proactively in-
clude being able to integrate the redundant infor-
mation with the new information, and avoiding 
conflict by removing the necessity for the listener 
to express a lack of understanding (Brown and Le-
vinson, 1987).  
We hypothesize that speakers also use redun-
dancy reactively, after the listener signals a lack of 
understanding, either verbally or non-verbally. 
This is redundancy in service of Fussell and 
Krauss? second method of message-tailoring. The 
advantages of providing redundant information 
reactively include increasing the efficiency of the 
exchange by only providing redundant information 
that the listener communicates a need for, and re-
ducing the burden on the speaker of having to de-
cide when to include redundant information.  
One important distinction between proactive 
and reactive redundancy is the grounding status of 
the redundant information. Reactive redundancy is 
likely to provide information that has not been ac-
cepted by the listener, and is therefore not part of 
the common ground (Clark and Schaeffer, 1989), 
even though it is discourse-old. In contrast, proac-
tive redundancy is likely to provide information 
from the interlocutors? common ground. Indeed, 
Walker (1996) describes Attitude redundant utter-
ances as providing evidence of grounding. Walk-
er?s other types of proactive redundancy 
(Consequence and Attention) make inferences 
based on grounded utterances explicit and make 
elements of the common ground salient again.  
Reactive redundancy is one type of repair, like 
expansions and replacements, which can be used in 
response to non-understanding or misunderstand-
ing (Hirst et al, 1994). The type of miscommuni-
cation may influence a speaker?s choice of repair 
strategy, with reactive redundancy being an appro-
priate response to mishearing or misremembering.  
However, producing redundant information, 
even when the listener signals a need for it, incurs 
a cost. Including redundant information increases 
the length of the dialogue and the speaker?s effort, 
and decreases the amount of new information pro-
vided within a certain length of time. In these cases 
the speaker must decide how much redundant in-
formation to provide and when to provide it.  
2.2 Signals of Confusion 
Listeners can express a need for information to be 
repeated or restated in a number of ways, both ver-
bally and non-verbally. Brinton et al (1988) used 
questions and statements of confusion (?I didn?t 
understand?) as signs of communication break-
downs. Morrow et al (1993) describe inaccurate 
and partial repetitions of instructions as elements 
of miscommunication. This prior work leads us to 
examine questions, utterances signaling non-
understanding (e.g. ?I don?t remember what?s 
38
next?), incorrect repetitions (e.g. ?take the third 
right? after the direction-giver said ?take the 
second right?) and abandoned utterances (e.g. 
?Then I?ll turn??) as possible signs of listener 
confusion. We predict redundancy after such 
statements because they all indicate that a piece of 
information has not been understood. 
We also examine eye-gaze as a non-verbal 
marker of listener comprehension. Goodwin (1981) 
described gaze towards the speaker as a sign of 
listener attention. However, Nakano et al (2003) 
found that speakers seemed to interpret a listener 
gazing at them rather than at a map as a sign of 
listener misunderstanding. Therefore, shifting eye-
gaze away from the speaker can signal that a lis-
tener is losing attention, perhaps due to confusion, 
while shifting gaze towards the speaker can signal 
misunderstanding. In this study there is no map, 
and listeners who can see the speaker spend most 
of the conversation gazing at the speaker. Still, due 
to the opposing findings in the literature, we ana-
lyze eye-gaze shifts both towards and away from 
the speaker as potential signs of listener confusion.  
2.3 Relationship and Communication 
Speakers are more explicit when communicating 
with strangers or people with whom they share less 
common ground. This explicitness can take the 
form of highly informative self-introductions on 
the phone (Hornstein, 1985), longer descriptions of 
abstract figures (Fussell and Krauss, 1989b), and 
explicit references to utterance topics (Svedsen and 
Evjemo, 2003). These studies indicate that speak-
ers attempt to make up for the small amount of 
common ground they share with strangers by in-
cluding more information in the discourse itself.  
Another difference between friends and non-
friends is that acquaintances tend to be more for-
mal, more concerned with self presentation, less 
negative, and less likely to disagree than friends 
(Schlenker, 1984; Tickle-Degnen and Rosenthal, 
1990; Planalp and Benson, 1992). Therefore, we 
expect that in an initial interaction, a speaker will 
try to appear competent and avoid conflict.  
As noted above, speakers talking to strangers 
are more explicit, leading us to predict more re-
dundancy overall. They are also more likely to try 
to impress their interlocutor and avoid conflict, 
leading to more reactive redundancy in response to 
confusion when the pair are strangers.  
2.4 Visibility and Communication 
Visibility also has a number of effects on commu-
nication. One of the most basic is that when inter-
locutors cannot see each other they cannot use non-
verbal signals to communicate, so they must rely 
on verbal communication. For example, the use of 
eye-gaze as a sign of listener attention (Argyle and 
Cook, 1976; Goodwin, 1981) is only possible 
when interlocutors can see each other. When they 
cannot see each other, they must indicate attention 
verbally or do without this information.  
Visibility affects both the form and the out-
comes of a conversation. When interlocutors can-
not see each other, conversations are longer and 
contain more, shorter, utterances than when they 
can (Nakano et al, 2003). Interlocutors in an in-
vestment game who could not see each other also 
did not establish trust to the same extent as those 
who met face-to-face (Bos et al, 2002).  
Because speakers who cannot see each other 
have fewer channels of communication available to 
them, their interaction can be more difficult than a 
face-to-face interaction. We predict that this will 
lead them to use more redundancy and more reac-
tive redundancy in an effort to be clear. 
2.5 Hypotheses 
In order to study how responsive speakers are to 
signs of listener confusion, we must first determine 
what signs speakers respond to. In this study we 
examine a number of verbal and non-verbal signs 
speakers may use to gauge listener confusion. In 
particular, we expect that speakers will provide 
redundancy in response to both verbal signs like 
questions, statements of non-understanding, incor-
rect statements, and abandoned utterances, and 
non-verbal signs like eye-gaze changes. We expect 
that speakers will strike a different balance be-
tween efficiency (minimizing speaker effort) and 
clarity (minimizing listener effort) depending on 
the relationship between the speaker and listener, 
and the physical context of the interaction. We ex-
pect speakers to use redundancy strategies focused 
on minimizing speaker effort when addressing 
friends and people they can see. Such strategies 
involve less redundancy (and therefore less speak-
ing), and less reactive redundancy (requiring less 
listener monitoring). Conversely, we expect to find 
redundancy strategies maximizing clarity when 
39
speakers address strangers and people they cannot 
see. Such strategies involve more redundancy 
overall (providing the listener with more informa-
tion in general) as well as more reactive redundan-
cy (which provides the listener with the specific 
information they may require). 
Hypothesis 1 - Redundancy and Non-
Understanding 
(a) Verbal cues - Direction-givers will provide 
redundancy when the receiver verbally expresses a 
lack of understanding by asking a question, aban-
doning an utterance, making an incorrect statement 
or explicitly expressing non-understanding.  
(b) Non-verbal cues - Givers will provide redun-
dancy when the receiver non-verbally expresses a 
lack of understanding by shifting eye-gaze.  
Hypothesis 2 - Redundancy and Relationship 
Givers will prioritize clarity over efficiency in their 
redundancy use when speaking to strangers, pro-
viding (a) more redundancy and (b) more reactive 
redundancy than when speaking to friends.  
Hypothesis 3 - Redundancy and Visual Contact 
Givers will prioritize clarity over efficiency in their 
redundancy use when they cannot see their partner, 
providing (a) more redundancy and (b) more reac-
tive redundancy than when they can see them. 
3 Methods 
3.1 Participants 
Twenty-four university students participated, re-
sulting in twelve dyads. All were paid $10 for their 
participation and received $5 gift certificates if 
they successfully completed the task. In each dyad 
the direction-giver was familiar with the building 
in which the experiment took place, and the direc-
tion-receiver was unfamiliar with it. Half the dyads 
were pairs of friends and half were strangers.  
3.2 Procedure 
The task consisted of three consecutive direction-
giving sessions, as described in Cassell et al 
(2007). At the start of each session, the experimen-
ter led the direction-giver to a point in the building, 
and back to the experiment room. Half of the dyads 
sat facing each other during the direction-giving 
(the Vision condition) and half sat back-to-back 
with a screen between them (the No-vision condi-
tion). The direction-giver then explained the route 
to the direction-receiver. There were no time limits 
or restrictions on what could be said, but the dyads 
could not use maps or props. When the dyad de-
cided that direction-giving was complete, they sig-
naled the experimenter, who the receiver led to the 
goal, following the directions.  
The direction-giving sessions were videotaped. 
Participants? speech was transcribed and coded for 
possible redundancy triggers and redundant utter-
ances using the coding scheme described below. 
The time-aligned codings for the giver and receiver 
were aligned with each other using scripts that cal-
culated which of the receiver?s utterances or ac-
tions directly preceded which of the giver?s 
utterances. The scripts classify a receiver?s utter-
ance or action as ?preceding? a giver?s utterance if 
its start precedes the start of the giver?s utterance 
and its end is not more than two seconds before the 
start of the giver?s utterance. The two-second limit 
was used to avoid positing connections between a 
giver?s utterance and receiver utterances that came 
long before it.  
3.3 Data Coding 
Each dialogue was divided into clauses, defined as 
units that include a subject and predicate and ex-
press a proposition. Each clause was coded using a 
modified version of DAMSL (Core and Allen, 
1997). Direction-givers? and receivers? speech was 
coded differently because we only studied redun-
dancy produced by the giver. We coded the receiv-
er?s speech for signs of confusion. We describe the 
labels we used in more detail below.  
Each direction-giver?s clauses were coded for 
Statements and Info-requests. The Info-request tag 
marks questions and other requests for informa-
tion. In a Statement, a speaker makes a claim about 
the world. The class of Statements was broken 
down into Non-redundant, in which the speaker is 
trying to change or add to the hearer?s beliefs, and 
Redundant, which contain only information that 
has already been stated or entailed. 
Each direction-receiver?s clauses were coded 
for Statements, Info-requests, Signal non-
understandings (S.N.U.), and Abandoned utter-
ances. The receiver?s Statements were classified as 
either Correct or Incorrect. If an utterance explicit-
ly expressed non-understanding of an earlier utter-
ance it was coded as Signal non-understanding. 
This label was only used for direct statements of 
non-understanding, such as ?I didn?t follow that,? 
40
and not for signals of non-understanding covered 
by other labels such as Info-requests and Incorrect 
Statements. Utterances that were abandoned (the 
speaker stops the utterance and it provides no con-
tent to the dialogue) were coded as Abandoned. 
Receiver utterances that were not coded as Info-
requests, Incorrect Statements, Signal-non-
understandings, or Abandoned, were coded as No-
trigger. No-trigger utterances included correct 
statements and statements about task management.  
4 Results 
We found that a large proportion of giver utter-
ances were redundant, ranging from 17% to 38% 
with a mean of 25%. Examples of redundancy 
from our recordings are listed in the Appendix.  
We first analyzed the data using a hierarchical 
loglinear analysis with the variables: visual condi-
tion (Vision, No-vision), relationship (Friends, 
Strangers), receiver-utterance (Info-request, Incor-
rect statement, Signal non-understanding, Aban-
doned, No-trigger), and giver-utterance 
(Redundant, Non-redundant). The overall model is 
significant (?2(39,5294)=13254.157,p<.001), justify-
ing chi-square comparisons of individual factors 
within the model. We report tests of partial associ-
ation and chi-square tests to indicate where signifi-
cant differences lie between groups.  
4.1 Redundancy and Non-Understanding 
Verbal Signals of Non-Understanding 
We tested part (a) of Hypothesis 1 by running a 
test of partial associations (adjusted for all effects 
in the model) and an unpartialled chi-square (ig-
noring variables not included in the effect being 
tested). These showed a significant association be-
tween receiver-utterance and giver-utterance type 
(Partial ?2(4,5294)=117.7, p<.001; 
?2(4,5294)=121.2,p<.001). 
Chi-square tests comparing giver-utterances fol-
lowing predicted redundancy triggers to giver-
utterances after No-trigger receiver utterances, in-
dicate that Info-requests, Incorrect statements and 
Abandoned utterances all significantly increase the 
likelihood that the giver will produce a redundant 
utterance (?2(1,4907)=57.3,p<.001; ?2(1,4562)=28.4, 
p<.001; ?2(1,4651)=49.1,p<.001, respectively). Expli-
cit Signal-non-understandings do not have signifi-
cant effects on the likelihood of a redundant-
utterance (?2(1,4539)=.3,p=.619). Figure 1 shows the 
percentages of giver utterances that were redundant 
following various receiver dialogue acts.  
Non-Verbal Signals of Non-Understanding 
We tested part (b) of Hypothesis 1 with a separate 
hierarchical loglinear analysis examining only the 
dyads in the Vision condition for the effects of: 
relationship, receiver-utterance, giver-utterance, 
and receiver-gaze (Gaze-to, Gaze-away, and No-
gaze-change). The first- and second-order effects 
are significant (?2(59,2815)=9582.4, p<.001).  
A test of partial associations and a chi-square 
test indicate a significant association between giv-
er-utterance and receiver-gaze (Partial ?2(2,2815)= 
22.7, p<.001; ?2(2,2815)=24.7,p<.001). Chi-square 
tests comparing receiver gaze changes to non-
changes show that redundant utterances are signifi-
cantly more likely after a gaze change toward the 
giver (?2(1,2433)=21.5,p<.001) and after a gaze 
change away from the giver (?2(1,2475)=6.5,p<.05) 
than after no gaze change. A chi-square test com-
paring gaze change toward the giver to gaze 
change away from the giver shows that the differ-
ence between them is not significant (?2(1,722)=2.7, 
p=.098). These effects are shown in Figure 2.  
 
 
     
52.2% 48.7% 41.0%
27.3% 24.0%
0%
10%
20%
30%
40%
50%
60%
Red
und
ant 
 GIv
er  U
ttera
nces
 (%) **
*
 
Figure 1. Percent of redundant giver utterances fol-
lowing various receiver dialogue acts. 
 
        
39.7%
33.8%
27.4%
0%5%
10%15%
20%25%
30%35%
40%45%
Towards Away No changeRed
und
ant 
 Giv
er  U
ttera
nces
 (%)
Gaze
* *
 
Figure 2. Percent of redundant giver utterances fol-
lowing receiver eye-gaze changes toward and away 
from the giver, and following no gaze change 
 
41
4.2 Redundancy and Relationship 
Part (a) of Hypothesis 2 was confirmed by the sig-
nificant association between relationship and giv-
er-utterance (Partial ?2(1,5294)=13.3, p<.001; 
?2(1,5294)=6, p<.05) in our original analysis. A larger 
percentage of giver utterances are redundant in the 
Strangers condition (27.8%) than in the Friends 
condition (24.8%). 
 
To examine part (b) of Hypothesis 2 we ran a 
hierarchical loglinear analysis after collapsing all 
receiver-utterances into question/non-question cat-
egories. This reveals a significant partial associa-
tion among giver-utterance, receiver-utterance, and 
relationship (Partial ?2(1,5294)=7.5, p<.01). A chi-
square test comparing utterances after questions in 
the Friends and Strangers conditions shows that 
redundant utterances are significantly more likely 
after questions in the Strangers condition than the 
Friends condition (?2(1,412)= 14.6, p<.0005), as 
shown in Figure 3.  
Three-way interactions among giver-utterance, 
receiver-utterance and relationship are not signifi-
cant in any of the other analyses. 
4.3 Redundancy and Visual Contact 
There is a trend-level association between visual 
condition and giver-utterance type (Partial ?2(1,5294) 
=4.6,p<.05; ?2(1,5294)=3.3,p=.071). Contrary to Hy-
pothesis 3, a larger percentage of utterances are 
redundant in the Vision condition (27.7%) than in 
the No-vision condition (25.5%). No significant 
association was found among giver-utterance, re-
ceiver-utterance, and visual condition, even when 
collapsed into question/non-question categories. 
5 Discussion 
This study set out to discover what verbal and non-
verbal behaviors increase the likelihood of redun-
dant utterances in direction-givers? speech. We 
also examined whether the interlocutors? relation-
ship or visual contact influence whether speakers 
provide redundant utterances in anticipation of and 
in response to listener confusion. We found that 
givers used a large proportion of redundant utter-
ances, (around 25% of utterances). Walker (1996) 
found that about 12% of utterances were redundant 
in a corpus of recordings from a call-in financial 
radio show. The higher proportion of redundant 
utterances in our study is predicted by Walker?s 
(1996) model, in which a task?s tolerance for com-
prehension errors influences whether redundant 
utterances are produced. In a radio advice show, a 
misunderstanding may be more easily recovered 
from than in direction-giving, in which one wrong 
turn could make it impossible to reach the goal.  
In addition to revealing the impact of task toler-
ance to error on redundancy, this study sheds light 
on other circumstances that influence redundancy 
use. Givers produced reactive redundancy in re-
sponse to the verbal triggers: Info-requests, Aban-
doned utterances, and Incorrect statements. 
However, even these triggers were not always fol-
lowed by redundancy. In fact, only around 50% of 
the utterances following these triggers were redun-
dant. Such a low response rate is surprising until 
we consider the diversity of utterances covered by 
these labels. For instance, some Info-requests seek 
new information (e.g. ?What?s at the top of the 
stairs??), and some receiver utterances are aban-
doned because the giver interrupts with new in-
formation. Our study lays the groundwork for 
future examinations of speaker responses to listen-
er confusion, which can refine these broad catego-
ries. We must also consider the variability in 
responses to listener confusion. We found that giv-
ers are more likely to provide redundant utterances 
in response to questions when speaking to stran-
gers, but this is only one of many factors that could 
affect levels of responsiveness, including speaker 
personality, time pressure, and task difficulty.  
The non-significant effect of Signals non-
understandings on redundancy is surprising. This 
may be due to the small number of examples of 
this category in our recordings. We found only 44 
instances of Signal non-understandings, in contrast 
to, for example, 156 Abandoned utterances.  
The non-verbal cue gaze change also increased 
the likelihood of a redundant utterance. Interesting-
ly, gaze changes both to and away from the giver 
              
37%
24%
52%
27%
0%
10%
20%
30%
40%
50%
60%
Question Non-question
Re
du
nd
an
t  
Gi
ve
r 
 
Ut
te
ra
nc
es
 
(%
) FriendsStrangers
*
 
Figure 3. Percent of redundant giver utterances fol-
lowing questions and non-questions, by relationship. 
 
42
triggered redundancy. This is consistent with both 
Nakano et al?s (2003) finding that gazing at the 
speaker signals listener misunderstanding and 
Goodwin?s (1981) finding that gazing away from 
the speaker indicates a lack of listener attention.  
It is interesting that 24% of giver utterances fol-
lowing No-trigger receiver utterances were redun-
dant. These probably include both redundant 
utterances triggered by signs of listener confusion 
that we did not code for, and proactive redundancy. 
Proactive redundancy can appear within the first 
description of some directions (see the No-trigger 
example in the Appendix) and when the whole set 
of directions is repeated as a memory aid.  
The relationship between the interlocutors does 
affect the amount of redundancy speakers produce 
overall and in response to listener signs of confu-
sion. Strangers used more redundant utterances 
than friends and provided more redundant utter-
ances after questions. This supports our hypothesis 
that direction-givers speaking to strangers will pri-
oritize clarity over efficiency. The more consistent 
use of reactive redundancy in the Strangers condi-
tion may be due to speakers? tendency to avoid 
confrontation with strangers. When responding to 
questions from friends, direction-givers may pro-
vide some new information because they know that 
their friend will feel comfortable asking another 
question if their answer is unclear. However, when 
answering questions from a stranger, the giver may 
wish to avoid the embarrassment of further confu-
sion by repeating more discourse-old information.  
However, contrary to our predictions, we did 
not find more redundancy or more reactive redun-
dancy in the No-vision condition than the Vision 
condition. In fact, we found numerically more re-
dundancy in the Vision condition. Given the low 
level of significance, we do not discuss this in de-
tail, however we suggest that this could be due to 
the fact that there are more ways of signaling non-
understanding available to the receivers in the Vi-
sion condition (both verbal and non-verbal). There-
fore, even if givers do not increase their rates of 
reactive redundancy in the Vision condition, they 
could provide more reactive redundancy (and more 
redundancy overall) because they are receiving 
more cues to react to. Not all situations leading to 
communication difficulties encourage more redun-
dancy or more reactive redundancy, but the in-
creased explicitness and positivity typical of 
conversation between strangers do encourage it.   
6 Conclusion 
This study explored the use of redundancy in task-
oriented dialogue, specifically the effects of listen-
er behavior and communicative context on the 
amount of redundancy produced. We found that 
direction-givers provided redundant utterances in 
response to verbal and non-verbal signs of listener 
confusion. As predicted, givers were more likely to 
prioritize clarity over efficiency in their redundan-
cy use (using more redundancy overall and more 
redundancy in response to questions) when speak-
ing to strangers than friends. Contrary to our pre-
dictions, givers did not provide more redundant 
utterances when they could not see their listener.   
Direction-giving, due to its high memory load 
and the need for the receiver to understand the giv-
er almost completely, is a type of discourse that 
may encourage more redundancy than other types. 
Indeed, we note that our data have a much greater 
proportion of redundancies than discussions taken 
from radio talk shows (Walker, 1996). Future work 
should examine the nature of proactive and reac-
tive redundancy in more varied discourse contexts, 
such as negotiation, teaching, and play. It should 
also explore the effects of memory load on redun-
dancy by varying task complexity, which may be 
easier with a more controlled task like the Map-
task. Researchers could study the relationship be-
tween saliency and redundancy by studying 
correlations between a segment?s salience and its 
likelihood of being used in a redundant utterance.  
Our findings can be used to improve the com-
municative efficacy of natural language generation 
systems like those used in Embodied Conversa-
tional Agents (ECAs; Kopp et al, 2008). For ex-
ample, like strangers, direction-giving ECAs could 
use increased overall and reactive redundancy to 
compensate for the lack of shared common ground 
with the human user of the system. Analyses of the 
syntactic structures of different types of redundant 
utterances will be important for incorporating these 
results into generation systems. 
Acknowledgments 
We thank Paul Tepper, Gregory Ward, Darren 
Gergle, Alex Podbelski, and our anonymous re-
viewers for their helpful advice and hard work. We 
are grateful for generous funding from Motorola 
and NSF HCC 0705901. 
43
References  
M. Argyle and M. Cook. 1976. Gaze and Mutual Gaze. 
Cambridge University Press, New York. 
E. Bard, A. Anderson, C. Sotillo, M. Aylett, G. Doher-
ty-Sneddon and A. Newlands. 2000. Controlling the 
intelligibility of referring expressions in dialogue. J. 
Memory and Language, 42(1):1-22. 
N. Bos, J. Olson, D. Gergle, G. Olson, and Z. Wright. 
2002. Effects of four computer-mediated communica-
tion channels on trust development. In Proceedings of 
SIGCHI 2002, pages 135-140, Minneapolis, MN. 
H. P. Branigan, J. F. McLean, and H. Reeve. 2003. 
Something old, something new: Addressee knowledge 
and the given-new contract. In Proceedings of the 
25th Annual Conference of the Cognitive Science So-
ciety, pages 180-185, Boston, MA. 
B. Brighton, M. Fujiki, and E. Sonnenberg. 1988. Res-
ponses to requests for clarification by linguistically 
normal and language-impaired children in conversa-
tion. J. Speech and Hearing Disorders, 53:383-391. 
P. Brown and S. C. Levinson. 1987. Politeness: Some 
Universals in Language Usage. Cambridge Universi-
ty Press, Cambridge, UK. 
J. Cassell, A. J. Gill, and P. Tepper. 2007. Coordination 
in conversation and rapport. In Proceedings of the 
Workshop on Embodied Language Processing at 
ACL, pages 41-50, Prague. 
H. H. Clark and E. F. Schaeffer. 1989. Contributing to 
discourse. Cognitive Science,13:259-294.  
M. G. Core and J. F. Allen. 1997. Coding dialogs with 
the DAMSL annotation scheme. In Proceedings of 
AAAI Fall Symposium on Communicative Action in 
Humans and Machines, pages 28-35, Boston, MA. 
H. Dalianis. 1999. Aggregation in natural language gen-
eration. J. Computational Intelligence, 15(4):384-414. 
S. R. Fussell and R. M. Krauss. 1989 a. The effects of 
intended audience on message production and com-
prehension: Reference in a common ground frame-
work. European J. Social Psychology, 25:203-219. 
S. R. Fussell and R. M. Kraus. 1989 b. Understanding 
friends and strangers: The effects of audience design 
on message comprehension. European J. Social Psy-
chology, 19:445-454. 
C. Goodwin. 1981. Conversational Organization: Inte-
raction between Speakers and Hearers. Academic 
Press, New York. 
H. P. Grice. 1975. Logic and conversation. In P. Cole 
and J. Morgan, editors, Syntax and Semantics III ? 
Speech Acts. Academic Press,New York, pages 41-58. 
G. Hirst, S. McRoy, P. Heeman, P. Edmonds, and D. 
Horton. 1994. Repairing conversational misunders-
tandings and non-understandings. Speech Communi-
cation, 15: 213-230. 
L. Horn. 1993. Economy and redundancy in a dualistic 
model of natural language. SKY 1993: Yearbook of 
the Linguistic Association of Finland: 33-72. 
A. Horning. 1991. Readable writing: The role of cohe-
sion and redundancy. J. Advanced Composition, 
11:135-145. 
G. Hornstein. 1985. Intimacy in conversational style as 
a function of the degree of closeness between mem-
bers of a dyad. J. Personality and Social Psychology, 
49(3):671-681. 
W. Horton and B. Keysar. 1996. When do speakers take 
into account common ground? Cognition, 59:91?117. 
S. Kopp, P. Tepper, K. Ferriman, K. Striegnitz and J. 
Cassell. 2008. Trading spaces: How humans and hu-
manoids use speech and gesture to give directions. In 
T. Nishida, editor, Conversational Informatics. John 
Wiley & Sons, New York, pages 133-160. 
B. Lindlom. 1990. Explaining phonetic variation: A 
sketch of the H and H theory. In W. Hardcastle & A. 
Marchal, editors, Speech Production and Speech 
Modeling. Kluwer, Dordrecht, pages 403-439. 
D. Morrow, A. Lee, and M. Rodvold. 1993. Analysis of 
problems in routine controller-pilot communication. 
International J. Aviation Psychology. 3(4): 285-302. 
Y. Nakano, G. Reinstein, T. Stocky, and J. Cassell. 
2003. Towards a model of face-to-face grounding. In 
Proceedings of ACL 2003, pages 553-561, Sapporo, 
Japan. 
S. Planalp and A. Benson. 1992. Friends' and acquain-
tances' conversations I: Perceived differences. J. So-
cial and Personal Relationships, 9:483-506.  
B. Schlenker. 1984. Identities, identifications, and rela-
tionships. In V. Derlega, editor, Communication, In-
timacy and Close Relationships. Academic Press, 
New York, pages 71-104. 
R. Stalnaker. 1978. Assertion. In P. Cole, editor, Syntax 
and Semantics, Volume 9: Pragmatics. Academic 
Press, New York, pages 315-332. 
G. Svendsen and B. Evjemo. 2003. Implicit referring as 
an indication of familiarity in face-to-face and phone 
conversations. In Proceedings of INTERACT '03: 
pages 920-923, Zurich.  
M. Theune, D. Hofs and M. van Kessel. 2007. The Vir-
tual guide: A direction giving embodied conversa-
tional agent. In Proceedings of Interspeech 2007, 
pages 2197-2200, Antwerp, Belgium. 
L. Tickle-Degnen and R. Rosenthal. 1990. The nature of 
rapport and its nonverbal correlates. Psychological 
Inquiry, 1(4):285-293. 
M. A. Walker. 1992. Redundancy in collaborative di-
alogue. In Proceedings of the 14th International Con-
ference on Computational Linguistics, pages 345-351, 
Nantes, France. 
M. A. Walker. 1996. The effect of resource limits and 
task complexity on collaborative planning in dialo-
gue. Artificial Intelligence Journal, 85:181-243. 
44
Appendix: Examples from Dialogues 
 
In the following examples, utterances in italics are 
the triggers produced by the receiver, and under-
lined utterances are redundant.  Commas indicate 
pauses.  Receiver utterances in square brackets 
overlap with the portion of the preceding giver ut-
terance in brackets.   
 
 
Question Example 
Giver (G): as soon as you come outta the door, 
uhh on the second floor you?ll [see like a win-
dow] in front of you 
Receiver (R): [mmhm] 
G: [and then], you?ll wanna take a left 
R: [hm] 
? 
G: if you look to your left you?ll see the exit sign, 
uhh with for the stairwell 
R: ok so then I go to this second floor 
G: mmhm 
R: and then do I go right? 
G: no 
R: or left? 
G: you go left [once you come outta] the second 
floor 
R: [you go left] 
 
 
Incorrect Statement Example 
G: and you?re gonna go towards the computer, and 
pass the computer, and there will be, copy ma-
chines on your right after you pass the computer  
R: mhmm  
G: so after you, walk, just past the copy machines 
you?re gonna want to take a hard left, almost like 
a U-turn 
? 
G: once you turn to the right at after the first stairs 
you?ll you?ll see a computer 
R: oh a computer right ok and then I?m gonna take 
a really hard left like a U-turn 
G: right well you go past the computer and then 
you?ll see copying machines 
R: oh ok  
G: and then but, the copy machines are like maybe 
three five feet after the computer 
R: ok 
G: and then that?s when you take the hard left 
 
Abandoned Example 
G: and then you?re gonna hear some kids and 
people talking and stuff, you?re gonna be head-
ing toward the clinic 
R: oh okay 
G: okay, the clinic you?re is gonna come up on 
your right, [there?s gonna] be, kind of, semi cir-
cular blue couches  
R: [okay], uhhuh 
G: down there, the stapler, is on the floor, right 
next to a pillar, [um] so basically you?re gonna 
like, you?re gonna kind of, turn right to look into 
the clinic 
R: [okay], okay 
G: and then, the stapler?s kinda just over there to 
the left, on the floor by one of the pillars 
? 
G: and you?re gonna hear people talking and 
there?s gonna [be kids] 
R: [okay] so and then the, pillar its? like gonna be 
one of the pillars on the, right by like I guess it?s 
on the  
G: basically, basically um you walk into, the clin-
ic, and there?s blue, couches 
R: mmhm 
G: and then it?s just a little bit over to the left 
R: oh okay 
G: on the floor 
 
 
No-Trigger Example 
G: open the door, and you?re gonna see a set of 
stairs 
R: okay  
G: go down those stairs, to the second floor 
R: mmhm 
G so you?re gonna be on the third floor, you?re 
gonna then you?re gonna take the stairs down to 
the second floor 
R: okay 
45
Proceedings of the 12th European Workshop on Natural Language Generation, pages 165?173,
Athens, Greece, 30 ? 31 March 2009. c?2009 Association for Computational Linguistics
Report on the First NLG Challenge on
Generating Instructions in Virtual Environments (GIVE)
Donna Byron
Northeastern University
dbyron@ccs.neu.edu
Alexander Koller
Saarland University
koller@mmci.uni-saarland.de
Kristina Striegnitz
Union College
striegnk@union.edu
Justine Cassell
Northwestern University
justine@northwestern.edu
Robert Dale
Macquarie University
Robert.Dale@mq.edu.au
Johanna Moore
University of Edinburgh
J.Moore@ed.ac.uk
Jon Oberlander
University of Edinburgh
J.Oberlander@ed.ac.uk
Abstract
We describe the first installment of the
Challenge on Generating Instructions in
Virtual Environments (GIVE), a new
shared task for the NLG community. We
motivate the design of the challenge, de-
scribe how we carried it out, and discuss
the results of the system evaluation.
1 Introduction
This paper reports on the methodology and results
of the First Challenge on Generating Instructions
in Virtual Environments (GIVE-1), which we ran
from March 2008 to February 2009. GIVE is a
new shared task for the NLG community. It pro-
vides an end-to-end evaluation methodology for
NLG systems that generate instructions which are
meant to help a user solve a treasure-hunt task in a
virtual 3D world. The most innovative aspect from
an NLG evaluation perspective is that the NLG
system and the user are connected over the Inter-
net. This makes it possible to cheaply collect large
amounts of evaluation data.
Five NLG systems were evaluated in GIVE-
1 over a period of three months from November
2008 to February 2009. During this time, we
collected 1143 games that were played by users
from 48 countries. As far as we know, this makes
GIVE-1 the largest evaluation effort in terms of
experimental subjects ever. We have evaluated the
five systems both on objective measures (success
rate, completion time, etc.) and subjective mea-
sures which were collected by asking the users to
fill in a questionnaire.
GIVE-1 was intended as a pilot experiment in
order to establish the validity of the evaluation
methodology and understand the challenges in-
volved in the instruction-giving task. We believe
that we have achieved these purposes. At the same
time, we provide evaluation results for the five
NLG systems which will help their developers im-
prove them for participation in a future challenge,
GIVE-2. GIVE-2 will retain the successful aspects
of GIVE-1, while refining the task to emphasize
aspects that we found to be challenging. We invite
the ENLG community to participate in designing
GIVE-2.
Plan of the paper. The paper is structured as
follows. In Section 2, we will describe and moti-
vate the GIVE Challenge. In Section 3, we will
then describe the evaluation method and infras-
tructure for the challenge. Section 4 reports on
the evaluation results. Finally, we conclude and
discuss future work in Section 5.
2 The GIVE Challenge
In the GIVE scenario, subjects try to solve a trea-
sure hunt in a virtual 3D world that they have not
seen before. The computer has a complete sym-
bolic representation of the virtual world. The chal-
lenge for the NLG system is to generate, in real
time, natural-language instructions that will guide
the users to the successful completion of their task.
Users participating in the GIVE evaluation
start the 3D game from our website at www.
give-challenge.org. They then see a 3D
game window as in Fig. 1, which displays instruc-
tions and allows them to move around in the world
and manipulate objects. The first room is a tuto-
rial room where users learn how to interact with
the system; they then enter one of three evaluation
worlds, where instructions for solving the treasure
hunt are generated by an NLG system. Users can
either finish a game successfully, lose it by trig-
gering an alarm, or cancel the game. This result is
stored in a database for later analysis, along with a
complete log of the game.
Complete maps of the game worlds used in the
evaluation are shown in Figs. 3?5: In these worlds,
players must pick up a trophy, which is in a wall
safe behind a picture. In order to access the tro-
165
Figure 1: What the user sees when playing with
the GIVE Challenge.
phy, they must first push a button to move the pic-
ture to the side, and then push another sequence of
buttons to open the safe. One floor tile is alarmed,
and players lose the game if they step on this tile
without deactivating the alarm first. There are also
a number of distractor buttons which either do
nothing when pressed or set off an alarm. These
distractor buttons are intended to make the game
harder and, more importantly, to require appropri-
ate reference to objects in the game world. Finally,
game worlds contained a number of objects such
as chairs and flowers that did not bear on the task,
but were available for use as landmarks in spatial
descriptions generated by the NLG systems.
2.1 Why a new NLG evaluation paradigm?
The GIVE Challenge addresses a need for a new
evaluation paradigm for natural language gener-
ation (NLG). NLG systems are notoriously hard
to evaluate. On the one hand, simply compar-
ing system outputs to a gold standard using auto-
matic comparison algorithms has limited value be-
cause there can be multiple generated outputs that
are equally good. Finding metrics that account
for this variability and produce results consistent
with human judgments and task performance mea-
sures is difficult (Belz and Gatt, 2008; Stent et
al., 2005; Foster, 2008). Human assessments of
system outputs are preferred, but lab-based eval-
uations that allow human subjects to assess each
aspect of the system?s functionality are expensive
and time-consuming, thereby favoring larger labs
with adequate resources to conduct human sub-
jects studies. Human assessment studies are also
difficult to replicate across sites, so system devel-
opers that are geographically separated find it dif-
ficult to compare different approaches to the same
problem, which in turn leads to an overall diffi-
culty in measuring progress in the field.
The GIVE-1 evaluation was conducted via a
client/server architecture which allows any user
with an Internet connection to provide system
evaluation data. Internet-based studies have been
shown to provide generous amounts of data in
other areas of AI (von Ahn and Dabbish, 2004;
Orkin and Roy, 2007). Our implementation allows
smaller teams to develop a system that will partici-
pate in the challenge, without taking on the burden
of running the human evaluation experiment, and
it provides a direct comparison of all participating
systems on the same evaluation data.
2.2 Why study instruction-giving?
Next to the Internet-based data collection method,
GIVE also differs from other NLG challenges by
its emphasis on generating instructions in a vir-
tual environment and in real time. This focus on
instruction giving is motivated by a growing in-
terest in dialogue-based agents for situated tasks
such as navigation and 3D animations. Due to its
appeal to younger students, the task can also be
used as a pedagogical exercise to stimulate interest
among secondary-school students in the research
challenges found in NLG or Computational Lin-
guistics more broadly.
Embedding the NLG task in a virtual world en-
courages the participating research teams to con-
sider communication in a situated setting. This
makes the NLG task quite different than in other
NLG challenges. For example, experiments have
shown that human instruction givers make the in-
struction follower move to a different location in
order to use a simpler referring expression (RE)
(Stoia et al, 2006). That is, RE generation be-
comes a very different problem than the classi-
cal non-situated Dale & Reiter style RE genera-
tion, which focuses on generating REs that are sin-
gle noun phrases in the context of an unchanging
world.
On the other hand, because the virtual environ-
ments scenario is so open-ended, it ? and specif-
ically the instruction-giving task ? can potentially
be of interest to a wide range of NLG researchers.
This is most obvious for research in sentence plan-
ning (GRE, aggregation, lexical choice) and real-
ization (the real-time nature of the task imposes
high demands on the system?s efficiency). But if
166
extended to two-way dialog, the task can also in-
volve issues of prosody generation (i.e., research
on text/concept-to-speech generation), discourse
generation, and human-robot interaction. Finally,
the game world can be scaled to focus on specific
issues in NLG, such as the generation of REs or
the generation of navigation instructions.
3 Evaluation Method and Logistics
Now we describe the method we applied to obtain
experimental data, and sketch the software infras-
tructure we developed for this purpose.
3.1 Software architecture
A crucial aspect of the GIVE evaluation methodol-
ogy is that it physically separates the user and the
NLG system and connects them over the Internet.
To achieve this, the GIVE software infrastructure
consists of three components (shown in Fig. 2):
1. the client, which displays the 3D world to
users and allows them to interact with it;
2. the NLG servers, which generate the natural-
language instructions; and
3. the Matchmaker, which establishes connec-
tions between clients and NLG servers.
These three components run on different ma-
chines. The client is downloaded by users from
our website and run on their local machine; each
NLG server is run on a server at the institution
that implemented it; and the Matchmaker runs on
a central server we provide. When a user starts the
client, it connects to the Matchmaker and is ran-
domly assigned an NLG server and a game world.
The client and NLG server then communicate over
the course of one game. At the end of the game,
the client displays a questionnaire to the user, and
the game log and questionnaire data are uploaded
to the Matchmaker and stored in a database. Note
that this division allows the challenge to be con-
ducted without making any assumptions about the
internal structure of an NLG system.
The GIVE software is implemented in Java and
available as an open-source Google Code project.
For more details about the software, see (Koller et
al., 2009).
3.2 Subjects
Participants were recruited using email distribu-
tion lists and press releases posted on the internet.
Game Client
Matchmaker
NLG Server
NLG Server
NLG Server
Figure 2: The GIVE architecture.
Collecting data from anonymous users over the
Internet presents a variety of issues that a lab-
based experiment does not. An Internet-based
evaluation skews the demographic of the subject
pool toward people who use the Internet, but prob-
ably no more so than if recruiting on a college
campus. More worrisome is that, without a face-
to-face meeting, the researcher has less confidence
in the veracity of self-reported demographic data
collected from the subject. For the purposes of
NLG software, the most important demographic
question is the subject?s fluency in English. Play-
ers of the GIVE 2009 challenge were asked to self-
report their command of English, age, and com-
puter experience. English proficiency did interact
with task completion, which leads us to conclude
that users were honest about their level of English
proficiency. See section 4.4 below for a discus-
sion of this interaction. All-in-all, we feel that the
advantage gained from the large increase in the
size of the subject pool offsets any disadvantage
accrued from the lack of accurate demographic in-
formation.
3.3 Materials
Figs. 3?5 show the layout of the three evaluation
worlds. The worlds were intended to provide vary-
ing levels of difficulty for the direction-giving sys-
tems and to focus on different aspects of the prob-
lem. World 1 is very similar to the development
world that the research teams were given to test
their system on. World 2 was intended to focus
on object descriptions - the world has only one
room which is full of objects and buttons, many of
which cannot be distinguished by simple descrip-
tions. World 3, on the other hand, puts more em-
phasis on navigation directions as the world has
many interconnected rooms and hallways.
The difference between the worlds clearly bears
out in the task completion rates reported below.
167
plant
chair
alarm
lamp
tutorial room
couch
safe
Figure 3: World 1
lamp
plant
chair
alarm
tutorial room
safe
Figure 4: World 2
plant
chair
lamp
safe
tutorial room
alarm
Figure 5: World 3
3.4 Timeline
After the GIVE Challenge was publicized in
March 2008, eight research teams signed up for
participation. We distributed an initial version of
the GIVE software and a development world to
these teams. In the end, four teams submitted
NLG systems. These were connected to a cen-
tral Matchmaker instance that ran for about three
months, from 7 November 2008 to 5 February
2009. During this time, we advertised participa-
tion in the GIVE Challenge to the public in order
to obtain experimental subjects.
3.5 NLG systems
Five NLG systems were evaluated in GIVE-1:
1. one system from the University of Texas at
Austin (?Austin? in the graphics below);
2. one system from Union College in Schenec-
tady, NY (?Union?);
3. one system from the Universidad Com-
plutense de Madrid (?Madrid?);
4. two systems from the University of Twente:
one serious contribution (?Twente?) and one
more playful one (?Warm-Cold?).
Of these systems, ?Austin? can serve as a base-
line: It computes a plan consisting of the actions
the user should take to achieve the goal, and at
each point in the game, it realizes the first step
in this plan as a single instruction. The ?Warm-
Cold? system generates very vague instructions
that only tell the user if they are getting closer
(?warmer?) to their next objective or if they are
moving away from it (?colder?). We included this
system in the evaluation to verify whether the eval-
uation methodology would be able to distinguish
such an obviously suboptimal instruction-giving
strategy from the others.
Detailed descriptions of these systems
as well as each team?s own analysis of
the evaluation results can be found at
http://www.give-challenge.org/
research/give-1.
4 Results
We now report on the results of GIVE-1. We start
with some basic demographics; then we discuss
objective and subjective evaluation measures.
Notice that some of our evaluation measures are
in tension with each other: For instance, a system
which gives very low-level instructions (?move
forward?; ?ok, now move forward?; ?ok, now turn
left?), such as the ?Austin? baseline, will lead the
user to completing the task in a minimum number
of steps; but it will require more instructions than
a system that aggregates these. This is intentional,
and emphasizes both the pilot experiment char-
acter of GIVE-1 and our desire to make GIVE a
friendly comparative challenge rather than a com-
petition with a clear winner.
4.1 Demographics
Over the course of three months, we collected
1143 valid games. A game counted as valid if the
game client didn?t crash, the game wasn?t marked
as a test game by the developers, and the player
completed the tutorial.
Of these games, 80.1% were played by males
and 9.9% by females; a further 10% didn?t specify
their gender. The players were widely distributed
over countries: 37% connected from an IP address
in the US, 33% from an IP address in Germany,
and 17% from China; Canada, the UK, and Aus-
tria also accounted for more than 2% of the partic-
168
037,5
75,0
112,5
150,0
N
o
v
 
7
D
e
c
 
1
J
a
n
 
1
F
e
b
 
1
F
e
b
 
5
# games per day
German
press release
US
press release
posted to
SIGGEN list
covered by
Chinese blog
Figure 6: Histogram of the connections per day.
ipants each, and the remaining 2% of participants
connected from 42 further countries. This imbal-
ance stems from very successful press releases that
were issued in Germany and the US and which
were further picked up by blogs, including one
in China. Nevertheless, over 90% of the partici-
pants who answered this question self-rated their
English proficiency as ?good? or better. About
75% of users connected with a client running on
Windows, with the rest split about evenly among
Linux and Mac OS X.
The effect of the press releases is also plainly
visible if we look at the distribution of the valid
games over the days from November 7 to Febru-
ary 5 (Fig. 6). There are huge peaks at the
very beginning of the evaluation period, coincid-
ing with press releases through Saarland Univer-
sity in Germany and Northwestern University in
the US, which were picked up by science and tech-
nology blogs on the Web. The US peak contains
a smaller peak of connections from China, which
were sparked by coverage in a Chinese blog.
4.2 Objective measures
We then extracted objective and subjective mea-
surements from the valid games. The objective
measures are summarized in Fig. 7. For each sys-
tem and game world, we measured the percent-
age of games which the users completed success-
fully. Furthermore, we counted the numbers of in-
structions the system sent to the user, measured
the time until task completion, and counted the
number of low-level steps executed by the user
(any key press, to either move or manipulate an
object) as well as the number of task-relevant ac-
tions (such as pushing a button to open a door).
? task success (Did the player get the trophy?)
? instructions (Number of instructions pro-
duced by the NLG system.?)
? steps (Number of all player actions.?)
? actions (Number of object manipulation
action.?)
? second (Time in seconds.?)
?
Measured from the end of the tutorial until the
end of the game.
Figure 7: Objective measurements
A
us
ti
n
M
ad
ri
d
Tw
en
te
U
ni
on
W
ar
m
-C
ol
d
task
success
40% 71% 35% 73% 18%
A A
B B
C
instructions
83.2 58.3 121.2 80.3 190.0
A
B B
C
D
steps
103.6 124.3 160.9 117.5 307.4
A A
B B
C
D
actions
11.2 8.7 14.3 9.0 14.3
A A
B
C C
seconds
129.3 174.8 207.0 175.2 312.2
A
B B
C
D
Figure 8: Objective measures by system. Task
success is reported as the percentage of suc-
cessfully completed games. The other measures
are reported as the mean number of instruc-
tions/steps/actions/seconds, respectively. Letters
group indistinguishable systems; systems that
don?t share a letter were found to be significantly
different with p < 0.05.
169
To ensure comparability, we only counted success-
fully completed games for all these measures, and
only started counting when the user left the tutorial
room. Crucially, all objective measures were col-
lected completely unobtrusively, without requiring
any action on the user?s part.
Fig. 8 shows the results of these objective mea-
sures. This figure assigns systems to groups A,
B, etc. for each evaluation measure. Systems in
group A are better than systems in group B, etc.;
if two systems don?t share the same letter, the dif-
ference between these two systems is significant
with p < 0.05. Significance was tested using a
?2-test for task success and ANOVAs for instruc-
tions, steps, actions, and seconds. These were fol-
lowed by post-hoc tests (pairwise ?2 and Tukey)
to compare the NLG systems pairwise.
Overall, there is a top group consisting of
the Austin, Madrid, and Union systems: While
Madrid and Union outperform Austin on task suc-
cess (with 70 to 80% of successfully completed
games, depending on the world), Austin signifi-
cantly outperforms all other systems in terms of
task completion time. As expected, the Warm-
Cold system performs significantly worse than all
others in almost all categories. This confirms the
ability of the GIVE evaluation method to distin-
guish between systems of very different qualities.
4.3 Subjective measures
The subjective measures, which were obtained by
asking the users to fill in a questionnaire after each
game, are shown in Fig. 9. Most of the questions
were answered on 5-point Likert scales (?overall?
on a 7-point scale); the ?informativity? and ?tim-
ing? questions had nominal answers. For each
question, the user could choose not to answer.
The results of the subjective measurements are
summarized in Fig. 10, in the same format as
above. We ran ?2-tests for the nominal variables
informativity and timing, and ANOVAs for the
scale data. Again, we used post-hoc pairwise ?2-
and Tukey-tests to compare the NLG systems to
each other one by one.
Here there are fewer significant differences be-
tween different groups than for the objective mea-
sures: For the ?play again? category, there is
no significant difference at all. Nevertheless,
?Austin? is shown to be particularly good at navi-
gation instructions and timing, whereas ?Madrid?
outperforms the rest of the field in ?informativ-
7-point scale items:
overall: What is your overall evaluation of the quality of the
direction-giving system? (very bad 1 . . . 7 very good)
5-point scale items:
task difficulty: How easy or difficult was the task for you to
solve? (very difficult 1 2 3 4 5 very easy)
goal clarity: How easy was it to understand what you were
supposed to do? (very difficult 1 2 3 4 5 very easy)
play again: Would you want to play this game again? (no
way! 1 2 3 4 5 yes please!)
instruction clarity: How clear were the directions? (totally
unclear 1 2 3 4 5 very clear)
instruction helpfulness: How effective were the directions at
helping you complete the task? (not effective 1 2 3 4 5
very effective)
choice of words: How easy to understand was the system?s
choice of wording in its directions to you? (totally un-
clear 1 2 3 4 5 very clear)
referring expressions: How easy was it to pick out which ob-
ject in the world the system was referring to? (very hard
1 2 3 4 5 very easy)
navigation instructions: How easy was it to navigate to a par-
ticular spot, based on the system?s directions? (very
hard 1 2 3 4 5 very easy)
friendliness: How would you rate the friendliness of the sys-
tem? (very unfriendly 1 2 3 4 5 very friendly)
Nominal items:
informativity: Did you feel the amount of information you
were given was: too little / just right / too much
timing: Did the directions come ... too early / just at the right
time / too late
Figure 9: Questionnaire items
ity?. In the overall subjective evaluation, the ear-
lier top group of Austin, Madrid, and Union is
confirmed, although the difference between Union
and Twente is not significant. However, ?Warm-
Cold? again performs significantly worse than all
other systems in most measures. Furthermore, al-
though most systems perform similarly on ?infor-
mativity? and ?timing? in terms of the number of
users who judged them as ?just right?, there are
differences in the tendencies: Twente and Union
tend to be overinformative, whereas Austin and
Warm-Cold tend to be underinformative; Twente
and Union tend to give their instructions too late,
whereas Madrid and Warm-Cold tend to give them
too early.
170
A
us
ti
n
M
ad
ri
d
Tw
en
te
U
ni
on
W
ar
m
-C
ol
d
task
difficulty
4.3 4.3 4.0 4.3 3.5
A A A A
B
goal clarity
4.0 3.7 3.9 3.7 3.3
A A A A
B
play again
2.8 2.6 2.4 2.9 2.5
A A A A A
instruction
clarity
4.0 3.6 3.8 3.6 3.0
A A A
B B B
C
instruction
helpfulness
3.8 3.9 3.6 3.7 2.9
A A A A
B
informativity
46% 68% 51% 56% 51%
A
B B B B
overall
4.9 4.9 4.3 4.6 3.6
A A A
B B
C
choice of
words
4.2 3.8 4.1 3.7 3.5
A A
B B
C C C
referring
expressions
3.4 3.9 3.7 3.7 3.5
A A A
B B B B
navigation
instructions
4.6 4.0 4.0 3.7 3.2
A
B B B
C
timing
78% 62% 60% 62% 49%
A
B B B
C C
friendliness
3.4 3.8 3.1 3.6 3.1
A A A
B B B
Figure 10: Subjective measures by system. Infor-
mativity and timing are reported as the percentage
of successfully completed games. The other mea-
sures are reported as the mean rating received by
the players. Letters group indistinguishable sys-
tems; systems that don?t share a letter were found
to be significantly different with p < 0.05.
4.4 Further analysis
In addition to the differences between NLG sys-
tems, there may be other factors which also influ-
ence the outcome of our objective and subjective
measures. We tested the following five factors:
evaluation world, gender, age, computer expertise,
and English proficiency (as reported by the users
on the questionnaire). We found that there is a sig-
nificant difference in task success rate for different
evaluation worlds and between users with different
levels of English proficiency.
The interaction graphs in Figs. 11 and 12 also
suggest that the NLG systems differ in their ro-
bustness with respect to these factors. ?2-tests
that compare the success rate of each system in
the three evaluation worlds show that while the
instructions of Union and Madrid seem to work
equally well in all three worlds, the performance
of the other three systems differs dramatically be-
tween the different worlds. Especially World 2
was challenging for some systems as it required
relational object descriptions, such as the blue but-
ton on the left of another blue button.
The players? English skills also affected the sys-
tems in different ways. While Austin, Madrid and
Warm Cold don?t manage to lead players with only
basic English skills to success as often as other
players, Union?s and Twente?s success rates do not
depend on the players? English skills (?2-tests do
not find significant differences in success rate be-
tween players with different levels of English pro-
ficiency for these two systems). However, if we
remove the players with the lowest level of En-
glish proficiency, language skills do not have an
effect on the task success rate anymore for any of
the systems.
5 Conclusion
In this document, we have described the first in-
stallment of the GIVE Challenge, our experimen-
tal methodology, and the results. Altogether, we
collected 1143 valid games for five NLG systems
over a period of three months. Given that this was
the first time we organized the challenge, that it
was meant as a pilot experiment from the begin-
ning, and that the number of games was sufficient
to get significant differences between systems on
a number of measures, we feel that GIVE-1 was a
success. We are in the process of preparing sev-
eral diagnostic utilities, such as heat maps and a
tool that lets the system developer replay an indi-
171
Figure 11: Effect of the evaluation worlds on the
success rate of the NLG systems.
vidual game, which will help the participants gain
further insight into their NLG systems.
Nevertheless, there are a number of improve-
ments we will make to GIVE for future install-
ments. For one thing, the timing of the challenge
was not optimal: A number of colleagues would
have been interested in participating, but the call
for participation came too late for them to acquire
funding or interest students in time for summer
projects or MSc theses. Secondly, although the
software performed very well in handling thou-
sands of user connections, there were still game-
invalidating issues with the 3D graphics and the
networking code that were individually rare, but
probably cost us several hundred games. These
should be fixed for GIVE-2. At the same time,
we are investigating ways in which the networking
and matchmaking core of GIVE can be factored
out into a separate, challenge-independent system
on which other Internet-based challenges can be
built. Among other things, it would be straightfor-
ward to use the GIVE platform to connect two hu-
man users and observe their dialogue while solv-
ing a problem. Judicious variation of parameters
(such as the familiarity of users or the visibility of
an instruction giving avatar) would allow the con-
struction of new dialogue corpora along such lines.
Finally, GIVE-1 focused on the generation of
navigation instructions and referring expressions,
in a relatively simple world, without giving the
Figure 12: Effect of the players? English skills on
the success rate of the NLG systems.
user a chance to talk back. The high success rate
of some systems in this challenge suggests that
we need to widen the focus for a future GIVE-
2 ? by allowing dialogue, by making the world
more complex (e.g., allowing continuous rather
than discrete movements and turns), by making the
communication multi-modal, etc. Such extensions
would require only rather limited changes to the
GIVE software infrastructure. We plan to come to
a decision about such future directions for GIVE
soon, and are looking forward to many fruitful dis-
cussions about this at ENLG.
Acknowledgments. We are grateful to the par-
ticipants of the 2007 NSF/SIGGEN Workshop on
Shared Tasks and Evaluation in NLG and many
other colleagues for fruitful discussions while we
were designing the GIVE Challenge, and to the
organizers of Generation Challenges 2009 and
ENLG 2009 for their support and the opportunity
to present the results at ENLG. We also thank the
four participating research teams for their contri-
butions and their patience while we were working
out bugs in the GIVE software. The creation of
the GIVE infrastructure was supported in part by
a Small Projects grant from the University of Ed-
inburgh.
172
References
A. Belz and A. Gatt. 2008. Intrinsic vs. extrinsic eval-
uation measures for referring expression generation.
In Proceedings of ACL-08:HLT, Short Papers, pages
197?200, Columbus, Ohio.
M. E. Foster. 2008. Automated metrics that agree
with human judgements on generated output for an
embodied conversational agent. In Proceedings of
INLG 2008, pages 95?103, Salt Fork, OH.
A. Koller, D. Byron, J. Cassell, R. Dale, J. Moore,
J. Oberlander, and K. Striegnitz. 2009. The soft-
ware architecture for the first challenge on generat-
ing instructions in virtual environments. In Proceed-
ings of the EACL-09 Demo Session.
J. Orkin and D. Roy. 2007. The restaurant game:
Learning social behavior and language from thou-
sands of players online. Journal of Game Develop-
ment, 3(1):39?60.
A. Stent, M. Marge, and M. Singhai. 2005. Evaluating
evaluation methods for generation in the presence of
variation. In Proceedings of CICLing 2005.
L. Stoia, D. M. Shockley, D. K. Byron, and E. Fosler-
Lussier. 2006. Noun phrase generation for situated
dialogs. In Proceedings of INLG, Sydney.
L. von Ahn and L. Dabbish. 2004. Labeling images
with a computer game. In Proceedings of the ACM
CHI Conference.
173
Proceedings of the EACL 2009 Demonstrations Session, pages 33?36,
Athens, Greece, 3 April 2009. c?2009 Association for Computational Linguistics
The Software Architecture for the
First Challenge on Generating Instructions in Virtual Environments
Alexander Koller
Saarland University
koller@mmci.uni-saarland.de
Donna Byron
Northeastern University
dbyron@ccs.neu.edu
Justine Cassell
Northwestern University
justine@northwestern.edu
Robert Dale
Macquarie University
Robert.Dale@mq.edu.au
Johanna Moore
University of Edinburgh
J.Moore@ed.ac.uk
Jon Oberlander
University of Edinburgh
J.Oberlander@ed.ac.uk
Kristina Striegnitz
Union College
striegnk@union.edu
Abstract
The GIVE Challenge is a new Internet-
based evaluation effort for natural lan-
guage generation systems. In this paper,
we motivate and describe the software in-
frastructure that we developed to support
this challenge.
1 Introduction
Natural language generation (NLG) systems are
notoriously hard to evaluate. On the one hand,
simply comparing system outputs to a gold stan-
dard is not appropriate because there can be mul-
tiple generated outputs that are equally good, and
finding metrics that account for this variability and
produce results consistent with human judgments
and task performance measures is difficult (Belz
and Gatt, 2008; Stent et al, 2005; Foster, 2008).
On the other hand, lab-based evaluations with hu-
man subjects to assess each aspect of the system?s
functionality are expensive and time-consuming.
These characteristics make it hard to compare dif-
ferent systems and measure progress.
GIVE (?Generating Instructions in Virtual En-
vironments?) (Koller et al, 2007) is a research
challenge for the NLG community designed to
provide a new approach to NLG system evalua-
tion. In the GIVE scenario, users try to solve
a treasure hunt in a virtual 3D world that they
have not seen before. The computer has a com-
plete symbolic representation of the virtual envi-
ronment. The challenge for the NLG system is
to generate, in real time, natural-language instruc-
tions that will guide the users to the successful
completion of their task (see Fig. 1). One cru-
cial advantage of this generation task is that the
NLG system and the user can be physically sepa-
rated. This makes it possible to carry out a task-
based evaluation over the Internet ? an approach
that has been shown to provide generous amounts
Figure 1: The GIVE Challenge.
of data in earlier studies (von Ahn and Dabbish,
2004; Orkin and Roy, 2007).
In this paper, we describe the software archi-
tecture underlying the GIVE Challenge. The soft-
ware connects each player in a 3D game world
with an NLG system over the Internet. It is imple-
mented and open source, and can be a used online
during EACL at www.give-challenge.org.
In Section 2, we give an introduction to the GIVE
evaluation methodology by describing the experi-
ence of a user participating in the evaluation, the
nature of the data we collect, and our scientific
goals. Then we explain the software architecture
behind the scenes and sketch the API that concrete
NLG systems must implement in Section 3. In
Section 4, we present some preliminary evaluation
results, before we conclude in Section 5.
2 Evaluation method
Users participating in the GIVE evaluation
start the 3D game from our website at www.
give-challenge.org. They then see a 3D
game window as in Fig. 1, which displays instruc-
tions and allows them to move around in the world
and manipulate objects. The first room is a tuto-
rial room where users learn how to interact with
33
b2 b3b4 b5
b6
b7
b1
player
b8b9
b10
b11 b14b13b12
safe 
door
b1 opens doorto room 3
b9 moves picture to
b8: part of safe sequencereveal safe
? to win you have to retrieve the trophy from the safe in room 1? use button b9 to move the picture (and get access to the safe)
? if the alarm sounds, the game is over and you have lost
? press buttons b8, b6, b13, b13, b10 (in this order) to open the safe;if a button is pressed in the wrong order, the whole sequence is reset
b14 makes alarm soundb10, b13: part of safe sequence door to room 2b7 opens/closesstepping on this tiletriggers alarm
alarm
room 3
b2 turns off alarm tileb3 opens/closes door to room 2
b6: part of safe sequence
room 1
b5 makes alarm sound
room 2
door
door
lampcouch
chair
flower
pictu
retrophy
Figure 2: The map of a virtual world.
the system; they then enter one of three evaluation
worlds, where instructions for solving the treasure
hunt are generated by an NLG system.
The map of one of the game worlds is shown in
Fig. 2: In this world, players must pick up a trophy,
which is in a wall safe behind a picture. In order
to access the trophy, they must first push a button
to move the picture to the side, and then push an-
other sequence of buttons to open the safe. One
floor tile is alarmed, and players lose the game
if they step on this tile without deactivating the
alarm first. There are also a number of distrac-
tor buttons which either do nothing when pressed
or set off an alarm. These distractor buttons are in-
tended to make the game harder and, more impor-
tantly, to require appropriate reference to objects
in the game world. Finally, game worlds can con-
tain a number of objects such as chairs and flowers
which are irrelevant for the task, but can be used
as landmarks by a generation system.
Users are asked to fill out a before- and after-
game questionnaire that collects some demo-
graphic data and asks the user to rate various as-
pects of the instructions they received. Every ac-
tion that players take in a game world, and every
instruction that a generation system generates for
them, is recorded in a database. In addition to the
questionnaire data, we are thus able to compute a
number of objective measures such as:
? the percentage of users each system leads to
a successful completion of the task;
? the average time, the average number of in-
structions, and the average number of in-
game actions that this success requires;
? the percentage of generated referring expres-
sions that the user resolves correctly; and
? average reaction times to instructions.
It is important to note that we have designed
the GIVE Challenge not as a competition, but as
a friendly evaluation effort where people try to
learn from each other?s successes. This is reflected
in the evaluation measures above, which are in
tension with one another: For instance, a system
which gives very low-level instructions (?move
forward?; ?ok, now move forward?; ?ok, now turn
left?) will enjoy short reaction times, but it will re-
quire more instructions than a system that aggre-
gates these. To further emphasize this perspective,
we will also provide a number of diagnostic tools,
such as heat maps that show how much time users
spent on each tile, or a playback function which
displays an entire game run in real time.
In summary, the GIVE Challenge is a novel
evaluation effort for NLG systems. It is motivated
by real applications (such as pedestrian navigation
and the generation of task instructions), makes
no assumptions about the internal structure of an
NLG system, and emphasizes the situated genera-
tion of discourse in a simulated physical environ-
ment. The game world is scalable; it can be made
more complex and it can be adapted to focus on
specific issues in natural language generation.
3 Architecture
A crucial aspect of the GIVE evaluation methodol-
ogy is that it physically separates the user and the
NLG system and connects them over the Internet.
To achieve this, the GIVE software infrastructure
consists of three components:
1. the client, which displays the 3D world to
users and allows them to interact with it;
2. the NLG servers, which generate the natural-
language instructions; and
3. the Matchmaker, which establishes connec-
tions between clients and NLG servers.
These three components run on different ma-
chines. The client is downloaded by users from
our website and run on their local machine; each
NLG server is run on a server at the institution
that implemented it; and the Matchmaker runs on
a central server we provide.
34
Game Client
Matchmaker
NLG Server
NLG Server
NLG Server
Figure 3: The GIVE architecture.
When a user starts the client, it connects over
the Internet to the Matchmaker. The Matchmaker
then selects a game world and an NLG server at
random, and requests the NLG server to spawn
a new server instance. It then sends the game
world to the client and the server instance and dis-
connects from them, ready to handle new connec-
tions from other clients. The client and the server
instance play one game together: Whenever the
user does something, the client sends a message
about this to the server instance, and the server in-
stance can also send a message back to the client
at any time, which will then be displayed as an in-
struction. When the game ends, the client and the
server instance disconnect from each other. The
server instance sends a log of all game events to
the Matchmaker, and the client sends the ques-
tionnaire results to the Matchmaker; these then are
stored in the database for later analysis.
All of these components are implemented in
Java. This allows the client to be portable across
all major operating systems, and to be started di-
rectly from the website via Java Web Start without
the need for software installation. We felt it was
important to make startup of the client as effort-
less as possible, in order to maximize the num-
ber of users willing to play the game. Unsurpris-
ingly, we had to spend the majority of the pro-
gramming time on the 3D graphics (based on the
free jMonkeyEngine library) and the networking
code. We could have reduced the effort required
for these programming tasks by building upon an
existing virtual 3D world system such as Second
Life. However, we judged that the effort needed to
adapt such a system to our needs would have been
at least as high (in particular, we would have had
to ensure that the user could only move according
to the rules of the GIVE game and to instrument
the virtual world to obtain real-time updates about
events), and the result would have been less exten-
abstract class NlgSystem:
void connectionEstablished();
void connectionDisconnected();
void handleStatusInformation(Position playerPosition,
Orientation playerOrientation,
List?String? visibleObjects);
void handleAction(Atom actionInstance,
List?Formula? updates);
void handleDidNotUnderstand();
void handleMoveTurnAction(Direction direction);
. . .
Figure 4: The interface of an NLG system.
sible to future installments of the challenge.
Since we provided all the 3D, networking, and
database code, the research teams being evaluated
were able to concentrate on the development of
their NLG systems. Our only requirement was
that they implement a concrete subclass of the
class NlgSystem, shown in Fig. 4. This involves
overriding the six abstract callback methods in
this class with concrete implementations in
which the NLG system reacts to specific events.
The methods connectionEstablished
and connectionDisconnected are called
when users enter the game world and when
they disconnect from the game. The method
handleAction gets called whenever the user
performs some physical action, such as pushing a
button, and specifies what changed in the world
due to this action; handleMoveTurnAction
gets called whenever the user moves;
handleDidNotUnderstand gets called
whenever users press the H key to signal that
they didn?t understand the previous instruction;
and handleStatusInformation gets called
once per second and after each user action to
inform the server of the player?s position and
orientation and the visible objects. Ultimately,
each of these method calls gets triggered by a
message that the client sends over the network
in reaction to some event; but this is completely
hidden from the NLG system developer.
The NLG system can use the method send to
send a string to the client to be displayed. It also
has access to various methods querying the state of
the game world and to an interface to an external
planner which can compute a sequence of actions
leading to the goal.
4 First results
For this first installment of the GIVE Challenge,
four research teams from the US, the Netherlands,
35
and Spain provided generation systems, and a
number of other research groups expressed their
interest in participating, but weren?t able to partic-
ipate due to time constraints. Given that this was
the first time we organized this task, we find this
a very encouraging number. All four of the teams
consisted primarily of students who implemented
the NLG systems over the Northern-hemisphere
summer. This is in line with our goal of tak-
ing this first iteration as a ?dry run? in which we
could fine-tune the software, learn about the easy
and hard aspects of the challenge, and validate the
evaluation methodology.
Public involvement in the GIVE Challenge was
launched with a press release in early Novem-
ber 2008; the Matchmaker and the NLG servers
were then kept running until late January 2009.
During this time, online users played over 1100
games, which translates into roughly 75 game runs
for each experimental condition (i.e., five differ-
ent NLG systems paired with three different game
worlds). To our knowledge, this makes GIVE the
largest NLG evaluation effort yet in terms of ex-
perimental subjects.
While we have not yet carried out the detailed
evaluation, the preliminary results look promising:
a casual inspection shows that there are consider-
able differences in task success rate among the dif-
ferent systems.
While there is growing evidence from differ-
ent research areas that the results of Internet-based
evaluations are consistent with more traditional
lab-based experiments (e.g., (Keller et al, 2008;
Gosling et al, 2004)), the issue is not yet set-
tled. Therefore, we are currently conducting a lab-
based evaluation of the GIVE NLG systems, and
will compare those results to the qualitative and
quantitative data provided by the online subjects.
5 Conclusion
In this paper, we have sketched the GIVE Chal-
lenge and the software infrastructure we have de-
veloped for it. The GIVE Challenge is, to the
best of our knowledge, the largest-scale NLG eval-
uation effort with human experimental subjects.
This is made possible by connecting users and
NLG systems over the Internet; we collect eval-
uation data automatically and unobtrusively while
the user simply plays a 3D game. While we will
report on the results of the evaluation in more de-
tail at a later time, first results seem encouraging
in that the performance of different NLG systems
differs considerably.
In the future, we will extend the GIVE Chal-
lenge to harder tasks. Possibilities includ mak-
ing GIVE into a dialogue challenge by allowing
the user to speak as well as act in the world; run-
ning the challenge in a continuous world rather
than a world that only allows discrete movements;
or making it multimodal by allowing the NLG
system to generate arrows or virtual human ges-
tures. All these changes would only require lim-
ited changes to the GIVE software architecture.
However, the exact nature of future directions re-
mains to be discussed with the community.
References
A. Belz and A. Gatt. 2008. Intrinsic vs. extrinsic eval-
uation measures for referring expression generation.
In Proceedings of ACL-08:HLT, Short Papers, pages
197?200, Columbus, Ohio.
M. E. Foster. 2008. Automated metrics that agree
with human judgements on generated output for an
embodied conversational agent. In Proceedings of
INLG 2008, pages 95?103, Salt Fork, OH.
S. D. Gosling, S. Vazire, S. Srivastava, and O. P. John.
2004. Should we trust Web-based studies? A com-
parative analysis of six preconceptions about Inter-
net questionnaires. American Psychologist, 59:93?
104.
F. Keller, S. Gunasekharan, N. Mayo, and M. Corley.
2008. Timing accuracy of web experiments: A case
study using the WebExp software package. Behav-
ior Research Methods, to appear.
A. Koller, J. Moore, B. di Eugenio, J. Lester, L. Stoia,
D. Byron, J. Oberlander, and K. Striegnitz. 2007.
Shared task proposal: Instruction giving in virtual
worlds. In M. White and R. Dale, editors, Work-
ing group reports of the Workshop on Shared Tasks
and Comparative Evaluation in Natural Language
Generation. Available at http://www.ling.
ohio-state.edu/nlgeval07/report.html.
J. Orkin and D. Roy. 2007. The restaurant game:
Learning social behavior and language from thou-
sands of players online. Journal of Game Develop-
ment, 3(1):39?60.
A. Stent, M. Marge, and M. Singhai. 2005. Evaluating
evaluation methods for generation in the presence of
variation. In Proceedings of CICLing 2005.
L. von Ahn and L. Dabbish. 2004. Labeling images
with a computer game. In Proceedings of the ACM
CHI Conference.
36
Report on the Second NLG Challenge on
Generating Instructions in Virtual Environments (GIVE-2)
Alexander Koller
Saarland University
koller@mmci.uni-saarland.de
Kristina Striegnitz
Union College
striegnk@union.edu
Andrew Gargett
Saarland University
gargett@mmci.uni-saarland.de
Donna Byron
Northeastern University
dbyron@ccs.neu.edu
Justine Cassell
Northwestern University
justine@northwestern.edu
Robert Dale
Macquarie University
Robert.Dale@mq.edu.au
Johanna Moore
University of Edinburgh
J.Moore@ed.ac.uk
Jon Oberlander
University of Edinburgh
J.Oberlander@ed.ac.uk
Abstract
We describe the second installment of the
Challenge on Generating Instructions in
Virtual Environments (GIVE-2), a shared
task for the NLG community which took
place in 2009-10. We evaluated seven
NLG systems by connecting them to 1825
users over the Internet, and report the re-
sults of this evaluation in terms of objec-
tive and subjective measures.
1 Introduction
This paper reports on the methodology and results
of the Second Challenge on Generating Instruc-
tions in Virtual Environments (GIVE-2), which
we ran from August 2009 to May 2010. GIVE
is a shared task for the NLG community which
we ran for the first time in 2008-09 (Koller et al,
2010). An NLG system in this task must generate
instructions which guide a human user in solving
a treasure-hunt task in a virtual 3D world, in real
time. For the evaluation, we connect these NLG
systems to users over the Internet, which makes
it possible to collect large amounts of evaluation
data cheaply.
While the GIVE-1 challenge was a success, in
that it evaluated five NLG systems on data from
1143 game runs in the virtual environments, it
was limited in that users could only move and
turn in discrete steps in the virtual environments.
This made the NLG task easier than intended; one
of the best-performing GIVE-1 systems generated
instructions of the form ?move three steps for-
ward?. The primary change in GIVE-2 compared
to GIVE-1 is that users could now move and turn
freely, which makes expressions like ?three steps?
meaningless, and makes it hard to predict the pre-
cise effect of instructing a user to ?turn left?.
We evaluated seven NLG systems from six in-
stitutions in GIVE-2 over a period of three months
from February to May 2010. During this time,
we collected 1825 games that were played by
users from 39 countries, which is an increase of
over 50% over the data we collected in GIVE-
1. We evaluated each system both on objec-
tive measures (success rate, completion time, etc.)
and subjective measures which were collected by
asking the users to fill in a questionnaire. We
completely revised the questionnaire for the sec-
ond challenge, which now consists of relatively
fine-grained questions that can be combined into
more high-level groups for reporting. We also in-
troduced several new objective measures, includ-
ing the point in the game in which users lost
or cancelled, and an experimental ?back-to-base?
task intended to measure how much users learned
about the virtual world while interacting with the
NLG system.
Plan of the paper. The paper is structured as fol-
lows. In Section 2, we describe and motivate the
GIVE-2 Challenge. In section 3, we describe the
evaluation method and infrastructure. Section 4
reports on the evaluation results. Finally, we con-
clude and discuss future work in Section 5.
2 The GIVE Challenge
GIVE-2 is the second installment of the GIVE
Challenge (?Generating Instructions in Virtual En-
vironments?), which we ran for the first time in
2008-09. In the GIVE scenario, subjects try to
solve a treasure hunt in a virtual 3Dworld that they
have not seen before. The computer has a com-
plete symbolic representation of the virtual world.
The challenge for the NLG system is to gener-
ate, in real time, natural-language instructions that
will guide the users to the successful completion
of their task.
Users participating in the GIVE evaluation
start the 3D game from our website at www.
give-challenge.org. They then see a 3D
Figure 1: What the user sees when playing with
the GIVE Challenge.
game window as in Fig. 1, which displays instruc-
tions and allows them to move around in the world
and manipulate objects. The first room is a tuto-
rial room where users learn how to interact with
the system; they then enter one of three evaluation
worlds, where instructions for solving the treasure
hunt are generated by an NLG system. Users can
either finish a game successfully, lose it by trig-
gering an alarm, or cancel the game. This result is
stored in a database for later analysis, along with a
complete log of the game.
In each game world we used in GIVE-2, players
must pick up a trophy, which is in a wall safe be-
hind a picture. In order to access the trophy, they
must first push a button to move the picture to the
side, and then push another sequence of buttons to
open the safe. One floor tile is alarmed, and play-
ers lose the game if they step on this tile without
deactivating the alarm first. There are also a num-
ber of distractor buttons which either do nothing
when pressed or set off an alarm. These distractor
buttons are intended to make the game harder and,
more importantly, to require appropriate reference
to objects in the game world. Finally, game worlds
contained a number of objects such as chairs and
flowers that did not bear on the task, but were
available for use as landmarks in spatial descrip-
tions generated by the NLG systems.
The crucial difference between this task and
the (very similar) GIVE-1 task was that in GIVE-
2, players could move and turn freely in the vir-
tual world. This is in contrast to GIVE-1, where
players could only turn by 90 degree increments,
and jump forward and backward by discrete steps.
This feature of the way the game controls were set
up made it possible for some systems to do very
well in GIVE-1 with only minimal intelligence,
using exclusively instructions such as ?turn right?
and ?move three steps forward?. Such instructions
are unrealistic ? they could not be carried over to
instruction-giving in the real world ?, and our aim
was to make GIVE harder for systems that relied
on them.
3 Method
Following the approach from the GIVE-1 Chal-
lenge (Koller et al, 2010), we connected the NLG
systems to users over the Internet. In each game
run, one user and one NLG system were paired up,
with the system trying to guide the user to success
in a specific game world.
3.1 Software infrastructure
We adapted the GIVE-1 software to the GIVE-2
setting. The GIVE software infrastructure (Koller
et al, 2009a) consists of three different mod-
ules: The client, which is the program which the
user runs on their machine to interact with the
virtual world (see Fig. 1); a collection of NLG
servers, which generate instructions in real-time
and send them to the client; and a matchmaker,
which chooses a random NLG server and virtual
world for each incoming connection from a client
and stores the game results in a database.
The most visible change compared to GIVE-1
was to modify the client so it permitted free move-
ment in the virtual world. This change further ne-
cessitated a number of modifications to the inter-
nal representation of the world. To support the de-
velopment of virtual worlds for GIVE, we changed
the file format for world descriptions to be much
more readable, and provided an automatic tool
for displaying virtual worlds graphically (see the
screenshots in Fig. 2).
3.2 Recruiting subjects
Participants were recruited using email distribu-
tion lists and press releases posted on the Internet
and in traditional newspapers. We further adver-
tised GIVE at the Cebit computer expo as part of
the Saarland University booth. Recruiting anony-
mous experimental subjects over the Internet car-
ries known risks (Gosling et al, 2004), but we
showed in GIVE-1 that the results obtained for
the GIVE Challenge are comparable and more in-
formative than those obtained from a laboratory-
World 1 World 2 World 3
Figure 2: The three GIVE-2 evaluation worlds.
based experiment (Koller et al, 2009b).
We also tried to leverage social networks for re-
cruiting participants by implementing and adver-
tising a Facebook application. Because of a soft-
ware bug, only about 50 participants could be re-
cruited in this way. Thus tapping the true poten-
tial of social networks for recruiting participants
remains a task for the next installment of GIVE.
3.3 Evaluation worlds
Fig. 2 shows the three virtual worlds we used in the
GIVE-2 evaluation. Overall, the worlds were more
difficult than the worlds used in GIVE-1, where
some NLG-systems had success rates around 80%
in some of the worlds. As for GIVE-1, the three
worlds were designed to pose different challenges
to the NLG systems. World 1 was intended to be
more similar to the development world and last
year?s worlds. It did have rooms with more than
one button of the same color, however, these but-
tons were not located close together. World 2 con-
tained several situations which required more so-
phisticated referring expressions, such as rooms
with several buttons of the same color (some of
them close together) and a grid of buttons. Fi-
nally, World 3 was designed to exercise the sys-
tems? navigation instructions: one room contained
a ?maze? of alarm tiles, and another room two
long rows of buttons hidden in ?booths? so that
they were not all visible at the same time.
3.4 Timeline
After the GIVE-2 Challenge was publicized in
June 2009, fifteen researchers and research teams
declared their interest in participating. We dis-
tributed a first version of the software to these
teams in August 2009. In the end, six teams sub-
mitted NLG systems (two more than in GIVE-1);
one team submitted two independent NLG sys-
tems, bringing the total number of NLG systems
up to seven (two more than in GIVE-1). These
were connected to a central matchmaker that ran
for a bit under three months, from 23 February to
17 May 2010.
3.5 NLG systems
Seven NLG systems were evaluated in GIVE-2:
? one system from the Dublin Institute of Tech-
nology (?D? in the discussion below);
? one system from Trinity College Dublin
(?T?);
? one system from the Universidad Com-
plutense de Madrid (?M?);
? one system from the University of Heidelberg
(?H?);
? one system from Saarland University (?S?);
? and two systems from INRIA Grand-Est in
Nancy (?NA? and ?NM?).
Detailed descriptions of these systems as well
as each team?s own analysis of the evalua-
tion results can be found at http://www.
give-challenge.org/research.
4 Results
We now report the results of GIVE-2. We start
with some basic demographics; then we discuss
objective and subjective evaluation measures. The
data for the objective measures are extracted from
the logs of the interactions; whereas the data for
the subjective measures are obtained from a ques-
tionnaire which asked subjects to rate various as-
pects of the NLG system they interacted with.
Notice that some of our evaluation measures are
in tension with each other: For instance, a sys-
tem which gives very low-level instructions may
allow the user to complete the task more quickly
(there is less chance of user errors), but it will re-
quire more instructions than a system that aggre-
gates these. This is intentional, and emphasizes
our desire to make GIVE a friendly comparative
challenge rather than a competition with a clear
winner.
4.1 Demographics
Over the course of three months, we collected
1825 valid games. This is an increase of almost
60% over the number of valid games we collected
in GIVE-1. A game counted as valid if the game
client did not crash, the game was not marked as a
test game by the developers, and the player com-
pleted the tutorial.
Of these games, 79.0% were played by males
and 9.6% by females; a further 11.4% did not
specify their gender. These numbers are compa-
rable to GIVE-1. About 42% of users connected
from an IP address in Germany; 12% from the US,
8% from France, 6% from Great Britain, and the
rest from 35 further countries. About 91% of the
participants who answered the question self-rated
their English language proficiency as ?good? or
better. About 65% of users connected from vari-
ous versions of Windows, the rest were split about
evenly between Linux and MacOS.
4.2 Objective measures
The objective measures are summarize in Fig. 3.
In addition to calculating the percentage of games
users completed successfully when being guided
by the different systems, we measured the time
until task completion, the distance traveled until
task completion, and the number of actions (such
as pushing a button to open a door) executed. Fur-
thermore, we counted howmany instructions users
received from each system, and how many words
these instructions contained on average. All objec-
tive measures were collected completely unobtru-
sively, without requiring any action on the user?s
part. To ensure comparability, we only counted
successfully completed games.
task success: Did the player get the trophy?
duration: Time in seconds from the end of the tu-
torial until the retrieval of the trophy.
distance: Distance traveled (measured in distance
units of the virtual environment).
actions: Number of object manipulation actions.
instructions: Number of instructions produced
by the NLG system.
words per instruction: Average number of
words the NLG system used per instruction.
Figure 3: Objective measures.
Fig. 4 shows the results of these objective mea-
sures. Task success is reported as the percent-
age of successfully completed games. The other
measures are reported as the mean number of sec-
onds/distance units/actions/instructions/words per
instruction, respectively. The figure also assigns
systems to groups A, B, etc. for each evaluation
measure. For example, users interacting with sys-
tems in group A had a higher task success rate,
needed less time, etc. than users interacting with
systems in group B. If two systems do not share
the same letter, the difference between these two
systems is significant with p < 0.05. Significance
was tested using a ?2-test for task success and
ANOVAs for the other objective measures. These
were followed by post-hoc tests (pairwise ?2 and
Tukey) to compare the NLG systems pairwise.
In terms of task success, the systems fall pretty
neatly into four groups. Note that systems D and
T had very low task success rates. That means
that, for these systems, the results for the other ob-
jective measures may not be reliable because they
are based on just a handful of games. Another
aspect in which systems clearly differed is how
many words they used per instruction. Interest-
ingly, the three systems with the best task success
rates also produced the most succinct instructions.
The distinctions between systems in terms of the
other measures is less clear.
4.3 Subjective measures
The subjective measures were obtained from re-
sponses to a questionnaire that was presented to
users after each game. The questionnaire asked
users to rate different statements about the NLG
D H M NA NM S T
task
success
9% 11% 13% 47% 30% 40% 3%
A A
B
C C C
D D
duration
888 470 407 344 435 467 266
A A A A A
B B B B B
C
distance
231 164 126 162 167 150 89
A A A A A A
B B B B B
actions
25 22 17 17 18 17 14
A A A A A A A
instructions
349 209 463 224 244 244 78
A A A A A A
B B
words per
instruction
15 11 16 6 10 6 18
A A
B
C
D
E E
Figure 4: Results for the objective measures.
system using a continuous slider. The slider posi-
tion was translated to a number between -100 and
100. Figs. 7 and 6 show the statements that users
were asked to rate as well as the results. These
results are based on all games, independent of the
success. We report the mean rating for each item,
and, as before, systems that do not share a letter,
were found to be significantly different (p< 0.05).
We used ANOVAs and post-hoc Tukey tests to test
for significance. Note that some items make a pos-
itive statement about the NLG system (e.g., Q1)
and some make a negative statement (e.g., Q2).
For negative statements, we report the reversed
scores, so that in Figs. 7 and 6 greater numbers are
always better, and systems in group A are always
better than systems in group B.
In addition to the items Q1?Q22, the ques-
tionnaire contained a statement about the over-
all instruction quality: ?Overall, the system gave
me good directions.? Furthermore notice that the
other items fall into two categories: items that as-
sess the quality of the instructions (Q1?Q15) and
items that assess the emotional affect of the in-
teraction (Q16?Q22). The ratings in these cate-
D H M NA NM S T
overall
quality
question
-33 -18 -12 36 18 19 -25
A
B B
C C C C
quality
measures
(summed)
-183 -148 -18 373 239 206 -44
A A A
B B B B
emotional
affect
measures
(summed)
-130 -103 -90 20 -5 0 -88
A A A A
B B B B B
C C C C C
Figure 5: Results for item assessing overall in-
struction quality and the aggregated quality and
emotional affect measures.
gories can be aggregated into just two ratings by
summing over them. Fig. 5 shows the results for
the overall question and the aggregated ratings for
quality measures and emotional affect measures.
The three systems with the highest task success
rate get rated highest for overall instruction qual-
ity. The aggregated quality measure also singles
out the same group of three systems.
4.4 Further analysis
In addition to the differences between NLG sys-
tems, some other factors also influence the out-
comes of our objective and subjective measures.
As in GIVE-1, we find that there is a significant
difference in task success rate for different evalua-
tion worlds and between users with different levels
of English proficiency. Fig. 8 illustrates the effect
of the different evaluation worlds on the task suc-
cess rate for different systems, and Fig. 9 shows
the effect that a player?s English skills have on the
task success rate. As in GIVE-1, some systems
seem to be more robust than others with respect to
changes in these factors.
None of the other factors we looked at (gender,
age, and computer expertise) have a significant ef-
fect on the task success rate. With a few excep-
tions the other objective measures were not influ-
enced by these demographic factors either. How-
ever, we do find a significant effect of age on the
time and number of actions a player needs to re-
trieve the trophy: younger players are faster and
need fewer actions. And we find that women travel
a significantly shorter distance than men on their
way to the trophy. Interestingly, we do not find
D H M NA NM S T
Q1: The system used words and phrases
that were easy to understand.
45 26 41 62 54 58 46
A A A A
B B B B
C C C
Q2: I had to re-read instructions to under-
stand what I needed to do.
-26 -9 3 40 8 19 0
A
B B B B
C C C
D D
Q3: The system gave me useful feedback
about my progress.
-17 -30 -31 9 11 -13 -27
A A
B B B B
C C C C
Q4: I was confused about what to do next.
-35 -27 -18 29 9 5 -31
A
B B
C C C C
Q5: I was confused about which direction
to go in.
-32 -20 -16 21 8 3 -25
A A
B B
C C C C
Q6: I had no difficulty with identifying
the objects the system described for me.
-21 -11 -5 18 13 20 -21
A A A
B B
C C C C
Q7: The system gave me a lot of unnec-
essary information.
-22 -9 6 15 10 10 -6
A A A A
B B B B
C C C
D D D
D H M NA NM S T
Q8: The system gave me too much infor-
mation all at once.
-28 -8 9 31 8 21 15
A A A
B B B B
C C
Q9: The system immediately offered help
when I was in trouble.
-15 -13 -13 32 3 -5 -23
A
B B B B B
C C C C
Q10: The system sent instructions too
late.
15 15 9 38 39 14 8
A A
B B B B B
Q11: The system?s instructions were de-
livered too early.
15 5 21 39 12 30 28
A A A
B B B B
C C C C
D D D D
Q12: The system?s instructions were vis-
ible long enough for me to read them.
-67 -21 -19 6 -14 0 -18
A A
B B B
C C C C
D
Q13: The system?s instructions were
clearly worded.
-20 -9 1 32 23 26 6
A A A
B B B
C C C
D D
Q14: The system?s instructions sounded
robotic.
16 -6 8 -4 -1 5 1
A A A A A A
B B B B B B
Q15: The system?s instructions were
repetitive.
-28 -26 -11 -31 -28 -26 -23
A A A A A
B B B B B B
Figure 7: Results for the subjective measures assessing the quality of the instructions.
D H M NA NM S T
Q16: I really wanted to find that trophy.
-10 -13 -9 -11 -8 -7 -12
A A A A A A A
Q17: I lost track of time while solving the
overall task.
-13 -18 -21 -16 -18 -11 -20
A A A A A A A
Q18: I enjoyed solving the overall task.
-21 -23 -20 -8 -4 -5 -21
A A A A A A
B B B B B
Q19: Interacting with the system was re-
ally annoying.
-14 -20 -12 8 -2 -2 -14
A A A
B B B B B
C C C C
Q20: I would recommend this game to a
friend.
-36 -39 -31 -30 -25 -24 -31
A A A A A A A
Q21: The system was very friendly.
0 -1 5 30 20 19 5
A A A
B B B B
C C C C
D D D D
Q22: I felt I could trust the system?s in-
structions.
-21 -6 -3 37 23 21 -13
A A A
B B B B
Figure 6: Results for the subjective measures as-
sessing the emotional affect of the instructions.
Figure 8: Effect of the evaluation worlds on the
success rate of the NLG systems.
Figure 9: Effect of the players? English skills on
the success rate of the NLG systems.
a significant effect of gender on the time players
need to retrieve the trophy as in GIVE-1 (although
the mean duration is somewhat higher for female
than for male players; 481 vs. 438 seconds).
5 Conclusion
In this paper, we have described the setup and re-
sults of the Second GIVE Challenge. Altogether,
we collected 1825 valid games for seven NLG sys-
tems over a period of three months. Given that this
is a 50% increase over GIVE-1, we feel that this
further justifies our basic experimental methodol-
ogy. As we are writing this, we are preparing de-
tailed results and analyses for each participating
team, which we hope will help them understand
and improve the performance of their systems.
The success rate is substantially worse in GIVE-
2 than in GIVE-1. This is probably due to the
Figure 10: Points at which players lose/cancel.
harder task (free movement) explained in Sec-
tion 2 and to the more complex evaluation worlds
(see Section 3.3). It was our intention to make
GIVE-2 more difficult, although we did not antic-
ipate such a dramatic drop in performance. GIVE-
2.5 next year will use the same task as GIVE-2 and
we hope to see an increase in task success as the
participating research teams learn from this year?s
results.
It is also noticeable that players gave mostly
negative ratings in response to statements about
immersion and engagement (Q16-Q20). We dis-
cussed last year how to make the task more engag-
ing on the one hand and how to manage expecta-
tions on the other hand, but none of the suggested
solutions ended up being implemented. It seems
that we need to revisit this issue.
Another indication that the task may not be able
to capture participants is that the vast majority of
cancelled and lost games end in the very begin-
ning. To analyze at what point players lose or give
up, we divide the game into phases demarcated
by manipulations of buttons that belong to the 6-
button safe sequence. Fig. 10 illustrates in which
phase of the game players lose or cancel.
We are currently preparing the GIVE-2.5 Chal-
lenge, which will take place in 2010-11. GIVE-2.5
will be very similar to GIVE-2, so that GIVE-2
systems will be able to participate with only mi-
nor changes. In order to support the development
of GIVE-2.5 systems, we have collected a multi-
lingual corpus of written English and German in-
structions in the GIVE-2 environment (Gargett et
al., 2010). We expect that GIVE-3 will then extend
the GIVE task substantially, perhaps in the direc-
tion of full dialogue or of multimodal interaction.
Acknowledgments. GIVE-2 was only possible
through the support and hard work of a number of
colleagues, especially Konstantina Garoufi (who
handled the website and other publicity-related is-
sues), Ielka van der Sluis (who contributed to the
design of the GIVE-2 questionnaire), and several
student assistants who programmed parts of the
GIVE-2 system. We thank the press offices of
Saarland University, the University of Edinburgh,
and Macquarie University for their helpful press
releases. We also thank the organizers of Gener-
ation Challenges 2010 and INLG 2010 for their
support and the opportunity to present our results,
and the seven participating research teams for their
contributions.
References
Andrew Gargett, Konstantina Garoufi, Alexander
Koller, and Kristina Striegnitz. 2010. The GIVE-
2 corpus of giving instructions in virtual environ-
ments. In Proceedings of the 7th International
Conference on Language Resources and Evaluation
(LREC), Malta.
S. D. Gosling, S. Vazire, S. Srivastava, and O. P. John.
2004. Should we trust Web-based studies? A com-
parative analysis of six preconceptions about Inter-
net questionnaires. American Psychologist, 59:93?
104.
A. Koller, D. Byron, J. Cassell, R. Dale, J. Moore,
J. Oberlander, and K. Striegnitz. 2009a. The soft-
ware architecture for the first challenge on generat-
ing instructions in virtual environments. In Proceed-
ings of the EACL-09 Demo Session.
Alexander Koller, Kristina Striegnitz, Donna Byron,
Justine Cassell, Robert Dale, Sara Dalzel-Job, Jo-
hanna Moore, and Jon Oberlander. 2009b. Validat-
ing the web-based evaluation of NLG systems. In
Proceedings of ACL-IJCNLP 2009 (Short Papers),
Singapore.
Alexander Koller, Kristina Striegnitz, Donna Byron,
Justine Cassell, Robert Dale, Johanna Moore, and
Jon Oberlander. 2010. The first challenge on
generating instructions in virtual environments. In
E. Krahmer and M. Theune, editors, Empirical
Methods in Natural Language Generation, volume
5790 of LNCS, pages 337?361. Springer.
Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL), pages 20?29,
Seoul, South Korea, 5-6 July 2012. c?2012 Association for Computational Linguistics
?Love ya, jerkface?: using Sparse Log-Linear Models to Build
Positive (and Impolite) Relationships with Teens
William Yang Wang, Samantha Finkelstein, Amy Ogan, Alan W Black, Justine Cassell
School of Computer Science, Carnegie Mellon University
{yww, slfink, aeo, awb, justine}@cs.cmu.edu
Abstract
One challenge of implementing spoken di-
alogue systems for long-term interaction is
how to adapt the dialogue as user and sys-
tem become more familiar. We believe this
challenge includes evoking and signaling as-
pects of long-term relationships such as rap-
port. For tutoring systems, this may addi-
tionally require knowing how relationships are
signaled among non-adult users. We therefore
investigate conversational strategies used by
teenagers in peer tutoring dialogues, and how
these strategies function differently among
friends or strangers. In particular, we use an-
notated and automatically extracted linguis-
tic devices to predict impoliteness and posi-
tivity in the next turn. To take into account
the sparse nature of these features in real data
we use models including Lasso, ridge estima-
tor, and elastic net. We evaluate the predictive
power of our models under various settings,
and compare our sparse models with stan-
dard non-sparse solutions. Our experiments
demonstrate that our models are more ac-
curate than non-sparse models quantitatively,
and that teens use unexpected kinds of lan-
guage to do relationship work such as signal-
ing rapport, but friends and strangers, tutors
and tutees, carry out this work in quite differ-
ent ways from one another.
1 Introduction and Related Work
Rapport, the harmonious synchrony between in-
terlocutors, has numerous benefits for a range of
dialogue types, including direction giving (Cas-
sell et al, 2007) or contributing to patient recov-
ery (Vowles and Thompson, 2012). In peer tutor-
ing, an educational paradigm in which students of
similar ability tutor one another, friendship among
tutors and tutees leads to better learning (Gartner et
al., 1971). With the burgeoning use of spoken dia-
logue systems in education, understanding the pro-
cess by which two humans build and signal rapport
during learning becomes a vital step for implement-
ing spoken dialogue systems (SDSs) that can initi-
ate (and, as importantly, maintain) a successful re-
lationship with students over time. However, im-
plementing a tutorial dialogue system that appropri-
ately challenges students in the way that peers do
so well (Sharpley et al, 1983), while still demon-
strating the rapport that peers can also provide, calls
for understanding the differences in communication
between peer tutors just meeting and those who are
already friends.
The Tickle-Degnen and Rosenthal (1990) model
provides a starting point by outlining the compo-
nents of rapport, including the finding that positiv-
ity decreases over the course of a relationship. The
popularity of this model, however, has not dimin-
ished the disproportionate attention that positivity
and politeness receive in analyses of rapport (Brown
and Levinson, 1978), including in the vast majority
of computational approaches to rapport-building in
dialogue (Stronks et al, 2002; Johnson and Rizzo,
2004; Bickmore and Picard, 2005; Gratch et al,
2006; McLaren et al, 2007; Cassell et al, 2007;
Baker et al, 2008; Bickmore et al, 2011). The
creation and expression of rapport is complex, and
can also be signaled through negative, or impolite,
exchanges (Straehle, 1993; Watts, 2003; Spencer-
Oatey, 2008) that communicate affection and re-
lationship security among intimates who can flout
common social norms (Culpeper, 2011; Kienpoint-
ner, 1997).
However, it is an open question as to whether such
rudeness is likely to impress a new student on the
first day of class. We must better understand how
and when impoliteness and other negative dialogue
moves can contribute to the development and ex-
pression of the rapport that is so important in educa-
tional relationships. In this analysis, then, we begin
with a corpus of tutoring chat data annotated with
a set of affectively-charged linguistic devices (e.g.
complaining, emoticons), and then differentiate be-
tween the linguistic devices that friend and stranger
interlocutors employ (with friendship standing as a
proxy for pre-existent rapport) and the resulting so-
cial effects or functions of those devices on the part-
ners.
Since our ultimate goal is to build an SDS that
can adapt to the user?s language in real time, we
also automatically extract lexical and syntactic fea-
tures from the conversations. And, in order to deter-
mine what the system should say to evoke particular
20
responses, we predict social effects in partner two
from the use of the linguistic devices in partner one.
Since we want to understand how the system can
deal with newly met peers as well as peers who
have become friends, we develop and evaluate our
model on dyads of friends and then evaluate the
same model with dyads of strangers, to examine
whether dyads with less a priori rapport react dif-
ferently to the same linguistic devices.
Of course, in addition to understanding the phe-
nomenon of rapport in all of its complexity, a major
challenge for building rapport-signaling SDS is to
construct a compact feature space that capture only
reliable rapport signals and generalizes well across
different speakers. Of course phenomena such as in-
sults, complaints and pet names, no matter how im-
portant, appear relatively rarely in data of this sort.
Training discriminative models with maximum like-
lihood estimators (MLE) on such datasets usually re-
sults in assigning too much weight on less frequent
signals. This standard MLE training method not
only produces dense models, but may also overes-
timates lower frequency features that might be unre-
liable signals and overfit to a particular set of speak-
ers. In recent studies on speaker state prediction that
use lexical features, it has been shown that MLE
estimators demonstrate large performance gaps be-
tween non-overlapping speaker datasets (Jeon et al,
2010; Wang et al, 2012a).
On the other hand, recent studies on `1/`2
based group penalty for evaluating dialogue systems
(Gonza?lez-Brenes and Mostow, 2011), structured
sparsity for linguistic structure prediction (Mar-
tins et al, 2011), and discovering historical legal
opinions with a sparse mixed-effects latent vari-
able model (Wang et al, 2012b) have all shown
concrete benefits of modeling sparsity in language-
related predictive tasks. We therefore apply sparsity-
sensitive models that can prevent less frequent
features from overfitting. We start with the `1-
regularized Lasso (Tibshirani, 1994) model, since,
compared to other covariance matrix based sparse
models, such as sparse Principal Component Anal-
ysis (PCA) and sparse Canonical Correlation Anal-
ysis (CCA), the Lasso model is straightforward and
requires fewer computing resources when the fea-
ture dimension is high. Hence, we compare the con-
tributions of both automated features and annotated
features using the proposed Lasso model to predict
impoliteness and positivity.
In addition to Lasso and a logistic regression base-
line, we introduce two alternative penalty models:
the non-sparse ridge (le Cessie and van Houwelin-
gen, 1992) estimator, and an elastic net model (Zou
and Hastie, 2005). The ridge estimator applies a
quadratic penalty for feature selection, resulting in
a smooth objective function and a non-sparse fea-
ture space, which can be seen as a strong non-sparse
penalty model. We investigate the elastic net model,
because it balances the pros and cons of Lasso and
ridge estimators, and enforces composite penalty. In
addition to the model comparisons, by varying the
different sizes of feature windows (number of turns
in the dialogue history), we empirically show that
our proposed sparse log-linear model is flexible, en-
abling the model to capture long-range dependency.
This approach also allows us to extend previous
work on speaker state prediction. Although speaker
state prediction has attracted much attention in the
dialogue research community, most studies have fo-
cused on the analysis of anger, frustration, and other
classic emotions (Litman and Forbes-Riley, 2004;
Liscombe et al, 2005; Devillers and Vidrascu, 2006;
Ai et al, 2006; Grimm et al, 2007; Gupta and Ni-
tendra., 2007; Metallinou et al, 2011). Recently,
Wang and Hirschberg (2011) proposed a hierarchi-
cal model that detects level of interest of speakers
in dialogue, using a multistream prediction feedback
technique. However, to the best of our knowledge,
we are among the first to study the problem of auto-
matic impoliteness and positivity prediction in dia-
logue. Because our ultimate goal is to build an SDS
that responds to users? language use over time, the
features from the user?s target turn that the model is
aiming to predict are not observable, which renders
the task more difficult than previous speaker state
detection tasks.
Our main contributions are three-fold: (1) analy-
sis of linguistic devices that function to signal rap-
port among friends - and their effects on non-friend
dyads; (2) detailed analyses of language behavior
features that predict these rapport behaviors - both
impoliteness and positivity - in the next turn of
teenagers? peer tutoring sessions; (3) an evaluation
of non-sparse and sparse log-linear models for pre-
dicting impoliteness and positivity.
By understanding the signals of rapport that a per-
son is likely to display in response to various lin-
guistic devices, we can begin to build an SDS that
can anticipate the social response and adapt to the
rapport-signaling efforts of its partner, both as a
newly introduced technology, and, over time, as a
system with whom the user has a rapport.
2 The Corpus
We use the data from a previous study evaluating the
impact of a peer tutoring intervention that monitored
students? collaboration and in some cases provided
adaptive support (Walker et al, 2011). In the inter-
vention, peer tutors observed the work of their tutee
21
and supported them through a chat interface as they
completed algebra problems. The system logged all
chat and other information about the problem steps.
Participants were 130 high school students (81 fe-
male) in grades 7-12 from one American high school
with some prior knowledge of the algebra material.
Participants were asked to sign up for the study with
a friend. Those who were interested but were un-
able to participate with a friend, were matched with
another unmatched participant. In an after-school
session, participants first took a 20-minute pre-test
on the math concepts, and then spent 20 minutes
working alone with the computer to prepare for tu-
toring. One student in each dyad was then randomly
assigned the role of tutor, while the other was given
the role of tutee, regardless of relative ability. They
spent the next 60 minutes engaging in tutoring. Fi-
nally, students were given a domain posttest isomor-
phic to the pretest.
54 dyads signed up as friends and 6 were un-
matched strangers. To compare behavior between
friends and strangers in the face of very different
data set sizes we use 48 friend dyads for training,
and select 6 friend and 6 stranger dyads as two sep-
arate test sets. The total number of utterances in the
friend training set, friend test set, and stranger test
set are 4538, 468 and 402. To perform turn-based
prediction experiments, we concatenate the text in
the utterances by the same speaker into a single turn,
and perform an ?OR? operation1 on features (See
Section 3 for details) in multiple utterances of the
same speaker to generate the turn-based binary fea-
tures.
3 Feature Engineering
In this section, we describe both the annotated and
automatically extracted features analyzed.
3.1 Annotated Features and Labels2
To understand what linguistic devices participated in
positivity and impoliteness during tutoring, we an-
notated all 60 dyads for surface-level language be-
haviors such as complaints, challenges (Culpeper,
1996) and praise. We also automatically identi-
fied chat features that socially color the communi-
cation, such as excessive punctuation[P] or capital-
ization[Ca]. Utterances could receive more than one
code, and inter-rater reliability ranged from K=.71
to K=1.
Because these linguistic behaviors may serve a
range of different functions in context, such as rude
1If any of the utterances within one turn has this feature
turned on, then we say that we have observed this feature in
this turn.
2We thank Erin Walker for data collection and annotation.
language serving to cement a relationship (Arding-
ton, 2006), or teasing to increase rapport (Straehle,
1993), we also annotate the social functionality
of each utterance in context, in terms of positivity
(K=.79)3 and impoliteness (K=.76), which are seen
as holding down opposite kinds of social functional-
ity (Terkourafi, 2008). Details of annotation can be
found in our recent work (Ogan et al, 2012).
Language Behavior Features
Language behavior features were annotated by
two raters, based on previous work on impo-
liteness (Culpeper, 1996), positivity (Boyer et
al., 2008), and computer-mediated communica-
tion (Herring and Zelenkauskaite, 2009), as fol-
lows:.
? Insults[Di] (?=1): Personalized negative voca-
tives or references. eg. ?you are so weird.?
? Challenges[Ch] (? =.91): Directly questioning
partner?s decision or ability. eg. Partner 1:
?see I am helping?, Partner 2: ?barely.?
? Condescensions / brags[C] (?=1): Asserting
authority or partner?s inferiority. eg. Tutee:
?nothing you have done has affected me what
so ever.?
? Message enforcer[Ef] (?=.85): Emphasizing
text or attracting partner?s attention. eg. ?Earth
to Erin.?
? Dismissal / Silencer / Curse[Cu] (? =.76): As-
serting unimportance of contribution/partner.
eg. ?shuttttt up computer.?
? Pet name[Pe] (? = .9): Vocatives that may or
may not be insulting. eg. ?whats up homie??
? Criticisms / exclusive complaints[EC] (?=.8):
Negative evaluation of partner. eg. ?You are so
bad at this dude.?
? Inclusive complaints[I] (?=.78): Complaints
directed outside the partner, such as at the task,
computers, or study. eg. ?This is really dumb,
ya think??
? Laughter[L] (?=1): eg. ?haha?, ?lol?
? Off-task[O] (?=.71): Doesn?t pertain to or ad-
vance tutorial dialogue. eg. ?Coming over after
this??
Impoliteness and Positivity Labels
While the surface-level features were coded based
on a single utterance, context determined the labels
for impoliteness and positivity, including the recent
tone of the dialogue and the partner?s response to
the utterance. Utterances were coded as positivity
(?=.79) when they included goals that directly added
positive affect into the exchange through praise, em-
pathy, reassurance, cooperative talk (McLaren et al,
3We use Cohen?s kappa in this study.
22
2011), task enthusiasm, and making or responding
to jokes. Impoliteness (?=.76) included both coop-
eratively rude utterances such as teasing (typical eg.
?hahah you?re the worst tutor ever?) and uncooper-
atively rude utterances that may cause offense (typ-
ical eg. ?um why don?t you try actually explainin
urself..?) (Kienpointner, 1997).
3.2 Automated Features
To compare the performance between what could be
automatically extracted from dialogue and hand an-
notation, we extracted 2,872 unigram and 12,016 bi-
gram features from the text corpus. Using the Stan-
ford PoS tagger4 with its attached model, we also
extracted 46 common part-of-speech tags from the
text. In addition to the above lexical and syntac-
tic features, we automatically extracted the capital-
ization features[Ca] that have at least one full word
(eg. ?CALM DOWN?) (Chovanec, 2009). Since
a recent text prediction task (Wang and McKeown,
2010) observed benefits from modeling punctua-
tion features[P], we extracted the expressive punc-
tuation that included at least one exclamation point
or more than one question-mark (eg. ?I don?t get
it?!??!?) (Crystal, 2001). We used a smiley dictio-
nary5 to extract the emoticons[E] that convey emo-
tional states (Sa?nchez et al, 2006) from text.
4 Sparse Log-Linear Models
We formulate our impoliteness and positivity predic-
tion problems as binary classifications. To do this,
we estimate the label y?t ? Bernoulli(??). First, we
introduce a standard log-linear parametrization6 to
our predictive tasks:
??~yt =
exp
?
i ~wi ~fi(~yt)
1 + exp
?
i ~wi ~fi(~yt)
, (1)
where ~f(~yt) is a set of feature functions computed
on the observation vector ~yt. The term ~wi puts a
weight on feature i for predicting impoliteness, and
our estimation problem is now to set these weights.
The log-likelihood and the gradient are:
` =
?
t
yt log ??~yt + (1? yt) log(1? ??~yt) (2)
?`
? ~w =
?
t
(
???~yt
? ~w
)(
yt
??~yt
? 1? yt
1? ??~yt
)
(3)
???~yt
? ~w =
(
??~yt ? (??~yt)2
)
~f(~yt), (4)
4http://nlp.stanford.edu/software/tagger.shtml
5http://www.techdictionary.com/emoticon.html
6We thank Jacob Eisenstein for the formulation of logistic
regression model.
so the parameters can be set using gradient as-
cent. To control the overall complexity, we can ap-
ply regularized models on the elements of ~w. A
sparsity-inducing model, such as the Lasso (Tibshi-
rani, 1994) or elastic net (Zou and Hastie, 2005)
model, will drive many of these weights to zero, re-
vealing important interactions between the impolite-
ness/positivity label and other features. Instead of
maximizing the log-likelihood, we can minimize the
following Lasso model that consists of the negative
log-likelihood loss function:
min
(
? `+
?
i
?1||~wi||
)
(5)
Since the Lasso penalty can introduce discontinu-
ities to the original convex function, we can also
consider an alternative non-sparse ridge estima-
tor (le Cessie and van Houwelingen, 1992) that has
the convex property:
min
(
? `+
?
i
?2||~wi||2
)
(6)
In addition to the Lasso and ridge estimators, the
composite penalty based elastic net model balances
the sparsity and smoothness properties of both Lasso
and ridge estimators:
min
(
? `+
?
i
?1||wi||+
?
i
?2||wi||2
)
(7)
Our log-linear model is quite flexible; by compar-
ing various restrictions, we can test different features
when modeling impoliteness and positivity. In addi-
tion, the model can incorporate features from previ-
ous time windows, which requires much less compu-
tational complexity compared to standard high order
Markov models. We use the L-BFGS method (Liu
and Nocedal, 1989) for the numerical optimization.
5 Empirical Experiments
We predict impoliteness vs. non-impoliteness and
positivity vs. non-positivity of an interlocutor in the
immediate future turn, given only information from
current/previous turns. Because accuracy, precision,
recall and F-measure are threshold-based point esti-
mation metrics that might prevent one from observ-
ing the big picture of system performance, we con-
sider the Receiver Operating Characteristic (ROC)
metric to evaluate the dynamics of the true posi-
tive rate vs. the false positive rate (Hanley and Mc-
Neil, 1982) in our system. We mainly use Area Un-
der Curve (AUC) as a metric to compare classifiers,
since it maps the ROC metric to a single scalar value
representing expected performance. A random clas-
sifier will have an AUC of 0.5 (Fawcett, 2006).
23
Models P Ca E L O Ef Pe Di C EC Ch Cu I
Impoliteness Prediction
Tr-Te .44 -1.10 .62 .72 .09 .64 .09 1.29 .96 .89 .69 .77 -0.19
Te-Tr -2.48 .54 -0.26 0.15 .59 1.62 .24 .22 .89 .72 .75 .04 -0.18
Positivity Prediction
Tr-Te -0.87 .19 .36 .55 1.06 -0.62 .69 -1.63 -1.57 .16 -0.41 1.22 .86
Te-Tr -1.39 -0.46 .70 .48 .46 .33 .62 -0.71 .70 -0.65 -0.47 -0.54 .78
Table 1: Comparing the Learned Weights of Different Features when Predicting the Partner?s Impoliteness in a Non-
Sparse Log-Linear Model. Tr-Te: predict tutee turn with tutor turn. Te-Tr: predict tutor turn with tutee turn. For full
name of features, see Section 3.
5.1 Comparing the Learned Weights of
Different Features
In our previous analysis of these data (Ogan et al,
2012), a PCA method allowed us to group linguistic
behaviors in order to address the issue of data spar-
sity. With the use of log-linear models, we are able
to investigate the contributions of individual lan-
guage behaviors in one student?s turn to the predic-
tion of social functions in their partner?s next turn. In
this experiment, we evaluate the weights of various
linguistic devices in a standard logistic regression
model. We found that behaviors commonly asso-
ciated with impoliteness were predictors of partner
impoliteness in the next turn, while positive behav-
iors such as laughter were predictors of upcoming
positivity. SDSs can leverage this knowledge to take
the partners lead during a tutoring session, using the
partners positivity or impoliteness to determine the
affect of the systems upcoming move. As we intend
to develop a system that acts as a tutee, however, we
further divided the analysis by tutoring role, inves-
tigating how partners in different roles employ lan-
guage features differently, such that the system can
act in accordance with its given role. Table 1 shows
the results.
Similarly to the collapsed factors in our previous
work, we found here that tutors and tutees do in
fact use language behaviors differently, and to ac-
complish different social functions. Effectively, this
means that certain language behaviors may instigate
impoliteness when said by one partner, but lead to
positivity when expressed by the other. For exam-
ple, tutee bragging predicts a response of positiv-
ity on behalf of the tutor (~w(TE)C = .7), perhaps be-cause the tutor wants to be supportive of a prote?ge??s
self-efficacy and success. Conversely, when the tu-
tor brags during a peer tutoring dialogue, the tu-
tee, who may feel threatened by the tutors bravado,
is extremely likely to respond with impoliteness (
~w(TR)C = .96). In a peer tutoring paradigm, whenthe more powerful partner (the tutor) expresses dom-
inance through self-inflation, the subordinate part-
ner may use impoliteness to regain some social con-
trol. On the other hand, some language behaviors
actively work to tear down this power imbalance,
such as inclusive complaining, where the partners
take an us against the task approach, building sol-
idarity through complaining about the experiment.
These utterances predict positivity whether used by
the tutor ( ~w(TR)I = .86) or tutee ( ~w(TE)I = .78).Other comparisons between weighted features by
role demonstrate similarly theoretically-motivated
findings that shed light on how language is used to
achieve social functions.
5.2 Comparing the Contributions of Different
Features on Friend and Stranger Datasets
A previous study (Ogan et al, 2012) on these same
data seemed to indicate that negative conversational
strategies composed of linguistic devices such as
complaining and insults were correlated with learn-
ing in the friend dyads and negatively correlated
with learning in strangers. However the small num-
ber of stranger dyads prevented them from draw-
ing conclusions about particular linguistic devices
from the data. Here, we empirically show the pre-
dictive performance of different feature sets on both
friend and stranger test sets in Table 2 , using a
sparse Lasso model with features from only the
current turn. In the impoliteness prediction task,
when predicting on the test set that consists of only
friends, we observe statistically significant improve-
ment over a random baseline, using surface-level
language behavior features, lexical, lexical + syn-
tactic, all automatic, and all features. When com-
bining all features, the best AUC is .621. The auto-
matic features, mainly including n-grams and part-
of-speech tags, have emerged as a useful automated
feature space. On the other hand, we do not observe
any significant results on the stranger datasets, sug-
gesting that strangers do not respond with impolite-
ness in the same way that friends do. When pre-
dicting positivity on the friend dataset, we see that
24
the performance of surface-level language behavior
features has dropped from the first task, and the sta-
tistical t-test is non-significant when comparing to
a random baseline. This is not surprising, because
we have shown in the previous section that surface-
level language behavior features are strong indica-
tors of impoliteness, but might not have advantages
in predicting positivity for friends. Interestingly, the
automated features outperform the combination of
all features, indicating a promising future for the ac-
tual deployment of an SDS that can interact using
appropriate positivity and impoliteness.
When predicting positivity in the stranger dataset,
we find the opposite trend. In contrast to the impo-
liteness prediction task, the overall performance on
the stranger dataset improved, and the lexical, lexi-
cal+syntactic, and all feature combination have sig-
nificantly outperformed the chance baseline. These
results suggest that positivity is a predictable behav-
ior among strangers, who may all express uniform
positivity across all dyads, while it is the impolite-
ness that is predictable among friends. Perhaps it
is that through the development of a rapport with a
partner, the particular ways in which positivity is ex-
pressed becomes personalized to the dyad, and can
no longer be applied to other groups who have their
own expressions of positivity. In other words, un-
like in Tolstoy?s world, here unhappy families are all
alike; every happy family is happy in its own way.
We must look to the easily-predictable impoliteness
among friends instead, arguing strongly for the in-
clusion of impoliteness in a model of rapport.
5.3 Comparing Logistic Regression, Lasso,
Ridge, and Elastic Net
While our previous work (Ogan et al, 2012) demon-
strated that PCA is a useful feature selection method
when there are only a dozen features, in this experi-
ment, the dimension of our feature space is substan-
tially higher, which aligns to the size of vocabulary.
Thus, covariance-based feature selection methods,
such as PCA, might be too slow. Here we compare
the performances of standard MLE trained logistic
regression, Lasso, non-sparse ridge, and elastic net
models. In particular, we demonstrate the predic-
tive power of Lasso and elastic net models, varying
distinct levels of sparsity. In the Figure 1, we show
the comparison of three different models in the im-
politeness prediction task. The horizontal axis rep-
resents different values of regularization coefficient
?. For the Lasso model and the elastic net model,
increasing the value ? will result in a sparser feature
space, and we set the ? = ?1 = ?2 in the elastic net
model to promote same level of sparsity and smooth-
ness. The result at ? = 0 represents the standard
Feature Sets F-AUC p S-AUC p
Impoliteness Prediction
Random .500 - .500 -
Behavior .596 .017 .505 .473
Lex .599 .014 .435 .819
Lex + POS .605 .009 .425 .857
All Auto .591 .022 .451 .751
All Features .621 .003 .427 .850
Positivity Prediction
Random .500 - .500 -
Behavior .549 .141 .527 .302
Lex .623 .003 .601 .025
Lex + POS .646 .001 .587 .047
All Auto .651 .001 .577 .070
All Features .641 .001 .608 .019
Table 2: Comparing contributions of different feature
streams on both friend and stranger testsets with Lasso
model when predicting impoliteness and positivity of the
next turn using only features from the current turn. ( F-:
the friend test set. S: the stranger test set. p: one-tailed
p-value by comparing to a random classifier. Behavior:
detailed surface-level language behavior features defined
in Section 3. Lex: unigram and bigram. POS: part-of-
speech features. All Auto: all automatically extracted
features (Lex + POS + punctuation + caps + emoti-
cons).)
non-sparse logistic regression model, which obtains
an AUC of .563. When introducing penalty for large
weights in this standard model, .4 to .5 significant
improvements (p = .003 for Lasso, p = .007 for
ridge, and p = .004 for elastic net) of AUC are
observed from Lasso, ridge and elastic net models
when ? = 1. The elastic net model that balances
sparsity and smoothness, has obtain the best result
in this experiment. The best result of elastic net
model is .63 when ? = 7. This experiment shows
that all three penalty models have outperformed the
non-sparse logistic regression model. The elastic net
model, which balances sparisty and smoothness, ob-
tains the best results when predicting impoliteness.
Figure 2 shows the comparison of three models on
the friend dataset in the positivity prediction task.
When ? = 0, the standard logistic regression model
has an AUC of .638. When increasing the ? to 1,
both Lasso and elastic net models have shown sig-
nificant improvements (both p < .001) in AUC, but
not the non-sparse ridge estimator. The Lasso model
is found to be the best model in this task: we obtain
better results when the model gets sparser until the
model is too sparse when ? = 6. In contrast to the
experiment in Figure 1, we see that both the ridge
and elastic net models do not very strong advantages
25
in this positivity prediction task. We hypothesize
that the reason why Lasso works better in the pos-
itivity task is that the frequency of positivity labels
is substantially higher than the impoliteness labels in
our corpus, so that a Lasso model that enforces full
`1 penalty fits better in this task. In contrast, since
the impoliteness label is less frequent, a denser elas-
tic net composite penalty model that preserve critical
features, works the best in the impoliteness predic-
tion task. In general, we can see that sparse log-
linear models outperform standard log-linear mod-
els as well as non-sparse ridge estimators in the two
tasks.
Figure 1: Comparing Impacts of Different Levels of Spar-
sity on the Friend Dataset When Predicting Impoliteness
with Lasso, Ridge, and Elastic Net Models
Figure 2: Comparing Impacts of Different Levels of Spar-
sity on the Friend Dataset When Predicting Positivity
with Lasso, Ridge, and Elastic Net Models
5.4 Comparing Impacts of Different Feature
Window Sizes
A practical problem for parameter estimation in both
generative and discriminative models for dialogue
processing is to evaluate how much history the sys-
tem should take into account, so that it can have
enough information to make correct predictions. In
this experiment, we investigate the impact of using
different feature window sizes using the elastic net
model. We compare the two-tailed student t-test be-
tween the baseline that only uses features from the
current turn and models that use current + previous
n turn(s). For the friend dataset, when only using
the features from the current turn to predict the im-
politeness in the immediate next turn, we observe
an AUC of .619. The best result is obtained when
we combine the previous two turns together with the
current feature turn: an AUC of .635, significantly
better (p = .03) than only using the current turn win-
dow. The patterns on the non-friend dataset are less
clear, while the model obtains the best result when
window size is +3 previous turns, the improvement
is not significant (p = .962). In the positivity task,
we also observe benefits to incorporating larger fea-
ture windows. The AUC on the friend test set starts
at .638, when only using the current feature window
in the elastic net model. After incorporating larger
feature windows, we obtain the best result of .675 at
the +4 window (p = .04). Similarly, the AUC on
non-friend test set initializes at .618, but climbs to
.632 at the +4 window.
6 Error Analysis and Discussion
We performed an error analysis to understand the
contexts under which our model failed to accurately
predict a students? social response, and discuss the
implications of these examples based on a theoret-
ical understanding of the roles of tutors and tutees
as well as friends and strangers. The following is
an example error produced when looking only at the
previous turn to predict the current turn:
? Tutee (impolite): ?dude thats def wrong i gotta
subract 16m not just 16? (the current turn)
? Tutor (non-impolite): ?16m is what has to be
subtracted from both sides? (the next turn, pre-
dicted incorrectly)
In the segment above the tutee challenges the tutor
by pointing out a ?def? mistake; the tutor responds
with a task-oriented contribution that moves the di-
alogue forward, but does not escalate the face threat
(Ogan et al, 2012). And, in fact, if we look one
more turn back in the history, the tutor once again
uses calm language: ?wait it says youre wrong i dont
know why ust wait?. The increased window size
is implicitly evoking the differential conversational
strategies of tutors vs. tutees. And while the current
data set is too small to build separate models for tu-
tors and tutees, in this case (and based on the prior
work in Ogan et al, 2012), accounting for role dis-
tinctions that differentiate strategies taken by tutors
and tutees is the likely reason behind the improve-
ment due to window size.
Conversely to the friend data set, the false nega-
tives that occur when predicting impoliteness in the
stranger data set are not improved by increasing the
26
window size, as is demonstrated in the following ex-
change:
? Tutor (non-impolite): ?subtract ym from both
sides.?
? Tutee (non-impolite): ?first step? first Step??
? Tutor (non-impolite): ?subtract hb from both
sides? (the current turn)
? Tutee (impolite): ?first step? FIRST
STEP??????????? (the next turn, predicted in-
correctly)
The impolite tutee utterance at turn 4 is predicted
to be non-impolite when analysis is limited to the
previous turn, as is also shown in the first example
in this section. However, unlike the previous ex-
ample which improved with an expanding window
size, looking back to turns 1 and 2 does not improve
the model. While we do not have enough stranger
dyads to completely explore this phenomenon, it
seems clear that strangers? responses do not follow
the same patterns as friends. The current unpre-
dictability of strangers can be due to a number of
social phenomena, such as less affect (both posi-
tive and negative) overall, which results in a differ-
ent conversational flow. Less overall affect means
that there is less likely to be useful information in
the previous utterances. This is an important dis-
tinction between designing models for dyads with
rapport and those without, which is a primary con-
cern in the development of social SDSs. Among
strangers, other techniques may need to be used to
increase model accuracy, such as looking at the con-
tent of the utterances to determine whether or not a
speaker had been repeating themselves, as is shown
in this example, which could likely be an indicator
of rudeness.
As a final example of how the error analysis
can reveal important phenomena for future study,
when examining the prediction of positivity on the
stranger test set, we first observe that emoticons
are useful indicators of positivity. However, some-
times emoticons serve quite different social func-
tions, which leads to false positives:
? Tutor (non-positivity): ?Simplify ! :)? (the cur-
rent turn)
? Tutee (non-positivity): ?y didnt it chang? (the
next turn, predicted incorrectly)
Here, the smiley face is used by the tutor primarily
to mitigate the face threat of an impolite command.
However, since the experiment reported in Section
6.1 shows that our model attributes more weight to
emoticons when predicting positivity, the model errs
on this utterance. Here the error analysis suggests
that in fact we might need to investigate more com-
plicated latent variable models to capture the subtle
social functionality of some language use in context.
7 Conclusion
Long-term relationships involve the expression of
both positive and negative sentiments and, paradox-
ically, both can serve to increase closeness. In this
paper, we have addressed the novel task of predict-
ing impoliteness and positivity in teenagers? peer tu-
toring conversations, and our results shed light on
what kinds of behaviors evoke these social functions
for friends and for strangers, and for tutors and tu-
tees. Our investigation has successfully predicted
impoliteness and positivity on the basis of both an-
notated and automatically extracted features, sug-
gesting that a dialogue system will one day be able to
employ analyses such as these to signal relationships
with users. And while social features such as those
we annotated are naturally quite rare in dialogue, our
quantitative experiments have demonstrated the ca-
pabilities of modeling sparsity in log-linear models:
elastic net and Lasso models outperformed standard
logistic regression model and the non-sparse ridge
penalty model.
We found that positivity is much more predictable
for strangers than is impoliteness, while the oppo-
site was true for friends. This could lend support for
the importance of positivity as a rapport-signaling
function in the early stages of a relationship (as
in (Tickle-Degnen and Rosenthal, 1990)), and indi-
cating the need for further research on the increasing
importance of impoliteness as a rapport signal over
the course of relationship development.
We also found that performance on the prediction
tasks increased with larger feature window sizes,
particularly for impoliteness among friends and pos-
itivity among strangers. From our error analysis,
we see that this improvement may arise because dif-
ferent behaviors predict impoliteness and positivity
based on the social role of the speaker. Thus tu-
tee bragging predicts positivity in tutors, while tu-
tor bragging negatively predicts positivity among tu-
tees. The power differential between the two may
lead tutees to want to take tutors ?down a peg? while
tutors struggle to maintain the position of power in
the dyad.
While results such as these may seem specific to
teenage peer tutors, the general conclusion remains,
that linguistic devices have different social functions
in different contexts, and dialogue systems that in-
tend to spend a lifetime on the job will do well to
adapt their language to the stage of relationship with
a user, and the social role they play.
27
References
Hua Ai, Diane J. Litman, Kate Forbes-Riley, Mihai Ro-
taru, Joel Tetreault, and Amruta Purandare. 2006.
Using system and user performance features to im-
prove emotion detection in spoken tutoring dialogs. In
Proceedings of the Ninth International Conference on
Spoken Language Processing (Interspeech 2006).
Angela M. Ardington. 2006. Playfully negotiated activ-
ity in girls talk. Journal of Pragmatics, 38(1):73 ? 95.
Rachel E. Baker, Alastair J. Gill, and Justine Cassell.
2008. Reactive redundancy and listener comprehen-
sion in direction-giving. In Proceedings of the 9th
SIGdial Workshop on Discourse and Dialogue.
Timothy W. Bickmore and Rosalind W. Picard. 2005.
Establishing and maintaining long-term human-
computer relationships. ACM Transactions on
Computer-Human Interaction.
Timothy Bickmore, Laura Pfeifer, and Daniel Schulman.
2011. Relational agents improve engagement and
learning in science museum visitors. In Proceedings
of the 10th international conference on Intelligent vir-
tual agents, IVA?11.
Kristy Elizabeth Boyer, Robert Phillips, Michael Wallis,
Mladen Vouk, and James Lester. 2008. Balancing
cognitive and motivational scaffolding in tutorial di-
alogue. In Proceedings of the 9th international con-
ference on Intelligent Tutoring Systems, ITS ?08.
Penelope Brown and Stephen Levinson. 1978. Uni-
versals in language usage: Politeness phenomena. In
Questions and politeness: Strategies in social interac-
tion.
Justine Cassell, Alastair J. Gill, and Paul A. Tepper.
2007. Coordination in conversation and rapport. In
Proceedings of the Workshop on Embodied Language
Processing, EmbodiedNLP ?07, pages 41?50, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Jan Chovanec. 2009. Simulation of spoken interaction in
written online media texts. Brno Studies in English.
David Crystal. 2001. Language and the internet. Cam-
bridge University Press.
Jonathan Culpeper. 1996. Towards an anatomy of impo-
liteness. In Journal of Pragmatics.
Jonathan Culpeper. 2011. Impoliteness: Using language
to cause offence.
Laurence Devillers and Laurence Vidrascu. 2006. Real-
life emotions detection with lexical and paralinguistic
cues on human-human call center dialogs. In Proceed-
ings of the Ninth International Conference on Spoken
Language Processing (Interspeech 2006).
A Gartner, M Kohler, and F Riessman. 1971. Children
teach children: Learning by teaching. In New York and
London: Harper and Row.
Jose? Gonza?lez-Brenes and Jack Mostow. 2011. Which
system differences matter? using l1/l2 regulariza-
tion to compare dialogue systems. In Proceedings of
the SIGDIAL 2011 Conference, pages 8?17, Portland,
Oregon, June. Association for Computational Linguis-
tics.
Jonathan Gratch, Anna Okhmatovskaia, Francois
Lamothe, Stacy Marsella, Mathieu Morales, Rick J.
van der Werf, and Louis-Philippe Morency. 2006.
Virtual rapport. In Proceedings of the International
Conference on Intelligent Virtual Agents (IVA 2006).
M. Grimm, E. Mower K. Kroschel, and S. Narayanan.
2007. Primitives-based evaluation and estimation of
emotions in speech. In Speech Communication.
P. Gupta and R. Nitendra. 2007. Two-stream emo-
tion recognition for call center monitoring. In Pro-
ceedings of the 8th Annual Conference of the Inter-
national Speech Communication Association (Inter-
speech 2007).
Susan C. Herring and Asta Zelenkauskaite. 2009. Sym-
bolic capital in a virtual heterosexual market. In Writ-
ten Communication.
Je Hun Jeon, Rui Xia, and Yang Liu. 2010. Level of in-
terest sensing in spoken dialog using multi-level fusion
of acoustic and lexical evidence. In Proceedings of the
11th Annual Conference of the International Speech
Communication Association (Interspeech 2010), IN-
TERSPEECH 2010.
W. Lewis Johnson and Paola Rizzo. 2004. Politeness in
tutoring dialogs: run the factory, thats what id do. In
Intelligent Tutoring Systems, Lecture Notes in Com-
puter Science.
Manfred Kienpointner. 1997. Varieties of rudeness:
types and functions of impolite utterances. In Func-
tions of Language.
S. le Cessie and J.C. van Houwelingen. 1992. Ridge
estimators in logistic regression. Applied Statistics,
41(1):191?201.
Jackson Liscombe, Julia Hirschberg, and Jennifer J. Ven-
ditti. 2005. Detecting certainness in spoken tutorial
dialogues. In Proceedings of the 6th Annual Confer-
ence of the International Speech Communication As-
sociation (Interspeech 2005).
D. Litman and K. Forbes-Riley. 2004. Predicting stu-
dent emotions in computer-human tutoring dialogues.
In Proceedings of the 42nd Annual Meeting of the As-
sociation for Computational Linguistics (ACL 2004).
Dong C. Liu and Jorge Nocedal. 1989. On the lim-
ited memory bfgs method for large scale optimization.
Mathematical Programming, 45:503?528.
Andre Martins, Noah Smith, Mario Figueiredo, and Pe-
dro Aguiar. 2011. Structured sparsity in structured
prediction. In Proceedings of the 2011 Conference on
28
Empirical Methods in Natural Language Processing,
pages 1500?1511, Edinburgh, Scotland, UK., July. As-
sociation for Computational Linguistics.
Bruce M. McLaren, Sung-Joo Lim, David Yaron, and
Ken Koedinger. 2007. Can a polite intelligent tutor-
ing system lead to improved learning outside of the
lab? In Proceedings of the 2007 conference on Arti-
ficial Intelligence in Education: Building Technology
Rich Learning Contexts That Work.
Bruce McLaren, DeLeeuwm Krista E., and Richard E.
Mayer. 2011. Polite web-based intelligent tutors: Can
they im-prove learning in classrooms? In Computers
and Education.
Angeliki Metallinou, Martin Wollmer, Athanasios
Katsamanis, Florian Eyben, Bjorn Schuller, and
Shrikanth S. Narayanan. 2011. Context-sensitive
learning for enhanced audiovisual emotion classifica-
tion. IEEE Transactions on Affective Computing.
Amy Ogan, Samantha Finkelstein, Erin Walker, Ryan
Carlson, and Justine Cassell. 2012. Rudeness and
rapport: Insults and learning gains in peer tutoring. In
Proceedings of the 11 International Conference on In-
telligence Tutoring Systems (ITS 2012).
J. Alfredo Sa?nchez, Norma P. Herna?ndez, Julio C. Pena-
gos, and Yulia Ostro?vskaya. 2006. Conveying mood
and emotion in instant messaging by using a two-
dimensional model for affective states. In Proceedings
of VII Brazilian symposium on Human factors in com-
puting systems, IHC ?06, pages 66?72, New York, NY,
USA. ACM.
A. Sharpley, J. Irvine, and C. Sharpley. 1983. An exami-
nation of the effectiveness of a cross-age tutoring pro-
gram in mathematics for elementary school children.
In American Educational Research Journal.
Helen Spencer-Oatey. 2008. Face (im)politeness and
rapport. In Culturally Speaking: Culture, Communi-
cation and Politeness Theory.
Carolyn A. Straehle. 1993. ?samuel?? ?yes dear?? teas-
ing and conversatrion rapport. In Framing in Dis-
course.
Bas Stronks, Anton Nijholt, Paul van Der Vet, Dirk
Heylen, and Aaron Machado. 2002. Designing for
friendship: Becoming friends with your eca. In Pro-
ceedings of Embodied conversational agents - let?s
specify and evaluate (AAMAS).
Marina Terkourafi. 2008. Toward a unified theory of po-
liteness, impoliteness, and rudeness. Impoliteness in
language: studies on its interplay with power in the-
ory and practice.
Robert Tibshirani. 1994. Regression shrinkage and se-
lection via the lasso. Journal of the Royal Statistical
Society, Series B, 58:267?288.
Linda Tickle-Degnen and Robert Rosenthal. 1990. The
nature of rapport and its nonverbal correlates. In Psy-
chological Inquiry.
Kevin E. Vowles and Miles Thompson. 2012. The
patient-provider relationship in chronic pain. In Psy-
chiatric Management of Pain.
Erin Walker, Nikol Rummel, and Kenneth R. Koedinger.
2011. Is it feedback relevance or increased account-
ability that matters? In Proceedings of the 10th Inter-
national Conference on Computer-Supported Collab-
orative Learning (CSCL 2011).
William Yang Wang and Julia Hirschberg. 2011. Detect-
ing levels of interest from spoken dialog with multi-
stream prediction feedback and similarity based hier-
archical fusion learning. In Proceedings of the 12th
annual SIGdial Meeting on Discourse and Dialogue
(SIGDIAL 2011), Portland, OR., USA, June. ACL.
William Yang Wang and Kathleen McKeown. 2010. ?got
you!?: Automatic vandalism detection in wikipedia
with web-based shallow syntactic-semantic modeling.
In Proceedings of the 23rd International Conference
on Computational Linguistics (Coling 2010), pages
1146?1154, Beijing, China, August. Coling 2010 Or-
ganizing Committee.
William Yang Wang, Fadi Biadsy, Andrew Rosenberg,
and Julia Hirschberg. 2012a. Automatic detection
of speaker state: Lexical, prosodic, and phonetic ap-
proaches to level-of-interest and intoxication classifi-
cation. Computer Speech & Language.
William Yang Wang, Elijah Mayfield, Suresh Naidu, and
Jeremiah Dittmar. 2012b. Historical analysis of le-
gal opinions with a sparse mixed-effects latent vari-
able model. In Proceedings of the 50th Annual Meet-
ing of the Association for Computational Linguistics
(ACL 2012).
Richard J. Watts. 2003. Politeness. Cambridge Univer-
sity Press.
Hui Zou and Trevor Hastie. 2005. Regularization and
variable selection via the elastic net. Journal of the
Royal Statistical Society, Series B, 67:301?320.
29
Proceedings of the SIGDIAL 2013 Conference, pages 51?60,
Metz, France, 22-24 August 2013. c?2013 Association for Computational Linguistics
Automatic Prediction of Friendship via Multi-model Dyadic Features 
Zhou Yu, David Gerritsen, Amy Ogan, Alan W Black, Justine Cassell 
School of Computer Science, Carnegie Mellon University 
{zhouyu, dgerrits, aeo, awb, justine }@cs.cmu.edu 
 
Abstract 
In this paper we focus on modeling 
friendships between humans as a way of 
working towards technology that can initiate 
and sustain a lifelong relationship with users. 
We do this by predicting friendship status in a 
dyad using a set of automatically harvested 
verbal and nonverbal features from videos of 
the interaction of students in a peer tutoring 
study. We propose a new computational 
model used to model friendship status in our 
data, based on a group sparse model (GSM) 
with L2,1 norm which is designed to 
accommodate the sparse and noisy properties 
of the multi-channel features. Our GSM model 
achieved the best overall performance 
compared to a non-sparse linear model (NLM) 
and a regular sparse linear model (SLM), as 
well as outperforming human raters. Dyadic 
features, such as number and length of 
conversational turns and mutual gaze, in 
addition to low level features such as F0 and 
gaze at task, were found to be good predictors 
of friendship status. 
1 Introduction and Related Work 
While significant advances have been made in 
detecting the speech and nonverbal social signals 
emitted by individuals (see Vinciarelli, Pantic & 
Bourlard, 2009, for a review), and research has 
addressed the social roles and states of 
individuals in groups (see Gatica-Perez, 2009, 
for a review), considerably less computational 
work has focused on the automatic detection of 
speech or nonverbal correlates of specifically 
dyadic states, such as rapport. And yet rapport 
has been shown to have important effects on 
interactions as diverse as survey interviewing 
(Berg, 1989), sales (Brooks, 1989), and health 
(Harrigan et al, 1985).  If we are to build 
interactive systems that are successful, then, we 
believe that the ability to build rapport with a 
human user will be essential. 
Rapport can be instantaneous and can also 
build over time. Granovetter (1973) describes the 
strength of an interpersonal ?tie? as a function of 
the time, emotional intensity, and reciprocity that 
accumulates between people. These ties mediate 
effects in myriad domains such as learning 
(Azmitia & Montgomery, 1993) and healthcare 
(Harrigan & Rosenthal, 1983).  
Accordingly, analysis of initial exchanges and 
those after many years of interaction suggests 
that the behavioral signals that indicate rapport 
change over time. For example, in Tickle-
Degnen and Rosenthal?s highly cited model 
(1990), rapport consists of mutual attention, 
positivity, and coordination. High levels of 
positivity between conversational partners are 
common in the initial phases of a relationship, 
but positivity has been shown to decline, without 
a loss in rapport, as the number of interactions 
increases. In fact, Ogan et al (2012) gave 
evidence that the use of playful rudeness 
between friends during peer tutoring correlates to 
greater learning. This leads to an associated 
challenge of spoken dialogue system 
development: creating systems that can develop 
social ties, and increase rapport with the user 
over repeated interactions to maximize beneficial 
outcomes. 
While little work has addressed automatic 
detection, some prior work has addressed the 
problem of emitting signals to build rapport in 
dialogue and agent systems (Stronks et al, 2002; 
Bickmore & Picard, 2005; Gratch et al, 2006; 
Cassell et al, 2007; Bickmore et al, 2011), and 
we turn to this research for what cues might be 
important in rapport. The majority of this prior 
work, however, has addressed harmony ? or 
instant rapport ? rather than rapport over time. 
For those systems that have addressed friendship 
or the growth of rapport, most commonly the 
number of interactions has been used as a meter 
of relationship progression, instigating changes 
in the dialogue system as the social odometer 
scrolls onward (Cassell & Bickmore, 2003; 
Vardoulakis et al, 2012). Counting the times a 
dyad has interacted is a crude approximation of a 
relationship state, however; being able to detect 
the behavioral signals that people actually use to 
indicate relationship status would be superior. 
In our own prior work (Cassell et al,2007) we 
looked at particular hand-annotated nonverbal 
signals (such as nodding and mutual gaze) as 
operationalizations of rapport, and found that 
friends and non-friends indeed show differing 
distributions of each signal as a function of 
relationship state. In the current study, we move 
to the next step and automatically harvest a set of 
multimodal dyadic and time contingent features 
to identify those features that play a significant 
role in predicting friendship state. A major 
51
challenge for predicting relational states such as 
these is to construct a compact feature space that 
captures only reliable rapport signals and also 
generalizes across different users. To provide 
strength to our model (as well as to fit the 
multimodal nature of embodied conversational 
agents), we look at both acoustic and visual 
features. Such an approach takes advantage of 
the fact that multimodal aspects of 
communication are not redundant, but often 
complementary (Cassell, 2000).  
    However, dyadic behaviors such as 
conversational turns, mutual/non-mutual smile, 
mutual/non-mutual gaze, and mutual/non-mutual 
lean forward provide an additional challenge in 
modeling; no matter how important, they appear 
relatively rarely in conversational data. Thus 
standard non-sparse linear models, normally 
trained on high frequency factors, might assign 
too much weight to low frequency (i.e., sparse) 
features. In order to address issues of this sort 
Yuan and Lin (2007) introduced the group 
lasso.   To address the sparse nature of our 
features in real-world data and the noise that 
occurs from different production sources, we 
propose an extension to this genre of technique 
in the form of a Group Sparse Model (GSM) 
which enforces sparsity with a L2,1 norm instead 
of the group lasso penalty (Chen, et  al., 2011), 
due to the relatively efficient optimization 
process of L2,1 norms (Liu, et al, 2009). Unlike 
a straightforward sparse linear model (SLM) 
(Yang et al, 2010), which treats each feature 
independently, GSMs group features which share 
the same production source in the optimization 
process. In the GSM linear model, the removal of 
the assumption of independence between 
features means that the penalty is on group rather 
than individual features. Thus the model has 
general robustness to noise, since grouping 
features from the same production source can 
increase the overall confidence of the feature 
group. 
Our contributions in this work, then, are three-
fold: we (1) designed and implemented a method 
for automatic dyadic feature extraction which is 
based on low level features, and which yields 
strong predictive power of friendship status, (2) 
propose a new Group Sparse Model (GSM) with 
L2,1 norm, that deals with the noisy and sparse 
nature of the feature sets, and (3) illuminate, 
from this model, the nature of verbal and 
nonverbal behavior between friends and non-
friends in a peer tutoring setting. 
The remainder of the paper is organized as 
follows. We first describe the data set and 
introduce the features used in our experiments. 
We then describe the performance of the three 
computational models we evaluated. Finally, we 
discuss the contributions of different features to 
friendship prediction and provide an error 
analysis of our proposed model.  
2 The Data Set 
  
Figure 1: Camera View 1 and Camera View 2 
We collected data from dyads of students 
engaged in a reciprocal peer tutoring task. We 
chose peer tutoring as it is a domain in which 
friendship has been shown to have a positive 
effect on student learning (see e.g. Ogan et al 
2012). In addition, tutoring systems that rely on 
dialogue are common, and peer tutoring dialogue 
systems are increasingly common. Thus, being 
able to assess friendship state in this domain is a 
useful step on the path to creating a peer tutoring 
agent that can use rapport to increase learning 
gains.  
    Each dyad consisted of two American English 
speakers with a mean age of 13.3 years (range = 
12 ? 15). We collected data from 12 dyads, of 
which 6 dyads were already friends. Dyads were 
either both girls or both boys, and each condition 
contained 3 boy dyads and 3 girl dyads.  
Each dyad came to the lab for 3 sessions, with 
an average interval between visits of 4.6 days 
(SD = 3.1), totaling 36 sessions across all dyads. 
Each session consisted of about 90 minutes of 
interaction recorded from three camera views (a 
frontal view of each participant and a side view 
of the two participants). With close talk 
microphones, we also recorded the participants? 
speech in separate audio channels for the purpose 
of automatic dyadic acoustic feature extraction. 
The setting is shown in Figure 1. 
Each session began with a short period of time 
for participants to become acquainted. After that, 
using a standard reciprocal tutoring procedure 
(see Fantuzzo et al, 1989), participants tutored 
each other on procedural and conceptual aspects 
of an algebra topic in which both participants 
were relatively novice. Order of seating and 
assignment of tutoring roles (tutor or tutee) was 
determined in the first session by alphabetical 
order of participant name. Tutoring roles 
alternated from that point on, such that both 
participants had the opportunity to take on the 
role of ?expert? during each session. After a 
period of individual study time to familiarize 
52
themselves with the material, the first tutoring 
period began and lasted approximately 25 
minutes. This was followed by a 5 minute break, 
after which students? tutoring roles were reversed 
for a second tutoring period of 25 minutes. 
Finally, each student answered a survey about 
the interaction.  
The current study examines only the tutoring 
sections of each session, which were divided into 
30-second clips or ?thin slices? (Ambady et al, 
2006). In total, the data points used for modeling 
comprise 2259 clips from the 12 dyads. 
3 Multimodal Information  
In our analyses, low-level audio and visual 
features were automatically extracted using three 
off-the-shelf toolkits. Dyadic features, which are 
a second order derivative of the low level 
features, and which capture the interaction of two 
participants, are also automatically produced. 
Taken together, analysis of these features allows 
us to determine if the verbal and nonverbal 
behaviors of the participants index their 
friendship status in any significant way.  
3.1 Low Level Audio Features (LA)  
Type # of Features 
Prosodic Features 
  F0 72 
  Energy 38 
  Duration 154 
Voice Quality Features 
  Jitter 68 
  Shimmer 34 
  Voicing 38 
Spectral Features 
  MFCC 570 
Total 974 
 
Table 1: Acoustic Feature Groups 
 
For acoustic feature extraction, a large set of 
acoustic low-level descriptors (LLD) and 
derivatives of LLDs combined with appropriate 
statistical functionals, i.e., maxPos (the absolute 
position of the maximum value in frames), 
minPos (the absolute position of the minimum 
value in frames), amean (The arithmetic mean of 
the contour), etc., were extracted for each of the 
split channel recordings. The ?INTERSPEECH 
2010 Paralinguistic Challenge Feature Set? in the 
openSMILE toolkit (Schuller et al, 2012) was 
used as our basic acoustic feature set. For 
spectral features, Mel Spectrum and LSP were 
excluded due to the possible overlap with 
MFCC. The set contained 974 features which 
resulted from a base of 32 low-level descriptors 
(LLD) with 32 corresponding delta coefficients, 
and 21 functionals applied to each of these 68 
LLD contours. In addition, 19 functionals were 
applied to the 4 pitch-based LLD and their four 
delta coefficient contours. Finally the number of 
pitch onsets (pseudo syllables) and the total 
duration of the input were included. The 
dimension of each feature group is shown in 
Table 1. 
3.2 Low Level Vision Features (LV) 
Type # of Features 
Face Position Feature 10 
38 Face Interest Points 114 
Gaze Features 3 
Face Direction  Features 4 
Mouth and Eye Openness 6 
Smile Intensity 1 
Discretized Smile 1 
Total 139 
 
Table 2: Vision Feature Groups 
 
Since participants were facing the camera 
directly most of the time, as seen in Fig 1, 
current technology for facial tracking can 
efficiently be applied to our dataset. OMRON?s 
OKAO Vision System was used in face 
detection, facial feature extraction, and basic face 
related features extrapolation. For each frame, 
the vision software returns a smile intensity (0-
100) and the gaze direction, using both 
horizontal and vertical angles expressed in 
degrees. Apart from gaze direction, the software 
also provides information about head orientation: 
horizontal, vertical, and roll (in or out). 38 
additional face interest points, position and 
confidence, were also extracted. These were 
normalized to pixel coordinates, which turned 
out to lead to quite noisy data, and hence to 
diminished utility of these 38 points (in the 
future we will consider normalizing to face 
coordinates). We also calculated the openness of 
the left eye, right eye, mouth, and the location of 
the face. Details are shown in Table 2. Similar to 
our audio feature extraction method, one static 
feature vector per 30 second video clip was 
produced. All the features were computed at the 
same rate as the original videos: 30 Hz. 
Altogether, 139 dimensions were extracted in 
each frame from each camera view. 
3.3 Dyadic Features (DF) 
All of the features discussed above are low-level 
acoustic and visual features, extracted with 
53
respect to individual participants. While 
individual behavior may index friendship state, 
we posit that patterns of interaction will be more 
effective. For example, prior research (Baker et 
al., 2008) suggests that the number and length of 
conversational turns (Cassell et al, 2007), 
presence of mutual smiles and non-mutual smiles 
(Prepin et al, 2012), mutual gaze and non-
mutual gaze (Nakano et al, 2010), as well as 
posture shifting (Cassell, et al, 2001; Tickle-
Degnen & Rosenthal, 1990), are important 
features to investigate in dyadic data. While 
other features such as gestures and mutual pitch 
shift may also play a role in indexing relationship 
state, these are not yet a part of the dyadic 
features we address here.  
3.3.1 Number and Average Length of 
Conversational Turns   
We recorded individual audio channels for each 
participant, which makes the automatic 
extraction of conversational turns possible. First, 
we extracted intervals of silence with toolbox 
SoX which produced speech chunks, and then 
identified the speaker by comparing the speech 
energy (loudness) in each audio channel, as 
speech from each speaker is carried by the 
other?s microphone. After that we combined the 
speech chunks and speaker ID to approximate 
conversational turns. The approximation quality 
is not perfect, given the variability of the audio 
recording, but noise can be mediated during 
model building. 
3.3.2 Mutual Smile and Non Mutual Smile  
Prepin et al (2012) describe the role of mutual 
smiles (smiles that occur during the same time 
period) in ?stance alignment? and make the point 
that interactional alignment of this behavior 
reflects synchronization of internal states. Such 
synchrony predicts mutual understanding and 
increased quality of interaction, and as such is a 
fundamental quality in the formation of 
adolescent friendships (Youniss, 1982). Cappella 
& Pelachaud (2002) likewise describe 
?mutuality? as the precondition for how smiles 
function in contingent ways in a dyad. Smiles are 
clearly therefore important to assess in data such 
as ours. We defined a maximum window of 500 
milliseconds between the end of one participant?s 
smile and the beginning of the next for smiles to 
be considered mutual.  
3.3.3 Mutual Gaze and Non-mutual Gaze 
Nakano & Ishii (2010) describe eye gaze as a 
clue to engagement, and integrate mutual gaze 
into their conversational agents. There is no 
feature for direct gaze at partner provided in the 
OKAO vision toolkit. Mutual gaze was therefore 
approximated by annotating a gaze ?in front,? 
achieved by combining the information from 
three directions of gaze: vertical, horizontal, and 
depth. Gaze ?in front?, or at the partner, was 
recorded only if the participant gaze had less 
than a 15 degree angle from straight forward in 
all of these three directions. A maximum window 
of 500 milliseconds for gaze to be considered 
mutual was also employed here.  
3.3.4 Mutual Lean Forward and Non-Mutual 
Lean Forward 
Forward leaning has been shown to be a 
significant predictor of the ability to establish 
rapport in a dyad (Harrigan et al, 1985). In fact, 
friends who lean in are seen as more socially 
competent, while strangers are seen as less 
socially competent when they lean in (Burgoon 
& Hale, 1988). For our study, lean forward was 
approximated by detecting the smooth trend of 
face enlargement within the video frame. In 
order to improve precision of the feature, the 
segments with high confidence in face detection 
were processed. Furthermore, posture shifting, 
i.e., forward leaning, is not as quickly executed 
as changes in gaze or smile. We therefore used a 
1 second sample window for lean forward, rather 
than a 500 millisecond window.  
3.3.5 Mutual Gaze followed by Mutual Smile 
Mutual gaze followed by mutual smile is also 
approximated using a similar approach as above. 
It is a relatively dense feature compared to all the 
other possible combinations of nonverbal 
behaviors, thus it is the only combination that is 
included in the feature set in this paper. The 
window within which mutual gaze is considered 
to be followed by mutual smile is set to be within 
2 seconds. 
4 Computational Model  
We formulate friendship prediction as a set of 
binary classifications. In order to have the least 
variance and make sure no participant appeared 
in both the training and testing set, a leave-one-
out cross-validation setting was adopted in all of 
our experiments. Each session had approximately 
180 30-second video clips, totaling 2259 data 
points. Z-score normalization by dyad was used 
to scale all the features into the same range. 
Early fusion, which is simple concatenation of 
feature vectors, was adopted throughout our 
experiments to combine different features. We 
evaluated our group sparse model (GSM), along 
with a non-sparse linear model (NLM) and 
sparse linear model (SLM). 
54
4.1 Non-sparse Linear Model (NLM) 
We began with a standard non-sparse linear 
model (NLM), which is a Support Vector 
Machine (SVM) (Cortes & Vapnik, 1995) with a 
linear kernel. The libsvm (Fan et al, 2008) 
package was used in our experiment, and the 
parameter, the slack value of SVM that controls 
the scale of the soft margin, was obtained by 
cross validation.  
4.2 Sparse Linear Model (SLM)  
In order to prevent over-fitting on rare dyadic 
features, a sparse sensitive model SLM was 
introduced. As well as preventing over-fitting, 
through weight shrinkage the sparse model can 
also exclude redundant features. In our 
experiment, an L2,1 norm sparse model with 
linear kernel (Yang et al, 2012) was selected as 
our baseline sparse model. 
4.3 Group Sparse Model (GSM) 
Based on the SLM, we propose a group-sparse 
model (GSM) with the novel use of an L2,1 
norm. Instead of assuming every feature is 
uncorrelated to other features, the GSM groups 
some of the features together and utilizes their 
correlated information to mediate the noise of the 
data. For an arbitrary matrix        , its 
          is defined as  
         ? ??    
  
   
 
     
Suppose that we have n training data indicated 
by            and sampled from c classes. In 
our setting, c = 2, friends or non-friends.     
{   }          is the corresponding label. 
The total scatter matrix    and between class 
scatter matrix    are defined as follows.  
         ?             
           
         ?               
              
where ? is the mean of all samples,    is the 
mean of samples in the i-th class.    is the 
number of samples in the i-th class,   
            . 
           
              
G is the scaled label matrix. A well-known 
method to utilize discriminate information is to 
find a low dimensional subspace in which     is 
maximized while    is minimized (Fukunaga et 
al., 1990). So the object function could be easily 
written as follows  
    ( 
 (    
  ) )            
           
The optimization of the above object function 
was introduced in Yang et al (2012). It is an 
adaptation of iterative singular value 
decomposition. In GSM, a block-wise constraint 
is imposed on the diagonal matrix (D) which is 
the intermediate result of the iterative single 
value decomposition. 
      (
 
       
     
 
       
  ) 
W in the equation is the weight function,    is 
the ith feature group in W, and there are a total 
number of G sub diagonal matrices 
corresponding to G groups of features. 
     For acoustic features, Steidl et al, (2012) 
designed a grouping schema which consists of 
Prosodic Features, Voice Quality Features and 
Spectral features which we adopted. For visual 
features, based on our observation of the highly 
unstable performance of the 38 feature points of 
the face, we introduced group bondage for the 
entire group to prevent single face features over-
fitting the classifier. Detailed group information 
is shown in Table 1 and Table 2. 
5 Human Baseline 
 
Figure 2: Boxplot of human rating accuracy with 
respect to gender. 
In order to establish a baseline of the difficulty 
of predicting friendship, we conducted an 
experiment with humans, rating whether two 
people in a video were friends or not, after 
watching a 30-second video/audio clip taken 
from the first session of tutoring (in which the 
behaviors of strangers are most likely to be 
distinct from friends). We recruited 14 people 
and screened out participants with prior 
theoretical knowledge of nonverbal behavior, 
gesture, friendship, and rapport, or who rated all 
12 clips in under 8 minutes, leaving 10 
participants, half male, with an average age of 23 
(SD 4.8). Each participant was asked to watch 
one 30-second clip per dyad, taken from 3 
minutes after tutoring began. The mean accuracy 
of their friendship prediction was 0.717 (SD 
0.119), which is significantly lower than our best 
GSM model (trained on all three sessions) 
applied to those same 12 clips, with a 
55
performance of 0.837 (t(11) = -2.1381 p.<.05). 
When we split the ratings by gender, we found 
females on average were more accurate than 
males (see Figure 2). According to Hall et al, 
(1979) females are generally better decoders of 
nonverbal behaviors, which may lead to better 
judgment of friendship. 
6 Results: Models  
 
 Human NLM SLM GSM 
LV  0.743 0.768 0.792* 
LA  0.674 0.664 0.682* 
LV+DF  0.752 0.769 0.801* 
LA+DF  0.679 0.681 0.683 
LV+LA  0.744 0.780 0.803* 
LV+LA+DF  0.717 0.749 0.782 0.814# 
 
Table 3: The classification accuracy of the three 
algorithms on different features sets. Feature sets 
were combined with early fusion (+). Values marked 
* are significantly better (p<.05, pairwise t-test) than 
other results in the same row. Values marked # are 
significantly better (p<.001, pairwise t-test) than other 
results in the same column. 
Our group sparse model (GSM) along with the 
non-sparse linear model (NLM) and sparse linear 
model (SLM) were evaluated on different 
combinations of three sets of features: low-level 
vision features (LV), low-level audio features 
(LA) and dyadic features (DF), and their 
performance is presented in Table 3. We did not 
evaluate dyadic features (DF) alone due to their 
sparse nature. 
     In particular, we found that adding the 
automatically extracted DF to LV and LA with 
early fusion improved the performance (t(2258)= 
-3.12,p<.001) of the GSM model. When using 
fewer modalities, our newly proposed GSM 
outperformed NLM and SLM (t(2258)=-1.65, 
p<0.05). However, when the number of feature 
sets increased, there was no statistical difference 
in performance between GSM and the other two 
models. We suspect that when features are 
abundant, the information that the features 
provide reaches a ceiling. The advantage of the 
GSM was gained by mediating the noise and 
sparseness of the data, which resulted in better 
weight assignment for each feature. Alternatively, 
when features are abundant, even NLM can have 
a comparative weight assignment by performing 
a greedy high dimensional feature space search. 
Thus there is limited room for further 
improvement by better weight assignment among 
the group features which GSM assumes. 
    When we looked at the top features selected 
by NLM using the vision modality alone, two 
(out of 38) face features, which had an unstable 
nature, appeared high in rank, which suggests the 
possibility of NLM over-fitting the noise of these 
features. Surprisingly, when more modalities are 
added, NLM stops picking single face features as 
top informative features. In GSM, none of the 38 
face features are listed in the top ranked features 
for any of the modalities, which demonstrates its 
ability in noise mediation. 
In real world applications, data sets which 
produce ideal, abundant, and accurate features 
are rarely encountered. We often end up with 
data that are poor in video quality, e.g. with no 
split channels for each participant or no frontal 
face view. Our newly proposed GSM may 
therefore be more robust when features are noisy 
or certain modalities are not available.  
7 Results: Contributions of Features 
Feature Name Weight 
Number of Conversational Turns & 
Average Length of Turns 
0.041 
Gaze Down -0.036 
Mutual Gaze 0.014 
F0 0.013 
Non-mutual Gaze -0.013 
Voicing 0.014 
MFCC -0.007 
Non-mutual Smile 0.004 
Non-mutual Lean Forward 0.004 
Mutual Gaze followed by Mutual 
Smile 
0.001 
 
Table 4:  The top 10 informative features and their 
weights as trained by GSM. Positive weight is 
associated with friends while negative weight is 
associated with non-friends. 
After building the model and ranking the 
features, we looked into the weights learned for 
each feature. This weight comprises not only the 
magnitude, which tells us if the feature is 
important, but also the polarity. A detailed list of 
the most informative features and their weights is 
shown in Table 4.  
The strongest feature is number and length of 
conversational turns which is grouped in the 
table and should be interpreted as meaning that 
friends have more and shorter conversational 
turns. This is consistent with previous research 
on direction giving (Cassell et al, 2007), and 
mirrors the fact that friends are more likely to 
interrupt one another. 
We expected that unfamiliar participants, 
seated about two feet across from one another, 
would maintain a low level of eye contact 
(Argyle & Dean, 1965). In fact we found that 
non-friends tend to gaze down more often. In 
this context, non-friends spend more time 
56
looking down at their study materials. In turn, 
mutual gaze is higher among friends. 
Among the audio features, F0, which captures 
pitch related information such as range and 
mean, has been shown to differ between 
conversational and non-conversational speech 
(Bolinger, 1986). Here, friends show that more 
conversational style in their speech, despite the 
tutoring nature of the interaction.  
In order to further examine the lessons to be 
learned from this GSM model about verbal and 
nonverbal behavior in friends and strangers, we 
also ran a repeated measures ANOVA, including 
both gender and friendship status as factors. 
There were no significant effects for gender, 
however, and so that factor was collapsed for 
further analysis. The four features described 
above were all significantly different between 
friends and strangers (although gaze down was 
simply a trend, at p<.08). 
The following features were also important to 
the model, but did not show significance in the 
ANOVA, perhaps because of their sparse nature 
in our data. MFCC (Mel-Frequency Cepstral 
Coefficients) was associated with strangers and 
the similar audio feature of voicing was 
associated with friends. Both of these features 
have been described as approximating speech 
style ? voicing, for example, may indicate more 
backchannels, such as ?uh huh? and ?hmm? 
(Ward, 2006). 
     In Nakano et al (2003), listener gaze at the 
speaker is interpreted as evidence of non-
understanding. We found similar results whereby 
non-friends were more likely to engage in non-
mutual gaze ? looking at one another when the 
other person was not looking back.  Mutual smile 
did not distinguish between friends and non-
friends, while non-mutual smile, on the other 
hand, provided indicative strength, in spite of its 
sparse nature, for friendship. This may relate to 
our prior work (Ogan et al, 2012) which found 
significant teasing and other behavior whereby 
friends appear comfortable enjoying themselves 
at the expense of the other.  
    Mutual lean forward lacked predictive power 
in our model, while non-mutual lean forward 
was more salient between friends. We often 
found, for example, that friends maintained very 
different postures, with a tutor leaning back 
much of the time, leaning forward only to answer 
a direct question from the tutee. Non-friends, on 
the other hand, tended to remain fixed on the 
study material. This may have been a display of 
formality, where a casual attitude would have 
been perceived as either impolite or 
inappropriate. In either relationship state, the 
tutee tended to sit hunched over the worksheet, 
and since we did not enter tutor state into the 
model, this may have washed out some tutor-
specific results.  
     For the time contingent feature, mutual gaze 
followed by mutual smile is informative and 
predictive of friends. 
8 Error Analysis and Discussion 
Dyad  
ID 
LA+DF LV+DF LA+LV+DF 
     1 0.732 0.809 0.819 
     2* 0.703 0.793 0.804 
     3* 0.574 0.771 0.778 
     4* 0.713 0.708 0.762 
     5 0.653 0.879 0.880 
     6 0.728 0.827 0.835 
     7 0.624 0.873 0.882 
     8* 0.712 0.861 0.852 
     9* 0.698 0.820 0.830 
    10 0.606 0.834 0.854 
    11* 0.700 0.682 0.743 
    12 0.749 0.780 0.785 
 
Table 5: The average accuracy of classification in 
each dyad using the group sparse model (GSM) with 
different combination of feature sets. Dyads marked 
with * are friends 
We performed an error analysis to understand the 
contexts under which our model failed to 
accurately predict friendship states, and here we 
discuss the implications of these examples for 
our work. Table 5 shows the average accuracy of 
each dyad using audio, visual, and dyadic 
features to predict friendship. Dyads 2, 3, 4, 8, 9 
and 11 are friend dyads, and the rest are 
strangers.  
Dyad 3 (friends) showed very low accuracy in 
audio and dyadic features alone, which might be 
explained by the fact that in one early session for 
this dyad, most of the 30-second clips contain 
very sparse numbers of low-level audio features 
(LA). An examination of the audio recording 
reveals that one of the participants was more 
aggressive than in the other sessions. The student 
told his friend, ?Just be quiet?I am trying to 
work,? and ?Shh, you don?t understand, so I 
basically have to teach you how to work that, but 
I'm trying to work.? At this point in the 
interaction, his partner stopped participating in 
the task and said virtually nothing for the rest of 
the session. This lack of speech led to a lower 
number of turns ? a pattern with a closer 
resemblance to strangers than friends. 
It seems that such rude behavior would be 
more likely between friends than strangers, 
meaning that ultimately our model will need to 
57
be sensitive to this kind of variance. With more 
pairs of friends, different styles of friendship can 
be further distinguished. However, this specific 
phenomenon signals that in the future, lexical 
information which could be obtained by 
automatic speech recognition could further 
improve performance. 
Dyad 11 also showed low relative accuracy in 
predication, particularly when the model used 
vision features. We found that one of the 
participants often tilted her head, which partially 
blocked the frontal camera view of the other 
participant, thus resulting in less confidence in 
automatically extracted visual features. In the 
future we will set our cameras in a better position 
in order to reach higher feature extraction 
accuracy.  
When we combined all our features, the 
prediction accuracy of Dyad 3 and 11 improved, 
further demonstrating that multimodal 
information improves friendship modeling. 
9 Conclusion and Future Work 
As a first step towards predicting the state of 
friendship between two interlocutors, we 
analyzed a set of automatically harvested low-
level and dyadic features from dialogues in a 
peer-tutoring task. Both low level features and 
dyadic features were shown to be useful in 
discriminating between those who are friends 
and those who are not.  
     To perform the analysis, we introduced a new 
computational group sparse model (GSM) in 
order to accommodate the sparse and noisy 
properties of multi-channel features. GSM 
outperformed a baseline of human raters who 
make these types of social judgments in 
everyday interactions. GSM also statistically 
outperformed a non-sparse linear model (NLM) 
and a sparse linear model (SLM) when the 
analysis used only a single set of low level 
features or single set of low level features 
combined with dyadic features. When all 
features were used, the distinctions between 
models decreased, since in a huge multimodal 
feature space, even a na?ve model could greedy 
search for a good weight assignment. Thus our 
newly proposed model did not significantly 
outperform the others in this scenario. And in 
general, more features produced more accurate 
prediction. 
    Based on the outcomes of the GSM model, we 
investigated differences between verbal and 
nonverbal behavior cues as a function of 
different friendship states. While much research 
on rapport detection and building in ECAs has 
focused on low level features, we found that 
dyadic features provided some of the most 
distinguishing differences between friends and 
non-friends. For example, mutual gaze and non-
mutual gaze were both indicative, as friends are 
comfortable looking directly at one another while 
non-friends may have used direct gaze only to 
signal non-understanding. This comfort between 
friends was also notable in other salient dyadic 
features; i.e., while non-friends often work in 
concert looking down at the task, friends were 
relaxed such that one partner could lean back, 
interrupt to take more conversational turns, and 
smile at the other without needing to reciprocate 
the smile each time. 
In future work we will look at temporal 
contingency more closely, examining whether 
participants? actions are contingent on the 
behavior of their partner. We will also examine 
whether the behavior of friends and strangers 
changes over multiple sessions. In this context, 
we include one suggestive graph, which shows 
that strangers increase their mutual gaze over 
sessions but friends decrease it. We are currently 
collecting further sessions for each dyad so as to 
be able to further analyze the nature of these 
relationships over time. 
 
Figure 3: Weight of the mutual gaze in each 
session, by friendship status 
 
To date we have found that the inclusion of 
automatically extracted dyadic features can lead 
to better prediction of friendship state. Both 
verbal and nonverbal behaviors were discovered 
that distinguish between different friendship 
status and that suggest how to design embodied 
dialogue systems that intend to spend a lifetime 
on the job. 
Acknowledgements 
Thanks to Angela Ng, Rachel Marino and Marissa 
Cross for data collection, Giota Stratou for visual 
feature extraction, Yi Yang, Louis-Philippe Morency, 
Shoou-I Yu, William Wang, and Eric Xing for 
valuable discussions, and the NSF IIS for generous 
funding. 
References  
Ambady, N., Krabbenhoft, M. A. & Hogan, D. 
(2006). The 30-sec sale: Using thin-slice 
0
2
4
6
1 2 3
Session Number 
Mutual gaze per clip 
strangers
friends
58
judgments to evaluate sales effectiveness. 
Journal of Consumer Psychology, 16(1), 4?13. 
Argyle, M. & Dean, J. (1965). Eye-contact, distance 
and affiliation. Sociometry, 28(3), 289?304. 
Azmitia, M. & Montgomery, R. (1993). Friendship, 
transactive dialogues, and the development of 
scientific reasoning. Social Development, 2(3), 
202?221.  
Baker, R. E., Gill, A. J., & Cassell, J. (2008). Reactive 
redundancy and listener comprehension in 
direction-giving. In Proceedings of the 9th 
SIGdial Workshop on Discourse and Dialogue 
(pp. 37?45). 
Brooks, M. (1989). Instant rapport (p. 205). New 
York: Warner Books. 
Berg, B. L. (1989). Qualitative research methods for 
the social sciences. Boston: Allyn and Bacon. 
Bernieri, F. J. (1988). Coordinated movement and 
rapport in teacher-student interactions. Journal 
of Nonverbal Behavior, 12(2), 120?138. 
Bickmore, T. W. & Picard, R. W. (2005). Establishing 
and maintaining long-term human-computer 
relationships. ACM Transactions on Computer-
Human Interaction, 12(2), 293?327. 
Bickmore, T. W., Pfeifer, L., & Schuman, D. (2011). 
Relational agents improve engagement and 
learning in science museum visitors. In 
Intelligent Virtual Agents (pp. 55?67). 
Reykjavik. 
Bolinger, D. (1986). Intonation and its parts: Melody 
in spoken English. Stanford, CA: Stanford 
University Press. 
Burgoon, J. K. & Hale, J. L. (1988). Nonverbal 
expectancy violations: Model elaboration and 
application to immediacy behaviors. 
Communications Monographs, (May 2013), 37?
41. 
Cappella, J. N.  & Pelachaud, C. (2002). Rules for 
responsive robots: Using human interactions to 
build virtual interactions. In Reis, Firzpatrick, & 
Vangelisti (Eds.), Stability and change in 
relationships. New York, NY: Cambridge 
University Press. 
Cassell, J. (2000). Nudge nudge wink wink: Elements 
of face-to-face conversation for embodied 
conversational agents. In J. Cassell, J. Sullivan, 
S. Prevost, & E. Churchill (Eds.), Embodied 
conversational agents (pp. 1?27). MIT Press. 
Cassell, J., Gill, A. J., & Tepper, P. A. (2007). 
Coordination in conversation and rapport. 
Proceedings of the ACL Workshop on Embodied 
Natural Language, 40 ?50. 
Cassell, J., Bickmore, T. W., Campbell, L., 
Vilhj?lmsson, H. H., & Yan, H. (2001). More 
than just a pretty face: Conversational protocols 
and the affordances of embodiment. Knowledge-
Based Systems, 14(1-2), 55?64. 
Cassell, J. & Bickmore, T. W. (2003). Negotiated 
collusion: Modeling social language and its 
relationship effects in intelligent agents. User 
Modeling and User-Adapted Interaction, 13(1), 
89?132. 
Chen, X., Lin, Q., Kim, S., Carbonell, J. G., & Xing, 
E. P. (2012). Smoothing proximal gradient 
method for general structured sparse regression. 
The Annals of Applied Statistics, 6(2), 719?752. 
Cortes, C. & Vapnik, V. (1995). Support-vector 
networks. Machine Learning, 20(3), 273?297. 
Fan, R., Chang, K., Hsieh, C., Wang, X., & Lin, C. 
(2008). LIBLINEAR: A library for large linear 
classification. The Journal of Machine Learning 
Research, 9, 1871?1874. 
Fantuzzo, J., Riggio, R., Connelly, S., & Dimeff, L. 
(1989). Effects of reciprocal peer tutoring on 
academic achievement and psychological 
adjustment: A component analysis. Journal of 
Educational Psychology, 81(2), 173-177. 
Fukunaga, K. (1990). Introduction to Statistical 
Pattern Recognition, Second Edition (2nd ed., p. 
592). San Diego, CA: Academic Press. 
Gatica-Perez, D. (2009). Automatic nonverbal 
analysis of social interaction in small groups: A 
review. Image and Vision Computing, 27(12), 
1775?1787. 
Granovetter, M. S. (1973). The strength of weak ties. 
American Journal of Sociology, 78(6), 1360?
1380.  
Gratch, J., Okhmatovskaia, A., Lamothe, F., Marsella, 
S.,  Morales, M., van der Werf, R. J., & 
Morency, L.-P. (2006). Virtual rapport. In 
Intelligent Virtual Agents (pp. 14?27). Springer 
Berlin/Heidelberg. 
Hall, J. A., DiMatteo, M. R., Rogers, P. L., & Archer, 
D. (1979). Sensitivity to nonverbal 
communication: The PONS test. Baltimore, MD: 
Johns Hopkins University Press. 
Harrigan, J. A., Oxman, T. E., & Rosenthal, R. 
(1985). Rapport expressed through nonverbal 
behavior. Journal of Nonverbal Behavior, 9, 
95?110. 
Harrigan, J. A. & Rosenthal, R. (1983). Physicians? 
head and body positions as determinants of 
perceived rapport. Applied Social Psychology, 
13(6), 496?509. 
Liu, J., Ji, S., & Ye, J. (2009). Multi-task feature 
learning via efficient l2, 1-norm minimization. 
In Proceedings of the Twenty-Fifth Conference 
on Uncertainty in Artificial Intelligence (pp. 
339?348). AUAI Press. 
Nakano, Y. I., Reinstein, G., Stocky, T., & Cassell, J. 
(2003). Towards a model of face-to-face 
grounding. In Proceedings of the 41st Annual 
Meeting on Association for Computational 
Linguistics. ACL?03 (Vol. 1, pp. 553?561). 
Sapporo: Association for Computational 
Linguistics. 
Nakano, Y. I. & Ishii, R. (2010). Estimating user?s 
engagement from eye-gaze behaviors in human-
agent conversations. In Proceedings of the 15th 
international conference on Intelligent user 
59
interfaces. IUI?10 (pp. 139?148). Hong Kong: 
ACM Press. 
Ogan, A., Finkelstein, S., Walker, E., Carlson, R., & 
Cassell, J. (2012). Rudeness and rapport: Insults 
and learning gains in peer tutoring. In 
Proceedings of the 11 International Conference 
on Intelligence Tutoring Systems (ITS 2012). 
Prepin, K., Ochs, M., & Pelachaud, C. (2012). Mutual 
stance building in dyad of virtual agents: Smile 
alignment and synchronisation. In Privacy, 
Security, Risk and Trust (PASSAT), 2012 
International Conference on Social Computing 
(SocialCom) (pp. 938?943). 
Schuller, B., Steidl, S., Batliner, A., N?th, E., 
Vinciarelli, A., Burkhardt, F., ? Weiss, B. 
(2012). The INTERSPEECH 2012 speaker trait 
challenge. In Proceedings of the 13th Annual 
Conference of the International Speech 
Communication Association (INTERSPEECH 
2012). Portland, OR: ISCA. 
Steidl, S., Polzehl, T., Bunnell, H. T., Dou, Y., 
Muthukumar, P. K., Perry, ? Metze, F. (2012). 
Emotion identification for evaluation of 
synthesized emotional speech. In Proceedings of 
the 6th International Conference on Speech 
Prosody 2012 (pp. 4?7). Shanghai: Tongji 
University Press. 
Stronks, B., Nijholt, A., van Der Vet, P., Heylen, D., 
& Machado, A. (2002). Designing for 
friendship: Becoming friends with your ECA. In 
A. Marriott, C. Pelachaud, T. Rist, Z. M. 
Ruttkay, & H. Vilhjalmsson (Eds.), Workshop 
on Embodied Conversational Agents - Let?s 
specify and evaluate them!, AMAAS 2002 (pp. 
91?96). Bologna: AMAAS. 
Tickle-Degnen, L. & Rosenthal, R. (1990). The nature 
of rapport and its nonverbal correlates. 
Psychological Inquiry, 1(4), 285?293. 
Vardoulakis, L. P., Ring, L., Barry, B., Sidner, C. L., 
& Bickmore, T. W. (2012). Designing relational 
agents as long term social companions for older 
adults. In Y. Nakano, M. Neff, A. Paiva, & M. 
Walker (Eds.), Intelligent Virtual Agents (pp. 
289?302). Santa Cruz, CA: Springer Berlin 
Heidelberg. 
Vinciarelli, A., Pantic, A., Bourlard, H. (2009) Social 
signal processing: Survey of an emerging 
domain. Image and Vision Computing, (27)12, 
1743-1759. 
Ward, N. (2006). Non-Lexical Conversational Sounds 
in American English. Pragmatics and 
Cognition, (14)1, 113-184. 
Wang, W. Y., Finkelstein, S., Ogan, A., Black, A. W., 
& Cassell, J. (2012). ?Love ya, jerkface?: Using 
sparse log-linear models to build positive (and 
impolite) relationships with teens. In 
Proceedings of the 13th Annual SIGDIAL 
Meeting on Discourse and Dialogue (pp. 20?
29). Seoul, South Korea. 
Yang, Y., Shen, H. T., Ma, Z., Huang, Z., & Zhou, X. 
(2010). l2,1-regularized discriminative feature 
selection for unsupervised learning. In 
Proceedings of the Twenty-Second International 
Joint Conference on Artificial Intelligence (pp. 
1589?1594). AAAI Press.  
Youniss, J. (1982). Parents and peers in social 
development: A Sullivan-Piaget perspective. 
University of Chicago Press. 
Yuan, M. & Lin, Y. (2007), Model selection and 
estimation in regression with grouped variables. 
Journal of the Royal Statistical Society, Series B 
68(1), 49-67. 
 
60
