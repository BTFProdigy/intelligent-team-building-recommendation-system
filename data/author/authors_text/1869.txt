107
108
109
110
111
112
113
114
Proceedings of the 43rd Annual Meeting of the ACL, pages 322?329,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Detecting Errors in Discontinuous Structural Annotation
Markus Dickinson
Department of Linguistics
The Ohio State University
dickinso@ling.osu.edu
W. Detmar Meurers
Department of Linguistics
The Ohio State University
dm@ling.osu.edu
Abstract
Consistency of corpus annotation is an
essential property for the many uses of
annotated corpora in computational and
theoretical linguistics. While some re-
search addresses the detection of inconsis-
tencies in positional annotation (e.g., part-
of-speech) and continuous structural an-
notation (e.g., syntactic constituency), no
approach has yet been developed for au-
tomatically detecting annotation errors in
discontinuous structural annotation. This
is significant since the annotation of po-
tentially discontinuous stretches of ma-
terial is increasingly relevant, from tree-
banks for free-word order languages to se-
mantic and discourse annotation.
In this paper we discuss how the variation
n-gram error detection approach (Dickin-
son and Meurers, 2003a) can be extended
to discontinuous structural annotation. We
exemplify the approach by showing how it
successfully detects errors in the syntactic
annotation of the German TIGER corpus
(Brants et al, 2002).
1 Introduction
Annotated corpora have at least two kinds of uses:
firstly, as training material and as ?gold standard?
testing material for the development of tools in com-
putational linguistics, and secondly, as a source of
data for theoretical linguists searching for analyti-
cally relevant language patterns.
Annotation errors and why they are a problem
The high quality annotation present in ?gold stan-
dard? corpora is generally the result of a manual
or semi-automatic mark-up process. The annota-
tion thus can contain annotation errors from auto-
matic (pre-)processes, human post-editing, or hu-
man annotation. The presence of errors creates prob-
lems for both computational and theoretical linguis-
tic uses, from unreliable training and evaluation of
natural language processing technology (e.g., van
Halteren, 2000; Kve?to?n and Oliva, 2002, and the
work mentioned below) to low precision and recall
of queries for already rare linguistic phenomena. In-
vestigating the quality of linguistic annotation and
improving it where possible thus is a key issue for
the use of annotated corpora in computational and
theoretical linguistics.
Illustrating the negative impact of annotation er-
rors on computational uses of annotated corpora,
van Halteren et al (2001) compare taggers trained
and tested on the Wall Street Journal (WSJ, Marcus
et al, 1993) and the Lancaster-Oslo-Bergen (LOB,
Johansson, 1986) corpora and find that the results for
the WSJ perform significantly worse. They report
that the lower accuracy figures are caused by incon-
sistencies in the WSJ annotation and that 44% of the
errors for their best tagging system were caused by
?inconsistently handled cases.?
Turning from training to evaluation, Padro and
Marquez (1998) highlight the fact that the true ac-
curacy of a classifier could be much better or worse
than reported, depending on the error rate of the cor-
pus used for the evaluation. Evaluating two taggers
on the WSJ, they find tagging accuracy rates for am-
322
biguous words of 91.35% and 92.82%. Given the
estimated 3% error rate of the WSJ tagging (Marcus
et al, 1993), they argue that the difference in perfor-
mance is not sufficient to establish which of the two
taggers is actually better.
In sum, corpus annotation errors, especially er-
rors which are inconsistencies, can have a profound
impact on the quality of the trained classifiers and
the evaluation of their performance. The problem is
compounded for syntactic annotation, given the dif-
ficulty of evaluating and comparing syntactic struc-
ture assignments, as known from the literature on
parser evaluation (e.g., Carroll et al, 2002).
The idea that variation in annotation can indicate
annotation errors has been explored to detect errors
in part-of-speech (POS) annotation (van Halteren,
2000; Eskin, 2000; Dickinson and Meurers, 2003a)
and syntactic annotation (Dickinson and Meurers,
2003b). But, as far as we are aware, the research
we report on here is the first approach to error detec-
tion for the increasing number of annotations which
make use of more general graph structures for the
syntactic annotation of free word order languages or
the annotation of semantic and discourse properties.
Discontinuous annotation and its relevance The
simplest kind of annotation is positional in nature,
such as the association of a part-of-speech tag with
each corpus position. On the other hand, struc-
tural annotation such as that used in syntactic tree-
banks (e.g., Marcus et al, 1993) assigns a syntactic
category to a contiguous sequence of corpus posi-
tions. For languages with relatively free constituent
order, such as German, Dutch, or the Slavic lan-
guages, the combinatorial potential of the language
encoded in constituency cannot be mapped straight-
forwardly onto the word order possibilities of those
languages. As a consequence, the treebanks that
have been created for German (NEGRA, Skut et al,
1997; VERBMOBIL, Hinrichs et al, 2000; TIGER,
Brants et al, 2002) have relaxed the requirement that
constituents have to be contiguous. This makes it
possible to syntactically annotate the language data
as such, i.e., without requiring postulation of empty
elements as placeholders or other theoretically mo-
tivated changes to the data. We note in passing that
discontinuous constituents have also received some
support in theoretical linguistics (cf., e.g., the arti-
cles collected in Huck and Ojeda, 1987; Bunt and
van Horck, 1996).
Discontinuous constituents are strings of words
which are not necessarily contiguous, yet form a
single constituent with a single label, such as the
noun phrase Ein Mann, der lacht in the German rel-
ative clause extraposition example (1) (Brants et al,
2002).1
(1) Ein
a
Mann
man
kommt
comes
,
,
der
who
lacht
laughs
?A man who laughs comes.?
In addition to their use in syntactic annotation,
discontinuous structural annotation is also rele-
vant for semantic and discourse-level annotation?
essentially any time that graph structures are needed
to encode relations that go beyond ordinary tree
structures. Such annotations are currently employed
in the mark-up for semantic roles (e.g., Kings-
bury et al, 2002) and multi-word expressions (e.g.,
Rayson et al, 2004), as well as for spoken language
corpora or corpora with multiple layers of annota-
tion which cross boundaries (e.g., Blache and Hirst,
2000).
In this paper, we present an approach to the de-
tection of errors in discontinuous structural annota-
tion. We focus on syntactic annotation with poten-
tially discontinuous constituents and show that the
approach successfully deals with the discontinuous
syntactic annotation found in the TIGER treebank
(Brants et al, 2002).
2 The variation n-gram method
Our approach builds on the variation n-gram al-
gorithm introduced in Dickinson and Meurers
(2003a,b). The basic idea behind that approach is
that a string occurring more than once can occur
with different labels in a corpus, which we refer to as
variation. Variation is caused by one of two reasons:
i) ambiguity: there is a type of string with multiple
possible labels and different corpus occurrences of
that string realize the different options, or ii) error:
the tagging of a string is inconsistent across compa-
rable occurrences.
1The ordinary way of marking a constituent with brack-
ets is inadequate for discontinuous constituents, so we instead
boldface and underline the words belonging to a discontinuous
constituent.
323
The more similar the context of a variation, the
more likely the variation is an error. In Dickin-
son and Meurers (2003a), contexts are composed
of words, and identity of the context is required.
The term variation n-gram refers to an n-gram (of
words) in a corpus that contains a string annotated
differently in another occurrence of the same n-gram
in the corpus. The string exhibiting the variation is
referred to as the variation nucleus.
2.1 Detecting variation in POS annotation
In Dickinson and Meurers (2003a), we explore this
idea for part-of-speech annotation. For example, in
the WSJ corpus the string in (2) is a variation 12-
gram since off is a variation nucleus that in one cor-
pus occurrence is tagged as a preposition (IN), while
in another it is tagged as a particle (RP).2
(2) to ward off a hostile takeover attempt by two
European shipping concerns
Once the variation n-grams for a corpus have
been computed, heuristics are employed to classify
the variations into errors and ambiguities. The first
heuristic encodes the basic fact that the label assign-
ment for a nucleus is dependent on the context: vari-
ation nuclei in long n-grams are likely to be errors.
The second takes into account that natural languages
favor the use of local dependencies over non-local
ones: nuclei found at the fringe of an n-gram are
more likely to be genuine ambiguities than those oc-
curring with at least one word of surrounding con-
text. Both of these heuristics are independent of a
specific corpus, annotation scheme, or language.
We tested the variation error detection method on
the WSJ and found 2495 distinct3 nuclei for the vari-
ation n-grams between the 6-grams and the 224-
grams. 2436 of these were actual errors, making for
a precision of 97.6%, which demonstrates the value
of the long context heuristic. 57 of the 59 genuine
ambiguities were fringe elements, confirming that
fringe elements are more indicative of a true ambi-
guity.
2To graphically distinguish the variation nucleus within a
variation n-gram, the nucleus is shown in grey.
3Being distinct means that each corpus position is only taken
into account for the longest variation n-gram it occurs in.
2.2 Detecting variation in syntactic annotation
In Dickinson and Meurers (2003b), we decompose
the variation n-gram detection for syntactic annota-
tion into a series of runs with different nucleus sizes.
This is needed to establish a one-to-one relation be-
tween a unit of data and a syntactic category annota-
tion for comparison. Each run detects the variation
in the annotation of strings of a specific length. By
performing such runs for strings from length 1 to
the length of the longest constituent in the corpus,
the approach ensures that all strings which are ana-
lyzed as a constituent somewhere in the corpus are
compared to the annotation of all other occurrences
of that string.
For example, the variation 4-gram from a year
earlier appears 76 times in the WSJ, where the nu-
cleus a year is labeled noun phrase (NP) 68 times,
and 8 times it is not annotated as a constituent and
is given the special label NIL. An example with
two syntactic categories involves the nucleus next
Tuesday as part of the variation 3-gram maturity
next Tuesday, which appears three times in the WSJ.
Twice it is labeled as a noun phrase (NP) and once as
a prepositional phrase (PP).
To be able to efficiently calculate all variation nu-
clei of a treebank, in Dickinson and Meurers (2003b)
we make use of the fact that a variation necessar-
ily involves at least one constituent occurrence of
a nucleus and calculate the set of nuclei for a win-
dow of length i by first finding the constituents of
that length. Based on this set, we then find non-
constituent occurrences of all strings occurring as
constituents. Finally, the variation n-grams for these
variation nuclei are obtained in the same way as for
POS annotation.
In the WSJ, the method found 34,564 variation
nuclei, up to size 46; an estimated 71% of the 6277
non-fringe distinct variation nuclei are errors.
3 Discontinuous constituents
In Dickinson and Meurers (2003b), we argued that
null elements need to be ignored as variation nuclei
because the variation in the annotation of a null el-
ement as the nucleus is largely independent of the
local environment. For example, in (3) the null el-
ement *EXP* (expletive) can be annotated a. as a
sentence (S) or b. as a relative/subordinate clause
324
(SBAR), depending on the properties of the clause
it refers to.
(3) a. For cities losing business to suburban shop-
ping centers , it *EXP* may be a wise busi-
ness investment [S * to help * keep those
jobs and sales taxes within city limits] .
b. But if the market moves quickly enough , it
*EXP* may be impossible [SBAR for the
broker to carry out the order] because the in-
vestment has passed the specified price .
We found that removing null elements as variation
nuclei of size 1 increased the precision of error de-
tection to 78.9%.
Essentially, null elements represent discontinu-
ous constituents in a formalism with a context-free
backbone (Bies et al, 1995). Null elements are co-
indexed with a non-adjacent constituent; in the pred-
icate argument structure, the constituent should be
interpreted where the null element is.
To be able to annotate discontinuous material
without making use of inserted null elements, some
treebanks have instead relaxed the definition of a lin-
guistic tree and have developed more complex graph
annotations. An error detection method for such cor-
pora thus does not have to deal with the problems
arising from inserted null elements discussed above,
but instead it must function appropriately even if
constituents are discontinuously realized.
A technique such as the variation n-gram method
is applicable to corpora with a one-to-one map-
ping between the text and the annotation. For
corpora with positional annotation?e.g., part-of-
speech annotated corpora?the mapping is triv-
ial given that the annotation consists of one-to-
one correspondences between words (i.e., tokens)
and labels. For corpora annotated with more
complex structural information?e.g., syntactically-
annotated corpora?the one-to-one mapping is ob-
tained by considering every interval (continuous
string of any length) which is assigned a category
label somewhere in the corpus.
While this works for treebanks with continuous
constituents, a one-to-one mapping is more com-
plicated to establish for syntactic annotation involv-
ing discontinuous constituents (NEGRA, Skut et al,
1997; TIGER, Brants et al, 2002). In order to apply
the variation n-gram method to discontinuous con-
stituents, we need to develop a technique which is
capable of comparing labels for any set of corpus
positions, instead of for any interval.
4 Extending the variation n-gram method
To extend the variation n-gram method to handle
discontinuous constituents, we first have to define
the characteristics of such a constituent (section 4.1),
in other words our units of data for comparison.
Then, we can find identical non-constituent (NIL)
strings (section 4.2) and expand the context into
variation n-grams (section 4.3).
4.1 Variation nuclei: Constituents
For traditional syntactic annotation, a variation nu-
cleus is defined as a contiguous string with a sin-
gle label; this allows the variation n-gram method
to be broken down into separate runs, one for each
constituent size in the corpus. For discontinuous
syntactic annotation, since we are still interested in
comparing cases where the nucleus is the same, we
will treat two constituents as having the same size if
they consist of the same number of words, regard-
less of the amount of intervening material, and we
can again break the method down into runs of differ-
ent sizes. The intervening material is accounted for
when expanding the context into n-grams.
A question arises concerning the word order of
elements in a constituent. Consider the German ex-
ample (4) (Mu?ller, 2004).
(4) weil
because
der
the
Mann
mannom
der
the
Frau
womandat
das
the
Buch
bookacc
gab.
gave
?because the man gave the woman the book.?
The three arguments of the verb gab (?give?) can be
permuted in all six possible ways and still result in a
well-formed sentence. It might seem, then, that we
would want to allow different permutations of nuclei
to be treated as identical. If das Buch der Frau gab
is a constituent in another sentence, for instance, it
should have the same category label as der Frau das
Buch gab.
Putting all permutations into one equivalence
class, however, amounts to stating that all order-
325
ings are always the same. But even ?free word or-
der? languages are more appropriately called free
constituent order; for example, in (4), the argument
noun phrases can be freely ordered, but each argu-
ment noun phrase is an atomic unit, and in each unit
the determiner precedes the noun.
Since we want our method to remain data-driven
and order can convey information which might be
reflected in an annotation system, we keep strings
with different orders of the same words distinct, i.e.,
ordering of elements is preserved in our method.
4.2 Variation nuclei: Non-constituents
The basic idea is to compare a string annotated as a
constituent with the same string found elsewhere?
whether annotated as a constituent or not. So we
need to develop a method for finding all string oc-
currences not analyzed as a constituent (and assign
them the special category label NIL). Following
Dickinson and Meurers (2003b), we only look for
non-constituent occurrences of those strings which
also occur at least once as a constituent.
But do we need to look for discontinuous NIL
strings or is it sufficient to assume only continuous
ones? Consider the TIGER treebank examples (5).
(5) a. in
on
diesem
this
Punkt
point
seien
are
sich
SELF
Bonn
Bonn
und
and
London
London
nicht
not
einig
agreed
.
.
?Bonn and London do not agree on this point.?
b. in
on
diesem
this
Punkt
point
seien
are
sich
SELF
Bonn
Bonn
und
and
London
London
offensichtlich
clearly
nicht einig
not agreed
.
.
In example (5a), sich einig (?SELF agree?) forms
an adjective phrase (AP) constituent. But in ex-
ample (5b), that same string is not analyzed as a
constituent, despite being in a nearly identical sen-
tence. We would thus like to assign the discontinu-
ous string sich einig in (5b) the label NIL, so that the
labeling of this string in (5a) can be compared to its
occurrence in (5b).
In consequence, our approach should be able to
detect NIL strings which are discontinuous?an is-
sue which requires special attention to obtain an al-
gorithm efficient enough to handle large corpora.
Use sentence boundary information The first
consideration makes use of the fact that syntactic an-
notation by its nature respects sentence boundaries.
In consequence, we never need to search for NIL
strings that span across sentences.4
Use tries to store constituent strings The sec-
ond consideration concerns how we calculate the
NIL strings. To find every non-constituent string in
the corpus, discontinuous or not, which is identical
to some constituent in the corpus, a basic approach
would first generate all possible strings within a sen-
tence and then test to see which ones occur as a
constituent elsewhere in the corpus. For example,
if the sentence is Nobody died when Clinton lied, we
would see if any of the 31 subsets of strings occur
as constituents (e.g., Nobody, Nobody when, Clin-
ton lied, Nobody when lied, etc.). But such a gener-
ate and test approach clearly is intractable given that
it generates generates 2n? 1 potential matches for a
sentence of n words.
We instead split the task of finding NIL strings into
two runs through the corpus. In the first, we store
all constituents in the corpus in a trie data structure
(Fredkin, 1960), with words as nodes. In the sec-
ond run through the corpus, we attempt to match the
strings in the corpus with a path in the trie, thus iden-
tifying all strings occurring as constituents some-
where in the corpus.
Filter out unwanted NIL strings The final con-
sideration removes ?noisy? NIL strings from the can-
didate set. Certain NIL strings are known to be use-
less for detecting annotation errors, so we should re-
move them to speed up the variation n-gram calcu-
lations. Consider example (6) from the TIGER cor-
pus, where the continuous constituent die Menschen
is annotated as a noun phrase (NP).
(6) Ohne
without
diese
these
Ausgaben,
expenses
so
according to
die
the
Weltbank,
world bank
seien
are
die Menschen
the people
totes
dead
Kapital
capital
?According to the world bank, the people are dead capital
without these expenses.?
4This restriction clearly is syntax specific and other topo-
logical domains need to be identified to make searching for NIL
strings tractable for other types of discontinuous annotation.
326
Our basic method of finding NIL strings would de-
tect another occurrence of die Menschen in the same
sentence since nothing rules out that the other occur-
rence of die in the sentence (preceding Weltbank)
forms a discontinuous NIL string with Menschen.
Comparing a constituent with a NIL string that con-
tains one of the words of the constituent clearly goes
against the original motivation for wanting to find
discontinuous strings, namely that they show varia-
tion between different occurrences of a string.
To prevent such unwanted variation, we eliminate
occurrences of NIL-labeled strings that overlap with
identical constituent strings from consideration.
4.3 Variation n-grams
The more similar the context surrounding a varia-
tion nucleus, the more likely it is for a variation in
its annotation to be an error. For detecting errors in
traditional syntactic annotation (see section 2.2), the
context consists of the elements to the left and the
right of the nucleus. When nuclei can be discontinu-
ous, however, there can also be internal context, i.e.,
elements which appear between the words forming
a discontinuous variation nucleus.
As in our earlier work, an instance of the a pri-
ori algorithm is used to expand a nucleus into a
longer n-gram by stepwise adding context elements.
Where previously it was possible to add an element
to the left or the right, we now also have the option of
adding it in the middle?as part of the new, internal
context. But depending on how we fill in the internal
context, we can face a serious tractability problem.
Given a nucleus with j gaps within it, we need to
potentially expand it in j + 2 directions, instead of
in just 2 directions (to the right and to the left).
For example, the potential nucleus was werden
appears as a verb phrase (VP) in the TIGER corpus in
the string was ein Seeufer werden; elsewhere in the
corpus was and werden appear in the same sentence
with 32 words between them. The chances of one of
the middle 32 elements matching something in the
internal context of the VP is relatively high, and in-
deed the twenty-sixth word is ein. However, if we
move stepwise out from the nucleus in order to try
to match was ein Seeufer werden, the only options
are to find ein directly to the right of was or Seeufer
directly to the left of werden, neither of which oc-
curs, thus stopping the search.
In conclusion, we obtain an efficient application
of the a priori algorithm by expanding the context
only to elements which are adjacent to an element
already in the n-gram. Note that this was already
implicitly assumed for the left and the right context.
There are two other efficiency-related issues
worth mentioning. Firstly, as with the variation nu-
cleus detection, we limit the n-grams expansion to
sentences only. Since the category labels do not rep-
resent cross-sentence dependencies, we gain no new
information if we find more context outside the sen-
tence, and in terms of efficiency, we cut off what
could potentially be a very large search space.5
Secondly, the methods for reducing the number
of variation nuclei discussed in section 4.2 have the
consequence of also reducing the number of possi-
ble variation n-grams. For example, in a test run
on the NEGRA corpus we allowed identical strings
to overlap; this generated a variation nucleus of size
63, with 16 gaps in it, varying between NP and NIL
within the same sentence. Fifteen of the gaps can be
filled in and still result in variation. The filter for un-
wanted NIL strings described in the previous section
eliminates the NIL value from consideration. Thus,
there is no variation and no tractability problem in
constructing n-grams.
4.3.1 Generalizing the n-gram context
So far, we assumed that the context added around
variation nuclei consists of words. Given that tree-
banks generally also provide part-of-speech infor-
mation for every token, we experimented with part-
of-speech tags as a less restrictive kind of context.
The idea is that it should be possible to find more
variation nuclei with comparable contexts if only the
part-of-speech tags of the surrounding words have to
be identical instead of the words themselves.
As we will see in section 5, generalizing n-gram
contexts in this way indeed results in more variation
n-grams being found, i.e., increased recall.
4.4 Adapting the heuristics
To determine which nuclei are errors, we can build
on the two heuristics from previous research (Dick-
5Note that similar sentences which were segmented differ-
ently could potentially cause varying n-gram strings not to be
found. We propose to treat this as a separate sentence segmen-
tation error detection phase in future work.
327
inson and Meurers, 2003a,b)?trust long contexts
and distrust the fringe?with some modification,
given that we have more fringe areas to deal with
for discontinuous strings. In addition to the right
and the left fringe, we also need to take into account
the internal context in a way that maintains the non-
fringe heuristic as a good indicator for errors. As
a solution that keeps internal context on a par with
the way external context is treated in our previous
work, we require one word of context around every
terminal element that is part of the variation nucleus.
As discussed below, this heuristic turns out to be a
good predictor of which variations are annotation er-
rors; expanding to the longest possible context, as in
Dickinson and Meurers (2003a), is not necessary.
5 Results on the TIGER Corpus
We ran the variation n-grams error detection method
for discontinuous syntactic constituents on v. 1 of
TIGER (Brants et al, 2002), a corpus of 712,332
tokens in 40,020 sentences. The method detected
a total of 10,964 variation nuclei. From these we
sampled 100 to get an estimate of the number of er-
rors in the corpus which concern variation. Of these
100, 13 variation nuclei pointed to an error; with this
point estimate of .13, we can derive a 95% confi-
dence interval of (0.0641, 0.1959),6 which means
that we are 95% confident that the true number of
variation-based errors is between 702 and 2148. The
effectiveness of a method which uses context to nar-
row down the set of variation nuclei can be judged
by how many of these variation errors it finds.
Using the non-fringe heuristic discussed in the
previous section, we selected the shortest non-fringe
variation n-grams to examine. Occurrences of the
same strings within larger n-grams were ignored, so
as not to artificially increase the resulting set of n-
grams.
When the context is defined as identical words,
we obtain 500 variation n-grams. Sampling 100 of
these and labeling for each position whether it is an
error or an ambiguity, we find that 80 out of the 100
samples point to at least one token error. The 95%
confidence interval for this point estimate of .80 is
6The 95% confidence interval was calculated using the stan-
dard formula of p?1.96
q
p(1?p)
n , where p is the point estimate
and n the sample size.
(0.7216, 0.8784), so we are 95% confident that the
true number of error types is between 361 and 439.
Note that this precision is comparable to the esti-
mates for continuous syntactic annotation in Dick-
inson and Meurers (2003b) of 71% (with null ele-
ments) and 78.9% (without null elements).
When the context is defined as identical parts of
speech, as described in section 4.3.1, we obtain 1498
variation n-grams. Again sampling 100 of these, we
find that 52 out of the 100 point to an error. And
the 95% confidence interval for this point estimate
of .52 is (0.4221, 0.6179), giving a larger estimated
number of errors, between 632 and 926.
Context Precision Errors
Word 80% 361?439
POS 52% 632?926
Figure 1: Accuracy rates for the different contexts
Words convey more information than part-of-
speech tags, and so we see a drop in precision when
using part-of-speech tags for context, but these re-
sults highlight a very practical benefit of using a
generalized context. By generalizing the context, we
maintain a precision rate of approximately 50%, and
we substantially increase the recall of the method.
There are, in fact, likely twice as many errors when
using POS contexts as opposed to word contexts.
Corpus annotation projects willing to put in some
extra effort thus can use this method of finding vari-
ation n-grams with a generalized context to detect
and correct more errors.
6 Summary and Outlook
We have described the first method for finding er-
rors in corpora with graph annotations. We showed
how the variation n-gram method can be extended
to discontinuous structural annotation, and how this
can be done efficiently and with as high a preci-
sion as reported for continuous syntactic annotation.
Our experiments with the TIGER corpus show that
generalizing the context to part-of-speech tags in-
creases recall while keeping precision above 50%.
The method can thus have a substantial practical
benefit when preparing a corpus with discontinuous
annotation.
Extending the error detection method to handle
328
discontinuous constituents, as we have done, has
significant potential for future work given the in-
creasing number of free word order languages for
which corpora and treebanks are being developed.
Acknowledgements We are grateful to George
Smith and Robert Langner of the University of Pots-
dam TIGER team for evaluating the variation we de-
tected in the samples. We would also like to thank
the three ACL reviewers for their detailed and help-
ful comments, and the participants of the OSU CLip-
pers meetings for their encouraging feedback.
References
Ann Bies, Mark Ferguson, Karen Katz and Robert
MacIntyre, 1995. Bracketing Guidelines for Tree-
bank II Style Penn Treebank Project. University
of Pennsylvania.
Philippe Blache and Daniel Hirst, 2000. Multi-level
annotation for spoken-language corpora. In Pro-
ceedings of ICSLP-00. Beijing, China.
Sabine Brants, Stefanie Dipper, Silvia Hansen,
Wolfgang Lezius and George Smith, 2002. The
TIGER Treebank. In Proceedings of TLT-02. So-
zopol, Bulgaria.
Harry Bunt and Arthur van Horck (eds.), 1996. Dis-
continuous Constituency. Mouton de Gruyter,
Berlin and New York.
John Carroll, Anette Frank, Dekang Lin, Detlef
Prescher and Hans Uszkoreit (eds.), 2002. Pro-
ceedings of the LREC Workshop ?Beyond PAR-
SEVAL. Towards Improved Evaluation Measures
for Parsing Systems?, Las Palmas, Gran Canaria.
Markus Dickinson and W. Detmar Meurers, 2003a.
Detecting Errors in Part-of-Speech Annotation. In
Proceedings of EACL-03. Budapest, Hungary.
Markus Dickinson and W. Detmar Meurers, 2003b.
Detecting Inconsistencies in Treebanks. In Pro-
ceedings of TLT-03. Va?xjo?, Sweden.
Eleazar Eskin, 2000. Automatic Corpus Correc-
tion with Anomaly Detection. In Proceedings of
NAACL-00. Seattle, Washington.
Edward Fredkin, 1960. Trie Memory. CACM,
3(9):490?499.
Erhard Hinrichs, Julia Bartels, Yasuhiro Kawata,
Valia Kordoni and Heike Telljohann, 2000. The
Tu?bingen Treebanks for Spoken German, En-
glish, and Japanese. In Wolfgang Wahlster (ed.),
Verbmobil: Foundations of Speech-to-Speech
Translation, Springer, Berlin, pp. 552?576.
Geoffrey Huck and Almerindo Ojeda (eds.), 1987.
Discontinuous Constituency. Academic Press,
New York.
Stig Johansson, 1986. The Tagged LOB Corpus:
Users? Manual. Norwegian Computing Centre for
the Humanities, Bergen.
Paul Kingsbury, Martha Palmer and Mitch Marcus,
2002. Adding Semantic Annotation to the Penn
TreeBank. In Proceedings of HLT-02. San Diego.
Pavel Kve?to?n and Karel Oliva, 2002. Achieving
an Almost Correct PoS-Tagged Corpus. In Petr
Sojka, Ivan Kopec?ek and Karel Pala (eds.), TSD
2002. Springer, Heidelberg, pp. 19?26.
M. Marcus, Beatrice Santorini and M. A.
Marcinkiewicz, 1993. Building a large an-
notated corpus of English: The Penn Treebank.
Computational Linguistics, 19(2):313?330.
Stefan Mu?ller, 2004. Continuous or Discontinu-
ous Constituents? A Comparison between Syn-
tactic Analyses for Constituent Order and Their
Processing Systems. Research on Language and
Computation, 2(2):209?257.
Lluis Padro and Lluis Marquez, 1998. On the Eval-
uation and Comparison of Taggers: the Effect of
Noise in Testing Corpora. In COLING/ACL-98.
Paul Rayson, Dawn Archer, Scott Piao and Tony
McEnery, 2004. The UCREL Semantic Analy-
sis System. In Proceedings of the Workshop on
Beyond Named Entity Recognition: Semantic la-
belling for NLP tasks. Lisbon, Portugal, pp. 7?12.
Wojciech Skut, Brigitte Krenn, Thorsten Brants and
Hans Uszkoreit, 1997. An Annotation Scheme
for Free Word Order Languages. In Proceedings
of ANLP-97. Washington, D.C.
Hans van Halteren, 2000. The Detection of Inconsis-
tency in Manually Tagged Text. In Anne Abeille?,
Thorsten Brants and Hans Uszkoreit (eds.), Pro-
ceedings of LINC-00. Luxembourg.
Hans van Halteren, Walter Daelemans and Jakub Za-
vrel, 2001. Improving Accuracy in Word Class
Tagging through the Combination of Machine
Learning Systems. Computational Linguistics,
27(2):199?229.
329
Proceedings of the Second ACL Workshop on Effective Tools and Methodologies for Teaching NLP and CL, pages 15?22,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
?Language and Computers?
Creating an Introduction for a General Undergraduate Audience
Chris Brew
Department of Linguistics
The Ohio State University
cbrew@ling.osu.edu
Markus Dickinson
Department of Linguistics
The Ohio State University
dickinso@ling.osu.edu
W. Detmar Meurers
Department of Linguistics
The Ohio State University
dm@ling.osu.edu
Abstract
This paper describes the creation
of Language and Computers, a new
course at the Ohio State University de-
signed to be a broad overview of topics
in computational linguistics, focusing
on applications which have the most
immediate relevance to students. This
course satisfies the mathematical and
logical analysis requirement at Ohio
State by using natural language sys-
tems to motivate students to exercise
and develop a range of basic skills in
formal and computational analysis. In
this paper we discuss the design of the
course, focusing on the success we have
had in offering it, as well as some of the
difficulties we have faced.
1 Introduction
In the autumn of 2003, we created Language
and Computers (Linguistics 384), a new course
at the Ohio State University that is designed to
be a broad overview of topics in computational
linguistics, focusing on applications which have
the most immediate relevance to students. Lan-
guage and Computers is a general enrollment
course designed to meet the Mathematical and
Logical Analysis requirement that is mandated
for all undergraduates at the Ohio State Uni-
versity (OSU), one of the largest universities in
the US. We are committed to serving the av-
erage undergraduate student at OSU, including
those for whom this is the first and last Lin-
guistics course. Some of the students take the
course because it is an alternative to calculus,
others because of curiosity about the subject
matter. The course was first taught in Win-
ter 2004, drawing a wide range of majors, and
has since expanded to three sections of up to 35
students each. In this paper we will discuss the
design of the course, focusing on the success we
have had in offering it, as well as some of the
difficulties we have faced.
2 General Context
The Linguistics Department at OSU is the home
of a leading graduate program in which 17 grad-
uate students are currently specializing in com-
putational linguistics. From the perspective of
the graduate program, the goal of the new course
development was to create more appropriate
teaching opportunities for the graduate students
specializing in computational linguistics. Much
of the undergraduate teaching load in Linguis-
tics at OSU is borne by graduate teaching assis-
tants (GTAs) who receive stipends directly from
the department. After a training course in the
first year, most such GTAs act as instructors on
the Department?s ?Introduction to Language,?
which is taught in multiple small sections. In-
structors are given considerable responsibility
for all aspects of course design, preparation, de-
livery, and grading. This works very well and
produces many superb instructors, but by 2003
it was apparent that increasing competition was
reducing the pool of undergraduates who want
to take this general overview course.
The Ohio State University has a distribution
requirement, the General Education Curricu-
15
lum (GEC), that is designed to ensure adequate
breadth in undergraduate education. The twin
demands of the student?s major and the distri-
bution requirement are sufficient to take up the
vast majority of the credit hours required for
graduation. In practice this means that students
tend to make course selections motivated pri-
marily by the goal of completing the necessary
requirements as quickly and efficiently as they
can, possibly at the expense of curiosity-driven
exploration. Linguistics, as an interdisciplnary
subject, can create courses that satisfy both cu-
riosity and GEC requirements.
To fill this interdisciplinary niche, the OSU
Department of Linguistics has created a range
of new courses such as Language and Gender,
Language and the Mind, Language and the Law,
and the Language and Computers course dis-
cussed in this paper. In addition to filling a dis-
tribution requirement niche for undergraduates,
the courses also allow the linguistics GTAs to
teach courses on topics that are related to their
area of specialization, which can be beneficial
both to the instructors and to those instructed.
Prior to creation of the new Language and Com-
puters course, there were virtually no opportu-
nities for student members of the computational
linguistics group to teach material close to their
focus.
3 Course overview
The mission statement for our course reads:
In the past decade, the widening use
of computers has had a profound influ-
ence on the way ordinary people com-
municate, search and store informa-
tion. For the overwhelming majority
of people and situations, the natural
vehicle for such information is natu-
ral language. Text and to a lesser ex-
tent speech are crucial encoding for-
mats for the information revolution.
This course will give students insight
into the fundamentals of how comput-
ers are used to represent, process and
organize textual and spoken informa-
tion, as well as providing tips on how
to effectively integrate this knowledge
into their working practice. The course
will cover the theory and practice of
human language technology.
The course was designed to meet the Math-
ematical and Logical Analysis (MLA) require-
ment for students at the Ohio State University,
which is characterized in the following way:
A student in a B.A. program must take
one course that focuses on argument in
a context that emphasizes natural lan-
guage, mathematics, computer science
or quantitative applications not pri-
marily involving data. Courses which
emphasize the nature of correct argu-
mentation either in natural languages
or in symbolic form would satisfy this
requirement, as would many mathe-
matics or computer science courses.
. . . The courses themselves should em-
phasize the logical processes involved
in mathematics, inductive or deductive
reasoning, or computing and the the-
ory of algorithms.
Linguistics 384 responds to this specification
by using natural language systems to motivate
students to exercise and develop a range of ba-
sic skills in formal and computational analysis.
The course combines lectures with group work
and in-class discussions, resulting in a seminar-
like environment. We enrol no more than 35
students per section, often significantly fewer at
unpopular times of day.
The course philosophy is to ground abstract
concepts in real world examples. We intro-
duce strings, regular expressions, finite-state
and context-free grammars, as well as algo-
rithms defined over these structures and tech-
niques for probing and evaluating systems that
rely on these algorithms. This meets the MLA
objective to emphasize the nature of correct ar-
gumentation in symbolic form as well as the logi-
cal processes involved in computing and the the-
ory of algorithms. These abstract ideas are em-
bedded in practical applications: web searching,
16
spelling correction, machine translation and di-
alogue systems. By covering the technologies
behind these applications, the course addresses
the requirement to sharpen a student?s ability
to reason critically, construct valid arguments,
think creatively, analyze objectively, assess ev-
idence, perceive tacit assumptions, and weigh
evidence.
Students have impressions about the quality
of such systems, but the course goes beyond
merely subjective evaluation of systems and em-
phasizes the use of formal reasoning to draw and
argue for valid conclusions about the design, ca-
pabilities and behavior of natural language sys-
tems.
In ten weeks, we cover eight topics, using a
data projector in class, with copies of the slides
being handed out to the student before each
class. There is no textbook, and there are rel-
atively few assigned readings, as we have been
unable to locate materials appropriate for an av-
erage student without required background who
may never take another (computational) linguis-
tics class. The topics covered are the following,
in this order:
? Text and speech encoding
? (Web-)Searching
? Spam filtering (and other classification
tasks, such as language identification)
? Writers? aids (Spelling and grammar correc-
tion)
? Machine translation (2 weeks)
? Dialogue systems (2 weeks)
? Computer-aided language learning
? Social context of language technology use
In contrast to the courses of which we are
aware that offer computational linguistics to un-
dergraduates, our Language and Computers is
supposed to be accessible without prerequisites
to students from every major (a requirement for
GEC courses). For example, we cannot assume
any linguistic background or language aware-
ness. Like Lillian Lee?s Cornell course (Lee,
2002), the course cannot presume programming
ability. But the GEC regulations additionally
prohibit us from requiring anything beyond high
school level abilities in algebraic manipulation.
We initially hoped that this meant that we
would be able to rely on the kind of math knowl-
edge that we ourselves acquired in secondary
school, but soon found that this was not real-
istic. The sample questions from Lee?s course
seem to us to be designed for students who ac-
tively enjoy math. Our goal is different: we
want to exercise and extend the math skills of
the general student population, ensuring that
the course is as accessible to the well-motivated
dance major as it is to the geekier people with
whom we are somewhat more familiar. This is
hard, but worthwhile.
The primary emphasis is on discrete math-
ematics, especially with regard to strings and
grammars. In addition, the text classification
and spam-filtering component exercise the abil-
ity to reason clearly using probabilities. All of
this can be achieved for students with no colle-
giate background in mathematics.
Specifically, Linguistics 384 uses non-trivial
mathematics at a level at or just beyond algebra
1 in the following contexts:
? Reasoning about finite-state automata and
regular expressions (in the contexts of web
searching and of information management).
Students reason about relationships be-
tween specific and general search terms.
? Reasoning about more elaborate syntactic
representations (such as context-free gram-
mars) and semantic representations (such
as predicate calculus), in order to better
understand grammar checking and machine
translation errors.
? Reasoning about the interaction between
components of natural language systems (in
the contexts of machine translation and of
dialog systems).
? Understanding the basics of dynamic pro-
gramming via spelling correction (edit dis-
tance) and applying algebraic thinking to
algorithm design.
17
? Simple probabilistic reasoning (in the con-
text of text classification).
There is also an Honors version of the course,
which is draws on a somewhat different pool
of students. In 2004 the participants in Hon-
ors 384 were equally split between Linguistics
majors looking for a challenging course, people
with a computer background and some interest
in language and people for whom the course was
a good way of meeting the math requirement at
Honors level. Most were seniors, so there was lit-
tle feed-through to further Linguistics courses.
The Honors course, which used to be
called Language Processing Technology, pre-
dates Language and Computers, and includes
more hands-on material. Originally the first half
of this course was an introduction to phonetics
and speech acoustics through Praat, while the
second was a Prolog-based introduction to sym-
bolic NLP. We took the opportunity to redesign
this course when we created the non-honors ver-
sion. In the current regime, the hands-on aspect
is less important than the opportunities offered
by the extra motivation and ability of these stu-
dents. Two reading assignments in the honors
version were Malcolm Gladwell?s book review on
the Social Life of Paper (Gladwell, 2001) and
Turing?s famous paper on the Imitation Game
(Turing, 1950). We wondered whether the ec-
centricity and dated language of the latter would
be a problem, but it was not.
Practical assignments in the laboratory are
possible in the honors course, because the class
size can be limited. One such assignment was
a straightforward run-through of the clock tu-
torial from the Festival speech synthesis system
and another a little machine translation system
between digits and number expressions. Having
established that they can make a system that
turns 24 into ?twenty four?, and so on, the stu-
dents are challenged to adapt it to speak ?Fairy
Tale English?: that is, to make it translate 24
into ?four and twenty?, and vice-versa.
1
1For a complete overview of the course materials,
there are several course webpages to check out. The web-
page for the first section of the course (Winter 2004)
4 General themes of the course
Across the eight different topics that are taught,
we try to maintain a cohesive feel by emphasiz-
ing and repeating different themes in computa-
tional linguistics. Each theme allows the stu-
dents to see that certain abstract ideas are quite
powerful and can inform different concrete tasks.
The themes which have been emphasized to this
point are as follows:
? There are both statistical and rule-based
methods for approaching a problem in nat-
ural language processing. We show this
most clearly in the spam filtering unit and
the machine translation unit with different
types of systems.
? There is a tension between developing tech-
nology in linguistically-informed ways and
developing technology so that a product is
effective. In the context of dialogue sys-
tems, for example, the lack of any linguistic
knowledge in ELIZA makes it fail quickly,
but an ELIZA with a larger database and
still no true linguistic knowledge could have
more success.
? Certain general techniques, such as n-gram
analysis, can be applied to different compu-
tational linguistic applications.
? Effective technology does not have to solve
every problem; focusing on a limited do-
main is typically more practical for the ap-
plications we look at. In machine transla-
tion, this means that a machine translation
system translating the weather (e.g., the
METEO system) will perform better than
a general-purpose system.
? Intelligent things are being done to improve
natural language technology, but the task is
a very difficult one, due to the complexities
of language. Part of each unit is devoted to
is at http://ling.osu.edu/~dickinso/384/wi04/. A
more recent section (Winter 2005) can be found at http:
//ling.osu.edu/~dm/05/winter/384/. For the honors
course, the most recent version is located at http:
//ling.osu.edu/~cbrew/2005/spring/H384/. A list of
weblinks to demos, software, and on-line tutorials cur-
rently used in connection with the course can be found
at http://ling.osu.edu/~xflu/384/384links.html
18
showing that the problem the technology is
addressing is a complex one.
5 Aspects of the course that work
The course has been a positive experience, and
students overall seemed pleased with it. This
is based on the official student evaluation of
instruction, anonymous, class specific question-
naires we handed out at the end of the class,
personal feedback, and new students enrolling
based on recommendations from students who
took the course. We attribute the positive re-
sponse to several different aspects of the course.
5.1 Topics they could relate to
Students seem to most enjoy those topics which
were most relevant to their everyday life. On the
technological end, this means that the units on
spam filtering, web searching, and spell check-
ing are generally the most well-received. The
more practical the focus, the more they seem
to appreciate it; for web searching, for instance,
they tend to express interest in becoming better
users of the web. On the linguistic end, discus-
sions of how dialogue works and how language
learning takes place, as part of the units on di-
alogue systems and CALL, respectively, tend to
resonate with many students. These topics are
only sketched out insofar as they were relevant
to the NLP technology in question, but this has
the advantage of not being too repetitive for the
few students who have had an introductory lin-
guistics class before.
5.2 Math they can understand
Students also seem to take pride in being able
to solve what originally appear to be difficult
mathematical concepts. To many, the concept
and look of a binary number is alien, but they
consistently find this to be fairly simple. The
basics of finite-state automata and boolean ex-
pressions (even quite complicated expressions)
provide opportunities for students to understand
that they are capable of learning concepts of log-
ical thinking. Students with more interest and
more of an enjoyment for math are encouraged
to go beyond the material and, e.g., figure out
the nature of more complicated finite-state au-
tomata. In this way, more advanced students are
able to stay interested without losing the other
students.
More difficult topics, such as calculating the
minimum edit distance between a word and its
misspelling via dynamic programming, can be
frustrating, but they just as often are a source
of a greater feeling of success for students. After
some in-class exercises, when it becomes appar-
ent that the material is learnable and that there
is a clear, well-motivated point to it, students
generally seem pleased in conquering somewhat
more difficult mathematical concepts.
5.3 Interactive demos
In-class demos of particular software are also
usually well-received, in particular when they
present applications that students themselves
can use. These demos often focus on the end
result of a product, such as simply listening to
the output of several text-to-speech synthesiz-
ers, but they can also be used for understanding
how the applications works. For example, some
sections attempt to figure out as a class where
a spelling checker fails and why. Likewise, an
in-class discussion with ELIZA has been fairly
popular, and students are able to deduce many
of the internal properties of ELIZA.
5.4 Fun materials
In many ways, we have tried to keep the tone
of the course fairly light. Even though we
are teaching mathematical and logical concepts,
these concepts are still connected to the real
world, and as such, there is much opportunity
to present the material in a fun and engaging
manner.
Group work One such way to make the learn-
ing process more enjoyable was to use group
work. In the past few quarters, we have been
refining these exercises. Because of the nature
of the topics, some topics are easier to derive
group exercises for than others. The more math-
ematical topics, such as regular expressions, suit
themselves well for straightforward group work
on problem sets in class; others can be more
19
creative. The group exercises usually serve as a
way for students to think about issues they al-
ready know something about, often as a way to
introduce the topic.
For example, on the first day, they are given
a sheet and asked to evaluate sets of opposing
claims, giving arguments for both sides, such as
the following:
1. A person will have better-quality papers if
they use a spell checker.
A person will have worse-quality papers if
they use a spell checker.
2. An English-German dictionary is the main
component needed to automatically trans-
late from English to German.
An English-German dictionary is not the
main component needed to automatically
translate from English to German.
3. Computers can make you sound like a na-
tive speaker of another language.
Computers cannot make you sound like a
native speaker of another language.
To take another example, to get students
thinking about the social aspects of the use of
language technology, they are asked in groups to
consider some of the implications of a particu-
lar technology. The following is an excerpt from
one such handout.
You work for a large software company
and are in charge of a team of com-
putational linguists. One day, you are
told: ?We?d like you and your team to
develop a spell checker for us. Do you
have any questions?? What questions
do you have for your boss?
...
Somehow or another, the details of
your spell checker have been leaked to
the public. This wouldn?t be too bad,
except that it?s really ticked some lin-
guists off. ?It?s just a big dictionary!?
they yell. ?It?s like you didn?t know
anything about morphology or syntax
or any of that good stuff.? There?s
a rumor that they might sue you for
defamation of linguistics. What do you
do?
Although the premise is somewhat ridiculous,
with such group work, students are able to con-
sider important topics in a relaxed setting. In
this case, they have to first consider the speci-
fications needed for a technology to work (who
will be using it, what the expectations are, etc.)
and, secondly, what the relationship is between
the study of language and designing a product
which is functional.
Fun homework questions In the home-
works, students are often instructed to use a
technology on the internet, or in some way to
take the material presented in class a step far-
ther. Additionally, most homework assignments
had at least one lighter question which allowed
students to be more creative in their responses
while at the same time reinforcing the material.
For example, instructors have asked students
to send them spam, and the most spam-worthy
message won a prize. Other homework ques-
tions have included sketching out what it would
take to convert an ELIZA system into a hostage
negotiator?and what the potential dangers are
in such a use. Although some students put down
minimal answers, many students offer pages of
detailed suggestions to answer such a question.
This gives students a taste of the creativity in-
volved in designing new technology without hav-
ing to deal with the technicalities.
6 Challenges for the course
Despite the positive response, there are several
aspects to the course which have needed im-
provement and continue to do so. Teaching
to a diverse audience of interests and capabili-
ties presents obstacles which are not easily over-
come. To that end, here we will review aspects
of the course which students did not generally
enjoy and which we are in the process of adapt-
ing to better suit our purposes and our students?
needs.
20
6.1 Topics they do not relate to
For such a range of students, there is the diffi-
culty of presenting abstract concepts. Although
we try to relate everything to something which
students actually use or could readily use, we
sometimes include topics from computational
linguistics that make one better able to think
logically in general and which we feel will be
of future use for our students. One such topic
is that of regular expressions, in the context of
searching for text in a document or corpus. As
most students only experience searching as part
of what they do on the web, and no web search
engine (to the best of our knowledge) currently
supports regular expression searching, students
often wonder what the point of the topic is. In
making most topics applicable to everyday life,
we had raised expect. In this particular case,
students seemed to accept regular expressions
more once it they saw that Microsoft Word has
something roughly analogous.
Another difficulty that presented itself for a
subset of the students was that of using for-
eign language text to assist in teaching ma-
chine translation and computer-aided language
learning. Every example was provided with an
English word-by-word gloss, as well as a para-
phrase, yet the examples can still be difficult to
understand without a basic appreciation for the
relevant languages. If the students know Span-
ish, the example is in Spanish and the instruc-
tor has a decent Spanish accent, things can go
well. But students tend to blame difficulties in
the machine translation homework on not know-
ing the languages used in the examples. Under-
standing the distinction between different kinds
of machine translation systems requires some
ability to grasp how languages can differ, so we
certainly must (unless we use proxies like fairy-
tale English) present some foreign material, but
we are in dire need of means to do this as gently
as possible
6.2 Math they do not understand
While some of the more difficult mathemati-
cal concepts were eventually understood, oth-
ers continued to frustrate students. The al-
ready mentioned regular expressions, for exam-
ple, caused trouble. Firstly, even if you do
understand them, they are not necessarily life-
enhancing, unless you are geeky enough to write
your papers in a text editor that properly sup-
ports them. Secondly, and more importantly,
many students saw them as unnecessarily ab-
stract and complex. For instance, some stu-
dents were simply unable to understand the no-
tion that the Kleene star is to be interpreted as
an operator rather than as a special character
occurring in place of any string.
Even though we thought we had calibrated
our expectations to respect the fact that our
students knew no math beyond high school, the
amount that they had retained from high school
was often less than we expected. For exam-
ple, many students behaved exactly as if they
had never seen Venn diagrams before, so time
had to be taken away from the main material
in order to explain them. Likewise, figuring
out how to calculate probabilities for a bag of
words model of statistical machine translation
required a step-by-step explanation of where
each number comes from. A midterm ques-
tion on Bayesian spam filtering needed the same
treatment, revealing that even good students
may have significant difficulties in deploying the
high school math knowledge they almost cer-
tainly possess.
6.3 Technology which did not work
Most assignments required students to use the
internet or the phone in some capacity, usu-
ally to try out a demo. With such tasks, there
is always the danger that the technology will
not work. For example, during the first quar-
ter the course was taught, students were asked
to call the CMU Communicator system and in-
teract with it, to get a feel for what it is like
to interact with a computer. As it turns out,
halfway through the week the assignment was
due, the system was down, and thus some stu-
dents could not finish the exercise. Follow-
ing this episode, homework questions now come
with alternate questions. In this case, if the sys-
tem is down, the first alternate is to listen to a
pre-recorded conversation to see how the Com-
21
municator works. Since some students are un-
able to listen to sounds in the campus computer
labs, the second alternate is to read a transcript.
Likewise, students were instructed to view the
page source code for ELIZA. However, some
campus computer labs at OSU do not allow stu-
dents to view the source of a webpage. In re-
sponse to this, current versions of the assign-
ment have a separate webpage with the source
code written out as plain text, so all students
can view it.
One final note is that students have often com-
plained of weblinks failing to work, but this ?fail-
ure? is most often due to students mistyping
the link provided in the homework. Providing
links directly on the course webpage or including
them in the web- or pdf-versions of the home-
work sheets is the simplest solution for this prob-
lem.
7 Summary and Outlook
We have described the course Language and
Computers (Linguistics 384), a general introduc-
tion to computational linguistics currently being
taught at OSU. While there are clear lessons
to be learned for developing similar courses at
other universities, there are also more general
points to be made. In courses which assume
some CS background, for instance, it is still
likely the case that students will want to see
some practical use of what they are doing and
learning.
There are several ways in which this course
can continue to be improved. The most pressing
priority is to develop a course packet and pos-
sibly a textbook. Right now, students rely only
on the instructor?s handouts, and we would like
to provide a more in-depth and cohesive source
of material. Along with this, we want to de-
velop a wider range of readings for students (e.g.
Dickinson, to appear) to provide students with
a wider variety of perspectives and explanations
for difficult concepts.
To address the wide range of interests and ca-
pabilities of the students taking this course as a
general education requirement, it would be good
to tailor some of the sections to audiences with
specific backgrounds?but given the lack of a
dedicated free time slot for all students of a par-
ticular major, etc., it is unclear whether this is
feasible in practice.
We are doing reasonably well in integrating
mathematical thinking into the course, but we
would like to give students more experience of
thinking about algorithms. Introducing a ba-
sic form of pseudocode might go some way to-
wards achieving this, provided we can find a mo-
tivating linguistic example that is both simple
enough to grasp and complex enough to justify
the overhead of introducing a new topic. Fur-
ther developments might assist us in developing
a course between Linguistics 384 and Linguistics
684, our graduate-level computational linguis-
tics course, as we currently have few options for
advanced undergraduates.
Acknowledgements We would like to thank
the instructors of Language and Computers for
their discussions and insights into making it a
better course: Stacey Bailey, Anna Feldman, Xi-
aofei Lu, Crystal Nakatsu, and Jihyun Park. We
are also grateful to the two ACL-TNLP review-
ers for their detailed and helpful comments.
References
Markus Dickinson, to appear. Writers? Aids. In
Keith Brown (ed.), Encyclopedia of Language
and Linguistics. Second Edition, Elsevier, Ox-
ford.
Malcolm Gladwell, 2001. The Social Life
of Paper. New Yorker . available from
http://www.gladwell.com/archive.html.
Lillian Lee, 2002. A non-programming introduc-
tion to computer science via NLP, IR, and
AI. In ACL Workshop on Effective Tools
and Methodologies for Teaching Natural Lan-
guage Processing and Computational Linguis-
tics. pp. 32?37.
A.M. Turing, 1950. Computing Machinery and
Intelligence. Mind , 59(236):433?460.
22
Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 201?208
Manchester, August 2008
Representations for category disambiguation
Markus Dickinson
Indiana University
Bloomington, IN 47405
md7@indiana.edu
Abstract
As it serves as a basis for POS tagging, cat-
egory induction, and human category ac-
quisition, we investigate the information
needed to disambiguate a word in a lo-
cal context, when using corpus categories.
Specifically, we increase the recall of an
error detection method by abstracting the
word to be disambiguated to a represen-
tation containing information about some
of its inherent properties, namely the set
of categories it can potentially have. This
work thus provides insights into the rela-
tion of corpus categories to categories de-
rived from local contexts.
1 Introduction and Motivation
Category induction techniques generally rely on
local contexts, i.e., surrounding words, to cluster
word types together (e.g., Clark, 2003; Sch?utze,
1995), using information of a kind also found
in human category acquisition tasks (e.g., Mintz,
2002, 2003). Such information is also at the
core of standard part-of-speech (POS) tagging, or
disambiguation, methods (see, e.g., Manning and
Sch?utze, 1999, ch. 10), with the contexts generally
abstracted to POS tags. The contextual informa-
tion is similar in both tasks because induction is
founded in part upon the notion that local contexts
are useful for disambiguation: one morphosyntac-
tically clusters words which should have the same
category in the same contexts. But which contexts
count as being the ?same?? And to what extent do
categories based on context distributions resemble
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
corpus annotation categories? Since disambigua-
tion is in some sense more primary, to begin to
answer these questions we investigate which rep-
resentations are effective for category disambigua-
tion.
Disambiguating a word?s category in context
has of course been explored in other situations,
especially POS tagging. Rarely, however, has it
been shown as to which information is the most
accurate at disambiguation and which information
is absolutely necessary, without mixing these is-
sues with other tagging issues, such as smoothing
and unknown word tagging. We need techniques
which isolate disambiguation, placing less empha-
sis on generalizing contexts to new data. To de-
termine the essential information needed for accu-
rate disambiguation, we start with a precise model
and generalize it. Changing the model in small
ways and evaluating the resulting precision will in-
dicate how particular aspects of the representation
are contributing to successful disambiguation.
The central question of this paper is: which
representation (of a word and its context) indi-
cates that two situations should be categorized the
same? In this context, POS annotation error detec-
tion provides an ideal setting to explore represen-
tations for disambiguation. Error detection relies
on the assumption that words should be annotated
consistently?in other words, contexts are grouped
which accurately identify the category of a word as
being consistent?and it does this with an empha-
sis on high precision. In essence, error detection
already investigates where disambiguation can be
done, often using local contexts (e.g., Dickinson,
2005). With an emphasis on high precision, how-
ever, many corpus instances are essentially uncat-
egorized and are thus in need of generalization.
To get at the central question of an appropriate
201
representation for disambiguation, then, our task is
to generalize error detection and increase the recall
of errors found in a corpus by exploiting more gen-
eral properties of a corpus. Given that annotation
errors can have a profound impact on the quality
of training and testing on such data (see Dickin-
son, 2005, ch. 1), this task also serves an immense
practical need in its own right.
In exploring error detection recall, we can con-
nect the task to another with much of the same em-
phasis. Human category acquisition experiments
have also focused on precision: instead of ask-
ing how every word is categorized, they examine
how some words are categorized, from which oth-
ers can be bootstrapped. As outlined in sections 2
and 3, we can use such studies as a starting point
for generalizing error detection.
2 Background
2.1 The variation n-gram method
The error detection method we build from is the
variation n-gram method (Dickinson and Meur-
ers, 2003; Dickinson, 2005). The approach de-
tects items which occur multiple times in the cor-
pus with varying annotation, the so-called varia-
tion nuclei. A nucleus with its repeated surround-
ing context is referred to as a variation n-gram.
Every detected variation in the annotation of a nu-
cleus is classified as an error or a genuine ambi-
guity using a basic heuristic requiring at least one
word of context on each side of the nucleus.
For example, in the WSJ corpus, part of the
Penn Treebank 3 release (Marcus et al, 1993), the
string in (1) is a variation 12-gram since off is a
variation nucleus that is tagged preposition (IN)
in one corpus occurrence and particle (RP) in an-
other.
1
Dickinson (2005) shows that examining
those cases with identical local context?in this
case, looking at ward off a?results in an estimated
error detection precision of 92.5%.
(1) to ward off a hostile takeover attempt by two
European shipping concerns
This method can be applied to syntactic annota-
tion, and for this annotation, one can increase the
recall of errors found by abstracting the nuclei to
POS tags (Boyd et al, 2007). Clearly, this is not
a feasible abstraction here, given that we are at-
tempting to detect errors in POS annotation.
1
To distinguish variation nuclei, we shade them in gray
and underline the immediately surrounding context.
2.2 Frames for language acquisition
Research on language acquisition has addressed
the question of how humans discover and learn cat-
egories of words, using virtually the same contexts
as in the variation n-gram method. Mintz (2002)
shows that local context, in the form of a frame
of two words surrounding a target word, leads to
categorization in adults of the target, and Mintz
(2003) shows that frequent frames supply category
information, consistent across child language cor-
pora. A frame is defined as ?two jointly occurring
words with one word intervening? (Mintz, 2003),
e.g., you it. The frame is not decomposed into its
left side and right side (cf., e.g., Redington et al,
1998; Clark, 2003) , but instead is taken as the oc-
currence of both sides. The target word is the in-
tervening word, but it is not included in the frame
(unlike variation nuclei).
For category acquisition, only frequent frames
are used, those with a frequency above a cer-
tain threshold. Frequent frames predict cate-
gory membership: the set of words appearing
in a given frame should represent a single cate-
gory. The frequent frame you it, for example,
largely identifies verbs, as shown in (2), taken from
the CHILDES database of child-directed speech
(MacWhinney, 2000). Analyzing the frequent
frames in six subcorpora of CHILDES, Mintz
(2003) obtains both high type and high token ac-
curacy in grouping words into the same categories.
(2) a. you put it
b. you see it
To take this work as a basis for investigating dis-
ambiguation, some points are in order about the re-
sults. First, accuracies slightly degrade when mov-
ing from the ?Standard Labeling? category set
2
to
the more fine-grained ?Expanded Labeling? cate-
gory set,
3
i.e., a .98 to .91 drop in token accuracy
and .93 to .91 drop in type accuracy. It is not clear
what happens with even more fine-grained corpus
tagsets. Secondly, Mintz (2003) assumes that, at
least for his experiments, each word has only one
class (see also Redington et al, 1998, p. 439-440).
The tasks of category induction and category dis-
ambiguation are thus conflated into a single step.
We do not know for sure whether frames induce
2
Categories = noun, verb, adjective, preposition, adverb,
determiner, wh-word, not, conjunction, and interjection.
3
Nouns split into nouns and pronouns; verbs split into
verbs, auxiliaries, and copula
202
coherent sets of words or whether they accurately
disambiguate a word, or both. In other words, can
frames be used to group the target words (induc-
tion) or to group the contexts (disambiguation)?
While we investigate using frames for disam-
biguation in English (and somewhat in German),
the concept of a frame has been shown to be cross-
linguistically viable (Chemla et al, in press), and
in principle could extend to languages encoding re-
lations through morphology instead of linear order
(see the discussion in Mintz, 2003).
3 Generalizing error detection via frames
Both strands of research employ local contexts for
identifying categories, but the variation n-gram
method relies on identical words to serve as vari-
ation nuclei, or target words to be disambiguated.
To increase the recall of the method in a way relat-
ing to acquisition, the nucleus should be abstracted
to something more general than a word. As a (fre-
quent) frame does not include the target, predicting
that the category within that context is always the
same, a first step in abstracting the nucleus is to
require no similarity between nuclei.
We thus search for all identical nuclei with
frame context?or what we will call framed vari-
ation nuclei?such that there is variation in label-
ing for the nucleus, but we require no identity of
the nucleus. We investigate the WSJ portion of the
Penn Treebank, and, to provide more robust evalu-
ation, also compare the TIGER corpus of German,
version 2 (Brants et al, 2002) where appropriate.
Given that punctuation is less informative for de-
termining a category, we remove from considera-
tion frames containing punctuation as one of the
context words, and obtain 48,717 variations in the
WSJ and 22,613 in TIGER.
Although basic hand-examination reveals some
errors, a majority of cases contain acceptable vari-
ations. As one example, in the WSJ the frame the
of occurs as the most frequent frame with vari-
ation in labeling for the target (5737 instances).
This is a nominal position, and thus we find varia-
tion between a variety of correct nominal tags: car-
dinal number (CD), adjective (JJ, JJR, JJS), com-
mon noun (NN, NNS), and proper noun (NNP,
NNPS), in addition to the erroneous verbal tags
VBD (past tense verb) and VBG (verb, -ing form).
Restricting our attention to the frequent frames, as
in Mintz (2003), is not helpful: the problem occurs
irrespective of frequency. Indeed, there is an aver-
age of 2.56 categories per variation, with one vari-
ation (and in) having 21 categories. This is con-
sistent in TIGER, which has 2.57 categories per
variation and 22 categories for und in.
While more context could help, the real issue is
the definition of a nucleus. In the example above,
which nominal tag is used depends upon inher-
ent properties of the word involved. Consider the
frame that the. Among the 18 possible tags,
there is variation between NN (common noun) for
words like afternoon and VBZ (present tense verb,
3rd person singular) for words like says. Both are
legitimate, and the primary way to tell is by exam-
ining information about the target word. In gen-
eralizing the nucleus, instead of abstracting it to
nothing, we need to abstract it to something indi-
cating broad characteristics of the word.
4 An appropriate level of abstraction
On the one hand, the variation n-gram method has
high precision; on the other, using frames results in
high recall, but too low a precision to sort through.
Both methods rely on the same identical contexts;
the issue is in finding which words are comparable.
Consider the frame n?t that. Some words are in-
herently similar and should have the same tags: the
correct n?t help/VB that and the erroneous n?t mat-
ter/NN that, for instance, are comparable. Other
cases are not: one/CD and shown/VBN can never
have the same category. We need to find classes
of words that, within the same context, should not
vary in their annotation, and it makes sense to com-
pare words in context if they have the same cate-
gory possibilities.
4.1 Complete ambiguity classes
Ambiguity classes capture the relevant property
we are interested in: words with the same cate-
gory possibilities are grouped together.
4
And am-
biguity classes have been shown to be success-
fully employed, in a variety of ways, to improve
POS tagging (e.g., Cutting et al, 1992; Daelemans
et al, 1996; Dickinson, 2007; Goldberg et al,
2008; Tseng et al, 2005). Only certain words can
take one of two (or more) tags, and these should be
disambiguated in the same way in context. As an
example of how using ambiguity classes as varia-
tion nuclei can increase recall, consider the frame
being by in example (3). There are at least 27
4
One could group affixes by ambiguity class for languages
like Chinese (cf. CTBMorph features in Tseng et al, 2005).
203
different VBN (past participle) verbs appearing be-
tween being and by (3a), but none of these verbs
ever appear as VBD here, even though all of them
could be VBD. Two other VBD/VBN verbs, re-
jected (3b) and played (3c), erroneously appear as
VBD here, but never as VBN. With the nucleus
VBD/VBN, we can find this erroneous variation.
(3) a. being { raised/VBN , infringed/VBN ,
supported/VBN , ... } by
b. as probable as being rejected/VBD by
the Book-of-the-Month Club
c. the ... role in takeover financing being
played/VBD by Japanese banks
Thus, to define complete ambiguity class varia-
tion nuclei, we make a first pass through the cor-
pus to calculate every word?s ambiguity class. On
a second pass, the ambiguity class serves as the
(framed) variation nucleus, e.g., being VBD/VBN
by. Ambiguity class nuclei with more than one tag
in a frame context are flagged as a potential error.
4.2 Pairwise ambiguity classes
While abstracting to a word?s possible classes can
increase the number of errors found, potentially
erroneous classes prevent further increased recall.
For example, the class for plans is erroneously
classified as NNS/VBP/VBZ, even though its one
instance of VBP (present tense verb, non-3rd per-
son singular) in the corpus is erroneous. Without
that case, we would have NNS/VBZ and more di-
rectly comparable words.
As a second experiment, then, we define pair-
wise ambiguity class variation nuclei, using sub-
sets of ambiguity classes to define a nucleus. If the
variation is only between NNS and VBZ, we need
to allow all words with NNS/VBZ variation to
count as comparable nuclei. As above, we calcu-
lated a word?s ambiguity class during a first pass.
In the second pass through the corpus, we break
the ambiguity class down into its pairs, and each
relevant pair is stored as a variation nucleus. The
relevant pairs of tags are those which contain the
tag at that position since classes without that tag
can never have meaningful variation. Taking the
example of company plans to, with the ambiguity
class NNS/VBP/VBZ for plans, if the current cor-
pus position marks plans as NNS, then we store
the two trigrams in (4).
(4) a. company NNS/VBZ to
b. company NNS/VBP to
Looking over the whole corpus, we find vari-
ation between NNS and VBZ, but none between
NNS and VBP. In principle, this instance of
plans/NNS could be in both an NNS/VBZ and an
NNS/VBP variation; this is necessary since we do
not a priori know which variations will be prob-
lematic.
5 Results and Insights
5.1 Complete ambiguity classes
Using complete ambiguity class variation nuclei,
we find 4131 framed variation nuclei in the WSJ.
Almost all variations involve only two or three
tags, with 2.03 tags per variation. TIGER has 626
framed variation nuclei, with 2.01 tags per varia-
tion.
From the 4131 variations, we randomly sampled
100 cases and hand-evaluated whether they contain
an error, and whether its detection is attributable to
the generalization to complete ambiguity classes.
Of the 100, 79 of the cases contain at least one
error, and 15 of these cases are new examples,
i.e., cases without identical words. With a point
estimate of .79, we estimate 3263 errors and ob-
tain a 95% confidence interval of (0.7102, 0.8698),
meaning that we predict between 2933 and 3593
of the 4131 cases contain errors. The 79 erroneous
cases point to 134 token errors, of which 23 are
new.
In addition to increasing the recall of the
method, the cases are arguably more thoroughly
grouped than before. For instance, we see in (5)
that both pretax and third-quarter vary between JJ
and NN in the variation said JJ/NN profit, with
first-half additionally appearing only as JJ. Since
JJ is the correct tag for all instances, the two NN
errors are detected with word nuclei, but here all
the relevant examples are together. This provides
evidence for the claim that an ambiguity class is
a level of abstraction supporting identical disam-
biguation in the same context.
(5) said { first-half/JJ , third-quarter/JJ ,
pretax/JJ , third-quarter/NN , pretax/NN }
profit
The recall has increased, but 79% is below the
92.5% precision previously obtained for the varia-
tion n-gram method with word nuclei (Dickinson,
2005). However, that result used distinct variation
204
nuclei, meaning that the longest contexts were ex-
amined before working down to shorter contexts.
Furthermore, it is not clear how well the original
word nuclei method scales up to larger corpora.
Some of the new false positives we observe would
likely be false positives for word nuclei, given
more data. For example, the new method turns
up generally VBD/VBN the as a false positive, as
in (6), because of the non-local tagset distinction
and short context. With more data, we are more
likely to see an acceptable use of, e.g., generally
favored/VBD the, a false positive for word nuclei.
In some sense, then, this 79% precision might be a
more general indication of the method?s precision
for this tagset and genre.
(6) a. TV news coverage has generally
favored/VBN the government
b. Members ... generally received/VBD the
regional officials
Finally, of the 21 false positives (20 of which are
new), five of them stem from an error in the ambi-
guity class, corresponding to five token errors. For
example, there is variation for JJ/NN words in the
frame of pills, as in (7). However, poison should
never be JJ: its ambiguity class should be NN, not
the incorrect JJ/NN. For error detection, this means
84 of the 100 samples lead to some kind of POS
error; for investigating disambiguation contexts,
this means that 83% (79/95) of the cases support
complete disambiguation. Thus, when abstracting
to ambiguity class nuclei, local context generally
provides sufficient information for disambiguation
(see also section 6).
(7) of { birth-control/JJ , poison/NN } pills
One limitation of the variation n-grammethod is
the fact that some distinctions often need non-local
information (cf. (6)). A bigger problem for group-
ing words by ambiguity classes is the fact that an-
notation can be semantically-based. For example,
the variation of JJ/NN bank is a legitimate ambi-
guity because the distinction between JJ and NN
is semantic. Compare a sort of merchant/NN bank
with an extension of senior/JJ bank debt: both nu-
clei are clearly in a noun modifier position, but
the tags are different based on what they denote.
This shows the limitations of local distributional
information without lexical information, for mak-
ing these tagset distinctions.
5.2 Pairwise ambiguity classes
With pairwise ambiguity classes serving as varia-
tion nuclei, we find 6235 variation frames in the
WSJ and 874 in TIGER, significant increases over
using complete ambiguity class nuclei. To evalu-
ate the method, we want to know: a) how many
total errors we detect, b) how many of these were
detected by using either complete or pairwise am-
biguity classes, and c) how many were detected
specifically with pairwise ambiguity classes.
A sample of 100 of the WSJ cases reveals (a)
59 total errors, (b) 18 of which involve ambigu-
ity class nuclei that would not have been found
with word nuclei. Of these 18, (c) 8 cases can
only be found by extending the method to pairwise
classes. For the point estimate of .59, we estimate
approximately 3679 variations to be errors (95%
CI: 3078 to 4280 errors). The 59 erroneous varia-
tions point to a total of (a) 134 token errors, (b) 30
of which were detected by ambiguity classes; (c)
17 of these were detected by pairwise ambiguity
classes. Clearly, using pairwise ambiguity classes
increases the number of errors found.
As an example, consider (8), centering on the
frame came for. The original variation n-gram
method turns up no variation here, but neither does
the complete ambiguity class extension: in has the
ambiguity class FW/IN/NN/RB/RBR/RP, and out
the class IN/JJ/NN/RB/RP. Since the only relevant
variation is between IN and RP, the pairwise nuclei
method turns up such cases with the variation came
IN/RP for, pointing to an error in the two cases of
out.
(8) a. accounts came in/RP for some blocks
b. numbers came out/IN for September
c. he again came out/IN for an amendment
But what of the 41 false positives, 22 of which
are due to the pairwise classes? We have increased
recall, but there is also a 20% absolute drop in pre-
cision. Is this tradeoff worth it? To answer this,
it is important to note that 15 of the false posi-
tives are due to faulty ambiguity classes, as dis-
cussed above, and 10 of those 15 are from pair-
wise classes. For error detection, this means 74 of
the 100 samples lead to some POS error; for inves-
tigating disambiguation contexts, this means 69%
(59/85) of the cases support disambiguation.
Additionally, the 15 cases point to 53 token
errors, much more than in the previous experi-
ment, due to 44 token errors from the new pair-
205
wise ambiguity classes. For example, in the varia-
tion frame as DT/JJ sales, the words which vary
are a (tagged DT (determiner), with a complete
ambiguity class of DT/FW/IN/JJ/LS/NNP/SYM)
and many (tagged JJ, with an ambiguity class
of DT/JJ/NNS/PDT/RB/VB). Unsurprisingly, a
should never have been tagged JJ in the corpus,
i.e., its ambiguity class is wrong.
In addition to the issue of erroneous tags in an
ambiguity class, atypical tags also pose a prob-
lem. Consider the frame that JJ/RB in, as illus-
trated in (9), with acceptable variation. It might
appear that sometime has a problem with its ambi-
guity class, but the use of JJ is actually correct, as
shown in (10), where sometime is atypically mod-
ifying a noun. To counter atypical uses, one could
use only ?typical? ambiguity classes (cf. Dickin-
son, 2007) or define ambiguity classes according
to order of frequency (cf. Daelemans et al, 1996),
e.g., JJ/RB vs. RB/JJ.
(9) a. a departure from the past that many/JJ in
the industry ...
b. hope that sometime/RB in the near fu-
ture
(10) real estate magnate and sometime/JJ raider
Donald Trump
This illustrates that the selection of an abstracted
class for a nucleus definition is non-trivial, and am-
biguity classes are simply an approximation.
POS contexts One problem for our method is
that word contexts are not always truly compara-
ble; identical context words can be used differ-
ently. For instance, with the variation that NN/VBP
along in (11), the uses of that are clearly distinct
and are marked as such by their tags.
(11) a. gifts that/WDT go/VBP along with pur-
chases
b. We are considering that/DT offer/NN
along with all other alternatives
But do tagset categories actually aid in local dis-
ambiguation? To quickly gauge this, we take the
previous sample of 100 variations and recover the
POS information for the context. Isolating those
cases with non-identical POS tags for the same
word contexts, we find 10 examples and hypothe-
size that these will more likely be acceptable varia-
tions. Interestingly, however, of those ten, six suc-
cessfully identified errors; it turns out that the POS
of the word is often irrelevant for disambiguation.
For the variation paid JJR/RBR than in (12), for
example, the tag of the context word paid is differ-
ent in these cases, but that does not matter for the
tag of more, which should be consistent.
(12) a. they paid/VBD more/JJR than $ 1 mil-
lion
b. he has paid/VBN more/RBR than $
70,000
More problematically, four of the erroneous
variation nuclei also contained POS errors in the
context, as in example (13). The variation all
CC/RB disappeared points to an error in the word
but, yet there is also a noticeable inconsistency in
the word all.
(13) a. have all/DT but/CC disappeared
b. have all/RB but/RB disappeared
In other words, it is often the case that we
should ignore the POS of the context words, due
to the fact that erroneous contexts exist and, more
importantly, that not all categories aid in disam-
biguation. Exploring which contextual categories
aid in target category disambiguation (cf., e.g.,
Brants, 1997) could aid in developing better dis-
ambiguation models, and perhaps also a better
sense of what categories are useful to induce (e.g.,
a broader category Verb in (12) for paid).
6 Representations for disambiguation
We have shown that local lexical context provides
a generally unambiguous context for corpus tags,
given sufficient information about the word to be
disambiguated. The information need not be very
abstract, either: frames using ambiguity class nu-
clei only require a word?s category possibilities.
Even for many unsupervised situations, this is
available from a lexicon (e.g., Banko and Moore,
2004; Goldberg et al, 2008).
We have only looked at cases with variation in
tagging; fully gauging the accuracy of such a data
representation for disambiguation requires more
of the framed nuclei from the corpus, including
those without variation. For this, we could take
all framed nuclei from a corpus and compare the
level of ambiguity for differing abstractions. How-
ever, most framed nuclei occur only once, and it
is not clear how meaningful it is to say that these
are unambiguous. Thus, we examine framed nu-
clei which occur at least twice and report in table 1
206
for the WSJ how unambiguous a particular level of
nucleus abstraction is.
5
Abstraction Unamb. Total Accuracy
Word 84,784 87,390 97.02%
Complete AC 90,341 94,472 95.63%
No info. 51,945 100,662 51.60%
Table 1: Disambiguation accuracy for the WSJ
While abstracting to the case where the nucleus
contains no information (No info.) creates more
cases which are classifiable?over 100,000?the
accuracy of disambiguation drops from the up-
per 90% range to 52%. Note, however, that the
abstraction to complete ambiguity class (AC) nu-
clei has minimal degradation in accuracy, yet in-
creases the number of accurately classified cases.
When we recall that approximately 79% of of the
4131 variation frames should have a single tag,
i.e., 3263 cases, this means that the overall dis-
ambiguation accuracy is estimated to be 99.08%
(93,604/94,472).
In addition to the disambiguation accuracy of
frames, we can look at the accuracy of word tokens
identified by frames. To gauge this, we identify the
most likely tag of each framed variation nucleus
and assign it to all instances of the nucleus. In the
case of ties, one tag is randomly selected; since we
are only calculating overall word token accuracy,
the exact tag selected is unimportant. The results
of comparing to the benchmark tags are given in
table 2. Even though the abstraction to no infor-
mation identifies more word tokens, the ambigu-
ity class abstraction correctly categorizes nearly as
many words.
Abstraction Correct Total Accuracy
Word 340,860 345,139 98.76%
Complete AC 441,603 448,402 98.74%
No info. 444,635 582,601 76.32%
Table 2: Word token accuracy for the WSJ
With the smaller and likely more accurately
tagged TIGER corpus, we find exactly the same
trends, as shown in table 3. This supports the
claim across corpora that local context is often suf-
ficient to disambiguate a word, if some information
from the word?here, the category possibilities?
is present in the nucleus.
5
As pairwise ambiguity classes involve more than one nu-
cleus per corpus position, we use complete ambiguity classes.
Abstraction Unamb. Total Accuracy
Word 37,038 37,324 99.23%
Complete AC 47,832 48,458 98.71%
No info. 33,881 56,494 59.97%
Table 3: Disambiguation accuracy for TIGER
The poor accuracy for framed nuclei with no
information indicates that methods which intend
to match corpus annotation categories could face
difficulties in obtaining a single category with-
out using more information. There is still much
space to explore, however, between using ambi-
guity class nuclei and no information, in order to
further increase the number of comparable cases
without losing accuracy and in order to be more
knowledge-free.
7 Summary and Outlook
Motivated by work on category acquisition, we
have shown that local contexts?i.e., immediately
surrounding words, or frames?can delineate cor-
pus categories when the level of abstraction for the
word to be disambiguated indicates some inherent
properties of the word, namely the categories it can
have. By abstracting away from lexical items to
broader classes of words, we have been able to in-
crease the recall of an error detection method with-
out much drop in its precision.
Having successfully defined a representation for
disambiguation, the next step is to make the rep-
resentation more general, in order to include more
comparable instances. As what we have done is es-
sentially a form of nearest neighbor classification,
one could in the future explore more sophisticated
techniques to cluster contexts.
At the same time, we wish to use as little
annotated knowledge as possible. Thus, an or-
thogonal line of research can involve inducing
classes for words which are more general than sin-
gle categories, i.e., something akin to ambiguity
classes (see, e.g., the discussion of ambiguity class
guessers in Goldberg et al, 2008). This could
make error detection completely independent of
the annotation and, more importantly, lead to an
improved understanding of the best knowledge-
free representation for disambiguation.
Since induction is founded to some extent upon
disambiguating contexts, this work has some bear-
ing on the evaluation of induced categories with
corpus annotation; not only is there more than
207
one tagset in existence (see discussion in Clark,
2003), but annotation schemes make distinctions
that morphosyntactic contexts cannot readily cap-
ture. For example, there is an implicit notion of in-
herency in the distinction between JJ and NN in the
Penn Treebank (Santorini, 1990, p. 12-13). Fully
outlining these inherent properties could provide
insights into induction and its evaluation.
Acknowledgments
Thanks to the three anonymous reviewers for their
useful comments and to Charles Jochim for helpful
discussion. This material is based upon work sup-
ported by the National Science Foundation under
Grant No. IIS-0623837.
References
Banko, Michele and Robert C. Moore (2004). Part-
of-Speech Tagging in Context. In Proceed-
ings of COLING 2004. Geneva, Switzerland, pp.
556?561.
Boyd, Adriane, Markus Dickinson and Detmar
Meurers (2007). Increasing the Recall of Cor-
pus Annotation Error Detection. In Proceedings
of TLT 2007. Bergen, Norway, pp. 19?30.
Brants, Sabine, Stefanie Dipper, Silvia Hansen,
Wolfgang Lezius and George Smith (2002). The
TIGER Treebank. In Proceedings of TLT-02.
Sozopol, Bulgaria.
Brants, Thorsten (1997). Internal and External
Tagsets in Part-of-Speech Tagging. In Proceed-
ings of Eurospeech. Rhodes, Greece.
Chemla, E., T. H. Mintz, S. Bernal and
A. Christophe (in press). Categorizing words
using ?Frequent Frames?: What cross-linguisic
analyses reveal about core principles. Develop-
mental Science .
Clark, Alexander (2003). Combining Distribu-
tional and Morphological Information for Part
of Speech Induction. In Proceedings of EACL-
03. Budapest, pp. 59?66.
Cutting, Doug, Julian Kupiec, Jan Pedersen and
Penelope Sibun (1992). A Practical part-of-
speech tagger. In Proceedings of the ANLP-92.
Trento, Italy, pp. 133?140.
Daelemans, Walter, Jakub Zavrel, Peter Berck and
Steven Gillis (1996). MBT: A Memory-Based
Part of Speech Tagger-Generator. In Proceed-
ings of the Fourth Workshop on Very Large Cor-
pora (VLC). Copenhagen, pp. 14?27.
Dickinson, Markus (2005). Error detection and
correction in annotated corpora. Ph.D. thesis,
The Ohio State University.
Dickinson, Markus (2007). Determining Ambigu-
ity Classes for Part-of-Speech Tagging. In Pro-
ceedings of RANLP-07. Borovets, Bulgaria.
Dickinson, Markus and W. Detmar Meurers
(2003). Detecting Errors in Part-of-Speech An-
notation. In Proceedings of EACL-03. Budapest,
pp. 107?114.
Goldberg, Yoav, Meni Adler and Michael Elhadad
(2008). EM Can Find Pretty Good HMM POS-
Taggers (When Given a Good Start). In Pro-
ceedings of ACL-08. Columbus, OH, pp. 746?
754.
MacWhinney, Brian (2000). The CHILDES
project: Tools for analyzing talk. Mahwah, NJ:
Lawrence Erlbaum Associates, third edn.
Manning, Christopher D. and Hinrich Sch?utze
(1999). Foundations of Statistical Natural Lan-
guage Processing. Cambridge, MA: The MIT
Press.
Marcus, M., Beatrice Santorini and M. A.
Marcinkiewicz (1993). Building a large anno-
tated corpus of English: The Penn Treebank.
Computational Linguistics 19(2), 313?330.
Mintz, Toben H. (2002). Category induction
from distributional cues in an artificial language.
Memory & Cognition 30, 678?686.
Mintz, Toben H. (2003). Frequent frames as a
cue for grammatical categories in child directed
speech. Cognition 90, 91?117.
Redington, Martin, Nick Chater and Steven Finch
(1998). Distributional Information: A Powerful
Cue for Acquiring Syntactic Categories. Cogni-
tive Science 22(4), 425?469.
Santorini, Beatrice (1990). Part-Of-Speech Tag-
ging Guidelines for the Penn Treebank Project
(3rd Revision, 2nd printing). Tech. Rep. MS-
CIS-90-47, The University of Pennsylvania,
Philadelphia, PA.
Sch?utze, Hinrich (1995). Distributional Part-of-
Speech Tagging. In Proceedings of EACL-95.
Dublin, Ireland, pp. 141?148.
Tseng, Huihsin, Daniel Jurafsky and Christopher
Manning (2005). Morphological features help
POS tagging of unknown words across language
varieties. In Proceedings of the Fourth SIGHAN
Workshop on Chinese Language Processing.
208
Proceedings of the 12th Conference of the European Chapter of the ACL, pages 193?201,
Athens, Greece, 30 March ? 3 April 2009. c?2009 Association for Computational Linguistics
Correcting Dependency Annotation Errors
Markus Dickinson
Indiana University
Bloomington, IN, USA
md7@indiana.edu
Abstract
Building on work detecting errors in de-
pendency annotation, we set out to correct
local dependency errors. To do this, we
outline the properties of annotation errors
that make the task challenging and their
existence problematic for learning. For
the task, we define a feature-based model
that explicitly accounts for non-relations
between words, and then use ambiguities
from one model to constrain a second,
more relaxed model. In this way, we are
successfully able to correct many errors,
in a way which is potentially applicable to
dependency parsing more generally.
1 Introduction and Motivation
Annotation error detection has been explored for
part-of-speech (POS), syntactic constituency, se-
mantic role, and syntactic dependency annotation
(see Boyd et al, 2008, and references therein).
Such work is extremely useful, given the harm-
fulness of annotation errors for training, including
the learning of noise (e.g., Hogan, 2007; Habash
et al, 2007), and for evaluation (e.g., Padro and
Marquez, 1998). But little work has been done
to show the full impact of errors, or what types
of cases are the most damaging, important since
noise can sometimes be overcome (cf. Osborne,
2002). Likewise, it is not clear how to learn from
consistently misannotated data; studies often only
note the presence of errors or eliminate them from
evaluation (e.g., Hogan, 2007), and a previous at-
tempt at correction was limited to POS annotation
(Dickinson, 2006). By moving from annotation
error detection to error correction, we can more
fully elucidate ways in which noise can be over-
come and ways it cannot.
We thus explore annotation error correction and
its feasibility for dependency annotation, a form
of annotation that provides argument relations
among words and is useful for training and testing
dependency parsers (e.g., Nivre, 2006; McDonald
and Pereira, 2006). A recent innovation in depen-
dency parsing, relevant here, is to use the predic-
tions made by one model to refine another (Nivre
and McDonald, 2008; Torres Martins et al, 2008).
This general notion can be employed here, as dif-
ferent models of the data have different predictions
about whch parts are erroneous and can highlight
the contributions of different features. Using dif-
ferences that complement one another, we can be-
gin to sort accurate from inaccurate patterns, by
integrating models in such a way as to learn the
true patterns and not the errors. Although we focus
on dependency annotation, the methods are poten-
tially applicable for different types of annotation,
given that they are based on the similar data repre-
sentations (see sections 2.1 and 3.2).
In order to examine the effects of errors and
to refine one model with another?s information,
we need to isolate the problematic cases. The
data representation must therefore be such that it
clearly allows for the specific identification of er-
rors between words. Thus, we explore relatively
simple models of the data, emphasizing small sub-
structures (see section 3.2). This simple model-
ing is not always rich enough for full dependency
parsing, but different models can reveal conflict-
ing information and are generally useful as part of
a larger system. Graph-based models of depen-
dency parsing (e.g., McDonald et al, 2006), for
example, rely on breaking parsing down into deci-
sions about smaller substructures, and focusing on
pairs of words has been used for domain adapta-
tion (Chen et al, 2008) and in memory-based pars-
ing (Canisius et al, 2006). Exploring annotation
error correction in this way can provide insights
into more general uses of the annotation, just as
previous work on correction for POS annotation
(Dickinson, 2006) led to a way to improve POS
193
tagging (Dickinson, 2007).
After describing previous work on error detec-
tion and correction in section 2, we outline in sec-
tion 3 how we model the data, focusing on individ-
ual relations between pairs of words. In section 4,
we illustrate the difficulties of error correction and
show how simple combinations of local features
perform poorly. Based on the idea that ambigui-
ties from strict, lexical models can constrain more
general POS models, we see improvement in error
correction in section 5.
2 Background
2.1 Error detection
We base our method of error correction on a
form of error detection for dependency annota-
tion (Boyd et al, 2008). The variation n-gram ap-
proach was developed for constituency-based tree-
banks (Dickinson and Meurers, 2003, 2005) and
it detects strings which occur multiple times in
the corpus with varying annotation, the so-called
variation nuclei. For example, the variation nu-
cleus next Tuesday occurs three times in the Wall
Street Journal portion of the Penn Treebank (Tay-
lor et al, 2003), twice labeled as NP and once as
PP (Dickinson and Meurers, 2003).
Every variation detected in the annotation of a
nucleus is classified as either an annotation error
or as a genuine ambiguity. The basic heuristic
for detecting errors requires one word of recur-
ring context on each side of the nucleus. The nu-
cleus with its repeated surrounding context is re-
ferred to as a variation n-gram. While the original
proposal expanded the context as far as possible
given the repeated n-gram, using only the immedi-
ately surrounding words as context is sufficient for
detecting errors with high precision (Boyd et al,
2008). This ?shortest? context heuristic receives
some support from research on first language ac-
quisition (Mintz, 2006) and unsupervised gram-
mar induction (Klein and Manning, 2002).
The approach can detect both bracketing and la-
beling errors in constituency annotation, and we
already saw a labeling error for next Tuesday. As
an example of a bracketing error, the variation nu-
cleus last month occurs within the NP its biggest
jolt last month once with the label NP and once as
a non-constituent, which in the algorithm is han-
dled through a special label NIL.
The method for detecting annotation errors can
be extended to discontinuous constituency annota-
tion (Dickinson and Meurers, 2005), making it ap-
plicable to dependency annotation, where words
in a relation can be arbitrarily far apart. Specifi-
cally, Boyd et al (2008) adapt the method by treat-
ing dependency pairs as variation nuclei, and they
include NIL elements for pairs of words not an-
notated as a relation. The method is successful
at detecting annotation errors in corpora for three
different languages, with precisions of 93% for
Swedish, 60% for Czech, and 48% for German.1
2.2 Error correction
Correcting POS annotation errors can be done by
applying a POS tagger and altering the input POS
tags (Dickinson, 2006). Namely, ambiguity class
information (e.g., IN/RB/RP) is added to each cor-
pus position for training, creating complex ambi-
guity tags, such as <IN/RB/RP,IN>. While this
results in successful correction, it is not clear how
it applies to annotation which is not positional and
uses NIL labels. However, ambiguity class infor-
mation is relevant when there is a choice between
labels; we return to this in section 5.
3 Modeling the data
3.1 The data
For our data set, we use the written portion (sec-
tions P and G) of the Swedish Talbanken05 tree-
bank (Nivre et al, 2006), a reconstruction of the
Talbanken76 corpus (Einarsson, 1976) The written
data of Talbanken05 consists of 11,431 sentences
with 197,123 tokens, annotated using 69 types of
dependency relations.
This is a small sample, but it matches the
data used for error detection, which results in
634 shortest non-fringe variation n-grams, corre-
sponding to 2490 tokens. From a subset of 210
nuclei (917 tokens), hand-evaluation reveals error
detection precision to be 93% (195/210), with 274
(of the 917) corpus positions in need of correction
(Boyd et al, 2008). This means that 643 positions
do not need to be corrected, setting a baseline of
70.1% (643/917) for error correction.2 Following
Dickinson (2006), we train our models on the en-
tire corpus, explicitly including NIL relations (see
1The German experiment uses a more relaxed heuristic;
precision is likely higher with the shortest context heuristic.
2Detection and correction precision are different measure-
ments: for detection, it is the percentage of variation nuclei
types where at least one is incorrect; for correction, it is the
percentage of corpus tokens with the true (corrected) label.
194
section 3.2); we train on the original annotation,
but not the corrections.
3.2 Individual relations
Annotation error correction involves overcoming
noise in the corpus, in order to learn the true
patterns underlying the data. This is a slightly
different goal from that of general dependency
parsing methods, which often integrate a vari-
ety of features in making decisions about depen-
dency relations (cf., e.g., Nivre, 2006; McDon-
ald and Pereira, 2006). Instead of maximizing a
feature model to improve parsing, we isolate in-
dividual pieces of information (e.g., context POS
tags), thereby being able to pinpoint, for example,
when non-local information is needed for particu-
lar types of relations and pointing to cases where
pieces of information conflict (cf. also McDonald
and Nivre, 2007).
To support this isolation of information, we use
dependency pairs as the basic unit of analysis and
assign a dependency label to each word pair. Fol-
lowing Boyd et al (2008), we add L or R to the
label to indicate which word is the head, the left
(L) or the right (R). This is tantamount to han-
dling pairs of words as single entries in a ?lex-
icon? and provides a natural way to talk of am-
biguities. Breaking the representation down into
strings whch receive a label also makes the method
applicable to other annotation types (e.g., Dickin-
son and Meurers, 2005).
A major issue in generating a lexicon is how
to handle pairs of words which are not dependen-
cies. We follow Boyd et al (2008) and generate
NIL labels for those pairs of words which also
occur as a true labeled relation. In other words,
only word pairs which can be relations can also be
NILs. For every sentence, then, when we produce
feature lists (see section 3.3), we produce them for
all word pairs that are related or could potentially
be related, but not those which have never been
observed as a dependency pair. This selection of
NIL items works because there are no unknown
words. We use the method in Dickinson and Meur-
ers (2005) to efficiently calculate the NIL tokens.
Focusing on word pairs and not attempting to
build a a whole dependency graph allows us to ex-
plore the relations between different kinds of fea-
tures, and it has the potential benefit of not rely-
ing on possibly erroneous sister relations. From
the perspective of error correction, we cannot as-
sume that information from the other relations in
the sentence is reliable.3 This representation also
fits nicely with previous work, both in error de-
tection (see section 2.1) and in dependency pars-
ing (e.g., Canisius et al, 2006; Chen et al, 2008).
Most directly, Canisius et al (2006) integrate such
a representation into a memory-based dependency
parser, treating each pair individually, with words
and POS tags as features.
3.3 Method of learning
We employ memory-based learning (MBL) for
correction. MBL stores all corpus instances as
vectors of features, and given a new instance, the
task of the classifier is to find the most similar
cases in memory to deduce the best class. Given
the previous discussion of the goals of correcting
errors, what seems to be needed is a way to find
patterns which do not fully generalize because of
noise appearing in very similar cases in the cor-
pus. As Zavrel et al (1997, p. 137) state about the
advantages of MBL:
Because language-processing tasks typ-
ically can only be described as a com-
plex interaction of regularities, sub-
regularities and (families of) exceptions,
storing all empirical data as potentially
useful in analogical extrapolation works
better than extracting the main regulari-
ties and forgetting the individual exam-
ples (Daelemans, 1996).
By storing all corpus examples, as MBL does,
both correct and incorrect data is maintained, al-
lowing us to pinpoint the effect of errors on train-
ing. For our experiments, we use TiMBL, version
6.1 (Daelemans et al, 2007), with the default set-
tings. We use the default overlap metric, as this
maintains a direct connection to majority-based
correction. We could run TiMBL with different
values of k, as this should lead to better feature
integration. However, this is difficult to explore
without development data, and initial experiments
with higher k values were not promising (see sec-
tion 4.2).
To fully correct every error, one could also ex-
periment with a real dependency parser in the fu-
ture, in order to look beyond the immediate con-
text and to account for interactions between rela-
3We use POS information, which is also prone to errors,
but on a different level of annotation. Still, this has its prob-
lems, as discussed in section 4.1.
195
tions. The approach to correction pursued here,
however, isolates problems for assigning depen-
dency structures, highlighting the effectiveness of
different features within the same local domain.
Initial experiments with a dependency parser were
again not promising (see section 4.2).
3.4 Integrating features
When using features for individual relations, we
have different options for integrating them. On
the one hand, one can simply additively combine
features into a larger vector for training, as de-
scribed in section 4.2. On the other hand, one can
use one set of features to constrain another set,
as described in section 5. Pulling apart the fea-
tures commonly employed in dependency parsing
can help indicate the contributions each has on the
classification.
This general idea is akin to the notion of clas-
sifier stacking, and in the realm of dependency
parsing, Nivre and McDonald (2008) successfully
stack classifiers to improve parsing by ?allow[ing]
a model to learn relative to the predictions of the
other? (p. 951). The output from one classifier
is used as a feature in the next one (see also Tor-
res Martins et al, 2008). Nivre and McDonald
(2008) use different kinds of learning paradigms,
but the general idea can be carried over to a situ-
ation using the same learning mechanism. Instead
of focusing on what one learning algorithm in-
forms another about, we ask what one set of more
or less informative features can inform another set
about, as described in section 5.1.
4 Performing error correction
4.1 Challenges
The task of automatic error correction in some
sense seems straightforward, in that there are no
unknown words. Furthermore, we are looking at
identical recurring words, which should for the
most part have consistent annotation. But it is pre-
cisely this similarity of local contexts that makes
the correction task challenging.
Given that variations contain sets of corpus po-
sitions with differing labels, it is tempting to take
the error detection output and use a heuristic of
?majority rules? for the correction cases, i.e., cor-
rect the cases to the majority label. When us-
ing only information from the word sequence, this
runs into problems quickly, however, in that there
are many non-majority labels which are correct.
Some of these non-majority cases pattern in uni-
form ways and are thus more correctable; oth-
ers are less tractable in being corrected, as they
behave in non-uniform and often non-local ways.
Exploring the differences will highlight what can
and cannot be easily corrected, underscoring the
difficulties in training from erroneous annotation.
Uniform non-majority cases The first problem
with correction to the majority label is an issue
of coverage: a large number of variations are ties
between two different labels. Out of 634 shortest
non-fringe variation nuclei, 342 (53.94%) have no
majority label; for the corresponding 2490 tokens,
749 (30.08%) have no majority tag.
The variation a?r va?g (?is way?), for example, ap-
pears twice with the same local context shown in
(1),4 once incorrectly labeled as OO-L (other ob-
ject [head on the left]) and once correctly as SP-
L (subjective predicative complement). To dis-
tinguish these two, more information is necessary
than the exact sequence of words. In this case, for
example, looking at the POS categories of the nu-
clei could potentially lead to accurate correction:
AV NN is SP-L 1032 times and OO-L 32 times
(AV = the verb ?vara? (be), NN = other noun).
While some ties might require non-local informa-
tion, we can see that local?but more general?
information could accurately break this tie.
(1) ka?rlekens
love?s
va?g
way
a?r/AV
is
en
a
la?ng
long
va?g/NN
way
och
and
. . .
. . .
Secondly, in a surprising number of cases where
there is a majority tag (122 out of the 917 tokens
we have a correction for), a non-majority label
is actually correct. For the example in (2), the
string institution kvarleva (?institution remnant?)
varies between CC-L (sister of first conjunct in bi-
nary branching analysis of coordination) and AN-
L (apposition).5 CC-L appears 5 times and AN-L
3 times, but the CC-L cases are incorrect and need
to be changed to AN-L.
(2) en
an
fo?ra?ldrad
obsolete
institution/NN
institution
,/IK
,
en/EN
a
kvarleva/NN
remnant
fra?n
from
1800-talets
the 1800s
4We put variation nuclei in bold and underline the imme-
diately surrounding context.
5Note that CC is a category introduced in the conversion
from the 1976 to the 2005 corpus.
196
Other cases with a non-majority label have
other problems. In example (3), for instance, the
string under ha?gnet (?under protection?) varies in
this context between HD-L (other head, 3 cases)
and PA-L (complement of preposition, 5 cases),
where the PA-L cases need to be corrected to HD-
L. Both of these categories are new, so part of the
issue here could be in the consistency of the con-
version.
(3) fria
free
liv
life
under/PR
under
ha?gnet/ID|NN
the protection
av/ID|PR
of
ett
a
en
one
ga?ng
time
givet
given
lo?fte
promise
The additional problem is that there are other,
correlated errors in the analysis, as shown in fig-
ure 1. In the case of the correct HD analysis, both
ha?gnet and av are POS-annotated as ID (part of id-
iom (multi-word unit)) and are HD dependents of
under, indicating that the three words make up an
idiom. The PA analysis is a non-idiomatic analy-
sis, with ha?gnet as NN.
AT ET HD HD
fria liv under ha?gnet av ...
AJ NN PR ID ID
AT ET PA PA
fria liv under ha?gnet av ...
AJ NN PR NN PR
Figure 1: Erroneous POS & dependency variation
Significantly, ha?gnet only appears 10 times in
the corpus, all with under as its head, 5 times HD-
L and 5 times PA-L. We will not focus explicitly
on correcting these types of cases, but the example
serves to emphasize the necessity of correction at
all levels of annotation.
Non-uniform non-majority cases All of the
above cases have in common that whatever change
is needed, it needs to be done for all positions in a
variation. But this is not sound, as error detection
precision is not 100%. Thus, there are variations
which clearly must not change.
For example, in (4), there is legitimate varia-
tion between PA-L (4a) and HD-L (4b), stemming
from the fact that one case is non-idiomatic, and
the other is idiomatic, despite having identical lo-
cal context. In these examples, at least the POS
labels are different. Note, though, that in (4) we
need to trust the POS labels to overcome the simi-
larity of text, and in (3) we need to distrust them.6
(4) a. Med/PR
with
andra
other
ord/NN
words
en
an
a?ndama?lsenlig
appropriate
...
b. Med/AB
with
andra
other
ord/ID
words
en
a
form
form
av
of
prostitution
prostitution
.
Without non-local information, some legitimate
variations are virtually irresolvable. Consider (5),
for instance: here, we find variation between SS-R
(other subject), as in (5a), and FS-R (dummy sub-
ject), as in (5b). Crucially, the POS tags are the
same, and the context is the same. What differen-
tiates these cases is that ga?r has a different set of
dependents in the two sentences, as shown in fig-
ure 2; to use this information would require us to
trust the rest of the dependency structure or to use
a dependency parser which accurately derives the
structural differences.
(5) a. Det/PO
it
ga?r/VV
goes
bara
just
inte
not
ihop
together
.
?It just doesn?t add up.?
b. Det/PO
it
ga?r/VV
goes
bara
just
inte
not
att
to
ha?lla
hold
ihop
together
...
...
4.2 Using local information
While some variations require non-local informa-
tion, we have seen that some cases are correctable
simply with different kinds of local information
(cf. (1)). In this paper, we will not attempt to
directly cover non-local cases or cases with POS
annotation problems, instead trying to improve the
integration of different pieces of local information.
In our experiments, we trained simple models of
the original corpus using TiMBL (see section 3.3)
and then tested on the same corpus. The models
we use include words (W) and/or tags (T) for nu-
cleus and/or context positions, where context here
6Rerunning the experiments in the paper by first running
a POS tagger showed slight degradations in precision.
197
SS MA NA PL
Det ga?r bara inte ihop
PO VV AB AB AB
FS CA NA IM ES
Det ga?r bara inte att ha?lla ...
PO VV AB AB IM VV
Figure 2: Correct dependency variation
refers only to the immediately surrounding words.
These are outlined in table 1, for different mod-
els of the nucleus (Nuc.) and the context (Con.).
For instance, the model 6 representation of exam-
ple (6) (=(1)) consists of all the underlined words
and tags.
(6) ka?rlekens va?g/NN a?r/AV en/EN la?ng/AJ
va?g/NN och/++ man go?r oklokt ...
In table 1, we report the precision figures for
different models on the 917 positions we have
corrections for. We report the correction preci-
sion for positions the classifier changed the label
of (Changed), and the overall correction precision
(Overall). We also report the precision TiMBL has
for the whole corpus, with respect to the original
tags (instead of the corrected tags).
# Nuc. Con. TiMBL Changed Overall
1 W - 86.6% 34.0% 62.5%
2 W, T - 88.1% 35.9% 64.8%
3 W W 99.8% 50.3% 72.7%
4 W W, T 99.9% 52.6% 73.5%
5 W, T W 99.9% 50.8% 72.4%
6 W, T W, T 99.9% 51.2% 72.6%
7 T - 73.4% 20.1% 49.5%
8 T T 92.7% 50.2% 73.2%
Table 1: The models tested
We can draw a few conclusions from these re-
sults. First, all models using contexual informa-
tion perform essentially the same?approximately
50% on changed positions and 73% overall. When
not generalizing to new data, simply adding fea-
tures (i.e., words or tags) to the model is less im-
portant than the sheer presence of context. This
is true even for some higher values of k: model
6, for example, has only 73.2% and 72.1% overall
precision for k = 2 and k = 3, respectively.
Secondly, these results confirm that the task is
difficult, even for a corpus with relatively high er-
ror detection precision (see section 2.1). Despite
high similarity of context (e.g., model 6), the best
results are only around 73%, and this is given a
baseline (no changes) of 70%. While a more ex-
pansive set of features would help, there are other
problems here, as the method appears to be over-
training. There is no question that we are learning
the ?correct? patterns, i.e., 99.9% similarity to the
benchmark in the best cases. The problem is that,
for error correction, we have to overcome noise in
the data. Training and testing with the dependency
parser MaltParser (Nivre et al, 2007, default set-
tings) is no better, with 72.1% overall precision
(despite a labeled attachment score of 98.3%).
Recall in this light that there are variations for
which the non-majority label is the correct one;
attempting to get a non-majority label correct us-
ing a strict lexical model does not work. To be
able not to learn the erroneous patterns requires
a more general model. Interestingly, a more gen-
eral model?e.g., treating the corpus as a sequence
of tags (model 8)?results in equally good correc-
tion, without being a good overall fit to the cor-
pus data (only 92.7%). This model, too, learns
noise, as it misses cases that the lexical models get
correct. Simply combining the features does not
help (cf. model 6); what we need is to use infor-
mation from both stricter and looser models in a
way that allows general patterns to emerge with-
out overgeneralizing.
5 Model combination
Given the discussion in section 4.1 surrounding
examples (1)-(5), it is clear that the information
needed for correction is sometimes within the
immediate context, although that information is
needed, however, is often different. Consider the
more general models, 7 and 8, which only use POS
tag information. While sometimes this general in-
formation is effective, at times it is dramatically
incorrect. For example, for (7), the original (incor-
rect) relation between finna and erbjuda is CC-L;
the model 7 classifier selects OO-L as the correct
tag; model 8 selects NIL; and the correct label is
+F-L (coordination at main clause level).
198
(7) fo?rso?ker
try
finna/VV
to find
ett
a
la?mpligt
suitable
arbete
job
i
in
o?ppna
open
marknaden
market
eller
or
erbjuda/VV
to offer
andra
other
arbetsmo?jligheter
work possibilities
.
The original variation for the nucleus finna erb-
juda (?find offer?) is between CC-L and +F-L, but
when represented as the POS tags VV VV (other
verb), there are 42 possible labels, with OO-L be-
ing the most frequent. This allows for too much
confusion. If model 7 had more restrictions on the
set of allowable tags, it could make a more sensi-
ble choice and, in this case, select the correct label.
5.1 Using ambiguity classes
Previous error correction work (Dickinson, 2006)
used ambiguity classes for POS annotation, and
this is precisely the type of information we need
to constrain the label to one which we know is rel-
evant to the current case. Here, we investigate am-
biguity class information derived from one model
integrated into another model.
There are at least two main ways we can use
ambiguity classes in our models. The first is what
we have just been describing: an ambiguity class
can serve as a constraint on the set of possible out-
comes for the system. If the correct label is in the
ambiguity class (as it usually is for error correc-
tion), this constraining can do no worse than the
original model. The other way to use an ambigu-
ity class is as a feature in the model. The success
of this approach depends on whether or not each
ambiguity class patterns in its own way, i.e., de-
fines a sub-regularity within a feature set.
5.2 Experiment details
We consider two different feature models, those
containing only tags (models 7 and 8), and add
to these ambiguity classes derived from two other
models, those containing only words (models 1
and 3). To correct the labels, we need models
which do not strictly adhere to the corpus, and the
tag-based models are best at this (see the TiMBL
results in table 1). The ambiguity classes, how-
ever, must be fairly constrained, and the word-
based models do this best (cf. example (7)).
5.2.1 Ambiguity classes as constraints
As described in section 5.1, we can use ambiguity
classes to constrain the output of a model. Specif-
ically, we take models 7 and 8 and constrain each
selected tag to be one which is within the ambi-
guity class of a lexical model, either 1 or 3. That
is, if the TiMBL-determined label is not in the am-
biguity class, we select the most likely tag of the
ones which are. If no majority label can be de-
cided from this restricted set, we fall back to the
TiMBL-selected tag. In (7), for instance, if we use
model 7, the TiMBL tag is OO-L, but model 3?s
ambiguity class restricts this to either CC-L or +F-
L. For the representation VV VV, the label CC-L
appears 315 times and +F-L 544 times, so +F-L is
correctly selected.7
The results are given in table 2, which can be
compared to the the original models 7 and 8 in ta-
ble 1, i.e., total precisions of 49.5% and 73.2%,
respectively. With these simple constraints, model
8 now outperforms any other model (75.5%), and
model 7 begins to approach all the models that use
contextual information (68.8%).
# AC Changed Total
7 1 28.5% (114/400) 57.4% (526/917)
7 3 45.9% (138/301) 68.8% (631/917)
8 1 54.0% (142/263) 74.8% (686/917)
8 3 56.7% (144/254) 75.5% (692/917)
Table 2: Constraining TiMBL with ACs
5.2.2 Ambiguity classes as features
Ambiguity classes from one model can also be
used as features for another (see section 5.1); in
this case, ambiguity class information from lexical
models (1 and 3) is used as a feature for POS tag
models (7 and 8). The results are given in table 3,
where we can see dramatically improved perfor-
mance from the original models (cf. table 1) and
generally improved performance over using ambi-
guity classes as constraints (cf. table 2).
# AC Changed Total
7 1 33.2% (122/368) 61.9% (568/917)
7 3 50.2% (131/261) 72.1% (661/917)
8 1 59.0% (148/251) 76.4% (701/917)
8 3 55.1% (130/236) 73.6% (675/917)
Table 3: TiMBL with ACs as features
If we compare the two results for model 7
(61.9% vs. 72.1%) and then the two results for
model 8 (76.4% vs. 73.6%), we observe that the
7Even if CC-L had been selected here, the choice is sig-
nificantly better than OO-L.
199
better use of ambiguity classes integrates contex-
tual and non-contextual features. Model 7 (POS,
no context) with model 3 ambiguity classes (lex-
ical, with context) is better than using ambiguity
classes derived from a non-contextual model. For
model 8, on the other hand, which uses contextual
POS features, using the ambiguity class without
context (model 1) does better. In some ways, this
combination of model 8 with model 1 ambiguity
classes makes the most sense: ambiguity classes
are derived from a lexicon, and for dependency an-
notation, a lexicon can be treated as a set of pairs
of words. It is also noteworthy that model 7, de-
spite not using context directly, achieves compara-
ble results to all the previous models using context,
once appropriate ambiguity classes are employed.
5.2.3 Both methods
Given that the results of ambiguity classes as fea-
tures are better than that of constraining, we can
now easily combine both methodologies, by con-
straining the output from section 5.2.2 with the
ambiguity class tags. The results are given in ta-
ble 4; as we can see, all results are a slight im-
provement over using ambiguity classes as fea-
tures without constraining the output (table 3). Us-
ing only local context, the best model here is 3.2%
points better than the best original model, repre-
senting an improvement in correction.
# AC Changed Total
7 1 33.5% (123/367) 62.2% (570/917)
7 3 55.8% (139/249) 74.1% (679/917)
8 1 59.6% (149/250) 76.7% (703/917)
8 3 57.1% (133/233) 74.3% (681/917)
Table 4: TiMBL w/ ACs as features & constraints
6 Summary and Outlook
After outlining the challenges of error correction,
we have shown how to integrate information from
different models of dependency annotation in or-
der to perform annotation error correction. By us-
ing ambiguity classes from lexical models, both as
features and as constraints on the final output, we
saw improvements in POS models that were able
to overcome noise, without using non-local infor-
mation.
A first step in further validating these methods
is to correct other dependency corpora; this is lim-
ited, of course, by the amount of corpora with cor-
rected data available. Secondly, because this work
is based on features and using ambiguity classes, it
can in principle be applied to other types of anno-
tation, e.g., syntactic constituency annotation and
semantic role annotation. In this light, it is inter-
esting to note the connection to annotation error
detection: the work here is in some sense an ex-
tension of the variation n-gram method. Whether
it can be employed as an error detection system on
its own requires future work.
Another way in which this work can be ex-
tended is to explore how these representations and
integration of features can be used for dependency
parsing. There are several issues to work out, how-
ever, in making insights from this work more gen-
eral. First, it is not clear that pairs of words are suf-
ficiently general to treat them as a lexicon, when
one is parsing new data. Secondly, we have ex-
plicit representations for word pairs not annotated
as a dependency relation (i.e., NILs), and these are
constrained by looking at those which are the same
words as real relations. Again, one would have to
determine which pairs of words need NIL repre-
sentations in new data.
Acknowledgements
Thanks to Yvonne Samuelsson for help with the
Swedish examples; to Joakim Nivre, Mattias Nils-
son, and Eva Pettersson for the evaluation data for
Talbanken05; and to the three anonymous review-
ers for their insightful comments.
References
Boyd, Adriane, Markus Dickinson and Detmar
Meurers (2008). On Detecting Errors in Depen-
dency Treebanks. Research on Language and
Computation 6(2), 113?137.
Canisius, Sander, Toine Bogers, Antal van den
Bosch, Jeroen Geertzen and Erik Tjong Kim
Sang (2006). Dependency parsing by infer-
ence over high-recall dependency predictions.
In Proceedings of CoNLL-X. New York.
Chen, Wenliang, Youzheng Wu and Hitoshi Isa-
hara (2008). Learning Reliable Information for
Dependency Parsing Adaptation. In Proceed-
ings of Coling 2008. Manchester.
Daelemans, Walter (1996). Abstraction Consid-
ered Harmful: Lazy Learning of Language Pro-
cessing. In Proceedings of the 6th Belgian-
Dutch Conference on Machine Learning. Maas-
tricht, The Netherlands.
200
Daelemans, Walter, Jakub Zavrel, Ko Van der
Sloot and Antal Van den Bosch (2007). TiMBL:
Tilburg Memory Based Learner, version 6.1,
Reference Guide. Tech. rep., ILK Research
Group. ILK Research Group Technical Report
Series no. 07-07.
Dickinson, Markus (2006). From Detecting Errors
to Automatically Correcting Them. In Proceed-
ings of EACL-06. Trento, Italy.
Dickinson, Markus (2007). Determining Ambigu-
ity Classes for Part-of-Speech Tagging. In Pro-
ceedings of RANLP-07. Borovets, Bulgaria.
Dickinson, Markus and W. Detmar Meurers
(2003). Detecting Inconsistencies in Treebanks.
In Proceedings of TLT-03. Va?xjo?, Sweden.
Dickinson, Markus and W. Detmar Meurers
(2005). Detecting Errors in Discontinuous
Structural Annotation. In Proceedings of ACL-
05.
Einarsson, Jan (1976). Talbankens skrift-
spr?akskonkordans. Tech. rep., Lund Univer-
sity, Dept. of Scandinavian Languages.
Habash, Nizar, Ryan Gabbard, Owen Rambow,
Seth Kulick and Mitch Marcus (2007). Deter-
mining Case in Arabic: Learning Complex Lin-
guistic Behavior Requires Complex Linguistic
Features. In Proceedings of EMNLP-07.
Hogan, Deirdre (2007). Coordinate Noun Phrase
Disambiguation in a Generative Parsing Model.
In Proceedings of ACL-07. Prague.
Klein, Dan and Christopher D. Manning (2002). A
Generative Constituent-Context Model for Im-
proved Grammar Induction. In Proceedings of
ACL-02. Philadelphia, PA.
McDonald, Ryan, Kevin Lerman and Fernando
Pereira (2006). Multilingual Dependency Anal-
ysis with a Two-Stage Discriminative Parser. In
Proceedings of CoNLL-X. New York City.
McDonald, Ryan and Joakim Nivre (2007). Char-
acterizing the Errors of Data-Driven Depen-
dency Parsing Models. In Proceedings of
EMNLP-CoNLL-07. Prague, pp. 122?131.
McDonald, Ryan and Fernando Pereira (2006).
Online learning of approximate dependency
parsing algorithms. In Proceedings of EACL-
06. Trento.
Mintz, Toben H. (2006). Finding the verbs: dis-
tributional cues to categories available to young
learners. In K. Hirsh-Pasek and R. M. Golinkoff
(eds.), Action Meets Word: How Children Learn
Verbs, New York: Oxford University Press, pp.
31?63.
Nivre, Joakim (2006). Inductive Dependency
Parsing. Berlin: Springer.
Nivre, Joakim, Johan Hall, Jens Nilsson, Atanas
Chanev, Gulsen Eryigit, Sandra Kubler, Sve-
toslav Marinov and Erwin Marsi (2007). Malt-
Parser: A language-independent system for
data-driven dependency parsing. Natural Lan-
guage Engineering 13(2), 95?135.
Nivre, Joakim and Ryan McDonald (2008). Inte-
grating Graph-Based and Transition-Based De-
pendency Parsers. In Proceedings of ACL-08:
HLT . Columbus, OH.
Nivre, Joakim, Jens Nilsson and Johan Hall
(2006). Talbanken05: A Swedish Treebank
with Phrase Structure and Dependency Annota-
tion. In Proceedings of LREC-06. Genoa, Italy.
Osborne, Miles (2002). Shallow Parsing using
Noisy and Non-Stationary Training Material. In
JMLR Special Issue on Machine Learning Ap-
proaches to Shallow Parsing, vol. 2, pp. 695?
719.
Padro, Lluis and Lluis Marquez (1998). On the
Evaluation and Comparison of Taggers: the Ef-
fect of Noise in Testing Corpora. In Proceed-
ings of ACL-COLING-98. San Francisco, CA.
Taylor, Ann, Mitchell Marcus and Beatrice San-
torini (2003). The Penn Treebank: An
Overview. In Anne Abeille? (ed.), Treebanks:
Building and using syntactically annotated cor-
pora, Dordrecht: Kluwer, chap. 1, pp. 5?22.
Torres Martins, Andre? Filipe, Dipanjan Das,
Noah A. Smith and Eric P. Xing (2008). Stack-
ing Dependency Parsers. In Proceedings of
EMNLP-08. Honolulu, Hawaii, pp. 157?166.
Zavrel, Jakub, Walter Daelemans and Jorn Veensta
(1997). Resolving PP attachment Ambiguities
with Memory-Based Learning. In Proceedings
of CoNLL-97. Madrid.
201
Proceedings of ACL-08: HLT, pages 362?370,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Ad Hoc Treebank Structures
Markus Dickinson
Department of Linguistics
Indiana University
md7@indiana.edu
Abstract
We outline the problem of ad hoc rules in
treebanks, rules used for specific construc-
tions in one data set and unlikely to be used
again. These include ungeneralizable rules,
erroneous rules, rules for ungrammatical text,
and rules which are not consistent with the rest
of the annotation scheme. Based on a sim-
ple notion of rule equivalence and on the idea
of finding rules unlike any others, we develop
two methods for detecting ad hoc rules in flat
treebanks and show they are successful in de-
tecting such rules. This is done by examin-
ing evidence across the grammar and without
making any reference to context.
1 Introduction and Motivation
When extracting rules from constituency-based tree-
banks employing flat structures, grammars often
limit the set of rules (e.g., Charniak, 1996), due
to the large number of rules (Krotov et al, 1998)
and ?leaky? rules that can lead to mis-analysis (Foth
and Menzel, 2006). Although frequency-based cri-
teria are often used, these are not without problems
because low-frequency rules can be valid and po-
tentially useful rules (see, e.g., Daelemans et al,
1999), and high-frequency rules can be erroneous
(see., e.g., Dickinson and Meurers, 2005). A key
issue in determining the rule set is rule generaliz-
ability: will these rules be needed to analyze new
data? This issue is of even more importance when
considering the task of porting a parser trained on
one genre to another genre (e.g., Gildea, 2001). In-
frequent rules in one genre may be quite frequent in
another (Sekine, 1997) and their frequency may be
unrelated to their usefulness for parsing (Foth and
Menzel, 2006). Thus, we need to carefully consider
the applicability of rules in a treebank to new text.
Specifically, we need to examine ad hoc rules,
rules used for particular constructions specific to one
data set and unlikely to be used on new data. This is
why low-frequency rules often do not extend to new
data: if they were only used once, it was likely for
a specific reason, not something we would expect to
see again. Ungeneralizable rules, however, do not
extend to new text for a variety of reasons, not all of
which can be captured strictly by frequency.
While there are simply phenomena which, for var-
ious reasons, are rarely used (e.g., long coordinated
lists), other ungeneralizable phenomena are poten-
tially more troubling. For example, when ungram-
matical or non-standard text is used, treebanks em-
ploy rules to cover it, but do not usually indicate un-
grammaticality in the annotation. These rules are
only to be used in certain situations, e.g., for ty-
pographical conventions such as footnotes, and the
fact that the situation is irregular would be useful
to know if the purpose of an induced grammar is
to support robust parsing. And these rules are out-
right damaging if the set of treebank rules is in-
tended to accurately capture the grammar of a lan-
guage. This is true of precision grammars, where
analyses can be more or less preferred (see, e.g.,
Wagner et al, 2007), and in applications like in-
telligent computer-aided language learning, where
learner input is parsed to detect what is correct or
not (see, e.g., Vandeventer Faltin, 2003, ch. 2). If a
treebank grammar is used (e.g., Metcalf and Boyd,
362
2006), then one needs to isolate rules for ungram-
matical data, to be able to distinguish grammatical
from ungrammatical input.
Detecting ad hoc rules can also reveal issues re-
lated to rule quality. Many ad hoc rules exist be-
cause they are erroneous. Not only are errors in-
herently undesirable for obtaining an accurate gram-
mar, but training on data with erroneous rules can
be detrimental to parsing performance (e.g., Dickin-
son and Meurers, 2005; Hogan, 2007) As annotation
schemes are not guaranteed to be completely con-
sistent, other ad hoc rules point to non-uniform as-
pects of the annotation scheme. Thus, identifying ad
hoc rules can also provide feedback on annotation
schemes, an especially important step if one is to
use the treebank for specific applications (see, e.g.,
Vadas and Curran, 2007), or if one is in the process
of developing a treebank.
Although statistical techniques have been em-
ployed to detect anomalous annotation (Ule and
Simov, 2004; Eskin, 2000), these methods do not
account for linguistically-motivated generalizations
across rules, and no full evaluation has been done
on a treebank. Our starting point for detecting ad
hoc rules is also that they are dissimilar to the rest
of the grammar, but we rely on a notion of equiva-
lence which accounts for linguistic generalizations,
as described in section 2. We generalize equivalence
in a corpus-independent way in section 3 to detect
ad hoc rules, using two different methods to deter-
mine when rules are dissimilar. The results in sec-
tion 4 show the success of the method in identifying
all types of ad hoc rules.
2 Background
2.1 Equivalence classes
To define dissimilarity, we need a notion of simi-
larity, and, a starting point for this is the error de-
tection method outlined in Dickinson and Meurers
(2005). Since most natural language expressions are
endocentric, i.e., a category projects to a phrase of
the same category (e.g., X-bar Schema, Jackendoff,
1977), daughters lists with more than one possible
mother are flagged as potentially containing an er-
ror. For example, IN NP1 has nine different mothers
in the Wall Street Journal (WSJ) portion of the Penn
1Appendix A lists all categories used in this paper.
Treebank (Marcus et al, 1993), six of which are er-
rors.
This method can be extended to increase recall, by
treating similar daughters lists as equivalent (Dick-
inson, 2006, 2008). For example, the daughters lists
ADVP RB ADVP and ADVP , RB ADVP in (1) can
be put into the same equivalence class, because they
predict the same mother category. With this equiv-
alence, the two different mothers, PP and ADVP,
point to an error (in PP).
(1) a. to slash its work force in the U.S. , [PP
[ADV P as] soon/RB [ADV P as next month]]
b. to report ... [ADV P [ADV P immediately] ,/,
not/RB [ADV P a month later]]
Anything not contributing to predicting the
mother is ignored in order to form equivalence
classes. Following the steps below, 15,989 daugh-
ters lists are grouped into 3783 classes in the WSJ.
1. Remove daughter categories that are always
non-predictive to phrase categorization, i.e., al-
ways adjuncts, such as punctuation and the par-
enthetical (PRN) category.
2. Group head-equivalent lexical categories, e.g.,
NN (common noun) and NNS (plural noun).
3. Model adjacent identical elements as a single
element, e.g., NN NN becomes NN.
While the sets of non-predictive and head-equivalent
categories are treebank-specific, they require only a
small amount of manual effort.
2.2 Non-equivalence classes
Rules in the same equivalence class not only pre-
dict the same mother, they provide support that the
daughters list is accurate?the more rules within a
class, the better evidence that the annotation scheme
legitimately licenses that sequence. A lack of simi-
lar rules indicates a potentially anomalous structure.
Of the 3783 equivalence classes for the whole
WSJ, 2141 are unique, i.e., have only one unique
daughters list. For example, in (2), the daughters
list RB TO JJ NNS is a daughters list with no corre-
lates in the treebank; it is erroneous because close to
wholesale needs another layer of structure, namely
adjective phrase (ADJP) (Bies et al, 1995, p. 179).
363
(2) they sell [merchandise] for [NP close/RB
to/TO wholesale/JJ prices/NNS ]
Using this strict equivalence to identify ad hoc
rules is quite successful (Dickinson, 2008), but
it misses a significant number of generalizations.
These equivalences were not designed to assist in
determining linguistic patterns from non-linguistic
patterns, but to predict the mother category, and thus
many correct rules are incorrectly flagged. To pro-
vide support for the correct rule NP ? DT CD JJS
NNP JJ NNS in (3), for instance, we need to look
at some highly similar rules in the treebank, e.g.,
the three instances of NP ? DT CD JJ NNP NNS,
which are not strictly equivalent to the rule in (3).
(3) [NP the/DT 100/CD largest/JJS Nasdaq/NNP
financial/JJ stocks/NNS ]
3 Rule dissimilarity and generalizability
3.1 Criteria for rule equivalence
With a notion of (non-)equivalence as a heuristic, we
can begin to detect ad hoc rules. First, however, we
need to redefine equivalence to better reflect syntac-
tic patterns.
Firstly, in order for two rules to be in the
same equivalence class?or even to be similar?the
mother must also be the same. This captures the
property that identical daughters lists with differ-
ent mothers are distinct (cf. Dickinson and Meurers,
2005). For example, looking back at (1), the one
occurrence of ADVP? ADVP , RB ADVP is very
similar to the 4 instances of ADVP ? RB ADVP,
whereas the one instance of PP?ADVP RB ADVP
is not and is erroneous. Daughters lists are thus now
only compared to rules with the same mother.
Secondly, we use only two steps to determine
equivalence: 1) remove non-predictive daughter cat-
egories, and 2) group head-equivalent lexical cat-
egories.2 While useful for predicting the same
mother, the step of Kleene reduction is less useful
for our purposes since it ignores potential differ-
ences in argument structure. It is important to know
how many identical categories can appear within a
given rule, to tell whether it is reliable; VP ? VB
2See Dickinson (2006) for the full mappings.
NP and VP? VB NP NP, for example, are two dif-
ferent rules.3
Thirdly, we base our scores on token counts, in or-
der to capture the fact that the more often we observe
a rule, the more reliable it seems to be. This is not
entirely true, as mentioned above, but this prevents
frequent rules such as NP? EX (1075 occurrences)
from being seen as an anomaly.
With this new notion of equivalence, we can now
proceed to accounting for similar rules in detecting
ad hoc rules.
3.2 Reliability scores
In order to devise a scoring method to reflect simi-
lar rules, the simplest way is to use a version of edit
distance between rules, as we do under the Whole
daughters scoring below. This reflects the intuition
that rules with similar lists of daughters reflect the
same properties. This is the ?positive? way of scor-
ing rules, in that we start with a basic notion of
equivalence and look for more positive evidence that
the rule is legitimate. Rules without such evidence
are likely ad hoc.
Our goal, though, is to take the results and exam-
ine the anomalous rules, i.e., those which lack strong
evidence from other rules. We can thus more di-
rectly look for ?negative? evidence that a rule is ad
hoc. To do this, we can examine the weakest parts
of each rule and compare those across the corpus, to
see which anomalous patterns emerge; we do this in
the Bigram scoring section below.
Because these methods exploit different proper-
ties of rules and use different levels of abstraction,
they have complementary aspects. Both start with
the same assumptions about what makes rules equiv-
alent, but diverge in how they look for rules which
do not fit well into these equivalences.
Whole daughters scoring The first method to de-
tect ad hoc rules directly accounts for similar rules
across equivalence classes. Each rule type is as-
signed a reliability score, calculated as follows:
1. Map a rule to its equivalence class.
2. For every rule token within the equivalence
class, add a score of 1.
3Experiments done with Kleene reduction show that the re-
sults are indeed worse.
364
3. For every rule token within a highly similar
equivalence class, add a score of 12 .
Positive evidence that a rule is legitimate is ob-
tained by looking at similar classes in step #3, and
then rules with the lowest scores are flagged as po-
tentially ad hoc (see section 4.1). To determine
similarity, we use a modified Levenshtein distance,
where only insertions and deletions are allowed; a
distance of one qualifies as highly similar.4 Allow-
ing two or more changes would be problematic for
unary rules (e.g., (4a), and in general, would allow
us to add and subtract dissimilar categories. We thus
remain conservative in determining similarity.
Also, we do not utilize substitutions: while they
might be useful in some cases, it is too problematic
to include them, given the difference in meaning of
each category. Consider the problematic rules in (4).
In (4a), which occurs once, if we allow substitutions,
then we will find 760 ?comparable? instances of VP
? VB, despite the vast difference in category (verb
vs. adverb). Likewise, the rule in (4b), which occurs
8 times, would be ?comparable? to the 602 instances
of PP ? IN PP, used for multi-word prepositions
like because of.5 To maintain these true differences,
substitutions are not allowed.
(4) a. VP? RB
b. PP? JJ PP
This notion of similarity captures many general-
izations, e.g., that adverbial phrases are optional.
For example, in (5), the rule reduces to S ? PP
ADVP NP ADVP VP. With a strict notion of equiv-
alence, there are no comparable rules. However, the
class S ? PP NP ADVP VP, with 198 members,
is highly similar, indicating more confidence in this
correct rule.
(5) [S [PP During his years in Chiriqui] ,/, [ADV P
however] ,/, [NP Mr. Noriega] [ADV P also]
[V P revealed himself as an officer as perverse
as he was ingenious] ./. ]
4The score is thus more generally 11+distance , although we
ascribe no theoretical meaning to this
5Rules like PP ? JJ PP might seem to be correct, but this
depends upon the annotation scheme. Phrases starting with due
to are sometimes annotated with this rule, but they also occur
as ADJP or ADVP or with due as RB. If PP? JJ PP is correct,
identifying this rule actually points to other erroneous rules.
Bigram scoring The other method of detecting ad
hoc rules calculates reliability scores by focusing
specifically on what the classes do not have in com-
mon. Instead of examining and comparing rules in
their entirety, this method abstracts a rule to its com-
ponent parts, similar to features using information
about n-grams of daughter nodes in parse reranking
models (e.g., Collins and Koo, 2005).
We abstract to bigrams, including added START
and END tags, as longer sequences risk missing gen-
eralizations; e.g., unary rules would have no compa-
rable rules. We score rule types as follows:
1. Map a rule to its equivalence class, resulting in
a reduced rule.
2. Calculate the frequency of each
<mother,bigram> pair in a reduced rule:
for every reduced rule token with the same
pair, add a score of 1 for that bigram pair.
3. Assign the score of the least-frequent bigram as
the score of the rule.
We assign the score of the lowest-scoring bigram
because we are interested in anomalous sequences.
This is in the spirit of Kve?ton and Oliva (2002),
who define invalid bigrams for POS annotation se-
quences in order to detect annotation errors..
As one example, consider (6), where the reduced
rule NP? NP DT NNP is composed of the bigrams
START NP, NP DT, DT NNP, and NNP END. All of
these are relatively common (more than a hundred
occurrences each), except for NP DT, which appears
in only two other rule types. Indeed, DT is an in-
correct tag (NNP is correct): when NP is the first
daughter of NP, it is generally a possessive, preclud-
ing the use of a determiner.
(6) (NP (NP ABC ?s) (?? ??) (DT This) (NNP
Week))
The whole daughters scoring misses such prob-
lematic structures because it does not explicitly look
for anomalies. The disadvantage of the bigram scor-
ing, however, is its missing of the big picture: for
example, the erroneous rule NP?NNP CC NP gets
a large score (1905) because each subsequence is
quite common. But this exact sequence is rather rare
(NNP and NP are not generally coordinated), so the
whole daughters scoring assigns a low score (4.0).
365
4 Evaluation
To gauge our success in detecting ad hoc rules, we
evaluate the reliability scores in two main ways: 1)
whether unreliable rules generalize to new data (sec-
tion 4.1), and, more importantly, 2) whether the un-
reliable rules which do generalize are ad hoc in other
ways?e.g., erroneous (section 4.2). To measure
this, we use sections 02-21 of the WSJ corpus as
training data to derive scores, section 23 as testing
data, and section 24 as development data.
4.1 Ungeneralizable rules
To compare the effectiveness of the two scoring
methods in identifying ungeneralizable rules, we ex-
amine how many rules from the training data do not
appear in the heldout data, for different thresholds.
In figure 1, for example, the method identifies 3548
rules with scores less than or equal to 50, 3439 of
which do not appear in the development data, result-
ing in an ungeneralizability rate of 96.93%.
To interpret the figures below, we first need to
know that of the 15,246 rules from the training data,
1832 occur in the development data, or only 12.02%,
corresponding to 27,038 rule tokens. There are also
396 new rules in the development data, making for a
total of 2228 rule types and 27,455 rule tokens.
4.1.1 Development data results
The results are shown in figure 1 for the whole
daughters scoring method and in figure 2 for the bi-
gram method. Both methods successfully identify
rules with little chance of occurring in new data, the
whole daughters method performing slightly better.
Thresh. Rules Unused Ungen.
1 311 311 100.00%
25 2683 2616 97.50%
50 3548 3439 96.93%
100 4596 4419 96.15%
Figure 1: Whole daughter ungeneralizability (devo.)
4.1.2 Comparing across data
Is this ungeneralizability consistent over different
data sets? To evaluate this, we use the whole daugh-
ters scoring method, since it had a higher ungener-
alizability rate in the development data, and we use
Thresh. Rules Unused Ungen.
1 599 592 98.83%
5 1661 1628 98.01%
10 2349 2289 97.44%
15 2749 2657 96.65%
20 3120 2997 96.06%
Figure 2: Bigram ungeneralizability (devo.)
section 23 of the WSJ and the Brown corpus portion
of the Penn Treebank.
Given different data sizes, we now report the cov-
erage of rules in the heldout data, for both type and
token counts. For instance, in figure 3, for a thresh-
old of 50, 108 rule types appear in the development
data, and they appear 141 times. With 2228 total
rule types and 27,455 rule tokens, this results in cov-
erages of 4.85% and 0.51%, respectively.
In figures 3, 4, and 5, we observe the same trends
for all data sets: low-scoring rules have little gener-
alizability to new data. For a cutoff of 50, for exam-
ple, rules at or below this mark account for approxi-
mately 5% of the rule types used in the data and half
a percent of the tokens.
Types Tokens
Thresh. Used Cov. Used Cov.
10 23 1.03% 25 0.09%
25 67 3.01% 78 0.28%
50 108 4.85% 141 0.51%
100 177 7.94% 263 0.96%
All 1832 82.22% 27,038 98.48%
Figure 3: Coverage of rules in WSJ, section 24
Types Tokens
Thresh. Used Cov. Used Cov.
10 33 1.17% 39 0.08%
25 82 2.90% 117 0.25%
50 155 5.49% 241 0.51%
100 242 8.57% 416 0.88%
All 2266 80.24% 46,375 98.74%
Figure 4: Coverage of rules in WSJ, section 23
Note in the results for the larger Brown corpus
that the percentage of overall rule types from the
366
Types Tokens
Thresh. Used Cov. Used Cov.
10 187 1.51% 603 0.15%
25 402 3.25% 1838 0.45%
50 562 4.54% 2628 0.64%
100 778 6.28% 5355 1.30%
All 4675 37.75% 398,136 96.77%
Figure 5: Coverage of rules in Brown corpus
training data is only 37.75%, vastly smaller than the
approximately 80% from either WSJ data set. This
illustrates the variety of the grammar needed to parse
this data versus the grammar used in training.
We have isolated thousands of rules with little
chance of being observed in the evaluation data, and,
as we will see in the next section, many of the rules
which appear are problematic in other ways. The
ungeneralizabilty results make sense, in light of the
fact that reliability scores are based on token counts.
Using reliability scores, however, has the advantage
of being able to identify infrequent but correct rules
(cf. example (5)) and also frequent but unhelpful
rules. For example, in (7), we find erroneous cases
from the development data of the rules WHNP ?
WHNP WHPP (five should be NP) and VP? NNP
NP (OKing should be VBG). These rules appear 27
and 16 times, respectively, but have scores of only
28.0 and 30.5, showing their unreliability. Future
work can separate the effect of frequency from the
effect of similarity (see also section 4.3).
(7) a. [WHNP [WHNP five] [WHPP of whom]]
b. received hefty sums for * [V P OKing/NNP
[NP the purchase of ...]]
4.2 Other ad hoc rules
The results in section 4.1 are perhaps unsuprising,
given that many of the identified rules are simply
rare. What is important, therefore, is to figure out
why some rules appeared in the heldout data at
all. As this requires qualitative analysis, we hand-
examined the rules appearing in the development
data. We set out to examine about 100 rules, and
so we report only for the corresponding threshold,
finding that ad hoc rules are predominant.
For the whole daughters scoring, at the 50 thresh-
old, 55 (50.93%) of the 108 rules in the development
data are errors. Adding these to the ungeneralizable
rules, 98.48% (3494/3548) of the 3548 rules are un-
helpful for parsing, at least for this data set. An ad-
ditional 12 rules cover non-English or fragmented
constructions, making for 67 clearly ad hoc rules.
For the bigram scoring, at the 20 threshold, 67
(54.47%) of the 123 rules in the development data
are erroneous, and 8 more are ungrammatical. This
means that 97.88% (3054/3120) of the rules at this
threshold are unhelpful for parsing this data, still
slightly lower than the whole daughters scoring.
4.2.1 Problematic cases
But what about the remaining rules for both meth-
ods which are not erroneous or ungrammatical?
First, as mentioned at the outset, there are several
cases which reveal non-uniformity in the annota-
tion scheme or guidelines. This may be justifiable,
but it has an impact on grammars using the annota-
tion scheme. Consider the case of NAC (not a con-
stituent), used for complex NP premodifiers. The
description for tagging titles in the guidelines (Bies
et al, 1995, p. 208-209) covers the exact case found
in section 24, shown in (8a). This rule, NAC? NP
PP, is one of the lowest-scoring rules which occurs,
with a whole daughters score of 2.5 and a bigram
score of 3, yet it is correct. Examining the guide-
lines more closely, however, we find examples such
as (8b). Here, no extra NP layer is added, and it is
not immediately clear what the criteria are for hav-
ing an intermediate NP.
(8) a. a ? [NAC [NP Points] [PP of Light]] ? foun-
dation
b. The Wall Street Journal ? [NAC American
Way [PP of Buying]] ? Survey
Secondly, rules with mothers which are simply
rare are prone to receive lower scores, regardless of
their generalizability. For example, the rules dom-
inated by SINV, SQ, or SBARQ are all correct (6
in whole daughters, 5 in bigram), but questions are
not very frequent in this news text: SQ appears
only 350 times and SBARQ 222 times in the train-
ing data. One might thus consider normalizing the
scores based on the overall frequency of the parent.
Finally, and most prominently, there are issues
with coordinate structures. For example, NP? NN
CC DT receives a low whole daughters score of 7.0,
367
despite the fact that NP ? NN and NP ? DT are
very common rules. This is a problem for both meth-
ods: for the whole daughters scoring, of the 108,
28 of them had a conjunct (CC or CONJP) in the
daughters list, and 18 of these were correct. Like-
wise, for the bigram scoring, 18 had a conjunct, and
12 were correct. Reworking similarity scores to re-
flect coordinate structures and handle each case sep-
arately would require treebank-specific knowledge:
the Penn Treebank, for instance, distinguishes unlike
coordinated phrases (UCP) from other coordinated
phrases, each behaving differently.
4.2.2 Comparing the methods
There are other cases in which one method out-
performs the other, highlighting their strengths and
weaknesses. In general, both methods fare badly
with clausal rules, i.e., those dominated by S, SBAR,
SINV, SQ, or SBARQ, but the effect is slightly
greater on the bigram scoring, where 20 of the 123
rules are clausal, and 16 of these are correct (i.e.,
80% of them are misclassified). To understand this,
we have to realize that most modifiers are adjoined
at the sentence level when there is any doubt about
their attachment (Bies et al, 1995, p. 13), leading to
correct but rare subsequences. In sentence (9), for
example, the reduced rule S ? SBAR PP NP VP
arises because both the introductory SBAR and the
PP are at the same level. This SBAR PP sequence is
fairly rare, resulting in a bigram score of 13.
(9) [S [SBAR As the best opportunities for corpo-
rate restructurings are exhausted * of course]
,/, [PP at some point] [NP the market] [V P will
start * to reject them] ./.]
Whole daughters scoring, on the other hand, assigns
this rule a high reliability score of 2775.0, due to
the fact that both SBAR NP VP and PP NP VP
sequences are common. For rules with long mod-
ifier sequences, whole daughters scoring seems to
be more effective since modifiers are easily skipped
over in comparing to other rules. Whole daughters
scoring is also imprecise with clausal rules (10/12
are misclassified), but identifies less of them, and
they tend to be for rare mothers (see above).
Various cases are worse for the whole daughters
scoring. First are quantifier phrases (QPs), which
have a highly varied set of possible heads and argu-
ments. QP is ?used for multiword numerical expres-
sions that occur within NP (and sometimes ADJP),
where the QP corresponds frequently to some kind
of complex determiner phrase? (Bies et al, 1995, p.
193). This definition leads to rules which look dif-
ferent from QP to QP. Some of the lowest-scoring,
correct rules are shown in (10). We can see that there
is not a great deal of commonality about what com-
prises quantifier phrases, even if subparts are com-
mon and thus not flagged by the bigram method.
(10) a. [QP only/RB three/CD of/IN the/DT
nine/CD] justices
b. [QP too/RB many/JJ] cooks
c. 10 % [QP or/CC more/JJR]
Secondly, whole daughters scoring relies on com-
plete sequences, and thus whether Kleene reduction
(step #3 in section 2) is used makes a marked dif-
ference. For example, in (11), the rule NP? DT JJ
NNP NNP JJ NN NN is completely correct, despite
its low whole daughters score of 15.5 and one oc-
currence. This rule is similar to the 10 occurrences
of NP ? DT JJ NNP JJ NN in the training set, but
we cannot see this without performing Kleene re-
duction. For noun phrases at least, using Kleene re-
duction might more accurately capture comparabil-
ity. This is less of an issue for bigram scoring, as
all the bigrams are perfectly valid, resulting here in
a relatively high score (556).
(11) [NP the/DT basic/JJ Macintosh/NNP
Plus/NNP central/JJ processing/NN unit/NN ]
4.3 Discriminating rare rules
In an effort to determine the effectiveness of the
scores on isolating structures which are not linguis-
tically sound, in a way which factors out frequency,
we sampled 50 rules occurring only once in the
training data. We marked for each whether it was
correct or how it was ad hoc, and we did this blindly,
i.e., without knowledge of the rule scores. Of these
50, only 9 are errors, 2 cover ungrammatical con-
structions, and 8 more are unclear. Looking at the
bottom 25 scores, we find that the whole daughters
and bigrams methods both find 6 errors, or 67% of
them, additionally finding 5 unclear cases for the
whole daughters and 6 for the bigrams method. Er-
roneous rules in the top half appear to be ones which
368
happened to be errors, but could actually be correct
in other contexts (e.g.,NP ? NN NNP NNP CD).
Although it is a small data set, the scores seem to be
effectively sorting rare rules.
5 Summary and Outlook
We have outlined the problem of ad hoc rules in
treebanks?ungeneralizable rules, erroneous rules,
rules for ungrammatical text, and rules which are not
necessarily consistent with the rest of the annotation
scheme. Based on the idea of finding rules unlike
any others, we have developed methods for detecting
ad hoc rules in flat treebanks, simply by examining
properties across the grammar and without making
any reference to context.
We have been careful not to say how to use
the reliability scores. First, without 100% accu-
racy, it is hard to know what their removal from
a parsing model would mean. Secondly, assign-
ing confidence scores to rules, as we have done,
has a number of other potential applications. Parse
reranking techniques, for instance, rely on knowl-
edge about features other than those found in the
core parsing model in order to determine the best
parse (e.g., Collins and Koo, 2005; Charniak and
Johnson, 2005). Active learning techniques also re-
quire a scoring function for parser confidence (e.g.,
Hwa et al, 2003), and often use uncertainty scores
of parse trees in order to select representative sam-
ples for learning (e.g., Tang et al, 2002). Both could
benefit from more information about rule reliability.
Given the success of the methods, we can strive
to make them more corpus-independent, by remov-
ing the dependence on equivalence classes. In some
ways, comparing rules to similar rules already natu-
rally captures equivalences among rules. In this pro-
cess, it will also be important to sort out the impact
of similarity from the impact of frequency on iden-
tifying ad hoc structures.
Acknowledgments
Thanks to the three anonymous reviewers for their
helpful comments. This material is based upon work
supported by the National Science Foundation under
Grant No. IIS-0623837.
A Relevant Penn Treebank categories
CC Coordinating conjunction
CD Cardinal number
DT Determiner
EX Existential there
IN Preposition or subordinating conjunction
JJ Adjective
JJR Adjective, comparative
JJS Adjective, superlative
NN Noun, singular or mass
NNS Noun, plural
NNP Proper noun, singular
RB Adverb
TO to
VB Verb, base form
VBG Verb, gerund or present participle
Figure 6: POS tags in the PTB (Santorini, 1990)
ADJP Adjective Phrase
ADVP Adverb Phrase
CONJP Conjunction Phrase
NAC Not A Constituent
NP Noun Phrase
PP Prepositional Phrase
PRN Parenthetical
QP Quantifier Phrase
S Simple declarative clause
SBAR Clause introduced by subordinating conjunction
SBARQ Direct question introduced by wh-word/phrase
SINV Inverted declarative sentence
SQ Inverted yes/no question
UCP Unlike Coordinated Phrase
VP Verb Phrase
WHNP Wh-noun Phrase
WHPP Wh-prepositional Phrase
Figure 7: Syntactic categories in the PTB (Bies et al,
1995)
References
Bies, Ann, Mark Ferguson, Karen Katz and Robert
MacIntyre (1995). Bracketing Guidelines for
Treebank II Style Penn Treebank Project. Univer-
sity of Pennsylvania.
Charniak, Eugene (1996). Tree-Bank Grammars.
Tech. Rep. CS-96-02, Department of Computer
Science, Brown University, Providence, RI.
369
Charniak, Eugene and Mark Johnson (2005).
Coarse-to-fine n-best parsing and MaxEnt dis-
criminative reranking. In Proceedings of ACL-05.
Ann Arbor, MI, USA, pp. 173?180.
Collins, Michael and Terry Koo (2005). Discrim-
inative Reranking for Natural Language Parsing.
Computational Linguistics 31(1), 25?69.
Daelemans, Walter, Antal van den Bosch and Jakub
Zavrel (1999). Forgetting Exceptions is Harmful
in Language Learning. Machine Learning 34, 11?
41.
Dickinson, Markus (2006). Rule Equivalence for
Error Detection. In Proceedings of TLT 2006.
Prague, Czech Republic.
Dickinson, Markus (2008). Similarity and Dissim-
ilarity in Treebank Grammars. In 18th Interna-
tional Congress of Linguists (CIL18). Seoul.
Dickinson, Markus and W. Detmar Meurers (2005).
Prune Diseased Branches to Get Healthy Trees!
How to Find Erroneous Local Trees in a Treebank
and Why It Matters. In Proceedings of TLT 2005.
Barcelona, Spain.
Eskin, Eleazar (2000). Automatic Corpus Correc-
tion with Anomaly Detection. In Proceedings of
NAACL-00. Seattle, Washington, pp. 148?153.
Foth, Kilian and Wolfgang Menzel (2006). Robust
Parsing: More with Less. In Proceedings of the
workshop on Robust Methods in Analysis of Nat-
ural Language Data (ROMAND 2006).
Gildea, Daniel (2001). Corpus Variation and Parser
Performance. In Proceedings of EMNLP-01.
Pittsburgh, PA.
Hogan, Deirdre (2007). Coordinate Noun Phrase
Disambiguation in a Generative Parsing Model.
In Proceedings of ACL-07. Prague, pp. 680?687.
Hwa, Rebecca, Miles Osborne, Anoop Sarkar and
Mark Steedman (2003). Corrected Co-training for
Statistical Parsers. In Proceedings of ICML-2003.
Washington, DC.
Jackendoff, Ray (1977). X? Syntax: A Study of
Phrase Structure. Cambridge, MA: MIT Press.
Krotov, Alexander, Mark Hepple, Robert J.
Gaizauskas and Yorick Wilks (1998). Compact-
ing the Penn Treebank Grammar. In Proceedings
of ACL-98. pp. 699?703.
Kve?ton, Pavel and Karel Oliva (2002). Achieving
an Almost Correct PoS-Tagged Corpus. In Text,
Speech and Dialogue (TSD). pp. 19?26.
Marcus, M., Beatrice Santorini and M. A.
Marcinkiewicz (1993). Building a large annotated
corpus of English: The Penn Treebank. Compu-
tational Linguistics 19(2), 313?330.
Metcalf, Vanessa and Adriane Boyd (2006). Head-
lexicalized PCFGs for Verb Subcategorization Er-
ror Diagnosis in ICALL. In Workshop on Inter-
faces of Intelligent Computer-Assisted Language
Learning. Columbus, OH.
Santorini, Beatrice (1990). Part-Of-Speech Tagging
Guidelines for the Penn Treebank Project (3rd Re-
vision, 2nd printing). Tech. Rep. MS-CIS-90-47,
The University of Pennsylvania, Philadelphia, PA.
Sekine, Satoshi (1997). The Domain Dependence of
Parsing. In Proceedings of ANLP-96. Washing-
ton, DC.
Tang, Min, Xiaoqiang Luo and Salim Roukos
(2002). Active Learning for Statistical Natural
Language Parsing. In Proceedings of ACL-02.
Philadelphia, pp. 120?127.
Ule, Tylman and Kiril Simov (2004). Unexpected
Productions May Well be Errors. In Proceedings
of LREC 2004. Lisbon, Portugal, pp. 1795?1798.
Vadas, David and James Curran (2007). Adding
Noun Phrase Structure to the Penn Treebank. In
Proceedings of ACL-07. Prague, pp. 240?247.
Vandeventer Faltin, Anne (2003). Syntactic error di-
agnosis in the context of computer assisted lan-
guage learning. The`se de doctorat, Universite? de
Gene`ve, Gene`ve.
Wagner, Joachim, Jennifer Foster and Josef van
Genabith (2007). A Comparative Evaluation of
Deep and Shallow Approaches to the Automatic
Detection of Common Grammatical Errors. In
Proceedings of EMNLP-CoNLL 2007. pp. 112?
121.
370
Proceedings of the Third ACL Workshop on Innovative Use of NLP for Building Educational Applications, pages 1?9,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Developing Online ICALL Exercises for Russian
Markus Dickinson
Department of Linguistics
Indiana University
md7@indiana.edu
Joshua Herring
Department of Linguistics
Indiana University
jwherrin@indiana.edu
Abstract
We outline a new ICALL system for learners
of Russian, focusing on the processing needed
for basic morphological errors. By setting out
an appropriate design for a lexicon and distin-
guishing the types of morphological errors to
be detected, we establish a foundation for er-
ror detection across exercises.
1 Introduction and Motivation
Intelligent computer-aided language learning
(ICALL) systems are ideal for language pedagogy,
aiding learners in the development of awareness of
language forms and rules (see, e.g., Amaral and
Meurers, 2006, and references therein) by providing
additional practice outside the classroom to enable
focus on grammatical form. But such utility comes
at a price, and the development of an ICALL system
takes a great deal of effort. For this reason, there
are only a few ICALL systems in existence today,
focusing on a limited range of languages.
In fact, current systems in use have specifically
been designed for three languages: German (Heift
and Nicholson, 2001), Portuguese (Amaral and
Meurers, 2006, 2007), and Japanese (Nagata, 1995).
Although techniques for processing ill-formed input
have been developed for particular languages (see
Vandeventer Faltin, 2003, ch. 2), many of them
are not currently in use or have not been integrated
into real systems. Given the vast array of languages
which are taught to adult learners, there is a great
need to develop systems for new languages and for
new types of languages.
There is also a need for re-usability. While there
will always be a significant amount of overhead in
developing an ICALL system, the effort involved in
producing such a system can be reduced by reusing
system architecture and by adapting existing natural
language processing (NLP) tools. ICALL systems
to date have been developed largely independently
of each other (though, see Felshin, 1995), employ-
ing system architectures and hand-crafted NLP tools
specific to the languages they target. Given the dif-
ficulty involved in producing systems this way for
even a single language, multilingual systems remain
a distant dream. Rather than inefficiently ?reinvent-
ing the wheel? each time we develop a new sys-
tem, however, a sensible strategy is to adapt exist-
ing systems for use with other languages, evaluating
and optimizing the architecture as needed, and open-
ing the door to eventual shared-component, multi-
lingual systems. Furthermore, rather than hand-
crafting NLP tools specific to the target language
of individual systems, it makes sense to explore the
possibility of adapting existing tools to the target
language of the system under construction, devel-
oping resource-light technology that can greatly re-
duce the effort needed to build new ICALL systems.
In this light, it is important to determine where and
how reuse of technology is appropriate.
In this spirit, we are developing an ICALL sys-
tem for beginning learners of Russian based on the
TAGARELA system for Portuguese, reusing many
significant components. The first priority is to deter-
mine how well and how much of the technology in
TAGARELA can be adapted for efficient and accu-
rate use with Russian, which we outline in section 2.
1
Focusing on Russian requires the development
of techniques to parse ill-formed input for a
morphologically-rich language. Compared with
other languages, a greater bulk of the work in pro-
cessing Russian is in the morphological analysis. As
there are relatively few natural language process-
ing tools freely available for Russian (though, see
Sharoff et al, 2008), we are somewhat limited in our
selection of components.
In terms of shaping an underlying NLP system,
though, the first question to ask for processing
learner input is, what types of constructions need
to be accounted for? This can be answered by
considering the particular context of the activities.
We therefore also need to outline the types of ex-
ercises used in our system, as done in section 3,
since constraining the exercises appropriately (i.e.,
in pedagogically and computationally sound ways)
can guide processing. Based on this design, we
can outline the types of errors we expect to find
for morphologically-rich languages, as done in sec-
tion 4. Once these pieces are in place, we can detail
the type of processing system(s) that we need and
determine whether and how existing resources can
be reused, as discussed in section 5.
2 System architecture
Our system is based on the TAGARELA system for
learners of Portuguese (Amaral and Meurers, 2006,
2007), predominantly in its overall system architec-
ture. As a starting point, we retain its modularity, in
particular the separation of activities from analysis.
Each type of activity has its own directory, which
reflects the fact that each type of activity loads dif-
ferent kinds of external files (e.g., sound files for lis-
tening activities), and that each type of activity could
require different processing (Amaral, 2007).
In addition to the modular design, we also retain
much of the web processing code - including the
programming code for handling things like user lo-
gins, and the design of user databases, for keeping
track of learner information. In this way, we min-
imize the amount of online overhead in our system
and are able to focus almost immediately on the lin-
guistic processing.
In addition to these more ?superficial? aspects of
TAGARELA, we also carry over the idea of using
annotation-based processing (cf. Amaral and Meur-
ers, 2007). Before any error detection or diagnosis
is performed, the first step is to annotate the learner
input with the linguistic properties which can be au-
tomatically determined. From this annotation and
from information about, e.g., the activity, a sepa-
rate error diagnosis module can determine the most
likely error.
Unfortunately, the ?annotator? (or the analysis
model) cannot be carried over, as it is designed
specifically for Portuguese, which differs greatly
from Russian in terms of how it encodes relevant
syntactic and morphological information. With an
annotation-based framework, the focus for process-
ing Russian is to determine which information can
provide the linguistic properties relevant to detecting
and diagnosing ill-formed input and thus which NLP
tools will provide analyses (full or partial) which
have a bearing on detecting the errors of interest.
3 Exercise design
A perennial question for ICALL systems in general
is what types of errors are learners allowed to make?
This is crucially dependent upon the design of the
activities. We want the processing of our system
to be general, but we also take as a priority mak-
ing the system usable, and so any analysis done in
an annotation-based framework must be relevant for
what learners are asked to do.
The goal of our system is to cover a range of ex-
ercises for students enrolled in an eight-week ?sur-
vival? Russian course. These students start the
course knowing nothing about Russian and finish it
comfortable enough to travel to Russia. The exer-
cises must therefore support the basics of grammar,
but also be contextualized with situations that a stu-
dent might encounter. To aid in contextualization,
we plan to incorporate both audio and video, in or-
der to provide additional ?real-life? listening (and
observing) practice outside of the classroom.
The exercises we plan to design include: listen-
ing exercises, video-based narrative exercises, read-
ing practice, exercises centered around maps and lo-
cations, as well as more standard fill-in-the-blank
(FIB) exercises. These exercises allow for variabil-
ity in difficulty and in learner input.
From the processing point of view, each will have
2
its own hurdles, but all require some morphosyntac-
tic analysis of Russian. To constrain the input for
development and testing purposes, we are starting
with an FIB exercise covering verbal morphology.
Although this is not the ideal type of exercise for dis-
playing the full range of ICALL benefits and capa-
bilities, it is indispensible from a pedagogical point
of view (given the high importance of rapid recog-
nition of verbal forms in a morphologically rich lan-
guage like Russian) and allows for rapid develop-
ment, testing, and perfection of the crucial morpho-
logical analysis component, as it deals with compli-
cated morphological processing in a suitably con-
strained environment. The successes and pitfalls of
this implementation are unlikely to differ radically
for morphological processing in other types of ex-
ercises; the techniques developed for this exercise
thus form the basis of a reusable framework for the
project as a whole.
A simple example of a Russian verbal exercise is
in (1), where the verb needs to be past tense and
agree with third person singular masculine noun.
(1) ?????
Yesterday
??
he
__
__
(??????)
(to see)
?????.
a film
4 Taxonomy for morphological errors
When considering the integration of NLP tools for
morphological error detection, we need to consider
the nature of learner language. In this context, an
analyzer cannot simply reject unrecognized or un-
grammatical strings, as does a typical spell-checker,
for example, but must additionally recognize what
was intended and provide meaningful feedback on
that basis. Formulating an error taxonomy delin-
eates what information from learner input must be
present in the linguistic analysis.
Our taxonomy is given in figure 1. As can be seen
at a glance, the errors become more complex and
require more information about the complete syntax
as we progress in the taxonomy.
To begin with, we have inappropriate verb stems.
For closed-form exercises, the only way that a
properly-spelled verb stem can be deemed appropri-
ate or inappropriate is by comparing it to the verb
that the student was asked to use. Thus, errors of
type #1b are straightforward to detect and to pro-
vide feedback on; all that needs to be consulted is
1. Inappropriate verb stem
(a) Always inappropriate
(b) Inappropriate for this context
2. Inappropriate verb affix
(a) Always inappropriate
(b) Always inappropriate for verbs
(c) Inappropriate for this verb
3. Inappropriate combination of stem and affix
4. Well-formed word in inappropriate context
(a) Inappropriate agreement features
(b) Inappropriate verb form (tense, perfec-
tive/imperfective, etc.)
Figure 1: Error taxonomy for Russian verbal morphology
the activity model.1 Errors of type #1a (and #2a) are
essentially misspellings and will thus require spell-
checking technology, which we do not focus on in
this paper, although we discuss it briefly in sec-
tion 5.3.
Secondly, there are inappropriate verb affixes,
which are largely suffixes in Russian. Other than
misspellings (#2a), there are two ways that affixes
can be incorrect, as shown in example (2). In exam-
ple (2a), we have the root for ?begin? (pronounced
nachina) followed by an ending (ev) which is never
an appropriate ending for any Russian verb, al-
though it is a legitimate nominal suffix (#2b). The
other subtype of error (#2c) involves affixes which
are appropriate for different stems within the same
POS category. In example (2b), a third person sin-
gular verb ending was used (it), but it is appropriate
for a different conjugation class. The appropriate
form for ?he/she/it begins? is ????????.
(2) a. *??????-??
begin-??
b. *??????-??
begin-3s
The third type of error is where the stem and affix
1Note that if one were allowing free input, this error type
could be the most difficult, in that the semantics of the sentence
would have to be known to determine if a verb was appropriate.
3
may both be correct, but they were put together in-
appropriately. In a sense, these are a specific type
of misspelling. For example, the infinitive ????
(moch, ?to be able to?) can be realized with different
stems, depending upon the ending, i.e., ???-? (mogu
?I can?) ???-?? (mozhem ?we can?). Thus, we
might expect to see errors such as *???-? (mozhu),
where both the stem and the affix are appropriate?
and appropriate for this verb?but are not combined
in a legitimate fashion. The technology needed to
detect these types of errors is no more than what is
needed for error type #2, as we discuss in section 5.
The final type of error is the one which requires
the most attention in terms of NLP processing. This
is the situation when we have a well-formed word
appearing in an inappropriate context. In other
words, there is a mismatch between the morpho-
logical properties of the verb and the morphological
properties dictated by the context for that verb.
There are of course different ways in which a verb
might display incorrect morphological features. In
the first case (#4a), there are inappropriate agree-
ment features. Verbs in Russian agree with the prop-
erties of their subject, as shown in example (3).
Thus, as before, we need to know the morphologi-
cal properties of the verb, but now we need not just
the possible analyses, but the best analysis in this
context. Furthermore, we need to know what the
morphological properties of the subject noun are, to
be able to check whether they agree. Access to the
subject is something which can generally be deter-
mined by short context, especially in relatively short
sentences.
(3) a. ?
I
?????
think-1sg
b. ??
He
??????
think-3sg
c. *?
I
??????
think-3sg
In the second case (#4b), the verb could be in an
inappropriate form: the tense could be inappropri-
ate; the verbal form (gerund, infinitive, etc.) could
be inappropriate; the distinction between perfective
and imperfective verbs could be mistakenly realized;
and so forth. Generally speaking, this kind of con-
textual information comes from two sources: 1) The
activity model can tell us, for example, whether a
perfective (generally, a completed action) or an im-
perfective verb is required. 2) The surrounding sen-
tence context can tell us, for example, whether an
infinitive verb is governed by a verb selecting for an
infinitive. Thus, we need the same tools that we need
for agreement error detection.
By breaking it down into this taxonomy, we can
more clearly delineate when we need external tech-
nology in dealing with morphological variation. For
error types #1 through #3, we make no use of context
and only need information from an activity model
and a lexicon to tell us whether the word is valid.
For these error types, the processing can proceed in a
relatively straightforward fashion, provided that we
have a lexicon, as outlined in section 5. Note also
that our error taxonomy is meant to range over the
space of logically possible error types for learners
from any language background of any language?s
morphological system. In this way, it differs from
the more heuristic approaches of earlier systems
such as Athena (Murray, 1995), which used tax-
onomies tailored to the native languages of the sys-
tem?s users.
That leaves category #4. These errors are mor-
phological in nature, but the words are well-formed,
and the errors have to do with properties conditioned
by the surrounding context. These are the kind for
which we need external technology, and we sketch a
proposed method of analysis in section 5.4.
Finally, we might have considered adding a fifth
type of error, as in the following:
5. Well-formed word appropriate to the sentence,
used inappropriately
(a) Inappropriate position
(b) Inappropriate argument structure
However, these issues of argument structure and
of pragmatically-conditioned word order variation
do not result in morphological errors of the verb,
but rather clearly syntactic errors. We are currently
only interested in morphological errors, given that
in certain exercises, as in the present cases, syntac-
tic errors are not even possible. With an FIB de-
sign, even though we might still generate a complete
analysis of the sentence, we know which word has
4
the potential for error. Even though we are not cur-
rently concerned with these types of errors, we can
note that argument structure errors can likely be han-
dled through the activity model and through a simi-
lar analysis to what described is in section 5.4 since
both context-dependent morphological errors (e.g.,
agreement errors) and argument structure errors rely
on relations between the verb and its arguments.
5 Linguistic analysis
Given the discussion of the previous section, we are
now in a position to discuss how to perform mor-
phological analysis in a way which supports error
diagnosis.
5.1 The nature of the lexicon
In much syntactic theory, sentences are built from
feature-rich lexical items, and grammatical sen-
tences are those in which the features of com-
ponent items agree in well-defined ways. In
morphologically-rich languages like Russian, the
heavy lifting of feature expression is done by overt
marking of words in the form of affixes (mainly pre-
fixes and suffixes in the case of Russian). To be able
to analyze words with morphological errors, then,
we need at least partially successful morphological
analysis of the word under analysis (as well as the
words in the context).
The representation of words, therefore, must be
such that we can readily obtain accurate partial in-
formation from both well-formed and ill-formed in-
put. A relatively straightforward approach for anal-
ysis is to structure a lexicon such that we can build
up partial (and competing) analyses of a word as the
word is processed. As more of the word is (incre-
mentally) processed, these analyses can be updated.
But how is this to be done exactly?
In our system, we plan to meet these criteria by
using a fully-specified lexicon, implemented as a Fi-
nite State Automaton (FSA) and indexed by both
word edges. Russian morphological information is
almost exclusively at word edges?i.e., is encoded
in the prefixes and suffixes?and thus an analysis
can proceed by working inwards, one character at
a time, beginning at each end of an input item.2
2See Roark and Sproat (2007) for a general overview
of implementational strategies for finite-state morphological
By fully-specified, we mean that each possible
form of a word is stored as a separate entity (path).
This is not as wasteful of memory as it may sound.
Since the lexicon is an FSA, sections shared across
forms need be stored only once with diversion rep-
resented by different paths from the point where the
shared segment ends. In fact, representing the lex-
icon as an FSA ensures that this process efficiently
encodes the word possibilities. Using an FSA over
all stored items, regular affixes need to be stored
only once, and stems which require such affixes sim-
ply point to them (Clemenceau, 1997). This gives
the analyzer the added advantage that it retains ex-
plicit knowledge of state, making it easy to simul-
taneously entertain competing analyses of a given
input string (C?avar, 2008), as well as to return to
previous points in an analysis to resolve ambiguities
(cf., e.g., Beesley and Karttunen, 2003).
We also need to represent hypothesized mor-
pheme boundaries within a word, allowing us to seg-
ment the word into its likely component parts and
to analyze each part independently of the others.
Such segmentation is crucial for obtaining accurate
information from each morpheme, i.e., being able
to ignore an erroneous morpheme while identifying
an adjoining correct morpheme. Note also that be-
cause an FSA encodes competing hypotheses, mul-
tiple segmentations can be easily maintained.
Consider example (4), for instance, for which the
correct analysis is the first person singular form of
the verb think. This only becomes clear at the point
where segmentation has been marked. Up to that
point, the word is identical to some form of ??-
?? (duma), ?parliament? (alternatively, ?thought?).
Once the system has seen ????, it automatically en-
tertains the competing hypotheses that the learner in-
tends ?parliament,? or any one of many forms of ?to
think,? as these are all legal continuations of what
it has seen so far. Any transition to ? after ????
carries with it the analysis that there is a morpheme
boundary here.
(4) ????|?
think-1sg
Obviously this bears non-trivial resemblance to
spell-checking technology. The crucial difference
analysis.
5
comes in the fact that an ICALL morphological an-
alyzer must be prepared to do more than simply re-
ject strings not found in the lexicon and thus must
be augmented with additional, morphological infor-
mation. Transitions in the lexicon FSA will need to
encode more information than just the next charac-
ter in the input; they also need to be marked with
possible morphological analyses at points where it
is possible that a morpheme boundary begins.
Maintaining hypothesized paths through a lexicon
based on erroneous input must obviously be con-
strained in some way (to prevent all possible paths
from being simultaneously entertained), and thus we
first developed the error taxonomy above. Knowing
what kinds of errors are possible is crucial to keep-
ing the whole process workable.
5.2 FSAs for error detection
But why not use an off-the-shelf morphological an-
alyzer which returns all possible analyses, or a more
traditional paradigm-based lexicon? There are a
number of reasons we prefer exploring an FSA im-
plementation to many other approaches to lexical
storage for the task of supporting error detection and
diagnosis.
First, traditional mophological analyzers gener-
ally assume well-formed input. And, unless they
segment a word, they do not seem to be well-
suited to providing information relevant to context-
independent errors.
Secondly, we need to readily have access to al-
ternative analyses, even for a legitimate word. With
phonetically similar forms used as different affixes,
learners can accidentally produce correct forms, and
thus multiple analyses are crucial. For example, -?
can be either a first person singular marker for cer-
tain verb classes or an accusative marker for certain
noun classes. Suppose a learner attempts to make a
verb out of the noun ??? (dush), meaning ?shower?
and thus forms the word ????. It so happens that
this incorrect form is identical to an actual Russian
word: the accusative form of the noun ?soul.? A
more traditional morphological analysis will likely
only find the attested form. Keeping track of the
history from left-to-right records that the ?shower?
reading is possible; keeping track of the history from
right-to-left records that a verbal ending is possible.
Compactly representing such ambiguity?especially
when the ambiguity is not in the language itself
but in the learner?s impression of how the language
works?is thus key to identifying errors.
Finally, and perhaps most importantly, morpho-
logical analysis over a FSA lexicon allows for easy
implementation of activity-specific heuristics. In the
current example, for instance, an activity might pri-
oritize a ?shower? reading over a ?soul? one. Since
entertained hypotheses are all those which represent
legal continuations (or slight alterations of legal con-
tinuations) through the lexicon from a given state in
the FSA, it is easy to bias the analyzer to return cer-
tain analyses through the use of weighted paths. Al-
ternatively, paths that we have strong reason to be-
lieve will not be needed can be ?disconnected.? In
the verbal morphology exercise, for example, suffix
paths for non-verbs can safely be ignored.
The crucial point about error detection in ICALL
morphological analysis is that the system must be
able to speculate, in some broadly-defined sense, on
what learners might have meant by their input, rather
than simply evaluating the input as correct or incor-
rect based on its (non)occurrence in a lexicon. For
this reason, we prefer to have a system where at least
one component of the analyzer has 100% recall, i.e.,
returns a set of all plausible analyses, one of which
can reasonbly be expected to be correct. Since an an-
alyzer based on an FSA lexicon has full access to the
lexicon at all stages of analysis, it efficiently meets
this requirement, and it does this without anticipat-
ing specific errors or being tailored to a specific type
of learner (cf., e.g., Felshin, 1995).
5.3 Error detection
Having established that an FSA lexicon supports er-
ror detection, let us outline how it will work. Anal-
ysis is a process of attempting to form independent
paths through the lexicon - one operating ?forward?
and the other operating ?backward.? For grammati-
cal input, there is generally one unique path through
the lexicon that joins both ends of the word. Mor-
phological analysis is found by reading information
from the transitions along the chain (cf. Beesley and
Karttunen, 2003). For ungrammatical input, the an-
alyzer works by trying to build a connecting path
based on the information it has.
Consider the case of the two ungrammatical verbs
in (5).
6
(5) a. *??????-??
begin-??
b. *??????-??
begin-3s
In (5a) (error type #2b) the analysis proceeding
from the end of the word would fail to detect that
the word is intended to be a verb. But it would, at
the point of reaching the ? in ??, recognize that it
had found a legitimate nominal suffix. The process-
ing from the beginning of the word, however, would
recognize that it has seen some form of begin. We
thus have enough information to know what the ver-
bal stem is and that there is probably a morpheme
boundary after ??????-. These two hypotheses do
not match up to form a legitimate word (thereby de-
tecting an error), but they provide crucial partial in-
formation to tell us how the word was misformed.
Detecting the error in (5b) (type #2c) works sim-
ilarly, and the diagnosis will be even easier. Again,
analyses proceeding from each end of the word will
agree on the location of the morpheme boundary and
that the type of suffix used (third person singular) is
a type appropriate to verbs, just not for this conjuga-
tion class. Having a higher-level rule recognize that
all features match, merely the form is wrong, is eas-
ily achieved in a system with an explicit taxonomy
of expected error types coded in.
Errors of type #3 are handled in exactly the same
fashion: information about which stem or which af-
fix is used is readily available, even if there is no
complete path to form a whole word.
Spelling errors within a stem or an affix (error
types #1a and #2a) require additional technology in
order to find the intended analysis?which we only
sketch here?but it is clear that such spell-checking
should be done separately on each morpheme.3 In
the above examples, if the stem had been misspelled,
that should not change the analysis of the suffix.
Integrating spell-checking by calculating edit dis-
tances between a realized string and a morpheme in
the lexicon should be relatively straightforward, as
that technology is well-understood (see, e.g., Mit-
ton, 1996) and since we are already analyzing sub-
parts of words.
3Clearly, we will be able to determine whether a word is
correctly spelled or not; the additional technology is needed to
determine the candidate corrections.
Obviously, in many cases there will be lingering
ambiguity, either because there are multiple gram-
matical analyses in the lexicon for a given input
form, or because the learner has entered an ungram-
matical form, the intention behind which cannot en-
tirely be determined from the input string alone. It
is for such cases that the morphological analyzer
we propose is most useful. Instead of returning
the most likely path through the analyzer (e.g., the
GPARS system of Loritz, 1992), our system pro-
poses to follow all plausible paths through the lexi-
con simultaneously?including those that are the re-
sult of string edit ?repair? operations.4 In short, we
intend a system that entertains competing hypothe-
ses ?online? as it processes input words.5
This results in a set of analyses, providing
sentence-level syntactic and semantic analysis mod-
ules quick access to competing hypotheses, from
which the the analysis most suitable to the context
can be chosen, including those which are misspelled.
The importance of this kind of functionality is espe-
cially well demonstrated in Pijls et al (1987), which
points out that in some languages?Dutch, in this
case?minor, phonologically vacuous spelling dif-
ferences are syntactically conditioned, making spell
checking and syntactic analysis mutually dependent.
Such cases are rarer in Russian, but the functionality
remains useful due to the considerable interdepen-
dence of morphological and syntactic analysis.
5.4 Morphological analysis in context
For the purposes of the FIB exercise currently un-
der development, the finite-state morphological ana-
lyzer we are building will of course be sufficient, but
as exercises grow in complexity, it will be necessary
to use it in conjunction with other tools. It is worth
briefly sketching how the components of this inte-
grated system will work together to provide useful
error feedback to our learners.
If the learner has formed a legitimate word, the
task becomes one of determining whether or not it
4These include transitions to states on no input symbol (IN-
SERTION), transitions to states on a different symbol from the
next input symbol (SUBSTITUTION), and consumption of an in-
put symbol without transition to a new state (DELETION).
5It is worth noting here that GPARS was actually a sentence-
level system; it is for the word-level morphological analysis dis-
cussed here that we expect the most gain from our approach.
7
is appropriate to the context. The FSA analyzer
will provide a list of possible analyses (i.e., aug-
mented POS tags) for each input item (ranked, if
need be). We can explore using a third-party tag-
ger to narrow down this output list to analyses that
make sense in context. We are considering both the
Hidden Markov Model tagger TnT (Brants, 2000)
and the Decision Tree Tagger (Schmid, 1997), with
parameter files from Sharoff et al (2008). Both of
these taggers use local context, but, as they provide
potentially different types of information, the final
system may use both in parallel, weighing the out-
put of each to the degree which each proves useful
in trial runs to make its decision.
Since POS tagging does not capture every syntac-
tic property that we might need access to, we are not
sure how accurate error detection can be. Thus, to
supplement its contextual information, we intend to
use shallow syntactic processing methods, perhaps
based on a small set of constraint grammar rules
(cf, e.g., Bick, 2004). This shallow syntactic recog-
nizer can operate over the string of now-annotated
tags to resolve any remaining ambiguities and point
out any mismatches between the items (for exam-
ple, a noun-adjective pair where the gender does not
match), thereby more accurately determining the re-
lations between words.
6 Summary and Outlook
We have outlined a system for Russian ICALL ex-
ercises, the first of its kind for a Slavic language,
and we have specifically delineated the types of
errors to which need to be analyzed for such a
morphologically-rich language. In that process, we
have proposed a method for analyzing the morphol-
ogy of learner language and noted where external
NLP tools will be useful, making it clear how all
these tools can be optimized for learning environ-
ments where the priority is to obtain a correct anal-
ysis, over obtaining any analysis.
The initial challenge is in creating the FSA lex-
icon, given that no such resource exists. However,
unsupervised approaches to calculating the mor-
phology of a language exist, and these can be di-
rectly connected to FSAs (Goldsmith and Hu, 2004).
Thus, by using a tool such as Linguistica6 on a cor-
6http://linguistica.uchicago.edu/
pus such as the freely available subset of the Russian
Internet Corpus (Sharoff et al, 2008),7 we can semi-
automatically construct an FSA lexicon, pruning it
by hand.
Once the lexicon is constructed?for even a small
subset of the language covering a few exercises?the
crucial steps will be in performing error detection
and error diagnosis on top of the linguistic analysis.
In our case, linguistic analysis is provided by sep-
arate (levels of) modules operating in parallel, and
error detection is largely a function of either notic-
ing where these modules disagree, or in recognizing
cases where ambiguity remains after one has been
used to constrain the output of the other.
We have also tried to advance the case that this
and future ICALL systems do better to build on ex-
isting technologies, rather than building from the
bottom up for each new language. We hope that the
approach we are taking to morphological analysis
will prove to be just such a general, scalable system,
one applicable?with some tweaking and to various
levels?to morphologically-rich languages and iso-
lating languages alike.
Acknowledgments We would like to thank Det-
mar Meurers and Luiz Amaral for providing us with
the TAGARELA sourcecode, as well as for valuable
insights into the workings of ICALL systems; and to
thank Anna Feldman and Jirka Hana for advice on
Russian resources. We also thank two anonymous
reviewers for insightful comments that have influ-
enced the final version of this paper. This research
was supported by grant P116S070001 through the
U.S. Department of Education?s Fund for the Im-
provement of Postsecondary Education.
References
Amaral, Luiz (2007). Designing Intelligent Lan-
guage Tutoring Systems: integrating Natural Lan-
guage Processing technology into foreign lan-
guage teaching. Ph.D. thesis, The Ohio State Uni-
versity.
Amaral, Luiz and Detmar Meurers (2006).
Where does ICALL Fit into Foreign Lan-
guage Teaching? Talk given at CALICO
Conference. University of Hawaii, http:
7http://corpus.leeds.ac.uk/mocky/
8
//purl.org/net/icall/handouts/
calico06-amaral-meurers.pdf.
Amaral, Luiz and Detmar Meurers (2007).
Putting activity models in the driver?s seat:
Towards a demand-driven NLP architecture
for ICALL. Talk given at EUROCALL. Uni-
versity of Ulster, Coleraine Campus, http:
//purl.org/net/icall/handouts/
eurocall07-amaral-meurers.pdf.
Beesley, Kenneth R. and Lauri Karttunen (2003). Fi-
nite State Morphology. CSLI Publications.
Bick, Eckhard (2004). PaNoLa: Integrating Con-
straint Grammar and CALL. In Henrik Holm-
boe (ed.), Nordic Language Technology, Copen-
haguen: Museum Tusculanum, pp. 183?190.
Brants, Thorsten (2000). TnT ? A Statistical Part-of-
Speech Tagger. In Proceedings of the Sixth Ap-
plied Natural Language Processing Conference
(ANLP 2000). Seattle, WA, pp. 224?231.
C?avar, Damir (2008). The Croatian Language
Repository: Quantitative and Qualitative Re-
sources for Linguistic Research and Language
Technologies. Invited talk, Indiana University
Department of Lingistics, January 2008.
Clemenceau, David (1997). Finite-State Morphol-
ogy: Inflections and Derivations in a Singl e
Framework Using Dictionaries and Rules. In Em-
manuel Roche and Yves Schabes (eds.), Finite
State Language Processing, The MIT Press.
Felshin, Sue (1995). The Athena Language Learn-
ing Project NLP System: A Multilingual Sys-
tem for Conversation-Based Language Learning.
In Intelligent Language Tutors: Theory Shap-
ing Technology, Lawrence Erlbaum Associates,
chap. 14, pp. 257?272.
Goldsmith, John and Yu Hu (2004). From Sig-
natures to Finite State Automata. In Midwest
Computational Linguistics Colloquium (MCLC-
04). Bloomington, IN.
Heift, Trude and Devlan Nicholson (2001). Web
delivery of adaptive and interactive language tu-
toring. International Journal of Artificial Intelli-
gence in Education 12(4), 310?325.
Loritz, D. (1992). Generalized Transition Network
Parsing for Language Study: the GPARS system
for English, Russian, Japanese and Chinese. CAL-
ICO Journal 10(1).
Mitton, Roger (1996). English Spelling and the
Computer. Longman.
Murray, Janet H. (1995). Lessons Learned from
the Athena Language Learning Project: Us-
ing Natural Language Processing, Graphics,
Speech Processing, and Interactive Video for
Communication-Based Language Learning. In
V. Melissa Holland, Michelle R. Sams and
Jonathan D. Kaplan (eds.), Intelligent Language
Tutors: Theory Shaping Technology, Lawrence
Erlbaum Associates, chap. 13, pp. 243?256.
Nagata, Noriko (1995). An Effective Application
of Natural Language Processing in Second Lan-
guage Instruction. CALICO Journal 13(1), 47?
67.
Pijls, Fieny, Walter Daelemans and Gerard Kempen
(1987). Artificial intelligence tools for grammar
and spelling instruction. Instructional Science 16,
319?336.
Roark, Brian and Richard Sproat (2007). Compu-
tational Approaches to Morphology and Syntax.
Oxford University Press.
Schmid, Helmut (1997). Probabilistic part-of-
speech tagging using decision trees. In D.H. Jones
and H.L. Somers (eds.), New Methods in Lan-
guage Processing, London: UCL Press, pp. 154?
164.
Sharoff, Serge, Mikhail Kopotev, Tomaz? Erjavec,
Anna Feldman and Dagmar Divjak (2008). De-
signing and evaluating Russian tagsets. In Pro-
ceedings of LREC 2008. Marrakech.
Vandeventer Faltin, Anne (2003). Syntactic error di-
agnosis in the context of computer assisted lan-
guage learning. The`se de doctorat, Universite? de
Gene`ve, Gene`ve.
9
Proceedings of the EACL 2009 Workshop on Cognitive Aspects of Computational Language Acquisition, pages 34?41,
Athens, Greece, 31 March 2009. c?2009 Association for Computational Linguistics
Categorizing Local Contexts as a Step in Grammatical Category
Induction
Markus Dickinson
Indiana University
Bloomington, IN USA
md7@indiana.edu
Charles Jochim
Indiana University
Bloomington, IN USA
cajochim@indiana.edu
Abstract
Building on the use of local contexts, or
frames, for human category acquisition,
we explore the treatment of contexts as
categories. This allows us to examine and
evaluate the categorical properties that lo-
cal unsupervised methods can distinguish
and their relationship to corpus POS tags.
From there, we use lexical information
to combine contexts in a way which pre-
serves the intended category, providing a
platform for grammatical category induc-
tion.
1 Introduction and Motivation
In human category acquisition, the immediate lo-
cal context of a word has proven to be a reliable
indicator of its grammatical category, or part of
speech (e.g., Mintz, 2002, 2003; Redington et al,
1998). Likewise, category induction techniques
cluster word types together (e.g., Clark, 2003;
Schu?tze, 1995), using similar information, i.e.,
distributions of local context information. These
methods are successful and useful (e.g. Koo et al,
2008), but in both cases it is not always clear
whether errors in lexical classification are due to a
problem in the induction algorithm or in what con-
texts count as identifying the same category (cf.
Dickinson, 2008). The question we ask, then, is:
what role does the context on its own play in defin-
ing a grammatical category? Specifically, when do
two contexts identify the same category?
Many category induction experiments start by
trying to categorize words, and Parisien et al
(2008) categorize word usages, a combination of
a word and its context. But to isolate the effect the
context has on the word, we take the approach of
categorizing contexts as a first step towards clus-
tering words. By separating out contexts for word
clustering, we can begin to speak of better dis-
ambiguation models as a foundation for induc-
tion. We aim in this paper to thoroughly investi-
gate what category properties contexts can or can-
not distinguish by themselves.
With this approach, we are able to more thor-
oughly examine the categories used for evaluation.
Evaluation of induction methods is difficult, due to
the variety of corpora and tagsets in existence (see
discussion in Clark, 2003) and the variety of po-
tential purposes for induced categories (e.g., Koo
et al, 2008; Miller et al, 2004). Yet improving the
evaluation of category induction is vital, as eval-
uation does not match up well with grammar in-
duction evaluation (Headden III et al, 2008). For
many evaluations, POS tags have been mapped
to a smaller tagset (e.g., Goldwater and Griffiths,
2007; Toutanova and Johnson, 2008), but there
have been few criteria for evaluating the quality
of these mappings. By isolating contexts, we can
investigate how each mapping affects the accuracy
of a method and the lexicon.
Using corpus annotation also allows us to ex-
plore the relation between induced categories
and computationally or theoretically-relevant cat-
egories (e.g., Elworthy, 1995). While human cate-
gory acquisition results successfully divide a lexi-
con into categories, these categories are not neces-
sarily ones which are appropriate for many com-
putational purposes or match theoretical syntactic
analysis. This work can also serve as a platform to
help drive the design of new tagsets, or refinement
of old ones, by outlining which types of categories
are or are not applicable for category induction.
After discussing some preliminary issues in sec-
tion 2, in section 3 we examine to what extent con-
texts by themselves can distinguish different cat-
egory properties and how this affects evaluation.
Namely, we propose that corpus tagsets should
be clear about identifying syntactic/distributional
properties and about how tagset mappings for
evaluation should outline how much information
34
is lost by mapping. In section 4, in more prelimi-
nary work, we add lexical information to contexts,
in order to merge them together and see which still
identify the same category.
2 Preliminaries
2.1 Background
Research on language acquisition has addressed
how humans learn categories of words, and we use
this as a starting point. Mintz (2002) shows that
local context, in the form of a frame of two words
surrounding a target word, leads to the target?s
categorization in adults, and Mintz (2003) shows
that frequent frames supply category information
in child language corpora. A frame is not decom-
posed into its left and right sides (cf., e.g., Reding-
ton et al, 1998; Clark, 2003; Schu?tze, 1995), but
is taken as their joint occurrence (Mintz, 2003).1
For category acquisition, frequent frames are
used, those with a frequency above a certain
threshold. These predict category membership, as
the set of words appearing in a given frame should
represent a single category. The frequent frame
you it, for example, largely identifies verbs, as
shown in (1), taken from child-directed speech in
the CHILDES database (MacWhinney, 2000). For
frequent frames in six subcorpora of CHILDES,
Mintz (2003) obtains both high type and token ac-
curacy in categorizing words.
(1) a. you put it
b. you see it
The categories do not reflect fine-grained lin-
guistic distinctions, though, nor do they fully ac-
count for ambiguous words. Indeed, accuracies
slightly degrade when moving from ?Standard La-
beling?2 to the more fine-grained ?Expanded La-
beling,?3 from .98 to .91 in token accuracy and
from .93 to .91 in type accuracy. In scaling the
method beyond child-directed speech, it would
be beneficial to use annotated data, which allows
for ambiguity and distinguishes a word?s cate-
gory across corpus instances. Furthermore, even
though many frames identify the same category,
1This use of frame is different than that used for subcate-
gorization frames, which are also used to induce word classes
(e.g., Korhonen et al, 2003).
2Categories = noun, verb, adjective, preposition, adverb,
determiner, wh-word, not, conjunction, and interjection.
3Nouns split into nouns and pronouns; verbs split into
verbs, auxiliaries, and copula
the method does not thoroughly specify how to re-
late them.
It has been recognized for some time that wider
contexts result in better induction models (e.g.,
Parisien et al, 2008; Redington et al, 1998), but
many linguistic distinctions rely on lexical infor-
mation that cannot be inferred from additional
context (Dickinson, 2008), so focusing on short
contexts can provide many insights. The use of
frames allows for frequent recurrent contexts and
a way to investigate corpus categories, or POS tags
(cf., e.g., Dickinson and Jochim, 2008). An added
benefit of starting with this method is that it can be
converted to a model of online acquisition (Wang
and Mintz, 2007). For this paper, however, we
only investigate the type of information input into
the model.
2.2 Some definitions
Frequency The core idea of using frames is that
words used in the same context are associated with
each other, and the more often these contexts oc-
cur, the more confidence we have that the frame in-
dicates a category. Setting a threshold to obtain the
45 most frequent frames in each subcorpus (about
80,000 words on average), (Mintz, 2003) allows a
frame to occur often enough to be meaningful and
have a variety of target words in the frame.
To determine what category properties frames
pinpoint (section 3), we use two thresholds to de-
fine frequent. Singly occurring frames cannot pro-
vide any information about groupings of words,
so we first consider frames that occur more than
once. This gives a large number of frames, cover-
ing much of the corpus (about 970,000 tokens), but
frames with few instances have very little informa-
tion. For the other threshold, frequent frames are
those which have a frequency of 200, about 0.03%
of the total number of frames in the corpus. One
could explore more thresholds, but for compar-
ing tagset mappings, these provide a good picture.
The higher threshold is appropriate for combining
contexts (section 4), as we need more information
to tell whether two frames behave similarly.
Accuracy To evaluate, we need a measure of the
accuracy of each frame. Mintz (2003) and Red-
ington et al (1998) calculate accuracy by counting
all pairs of words (types or tokens) that are from
the same category, divided by all possible pairs of
words in a grouping. This captures the idea that
each word should have the same category as every
35
other word in its category set.
Viewing the task as disambiguating contexts
(see section 3), however, this measurement does
not seem to adequately represent cases with a ma-
jority label. For example, if three words have
the tag X and one Y , pairwise comparison re-
sults in an accuracy of 50%, even though X is
dominant. To account for this, we measure the
precision of the most frequent category instances
among all instances, e.g., 75% for the above ex-
ample (cf. the notion of purity in Manning et al,
2008). Additionally, we only use measurements
of token precision. Token precision naturally han-
dles ambiguous words and is easy to calculate in a
POS-annotated corpus.
3 Categories in local contexts
In automatic category induction, a category is of-
ten treated as a set, or cluster, of words (Clark,
2003; Schu?tze, 1995), and category ambiguity is
represented by the fact that words can appear in
more than one set. Relatedly, one can cluster word
usages, a combination of a word and its context
(Parisien et al, 2008). An erroneous classification
occurs when a word is in an incorrect set, and one
source of error is when the contexts being treated
as indicative of the same category are actually am-
biguous. For example, in a bigram model, the con-
text be identifies nouns, adjectives, and verbs,
among others.
Viewed in this way, it is important to gauge
the precision of contexts for distinguishing a cat-
egory (cf. also Dickinson, 2008). In other words,
how often does the same context identify the same
category? And how fine-grained is the category
that the context distinguishes? To test whether
a frame defines a single category in non-child-
directed speech, we focus on which categorical
properties frames define, and for this we use a
POS-annotated corpus. Due to its popularity for
unsupervised POS induction research (e.g., Gold-
berg et al, 2008; Goldwater and Griffiths, 2007;
Toutanova and Johnson, 2008) and its often-used
tagset, for our initial research, we use the Wall
Street Journal (WSJ) portion of the Penn Treebank
(Marcus et al, 1993), with 36 tags (plus 9 punc-
tuation tags), and we use sections 00-18, leaving
held-out data for future experiments.4
Defining frequent frames as those occurring at
4Even if we wanted child-directed speech, the CHILDES
database (MacWhinney, 2000) uses coarse POS tags.
least 200 times, we find 79.5% token precision.
Additionally, we have 99 frames, identifying 14
types of categories as the majority tag (common
noun (NN) being the most prevalent (37 frames)).
For a threshold of 2, we have 77.3% precision for
67,721 frames and 35 categories.5 With precision
below 80%, we observe that frames are not fully
able to disambiguate these corpus categories.
3.1 Frame-defined categories
These corpus categories, however, are composed
of a variety of morphological and syntactic fea-
tures, the exact nature of which varies from tagset
to tagset. By merging different tags, we can factor
out different types of morphological and syntac-
tic properties to determine which ones are more or
less easily identified by frames. Accuracy will of
course improve by merging tags; what is important
is for which mappings it improves.
We start with basic categories, akin to those
in Mintz (2003). Despite the differences among
tagsets, these basic categories are common, and
merging POS tags into basic categories can show
that differences in accuracy have more to do with
stricter category labels than language type. We
merged tags to create basic categories, as in table 1
(adapted from Hepple and van Genabith (2000);
see appendix A for descriptions).6
Category Corpus tags
Determiner DT, PDT, PRP$
Adjective JJ, JJR, JJS
Noun NN, NNS, PRP, NNP, NNPS
Adverb RB, RBR, RBS
Verb MD, VB, VBD, VBG, VBN,
VBP, VBZ
Wh-Det. WDT, WP$
Table 1: Tag mappings into basic categories
These broader categories result in the accuracies
in table 2, and we also record accuracies for the
similar PTB-17 tagset used in a variety of unsu-
pervised tagging experiments (Smith and Eisner,
2005), which mainly differs by treating VBG and
VBN uniquely. With token precision around 90%,
it seems that frame-based disambiguation is gener-
ally identifying basic categories, though with less
5LS (List item marker) is not identified; UH (interjection)
appears in one repeating frame, and SYM (symbol) in two.
6The 13 other linguistic tags were not merged, i.e., CC,
CD, EX, FW, IN, LS, POS, RP, SYM, TO, UH, WP, WRB.
36
accuracy than in Mintz (2003).
? 2 ? 200
Orig. 77.3% 79.5%
Merged 85.9% 91.0%
PTB-17 85.1% 89.7%
Table 2: Effect of mappings on precision
But which properties of the tagset do the
frame contexts accurately capture and which do
they not? To get at this question, we ex-
plore linguistically-motivated mappings between
the original tagset and the fully-merged tagset in
table 1. Given the predominance of verbs and
nouns, we focus on distinguishing linguistic prop-
erties within these categories. For example, sim-
ply by merging nouns and leaving all other orig-
inal tags unchanged, we move from 79.5% token
precision to 88.4% (for the threshold of 200).
Leaving all other mappings as in table 1, we
merge nouns and verbs along two dimensions:
their common syntactic properties or their com-
mon morphological properties. Ideally, we pre-
fer frames to pick out syntactic properties, since
morphological properties can assumedly be deter-
mined from word-internal properties (see Clark,
2003; Christiansen and Monaghan, 2006).
Specifically, we can merge nouns by noun
type (PRP [pronoun], NN/NNS [common noun],
NNP/NNPS [proper noun]) or by noun form, in
this case based on grammatical number (PRP
[pronoun], NN/NNP [singular noun], NNS/NNPS
[plural noun]). We can merge verbs by finite-
ness (MD [modal], VBP/VBZ/VBD [finite verb],
VB/VBG/VBN [nonfinite verb]) or by verb form
(MD [modal], VB/VBP [base], VBD/VBN [-ed],
VBG [-ing], VBZ [-s]). In the latter case, verbs
with consistently similar forms are grouped?e.g.,
see can be a baseform (VB) or a present tense verb
(VBP).
The results are given in tables 3 and 4. We
find that merging verbs by finiteness and nouns by
noun type results in higher precision. This con-
firms that contexts can better distinguish syntactic,
but not necessarily morphological, properties. As
we will see in the next section, this mapping also
maintains distinctions in the lexicon. Such use of
local contexts, along with tag merging, can be used
to evaluate tagsets which claim to be distributional
(see, e.g., Dickinson and Jochim, 2008).
It should be noted that we have only explored
Noun type Noun form
Finiteness 82.9% 81.2%
Verb form 81.2% 79.5%
Table 3: Mapping precision (freq. ? 2)
Noun type Noun form
Finiteness 86.4% 85.3%
Verb form 84.5% 83.4%
Table 4: Mapping precision (freq. ? 200)
category mappings which merge tags, ignoring
possible splits. While splitting a tag like TO (to)
into prepositional and infinitival uses would be
ideal, we do not have the information automati-
cally available. We are thus limited in our eval-
uation by what the tagset offers. Some tag splits
can be automatically recovered (e.g., splitting PRP
based on properties such as person), but if it is au-
tomatically recoverable from the lexicon, we do
not necessarily need context to identify it, an idea
we turn to in the next section.
3.2 Evaluating tagset mappings
Some of the category distinctions made by frames
are more or less important for the context to make.
For example, it is detrimental if we conflate VB
and VBP because this is a prominent ambiguity for
many words (e.g., see). On the other hand, there
are no words which can be both VBP (e.g., see)
and VBZ (e.g., sees). Ideally, induction methods
would be able to distinguish all these cases?just
as they often make distinctions beyond what is in a
tagset?but there are differences in how problem-
atic the mappings are. If we group VB and VBP
into one tag, there is no way to recover that distinc-
tion; for VBP and VBZ, there are at least different
words which inherently take the different tags.
Thus, a mapping is preferred which does not
conflate tags that vary for individual words. To
calculate this, we compare the original lexicon
with a mapped lexicon and count the number of
words which lose a distinction. Consider the
words accept and accepts: accept varies between
VB and VBP; accepts is only VBZ. When we map
tags based on verb form, we count 1 for accept,
as VB and VBP are now one tag (Verb). When
we map verbs based on finiteness, we count 0 for
these two words, as accept still has two tags (V-
nonfin, V-fin) and accepts has one tag (V-fin).
37
We evaluate our mappings in table 5 by enumer-
ating the number of word types whose distinctions
are lost by a particular mapping (out of 44,520
word types); we also repeat the token precision
values for comparison. Perhaps unsurprisingly,
grouping words based on form results in high con-
fusability (cf. the discussion of see in section 3.1).
On the other hand, merging nouns by type and
verbs by finiteness results in something of a bal-
ance between precision and non-confusability. It
is thus these types of categorizations which we can
reasonably expect induction models to capture.
Lost Precision
Mapping tags ? 2 ? 200
All mappings 3003 85.9% 91.0%
PTB-17 2038 85.1% 89.7%
N. form/V. form 2699 79.5% 83.4%
N. type/V. form 2148 81.2% 84.5%
N. form/Finite 951 81.2% 85.3%
N. type/Finite 399 82.9% 86.4%
No mappings 0 77.3% 79.5%
Table 5: Confusable word types
For induction evaluation, in addition to an ac-
curacy metric, a metric such as the one we have
just proposed is important to gauge how much cor-
pus annotation information is lost when perform-
ing tagset mappings. For example, the PTB-17
mapping (Smith and Eisner, 2005) is commonly
used for evaluating category induction (Goldwa-
ter and Griffiths, 2007; Toutanova and Johnson,
2008), yet it loses distinctions for 2038 words.
We could also define mappings which lose no
distinctions in the lexicon. Initial experiments
show that this allows no merging of nouns, and
that the resulting precision is only minimally bet-
ter than no mapping at all. We should also note
that the number of confusable words may be too
high, given errors in the lexicon (cf. Dickinson,
2008). For example, removing tags occurring less
than 10% of the time for a word results in only 305
confusable words for the Noun type/Finiteness
(NF) mapping and 1575 for PTB-17.
4 Combining contexts
We have narrowly focused on identical contexts,
or frames, for identifying categories, but this could
leave us with as many categories as frames (67,721
for ? 2, 99 for ? 200, instead of 35 and 30). We
need to reduce the number of categories without
inappropriately merging them (cf. the notion of
?completeness? in Mintz, 2003; Christiansen and
Monaghan, 2006). Thus far, we have not utilized
a frame?s target words; we turn to these now, in
order to better gauge the effectiveness of frames
for identifying categories. Although the work is
somewhat preliminary, our goal is to continue to
investigate when contexts identify the same cate-
gory. This merging of contexts is different than
clustering words (e.g., Clark, 2000; Brown et al,
1992), but is applicable, as word clustering relies
on knowing which contexts identify the same cat-
egory.
4.1 Word-based combination
On their own, frames at best distinguish only very
broad categorical properties. This is perhaps un-
surprising, as the finer-grained distinctions in cor-
pora seem to be based on lexical properties more
than on additional context (see, e.g., Dickinson,
2008). If we want to combine contexts in a way
which maps to corpus tagsets, then, we need to
examine the target words. It is likely that two sets
share the same tag if they contain the same words
(cf. overlap in Mintz, 2003). In fact, the more a
frame?s word set overlaps with another?s word set,
the more likely it is unambiguous in the first place,
as the other set provides corroborating evidence.
Therefore, we use overlap of frames? word sets as
a criterion to combine them.
This allows us to combine frames which do not
share context words. For example, in (2) we find
frames identifying baseform verbs (VB) (2a) and
frames identifying cardinal numbers (CD) (2b),
despite having a variety of context words. Their
target word sets, however, are sufficiently similar.
(2) a. will to, will the, to the, to up,
would the, to their, n?t the,
to a, to its, to that, to to
b. or cents, $ million, rose %,
a %, about %, to %, $ a,
$ billion
By viewing frames as categories, in the fu-
ture we could also investigate splitting cate-
gories, based on subsets of words, morpho-
logical/phonological cues (e.g., Christiansen and
Monaghan, 2006), or on additional context words,
better handling frames that are ambiguous.
Calculating overlap We merge frames whose
word sets overlap, using a simple weighted fre-
38
quency distance metric. We define sufficient over-
lap as the case where a given percent of the words
in one frame?s word set are found in the other?s
word set. We define this test in either direction,
as smaller sets can be a subset of a larger set. For
example, the frames the on (224 tokens) and the
of (4304 tokens) have an overlap of 78 tokens;
overlap here is 34.8% (78/224). While we could
use a more sophisticated form of clustering (see,
e.g., Manning et al, 2008), this will help deter-
mine the viability of this general approach.
Of course, two sets may share a category with
relatively few shared words, and so we transitively
combine sets of contexts. If the overlap of frames
A andB meet our overlap criterion and the overlap
of frames A and C also meet the criterion, then all
three sets are merged, even if B and C have only
a small amount of overlap.7
Using the threshold of 200, we test criteria of
30%, 40%, and 50% overlap and consider the
frames? overlap calculated as a percentage of word
types or as a percentage of word tokens. For exam-
ple, if a word type occurs 10 times in one word set
and 20 in the other, the overlap of types is 1, and
the overlap of tokens is 10. Token overlap better
captures similarities in distributions of words.
4.2 Evaluation
Table 6 shows the number of categories for the
30%, 40%, and 50% type-based (TyB) and token-
based (ToB) overlap criteria for merging. As we
can see, the overlap based on tokens in word sets
results in more categories, i.e., fewer merges.
% TyB ToB
50% 59 75
40% 42 64
30% 27 50
Table 6: Number of categories by condition
The precision of each of these criteria is given
in table 7, evaluating on both the original tagset
and the noun type/finiteness (NF) mapping. We
can see that the token-based overlap is consistently
more accurate than type-based overlap, and there
is virtually no drop in precision for any of the
token-based conditions.8 Thus, for the rest of the
evaluation, we use only the token-based overlap.
7We currently do not consider overlap of already merged
sets, e.g., between A+B and C.
8Experiments at 20% show a noticeable drop in precision.
% Tags Frames TyB ToB
50% Orig. 79.5% 76.4% 79.5%
NF 86.4% 82.8% 86.4%
40% Orig. 79.5% 75.7% 79.3%
NF 86.4% 81.8% 86.1%
30% Orig. 79.5% 74.7% 79.1%
NF 86.4% 81.7% 86.1%
Table 7: Precision of merged frames
We mentioned that if frame word sets overlap,
the less ambiguous their category should be. We
check this by looking at the difference between
merged and unmerged frames, as shown in table 8.
The number of categories are also given in paren-
theses; for example, for 30% overlap, 41 frames
are unmerged, and the remaining 58 make up 9
categories. These results confirm for this data that
frames which are merged have a higher precision.
Merged Unmerged Overall
50% 93.4% (7) 79.9% (68) 86.4% (75)
40% 89.7% (10) 81.1% (54) 86.1% (64)
30% 89.7% (9) 77.4% (41) 86.1% (50)
Table 8: Precision of merged & unmerged frames
for NF mapping (with number of categories)
But are we only merging a select, small set of
words? To gauge this, we measure how much
of the corpus is categorized by the 99 most fre-
quent frames. Namely, 46,874 tokens occur as tar-
gets in our threshold of 99 frequent frames out of
663,608 target tokens in the entire corpus,9 a re-
call of 7.1%. Table 9 shows some recall figures for
the frequent frames. There are 9621 word types in
the set of target words for the 99 frequent frames,
which is 27.2% of the target lexicon. Crucially,
though, these 9621 are realized as 523,662 target
tokens in the corpus, or 78.9%. The words cate-
gorized by the frequent frames extend to a large
portion of the corpus (cf. also Mintz, 2003).
Tokens Types Coverage
Merged (30%) 5.0% 20.0% 61.5%
Unmerged (30%) 2.0% 11.5% 65.9%
Total Overlap 7.1% 27.2% 78.9%
Table 9: Recall of frames
9Because we remove frames which contain punctuation,
the set of target tokens is a subset of all words in the corpus.
39
4.2.1 Qualitative analysis
To better analyze what is happening for future
work, we look more closely at 30% overlap. Of
the 58 frames merged into 9 categories, 54 of them
have the samemajority tag after merging. The four
frames which get merged into a different category
are worth investigating, to see the method?s limi-
tations and potential for improvement.
Of the four frames which lose their majority
tag after merging, two can be ignored when map-
ping to the NF tags. The frame it the with ma-
jority tag VBZ becomes VBD when merged, but
both are V-fin. Likewise, n?t to changes from
VB to VBN, both cases of V-nonfin. The third
case reveals an evaluation problem with the orig-
inal tagset: the frames million $ (IN) and %
$ (TO) are merged into a category labeled TO.
The tag TO is for the word to and is not split into
prepositional and infinitival uses. Corpus cate-
gories such as these, which overlap in their def-
initions yet cannot be merged (due to their non-
overlapping uses), are particularly problematic for
evaluation.
The final case which does not properly merge is
the most serious. The frame is the (37% of to-
kens as preposition (IN)) merges with is a (41%
of tokens as VBG); the merged VBG category has
an precision of 34%. The distribution of tags is rel-
atively similar, the highest percentages being for
IN and VBG in both. This highlights the point
made earlier, that more information is needed, to
split the word sets.
4.2.2 TIGER Corpus
To better evaluate frequent frames for determin-
ing categories, we also test them on the German
TIGER corpus (Brants et al, 2002), version 2,
to see how the method handles data with freer
word order and more morphological complexity.
We use the training data, with the data split as
in Dubey (2004). The frequency threshold for
the WSJ (0.03% of all frames) leaves us with
only 60 frames in the TIGER corpus, and 51 of
these frames have a majority tag of NN.10 Thus,
we adjusted the threshold to 0.02% (102 mini-
mum occurrences), thereby obtaining 119 frequent
frames, with a precision of 82.0%. For the 30%
token-based overlap (the best result for English),
frames merged into 81 classes, with 79.1% pre-
cision. These precision figures are on a par with
10We use no tagset mappings for our TIGER experiments.
English (cf. table 7).11 Part of this might be due
to the fact that NN is still a large majority (76% of
the frames). Additionally, we find that, although
the frame tokens make up only 5.2% of the corpus
and the types make up 15.9% of the target lexi-
con, those types correspond to 67.2% of the target
corpus tokens.
5 Summary and Outlook
Building on the use of frames for human category
acquisition, we have explored the benefits of treat-
ing contexts?in this case, frames?as categories
and analyzed the consequences. This allowed us
to examine a way to evaluate tagset mappings and
provide feedback on distributional tagset design.
From there, we explored using lexical information
to combine contexts in a way which generally pre-
serves the intended category.
We evaluated this on English and German, but,
to fully verify our findings, a high priority is to
perform similar experiments on more corpora, em-
ploying different tagsets, for different languages.
Additionally, we need to expand the definition of
a context to more accurately categorize contexts,
while at the same time not lowering recall.
Acknowledgements
We wish to thank the Indiana University Compu-
tational Linguistics discussion group for feedback,
as well as the three anonymous reviewers.
A Some Penn Treebank POS tags
DT Determiner
JJ Adjective
JJR Adjective, comparative
JJS Adjective, superlative
MD Modal
NN Noun, singular or mass
NNS Noun, plural
NNP Proper noun, singular
NNPS Proper noun, plural
PDT Predeterminer
PRP Personal pronoun
PRP$ Possessive pronoun
RB Adverb
RBR Adverb, comparative
RBS Adverb, superlative
VB Verb, base form
VBD Verb, past tense
VBG Verb, gerund or present participle
VBN Verb, past participle
VBP Verb, non-3rd person singular present
VBZ Verb, 3rd person singular present
WDT Wh-determiner
WP$ Possessive wh-pronoun
11Interestingly, thresholds of 20% and 10% result in simi-
larly high precision.
40
References
Brants, Sabine, Stefanie Dipper, Silvia Hansen,
Wolfgang Lezius and George Smith (2002). The
TIGER Treebank. In Proceedings of TLT-02.
Sozopol, Bulgaria.
Brown, Peter F., Peter V. deSouza, Robert L. Mer-
cer, T. J. Watson, Vincent J. Della Pietra and
Jenifer C. Lai (1992). Class-Based n-gram
Models of Natural Language. Computational
Linguistics 18(4), 467?479.
Christiansen, Morten H. and Padraic Monaghan
(2006). Discovering verbs through multiple-cue
integration. In Action Meets Word: How Chil-
dren Learn Verbs, Oxford: OUP.
Clark, Alexander (2000). Inducing Syntactic Cat-
egories by Context Distribution Clustering. In
Proceedings of CoNLL-00. Lisbon, Portugal.
Clark, Alexander (2003). Combining Distribu-
tional and Morphological Information for Part
of Speech Induction. In Proceedings of EACL-
03. Budapest.
Dickinson, Markus (2008). Representations for
category disambiguation. In Proceedings of
Coling 2008. Manchester.
Dickinson, Markus and Charles Jochim (2008). A
Simple Method for Tagset Comparison. In Pro-
ceedings of LREC 2008. Marrakech, Morocco.
Dubey, Amit (2004). Statistical Parsing for Ger-
man: Modeling syntactic properties and anno-
tation differences. Ph.D. thesis, Saarland Uni-
versity, Germany.
Elworthy, David (1995). Tagset Design and In-
flected Languages. In Proceedings of the ACL-
SIGDAT Workshop. Dublin.
Goldberg, Yoav, Meni Adler and Michael Elhadad
(2008). EM Can Find Pretty Good HMM POS-
Taggers (When Given a Good Start). In Pro-
ceedings of ACL-08. Columbus, OH.
Goldwater, Sharon and Tom Griffiths (2007). A
fully Bayesian approach to unsupervised part-
of-speech tagging. In Proceedings of ACL-07.
Prague.
Headden III, William P., David McClosky and
Eugene Charniak (2008). Evaluating Unsu-
pervised Part-of-Speech Tagging for Grammar
Induction. In Proceedings of Coling 2008.
Manchester.
Hepple, Mark and Josef van Genabith (2000).
Experiments in Structure-Preserving Grammar
Compaction. In 1st Meeting on Speech Tech-
nology Transfer. Seville, Spain.
Koo, Terry, Xavier Carreras and Michael Collins
(2008). Simple Semi-supervised Dependency
Parsing. In Proceedings of ACL-08. Columbus,
OH.
Korhonen, Anna, Yuval Krymolowski and Zvika
Marx (2003). Clustering Polysemic Subcatego-
rization Frame Distributions Semantically. In
Proceedings of ACL-03. Sapporo.
MacWhinney, Brian (2000). The CHILDES
project: Tools for analyzing talk. Mahwah, NJ:
Lawrence Erlbaum Associates, third edn.
Manning, Christopher D., Prabhakar Raghavan
and Hinrich Schu?tze (2008). Introduction to In-
formation Retrieval. CUP.
Marcus, M., Beatrice Santorini and M. A.
Marcinkiewicz (1993). Building a large anno-
tated corpus of English: The Penn Treebank.
Computational Linguistics 19(2), 313?330.
Miller, Scott, Jethran Guinness and Alex Zama-
nian (2004). Name Tagging with Word Clusters
and Discriminative Training. In Proceedings of
HLT-NAACL 2004. Boston, MA.
Mintz, Toben H. (2002). Category induction
from distributional cues in an artificial lan-
guage. Memory & Cognition 30, 678?686.
Mintz, Toben H. (2003). Frequent frames as a
cue for grammatical categories in child directed
speech. Cognition 90, 91?117.
Parisien, Christopher, Afsaneh Fazly and Suzanne
Stevenson (2008). An Incremental Bayesian
Model for Learning Syntactic Categories. In
Proceedings of CoNLL-08. Manchester.
Redington, Martin, Nick Chater and Steven Finch
(1998). Distributional Information: A Powerful
Cue for Acquiring Syntactic Categories. Cogni-
tive Science 22(4), 425?469.
Schu?tze, Hinrich (1995). Distributional Part-of-
Speech Tagging. In Proceedings of EACL-95.
Dublin, Ireland.
Smith, Noah A. and Jason Eisner (2005). Con-
trastive Estimation: Training Log-Linear Mod-
els on Unlabeled Data. In Proceedings of
ACL?05. Ann Arbor, MI.
Toutanova, Kristina and Mark Johnson (2008).
A Bayesian LDA-based Model for Semi-
Supervised Part-of-speech Tagging. In Pro-
ceedings of NIPS 2008. Vancouver.
Wang, Hao and Toben H. Mintz (2007). A Dy-
namic Learning Model for Categorizing Words
Using Frames. In Proceedings of BUCLD 32.
pp. 525?536.
41
From detecting errors to automatically correcting them
Markus Dickinson
Department of Linguistics
Georgetown University
mad87@georgetown.edu
Abstract
Faced with the problem of annotation er-
rors in part-of-speech (POS) annotated
corpora, we develop a method for auto-
matically correcting such errors. Build-
ing on top of a successful error detection
method, we first try correcting a corpus us-
ing two off-the-shelf POS taggers, based
on the idea that they enforce consistency;
with this, we find some improvement. Af-
ter some discussion of the tagging process,
we alter the tagging model to better ac-
count for problematic tagging distinctions.
This modification results in significantly
improved performance, reducing the error
rate of the corpus.
1 Introduction
Annotated corpora serve as training material and
as ?gold standard? testing material for the devel-
opment of tools in computational linguistics, and
as a source of data for theoretical linguists search-
ing for relevant language patterns. However, they
contain annotation errors, and such errors provide
unreliable training and evaluation data, as has been
previously shown (see ch. 1 of Dickinson (2005)
and references therein). Improving the quality of
linguistic annotation where possible is thus a key
issue for the use of annotated corpora in computa-
tional and theoretical linguistics.
Research has gone into automatically detect-
ing annotation errors for part-of-speech annota-
tion (van Halteren, 2000; Kve?to?n and Oliva, 2002;
Dickinson and Meurers, 2003), yet there has
been virtually no work on automatically or semi-
automatically correcting such annotation errors.1
1Oliva (2001) specifies hand-written rules to detect and
Automatic correction can speed up corpus im-
provement efforts and provide new data for NLP
technology training on the corpus. Additionally,
an investigation into automatic correction forces
us to re-evaluate the technology using the corpus,
providing new insights into such technology.
We propose in this paper to automatically cor-
rect part-of-speech (POS) annotation errors in cor-
pora, by adapting existing technology for POS dis-
ambiguation. We build the correction work on
top of a POS error detection phase, described in
section 2. In section 3 we discuss how to eval-
uate corpus correction work, given that we have
no benchmark corpus to compare with. We turn
to the actual work of correction in section 4, us-
ing two different POS taggers as automatic cor-
rectors and using the Wall Street Journal (WSJ)
corpus as our data. After more thoroughly investi-
gating how problematic tagging distinctions affect
the POS disambiguation task, in section 5 wemod-
ify the tagging model in order to better account
for these distinctions, and we show this to signifi-
cantly reduce the error rate of a corpus.
It might be objected that automatic correction
of annotation errors will cause information to be
lost or will make the corpus worse than it was,
but the construction of a large corpus generally
requires semi-automated methods of annotation,
and automatic tools must be used sensibly at every
stage in the corpus building process. Automated
annotation methods are not perfect, but humans
also add errors, from biases and inconsistent judg-
ments. Thus, automatic corpus correction methods
can be used semi-automatically, just as the original
corpus creation methods were used.
then correct errors, but there is no general correction scheme.
265
2 Detecting POS Annotation Errors
To correct part-of-speech (POS) annotation errors,
one has to first detect such errors. Although there
are POS error detection approaches, using, e.g.,
anomaly detection (Eskin, 2000), our approach
builds on the variation n-gram algorithm intro-
duced in Dickinson and Meurers (2003) and Dick-
inson (2005). As we will show in section 5, such
a method is useful for correction because it high-
lights recurring problematic tag distinctions in the
corpus.
The idea behind the variation n-gram approach
is that a string occurring more than once can oc-
cur with different labels in a corpus, which is re-
ferred to as variation. Variation is caused by one
of two reasons: i) ambiguity: there is a type of
string with multiple possible labels and different
corpus occurrences of that string realize the differ-
ent options, or ii) error: the tagging of a string is
inconsistent across comparable occurrences.
The more similar the context of a variation, the
more likely the variation is an error. In Dickin-
son and Meurers (2003), contexts are composed
of words, and identity of the context is required.
The term variation n-gram refers to an n-gram (of
words) in a corpus that contains a string annotated
differently in another occurrence of the same n-
gram in the corpus. The string exhibiting the vari-
ation is referred to as the variation nucleus.
For example, in the WSJ corpus, part of the
Penn Treebank 3 release (Marcus et al, 1993), the
string in (1) is a variation 12-gram since off is a
variation nucleus that in one corpus occurrence is
tagged as a preposition (IN), while in another it is
tagged as a particle (RP).
(1) to ward off a hostile takeover attempt by
two European shipping concerns
Once the variation n-grams for a corpus have
been computed, heuristics are employed to clas-
sify the variations into errors and ambiguities. The
most effective heuristic takes into account the fact
that natural languages favor the use of local de-
pendencies over non-local ones: nuclei found at
the fringe of an n-gram are more likely to be gen-
uine ambiguities than those occurring with at least
one word of surrounding context.
Running the variation n-gram error detection
method on the WSJ turns up 7141 distinct2 non-
2Being distinct means each corpus position is only taken
into account for the longest variation n-gram it occurs in.
fringe nuclei, of which an estimated 92.8%, or
6627, are erroneous.3 Since a variation nucleus
refers to multiple corpus positions, this precision
is a precision on types; we, however, are correct-
ing tokens. Still, this precision is high enough to
experiment with error correction.
3 Methodology
Since we intend to correct a corpus with POS an-
notation errors, we have no true benchmark by
which to gauge the accuracy of the corrected cor-
pus, and we thus created a hand-checked sub-
corpus. Using the variation n-gram output, we
flagged every non-fringe variation nucleus (token)
as a potential error, giving us 21,575 flagged po-
sitions in the WSJ. From this set, we sampled 300
positions, removed the tag for each position, and
hand-marked what the correct tag should be, based
solely on the tagset definitions given in the WSJ
tagging manual (Santorini, 1990), i.e., blind to the
original data. Because some of the tagset distinc-
tions were not defined clearly enough in the guide-
lines, in 20 cases we could not decide what the ex-
act tag should be. For the purposes of comparison,
we score a match with either tag as correct since a
human could not disambiguate such cases.
For the benchmark, we find that 201 positions
in our sample set of 300 are correct, giving us a
precision of 67%. A correction method must then
surpass this precision figure in order to be useful.
4 Approach to correction
Since our error detection phase relies on variation
in annotation, i.e., the inconsistent application of
POS labels across the corpus, we propose to cor-
rect such errors by enforcing consistency in the
text. As van Halteren (2000) points out, POS tag-
gers can be used to enforce consistency, and so we
employ off-the-shelf supervised POS taggers for
error correction. The procedure is as follows:
1. Train the tagger on the entire corpus.
2. Run the trained tagger over the same corpus.
3. For the positions the variation n-gram detec-
tion method flags as potentially erroneous,
choose the label obtained in step 2.
We do not split training data from testing data be-
cause we want to apply the patterns found in the
3The recall cannot easily be estimated, but this is still a
significant number of errors.
266
whole corpus to the corpus we want to correct,
which happens to be the same corpus.4 If the tag-
ger has learned the consistent patterns in the cor-
pus, it will then generalize these patterns to the
problematic parts of the corpus.
This approach hinges on high-quality error de-
tection since in general we cannot assume that dis-
crepancies between a POS tagger and the bench-
mark are errors in the benchmark. Van Hal-
teren (2000), for example, found that his tagger
was correct in only 20% of disagreements with the
benchmark. By focusing only on the variation-
flagged positions, we expect the tagger decisions
to be more often correct than incorrect.
We use two off-the-shelf taggers for correc-
tion, the Markov model tagger TnT (Brants, 2000)
and the Decision Tree Tagger (Schmid, 1997),
which we will abbreviate as DTT. Both taggers
use probabilistic contextual and lexical informa-
tion to disambiguate a tag at a particular cor-
pus position. The difference is that TnT obtains
contextual probabilities from maximum likelihood
counts, whereas DTT constructs binary-branching
decision trees to obtain contextual probabilities.
In both cases, instead of looking at n-grams of
words, the taggers use n-grams of tags. This gen-
eralization is desirable, as the variation n-gram
method shows that the corpus has conflicting la-
bels for the exact same sequence of n words.
Results For the TnT tagger, we obtain an overall
precision of 71.67% (215/300) on the 300 hand-
annotated samples. For the DTT tagger, we get a
higher precision, that of 76.33% (229/300). The
DTT results are a significant improvement over
the original corpus precision of 67% (p = .0045),
while the TnT results are not.
As mentioned, tagger-benchmark disagree-
ments are more commonly tagger errors, but we
find the opposite for variation-flagged positions.
Narrowing in on the positions which the tagger
changed, we find a precision of 58.56% (65/111)
for TnT and 65.59% (69/107) for DTT. As the goal
of correction is to change tags with 100% accu-
racy, we place a priority in improving these fig-
ures.
One likely reason that DTT outperforms TnT is
4Note, then, that some typical tagging issues, such as
dealing with unknown words, are not an issue for us.
5All p-values in this paper are from McNemar?s Test (Mc-
Nemar, 1947) for analyzing matched dichotomous data (i.e.,
a correct or incorrect score for each corpus position from both
models).
its more flexible context. For instance, in example
(2)?which DTT correctly changes and TnT does
not? to know that such should be changed from
adjective (JJ) to pre-determiner (PDT), one only
need look at the following determiner (DT) an,
and that provides enough context to disambiguate.
TnT uses a fixed context of trigrams, and so can
be swayed by irrelevant tags?here, the previous
tags?which DTT can in principle ignore.6
(2) Mr. Bush was n?t interested in such/JJ an in-
formal get-together .
5 Modifying the tagging model
The errors detected by the variation n-gram
method arise from variation in the corpus, of-
ten reflecting decisions difficult for annotators to
maintain over the entire corpus, for example, the
distinction between preposition (IN) and particle
(RP) (as in (1)). Although these distinctions are
listed in the tagging guidelines (Santorini, 1990),
nowhere are they encoded in the tags themselves;
thus, a tagger has no direct way of knowing that IN
and RP are easily confusable but IN and NN (com-
mon noun) are not. In order to improve automatic
correction, we can add information about these re-
curring distinctions to the tagging model, making
the tagger aware of the difficult distinctions. But
how do we make a tagger ?aware? of a relevant
problematic distinction?
Consider the domain of POS tagging. Every
word patterns uniquely, yet there are generaliza-
tions about words which we capture by group-
ing them into POS classes. By grouping words
into the same class, there is often a claim that
these words share distributional properties. But
how true this is depends on one?s tagset (see, e.g.,
De?jean (2000)). If we can alter the tagset to bet-
ter match the distributional facts, we can improve
correction.
To see how problematic distinctions can assist
in altering the tagset, consider the words away and
aboard, both of which can be adverbs (RB) in the
Penn Treebank, as shown in (3a) and (4a). In ex-
ample (3b), we find that away can also be a par-
ticle (RP), thus making it a part of the ambigu-
ity class RB/RP. On the other hand, as shown in
(4b), aboard can be a preposition (IN), but not a
particle, putting it in the ambiguity class IN/RB.
Crucially, not only do away and aboard belong
6As DTT does not provide a way of viewing output trees,
we cannot confirm that this is the reason for improvement.
267
to different ambiguity classes, but their adverbial
uses are also distinguished. The adverbial away
is followed by from, a construction forbidden for
aboard. When we examine the RB/RP words, we
find that they form a natural class: apart, aside,
and away, all of which can be followed by from.
(3) a. the Cray-3 machine is at least another
year away/RB from a ... prototype
b. A lot of people think 0 I will give
away/RP the store
(4) a. Saturday ?s crash ... that *T* killed 132
of the 146 people aboard/RB
b. These are used * aboard/IN military heli-
copters
Although not every ambiguity class is so
cleanly delineated, this example demonstrates that
such classes can be used to redefine a tagging
model with more unified groupings.
5.1 Using complex ambiguity tags
We thus propose splitting a class such as RB into
subclasses, using these ambiguity classes?JJ/RB,
NN/RB, IN/RB, etc.?akin to previous work on
splitting labels in order to obtain better statistics
(e.g., Brants (1996); Ule (2003)) for situations
with ?the same label but different usage? (Ule,
2003, p. 181). By taking this approach, we are
narrowing in on what annotators were instructed
to focus on, namely ?difficult tagging decisions,?
(Santorini, 1990, p. 7).
We implement this idea by assigning words a
new, complex tag composed of its ambiguity class
and the benchmark tag for that position. For ex-
ample, ago has the ambiguity class IN/RB, and in
example (5a), it resolves to RB. Thus, following
the notation in Pla and Molina (2004), we assign
ago the complex ambiguity tag <IN/RB,RB> in
the training data, as shown in (5b).
(5) a. ago/RB
b. ago/<IN/RB,RB>
Complex ambiguity tags can provide better dis-
tinctions than the unaltered tags. For example,
words which vary between IN and RB and tagged
as IN (e.g., ago, tagged <IN/RB,IN>) can ignore
the contextual information that words varying be-
tween DT (determiner) and IN (e.g., that, tagged
<DT/IN,IN>) provide. This proposal is in the
spirit of a tagger like that described in Marquez et
al (2000), which breaks the POS tagging problem
into one problem for each ambiguity class, but be-
cause we alter the tagset here, different underlying
tagging algorithms can be used.
To take an example, consider the 5-gram rev-
enue of about $ 370 as it is tagged by TnT. The
5-gram (at position 1344) in the WSJ is annotated
as in (6). The tag for about is incorrect since
?about when used to mean ?approximately? should
be tagged as an adverb (RB), rather than a prepo-
sition (IN)? (Santorini, 1990, p. 22).
(6) revenue/NN of/IN about/IN $/$ 370/CD
Between of and $, the word about varies be-
tween preposition (IN) and adverb (RB): it is IN
67 times and RB 65 times. After training TnT on
the original corpus, we find that RB is a slightly
better predictor of the following $ tag, as shown in
(7), but, due to the surrounding probabilities, IN is
the tag TnT assigns.
(7) a. p($|IN,RB) = .0859
b. p($|IN,IN) = .0635
The difference between probabilities is more
pronounced in the model with complex ambigu-
ity tags. The word about generally varies between
three tags: IN, RB, and RP (particle), receiving the
ambiguity class IN/RB/RP (as of also does). For
IN/RB/RP words, RB is significantly more proba-
ble in this context than IN, as shown in (8).
(8) a. p($|<IN/RB/RP,IN>,<IN/RB/RP,RB>)
= .6016
b. p($|<IN/RB/RP,IN>,<IN/RB/RP,IN>)
= .1256
Comparing (7) and (8), we see that RB for the
ambiguity class of IN/RB/RP behaves differently
than the general class of RB words.
We have just shown that the contextual proba-
bilities of an n-gram tagger are affected when us-
ing complex ambiguity tags; lexical probabilities
are also dramatically changed. The relevant prob-
abilities were originally as in (9), but for the mod-
ified corpus, we have the probabilities in (10).
(9) a. p(about|IN) = 2074/134926 = .0154
b. p(about|RB) = 785/42207 = .0186
(10) a. p(about|<IN/RB/RP,IN>)
= 2074/64046 = .0324
b. p(about|<IN/RB/RP,RB>)
= 785/2045 = .3839
268
These altered probabilities provide information
similar to that found in a lexicalized tagger?
i.e., about behaves differently than the rest of its
class?but the altered contextual probabilities, un-
like a lexicalized tagger, bring general IN/RB/RP
class information to bear on this tagging situation.
Combining the two, we get the correct tag RB at
this position.
Since variation errors are errors for words with
prominent ambiguity classes, zeroing in on these
ambiguity classes should provide more accurate
probabilities. For this to work, however, we have
to ensure that we have the most effective ambigu-
ity class for every word.
5.2 Assigning complex ambiguity tags
In the tagging literature (e.g., Cutting et al(1992))
an ambiguity class is often composed of the set of
every possible tag for a word. For correction, us-
ing every possible tag for an ambiguity class will
result in too many classes, for two reasons: 1)
there are erroneous tags which should not be part
of the ambiguity class, and 2) some classes are ir-
relevant for disambiguating variation positions.
Guided by these considerations, we use the pro-
cedure below to assign complex ambiguity tags to
all words in the corpus, based on whether a word is
a non-fringe variation nucleus and thus flagged as
a potential error by the variation n-gram method
(choice 1), or is not a nucleus (choice 2).
1. Every word which is a variation word (nu-
cleus of a non-fringe variation) or type-
identical to a variation word is assigned:
(a) a complex tag reflecting the ambiguity
class of all relevant ambiguities in the
non-fringe variation nuclei; or
(b) a simple tag reflecting no ambiguity, if
the tag is irrelevant.
2. Based on their relevant unigram tags, non-
variation words are assigned:
(a) a complex tag, if the word?s ambiguity
tag also appears as a variation ambigu-
ity; or
(b) a simple tag, otherwise.
Variation words (choice 1) We start with vari-
ation nuclei because these are the potential errors
we wish to correct. An example of choice 1a is
ago, which varies between IN and RB as a nu-
cleus, and so receives the tag <IN/RB,IN> when
it resolves to IN and <IN/RB,RB> when it re-
solves to RB.
The choices are based on relevance, though; in-
stead of simply assigning all tags occurring in an
ambiguity to an ambiguity class, we filter out am-
biguities which we deem irrelevant. Similar to
Brill and Pop (1999) and Schmid (1997), we do
this by examining the variation unigrams and re-
moving tags which occur less than 0.01 of the
time for a word and less than 10 times overall.
This eliminates variations like ,/DT where DT ap-
pears 4210 times for an, but the comma tag ap-
pears only once. Doing this means that an can
now be grouped with other unambiguous deter-
miners (DT). In addition to removing some erro-
neous classes, we gain generality and avoid data
sparseness by using fewer ambiguity classes.
This pruning also means that some variation
words will receive tags which are not part of a
variation, which is when choice 1b is selected. For
instance, if the class is IN/RB and the current tag
is JJ, it gets JJ instead of <IN/RB,JJ> because a
word varying between IN and RB should not re-
solve to JJ. This situation also arises because we
are deriving the ambiguity tags only from the non-
fringe nuclei but are additionally assigning them
to type-identical words in the corpus. Words in-
volved in a variation may elsewhere have tags
never involved in a variation. For example, Ad-
vertisers occurs as a non-fringe nucleus varying
between NNP (proper noun) and NNPS (plural
proper noun). In non-variation positions, it ap-
pears as a plural common noun (NNS), which we
tag as NNS because NNS is not relevant to the
variation (NNP/NNPS) we wish to distinguish.
Onemore note is needed to explain how we han-
dled the vertical slashes used in the Penn Tree-
bank annotation. Vertical slashes represent uncer-
tainty between two tags?e.g., JJ|VBN means the
annotator could not decide between JJ and VBN
(past participle). Variation between JJ, VBN, and
JJ|VBN is simply variation between JJ and VBN,
and we represent it by the class JJ/VBN, thereby
ensuring that JJ/VBN has more data.
In short, we assign complex ambiguity tags to
variation words whenever possible (choice 1a), but
because of pruning and because of non-variation
tags for a word, we have to assign simple tags to
some corpus positions (choice 1b).
Non-variation words (choice 2) In order to
have more data for a tag, non-variation words also
269
take complex ambiguity tags. For words which
are not a part of a variation nucleus, we simi-
larly determine relevance and then assign a com-
plex ambiguity tag if the ambiguity is elsewhere
involved in a non-fringe nucleus (choice 2a). For
instance, even though join is never a non-fringe
variation nucleus, it gets the tag <VB/VBP,VB>
in the first sentence of the treebank because its am-
biguity class VB/VBP is represented in the non-
fringe nuclei.
On the other hand, we ignore ambiguity classes
which have no bearing on correction (choice 2b).
For example, ours varies between JJ and PRP (per-
sonal pronoun), but no non-fringe variation nuclei
have this same ambiguity class, so no complex
ambiguity tag is assigned. Our treatment of non-
variation words increases the amount of relevant
data (choice 2a) and still puts all non-varying data
together (choice 2b).
Uniform assignment of tags Why do we allow
only one ambiguity class per word over the whole
corpus? Consider the variation nucleus traded:
in publicly traded investments, traded varies be-
tween JJ and VBN, but in contracts traded on, it
varies between VBN and VBD (past tense verb). It
seems like it would be useful to keep the JJ/VBN
cases separate from the VBD/VBN ones, so that a
tagger can learn one set of patterns for JJ/VBN and
a different set for VBD/VBN. While that might
have its benefits, there are several reasons why re-
stricting words to a single ambiguity class is de-
sirable, i.e., why we assign traded the ambiguity
class JJ/VBD/VBN in this case.
First, we want to group as many of the word oc-
currences as possible together into a single class.
Using JJ/VBN and VBD/VBN as two separate am-
biguity classes would mean that traded as VBN
lacks a pattern of its own.
Secondly, multiple ambiguity classes for a
word can increase the number of possible tags
for a word. For example, instead of having
only the tag <JJ/VBD/VBN,VBN> for traded as
VBN, we would have both <JJ/VBN,VBN> and
<VBD/VBN,VBN>. With such an increase in the
number of tags, data sparseness becomes a prob-
lem.
Finally, although we know what the exact ambi-
guity in question is for a non-fringe nucleus, it is
too difficult to go through position by position to
guess the correct ambiguity for every other spot. If
we encounter a JJ/VBD/VBN word like followed
tagged as VBN, for example, we cannot know for
sure whether this is an instance where JJ/VBNwas
the decision which had to be made or if VBD/VBN
was the difficult choice; keeping only one ambigu-
ity class per word allows us to avoid guessing.
5.3 Results with complex ambiguity tags
Using complex ambiguity tags increases the size
of the tagset from 80 tags in the original corpus 7
to 418 tags in the altered tagset, 53 of which are
simple (e.g. IN) and 365 of which are complex
(e.g. <IN/RB,IN>).
TnT Examining the 300 samples of variation
positions from the WSJ corpus for the TnT tag-
ger with complex ambiguity tags, we find that
234 spots are correctly tagged, for a precision of
78.00%. Additionally, we find 73.86% (65/88)
precision for tags which have been changed from
the original corpus. The 78% precision is a signif-
icant improvement both over the original TnT pre-
cision of 71.67% (p = .008) and the benchmark of
67% (p = .001). Perhaps more revealing is the im-
provement in the precision of the changed tokens,
from 58.56% to 73.86%. With 73.86% precision
for changed positions, this means that we expect
approximately 3968 of the 5373 changes that the
tagger makes, out of 21,575 flagged positions, to
be correct changes. Thus, the error rate of the cor-
pus will be reduced.
Decision Tree Tagger (DTT) Using complex
ambiguity tags with DTT results in an overall pre-
cision of 78.33% (235/300) and a precision of
73.56% (64/87) for the changed positions. We im-
prove the overall error correction precision, from
76.33% to 78.33%, and the tagging of changed po-
sitions, going from 65.59% to 73.56%.
The results for all four models, plus the base-
line, are summarized in figure 1. From these fig-
ures, it seems that the solution for error correction
lies less in what tagging method is used and more
in the information we give each method.
The improvement in changed positions for both
TnT and DTT is partly attributable to the fact that
both tagging models are making fewer changes.
Indeed, training TnT on the original corpus and
then testing on the same corpus results in a 97.37%
similarity, but a TnT model trained on complex
ambiguity tags results in 98.49% similarity with
7The number of tags here counts tags with vertical slashes
separately.
270
Total Changed
Baseline 67.00% N/A
TnT 71.67% 58.56% (65/111)
C.A. TnT 78.00% 73.86% (65/88)
DTT 76.33% 65.59% (69/107)
C.A. DTT 78.33% 73.56% (64/87)
Figure 1: Summary of results
the original. DTT sees a parallel overall improve-
ment, from 97.47% to 98.33%. Clearly, then, each
complex ambiguity model is a closer fit to the orig-
inal corpus. Whether this means it is an overall
better POS tagging model is an open question.
Remaining issues We have shown that we can
improve the annotation of a corpus by using tag-
ging models with complex ambiguity tags, but can
we improve even further? To do so, there are sev-
eral obstacles to overcome.
First, some distinctions cannot be handled by an
automated system without semantic or non-local
information. As Marquez and Padro (1997) point
out, distinctions such as that between JJ and VBN
are essentially semantic distinctions without any
structural basis. For example, in the phrase pro-
posed offering, the reason that proposed should be
VBN is that it indicates a specific event. Since our
method uses no external semantic information, we
have no way to know how to correct this.8
Other distinctions, such as the one between
VBD and VBN, require some form of non-local
knowledge in order to disambiguate because it de-
pends on the presence or absence of an auxiliary
verb, which can be arbitrarily far away.
Secondly, sometimes the corpus was more of-
ten wrong than right for a particular pattern. This
can be illustrated by looking at the word later in
example (11), from the WSJ corpus. In the tag-
ging manual (Santorini, 1990, p. 25), we find the
description of later as in (12).
(11) Now , 13 years later , Mr. Lane has revived
his Artist ...
(12) later should be tagged as a simple
adverb (RB) rather than as a com-
parative adverb (RBR), unless its
meaning is clearly comparative. A
8Note that it could be argued that this lack of a structural
distinction contributed to the inconsistency among annotators
in the first place and thus made error detection successful.
useful diagnostic is that the com-
parative later can be preceded by
even or still.
In example (11), along with the fact that this
is 13 years later as compared to now (i.e., com-
parative), one can say Now, (even) 13 years later,
Mr. Lane has revived his Artist ..., favoring RBR
as a tag. But the trigram years later , occurs 16
times, 12 as RB and 4 as RBR. Assuming RBR is
correct, we clearly have a lot of wrong annotation
in the corpus, even though here the corpus is cor-
rectly annotated as RBR. As seen in (13), in the
context of following CD and NNS, RBR is much
less likely for TnT than either RB or JJ.
(13) a. p(JJ|CD,NNS) = .0366
b. p(RB|CD,NNS) = .0531
c. p(RBR|CD,NNS) = .0044
As shown in (14), even when we use complex
ambiguity tags, we still find this favoritism for RB
because of the overwhelmingly wrong data in the
corpus. However, we note that although RB is fa-
vored, its next closest competitor is now RBR?
not JJ?and RB is no longer favored by as much
as it was over RBR. We have more appropriately
narrowed down the list of proper tags for this posi-
tion by using complex ambiguity tags, but because
of too much incorrect annotation, we still generate
the wrong tag.
(14) a. p(<JJ/RB/RBR,JJ>|CD,NNS) = .0002
b. p(<JJ/RB/RBR,RB>|CD,NNS)= .0054
c. p(<JJ/RB/RBR,RBR>|CD,NNS)=.0017
These issues show that automatic correction
must be used with care, but they also highlight par-
ticular aspects of this tagset that any POS tagging
method will have difficulty overcoming, and the
effect of wrong data again serves to illustrate the
problem of annotation errors in training data.
6 Summary and Outlook
We have demonstrated the effectiveness of using
POS tagging technology to correct a corpus, once
an error detection method has identified poten-
tially erroneous corpus positions. We first showed
that using a tagger as is provides moderate re-
sults, but adapting a tagger to account for problem-
atic tag distinctions in the data?i.e., using com-
plex ambiguity tags?performs much better and
271
reduces the true error rate of a corpus. The distinc-
tions in the tagging model have more of an impact
on the precision of correction than the underlying
tagging algorithm.
Despite the gain in accuracy, we pointed out
that there are still several residual problems which
are difficult for any tagging system. Future work
will go into automatically sorting the tags so that
the difficult disambiguation decisions can be dealt
with differently from the easily disambiguated
corpus positions. Additionally, we will want to
test the method on a variety of corpora and tag-
ging schemes and gauge the impact of correc-
tion on POS tagger training and evaluation. We
hypothesize that this method will work for any
tagset with potentially confusing distinctions be-
tween tags, but this is yet to be tested.
The method of adapting a tagging model by us-
ing complex ambiguity tags originated from an
understanding that the POS tagging process is
crucially dependent upon the tagset distinctions.
Based on this, the correction work described in
this paper can be extended to the general task of
POS tagging, as a tagger using complex ambiguity
classes is attempting to tackle the difficult distinc-
tions in a corpus. To pursue this line of research,
work has to go into defining ambiguity classes for
all words in the corpus, instead of focusing on
words involved in variations.
Acknowledgments I would like to thank Det-
mar Meurers for helpful discussion, Stephanie
Dickinson for her statistical assistance, and the
three anonymous reviewers for their comments.
References
Thorsten Brants. 1996. Estimating Markov model
structures. In Proceedings of ICSLP-96, pages 893?
896, Philadelphia, PA.
Thorsten Brants. 2000. TnT ? a statistical part-of-
speech tagger. In Proceedings of ANLP-2000, pages
224?231, Seattle, WA.
Eric Brill and Mihai Pop. 1999. Unsupervised learn-
ing of disambiguation rules for part of speech tag-
ging. In Kenneth W. Church, editor, Natural Lan-
guage Processing Using Very Large Corpora, pages
27?42. Kluwer Academic Press, Dordrecht.
Doug Cutting, Julian Kupiec, Jan Pedersen, and Pene-
lope Sibun. 1992. A practical part-of-speech tagger.
In Proceedings of ANLP-92, pages 133?140, Trento,
Italy.
Herve? De?jean. 2000. How to evaluate and compare
tagsets? a proposal. In Proceedings of LREC-00,
Athens.
Markus Dickinson and W. Detmar Meurers. 2003.
Detecting errors in part-of-speech annotation. In
Proceedings of EACL-03, pages 107?114, Budapest,
Hungary.
Markus Dickinson. 2005. Error detection and correc-
tion in annotated corpora. Ph.D. thesis, The Ohio
State University.
Eleazar Eskin. 2000. Automatic corpus correction
with anomaly detection. In Proceedings of NAACL-
00, pages 148?153, Seattle, Washington.
Pavel Kve?to?n and Karel Oliva. 2002. Achieving an
almost correct PoS-tagged corpus. In Text, Speech
and Dialogue (TSD 2002), pages 19?26, Heidelberg.
Springer.
Mitchell Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated
corpus of English: The Penn Treebank. Computa-
tional Linguistics, 19(2):313?330.
Lluis Marquez and Lluis Padro. 1997. A flexible
POS tagger using an automatically acquired lan-
guage model. In Proceedings of ACL-97, pages
238?245, Madrid, Spain.
Lluis Marquez, Lluis Padro, and Horacio Rodriguez.
2000. A machine learning approach to POS tagging.
Machine Learning, 39(1):59?91.
Quinn McNemar. 1947. Note on the sampling error
of the difference between correlated proportions or
percentages. Psychometrika, 12:153?157.
Karel Oliva. 2001. The possibilities of automatic de-
tection/correction of errors in tagged corpora: a pilot
study on a German corpus. In Text, Speech and Di-
alogue (TSD 2001), pages 39?46. Springer.
Ferran Pla and AntonioMolina. 2004. Improving part-
of-speech tagging using lexicalized HMMs. Natural
Language Engineering, 10(2):167?189.
Beatrice Santorini. 1990. Part-of-speech tagging
guidelines for the Penn Treebank project (3rd revi-
sion, 2nd printing). Technical Report MS-CIS-90-
47, The University of Pennsylvania, Philadelphia,
PA, June.
Helmut Schmid. 1997. Probabilistic part-of-speech
tagging using decision trees. In D.H. Jones and H.L.
Somers, editors, New Methods in Language Process-
ing, pages 154?164. UCL Press, London.
Tylman Ule. 2003. Directed treebank refinement for
PCFG parsing. In Proceedings of TLT 2003, pages
177?188, Va?xjo?, Sweden.
Hans van Halteren. 2000. The detection of incon-
sistency in manually tagged text. In Anne Abeille?,
Thosten Brants, and Hans Uszkoreit, editors, Pro-
ceedings of LINC-00, Luxembourg.
272
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 259?267,
Beijing, August 2010
Generating Learner-Like Morphological Errors in Russian
Markus Dickinson
Indiana University
md7@indiana.edu
Abstract
To speed up the process of categorizing
learner errors and obtaining data for lan-
guages which lack error-annotated data,
we describe a linguistically-informed
method for generating learner-like mor-
phological errors, focusing on Russian.
We outline a procedure to select likely er-
rors, relying on guiding stem and suffix
combinations from a segmented lexicon to
match particular error categories and rely-
ing on grammatical information from the
original context.
1 Introduction
Work on detecting grammatical errors in the lan-
guage of non-native speakers covers a range of
errors, but it has largely focused on syntax in
a small number of languages (e.g., Vandeven-
ter Faltin, 2003; Tetreault and Chodorow, 2008).
In more morphologically-rich languages, learn-
ers naturally make many errors in morphology
(Dickinson and Herring, 2008). Yet for many lan-
guages, there is a major bottleneck in system de-
velopment: there are not enough error-annotated
learner corpora which can be mined to discover
the nature of learner errors, let alne enough data
to train or evaluate a system. Our perspective is
that one can speed up the process of determin-
ing the nature of learner errors via semi-automatic
means, by generating plausible errors.
We set out to generate linguistically-plausible
morphological errors for Russian, a language with
rich inflections. Generating learner-like errors has
practical and theoretical benefits. First, there is
the issue of obtaining training data; as Foster and
Andersen (2009) state, ?The ideal situation for a
grammatical error detection system is one where a
large amount of labelled positive and negative ev-
idence is available.? Generated errors can bridge
this gap by creating realistic negative evidence
(see also Rozovskaya and Roth, 2010). As for
evaluation data, generated errors have at least one
advantage over real errors, in that we know pre-
cisely what the correct form is supposed to be, a
problem for real learner data (e.g., Boyd, 2010).
By starting with a coarse error taxonomy, gen-
erating errors can improve categorization. Gener-
ated errors provide data for an expert?e.g., a lan-
guage teacher?to search through, expanding the
taxonomy with new error types or subtypes and/or
deprecating error types which are unlikely. Given
the lack of real learner data, this has the potential
to speed up error categorization and subsequent
system development. Furthermore, error genera-
tion techniques can be re-used, adjusting the er-
rors for different learner levels, first languages,
and so forth.
The error generation process can benefit by us-
ing linguistic properties to mimic learner varia-
tions. This can lead to more realistic errors, a ben-
efit for machine learning (Foster and Andersen,
2009), and can also provide feedback for the lin-
guistic representation used to generate errors by,
e.g., demonstrating under which linguistic condi-
tions certain error types are generated and under
which they are not.
We are specifically interested in generating
Russian morphological errors. To do this, we need
a knowledge base representing Russian morphol-
ogy, allowing us to manipulate linguistic proper-
ties. After outlining the coarse error taxonomy
259
(section 2), we discuss enriching a part-of-speech
(POS) tagger lexicon with segmentation informa-
tion (section 3). We then describe the steps in er-
ror generation (section 4), highlighting decisions
which provide insight for the analysis of learner
language, and show the impact on POS tagging in
section 5.
2 Error taxonomy
Russian is an inflecting language with relatively
free word order, meaning that morphological syn-
tactic properties are often encoded by affixes. In
(1a), for example, the verb ?????? needs a suf-
fix to indicate person and number, and ?? is the
third person singular form.1 By contrast, (1b) il-
lustrates a paradigm error: the suffix ?? is third
singular, but not the correct one. Generating such
a form requires having access to individual mor-
phemes and their linguistic properties.
(1) a. ??????+??
begin-3s
[nachina+et]
b. *??????+??
begin-3s
[nachina+it]
(diff. verb paradigm)
This error is categorized as a suffix error in fig-
ure 1, expanding the taxonomy in Dickinson and
Herring (2008). Stem errors are similarly catego-
rized, with Semantic errors defined with respect
to a particular context (e.g., using a different stem
than required by an activity).
For formation errors (#3), one needs to know
how stems relate. For instance, some verbs
change their form depending on the suffix, as in
(2). In (2c), the stem and suffix are morpholog-
ically compatible, just not a valid combination.
One needs to know that ??? is a variant of ???.
(2) a. ???+??
can-3p
[mog+ut]
b. ???+??
can-3s
[mozh+et]
c. *???+??
can-3p
[mozh+ut] (#3)
(wrong formation)
Using a basic lexicon without such knowledge,
it is hard to tell formation errors apart from lex-
1For examples, we write the Cyrillic form and include a
Roman transliteration (SEV 1362-78) for ease of reading.
0. Correct: The word is well-formed.
1. Stem errors:
(a) Stem spelling error
(b) Semantic error
2. Suffix errors:
(a) Suffix spelling error
(b) Lexicon error:
i. Derivation error: The wrong POS is
used (e.g., a noun as a verb).
ii. Inherency error: The ending is for a
different subclass (e.g., inanimate as
an animate noun).
(c) Paradigm error: The ending is from the
wrong paradigm.
3. Formation errors: The stem does not follow
appropriate spelling/sound change rules.
4. Syntactic errors: The form is correct, but
used in an in appropriate syntactic context
(e.g., nominative case in a dative context)
? Lexicon incompleteness: The form may be
possible, but is not attested.
Figure 1: Error taxonomy
icon incompleteness (see section 4.2.2). If ??-
??? (2c) is generated and is not in the lexicon,
we do not know whether it is misformed or simply
unattested. In this paper, we group together such
cases, since this allows for a simpler and more
quickly-derivable lexicon.
We have added syntactic errors, whereas Dick-
inson and Herring (2008) focused on strictly mor-
phological errors. Learners make syntactic errors
(e.g., Rubinstein, 1995; Rosengrant, 1987), and
when creating errors, a well-formed word may re-
sult. In the future, syntactic errors can be subdi-
vided (Boyd, 2010).
This classification is of possible errors, making
no claim about the actual distribution of learner
errors, and does not delve into issues such as
errors stemming from first language interference
(Rubinstein, 1995). Generating errors from the
possible types allows one to investigate which
types are plausible in which contexts.
260
It should be noted that we focus on inflec-
tional morphology in Russian, meaning that we
focus on suffixes. Prefixes are rarely used in Rus-
sian as inflectional markers; for example, prefixes
mark semantically-relevant properties for verbs of
motion. The choice of prefix is thus related to
the overall word choice, an issue discussed under
Random stem generation in section 4.2.4.
3 Enriching a POS lexicon
To create errors, we need a segmented lexicon
with morphological information, as in (3). Here,
the word ???? (mogu, ?I am able to?) is split into
stem and suffix, with corresponding POS tags.2
(3) a. ???,Vm-----a-p,?,Vmip1s-a-p
b. ???,Vm-----a-p,??,Vmip3s-a-p
c. ???,Vm-----a-p,NULL,Vmis-sma-p
The freely-available POS lexicon from Sharoff
et al (2008), specifically the file for the POS
tagger TnT (Brants, 2000), contains full words
(239,889 unique forms), with frequency informa-
tion. Working with such a rich database, we only
need segmentation, providing a quickly-obtained
lexicon (cf. five years for a German lexicon in
Geyken and Hanneforth, 2005).
In the future, one could switch to a different
tagset, such as that in Hana and Feldman (2010),
which includes reflexivity, animacy, and aspect
features. One could also expand the lexicon, by
adapting algorithms for analyzing unknown words
(e.g., Mikheev, 1997), as suggested by Feldman
and Hana (2010). Still, our lexicon continues the
trend of linking traditional categories used for tag-
ging with deeper analyses (Sharoff et al, 2008;
Hana and Feldman, 2010).3
3.1 Finding segments/morphemes
We use a set of hand-crafted rules to segment
words into morphemes, of the form: if the tag is x
and the word ends with y, make y the suffix. Such
rules are easily and quickly derivable from a text-
book listing of paradigms. For certain exceptional
2POS tags are from the compositional tagset in
Sharoff et al (2008). A full description is at: http://
corpus.leeds.ac.uk/mocky/msd-ru.html.
3This lexicon now includes lemma information, but each
word is not segmented (Erjavec, 2010).
cases, we write word-specific rules. Additionally,
we remove word, tag pairs indicating punctuation
or non-words (PUNC, SENT, -).
One could use a sophisticated method for lem-
matizing words (e.g., Chew et al, 2008; Schone
and Jurafsky, 2001), but we would likely have
to clean the lexicon later; as Feldman and Hana
(2010) point out, it is difficult to automatically
guess the entries for a word, without POS in-
formation. Essentially, we write precise rules to
specify part of the Russian system of suffixes; the
lexicon then provides the stems for free.
We use the lexicon for generating errors, but
it should be compatible with analysis. Thus, we
focus on suffixes for beginning and intermediate
learners. We can easily prune or add to the rule
set later. From an analysis perspective, we need to
specify that certain grammatical properties are in
a tag (see below), as an analyzer is to support the
provision of feedback. Since the rules are freely
available,4 changing these criteria for other pur-
poses is straightforward.
3.1.1 Segmentation rules
We have written 1112 general morphology
rules and 59 rules for the numerals ?one? through
?four,? based on the Nachalo textbooks (Ervin
et al, 1997). A rule is simply a tag, suffix pair.
For example, in (4), Ncmsay (Noun, common,
masculine, singular, accusative, animate [yes])
words should end in either ? (a) or ? (ya).
(4) a. Ncmsay, ?
b. Ncmsay, ?
A program consults this list and segments a
word appropriately, requiring at least one charac-
ter in the stem. In the case where multiple suffixes
match (e.g., ??? (eni) and ? (i) for singular neuter
locative nouns), the longer one is chosen, as it is
unambiguously correct.
We add information in 101 of the 1112
rules. All numerals, for instance, are tagged as
Mc-s (Numeral, cardinal, [unspecified gender],
singular). The tagset in theory includes properties
such as case; they just were not marked (see foot-
note 6, though). Based on the ending, we add all
4http://cl.indiana.edu/
?boltundevelopment/
261
possible analyses. Using an optional output tag,
in (5), Mc-s could be genitive (g), locative (l),
or dative (d) when it ends in ? (i). These rules
increase ambiguity, but are necessary for learner
feedback.
(5) a. Mc-s, ?, Mc-sg
b. Mc-s, ?, Mc-sl
c. Mc-s, ?, Mc-sd
In applying the rules, we generate stem tags, en-
coding properties constant across suffixes. Based
on the word?s tag (e.g., Ncmsay, cf. (4)) a stem
is given a more basic tag (e.g., Ncm--y).
3.2 Lexicon statistics
To be flexible for future use, we have only en-
riched 90% of the words (248,014), removing ev-
ery 10th word. Using the set of 1112 rules results
in a lexicon with 190,450 analyses, where analy-
ses are as in (3). For these 190,450 analyses, there
are 117 suffix forms (e.g., ?, ya) corresponding to
808 suffix analyses (e.g., <?, Ncmsay>). On av-
erage 3.6 suffix tags are observed with each stem-
tag pair, but 22.2 tags are compatible, indicating
incomplete paradigms.
4 Generating errors
4.1 Basic procedure
Taking the morpheme-based lexicon, we generate
errors by randomly combining morphemes into
full forms. Such randomness must be constrained,
taking into account what types of errors are likely
to occur.
The procedure is given in figure 2 and de-
tailed in the following sections. First, we use the
contextually-determined POS tag to restrict the
space of possibilities. Secondly, given that ran-
dom combinations of a stem and a suffix can result
in many unlikely errors, we guide the combina-
tions, using a loose notion of likelihood to ensure
that the errors fall into a reasonable distribution.
After examining the generated errors, one could
restrict the errors even further. Thirdly, we com-
pare the stem and suffix to determine the possible
types of errors. A full form may have several dif-
ferent interpretations, and thus, lastly, we select
the best interpretation(s).
1. Determine POS properties of the word to be
generated (section 4.2.1).
2. Generate a full-form, via guided random
stem and suffix combination (section 4.2.4).
3. Determine possible error analyses for the full
form (section 4.2.2).
4. Select the error type(s) from among multiple
possible interpretations (section 4.2.3).
Figure 2: Error generation procedure
By trying to determine the best error type in
step 4, the generation process can provide in-
sight into error analysis. This is important, given
that suffixes are highly ambiguous; for example,
?? (-oj) has at least 6 different uses for adjec-
tives. Analysis is not simply generation in reverse,
though. Importantly, error generation relies upon
the context POS tag for the intended form, for
the whole process. To morphologically analyze
the corrupted data, one has to POS tag corrupted
forms (see section 5).
4.2 Corruption
We use a corpus of 5 million words automatically
tagged by TnT (Brants, 2000) and freely avail-
able online (Sharoff et al, 2008).5 Because we
want to make linguistically-informed corruptions,
we corrupt only the words we have information
for, identifying the words in the corpus which are
found in the lexicon with the appropriate POS
tag.6 We also select only words which have in-
flectional morphology: nouns, verbs, adjectives,
pronouns, and numerals.7
4.2.1 Determining word properties (step 1)
We use the POS tag to restrict the properties of
a word, regardless of how exactly we corrupt it.
Either the stem and its tag or the suffix and its tag
5See http://corpus.leeds.ac.uk/mocky/.
6We downloaded the TnT lexicon in 2008, but the corpus
in 2009; although no versions are listed on the website, there
are some discrepancies in the tags used (e.g., numeral tags
now have more information). To accommodate, we use a
looser match for determining whether a tag is known, namely
checking whether the tags are compatible. In the future, one
can tweak the rules to match the newer lexicon.
7Adverbs inflect for comparative forms, but we do not
consider them here.
262
can be used as an invariant, to guide the gener-
ated form (section 4.2.4). In (6a), for instance, the
adjective (Af) stem or plural instrumental suffix
(Afp-pif) can be used as the basis for genera-
tion.
(6) a. Original: ?????? (serymi, ?gray?)
7? ???/Af+???/Afp-pif
b. Corrupted: ???+?? (seroj)
The error type is defined in terms of the original
word?s POS tag. For example, when we generate a
correctly-formed word, as in (6b), it is a syntactic
error if it does not match this POS tag.
4.2.2 Determining error types (step 3)
Before discussing word corruption in step 2
(section 4.2.4), we need to discuss how error types
are determined (this section) and how to han-
dle multiple possibilities (section 4.2.3), as these
steps help guide step 2. After creating a corrupted
word, we elucidate all possible interpretations in
step 3 by comparing each suffix analysis with the
stem. If the stem and suffix form a legitimate
word (in the wrong context), it is a syntactic er-
ror. Incompatible features means a derivation or
inherency error, depending upon which features
are incompatible. If the features are compati-
ble, but there is no attested form, it is either a
paradigm error?if we know of a different suffix
with the same grammatical features?or a forma-
tion/incompleteness issue, if not.
This is a crude morphological analyzer (cf.
Dickinson and Herring, 2008), but bases its anal-
yses on what is known about the invariant part of
the original word. If we use ??? (ymi) from (6a)
as an invariant, for instance, we know to treat it as
a plural instrumental adjective ending, regardless
of any other possible interpretations, because that
is how it was used in this context.
4.2.3 Selecting the error type (step 4)
Corrupted forms may have many possible anal-
yses. For example, in (6b), the suffix ?? (oj)
has been randomly attached to the stem ??? (ser).
With the stem fixed as an adjective, the suf-
fix could be a feminine locative adjective (syn-
tactic error), a masculine nominative adjective
(paradigm error), or an instrumental feminine
noun (derivation error). Given what learners are
likely to do, we can use some heuristics to restrict
the set of possible error types.
First, we hypothesize that a correctly-formed
word is more likely a correct form than a mis-
formed word. This means that correct words
and syntactic errors?correctly-formed words in
the wrong context?have priority over other error
types. For (6b), for instance, the syntactic error
outranks the paradigm and derivation errors.
Secondly, we hypothesize that a contextually-
appropriate word, even if misformed, is
more likely the correct interpretation than a
contextually-inappropriate word. When we have
cases where there is: a) a correctly-formed word
not matching the context (a syntactic error), and
b) a malformed word which matches the context
(e.g., a paradigm error), we list both possibilities.
Finally, derivation errors seem less likely than
the others (a point confirmed by native speakers),
giving them lower priority. Given these heuristics,
not only can we rule out error types after gener-
ating new forms, but we can also split the error
generation process into different steps.
4.2.4 Corrupting selected words (step 2)
Using these heuristics, we take a known word
and generate errors based on a series of choices.
For each choice, we randomly generate a num-
ber between 0 and 1 and choose based on a given
threshold. Thresholds should be reset when more
is known about error frequency, and more deci-
sions added as error subtypes are added.
Decision #1: Correct forms The first choice is
whether to corrupt the word or not. Currently, the
threshold is set at 0.5. If we corrupt the word, we
continue on to the next decision.
Decision #2: Syntactic errors We can either
generate a syntactic or a morphological error. On
the assumption that syntactic errors are more com-
mon, we currently set a threshold of 0.7, generat-
ing syntactic errors 70% of the time and morpho-
logical form errors 30% of the time.
To generate a correct form used incorrectly, we
extract the stem from the word and randomly se-
lect a new suffix. We keep selecting a suffix until
263
we obtain a valid form.8 An example is given in
(7): the original (7a) is a plural instrumental ad-
jective, unspecified for gender; in (7b), it is singu-
lar nominative feminine.
(7) a. ??????
gray
Afp-pif
???????
eyes
Ncmpin
.
.
SENT
b. ?????
Afpfsnf
???????
Ncmpin
.
SENT
One might consider ensuring that each error
differs from the original in only one property. Or
one might want to co-vary errors, such that, in
this case, the adjective and noun both change from
instrumental to nominative. While this is eas-
ily accomplished algorithmically, we do not know
whether learners obey these constraints. Generat-
ing errors in a relatively unbounded way can help
pinpoint these types of constraints.
While the form in (7b) is unambiguous, syntac-
tic errors can have more than one possible analy-
sis. In (8), for instance, this word could be cor-
rupted with an -?? (-oj) ending, indicating fem-
inine singular genitive, instrumental, or locative.
We include all possible forms.
(8) ?????
Afpfsg.Afpfsi.Afpfsl
???????
Ncmpin
.
SENT
Likewise, considering the heuristics in sec-
tion 4.2.3, generating a syntactic error may lead
to a form which may be contextually-appropriate.
Consider (9): in (9a), the verb-preposition com-
bination requires an accusative (Ncnsan). By
changing -? to -?, we generate a form which could
be locative case (Ncnsln, type #4) or, since -
? can be an accusative marker, a misformed ac-
cusative with the incorrect paradigm (#2c). We
list both possibilities.
(9) a. . . . ???????
. . . (he) looked
. . . Vmis-sma-p
?
into
Sp-a
????
the sky
Ncnsan
b. . . . ?
. . . Sp-a
????
Ncnsan+2c.Ncnsln+4
Syntactic errors obviously conflate many dif-
ferent error types. The taxonomy for German
8We ensure that we do not generate the original form, so
that the new form is contextually-inappropriate.
from Boyd (2010), for example, includes selec-
tion, agreement, and word order errors. Our syn-
tactic errors are either selection (e.g., wrong case
as object of preposition) or agreement errors (e.g.,
subject-verb disagreement in number). However,
without accurate syntactic information, we cannot
divvy up the error space as precisely. With the
POS information, we can at least sort errors based
on the ways in which they vary from the original
(e.g., incorrect case).
Finally, if no syntactic error can be derived, we
revert to the correct form. This happens when the
lexicon contains only one form for a given stem.
Without changing the stem, we cannot generate a
new form which is verifiably correct.
Decision #3: Morphological errors The next
decision is: should we generate a true morpholog-
ical error or a spelling error? We currently bias
this by setting a 0.9 threshold. The process for
generating morphological errors (0.9) is described
in the next few sections, after which spelling er-
rors (0.1) are described. Surely, 10% is an un-
derestimate of the amount of spelling errors (cf.
Rosengrant, 1987); however, for refining a mor-
phological error taxonomy, biasing towards mor-
phological errors is appropriate.
Decision #4: Invariant morphemes When cre-
ating a context-dependent morphological error,
we have to ask what the unit, or morpheme, is
upon which the full form is dependent. The final
choice is thus to select whether we keep the stem
analysis constant and randomize the suffix or keep
the suffix and randomize the stem. Consider that
the stem is the locus of a word?s semantic proper-
ties, and the (inflectional) suffix reflects syntactic
properties. If we change the stem of a word, we
completely change the semantics (error type #1b).
Changing the suffix, on the other hand, creates a
morphological error with the same basic seman-
tics. We thus currently randomly generate a suffix
90% of the time.
Random suffix generation Randomly attach-
ing a suffix to a fixed stem is the same procedure
used above to generate syntactic errors. Here,
however, we force the form to be incorrect, not
allowing syntactic errors. If attaching a suffix re-
264
sults in a correct form (contextually-appropriate
or not), we re-select a random suffix.
Similarly, the intention is to generate inherency
(#2bii), paradigm (#2c), and formation (#3) errors
(or lexicon incompleteness). All of these seem
to be more likely than derivation (#2bi) errors, as
discussed in section 4.2.3. If we allow any suffix
to combine, we will overwhelmingly find deriva-
tion errors. As pointed out in Dickinson and Her-
ring (2008), such errors can arise when a learner
takes a Russian noun, e.g., ??? (dush, ?shower?)
and attempts to use it as a verb, as in English, e.g.,
???? (dushu) with first person singular morphol-
ogy. In such cases, we have the wrong stem be-
ing used with a contextually-appropriate ending.
Derviation errors are thus best served with ran-
dom stem selection, as described in the next sec-
tion. To rule out derivation errors, we only keep
suffix analyses which have the same major POS as
the stem.
For some stems, particular types of errors are
impossible to generate. a) Inherency errors do not
occur for underspecified stems, as happens with
adjectives. For example, ???- (nov-, ?new?) is an
adjective stem which is compatible with any ad-
jective ending. b) Paradigm errors cannot occur
for words whose suffixes in the lexicon have no al-
ternate forms; for instance, there is only one way
to realize a third singular nominative pronoun. c)
Lexicon incompleteness cannot be posited for a
word with a complete paradigm. These facts show
that the generated error types are biased, depend-
ing upon the POS and the completeness of the lex-
icon.
Random stem generation Keeping the suffix
fixed and randomly selecting a stem ties the gen-
erated form to the syntactic context, but changes
the semantics. Thus, these generated errors are
firstly semantic errors (#1b), featuring stems in-
appropriate for the context, in addition to having
some other morphological error. The fact that,
given a context, we have to generate two errors
lends weight to the idea that these are less likely.
A randomly-generated stem will most likely
be of a different POS class than the suffix, re-
sulting in a derivation error (#2bi). Further, as
with all morphological errors, we restrict the gen-
erated word not to be a correctly-formed word,
and we do not allow the stem or the suffix to be
closed class items. It makes little sense to put
noun inflections on a preposition, for example,
and derivation errors involve open class words.9
Spelling errors For spelling errors, we create an
error simply by randomly inserting, deleting, or
substituting a single character in the word.10 This
will either be a stem (#1a) or a suffix (#2a) error. It
is worth noting that since we know the process of
creating this error, we are able to compartmental-
ize spelling errors from morphological ones. An
error analyzer, however, will have a harder time
distinguishing them.
5 Tagging the corpus
Figure 3 presents the distribution of error types
generated, where Word refers to the number of
words with a particular error type, as opposed to
the count of error type+POS pairs, as each word
can have more than one POS for an error type (cf.
(9b)). For the 780,924 corrupted words, there are
2.67 error type+POS pairs per corrupted word. In-
herency (#2bii) errors in particular have many tags
per word, since the same suffix can have multiple
similar deviations from the original (cf. (8)). Fig-
ure 3 shows that we have generated roughly the
distribution we wanted, based on our initial ideas
of linguisic plausibility.
Type Word POS Type Word POS
1a 19,661 19,661 1b-2bi 11,772 11,772
2a 6,560 6,560 1b-2bii 5,529 5,529
2bii 150,710 749,292 1b-2c 279 279
2c 94,211 94,211 1b-3+ 1,770 1,770
4 524,269 721,051
3+ 83,763 208,208 1b-all 19,350 19,350
Figure 3: Distribution of generated errors
Without an error detection system, it is hard to
gauge the impact of the error generation process.
Although it is not a true evaluation of the error
generation process, as a first step, we test a POS
9Learners often misuse, e.g., prepositions, but these er-
rors do not affect morphology. Future work should examine
the relation between word choice and derivation errors, in-
cluding changes in prefixes.
10One could base spelling errors on known or assumed
phonological confusions (cf. Hovermale and Martin, 2008).
265
tagger against the newly-created data. This helps
test the difficulty of tagging corrupted forms, a
needed step in the process of analyzing learner
language. Note that for providing feedback, it
seems desirable to have the POS tagger match
the tag of the corrupted form. This is a different
goal than developing POS taggers which are ro-
bust to noise (e.g., Bigert et al, 2003), where the
tag should be of the original word.
To POS tag, we use the HMM tagger TnT
(Brants, 2000) with the model from http://
corpus.leeds.ac.uk/mocky/. The re-
sults on the generated data are in figure 4, using
a lenient measure of accuracy: a POS tag is cor-
rect if it matches any of the tags for the hypoth-
esized error types. The best performance is for
uncorrupted known words,11 but notable is that,
out of the box, the tagger obtains 79% precision
on corrupted words when compared to the gener-
ated tags, but is strongly divergent from the orig-
inal (no longer correct) tags. Given that 67%
(524,269780,924 ) of words have a syntactic error?i.e., awell-formed word in the wrong context?this in-
dicates that the tagger is likely relying on the form
in the lexicon more than the context.
Gold Tags
Original Error # words
Corrupted 3.8% 79.0% 780,924
Unchanged:
Known 92.1% 92.1% 965,280
Unknown 81.9% 81.9% 3,484,909
Overall 72.1% 83.4% 5,231,113
Figure 4: POS tagging results, comparing tagger
output to Original tags and Error tags
It is difficult to break down the results for cor-
rupted words by error type, since many words are
ambiguous between several different error types,
and each interpretation may have a different POS
tag. Still, we can say that words which are syn-
tactic errors have the best tagging accuracy. Of
the 524,269 words which may be syntactic er-
rors, TnT matches a tag in 96.1% of cases. Suffix
spelling errors are particularly in need of improve-
11Known here refers to being in the enriched lexicon, as
these are the cases we specificaly did not corrupt.
ment: only 17.3% of these words are correctly
tagged (compared to 62% for stem spelling er-
rors). With an ill-formed suffix, the tagger simply
does not have reliable information. To improve
tagging for morphological errors, one should in-
vestigate which linguistic properties are being in-
correctly tagged (cf. sub-tagging in Hana et al,
2004) and what roles distributional, morphologi-
cal, or lexicon cues should play in tagging learner
language (see also D??az-Negrillo et al, 2010).
6 Conclusions and Outlook
We have developed a general method for gener-
ating learner-like morphological errors, and we
have demonstrated how to do this for Russian.
While many insights are useful for doing error
analysis (including our results for POS tagging
the resulting corpus), generation proceeds from
knowing grammatical properties of the original
word. Generating errors based on linguistic prop-
erties has the potential to speed up the process of
categorizing learner errors, in addition to creating
realistic data for machine learning systems. As a
side effect, we also added segmentation to a wide-
coverage POS lexicon.
There are several directions to pursue. The
most immediate step is to properly evaluate the
quality of generated errors. Based on this analysis,
one can refine the taxonomy of errors, and thereby
generate even more realistic errors in a future iter-
ation. Additionally, building from the initial POS
tagging results, one can work on generally analyz-
ing the morphology of learner language, includ-
ing teasing apart what information a POS tagger
needs to examine and dealing with multiple hy-
potheses (Dickinson and Herring, 2008).
Acknowledgements
I would like to thank Josh Herring, Anna Feld-
man, Jennifer Foster, and three anonymous re-
viewers for useful comments on this work.
266
References
Bigert, Johnny, Ola Knutsson and Jonas Sjo?bergh
(2003). Automatic Evaluation of Robustness
and Degradation in Tagging and Parsing. In
Proceedings of RANLP-2003. Borovets, Bul-
garia, pp. 51?57.
Boyd, Adriane (2010). EAGLE: an Error-
Annotated Corpus of Beginning Learner Ger-
man. In Proceedings of LREC-10. Malta.
Brants, Thorsten (2000). TnT ? A Statistical Part-
of-Speech Tagger. In Proceedings of ANLP-00.
Seattle, WA, pp. 224?231.
Chew, Peter A., Brett W. Bader and Ahmed Abde-
lali (2008). Latent Morpho-Semantic Analysis:
Multilingual Information Retrieval with Char-
acter N-Grams and Mutual Information. In Pro-
ceedings of Coling 2008. Manchester, pp. 129?
136.
D??az-Negrillo, Ana, Detmar Meurers, Salvador
Valera and Holger Wunsch (2010). Towards
interlanguage POS annotation for effective
learner corpora in SLA and FLT. Language Fo-
rum .
Dickinson, Markus and Joshua Herring (2008).
Developing Online ICALL Exercises for Rus-
sian. In The 3rd Workshop on Innovative Use
of NLP for Building Educational Applications.
Columbus, OH, pp. 1?9.
Erjavec, Tomaz? (2010). MULTEXT-East Ver-
sion 4: Multilingual Morphosyntactic Specifi-
cations, Lexicons and Corpora. In Proceedings
of LREC-10. Malta.
Ervin, Gerard L., Sophia Lubensky and Donald K.
Jarvis (1997). Nachalo: When in Russia . . . .
New York: McGraw-Hill.
Feldman, Anna and Jirka Hana (2010). A
Resource-light Approach to Morpho-syntactic
Tagging. Amsterdam: Rodopi.
Foster, Jennifer and Oistein Andersen (2009).
GenERRate: Generating Errors for Use in
Grammatical Error Detection. In The 4th Work-
shop on Innovative Use of NLP for Building Ed-
ucational Applications. Boulder, CO, pp. 82?
90.
Geyken, Alexander and Thomas Hanneforth
(2005). TAGH: A Complete Morphology for
German Based on Weighted Finite State Au-
tomata. In FSMNLP 2005. Springer, pp. 55?66.
Hana, Jirka and Anna Feldman (2010). A Posi-
tional Tagset for Russian. In Proceedings of
LREC-10. Malta.
Hana, Jirka, Anna Feldman and Chris Brew
(2004). A Resource-light Approach to Russian
Morphology: Tagging Russian using Czech
resources. In Proceedings of EMNLP-04.
Barcelona, Spain.
Hovermale, DJ and Scott Martin (2008). Devel-
oping an Annotation Scheme for ELL Spelling
Errors. In Proceedings of MCLC-5 (Midwest
Computational Linguistics Colloquium). East
Lansing, MI.
Mikheev, Andrei (1997). Automatic Rule Induc-
tion for Unknown-Word Guessing. Computa-
tional Linguistics 23(3), 405?423.
Rosengrant, Sandra F. (1987). Error Patterns in
Written Russian. The Modern Language Jour-
nal 71(2), 138?145.
Rozovskaya, Alla and Dan Roth (2010). Training
Paradigms for Correcting Errors in Grammar
and Usage. In Proceedings of HLT-NAACL-10.
Los Angeles, California, pp. 154?162.
Rubinstein, George (1995). On Case Errors Made
in Oral Speech by American Learners of Rus-
sian. Slavic and East European Journal 39(3),
408?429.
Schone, Patrick and Daniel Jurafsky (2001).
Knowledge-Free Induction of Inflectional Mor-
phologies. In Proceedings of NAACL-01. Pitts-
burgh, PA.
Sharoff, Serge, Mikhail Kopotev, Tomaz? Erjavec,
Anna Feldman and Dagmar Divjak (2008). De-
signing and evaluating Russian tagsets. In Pro-
ceedings of LREC-08. Marrakech.
Tetreault, Joel and Martin Chodorow (2008). The
Ups and Downs of Preposition Error Detection
in ESL Writing. In Proceedings of COLING-
08. Manchester.
Vandeventer Faltin, Anne (2003). Syntactic error
diagnosis in the context of computer assisted
language learning. The`se de doctorat, Univer-
site? de Gene`ve, Gene`ve.
267
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 729?738,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Detecting Errors in Automatically-Parsed Dependency Relations
Markus Dickinson
Indiana University
md7@indiana.edu
Abstract
We outline different methods to detect er-
rors in automatically-parsed dependency
corpora, by comparing so-called depen-
dency rules to their representation in the
training data and flagging anomalous ones.
By comparing each new rule to every rel-
evant rule from training, we can identify
parts of parse trees which are likely erro-
neous. Even the relatively simple methods
of comparison we propose show promise
for speeding up the annotation process.
1 Introduction and Motivation
Given the need for high-quality dependency parses
in applications such as statistical machine transla-
tion (Xu et al, 2009), natural language generation
(Wan et al, 2009), and text summarization evalu-
ation (Owczarzak, 2009), there is a corresponding
need for high-quality dependency annotation, for
the training and evaluation of dependency parsers
(Buchholz and Marsi, 2006). Furthermore, pars-
ing accuracy degrades unless sufficient amounts
of labeled training data from the same domain
are available (e.g., Gildea, 2001; Sekine, 1997),
and thus we need larger and more varied anno-
tated treebanks, covering a wide range of domains.
However, there is a bottleneck in obtaining an-
notation, due to the need for manual interven-
tion in annotating a treebank. One approach is
to develop automatically-parsed corpora (van No-
ord and Bouma, 2009), but a natural disadvantage
with such data is that it contains parsing errors.
Identifying the most problematic parses for human
post-processing could combine the benefits of au-
tomatic and manual annotation, by allowing a hu-
man annotator to efficiently correct automatic er-
rors. We thus set out in this paper to detect errors
in automatically-parsed data.
If annotated corpora are to grow in scale and re-
tain a high quality, annotation errors which arise
from automatic processing must be minimized, as
errors have a negative impact on training and eval-
uation of NLP technology (see discussion and ref-
erences in Boyd et al, 2008, sec. 1). There is work
on detecting errors in dependency corpus annota-
tion (Boyd et al, 2008), but this is based on finding
inconsistencies in annotation for identical recur-
ring strings. This emphasis on identical strings can
result in high precision, but many strings do not re-
cur, negatively impacting the recall of error detec-
tion. Furthermore, since the same strings often re-
ceive the same automatic parse, the types of incon-
sistencies detected are likely to have resulted from
manual annotation. While we can build from the
insight that simple methods can provide reliable
annotation checks, we need an approach which re-
lies on more general properties of the dependency
structures, in order to develop techniques which
work for automatically-parsed corpora.
Developing techniques to detect errors in parses
in a way which is independent of corpus and
parser has fairly broad implications. By using
only the information available in a training corpus,
the methods we explore are applicable to annota-
tion error detection for either hand-annotated or
automatically-parsed corpora and can also provide
insights for parse reranking (e.g., Hall and Nova?k,
2005) or parse revision (Attardi and Ciaramita,
2007). Although we focus only on detecting errors
in automatically-parsed data, similar techniques
have been applied for hand-annotated data (Dick-
inson, 2008; Dickinson and Foster, 2009).
Our general approach is based on extracting
a grammar from an annotated corpus and com-
paring dependency rules in a new (automatically-
annotated) corpus to the grammar. Roughly speak-
ing, if a dependency rule?which represents all the
dependents of a head together (see section 3.1)?
does not fit well with the grammar, it is flagged as
potentially erroneous. The methods do not have
to be retrained for a given parser?s output (e.g.,
729
Campbell and Johnson, 2002), but work by com-
paring any tree to what is in the training grammar
(cf. also approaches stacking hand-written rules
on top of other parsers (Bick, 2007)).
We propose to flag erroneous parse rules, using
information which reflects different grammatical
properties: POS lookup, bigram information, and
full rule comparisons. We build on a method to
detect so-called ad hoc rules, as described in sec-
tion 2, and then turn to the main approaches in sec-
tion 3. After a discussion of a simple way to flag
POS anomalies in section 4, we evaluate the dif-
ferent methods in section 5, using the outputs from
two different parsers. The methodology proposed
in this paper is easy to implement and independent
of corpus, language, or parser.
2 Approach
We take as a starting point two methods for detect-
ing ad hoc rules in constituency annotation (Dick-
inson, 2008). Ad hoc rules are CFG productions
extracted from a treebank which are ?used for spe-
cific constructions and unlikely to be used again,?
indicating annotation errors and rules for ungram-
maticalities (see also Dickinson and Foster, 2009).
Each method compares a given CFG rule to all
the rules in a treebank grammar. Based on the
number of similar rules, a score is assigned, and
rules with the lowest scores are flagged as poten-
tially ad hoc. This procedure is applicable whether
the rules in question are from a new data set?as in
this paper, where parses are compared to a training
data grammar?or drawn from the treebank gram-
mar itself (i.e., an internal consistency check).
The two methods differ in how the comparisons
are done. First, the bigram method abstracts a
rule to its bigrams. Thus, a rule such as NP ?
JJ NN provides support for NP ? DT JJ JJ NN,
in that it shares the JJ NN sequence. By con-
trast, in the other method, which we call the whole
rule method,1 a rule is compared in its totality
to the grammar rules, using Levenshtein distance.
There is no abstraction, meaning all elements are
present?e.g., NP? DT JJ JJ NN is very similar
to NP ? DT JJ NN because the sequences differ
by only one category.
While previously used for constituencies, what
is at issue is simply the valency of a rule, where
by valency we refer to a head and its entire set
1This is referred to whole daughters in Dickinson (2008),
but the meaning of ?daughters? is less clear for dependencies.
of arguments and adjuncts (cf. Przepio?rkowski,
2006)?that is, a head and all its dependents. The
methods work because we expect there to be reg-
ularities in valency structure in a treebank gram-
mar; non-conformity to such regularities indicates
a potential problem.
3 Ad hoc rule detection
3.1 An appropriate representation
To capture valency, consider the dependency tree
from the Talbanken05 corpus (Nilsson and Hall,
2005) in figure 1, for the Swedish sentence in (1),
which has four dependency pairs.2
(1) Det
it
ga?r
goes
bara
just
inte
not
ihop
together
.
?It just doesn?t add up.?
SS MA NA PL
Det ga?r bara inte ihop
PO VV AB AB AB
Figure 1: Dependency graph example
On a par with constituency rules, we define a
grammar rule as a dependency relation rewriting
as a head with its sequence of POS/dependent
pairs (cf. Kuhlmann and Satta, 2009), as in fig-
ure 2. This representation supports the detection
of idiosyncracies in valency.3
1. TOP? root ROOT:VV
2. ROOT? SS:PO VV MA:AB NA:AB PL:AB
3. SS? PO 5. NA? AB
4. MA? AB 6. PL? AB
Figure 2: Rule representation for (1)
For example, for the ROOT category, the head
is a verb (VV), and it has 4 dependents. The
extent to which this rule is odd depends upon
whether comparable rules?i.e., other ROOT rules
or other VV rules (see section 3.2)?have a simi-
lar set of dependents. While many of the other
rules seem rather spare, they provide useful infor-
mation, showing categories which have no depen-
dents. With a TOP rule, we have a rule for every
2Category definitions are in appendix A.
3Valency is difficult to define for coordination and is spe-
cific to an annotation scheme. We leave this for the future.
730
head, including the virtual root. Thus, we can find
anomalous rules such as TOP ? root ROOT:AV
ROOT:NN, where multiple categories have been
parsed as ROOT.
3.2 Making appropriate comparisons
In comparing rules, we are trying to find evidence
that a particular (parsed) rule is valid by examining
the evidence from the (training) grammar.
Units of comparison To determine similarity,
one can compare dependency relations, POS tags,
or both. Valency refers to both properties, e.g.,
verbs which allow verbal (POS) subjects (depen-
dency). Thus, we use the pairs of dependency re-
lations and POS tags as the units of comparison.
Flagging individual elements Previous work
scored only entire rules, but some dependencies
are problematic and others are not. Thus, our
methods score individual elements of a rule.
Comparable rules We do not want to com-
pare a rule to all grammar rules, only to those
which should have the same valents. Compara-
bility could be defined in terms of a rule?s depen-
dency relation (LHS) or in terms of its head. Con-
sider the four different object (OO) rules in (2).
These vary a great deal, and much of the variabil-
ity comes from the fact that they are headed by
different POS categories, which tend to have dif-
ferent selectional properties. The head POS thus
seems to be predictive of a rule?s valency.
(2) a. OO? PO
b. OO? DT:EN AT:AJ NN ET:VV
c. OO? SS:PO QV VG:VV
d. OO? DT:PO AT:AJ VN
But we might lose information by ignoring rules
with the same left-hand side (LHS). Our approach
is thus to take the greater value of scores when
comparing to rules either with the same depen-
dency relation or with the same head. A rule has
multiple chances to prove its value, and low scores
will only be for rules without any type of support.
Taking these points together, for a given rule of
interest r, we assign a score (S) to each element ei
in r, where r = e1...em by taking the maximum
of scores for rules with the same head (h) or same
LHS (lhs), as in (3). For the first element in (2b),
for example, S(DT:EN) = max{s(DT:EN, NN),
s(DT:EN, OO)}. The question is now how we de-
fine s(ei, c) for the comparable element c.
(3) S(ei) = max{s(ei, h), s(ei, lhs)}
3.3 Whole rule anomalies
3.3.1 Motivation
The whole rule method compares a list of a rule?s
dependents to rules in a database, and then flags
rule elements without much support. By using all
dependents as a basis for comparison, this method
detects improper dependencies (e.g., an adverb
modifying a noun), dependencies in the wrong
overall location of a rule (e.g., an adverb before
an object), and rules with unnecessarily long ar-
gument structures. For example, in (4), we have
an improper relation between skall (?shall?) and
sambeskattas (?be taxed together?), as in figure 3.
It is parsed as an adverb (AA), whereas it should
be a verb group (VG). The rule for this part of the
tree is +F ? ++:++ SV AA:VV, and the AA:VV
position will be low-scoring because the ++:++ SV
context does not support it.
(4) Makars
spouses?
o?vriga
other
inkomster
incomes
a?r
are
B-inkomster
B-incomes
och
and
skall
shall
som
as
tidigare
previously
sambeskattas
be taxed togeher
.
.
?The other incomes of spouses are B-incomes and
shall, as previously, be taxed together.?
++ +F UK KA VG
och skall som tidigare sambeskattas
++ SV UK AJ VV
++ +F UK SS AA
och skall som tidigare sambeskattas
++ SV UK AJ VV
Figure 3: Wrong label (top=gold, bottom=parsed)
3.3.2 Implementation
The method we use to determine similarity arises
from considering what a rule is like without a
problematic element. Consider +F ? ++:++ SV
AA:VV from figure 3, where AA should be a dif-
ferent category (VG). The rule without this er-
ror, +F ? ++:++ SV, starts several rules in the
731
training data, including some with VG:VV as the
next item. The subrule ++:++ SV seems to be
reliable, whereas the subrules containing AA:VV
(++:++ AA:VV and SV AA:VV) are less reliable.
We thus determine reliability by seeing how often
each subsequence occurs in the training rule set.
Throughout this paper, we use the term subrule
to refer to a rule subsequence which is exactly one
element shorter than the rule it is a component
of. We examine subrules, counting their frequency
as subrules, not as complete rules. For example,
TOP rules with more than one dependent are prob-
lematic, e.g., TOP ? root ROOT:AV ROOT:NN.
Correspondingly, there are no rules with three ele-
ments containing the subrule root ROOT:AV.
We formalize this by setting the score s(ei, c)
equal to the summation of the frequencies of all
comparable subrules containing ei from the train-
ing data, as in (5), where B is the set of subrules
of r with length one less.
(5) s(ei, c) =
?
sub?B:ei?subC(sub, c)
For example, with c = +F, the frequency of +F
? ++:++ SV as a subrule is added to the scores
for ++:++ and SV. In this case, +F ? ++:++
SV VG:BV, +F ? ++:++ SV VG:AV, and +F
? ++:++ SV VG:VV all add support for +F ?
++:++ SV being a legitimate subrule. Thus, ++:++
and SV are less likely to be the sources of any
problems. Since +F ? SV AA:VV and +F ?
++:++ AA:VV have very little support in the train-
ing data, AA:VV receives a low score.
Note that the subrule count C(sub, c) is differ-
ent than counting the number of rules containing
a subrule, as can be seen with identical elements.
For example, for SS? VN ET:PR ET:PR, C(VN
ET:PR, SS) = 2, in keeping with the fact that there
are 2 pieces of evidence for its legitimacy.
3.4 Bigram anomalies
3.4.1 Motivation
The bigram method examines relationships be-
tween adjacent sisters, complementing the whole
rule method by focusing on local properties. For
(6), for example, we find the gold and parsed trees
in figure 4. For the long parsed rule TA ? PR
HD:ID HD:ID IR:IR AN:RO JR:IR, all elements
get low whole rule scores, i.e., are flagged as po-
tentially erroneous. But only the final elements
have anomalous bigrams: HD:ID IR:IR, IR:IR
AN:RO, and AN:RO JR:IR all never occur.
(6) Na?r
when
det
it
ga?ller
concerns
inkomsta?ret
the income year
1971
1971
(
(
taxeringsa?ret
assessment year
1972
1972
)
)
skall
shall
barnet
the child
. . .
. . .
?Concerning the income year of 1971 (assessment year
1972), the child . . . ?
3.4.2 Implementation
To obtain a bigram score for an element, we sim-
ply add together the bigrams which contain the el-
ement in question, as in (7).
(7) s(ei, c) = C(ei?1ei, c) + C(eiei+1, c)
Consider the rule from figure 4. With c =
TA, the bigram HD:ID IR:IR never occurs, so
both HD:ID and IR:IR get 0 added to their score.
HD:ID HD:ID, however, is a frequent bigram, so
it adds weight to HD:ID, i.e., positive evidence
comes from the bigram on the left. If we look at
IR:IR, on the other hand, IR:IR AN:RO occurs 0
times, and so IR:IR gets a total score of 0.
Both scoring methods treat each element inde-
pendently. Every single element could be given a
low score, even though once one is corrected, an-
other would have a higher score. Future work can
examine factoring in all elements at once.
4 Additional information
The methods presented so far have limited defini-
tions of comparability. As using complementary
information has been useful in, e.g., POS error de-
tection (Loftsson, 2009), we explore other simple
comparable properties of a dependency grammar.
Namely, we include: a) frequency information of
an overall dependency rule and b) information on
how likely each dependent is to be in a relation
with its head, described next.
4.1 Including POS information
Consider PA ? SS:NN XX:XX HV OO:VN, as
illustrated in figure 5 for the sentence in (8). This
rule is entirely correct, yet the XX:XX position has
low whole rule and bigram scores.
(8) Uppgift
information
om
of
vilka
which
orter
neighborhood
som
who
har
has
utko?rning
delivery
finner
find
Ni
you
ocksa?
also
i
in
. . .
. . .
?You can also find information about which neighbor-
hoods have delivery services in . . . ?
732
AA HD HD DT PA IR DT AN JR ...
Na?r det ga?ller inkomsta?ret 1971 ( taxeringsa?ret 1972 ) ...
PR ID ID NN RO IR NN RO IR ...
TA HD HD PA ET IR DT AN JR ...
Na?r det ga?ller inkomsta?ret 1971 ( taxeringsa?ret 1972 ) ...
PR ID ID NN RO IR NN RO IR ...
Figure 4: A rule with extra dependents (top=gold, bottom=parsed)
ET DT SS XX PA OO
Uppgift om vilka orter som har utko?rning
NN PR PO NN XX HV VN
Figure 5: Overflagging (gold=parsed)
One method which does not have this problem
of overflagging uses a ?lexicon? of POS tag pairs,
examining relations between POS, irrespective of
position. We extract POS pairs, note their depen-
dency relation, and add a L/R to the label to in-
dicate which is the head (Boyd et al, 2008). Ad-
ditionally, we note how often two POS categories
occur as a non-depenency, using the label NIL, to
help determine whether there should be any at-
tachment. We generate NILs by enumerating all
POS pairs in a sentence. For example, from fig-
ure 5, the parsed POS pairs include NN PR 7? ET-
L, NN PO 7? NIL, etc.
We convert the frequencies to probabilities. For
example, of 4 total occurrences of XX HV in the
training data, 2 are XX-R (cf. figure 5). A proba-
bility of 0.5 is quite high, given that NILs are often
the most frequent label for POS pairs.
5 Evaluation
In evaluating the methods, our main question is:
how accurate are the dependencies, in terms of
both attachment and labeling? We therefore cur-
rently examine the scores for elements functioning
as dependents in a rule. In figure 5, for example,
for har (?has?), we look at its score within ET ?
PR PA:HV and not when it functions as a head, as
in PA? SS:NN XX:XX HV OO:VN.
Relatedly, for each method, we are interested
in whether elements with scores below a thresh-
old have worse attachment accuracy than scores
above, as we predict they do. We can measure
this by scoring each testing data position below
the threshold as a 1 if it has the correct head and
dependency relation and a 0 otherwise. These are
simply labeled attachment scores (LAS). Scoring
separately for positions above and below a thresh-
old views the task as one of sorting parser output
into two bins, those more or less likely to be cor-
rectly parsed. For development, we also report un-
labeled attachement scores (UAS).
Since the goal is to speed up the post-editing of
corpus data by flagging erroneous rules, we also
report the precision and recall for error detection.
We count either attachment or labeling errors as
an error, and precision and recall are measured
with respect to how many errors are found below
the threshold. For development, we use two F-
scores to provide a measure of the settings to ex-
amine across language, corpus, and parser condi-
tions: the balanced F1 measure and the F0.5 mea-
sure, weighing precision twice as much. Precision
is likely more important in this context, so as to
prevent annotators from sorting through too many
false positives. In practice, one way to use these
methods is to start with the lowest thresholds and
work upwards until there are too many non-errors.
To establish a basis for comparison, we compare
733
method performance to a parser on its own.4 By
examining the parser output without any automatic
assistance, how often does a correction need to be
made?
5.1 The data
All our data comes from the CoNLL-X Shared
Task (Buchholz and Marsi, 2006), specifically the
4 data sets freely available online. We use the
Swedish Talbanken data (Nilsson and Hall, 2005)
and the transition-based dependency parser Malt-
Parser (Nivre et al, 2007), with the default set-
tings, for developing the method. To test across
languages and corpora, we use MaltParser on the
other 3 corpora: the Danish DDT (Kromann,
2003), Dutch Alpino (van der Beek et al, 2002),
and Portuguese Bosque data (Afonso et al, 2002).
Then, we present results using the graph-based
parser MSTParser (McDonald and Pereira, 2006),
again with default settings, to test the methods
across parsers. We use the gold standard POS tags
for all experiments.
5.2 Development data
In the first line of table 1, we report the baseline
MaltParser accuracies on the Swedish test data,
including baseline error detection precision (=1-
LASb), recall, and (the best) F-scores. In the rest
of table 1, we report the best-performing results
for each of the methods,5 providing the number
of rules below and above a particular threshold,
along with corresponding UAS and LAS values.
To get the raw number of identified rules, multiply
the number of corpus position below a threshold
(b) times the error detection precision (P ). For ex-
ample, the bigram method with a threshold of 39
leads to finding 283 errors (455 ? .622).
Dependency elements with frequency below the
lowest threshold have lower attachment scores
(66.6% vs. 90.1% LAS), showing that simply us-
ing a complete rule helps sort dependencies. How-
ever, frequency thresholds have fairly low preci-
sion, i.e., 33.4% at their best. The whole rule and
bigram methods reveal greater precision in iden-
tifying problematic dependencies, isolating ele-
ments with lower UAS and LAS scores than with
frequency, along with corresponding greater pre-
4One may also use parser confidence or parser revision
methods as a basis of comparison, but we are aware of no sys-
tematic evaluation of these approaches for detecting errors.
5Freq=rule frequency, WR=whole rule, Bi=bigram,
POS=POS-based (POS scores multiplied by 10,000)
cision and F-scores. The bigram method is more
fine-grained, identifying small numbers of rule el-
ements at each threshold, resulting in high error
detection precision. With a threshold of 39, for ex-
ample, we find over a quarter of the parser errors
with 62% precision, from this one piece of infor-
mation. For POS information, we flag 23.6% of
the cases with over 60% precision (at 81.6).
Taking all these results together, we can begin
to sort more reliable from less reliable dependency
tree elements, using very simple information. Ad-
ditionally, these methods naturally group cases
together by linguistic properties (e.g., adverbial-
verb dependencies within a particualr context), al-
lowing a human to uncover the principle behind
parse failure and ajudicate similar cases at the
same time (cf. Wallis, 2003).
5.3 Discussion
Examining some of the output from the Tal-
banken test data by hand, we find that a promi-
nent cause of false positives, i.e., correctly-parsed
cases with low scores, stems from low-frequency
dependency-POS label pairs. If the dependency
rarely occurs in the training data with the partic-
ular POS, then it receives a low score, regardless
of its context. For example, the parsed rule TA
? IG:IG RO has a correct dependency relation
(IG) between the POS tags IG and its head RO, yet
is assigned a whole rule score of 2 and a bigram
score of 20. It turns out that IG:IG only occurs
144 times in the training data, and in 11 of those
cases (7.6%) it appears immediately before RO.
One might consider normalizing the scores based
on overall frequency or adjusting the scores to ac-
count for other dependency rules in the sentence:
in this case, there may be no better attachment.
Other false positives are correctly-parsed ele-
ments that are a part of erroneous rules. For in-
stance, in AA? UK:UK SS:PO TA:AJ AV SP:AJ
OA:PR +F:HV +F:HV, the first +F:HV is correct,
yet given a low score (0 whole rule, 1 bigram).
The following and erroneous +F:HV is similarly
given a low score. As above, such cases might
be handled by looking for attachments in other
rules (cf. Attardi and Ciaramita, 2007), but these
cases should be relatively unproblematic for hand-
correction, given the neighboring error.
We also examined false negatives, i.e., errors
with high scores. There are many examples of PR
PA:NN rules, for instance, with the NN improp-
734
Score Thr. b a UASb LASb UASa LASa P R F1 F0.5
None n/a 5656 0 87.4% 82.0% 0% 0% 18.0% 100% 30.5% 21.5%
Freq 0 1951 3705 76.6% 66.6% 93.1% 90.1% 33.4% 64.1% 43.9% 36.9%
WR 0 894 4762 64.7% 54.0% 91.7% 87.3% 46.0% 40.5% 43.0% 44.8%
6 1478 4178 71.1% 60.9% 93.2% 89.5% 39.1% 56.9% 46.4% 41.7%
Bi 0 56 5600 10.7% 7.1% 88.2% 82.8% 92.9% 5.1% 9.7% 21.0%
39 455 5201 51.6% 37.8% 90.6% 85.9% 62.2% 27.9% 38.5% 49.9%
431 1685 3971 74.1% 63.7% 93.1% 89.8% 36.3% 60.1% 45.2% 39.4%
POS 0 54 5602 27.8% 22.2% 87.4% 82.6% 77.8% 4.1% 7.9% 17.0%
81.6 388 5268 48.5% 38.4% 90.3% 85.3% 61.6% 23.5% 34.0% 46.5%
763 1863 3793 75.4% 65.8% 93.3% 90.0% 34.2% 62.8% 44.3% 37.7%
Table 1: MaltParser results for Talbanken, for select values (b = below, a = above threshold (Thr.))
erly attached, but there are also many correct in-
stances of PR PA:NN. To sort out the errors, one
needs to look at lexical knowledge and/or other de-
pendencies in the tree. With so little context, fre-
quent rules with only one dependent are not prime
candidates for our methods of error detection.
5.4 Other corpora
We now turn to the parsed data from three other
corpora. The Alpino and Bosque corpora are ap-
proximately the same size as Talbanken, so we use
the same thresholds for them. The DDT data is
approximately half the size; to adjust, we simply
halve the scores. In tables 2, 3, and 4, we present
the results, using the best F0.5 and F1 settings from
development. At a glance, we observe that the best
method differs for each corpus and depending on
an emphasis of precision or recall, with the bigram
method generally having high precision.
Score Thr. b LASb LASa P R
None n/a 5585 73.8% 0% 26.2% 100%
Freq 0 1174 43.2% 81.9% 56.8% 45.6%
WR 0 483 32.5% 77.7% 67.5% 22.3%
6 787 39.4% 79.4% 60.6% 32.6%
Bi 39 253 33.6% 75.7% 66.4% 11.5%
431 845 45.6% 78.8% 54.4% 31.4%
POS 81.6 317 51.7% 75.1% 48.3% 10.5%
763 1767 53.5% 83.2% 46.5% 56.1%
Table 2: MaltParser results for Alpino
For Alpino, error detection is better with fre-
quency than, for example, bigram scores. This is
likely due to the fact that Alpino has the small-
est label set of any of the corpora, with only 24
dependency labels and 12 POS tags (cf. 64 and
41 in Talbanken, respectively). With a smaller la-
bel set, there are less possible bigrams that could
be anomalous, but more reliable statistics about a
Score Thr. b LASb LASa P R
None n/a 5867 82.2% 0% 17.8% 100%
Freq 0 1561 61.2% 89.9% 38.8% 58.1%
WR 0 693 48.1% 86.8% 51.9% 34.5%
6 1074 54.4% 88.5% 45.6% 47.0%
Bi 39 227 15.4% 84.9% 84.6% 18.4%
431 776 51.0% 87.0% 49.0% 36.5%
POS 81.6 369 33.3% 85.5% 66.7% 23.6%
763 1681 60.1% 91.1% 39.9% 64.3%
Table 3: MaltParser results for Bosque
Score Thr. b LASb LASa P R
None n/a 5852 81.0% 0% 19.0% 100%
Freq 0 1835 65.9% 88.0% 34.1% 56.4%
WR 0 739 53.9% 85.0% 46.1% 30.7%
3 1109 60.1% 85.9% 39.9% 39.9%
Bi 19.5 185 25.4% 82.9% 74.6% 12.4%
215.5 884 56.8% 85.4% 43.2% 34.4%
POS 40.8 179 30.2% 82.7% 69.8% 11.3%
381.5 1214 62.5% 85.9% 37.5% 41.0%
Table 4: MaltParser results for DDT
whole rule. Likewise, with fewer possible POS
tag pairs, Alpino has lower precision for the low-
threshold POS scores than the other corpora.
For the whole rule scores, the DDT data is
worse (compare its 46.1% precision with Bosque?s
45.6%, with vastly different recall values), which
could be due to the smaller training data. One
might also consider the qualitative differences in
the dependency inventory of DDT compared to the
others?e.g., appositions, distinctions in names,
and more types of modifiers.
5.5 MSTParser
Turning to the results of running the methods
on the output of MSTParser, we find similar but
slightly worse values for the whole rule and bi-
gram methods, as shown in tables 5-8. What is
735
most striking are the differences in the POS-based
method for Bosque and DDT (tables 7 and 8),
where a large percentage of the test corpus is un-
derneath the threshold. MSTParser is apparently
positing fewer distinct head-dependent pairs, as
most of them fall under the given thresholds. With
the exception of the POS-based method for DDT
(where LASb is actually higher than LASa) the
different methods seem to be accurate enough to
be used as part of corpus post-editing.
Score Thr. b LASb LASa P R
None n/a 5656 81.1% 0% 18.9% 100%
Freq 0 3659 65.2% 89.7% 34.8% 64.9%
WR 0 4740 55.7% 86.0% 44.3% 37.9%
6 4217 59.9% 88.3% 40.1% 53.9%
Bi 39 5183 38.9% 84.9% 61.1% 27.0%
431 3997 63.2% 88.5% 36.8% 57.1%
POS 81.6 327 42.8% 83.4% 57.2% 17.5%
763 1764 68.0% 87.0% 32.0% 52.7%
Table 5: MSTParser results for Talbanken
Score Thr. b LASb LASa P R
None n/a 5585 75.4% 0% 24.6% 100%
Freq 0 1371 49.5% 83.9% 50.5% 50.5%
WR 0 453 40.0% 78.5% 60.0% 19.8%
6 685 45.4% 79.6% 54.6% 27.2%
Bi 39 226 39.8% 76.9% 60.2% 9.9%
431 745 48.2% 79.6% 51.8% 28.1%
POS 81.6 570 60.4% 77.1% 39.6% 16.5%
763 1860 61.9% 82.1% 38.1% 51.6%
Table 6: MSTParser results for Alpino
Score Thr. b LASb LASa P R
None n/a 5867 82.5% 0% 17.5% 100%
Freq 0 1562 63.9% 89.3% 36.1% 55.0%
WR 0 540 50.6% 85.8% 49.4% 26.0%
6 985 58.0% 87.5% 42.0% 40.4%
Bi 39 117 34.2% 83.5% 65.8% 7.5%
431 736 56.4% 86.3% 43.6% 31.3%
POS 81.6 2978 75.8% 89.4% 24.2% 70.3%
763 3618 74.3% 95.8% 25.7% 90.7%
Table 7: MSTParser results for Bosque
Score Thr. b LASb LASa P R
None n/a 5852 82.9% 0% 17.1% 100%
Freq 0 1864 70.3% 88.8% 29.7% 55.3%
WR 0 624 60.6% 85.6% 39.4% 24.6%
3 1019 65.4% 86.6% 34.6% 35.3%
Bi 19.5 168 28.6% 84.5% 71.4% 12.0%
215.5 839 61.6% 86.5% 38.4% 32.2%
POS 40.8 5714 83.0% 79.0% 17.0% 97.1%
381.5 5757 82.9% 80.0% 17.1% 98.1%
Table 8: MSTParser results for DDT
6 Summary and Outlook
We have proposed different methods for flag-
ging the errors in automatically-parsed corpora, by
treating the problem as one of looking for anoma-
lous rules with respect to a treebank grammar.
The different methods incorporate differing types
and amounts of information, notably comparisons
among dependency rules and bigrams within such
rules. Using these methods, we demonstrated suc-
cess in sorting well-formed output from erroneous
output across language, corpora, and parsers.
Given that the rule representations and compar-
ison methods use both POS and dependency in-
formation, a next step in evaluating and improv-
ing the methods is to examine automatically POS-
tagged data. Our methods should be able to find
POS errors in addition to dependency errors. Fur-
thermore, although we have indicated that differ-
ences in accuracy can be linked to differences in
the granularity and particular distinctions of the
annotation scheme, it is still an open question as
to which methods work best for which schemes
and for which constructions (e.g., coordination).
Acknowledgments
Thanks to Sandra Ku?bler and Amber Smith for
comments on an earlier draft; Yvonne Samuels-
son for help with the Swedish translations; the IU
Computational Linguistics discussion group for
feedback; and Julia Hockenmaier, Chris Brew, and
Rebecca Hwa for discussion on the general topic.
A Some Talbanken05 categories
POS tags
++ coord. conj.
AB adverb
AJ adjective
AV vara (be)
EN indef. article
HV ha(va) (have)
ID part of idiom
IG punctuation
IR parenthesis
NN noun
PO pronoun
PR preposition
RO numeral
QV kunna (can)
SV skola (will)
UK sub. conj.
VN verbal noun
VV verb
XX unclassifiable
Dependencies
++ coord. conj.
+F main clause coord.
AA adverbial
AN apposition
AT nomainl pre-modifier
DT determiner
ET nominal post-modifier
HD head
IG punctuation
IR parenthesis
JR second parenthesis
KA comparative adverbial
MA attitude adverbial
NA negation adverbial
OO object
PA preposition comp.
PL verb particle
SS subject
TA time adverbial
UK sub. conj.
VG verb group
XX unclassifiable
736
References
Afonso, Susana, Eckhard Bick, Renato Haber and
Diana Santos (2002). Floresta Sinta?(c)tica: a
treebank for Portuguese. In Proceedings of
LREC 2002. Las Palmas, pp. 1698?1703.
Attardi, Giuseppe and Massimiliano Ciaramita
(2007). Tree Revision Learning for Dependency
Parsing. In Proceedings of NAACL-HLT-07.
Rochester, NY, pp. 388?395.
Bick, Eckhard (2007). Hybrid Ways to Improve
Domain Independence in an ML Dependency
Parser. In Proceedings of the CoNLL Shared
Task Session of EMNLP-CoNLL 2007. Prague,
Czech Republic, pp. 1119?1123.
Boyd, Adriane, Markus Dickinson and Detmar
Meurers (2008). On Detecting Errors in Depen-
dency Treebanks. Research on Language and
Computation 6(2), 113?137.
Buchholz, Sabine and Erwin Marsi (2006).
CoNLL-X Shared Task on Multilingual Depen-
dency Parsing. In Proceedings of CoNLL-X.
New York City, pp. 149?164.
Campbell, David and Stephen Johnson (2002). A
transformational-based learner for dependency
grammars in discharge summaries. In Proceed-
ings of the ACL-02 Workshop on Natural Lan-
guage Processing in the Biomedical Domain.
Phildadelphia, pp. 37?44.
Dickinson, Markus (2008). Ad Hoc Treebank
Structures. In Proceedings of ACL-08. Colum-
bus, OH.
Dickinson, Markus and Jennifer Foster (2009).
Similarity Rules! Exploring Methods for Ad-
Hoc Rule Detection. In Proceedings of TLT-7.
Groningen, The Netherlands.
Gildea, Daniel (2001). Corpus Variation and
Parser Performance. In Proceedings of
EMNLP-01. Pittsburgh, PA.
Hall, Keith and Va?clav Nova?k (2005). Corrective
Modeling for Non-Projective Dependency Pars-
ing. In Proceedings of IWPT-05. Vancouver, pp.
42?52.
Kromann, Matthias Trautner (2003). The Danish
Dependency Treebank and the underlying lin-
guistic theory. In Proceedings of TLT-03.
Kuhlmann, Marco and Giorgio Satta (2009). Tree-
bank Grammar Techniques for Non-Projective
Dependency Parsing. In Proceedings of EACL-
09. Athens, Greece, pp. 478?486.
Loftsson, Hrafn (2009). Correcting a POS-Tagged
Corpus Using Three Complementary Methods.
In Proceedings of EACL-09. Athens, Greece,
pp. 523?531.
McDonald, Ryan and Fernando Pereira (2006).
Online learning of approximate dependency
parsing algorithms. In Proceedings of EACL-
06. Trento.
Nilsson, Jens and Johan Hall (2005). Recon-
struction of the Swedish Treebank Talbanken.
MSI report 05067, Va?xjo? University: School of
Mathematics and Systems Engineering.
Nivre, Joakim, Johan Hall, Jens Nilsson, Atanas
Chanev, Gulsen Eryigit, Sandra Ku?bler, Sve-
toslav Marinov and Erwin Marsi (2007). Malt-
Parser: A language-independent system for
data-driven dependency parsing. Natural Lan-
guage Engineering 13(2), 95?135.
Owczarzak, Karolina (2009). DEPEVAL(summ):
Dependency-based Evaluation for Automatic
Summaries. In Proceedings of ACL-AFNLP-09.
Suntec, Singapore, pp. 190?198.
Przepio?rkowski, Adam (2006). What to ac-
quire from corpora in automatic valence ac-
quisition. In Violetta Koseska-Toszewa and
Roman Roszko (eds.), Semantyka a kon-
frontacja jezykowa, tom 3, Warsaw: Slawisty-
czny Os?rodek Wydawniczy PAN, pp. 25?41.
Sekine, Satoshi (1997). The Domain Dependence
of Parsing. In Proceedings of ANLP-96. Wash-
ington, DC.
van der Beek, Leonoor, Gosse Bouma, Robert
Malouf and Gertjan van Noord (2002). The
Alpino Dependency Treebank. In Proceedings
of CLIN 2001. Rodopi.
van Noord, Gertjan and Gosse Bouma (2009).
Parsed Corpora for Linguistics. In Proceed-
ings of the EACL 2009 Workshop on the In-
teraction between Linguistics and Computa-
tional Linguistics: Virtuous, Vicious or Vacu-
ous?. Athens, pp. 33?39.
Wallis, Sean (2003). Completing Parsed Corpora.
In Anne Abeille? (ed.), Treebanks: Building and
using syntactically annoted corpora, Dordrecht:
Kluwer Academic Publishers, pp. 61?71.
Wan, Stephen, Mark Dras, Robert Dale and Ce?cile
Paris (2009). Improving Grammaticality in Sta-
737
tistical Sentence Generation: Introducing a De-
pendency Spanning Tree Algorithm with an Ar-
gument Satisfaction Model. In Proceedings of
EACL-09. Athens, Greece, pp. 852?860.
Xu, Peng, Jaeho Kang, Michael Ringgaard and
Franz Och (2009). Using a Dependency Parser
to Improve SMT for Subject-Object-Verb Lan-
guages. In Proceedings of NAACL-HLT-09.
Boulder, Colorado, pp. 245?253.
738
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 356?360,
Dublin, Ireland, August 23-24, 2014.
IUCL: Combining Information Sources for SemEval Task 5
Alex Rudnick, Levi King, Can Liu, Markus Dickinson, Sandra K?ubler
Indiana University
Bloomington, IN, USA
{alexr,leviking,liucan,md7,skuebler}@indiana.edu
Abstract
We describe the Indiana University sys-
tem for SemEval Task 5, the L2 writ-
ing assistant task, as well as some exten-
sions to the system that were completed
after the main evaluation. Our team sub-
mitted translations for all four language
pairs in the evaluation, yielding the top
scores for English-German. The system
is based on combining several information
sources to arrive at a final L2 translation
for a given L1 text fragment, incorporating
phrase tables extracted from bitexts, an L2
language model, a multilingual dictionary,
and dependency-based collocational mod-
els derived from large samples of target-
language text.
1 Introduction
In the L2 writing assistant task, we must translate
an L1 fragment in the midst of an existing, nearly
complete, L2 sentence. With the presence of this
rich target-language context, the task is rather dif-
ferent from a standard machine translation setting,
and our goal with our design was to make effec-
tive use of the L2 context, exploiting collocational
relationships between tokens anywhere in the L2
context and the proposed fragment translations.
Our system proceeds in several stages: (1) look-
ing up or constructing candidate translations for
the L1 fragment, (2) scoring candidate transla-
tions via a language model of the L2, (3) scoring
candidate translations with a dependency-driven
word similarity measure (Lin, 1998) (which we
call SIM), and (4) combining the previous scores
in a log-linear model to arrive at a final n-best
list. Step 1 models transfer knowledge between
This work is licenced under a Creative Commons Attribution
4.0 International License. Page numbers and proceedings
footer are added by the organizers. License details: http:
//creativecommons.org/licenses/by/4.0/
the L1 and L2; step 2 models facts about the L2
syntax, i.e., which translations fit well into the lo-
cal context; step 3 models collocational and se-
mantic tendencies of the L2; and step 4 gives dif-
ferent weights to each of the three sources of in-
formation. Although we did not finish step 3 in
time for the official results, we discuss it here, as
it represents the most novel aspect of the system ?
namely, steps towards the exploitation of the rich
L2 context. In general, our approach is language-
independent, with accuracy varying due to the size
of data sources and quality of input technology
(e.g., syntactic parse accuracy). More features
could easily be added to the log-linear model, and
further explorations of ways to make use of target-
language knowledge could be promising.
2 Data Sources
The data sources serve two major purposes for our
system: For L2 candidate generation, we use Eu-
roparl and BabelNet; and for candidate ranking
based on L2 context, we use Wikipedia and the
Google Books Syntactic N-grams.
Europarl The Europarl Parallel Corpus (Eu-
roparl, v7) (Koehn, 2005) is a corpus of pro-
ceedings of the European Parliament, contain-
ing 21 European languages with sentence align-
ments. From this corpus, we build phrase tables
for English-Spanish, English-German, French-
English, Dutch-English.
BabelNet In the cases where the constructed
phrase tables do not contain a translation for a
source phrase, we need to back off to smaller
phrases and find candidate translations for these
components. To better handle sparsity, we extend
look-up using the multilingual dictionary Babel-
Net, v2.0 (Navigli and Ponzetto, 2012) as a way to
find translation candidates.
356
Wikipedia For German and Spanish, we use re-
cent Wikipedia dumps, which were converted to
plain text with the Wikipedia Extractor tool.
1
To
save time during parsing, sentences longer than 25
words are removed. The remaining sentences are
POS-tagged and dependency parsed using Mate
Parser with its pre-trained models (Bohnet, 2010;
Bohnet and Kuhn, 2012; Seeker and Kuhn, 2013).
To keep our English Wikipedia dataset to a man-
ageable size, we choose an older (2006), smaller
dump. Long sentences are removed, and the re-
maining sentences are POS-tagged and depen-
dency parsed using the pre-trained Stanford Parser
(Klein and Manning, 2003; de Marneffe et al.,
2006). The resulting sizes of the datasets are
(roughly): German: 389M words, 28M sentences;
Spanish: 147M words, 12M sentences; English:
253M words, 15M sentences. Dependencies ex-
tracted from these parsed datasets serve as training
for the SIM system described in section 3.3.
Google Books Syntactic N-grams For English,
we also obtained dependency relationships for our
word similarity statistics using the arcs dataset of
the Google Books Syntactic N-Grams (Goldberg
and Orwant, 2013), which has 919M items, each
of which is a small ?syntactic n-gram?, a term
Goldberg and Orwant use to describe short de-
pendency chains, each of which may contain sev-
eral tokens. This data set does not contain the ac-
tual parses of books from the Google Books cor-
pus, but counts of these dependency chains. We
converted the longer chains into their component
(head, dependent, label) triples and then collated
these triples into counts, also for use in the SIM
system.
3 System Design
As previously mentioned, at run-time, our system
decomposes the fragment translation task into two
parts: generating many possible candidate transla-
tions, then scoring and ranking them in the target-
language context.
3.1 Constructing Candidate Translations
As a starting point, we use phrase tables con-
structed in typical SMT fashion, built with the
training scripts packaged with Moses (Koehn et
al., 2007). These scripts preprocess the bitext, es-
timate word alignments with GIZA++ (Och and
1
http://medialab.di.unipi.it/wiki/
Wikipedia_Extractor
Ney, 2000) and then extract phrases with the
grow-diag-final-and heuristic.
At translation time, we look for the given
source-language phrase in the phrase table, and if
it is found, we take all translations of that phrase
as our candidates.
When translating a phrase that is not found in
the phrase table, we try to construct a ?synthetic
phrase? out of the available components. This
is done by listing, combinatorially, all ways to
decompose the L1 phrase into sub-phrases of at
least one token long. Then for each decomposi-
tion of the input phrase, such that all of its compo-
nents can be found in the phrase table, we gen-
erate a translation by concatenating their target-
language sides. This approach naively assumes
that generating valid L2 text requires no reorder-
ing of the components. Also, since there are 2
n?1
possible ways to split an n-token phrase into sub-
sequences (i.e., each token is either the first token
in a new sub-sequence, or it is not), we perform
some heuristic pruning at this step, taking only
the first 100 decompositions, preferring those built
from longer phrase-table entries. Every phrase in
the phrase table, including these synthetic phrases,
has both a ?direct? and ?inverse? probability score;
for synthetic phrases, we estimate these scores by
taking the product of the corresponding probabili-
ties for the individual components.
In the case that an individual word cannot be
found in the phrase table, the system attempts to
look up the word in BabelNet, estimating the prob-
abilities as uniformly distributed over the available
BabelNet entries. Thus, synthetic phrase table
entries can be constructed by combining phrases
found in the training data and words available in
BabelNet.
For the evaluation, in cases where an L1 phrase
contained words that were neither in our train-
ing data nor BabelNet (and thus were simply out-
of-vocabulary for our system), we took the first
translation for that phrase, without regard to con-
text, from Google Translate, through the semi-
automated Google Docs interface. This approach
is not particularly scalable or reproducible, but
simulates what a user might do in such a situation.
3.2 Scoring Candidate Translations via a L2
Language Model
To model how well a phrase fits into the L2 con-
text, we score candidates with an n-gram lan-
357
guage model (LM) trained on a large sample of
target-language text. Constructing and querying
a large language model is potentially computa-
tionally expensive, so here we use the KenLM
Language Model Toolkit and its Python interface
(Heafield, 2011). Here our models were trained
on the Wikipedia text mentioned previously (with-
out filtering long sentences), with KenLM set to
5-grams and the default settings.
3.3 Scoring Candidate Translations via
Dependency-Based Word Similarity
The candidate ranking based on the n-gram lan-
guage model ? while quite useful ? is based on
very shallow information. We can also rank the
candidate phrases based on how well each of the
components fits into the L2 context using syntactic
information. In this case, the fitness is measured in
terms of dependency-based word similarity com-
puted from dependency triples consisting of the
the head, the dependent, and the dependency la-
bel. We slightly adapted the word similarity mea-
sure by Lin (1998):
SIM(w
1
, w
2
) =
2 ? c(h, d, l)
c(h,?, l) + c(?, d, l)
(1)
where h = w
1
and d = w
2
and c(h, d, l)
is the frequency with which a particular
(head, dependent, label) dependency triple
occurs in the L2 corpus. c(h,?, l) is the fre-
quency with which a word occurs as a head
in a dependency labeled l with any dependent.
c(?, d, l) is the frequency with which a word
occurs as a dependent in a dependency labeled
l with any head. In the measure by Lin (1998),
the numerator is defined as the information of all
dependency features that w
1
and w
2
share, com-
puted as the negative sum of the log probability of
each dependency feature. Similarly, the denom-
inator is computed as the sum of information of
dependency features for w
1
and w
2
.
To compute the fitness of a word w
i
for its
context, we consider a set D of all words that are
directly dependency-related to w
i
. The fitness of
w
i
is thus computed as:
FIT (w
i
) =
?
D
w
j
SIM(w
i
, w
j
)
|D|
(2)
The fitness of a phrase is the average word sim-
ilarity over all its components. For example, the
fitness of the phrase ?eat with chopsticks? would
be computed as:
FIT (eat with chopsticks) =
FIT (eat) + FIT (with) + FIT (chopsticks)
3
(3)
Since we consider the heads and dependents
of a target phrase component, these may be situ-
ated inside or outside the phrase. Both cases are
included in our calculation, thus enabling us to
consider a broader, syntactically determined local
context of the phrase. By basing the calculation on
a single word?s head and dependents, we attempt
to avoid data sparseness issues that we might get
from rare n-gram contexts.
Back-Off Lexical-based dependency triples suf-
fer from data sparsity, so in addition to computing
the lexical fitness of a phrase, we also calculate the
POS fitness. For example, the POS fitness of ?eat
with chopsticks? would be computed as follows:
FIT (eat/VBG with/IN chopsticks/NNS) =
FIT (VBG) + FIT (IN) + FIT (NNS)
3
(4)
Storing and Caching The large vocabulary
and huge number of combinations of our
(head, dependent, label) triples poses an effi-
ciency problem when querying the dependency-
based word similarity values. Thus, we stored
the dependency triples in a database with a
Python programming interface (SQLite3) and
built database indices on the frequent query types.
However, for frequently searched dependency
triples, re-querying the database is still inefficient.
Thus, we built a query cache to store the recently-
queried triples. Using the database and cache sig-
nificantly speeds up our system.
This database only stores dependency triples
and their corresponding counts; the dependency-
based similarity value is calculated as needed, for
each particular context. Then, these FIT scores
are combined with the scores from the phrase ta-
ble and language model, using weights tuned by
MERT.
358
system acc wordacc oofacc oofwordacc
run2 0.665 0.722 0.806 0.857
SIM 0.647 0.706 0.800 0.852
nb 0.657 0.717 0.834 0.868
Figure 1: Scores on the test set for English-
German; here next-best is CNRC-run1.
system acc wordacc oofacc oofwordacc
run2 0.633 0.72 0.781 0.847
SIM 0.359 0.482 0.462 0.607
best 0.755 0.827 0.920 0.944
Figure 2: Scores on the test set for English-
Spanish; here best is UEdin-run2.
3.4 Tuning Weights with MERT
In order to rank the various candidate translations,
we must combine the different sources of infor-
mation in some way. Here we use a familiar log-
linear model, taking the log of each score ? the di-
rect and inverse translation probabilities, the LM
probability, and the surface and POS SIM scores ?
and producing a weighted sum. Since the original
scores are either probabilities or probability-like
(in the range [0, 1]), their logs are negative num-
bers, and at translation time we return the trans-
lation (or n-best) with the highest (least negative)
score.
This leaves us with the question of how to
set the weights for the log-linear model; in this
work, we use the ZMERT package (Zaidan, 2009),
which implements the MERT optimization algo-
rithm (Och, 2003), iteratively tuning the feature
weights by repeatedly requesting n-best lists from
the system. We used ZMERT with its default
settings, optimizing our system?s BLEU scores
on the provided development set. We chose, for
convenience, BLEU as a stand-in for the word-
level accuracy score, as BLEU scores are maxi-
mized when the system output matches the refer-
ence translations.
4 Experiments
In figures 1-4, we show the scores on this year?s
test set for running the two variations of our sys-
tem: run2, the version without the SIM exten-
sions, which we submitted for the evaluation, and
SIM, with the extensions enabled. For compar-
ison, we also include the best (or for English-
German, next-best) submitted system. We see here
system acc wordacc oofacc oofwordacc
run2 0.545 0.682 0.691 0.800
SIM 0.549 0.687 0.693 0.800
best 0.733 0.824 0.905 0.938
Figure 3: Scores on the test set for French-English;
here best is UEdin-run1.
system acc wordacc oofacc oofwordacc
run2 0.544 0.679 0.634 0.753
SIM 0.540 0.676 0.635 0.753
best 0.575 0.692 0.733 0.811
Figure 4: Scores on the test set for Dutch-English;
here best is UEdin-run1.
that the use of the SIM features did not improve
the performance of the base system, and in the
case of English-Spanish caused significant degra-
dation, which is as of yet unexplained, though we
suspect difficulties parsing the Spanish test set, as
for all of the other language pairs, the effects of
adding SIM features were small.
5 Conclusion
We have described our entry for the initial run-
ning of the ?L2 Writing Assistant? task and ex-
plained some possible extensions to our base log-
linear model system.
In developing the SIM extensions, we faced
some interesting software engineering challenges,
and we can now produce large databases of depen-
dency relationship counts for various languages.
Unfortunately, these extensions have not yet led
to improvements in performance on this particu-
lar task. The databases themselves seem at least
intuitively promising, capturing interesting infor-
mation about common usage patterns of the tar-
get language. Finding a good way to make use
of this information may involve computing some
measure that we have not yet considered, or per-
haps the insights captured by SIM are covered ef-
fectively by the language model.
We look forward to future developments around
this task and associated applications in helping
language learners communicate effectively.
359
References
Bernd Bohnet and Jonas Kuhn. 2012. The best of
both worlds ? A graph-based completion model for
transition-based parsers. In Proceedings of the 13th
Conference of the European Chapter of the Associ-
ation for Computational Linguistics (EACL), pages
77?87, Avignon, France.
Bernd Bohnet. 2010. Top accuracy and fast depen-
dency parsing is not a contradiction. In Proceedings
of the 23rd International Conference on Computa-
tional Linguistics (COLING), pages 89?97, Beijing,
China.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher Manning. 2006. Generating typed de-
pendency parses from phrase structure parses. In
Proceedings of LREC-06, Genoa, Italy.
Yoav Goldberg and Jon Orwant. 2013. A dataset of
syntactic-ngrams over time from a very large cor-
pus of English books. In Second Joint Conference
on Lexical and Computational Semantics (*SEM),
pages 241?247, Atlanta, GA.
Kenneth Heafield. 2011. KenLM: faster and smaller
language model queries. In Proceedings of the
EMNLP 2011 Sixth Workshop on Statistical Ma-
chine Translation, pages 187?197, Edinburgh, Scot-
land, United Kingdom, July.
Dan Klein and Christopher Manning. 2003. Accurate
unlexicalized parsing. In Proceedings of ACL-2003,
pages 423?430, Sapporo, Japan.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-
dra Constantin, and Evan Herbst. 2007. Moses:
Open source toolkit for statistical machine transla-
tion. In Proceedings of the 45th Annual Meeting of
the Association for Computational Linguistics Com-
panion Volume Proceedings of the Demo and Poster
Sessions, Prague, Czech Republic.
Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. In MT Summit.
Dekang Lin. 1998. An information-theoretic defini-
tion of similarity. In International Conference on
Machine Learning (ICML), volume 98, pages 296?
304.
Roberto Navigli and Simone Paolo Ponzetto. 2012.
BabelNet: The automatic construction, evaluation
and application of a wide-coverage multilingual se-
mantic network. Artificial Intelligence, 193:217?
250.
Franz Josef Och and Hermann Ney. 2000. Improved
statistical alignment models. In Proceedings of the
38th Annual Meeting of the Association for Com-
putational Linguistics (ACL), pages 440?447, Hong
Kong.
Franz Josef Och. 2003. Minimum error rate train-
ing in statistical machine translation. In Proceed-
ings of the 41st Annual Meeting of the Association
for Computational Linguistics, pages 160?167, Sap-
poro, Japan, July.
Wolfgang Seeker and Jonas Kuhn. 2013. Morphologi-
cal and syntactic case in statistical dependency pars-
ing. Computational Linguistics, 39(1):23?55.
Omar F. Zaidan. 2009. Z-MERT: A fully configurable
open source tool for minimum error rate training of
machine translation systems. The Prague Bulletin of
Mathematical Linguistics, 91:79?88.
360
Proceedings of the NAACL HLT 2010 Sixth Web as Corpus Workshop, pages 8?16,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Building a Korean Web Corpus for Analyzing Learner Language
Markus Dickinson
Indiana University
md7@indiana.edu
Ross Israel
Indiana University
raisrael@indiana.edu
Sun-Hee Lee
Wellesley College
slee6@wellesley.edu
Abstract
Post-positional particles are a significant
source of errors for learners of Korean. Fol-
lowing methodology that has proven effective
in handling English preposition errors, we are
beginning the process of building a machine
learner for particle error detection in L2 Ko-
rean writing. As a first step, however, we must
acquire data, and thus we present a method-
ology for constructing large-scale corpora of
Korean from the Web, exploring the feasibil-
ity of building corpora appropriate for a given
topic and grammatical construction.
1 Introduction
Applications for assisting second language learners
can be extremely useful when they make learners
more aware of the non-native characteristics in their
writing (Amaral and Meurers, 2006). Certain con-
structions, such as English prepositions, are difficult
to characterize by grammar rules and thus are well-
suited for machine learning approaches (Tetreault
and Chodorow, 2008; De Felice and Pulman, 2008).
Machine learning techniques are relatively portable
to new languages, but new languages bring issues in
terms of defining the language learning problem and
in terms of acquiring appropriate data for training a
machine learner.
We focus in this paper mainly on acquiring data
for training a machine learning system. In partic-
ular, we are interested in situations where the task
is constant?e.g., detecting grammatical errors in
particles?but the domain might fluctuate. This is
the case when a learner is asked to write an essay on
a prompt (e.g., ?What do you hope to do in life??),
and the prompts may vary by student, by semester,
by instructor, etc. By isolating a particular domain,
we can hope for greater degrees of accuracy; see,
for example, the high accuracies for domain-specific
grammar correction in Lee and Seneff (2006).
In this situation, we face the challenge of obtain-
ing data which is appropriate both for: a) the topic
the learners are writing about, and b) the linguistic
construction of interest, i.e., containing enough rel-
evant instances. In the ideal case, one could build
a corpus directly for the types of learner data to
analyze. Luckily, using the web as a data source
can provide such specialized corpora (Baroni and
Bernardini, 2004), in addition to larger, more gen-
eral corpora (Sharoff, 2006). A crucial question,
though, is how one goes about designing the right
web corpus for analyzing learner language (see, e.g.,
Sharoff, 2006, for other contexts)
The area of difficulty for language learners which
we focus on is that of Korean post-positional parti-
cles, akin to English prepositions (Lee et al, 2009;
Ko et al, 2004). Korean is an important language
to develop NLP techniques for (see, e.g., discussion
in Dickinson et al, 2008), presenting a variety of
features which are less prevalent in many Western
languages, such as agglutinative morphology, a rich
system of case marking, and relatively free word or-
der. Obtaining data is important in the general case,
as non-English languages tend to lack resources.
The correct usage of Korean particles relies on
knowing lexical, syntactic, semantic, and discourse
information (Lee et al, 2005), which makes this
challenging for both learners and machines (cf. En-
8
glish determiners in Han et al, 2006). The only
other approach we know of, a parser-based one, had
very low precision (Dickinson and Lee, 2009). A
secondary contribution of this work is thus defin-
ing the particle error detection problem for a ma-
chine learner. It is important that the data represent
the relationships between specific lexical items: in
the comparable English case, for example, interest
is usually found with in: interest in/*with learning.
The basic framework we employ is to train a ma-
chine learner on correct Korean data and then apply
this system to learner text, to predict correct parti-
cle usage, which may differ from the learner?s (cf.
Tetreault and Chodorow, 2008). After describing the
grammatical properties of particles in section 2, we
turn to the general approach for obtaining relevant
web data in section 3, reporting basic statistics for
our corpora in section 4. We outline the machine
learing set-up in section 5 and present initial results
in section 6. These results help evaluate the best way
to build specialized corpora for learner language.
2 Korean particles
Similar to English prepositions, Korean postposi-
tional particles add specific meanings or grammat-
ical functions to nominals. However, a particle can-
not stand alone in Korean and needs to be attached
to the preceding nominal. More importantly, par-
ticles indicate a wide range of linguistic functions,
specifying grammatical functions, e.g., subject and
object; semantic roles; and discourse functions. In
(1), for instance, ka marks both the subject (func-
tion) and agent (semantic role), eykey the dative and
beneficiary; and so forth.1
(1) Sumi-ka
Sumi-SBJ
John-eykey
John-to
chayk-ul
book-OBJ
ilhke-yo
read-polite
?Sumi reads a book to John.?
Particles can also combine with nominals to form
modifiers, adding meanings of time, location, instru-
ment, possession, and so forth, as shown in (2). Note
in this case that the marker ul/lul has multiple uses.2
1We use the Yale Romanization scheme for writing Korean.
2Ul/lul, un/nun, etc. only differ phonologically.
(2) Sumi-ka
Sumi-SBJ
John-uy
John-GEN
cip-eyse
house-LOC
ku-lul
he-OBJ
twu
two
sikan-ul
hours-OBJ
kitaly-ess-ta.
wait-PAST-END
?Sumi waited for John for (the whole) two hours in
his house.?
There are also particles associated with discourse
meanings. For example, in (3) the topic marker nun
is used to indicate old information or a discourse-
salient entity, while the delimiter to implies that
there is someone else Sumi likes. In this paper, we
focus on syntactic/semantic particle usage for nom-
inals, planning to extend to other cases in the future.
(3) Sumi-nun
Sumi-TOP
John-to
John-also
cohahay.
like
?Sumi likes John also.?
Due to these complex linguistic properties, parti-
cles are one of the most difficult topics for Korean
language learners. In (4b), for instance, a learner
might replace a subject particle (as in (4a)) with an
object (Dickinson et al, 2008). Ko et al (2004) re-
port that particle errors were the second most fre-
quent error in a study across different levels of Ko-
rean learners, and errors persist across levels (see
also Lee et al, 2009).
(4) a. Sumi-nun
Sumi-TOP
chayk-i
book-SBJ
philyohay-yo
need-polite
?Sumi needs a book.?
b. *Sumi-nun
Sumi-TOP
chayk-ul
book-OBJ
philyohay-yo
need-polite
?Sumi needs a book.?
3 Approach
3.1 Acquiring training data
Due to the lexical relationships involved, machine
learning has proven to be a good method for sim-
ilar NLP problems like detecting errors in En-
glish preposition use. For example Tetreault and
Chodorow (2008) use a maximum entropy classifier
to build a model of correct preposition usage, with
7 million instances in their training set, and Lee and
Knutsson (2008) use memory-based learning, with
10 million sentences in their training set. In expand-
ing the paradigm to other languages, one problem
9
is a dearth of data. It seems like a large data set is
essential for moving forward.
For Korean, there are at least two corpora pub-
licly available right now, the Penn Korean Treebank
(Han et al, 2002), with hundreds of thousands of
words, and the Sejong Corpus (a.k.a., The Korean
National Corpus, The National Institute of Korean
Language, 2007), with tens of millions of words.
While we plan to include the Sejong corpus in fu-
ture data, there are several reasons we pursue a dif-
ferent tack here. First, not every language has such
resources, and we want to work towards a language-
independent platform of data acquisition. Secondly,
these corpora may not be a good model for the kinds
of topics learners write about. For example, news
texts are typically written more formally than learner
writing. We want to explore ways to quickly build
topic-specific corpora, and Web as Corpus (WaC)
technology gives us tools to do this.3
3.2 Web as Corpus
To build web corpora, we use BootCat (Baroni and
Bernardini, 2004). The process is an iterative algo-
rithm to bootstrap corpora, starting with various seed
terms. The procedure is as follows:
1. Select initial seeds (terms).
2. Combine seeds randomly.
3. Run Google/Yahoo queries.
4. Retrieve corpus.
5. Extract new seeds via corpus comparison.
6. Repeat steps #2-#5.
For non-ASCII languages, one needs to check
the encoding of webpages in order to convert the
text into UTF-8 for output, as has been done for,
e.g., Japanese (e.g., Erjavec et al, 2008; Baroni and
Ueyama, 2004). Using a UTF-8 version of Boot-
Cat, we modified the system by using a simple Perl
module (Encode::Guess) to look for the EUC-
KR encoding of most Korean webpages and switch
it to UTF-8. The pages already in UTF-8 do not need
to be changed.
3.3 Obtaining data
A crucial first step in constructing a web corpus is
the selection of appropriate seed terms for construct-
ing the corpus (e.g., Sharoff, 2006; Ueyama, 2006).
3Tetreault and Chodorow (2009) use the web to derive
learner errors; our work, however, tries to obtain correct data.
In our particular case, this begins the question of
how one builds a corpus which models native Ko-
rean and which provides appropriate data for the task
of particle error detection. The data should be genre-
appropriate and contain enough instances of the par-
ticles learners know and used in ways they are ex-
pected to use them (e.g., as temporal modifiers). A
large corpus will likely satisfy these criteria, but has
the potential to contain distracting information. In
Korean, for example, less formal writing often omits
particles, thereby biasing a machine learner towards
under-guessing particles. Likewise, a topic with dif-
ferent typical arguments than the one in question
may mislead the machine. We compare the effec-
tiveness of corpora built in different ways in training
a machine learner.
3.3.1 A general corpus
To construct a general corpus, we identify words
likely to be in a learner?s lexicon, using a list of 50
nouns for beginning Korean students for seeds. This
includes basic vocabulary entries like the words for
mother, father, cat, dog, student, teacher, etc.
3.3.2 A focused corpus
Since we often know what domain4 learner es-
says are written about, we experiment with building
a more topic-appropriate corpus. Accordingly, we
select a smaller set of 10 seed terms based on the
range of topics covered in our test corpus (see sec-
tion 6.1), shown in figure 1. As a first trial, we select
terms that are, like the aforementioned general cor-
pus seeds, level-appropriate for learners of Korean.
han-kwuk ?Korea? sa-lam ?person(s)?
han-kwuk-e ?Korean (lg.)? chin-kwu ?friend?
kyey-cel ?season? ga-jok ?family?
hayng-pok ?happiness? wun-tong ?exercise?
ye-hayng ?travel? mo-im ?gathering?
Figure 1: Seed terms for the focused corpus
3.3.3 A second focused corpus
There are several issues with the quality of data
we obtain from our focused terms. From an ini-
tial observation (see section 4.1), the difficulty stems
in part from the simplicity of the seed terms above,
4By domain, we refer to the subject of a discourse.
10
leading to, for example, actual Korean learner data.
To avoid some of this noise, we use a second set of
seed terms, representing relevant words in the same
domains, but of a more advanced nature, i.e., topic-
appropriate words that may be outside of a typical
learner?s lexicon. Our hypothesis is that this is more
likely to lead to native, quality Korean. For each
one of the simple words above, we posit two more
advanced words, as given in figure 2.
kyo-sa ?teacher? in-kan ?human?
phyung-ka ?evaluation? cik-cang ?workplace?
pen-yuk ?translation? wu-ceng ?friendship?
mwun-hak ?literature? sin-loy ?trust?
ci-kwu ?earth? cwu-min ?resident?
swun-hwan ?circulation? kwan-kye ?relation?
myeng-sang ?meditation? co-cik ?organization?
phyeng-hwa ?peace? sik-i-yo-pep ?diet?
tham-hem ?exploration? yen-mal ?end of a year?
cwun-pi ?preparation? hayng-sa ?event?
Figure 2: Seed terms for the second focused corpus
3.4 Web corpus parameters
One can create corpora of varying size and general-
ity, by varying the parameters given to BootCaT. We
examine three parameters here.
Number of seeds The first way to vary the type
and size of corpus obtained is by varying the number
of seed terms. The exact words given to BootCaT af-
fect the domain of the resulting corpus, and utilizintg
a larger set of seeds leads to more potential to create
a bigger corpus. With 50 seed terms, for example,
there are 19,600 possible 3-tuples, while there are
only 120 possible 3-tuples for 10 seed terms, limit-
ing the relevant pages that can be returned.
For the general (G) corpus, we use: G1) all 50
seed terms, G2) 5 sets of 10 seeds, the result of split-
ting the 50 seeds randomly into 5 buckets, and G3)
5 sets of 20 seeds, which expand the 10-seed sets in
G2 by randomly selecting 10 other terms from the
remaining 40 seeds. This breakdown into 11 sets (1
G1, 5 G2, 5 G3) allows us to examine the effect of
using different amounts of general terms and facili-
tates easy comparison with the first focused corpus,
which has only 10 seed terms.
For the first focused (F1) corpus, we use: F11) the
10 seed terms, and F12) 5 sets of 20 seeds, obtained
by combining F11 with each seed set from G2. This
second group provides an opportunity to examine
what happens when augmenting the focused seeds
with more general terms; as such, this is a first step
towards larger corpora which retain some focus. For
the second focused corpus (F2), we simply use the
set of 20 seeds. We have 7 sets here (1 F11, 5 F12, 1
F2), giving us a total of 18 seed term sets at this step.
Tuple length One can also experiment with tuple
length in BootCat. The shorter the tuple, the more
webpages that can potentially be returned, as short
tuples are likely to occur in several pages (e.g., com-
pare the number of pages that all of person happi-
ness season occur in vs. person happiness season
exercise travel). On the other hand, longer tuples are
more likely truly relevant to the type of data of inter-
est, more likely to lead to well-formed language. We
experiment with tuples of different lengths, namely
3 and 5. With 2 different tuple lengths and 18 seed
sets, we now have 36 sets.
Number of queries We still need to specify how
many queries to send to the search engine. The max-
imum number is determined by the number of seeds
and the tuple size. For 3-word tuples with 10 seed
terms, for instance, there are 10 items to choose 3
objects from:
(10
3
)
= 10!3!(10?3)! = 120 possibilities.
Using all combinations is feasible for small seed
sets, but becomes infeasible for larger seed sets, e.g.,
(50
5
)
= 2, 118, 760 possibilities. To reduce this, we
opt for the following: for 3-word tuples, we generate
120 queries for all cases and 240 queries for the con-
ditions with 20 and 50 seeds. Similarly, for 5-word
tuples, we generate the maximum 252 queries with
10 seeds, and both 252 and 504 for the other condi-
tions. With the previous 36 sets (12 of which have
10 seed terms), evenly split between 3 and 5-word
tuples, we now have 60 total corpora, as in table 1.
# of seeds
tuple # of General F1 F2
len. queries 10 20 50 10 20 20
3 120 5 5 1 1 5 1
240 n/a 5 1 n/a 5 1
5 252 5 5 1 1 5 1
504 n/a 5 1 n/a 5 1
Table 1: Number of corpora based on parameters
11
Other possibilities There are other ways to in-
crease the size of a web corpus using BootCaT. First,
one can increase the number of returned pages for a
particular query. We set the limit at 20, as anything
higher will more likely result in non-relevant data
for the focused corpora and/or duplicate documents.
Secondly, one can perform iterations of search-
ing, extracting new seed terms with every iteration.
Again, the concern is that by iterating away from the
initial seeds, a corpus could begin to lose focus. We
are considering both extensions for the future.
Language check One other constraint we use is to
specify the particular language of interest, namely
that we want Korean pages. This parameter is set
using the language option when collecting URLs.
We note that a fair amount of English, Chinese, and
Japanese appears in these pages, and we are cur-
rently developing our own Korean filter.
4 Corpus statistics
To gauge the properties of size, genre, and degree of
particle usage in the corpora, independent of appli-
cation, basic statistics of the different web corpora
are given in table 2, where we average over multiple
corpora for conditions with 5 corpora.5
There are a few points to understand in the table.
First, it is hard to count true words in Korean, as
compounds are frequent, and particles have a de-
batable status. From a theory-neutral perspective,
we count ejels, which are tokens occurring between
white spaces. Secondly, we need to know about the
number of particles and number of nominals, i.e.,
words which could potentially bear particles, as our
machine learning paradigm considers any nominal a
test case for possible particle attachment. We use a
POS tagger (Han and Palmer, 2004) for this.
Some significant trends emerge when comparing
the corpora in the table. First of all, longer queries
(length 5) result in not only more returned unique
webpages, but also longer webpages on average than
shorter queries (length 3). This effect is most dra-
matic for the F2 corpora. The F2 corpora also exhibit
a higher ratio of particles to nominals than the other
web corpora, which means there will be more pos-
5For the 252 5-tuple 20 seed General corpora, we average
over four corpora, due to POS tagging failure on the fifth corpus.
itive examples in the training data for the machine
learner based on the F2 corpora.
4.1 Qualitative evaluation
In tandem with the basic statistics, it is also impor-
tant to gauge the quality of the Korean data from
a more qualitative perspective. Thus, we examined
the 120 3-tuple F1 corpus and discovered a number
of problems with the data.
First, there are issues concerning collecting data
which is not pure Korean. We find data extracted
from Chinese travel sites, where there is a mixture of
non-standard foreign words and unnatural-sounding
translated words in Korean. Ironically, we also find
learner data of Korean in our search for correct Ko-
rean data. Secondly, there are topics which, while
exhibiting valid forms of Korean, are too far afield
from what we expect learners to know, including re-
ligious sites with rare expressions; poems, which
commonly drop particles; gambling sites; and so
forth. Finally, there are cases of ungrammatical uses
of Korean, which are used in specific contexts not
appropriate for our purposes. These include newspa-
per titles, lists of personal names and addresses, and
incomplete phrases from advertisements and chats.
In these cases, we tend to find less particles.
Based on these properties, we developed the
aforementioned second focused corpus with more
advanced Korean words and examined the 240 3-
tuple F2 corpus. The F2 seeds allow us to capture a
greater percentage of well-formed data, namely data
from news articles, encyclopedic texts, and blogs
about more serious topics such as politics, literature,
and economics. While some of this data might be
above learners? heads, it is, for the most part, well-
formed native-like Korean. Also, the inclusion of
learner data has been dramatically reduced. How-
ever, some of the same problems from the F1 corpus
persist, namely the inclusion of poetry, newspaper
titles, religious text, and non-Korean data.
Based on this qualitative analysis, it is clear that
we need to filter out more data than is currently be-
ing filtered, in order to obtain valid Korean of a type
which uses a sufficient number of particles in gram-
matical ways. In the future, we plan on restrict-
ing the genre, filtering based on the number of rare
words (e.g., religious words), and using a trigram
language model to check the validity.
12
Ejel Particles Nominals
Corpus Seeds Len. Queries URLs Total Avg. Total Avg. Total Avg.
Gen. 10 3 120 1096.2 1,140,394.6 1044.8 363,145.6 331.5 915,025 838.7
5 252 1388.2 2,430,346.4 1779.9 839,005.8 618.9 1,929,266.0 1415.3
20 3 120 1375.2 1,671,549.2 1222.1 540,918 394.9 1,350,976.6 988.6
3 240 2492.4 2,735,201.6 1099.4 889,089 357.3 2,195,703 882.4
5 252 1989.6 4,533,642.4 2356 1,359,137.2 724.5 3,180,560.6 1701.5
5 504 3487 7,463,776 2193.5 2,515,235.8 741.6 5,795,455.8 1709.7
50 3 120 1533 1,720,261 1122.1 584,065 380.9 1,339,308 873.6
3 240 2868 3,170,043 1105.3 1,049,975 366.1 2,506,995 874.1
5 252 1899.5 4,380,684.2 2397.6 1,501,358.7 821.5 3,523,746.2 1934.6
5 504 5636 5,735,859 1017.7 1,773,596 314.6 4,448,815 789.3
F1 10 3 120 1315 628,819 478.1 172,415 131.1 510,620 388.3
5 252 1577 1,364,885 865.4 436,985 277.1 1,069,898 678.4
20 3 120 1462.6 1,093,772.4 747.7 331,457.8 226.8 885,157.2 604.9
240 2637.2 1,962,741.8 745.2 595,570.6 226.1 1,585,730.4 602.1
5 252 2757.6 2,015,077.8 730.8 616,163.8 223.4 1,621,306.2 588
504 4734 3,093,140.4 652.9 754,610 159.8 1,993,104.4 422.1
F2 20 3 120 1417 1,054,925 744.5 358,297 252.9 829,416 585.3
240 2769 1,898,383 685.6 655,757 236.8 1,469,623 530.7
5 252 1727 4,510,742 2611.9 1,348,240 780.7 2,790,667 1615.9
504 2680 6,916,574 2580.8 2,077,171 775.1 4,380,571 1634.5
Table 2: Basic statistics of different web corpora
Note that one might consider building even larger
corpora from the start and using the filtering step to
winnow down the corpus for a particular application,
such as particle error detection. However, while re-
moving ungrammatical Korean is a process of re-
moving noise, identifying whether a corpus is about
traveling, for example, is a content-based decision.
Given that this is what a search engine is designed
to do, we prefer filtering based only on grammatical
and genre properties.
5 Classification
We describe the classification paradigm used to de-
termine how effective each corpus is for detecting
correct particle usage; evaluation is in section 6.
5.1 Machine learning paradigm
Based on the parallel between Korean particles and
English prepositions, we use preposition error de-
tection as a starting point for developing a classifier.
For prepositions, Tetreault and Chodorow (2008) ex-
tract 25 features to guess the correct preposition (out
of 34 selected prepositions), including features cap-
turing the lexical and grammatical context (e.g., the
words and POS tags in a two-word window around
the preposition) and features capturing various rel-
evant selectional properties (e.g., the head verb and
noun of the preceding VP and NP).
We are currently using TiMBL (Daelemans et al,
2007) for development purposes, as it provides a
range of options for testing. Given that learner
data needs to be processed instantaneously and that
memory-based learning can take a long time to clas-
sify, we will revisit this choice in the future.
5.2 Defining features
5.2.1 Relevant properties of Korean
As discussed in section 2, Korean has major dif-
ferences from English, leading to different features.
First, the base word order of Korean is SOV, which
means that the following verb and following noun
could determine how the current word functions.
However, since Korean allows for freer word order
than English, we do not want to completely disre-
gard the previous noun or verb, either.
Secondly, the composition of words is different
than English. Words contain a stem and an arbitrary
number of suffixes, which may be derivational mor-
13
phemes as well as particles, meaning that we must
consider sub-word features, i.e., segment words into
their component morphemes.
Finally, particles have more functions than prepo-
sitions, requiring a potentially richer space of fea-
tures. Case marking, for example, is even more de-
pendent upon the word?s grammatical function in
a sentence. In order to ensure that our system can
correctly handle all of the typical relations between
words without failing on less frequent constructions,
we need (large amounts of) appropriate data.
5.2.2 Feature set
To begin with, we segment and POS tag the text,
using a hybrid (trigram + rule-based) morphological
tagger for Korean (Han and Palmer, 2004). This seg-
mentation phase means that we can define subword
features and isolate the particles in question. For our
features, we break each word into: a) its stem and b)
its combined affixes (excluding particles), and each
of these components has its own POS, possibly a
combined tag (e.g., EPF+EFN), with tags from the
Penn Korean Treebank (Han et al, 2002).
The feature vector uses a five word window that
includes the target word and two words on either
side for context. Each word is broken down into four
features: stem, affixes, stem POS, and affixes POS.
Given the importance of surrounding noun and verbs
for attachment in Korean, we have features for the
preceding as well as the following noun and verb.
For the noun/verb features, only the stem is used, as
this is largely a semantically-based property.
In terms of defining a class, if the target word?s
affixes contain a particle, it is removed and used as
the basis for the class; otherwise the class is NONE.
We also remove particles in the context affixes, as
we cannot rely on surrounding learner particles.
As an example, consider predicting the particle
for the word Yenge (?English?) in (5a). We gener-
ate the instance in (5b). The first five lines refer
to the previous two words, the target word, and the
following two words, each split into stem and suf-
fixes along with their POS tags, and with particles
removed. The sixth line contains the stems of the
preceding and following noun and verb, and finally,
there is the class (YES/NO).
(5) a. Mikwuk-eyse
America-in
sal-myense
live-while
Yenge-man-ul
English-only-OBJ
cip-eyse
home-at
ss-ess-eyo.
use-Past-Decl
?While living in America, (I/she/he) used only
English at home.?
b. Mikwuk NPR NONE NONE
sal VV myense ECS
Yenge NPR NONE NONE
cip NNC NONE NONE
ss VV ess+eyo EPF+EFN
sal Mikwuk ss cip
YES
For the purposes of evaluating the different cor-
pora, we keep the task simple and only guess YES
or NO for the existence of a particle. We envision
this as a first pass, where the specific particle can
be guessed later. This is also a practical task, in
that learners can benefit from accurate feedback on
knowing whether or not a particle is needed.
6 Evaluation
We evaluate the web corpora for the task of predict-
ing particle usage, after describing the test corpus.
6.1 Learner Corpus
To evaluate, we use a corpus of learner Korean made
up of essays from college students (Lee et al, 2009).
The corpus is divided according to student level (be-
ginner, intermediate) and student background (her-
itage, non-heritage),6 and is hand-annotated for par-
ticle errors. We expect beginners to be less accurate
than intermediates and non-heritage less accurate
than heritage learners. To pick a middle ground, the
current research has been conducted on non-heritage
intermediate learners. The test corpus covers a range
of common language classroom topics such as Ko-
rean language, Korea, friends, family, and traveling.
We run our system on raw learner data, i.e, un-
segmented and with spelling and spacing errors in-
cluded. As mentioned in section 5.2.2, we use a POS
tagger to segment the words into morphemes, a cru-
cial step for particle error detection.7
6Heritage learners have had exposure to Korean at a young
age, such as growing up with Korean spoken at home.
7In the case of segmentation errors, we cannot possibly get
the particle correct. We are currently investigating this issue.
14
Seeds Len. Quer. P R F
Gen. 10 3 120 81.54% 76.21% 78.77%
5 252 82.98% 77.77% 80.28%
20 3 120 81.56% 77.26% 79.33%
3 240 82.89% 78.37% 80.55%
5 252 83.79% 78.17% 80.87%
5 504 84.30% 79.44% 81.79%
50 3 120 82.97% 77.97% 80.39%
3 240 83.62% 80.46% 82.00%
5 252 82.57% 78.45% 80.44%
5 504 84.25% 78.69% 81.36%
F1 10 3 120 81.41% 74.67% 77.88%
5 252 83.82% 77.09% 80.30%
20 3 120 82.23% 76.40% 79.20%
240 82.57% 77.19% 79.78%
5 252 83.62% 77.97% 80.68%
504 81.86% 75.88% 78.73%
F2 20 3 120 81.63% 76.44% 78.93%
240 82.57% 78.45% 80.44%
5 252 84.21% 80.62% 82.37%
504 83.87% 81.51% 82.67%
Table 3: Results of guessing particle existence, training
with different corpora
The non-heritage intermediate (NHI) corpus gives
us 3198 words, with 1288 particles and 1836 nom-
inals. That is, about 70% of the nominals in the
learner corpus are followed by a particle. This is a
much higher average than in the 252 5-tuple F2 cor-
pus, which exhibits the highest average of all of the
web corpora at about 48% ( 7811616 ; see table 2).
6.2 Results
We use the default settings for TiMBL for all the re-
sults we report here. Though we have obtained 4-5%
higher F-scores using different settings, the compar-
isons between corpora are the important measure for
the current task. The results are given in table 3.
The best results were achieved when training
on the 5-tuple F2 corpora, leading to F-scores of
82.37% and 82.67% for the 252 tuple and 504 tu-
ple corpora, respectively. This finding reinforces our
hypothesis that more advanced seed terms result in
more reliable Korean data, while staying within the
domain of the test corpus. Both longer tuple lengths
and greater amounts of queries have an effect on the
reliability of the resulting corpora. Specificaly, 5-
tuple corpora produce better results than similar 3-
tuple corpora, and corpora with double the amount
of queries of n-length perform better than smaller
comparable corpora. Although larger corpora tend
to do better, it is important to note that there is not
a clear relationship. The general 50/5/252 corpus,
for instance, is similarly-sized to the F2 focused
20/5/252 corpus, with over 4 million ejels (see ta-
ble 2). The focused corpus?based on fewer yet
more relevant seed terms?has 2% better F-score.
7 Summary and Outlook
In this paper, we have examined different ways to
build web corpora for analyzing learner language
to support the detection of errors in Korean parti-
cles. This type of investigation is most useful for
lesser-resourced languages, where the error detec-
tion task stays constant, but the topic changes fre-
quently. In order to develop a framework for testing
web corpora, we have also begun developing a ma-
chine learning system for detecting particle errors.
The current web data, as we have demonstrated, is
not perfect, and thus we need to continue improving
that. One approach will be to filter out clearly non-
Korean data, as suggested in section 4.1. We may
also explore instance sampling (e.g., Wunsch et al,
2009) to remove many of the non-particle nominal
(negative) instances, which will reduce the differ-
ence between the ratios of negative-to-positive in-
stances of the web and learner corpora. We still feel
that there is room for improvement in our seed term
selection, and plan on constructing specific web cor-
pora for each topic covered in the learner corpus.
We will also consider adding currently available cor-
pora, such as the Sejong Corpus (The National Insti-
tute of Korean Language, 2007), to our web data.
With better data, we can work on improving the
machine learning system. This includes optimizing
the set of features, the parameter settings, and the
choice of machine learning algorithm. Once the sys-
tem has been optimized, we will need to test the re-
sults on a wider range of learner data.
Acknowledgments
We would like to thank Marco Baroni and Jan
Pomika?lek for kindly providing a UTF-8 version of
BootCat; Chong Min Lee for help with the POS tag-
ger, provided by Chung-Hye Han; and Joel Tetreault
for useful discussion.
15
References
Amaral, Luiz and Detmar Meurers (2006). Where
does ICALL Fit into Foreign Language Teach-
ing? Talk given at CALICO Conference. May
19, 2006. University of Hawaii.
Baroni, Marco and Silvia Bernardini (2004). Boot-
CaT: Bootstrapping Corpora and Terms from the
Web. In Proceedings of LREC 2004. pp. 1313?
1316.
Baroni, Marco and Motoko Ueyama (2004). Re-
trieving Japanese specialized terms and corpora
from the World Wide Web. In Proceedings of
KONVENS 2004.
Daelemans, Walter, Jakub Zavrel, Ko van der Sloot,
Antal van den Bosch, Timbl Tilburg and Memory
based Learner (2007). TiMBL: Tilburg Memory-
Based Learner - version 6.1 - Reference Guide.
De Felice, Rachele and Stephen Pulman (2008). A
classifier-baed approach to preposition and deter-
miner error correction in L2 English. In Proceed-
ings of COLING-08. Manchester.
Dickinson, Markus, Soojeong Eom, Yunkyoung
Kang, Chong Min Lee and Rebecca Sachs (2008).
A Balancing Act: How can intelligent computer-
generated feedback be provided in learner-to-
learner interactions. Computer Assisted Language
Learning 21(5), 369?382.
Dickinson, Markus and Chong Min Lee (2009).
Modifying Corpus Annotation to Support the
Analysis of Learner Language. CALICO Journal
26(3).
Erjavec, Irena Srdanovic`, Tomaz Erjavec and Adam
Kilgarriff (2008). A Web Corpus and Word
Sketches for Japanese. Information and Media
Technologies 3(3), 529?551.
Han, Chung-Hye, Na-Rare Han, Eon-Suk Ko and
Martha Palmer (2002). Development and Eval-
uation of a Korean Treebank and its Application
to NLP. In Proceedings of LREC-02.
Han, Chung-Hye and Martha Palmer (2004). A Mor-
phological Tagger for Korean: Statistical Tag-
ging Combined with Corpus-Based Morphologi-
cal Rule Application. Machine Translation 18(4),
275?297.
Han, Na-Rae, Martin Chodorow and Claudia Lea-
cock (2006). Detecting Errors in English Arti-
cle Usage by Non-Native Speakers. Natural Lan-
guage Engineering 12(2).
Ko, S., M. Kim, J. Kim, S. Seo, H. Chung and S. Han
(2004). An analysis of Korean learner corpora
and errors. Hanguk Publishing Co.
Lee, John and Ola Knutsson (2008). The Role of
PP Attachment in Preposition Generation. In Pro-
ceedings of CICLing 2008. Haifa, Israel.
Lee, John and Stephanie Seneff (2006). Auto-
matic Grammar Correction for Second-Language
Learners. In INTERSPEECH 2006. Pittsburgh,
pp. 1978?1981.
Lee, Sun-Hee, Donna K. Byron and Seok Bae Jang
(2005). Why is Zero Marking Important in Ko-
rean? In Proceedings of IJCNLP-05. Jeju Island,
Korea.
Lee, Sun-Hee, Seok Bae Jang and Sang kyu Seo
(2009). Annotation of Korean Learner Corpora
for Particle Error Detection. CALICO Journal
26(3).
Sharoff, Serge (2006). Creating General-Purpose
Corpora Using Automated Search Engine
Queries. In WaCky! Working papers on the Web
as Corpus. Gedit.
Tetreault, Joel and Martin Chodorow (2008). The
Ups and Downs of Preposition Error Detection
in ESL Writing. In Proceedings of COLING-08.
Manchester.
Tetreault, Joel and Martin Chodorow (2009). Exam-
ining the Use of Region Web Counts for ESL Er-
ror Detection. In Web as Corpus Workshop (WAC-
5). San Sebastian, Spain.
The National Institute of Korean Language (2007).
The Sejong Corpus.
Ueyama, Motoko (2006). Evaluation of Japanese
Web-based Reference Corpora: Effects of Seed
Selection and Time Interval. In WaCky! Working
papers on the Web as Corpus. Gedit.
Wunsch, Holger, Sandra Ku?bler and Rachael
Cantrell (2009). Instance Sampling Methods for
Pronoun Resolution. In Proceedings of RANLP
2009. Borovets, Bulgaria.
16
Proceedings of the Fourth Linguistic Annotation Workshop, ACL 2010, pages 38?46,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
Consistency Checking for Treebank Alignment
Markus Dickinson
Indiana University
md7@indiana.edu
Yvonne Samuelsson
Stockholm University
yvonne.samuelsson@ling.su.se
Abstract
This paper explores ways to detect errors
in aligned corpora, using very little tech-
nology. In the first method, applicable
to any aligned corpus, we consider align-
ment as a string-to-string mapping. Treat-
ing the target string as a label, we ex-
amine each source string to find incon-
sistencies in alignment. Despite setting
up the problem on a par with grammat-
ical annotation, we demonstrate crucial
differences in sorting errors from legiti-
mate variations. The second method ex-
amines phrase nodes which are predicted
to be aligned, based on the alignment of
their yields. Both methods are effective in
complementary ways.
1 Introduction
Parallel corpora?texts and their translations?
have become essential in the development of
machine translation (MT) systems. Alignment
quality is crucial to these corpora; as Tiede-
mann (2003) states, ?[t]he most important fea-
ture of texts and their translations is the corre-
spondence between source and target segments?
(p. 2). While being useful for translation studies
and foreign language pedagogy (see, e.g., Botley
et al, 2000; McEnery and Wilson, 1996), PARAL-
LEL TREEBANKS?syntactically-annotated paral-
lel corpora?offer additional useful information
for machine translation, cross-language infor-
mation retrieval, and word-sense disambiguation
(see, e.g., Tiedemann, 2003),
While high-quality alignments are desirable,
even gold standard annotation can contain anno-
tation errors. For other forms of linguistic an-
notation, the presence of errors has been shown
to create various problems, from unreliable train-
ing and evaluation of NLP technology (e.g., Padro
and Marquez, 1998) to low precision and recall
of queries for already rare linguistic phenomena
(e.g., Meurers and Mu?ller, 2008). Even a small
number of errors can have a significant impact
on the uses of linguistic annotation, e.g., chang-
ing the assessment of parsers (e.g., Habash et al,
2007). One could remove potentially unfavorable
sentence pairs when training a statistical MT sys-
tem, to avoid incorrect word alignments (Okita,
2009), but this removes all relevant data from
those sentences and does not help evaluation.
We thus focus on detecting errors in the anno-
tation of alignments. Annotation error detection
has been explored for part-of-speech (POS) anno-
tation (e.g., Loftsson, 2009) and syntactic anno-
tation (e.g., Ule and Simov, 2004; Dickinson and
Meurers, 2005), but there have been few, if any, at-
tempts to develop general approaches to error de-
tection for aligned corpora. Alignments are differ-
ent in nature, as the annotation does not introduce
abstract categories such as POS, but relies upon
defining translation units with equivalent mean-
ings.
We use the idea that variation in annotation can
indicate errors (section 2), for consistency check-
ing of alignments, as detailed in section 3. In sec-
tion 4, we outline language-independent heuristics
to sort true ambiguities from errors, and evaluate
them on a parallel treebank in section 5. In sec-
tion 6 we turn to a complementary method, ex-
ploiting compositional properties of aligned tree-
banks, to align more nodes. The methods are sim-
ple, effective, and applicable to any aligned tree-
bank. As far as we know, this is the first attempt to
thoroughly investigate and empirically verify er-
ror detection methods for aligned corpora.
38
2 Background
2.1 Variation N -gram Method
As a starting point for an error detection method
for aligned corpora, we use the variation n-gram
approach for syntactic annotation (Dickinson and
Meurers, 2003, 2005). The approach is based on
detecting strings which occur multiple times in
the corpus with varying annotation, the so-called
VARIATION NUCLEI. The nucleus with repeated
surrounding context is referred to as a VARIATION
n-GRAM. The basic heuristic for detecting anno-
tation errors requires one word of recurring con-
text on each side of the nucleus, which is suffi-
cient for detecting errors in grammatical annota-
tion with high precision (Dickinson, 2008).
The approach detects bracketing and labeling
errors in constituency annotation. For example,
the variation nucleus last month occurs once in
the Penn Treebank (Taylor et al, 2003) with the
label NP and once as a non-constituent, handled
through a special label NIL. As a labeling error
example, next Tuesday occurs three times, twice
as NP and once as PP (Dickinson and Meur-
ers, 2003). The method works for discontinuous
constituency annotation (Dickinson and Meurers,
2005), allowing one to apply it to alignments,
which may span over several words.
2.2 Parallel Treebank Consistency Checking
For the experiments in this paper we will use
the SMULTRON parallel treebank of Swedish,
German, and English (Gustafson-C?apkova? et al,
2007), containing syntactic annotation and align-
ment on both word and phrase levels.1 Addition-
ally, alignments are marked as showing either an
EXACT or a FUZZY (approximate) equivalence.
Corpora with alignments often have under-
gone some error-checking. Previous consistency
checks for SMULTRON, for example, consisted
of running one script for comparing differences
in length between the source and target language
items, and one script for comparing alignment
labels, to detect variation between EXACT and
FUZZY links. For example, the pair and (English)
and samt (German, ?together with?) had 20 FUZZY
matches and 1 (erroneous) EXACT match. Such
1SMULTRON is freely available for research purposes, see
http://www.cl.uzh.ch/kitt/smultron/.
methods are limited, in that they do not, e.g., han-
dle missing alignments.
The TreeAligner2 tool for annotating and
querying aligned parallel treebanks (Volk et al,
2007) employs its own consistency checking, re-
cently developed by Torsten Marek. One method
uses 2 ? 2 contingency tables over words, look-
ing, e.g., at the word-word or POS-POS combina-
tions, pinpointing anomalous translation equiva-
lents. While potentially effective, this does not ad-
dress the use of alignments in context, i.e., when
we might expect to see a rare translation.
A second, more treebank-specific method
checks for so-called branch link locality: if two
nodes are aligned, any node dominating one of
them can only be aligned to a node dominating the
other one. While this constraint can flag erroneous
links, it too does not address missing alignments.
The two methods we propose in this paper address
these limitations and can be used to complement
this work. Furthermore, these methods have not
been evaluated, whereas we evaluate our methods.
3 Consistency of Alignment
To adapt the variation n-gram method and deter-
mine whether strings in a corpus are consistently
aligned, we must: 1) define the units of data we
expect to be consistently annotated (this section),
and 2) define which information effectively iden-
tifies the erroneous cases (section 4).
3.1 Units of Data
Alignment relates words in a source language and
words in a target language, potentially mediated
by phrase nodes. Following the variation n-gram
method, we define the units of data, i.e., the vari-
ation nuclei, as strings. Then, we break the prob-
lem into two different source-to-target mappings,
mapping a source variation nucleus to a target lan-
guage label. With a German-English aligned cor-
pus, for example, we look for the consistency of
aligning German words to their English counter-
parts and separately examine the consistency of
aligning English words with their German ?la-
bels.? Because a translated word can be used in
different parts of a sentence, we also normalize all
target labels into lower-case, preventing variation
between, e.g., the and The.
2http://www.cl.uzh.ch/kitt/treealigner
39
.$.bl?htenVVFINInAPPR einigenPIAT G?rtenNN unterAPPR denART Obstb?umenNN Kr?nzeNNdichteADJA vonAPPR OsterglockenNN
..theDT fruitNN treesNNSInIN someDT ofIN theDT gardensNNS wereVBD encircledVBN ---NONE- withIN denseJJ clustersNNS ofIN daffodilsNNS
*
HD
HD
NK HDNP
NKPP
MO
HD
NK HDNP
NKPP
MO
HD
HDAP
NK
HD
HDNP
NKPP
MNRNP
SBS
VROOT
NP
SBJ
NP
PP
NP
PP
LOC
NP NP NP
PP
NP
PPCLRVP
VP
S
VROOT
.$.bl?hteennVFI NAbPRRi PhegTGeII ?trPiu dGtONbbrIIsArthberPmKPhtl?eRuzIcd rthONAPmnmNbPiu DJ?l?rhII tOPRRiPiu votrkrAII
..u?rmu ktGAII thFI e?rmu OtGGTGII ?t?nfm hTeif GrNlenf gte?FI Nbif OSl?if NbFI Nmu egtel?II
am
am
amNP
IzPP
Dw
Iz am
amAP
Iz
Id amASP
Dw NP
wP
Iz am
am
amNP
IzPP
DIiNP
vf V
SROOT
NP NP
PPBw-NP
vfK
A	SP NP
PP
PP-BiSP
SP
V
SROOT
Figure 1: Word and phrase alignments span the
same string on the left, but not on the right.
Although alignment maps strings to strings for
this method, complications arise when mediated
by phrase nodes: if a phrase node spans over only
one word, it could have two distinct mappings,
one as a word and one as a phrase, which may
or may not result in the same yield. Figure 1 il-
lustrates this. On the left side, Osterglocken is
aligned to daffodils at the word level, and the same
string is aligned on the phrase level (NP to NP).
In contrast, on the right side, the word Spiegel is
aligned to the word mirror, while at the phrase
level, Spiegel (NP) is aligned to the mirror (NP).
As word and phrase level strings can behave dif-
ferently, we split error detection into word-level
and phrase-level methods, to avoid unnecessary
variation. By splitting the problem first into differ-
ent source-to-target mappings and then into words
and phrases, we do not have to change the under-
lying way of finding consistency.
Multiple Alignment The mapping between
source strings and target labels handles n-to-m
alignments. For example, if Ga?rten maps to the
gardens, the and gardens is considered one string.
Likewise, in the opposite direction, the gardens
maps as a unit to Ga?rten, even if discontinuous.
Unary Branches With syntactic annotation,
unary branches present a potential difficulty, in
that a single string could have more than one la-
bel, violating the assumption that the string-to-
.
$.
b
$l
l
$l
?htene
VVFIN
APPnR
NN
ign
TG?
TRerude
NN
ihRR
TOV
snRR
mKzc
DdhJnR
VVFIN
rgd
vvAG
onkhRinR
vIc
gR
TvvG
ngRnk
TG?
fhRi
NN
kge
TvvG
StRJndPRue
NN
.. lleanO? hRPrndNN gPVwB DuuiNNIDIN rnvGv hP-Vwv PuknuRnNN ?gEgRJVw* uRIN eanO? niJnNN uDIN PehdEheguRNN
SO
SO
NP
vO
Nm SO
NP
cw
vS
Hv SO
SO
NP
cw
SO
NP
KT
SO
Nm SO
SO
SO
NPNm
PPMNG
NPNm
PPMK
AGA
SVPMK
A
VROOT
NP
cwL
NP
vGOVP
NP
cwL
NP
cwL
NP NP
PP
NP
PP
HfGVP
A
VP
A
A	SR
TOV A
VROOT
.
$b
l?ht
enVFI
NAt
PPRi
gTGG
n?e
Trud
n?e
tAGt
niO
TGgtht
PFs
lmrKgz? .bNdtPiP cdtGiD dTJteD vttGeDI NmotmGtII tKNtiD
k?
k?
NPsD
k?
ASPzf
k?
ASPzf
IS IS
NPP?
V
SROOT
NP
sDa
A	SP A	SP
NP
Pi?SP
SP
V

SROOT
Figure 2: The word someone aligned as a phrase
on the left, but not a phrase by itself on the right.
label mapping is a function. For example, in
Penn Treebank-style annotation, an NP node can
dominate a QP (quantifier phrase) node via a
unary branch. Thus, an annotator could (likely
erroneously) assign different alignments to each
phrasal node, one for the NP and one for the QP,
resulting in different target labels.
We handle all the (source) unary branch align-
ments as a conjunction of possibilities, ordered
from top to bottom. Just as the syntactic struc-
ture can be relabeled as NP/QP (Dickinson and
Meurers, 2003), we can relabel a string as, e.g.,
the man/man. If different unary nodes result in the
same string (the man/the man), we combine them
(the man). Note that unary branches are unprob-
lematic in the target language since they always
yield the same string, i.e., are still one label.
3.2 Consistency and Completeness
Error detection for syntactic annotation finds in-
consistencies in constituent labeling (e.g., NP vs.
QP) and inconsistencies in bracketing (e.g., NP vs.
NIL). Likewise, we can distinguish inconsistency
in labeling (different translations) from inconsis-
tency in alignment (aligned/unaligned). Detecting
inconsistency in alignment deals with the com-
pleteness of the annotation, by using the label NIL
for unaligned strings.
We use the method from Dickinson and Meur-
ers (2005) to generate NILs, but using NIL for un-
aligned strings is too coarse-grained for phrase-
level alignment. A string mapping to NIL might
be a phrase which has no alignment, or it might
40
not be a phrase and thus could not possibly have
an alignment. Thus, we create NIL-C as a new
label, indicating a constituent with no alignment,
differing from NIL strings which do not even form
a phrase. For example, on the left side of Fig-
ure 2, the string someone aligns to jemanden on
the phrase level. On the right side of Figure 2,
the string someone by itself does not constitute a
phrase (even though the alignment in this instance
is correct) and is labeled NIL. If there were in-
stances of someone as an NP with no alignment,
this would be NIL-C. NIL-C cases seem to be use-
ful for inconsistency detection, as we expect con-
sistency for items annotated as a phrase.
3.3 Alignment Types
Aligned corpora often specify additional informa-
tion about each alignment, e.g., a ?sure? or ?pos-
sible? alignment (Och and Ney, 2003). In SMUL-
TRON, for instance, an EXACT alignment means
that the strings are considered direct translation
equivalents outside the current sentence context,
whereas a FUZZY one is not as strict an equiva-
lent. For example, something in English EXACT-
aligns with etwas in German. However, if some-
thing and irgend etwas (?something or other?) are
constituents on the phrase level, <something, ir-
gend etwas> is an acceptable alignment (since the
corpus aligns as much as possible), but is FUZZY.
Since EXACT alignments are the ones we expect
to consistently align with the same string across
the corpus, we attach information about the align-
ment type to each corpus position. This can be
used to filter out variations involving, e.g., FUZZY
alignments (see section 4.4). When multiple
alignments form a single variation nucleus, there
could be different types of alignment for each link,
e.g., dog EXACT-aligning and the FUZZY-aligning
with Hund. We did not observe this, but one can
easily allow for a mixed type (EXACT-FUZZY).
3.4 Algorithm
The algorithm first splits the data into appropriate
units (SL=source language, TL=target language):
1. Divide the alignments into two SL-to-TL mappings.
2. Divide each SL-to-TL alignment set into word-level
and phrase-level alignments.
For each of the four sets of alignments:
1. Map each string in SL with an alignment to a label
? Label = <(lower-cased) TL translation, EX-
ACT|FUZZY|EXACT-FUZZY>
? (For phrases) Constituent phrases with no align-
ment are given the special label, NIL-C.
? (For phrases) Constituent phrases which are
unary branches are given a single, normalized la-
bel representing all target strings.
2. Generate NIL alignments for string tokens which occur
in SL, but have no alignment to TL, using the method
described in Dickinson and Meurers (2005).
3. Find SL strings which have variation in labeling.
4. Filter the variations from step 3, based on likelihood of
being an error (see section 4).
4 Identifying Inconsistent Alignments
As words and phrases have acceptable variants for
translation, the method in section 3 will lead to
detecting acceptable variations. We use several
heuristics to filter the set of variations.
4.1 NIL-only Variation
As discussed in section 3.2, we use the label NIL-
C to refer to syntactic constituents which do not
receive an alignment, while NIL refers to non-
constituent strings without an alignment. A string
which varies between NIL and NIL-C, then, is not
really varying in its alignment?i.e., it is always
unaligned. We thus remove cases varying only be-
tween NIL and NIL-C.
4.2 Context-based Filtering
The variation n-gram method has generally relied
upon immediate lexical context around the vari-
ation nucleus, in order to sort errors from ambi-
guities (Dickinson, 2008). However, while use-
ful for grammatical annotation, it is not clear how
useful the surrounding context is for translation
tasks, given the wide range of possible translations
for the same context. Further, requiring identical
context around source words is very strict, leading
to sparse data problems, and it ignores alignment-
specific information (see sections 4.3 and 4.4).
We test three different notions of context.
Matching the variation n-gram method, we first
employ a filter identifying those nuclei which
share the ?shortest? identical context, i.e., one
word of context on every side of a nucleus. Sec-
ondly, we relax this to require only one word of
41
context, on either the left or right side. Finally, we
require no identical context in the source language
and rely only on other filters. For example, with
the nucleus come in the context Where does the
world come from, the first notion requires world
come from to recur, the second either world come
or come from, and the third only requires that the
nucleus itself recur (come).
4.3 Target Language Filtering
Because translation is open-ended, there can be
different translations in a corpus. We want to
filter out cases where there is variation in align-
ment stemming from multiple translation possibil-
ities. We implement a TARGET LANGUAGE FIL-
TER, which keeps only the variations where the
target words are present in the same sentence. If
word x is sometimes aligned to y1 and sometimes
to y2 , and word y2 occurs in at least one sentence
where y1 is the chosen target, then we keep the
variation. If y1 and y2 do not occur in any of the
same sentences, we remove the variation: given
the translations, there is no possibility of having
the same alignment.
This also works for NIL labels, given sentence
alignments.3 For NILs, the check is in only one
direction: the aligned sentence must contain the
target string used as the label elsewhere in the cor-
pus. For instance, the word All aligns once with
alle and twice with NIL. We check the two NIL
cases to see whether one of them contains alle.
Sentences which are completely unaligned lead
to NILs for every word and phrase, and we always
keep the variation. In practice, the issue of having
no alignment should be handled separately.
4.4 Alignment Type Filtering
A final filter relies on alignment type informa-
tion. Namely, the FUZZY label already indicates
that the alignment is not perfect, i.e., not nec-
essarily applicable in other contexts. For exam-
ple, the English word dead FUZZY-aligns with the
German verschwunden (?gone, missing?), the best
translation in its context. In another part of the
corpus, dead EXACT-aligns with leblosen (?life-
less?). While this is variation between verschwun-
den and leblosen, the presence of the FUZZY label
3In SMULTRON, sentence alignments are not given di-
rectly, but can be deduced from the set of word alignments.
word phrase
all 540 251
oneword 340 182
shortest 96 21
all-TL 194 140
oneword-TL 130 94
shortest-TL 30 16
Table 1: Number of variations across contexts
alerts us to the fact that it should vary with another
word. The ALIGNMENT TYPE FILTER removes
cases varying between one EXACT label and one
or more FUZZY labels.
5 Evaluation
Evaluation was done for English to German on
half of SMULTRON (the part taken from the novel
Sophie?s World), with approximately 7500 words
from each language and 7600 alignments (roughly
4800 word-level and 2800 phrase-level). Basic
statistics are in Table 1. We filter based on the
target language (TL) and provide three different
contextual definitions: no context, i.e., all varia-
tions (all); one word of context on the left or right
(oneword); and one word of context on the left and
right, i.e., the shortest surrounding context (short-
est). The filters reduce the number of variations,
with a dramatic loss for the shortest contexts.
A main question concerns the impact of the fil-
tering conditions on error detection. To gauge this,
we randomly selected 50 (all) variations for the
word level and 50 for the phrase level, each corre-
sponding to just under 400 corpus instances. The
variations were checked manually to see which
were true variations and which were errors.
We report the effect of different filters on preci-
sion and recall in Table 2, where recall is with re-
spect to the all condition.4 Adding too much lexi-
cal context in the source language (i.e., the short-
est conditions) results in too low a recall to be
practically effective. Using one word of context
on either side has higher recall, but the precision
is no better than using no source language con-
text at all. What seems to be most effective is to
only use the target language filter (all-TL). Here,
we find higher precision?higher than any source
language filter?and the recall is respectable.
4Future work should test for recall of all alignment errors,
by first manually checking a small section of the corpus.
42
Word Phrase
Cases Errors P R Cases Errors P R
all 50 17 34% 100% 50 15 30% 100%
oneword 33 12 36% 71% 33 8 24% 53%
shortest 8 2 25% 12% 4 1 25% 7%
all-TL 20 11 55% 65% 27 12 44% 80%
oneword-TL 15 6 40% 35% 14 7 50% 47%
shortest-TL 2 1 50% 6% 3 1 33% 7%
Table 2: Error precision and recall
TL filter An advantage of the target language
filter is its ability to handle lexical (e.g., case) vari-
ations. One example of this is the English phrase
a dog, which varies between German einem Hund
(dative singular), einen Hund (accusative singu-
lar) and Hunde (accusative plural). Similar to us-
ing lower-case labels, one could map strings to
canonical forms. However, the target language
filter naturally eliminates such unwanted varia-
tion, without any language-specific information,
because the other forms do not appear across sen-
tences.
Several of the variations which the target lan-
guage filter incorrectly removes would, once the
error is fixed, still have variation. As an example,
consider cat, which varies between Katze (5 to-
kens) and NIL (2 tokens). In one of the NIL cases,
the word needs to be FUZZY-aligned with the Ger-
man Tigerkatze. The variation points out the error,
but there would still be variation (between Katze,
Tigerkatze, and NIL) after correction. This shows
the limitation of the heuristic in identifying the re-
quired non-exact alignments.
Another case the filter misses is the variation
nucleus heard, which varies between geho?rt (2 to-
kens) and ho?ren (1 token). In this case, one of the
instances of <heard, geho?rt> should be <heard,
geho?rt hatte>. Note that here the erroneous case
is not variation-based at all; it is a problem with
the label geho?rt. What is needed is a method to
detect more translation possibilities.
As an example of a problem for phrases, con-
sider the variation for the nucleus end with 5 in-
stances of NIL and 1 of ein Ende. In one NIL
instance, the proper alignment should be <the
end, Ende>, with a longer source string. Since
the target label is Ende and not ein Ende, the fil-
ter removes this variation. One might explore
more fuzzily matching NIL strings, so that Ende
matches with ein Ende. We explore a different
method for phrases next, which deals with some
of these NIL cases.
6 A Complementary Method
Although it works for any type of aligned corpus,
the string-based variation method of detecting er-
rors is limited in the types of errors it can de-
tect. There might be ways to generalize the vari-
ation n-gram method (cf. Dickinson, 2008), but
this does not exploit properties inherent to aligned
treebanks. We pursue a complementary approach,
as this can fill in some gaps a string-based method
cannot deal with (cf. Loftsson, 2009).
6.1 Phrase Alignment Based on Word Links
Using the existing word alignments, we can search
for missing or erroneous phrase alignments. If
the words dominated by a phrase are aligned, the
phrases generally should be, too (cf. Lavie et al,
2008). We take the yield of a constituent in one
side of a corpus, find the word alignments of this
yield, and use these alignments to predict a phrasal
alignment for the constituent. If the predicted
alignment is not annotated, it is flagged as a possi-
ble error. This is similar to the branch link locality
of the TreeAligner (see section 2.2), but here as a
prediction, rather than a restriction, of alignment.
For example, consider the English VP choose
her own friends in (1). Most of the words are
aligned to words within Ihre Freunde vielleicht
wa?hlen (?possibly choose her friends?), with no
alignment to words outside of this German VP. We
want to predict that the phrases be aligned.
(1) a. [VP choose1 her2 own friends3 ]
b. [VP Ihre2 Freunde3 vielleicht wa?hlen1 ]
The algorithm works as follows:
1. For every phrasal node s in the source treebank:
(a) Predict a target phrase node t to align with,
where t could be non-alignment (NIL):
43
i. Obtain the yield (i.e., child nodes) of the
phrase node s: s1 , ... sn .
ii. Obtain the alignments for each child node
si , resulting in a set of child nodes in the
target language (t1 , ... tm ).
iii. Store every mother node t? covering all the
target child nodes, i.e., all <s, t?> pairs.
(b) If a predicted alignment (<s, t?>) is not in the
set of actual alignments (<s, t>), add it to the
set of potential alignments, AS 7?T .
i. For nodes which are predicted to have non-
alignment (but are actually aligned), output
them to a separate file.
2. Perform step 1 with the source and target reversed,
thereby generating both AS 7?T and AT 7?S .
3. Intersect AS 7?T and AT 7?S , to obtain the set of pre-
dicted phrasal alignments not currently aligned.
The main idea in 1a is to find the children of a
source node and their alignments and then obtain
the target nodes which have all of these aligned
nodes as children. A node covering all these target
children is a plausible candidate for alignment.
Consider example (2). Within the 8-word En-
glish ADVP (almost twice . . . ), there are six words
which align to words in the corresponding Ger-
man sentence, all under the same NP.5 It does not
matter that some words are unaligned; the fact
that the English ADVP and the German NP cover
basically the same set of words suggests that the
phrases should be aligned, as is the case here.
(2) a. Sophie lived on2 [NP1 the2 outskirts3 of a4
sprawling5? suburb6?] and had [ADVP almost7
twice8 as9 far10 to school as11 Joanna12?] .
b. Sophie wohnte am2 [NP1 Ende3 eines4
ausgedehnten5? Viertels6? mit Einfam-
ilienha?usern] und hatte [NP einen fast7
doppelt8 so9 langen10 Schulweg wie11
Jorunn12?] .
The prediction of an aligned node in 1a allows
for multiple possibilities: in 1aiii, we only check
that a mother node t? covers all the target children,
disregarding extra children, since translations can
contain extra words. In general, many such dom-
inating nodes exist, and most are poor candidates
for alignment of the node in question. This is the
reason for the bidirectional check in steps 2 and 3.
For example, in (3), we correctly predict align-
ment between the NP dominating you in English
and the NP dominating man in German. From
the word alignment, we generate a list of mother
5FUZZY labels are marked by an asterisk, but are not used.
nodes of man as potential alignments for the you
NP. Two of these (six) nodes are shown in (3b).
In the other direction, there are eight nodes con-
taining you; two are shown in (3a). These are the
predicted alignment nodes for the NP dominating
man. In either direction, this overgenerates; the
intersection, however, only contains alignment be-
tween the lowest NPs.
(3) a. But it ?s just as impossible to realize [S [NP
you1 ] have to die without thinking how incred-
ibly amazing it is to be alive ] .
b. [S Und es ist genauso unmo?glich , daru?ber
nachzudenken , dass [NP man1 ] sterben muss
, ohne zugleich daran zu denken , wie phan-
tastisch das Leben ist . ]
While generally effective, certain predictions
are less likely to be errors. In figure 3, for ex-
ample, the sentence pair is an entire rephrasing;
<her, ihr> is the only word alignment. For each
phrasal node in the SL, the method only requires
that all its words be aligned with the words under
the TL node. Thus, the English PP on her, the VP
had just been dumped on her, and the two VPs in
between are predicted as possible alignments with
the German VP ihr einfach in die Wiege gelegt
worden or its immediate VP daughter: they all
have her and ihr aligned, and no contradicting
alignments. Sparse word alignments lead to mul-
tiple possible phrase alignments. After intersect-
ing, we mark cases with more than one predicted
source or target phrase and do not evaluate them.
If in step 1aiii, no target mother (t?) exists, but
there is alignment in the corpus, then in step 1bi,
we output predicted non-alignment. In Example
(2), for instance, the English NP the outskirts of
a sprawling suburb is (incorrectly) predicted to
have no alignment, although most words align to
words within the same German NP. This predic-
tion arises because the aligns to a word (am) out-
side of the German NP, due to am being a contrac-
tion of the preposition an and the article dem, (cf.
on and the, respectively). The method for predict-
ing phrase alignments, however, relies upon words
being within the constituent. We thus conclude
that: 1) the cases in step 1bi are unlikely to be er-
rors, and 2) there are types of alignments which
we simply will not find, a problem also for au-
tomatic alignment based on similar assumptions
(e.g., Zhechev and Way, 2008). In (2), for in-
stance, were there not already alignment between
44
.$.bl?htenVFlINFA bP?RightNNTiGiT?hhNNru?NNdO irgslmutFh rgtNNO RritOK zriTiVV
..ci?NON$ GPPDIVVA ulRhJF voI?OJ kiighJV RofSiRhJV aaaVwVda PgnV ui?NON
B
cF
cFNP
AJ
cF
cF
cFNP
Ft
cFASP
-w
cF
VE cFNP
VEPP
-wSP
w* SP
w*V
NP
AJH
ARSP NP NP
PP*MOSP
SP
SP
V
Figure 3: A sentence with minimal alignment
the NPs, we would not predict it.
6.2 Evaluation
The method returns 318 cases, in addition to 135
cases with multiple source/target phrases and 104
predicted non-alignments. To evaluate, we sam-
pled 55 of the 318 flagged phrases and found that
25 should have been aligned as suggested. 21
of the phrases have zero difference in length be-
tween source and target, while 34 have differences
of up to 9 tokens. Of the phrases with zero-
length difference, 18 should have been aligned
(precision=85.7%), while only 7 with length dif-
ferences should have been aligned. This is in line
with previous findings that length difference can
help predict alignment (cf., e.g., Gale and Church,
1993). About half of all phrase pairs that should
be aligned should be EXACT, regardless of the
length difference.
The method is good at predicting the alignment
of one-word phrases, e.g., pronouns, as in (3). Of
the 11 suggested alignments where both source
and target have a length of 1, all were correct sug-
gestions. This is not surprising, since all words
under the phrases are (trivially) aligned. Although
shorter phrases with short length differences gen-
erally means a higher rate of correct suggestions,
we do not want to filter out items based on phrase
length, since there are outliers that are correct sug-
gestions, e.g., phrase pairs with lengths of 15 and
13 (difference=2) or 31 and 36 (difference=5). It
is worth noting that checking the suggestions took
very little time.
7 Summary and Outlook
This paper explores two simple, language-
independent ways to detect errors in aligned cor-
pora. In the first method, applicable to any aligned
corpus, we consider alignment as a string-to-string
mapping, where a string could be the yield of a
phrase. Treating the target string as a label, we
find inconsistencies in the labeling of each source
string. Despite setting the problem up in a similar
way to grammatical annotation, we also demon-
strated that new heuristics are needed to sort er-
rors. The second method examines phrase nodes
which are predicted to be aligned, based on the
alignment of their yields. Both methods are ef-
fective, in complementary ways, and can be used
to suggest alignments for annotators or to suggest
revisions for incorrect alignments.
The wide range of possible translations and the
linguistic information which goes into them indi-
cate that there should be other ways of finding er-
rors. One possibility is to use more abstract source
or target language representations, such as POS,
to overcome the limitations of string-based meth-
ods. This will likely also be a useful avenue to
explore for language pairs more dissimilar than
English and German. By investigating different
ways to ensure alignment consistency, one can be-
gin to provide insights into automatic alignment
(Zhechev and Way, 2008). Additionally, by cor-
recting the errors, one can determine the effect on
machine translation evaluation.
Acknowledgments
We would like to thank Martin Volk and Thorsten
Marek for useful discussion and feedback of ear-
lier versions of this paper and three anonymous
reviewers for their comments.
45
References
Botley, S. P., McEnery, A. M., and Wilson, A.,
editors (2000). Multilingual Corpora in Teach-
ing and Research. Rodopi, Amsterdam, Atlanta
GA.
Dickinson, M. (2008). Representations for cat-
egory disambiguation. In Proceedings of
COLING-08, pages 201?208, Manchester.
Dickinson, M. and Meurers, W. D. (2003). Detect-
ing inconsistencies in treebanks. In Proceed-
ings of TLT-03, pages 45?56, Va?xjo?, Sweden.
Dickinson, M. and Meurers, W. D. (2005). De-
tecting errors in discontinuous structural anno-
tation. In Proceedings of ACL-05, pages 322?
329.
Gale, W. A. and Church, K. W. (1993). A pro-
gram for aligning sentences in bilingual cor-
pora. Computational Linguistics, 19(1):75?
102.
Gustafson-C?apkova?, S., Samuelsson,
Y., and Volk, M. (2007). SMUL-
TRON (version 1.0) - The Stock-
holm MULtilingual parallel TReebank.
www.ling.su.se/dali/research/smultron/index.htm.
Habash, N., Gabbard, R., Rambow, O., Kulick, S.,
and Marcus, M. (2007). Determining case in
Arabic: Learning complex linguistic behavior
requires complex linguistic features. In Pro-
ceedings of EMNLP-CoNLL-07, pages 1084?
1092.
Lavie, A., Parlikar, A., and Ambati, V. (2008).
Syntax-driven learning of sub-sentential trans-
lation equivalents and translation rules from
parsed parallel corpora. In Proceedings of the
ACL-08: HLT Second Workshop on Syntax and
Structure in Statistical Translation (SSST-2),
pages 87?95, Columbus, OH.
Loftsson, H. (2009). Correcting a POS-tagged
corpus using three complementary methods.
In Proceedings of EACL-09, pages 523?531,
Athens, Greece.
McEnery, T. and Wilson, A. (1996). Corpus Lin-
guistics. Edinburgh University Press, Edin-
burgh.
Meurers, D. and Mu?ller, S. (2008). Corpora
and syntax (article 44). In Lu?deling, A. and
Kyto?, M., editors, Corpus Linguistics. An In-
ternational Handbook, Handbooks of Linguis-
tics and Communication Science. Mouton de
Gruyter, Berlin.
Och, F. J. and Ney, H. (2003). A systematic com-
parison of various statistical alignment models.
Computational Linguistics, 29(1):19?51.
Okita, T. (2009). Data cleaning for word align-
ment. In Proceedings of the ACL-IJCNLP 2009
Student Research Workshop, pages 72?80, Sun-
tec, Singapore.
Padro, L. and Marquez, L. (1998). On the eval-
uation and comparison of taggers: the effect
of noise in testing corpora. In Proceedings of
ACL-COLING-98, pages 997?1002, San Fran-
cisco, California.
Taylor, A., Marcus, M., and Santorini, B. (2003).
The penn treebank: An overview. In Abeille?,
A., editor, Treebanks: Building and using syn-
tactically annotated corpora, chapter 1, pages
5?22. Kluwer, Dordrecht.
Tiedemann, J. (2003). Recycling Translations -
Extraction of Lexical Data from Parallel Cor-
pora and their Application in Natural Lan-
guage Processing. PhD thesis, Uppsala univer-
sity.
Ule, T. and Simov, K. (2004). Unexpected pro-
ductions may well be errors. In Proceedings of
LREC-04, Lisbon, Portugal.
Volk, M., Lundborg, J., and Mettler, M. (2007).
A search tool for parallel treebanks. In Pro-
ceedings of the Linguistic Annotation Workshop
(LAW) at ACL, pages 85?92, Prague, Czech Re-
public. Association for Computational Linguis-
tics.
Zhechev, V. and Way, A. (2008). Automatic gen-
eration of parallel treebanks. In Proceedings
of Coling 2008, pages 1105?1112, Manchester,
UK.
46
Proceedings of the Sixth Workshop on Innovative Use of NLP for Building Educational Applications, pages 81?86,
Portland, Oregon, 24 June 2011. c?2011 Association for Computational Linguistics
Developing Methodology for Korean Particle Error Detection
Markus Dickinson
Indiana University
md7@indiana.edu
Ross Israel
Indiana University
raisrael@indiana.edu
Sun-Hee Lee
Wellesley College
slee6@wellesley.edu
Abstract
We further work on detecting errors in post-
positional particle usage by learners of Korean
by improving the training data and develop-
ing a complete pipeline of particle selection.
We improve the data by filtering non-Korean
data and sampling instances to better match
the particle distribution. Our evaluation shows
that, while the data selection is effective, there
is much work to be done with preprocessing
and system optimization.
1 Introduction
A growing area of research in analyzing learner lan-
guage is to detect errors in function words, namely
categories such as prepositions and articles (see Lea-
cock et al, 2010, and references therein). This work
has mostly been for English, and there are issues,
such as greater morphological complexity, in mov-
ing to other languages (see, e.g., de Ilarraza et al,
2008; Dickinson et al, 2010). Our goal is to build a
machine learning system for detecting errors in post-
positional particles in Korean, a significant source of
learner errors (Ko et al, 2004; Lee et al, 2009b).
Korean postpositional particles are morphemes
that attach to a preceding nominal to indicate a range
of linguistic functions, including grammatical func-
tions, e.g., subject and object; semantic roles; and
discourse functions. In (1), for instance, ka marks
the subject (function) and agent (semantic role).1
Similar to English prepositions, particles can also
have modifier functions, adding meanings of time,
location, instrument, possession, and so forth.
1We use the Yale Romanization scheme for writing Korean.
(1) Sumi-ka
Sumi-SBJ
John-uy
John-GEN
cip-eyse
house-LOC
ku-lul
he-OBJ
twu
two
sikan-ul
hours-OBJ
kitaly-ess-ta.
wait-PAST-END
?Sumi waited for John for (the whole) two hours in
his house.?
We treat the task of particle error detection as
one of particle selection, and we use machine learn-
ing because it has proven effective in similar tasks
for other languages (e.g., Chodorow et al, 2007;
Oyama, 2010). Training on a corpus of well-formed
Korean, we predict which particle should appear af-
ter a given nominal; if this is different from the
learner?s, we have detected an error. Using a ma-
chine learner has the advantage of being able to per-
form well without a researcher having to specify
rules, especially with the complex set of linguistic
relationships motivating particle selection.2
We build from Dickinson et al (2010) in two
main ways: first, we implement a presence-selection
pipeline that has proven effective for English prepo-
sition error detection (cf. Gamon et al, 2008). As
the task is understudied, the work is preliminary, but
it nonetheless is able to highlight the primary ar-
eas of focus for future work. Secondly, we improve
upon the training data, in particular doing a better
job of selecting relevant instances for the machine
learner. Obtaining better-quality training data is a
major issue for machine learning applied to learner
language, as the domain of writing is different from
news-heavy training domains (Gamon, 2010).
2See Dickinson and Lee (2009); de Ilarraza et al (2008);
Oyama (2010) for related work in other languages.
81
2 Particle error detection
2.1 Pre-processing
Korean is an agglutinative language: Korean words
(referred to as ecels) are usually composed of a
root with a number of functional affixes. We thus
first segment and POS tag the text, for both train-
ing and testing, using a hybrid (trigram + rule-
based) morphological tagger for Korean (Han and
Palmer, 2004). The tagger is designed for native
language and is not optimized to make guesses for
ill-formed input. While the POS tags assigned to the
learner corpus are thus often incorrect (see Lee et al,
2009a), there is the more primary problem of seg-
mentation, as discussed in more detail in section 4.
2.2 Machine learning
We use the Maximum Entropy Toolkit (Le, 2004)
for machine learning. Training on a corpus of well-
formed Korean, we predict which particle should ap-
pear after a given nominal; if this is different from
what the learner used, we have detected an error. It
is important that the data represent the relationships
between specific lexical items: in the comparable
English case, for example, interest is usually found
with in: interest in/*with learning.
Treating the ends of nominal elements as possible
particle slots, we break classification into two steps:
1) Is there a particle? (Yes/No); and 2) What is the
exact particle? Using two steps eases the task of ac-
tual particle prediction: with a successful classifica-
tion of negative and positive instances, there is no
need to handle nominals that have no particle in step
2. To evaluate our parameters for obtaining the most
relevant instances, we keep the task simple and per-
form only step 1, as this step provides information
about the usability of the training data. For actual
system performance, we evaluate both steps.
In selecting features for Korean, we have to ac-
count for relatively free word order (Chung et al,
2010). We follow our previous work (Dickinson
et al, 2010) in our feature choices, using a five-
word window that includes the target stem and two
words on either side for context (see also Tetreault
and Chodorow, 2008). Each word is broken down
into: stem, affixes, stem POS, and affixes POS. We
also have features for the preceding and following
noun and verb, thereby approximating relevant se-
lectional properties. Although these are relatively
shallow features, they provide enough lexical and
grammatical context to help select better or worse
training data (section 3) and to provide a basis for a
preliminary system (section 4).
3 Obtaining the most relevant instances
We need well-formed Korean data in order to train
a machine learner. To acquire this, we use web-
based corpora, as this allows us to find data similar
to learner language, and using web as corpus (WaC)
tools allows us to adjust parameters for new data
(Dickinson et al, 2010). However, the methodology
outlined in Dickinson et al (2010) can be improved
in at least three ways, outlined next.
3.1 Using sub-corpora
Web corpora can be built by searching for a set of
seed terms, extracting documents with those terms
(Baroni and Bernardini, 2004). One way to improve
such corpora is to use better seeds, namely, those
which are: 1) domain-appropriate (e.g., about trav-
eling), and 2) of an appropriate level. In Dickinson
et al (2010), we show that basic terms result in poor
quality Korean, but slightly more advanced terms on
the same topics result in better-formed data.
Rather than use all of the seed terms to create a
single corpus, we divide the seed terms into 13 sep-
arate sets, based on the individual topics from our
learner corpus. The sub-corpora are then combined
to create a cohesive corpus covering all the topics.
For example, we use 10 Travel words to build a
subcorpus, 10 Learning Korean words for a differ-
ent subcorpus, and so forth. This means that terms
appropriate for one topic are not mixed with terms
for a different topic, ensuring more coherent web
documents. Otherwise, we might obtain a Health
Management word, such as pyengwen (?hospital?),
mixed with a Generation Gap word, such as kaltung
(?conflict?)?in this case, leading to webpages on
war, a topic not represented in our learner corpus.
3.2 Filtering
One difficulty with our web corpora is that some of
them have large amounts of other languages along
with Korean. The keywords are in the corpora, but
there is additional text, often in Chinese, English, or
Japanese. These types of pages are unreliable for
82
our purposes, as they may not exhibit natural Ko-
rean. By using a simple filter, we check whether a
majority of the characters in a webpage are indeed
from the Korean writing system, and remove pages
beneath a certain threshold.
3.3 Instance sampling
Particles are often dropped in colloquial and even
written Korean, whereas learners are more often
required to use them. It is not always the case
that the web pages contain the same ratio of par-
ticles as learners are expected to use. To alleviate
this over-weighting of having no particle attached
to a noun, we propose to downsample our corpora
for the machine learning experiments, by remov-
ing a randomly-selected proportion of (negative) in-
stances. Instance sampling has been effective for
other NLP tasks, e.g., anaphora resolution (Wunsch
et al, 2009), when the number of negative instances
is much greater than the positive ones. In our web
corpora, nouns have a greater than 50% chance of
having no particle; in section 3.4, we thus downsam-
ple to varying amounts of negative instances from
about 45% to as little as 10% of the total corpus.
3.4 Training data selection
In Dickinson et al (2010), we used a Korean learner
data set from Lee et al (2009b) for development. It
contains 3198 ecels, 1842 of which are nominals,
and 1271 (?70%) of those have particles. We use
this same corpus for development, to evaluate filter-
ing and down-sampling. Evaluating on (yes/no) par-
ticle presence, in tables 1 and 2, recall is the percent-
age of positive instances we correctly find and pre-
cision is the percentage of instances that we classify
as positive that actually are. A baseline of always
guessing a particle gives 100% recall, 69% preci-
sion, and 81.7% F-score.
Table 1 shows the results of the MaxEnt system
for step 1, using training data built for the topics in
the data with filter thresholds of 50%, 70%, 90%,
and 100%?i.e., requiring that percentage of Korean
characters?as well as the unfiltered corpus. The
best F-score is with the filter set at 90%, despite the
size of the filtered corpus being smaller than the full
corpus. Accordingly, we use the 90% filter on our
training corpus for the experiments described below.
Threshold 100% 90% 70% 50% Full
Ecel 67k 9.6m 10.3m 11.1m 12.7m
Instances 37k 5.8m 6.3m 7.1m 8.4m
Accuracy 74.75 81.11 74.64 80.29 80.46
Precision 80.03 86.14 79.65 85.41 85.56
Recall 84.50 86.55 84.97 86.15 86.23
F-score 82.20 86.34 82.22 85.78 85.89
Table 1: Step 1 (particle presence) results with filters
The results for instance sampling are given in ta-
ble 2. We experiment with positive to negative sam-
pling ratios of 1.3/1 (?43% negative instances), 2/1
(?33%), 4/1 (?20%), and 10/1 (?10%). We select
the 90% filter, 1.3/1 downsampling settings and ap-
ply them to the training corpus (section 3.1) for all
experiments below.
P/N ratio 10/1 4/1 2/1 1.3/1 1/1.05
Instances 3.1m 3.5m 4.3m 5m 5.8m
Accuracy 74.75 77.85 80.23 81.59 81.11
Precision 73.38 76.72 80.75 84.26 86.14
Recall 99.53 97.48 93.71 90.17 86.55
F-score 84.47 85.86 86.74 87.12 86.34
Table 2: Step 1 (presence) results with instance sampling
One goal has been to improve the web as corpus
corpus methodology for training a machine learning
system. The results in tables 1 and 2 reinforce our
earlier finding that size is not necessarily the most
important variable in determining the usefulness or
overall quality of data collected from the web for
NLP tasks (Dickinson et al, 2010). Indeed, the cor-
pus producing best results (90% filter, 1.3:1 down-
sampling) is more than 3 million instances smaller
than the unfiltered, unsampled corpus.
4 Initial system evaluation
We have obtained an annotated corpus of 25 essays
from heritage intermediate learners,3 with 299 sen-
tences and 2515 ecels (2676 ecels after correcting
spacing errors). There are 1138 nominals, with 93
particle errors (5 added particles, 35 omissions, 53
substitutions)?in other words, less than 10% of par-
ticles are errors. There are 979 particles after cor-
rection. We focus on 38 particles that intermediate
3Heritage learners have had exposure to Korean at a young
age, such as growing up with Korean spoken at home.
83
students can be reasonably expected to use. A parti-
cle is one of three types (cf. Nam and Ko, 2005): 1)
case markers, 2) adverbials (cf. prepositions), and
3) auxiliary particles.4
Table 3 gives the results for the entire system on
the test corpus, with separate results for each cat-
egory of particle, (Case, Adv., and Aux.) as well
as the concatenation of the three (All). The ac-
curacy presented here is in terms of only the par-
ticle in question, as opposed to the full form of
root+particle(s). Step 2 is presented in 2 ways: Clas-
sified, meaning that all of the instances classified as
needing a particle by step 1 are processed, or Gold,
in which we rely on the annotation to determine par-
ticle presence. It is not surprising, then, that Gold
experiments are more accurate than Classified ex-
periments, due to step 1 errors and also preprocess-
ing issues, discussed next.
Step 1 Step 2
Data # Classified Gold
Case 504 95.83% 71.23% 72.22%
Adv. 205 82.43% 30.24% 32.68%
Aux. 207 89.37% 31.41% 35.74%
All 916 91.37% 53.05% 55.13%
Table 3: Accuracy for step 1 (particle presence) & step 2
(particle selection), with number (#) of instances
Preprocessing For the particles we examine, there
are 135 mis-segmented nominals. The problem is
more conspicuous if we look at the entire corpus:
the tagger identifies 1547 nominal roots, but there
are only 1138. Some are errors in segmentation, i.e.,
mis-identifying the proper root of the ecel, and some
are problems with tagging the root, e.g., a nominal
mistagged as a verb. Table 4 provides results divided
by cases with only correctly pre-processed ecels and
where the target ecel has been mis-handled by the
tagger. This checks whether the system particle is
correct, ignoring whether the whole form is correct;
if full-form accuracy is considered, we have no way
to get the 135 inaccurate cases correct.
Error detection While our goal now is to estab-
lish a starting point, the ultimate, on-going goal of
4Full corpus details will be made available at: http://
cl.indiana.edu/?particles/.
Step 1 Step 2
Data # Classified Gold
Accurate 781 94.24% 55.95% 58.13%
Inaccurate 135 74.81% 36.29% 38.51%
Table 4: Overall accuracy divided by accurate and inac-
curate preprocessing
Case Adv. Aux. All
Precision 28.82% 7.69% 5.51% 15.45%
Recall 87.50% 100% 77.78% 88.00%
Table 5: Error detection (using Gold step 1)
this work is to develop a robust system for automati-
cally detecting errors in learner data. Thus, it is nec-
essary to measure our performance at actually find-
ing the erroneous instances extracted from our test
corpus. Table 5 provides results for step 2 in terms
of our ability to detect erroneous instances. We re-
port precision and recall, calculated as in figure 1.
From the set of erroneous instances:
True Positive (TP) ML class 6= student class
False Negative (FN) ML class = student class
From the set of correct instances:
False Positive (FP) ML class 6= student class
True Negative (TN) ML class = student class
Precision (P) TPTP+FP
Recall (R) TPTP+FN
Figure 1: Precision and recall for error detection
4.1 Discussion and Outlook
One striking aspect about the results in table 3 is the
gap in accuracy between case particles and the other
two categories, particularly in step 2. This points at
a need to develop independent systems for each type
of particle, each relying on different types of linguis-
tic information. Auxiliary particles, for example, in-
clude topic particles which?similar to English arti-
cles (Han et al, 2006)?require discourse informa-
tion to get correct. Still, as case particles comprise
more than half of all particles in our corpus, the sys-
tem is already potentially useful to learners.
Comparing the rows in table 4, the dramatic drop
in accuracy when moving to inaccurately-processed
84
cases shows a clear need for preprocessing adapted
to learner data. While it is disconcerting that nearly
15% (135/916) of the cases have no chance of re-
sulting in a correct full form, the results indicate that
we can obtain reliable accuracy (cf. 94.24%) for pre-
dicting particle presence across all types of particles,
assuming good morphological tagging.
From table 5, it is apparent that we are overguess-
ing errors; recall that only 10% of particles are er-
roneous, whereas we more often guess a different
particle. While this tendency results in high recall,
a tool for learners should have higher precision, so
that correct usage is not flagged. However, this is
a first attempt at error detection, and simply know-
ing that precision is low means we can take steps
to solve this deficiency. Our training data may have
too many possible classes in it, and we have not yet
accounted for phonological alternations; e.g. if the
system guesses ul when lul is correct, we count a
miss, even though they are different realizations of
the same morpheme.
To try and alleviate the over-prediction of errors,
we have begun to explore implementing a confi-
dence filter. As a first pass, we use a simple fil-
ter that compares the probability of the best parti-
cle to the probability of the particle the learner pro-
vided; the absolute difference in probabilities must
be above a certain threshold. Table 6 provides the er-
ror detection results for each type of particle, incor-
porating confidence filters of 10%, 20%, 30%, 40%,
50%, and 60%. The results show that increasing the
threshold at which we accept the classifier?s answer
can significantly increase precision, at the cost of re-
call. As noted above, higher precision is desirable,
so we plan on further developing this confidence fil-
ter. We may also include heuristic-based filters, such
as the ones implemented in Criterion (see Leacock
et al, 2010), as well as a language model approach
(Gamon et al, 2008).
Finally, we are currently working on improving
the POS tagger, testing other taggers in the pro-
cess, and developing optimal feature sets for differ-
ent kinds of particles.
Acknowledgments
We would like to thank the IU CL discussion group
and Joel Tetreault for feedback at various points.
Adv Aux Case All
10
% P 10.0% 6.3% 29.9% 16.3%
R 100% 77.8% 67.8% 73.3%
20
% P 13.5% 7.8% 32.6% 18.0%
R 100% 77.8% 50.0% 60.0%
30
% P 20.0% 8.3% 36.1% 20.8%
R 100% 66.7% 39.3% 50.7%
40
% P 19.4% 14.3% 48.6% 26.9%
R 60.0% 66.7% 30.4% 38.7%
50
% P 23.1% 16.7% 57.9% 32.1%
R 30.0% 44.4% 19.6% 24.0%
60
% P 40.0% 26.7% 72.3% 45.2%
R 20.0% 44.4% 14.3% 18.7%
Table 6: Error detection with confidence filters
References
Marco Baroni and Silvia Bernardini. 2004. Bootcat:
Bootstrapping corpora and terms from the web. In
Proceedings of LREC 2004, pages 1313?1316.
Martin Chodorow, Joel Tetreault, and Na-Rae Han.
2007. Detection of grammatical errors involv-
ing prepositions. In Proceedings of the 4th ACL-
SIGSEM Workshop on Prepositions, pages 25?30.
Prague.
Tagyoung Chung, Matt Post, and Daniel Gildea.
2010. Factors affecting the accuracy of korean
parsing. In Proceedings of the NAACL HLT
2010 First Workshop on Statistical Parsing of
Morphologically-Rich Languages, pages 49?57.
Los Angeles, CA, USA.
Arantza D??az de Ilarraza, Koldo Gojenola, and
Maite Oronoz. 2008. Detecting erroneous uses
of complex postpositions in an agglutinative lan-
guage. In Proceedings of COLING-08. Manch-
ester.
Markus Dickinson, Ross Israel, and Sun-Hee Lee.
2010. Building a korean web corpus for analyz-
ing learner language. In Proceedings of the 6th
Workshop on the Web as Corpus (WAC-6). Los
Angeles.
Markus Dickinson and Chong Min Lee. 2009. Mod-
ifying corpus annotation to support the analysis of
learner language. CALICO Journal, 26(3).
Michael Gamon. 2010. Using mostly native data
85
to correct errors in learners? writing. In Human
Language Technologies: The 2010 Annual Con-
ference of the North American Chapter of the
Association for Computational Linguistics, pages
163?171. Los Angeles, California.
Michael Gamon, Jianfeng Gao, Chris Brockett,
Alexander Klementiev, William Dolan, Dmitriy
Belenko, and Lucy Vanderwende. 2008. Using
contextual speller techniques and language mod-
eling for esl error correction. In Proceedings of
IJCNLP. Hyderabad, India.
Chung-Hye Han and Martha Palmer. 2004. A mor-
phological tagger for korean: Statistical tagging
combined with corpus-based morphological rule
application. Machine Translation, 18(4):275?
297.
Na-Rae Han, Martin Chodorow, and Claudia Lea-
cock. 2006. Detecting errors in english article us-
age by non-native speakers. Natural Language
Engineering, 12(2).
S. Ko, M. Kim, J. Kim, S. Seo, H. Chung, and
S. Han. 2004. An analysis of Korean learner cor-
pora and errors. Hanguk Publishing Co.
Zhang Le. 2004. Maximum Entropy Mod-
eling Toolkit for Python and C++. URL
http://homepages.inf.ed.ac.uk/
s0450736/maxent_toolkit.html.
Claudia Leacock, Martin Chodorow, Michael Ga-
mon, and Joel Tetreault. 2010. Automated Gram-
matical Error Detection for Language Learners.
Synthesis Lectures on Human Language Tech-
nologies. Morgan & Claypool.
Chong Min Lee, Soojeong Eom, and Markus Dick-
inson. 2009a. Towards analyzing korean learner
particles. Talk given at CALICO ?09 Pre-
Conference Workshop on Automatic Analysis of
Learner Language. Tempe, AZ.
Sun-Hee Lee, Seok Bae Jang, and Sang kyu Seo.
2009b. Annotation of korean learner corpora for
particle error detection. CALICO Journal, 26(3).
Ki-shim Nam and Yong-kun Ko. 2005. Korean
Grammar (phyocwun kwuke mwunpeplon). Top
Publisher, Seoul.
Hiromi Oyama. 2010. Automatic error detection
method for japanese particles. Polyglossia, 18.
Joel Tetreault and Martin Chodorow. 2008. The ups
and downs of preposition error detection in esl
writing. In Proceedings of COLING-08. Manch-
ester.
Holger Wunsch, Sandra Ku?bler, and Rachael
Cantrell. 2009. Instance sampling methods for
pronoun resolution. In Proceedings of RANLP
2009. Borovets, Bulgaria.
86
The 7th Workshop on the Innovative Use of NLP for Building Educational Applications, pages 95?104,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Predicting Learner Levels for Online Exercises of Hebrew
Markus Dickinson, Sandra Ku?bler, Anthony Meyer
Indiana University
Bloomington, IN, USA
{md7,skuebler,antmeyer}@indiana.edu
Abstract
We develop a system for predicting the level of
language learners, using only a small amount
of targeted language data. In particular, we
focus on learners of Hebrew and predict level
based on restricted placement exam exercises.
As with many language teaching situations, a
major problem is data sparsity, which we ac-
count for in our feature selection, learning al-
gorithm, and in the setup. Specifically, we de-
fine a two-phase classification process, isolat-
ing individual errors and linguistic construc-
tions which are then aggregated into a second
phase; such a two-step process allows for easy
integration of other exercises and features in
the future. The aggregation of information
also allows us to smooth over sparse features.
1 Introduction and Motivation
Several strands of research in intelligent computer-
assisted language learning (ICALL) focus on deter-
mining learner ability (Attali and Burstein, 2006;
Yannakoudakis et al, 2011). One of the tasks, de-
tecting errors in a range of languages and for a range
of types of errors, is becoming an increasingly popu-
lar topic (Rozovskaya and Roth, 2011; Tetreault and
Chodorow, 2008); see, for example, the recent HOO
(Helping Our Own) Challenge for Automated Writ-
ing Assistance (Dale and Kilgarriff, 2011). Only
rarely has there been work on detecting errors in
more morphologically-complex languages (Dickin-
son et al, 2011).
In our work, we extend the task to predicting the
learner?s level based on the errors, focusing on He-
brew. Our system is targeted to be used in a uni-
versity setting where incoming students need to be
placed into the appropriate language level?i.e., the
appropriate course?based on their proficiency in
the language. Such a level prediction system for He-
brew faces a number of challenges: 1) unclear cor-
respondence between errors and levels, 2) missing
NLP resources, and, most critically, 3) data sparsity.
Placing learners into levels is generally done by
a human, based on a written exam (e.g. (Fulcher,
1997)). To model the decision process automati-
cally, we need to understand how the types of er-
rors, as well as their frequencies, correspond to
learner levels. There is only little work investigat-
ing this correspondence formally (see (Hawkins and
Filipovic?, 2010; Alexopoulou et al, 2010) for dis-
cussion) and only on error-annotated English learner
corpora. For this reason, we follow a data-driven
approach to learn the correspondence between er-
rors and levels, based on exercises from written
placement exams. Although the exact exercises will
vary across languages and language programs, the
methodology is widely applicable, as developing
a small set of exercises requires minimal effort?
effort already largely expended for paper exams.
Currently, we focus on an exercise in which the
learner has to order a set of words into a grammat-
ical sentence. Our goal is to move towards freer
language production and to analyze language pro-
ficiency through more variables, but, in the interest
of practicality, we start in a more restricted way.
For lesser-resourced languages, there is generally
little data and few NLP resources available. For He-
brew, for example, we must create our own pool of
95
learner data, and while NLP tools and resources ex-
ist (Goldberg and Elhadad, 2011; Yona and Wintner,
2008; Itai and Wintner, 2008), they are not adapted
for dealing with potentially ill-formed learner pro-
ductions. For this reason, we are performing linguis-
tic analysis on the gold standard answers to obtain
optimal linguistic analyses. Then, the system aligns
the learner answer to the gold standard answer and
determines the types of deviations.
Since Hebrew is a less commonly taught language
(LCTL), we have few placement exams from which
to learn correspondences. Compounding the data
sparsity problem is that each piece of data is com-
plex: if a learner produces an erroneous answer,
there are potentially a number of ways to analyze it
(cf. e.g. (Dickinson, 2011)). An error could feature,
for instance, a letter inserted in an irregular verb
stem, or between two nouns; any of these proper-
ties may be relevant to describing the error (cf. how
errors are described in different taxonomies, e.g.
(D??az-Negrillo and Ferna?ndez-Dom??nguez, 2006;
Boyd, 2010)). Specific error types are unlikely to
recur, making sparsity even more of a concern. We
thus need to develop methods which generalize well,
finding the most useful aspects of the data.
Our system is an online system to be used at the
Hebrew Language Program at our university. The
system is intended to semi-automatically place in-
coming students into the appropriate Hebrew course,
i.e., level. As with many exams, the main purpose is
to ?reduce the number of students who attend an oral
interview? (Fulcher, 1997).
2 The Data
Exercise type We focus on a scrambled sentence
exercise, in which learners are given a set of well-
formed words and must put them into the correct or-
der. For example, given (1), they must produce one
of the correct choices in (2). This gives them the
opportunity to practice skills in syntactic ordering.1
(1) barC beph dibrw hybrit ieral la tmid
(2) a. la
not
tmid
always
dibrw
spoke
beph
in-the-language
hybrit
the-Hebrew
barC
in-land-of
ieral
Israel
.
.
1We follow the transliteration scheme of the Hebrew Tree-
bank (Sima?an et al, 2001).
?They did not always speak in the He-
brew language in the land of Israel.?
b. barC ieral la dibrw tmid beph hybrit .
c. la tmid dibrw barC ieral beph hybrit .
(3) barC ieral la tmid dibrw beph hybriM .
Although the lexical choice is restricted?in that
learners are to select from a set of words?learners
must write the words. Thus, in addition to syntactic
errors, there is possible variation in word form, as in
(3), where hybrit is misspelled as hybriM.
This exercise was chosen because: a) it has been
used on Hebrew placement exams for many years;
and b) the amount of expected answers is con-
strained. Starting here also allows us to focus less
on the NLP preprocessing and more on designing
a machine learning set-up to analyze proficiency.
It is important to note that the proficiency level is
determined by experts looking at the whole exam,
whereas we are currently predicting the proficiency
level on the basis of a single exercise.
Placement exams The data for training and test-
ing is pooled from previous placement exams at our
university. Students who intend to take Hebrew have
in past years been given written placement exams,
covering a range of question types, including scram-
bled sentences. The learners are grouped into the
first to the sixth semester, or they test out. We are
using the following levels: the first four semesters
(100, 150, 200, 250), and anything above (300+).
We use a small set of data?38 learners covering
128 sentences across 11 exercises?all the data that
is available. While this is very small, it is indicative
of the type of situation we expect for resource-poor
languages, and it underscores the need to develop
methods appropriate for data-scarce situations.
(Manual) annotation For each of the 11 unique
exercises, we annotate an ordered list of correct an-
swers, ranked from best to worst. Since Hebrew pos-
sesses free word order, there are between 1 and 10
correct answers per exercise, with an average of 3.4
gold standard answers. The sentences have between
8 and 15 words, with an average of 9.7 words per ex-
ercise. This annotation concerns only the gold stan-
dard answers. It requires minimal effort and needs
to be performed only once per exercise.
96
T09: SURFACE qnw
SEGMENTATION (VB-BR3V qnw)
PRE_PARTICLES -
MAIN_WORD:
INDICES 0,1,2,
TAG VB-BR3V
BINYAN PAAL
INFL_PREFIX -
STEM 0,1,
ROOT 0,1,h,
INFL_SUFFIX 2,
PRO_SUFFIX -
Figure 1: An example annotated word for qnw (?bought?),
token T09 in one particular exercise
To annotate, we note that all the correct answers
share the same set of words, varying in word or-
der and not in morphological properties. Thus,
we store word orders separately from morphologi-
cal annotation, annotating morphology once for all
possible word orders. An example of morpholog-
ical annotation is given in fig. 1 for the verb qnw
(?bought?). Segmentation information is provided
by referring to indices (e.g., STEM 0,1), while TAG
and BINYAN provide morphosyntactic properties.
Since the annotation is on controlled, correct data,
i.e., not potentially malformed learner data, we can
explore automatically annotating exercises in the fu-
ture, as we expect relatively high accuracy.
3 System overview
The overall system architecture is given in fig. 2; the
individual modules are described below. In brief,
we align a learner sentence with the gold standard;
use three specialized classifiers to classify individ-
ual phenomena; and then combine the information
from these classifiers into an overall classifier for the
learner level. This means the classification is per-
formed in two phases: the first phase looks at indi-
vidual phenomena (i.e., errors and other properties);
the second phase aggregates all phenomena of one
learner over all exercises and makes a final decision.
4 Feature extraction
To categorize learners into levels, we first need to ex-
tract relevant information from each sentence. That
is, we need to perform a linguistic and/or error anal-
ysis on each sentence, which can be used for classi-
Learner
sentence
(L)
Alignment
Gold
standard
answers
(G1 . . . G2)
Feature
extraction
Intertoken
errors
Intratoken
errors
Global
features
Intra-
Classifier
Inter-
Classifier
Global-
Classifier
Classified
intra
vectors
Classified
inter
vectors
Classified
global
vectors
Learner
classifier
L? Gi
Figure 2: Overall system architecture (boxes = system
components, circles = data)
fication (sec. 5). Although we extract features for
classification, this analysis could also be used for
other purposes, such as providing feedback.
4.1 Phenomena of interest
We extract features capturing individual phenom-
ena. These can be at the level of individual words,
bigrams of words, or anything up to a whole sen-
tence; and they may represent errors or correctly-
produced language. Importantly, at this stage, each
phenomenon is treated uniquely and is not combined
or aggregated until the second phase (see sec. 5).
While features can be based on individual phe-
nomena of any type, we base our extracted features
largely upon learner errors. Errors have been shown
to have a significant impact on predicting learner
level (Yannakoudakis et al, 2011). To detect errors,
we align the learner sentence with a gold standard
and extract the features. Although we focus on er-
rors, we model some correct language (sec. 4.3.3).
4.2 Token alignment
With a listing of correct answers, we align the
learner sentence to the answer which matches best:
97
We iterate over the correct answers and align learner
tokens with correct tokens, based on the cost of map-
ping one to the other. The aligned sentence is the
one with the lowest overall cost. The cost between a
source token ws and target token wt accounts for:
1. Levenshtein distance between ws & wt (Lev)
2. similiarity between ws & wt (longest common
subsequence (LCSq) & substring (LCSt))
3. displacement between ws & wt (Displ)
This method is reminiscent of alignment ap-
proaches in paraphrasing (e.g. (Grigonyte` et al,
2010)), but note that our problem is more restricted
in that we have the same number of words, and in
most cases identical words. We use different dis-
tance and similarity metrics, to ensure robustness
across different kinds of errors. The third metric is
the least important, as learners can shift tokens far
from their original slot, and thus it is given a low
weight. The only reason to use it at all is to distin-
guish cases where more than one target word is a
strong possibility, favoring the closer one.
The formula for the cost between source and tar-
get words ws and wt is given in (4), where the dis-
tance metrics are averaged and normalized by the
length of the target word wt. This length is also used
to convert the similarity measures into distances, as
in (5). We non-exhaustively tested different weight
distributions on about half the data, and our final set
is given in (6), where slightly less weight is given
for the longest common substring and only a minor
amount for the displacement score.
(4) cost(ws, wt) = ?1Displ(ws, wt) +
?2Lev(ws,wt)+?3dLCSq(ws,wt)+?4dLCSt(ws,wt)
3?len(wt)
(5) dLCS(ws, wt) = len(wt)? LCS(ws, wt)
(6) ?1 = 0.05; ?2 = 1.0; ?3 = 1.0; ?4 = 0.7
In calculating Levenshtein distance, we hand-
created a small table of weights for insertions, dele-
tions, and substitutions, to reflect likely modifica-
tions in Hebrew. All weights can be tweaked in the
future, but we have observed good results thus far.
The total alignment is the one which minimizes
the total cost (7). A is an alignment between the
learner sentence s and a given correct sentence t.
Alignments to NULL have a cost of 0.6, so that
words with high costs can instead align to nothing.
(7) align = argminA
?
(ws,wt)?A cost(ws, wt)
4.3 Extracted features
We extract three different types of features; as these
have different feature sets, we correspondingly have
three different classifiers, as detailed in sec. 5.1.
They are followed by a fourth classifier that tallies
up the results of these three classifiers.
4.3.1 Intra-token features
Based on the token alignments, it is straightfor-
ward to calculate differences within the tokens and
thus to determine values for many features (e.g., a
deleted letter in a prefix). We calculate such intra-
token feature vectors for each word-internal error.
For instance, consider the learner attempt (8b) for
the target in (8a). We find in the learner answer two
intra-token errors: one in hmtnwt (cf. hmtnh), where
the fem.pl. suffix -wt has been substituted for the
fem.sg. ending -h, and another in hnw (cf. qnw),
where h has been substituted for q. These two errors
yield the feature vectors presented as example cases
in table 1.
(8) a. haM
Q
hN
they.FEM
eilmw
paid
hrbh
much
ksP
money
bebil
for
hmtnh1
the-gift
ehN
which-they.FEM
qnw2
bought
?
?
?Did they pay much money for the gift
that they bought??
b. haM hN eilmw hrbh ksP bebil hmtnwt1
ehN hnw2 ?
Features 1 and 11 in table 1 are the POS tags of the
morphemes preceding and following the erroneous
morpheme, respectively. The POS tag of the mor-
pheme containing the error is given by feature 2, and
its person, gender, and number by feature 3. The re-
maining features describe the error itself (f. 6?8), as
well as its word-internal context, i.e., both its left (f.
4?5) and right (f. 9?10) contexts.
The context features refer to individual character
slots, which may or may not be occupied by actual
characters. For example, since the error in hmtnwt
is word-final, its two right-context slots are empty,
hence the ?#? symbol for both features 9 and 10.
The feature values for these character slots are
generally not literal characters, but rather abstract la-
bels representing various categories, most of which
98
Features hnw hmtnwt
1. Preceding POS PRP H
2. Current POS VB NN
3. Per.Gen.Num. 3cp -fs
4. Left Context (2) # R2
5. Left Context (1) # R3
6. Source String h wt
7. Target String q h
8. Anomaly h?q wt?h
9. Right Context (1) R2 #
10. Right Context (2) INFL-SFX #
11. Following POS yyQM REL
Table 1: Intra-token feature categories
are morphological in nature. In hmtnwt, for exam-
ple, the two left-context characters t and n are the
second and third radicals of the root, hence the fea-
ture values R2 and R3, respectively.
4.3.2 Inter-token features
The inter-token features encode anomalies whose
scope is not confined to a particular token. Such
anomalies include token displacements and missing
tokens. We again use the Levenshtein algorithm to
detect inter-token anomalies, but we disable the sub-
stitution operation here so that we can link up corre-
sponding deletions and insertions to yield ?shifts.?
For example, suppose the target is A B C D, and
the learner has D A B C. Without substitutions, the
minimal cost edit sequence is to delete D from the
beginning of the learner?s input and insert D at the
end. Merging the two operations results in a D shift.
The learner sentence in (9b) shows two inter-
token anomalies with respect to the target in (9a).
First, the learner has transposed the two tokens in
sequence 1, namely the verb dibrw (?speak-PAST?)
and the adverb tmid (?always?). Second, sequence 2
(the PP beph hybrit, ?in the Hebrew language?) has
been shifted from its position in the target sentence.
(9) a. barC
in-land-of
ieral
Israel
la
not
dibrw1
speak-PAST
tmid1
always
beph2
in-the-language
hybrit2
the-Hebrew
.
.
b. barC ieral beph2 hybrit2 la tmid1
dibrw1 .
Table 2 presents the inter-token feature vectors
for the two anomalies in (9b). After Anomaly,
Features Seq. 1 Seq. 2
1. Anomaly TRNS SHFT
2. Sequence Label RB?VP PP
3. Head Per.Gen.Num. 3cp ---
4. Head POS.(Binyan) VB.PIEL IN
5. Sequence-Initial POS VB IN
6. Sequence-Final POS RB JJ
7. Left POS (Learner) RB NNP
8. Right POS (Learner) IN RB
9. Left POS (Target) RB RB
10. Right POS (Target) IN yyDOT
11. Sequence Length 2 2
12. Normalized Error Cost 0.625 0.250
13. Sent-Level@Rank 200@2 200@2
Table 2: Inter-token feature categories
the next three features provide approximations of
phrasal properties, e.g., the phrasal category and
head, based on a few syntactically-motivated heuris-
tics. Sequence Label identifies the lexical or phrasal
category of the shifted token/token-sequence (e.g.,
PP). Note that sequence labels for transpositions are
special cases consisting of two category labels sep-
arated by an arrow. Head Per.Gen.Num and Head
POS.(Binyan) represent the morphosyntactic prop-
erties of the sequence?s (approximate) head word,
namely its person, gender, and number, and its POS
tag. If the head is a verb, the POS tag is followed by
the verb?s binyan (i.e., verb class), as in VB.PIEL.
The cost feature, Normalized Error Cost, is com-
puted as follows: for missing, extra, and transposed
sequences, the cost is simply the sequence length
divided by the sentence length. For shifts, the se-
quence length and the shift distance are summed
and then divided by the sentence length. Sent-
Level@Rank indicates both the difficulty level of the
exercise and the word-order rank of target sentence
to which the learner sentence has been matched.
4.3.3 Global features
In addition to errors, we also look at global fea-
tures capturing global trends in a sentence, in order
to integrate information about the learner?s overall
performance on a sentence. For example, we note
the percentage of target POS bigrams present in the
learner sentence (POS recall). Table 3 presents the
global features. The two example feature vectors are
those for the sentences (8b) and (9b) above.
99
Features Ex. (8b) Ex. (9b)
1. POS Bigram Recall 2.000 1.273
2. LCSeq Ratio 2.000 1.250
3. LCStr Ratio 1.200 0.500
4. Relaxed LCStr Ratio 2.000 0.500
5. Intra-token Error Count 1.500 0.000
6. Inter-token Error Count 0.000 1.500
7. Intra-token Net Cost 1.875 0.000
8. Norm. Aggregate Displ. 0.000 0.422
9. Sentence Level 200 200
Table 3: Global feature categories
Except for feature 9 (Sentence Level), every fea-
ture in table 3 is multiplied by a weight derived from
the sentence level. These weights serve either to pe-
nalize or compensate for a sentence?s difficulty, de-
pending on the feature type. Because features 1?
4 are ?positive? measures, they are multiplied by
a factor proportional to the sentence level, namely
l = 1. . . 4, whose values correspond directly to the
levels 150?300+, respectively. Features 5?8, in con-
trast, are ?negative? measures, so they are multiplied
by a factor inversely proportional to l, namely 5?l4 .
Among the positive features, LCSeq looks for the
longest common subsequence between the learner
sentence and the target, while LCStr Ratio and Re-
laxed LCStr Ratio both look for longest common
substrings. However, Relaxed LCStr Ratio allows
for token-internal anomalies (as long as the token it-
self is present) while LCStr Ratio does not.
As for the negative features, the two Error Count
features simply tally up the errors of each type
present in the sentence. The Intra-token Net Cost
sums over the token-internal Levenshtein distances
between corresponding learner and target tokens.
Normalized Aggregate Displacement is the sum of
insertions and deletions carried out during inter-
token alignment, normalized by sentence length.
5 Two-phase classification
To combine the features for individual phenomena,
we run a two-phase classifier. In the first phase, we
classify each feature vector for each phenomenon
into a level. In the second phase, we aggregate over
this output to classify the overall learner level.
We use two-phase classification in order to: 1)
modularize each individual phenomenon, mean-
ing that new phenomena are more easily incorpo-
rated into future models; 2) better capture sparsely-
represented phenomena, by aggregating over them;
and 3) easily integrate other exercise types simply
by having more specialized phase 1 classifiers and
by then integrating the results into phase 2.
One potential drawback of two-phase classifica-
tion is that of not having gold standard annotation of
phase 1 levels or even knowing for sure whether in-
dividual phenomena can be classified into consistent
and useful categories. That is, even if a 200-level
learner makes an error, that error is not necessarily a
200-level error. We discuss this next.
5.1 Classifying individual phenomena
With our three sets of features (sec. 4), we set up
three classifiers. Depending upon the type, the ap-
propriate classifier is used to categorize each phase
1 vector. For classification, every phase 1 vector is
assigned a single learner level. However, this as-
sumes that each error indicates a unique level, which
is not always true. A substitution of i for w, for ex-
ample, may largely be made by 250-level (interme-
diate) learners, but also by learners of other levels.
One approach is to thus view each phenomenon as
mapping to a set of levels, and for a new vector, clas-
sification predicts the set of appropriate levels, and
their likelihood. Another approach to overcome the
fact that each uniquely-classified phenomenon can
be indicative of many levels is to rely on phase 2
to aggregate over different phenomena. The advan-
tage of the first approach is that it makes no assump-
tions about individual phenomena being indicative
of a single level, but the disadvantage is that one
may start to add confusion for phase 2 by includ-
ing less relevant levels, especially when using little
training data. The second approach counteracts this
confusion by selecting the most prototypical level
for an individual phenomenon (cf. criterial features
in (Hawkins and Buttery, 2010)), giving less noise
to phase 2. We may lose important non-best level
information, but as we show in sec. 6, with a range
of classifications from phase 1, the second phase can
learn the proper learner level.
In either case, from the perspective of training,
an individual phenomenon can be seen, in terms of
level, as the set of learners who produced such a phe-
nomenon. We thus approximate the level of each
100
Feature type Feature type
1. 100-level classes 7. Intra-token error sum
2. 150-level classes 8. Inter-token error sum
3. 200-level classes 9. Sentences attempted
4. 250-level classes 10. 250-level attempts
5. 300-level classes 11. 300-level attempts
6. Composite error
Table 4: Feature categories for learner level prediction
phenomenon by using the level of the learner from
the gold standard training data. This allows us not to
make a theoretical classification of phenomena (as
opposed to taxonomically labeling phenomena).
5.2 Predicting learner level
We aggregate the information from phase 1 classifi-
cation to classify overall learner levels. We assume
that the set of all individual phenomena and their
quantities (e.g., proportion of phenomena classified
as 200-level in phase 1) characterize a learner?s level
(Hawkins and Buttery, 2010). The feature types
are given in table 4. Features 1?6 are discussed in
sec. 6.1; features 7?8 are (normalized) sums; and the
rest record the number of sentences attempted, bro-
ken down by intended level of the sentence. Lower-
level attempts are not included, as they are the same
values for nearly all students. When we incorporate
other exercise types in the future, additional features
can be added?and the current features modified?
to fold in information from those exercise types.
An example To take an example, one of our
(300+) learners attempts four sentences, giving four
sets of global features, and makes four errors, for
a total of eight phase 1 individual phenomena. One
phenomenon is automatically classified as 100-level,
one as 150, four as 200, one as 250, and one as 300+.
Taking the 1-best phase 1 output (see section 6.3),
the phase 2 vector in this case is as in (10a), corre-
sponding directly to the features in table 4.
(10) a. 0.25, 0.25, 1.00, 0.25, 0.25, 2.00, 0.50,
0.50, 4, 1, 0
b. 0.25, 0.00, 1.00, 0.25, 0.00, 1.625, 0.00,
0.50, 4, 1, 0
In training, we find a 300+-level learner with a
very similar vector, namely that of (10b). Depending
upon the exact experimental set-up (e.g., k2 = 1,
see section 6.3), then, this vector helps the system to
correctly classify our learner as 300+.
6 Evaluation
6.1 Details of the experiments
We use TiMBL (Daelemans et al, 2010; Daelemans
et al, 1999), a memory-based learner (MBL), for
both phases. We use TiMBL because MBL has been
shown to work well with small data sets (Banko and
Brill, 2001); allows for the use of both text-based
and numeric features; and does not suffer from a
fragmented class space. We mostly use the default
settings of TiMBL?the IB1 learning algorithm and
overlap comparison metric between instances?and
experiment with different values of k.
For prediction of phenomenon level (phase 1) and
learner level (phase 2), the system is trained on data
from placement exams previously collected in a He-
brew language program, as described in sec. 2. With
only 38 learners, we use leave-one-out testing, train-
ing on the data from the 37 other learners in order
to run a model on each learner?s sentences. All of
phase 1 is completed (i.e., automatically analyzed)
before training the phase 2 models. As a baseline,
we use the majority class (level 150); choosing this
for all learners gives an accuracy of 34.2% (13/38).2
Phase 1 probability distributions Because
TiMBL retrieves all neighbor with the k nearest
distances rather than the k nearest neighbors, we
can use the number of neighbors in phase 1 to adjust
the values of, e.g., 150-level classes. For example,
the output from phase 1 for two different vectors
might be as in (11). Both have a distribution of 23
150-level and 13 200-level; however, in one case,
this is based on 6 neighbors, whereas for the other,
there are 12 neighbors within the nearest distance.
(11) a) 150:4, 200:2 b) 150:8, 200:4
With more data, we may have more confidence
in the prediction of the second case. The classes
features (fx) of table 4 are thus calculated as in
(12), multiplying counts of each class (c(x)) by their
probabilities (p(x)).
2We are aware that the baseline is not very strong, but the
only alternative would be to use a classifier since we observed
no direct correlation between level and number of errors.
101
k Intra Inter Global Overall
1 28.1% 38.6% 34.4% 34.7%
3 34.2% 44.6% 44.6% 41.9%
5 34.2% 37.1% 36.7% 36.3%
Table 5: Phase 1 accuracies
(12) fx =
?
phase1
c(x)p(x)
The Composite error feature combines all classes
features into one score, inversely weighing them by
level, so that more low-level errors give a high value.
6.2 Predicting phenomena levels
We first evaluate phase 1 accuracy, as in table 5. Us-
ing k = 3 gives the best phase 1 result, 41.9%. We
evaluate with respect to the single-best class, i.e.,
the level of the learner of interest. Accuracy is the
percentage of correctly-classified instances out of all
instances. We assume an instance is classified cor-
rectly if its class corresponds to the learner level.
Accuracy is rather low, at 41.9%. However, we
must bear in mind that we cannot expect 100% accu-
racy, given that individual phenomena do not clearly
belong to a single level. Intra-token classification is
lowest, likely due to greater issues of sparsity: ran-
dom typos are unlikely to occur more than once.
6.3 Predicting learner level
For the second phase, we use different settings for
phase 1 instances. The results are shown in table 6.
The overall best results are reached using single-best
classification for phase 1 and k = 1 for phase 2, giv-
ing an accuracy of 60.5%. Note that the best result
does not use the best performing setting for phase 1
but rather the one with the lowest performance for
phase 1. This shows clearly that optimizing the two
phases individually is not feasible. We obtain the
same accuracy using k = 5 for both phases.
Since we are interested in how these two settings
differ, we extract confusion matrices for them; they
are shown in table 7. The matrices show that the in-
herent smoothing via the k nearest neighbors leads
to a good performance for lower levels, to the ex-
clusion of levels higher than 200. The higher lev-
els are also the least frequent: the k1 = 5/k2 = 5
case shows a bias towards the overall distribution of
levels, whereas the 1-best/k2 = 1 setting is more
Phase 1
1-best k1 = 1 k1 = 3 k1 = 5
P
ha
se
2 Max 42.1 47.4 57.9 42.1
k2 = 1 60.5 57.9 36.8 39.5
k2 = 3 42.1 44.7 44.7 42.1
k2 = 5 39.5 42.1 44.7 60.5
Table 6: Phase 2 accuracies for different phase 1 settings
System
1-best 100 150 200 250 300+ Acc.
G
ol
d
100 6 1 6/7
150 2 7 3 1 7/13
200 2 7 1 1 7/11
250 1 1 0/2
300+ 1 1 3 3/5
k=5 100 150 200 250 300+ Acc.
G
ol
d
100 5 2 5/7
150 2 9 2 9/13
200 2 9 9/11
250 2 0/2
300+ 1 4 0/5
Table 7: Classification confusion matrices
likely to guess neighboring classes. In order to better
account for incorrect classifications which are close
to the correct answer (e.g., 250 for 200), we also
calculated weighted kappa for all the results in ta-
ble 6. Based on kappa, the best result is based on
the setting k1 = 1/k2 = 1 (0.647), followed by
1-best/k2 = 1 (0.639). The weighted kappa for
k1 = 5/k2 = 5 is significantly lower (0.503).
We are also interested in whether we need such
a complex system: phase 1 can outputs a distribu-
tion of senses (k1 = n), or we can use the single
best class as input to phase 2 (1-best). In a different
vein, phase 2 is a machine learner (k2 = n) trained
on phase 1 classified data, but could be simplified
to take the maximum phase 1 class (Max). The re-
sults in table 6 show that using the single-best result
from phase 1 in combination with k2 = 1 provides
the best results, indicating that phase 2 can properly
aggregate over individual phenomena (see sec. 5.1).
However, for all other phase 2 settings, adding the
distribution over phase 1 results increases accuracy.
Using the maximum class rather than the machine
learner in phase 2 generally works best in combina-
102
tion with more nearest neighbors in phase 1, provid-
ing a type of smoothing. However, using the maxi-
mum has an overall detrimental effect.
While the results may not be robust enough to de-
ploy, they are high, given that this is only one type of
exercise, and we have used a very small set of train-
ing data. When performing the error analysis, we
found one student who had attempted only half of
the sentences?generally a sign of a low level?who
was put into level 300. We assume this student per-
formed better on other exercises in the exam. Given
this picture, it is not surprising that our system con-
sistently groups this student into a lower level.
6.4 Ablation studies
We are particularly interested in how the different
phases interact, 1) because one major way to expand
the system is to add different exercises and incor-
porate them into the second phase, and 2) because
the results in table 6 show a strong interdependence
between phases. We thus performed a set of exper-
iments to gauge the effect of different types of fea-
tures. By running ablation studies?i.e., removing
one or more sets of features (cf. e.g. (Yannakoudakis
et al, 2011))?we can determine their relative impor-
tance and usefulness. We run phase 2 (k = 1) using
different combinations of phase 1 classifiers (1-best)
as input. The results are presented in table 8.
Intra Inter Global Acc.
Y Y Y 60.5%
Y Y N 47.4%
Y N N 42.1%
N Y Y 42.1%
N Y N 42.1%
Y N Y 36.8%
N N Y 34.2%
Table 8: Ablation studies, evaluating on phase 2 accuracy
Perhaps unsurprisingly, the combination of all
feature types results in the highest results of 60.5%.
Also, using only one type of features results in the
lowest performance, with the global features being
the least informative set, on par with the baseline of
34.2%. If we use only two feature sets, removing
the global features results in the least deterioration.
Since these features do not directly model errors but
rather global sentence trends, this is to be expected.
However, leaving out inter-token features results in
the second-lowest results (36.8%), thus showing that
this set is extremely important?again not surprising
given that we are working with an exercise designed
to test word order skills.
7 Summary and Outlook
We have developed a system for predicting the level
of Hebrew language learners, using only a small
amount of targeted language data. We have pre-
dicted level based on a single placement exam exer-
cise, finding a surprising degree of accuracy despite
missing much of the information normally used on
such exams. We accounted for the problem of data
sparsity by breaking the problem into a two-phase
classification and through our choice of learning al-
gorithm. The classification process isolates individ-
ual errors and linguistic constructions which are then
aggregated into a second phase; such a two-step pro-
cess allows for easy integration of other exercises
and features in the future. The aggregation of infor-
mation allows us to smooth over sparse features.
In the immediate future, we are integrating other
exercises, to improve the overall accuracy of level
prediction (i.e., the second phase) and make auto-
matic testing more valid (cf. e.g. (Fulcher, 1997)),
while at the same time incorporating more linguistic
processing for more complex input. For example,
with question formation exercises, no closed set of
correct answers exists, and one must use parse tree
distance to delineate features. With multiple exer-
cises, we have plans to test the system with incoming
students to the Hebrew program at our university.
Acknowledgments
We would like to thank Ayelet Weiss for help
throughout, as well as Chris Riley and Amber Smith.
Part of this work was funded by the IU Jewish Stud-
ies Center, as well as by the Institute for Digital Arts
and Humanities and the Data to Insight Center at IU.
We also thank Stephanie Dickinson from the Indiana
Statistical Consulting Center (ISCC) for analysis as-
sistance and the three anonymous reviewers for their
comments.
103
References
Dora Alexopoulou, Helen Yannakoudakis, and Ted
Briscoe. 2010. From discriminative features to learner
grammars: a data driven approach to learner corpora.
Talk given at Second Language Research Forum, Uni-
versity of Maryland, October 2010.
Yigal Attali and Jill Burstein. 2006. Automated essay
scoring with e-rater v.2. Journal of Technology, Learn-
ing, and Assessment, 4(3), February.
Michele Banko and Eric Brill. 2001. Mitigating the
paucity-of-data problem: Exploring the effect of train-
ing corpus size on classifier performance for natural
language processing. In Proceedings of HLT 2001,
First International Conference on Human Language
Technology Research, pages 253?257, San Diego, CA.
Adriane Boyd. 2010. EAGLE: an error-annotated cor-
pus of beginning learner German. In Proceedings of
LREC-10, Valetta, Malta.
Walter Daelemans, Antal van den Bosch, and Jakub Za-
vrel. 1999. Forgetting exceptions is harmful in lan-
guage learning. Machine Learning, 34:11?41.
Walter Daelemans, Jakub Zavrel, Ko van der Sloot, and
Antal van den Bosch. 2010. Timbl: Tilburg memory
based learner, version 6.3, reference guide. Technical
report, ILK Research Group. Technical Report Series
no. 10-01.
Robert Dale and Adam Kilgarriff. 2011. Helping our
own: The HOO 2011 pilot shared task. In Proceedings
of the Generation Challenges Session at the 13th Eu-
ropean Workshop on Natural Language Generation,
pages 242?249, Nancy, France, September.
Ana D??az-Negrillo and Jesu?s Ferna?ndez-Dom??nguez.
2006. Error tagging systems for learner corpora.
Spanish Journal of Applied Linguistics (RESLA),
19:83?102.
Markus Dickinson, Ross Israel, and Sun-Hee Lee. 2011.
Developing methodology for Korean particle error de-
tection. In Proceedings of the Sixth Workshop on In-
novative Use of NLP for Building Educational Appli-
cations, pages 81?86, Portland, OR, June.
Markus Dickinson. 2011. On morphological analysis for
learner language, focusing on Russian. Research on
Language and Computation, 8(4):273?298.
Glenn Fulcher. 1997. An English language placement
test: issues in reliability and validity. Language Test-
ing, 14(2):113?138.
Yoav Goldberg and Michael Elhadad. 2011. Joint He-
brew segmentation and parsing using a PCFGLA lat-
tice parser. In Proceedings of ACL-HLT, pages 704?
709, Portland, OR, June.
Gintare` Grigonyte`, Joa?o Paulo Cordeiro, Gae?l Dias, Ru-
men Moraliyski, and Pavel Brazdil. 2010. Para-
phrase alignment for synonym evidence discovery.
In Proceedings of the 23rd International Conference
on Computational Linguistics (COLING), pages 403?
411, Beijing, China.
John A. Hawkins and Paula Buttery. 2010. Criterial fea-
tures in learner corpora: Theory and illustrations. En-
glish Profile Journal, 1(1):1?23.
John A. Hawkins and Luna Filipovic?. 2010. Criterial
Features in L2 English: Specifying the Reference Lev-
els of the Common European Framework. Cambridge
University Press.
Alon Itai and Shuly Wintner. 2008. Language resources
for Hebrew. Language Resources and Evaluation,
42(1):75?98.
Alla Rozovskaya and Dan Roth. 2011. Algorithm selec-
tion and model adaptation for ESL correction tasks. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies, pages 924?933, Portland, OR,
June.
Khalil Sima?an, Alon Itai, Yoad Winter, Alon Altman,
and N. Nativ. 2001. Building a tree-bank of Mod-
ern Hebrew text. Traitment Automatique des Langues,
42(2).
Joel Tetreault and Martin Chodorow. 2008. The ups and
downs of preposition error detection in ESL writing.
In Proceedings of COLING-08, Manchester.
Helen Yannakoudakis, Ted Briscoe, and Ben Medlock.
2011. A new dataset and method for automatically
grading ESOL texts. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies, pages
180?189, Portland, OR, June.
Shlomo Yona and Shuly Wintner. 2008. A finite-state
morphological grammar of Hebrew. Natural Lan-
guage Engineering, 14(2):173?190.
104
The 7th Workshop on the Innovative Use of NLP for Building Educational Applications, pages 316?325,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Sense-Specific Lexical Information for Reading Assistance
Soojeong Eom
Georgetown University
se48@georgetown.edu
Markus Dickinson
Indiana University
md7@indiana.edu
Rebecca Sachs
Georgetown University
rrs8@georgetown.edu
Abstract
To support vocabulary acquisition and read-
ing comprehension in a second language, we
have developed a system to display sense-
appropriate examples to learners for difficult
words. We describe the construction of the
system, incorporating word sense disambigua-
tion, and an experiment we conducted testing
it on a group of 60 learners of English as a
second language (ESL). We show that sense-
specific information in an intelligent reading
system helps learners in their vocabulary ac-
quisition, even if the sense information con-
tains some noise from automatic processing.
We also show that it helps learners, to some
extent, with their reading comprehension.
1 Introduction and Motivation
Reading texts in a second language presents the
language learner with a number of comprehension
problems, including the problem of interpreting
words that are unknown or are used in unfamiliar
ways. These problems are exacerbated by the preva-
lence of lexical ambiguity. Landes et al (1998) re-
port that more than half the content words in English
texts are lexically ambiguous, with the most frequent
words having a large number of meanings. The
word face, for example, is listed in WordNet (Fell-
baum, 1998) with twelve different nominal senses;
although not all are equally prevalent, there is still
much potential for confusion.
To address this, we have designed an online read-
ing assistant to provide sense-specific lexical in-
formation to readers. By sense-specific, we refer
to information applicable only for one given sense
(meaning) of a word. In this paper, we focus on
the system design and whether such a system can be
beneficial. Our experiment with learners illustrates
the effectiveness of such information for vocabulary
acquisition and reading comprehension.
The problem of lexical ambiguity in reading com-
prehension is a significant one. While dictionar-
ies can help improve comprehension and acquisition
(see, e.g., Prichard, 2008), lexical ambiguity may
lead to misunderstandings and unsuccessful vocabu-
lary acquisition (Luppescu and Day, 1993), as learn-
ers may become confused when trying to locate an
appropriate meaning for an unknown word among
numerous sense entries. Luppescu and Day showed
that readers who use a (printed) dictionary have im-
proved comprehension and acquisition, but to the
detriment of their reading speed.
For electronic dictionaries as well, lexical am-
biguity remains a problem (Koyama and Takeuchi,
2004; Laufer and Hill, 2000; Leffa, 1992; Prichard,
2008), as readers need specific information about a
word as it is used in context in order to effectively
comprehend the text and thus learn the word. Kulka-
rni et al (2008) demonstrated that providing readers
with sense-specific information led learners to sig-
nificantly better vocabulary acquisition than provid-
ing them with general word meaning information.
We have developed an online system to provide
vocabulary assistance to learners of English as a
Second Language (ESL), allowing them to click
on unfamiliar words and see lexical information?
target word definitions and examples?relevant to
that particular usage. We discuss previous online
316
systems in section 2. Importantly, the examples we
present are from the COBUILD dictionary (Sinclair,
2006), which is designed for language learners. To
present these for any text, our system must map au-
tomatic word sense disambiguation (WSD) system
output (using WordNet senses (Fellbaum, 1998)) to
COBUILD, as covered in section 3, where we also
describe general properties of the web system.
The main contribution of this work is to investi-
gate whether high-quality sense-specific lexical in-
formation presented in an intelligent reading system
helps learners in their vocabulary acquisition and
reading comprehension and to investigate the effect
of automatic errors on learning. We accordingly ask
the following research questions:
1. Does sense-specific lexical information facili-
tate vocabulary acquisition to a greater extent
than: a) no lexical information, and b) lexical
information on all senses of each chosen word?
2. Does sense-specific lexical information facili-
tate learners? reading comprehension?
The method and analysis for investigating these
questions with a group of 60 ESL learners is given in
section 4, and the results are discussed in section 5.
2 Background
While there are many studies in second language
acquisition (SLA) on providing vocabulary and
reading assistance (e.g., Prichard, 2008; Luppescu
and Day, 1993), we focus on outlining intelligent
computer-assisted language learning (ICALL) sys-
tems here (see also discussion in Dela Rosa and Es-
kenazi, 2011). Such systems hold the promise of al-
leviating some problems of acquiring words while
reading by providing information specific to each
word as it is used in context (Nerbonne and Smit,
1996; Kulkarni et al, 2008). The GLOSSER-RuG
system (Nerbonne and Smit, 1996) disambiguates
on the basis of part of speech (POS). This is helpful
in distinguishing verbal and nominal uses, for ex-
ample, but is, of course, ineffective when a word
has more than one sense in the same POS (e.g.,
face). More effective is the REAP Tutor (Heilman
et al, 2006), which uses word sense disambigua-
tion to provide lexicographic information and has
been shown to benefit learners by providing sense-
specific lexical information (Dela Rosa and Eske-
nazi, 2011; Kulkarni et al, 2008).
We build from this work by further demonstrat-
ing the utility of sense-specific information. What
distinguishes our work is how we build from the no-
tion that the lexical information provided needs to
be tuned to the capacities of ESL learners. For ex-
ample, definitions and illustrative examples should
make use of familiar vocabulary if they are to aid
language learners; example sentences directly taken
from corpora or from the web seem less appropriate
because the information in them might be less ac-
cessible (Groot, 2000; Kilgarriff et al, 2008; Segler
et al, 2002). On the other hand, examples con-
structed by lexicographers for learner dictionaries
typically control for syntactic and lexical complexity
(Segler et al, 2002). We thus make use of examples
from a dictionary targeting learners.
Specifically, we make use of the examples from
the Collins COBUILD Student?s Dictionary (Sin-
clair, 2006), as it is widely used by ESL learners.
The content in COBUILD is based on actual English
usage and derived from analysis of a large corpus of
written and spoken English, thereby providing au-
thentic examples (Sinclair, 2006). COBUILD also
focuses on collocations in choosing example sen-
tences, so that the example sentences present nat-
ural, reliable expressions, which can play an im-
portant role in learners? vocabulary acquisition and
reading comprehension. We discuss this resource
more in section 3.3.
3 The web system
To support vocabulary acquisition and reading com-
prehension for language learners, we have designed
a system for learners to upload texts and click on
words in order to obtain sense-appropriate examples
for difficult words while reading, as shown in fig-
ure 1. Although the experiment reported upon here
focused on 2 preselected texts, the system is able to
present lexical information for any content words.
Beyond the web interface, the system has three com-
ponents: 1) a system manager, 2) a natural language
processing (NLP) server, and 3) a lexical database.
317
Figure 1: A screenshot showing the effect of clicking on unveiling and receiving sense-specific information
3.1 System manager
The system manager controls the interaction among
each learner, the NLP server, and the lexical
database. When the manager receives a raw text as
an input from the learner, it first sends the input text
to the server and returns an analyzed text (i.e., tok-
enized, POS-tagged, and sense-tagged) back to the
learner, with content words made clickable. Then,
when the learner clicks on a word while reading, the
manager sends the word with its sense information
to the lexical database and brings the word with its
sense-specific lexical information back to the learner
from the lexical database.
Upon completion of the reading, the manager
sends the learner to a page of tests?i.e., a read-
ing test and a vocabulary test, as described in sec-
tion 4?and records the responses.
3.2 NLP preprocessing
To convert raw input into a linguistically-analyzed
text, the system relies on several basic NLP modules
for tokenizing, lemmatizing, POS tagging, and col-
location identification. Although for some internal
testing with different WSD systems we used other
third-party software (e.g., the Stanford POS tagger
(Toutanova et al, 2003)), our word sense disam-
biguator (see below) provides tokenization, lemma-
tization, and POS tagging, as well as collocation
identification. Since the words making up a colloca-
tion may be basic, learners can easily overlook them,
and so we intend to improve this module in the fu-
ture, to reduce underflagging of collocations.
3.3 Lexical database
The lexical database is used to provide a sense-
appropriate definition and example sentences of an
input word to a learner. To obtain the sense-
appropriate information, we must perform word
sense disambiguation (WSD) on the input text. We
use SenseRelate::AllWords (SR:AW) (Pedersen and
Kolhatkar, 2009) to perform WSD of input texts, as
this system has broad coverage of content words.
Given that SR:AW does not outperform the most fre-
quent sense (MFS) baseline, we intend to explore
using the MFS in the future, as well as other WSD
systems, such as SenseLearner (Mihalcea and Cso-
mai, 2005). However, the quality of SR:AW (F-
measure of 54?61% on different corpora) is suffi-
cient to explore in our system and gives us a point
to work from. Indeed, as we will see in section 5.3,
318
while SR:AW makes errors, vocabulary learning is,
in some ways, perhaps not dramatically impeded.
Even with a WSD system, pointing to appropriate
examples is complicated by the fact that the database
of learner-appropriate examples is from one repos-
itory (COBUILD, see section 2), while automatic
WSD systems generally use senses from another
(WordNet). The lexical database, then, is indexed by
WordNet senses, each of which points to an appro-
priate corresponding COBUILD sense. While we
would prefer disambiguating COBUILD senses di-
rectly, we are not aware of any systems which do
this or any COBUILD sense-tagged data to train a
system on. If the benefits for vocabulary acquisition
gained by providing learner-friendly examples from
COBUILD merit it, future work could explore build-
ing a collection of COBUILD-tagged data to train
a WSD system?perhaps a semi-automatic process
using the automatic system we describe next.
To build a lexical database covering all words, we
built a word sense alignment (WSA) system; this
is also in line with a related research agenda in-
vestigating the correspondences between sense in-
ventories (Eom et al, 2012). Space limitations pre-
clude a more detailed discussion, but the WSA sys-
tem works by running SR:AW on COBUILD exam-
ples in order to induce a basic alignment structure
between WordNet and COBUILD. We then post-
process this structure, relying on a heuristic of favor-
ing flatter alignment structures?i.e., links spread
out more evenly between senses in each inventory.1
Iteratively replacing one link with another, to give
flatter structures, we weight each type of proposed
alignment and accept a new alignment if the weight
combined with the probability originally assigned
by the WSD system is the best improvement over
that of the original alignment structure. After all
these steps, the alignments give the lexical database
for linking WSD output to COBUILD senses.
We consider alignment structures wherein each
WordNet sense maps to exactly one COBUILD
sense, to match the task at hand, i.e., mapping each
disambiguated WordNet sense to a single set of
COBUILD examples. This assumption also makes
postprocessing feasible: instead of considering an
1The general idea is to use information about the alignment
structure as a whole; flatter alignments is a convenient heuristic,
in lieu of having any other additional information.
exponential number of alignment structures, we con-
sider only a polynomial number.
Having collected alignment judgments from lin-
guistics students and faculty, we evaluated the sys-
tem against a small set of nine words, covering 63
WordNet senses (Eom et al, 2012). The WSA sys-
tem had a precision of 42.7% (recall=44.5%) when
evaluating against the most popular sense, but a
precision of 60.7% (recall=36.5%) when evaluating
against all senses that seem to be related. We focus
on precision since it is important to know whether
a learner is being pointed to a correct set of exam-
ples or not; whether there are other possibly relevant
examples to show is less important. In Eom et al
(2012), we discuss some difficulties of aligning be-
tween the two resources in the general case; while
some senses go unaligned between the resources,
this was not the case for the words used in this study.
For this study, since we use pre-determined in-
put texts, we also created gold-standard information,
where each word in the text is manually given a link
to the appropriate COBUILD information; note that
here there is no intermediate WordNet sense to ac-
count for. This lets us gauge: a) whether the gold-
standard information is helpful to learners, and b)
comparatively speaking, what the effects are of us-
ing the potentially noisy information provided by the
functioning system.
4 The study
We now turn to evaluating whether this set-up
of providing sense-specific lexical information can
lead learners to improve their vocabulary acquisition
and their reading comprehension.
4.1 Method
4.1.1 Participants
The participants were recruited from three univer-
sities and a private institute in Seoul, Korea, giv-
ing 60 participants (34 male, 26 female). They
ranged in age from 21 to 39 (avg.=23.8) and the
length of studying English ranged from 8 to 25 years
(avg.=11.32).
The 40 participants from the three universities
were taking English courses to prepare for English
proficiency testing. The 20 participants from the
private institute were mostly university graduates
319
taking teacher training courses designed for ele-
mentary English teachers. All participants were
intermediate-level learners, scoring between 15 and
21 on the reading section of the TOEFL iBT R?. We
targeted intermediate learners, so as to test the sys-
tem with learners generally able to understand texts,
yet still encounter many unknown words.
The 60 participants were randomly assigned to
one of four groups, with 15 participants in each
group. The first three received some treatment,
while the fourth was a control group:
1. Gold Senses (GS): reading with support of gold
standard sense-specific lexical information
2. System Senses (SS): reading with support of
system-derived sense-specific lexical informa-
tion
3. All Senses (AS): reading with support of lexi-
cal information of all senses of the chosen word
4. No Senses (NS): reading without any support
of lexical information
For example, when presented with the example
in (1), if chains is clicked, the GS learners see the
correct sense, as in (2a), along with associated ex-
ample sentences (not shown). The automatic system
happens to be incorrect, so the SS learners see a re-
lated, though incorrect, sense and examples, as in
(2b). The AS learners will see those two senses and
examples, as well as the three others for chain. And
the NS learners have no chance to click on a word.
(1) There?s a chance that there will be new items
if you shop at any of the retail chains that
use the ?fast fashion? model of business.
(2) a. Gold: A chain of shops, hotels, or other
businesses is a number of them owned
by the same person or company.
b. System: A chain of things is a group of
them existing or arranged in a line.
4.1.2 Materials
Reading texts After piloting various reading texts
and drawing on the ESL teaching experience of
two of the authors, two texts deemed appropriate
for learners at the (high-)intermediate level were
adopted: Fashion Victim (adapted from Focus on
Vocabulary 1: Bridging Vocabulary (Schmitt et al,
Fashion Victim Sleep Research
resilient.a, chain.n,
conscience.n, cradle.n,
expenditure.n, mend.v,
outfit.n, sector.n,
unveil.v
alternate.a, trivial.a,
deliberately.r, aspect.n,
fatigue.n, obedience.n,
agitate.v, banish.v,
indicate.v, resist.v,
trigger.v
Table 1: Target words used in the study
2011), 589 words) and Sleep Research (adapted
from The Official SAT Study Guide (The College
Board, 2009), 583 words).
The texts were modified to simplify their syntax,
to use more ambiguous words in order to allow for
a stronger test of the system, and to shorten them to
about 600 words. The texts were placed in the online
system, and all content words were made clickable.
Target words A total of 20 target words (9 from
Fashion Victim, 11 from Sleep Research) were se-
lected by piloting a number of possible words with
20 learners from a similar population and identify-
ing ones which were the most unfamiliar, which also
had multiple senses. They appear in table 1.
Reading comprehension tests For reading com-
prehension, two tests were developed, each with
4 multiple-choice and 6 true-false questions. The
questions focused on general content, and partici-
pants could not refer back to the text to answer the
questions. For the multiple-choice questions, more
than one answer could be selected, and each choice
was scored as 1 or 0 (e.g., for 5 choices, the maxi-
mum score for the question was 5); for the true-false
questions, answers were scored simply 1 or 0. The
maximum score for a test was 21.
Vocabulary tests There were one pretest and four
immediate posttests, one of which had the same for-
mat as the pretest. The pretest and all immediate
posttests had the same 30 words (20 target and 10
distractor words). Of 10 distractors, five were words
appearing in the text (obscure.a, correlation.n, in-
tervention.n, discipline.v, facilitate.v), and five were
target words but used with a sense that was different
from the one used in the reading passage (deliber-
ately.r, chain.n, outfit.n, mend.v, indicate.v). Each
test consisted of a word bank and sentences with
320
blanks (cf. Kim, 2008). For the pretest, the sentences
were taken from other sources, whereas the posttest
sentences came from the reading texts themselves.
Although we used four posttests in order to test
different kinds of vocabulary learning (giving more
or fewer hints at meaning), we focus on one posttest
in this paper, the one which matches the form of the
pretest. Each correct answer was scored as 1; incor-
rect as 0.
4.1.3 Procedure
The pretest was administered two weeks before
the actual experiment and posttests, so as to prevent
learners from focusing on those words. Participants
who knew more than 16 out of the 20 target words
were excluded from the experiment.
After reading one text, learners took a reading
comprehension test. Then, they did the same for the
second text. After these two rounds, they took the
series of vocabulary posttests.
4.1.4 Data analysis
We ran a variety of tests to analyze the data.2
First, we ran Levene?s test of homogeneity of vari-
ances, to test whether the variances of the error be-
tween groups were equal at the outset of the study.
This makes it clearer that the effects from the main
tests are due to the variables of interest and not from
inherent differences between groups (Larson-Hall,
2010).
Secondly, to test the first research question about
whether participants show better vocabulary acqui-
sition with sense-specific lexical information, we
used a repeated-measures analysis of variance (RM
ANOVA). Time (pre/post) was the within-subject
variable and Group (GS, SS, AS, NS) was the
between-subject. Post-hoc pairwise comparisons
were run in the case of significant results, to deter-
mine which groups differed from each other. We
also examined the pre-post gain only for the target
words which were clicked and for which we might
thus expect more improvement.
Thirdly, to test the second research question about
whether participants improved in reading compre-
hension, we used a one-way ANOVA, with reading
comprehension scores as a dependent variable and
2We used SPSS,version 20.0, http://www-01.ibm.
com/software/analytics/spss/
Pretest Posttest
Mean SD Mean SD
GS 10.73 (54%) 3.43 15.93 (80%) 3.96
SS 10.93 (55%) 2.82 15.47 (77%) 3.80
AS 10.87 (54%) 3.34 13.47 (67%) 3.83
NS 10.87 (54%) 3.25 11.27 (56%) 3.39
Table 3: Descriptive statistics across groups for vocabu-
lary acquisition (Mean = average, SD = standard devia-
tion, percentage out of all 20 answers in parentheses)
the four groups as an independent variable, to ex-
plore if there was any significant main effect of the
group on reading comprehension scores. Post-hoc
tests were then used, in order to determine specifi-
cally which groups differed from each other.
In order to gauge the effect of automatic system
errors?distinguishing the SS (System Senses) and
GS (Gold Senses) conditions?on vocabulary acqui-
sition, we also examined target words where the sys-
tem gave incorrect information.
5 Results and Discussion
5.1 Vocabulary acquisition
Since the first research question is to examine the
improvement between the pretest and the posttest,
the test of homogeneity of variance was carried
out to ensure that the pretest/posttest scores of the
participants across the four groups showed similar
variances. Levene?s test of homogeneity of vari-
ances suggested that the 4 groups could be con-
sidered to have similar variances on both the pre-
test (F (3, 55) = 0.49, p = 0.69) and the post-test
(F (3, 56) = 0.13, p = 0.94), meaning that this as-
sumption underlying the use of ANOVA was met.
Looking at the descriptive statistics in table 3,
none of the groups differed from each other by more
than a quarter of a point (or 1 percentage point) on
the pretest. Thus, the groups are also comparable
with respect to their levels of performance on the
pre-test.
Turning to the results of the treatments in ta-
ble 3, the four groups show larger differences on
their posttest. The GS and SS groups show the clear-
est gains, suggesting greater vocabulary acquisition
than the AS and NS groups, as expected. If we look
at percentage gain, GS gained 26% and SS 23%,
321
Partial Obs.
Source df df2 F p Eta2 Power
Test of Within-Subjects Effects
Time 1 56 62.67 <0.01 0.53 1.00
Time*Group 3 56 7.20 <0.01 0.28 0.98
Test of Between-Subjects Effects
Group 3 56 1.71 0.18 0.08 0.42
Table 2: Results of RM ANOVA comparing vocabulary test scores across the four groups over time
while AS gained only 13% and NS 2%.
In order to examine whether the above differ-
ences among groups were statistically significant, a
repeated-measures ANOVA was run on those pretest
and posttest scores, with Group as the between-
subject variable and Time as the within-subject vari-
able. The results of the RM ANOVA are presented
in table 2.
With respect to the within-subject variable, the ef-
fect of Time shows a statistically significant differ-
ence (F (1, 56) = 62.67, p < .001, partial eta2=
0.53). In other words, not considering Group, there
is evidence of improvement from pre to posttest.
Most crucially related to the first research ques-
tion about whether the groups would have different
amounts of vocabulary acquisition over time, we see
a significant Time*Group effect (F (3, 56) = 7.20,
p < .001, partial eta2= 0.28). The partial eta2 val-
ues for Time (0.53) and Time*Group (0.28) in ta-
ble 2 represent large effect sizes which thus provide
strong evidence for the differences.
Two sets of post-hoc comparisons were con-
ducted. The first comparisons, in table 4, show sig-
nificant mean differences between the pretest and
posttest for three groups (GS, SS, AS), whereas no
significant difference is observed in the NS group,
meaning that the three groups who received lexi-
cal information showed improvement whereas the
group who received no information did not.
Then, a second set of post-hoc tests were run to
compare the three groups which showed significant
pre-post gains (GS, SS, AS). In table 5, the Contrast
Estimate (Est.) looks at the differences in the mean
pre-post gains and shows that the GS group is sig-
nificantly different from the AS group, whereas the
difference between the mean gains of the SS and AS
groups is not quite significant. (The GS-SS contrast
Mean Std.
Group I J Diff. Error p
GS pre post -5.20 0.80 <0.01
SS pre post -4.23 0.80 <0.01
AS pre post -2.60 0.80 <0.01
NS pre post -0.40 0.80 0.62
Table 4: Post-hoc comparisons for Time*Group, for vo-
cabulary acquisition
Group
Contrast Est. Sig.
GS-AS 2.60 0.02
SS-AS 1.93 0.09
GS-SS 0.67 0.56
Table 5: Contrast results for Time*Group, where the de-
pendent variable is the difference in mean pre-post gains
is non-significant.) In other words, these post-hoc
comparisons on the Time*Group interaction effect
found a significant difference between the GS and
AS groups in their vocabulary learning over time,
with the GS group showing greater pretest-posttest
improvement, whereas the SS?s group apparent ad-
vantage over the AS group with their mean gains fell
slightly short of statistical signficance.
Clicked words In addition to analyzing learners?
performance on the overall scores of their pretest
and posttest, we examine their performance over
their pretest and posttest only on words they clicked
while reading, as well as how much they clicked.
In the three treatments, we find: GS, 28.27 words
clicked on average (7.00 target words); SS, 21.80
(5.93); and AS, 20.87 (5.60). Although these dif-
ferences are not statistically significant, the appar-
ent trend may suggest that the GS group realized
322
Pretest Posttest
Mean SD Mean SD Gain
GS 40% 32% 85% 22% 45%
SS 25% 18% 81% 25% 56%
AS 23% 25% 68% 32% 45%
Table 6: Descriptive statistics for vocabulary acquisition
for clicked words (percentage correct)
they could get high-quality lexical information from
clicking words and so clicked more often.
Examining only clicked target words, the test of
homogeneity confirmed the error variance of all par-
ticipants were equivalent at the outset of the study
(p = 0.15). The percentages correct of the words
that were clicked in the pretest and posttest are in
table 6. The pre to post gain here conveys a gen-
eral trend: for the words participants clicked on, they
showed improvement, with larger gains than for all
words (compare the best gain of 26% in table 3).
As with all words, in the RM ANOVA the effect
of Time shows a statistically significant difference
(F (1, 42) = 96.20, p < 0.01). However, the ef-
fect of Time*Group shows no significant difference
in this case (F (2, 42) = 0.60, p = 0.55).
Despite non-significance, two potentially interest-
ing points emerge which can be followed up on in
the future: 1) descriptively speaking, the SS group
shows the largest gain between pretest and posttest
(56%); and 2) the AS group shows as much improve-
ment as the GS group (45%). This may come from
the fact that the number of senses listed for many
clicked words was small enough (e.g., 2?3) to find
an appropriate sense. Future work could investigate
a greater number of target words to verify and shed
more light on these trends.
Discussion In sum, our results suggest a positive
answer to the first research question about whether
sense-specific lexical information leads learners to
better vocabulary acquisition. The results from
several different analyses suggest that: 1) learn-
ers provided with lexical information during read-
ing have more vocabulary acquisition, with sense-
specific information having a greater increase; 2)
learning gains appear to be greater for the subset of
clicked target words than for all words (though fur-
ther research is needed to substantiate this); and 3)
Mean SD
GS 35.80 (85%) 3.98
SS 37.07 (88%) 2.46
AS 34.93 (83%) 3.08
NS 33.27 (79%) 3.69
Table 7: Descriptive statistics for reading comprehension
Source df df2 F p
Group 3 56 4.01 0.01
Table 8: Results of one-way ANOVA for reading com-
prehension scores
they seem to check the meaning more when disam-
biguated correctly (again needing further research).
5.2 Reading comprehension
The second research question explores whether
sense-specific lexical information facilitates reading
comprehension. The descriptive statistics for read-
ing comprehension mean scores of the four groups
are in table 7. The difference among the reading
comprehension mean scores of the four groups was
within about 4 points, corresponding to a 9% differ-
ence (SS, 88%; NS, 79%). The GS and SS groups
have the highest values, but only small differences.
In order to examine whether the above differ-
ences among groups were statistically significant,
a one-way ANOVA was run on reading compre-
hension scores. The test of homogeneity of vari-
ances confirmed the error variances were equivalent
(p = 0.42). The results of the one-way ANOVA are
in table 8.
As shown, the effect of Group shows a sta-
tistically significant difference, indicating that the
groups are different in their reading comprehension
(F (3, 56) = 4.01, p = 0.01). With this significant
difference in reading comprehension performance, it
is necessary to locate where the differences existed
among the groups. Tukey post0hoc tests compared
all four groups in pairs and revealed a significant
difference between the SS group and the NS group
(p = 0.007), with no significant differences between
the other pairs.3
To some extent, the results support the idea that
3GS vs. SS: p = 0.68; GS vs. AS: p = 0.87; GS vs. NS:
p = 0.12; SS vs. AS: p = 0.24; AS vs. NS: p = 0.46.
323
System Pretest Posttest Accuracy
Appropriate + (16) + (14) 88% (14/16)
- (42) + (32) 76% (32/42)
Inappropriate + (12) + (10) 83% (10/12)
- (18) + (9) 50% (9/18)
Table 9: Pre/Posttest performance for SS condition,
summed over learners, broken down by whether system
sense was appropriate (+ = learner got correct; - = learner
got incorrect; numbers in parentheses = actual values)
sense-specific lexical information facilitates learn-
ers? reading comprehension. Curiously, the GS
group, which received more accurate sense infor-
mation than the SS group, was not found to outper-
form the control group (p = 0.12)?despite descrip-
tively showing slightly higher reading comprehen-
sion scores. This issue warrants future investigation.
5.3 Quality of sense information
We have observed some differences between the
Gold Senses (GS) and System Senses (SS) con-
ditions, but we still want to explore to what ex-
tent the learners in SS group were impacted by
words which were incorrectly disambiguated. There
were nine words which the automatic system incor-
rectly assigned senses to (inappropriate target-sense
words),4 and eleven words which it correctly as-
signed. One can see the different performance for
these two types in table 9, for words that learners
clicked on.
There are two take-home points from this table.
First, when learners were correct in the pretest, they
generally did not un-learn that information, regard-
less of whether they were receiving correct sense in-
formation or not (88% vs. 83%). This is important,
as it seems to indicate that wrong sense information
is not leading learners astray. However, the second
point is that when learners were wrong in the pretest,
they were in general able to learn the sense with cor-
rect information (76%), but not as effectively when
given incorrect information (50%). This, unsurpris-
ingly, shows the value of correct sense information.
4aspect.n, chain.n, conscience.n, expenditure.n, sector.n, ag-
itate.v, banish.v, indicate.v, resist.v
6 Summary and Outlook
We have developed a web system for displaying
sense-specific information to language learners and
tested it on a group of 60 ESL learners. We showed
that sense-specific information in an intelligent read-
ing system can help learners in their vocabulary ac-
quisition and, to some extent, may also help with
overall reading comprehension. We also showed
preliminary results suggesting that learners might
learn more of the words whose definitions they
check than words they simply encounter while read-
ing. We can also be optimistic that, while there is
still much room for improvement in presenting sense
information automatically, errors made by the sys-
tem do not seem to interfere with language learners?
previously-known meanings.
There are a number of avenues to pursue in the fu-
ture. One thing to note from the results was that the
group receiving help in the form of all senses (AS)
demonstrated relatively high performance in vo-
cabulary acquisition and reading comprehension, at
times similar to the groups receiving sense-specific
information (GS, SS). This may be related to the
small number of sense entries of the target words
(average = 2.95), and a further study should be done
on target words with more sense entries, in addition
to validating some of the preliminary results pre-
sented in this paper regarding clicked words. Sec-
ondly, the word sense disambiguation methods and
construction of the lexical database can be improved
to consistently provide more accurate sense infor-
mation. Finally, as mentioned earlier, there are pre-
processing improvements to be made, such as im-
proving the search for collocations.
Acknowledgments
We would like to thank Stephanie Dickinson and
Chelsea Heaven from the Indiana Statstical Consult-
ing Center (ISCC) for their assistance, as well as
Graham Katz and the three anonymous reviewers for
their useful comments.
References
Kevin Dela Rosa and Maxine Eskenazi. 2011. Im-
pact of word sense disambiguation on ordering
dictionary definitions in vocabulary learning tu-
tors. In Proceedings of FLAIRS 2011.
324
Soojeong Eom, Markus Dickinson, and Graham
Katz. 2012. Using semi-experts to derive judg-
ments on word sense alignment: a pilot study. In
Proceedings of LREC-12.
Christiane Fellbaum, editor. 1998. WordNet: An
Electronic Lexical Database. The MIT Press,
Cambridge, MA.
Peter J. M. Groot. 2000. Computer assisted sec-
ond language vocabulary acquisition. Language
Learning and Technology, 4(1):60?81.
Michael Heilman, Kevyn Collins-Thompson, Jamie
Callan, and Maxine Eskenazi. 2006. Classroom
success of an intelligent tutoring system for lexi-
cal practice and reading comprehension. In Pro-
ceedings of the 9th International Conference on
Spoken Language Processing.
Adam Kilgarriff, Milos Husa?k, Katy McAdam,
Michael Rundell, and Pavel Rychly?. 2008. Gdex:
Automatically finding good dictionary examples
in a corpus. In Proceedings of EURALEX-08.
Barcelona.
YouJin Kim. 2008. The role of task-induced involve-
ment and learner proficiency in L2 vocabulary ac-
quisition. Language Learning, 58:285?325.
Toshiko Koyama and Osamu Takeuchi. 2004. How
look-up frequency affects EFL learning: An em-
pirical study on the use of handheld-electronic
dictionaries. In Proceedings of CLaSIC 2004,
pages 1018?1024.
Anagha Kulkarni, Michael Heilman, Maxine Eske-
nazi, and Jamie Callan. 2008. Word sense disam-
biguation for vocabulary learning. In Ninth Inter-
national Conference on Intelligent Tutoring Sys-
tems.
Shari Landes, Claudia Leacock, and Randee I.
Tengi. 1998. Building semantic concordances. In
Christiane Fellbaum, editor, WordNet: an elec-
tronic lexical database, chapter 8, pages 199?216.
MIT.
Jenifer Larson-Hall. 2010. A guide to doing statis-
tics in second language research using SPSS.
Routledge, New York, NY.
Baita Laufer and Monica Hill. 2000. What lexi-
cal information do L2 learners select in a CALL
dictionary and how does it affect word retention?
Language Learning and Technology, 3(2):58?76.
Vilson J. Leffa. 1992. Making foreign language texts
comprehensible for beginners: An experiment
with an electronic glossary. System, 20(1):63?73.
S. Luppescu and R. R. Day. 1993. Reading, dictio-
naries, and vocabulary learning. Language Learn-
ing, 43:263?287.
Rada Mihalcea and Andras Csomai. 2005. Sense-
Learner: Word sense disambiguation for all words
in unrestricted text. In Proceedings of the ACL
Interactive Poster and Demonstration Sessions,
pages 53?56. Ann Arbor, MI.
John Nerbonne and Petra Smit. 1996. GLOSSER-
RuG: in support of reading. In Proceedings of
COLING-96.
Ted Pedersen and Varada Kolhatkar. 2009. Word-
Net::SenseRelate::AllWords - a broad coverage
word sense tagger that maximizes semantic relat-
edness. In Proceedings of HLT-NAACL-09. Boul-
der, CO.
Caleb Prichard. 2008. Evaluating L2 readers? vo-
cabulary strategies and dictionary use. Reading in
a Foreign Language, 20(2):216?231.
Diane Schmitt, Norbert Schmitt, and David Mann.
2011. Focus on Vocabulary 1: Bridging Vocabu-
lary. Pearson ESL, second edition.
Thomas Segler, Helen Pain, and Antonella So-
race. 2002. Second language vocabulary acqui-
sition and learning strategies in ICALL environ-
ments. Computer Assisted Language Learning,
15(4):409?422.
John Sinclair, editor. 2006. Collins COBUILD
Advanced Lerner?s English Dictionary. Harper
Collins.
The College Board. 2009. The Official SAT Study
Guide. College Board, second edition.
Kristina Toutanova, Dan Klein, Christopher D.
Manning, and Yoram Singer. 2003. Feature-rich
part-of-speech tagging with a cyclic dependency
network. In Proceedings of HLT-NAACL 2003,
pages 252?259.
325
Proceedings of the Workshop on Language in Social Media (LASM 2013), pages 1?10,
Atlanta, Georgia, June 13 2013. c?2013 Association for Computational Linguistics
Does Size Matter?
Text and Grammar Revision for Parsing Social Media Data
Mohammad Khan
Indiana University
Bloomington, IN USA
khanms@indiana.edu
Markus Dickinson
Indiana University
Bloomington, IN USA
md7@indiana.edu
Sandra Ku?bler
Indiana University
Bloomington, IN USA
skuebler@indiana.edu
Abstract
We explore improving parsing social media
and other web data by altering the input data,
namely by normalizing web text, and by revis-
ing output parses. We find that text normal-
ization improves performance, though spell
checking has more of a mixed impact. We also
find that a very simple tree reviser based on
grammar comparisons performs slightly but
significantly better than the baseline and well
outperforms a machine learning model. The
results also demonstrate that, more than the
size of the training data, the goodness of fit
of the data has a great impact on the parser.
1 Introduction and Motivation
Parsing data from social media data, as well as other
data from the web, is notoriously difficult, as parsers
are generally trained on news data (Petrov and Mc-
Donald, 2012), which is not a good fit for social me-
dia data. The language used in social media does not
follow standard conventions (e.g., containing many
sentence fragments), is largely unedited, and tends
to be on different topics than standard NLP technol-
ogy is trained for. At the same time, there is a clear
need to develop even basic NLP technology for a
variety of types of social media and contexts (e.g.,
Twitter, Facebook, YouTube comments, discussion
forums, blogs, etc.). To perform tasks such as sen-
timent analysis (Nakagawa et al, 2010) or informa-
tion extraction (McClosky et al, 2011), it helps to
perform tagging and parsing, with an eye towards
providing a shallow semantic analysis.
We advance this line of research by investigating
adapting parsing to social media and other web data.
Specifically, we focus on two areas: 1) We compare
the impact of various text normalization techniques
on parsing web data; and 2) we explore parse revi-
sion techniques for dependency parsing web data to
improve the fit of the grammar learned by the parser.
One of the major problems in processing social
media data is the common usage of non-standard
terms (e.g., kawaii, a Japanese-borrowed net term
for ?cute?), ungrammatical and (intentionally) mis-
spelled text (e.g., cuttie), emoticons, and short posts
with little contextual information, as exemplified in
(1).1
(1) Awww cuttie little kitten, so Kawaii <3
To process such data, with its non-standard words,
we first develop techniques for normalizing the text,
so as to be able to accommodate the wide range of
realizations of a given token, e.g., all the different
spellings and intentional misspellings of cute. While
previous research has shown the benefit of text nor-
malization (Foster et al, 2011; Gadde et al, 2011;
Foster, 2010), it has not teased apart which parts
of the normalization are beneficial under which cir-
cumstances.
A second problem with parsing social media data
is the data situation: parsers can be trained on the
standard training set, the Penn Treebank (Marcus
et al, 1993), which has a sufficient size for train-
ing a statistical parser, but has the distinct down-
side of modeling language that is very dissimilar
1Taken from: http://www.youtube.com/watch?
v=eHSpHCprXLA
1
from the target. Or one can train parsers on the En-
glish Web Treebank (Bies et al, 2012), which cov-
ers web language, including social media data, but
is rather small. Our focus on improving parsing for
such data is on exploring parse revision techniques
for dependency parsers. As far as we know, de-
spite being efficient and trainable on a small amount
of data, parse revision (Henestroza Anguiano and
Candito, 2011; Cetinoglu et al, 2011; Attardi and
Dell?Orletta, 2009; Attardi and Ciaramita, 2007)
has not been used for web data, or more generally
for adapting a parser to out-of-domain data; an in-
vestigation of its strengths and weaknesses is thus
needed.
We describe the data sets used in our experiments
in section 2 and the process of normalization in sec-
tion 3 before turning to the main task of parsing in
section 4. Within this section, we discuss our main
parser as well as two different parse revision meth-
ods (sections 4.2 and 4.3). In the evaluation in sec-
tion 5, we will find that normalization has a positive
impact, although spell checking has mixed results,
and that a simple tree anomaly detection method
(Dickinson and Smith, 2011) outperforms a machine
learning reviser (Attardi and Ciaramita, 2007), espe-
cially when integrated with confidence scores from
the parser itself. In addition to the machine learner
requiring a weak baseline parser, some of the main
differences include the higher recall of the simple
method at positing revisions and the fact that it de-
tects odd structures, which parser confidence can
then sort out as incorrect or not.
2 Data
For our experiments, we use two main resources, the
Wall Street Journal (WSJ) portion of the Penn Tree-
bank (PTB) (Marcus et al, 1993) and the English
Web Treebank (EWT) (Bies et al, 2012). The two
corpora were converted from PTB constituency trees
into dependency trees using the Stanford depen-
dency converter (de Marneffe and Manning, 2008).2
The EWT is comprised of approximately 16,000
sentences from weblogs, newsgroups, emails, re-
views, and question-answers. Instead of examining
each group individually, we chose to treat all web
2http://nlp.stanford.edu/software/
stanford-dependencies.shtml
1 <<_ -LRB--LRB-_ 2 punct _ _
2 File _ NN NN _ 0 root _ _
3 : _ : : _ 2 punct _ _
4 220b _ GW GW _ 11 dep _ _
5 -_ GW GW _ 11 dep _ _
6 dg _ GW GW _ 11 dep _ _
7 -_ GW GW _ 11 dep _ _
8 Agreement _ GW GW _ 11 dep _ _
9 for _ GW GW _ 11 dep _ _
10 Recruiting _ GW GW _ 11 dep _ _
11 Services.doc _ NN NN _ 2 dep _ _
12 >>_ -RRB--RRB-_ 2 punct _ _
13 <<_ -LRB--LRB-_ 14 punct _ _
14 File _ NN NN _ 2 dep _ _
15 : _ : : _ 14 punct _ _
16 220a _ GW GW _ 22 dep _ _
17 DG _ GW GW _ 22 dep _ _
18 -_ GW GW _ 22 dep _ _
19 Agreement _ GW GW _ 22 dep _ _
20 for _ GW GW _ 22 dep _ _
21 Contract _ GW GW _ 22 dep _ _
22 Services.DOC _ NN NN _ 14 dep _ _
23 >>_ -RRB--RRB-_ 14 punct _ _
Figure 1: A sentence with GW POS tags.
data equally, pulling from each type of data in the
training/testing split.
Additionally, for our experiments, we deleted the
212 sentences from EWT that contain the POS tags
AFX and GW tags. EWT uses the POS tag AFX for
cases where a prefix is written as a separate word
from its root, e.g., semi/AFX automatic/JJ. Such
segmentation and tagging would interfere with our
normalization process. The POS tag GW is used for
other non-standard words, such as document names.
Such ?sentences? are often difficult to analyze and
do not correspond to phenomena found in the PTB
(cf., figure 1).
To create training and test sets, we broke the data
into the following sets:
? WSJ training: sections 02-22 (42,009 sen-
tences)
? WSJ testing: section 23 (2,416 sentences)
? EWT training: 80% of the data, taking the first
four out of every five sentences (13,130 sen-
tences)
? EWT testing: 20% of the data, taking every
fifth sentence (3,282 sentences)
2
3 Text normalization
Previous work has shown that accounting for vari-
ability in form (e.g., misspellings) on the web, e.g.,
by mapping each form to a normalized form (Fos-
ter, 2010; Gadde et al, 2011) or by delexicaliz-
ing the parser to reduce the impact of unknown
words (?vrelid and Skj?rholt, 2012), leads to some
parser or tagger improvement. Foster (2010), for
example, lists adapting the parser?s unknown word
model to handle capitalization and misspellings of
function words as a possibility for improvement.
Gadde et al (2011) find that a model which posits
a corrected sentence and then is POS-tagged?their
tagging after correction (TAC) model?outperforms
one which cleans POS tags in a postprocessing step.
We follow this line of inquiry by developing text
normalization techniques prior to parsing.
3.1 Basic text normalization
Machine learning algorithms and parsers are sensi-
tive to the surface form of words, and different forms
of a word can mislead the learner/parser. Our ba-
sic text normalization is centered around the idea
that reducing unnecessary variation will lead to im-
proved parsing performance.
For basic text normalization, we reduce all web
URLs to a single token, i.e., each web URL is re-
placed with a uniform place-holder in the entire
EWT, marking it as a URL. Similarly, all emoticons
are replaced by a single marker indicating an emoti-
con. Repeated use of punctuation, e.g., !!!, is re-
duced to a single punctuation token.
We also have a module to shorten words with con-
secutive sequences of the same character: Any char-
acter that occurs more than twice in sequence will
be shortened to one character, unless they appear in
a dictionary, including the internet and slang dictio-
naries discussed below, in which case they map to
the dictionary form. Thus, the word Awww in ex-
ample (1) is shortened to Aw, and cooool maps to
the dictionary form cool. However, since we use
gold POS tags for our experiments, this module is
not used in the experiments reported here.
3.2 Spell checking
Next, we run a spell checker to normalize mis-
spellings, as online data often contains spelling
errors (e.g. cuttie in example (1)). Various sys-
tems for parsing web data (e.g., from the SANCL
shared task) have thus also explored spelling cor-
rection; McClosky et al (2012), for example, used
1,057 autocorrect rules, though?since these did
not make many changes?the system was not ex-
plored after that. Spell checking web data, such as
YouTube comments or blog data, is a challenge be-
cause it contains non-standard orthography, as well
as acronyms and other short-hand forms unknown
to a standard spelling dictionary. Therefore, be-
fore mapping to a corrected spelling, it is vital to
differentiate between a misspelled word and a non-
standard one.
We use Aspell3 as our spell checker to recognize
and correct misspelled words. If asked to correct
non-standard words, the spell checker would choose
the closest standard English word, inappropriate to
the context. For example, Aspell suggests Lil for
lol. Thus, before correcting, we first check whether
a word is an instance of internet speech, i.e., an ab-
breviation or a slang term.
We use a list of more than 3,000 acronyms to
identify acronyms and other abbreviations not used
commonly in formal registers of language. The list
was obtained from NetLingo, restricted to the en-
tries listed as chat acronyms and text message short-
hand.4 To identify slang terminology, we use the
Urban Dictionary5. In a last step, we combine both
lists with the list of words extracted from the WSJ.
If a word is not found in these lists, Aspell is used
to suggest a correct spelling. In order to restrict As-
pell from suggesting spellings that are too different
from the word in question, we use Levenshtein dis-
tance (Levenshtein, 1966) to measure the degree of
similarity between the original form and the sug-
gested spelling; only words with small distances
are accepted as spelling corrections. Since we have
words of varying length, the Levenshtein distance is
normalized by the length of the suggested spelling
(i.e., number of characters). In non-exhaustive tests
on a subset of the test set, we found that a normal-
ized score of 0.301, i.e., a relatively low score ac-
cepting only conservative changes, achieves the best
results when used as a threshold for accepting a sug-
3www.aspell.net
4http://www.netlingo.com/acronyms.php
5www.urbandictionary.com
3
gested spelling. The utilization of the threshold re-
stricts Aspell from suggesting wrong spellings for
a majority of the cases. For example, for the word
mujahidin, Aspell suggested Mukden, which has a
score of 1.0 and is thus rejected. Since we do not
consider context or any other information besides
edit distance, spell checking is not perfect and is
subject to making errors, but the number of errors
is considerably smaller than the number of correct
revisions. For example, lol would be changed into
Lil if it were not listed in the extended lexicon. Ad-
ditionally, since the errors are consistent throughout
the data, they result in normalization even when the
spelling is wrong.
4 Parser revision
We use a state of the art dependency parser, MST-
Parser (McDonald and Pereira, 2006), as our main
parser; and we use two parse revision methods: a
machine learning model and a simple tree anomaly
model. The goal is to be able to learn where the
parser errs and to adjust the parses to be more appro-
priate given the target domain of social media texts.
4.1 Basic parser
MSTParser (McDonald and Pereira, 2006)6 is a
freely available parser which reaches state-of-the-art
accuracy in dependency parsing for English. MST is
a graph-based parser which optimizes its parse tree
globally (McDonald et al, 2005), using a variety of
feature sets, i.e., edge, sibling, context, and non-
local features, employing information from words
and POS tags. We use its default settings for all ex-
periments.
We use MST as our base parser, training it in dif-
ferent conditions on the WSJ and the EWT. Also,
MST offers the possibility to retrieve confidence
scores for each dependency edge: We use the KD-
Fix edge confidence scores discussed by Mejer and
Crammer (2012) to assist in parse revision. As de-
scribed in section 4.4, the scores are used to limit
which dependencies are candidates for revision: if
a dependency has a low confidence score, it may be
revised, while high confidence dependencies are not
considered for revision.
6http://sourceforge.net/projects/
mstparser/
4.2 Reviser #1: machine learning model
We use DeSR (Attardi and Ciaramita, 2007) as a ma-
chine learning model of parse revision. DeSR uses a
tree revision method based on decomposing revision
actions into basic graph movements and learning se-
quences of such movements, referred to as a revision
rule. For example, the rule -1u indicates that the
reviser should change a dependent?s head one word
to the left (-1) and then up one element in the tree
(u). Note that DeSR only changes the heads of de-
pendencies, but not their labels. Such revision rules
are learned for a base parser by comparing the base
parser output and the gold-standard of some unseen
data, based on a maximum entropy model.
In experiments, DeSR generally only considers
the most frequent rules (e.g., 20), as these cover
most of the errors. For best results, the reviser
should: a) be trained on extra data other than the
data the base parser is trained on, and b) begin with
a relatively poor base parsing model. As we will see,
using a fairly strong base parser presents difficulties
for DeSR.
4.3 Reviser #2: simple tree anomaly model
Another method we use for building parse revisions
is based on a method to detect anomalies in parse
structures (APS) using n-gram sequences of depen-
dency structures (Dickinson and Smith, 2011; Dick-
inson, 2010). The method checks whether the same
head category (e.g., verb) has a set of dependents
similar to others of the same category (Dickinson,
2010).
To see this, consider the partial tree in figure 2,
from the dependency-converted EWT.7 This tree is
converted to a rule as in (2), where all dependents of
a head are realized.
... DT NN IN ...
dobj
det prep
Figure 2: A sketch of a basic dependency tree
(2) dobj? det:DT NN prep:IN
7DT/det=determiner, NN=noun, IN/prep=preposition,
dobj=direct object
4
This rule is then broken down into its component
n-grams and compared to other rules, using the for-
mula for scoring an element (ei) in (3). N -gram
counts (C(ngrm)) come from a training corpus; an
instantiation for this rule is in (4).
(3) s(ei) =
?
ngrm:ei?ngrm?n?3
C(ngrm)
(4) s(prep:IN) = C(det:DT NN prep:IN)
+ C(NN prep:IN END)
+ C(START det:DT NN prep:IN)
+ C(det:DT NN prep:IN END)
+ C(START det:DT NN prep:IN END)
We modify the scoring slightly, incorporating bi-
grams (n ? 2), but weighing them as 0.01 of a count
(C(ngrm)); this handles the issue that bigrams are
not very informative, yet having some bigrams is
better than none (Dickinson and Smith, 2011).
The method detects non-standard parses which
may result from parser error or because the text
is unusual in some other way, e.g., ungrammatical
(Dickinson, 2011). The structures deemed atypical
depend upon the corpus used for obtaining the gram-
mar that parser output is compared to.
With a method of scoring the quality of individual
dependents in a tree, one can compare the score of
a dependent to the score obtaining by hypothesizing
a revision. For error detection, this ameliorates the
effect of odd structures for which no better parse is
available. The revision checking algorithm in Dick-
inson and Smith (2011) posits new labelings and
attachments?maintaining projectivity and acyclic-
ity, to consider only reasonable candidates8?and
checks whether any have a higher score.9 If so, the
token is flagged as having a better revision and is
more likely to be an error.
In other words, the method checks revisions for
error detection. With a simple modification of the
code,10 one can also keep track of the best revision
8We remove the cyclicity check, in order to be able to detect
errors where the head and dependent are flipped.
9We actually check whether a new score is greater than or
equal to twice the original score, to account for meaningless
differences for large values, e.g., 1001 vs. 1000. We do not
expect our minor modifications to have a huge impact, though
more robust testing is surely required.
10http://cl.indiana.edu/?md7/papers/
dickinson-smith11.html
for each token and actually change the tree structure.
This is precisely what we do. Because the method
relies upon very coarse scores, it can suggest too
many revisions; in tandem with parser confidence,
though, this can filter the set of revisions to a rea-
sonable amount, as discussed next.
4.4 Pinpointing erroneous parses
The parse revision methods rely both on being able
to detect errors and on being able to correct them.
We can assist the methods by using MST confidence
scores (Mejer and Crammer, 2012) to pinpoint can-
didates for revision, and only pass these candidates
on to the parse revisers. For example, since APS
(anomaly detection) detects atypical structures (sec-
tion 4.3), some of which may not be errors, it will
find many strange parses and revise many positions
on its own, though some be questionable revisions.
By using a confidence filter, though, we only con-
sider ones flagged below a certain MST confidence
score. We follow Mejer and Crammer (2012) and
use confidence?0.5 as our threshold for identifying
errors. Non-exhaustive tests on a subset of the test
set show good performance with this threshold.
In the experiments reported in section 5, if we use
the revision methods to revise everything, we refer
to this as the DeSR and the APS models; if we fil-
ter out high confidence cases and restrict revisions
to low confidence scoring cases, we refer to this as
DeSR restricted and APS restricted.
Before using the MST confidence scores as part
of the revision process, then, we first report on using
the scores for error detection at the ?0.5 threshold,
as shown in table 1. As we can see, using confi-
dence scores allows us to pinpoint errors with high
precision. With a recall around 40?50%, we find er-
rors with upwards of 90% precision, meaning that
these cases are in need of revision. Interestingly, the
highest error detection precision comes with WSJ
as part of the training data and EWT as the test-
ing. This could be related to the great difference be-
tween the WSJ and EWT grammatical models and
the greater number of unknown words in this ex-
periment, though more investigation is needed. Al-
though data sets are hard to compare, the precision
seems to outperform that of more generic (i.e., non-
parser-specific) error detection methods (Dickinson
and Smith, 2011).
5
Normalization Attach. Label. Total
Train Test (on test) Tokens Errors Errors Errors Precision Recall
WSJ WSJ none 4,621 2,452 1,297 3,749 0.81 0.40
WSJ EWT none 5,855 3,621 2,169 5,790 0.99 0.38
WSJ EWT full 5,617 3,484 1,959 5,443 0.97 0.37
EWT EWT none 7,268 4,083 2,202 6,285 0.86 0.51
EWT EWT full 7,131 3,905 2,147 6,052 0.85 0.50
WSJ+EWT EWT none 5,622 3,338 1,849 5,187 0.92 0.40
WSJ+EWT EWT full 5,640 3,379 1,862 5,241 0.93 0.41
Table 1: Error detection results for MST confidence scores (? 0.5) for different conditions and normalization settings.
Number of tokens and errors below the threshold are reported.
5 Experiments
We report three major sets of experiments: the first
set compares the two parse revision strategies; the
second looks into text normalization strategies; and
the third set investigates whether the size of the
training set or its similarity to the target domain is
more important. Since we are interested in parsing
in these experiments, we use gold POS tags as in-
put for the parser, in order to exclude any unwanted
interaction between POS tagging and parsing.
5.1 Parser revision
In this experiment, we are interested in comparing a
machine learning method to a simple n-gram revi-
sion model. For all experiments, we use the original
version of the EWT data, without any normalization.
The results of this set of experiments are shown
in table 2. The first row reports MST?s performance
on the standard WSJ data split, giving an idea of an
upper bound for these experiments. The second part
shows MST?s performance on the EWT data, when
trained on WSJ or the combination of the WSJ and
EWT training sets. Note that there is considerable
decrease for both settings in terms of unlabeled ac-
curacy (UAS) and labeled accuracy (LAS), of ap-
proximately 8% when trained on WSJ and 5.5% on
WSJ+EWT. This drop in score is consistent with
previous work on non-canonical data, e.g., web data
(Foster et al, 2011) and learner language (Krivanek
and Meurers, 2011). It is difficult to compare these
results, due to different training and testing condi-
tions, but MST (without any modifications) reaches
results that are in the mid-high range of results re-
ported by Petrov and McDonald (2012, table 4) in
their overview of the SANCL shared task using the
EWT data: 80.10?87.62% UAS; 71.04%?83.46%
LAS.
Next, we look at the performance of the two re-
visers on the same data sets. Note that since DeSR
requires training data for the revision part that is dif-
ferent from the training set of the base parser, we
conduct parsing and revision in DeSR with two dif-
ferent data sets. Thus, for the WSJ experiment, we
split the WSJ training set into two parts, WSJ02-
11 and WSJ12-2, instead of training on the whole
WSJ. For the EWT training set, we split this set into
two parts and use 25% of it for training the parser
(EWTs) and the rest for training the reviser (EWTr).
In contrast, APS does not need extra data for train-
ing and thus was trained on the same data as the
base parser. While this means that the base parser
for DeSR has a smaller training set, note that DeSR
works best with a weak base parser (Attardi, p.c.).
The results show that DeSR?s performance is be-
low MST?s on the same data. In other words,
adding DeSRs revisions decreases accuracy. APS
also shows a deterioration in the results, but the dif-
ference is much smaller. Also, training on a combi-
nation of WSJ and EWT data increases the perfor-
mance of both revisers by 2-3% over training solely
on WSJ.
Since these results show that the revisions are
harmful, we decided to restrict the revisions further
by using MST?s KD-Fix edge confidence scores, as
described in section 4.4. We apply the revisions only
if MST?s confidence in this dependency is low (i.e.,
below or equal to 0.5). The results of this experiment
are shown in the last section of table 2. We can see
6
Method Parser Train Reviser Train Test UAS LAS
MST WSJ n/a WSJ 89.94 87.24
MST WSJ n/a EWT 81.98 78.65
MST WSJ+EWT n/a EWT 84.50 81.61
DeSR WSJ02-11 WSJ12-22 EWT 80.63 77.33
DeSR WSJ+EWTs EWTr EWT 82.68 79.77
APS WSJ WSJ EWT 81.96 78.40
APS WSJ+EWT WSJ+EWT EWT 84.45 81.29
DeSR restricted WSJ+EWTs EWTr EWT 84.40 81.50
APS restricted WSJ+EWT WSJ+EWT EWT 84.53 *81.66
Table 2: Results of comparing a machine learning reviser (DeSR) with a tree anomaly model (APS), with base parser
MST (* = sig. at the 0.05 level, as compared to row 2).
that both revisers improve over their non-restricted
versions. However, while DeSR?s results are still
below MST?s baseline results, APS shows slight im-
provements over the MST baseline, significant in the
LAS. Significance was tested using the CoNLL-X
evaluation script in combination with Dan Bikel?s
Randomized Parsing Evaluation Comparator, which
is based on sampling.11
For the original experiment, APS changes 1,402
labels and 272 attachments of the MST output. In
the restricted version, label changes are reduced to
610, and attachment to 167. In contrast, DeSR
changes 1,509 attachments but only 303 in the re-
stricted version. The small numbers, given that
we have more than 3,000 sentences in the test set,
show that finding reliable revisions is a difficult task.
Since both revisers are used more or less off the
shelf, there is much room to improve.
Based on these results and other results based on
different settings, which, for DeSR, resulted in low
accuracy, we decided to concentrate on APS in the
following experiments, and more specifically focus
on the restricted version of APS to see whether there
are significant improvements under different data
conditions.
5.2 Text normalization
In this set of experiments, we investigate the influ-
ence of the text normalization strategies presented
in section 3 on parsing and more specifically on our
parse revision strategy. Thus, we first apply a par-
tial normalization, using only the basic text normal-
11http://ilk.uvt.nl/conll/software.html
ization. For the full normalization, we combine the
basic text normalization with the spell checker. For
these experiments, we use the restricted APS reviser
and the EWT treebank for training and testing.
The results are shown in table 3. Note that since
we also normalize the training set, MST will also
profit from the normalizations. For this reason, we
present MST and APS (restricted) results for each
type of normalization. The first part of the table
shows the results for MST and APS without any nor-
malization; the numbers here are higher than in ta-
ble 2 because we now train only on EWT?an issue
we take up in section 5.3. The second part shows the
results for partial normalization. These results show
that both approaches profit from the normalization
to the same degree: both UAS and LAS increase by
approximately 0.25 percent points. When we look at
the full normalization, including spell checking, we
can see that it does not have a positive effect on MST
but that APS?s results increase, especially unlabeled
accuracy. Note that all APS versions significantly
outperform the MST versions but also that both nor-
malized MST versions significantly outperform the
non-normalized MST.
5.3 WSJ versus domain data
In these experiments, we are interested in which type
of training data allows us to reach the highest accu-
racy in parsing. Is it more useful to use a large, out-
of-domain training set (WSJ in our case), a small,
in-domain training set, or a combination of both?
Our assumption was that the largest data set, con-
sisting of the WSJ and the EWT training sets, would
7
Norm. Method UAS LAS
Train:no; Test:no MST 84.87 82.21
Train:no; Test:no APS restr. **84.90 *82.23
Train:part; Test:part MST *85.12 *82.45
Train:part; Test:part APS restr. **85.18 *82.50
Train:full; Test:full MST **85.20 *82.45
Train:full; Test:full APS restr. **85.24 **82.52
Table 3: Results of comparing different types of text normalization, training and testing on EWT sets. (Significance
tested for APS versions as compared to the corresponding MST version and for each MST with the non-normalized
MST: * = sig. at the 0.05 level, ** = significance at the 0.01 level).
give the best results. For these experiments, we use
the EWT test set and different combinations of text
normalization, and the results are shown in table 4.
The first three sections in the table show the re-
sults of training on the WSJ and testing on the EWT.
The results show that both MST and APS profit from
text normalization. Surprisingly, the best results are
gained by using the partial normalization; adding the
spell checker (for full normalization) is detrimental,
because the spell checker introduces additional er-
rors that result in extra, non-standard words in EWT.
Such additional variation in words is not present in
the original training model of the base parser.
For the experiments with the EWT and the com-
bined WSJ+EWT training sets, spell checking does
help, and we report only the results with full normal-
ization since this setting gave us the best results. To
our surprise, results with only the EWT as training
set surpass those of using the full WSJ+EWT train-
ing sets (a UAS of 85.24% and a LAS of 82.52% for
EWT vs. a UAS of 82.34% and a LAS of 79.31%).
Note, however, that when we reduce the size of the
WSJ data such that it matches the size of the EWT
data, performance increases to the highest results,
a UAS of 86.41% and a LAS of 83.67%. Taken
together, these results seem to indicate that quality
(i.e., in-domain data) is more important than mere
(out-of-domain) quantity, but also that more out-of-
domain data can help if it does not overwhelm the
in-domain data. It is also obvious that MST per
se profits the most from normalization, but that the
APS consistently provides small but significant im-
provements over the MST baseline.
6 Summary and Outlook
We examined ways to improve parsing social me-
dia and other web data by altering the input data,
namely by normalizing such texts, and by revis-
ing output parses. We found that normalization im-
proves performance, though spell checking has more
of a mixed impact. We also found that a very sim-
ple tree reviser based on grammar comparisons per-
forms slightly but significantly better than the base-
line, across different experimental conditions, and
well outperforms a machine learning model. The re-
sults also demonstrated that, more than the size of
the training data, the goodness of fit of the data has
a great impact on the parser. Perhaps surprisingly,
adding the entire WSJ training data to web training
data leads to a deteriment in performance, whereas
balancing it with web data has the best performance.
There are many ways to take this work in the
future. The small, significant improvements from
the APS restricted reviser indicate that there is po-
tential for improvement in pursuing such grammar-
corrective models for parse revision. The model we
use relies on a simplistic notion of revisions, nei-
ther checking the resulting well-formedness of the
tree nor how one correction influences other cor-
rections. One could also, for example, treat gram-
mars from different domains in different ways to
improve scoring and revision. Another possibility
would be to apply the parse revisions also to the out-
of-domain training data, to make it more similar to
the in-domain data.
For text normalization, the module could benefit
from a few different improvements. For example,
non-contracted words such as well to mean we?ll
require a more complicated normalization step, in-
8
Train Test Normalization Method UAS LAS
WSJ EWT train:no; test:no MST 81.98 78.65
WSJ EWT train:no; test:no APS 81.96 78.40
WSJ EWT train:no; test:no APS restr 82.02 **78.71
WSJ EWT train:no; test:part MST 82.31 79.27
WSJ EWT train:no; test:part APS restr. *82.36 *79.32
WSJ EWT train:no; test:full MST 82.30 79.26
WSJ EWT train:no; test:full APS restr. 82.34 *79.31
EWT EWT train:full; test:full MST 85.20 82.45
EWT EWT train:full; test:full APS restr. **85.24 **82.52
WSJ+EWT EWT train:full; test:full MST 84.59 81.68
WSJ+EWT EWT train:full; test:full APS restr. **84.63 *81.73
Balanced WSJ+EWT EWT train:full; test:full MST 86.38 83.62
Balanced WSJ+EWT EWT train:full; test:full APS restr. *86.41 **83.67
Table 4: Results of different training data sets and normalization patterns on parsing the EWT test data. (Significance
tested for APS versions as compared to the corresponding MST: * = sig. at the 0.05 level, ** = sig. at the 0.01 level)
volving machine learning or n-gram language mod-
els. In general, language models could be used for
more context-sensitive spelling correction. Given
the preponderance of terms on the web, using a
named entity recognizer (e.g., Finkel et al, 2005)
for preprocessing may also provide benefits.
Acknowledgments
We would like to thank Giuseppe Attardi for his help
in using DeSR; Can Liu, Shoshana Berleant, and the
IU CL discussion group for discussion; and the three
anonymous reviewers for their helpful comments.
References
Giuseppe Attardi and Massimiliano Ciaramita.
2007. Tree revision learning for dependency pars-
ing. In Proceedings of HLT-NAACL-07, pages
388?395. Rochester, NY.
Giuseppe Attardi and Felice Dell?Orletta. 2009. Re-
verse revision and linear tree combination for
dependency parsing. In Proceedings of HLT-
NAACL-09, Short Papers, pages 261?264. Boul-
der, CO.
Ann Bies, Justin Mott, Colin Warner, and Seth
Kulick. 2012. English Web Treebank. Linguis-
tic Data Consortium, Philadelphia, PA.
Ozlem Cetinoglu, Anton Bryl, Jennifer Foster, and
Josef Van Genabith. 2011. Improving dependency
label accuracy using statistical post-editing: A
cross-framework study. In Proceedings of the In-
ternational Conference on Dependency Linguis-
tics, pages 300?309. Barcelona, Spain.
Marie-Catherine de Marneffe and Christopher D.
Manning. 2008. The Stanford typed dependencies
representation. In COLING 2008 Workshop on
Cross-framework and Cross-domain Parser Eval-
uation. Manchester, England.
Markus Dickinson. 2010. Detecting errors in
automatically-parsed dependency relations. In
Proceedings of ACL-10. Uppsala, Sweden.
Markus Dickinson. 2011. Detecting ad hoc rules for
treebank development. Linguistic Issues in Lan-
guage Technology, 4(3).
Markus Dickinson and Amber Smith. 2011. De-
tecting dependency parse errors with minimal re-
sources. In Proceedings of IWPT-11, pages 241?
252. Dublin, Ireland.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local informa-
tion into information extraction systems by gibbs
sampling. In Proceedings of ACL?05, pages 363?
370. Ann Arbor, MI.
Jennifer Foster. 2010. ?cba to check the spelling?:
Investigating parser performance on discussion
forum posts. In Proceedings of NAACL-HLT
2010, pages 381?384. Los Angeles, CA.
9
Jennifer Foster, Ozlem Cetinoglu, Joachim Wagner,
Joseph Le Roux, Joakim Nivre, Deirdre Hogan,
and Josef van Genabith. 2011. From news to com-
ment: Resources and benchmarks for parsing the
language of web 2.0. In Proceedings of IJCNLP-
11, pages 893?901. Chiang Mai, Thailand.
Phani Gadde, L. V. Subramaniam, and Tanveer A.
Faruquie. 2011. Adapting a WSJ trained part-of-
speech tagger to noisy text: Preliminary results.
In Proceedings of Joint Workshop on Multilingual
OCR and Analytics for Noisy Unstructured Text
Data. Beijing, China.
Enrique Henestroza Anguiano and Marie Candito.
2011. Parse correction with specialized models
for difficult attachment types. In Proceedings of
EMNLP-11, pages 1222?1233. Edinburgh, UK.
Julia Krivanek and Detmar Meurers. 2011. Compar-
ing rule-based and data-driven dependency pars-
ing of learner language. In Proceedings of the Int.
Conference on Dependency Linguistics (Depling
2011), pages 310?317. Barcelona.
Vladimir I. Levenshtein. 1966. Binary codes capable
of correcting deletions, insertions, and reversals.
Cybernetics and Control Theory, 10(8):707?710.
Mitchell Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated
corpus of English: The Penn Treebank. Compu-
tational Linguistics, 19(2):313?330.
David McClosky, Wanxiang Che, Marta Recasens,
Mengqiu Wang, Richard Socher, and Christopher
Manning. 2012. Stanford?s system for parsing the
English web. In Workshop on the Syntactic Anal-
ysis of Non-Canonical Language (SANCL 2012).
Montreal, Canada.
David McClosky, Mihai Surdeanu, and Christopher
Manning. 2011. Event extraction as dependency
parsing. In Proceedings of ACL-HLT-11, pages
1626?1635. Portland, OR.
Ryan McDonald, Koby Crammer, and Fernando
Pereira. 2005. Online large-margin training of
dependency parsers. In Proceedings of ACL-05,
pages 91?98. Ann Arbor, MI.
Ryan McDonald and Fernando Pereira. 2006. On-
line learning of approximate dependency parsing
algorithms. In Proceedings of EACL-06. Trento,
Italy.
Avihai Mejer and Koby Crammer. 2012. Are you
sure? Confidence in prediction of dependency
tree edges. In Proceedings of the NAACL-HTL
2012, pages 573?576. Montre?al, Canada.
Tetsuji Nakagawa, Kentaro Inui, and Sadao Kuro-
hashi. 2010. Dependency tree-based sentiment
classification using CRFs with hidden variables.
In Proceedings of NAACL-HLT 2010, pages 786?
794. Los Angeles, CA.
Lilja ?vrelid and Arne Skj?rholt. 2012. Lexical
categories for improved parsing of web data. In
Proceedings of the 24th International Conference
on Computational Linguistics (COLING 2012),
pages 903?912. Mumbai, India.
Slav Petrov and Ryan McDonald. 2012. Overview
of the 2012 shared task on parsing the web.
In Workshop on the Syntactic Analysis of Non-
Canonical Language (SANCL 2012). Montreal,
Canada.
10
Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications, pages 11?21,
Atlanta, Georgia, June 13 2013. c?2013 Association for Computational Linguistics
Shallow Semantic Analysis of Interactive Learner Sentences
Levi King
Indiana University
Bloomington, IN USA
leviking@indiana.edu
Markus Dickinson
Indiana University
Bloomington, IN USA
md7@indiana.edu
Abstract
Focusing on applications for analyzing learner
language which evaluate semantic appropri-
ateness and accuracy, we collect data from a
task which models some aspects of interac-
tion, namely a picture description task (PDT).
We parse responses to the PDT into depen-
dency graphs with an an off-the-shelf parser,
then use a decision tree to classify sentences
into syntactic types and extract the logical sub-
ject, verb, and object, finding 92% accuracy in
such extraction. The specific goal in this paper
is to examine the challenges involved in ex-
tracting these simple semantic representations
from interactive learner sentences.
1 Motivation
While there is much current work on analyzing
learner language, it usually focuses on grammati-
cal error detection and correction (e.g., Dale et al,
2012) and less on semantic analysis. At the
same time, Intelligent Computer-Assisted Language
Learning (ICALL) and Intelligent Language Tutor-
ing (ILT) systems (e.g., Heift and Schulze, 2007;
Meurers, 2012) also tend to focus more on gram-
matical feedback. An exception to this rule is Herr
Komissar, an ILT for German learners that includes
rather robust content analysis and sentence genera-
tion (DeSmedt, 1995), but this involves a great deal
of hand-built tools and does not connect to modern
NLP. Some work addresses content assessment for
short answer tasks (Meurers et al, 2011), but this is
still far from naturalistic, more conversational inter-
actions (though, see Petersen, 2010).
Our overarching goal is to facilitate ILTs and lan-
guage assessment tools that maximize free interac-
tion, building as much as possible from existing
NLP resources. While that goal is in the distant
future, the more immediate goal in this paper is
to pinpoint the precise challenges which interactive
learner sentences present to constructing semantic
analyses, even when greatly constrained. We ap-
proximate this by collecting data from a task which
models some aspects of interaction, namely a picture
description task (PDT), parsing it with an off-the-
shelf parser, extracting semantic forms, and noting
the challenges throughout.
The focus towards interaction is in accord with
contemporary theory and research in Second Lan-
guage Acquisition (SLA) and best practices in sec-
ond language instruction, which emphasize the lim-
iting of explicit grammar instruction and feedback in
favor of an approach that subtly integrates the teach-
ing of form with conversation and task-based learn-
ing (Celce-Murcia, 1991, 2002; Larsen-Freeman,
2002). Indeed, Ellis (2006) states, ?a traditional ap-
proach to teaching grammar based on explicit expla-
nations and drill-like practice is unlikely to result in
the acquisition of the implicit knowledge needed for
fluent and accurate communication.? For our pur-
poses, this means shifting the primary task of an
ICALL application from analyzing grammar to eval-
uating semantic appropriateness and accuracy.
The data for error detection work is ideal for de-
veloping systems which provide feedback on essays,
but not necessarily for more interactive communica-
tion. Thus, our first step is to collect data similar to
what we envision processing in something like an
11
ILT game, data which?as far as we know?does
not exist. While we desire relatively free produc-
tion, there are still constraints; for games, for exam-
ple, this comes in the form of contextual knowledge
(pictures, rules, previous interactions). To get a han-
dle on variability under a set of known constraints
and to systematically monitor deviations from tar-
get meanings, we select a PDT as a constrained task
that still promotes interactive communication. Col-
lecting and analyzing this data is our first major con-
tribution, as described in section 3.
Once we have the data, we can begin to extract se-
mantic forms, and our second major contribution is
to outline successes and pitfalls in obtaining shal-
low semantic forms in interactive learner data, as
described in section 4, working from existing tools.
Although we observe a lot of grammatical variation,
we will demonstrate in section 5 how careful se-
lection of output representations (e.g., the treatment
of prepositions) from an off-the-shelf parser and a
handful of syntax-to-semantics rules allow us to de-
rive accurate semantic forms for most types of tran-
sitive verb constructions in our data. At the same
time, we will discuss the difficulties in defining a
true gold standard of meanings for such a task. This
work paves the way for increasing the range of con-
structions and further exploring the space between
free and constrained productions (see also the dis-
cussion in Amaral and Meurers, 2011).
2 Related Work
In terms of our overarching goals of developing
an interactive ILT, a number of systems exist (e.g.,
TAGARELA (Amaral et al, 2011), e-Tutor (Heift
and Nicholson, 2001)), but few focus on matching
semantic forms. Herr Komissar (DeSmedt (1995))
is one counter-example; in this game, learners take
on the role of a detective tasked with interviewing
suspects and witnesses. The system relies largely on
a custom-built database of verb classes and related
lexical items. Likewise, Petersen (2010) designed
a system to provide feedback on questions in En-
glish, extracting meanings from the Collins parser
(Collins, 1999). Our work is is in the spirit of his,
though our starting point is to collect data of the type
of task we aim to analyze, thereby pinpointing how
one should begin to build a system.
The basic semantic analysis in this paper paral-
lels work on content assessment (e.g., ETS?s c-rater
system (Leacock and Chodorow, 2003)). Different
from our task, these systems are mostly focused on
essay and short answer scoring, though many fo-
cus on semantic analysis under restricted conditions.
As one example, Meurers et al (2011) evaluate En-
glish language learners? short answers to reading
comprehension questions, constrained by the topic
at hand. Their approach performs multiple levels of
annotation on the reading prompt, including depen-
dency parsing and lexical analysis from WordNet
(Fellbaum, 1998), then attempts to align elements of
the sentence with those of the (similarly annotated)
reading prompt, the question, and target answers to
determine whether a response is adequate or what it
might be missing. Our scenario is based on images,
not text, but our future processing will most likely
need to include similar elements, e.g., determining
lexical relations from WordNet.
3 Data Collection
The data involved in this study shares much in com-
mon with other investigations into semantic anal-
ysis of descriptions of images and video, such
as the Microsoft Research Video Description Cor-
pus (MSRvid; Chen and Dolan (2011)) and the
SemEval-2012 Semantic Textual Similarity (STS)
task utilizing MSRvid as training data for assigning
similarity scores to pairs of sentences (Agirre et al,
2012). However, because our approach requires
both native speaker (NS) and non-native speaker
(NNS) responses and necessitates constraining both
the form and content of responses, we assembled
our own small corpus of NS and NNS responses to
a PDT. Research in SLA often relies on the ability
of task design to induce particular linguistic behav-
ior (Skehan et al, 1998), and the PDT should in-
duce more interactive behavior. Moreover, the use
of the PDT as a reliable language research tool is
well-established in areas of study ranging from SLA
to Alzheimer?s disease (Ellis, 2000; Forbes-McKay
and Venneri, 2005).
The NNSs were intermediate and upper-level
adult English learners in an intensive English as
a Second Language program at Indiana University.
We rely on visual stimuli here for a number of rea-
12
sons. Firstly, computer games tend to be highly
visual, so collecting responses to visual prompts is
in keeping with the nature of our desired ILT. Sec-
ondly, by using images, the information the response
should contain is limited to the information con-
tained in the image. Relatedly, particularly simple
images should restrict elicited responses to a tight
range of expected contents. For this initial experi-
ment, we chose or developed each of the visual stim-
uli because it presents an event that we believe to be
transitive in nature and likely to elicit responses with
an unambiguous subject, verb and object, thereby re-
stricting form in addition to content. Finally, this
format allows us to investigate pure interlanguage
without the influence of verbal prompts and shows
learner language in a functional context, modeling
real language use.
Response (L1)
He is droning his wife pitcher. (Arabic)
The artist is drawing a pretty women. (Chinese)
The artist is painting a portrait of a lady. (English)
The painter is painting a woman?s paint. (Spanish)
Figure 1: Example item and responses
The PDT consists of 10 items (8 line drawings
and 2 photographs) intended to elicit a single sen-
tence each; an example is given in Figure 1. Par-
ticipants were asked to view the image and describe
the action, and care was taken to explain to partici-
pants that either past or present tense (and simple or
progressive aspect) was acceptable. Responses were
typed by the participants themselves (without auto-
matic spell checking). To date, we have collected
responses from 53 informants (14 NSs, 39 NNSs),
for a total of 530 sentences. The distribution of first
languages (L1s) is as follows: 14 English, 16 Ara-
bic, 7 Chinese, 2 Japanese, 4 Korean, 1 Kurdish, 1
Polish, 2 Portuguese, and 6 Spanish.
4 Method
We parse a sentence into a dependency representa-
tion (section 4.1) and then extract a simple seman-
tic form from this parse (section 4.2), to compare to
gold standard semantic forms.
4.1 Obtaining a syntactic form
We start analysis with a dependency parse. Because
dependency parsing focuses on labeling dependency
relations, rather than constituents or phrase struc-
ture, it easily finds the subject, verb and object of
a sentence, which can then map to a semantic form
(Ku?bler et al, 2009). Our approach must eventually
account for other relations, such as negation and ad-
verbial modification, but at this point, since we fo-
cus on transitive verbs, we take an na??ve approach in
which subject, verb and object are considered suffi-
cient for deciding whether or not a response accu-
rately describes the visual prompt.
We use the Stanford Parser for this task, trained on
the Penn Treebank (de Marneffe et al, 2006; Klein
and Manning, 2003).1 Using the parser?s options,
we set the output to be Stanford typed dependencies,
a set of labels for dependency relations. The Stan-
ford parser has a variety of options to choose from
for the specific parser ouput, e.g., how one wishes to
treat prepositions (de Marneffe and Manning, 2012).
We use the CCPropagatedDependencies /
CCprocessed option to accomplish two things:2
1) omit prepositions and conjunctions from the sen-
tence text and instead add the word to the depen-
dency label between content words; and 2) propa-
gate relations across any conjunctions. These deci-
sions are important to consider for any semantically-
informed processing of learner language.
1http://nlp.stanford.edu/software/
lex-parser.shtml
2http://nlp.stanford.edu/software/
dependencies_manual.pdf
13
To see the impetus for removing prepositions,
consider the learner response (1), where the prepo-
sition with is relatively unimportant to collecting the
meaning. Additionally, learners often omit, insert,
or otherwise use the wrong preposition (Chodorow
et al, 2007). The default parser would present a
prep relation between played and with, obscuring
what the object is; with the options set as above,
however, the dependency representation folds the
preposition into the label (prep with), instead of
keeping it in the parsed string, as shown in Figure 2.
(1) The boy played with a ball.
vroot The boy played with a ball
nsubj
root
prep with
detdet
Figure 2: The dependency parse of (1)
This is a very lenient approach to prepositions,
as prepositions certainly carry semantic meaning?
e.g., the boy played in a ball means something quite
different than what (1) means. However, because
we ultimately compare the meaning to an expected
semantic form (e.g., play(boy,ball)), it is easier to
give the benefit of the doubt. In the future, one may
want to consider using a semantic role labeler (e.g.,
SENNA (Collobert et al, 2011)).
As for propagating relations across conjunctions,
this ensures that each main verb connects to its argu-
ments, as needed for a semantic form. For example,
in (2), the default parser returns the relation between
the first verb of the conjunction structure, setting and
its subject, man, but not between reading and man.
The options we select, however, return an nsubj
relation between setting and man and also between
reading and man (similarly for the object, paper).
(2) The man is setting and reading the paper.
In addition to these options, many dependency re-
lations are irrelevant for the next step of obtaining
a semantic form. For example, we can essentially
ignore determiner (det) relations between a noun
and its determiner, allowing for variability in how a
learner produces or does not produce determiners.
4.2 Obtaining a semantic form
4.2.1 Sentence types
We categorized the sentences in the corpus into
12 types, shown in Table 1. We established these
types because each type corresponds to a basic sen-
tence structure and thus has consistent syntactic fea-
tures, leading to predictable patterns in the depen-
dency parses. We discuss the distribution of sen-
tence types in section 5.1.
4.2.2 Rules for sentence types
A sentence type indicates that the logical (i.e., se-
mantic) subject, verb, and object can be found in a
particular place in the parse, e.g., under a particular
dependency label. For example, for simple transi-
tive sentences of type A, the words labeled nsubj,
root, and dobj exactly pinpoint the information
we require. Thus, the patterns for extracting se-
mantic information?in the form of verb(subj,obj)
triples?reference particular Stanford typed depen-
dency labels, part-of-speech (POS) tags, and inter-
actions with word indices.
More complicated sentences or those containing
common learner errors (e.g., omission of the cop-
ula be) require slightly more complicated extraction
rules, but, since we examine only transitive verbs at
this juncture, these still boil down to identifying the
sentence type and extracting the appropriate triple.
We do this by arranging a small set of binary fea-
tures into a decision tree to determine the sentence
type, as shown in Figure 3.
To illustrate this process, consider (3). We pass
this sentence through the parser to obtain the depen-
dency parse shown in Figure 4. The parsed sentence
then moves to the decision tree shown in Figure 3.
At the top of the tree, the sentence is checked for an
expl (expletive) label; having none, it moves right-
ward to the nsubjpass (noun subject, passive)
node. Because we find an nsubjpass label, the
sentence moves leftward to the agent node. This
label is also found, thereby reaching a terminal node
and being labeled as a type F2 sentence.
(3) A bird is shot by a man.
With the sentence now typed as F2, we apply
specific F2 extraction rules. The logical subject is
taken from under the agent label, the verb from
14
Type Description Example NS NNS
A Simple declarative transitive The boy is kicking the ball. 117 286
B Simple + preposition The boy played with a ball. 5 23
C Missing tensed verb Girl driving bicycle. 10 44
D Missing tensed verb + preposition Boy playing with a ball. 0 1
E Intransitive (No object) A woman is cycling. 2 21
F1 Passive An apple is being cut. 4 2
F2 Passive with agent A bird is shot by a man. 0 6
Ax Existential version of A or C There is a boy kicking a ball. 0 0
Bx Existential version of B or D There was a boy playing with a ball. 0 0
Ex Existential version of E There is a woman cycling. 0 0
F1x Existential version of F1 There is an apple being cut. 0 1
F2x Existential version of F2 There is a bird being shot by a man. 0 0
Z All other forms The man is trying to hunt a bird. 2 6
Table 1: Sentence type examples, with distributions of types for native speakers (NS) and non-native speakers (NNS)
expl?
nsubjpass?
dobj?
nsubj?
Dprep ??
EB
Y N
Y N
nsubj?
CA
Y N
Y N
agent?
F1F2
Y N
Y N
auxpass?
dobj?
prep ??
ExBx
Y N
Ax
Y N
agent?
F1xF2x
Y N
Y N
Y N
Figure 3: Decision tree for determining sentence type and extracting semantic information
vroot A bird is shot by a man
root
det
nsubjpass
auxpass
agent
det
Figure 4: The dependency parse of (3)
root, and the logical object from nsubjpass,
to obtain shot(man,bird), which can be lemmatized
to shoot(man,bird). Very little effort goes into this
process: the parser is pre-built; the decision tree is
small; and the extraction rules are minimal.
We are able to use little effort in part due to the
constraints in the pictures. For figure 1, for exam-
ple, the artist, the man in the beret, and the man are
all acceptable subjects, whereas if there were multi-
ple men in the picture, the man would not be specific
enough. In future work, we expect to relax such con-
straints on image contents by including rules to han-
dle relative clauses, adjectives and other modifiers
in order to distinguish between references to simi-
15
lar elements, e.g., a man shooting a bird vs. a man
reading the newspaper.
5 Evaluation
To evaluate this work, we need to address two major
questions. First, how accurately do we extract se-
mantic information from potentially innovative sen-
tences (section 5.2)? Due to the simple structures
of the sentences (section 5.1), we find high accu-
racy with our simple system. Secondly, how many
semantic forms does one need in order to capture
the variability in meaning in learner sentences (sec-
tion 5.3)? We operationalize this second question
by asking how well the set of native speaker seman-
tic forms models a gold standard, with the intuition
that a language is defined by native speaker usage,
so their answers can serve as targets. As we will
see, this is a na??ve view.
5.1 Basic distribution of sentences
Before a more thorough analysis, we look at the dis-
tribution of sentence types, shown in Table 1, broken
down between native speakers (NSs) and non-native
speakers (NNSs). A few sentence types clearly dom-
inate here: if one looks only at simple declaratives,
with or without a main verb (types A and C), one
accounts for 90.7% of the NS forms and 84.6% of
the NNS ones, slightly less. Adding prepositional
forms (types B and D) brings the total to 94.3% and
90.8%, respectively. Although there will always be
variability and novel forms (cf. type Z), this shows
that, for situations with basic transitive actions, de-
veloping a system (by hand) for a few sentence types
is manageable. More broadly, we see that clear and
simple images nicely constrain the task to the point
where shallow processing is feasible.
5.2 Semantic extraction
For the purpose of evaluating our extraction system,
we define two major classes of errors. The first are
triple errors, responses for which our system fails to
extract one or more of the desired subject, verb, or
object, based on the sentence at hand and without re-
gard to the target content. Second are content errors,
responses for which our system extracts the desired
subject, verb and object, but the resulting triple does
not accurately describe the image (i.e., is an error of
the participant?s). We are of course concerned with
reducing the triple errors. Examples are in Table 2.
Triple errors are subcategorized as speaker,
parser, or extraction errors, based on the earliest
part of the process that led to the error. Speaker
errors typically involve misspellings in the original
sentence, leading to an incorrect POS tag and parse.
Parser errors involve a correct sentence parsed in-
correctly or in such a way as to indicate a different
meaning from the one intended; an example is given
in Figure 5. Extraction errors involve a failure of the
extraction script to find one or more of the desired
subject, verb or object in a correct sentence. These
typically involve more complex sentence structures
such as conjoined or embedded clauses.
vroot Two boys boat
CD NNS NN
num
root
dep
NONE(boys,NONE)
vroot Two boys boat
CD NNS VBP
num
root
nsubj
boat(boys,NONE)
Figure 5: A parser error leading to a triple error (top), and
the desired parse and triple (bottom).
As shown in table 2, we obtain 92.3% accuracy on
extraction for NNS data and roughly the same for
NS data, 92.9%. However, many of the errors for
NNSs involve misspellings, while for NSs a higher
percentage of the extraction errors stem only from
our hand-written extractor, due to native speakers
using more complex structures. For a system inter-
acting with learners, spelling errors are thus more of
a priority (cf. Hovermale, 2008).
Content errors are subcategorized as spelling or
meaning errors. Spelling errors involve one or more
of the extracted subject, verb or object being mis-
spelled severely enough that the intended spelling
cannot be discerned. A spelling error here is un-
like those included in speaker errors above in that it
does not result in downstream errors and is a well-
16
Error Example
type Sentence Triple Count (%)
T
ri
pl
e
er
ro
r N
N
S
Speaker A man swipped leaves. leaves(swipped,man) 16 (4.1%)
Parser Two boys boat. NONE(boys,NONE) 5 (1.3%)
Extraction A man is gathering lots of leafs. gathering(man,lots) 9 (2.3%)
Total (390) 30 (7.7%)
N
S
Speaker (None) 0 (0%)
Parser An old man raking leaves on a path. leaves(man,path) 2 (1.4%)
Extraction A man has shot a bird that is falling from the sky. shot(bird,sky) 8 (5.7%)
Total (140) 10 (7.1%)
C
on
te
nt
er
ro
r
N
N
S Spelling The artiest is drawing a portret. drawing(artiest,portret) 36 (9.2%)
Meaning The woman is making her laundry. making(woman,laundry) 23 (5.9%)
Total (390) 59 (15.1%)
N
S
Spelling (None) 0 (0%)
Meaning A picture is being taken of a girl on a bike. taken(NONE,picture) 3 (2.1%)
Total (140) 3 (2.1%)
Table 2: Triple errors and content errors by subcategory, with error rates reported (e.g., 7.7% error = 92.3% accuracy)
formed triple except for a misspelled target word.
Meaning errors involve an inaccurate word within
the triple. This includes misspellings that result in a
real but unintended word (e.g., shout(man,bird) in-
stead of shoot(man,bird)).
The goal of a system is to identify the 15.1% of
NNS sentences which are content errors, in order
to provide feedback. Currently, the 7.7% triple er-
rors would also be grouped into this set, showing
the need for further extraction improvements. Also
notable is that three content errors were encountered
among the NS responses. All three were meaning
errors involving some meta-description of the image
prompt rather than a direct description of the image
contents, e.g., A picture is being taken of a girl on a
bike vs. A girl is riding a bike.
5.3 Semantic coverage
Given a fairly accurate extraction system, as re-
ported above, we now turn to evaluating how well
a gold standard represents unseen data, in terms of
semantic matching. To measure coverage, we take
the intuition that a language is defined by native
speaker usage, so their answers can serve as targets,
and use NS triples as our gold standard. The set
of NS responses was manually arbitrated to remove
any unacceptable triples (both triple and content er-
rors), and the remaining set of lemmatized triples
was taken as a gold standard set for each item.
Similarly, with the focus on coverage, the NNS
triples were amended to remove any triple errors.
From the remaining NNS triples, we call an appro-
priate NNS triple found in the gold standard set a
true positive (TP) (i.e., a correct match), and an
appropriate NNS triple not found in the gold stan-
dard set a false negative (FN) (i.e., an incorrect non-
match), as shown in Table 4. We adopt standard ter-
minology here (TP, FN), but note that we are inves-
tigating what should be in the gold standard, mak-
ing these false negatives and not false positives. To
address the question of how many (NS) sentences
we need to obtain good coverage, we define cover-
age (=recall) as TP/(TP+FN), and report, in Table 3,
23.5% coverage for unique triple types and 50.8%
coverage for triple tokens.
NNS
+ ?
NS
Y TP FP
N FN TN
Table 4: Contingency table comparing presence of NS
forms (Y/N) with correctness (+/?) of NNS forms
We define an inappropriate NNS triple (i.e., a con-
tent error) not found in the gold standard set as a true
17
Coverage Accuracy
Item NS NNS TP TN FN Ty. Tok. Ty. Tok.
1 5 14 3 2 9 3/12 23/38 5/14 25/39
2 6 14 3 5 6 3/9 15/28 8/14 20/32
3 6 19 5 7 7 5/12 23/30 12/19 30/36
4 4 8 2 2 4 2/6 32/37 4/8 34/39
5 4 24 1 8 15 1/16 3/25 9/24 11/33
6 8 22 3 5 14 3/17 16/31 8/22 21/36
7 7 23 5 4 14 5/19 14/35 9/23 18/39
8 6 23 5 6 11 5/16 10/30 11/22 17/36
9 7 33 3 12 18 3/21 3/23 15/33 15/35
10 5 21 2 13 6 2/8 14/24 15/21 27/35
Total 58 201 32 64 104 32/136 153/301 96/200 218/360
23.5% 50.8% 48.0% 60.6%
Table 3: Matching of semantic triples: NS/NNS: number of unique triples for NSs/NNSs. Comparing NNS types to NS
triples, TP: number of true positives (types); TN: number of true negatives; FN: number of false negatives. Coverage
for Types and Tokens = TPTP+FN ; Accuracy for Types and Tokens =
TP+TN
TP+TN+FN
negative (TN) (i.e., a correct non-match). Accu-
racy based on this gold standard?assuming perfect
extraction?is defined as (TP+TN)/(TP+TN+FN).3
We report 48.0% accuracy for types and 60.6% ac-
curacy for tokens.
The immediate lesson here is: NS data alone may
not make a sufficient gold standard, in that many cor-
rect NNS answers are not counted as correct. How-
ever, there are a couple of issues to consider here.
First, we require exact matching of triples. If
maximizing coverage is desired, extracting indi-
vidual subjects, verbs and objects from NS triples
and recombining them into the various possible
verb(subj,obj) combinations would lead to a sizable
improvement. An example of triples distribution and
coverage for a single item, along with this recombi-
nation approach is presented in Table 5.
It should be noted, however, that automat-
ing this recombination without lexical knowledge
could lead to the presence of unwanted triples
in the gold standard set. Consider, for exam-
ple, do(woman,shirt)?an incorrect triple derived
from the correct NS triples, wash(woman,shirt) and
do(woman,laundry). In addition to handling pro-
3Accuracy is typically defined as
(TP+TN)/(TP+TN+FN+FP), but false positives (FPs) are
cases where an incorrect learner response was in the gold
standard, and we have already removed such cases (i.e., FP=0).
Type NNS NS Coverage
cut(woman,apple) 5 0 (5)
cut(someone,apple) 4 2 4
cut(somebody,apple) 3 0
cut(she,apple) 3 0
slice(someone,apple) 2 5 2
cut(person,apple) 2 1 2
cut(NONE,apple) 2 0 (2)
slice(woman,apple) 1 1 1
slice(person,apple) 1 1 1
slice(man,apple) 1 0
cut(person,fruit) 1 0
cut(people,apple) 1 0
cut(man,apple) 1 0
cut(knife,apple) 1 0
chop(woman,apple) 1 0
chop(person,apple) 1 0
slice(NONE,apple) 0 2
Total 30 12 10 (17)
Table 5: Distribution of valid tokens across types for a
single PDT item. Types in italics do not occur in the NS
sample, but could be inferred to expand coverage by re-
combining elements of NS types that do occur.
nouns (e.g., cut(she,apple)) and lexical relations
(e.g., apple as a type of fruit), one approach might be
18
to prompt NSs to give multiple alternative descrip-
tions of each PDT item.
A second issue to consider is that, even when only
examining cases where the meaning is literally cor-
rect, NNSs produce a wider range of forms to de-
scribe the prompts than NSs. For example, for a pic-
ture showing what NSs overwhelmingly described
as a raking action, many NNSs referred to a man
cleaning an area. Literally, this may be true, but it is
not native-like. This behavior is somewhat expected,
given that learners are encouraged to use words they
know to compensate for gaps in their vocabularies
(Agust??n Llach, 2010). This also parallels the obser-
vation in SLA research that while second language
learners may attain native-like grammar, their abil-
ity to use pragmatically native-like language is often
much lower (Bardovi-Harlig and Do?rnyei, 1998).
The answer to what counts as a correct meaning
will most likely lie in the purpose of an application,
reflecting whether one is developing native-ness or
whether the facts of a situation are expressed cor-
rectly. In other words, rather than rejecting all non-
native-like responses, an ILT may need to consider
whether a sentence is native-like or non-native-like
as well as whether it is semantically appropriate.
6 Summary and Outlook
We have begun the process of examining appro-
priate ways to analyze the semantics of language
learner constructions for interactive situations by
describing data collected for a picture description
task. We parsed this data using an off-the-shelf
parser with settings geared towards obtaining appro-
priate semantic forms, wrote a small set of seman-
tic extraction rules, and obtained 92?93% extrac-
tion accuracy. This shows promise at using images
to constrain the syntactic form of a ?free? learner
text and thus be able to use pre-built software. At
the same time, we discussed how learners give re-
sponses which are literally correct, but are non-
native-like. These results can help guide the de-
velopment of ILTs which aim to process the mean-
ing of interactive statements: there is much to be
gained with a small amount of computational effort,
but much work needs to go into delineating a proper
set of gold standard forms.
There are several ways to take this work. First,
given the preponderance of spelling errors in NNS
data and its effect on downstream processing, the ef-
fect of automatic spelling correction must be taken
into account. Secondly, we only investigated tran-
sitive verbs, and much needs to be done to investi-
gate interactions with other types of constructions,
including the definition of more elaborate semantic
forms (Hahn and Meurers, 2012). Finally, to bet-
ter model ILTs and the interactions found in activ-
ities and games, one can begin by modeling more
complex visual prompts. By using video description
tasks or story retell tasks, we can elicit more com-
plex narrative responses. This would allow us to
investigate the possibility of extending our current
approach to tasks that involve greater learner inter-
action.
Acknowledgments
We would like to thank the task participants, David
Stringer for assistance in developing the task, Kath-
leen Bardovi-Harlig, Marlin Howard and Jayson
Deese for recruitment help, and Ross Israel for eval-
uation discussion. For their helpful feedback, we
would also like to thank the three anonymous re-
viewers and the attendees of the Indiana University
Linguistics Department Graduate Student Confer-
ence.
References
Eneko Agirre, Mona Diab, Daniel Cer, and Aitor
Gonzalez-Agirre. 2012. Semeval-2012 task 6: a
pilot on semantic textual similarity. In Proceed-
ings of the First Joint Conference on Lexical and
Computational Semantics - Volume 1: Proceed-
ings of the main conference and the shared task,
and Volume 2: Proceedings of the Sixth Inter-
national Workshop on Semantic Evaluation, Se-
mEval ?12, pages 385?393. Association for Com-
putational Linguistics, Stroudsburg, PA, USA.
Maria Pilar Agust??n Llach. 2010. Lexical gap-filling
mechanisms in foreign language writing. System,
38(4):529 ? 538.
Luiz Amaral and Detmar Meurers. 2011. On using
intelligent computer-assisted language learning in
real-life foreign language teaching and learning.
ReCALL, 23(1):4?24.
19
Luiz Amaral, Detmar Meurers, and Ramon Ziai.
2011. Analyzing learner language: Towards a
flexible NLP architecture for intelligent language
tutors. Computer Assisted Language Learning,
24(1):1?16.
Kathleen Bardovi-Harlig and Zolta?n Do?rnyei. 1998.
Do language learners recognize pragmatic vio-
lations? Pragmatic versus grammatical aware-
ness in instructed L2 learning. TESOL Quarterly,
32(2):233?259.
Marianne Celce-Murcia. 1991. Grammar pedagogy
in second and foreign language teaching. TESOL
Quarterly, 25:459?480.
Marianne Celce-Murcia. 2002. Why it makes sense
to teach grammar through context and through
discourse. In Eli Hinkel and Sandra Fotos, editors,
New perspectives on grammar teaching in second
language classrooms, pages 119?134. Lawrence
Erlbaum, Mahwah, NJ.
David L. Chen and William B. Dolan. 2011. Collect-
ing highly parallel data for paraphrase evaluation.
In Proceedings of the 49th Annual Meeting of the
Association for Computational Linguistics (ACL-
2011). Portland, OR.
Martin Chodorow, Joel Tetreault, and Na-Rae Han.
2007. Detection of grammatical errors involv-
ing prepositions. In Proceedings of the 4th ACL-
SIGSEM Workshop on Prepositions, pages 25?30.
Prague.
Michael Collins. 1999. Head-Driven Statistical
Models for Natural Language Parsing. Ph.D. the-
sis, University of Pennsylvania, Philadelphia, PA.
Ronan Collobert, Jason Weston, Le?on Bottou,
Michael Karlen, Koray Kavukcuoglu, and Pavel
Kuksa. 2011. Natural language processing (al-
most) from scratch. Journal of Machine Learning
Research (JMLR), 12:2461?2505.
Robert Dale, Ilya Anisimoff, and George Narroway.
2012. HOO 2012: A report on the preposition
and determiner error correction shared task. In
Proceedings of the Seventh Workshop on Building
Educational Applications Using NLP, pages 54?
62. Montre?al.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses.
In Proceedings of LREC 2006. Genoa, Italy.
Marie-Catherine de Marneffe and Christopher D.
Manning. 2012. Stanford typed dependencies
manual. Originally published in September 2008;
Revised for Stanford Parser v. 2.0.4 in November
2012.
William DeSmedt. 1995. Herr Kommissar: An
ICALL conversation simulator for intermedi-
ate german. In V. Holland, J. Kaplan, and
M. Sams, editors, Intelligent Language Tutors.
Theory Shaping Technology, pages 153?174.
Lawrence Erlbaum Associates, Inc., New Jersey.
Rod Ellis. 2000. Task-based research and lan-
guage pedagogy. Language teaching research,
4(3):193?220.
Rod Ellis. 2006. Current issues in the teaching of
grammar: An SLA perspective. TESOL Quar-
terly, 40:83?107.
Christiane Fellbaum, editor. 1998. WordNet: An
Electronic Lexical Database. The MIT Press,
Cambridge, MA.
Katrina Forbes-McKay and Annalena Venneri.
2005. Detecting subtle spontaneous language de-
cline in early Alzheimer?s disease with a pic-
ture description task. Neurological sciences,
26(4):243?254.
Michael Hahn and Detmar Meurers. 2012. Evalu-
ating the meaning of answers to reading compre-
hension questions: A semantics-based approach.
In Proceedings of the 7th Workshop on Innova-
tive Use of NLP for Building Educational Appli-
cations (BEA7), pages 326?336. Association for
Computational Linguistics, Montreal, Canada.
Trude Heift and Devlan Nicholson. 2001. Web de-
livery of adaptive and interactive language tutor-
ing. International Journal of Artificial Intelli-
gence in Education, 12(4):310?325.
Trude Heift and Mathias Schulze. 2007. Errors
and Intelligence in Computer-Assisted Language
Learning: Parsers and Pedagogues. Routledge.
DJ Hovermale. 2008. Scale: Spelling correction
adapted for learners of English. Pre-CALICO
Workshop on ?Automatic Analysis of Learner
20
Language: Bridging Foreign Language Teach-
ing Needs and NLP Possibilities?. March 18-19,
2008. San Francisco, CA.
Dan Klein and Christopher D. Manning. 2003. Ac-
curate unlexicalized parsing. In Proceedings of
ACL-03. Sapporo, Japan.
Sandra Ku?bler, Ryan McDonald, and Joakim Nivre.
2009. Dependency Parsing. Morgan & Claypool
Publishers.
Diane Larsen-Freeman. 2002. Teaching grammar.
In Diane Celce-Murcia, editor, Teaching English
as a second or foreign language, pages 251?266.
Heinle & Heinle, Boston, third edition.
Claudia Leacock and Martin Chodorow. 2003. C-
rater: Automated scoring of short-answer ques-
tions. Computers and Humanities, pages 389?
405.
Detmar Meurers. 2012. Natural language processing
and language learning. In Carol A. Chapelle, ed-
itor, Encyclopedia of Applied Linguistics. Black-
well. to appear.
Detmar Meurers, Ramon Ziai, Niels Ott, and Stacey
Bailey. 2011. Integrating parallel analysis mod-
ules to evaluate the meaning of answers to reading
comprehension questions. Special Issue on Free-
text Automatic Evaluation. International Journal
of Continuing Engineering Education and Life-
Long Learning (IJCEELL), 21(4):355?369.
Kenneth A. Petersen. 2010. Implicit Corrective
Feedback in Computer-Guided Interaction: Does
Mode Matter? Ph.D. thesis, Georgetown Univer-
sity, Washington, DC.
Peter Skehan, Pauline Foster, and Uta Mehnert.
1998. Assessing and using tasks. In Willy Re-
nandya and George Jacobs, editors, Learners and
language learning, pages 227?248. Seameo Re-
gional Language Centre.
21
Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications, pages 169?179,
Atlanta, Georgia, June 13 2013. c?2013 Association for Computational Linguistics
Inter-annotator Agreement for Dependency Annotation of Learner
Language
Marwa Ragheb
Indiana University
Bloomington, IN USA
mragheb@indiana.edu
Markus Dickinson
Indiana University
Bloomington, IN USA
md7@indiana.edu
Abstract
This paper reports on a study of inter-
annotator agreement (IAA) for a dependency
annotation scheme designed for learner En-
glish. Reliably-annotated learner corpora are
a necessary step for the development of POS
tagging and parsing of learner language. In
our study, three annotators marked several
layers of annotation over different levels of
learner texts, and they were able to obtain
generally high agreement, especially after dis-
cussing the disagreements among themselves,
without researcher intervention, illustrating
the feasibility of the scheme. We pinpoint
some of the problems in obtaining full agree-
ment, including annotation scheme vagueness
for certain learner innovations, interface de-
sign issues, and difficult syntactic construc-
tions. In the process, we also develop ways to
calculate agreements for sets of dependencies.
1 Introduction
Learner corpora have been essential for develop-
ing error correction systems and intelligent tutor-
ing systems (e.g., Nagata et al, 2011; Rozovskaya
and Roth, 2010). So far, error annotation has been
the main focus, to the exclusion of corpora and an-
notation for more basic NLP development, despite
the need for parse information for error detection
(Tetreault et al, 2010), learner proficiency identifi-
cation (Hawkins and Buttery, 2010), and acquisition
research (Ragheb and Dickinson, 2011). Indeed,
there is very little work on POS tagging (Thoue?sny,
2009; van Rooy and Scha?fer, 2002; de Haan, 2000)
or parsing (Rehbein et al, 2012; Krivanek and Meur-
ers, 2011; Ott and Ziai, 2010) learner language, and,
not coincidentally, there is a lack of annotated data
and standards for these tasks. One issue is in know-
ing how to handle innovative learner forms: some
map to a target form before annotating syntax (e.g.,
Hirschmann et al, 2010), while others propose di-
rectly annotating the text (e.g., Ragheb and Dick-
inson, 2011). We follow this latter strand and fur-
ther our work towards a syntactically-annotated cor-
pus of learner English by: a) presenting an annota-
tion scheme for dependencies, integrated with other
annotation layers, and b) testing the inter-annotator
agreement for this scheme. Despite concerns that di-
rect annotation of the linguistic properties of learn-
ers may not be feasible (e.g., Rose?n and Smedt,
2010), we find that annotators have generally strong
agreement, especially after adjudication, and the
reasons for disagreement often have as much to do
with the complexities of syntax or interface issues as
they do with learner innovations.
Probing grammatical annotation can lead to ad-
vancements in research on POS tagging and syntac-
tic parsing of learner language, for it shows what can
be annotated reliably and what needs additional di-
agnostics. We specifically report on inter-annotator
agreement (IAA) for the annotation scheme de-
scribed in section 2, focusing on dependency an-
notation. There are numerous studies investigating
inter-annotator agreement between coders for differ-
ent types of grammatical annotation schemes, focus-
ing on part-of-speech, syntactic, or semantic anno-
tation (e.g., Passonneau et al, 2006; Babarczy et al,
2006; Civit et al, 2003). For learner language, a
169
number of error annotation projects include mea-
sures of interannotator agreement, (see, e.g., Boyd,
2012; Lee et al, 2012; Rozovskaya and Roth, 2010;
Tetreault and Chodorow, 2008; Bonaventura et al,
2000), but as far as we are aware, there have been no
studies on IAA for grammatical annotation.
We have conducted an IAA study to investigate
the quality and robustness of our annotation scheme,
as reported in section 3. In section 4, we report quan-
titative results and a qualitative analysis of this study
to tease apart disagreements due to inherent ambigu-
ity or text difficulty from those due to the annotation
scheme and/or the guidelines. The study has already
reaped benefits by helping us to revise our annota-
tion scheme and guidelines, and the insights gained
here should be applicable for future development of
other annotation schemes and to parsing studies.
On a final note, our dependency annotation allows
for multiple heads for each token in the corpus, vi-
olating the so-called single-head constraint (Ku?bler
et al, 2009). In the process of evaluating these de-
pendencies (see section 4.1), we also make some mi-
nor contributions towards comparing sets of depen-
dencies, moving beyond just F-measure (e.g., Cer
et al, 2010) to account for partial agreements.
2 Annotation scheme
We present a sketch of the annotation scheme here,
outlining the layers and the general motivation. Our
general perspective is to annotate as closely as pos-
sible to what the learner wrote, marking grammat-
ical properties even if the meaning of the sentence
or clause is unclear within the particular grammat-
ical analysis. For example, in the learner sentence
(1), the verb admit clearly occurs in the form of
an active verb, and is annotated as such, regard-
less of the (passive) meaning of the sentence (cf.
was admitted). In this case, basing the annotation
on syntactic evidence makes for a more straightfor-
ward task. Moreover, adhering to a syntactic anal-
ysis helps outline the grammatical properties of a
learner?s interlanguage and can thus assist in auto-
matic tasks such as native language identification
(e.g., Tetreault et al, 2012), and proficiency level de-
termination (Yannakoudakis et al, 2011).
(1) When I admit to Korea University, I decide
...
Another part of the motivation for shying away
from marking target forms and annotating the syn-
tactic properties of those (cf., e.g., Rehbein et al,
2012) is that, for general essays from learners of
many levels, the grammatical evidence can be un-
derstood even when the intended meaning is not.
Consider (2): in the context of the learner?s es-
say, the sentence probably means that this person
guards their personal belongings very well because
of prevalent theft in the city they are talking about.
(2) Now I take very hard my personal stuffs.
Annotating the syntax of a target form here could
obscure the grammatical properties of the learner?s
production (e.g., pluralizing a mass noun). Encour-
aging annotators to focus on the syntactic properties
and not intended meanings makes identifying the de-
pendency relations in a sentence like this one easy.
Another aspect of our annotation scheme is that
we do not directly annotate errors (except for lexi-
cal violations; see section 2.1). Annotators had ac-
cess to an extensive manual detailing the annotation
scheme, which will be made public soon.1 A brief
outline of the guidelines is in section 3.3.
2.1 Initial annotation layers
Using ideas developed for annotating learner lan-
guage (Ragheb and Dickinson, 2012, 2011; D??az-
Negrillo et al, 2010; Dickinson and Ragheb, 2009),
we annotate several layers before targeting depen-
dencies: 1) lemmas (i.e., normalized forms), 2) mor-
phological part-of-speech (POS), 3) distributional
POS, and 4) lexical violations.
The idea for lemma annotation is to normalize a
word to its dictionary form. In (3), for example, the
misspelled excersice is normalized to the correctly-
spelled exercise for the lemma annotation. We spec-
ify that only ?reasonable? orthographic or phonetic
changes are allowed; thus, for prison, it is lemma-
annotated as prison, not person. In this case, the
lemma annotation does not affect the rest of the an-
notation, as prison and person are both nouns, but
for no, the entire analysis changes based on whether
we annotate the lemma as no or not. Marking no
makes the final tree more difficult, but fits with the
principle of staying true to the form the learner has
1See: http://cl.indiana.edu/?salle
170
presented. As we will see in section 4.3, determining
the lemma can pose challenges for building trees.
(3) After to start , I want to tell that this excer-
sice is very important in the life , no only as
a prison .
We annotate two POS layers, one capturing mor-
phological evidence and one for distributional. For
most words, the layers include the same informa-
tion, but mismatches arise with non-canonical struc-
tures. For instance, in (3) the verb (to) start has a
morphological POS of base form verb (VV0), but
it appears in a context where some other verb form
would better be licensed, e.g., a gerund. Since we
do not want to overstate claims, we allow for un-
derspecified POS tags and annotate the distributional
POS simply as verb (VV). The use of two POS lay-
ers captures the mismatch between morphology and
distribution without referencing a unified POS.
Finally, annotators can mark lexical violations
when nothing else appears to capture a non-standard
form. Specifically, lexical violations are for syntac-
tically ungrammatical forms where the specific word
choice seems to cause the ungrammaticality. In (4),
for example, about should be marked as a lexical vi-
olation. Lexical violations were intended as a last re-
sort, but as we will see in section 4.3, there was con-
fusion about when to use lexical violations and when
to use other annotations, e.g., POS mismatches.
(4) ... I agree about me that my country ?s help
and cooperation influenced . . .
2.2 Dependencies
While the initial annotation layers are used to build
the syntactic annotation, the real focus of the anno-
tation concerns dependencies. Using a set of 45 de-
pendencies,2 we mark two types of annotations here:
1) dependency relations rooted in the lemma and the
morphological POS tag, and 2) subcategorization in-
formation, reflecting not necessarily what is in the
tree, but what is required. Justification for a mor-
phological, or morphosyntactic, layer of dependen-
cies, along with a layer of subcategorization, is given
in Ragheb and Dickinson (2012). Essentially, these
two layers allow one to capture issues involving ar-
gument structure (e.g., missing argument), without
2We use a label set adapted from Sagae et al (2010).
having to make the kind of strong claims a layer of
distributional dependencies would require. In (5),
for example, wondered subcategorizes for a finite
complement (COMP), but finds a non-finite comple-
ment (XCOMP), as the tree is based on the morpho-
logical forms (e.g., to).
(5) I wondered what success to be .
An example tree is shown in figure 1, where we
can see a number of properties of our trees: a) we
annotate many ?raised? subjects, such as I being the
subject (SUBJ) of both would and like, thereby al-
lowing for multiple heads for a single token; b) we
ignore semantic anomalies, such as the fact that life
is the subject of be (successful); and c) dependencies
can be selected for, but not realized, as in the case of
career subcategorizing for a determiner (DET).
3 Inter-annotator agreement study
3.1 Selection of annotation texts
From a learner corpus of written essays we have col-
lected from students entering Indiana University, we
chose a topic (What Are Your Plans for Life?) and
randomly selected six essays, based on both learner
proficiency (beginner, intermediate, advanced) and
the native language of the speaker (L1).3 From each
essay, we selected the first paragraph and put the six
paragraphs into two texts; each text contained, in
order, one beginner, one intermediate, and one ad-
vanced paragraph. Text 1 contained 19 sentences
(333 tokens), and Text 2 contained 22 sentences
(271 tokens). Annotators were asked to annotate
only these excerpts, but had access to the entire es-
says, if they wanted to view them.
While the total number of tokens is only 604, the
depth of the annotation is quite significant, in that
there are at least seven decisions to be made for ev-
ery token: lemma, lexical violation, morphological
POS, distributional POS, subcategorization, attach-
ment, and dependency label, in addition to possi-
ble extra dependencies for a given word, i.e., a few
thousand decisions. It is hard to quantify the ef-
fort, as some layers are automatically pre-annotated
(see section 3.5) and some are used sparingly (lexi-
cal violations), but we estimate around 2000 new or
changed annotations from each annotator.
3Korean, Spanish, Chinese, Arabic, Japanese, Hungarian.
171
ROOT I would like my life to be successful in career ...
<ROOT> <SUBJ,VC> <SUBJ,OBJ,XCOMP> <DET> <VC> <SUBJ,PRED> <POBJ> <DET> ...
SUBJ
SUBJ
ROOT VC
DET
OBJ
SUBJ
XCOMP
VC PRED JCT POBJ
Figure 1: Morphosyntactic dependency tree with subcategorization information
3.2 Annotators
This study involved three annotators, who were un-
dergraduate students at Indiana. They were native
speakers of English and majors in Linguistics (2 ju-
niors, 1 senior). Two had had a syntax course before
the semester, and one was taking it concurrently.
We trained them over the course of an academic
semester (fall 2012), by means of weekly meetings
to discuss relevant readings, familiarize them with
the scheme, and give feedback about their annota-
tion. The IAA study took place Nov. 9?Dec. 15.
Annotators were taking course credit for partici-
pating in this project. This being the case, they were
encouraged to learn from the experience, and part
of their training was to make notes of challenging
cases and their decision-making process. This has
provided significant depth in qualitatively analyzing
the IAA outcomes (section 4.3).
3.3 Guidelines
At the start of the study, the annotators were given
a set of guidelines (around 100 pages) to reference
as they made decisions. These guidelines outline
the general principles of the scheme (e.g., give the
learner the benefit of the doubt), an overview of the
annotation layers, and annotation examples for each
layer. The guidelines refer to the label sets used
for POS (Sampson, 1995) and dependencies (Sagae
et al, 2010), but emphasize the properties of our
scheme. Although the guidelines discuss general
syntactic treatment (e.g., ?attach high? in the case of
attachment ambiguities), a considerable focus is on
handling learner innovations, across different layers.
While we cannot list every example of how learners
innovate, we include instructions and examples that
should generalize to other non-native constructions
(e.g., when to underspecify a label). Examples of
Text 1 Text 2
Time Avg. Min. Max. Time Avg. Min. Max.
A 224 11.8 3 25 151 6.9 2 21
B 280 14.7 4 30 170* 8.5 3 20
C 480 25.3 8 60 385 17.5 10 45
Table 1: Annotation time, in minutes, for phase 1 (*times
for two sentences were not reported and are omitted)
how to treat difficult syntactic constructions are also
illustrated (e.g., coordination).
3.4 Annotation task
Via oral and written instructions, the annotators
were asked to independently annotate the two texts
and take notes on difficult issues, in addition to
marking how long they spent on each sentence.
Times are reported in table 1 for the first phase, as
described next. Longer sentences take more time
(cf. Text 1 vs. Text 2), and annotator times vary,
but, given the times of nearly 30?60 minutes per sen-
tence at the start of the semester, these times seemed
reasonable for the depth of annotation required.
The annotation task proceeded in phases. Phase
1: Text 1 was annotated over the course of one
week, and Text 2 over the next week. Phase 2: Af-
ter an hour-long meeting with annotators covering
general annotation points that seemed to be prob-
lematic (e.g., lemma definitions), they were given
another week to individually go over their annota-
tions and make modifications. At the meeting, noth-
ing about the scheme or guidelines was added, and
no specific examples from the data being annotated
were used (only ones from earlier in the semester).
Phase 3: Each annotator received a document point-
ing out pairwise disagreements between annotators,
in a simple textual format like (6). Each annota-
172
tor was asked to use this document and make any
changes where they thought that their analysis was
not the best one, given the other two. This process
took approximately a week. Phase 4: The annota-
tors met (for three hours) and discussed remaining
differences, to see whether they could reach a con-
sensus. Each annotator fixed their own file based on
the results of this discussion. At each point, we took
a snapshot of the data, but at no point did we provide
feedback to the annotators on their decisions.
(6) Sentence 2, word 1: relation ... JCT NJCT
3.5 Annotation interface
The annotation is done via the Brat rapid annotation
tool (Stenetorp et al, 2012).4 This online interface,
shown in figure 2, allows an annotator to drag an
arrow between words to create a dependency. An-
notators were given automatically-derived POS tags
from TnT (Brants, 2000), trained on the SUSANNE
corpus (Sampson, 1995), but created the dependen-
cies from scratch.5 Subcategorizations, lemmas, and
lexical violations are annotated within one of the
POS layers; lemmas are noted by the blue shading,
and the presence of other layers is noted by asterisks,
an interface point discussed in section 4.2.3. Anno-
tators liked the tool, but complained of its slowness.
4 Evaluation
4.1 Methods of comparison
For lemma and POS annotation, we can calculate
basic agreement statistics, as there is one annotation
for each token. But our primary focus is on subcat-
egorization and dependency annotation, where there
can be multiple elements (or none) for a given token.
For subcategorization, we treat elements as mem-
bers of a set, as annotators were told that order was
unimportant (e.g., <SUBJ,OBJ> = <OBJ,SUBJ>);
we discuss metrics for this in section 4.1.1. For de-
pendencies, we adapt standard parse evaluation (see
Ku?bler et al, 2009, ch. 6). In brief, unlabeled at-
tachment agreement (UAA) measures the number
of attachments annotators agree upon for each token,
disregarding the label, whereas labeled attachment
4http://brat.nlplab.org
5Annotators need to provide the dependency annotations
since we lacked an appropriate L2 parser. It is a goal of this
project to provide annotated data for parser development.
agreement (LAA) requires both the attachment and
labeling to be the same to count as an agreement.
Label only agreement (LOA) ignores the head a
token attaches to and only compares labels.
All three metrics (UAA, LAA, LOA) require cal-
culations for sets of dependencies, described in sec-
tions 4.1.1 and 4.1.2. In figure 3, for instance, one
annotator (accidentally) drew a JCT arrow in the
wrong direction, resulting in two heads for is. For
is, the annotator?s set of dependencies is {(0,ROOT),
(1,JCT)}, compared to another?s of {(0,ROOT)}. We
thus treat dependencies as sets of (head, label) pairs.
4.1.1 Metrics
For sets, we use two different calculations. First is
MASI (Measuring Agreement on Set-valued Items,
Passonneau et al, 2006), which assigns each com-
parison between sets a value between 0 and 1, as-
signing partial credit for partial set matches and al-
lowing one to treat agreement on a per-token basis.
We use a simplified form of MASI as follows: 1 =
identical sets, 23 = one set is a subset of the other,
1
3
= the intersection of the sets is non-null, and so are
the set differences, & 0 = disjoint sets.6
The second method is a global comparison
method (GCM), which counts all the elements in
each annotator?s sets in the whole file and counts
up the total number of agreements. In the following
subcategorization example over three tokens, there
are two agreements, compared to four total elements
used by A1 (GCMA1 = 24 ) and compared to three
elements used by A2 (GCMA2 = 23 ). These metrics
are essentially precision and recall, depending upon
which annotator is seen as the ?gold? (Ku?bler et al,
2009, ch. 6). For MASI scores, we have 0, 1, and 13 ,
respectively, giving 113/3, or 0.44.
? A1: {SUBJ}, A2: {}
? A1: {SUBJ}, A2: {SUBJ}
? A1: {SUBJ,PRED}, A2: {SUBJ,OBJ}
Since every word is annotated, the methods as-
sign similar numbers for dependencies. Subcatego-
rization gives different results, due to empty sets. If
annotator 1 and annotator 2 both mark an empty set,
6Since our sets tend to be small (rarely bigger than two), we
do not expect much change with a full MASI calculation.
173
Figure 2: Example of the annotation interface
root In my opinion , My Age is Very Young
JCT
DET
POBJ
PUNCT
DET SUBJ
ROOT
JCT
PRED
Figure 3: A mistaken arrow (JCT) leading to two dependencies for is ((0,ROOT),(1,JCT))
we count full agreement for MASI, i.e., a score of 1;
for GCM, nothing gets added to the totals.
We could, of course, report various coefficients
commonly used in IAA studies, such as kappa or
alpha (see Artstein and Poesio, 2008), but, given
the large number of classes and lack of predominant
classes, chance agreement seems very small.
4.1.2 Dependency-specific issues
As a minor point: for dependencies, we calcu-
late agreements for matches in only attachment or
labeling. Consider (7), where there is one match
only in attachment ((24,OBJ)-(24,JCT)), counting to-
wards UAA, and one only in labeling ((24,SUBJ)-
(22,SUBJ)) for LOA. Importantly, we have to ensure
that (24,SUBJ) and (24,JCT) are not linked.
(7) A1: {(24,SUBJ), (24,OBJ)}
A2: {(22,SUBJ), (24,JCT)}
In general, we prioritize identical attachment over
labeling, if a dependency could match in either.
We wrote a short script to align attachment/label
matches between two sets, but omit details here, due
to space. We generally do not have large sets of de-
pendencies to compare, but these technical decisions
should allow for any situation in the future.
4.2 Results
4.2.1 Bird?s-eye view
Table 2 presents an overview of pairwise agree-
ments between annotators for all 604 tokens. Of the
four phases of annotation, we report two: the files
they annotated (and revised) independently (phase
2) and the final files after discussion of problematic
cases (phase 4). Annotators reported feeling rushed
during phase 1, so phase 2 numbers likely better
indicate the ability to independently annotate, and
phase 4 can help to investigate the reasons for lin-
gering disagreements. The numbers for subcatego-
rization and dependency (UAA, LAA) agreements
are the MASI agreement rates.
A few observations are evident from these fig-
ures. First, for both POSm (morphology) and POSd
(distribution), the high agreement rates reflect the
fact that annotators made very few changes to the
automatic pre-annotation, partly because such lay-
ers were not heavily emphasized. Lemmas were
also pre-annotated, as identical to the surface form,
but more changes were made here (decapitaliza-
tion, affix-stripping, etc.). Comparing phases 2 and
4 shows an improvement in agreement, although
agreement seems like it could be higher, given the
simplicity of lemma information. We discuss lem-
mas, and associated lexical violations, more in sec-
174
Annotators lemma POSm POSd Subcat. UAA LAA
P2 P4 P2 P4 P2 P4 P2 P4 P2 P4 P2 P4
A, B 93.4 96.9 99.0 98.7 99.2 98.7 85.5 94.0 86.6 97.0 80.0 95.2
B, C 94.4 97.7 99.0 99.5 98.7 99.3 86.1 95.7 86.7 97.1 80.3 96.0
C, A 92.4 96.9 99.7 99.7 98.5 99.3 86.1 96.6 86.9 97.7 82.4 96.7
Table 2: Overview of agreement rates before & after discussion (phases 2 & 4)
tion 4.3.
Dependency-related annotations had no pre-
annotation. While the starting value of agreement
rates for these last three layers is not as high as for
lemma and POS annotation, agreement rates around
80?85% still seem moderately high. More important
is how much the agreement rates improved after dis-
cussion, achieving approximately 95% agreement.
This was without any direct intervention from the re-
searchers regarding how to annotate disagreements.
We examine dependencies in section 4.2.2 and sub-
categorization in 4.2.3, breaking results down by
text to see differences in difficulty.
4.2.2 Dependencies
We report MASI agreement rates for dependen-
cies in tables 3 and 4 for Text 1 and Text 2, re-
spectively.7 Comparing the starting agreement val-
ues (e.g., 73.6% vs. 87.8% LAA for annotators A
and B), it is clear that text difficulty had an enor-
mous impact on annotator agreement. The clear dif-
ference in tokens per sentence (17.5 in Text 1 vs.
12.3 in Text 2; see section 3.1) contributed to the
differences. The reported difficulty from annotators
referred to more non-native properties present in the
text, and, to a smaller extent, the presence of more
complex syntactic structures. Though we take up
some of these issues up again in section 4.3, an in-
depth analysis of how text difficulty affects the an-
notation task is beyond the scope of this paper, and
we leave it for future investigation.
Looking at the agreement rates for Text 1 in ta-
ble 3, we can see that the initial rates of agree-
ment for UAA and LOA are moderately high, indi-
cating that annotator training and guideline descrip-
tions were working moderately well. However, they
7We only report MASI scores for dependencies, since the
GCM scores are nearly the same. For example, for raters A &
B, the GCM value for phase 4 is 96.15% with respect to either
annotator vs. 96.10% for MASI.
Ann. UAA LAA LOA
P2 P4 P2 P4 P2 P4
A, B 81.8 96.1 73.6 93.4 80.3 95.5
B, C 80.9 96.2 73.4 94.4 79.3 97.1
A,C 83.6 97.6 79.7 96.7 81.8 97.9
Table 3: MASI percentages for dependencies, Text 1
Ann. UAA LAA LOA
P2 P4 P2 P4 P2 P4
A, B 92.6 98.1 87.8 97.4 89.3 97.8
B, C 93.8 98.3 88.7 97.9 90.2 98.6
A, C 90.9 97.9 85.7 96.8 87.6 97.9
Table 4: MASI percentages for dependencies, Text 2
are only 73% for LAA. Note, though, that this may
be more related to issues of fatigue and hurry than
of understanding of the guidelines: the numbers im-
prove considerably by phase 4. The labeled attach-
ment rates, for example, increase between 17 and 21
percent, to reach values around 95%.
For Text 2 in table 4, we notice again the higher
phase 2 rates and the similar improvement in phase
4, with LAA around 97%. Encouragingly, despite
the initially lower agreements for Text 1, annotators
were able to achieve nearly the same level of agree-
ment as for the ?easier? text. This illustrates that
annotators can learn the scheme, even for difficult
sentences, though there may be a tradeoff between
speed and accuracy.
4.2.3 Subcategorization
For subcategorization, we present both MASI and
GCM percentage rates, as they give different em-
phases. Results are again broken down by text, in
tables 5 and 6. As with dependencies, we see solid
improvement from phase 2 to phase 4, and we see
175
generally higher agreement for Text 2.
Ann. MASI GCM1 GCM2
P2 P4 P2 P4 P2 P4
A,B 84.3 92.4 81.9 90.8 72.8 88.1
B,C 83.6 93.8 74.4 91.6 73.6 90.2
A,C 84.9 96.1 83.0 96.4 73.1 92.2
Table 5: Agreement rates for subcategorization, Text 1
Ann. MASI GCM1 GCM2
P2 P4 P2 P4 P2 P4
A,B 87.1 95.9 88.9 96.0 77.2 94.1
B,C 89.3 98.0 88.3 98.0 82.0 96.8
A,C 87.6 97.2 91.2 97.3 73.7 94.2
Table 6: Agreement rates for subcategorization, Text 2
The GCM numbers are much lower because of the
way empty subcategorization values are handled?
being counted towards agreement for MASI and
not for GCM (see section 4.1.1). A further issue,
though, is that one annotator often simply left out
subcategorization annotation for a token. In table 6,
for example, annotators A and C have vastly differ-
ent GCM values for phase 2 (91.2% vs. 73.7%), due
to annotator C annotating many more subcategoriza-
tion labels. This is discussed more in section 4.3.2.
4.3 Qualitative differences
We highlight some of the important issues that stand
out when we take a closer look at the nature of the
disagreements in the final phase.
4.3.1 Text-related issues
As pointed out earlier regarding the differences
between Text 1 and Text 2 (section 4.2.2), some dis-
agreements are likely due to the nature of the text
itself, both because of its non-native properties and
because of the syntactic complexity. Starting with
unique learner innovations leading to non-uniform
treatment, several cases stemmed from not agreeing
on the lemma, when a word looks non-English or
does not fit the context. An example is cares in (8):
although the guidelines should lead the annotators to
choose care as the lemma, staying true to the learner
form, one annotator chose to accommodate the con-
text and changed the lemma to case. This relying
too heavily on intended meaning and not enough on
syntactic evidence?as the scheme is designed for?
was a consistent problem.
(8) My majors are bankruptcy , corporate reor-
ganizations . . . and arquisisiton cares .
For (8), the trees do not change because the dif-
ferent lemmas are of the same syntactic category,
but more problematic are cases where the trees differ
based on different readings. In the learner sentence
(9), the non-agreement between this and cause led to
a disagreement of this being a COORD of and vs. this
being an APPOS (appositive) of factors. The anno-
tator reported that the choice for this latter analysis
came from treating this as these, again contrary to
guidelines but consistent with one meaning.
(9) Sometimes animals are subjected to changed
environmental factors during their develop-
mental process and this cause FA .
Another great source of disagreement stems from
the syntactic complexity of some of the structures,
even if native-like, though this can be intertwined
with non-native properties, as in (10). Although an-
notators eventually agreed on the annotation here,
there was initial disagreement on the coordination
structure of this sentence, questioning whether to be
coordinates with pursuing or only with to earn, or
whether pursuing coordinates only with to earn (the
analysis they finally chose).
(10) My most important goals are pursuing the
profession to be a top marketing manager
and then to earn a lot of money to buy a
beautiful house and a good car .
4.3.2 Task-related issues
Annotator disagreements stemmed not only from
the text, but from other factors as well, such as as-
pects of the scheme that needed more clarification,
some interface issues, and the fact that the guidelines
though extensive, are still not comprehensive.
A few parts of the annotation scheme were con-
fusing to annotators and likely need refinement. For
example, if the form of a word was incorrect, we
saw a lot of lexical violation annotation, even if it
176
was only an issue of grammatical marking and POS
(e.g., did/VVD instead of done/VVN), as opposed
to a truly different word choice. We are currently
tightening the annotation scheme and adding clarifi-
cations about lexical violations in our guidelines.
As another example, verb raising was often not
marked (cf. figure 1), in spite of the scheme and
guidelines requiring it. In their comments, annota-
tors mentioned that it seemed ?redundant? to them
and that it caused arcs to cross, which they found
?unappealing.? One annotator commented that they
did not have enough syntactic background to see
why marking multiple subjects was necessary. We
are thus considering a simpler treatment. Another
option in the future is to hire annotators with more
background in syntax.
The interface may be partly to blame for some dis-
agreements, including subcategorizations which an-
notators often left unmarked (section 4.2.3) or only
partly marked (e.g., leaving off a SUBJect for a verb
which has been raised). There are a few reasons for
this. First, marking subcategorization likely needed
more emphasis in the training period, seeing as how
it relates to complicated linguistic notions like dis-
tinguishing arguments and adjuncts. Secondly, the
interface is an issue, as the subcategorization field is
not directly visible, compared to the arcs drawn for
dependencies; in figure 2, for instance, subcatego-
rization can only be seen in the asterisks, which need
to be clicked on to be seen and changed. Relatedly,
because it is not always necessary, subcategorization
may seem more optional and thus forgettable.
By the nature of being an in-progress project, the
guidelines were necessarily not comprehensive. As
one example, the TRANS(ition) label was only gen-
erally defined, leading to disagreements. As another,
a slash could indicate coordination (actor/actress),
and annotators differed on its POS labeling, as either
CC (coordinating conjunction), or a PUNCT (punc-
tuation). The different POS labels then led to vastly
different dependency graphs. In spite of a lengthy
section on how to handle coordination in the guide-
lines, it seems that an additional case needs to be
added to the guidelines to cover when punctuation is
used as a conjunction.
5 Conclusion and outlook
Developing reliable annotation schemes for learner
language is an important step towards better POS
tagging and parsing of learner corpora. We have de-
scribed an inter-annotator agreement study that has
helped shed light on several issues, such as the re-
liability of our annotation scheme, and has helped
identify room for improvement. This study shows
that it is possible to apply a multi-layered depen-
dency annotation scheme to learner text with consid-
erably good agreement rates between three trained
annotators. In the future, we will of course be
applying the (revised) annotation scheme to larger
data sets, but we hope other grammatical annota-
tion schemes can learn from our experience. In the
shorter term, we are constructing a gold standard of
the text files used here, to test annotation accuracy
and whether any (or all) annotators had consistent
difficulties. Another next step is to gather a larger
pool of data and focus more on analyzing the ef-
fects of L1 and learner proficiency level on anno-
tation. Finally, given that syntactic representations
can assist in automating tasks such as developmen-
tal profiling of learners (e.g., Vyatkina, 2013), gram-
matical error detection (Tetreault et al, 2010), iden-
tification of native language (e.g., Tetreault et al,
2012), and proficiency level determination (Dickin-
son et al, 2012)?all of which impact NLP-based
educational tools?one can explore the effect of spe-
cific syntactic decisions on such tasks, as a way to
provide feedback on the annotation scheme.
Acknowledgments
We would like to thank the three annotators for their
help with this experiment. We also thank the IU CL
discussion group, as well as the three anonymous
reviewers, for their feedback and comments.
References
Ron Artstein and Massimo Poesio. 2008. Inter-coder
agreement for computational linguistics. Compu-
tational Linguistics, 34(4):555?596.
Anna Babarczy, John Carroll, and Geoffrey Samp-
son. 2006. Definitional, personal, and mechanical
constraints on part of speech annotation perfor-
mance. Natural Language Engineering, 12:77?
90.
177
Patrizia Bonaventura, Peter Howarth, and Wolfgang
Menzel. 2000. Phonetic annotation of a non-
native speech corpus. In Proceedings Interna-
tional Workshop on Integrating Speech Technol-
ogy in the (Language) Learning and Assistive In-
terface, InStil, pages 10?17.
Adriane Amelia Boyd. 2012. Detecting and Diag-
nosing Grammatical Errors for Beginning Learn-
ers of German: From Learner Corpus Annotation
to Constraint Satisfaction Problems. Ph.D. thesis,
Ohio State University.
Thorsten Brants. 2000. TnT ? a statistical part-of-
speech tagger. In Proceedings of the Sixth Applied
Natural Language Processing Conference (ANLP
2000), pages 224?231. Seattle, WA.
Daniel Cer, Marie-Catherine de Marneffe, Daniel
Jurafsky, and Christopher D. Manning. 2010.
Parsing to Stanford dependencies: Trade-offs be-
tween speed and accuracy. In Proceedings of
LREC-10. Malta.
M. Civit, A. Ageno, B. Navarro, N. Buf??, and M. A.
Mart??. 2003. Qualitative and quantitative analy-
sis of annotators? agreement in the development
of Cast3LB. In Proceedings of 2nd Workshop on
Treebanks and Linguistics Theories (TLT-2003),
pages 33?45.
Pieter de Haan. 2000. Tagging non-native En-
glish with the TOSCA-ICLE tagger. In Christian
Mair and Markus Hundt, editors, Corpus Linguis-
tics and Linguistic Theory, pages 69?79. Rodopi,
Amsterdam.
Ana D??az-Negrillo, Detmar Meurers, Salvador
Valera, and Holger Wunsch. 2010. Towards in-
terlanguage POS annotation for effective learner
corpora in SLA and FLT. Language Forum, 36(1?
2):139?154. Special Issue on New Trends in Lan-
guage Teaching.
Markus Dickinson, Sandra Ku?bler, and Anthony
Meyer. 2012. Predicting learner levels for online
exercises of Hebrew. In Proceedings of the Sev-
enth Workshop on Building Educational Applica-
tions Using NLP, pages 95?104. Association for
Computational Linguistics, Montre?al, Canada.
Markus Dickinson and Marwa Ragheb. 2009. De-
pendency annotation for learner corpora. In Pro-
ceedings of the Eighth Workshop on Treebanks
and Linguistic Theories (TLT-8), pages 59?70.
Milan, Italy.
John A. Hawkins and Paula Buttery. 2010. Criterial
features in learner corpora: Theory and illustra-
tions. English Profile Journal, 1(1):1?23.
Hagen Hirschmann, Anke Lu?deling, Ines Rehbein,
Marc Reznicek, and Amir Zeldes. 2010. Syntactic
overuse and underuse: A study of a parsed learner
corpus and its target hypothesis. Talk given at
the Ninth Workshop on Treebanks and Linguistic
Theory.
Julia Krivanek and Detmar Meurers. 2011. Compar-
ing rule-based and data-driven dependency pars-
ing of learner language. In Proceedings of the Int.
Conference on Dependency Linguistics (Depling
2011). Barcelona.
Sandra Ku?bler, Ryan McDonald, and Joakim Nivre.
2009. Dependency Parsing. Morgan & Claypool
Publishers.
Sun-Hee Lee, Markus Dickinson, and Ross Israel.
2012. Developing learner corpus annotation for
Korean particle errors. In Proceedings of the
Sixth Linguistic Annotation Workshop, LAW VI
?12, pages 129?133. Association for Computa-
tional Linguistics, Stroudsburg, PA, USA.
Ryo Nagata, Edward Whittaker, and Vera Shein-
man. 2011. Creating a manually error-tagged and
shallow-parsed learner corpus. In Proceedings
of the 49th Annual Meeting of the Association
for Computational Linguistics: Human Language
Technologies, pages 1210?1219. Portland, OR.
Niels Ott and Ramon Ziai. 2010. Evaluating de-
pendency parsing performance on German learner
language. In Proceedings of TLT-9, volume 9,
pages 175?186.
Rebecca Passonneau, Nizar Habash, and Owen
Rambow. 2006. Inter-annotator agreement on a
multilingual semantic annotation task. In Pro-
ceedings of the Fifth International Conference
on Language Resources and Evaluation (LREC),
pages 1951?1956.
Marwa Ragheb and Markus Dickinson. 2011.
Avoiding the comparative fallacy in the annota-
tion of learner corpora. In Selected Proceedings of
the 2010 Second Language Research Forum: Re-
178
considering SLA Research, Dimensions, and Di-
rections, pages 114?124. Cascadilla Proceedings
Project, Somerville, MA.
Marwa Ragheb and Markus Dickinson. 2012. Defin-
ing syntax for learner language annotation. In
Proceedings of the 24th International Conference
on Computational Linguistics (Coling 2012),
Poster Session. Mumbai, India.
Ines Rehbein, Hagen Hirschmann, Anke Lu?deling,
and Marc Reznicek. 2012. Better tags give better
trees - or do they? Linguistic Issues in Language
Technology (LiLT), 7(10).
Victoria Rose?n and Koenraad De Smedt. 2010. Syn-
tactic annotation of learner corpora. In Hilde Jo-
hansen, Anne Golden, Jon Erik Hagen, and Ann-
Kristin Helland, editors, Systematisk, variert, men
ikke tilfeldig. Antologi om norsk som andrespra?k i
anledning Kari Tenfjords 60-a?rsdag [Systematic,
varied, but not arbitrary. Anthology about Norwe-
gian as a second language on the occasion of Kari
Tenfjord?s 60th birthday], pages 120?132. Novus
forlag, Oslo.
Alla Rozovskaya and Dan Roth. 2010. Annotating
ESL errors: Challenges and rewards. In Proceed-
ings of the NAACL HLT 2010 Fifth Workshop on
Innovative Use of NLP for Building Educational
Applications, pages 28?36. Los Angeles, Califor-
nia.
Kenji Sagae, Eric Davis, Alon Lavie, and
Brian MacWhinney an Shuly Wintner. 2010.
Morphosyntactic annotation of childes tran-
scripts. Journal of Child Language, 37(3):705?
729.
Geoffrey Sampson. 1995. English for the Computer:
The SUSANNE Corpus and Analytic Scheme.
Clarendon Press, Oxford.
Pontus Stenetorp, Sampo Pyysalo, Goran Topic?,
Tomoko Ohta, Sophia Ananiadou, and Jun?ichi
Tsujii. 2012. brat: a web-based tool for nlp-
assisted text annotation. In Proceedings of the
Demonstrations at the 13th Conference of the
European Chapter of the Association for Com-
putational Linguistics, pages 102?107. Avignon,
France.
Joel Tetreault, Daniel Blanchard, Aoife Cahill, and
Martin Chodorow. 2012. Native tongues, lost and
found: Resources and empirical evaluations in na-
tive language identification. In Proceedings of
COLING 2012, pages 2585?2602. Mumbai, In-
dia.
Joel Tetreault and Martin Chodorow. 2008. Na-
tive judgments of non-native usage: experiments
in preposition error detection. In Proceedings
of the Workshop on Human Judgements in Com-
putational Linguistics, HumanJudge ?08, pages
24?32. Association for Computational Linguis-
tics, Stroudsburg, PA, USA.
Joel Tetreault, Jennifer Foster, and Martin
Chodorow. 2010. Using parse features for
preposition selection and error detection. In
Proceedings of the ACL 2010 Conference Short
Papers, pages 353?358. Uppsala, Sweden.
Sylvie Thoue?sny. 2009. Increasing the reliability of
a part-of-speech tagging tool for use with learner
language. Presentation given at the Automatic
Analysis of Learner Language (AALL?09) work-
shop on automatic analysis of learner language:
from a better understanding of annotation needs
to the development and standardization of anno-
tation schemes.
Bertus van Rooy and Lande Scha?fer. 2002. The ef-
fect of learner errors on POS tag errors during au-
tomatic POS tagging. Southern African Linguis-
tics and Applied Language Studies, 20:325?335.
Nina Vyatkina. 2013. Specific syntactic complex-
ity: Developmental profiling of individuals based
on an annotated learner corpus. The Modern Lan-
guage Journal, 97(S1):1?20.
Helen Yannakoudakis, Ted Briscoe, and Ben Med-
lock. 2011. A new dataset and method for au-
tomatically grading ESOL texts. In Proceedings
of the 49th Annual Meeting of the Association
for Computational Linguistics: Human Language
Technologies, pages 180?189. Portland, OR.
179
