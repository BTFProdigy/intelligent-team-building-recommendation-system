Proceedings of ACL-08: HLT, pages 272?280,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Automatic Image Annotation Using Auxiliary Text Information
Yansong Feng and Mirella Lapata
School of Informatics, University of Edinburgh
2 Buccleuch Place, Edinburgh EH8 9LW, UK
Y.Feng-4@sms.ed.ac.uk, mlap@inf.ed.ac.uk
Abstract
The availability of databases of images labeled
with keywords is necessary for developing and
evaluating image annotation models. Dataset
collection is however a costly and time con-
suming task. In this paper we exploit the vast
resource of images available on the web. We
create a database of pictures that are natu-
rally embedded into news articles and propose
to use their captions as a proxy for annota-
tion keywords. Experimental results show that
an image annotation model can be developed
on this dataset alne without the overhead of
manual annotation. We also demonstrate that
the news article associated with the picture
can be used to boost image annotation perfor-
mance.
1 Introduction
As the number of image collections is rapidly grow-
ing, so does the need to browse and search them.
Recent years have witnessed significant progress in
developing methods for image retrieval1, many of
which are query-based. Given a database of images,
each annotated with keywords, the query is used to
retrieve relevant pictures under the assumption that
the annotations can essentially capture their seman-
tics.
One stumbling block to the widespread use of
query-based image retrieval systems is obtaining the
keywords for the images. Since manual annotation
is expensive, time-consuming and practically infea-
sible for large databases, there has been great in-
1The approaches are too numerous to list; we refer the inter-
ested reader to Datta et al (2005) for an overview.
terest in automating the image annotation process
(see references). More formally, given an image I
with visual features Vi = {v1,v2, . . . ,vN} and a set
of keywords W = {w1,w2, . . . ,wM}, the task con-
sists in finding automatically the keyword subset
WI ? W , which can appropriately describe the im-
age I. Indeed, several approaches have been pro-
posed to solve this problem under a variety of learn-
ing paradigms. These range from supervised clas-
sification (Vailaya et al, 2001; Smeulders et al,
2000) to instantiations of the noisy-channel model
(Duygulu et al, 2002), to clustering (Barnard et al,
2002), and methods inspired by information retrieval
(Lavrenko et al, 2003; Feng et al, 2004).
Obviously in order to develop accurate image an-
notation models, some manually labeled data is re-
quired. Previous approaches have been developed
and tested almost exclusively on the Corel database.
The latter contains 600 CD-ROMs, each contain-
ing about 100 images representing the same topic
or concept, e.g., people, landscape, male. Each topic
is associated with keywords and these are assumed
to also describe the images under this topic. As an
example consider the pictures in Figure 1 which are
classified under the topic male and have the descrip-
tion keywords man, male, people, cloth, and face.
Current image annotation methods work well
when large amounts of labeled images are available
but can run into severe difficulties when the number
of images and keywords for a given topic is rela-
tively small. Unfortunately, databases like Corel are
few and far between and somewhat idealized. Corel
contains clusters of many closely related images
which in turn share keyword descriptions, thus al-
lowing models to learn image-keyword associations
272
Figure 1: Images from the Corel database, exemplifying
the concept male with keyword descriptions man, male,
people, cloth, and face.
reliably (Tang and Lewis, 2007). It is unlikely that
models trained on this database will perform well
out-of-domain on other image collections which are
more noisy and do not share these characteristics.
Furthermore, in order to develop robust image anno-
tation models, it is crucial to have large and diverse
datasets both for training and evaluation.
In this work, we aim to relieve the data acquisition
bottleneck associated with automatic image annota-
tion by taking advantage of resources where images
and their annotations co-occur naturally. News arti-
cles associated with images and their captions spring
readily to mind (e.g., BBC News, Yahoo News). So,
rather than laboriously annotating images with their
keywords, we simply treat captions as labels. These
annotations are admittedly noisy and far from ideal.
Captions can be denotative (describing the objects
the image depicts) but also connotative (describ-
ing sociological, political, or economic attitudes re-
flected in the image). Importantly, our images are not
standalone, they come with news articles whose con-
tent is shared with the image. So, by processing the
accompanying document, we can effectively learn
about the image and reduce the effect of noise due
to the approximate nature of the caption labels. To
give a simple example, if two words appear both in
the caption and the document, it is more likely that
the annotation is genuine.
In what follows, we present a new database con-
sisting of articles, images, and their captions which
we collected from an on-line news source. We
then propose an image annotation model which can
learn from our noisy annotations and the auxil-
iary documents. Specifically, we extend and mod-
ify Lavrenko?s (2003) continuous relevance model
to suit our task. Our experimental results show that
this model can successfully scale to our database,
without making use of explicit human annotations
in any way. We also show that the auxiliary docu-
ment contains important information for generating
more accurate image descriptions.
2 Related Work
Automatic image annotation is a popular task in
computer vision. The earliest approaches are closely
related to image classification (Vailaya et al, 2001;
Smeulders et al, 2000), where pictures are assigned
a set of simple descriptions such as indoor, out-
door, landscape, people, animal. A binary classifier
is trained for each concept, sometimes in a ?one vs
all? setting. The focus here is mostly on image pro-
cessing and good feature selection (e.g., colour, tex-
ture, contours) rather than the annotation task itself.
Recently, much progress has been made on the
image annotation task thanks to three factors. The
availability of the Corel database, the use of unsu-
pervised methods and new insights from the related
fields of natural language processing and informa-
tion retrieval. The co-occurrence model (Mori et al,
1999) collects co-occurrence counts between words
and image features and uses them to predict anno-
tations for new images. Duygulu et al (2002) im-
prove on this model by treating image regions and
keywords as a bi-text and using the EM algorithm to
construct an image region-word dictionary.
Another way of capturing co-occurrence informa-
tion is to introduce latent variables linking image
features with words. Standard latent semantic anal-
ysis (LSA) and its probabilistic variant (PLSA) have
been applied to this task (Hofmann, 1998). Barnard
et al (2002) propose a hierarchical latent model
in order to account for the fact that some words
are more general than others. More sophisticated
graphical models (Blei and Jordan, 2003) have also
been employed including Gaussian Mixture Models
(GMM) and Latent Dirichlet Allocation (LDA).
Finally, relevance models originally developed for
information retrieval, have been successfully applied
to image annotation (Lavrenko et al, 2003; Feng et
al., 2004). A key idea behind these models is to find
the images most similar to the test image and then
use their shared keywords for annotation.
Our approach differs from previous work in two
273
important respects. Firstly, our ultimate goal is to de-
velop an image annotation model that can cope with
real-world images and noisy data sets. To this end
we are faced with the challenge of building an ap-
propriate database for testing and training purposes.
Our solution is to leverage the vast resource of im-
ages available on the web but also the fact that many
of these images are implicitly annotated. For exam-
ple, news articles often contain images whose cap-
tions can be thought of as annotations. Secondly, we
allow our image annotation model access to knowl-
edge sources other than the image and its keywords.
This is relatively straightforward in our case; an im-
age and its accompanying document have shared
content, and we can use the latter to glean informa-
tion about the former. But we hope to illustrate the
more general point that auxiliary linguistic informa-
tion can indeed bring performance improvements on
the image annotation task.
3 BBC News Database
Our database consists of news images which are
abundant. Many on-line news providers supply pic-
tures with news articles, some even classify news
into broad topic categories (e.g., business, world,
sports, entertainment). Importantly, news images of-
ten display several objects and complex scenes and
are usually associated with captions describing their
contents. The captions are image specific and use a
rich vocabulary. This is in marked contrast to the
Corel database whose images contain one or two
salient objects and a limited vocabulary (typically
around 300 words).
We downloaded 3,361 news articles from the
BBC News website.2 Each article was accompa-
nied with an image and its caption. We thus created
a database of image-caption-document tuples. The
documents cover a wide range of topics including
national and international politics, advanced tech-
nology, sports, education, etc. An example of an en-
try in our database is illustrated in Figure 2. Here,
the image caption is Marcin and Florent face intense
competition from outside Europe and the accompa-
nying article discusses EU subsidies to farmers. The
images are usually 203 pixels wide and 152 pix-
els high. The average caption length is 5.35 tokens,
and the average document length 133.85 tokens. Our
2http://news.bbc.co.uk/
Figure 2: A sample from our BBC News database. Each
entry contains an image, a caption for the image, and the
accompanying document with its title.
captions have a vocabulary of 2,167 words and our
documents 6,253. The vocabulary shared between
captions and documents is 2,056 words.
4 Extending the Continuous Relevance
Annotation Model
Our work is an extension of the continuous rele-
vance annotation model put forward in Lavrenko
et al (2003). Unlike other unsupervised approaches
where a set of latent variables is introduced, each
defining a joint distribution on the space of key-
words and image features, the relevance model cap-
tures the joint probability of images and annotated
words directly, without requiring an intermediate
clustering stage. This model is a good point of de-
parture for our task for several reasons, both theo-
retical and empirical. Firstly, expectations are com-
puted over every single point in the training set and
274
therefore parameters can be estimated without EM.
Indeed, Lavrenko et al achieve competitive perfor-
mance with latent variable models. Secondly, the
generation of feature vectors is modeled directly,
so there is no need for quantization. Thirdly, as we
show below the model can be easily extended to in-
corporate information outside the image and its key-
words.
In the following we first lay out the assumptions
underlying our model. We next describe the contin-
uous relevance model in more detail and present our
extensions and modifications.
Assumptions Since we are using a non-
standard database, namely images embedded in doc-
uments, it is important to clarify what we mean by
image annotation, and how the precise nature of our
data impacts the task. We thus make the following
assumptions:
1. The caption describes the content of the image
directly or indirectly. Unlike traditional image
annotation where keywords describe salient ob-
jects, captions supply more detailed informa-
tion, not only about objects, and their attributes,
but also events. In Figure 2 the caption men-
tions Marcin and Florent the two individuals
shown in the picture but also the fact that they
face competition from outside Europe.
2. Since our images are implicitly rather than ex-
plicitly labeled, we do not assume that we can
annotate all objects present in the image. In-
stead, we hope to be able to model event-related
information such as ?what happened?, ?who
did it?, ?when? and ?where?. Our annotation
task is therefore more semantic in nature than
traditionally assumed.
3. The accompanying document describes the
content of the image. This is trivially true for
news documents where the images convention-
ally depict events, objects or people mentioned
in the article.
To validate these assumptions, we performed the
following experiment on our BBC News dataset.
We randomly selected 240 image-caption pairs
and manually assessed whether the caption content
words (i.e., nouns, verbs, and adjectives) could de-
scribe the image. We found out that the captions
express the picture?s content 90% of the time. Fur-
thermore, approximately 88% of the nouns in sub-
ject or object position directly denote salient picture
objects. We thus conclude that the captions contain
useful information about the picture and can be used
for annotation purposes.
Model Description The continuous relevance
image annotation model (Lavrenko et al, 2003)
generatively learns the joint probability distribu-
tion P(V,W ) of words W and image regions V . The
key assumption here is that the process of generating
images is conditionally independent from the pro-
cess of generating words. Each annotated image in
the training set is treated as a latent variable. Then
for an unknown image I, we estimate:
P(VI,WI) = ?
s?D
P(VI|s)P(WI|s)P(s), (1)
where D is the number of images in the training
database, VI are visual features of the image regions
representing I, WI are the keywords of I, s is a la-
tent variable (i.e., an image-annotation pair), and
P(s) the prior probability of s. The latter is drawn
from a uniform distribution:
P(s) =
1
ND
(2)
where ND is number of the latent variables in the
training database D.
When estimating P(VI|s), the probability of im-
age regions and words, Lavrenko et al (2003) rea-
sonably assume a generative Gaussian kernel distri-
bution for the image regions:
(3)P(VI|s) =
NVI
?
r=1
Pg(vr|s)
=
NVI
?
r=1
1
nsv
nsv
?
i=1
exp{(vr ? vi)T??1(vr ? vi)}
?
2kpik |?|
where NVI is the number of regions in image I, vr the
feature vector for region r in image I, nsv the number
of regions in the image of latent variable s, vi the fea-
ture vector for region i in s?s image, k the dimension
of the image feature vectors and ? the feature covari-
ance matrix. According to equation (3), a Gaussian
kernel is fit to every feature vector vi corresponding
to region i in the image of the latent variable s. Each
kernel here is determined by the feature covariance
matrix ?, and for simplicity, ? is assumed to be a
diagonal matrix: ?= ?I, where I is the identity ma-
trix; and ? is a scalar modulating the bandwidth of
275
the kernel whose value is optimized on the develop-
ment set.
Lavrenko et al (2003) estimate the word prob-
abilities P(WI|s) using a multinomial distribution.
This is a reasonable assumption in the Corel dataset,
where the annotations have similar lengths and the
words reflect the salience of objects in the image (the
multinomial model tends to favor words that appear
multiple times in the annotation). However, in our
dataset the annotations have varying lengths, and do
not necessarily reflect object salience. We are more
interested in modeling the presence or absence of
words in the annotation and thus use the multiple-
Bernoulli distribution to generate words (Feng et al,
2004). And rather than relying solely on annotations
in the training database, we can also take the accom-
panying document into account using a weighted
combination.
The probability of sampling a set of words W
given a latent variable s from the underlying multiple
Bernoulli distribution that has generated the training
set D is:
P(W |s) = ?
w?W
P(w|s) ?
w/?W
(1?P(w|s)) (4)
where P(w|s) denotes the probability of the w?th
component of the multiple Bernoulli distribution.
Now, in estimating P(w|s) we can include the docu-
ment as:
Pest(w|s) = ?Pest(w|sa)+(1??)Pest(w|sd) (5)
where ? is a smoothing parameter tuned on the de-
velopment set, sa is the annotation for the latent vari-
able s and sd its corresponding document.
Equation (5) smooths the influence of the annota-
tion words and allows to offset the negative effect of
the noise inherent in our dataset. Since our images
are implicitly annotated, there is no guarantee that
the annotations are all appropriate. By taking into
account Pest(w|sd), it is possible to annotate an im-
age with a word that appears in the document but is
not included in the caption.
We use a Bayesian framework for estimat-
ing Pest(w|sa). Specifically, we assume a beta prior
(conjugate to the Bernoulli distribution) for each
word:
Pest(w|sa) =
? bw,sa +Nw
?+D
(6)
where ? is a smoothing parameter estimated on the
development set, bw,sa is a Boolean variable denoting
whether w appears in the annotation sa, and Nw is
the number of latent variables that contain w in their
annotations.
We estimate Pest(w|sd) using maximum likeli-
hood estimation (Ponte and Croft, 1998):
Pest(w|sd) =
numw,sd
numsd
(7)
where numw,sd denotes the frequency of w in the ac-
companying document of latent variable s and numsd
the number of all tokens in the document. Note that
we purposely leave Pest unsmoothed, since it is used
as a means of balancing the weight of word frequen-
cies in annotations. So, if a word does not appear in
the document, the possibility of selecting it will not
be greater than ? (see Equation (5)).
Unfortunately, including the document in the es-
timation of Pest(w|s) increases the vocabulary which
in turn increases computation time. Given a test
image-document pair, we must evaluate P(w|VI) for
every w in our vocabulary which is the union of
the caption and document words. We reduce the
search space, by scoring each document word with
its tf ? idf weight (Salton and McGill, 1983) and
adding the n-best candidates to our caption vocabu-
lary. This way the vocabulary is not fixed in advance
for all images but changes dynamically depending
on the document at hand.
Re-ranking the Annotation Hypotheses It is
easy to see that the output of our model is a ranked
word list. Typically, the k-best words are taken to
be the automatic annotations for a test image I
(Duygulu et al, 2002; Lavrenko et al, 2003; Jeon
andManmatha, 2004) where k is a small number and
the same for all images.
So far we have taken account of the auxiliary doc-
ument rather naively, by considering its vocabulary
in the estimation of P(W |s). Crucially, documents
are written with one or more topics in mind. The im-
age (and its annotations) are likely to represent these
topics, so ideally our model should prefer words that
are strong topic indicators. A simple way to imple-
ment this idea is by re-ranking our k-best list accord-
ing to a topic model estimated from the entire docu-
ment collection.
Specifically, we use Latent Dirichlet Allocation
(LDA) as our topic model (Blei et al, 2003). LDA
276
represents documents as a mixture of topics and has
been previously used to perform document classi-
fication (Blei et al, 2003) and ad-hoc information
retrieval (Wei and Croft, 2006) with good results.
Given a collection of documents and a set of latent
variables (i.e., the number of topics), the LDAmodel
estimates the probability of topics per document and
the probability of words per topic. The topic mix-
ture is drawn from a conjugate Dirichlet prior that
remains the same for all documents.
For our re-ranking task, we use the LDA model
to infer the m-best topics in the accompanying doc-
ument. We then select from the output of our model
those words that are most likely according to these
topics. To give a concrete example, let us assume
that for a given image our model has produced
five annotations, w1, w2, w3, w4, and w5. However,
according to the LDA model neither w2 nor w5
are likely topic indicators. We therefore remove w2
and w5 and substitute them with words further down
the ranked list that are topical (e.g., w6 and w7).
An advantage of using LDA is that at test time we
can perform inference without retraining the topic
model.
5 Experimental Setup
In this section we discuss our experimental design
for assessing the performance of the model pre-
sented above. We give details on our training pro-
cedure and parameter estimation, describe our fea-
tures, and present the baseline methods used for
comparison with our approach.
Data Our model was trained and tested on the
database introduced in Section 3. We used 2,881
image-caption-document tuples for training, 240 tu-
ples for development and 240 for testing. The docu-
ments and captions were part-of-speech tagged and
lemmatized with Tree Tagger (Schmid, 1994).Words
other than nouns, verbs, and adjectives were dis-
carded. Words that were attested less than five times
in the training set were also removed to avoid unre-
liable estimation. In total, our vocabulary consisted
of 8,309 words.
Model Parameters Images are typically seg-
mented into regions prior to training. We impose a
fixed-size rectangular grid on each image rather than
attempting segmentation using a general purpose al-
gorithm such as normalized cuts (Shi and Malik,
Color
average of RGB components, standard deviation
average of LUV components, standard deviation
average of LAB components, standard deviation
Texture
output of DCT transformation
output of Gabor filtering (4 directions, 3 scales)
Shape
oriented edge (4 directions)
ratio of edge to non-edge
Table 2: Set of image features used in our experiments.
2000). Using a grid avoids unnecessary errors from
image segmentation algorithms, reduces computa-
tion time, and simplifies parameter estimation (Feng
et al, 2004). Taking the small size and low resolu-
tion of the BBC News images into account, we av-
eragely divide each image into 6?5 rectangles and
extract features for each region. We use 46 features
based on color, texture, and shape. They are summa-
rized in Table 2.
The model presented in Section 4 has a few pa-
rameters that must be selected empirically on the
development set. These include the vocabulary size,
which is dependent on the n words with the high-
est tf ? idf scores in each document, and the num-
ber of topics for the LDA-based re-ranker. We ob-
tained best performance with n set to 100 (no cutoff
was applied in cases where the vocabulary was less
than 100). We trained an LDA model with 20 top-
ics on our document collection using David Blei?s
implementation.3 We used this model to re-rank the
output of our annotation model according to the
three most likely topics in each document.
Baselines We compared our model against
three baselines. The first baseline is based on tf ? idf
(Salton and McGill, 1983). We rank the document?s
content words (i.e., nouns, verbs, and adjectives) ac-
cording to their tf ? idf weight and select the top k
to be the final annotations. Our second baseline sim-
ply annotates the image with the document?s title.
Again we only use content words (the average title
length in the training set was 4.0 words). Our third
baseline is Lavrenko et al?s (2003) continuous rel-
evance model. It is trained solely on image-caption
3Available from http://www.cs.princeton.edu/?blei/
lda-c/index.html.
277
Model Top 10 Top 15 Top 20
Precision Recall F1 Precision Recall F1 Precision Recall F1
tf ? idf 4.37 7.09 5.41 3.57 8.12 4.86 2.65 8.89 4.00
DocTitle 9.22 7.03 7.20 9.22 7.03 7.20 9.22 7.03 7.20
Lavrenko03 9.05 16.01 11.81 7.73 17.87 10.71 6.55 19.38 9.79
ExtModel 14.72 27.95 19.82 11.62 32.99 17.18 9.72 36.77 15.39
Table 1: Automatic image annotation results on the BBC News database.
pairs, uses a vocabulary of 2,167 words and the same
features as our extended model.
Evaluation Our evaluation follows the exper-
imental methodology proposed in Duygulu et al
(2002). We are given an un-annotated image I and
are asked to automatically produce suitable anno-
tations for I. Given a set of image regions VI , we
use equation (1) to derive the conditional distribu-
tion P(w|VI). We consider the k-best words as the an-
notations for I. We present results using the top 10,
15, and 20 annotation words. We assess our model?s
performance using precision/recall and F1. In our
task, precision is the percentage of correctly anno-
tated words over all annotations that the system sug-
gested. Recall, is the percentage of correctly anno-
tated words over the number of genuine annotations
in the test data. F1 is the harmonic mean of precision
and recall. These measures are averaged over the set
of test words.
6 Results
Our experiments were driven by three questions:
(1) Is it possible to create an annotation model from
noisy data that has not been explicitly hand labeled
for this task? (2) What is the contribution of the
auxiliary document? As mentioned earlier, consid-
ering the document increases the model?s compu-
tational complexity, which can be justified as long
as we demonstrate a substantial increase in perfor-
mance. (3) What is the contribution of the image?
Here, we are trying to assess if the image features
matter. For instance, we could simply generate an-
notation words by processing the document alone.
Our results are summarized in Table 1. We com-
pare the annotation performance of the model pro-
posed in this paper (ExtModel) with Lavrenko et
al.?s (2003) original continuous relevance model
(Lavrenko03) and two other simpler models which
do not take the image into account (tf ? idf and Doc-
Title). First, note that the original relevance model
performs best when the annotation output is re-
stricted to 10 words with an F1 of 11.81% (recall
is 9.05 and precision 16.01). F1 is marginally worse
with 15 output words and decreases by 2% with 20.
This model does not take any document-based in-
formation into account, it is trained solely on image-
caption pairs. On the Corel test set the same model
obtains a precision of 19.0% and a recall of 16.0%
with a vocabulary of 260 words. Although these re-
sults are not strictly comparable with ours due to the
different nature of the training data (in addition, we
output 10 annotation words, whereas Lavrenko et al
(2003) output 5), they give some indication of the
decrease in performance incurred when using a more
challenging dataset. Unlike Corel, our images have
greater variety, non-overlapping content and employ
a larger vocabulary (2,167 vs. 260 words).
When the document is taken into account (see
ExtModel in Table 1), F1 improves by 8.01% (re-
call is 14.72% and precision 27.95%). Increasing
the size of the output annotations to 15 or 20 yields
better recall, at the expense of precision. Eliminat-
ing the LDA reranker from the extended model de-
creases F1 by 0.62%. Incidentally, LDA can be also
used to rerank the output of Lavrenko et al?s (2003)
model. LDA also increases the performance of this
model by 0.41%.
Finally, considering the document alone, without
the image yields inferior performance. This is true
for the tf ? idf model and the model based on the
document titles.4 Interestingly, the latter yields pre-
cision similar to Lavrenko et al (2003). This is prob-
ably due to the fact that the document?s title is in a
sense similar to a caption. It often contains words
that describe the document?s gist and expectedly
4Reranking the output of these models with LDA slightly
decreases performance (approximately by 0.2%).
278
tf ? idf breastfeed, medical,
intelligent, health, child
culturalism, faith, Muslim, sepa-
rateness, ethnic
ceasefire, Lebanese, disarm, cab-
inet, Haaretz
DocTitle Breast milk does not boost IQ UK must tackle ethnic tensions Mid-East hope as ceasefire begins
Lavrenko03 woman, baby, hospital, new,
day, lead, good, England,
look, family
bomb, city, want, day, fight,
child, attack, face, help, govern-
ment
war, carry, city, security, Israeli,
attack, minister, force, govern-
ment, leader
ExtModel breastfeed, intelligent, baby,
mother, tend, child, study,
woman, sibling, advantage
aim, Kelly, faith, culturalism,
community, Ms, tension, com-
mission, multi, tackle, school
Lebanon, Israeli, Lebanese,
aeroplane, troop, Hezbollah,
Israel, force, ceasefire, grey
Caption Breastfed babies tend to be
brighter
Segregation problems were
blamed for 2001?s Bradford riots
Thousands of Israeli troops are in
Lebanon as the ceasefire begins
Figure 3: Examples of annotations generated by our model (ExtModel), the continuous relevance model (Lavrenko03),
and the two baselines based on tf ? idf and the document title (DocTitle). Words in bold face indicate exact matches,
underlined words are semantically compatible. The original captions are in the last row.
some of these words will be also appropriate for the
image. In fact, in our dataset, the title words are a
subset of those found in the captions.
Examples of the annotations generated by our
model are shown in Figure 3. We also include the
annotations produced by Lavrenko et. al?s (2003)
model and the two baselines. As we can see our
model annotates the image with words that are not
always included in the caption. Some of these are
synonyms of the caption words (e.g., child and intel-
ligent in left image of Figure 3), whereas others ex-
press additional information (e.g., mother, woman).
Also note that complex scene images remain chal-
lenging (see the center image in Figure 3). Such im-
ages are better analyzed at a higher resolution and
probably require more training examples.
7 Conclusions and Future Work
In this paper, we describe a new approach for the
collection of image annotation datasets. Specifically,
we leverage the vast resource of images available
on the Internet while exploiting the fact that many
of them are labeled with captions. Our experiments
show that it is possible to learn an image annotation
model from caption-picture pairs even if these are
not explicitly annotated in any way. We also show
that the annotation model benefits substantially from
additional information, beyond the caption or image.
In our case this information is provided by the news
documents associated with the pictures. But more
generally our results indicate that further linguistic
knowledge is needed to improve performance on the
image annotation task. For instance, resources like
WordNet (Fellbaum, 1998) can be used to expand
the annotations by exploiting information about is-a
relationships.
The uses of the database discussed in this article
are many and varied. An interesting future direction
concerns the application of the proposed model in a
semi-supervised setting where the annotation output
is iteratively refined with some manual intervention.
Another possibility would be to use the document
to increase the annotation keywords by identifying
synonyms or even sentences that are similar to the
image caption. Also note that our analysis of the ac-
companying document was rather shallow, limited
to part of speech tagging. It is reasonable to assume
that results would improve with more sophisticated
preprocessing (i.e., named entity recognition, pars-
ing, word sense disambiguation). Finally, we also
believe that the model proposed here can be usefully
employed in an information retrieval setting, where
the goal is to find the image most relevant for a given
query or document.
279
References
K. Barnard, P. Duygulu, D. Forsyth, N. de Freitas,
D. Blei, and M. Jordan. 2002. Matching words
and pictures. Journal of Machine Learning Research,
3:1107?1135.
D. Blei and M. Jordan. 2003. Modeling annotated data.
In Proceedings of the 26th Annual International ACM
SIGIR Conference, pages 127?134, Toronto, ON.
D. Blei, A. Ng, and M. Jordan. 2003. Latent Dirich-
let alocation. Journal of Machine Learning Research,
3:993?1022.
R. Datta, J. Li, and J. Z. Wang. 2005. Content-based im-
age retrieval ? approaches and trends of the new age.
In Proceedings of the International Workshop on Mul-
timedia Information Retrieval, pages 253?262, Singa-
pore.
P. Duygulu, K. Barnard, J. de Freitas, and D. Forsyth.
2002. Object recognition as machine translation:
Learning a lexicon for a fixed image vocabulary. In
Proceedings of the 7th European Conference on Com-
puter Vision, pages 97?112, Copenhagen, Danemark.
C. Fellbaum, editor. 1998. WordNet: An Electronic
Database. MIT Press, Cambridge, MA.
S. Feng, V. Lavrenko, and R. Manmatha. 2004. Mul-
tiple Bernoulli relevance models for image and video
annotation. In Proceedings of the International Con-
ference on Computer Vision and Pattern Recognition,
pages 1002?1009, Washington, DC.
T. Hofmann. 1998. Learning and representing topic.
A hierarchical mixture model for word occurrences
in document databases. In Proceedings of the Con-
ference for Automated Learning and Discovery, pages
408?415, Pittsburgh, PA.
J. Jeon and R. Manmatha. 2004. Using maximum en-
tropy for automatic image annotation. In Proceed-
ings of the 3rd International Conference on Image and
Video Retrieval, pages 24?32, Dublin City, Ireland.
V. Lavrenko, R. Manmatha, and J. Jeon. 2003. A model
for learning the semantics of pictures. In Proceedings
of the 16th Conference on Advances in Neural Infor-
mation Processing Systems, Vancouver, BC.
Y.Mori, H. Takahashi, and R. Oka. 1999. Image-to-word
transformation based on dividing and vector quantiz-
ing images with words. In Proceedings of the 1st In-
ternational Workshop on Multimedia Intelligent Stor-
age and Retrieval Management, Orlando, FL.
J. M. Ponte and W. Bruce Croft. 1998. A language
modeling approach to information retrieval. In Pro-
ceedings of the 21st Annual International ACM SIGIR
Conference, pages 275?281, New York, NY.
G. Salton and M.J. McGill. 1983. Introduction to Mod-
ern Information Retrieval. McGraw-Hill, New York.
H. Schmid. 1994. Probabilistic part-of-speech tagging
using decision trees. In Proceedings of the Interna-
tional Conference on New Methods in Language Pro-
cessing, Manchester, UK.
J. Shi and J. Malik. 2000. Normalized cuts and image
segmentation. IEEE Transactions on Pattern Analysis
and Machine Intelligence, 22(8):888?905.
A. W. Smeulders, M. Worring, S. Santini, A. Gupta, and
R. Jain. 2000. Content-based image retrieval at the
end of the early years. IEEE Transactions on Pat-
tern Analysis and Machine Intelligence, 22(12):1349?
1380.
J. Tang and P. H. Lewis. 2007. A study of quality is-
sues for image auto-annotation with the Corel data-set.
IEEE Transactions on Circuits and Systems for Video
Technology, 17(3):384?389.
A. Vailaya, M. Figueiredo, A. Jain, and H. Zhang. 2001.
Image classification for content-based indexing. IEEE
Transactions on Image Processing, 10:117?130.
X. Wei and B. W. Croft. 2006. LDA-based document
models for ad-hoc retrieval. In Proeedings of the 29th
Annual International ACM SIGIR Conference, pages
178?185, Seattle, WA.
280
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 513?523,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Title Generation with Quasi-Synchronous Grammar
Kristian Woodsend, Yansong Feng and Mirella Lapata
School of Informatics, University of Edinburgh
Edinburgh EH8 9AB, United Kingdom
k.woodsend@ed.ac.uk, Y.Feng-4@sms.ed.ac.uk, mlap@inf.ed.ac.uk
Abstract
The task of selecting information and render-
ing it appropriately appears in multiple con-
texts in summarization. In this paper we
present a model that simultaneously optimizes
selection and rendering preferences. The
model operates over a phrase-based represen-
tation of the source document which we ob-
tain by merging PCFG parse trees and depen-
dency graphs. Selection preferences for in-
dividual phrases are learned discriminatively,
while a quasi-synchronous grammar (Smith
and Eisner, 2006) captures rendering prefer-
ences such as paraphrases and compressions.
Based on an integer linear programming for-
mulation, the model learns to generate sum-
maries that satisfy both types of preferences,
while ensuring that length, topic coverage and
grammar constraints are met. Experiments on
headline and image caption generation show
that our method obtains state-of-the-art per-
formance using essentially the same model for
both tasks without any major modifications.
1 Introduction
Summarization is the process of condensing a source
text into a shorter version while preserving its infor-
mation content. Humans summarize on a daily ba-
sis and effortlessly, yet the automatic production of
high-quality summaries remains a challenge.
Most work today focuses on extractive summa-
rization, where a summary is created by identifying
and subsequently concatenating the most important
sentences in a document. The advantage of this ap-
proach is that it does not require a great deal of lin-
guistic analysis to generate grammatical sentences,
assuming the source document was well written.
Unfortunately, extracts generated this way are often
documents of low readability and text quality, and
contain much redundant information. The concise-
ness can be improved when sentence extraction is
interfaced with sentence compression, where words
and clauses are deleted based on rules typically op-
erating over parsed input (Jing, 2000; Daume? III
and Marcu, 2002; Lin, 2003; Daume? III, 2006; Zajic
et al, 2007; Martins and Smith, 2009).
An alternative abstractive or ?bottom-up? ap-
proach involves identifying high-interest words and
phrases in the source text, and combining them into
new sentences guided by a language model (Banko
et al, 2000; Soricut and Marcu, 2007). This ap-
proach has the potential to work well, breaking out
of the single-sentence paradigm. Unfortunately, the
resulting summaries are not always coherent ? indi-
vidual constituent phrases are often combined with-
out any semantic constraints ? or grammatical be-
yond the n-gram horizon imposed by the language
model.
Constituent deletion and recombination are
merely two of the many rewrite operations profes-
sional editors and abstractors employ when creating
summaries (Jing, 2002). Additional operations in-
clude truncating sentences, aggregating them, and
paraphrasing at word or syntax level. Furthermore,
professionals write summaries in a task-specific
style. News headlines for example are typically
short (three to six words), written in the present
tense and active voice, and often leave out forms of
the verb be. There are also different ways of writing
a headline either directly by stating what the docu-
513
ment is about or indirectly by raising a question in
the reader?s mind, which the document answers.
The automatic generation of summaries similar to
those produced by human abstractors is challenging
because of the many constraints imposed by the task:
the summary must be maximally informative and
minimally redundant, grammatical, coherent, adhere
to a pre-specified length and stylistic conventions.
Importantly, these constraints are conflicting; the
deletion of certain phrases may avoid redundancy
but result in ungrammatical output and information
loss.
In this paper we propose a model for summariza-
tion that attempts to capture and optimize these con-
straints jointly. We learn both how to select the
most important information (the content), and how
to render it appropriately (the style). Selection pref-
erences are learned discriminatively, while a quasi-
synchronous grammar (QG, Smith and Eisner 2006)
captures rendering preferences such as paraphrases
and compressions. The entire solution space of
possible extractions and QG-generated paraphrases
is searched efficiently through use of integer lin-
ear programming. The ILP framework allows us to
model naturally as constraints, additional require-
ments such as sentence length, overall summary
length, topic coverage and, importantly, grammati-
cality.
We argue that QG is attractive for describ-
ing rewrite operations common in summarization.
Rather than assuming a strictly synchronous struc-
ture over the source and target sentences, QG iden-
tifies a ?sloppy? alignment of parse trees assuming
that the target tree is in some way ?inspired by? the
source tree. A key insight in our approach is to
formulate the summarization problem at the phrase
level: both QG rules and information extraction op-
erate over individual phrases rather than (as is the
norm) sentences. At this smaller unit level, QG
rules become more widely applicable and compres-
sion falls naturally because only phrases deemed im-
portant should appear in the summary.
We evaluate the proposed model on headline gen-
eration and the related task of image caption gen-
eration. However, there is nothing inherent in our
formulation that is specific to those two tasks; it is
possible for the model to generate longer or shorter
summaries, for a single or multiple documents. Ex-
perimental results show that our method obtains
state-of-the-art performance, both in terms of gram-
maticality and informativeness for both tasks using
the same summarization model.
2 Related work
Much effort in automatic summarization has been
devoted to sentence extraction which is often for-
malized as a classification task (Kupiec et al, 1995).
Given appropriately annotated training data, a bi-
nary classifier learns to predict for each document
sentence if it is worth extracting. A few previ-
ous approaches have attempted to interface sentence
compression with summarization. A straightforward
way to achieve this is by adopting a two-stage ar-
chitecture (e.g., Lin 2003) where the sentences are
first extracted and then compressed or the other way
round.
Other work implements a joint model where
words are deleted and sentences selected from a doc-
ument simultaneously (Daume? III and Marcu, 2002;
Martins and Smith, 2009; Woodsend and Lapata,
2010). ILP models have also been developed for
sentence rather than document compression (Clarke
and Lapata, 2008). Dras (1999) discusses the appli-
cation of ILP to reluctant paraphrasing, i.e., the task
of choosing between paraphrases while conforming
to length, readability, or style constraints. Again,
the aim is to rewrite text without, however, con-
tent selection. Rewrite operations other than dele-
tion tend to be hand-crafted and domain specific
(Jing and McKeown, 2000). Notable exceptions are
Cohn and Lapata (2008) and Zhao et al (2009) who
present a model that can both compress and para-
phrase individual sentences without however gener-
ating document-level summaries.
Headline generation is a well-studied task within
single-document summarization, due to its promi-
nence in the DUC-03 and DUC-04 evaluation com-
petitions.1 Many approaches identify the most infor-
mative sentence in a given document (typically the
first sentence for the news genre) and subsequently
apply a form of sentence compression such that
the headline meets some length requirement (Dorr
1Approaches to headline generation are too numerous to list
in detail; see the proceedings of DUC-03 and DUC-04 for an
overview.
514
et al, 2003). The compressed sentence may also be
?padded? with important content words or phrases
to ensure that the topic of the document is covered
(Zajic et al, 2004). Other work generates headlines
in a bottom-up fashion starting from important, indi-
vidual words and phrases, that are glued together to
create a fluent sentence. For example, Banko et al
(2000) draw inspiration from Machine Translation
and generate headlines using statistical models for
content selection and sentence realization.
Relatively little work has focused on caption gen-
eration, a task related to headline generation. The
aim here is to create a short, title-like description of
an image embedded in a news article. Like head-
lines, captions have to be short and informative. In
addition, a good caption must clearly identify the
subject of the picture and establish its relevance to
the article. Feng and Lapata (2010a) develop ex-
tractive and abstractive caption generation models
that operate over the output of a probabilistic im-
age annotation model that preprocesses the pictures
and suggests keywords to describe their content.
Their best model is an extension of Banko et al?s
(2000) word-based model for headline generation to
phrases.
Our own work develops an ILP-based summariza-
tion model with rewrite operations that are not lim-
ited to deletion, are defined over phrases, and en-
coded in quasi-synchronous grammar. The QG for-
malism has been previously applied to parser adap-
tation and projection (Smith and Eisner, 2009), para-
phrase identification (Das and Smith, 2009), and
question answering (Wang et al, 2007); however
the use of QG in summarization is novel to our
knowledge. Unlike most synchronous grammar for-
malisms, QG does not posit a strict isomorphism be-
tween a source sentence and its target translation; it
only loosely links the syntactic structure of the two,
and is therefore well suited to describing the rela-
tionship between a document and its abstract. We
propose an ILP formulation which not only allows
to efficiently search through the space of many QG
rules but also to incorporate constraints relating to
content, style, and the task at hand.
3 Modeling
There are three components to our model. Content
selection is performed discriminatively; an SVM
learns which information in the source document
should be in the summary, and gives a real-valued
salience score for each phrase. QG rules are used
to generate compressions and paraphrases of the
source sentences. An ILP model combines the out-
put of these two components into an output sum-
mary, while optimizing content selection and surface
realization preferences jointly.
3.1 Document Representation
Our model operates on documents annotated with
syntactic information which we obtain by parsing
every sentence twice, once with a phrase structure
parser and once with a dependency parser. The out-
put from the two representations is combined into a
single data structure, by mapping the dependencies
to the edges of the phrase structure tree. The proce-
dure is described in detail in Woodsend and Lapata
(2010). However, we do not merge the leaf nodes
into phrases here, but keep the full tree structure,
as we will apply compression to phrases through
the QG. In our experiments, we obtain this com-
bined representation from the output of the Stan-
ford parser (Klein and Manning, 2003) but any other
broadly similar parser could be used instead.
3.2 Quasi-synchronous grammar
Given an input sentence S1 or its parse tree T1, the
QG constructs a monolingual grammar for parsing,
or generating, the possible translation (or here, para-
phrase) trees T2. A grammar node in the target tree
T2 is modeled on a subset of nodes in the source tree,
with a rather loose alignment between the trees.
In our approach, the process of learning the gram-
mar is unsupervised. Each sentence of the source
document is compared to each sentence in the target
document ? headline or caption, depending on the
task. Using the combined PCFG-dependency tree
representation described above, we build up a list of
leaf node alignments based on lexical identity, after
stemming and removing stop words. We align direct
parent nodes where more than one child node aligns.
A grammar rule is created if the all the nodes in the
target tree can be explained using nodes from the
515
(a) NP
NNP/nn
Saudi
JJ/amod
dissident
NNP/nn
Osama
NNP/nn
bin
NNP/?
Laden
NP
NNP/nn
bin
NNP/?
Laden
(b) PP/prep in
IN/?
in
DT/det
the
JJ/amod
disputed
NN/?
territory
PP/prep of
IN/?
of
NNP/nn
East
NNP/?
Timor
PP/prep in
IN/?
in
NNP/nn
East
NNP/?
Timor
(c) NP/dobj
DT/det
the
NN/?
extradition
PP/prep of
IN/?
of
NNP/nn
Kurdish
NN/nn
leader
NNP/?
Ocalan
NP/dobj
NP/poss
Ocalan?s
NN/?
extradition
Figure 1: Examples of QG alignments between
source node (left) and target node (right). (a) align-
ment of child nodes, involving compression through
deletion; (b) rewriting involving child and grand-
child nodes; (c) reordering of child nodes (with fur-
ther compression through applying other QG rules
on children). Nodes bear phrase and dependency la-
bels. Dotted lines show alignments in the grammar
between source and target child nodes. Examples
are taken from the QG rules discovered in the DUC-
03 data set of headlines.
source; this helps to improve the quality in what is
inherently a noisy process. Finally, QG rules are cre-
ated from aligned nodes above the leaf node level,
recording the phrase and dependency label of nodes,
and the alignment of child nodes.
Unlike previous work involving QG which has
used dependency graphs exclusively (e.g., Wang
et al 2007; Das and Smith 2009), our approach op-
erates over a combined PCFG-dependency represen-
tation. As a result, some configurations in Smith and
Eisner (2006) are not so relevant here ? instead,
we found that deletions, reorderings, flattening of
nodes, and the addition of text elements were im-
CHOICE/?
PP/prep in
IN/?
in
DT/det
the
JJ/amod
disputed
NN/?
territory
PP/prep of
IN/?
of
NNP/nn
East
NNP/?
Timor
PP/prep in
IN/?
in
NNP/nn
East
NNP/?
Timor
Figure 2: Alternative paraphrases are represented as
a CHOICE sub-tree.
portant operations for the grammar.
Figure 1 shows some example alignments that are
captured by the QG, with the source node on the
left and the target node on the right. Leaf nodes
have their original text, while other nodes have a
combined phrase and dependency label that they ob-
tain in the merged representation described in Sec-
tion 3.1 above (e.g., NP/dobj is a noun phrase and a
direct object, NNP/nn is a proper noun and a nomi-
nal modifier, whereas NN/? is a head noun). Align-
ments between the children are shown by dotted
lines. In Figure 1(a), some child nodes are aligned
while others are not present in the target tree. This
type of rule is common in our training data, and typ-
ically arises from the compression of names in noun
phrases. Another frequent compression, shown in
Figure 1(b), is flattening the tree structure by in-
corporating grand-child elements at the child level.
Figure 1(c) shows a rule involving the reordering
of child nodes, and where additional rules are ap-
plied recursively to achieve further compression and
a transformation in the phrase constituency.
Paraphrases are created from source sentence
parse trees by applying suitable rules recursively.
Suitable rules have matching structure in terms of
phrase and dependency label, for both the parent and
child nodes. Additionally, the proposed paraphrase
sub-tree must be suitable for the target tree being
created (i.e., the root node of the paraphrase must
match the phrase and dependency label of the corre-
sponding node in the target tree). Where more than
one paraphrase is possible, the alternatives are incor-
porated into the target parse tree under a CHOICE
node, as is shown in Figure 2. Note that unlike pre-
516
vious QG approaches, we do not use the probability
model proposed by Smith and Eisner (2006); instead
the QG is used to represent rewrite operations, and
we simply record a frequency count for how often
each rule is encountered in the training data.
3.3 ILP model
The objective of our model is to create the most in-
formative text possible, subject to constraints which
can be tailored to the specific task. These relate to
sentence length, overall summary length, the inclu-
sion of specific topics, and grammaticality. These
constraints are global in their scope, and cannot be
adequately satisfied by optimizing each one of them
individually. Our approach therefore uses an ILP
formulation which will provide a globally optimal
solution, and which can be efficiently solved using
standard optimization tools. Specifically, the model
selects phrases and paraphrases from which to form
the output sentence. Here, we focus on a single
sentence as this is most appropriate for title gener-
ation. However, multi-sentence output can be easily
generated by setting a summary length constraint.
The model operates over the merged phrase struc-
ture trees described in Section 3.1, augmented with
paraphrase choice nodes such as shown in Figure 2
rather than raw text.
Let S be the set of sentences in a document, P be
the set of phrases, and Ps ? P be the set of phrases
in each sentence s ? S . Let the sets Di ? P , ?i ? P
capture the phrase dependency information for each
phrase i, where each set Di contains the phrases that
depend on the presence of i. In a similar fashion,
C ? P is the set of choice nodes throughout the doc-
ument, which represent nodes in the tree where more
than one QG rule can be applied; Ci ? P , i ? C are
the sets of phrases that are direct children of each
choice node, in other words they are the individual
alternative paraphrases. Let li be the length of each
phrase i, in tokens.
For caption generation, the model has as addi-
tional input a list of tags (keywords drawn from the
source document) that correspond to the image, and
we refer to this set of tags as T . Pt ? P is the set of
phrases containing the tag t ? T . We use the proba-
bilistic image annotation model of Feng and Lapata
(2010a) to generate the list of keywords. The lat-
ter highlight the objects depicted in the image and
should be in all likelihood included in the caption.
The model is cast as an integer linear program:
max
x ?
i?P
( fi +?gi)xi (1a)
s.t. ?
i?P
lixi ? Lmax (1b)
?
i?P
lixi ? Lmin (1c)
?
i?Pt ,t?T
xi ? Tmin (1d)
x j? xi ?i ? P , j ?Di (1e)
?
j?Ci
x j = xi ?i ? C , j ? Ci (1f)
xi? ys ?s ? S , i ? Ps (1g)
?
s?S
ys ? NS (1h)
xi ? {0,1} ?i ? P (1i)
ys ? {0,1} ?s ? S . (1j)
A vector of binary variables x? {0,1}|P | indicates
if each phrase is to be part of the output. The vector
of auxiliary binary variables y ? {0,1}|S | indicates
from which sentences the chosen phrases come, see
Equation (1g).
Our objective function (1a) is the weighted sum of
two components for each phrase: a salience score,
and a measure of how frequently the QG rule was
seen in the training data. Let fi denote the salience
score for phrase i, determined by the machine learn-
ing algorithm. We apply a paraphrase penalty gi to
each phrase,
gi = log
(
nr
Nr
)
,
where nr is a count of the number of times this par-
ticular QG rule r was seen in the training data, and
Nr is the number of times all suitable rules for this
phrase node were seen. If no suitable rules exist,
we set gi = 0. The intuition here is that common
paraphrases should be more trustworthy, and thus
are given a smaller penalty than rare ones. Para-
phrase penalties are weighted by the constant param-
eter ?. which controls the amount of paraphrasing
we allow in the output. The objective function is
the sum of the salience scores and paraphrase penal-
ties of all the phrases chosen to form the output of a
given document, subject to the constraints in Equa-
517
tions (1b)?(1j). The latter provide a natural way of
describing the requirements the output must meet.
Constraints (1b) and (1c) ensure that the gener-
ated output stays within the acceptable length range
of (Lmin,Lmax) tokens. Equation (1d) is a set-
covering constraint, requiring that at least Tmin words
in T appear in the output. This is important where
we want to focus on some aspect of the source doc-
ument, for instance on the subject of an image.
Constraint (1e) ensures that the phrase dependen-
cies are respected and thus enforces grammatical
correctness. Phrases that depend on phrase i are con-
tained in the set Di. Variable xi is true, and therefore
phrase i will be included, if any of its dependents
x j ?Di are true. The phrase dependency constraints,
contained in the set Di and enforced by (1e), are the
result of three principles based on the typed depen-
dency information:
1. Where the QG provides alternative para-
phrases, it makes sense of course to select only
one. This is controlled by constraint (1f), and
by placing all paraphrases in the set Di for the
choice node i.
2. Where there are no applicable QG rules to
guide the model, in general we require all child
nodes j of the current node i to be included in
the summary if node i is included. As excep-
tions, we allow the subtree represented by node
j to be deleted if the dependency label for the
connecting edge i? j is of type advcl (adver-
bial clause) or some form of conj (conjunction).
3. In general, we force the parent node p of the
current node i to be included in the output if i
is, resulting in all ancestors up to the root node
being included. We allow a break, and the sub-
tree at i to be used as a stand-alone sentence, if
the PCFG parser has marked i with an S (sen-
tence) label.
Constraint (1g) tells the ILP to output a sentence if
one of its constituent phrases is chosen. Finally, (1h)
limits the output to a maximum of NS sentences.
4 Experimental Set-up
As mentioned earlier we evaluated the performance
of our model on two title generation tasks, namely
headline and caption generation. In this section we
give details on the corpora and grammars we used,
model parameters and features. We also describe the
baselines used for comparison with our approach,
and explain how system output was evaluated.
Training We obtained phrase-based salience
scores using a supervised machine learning algo-
rithm. For the headline generation task, the full
DUC-03 (Task 1) corpus was used for training;
it contains 500 documents and 4 headline-style
summaries per document. For the captions, training
data was gathered from the CNN news website.2
We used 200 documents and their corresponding
captions. Sentences were first tokenized to separate
words and punctuation, and then parsed to obtain
phrases and dependencies as described in Section 3
using the Stanford parser (Klein and Manning,
2003). Document phrases were marked as positive
or negative automatically. If there was a unigram
overlap (excluding stop words) between the phrase
and any of the original title or caption, we marked
this phrase with a positive label. Non-overlapping
phrases were given negative labels.
Our feature set comprised surface features such as
sentence and paragraph position information, POS
tags, and whether high-scoring tf.idf words were
present in the phrase. Additionally, the caption train-
ing set contained features for unigram and bigram
overlap with the title. We learned the feature weights
with a linear SVM, using the software SVM-OOPS
(Woodsend and Gondzio, 2009). This tool gave us
directly the feature weights as well as support vec-
tor values, and it allowed different penalties to be
applied to positive and negative misclassifications,
enabling us to compensate for the unbalanced data
set. The penalty hyper-parameters chosen were the
ones that gave the best F-scores, using 10-fold vali-
dation.
For each of the two tasks, QG rules were extracted
from the same data used to train the SVM, resulting
in 2,910 distinct rules for headlines and 2,757 rules
for the captions. Table 1 shows that for both tasks,
the majority of rules apply to PP and NP phrases.
Both tasks involve considerable compression, but
the proportions of the rewrite operations involved in-
dicate differences in style between them. Compared
2See http://edition.cnn.com/.
518
Label Prop?n Proportion for Label
of set Unmod Del Ins Re-ord
PP 40% 5% 93% 12% 6%
NP 31% 5% 87% 14% 7%
S 20% 1% 96% 15% 7%
SBAR 6% 4% 95% 28% 6%
(a) Headlines
Label Prop?n Proportion for Label
of set Unmod Del Ins Re-ord
PP 30% 17% 81% 7% 4%
NP 29% 17% 76% 11% 3%
S 27% 10% 84% 16% 6%
SBAR 10% 13% 80% 16% 3%
(b) Captions
Table 1: QG rules generated for (a) headline and
(b) caption tasks (top 4 labels shown). The columns
show label of root node, proportion of the full rule-
set, then the proportions of rules for this label in-
volving no modification, deletions, insertions and
re-orderings.
to headlines, captions involve slightly less deletion
and a higher proportion of the phrases are unmod-
ified. The QG learning mechanism also discovers
more alignments between source sentences and cap-
tions than it does for the headline task.
Title generation For the headline generation task,
we evaluated our model on a testing partition from
the DUC-04 corpus (75 documents, Task 1). For the
caption task, we used the test set (240 documents)
described in Feng and Lapata (2010a). Their corpus
was downloaded from the BBC news site and con-
tains documents, images, and their captions.3
We created and solved an ILP for each docu-
ment. For each phrase, features were extracted and
salience scores calculated from the feature weights
determined through SVM training. The distance
from the SVM hyperplane represents the salience
score. Parameters for the ILP models for the two
tasks are shown in Table 2. The ? parameter was
set to 0.2 to ensure that paraphrases were included;
other parameters were chosen to capture the prop-
3Available from http://homepages.inf.ed.ac.uk/
s0677528/data.html.
Parameter Headlines Captions
Min length Lmin 8 8
Max length Lmax 16 20
Min keywords Tmin 0 2
Max sentences NS 5 1
Paraphrase ? 0.2 0.1
Table 2: ILP model parameters for the two tasks.
erties seen in the majority of the training set. Note
the maximum number of sentences allowed to form
a headline is set to 5 as some of the headlines in the
DUC dataset contained multiple sentences.
To solve the ILP model we used the ZIB Opti-
mization Suite software (Achterberg, 2007; Koch,
2004). The solution was converted into a sentence
by removing nodes not chosen from the tree rep-
resentation, then concatenating the remaining leaf
nodes in order.
Model Comparison For the headline task, we
compared our model to the DUC-04 standard base-
line of the first sentence, truncated at the first word
boundary after 75 characters; and the output of the
Topiary system (Zajic et al, 2004), which came top
in almost all measures in the DUC-04 evaluation.
In order to generate a headline, Topiary first com-
presses the lead sentence using linguistically moti-
vated heuristics and then enhances it with topic key-
words. For the captions, we compared our model
against the highest-scoring document sentence ac-
cording to the SVM and against the probabilistic
model presented in Feng and Lapata (2010a). The
latter estimates the probability of a phrase appear-
ing in the caption given the same phrase appearing
in the corresponding document and uses a language
model to select among many different surface real-
izations. The language model is adapted with prob-
abilities from an image annotation model (Feng and
Lapata, 2010b).
Evaluation We evaluated the quality of the head-
lines using ROUGE (Lin and Hovy, 2003). The
DUC-04 dataset provides four reference head-
lines per document. We report unigram overlap
(ROUGE-1) and bigram overlap (ROUGE-2) as a
means of assessing informativeness, and the longest
common subsequence (ROUGE-L) as a means of as-
519
sessing fluency. Original DUC-04 ROUGE parame-
ters were used. We also use ROUGE to evaluate the
automatic captions with the original BBC captions
as reference.
In addition, we evaluated the generated headlines
by eliciting human judgments. Participants were
presented with a news article and its correspond-
ing headline and were asked to rate the latter along
two dimensions: informativeness (does the headline
capture the article?s most important information?),
and grammaticality (is it fluent and easy to under-
stand?). The subjects used a seven point rating scale;
an ideal system would receive high numbers for
both measures. We randomly selected twelve docu-
ments from the test set and generated headlines with
our model. We also included the output of Topiary
and the human written DUC-04 headlines as a gold
standard. We thus obtained ratings for 48 (12 ? 4)
document-highlights pairs.
We elicited judgments for the generated captions
in a similar fashion. Participants were presented
with a document, an associated image, and its cap-
tion, and asked to rate the latter (using a 1?7 rating
scale) with respect to grammaticality and informa-
tiveness (does it describe succinctly the content of
the image and document?). Again, we randomly se-
lected 12 document-image pairs from the test set and
generated captions for them using the highest scor-
ing document sentence according to the SVM, our
ILP-based model, and the output of Feng and Lap-
ata?s (2010a) system. We also included the original
BBC captions as an upper bound. Both studies were
conducted over the Internet using WebExp (Keller
et al, 2009). 80 unpaid volunteers rated the head-
lines and 65 the captions, all self reported native En-
glish speakers.
5 Results
We report results on the headline generation task in
Figure 3, with ROUGE-1, ROUGE-2 and ROUGE-
L. In ROUGE-1 and ROUGE-L measures, the best
scores are obtained by the Topiary system, slightly
better than the lead sentence baseline, while for
ROUGE-2 the ordering is reversed. Our model does
not outperform the lead sentence or Topiary. Note
that the 95% confidence level intervals reported by
ROUGE are so large that no results are statistically
Lead The chances for a new, strictly secular government in
Turkey faded Wednesday.
Topiary TURKEY YILMAZ PARTY ECEVIT chances strictly
secular government faded.
ILP Bulent Ecevit needs Turkey?s two-center right parties to
hammer together secular coalition.
DUC Chance for new, secular, Turkish government fades; what
will Ecevit do now?
Source Premier-designate Bulent Ecevit needs Turkey?s two-
center right parties to hammer together a secular coali-
tion, but Tansu Ciller, the ex-premier who commands 99
votes in parliament, rebuffed him Wednesday.
Lead U.S. President Bill Clinton won South Korea?s support
Saturday for confronting.
Topiary NUCLEAR U.S. President Bill Clinton won for con-
fronting North Korea.
ILP North Koreans have denied construction site has nuclear
purpose.
DUC U.S. warns N. Korea not to waste chance for peace over
alleged nuclear site.
Source The North Koreans have denied the underground con-
struction site has any nuclear purpose, and it has de-
manded a dlrs 300 million payment for proving that.
Lead By only one vote, the center-left prime minister of Italy,
Romano Prodi.
Topiary PRODI By only one vote center left prime minister and
toppled from power.
ILP Political system changes, Italy is condemned to political
instability.
DUC Prodi loses confidence vote; will stay as caretaker until
new government.
Source ?Unless the Italian political system changes, Italy is con-
demned to political instability,? said Sergio Romano, a
former diplomat and political science professor.
Table 3: Example headline output.
F&L The former paramedic training officer stood at the next
general election.
ILP The majority are now believing that war in Iraq was
wrong.
BBC L/Cpl Thomas Keys was shot 18 times, his inquest heard.
Source The majority of people in this country are now believing
that the war in Iraq was wrong, and I do believe we will
get support.
F&L The state government of Victoria take as those tests for
cannabis.
ILP Police in Victoria have begun randomly testing drivers for
the drug ecstasy.
BBC Police say drugs like Ecstasy can be as dangerous as al-
cohol for drivers.
Source Police in the Australian state of Victoria have begun ran-
domly testing drivers for the drug ecstasy.
F&L The US Government Professor Holdren called for more
than a year.
ILP ?We are experiencing dangerous human disruption of
global climate,? Professor Holdren said.
BBC Sea levels could rise by 4m over the coming century, he
warns.
Source ?We are experiencing dangerous human disruption of the
global climate and we?re going to experience more,? Pro-
fessor Holdren said.
Table 4: Example caption output.
520
 0
 0.05
 0.1
 0.15
 0.2
 0.25
 0.3
Lead-1 Topiary ILP
Sc
or
e
Rouge-1
Rouge-L
Rouge-2
Figure 3: ROUGE-1, ROUGE-2 and ROUGE-L re-
sults on the DUC-04 headlines for our ILP model,
the lead sentence baseline and Topiary.
 0
 0.05
 0.1
 0.15
 0.2
SVM F&L ILP
Sc
or
e
Rouge-1
Rouge-L
Rouge-2
Figure 4: ROUGE-1, ROUGE-2 and ROUGE-L re-
sults on the BBC captions for our ILP model, the
sentence baseline chosen by the SVM, and Feng and
Lapata?s (2010) model.
significant. We also investigated using an ILP model
with just the QG rules or just dependency label in-
formation (see constraint (1e) in Section 3.3). Both
settings gave less compressed output, and the result-
ing ROUGE scores were lower on all measures. The
ROUGE results for the caption generation task fol-
low a similar pattern (see Figure 4). Our model is
slightly better than the best sentence baseline but
performs worse than Feng and Lapata (2010a). Ta-
bles 3 and 4 show example output for the ILP model
and the baselines on the headline and caption tasks
respectively. In the tables, Source refers to the sen-
tence chosen by the ILP, but before any paraphrasing
is applied. We can see that deletion rules dominate,
and a more compressive style of paraphrasing has
been learned for the headline task.
The results of our human evaluation study for
the DUC-04 headlines are summarized in Table 5.
Means differences were compared using a Post-hoc
Model Grammaticality Importance
Lead-1 4.95 3.30
Topiary 3.03 3.43
ILP 5.36 4.94
Reference 5.12 5.17
Table 5: Average human ratings of DUC-04 head-
lines, for our ILP model, the lead sentence baseline,
the output of Topiary and the human-written refer-
ence.
Model Grammaticality Importance
SVM 5.24 5.01
F&L 4.42 4.74
ILP 5.49 5.25
Reference 5.61 5.18
Table 6: Average human ratings of captions, for
our ILP model, the sentence baseline chosen by the
SVM, Feng and Lapata?s (2010) model and the ref-
erence BBC caption.
Tukey test. The headlines created by our model
were considered significantly more important and
more grammatical than those of the Topiary sys-
tem (? < 0.01), despite the better overlap of Topi-
ary with the reference headlines as indicated in the
Rouge results above. Compared to the lead sentence
of the article (the DUC-04 baseline), our model was
also rated significantly higher in terms of importance
(? < 0.01) but not grammaticality.
Table 6 summarizes the results of our second
judgment elicitation study. The captions generated
by our model are significantly more grammatical
than those of Feng and Lapata (2010a) (? < 0.01).
The SVM, ILP model and reference captions do not
differ significantly in terms of grammaticality. In
terms of importance, the ILP model is significantly
better than the SVM (? < 0.01) and Feng and Lap-
ata (? < 0.01) and comparable to the reference.
The human ratings are more favorable to our
model than ROUGE for both tasks. There are two
reasons for this. Firstly, the model is not bi-
ased towards selecting the lead sentence as a head-
line/caption and is disadvantaged in ROUGE evalua-
tions as professional abstractors often reuse the lead
or parts of it to create a title. Secondly, the model
often generates an appropriate title that is lexically
521
distinct from the reference even though it expresses
similar meaning.
6 Conclusions
In this paper we proposed a joint content se-
lection and surface realization model for single-
document summarization. The model operates over
a syntax-rich representation of the source docu-
ment and learns which phrases should be in the
summary. Content selection preferences are cou-
pled with a quasi-synchronous grammar whose rules
encode surface realization preferences (e.g., para-
phrases and compressions). Both types of prefer-
ences are optimized simultaneously in an integer lin-
ear program subject to grammaticality, length and
coverage constraints. Importantly, the QG allows
the model to adapt to the writing and stylistic con-
ventions of different tasks. The results of our hu-
man studies show that our system creates grammati-
cal and informative summaries whilst outperforming
several competitive baselines.
The model itself is relatively simple and achieves
good performance without any task-specific modifi-
cation. One potential stumbling block may be the
availability of parallel data for acquiring the QG.
The Internet provides a large repository of news
documents with headlines, images and captions. In
some cases news articles are even accompanied with
?story highlights? which could be used as training
data for longer summaries.4 For other domains ob-
taining such data may be more difficult. However,
our experiments have shown that relatively small
parallel corpora (in the range of 200?500 pairs) suf-
fice to learn many of the writing conventions for a
given task.
In the future, we plan to explore how to inte-
grate more sophisticated QG rules in the generation
process. Currently we consider deletions, reorder-
ings and insertions. Ideally, we would also like to
model arbitrary substitutions between words but also
larger constituents (e.g., subclauses, sentence aggre-
gation). Beyond summarization, we would also like
to apply our model to other generation tasks, such as
paraphrasing and text simplification.
4On-line CNN news articles are prefaced by story
highlights?three or four short sentences that are written by hu-
mans and give a brief overview of the article.
Acknowledgments We are grateful to David Chi-
ang and Noah Smith for their input on earlier ver-
sions of this work. We would also like to thank
Andreas Grothey and members of ICCS at the
School of Informatics for valuable discussions and
comments. We acknowledge the support of EP-
SRC through project grants EP/F055765/1 and
GR/T04540/01.
References
Achterberg, Tobias. 2007. Constraint Integer Program-
ming. Ph.D. thesis, Technische Universita?t Berlin.
Banko, Michele, Vibhu O. Mittal, and Michael J. Wit-
brock. 2000. Headline generation based on statisti-
cal translation. In Proceedings of the 38th ACL. Hong
Kong, pages 318?325.
Clarke, James and Mirella Lapata. 2008. Global infer-
ence for sentence compression: An integer linear pro-
gramming approach. Journal of Artificial Intelligence
Research 31:399?429.
Cohn, Trevor and Mirella Lapata. 2008. Sentence com-
pression beyond word deletion. In Proceedings of the
22nd COLING. Manchester, UK, pages 137?144.
Das, Dipanjan and Noah A. Smith. 2009. Paraphrase
identification as probabilistic quasi-synchronous
recognition. In Proceedings of the ACL-IJCNLP.
Suntec, Singapore, pages 468?476.
Daume? III, Hal. 2006. Practical Structured Learning
Techniques for Natural Language Processing. Ph.D.
thesis, University of Southern California.
Daume? III, Hal and Daniel Marcu. 2002. A noisy-channel
model for document compression. In Proceedings of
the 40th ACL. Philadelphia, PA, pages 449?456.
Dorr, Bonnie, David Zajic, and Richard Schwartz.
2003. Hedge trimmer: A parse-and-trim approach
to headline generation. In Proceedings of the HLT-
NAACL 2003 Text Summarization Workshop and Doc-
ument Understanding Conference. Edmondon, Al-
berta, pages 1?8.
Dras, Mark. 1999. Tree Adjoining Grammar and the Re-
luctant Paraphrasing of Text.. Ph.D. thesis, Macquarie
University.
Feng, Yansong and Mirella Lapata. 2010a. How many
words is a picture worth? Automatic caption gener-
ation for news images. In Proceedings of the 48th
Annual Meeting of the Association for Computational
Linguistics. Association for Computational Linguis-
tics, Uppsala, Sweden, pages 1239?1249.
Feng, Yansong and Mirella Lapata. 2010b. Topic mod-
els for image annotation and text illustration. In Pro-
522
ceedings of the NAACL HLT . Association for Com-
putational Linguistics, Los Angeles, California, pages
831?839.
Jing, Hongyan. 2000. Sentence reduction for automatic
text summarization. In Proceedings of the 6th ANLP.
Seattle, WA, pages 310?315.
Jing, Hongyan. 2002. Using hidden Markov modeling to
decompose human-written summaries. Computational
Linguistics 28(4):527?544.
Jing, Hongyan and Kathleen McKeown. 2000. Cut
and paste summarization. In Proceedings of the 1st
NAACL. Seattle, WA, pages 178?185.
Keller, Frank, Subahshini Gunasekharan, Neil Mayo, and
Martin Corley. 2009. Timing accuracy of web experi-
ments: A case study using the WebExp software pack-
age. Behavior Research Methods 41(1):1?12.
Klein, Dan and Christopher D. Manning. 2003. Accurate
unlexicalized parsing. In Proceedings of the 41st ACL.
Sapporo, Japan, pages 423?430.
Koch, Thorsten. 2004. Rapid Mathematical Prototyping.
Ph.D. thesis, Technische Universita?t Berlin.
Kupiec, Julian, Jan O. Pedersen, and Francine Chen.
1995. A trainable document summarizer. In Proceed-
ings of SIGIR-95. Seattle, WA, pages 68?73.
Lin, Chin-Yew. 2003. Improving summarization perfor-
mance by sentence compression ? a pilot study. In
Proceedings of the 6th International Workshop on In-
formation Retrieval with Asian Languages. Sapporo,
Japan, pages 1?8.
Lin, Chin-Yew and Eduard H. Hovy. 2003. Automatic
evaluation of summaries using n-gram co-occurrence
statistics. In Proceedings of HLT NAACL. Edmonton,
Canada, pages 71?78.
Martins, Andre? and Noah A. Smith. 2009. Summariza-
tion with a joint model for sentence extraction and
compression. In Proceedings of the Workshop on In-
teger Linear Programming for Natural Language Pro-
cessing. Boulder, Colorado, pages 1?9.
Smith, David and Jason Eisner. 2006. Quasi-synchronous
grammars: Alignment by soft projection of syntactic
dependencies. In Proceedings on the Workshop on Sta-
tistical Machine Translation. Association for Compu-
tational Linguistics, New York City, pages 23?30.
Smith, David A. and Jason Eisner. 2009. Parser adapta-
tion and projection with quasi-synchronous grammar
features. In Proceedings of the EMNLP. Suntec, Sin-
gapore, pages 822?831.
Soricut, R. and D. Marcu. 2007. Abstractive head-
line generation using WIDL-expressions. Information
Processing and Management 43(6):1536?1548. Text
Summarization.
Wang, Mengqiu, Noah A. Smith, and Teruko Mita-
mura. 2007. What is the Jeopardy model? a quasi-
synchronous grammar for QA. In Proceedings of the
EMNLP-CoNLL. Prague, Czech Republic, pages 22?
32.
Woodsend, Kristian and Jacek Gondzio. 2009. Exploiting
separability in large-scale linear support vector ma-
chine training. Computational Optimization and Ap-
plications Published online.
Woodsend, Kristian and Mirella Lapata. 2010. Automatic
generation of story highlights. In Sandra Carberry and
Stephen Clark, editors, Proceedings of the 48th ACL.
Uppsala, Sweden, pages 565?574.
Zajic, David, Bonnie Dorr, and Richard Schwartz. 2004.
BBN/UMD at DUC-2004: Topiary. In Proceedings
of the NAACL Workshop on Document Understanding.
Boston, MA, pages 112?119.
Zajic, David, Bonnie J. Dorr, Jimmy Lin, and Richard
Schwartz. 2007. Multi-candidate reduction: Sentence
compression as a tool for document summarization
tasks. Information Processing Management Special Is-
sue on Summarization 43(6):1549?1570.
Zhao, Shiqi, Xiang Lan, Ting Liu, and Sheng Li. 2009.
Application-driven statistical paraphrase generation.
In Proceedings of the Joint Conference of the 47th
Annual Meeting of the ACL and the 4th International
Joint Conference on Natural Language Processing of
the AFNLP. Suntec, Singapore, pages 834?842.
523
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 832?842, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Explore Person Specific Evidence in Web Person Name Disambiguation
Liwei Chen, Yansong Feng, Lei Zou, Dongyan Zhao
Institute of Computer Science and Technology
Peking University
Beijing
{clwclw88,fengyansong,zoulei,zhaodongyan}@pku.edu.cn
Abstract
In this paper, we investigate different usages
of feature representations in the web person
name disambiguation task which has been suf-
fering from the mismatch of vocabulary and
lack of clues in web environments. In litera-
ture, the latter receives less attention and re-
mains more challenging. We explore the fea-
ture space in this task and argue that collecting
person specific evidences from a corpus level
can provide a more reasonable and robust es-
timation for evaluating a feature?s importance
in a given web page. This can alleviate the
lack of clues where discriminative features can
be reasonably weighted by taking their corpus
level importance into account, not just relying
on the current local context. We therefore pro-
pose a topic-based model to exploit the person
specific global importance and embed it into
the person name similarity. The experimen-
tal results show that the corpus level topic in-
formation provides more stable evidences for
discriminative features and our method out-
performs the state-of-the-art systems on three
WePS datasets.
1 Introduction
Resolving ambiguity associated with person names
found on the Web is a key challenge in many Internet
applications, such as information retrieval, question
answering, open information extraction, automatic
knowledge acquisition(Wu and Weld, 2008) and so
on. For example, if you want to know more about a
guy named George Foster and feed Yahoo! with his
name, the results are not satisfactory where you get
more than 40 different persons named George Fos-
ter scattering in the top 100 returned pages. None
of the dominant search engines currently helps users
group those returned pages into clusters according
to whether they refer to the same person. Users thus
have to either read those pages carefully or adjust
their queries by adding extra modifiers. This moti-
vates an intensive study in automatically resolving
person name ambiguity in various web applications.
However, resolving web person name ambiguity
is not a trivial task. Due to the difficulties in fig-
uring out or predicting the number of namesakes
in the returned pages, the task has been investi-
gated in an unsupervised learning fashion in the lit-
erature, which is apparently different from the tra-
ditional word sense disambiguation or entity link-
ing/disambiguation tasks, where the inventories of
candidate word senses or entities are usually known
given the target word or entity mention.
A general framework for this task can be formu-
lated as first extracting various features from the web
pages, and then grouping these pages into several
clusters each of which is assumed to represent one
specific person. Despite of the inevitably noisy na-
ture of web data, a key challenge is how to handle
the data sparsity problem which we mean as: mis-
match of vocabulary and lack of clues. The for-
mer refers to the case that two web pages may de-
scribe the same person but use different words thus
the word overlap between them are small. Vari-
ous features, including entities, biographical infor-
mation, URL, etc., have been introduced to bridge
the gap(Mann and Yarowsky, 2003; Kalashnikov
et al2008a; Ikeda et al2009; Jiang et al2009),
832
and external knowledge resources are also employed
to capture the semantic relationship between enti-
ties(Han and Zhao, 2009, 2010). However, a more
challenging scenario is that there are few clues avail-
able in the web pages. For example, there is a page
mentioning a nutritionist Emily Bender in WePS2
dataset(Javier et al2009). Throughout the whole
page we can find only one word, nutrition, related to
her identification, while other pages about the nu-
tritionist in the dataset contain substantial materi-
als about her profession and job. In this case, cur-
rent efforts, focusing on either feature engineering
or background knowledge, are incapable to exploit-
ing these limited clues from the current page to the
whole Emily Bender document set, where nutrition,
as an important feature for recognizing a nutritionist,
should be paid more attention.
As far as we know, there is less work focusing
on exploring person specific information to relieve
the lack of clues problem. Traditional vector space
model (VSM) is most widely used to accommodate
various features, but it ignores any relations between
them(Mann and Yarowsky, 2003; Ikeda et al2009).
Beyond bag-of-features, two kinds of features are
explored, co-occurrences of entities and Wikipedia
based semantic relationship between entities, both
of which provide a reasonable relatedness for en-
tity pairs. More recent works adopt one of these
relationships(Jiang et al2009; Kalashnikov et al
2008a; Han and Zhao, 2009). Han and Zhao try
to model both aspects, but their co-occurrence es-
timation, estimated from held-out resources, fails to
capture the person specific importance for a feature,
which is crucial to enhance limited clues in a cor-
pus level, e.g., the significance of nutrition for Emily
Bender in WePS1 dataset.
In this paper, we explore different usages of fea-
tures and propose an approach which mines cross
document information to capture the person specific
importance for a feature. Specifically, we construct a
semantic graph from Wikipedia concepts appearing
in all documents that contain the target name (which
we refer to name observation set), then group them
into several topics and further weight each feature by
considering both the relatedness of the feature to its
corresponding topic and the importance of this topic
in the current name observation set. By incorporat-
ing both the Wikipedia and topic information into
our person name similarity, our model exploits both
Wikipedia based background knowledge and per-
son specific importance. We argue that the corpus
level importance provides more stable evidences for
discriminative features in various scenarios, espe-
cially the tough case. We compared our model with
the state of the arts on three WePS datasets (from
the First and Second Web People Search Cluster-
ing Task), and our experiments show that our model
consistently outperforms other competitive models
on all three datasets.
In the rest of this paper, we first review related
work, and in Section 3, show how we exploit the
person specific importance in our disambiguation
model. Experiment results are discussed in Sec-
tion 4. We conclude this paper in Section 5.
2 Related Work
Web person name ambiguity resolution can be for-
mally defined as follows: Given a set of web
pages {d1, d2, ..., dn}, where each page di (i =
1, ..., n) contains an ambiguous name N which may
correspond to several persons holding this name
among these pages. The disambiguation system
should group these name observations into j cluster
{c1, c2, ..., cj} each of which is expected to contain
web pages about the same person.
As mentioned before, the task is usually formu-
lated in a unsupervised fashion, including two steps:
feature extraction and person clustering. Most re-
search efforts so far have been made to the for-
mer, exploring various features according to spe-
cific applications, while the second step is currently
dominated by hierarchical agglomerative cluster-
ing (HAC). According to the reliance of extra
knowledge resources, existing works can be catego-
rized into non-resource methods and resource-based
methods. Non-resource methods extract various lo-
cal features from the context of ambiguous names,
and compute the similarity between feature vectors.
These features include plain words(Bagga and Bald-
win, 1998), biographical information(Mann and
Yarowsky, 2003; Niu et al2004), named enti-
ties, compound key phrases, hyperlinks(Ikeda et al
2009), etc. The similarity between namesakes are
usually measured by the cosine similarity(Bagga
and Baldwin, 1998), or other graph based met-
833
rics(Iria et al2007; Kalashnikov et al2008a;
Jiang et al2009). Those methods pay more at-
tention to extracting informative features and their
co-occurrences, but they usually treat the features lo-
cally, and ignore the semantic relatedness of features
beyond the current document.
Resource-based approaches, on the other hand,
can leverage external resources to benefit from rich
background knowledge, which is crucial to rem-
edy the data sparsity problem. The employed re-
sources include raw texts available on the web and
online encyclopedias. Kalashnikov et alnd Yim-
ing et alse extra web corpora to obtain co-
occurrences between named entities. Rao et alse
Google Snippets to provide more contexts. By em-
ploying Wikipedia, the largest online encyclopedia,
rich background knowledge about the semantic re-
latedness between entities can be leveraged to im-
prove the disambiguation performance, and relieve
the coverage problem, to some extent. Bunescu
and Pasca and Cucerzan utilize Wikipedia?s cate-
gory hierarchy to disambiguate entities, while Pilz
uses Wikipedia?s link information. Han and Zhao
adopt Wikipedia semantic relatedness to compute
the similarity between name observations. They also
combine multiple knowledge sources and capture
explicit semantic relatedness between concepts and
implicit semantic relationship embedded in a seman-
tic graph simultaneously(Han and Zhao, 2010).
Most approaches discussed above explore vari-
ous features in the current page or rely on exter-
nal knowledge resources to bridge the vocabulary
gap, but pay less attention to the lack of clues since
they ignore the person specific evidence in the cur-
rent corpus level. Our model focuses on solving the
data sparsity problem by utilizing other web pages
in the same name observation set to provide a robust
but person specific weighting for discriminative fea-
tures beyond the current document alone. In terms
of extra resources, the Wikipedia based model (WS)
by Han and Zhao (2009) is close to our model. The
WS model uses Wikipedia to capture the relation-
ship between entities in the local context to bridge
the vocabulary gap, but it is incapable to evaluate
the importance of a feature with regarding to the tar-
get name, hence is unable to make use of limited
clues in the current web page. Our method captures
person specific evidences by generating topics from
all concepts in the current name observation set and
weighting a feature accordingly. In this case, dis-
criminative features that are sparse in the current
page can be globally weighted so as to provide a
more accurate and stable person name similarity.
3 The Model
Our model consists of three steps: feature extrac-
tion, topic generation and name disambiguation. For
an ambiguous name, we first extract three types of
features and construct a semantic graph from all
Wikipedia concepts extracted from the current name
observation set. We then collect global person spe-
cific evidences by clustering these concepts on the
graph into different topics, which in turn are used
to weight each concept by considering the impor-
tance of its corresponding topic in the current name
observation set and its highly related neighbors in
both the topic and its local context. At last, we in-
corporate the proposed topic representation into the
person name similarity functionand adopt the hierar-
chical agglomerative clustering (HAC) algorithm to
group these web pages.
3.1 Feature Extraction
We extract features from the contexts of ambiguous
names, including Wikipedia concepts, named enti-
ties and biographical information, such as email ad-
dresses, phone numbers and birth years.
Wikipedia Concept Extraction Each concept in
Wikipedia is described by an article containing hy-
perlinks to other concepts which are supposed to
related to the current one. All the linking rela-
tions in Wikipedia construct a huge semantic graph,
where we can mine rich semantic relationship be-
tween concepts(David and Ian, 2008). We col-
lect Wikipedia concepts from all web pages in the
dataset by comparing all n-grams (up to 8) from
the dataset to Wikipedia anchor text dictionary and
checking whether it is a Wikipedia concept surface
form. We further prune the extracted concepts ac-
cording to their keyphraseness(Mihalcea and Cso-
mai, 2007). Initially, each concept is weighted ac-
cording to its average semantic relateness(David and
Ian, 2008) with other concepts in the current page.
Named Entity and Biographical Information Ex-
traction Although Wikipedia concepts can pro-
834
vide rich background knowledge, they suffer from
the limited coverage. It is common that some
discriminative features are not likely to be found
in Wikipedia, such as names of infamous people
or organizations, email addresses, phone numbers,
etc. We therefore extract two extra kinds of fea-
tures, named entities that do not appear in the
Wikipedia anchor text dictionary, and biographical
information. We use Stanford Named Entity Rec-
ognizer(Finkel et al2005) to collect named entities
which are not in the Wikipedia list. We use regular
expressions to extract email address, phone numbers
and birth years. For convenience, we will also call
concept features for Wikipedia concept features and
non-concept features for the other two in the rest of
this paper.
3.2 Topic Generation and Weighting Scheme
Now we proceed to describe the key step of our
model, topic generation and weighting strategy. The
purpose of introducing topics into our model is to
exploit the corpus level importance of a feature for
a given name so that we will not miss any discrim-
inative features which are few in the current name
observation but have shown significant importance
over the whole name observation set.
Graph Construction In our model, we capture
the topic structure through a semantic graph. Specif-
ically, for each name observation set, we connect
all Wikipedia concepts appearing in the current ob-
servation set by their pairwise semantic relatedness-
David and Ian (2008)to form a semantic graph.
The constructed graph is usually very dense since
any pair of unrelated concepts would be connected
by a small semantic relatedness resulting in many
light-weighted or even meaningless edges. We
therefore propose to prune some light-weighted
edges to make the graph stable and easier to harvest
reasonable topics. We use the following strategies to
prune the graph:
? If an edge?s weight is lower than a predefined
threshold, it will be pruned.
? If two vertices of an edge do not co-occur in
any web page of the current observation set,
then this edge will be pruned.
Home 
Run
Major 
League 
Baseball
Stolen 
Base
Cincinnati 
Reds
Shortstop
Sports 
League
Cornerback
Tackle
National 
Football 
League Pro 
Football 
Weekly
0.3862
0.4228
0.3799
0.2976
0.3296
0.2697
0.2445
0.3467
0.3628
0.4145
0.4008 0.3205
0.2738
0.3567
0.3201
0.3136 0.2245
Figure 1: An abridged example of the semantic graph
for George Foster. The green node Sports League is a
hub node, and the yellow node Pro Football Weekly is an
outlier.
The second rule is set to be strict and is proposed
to handle the following circumstance. Some gen-
eral concepts, such as swimming, football, basket-
ball and golf, will be measured highly related with
each other by Wikipedia semantic relatedness and
thus are very likely to be grouped into one topic,
however, they are discriminative on their own when
disambiguating different persons. For example, the
concept swimming is discriminative enough to dis-
tinguish Russian swimmer Popov from basketball
player Popov. So it is not a good idea to group
these concepts into one topic. The proposed co-
occurrence rule is based on the above observation
that it is rare that such kind of general concepts,
e.g., swim and basketball, often co-occur with each
other when talking about one specific person. Af-
ter the pruning step, for each ambiguous name, we
get a semantic graph from all Wikipedia concepts
extracted in this name observation set. Figure 1 il-
lustrates an abridged version of a semantic graph for
George Foster.
835
Graph Clustering Considering the graph con-
struction strategy we use, it is more suitable for us
to group the concepts on the graph into several top-
ics using a density-based clustering model.
We choose SCAN algorithm Xu et al2007) to
perform the clustering step. The SCAN algorithm
utilizes a neighborhood structure to measure the
similarity between two vertices. If a vertex has a
minimal of ? neighbors with a similarity larger than
?, it is called a core. The algorithm1 starts from a
random vertex in a graph, examining whether it is a
core or not. If yes, the algorithm will expand a clus-
ter from this vertex recursively, otherwise the vertex
will be assigned either a hub node or an outlier de-
pending on the number of its neighboring clusters.
A hub node connects to more than one cluster, while
an outlier connects to one or no cluster. Take the
semantic graph in Figure 1 for example, the node
Sports League is a hub node, while the node Pro
Football Weekly is an outlier. Finally, all concepts
in the graph are grouped into K + 2 parts (K is the
number of the clusters, and is determined automat-
ically), including K clusters, the set of hub nodes
and the set of outliers.
One problem of applying SCAN in our work is
that it is originally designed for unweighted graphs.
We have to adapt it to our weighted graph by mod-
ifying the similarity function between two nodes as
follows:
sim(c1, c2) = ??
simnb(c1, c2)
1 + ?
+
sr(c1, c2)
1 + ?
(1)
and simnb(c1, c2) is defined as:
simnb(c1, c2) =
?
c?N(c1)?N(c2)
sr(c1,c)+sr(c2,c)
2
|N(c1) ?N(c2)|
where N(c) is the neighbor set of concept c. This
new similarity function contains two parts: the
neighborhood similarity and the semantic related-
ness between two concepts. We combine them us-
ing a linear combination, where ? is a weight tuned
during training.
Topic Generation Now we will map the cluster-
ing results into different topics. Intuitively, each
1We omit the details of SCAN for brevity, and refer inter-
ested reader to Xu et al2007) for more details.
cluster will be treated as a topic. However, we found
that hub nodes usually correspond to general con-
cepts which may be related to many topics, but with
a loose relatedness. We thus distribute each general
concept into its every related topic, but with a lower
weight to distinguish from ordinary concepts in this
topic.
Outliers may be concepts which are far away from
main themes of the corpus, or noise concepts. We
calculate the average semantic relatedness of an out-
lier with its neighbor concepts that belong to one
topic. If the result is lower than a threshold, this
outlier will be discarded, otherwise it will be treated
as a non-concept feature.
Now we are able to map the clustering results
into different topics. Intuitively, each cluster will
be treated as a topic. However, we found that hub
nodes usually correspond to general concepts, e.g.,
education or public, which may be related to many
topics, but with a loose relatedness. We thus dis-
tribute each general concept into its every related
topic, but with a lower weight to distinguish from
ordinary concepts in this topic. Outliers are found
to contain concepts which are far away from main
topics of the document set and look like noise con-
cepts. We therefore calculate the average semantic
relatedness of an outlier node with its neighboring
concepts which belong to some topics. If the aver-
age relatedness is lower than a threshold, this node
will be discarded, otherwise it will be treated as a
non-concept feature.
Weighting Topics After generating all topics, we
should weight each topic according to its importance
in the current name observation set as well as the
quality of the topic (cluster). Intuitively, if most con-
cepts in the topic are considered to be discriminative
in the current name set and they are closely related
to each other, this topic should be weighted as im-
portant. By properly weighting the generated topics,
we can capture the importance of a concept reliably
in the corpus level (in the current name observation
set) rather than in the current page solely.
Before we weight a topic, we first explain how
we re-weight a hub concept in a topic since our ini-
tial feature weighting scheme(Han and Zhao, 2009)
works on individual web page, lacks cross document
information and is likely to over-estimate the impor-
836
tance of a hub node (general concept) by by assign-
ing a higher weight. Suppose a hub node h connects
to a topic t with n neighbors, namely c1, c2, ? ? ? , cn.
The similarity between this hub node and the topic
is computed by averaging the semantic relatedness
between this hub node and these n neighbors:
sim(h, t) =
1
n
n?
i=1
sr(h, ci). (2)
We then update the weight of this hub node by
considering its similarity with this topic: wt(h) =
w(h) ? sim(h, t) from which we can see that the
hub node receives a lower weight than before indi-
cating that it is not as important as ordinary concepts
in a topic.
Now we proceed to weight the topic t by taking
into account the frequencies of its concepts and the
coherence between the concepts and their neighbor-
hood in topic t:
w(t) =
n?
i=1
f(ci)
n
?
n?
i=1
n coh(ci, t)
n
(3)
where topic t contains n concepts {c1, c2, ..., cn},
f(c) is the frequency of concept c over current name
observation set, specially, when c is a hub node con-
cept, we will distribute its frequency according to
equation (2), having ft(c) = f(c)sim(c, t). And
n coh(c, t) is the neighborhood coherence of con-
cept c with topic t, defined as:
n coh(c, t) =
?
q?N(c)?t
sr(q, c)
|N(c) ? t|
(4)
where N(c) is the neighboring node set of concept
c.
By incorporating corpus level concept frequen-
cies into topic weighting, discriminative concepts
that are sparse in one document and suppressed by
conventional models can benefit from their corpus
level importance as well as their coherence in related
topics.
3.3 Clustering Person Name Observations
Now the remaining key step is to compute the sim-
ilarity between two name observations. The simi-
larity proposed in GRAPE(Jiang et al2009) mea-
sures two documents by bridge tags (common fea-
tures) shared by two document graphs. Specifically,
Jiang et altilize cohesion to weight a bridge tag in
a document. The more bridge tags two documents
share, the stronger the cohesion of each bridge tag
is, and in turn the more similar the two documents
are.
However, this similarity bears a shortcoming that
the bridge tags shared by the two documents re-
quire an exact match of features, which does not
take any semantic relatedness into consideration. If
two web pages mentioning the same person but have
few features in common, the GRAPE similarity may
not work properly. We, therefore, propose a new
similarity measure combining topic similarity, topic
based connectivity strength and GRAPE?s connec-
tivity strength.
Matching Topics to Person Name Observations
We first describe how to match the generated top-
ics to different name observations. In order to avoid
unreliable estimation, we only match a topic to a
name observation when they share at least one con-
cept. To measure the relatedness between a topic
and a name observation, we formulate this similar-
ity as the weighted average of semantic relatedness
between each concept from one side and its closely
related counterpart from the other side,defined as:
sim(A? B) =
?
a?A
wA(a)? wB(ba)? sr(a, ba)
?
a?A
wA(a)? wB(ba)
(5)
sim(A,B) = (sim(A? B) + sim(B ? A))/2,
where A can be a topic and B a name observation or
vice versa, ba is a concept inB that is most related to
concept a, wA(a) represents the weight of concept a
estimated by the averaged relatedness between a and
other concepts in A.
Person Name Similarity Now we describe the
first component in our proposed measure: topic sim-
ilarity, which is calculated through the common top-
ics shared by the two name observations, o1 and o2:
TSm(o1, o2) =
?
t?T (o1,o2)
sim(o1, t)? sim(o2, t) (6)
?sim(o1 ? t, o2 ? t)? w(t)
where T (o1, o2) contains all common topics of o1
and o2, w(t) is the weight of topic t estimated using
837
equation (3), both sim(oi, t) and sim(o1 ? t, o2 ? t)
measure the similarity between two concept sets and
can be estimated using equation (5). The underly-
ing idea of the equation is, if two name observations
share more and closer common topics, and also these
topics receive higher weights according to the cur-
rent name observation set, then the two observations
should be more related to each other.
Specifically, the factor sim(o1 ? t, o2 ? t) is de-
signed to measure the fine relatedness between o1
and o2 given the topic t. Sometimes, both o1 and
o2 are mapped to t and both close to this topic, but
in fact they depict different aspects of t since some
of our topics are more general thus include several
aspects. The comparison of their intersections will
provide a more detailed view for their similarity.
Inspired by the use of bridge tags in
GRAPE(Jiang et al2009), we propose to capture
the connection strength between concept sets by the
means of our topics. We consider common topics
as the bridge tags and define our topic based con-
nectivity strength between two name observations
as:
TCS(o1, o2) =
1
2
?
t?T (o1,o2)
sim(o1 ? t, o2 ? t)?
(Cohs(o1, t) + Cohs(o2, t)) (7)
Note that we still need sim(o1 ? t, o2 ? t) to capture
the fine differences inside a topic. Cohs(o, t) is a
cohesion measure to capture the relatedness between
non-concept features in o and concept features in t,
defined as:
Cohs(o, t) =
?
c?o?t
w(t)?
?
q?EB(o)
occ(c, q)fo(c)fo(q)
(8)
where EB(o) contains all non-concept features in
o (e.g., non-Wikipedia entities and biographical in-
formation), occ(c, q) is the co-occurring number of
concept c and feature q, fo(q) is the relative fre-
quency of q in observation o. It is easy to find that
a higher cohesion can be achieved by larger overlap
between o and t, higher topic weight and more co-
occurrences of concept features in t and other fea-
tures in o.
The third part is the original connectivity strength
defined in GRAPE(Jiang et al2009): CS(o1, o2),
calculated using plain features without topics (we
omit the details for brevity). Finally, we linearly
combine equation (6), (7) and CS(o1, o2) into the
person name similarity function as:
S(o1, o2)= ?1 ? TSm(o1, o2) + ?2 ? TCS(o1, o2)
+(1? ?1 ? ?2)? CS(o1, o2) (9)
where ?1 and ?2 are optimized during training.
This final similarity function will then be embed-
ded into a normal HAC algorithm to group the web
pages into different namesakes where we compute
the centroid-based distance between clusters(Mann
and Yarowsky, 2003).
4 Experiments
We compare our model with competitive baselines
on three WePS datasets. In the following, we first
describe the experimental setup, and then discuss the
their performances.
4.1 Data
Wikipedia Data Wikipedia offers free copies of
all available data to interested users in their website.
We used the one released in March 6th, 2009 in our
experiments. We identified over 4,000,000 highly
connected concepts in this dump; each concept links
to 10 other concepts in average.
WePS Datasets We used three datasets in our
experiments, WePS1 Training and Testing (Artiles
et al2007), WePS2 Testing (Javier et al2009).
These datasets collected names from three differ-
ent resources including Wikipedia names, program
committee of a computer science conference and US
census. Each name were queried in Yahoo! Search
and top N result pages (100 pages in WePS1 and
150 pages in WePS2) were obtained and manually
labeled.
4.2 Baselines
We compare our model TM with four baseline meth-
ods: (1)VSM: traditional vector space model with
cosine similarity. We use features extracted in Sec-
tion 3.1 and weight them using TFIDF. The docu-
ments are grouped using standard HAC algorithm.
(2)GRAPE(Jiang et al2009): we re-implement the
state-of-the-art system which outperforms any mod-
els that do not use extra knowledge resources re-
ported in WePS1 and WePS2. (3)WS: the Wikipedia
838
Semantic method(Han and Zhao, 2009). This sys-
tem uses Wikipedia to enhance the results of name
disambiguation. (4)SSR: the Structural Semantic re-
latedness model(Han and Zhao, 2010) creates a se-
mantic graph to re-calculate the semantic related-
ness between features, and captures both explicit
semantic relations and implicit structural semantic
knowledge. We also build two variants of TM: TM-
nTW which removes topic weighting to examine
what effect the topic weighting strategy can make
and whether it can provide a person specific evi-
dence and TM-nCP which does not use co-occurring
information to prune the semantic graph to examine
whether the pruning is effective.
4.3 Parameters
There are several parameters to be tuned in our
model. In the SCAN algorithm, we use default pa-
rameters according to (Xu et al2007) with an ex-
ception: the weight ? is tuned exhaustively to be 0.2.
Note that the number of topics are automatically de-
cided by SCAN. The semantic graph pruning thresh-
old is set to 0.27 tuned on a held out set. The
smoothing parameters in equation (9) are: ?1 = 0.3,
?2 = 0.2 which are tuned using cross validation.
Optimization of some parameters will be addressed
in detail in the following subsection. In HAC, all
optimal merging thresholds are selected by applying
leave-one-out cross validation.
4.4 Results and Discussion
We adopt the same evaluation process as (Han and
Zhao, 2009), and evaluating these models using Pu-
rity, Inverse Purity and the F-measure (also used in
WePS Task Artiles et al2007)). The overall perfor-
mance is shown in Table 1, and the best scores are
in boldface.
Let us first look at our model and its variants,
TM-nTW and TM-nCP. By introducing the corpus
level topic weighting scheme, our model improves
in average 1.6% consistently over all datasets. Re-
call that our topic weightings are obtained over the
whole name observation set beyond local context,
this improvement indicates that this corpus level per-
son specific evidences render the person similarity
more reasonably than that of single document. On
the other hand, by pruning the semantic graph, our
model improves averagely 1.3% over TM-nCP. This
Table 1: Web person name disambiguation results on all
three WePS datasets
WePS1 Training
Method P IP FMeasure
VSM 0.86 0.86 0.85
GRAPE 0.93 0.90 0.91
WS 0.88 0.89 0.87
SSR 0.82 0.92 0.85
TM-nTW 0.91 0.89 0.90
TM-nCP 0.92 0.90 0.91
TM 0.93 0.91 0.91
WePS1 Testing
Method P IP FMeasure
VSM 0.79 0.85 0.81
GRAPE 0.93 0.83 0.87
WS 0.88 0.90 0.88
SSR 0.85 0.83 0.84
TM-nTW 0.93 0.85 0.88
TM-nCP 0.92 0.86 0.88
TM 0.94 0.86 0.90
WePS2 Testing
Method P IP FMeasure
VSM 0.82 0.87 0.83
GRAPE 0.88 0.90 0.89
WS 0.85 0.89 0.86
SSR 0.89 0.84 0.86
TM-nTW 0.92 0.87 0.89
TM-nCP 0.93 0.88 0.90
TM 0.93 0.89 0.91
shows that our co-occurrence based pruning strategy
can help render the semantic graph with less noisy
edges, thus generate more reasonable topics.
Generally, our proposed model works best con-
sistently over all three datasets. Our method gains
9.3% improvement on average in three datasets com-
pared with VSM, 1.7% improvement compared to
GRAPE, 3.8% over WS and 6.7% over SSR. We also
performed significance testing on F-measures: the
differences between our model and other models are
significant. We notice there are many noisy or short
web pages which lead to inaccurate concept extrac-
tion, but this cross document evidences, to some ex-
tent, can remedy this. In the Emily Bender exam-
ple, our system correctly groups the odd page, which
contains limited clues, into the nutritionist cluster,
839
while the rest, excluding WS and SSR, failed. Sur-
prisingly, SSR combines both kinds of relations and
implicit structural knowledge, but performs in the
same bulk with VSM in WePS1 training set. We
think the reason may be that some name observation
sets are too small to estimate non-concept related-
ness via random walk. In WePS1 training set, many
names in this dataset contains several namesakes,
each of which corresponds to a few web pages. In
this case, our corpus level weighting scheme and
WS show no advantage over GRAPE which consid-
ers word co-occurrences solely. From the results,
we can also find that there is no clear winner be-
tween GRAPE and WS. The former does not use
Wikipedia relatedness but only includes local rela-
tionship, and performs even slightly better than WS
in WePS2, which indicates that non-Wikipedia con-
cepts are important disambiguation features as well.
4.5 Parameter Optimization
In this subsection, we discuss the optimization of
several parameters in the proposed method. In total
we need to set four parameters. The first one is the
edge pruning threshold during graph construction;
the second one is the weight ? in SCAN algorithm;
the third one and the forth one are the combination
parameters in the final similarity function. We will
address the first two in the following. The last two
combination parameters are tuned by exhaustively
searching the space and omitted here for brevity
First, we configure the pruning threshold. Intu-
itively, larger threshold can prune more unimpor-
tant edges and improve the disambiguation perfor-
mance. However, if the threshold is too large, we
may prune important edges and harm the results.
The F-measure of our method with respect to the
pruning threshold is plotted in Figure 2.
From Figure 2, we can know that in all three
data sets, a pruning threshold of 0.27 will lead to
the best performance. Both increasing and decreas-
ing of this pruning threshold will cause a decline of
the F-Measure, because they will either leave more
noisy light-weighted edges or prune some important
edges.
Secondly, we configure the neighborhood similar-
ity weight. The larger this weight is, the more neigh-
borhood information can influence the similarity be-
tween two nodes in the semantic graph. We plot the
0.22 0.24 0.26 0.28 0.3 0.32 0.34 0.36 0.380.86
0.87
0.88
0.89
0.9
0.91
0.92
Edge Pruning Threshold
FMea
sure
 
 WePS1 TrainingWePS1 TestingWePS2 Testing
Figure 2: The F-Measure v.s. the edge pruning threshold
on three data sets.
0 0.2 0.4 0.6 0.8 10.88
0.885
0.89
0.895
0.9
0.905
0.91
0.915
0.92
0.925
0.93
Neighborhood Weight
FMea
sure
 
 WePS1 TrainingWePS1 TestingWePS2 Testing
Figure 3: The F-Measure v.s. the neighborhood similarity
weight on three data sets.
performance of our method regarding to the neigh-
borhood similarity weight in Figure 3.
From Figure 3, we know that for the WePS 1 Test-
ing and WePS2 Testing data sets, a neighborhood
similarity weight of 0.2 can result in the best perfor-
mance, but for WePS 1 Training set, the weight for
the best performance is 0.6. In fact, when the neigh-
borhood similarity weight varies from 0 to 1, the dif-
ference between the best and worst performance are
less than 0.01, which indicates that neighborhood
similarity is as considerable as semantic relatedness.
840
5 Conclusion and Future Work
In this paper, we explore the feature space in the
web person name disambiguation task and propose
a topic-based model which exploits corpus level
person specific evidences to handle the data spar-
sity challenges, especially the case that limited ev-
idences can be collected from the local context. In
particular, we harvest topics from wikipedia con-
cepts appearing in the name observation set, and
weight a concept based on both the relatedness of
the concept to its corresponding topic and the im-
portance of this topic in the current name observa-
tion set, so that some discriminative but sparse fea-
tures can obtain more reliable weights. Experimen-
tal results show that our weighting strategy does its
job and the proposed model outperforms the-state-
of-the-art systems. Our current work utilizes the
topic information shared in one name observation
set but is incapable to handle sparse name set, which
needs more accurate relation extraction inside the
name observations. Jointly modeling entity link-
ing and person (entity) disambiguation tasks will
be an interesting direction where the two tasks are
closely related and usually need to be considered at
the same time. Investigating the person name dis-
ambiguation task in different web applications will
also be of great importance, e.g., disambiguating a
name in streaming data or during knowledge base
construction. In addition, graphical model, which
has been studied in academic author disambiguation,
may be a good choice to cope with the noises and
non-standard forms in web data.
Acknowledgments
We would like to thank Yidong Chen, Wei Wang
and Tinghua Wang for their useful discussions and
the anonymous reviewers for their helpful comments
which greatly improved the work and the presen-
tation. This work was supported by the National
High Technology Research and Development Pro-
gram of China (Grant No. 2012AA011101), Na-
tional Natural Science Foundation of China (Grant
No.61003009) and Research Fund for the Doc-
toral Program of Higher Education of China (Grant
No.20100001120029).
References
Artiles, J., Gonzalo, J., and Sekine, S. (2007). The
semeval-2007 weps evaluation: establishing a
benchmark for the web people search task. In
SemEval, SemEval ?07, pages 64?69, Strouds-
burg, PA, USA. Association for Computational
Linguistics.
Bagga, A. and Baldwin, B. (1998). Entity-based
cross-document coreferencing using the vector
space model. In ACL, pages 79?85, Strouds-
burg, PA, USA. Association for Computational
Linguistics.
Bunescu, R. C. and Pasca, M. (2006). Using ency-
clopedic knowledge for named entity disambigua-
tion. In EACL. The Association for Computer
Linguistics.
Cucerzan, S. (2007). Large-scale named entity dis-
ambiguation based on wikipedia data. In EMNLP-
CoNLL, pages 708?716. ACL.
David, M. and Ian, H. (2008). An effective, low-cost
measure of semantic relatedness obtained from
wikipedia links. In AAAI, AAAI ?08.
Finkel, J. R., Grenager, T., and Manning, C. (2005).
Incorporating non-local information into informa-
tion extraction systems by gibbs sampling. In
ACL, pages 363?370, Ann Arbor, Michigan. As-
sociation for Computational Linguistics.
Han, X. and Zhao, J. (2009). Named entity dis-
ambiguation by leveraging wikipedia semantic
knowledge. In CIKM, CIKM ?09, pages 215?224,
New York, NY, USA. ACM.
Han, X. and Zhao, J. (2010). Structural semantic
relatedness: a knowledge-based method to named
entity disambiguation. In ACL, ACL ?10, pages
50?59, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Ikeda, M., Ono, S., Sato, I., Yoshida, M., and Naka-
gawa, H. (2009). Person name disambiguation on
the web by two-stage clustering. In WWW.
Iria, J., Xia, L., and Zhang, Z. (2007). Wit: web peo-
ple search disambiguation using random walks. In
SemEval, SemEval ?07, pages 480?483, Strouds-
burg, PA, USA. Association for Computational
Linguistics.
841
Javier, A., Julio, G., and Satoshi, S. (2009). Weps 2
evaluation campaign: Overview of the web peo-
ple search clustering task. In WWW 2009.
Jiang, L., Wang, J., An, N., Wang, S., Zhan, J.,
and Li, L. (2009). Grape: A graph-based frame-
work for disambiguating people appearances in
web search. In ICDM, ICDM ?09, pages 199?208,
Washington, DC, USA. IEEE Computer Society.
Kalashnikov, D. V., Chen, Z., Mehrotra, S., and
Nuray-Turan, R. (2008a). Web people search via
connection analysis. IEEE Trans. on Knowl. and
Data Eng., 20:1550?1565.
Kalashnikov, D. V., Nuray-Turan, R., and Mehrotra,
S. (2008b). Towards breaking the quality curse.: a
web-querying approach to web people search. In
SIGIR, SIGIR ?08, pages 27?34, New York, NY,
USA. ACM.
Mann, G. S. and Yarowsky, D. (2003). Unsuper-
vised personal name disambiguation. In CONLL,
CONLL ?03, pages 33?40, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Mihalcea, R. and Csomai, A. (2007). Wikify!: link-
ing documents to encyclopedic knowledge. In
Proceedings of CIKM?07, pages 233?242.
Niu, C., Li, W., and Srihari, R. K. (2004). Weakly
supervised learning for cross-document person
name disambiguation supported by information
extraction. In ACL, ACL ?04, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Pilz, A. (2010). Entity disambiguation using link
based relations extracted from wikipedia. In
ICML.
Rao, D., Garera, N., and Yarowsky, D. (2007). Jhu1:
an unsupervised approach to person name dis-
ambiguation using web snippets. In SemEval,
SemEval ?07, pages 199?202, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Wu, F. and Weld, D. S. (2008). Automatically re-
fining the wikipedia infobox ontology. In WWW,
WWW ?08, pages 635?644, New York, NY, USA.
ACM.
Xu, X., Yuruk, N., Feng, Z., and Schweiger, T. A. J.
(2007). Scan: a structural clustering algorithm
for networks. In Proceedings of KDD, KDD ?07,
pages 824?833, New York, NY, USA. ACM.
Yiming, L., Zaiqing, N., Taoyuan, C., Ying, G., and
Ji-Rong, W. (2007). Name disambiguation using
web connection. In AAAI.
842
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1912?1923,
October 25-29, 2014, Doha, Qatar.
c
?2014 Association for Computational Linguistics
Joint Inference for Knowledge Base Population
Liwei Chen
1
, Yansong Feng
1?
, Jinghui Mo
1
, Songfang Huang
2
, and Dongyan Zhao
1
1
ICST, Peking University, Beijing, China
2
IBM China Research Lab, Beijing, China
{chenliwei,fengyansong,mojinghui,zhaodongyan}@pku.edu.cn
huangsf@cn.ibm.com
Abstract
Populating Knowledge Base (KB) with
new knowledge facts from reliable text re-
sources usually consists of linking name
mentions to KB entities and identifying
relationship between entity pairs. How-
ever, the task often suffers from errors
propagating from upstream entity linkers
to downstream relation extractors. In this
paper, we propose a novel joint infer-
ence framework to allow interactions be-
tween the two subtasks and find an opti-
mal assignment by addressing the coher-
ence among preliminary local predictions:
whether the types of entities meet the ex-
pectations of relations explicitly or implic-
itly, and whether the local predictions are
globally compatible. We further measure
the confidence of the extracted triples by
looking at the details of the complete ex-
traction process. Experiments show that
the proposed framework can significantly
reduce the error propagations thus obtain
more reliable facts, and outperforms com-
petitive baselines with state-of-the-art re-
lation extraction models.
1 Introduction
Recent advances in natural language processing
have made it possible to construct structured KBs
from online encyclopedia resources, at an un-
precedented scale and much more efficiently than
traditional manual edit. However, in those KBs,
entities which are popular to the community usu-
ally contain more knowledge facts, e.g., the bas-
ketball player LeBron James, the actor Nicholas
Cage, etc., while most other entities often have
fewer facts. On the other hand, knowledge facts
should be updated as the development of entities,
such as changes in the cabinet, a marriage event,
or an acquisition between two companies, etc.
In order to address the above issues, we could
consult populating existing KBs from reliable text
resources, e.g., newswire, which usually involves
enriching KBs with new entities and populating
KBs with new knowledge facts, in the form of
<Entity, Relation, Entity> triple. In this paper, we
will focus on the latter, identifying relationship be-
tween two existing KB entities. This task can be
intuitively considered in a pipeline paradigm, that
is, name mentions in the texts are first linked to
entities in the KB (entity linking, EL), and then
the relationship between them are identified (re-
lation extraction, RE). It is worth mentioning that
the first task EL is different from the task of named
entity recognition (NER) in traditional informa-
tion extraction (IE) tasks, where NER recognizes
and classifies the entity mentions (to several pre-
defined types) in the texts, but EL focuses on link-
ing the mentions to their corresponding entities in
the KB. Such pipeline systems often suffer from
errors propagating from upstream to downstream,
since only the local best results are selected to the
next step. One idea to solve the problem is to allow
interactions among the local predictions of both
subtasks and jointly select an optimal assignment
to eliminate possible errors in the pipeline.
Let us first look at an example. Suppose we are
extracting knowledge facts from two sentences in
Figure 1: in sentence [1], if we are more confi-
dent to extract the relation fb:org.headquarters
1
,
we will be then prompted to select Bryant Univer-
sity, which indeed favors the RE prediction that
requires an organization to be its subject. On
the other side, if we are sure to link to Kobe
Bryant in sentence [2], we will probably select
fb:pro athlete.teams, whose subject position ex-
pects an athlete, e.g., an NBA player. It is not dif-
ficult to see that the argument type expectations of
relations can encourage the two subtasks interact
with each other and select coherent predictions for
1
The prefix fb means the relations are defined in Freebase.
1912
Sentence 1: ? [Bryant] is a private university located in [Smithfield]. ?
Sentence 2 : ? Shaq and [Bryant] led the [Lakers] to three consecutive championships ?
Bryant, Illinois
fb:people.person.place_of_birth
fb:sports.pro_athlete.teams
fb:org.org.headquarters
fb:business.board_member.leader_of
Bryant University
...Kobe Bryant
Kobe Bryant
Bryant University
Bryant, Illinois
...
Smithfield, Rhode Island 
Smithfield, Illinois
...
Los Angeles Lakers
Laguna Lakers
...
Figure 1: Two example sentences from which we
can harvest knowledge facts.
both of them. In KBs with well-defined schemas,
such as Freebase, type requirements can be col-
lected and utilized explicitly (Yao et al., 2010).
However, in other KBs with less reliable or even
no schemas, it is more appropriate to implicitly
capture the type expectations for a given relation
(Riedel et al., 2013).
Furthermore, previous RE approaches usually
process each triple individually, which ignores
whether those local predictions are compatible
with each other. For example, suppose the local
predictions of the two sentences above are <Kobe
Bryant, fb:org.headquarters, Smithfield, Rhode Is-
land> and <Kobe Bryant, fb:pro athlete.teams,
Los Angeles Lakers>, respectively, which, in fact,
disagree with each other with respect to the KB,
since, in most cases, these two relations cannot
share subjects. Now we can see that either the re-
lation predictions or the EL results for ?Bryant?
are incorrect. Those disagreements provide us an
effective way to remove the possible incorrect pre-
dictions that cause the incompatibilities.
On the other hand, the automatically extracted
knowledge facts inevitably contain errors, espe-
cially for those triples collected from open do-
main. Extractions with confidence scores will be
more than useful for users to make proper deci-
sions according to their requirements, such as trad-
ing recall for precision, or supporting approximate
queries.
In this paper, we propose a joint framework to
populate an existing KB with new knowledge facts
extracted from reliable text resources. The joint
framework is designed to address the error propa-
gation issue in a pipeline system, where subtasks
are optimized in isolation and locally. We find an
optimal configuration from top k results of both
subtasks, which maximizes the scores of each step,
fulfills the argument type expectations of relations,
which can be captured explicitly or implicitly, in
the KB, and avoids globally incoherent predic-
tions. We formulate this optimization problem in
an Integer Linear Program (ILP) framework, and
further adopt a logistic regression model to mea-
sure the reliability of the whole process, and assign
confidences to all extracted triples to facilitate fur-
ther applications. The experiments on a real-world
case study show that our framework can elimi-
nate error propagations in the pipeline systems by
taking relations? argument type expectations and
global compatibilities into account, thus outper-
forms the pipeline approaches based on state-of-
the-art relation extractors by a large margin. Fur-
thermore, we investigate both explicit and implicit
type clues for relations, and provide suggestions
about which to choose according to the character-
istics of existing KBs. Additionally, our proposed
confidence estimations can help to achieve a pre-
cision of over 85% for a considerable amount of
high quality extractions.
In the rest of the paper, we first review related
work and then define the knowledge base popula-
tion task that we will address in this paper. Next
we detail the proposed framework and present our
experiments and results. Finally, we conclude this
paper with future directions.
2 Related Work
Knowledge base population (KBP), the task of ex-
tending existing KBs with entities and relations,
has been studied in the TAC-KBP evaluations (Ji et
al., 2011), containing three tasks. The entity link-
ing task links entity mentions to existing KB nodes
and creates new nodes for the entities absent in the
current KBs, which can be considered as a kind
of entity population (Dredze et al., 2010; Tamang
et al., 2012; Cassidy et al., 2011). The slot-filling
task populates new relations to the KB (Tamang
et al., 2012; Roth et al., 2012; Liu and Zhao,
2012), but the relations are limited to a predefined
sets of attributes according to the types of enti-
ties. In contrast, our RE models only require min-
imal supervision and do not need well-annotated
training data. Our framework is therefore easy to
adapt to new scenarios and suits real-world appli-
cations. The cold-start task aims at constructing a
KB from scratch in a slot-filling style (Sun et al.,
2012; Monahan and Carpenter, 2012).
Entity linking is a crucial part in many KB re-
1913
lated tasks. Many EL models explore local con-
texts of entity mentions to measure the similarity
between mentions and candidate entities (Han et
al., 2011; Han and Sun, 2011; Ratinov et al., 2011;
Cheng and Roth, 2013). Some methods further ex-
ploit global coherence among candidate entities in
the same document by assuming that these enti-
ties should be closely related (Han et al., 2011;
Ratinov et al., 2011; Sen, 2012; Cheng and Roth,
2013). There are also some approaches regarding
entity linking as a ranking task (Zhou et al., 2010;
Chen and Ji, 2011). Lin et al. (2012) propose an
approach to detect and type entities that are cur-
rently not in the KB.
Note that the EL task in KBP is different from
the name entity mention extraction task, mainly
in the ACE task style, which mainly identifies the
boundaries and types of entity mentions and does
not explicitly link entity mentions into a KB (ACE,
2004; Florian et al., 2006; Florian et al., 2010; Li
and Ji, 2014), thus are different from our work.
Meanwhile, relation extraction has also been
studied extensively in recent years, ranging from
supervised learning methods (ACE, 2004; Zhao
and Grishman, 2005; Li and Ji, 2014) to unsuper-
vised open extractions (Fader et al., 2011; Carl-
son et al., 2010). There are also models, with dis-
tant supervision (DS), utilizing reliable texts re-
sources and existing KBs to predict relations for a
large amount of texts (Mintz et al., 2009; Riedel et
al., 2010; Hoffmann et al., 2011; Surdeanu et al.,
2012). These distantly supervised models can ex-
tract relations from texts in open domain, and do
not need much human involvement. Hence, DS is
more suitable for our task compared to other tradi-
tional RE approaches.
Joint inference over multiple local models has
been applied to many NLP tasks. Our task is dif-
ferent from the traditional joint IE works based in
the ACE framework (Singh et al., 2013; Li and
Ji, 2014; Kate and Mooney, 2010), which jointly
extract and/or classify named entity mentions to
several predefined types in a sentence and iden-
tify in a sentence level which relation this specific
sentence describes (between a pair of entity men-
tions in this sentence). Li and Ji (2014) follow
the ACE task definitions and present a neat incre-
mental joint framework to simultaneously extract
entity mentions and relations by structure percep-
tron. In contrast, we link entity mentions from a
text corpus to their corresponding entities in an ex-
isting KB and identify the relations between pairs
of entities based on that text corpus. Choi et al.
(2006) jointly extracts the expressions and sources
of opinion as well as the linking relations (i.e., a
source entity expresses an opinion expression) be-
tween them, while we focus on jointly modeling
EL and RE in open domain, which is a different
and challenging task.
Since the automatically extracted knowledge
facts inevitably contain errors, many approaches
manage to assign confidences for those extracted
facts (Fader et al., 2011; Wick et al., 2013). Wick
et al. (2013) also point out that confidence estima-
tion should be a crucial part in the automated KB
constructions and will play a key role for the wide
applications of automatically built KBs. We thus
propose to model the reliability of the complete
extraction process and take the argument type ex-
pectations of the relation, coherence with other
predictions and the triples in the existing KB into
account for each populated triple.
3 Task definition
We formalize our task as follows. Given a set
of entities sampled from an existing KB, E =
{e
1
, e
2
, ..., e
|E|
}, a set of canonicalized relations
from the same KB, R = {r
1
, r
2
, ..., r
|R|
}, a set
of sentences extracted from news corpus, SN =
{sn
1
, sn
2
, ..., sn
|SN |
}, each contains two men-
tions m
1
and m
2
whose candidate entities belong
to E, a set of text fragments T = {t
1
, t
2
, ..., t
|T |
},
where t
i
contains its corresponding target sentence
sn
i
and acts as its context. Our task is to link those
mentions to entities in the given KB, identify the
relationship between entity pairs and populate new
knowledge facts into the KB.
4 The Framework
We propose to perform joint inference over sub-
tasks involved. For each sentence with two entity
mentions, we first employ a preliminary EL model
and RE model to obtain entity candidates and pos-
sible relation candidates between the two men-
tions, respectively. Our joint inference framework
will then find an optimal assignment by taking the
preliminary prediction scores, the argument type
expectations of relations and the global compati-
bilities among the predictions into account. In the
task of KBP, an entity pair may appear in multiple
sentences as different relation instances, and the
crucial point is whether we can identify all the cor-
1914
Sentence 1: ? [Bryant] is a private university located in [Smithfield]. ?
Sentence 2: ? Shaq and [Bryant] led the [Lakers] to three consecutive 
championships from 20 0 0  to 20 0 2. ?
Bryant, Illinois
fb:people.place_of_birth
fb:pro_athlete.teams
fb:org.headquarters
fb:business.leader_of
Bryant University
...
Kobe Bryant
Kobe Bryant
Bryant University
Bryant, Illinois
...
Smithfield, Rhode Island 
Smithfield, Illinois
...
Los Angeles Lakers
Laguna Lakers
...
D isagreement!
Figure 2: An example of our joint inference framework. The top and bottom are two example sentences
with entity mentions in the square brackets, candidate entities in the white boxes, candidate relations in
the grey boxes, and the solid lines with arrows between relations and entities represent their preference
scores, with thickness indicating the preferences? value.
rect relations for an entity pair. Thus, after finding
an optimal sentence-level assignment, we aggre-
gate those local predictions by ORing them into
the entity pair level. Finally, we employ a regres-
sion model to capture the reliability of the com-
plete extraction process.
4.1 Preliminary Models
Entity Linking The preliminary EL model can
be any approach which outputs a score for each
entity candidate. Note that a recall-oriented model
will be more than welcome, since we expect to in-
troduce more potentially correct local predictions
into the inference step. In this paper, we adopt
an unsupervised approach in (Han et al., 2011)
to avoid preparing training data. Note the chal-
lenging NIL problem, i.e., identifying which en-
tity mentions do not have corresponding entities
in the KB (labeled as NIL) and clustering those
mentions, will be our future work. For each men-
tion we retain the entities with top p scores for the
succeeding inference step.
Relation Extraction The choice of RE model is
also broad. Any sentence level extractor whose
results are easy to be aggregated to entity pair
level can be utilized here (again, a recall-oriented
version will be welcome), such as Mintz++ men-
tioned in (Surdeanu et al., 2012), which we adapt
into a Maximum Entropy version. We also include
a special label, NA, to represent the case where
there is no predefined relationship between an en-
tity pair. For each sentence, we retain the relations
with top q scores for the inference step, and we
also call that this sentence supports those candi-
date relations. As for the features of RE models,
we use the same features (lexical features and syn-
tactic features) with the previous works (Chen et
al., 2014; Mintz et al., 2009; Riedel et al., 2010;
Hoffmann et al., 2011).
4.2 Relations? Expectations for Argument
Types
In most KBs? schemas, canonicalized relations are
designed to expect specific types of entities to be
their arguments. For example, in Figure 2, it is
more likely that an entity Kobe Bryant takes the
subject position of a relation fb:pro athlete.teams,
but it is unlikely for this entity to take the subject
position of a relation fb:org.headquarters. Making
use of these type requirements can encourage the
framework to select relation and entity candidates
which are coherent with each other, and discard
incoherent choices.
In order to obtain the preference scores between
1915
the entities in E and the relations in R, we gener-
ate two matrices with |E| rows and |R| columns,
whose elements sp
ij
indicates the preference score
of entity i and relation j. The matrix S
subj
is for
relations and their subjects, and the matrix S
obj
is
for relations and their objects. We initialize the
two matrices using the KB as follows: for entity i
and relation j, if relation j takes entity i as its sub-
ject/object in the KB, the element at the position
(i, j) of the corresponding matrix will be 1, oth-
erwise it will be 0. Note that in our experiments,
we do not count the triples that are evaluated in the
testing data, to build the matrices. Now the prob-
lem is how we can obtain the unknown elements
in the matrices.
Explicit Type Information Intuitively, we
should examine whether the explicit types of the
entities fulfill the expectations of relations in the
KB. For each unknown element S
subj
(i, j), we
first obtain the type of entity i, which is collected
from the lowest level of the KB?s type hierarchy,
and examine whether there is another entity
with the same type taking the subject position
of relation j in the initial matrix. If such an
entity exists, S
subj
(i, j) = 1, otherwise 0. For
example, for the subject Jay Fletcher Vincent and
the relation fb:pro athlete.teams, we first obtain
the subject?s type basketball player, and then we
go through the initial matrix and find another
entity Kobe Bryant with the same type taking
the subject position of fb:pro athlete.teams,
indicating that Jay Fletcher Vincent may take the
relation fb:pro athlete.teams. The matrix S
obj
is
processed in the same way.
Implicit Type Expectations In practice, few
KBs have well-defined schemas. In order to make
our framework more flexible, we need to come up
with an approach to implicitly capture the rela-
tions? type expectations, which will also be rep-
resented as preference scores.
Inspired by Riedel et al. (2013) who use a ma-
trix factorization approach to capture the associa-
tion between textual patterns, relations and entities
based on large text corpora, we adopt a collabora-
tive filtering (CF) method to compute the prefer-
ence scores between entities and relations based
on the statistics obtained from an existing KB.
In CF, the preferences between customers and
items are calculated via matrix factorization over
the initial customer-item matrix. In our frame-
work, we compute the preference scores between
entities and relations via the same approach over
the two initialized matrices S
subj
and S
obj
, re-
sulting in two entity-relation matrices with esti-
mated preference values. We use ALS-WR (Zhou
et al., 2008) to process the matrices and compute
the preference of a relation taking an entity as its
subject and object, respectively. We normalize the
preference scores of each entity using their means
? and standard deviations ?.
4.3 Compatibilities among Predicted Triples
The second aspect we investigate is whether the
extracted triples are compatible with respect to all
other knowledge facts. For example, according to
the KB, the two relations fb:org.headquarters and
fb:pro athlete.teams in Figure 2 cannot share the
same entity as their subjects. So if such sharing
happens, that will indicate either the predictions
of the relations or the entities are incorrect. The
clues can be roughly grouped into three categories,
namely whether two relations can share the same
subjects, whether two relations can share the same
objects, and whether one relation?s subject can be
the other relation?s object.
Global compatibilities among local predictions
have been investigated by several joint models (Li
et al., 2011; Li and Ji, 2014; Chen et al., 2014) to
eliminate the errors propagating in a pipeline sys-
tem. Specifically, Chen et al. (2014) utilized the
clues with respect to the compatibilities of rela-
tions in the task of relation extraction. Following
(Li et al., 2011; Chen et al., 2014), we extend the
idea of global compatibilities to the entity and re-
lation predictions during knowledge base popula-
tion. We examine the pointwise mutual informa-
tion (PMI) between the argument sets of two re-
lations to collect such clues. For example, if we
want to learn whether two relations can share the
same subject, we first collect the subject sets of
both relations from the KB, and then compute the
PMI value between them. If the value is lower
than a certain threshold (set to -3 in this paper), the
clue that the two relations cannot share the same
subject is added. These clues can be easily inte-
grated into an optimization framework in the form
of constraints.
4.4 Integer Linear Program Formulation
Now we describe how we aggregate the above
components, and formulate the joint inference
problem into an ILP framework. For each candi-
1916
date entity e of mention m in text fragment t, we
define a boolean decision variable d
m,e
t
, which de-
notes whether this entity is selected into the final
configuration or not. Similarly, for each candidate
relation r of fragment t, we define a boolean de-
cision variable d
r
t
. In order to introduce the pref-
erence scores into the model, we also need a deci-
sion variable d
r,m,e
t
, which denotes whether both
relation r and candidate entity e of mention m are
selected in t.
We use s
t,m,e
el
to represent the score of mention
m in t disambiguated to entity e, which is output
by the EL model, s
t,r
re
representing the score of re-
lation r assigned to t, which is output by the RE
model, s
r,e
p
the explicit/implicit preference score
between relation r and entity e.
Our goal is to find the best assignment to the
variables d
r
t
and d
m,e
t
, such that it maximizes the
overall scores of the two subtasks and the co-
herence among the preliminary predictions, while
satisfying the constraints between the predicted
triples as well. Our objective function can be writ-
ten as:
max el? conf
ent
+ re? conf
rel
+ sp? coh
e?r
(1)
where el, re and sp are three weighting parameters
tuned on development set. conf
ent
is the overall
score of entity linking:
conf
ent
=
?
t
?
m?M(t)
?
e?C
e
(m)
s
t,m,e
el
d
m,e
t
(2)
where M(t) is the set of mentions in t, C
e
(m) is
the candidate entity set of the mention m. conf
rel
represents the overall score of relation extraction:
conf
rel
=
?
t
?
r?C
r
(t)
s
t,r
re
d
r
t
(3)
where C
r
(t) is the set of candidate relations in t.
coh
e?r
is the coherence between the candidate re-
lations and entities in the framework:
coh
e?r
=
?
t
?
r?C
r
(t)
?
m?M(t)
?
e?C
e
(m)
s
r,e
p
d
r,m,e
t
(4)
Now we describe the constraints used in our ILP
problem. The first kind of constraints is intro-
duced to ensure that each mention should be dis-
ambiguated to only one entity:
?t,?m ?M(t),
?
e?C
e
(m)
d
m,e
t
? 1 (5)
The second type of constraints ensure that each en-
tity mention pair in one sentence can only take one
relation label:
?t,
?
r?C
r
(t)
d
t
r
? 1 (6)
The third is introduced to ensure the decision vari-
able d
r,m,e
t
equals 1 if and only if both the corre-
sponding variables d
r
t
and d
m,e
t
equal 1.
?t,?r ? C
r
(t), ?m ?M(t),?e ? C
e
(m)
d
r,m,e
t
? d
r
t
(7)
d
r,m,e
t
? d
m,e
t
(8)
d
r
t
+ d
m,e
t
? d
r,m,e
t
+ 1 (9)
As for the compatibility constraints, we need to
introduce another type of boolean decision vari-
ables. If a mention m
1
in t
1
and another mention
m
2
in t
2
share an entity candidate e, we add a vari-
able y for this mention pair, which equals 1 if and
only if both d
m
1
,e
t
1
and d
m
2
,e
t
2
equal 1. So we add
the following constraints for each mention pairm
1
and m
2
satisfies the previous condition:
y ? d
m
1
,e
t
1
(10)
y ? d
m
2
,e
t
2
(11)
d
m
1
,e
t
1
+ d
m
2
,e
t
2
? y + 1 (12)
Then we further add the following constraints for
each mention pair to avoid incompatible predic-
tions:
?r
1
? C
r
(t
1
), r
2
? C
r
(t
2
)
If (r
1
, r
2
) ? C
sr
, p(m
1
) = subj, p(m
2
) = subj
d
r
1
t
1
+ d
r
2
t
2
+ y ? 2 (13)
If (r
1
, r
2
) ? C
ro
, p(m
1
) = obj, p(m
2
) = obj
d
r
1
t
1
+ d
r
2
t
2
+ y ? 2 (14)
If (r
1
, r
2
) ? C
sro
, p(m
1
) = obj, p(m
2
) = subj
d
r
1
t
1
+ d
r
2
t
2
+ y ? 2 (15)
where p(m) returns the position of mention m, ei-
ther subj (subject) or obj (object). C
sr
is the pairs
of relations which cannot share the same subject,
C
ro
is the pairs of relations which cannot share the
same object, C
sro
is the pairs of relations in which
one relation?s subject cannot be the other one?s ob-
ject.
We use IBM ILOG Cplex
2
to solve the above
ILP problem.
2
http://www.cplex.com
1917
Table 1: The features used to calculate the confi-
dence scores.
Type Feature
Real The RE score of the relation.
Real The EL score of the subject.
Real The EL score of the object.
Real
The preference score between the relation
and the subject.
Real
The preference score between the relation
and the object.
Real
The ratio of the highest and the second highest
relation score in this entity pair.
Real
The ratio of the current relation score and the
maximum relation score in this entity pair.
Real
The ratio of the number of sentences supporting
the current relation and the total number
of sentences in this entity pair.
Real
Whether the extracted triple is coherent with the KB
according to the constraints in Section 4.3.
4.5 Confidence Estimation for Extracted
Triples
The automatically extracted triples inevitably con-
tain errors and are often considered as with high
recall but low precision. Since our aim is to pop-
ulate the extracted triples into an existing KB,
which requires highly reliable knowledge facts,
we need a measure of confidence for those ex-
tracted triples, so that others can properly utilize
them.
Here, we use a logistic regression model to mea-
sure the reliability of the process, how the entities
are disambiguated, how the relationships are iden-
tified, and whether those predictions are compat-
ible. The features we used are listed in Table 1,
which are all efficiently computable and indepen-
dent from specific relations or entities. We manu-
ally annotate 1000 triples as correct or incorrect to
prepare the training data.
5 Experiments
We evaluate the proposed framework in a real-
world scenario: given a set of news texts with en-
tity mentions and a KB, a model should find more
and accurate new knowledge facts between pairs
of those entities.
5.1 Dataset
We use New York Times dataset from 2005 to
2007 as the text corpus, and Freebase as the KB.
We divide the corpus into two equal parts, one for
creating training data for the RE models using the
distant supervision strategy (we do not need train-
ing data for EL), and the other as the testing data.
For the convenience of experimentation, we ran-
domly sample a subset of entities for testing. We
first collect all sentences containing two mentions
which may refer to the sampled entities, and prune
them according to: (1)there should be no more
than 10 words between the two mentions; (2)the
prior probability of the mention referring to the
target entity is higher than a threshold (set to 0.1
in this paper), which is set to filter the impossi-
ble mappings; (3)the mention pairs should not be-
long to different clauses. The resulting test set is
split into 10 parts and a development set, each with
3500 entity pairs roughly, which leads to averagely
200,000 variables and 900,000 constraints per split
and may take 1 hour for Cplex to solve. Note that
we do not count the triples that will be evaluated
in the testing data when we learn the preferences
and the clues from the KB.
5.2 Experimental Setup
We compare our framework with three baselines.
The first one, ME-pl, is the pipeline system con-
structed by the entity linker in (Han et al., 2011)
and the MaxEnt version of Mintz++ extractor
mentioned in (Surdeanu et al., 2012). The sec-
ond and third baselines are the pipeline systems
constructed by the same linker and two state-of-
the-art DS approaches, MultiR (Hoffmann et al.,
2011) and MIML-RE (Surdeanu et al., 2012), re-
spectively. They are referred to as MultiR-pl and
MIML-pl in the rest of this paper.
We also implement several variants of our
framework to investigate the following two com-
ponents in our framework: whether to use ex-
plicit (E) or implicit (I) argument type expecta-
tions, whether to take global (G) compatibilities
into account, resulting in four variants: ME-JE,
ME-JI, ME-JEG, ME-JIG.
We tune the parameters in the objective func-
tion on the development set to be re = 1, el = 4,
sp = 1. The numbers of preliminary results re-
tained to the inference step are set to p = 2, q = 3.
Three metrics used in our experiments include:
(1)the precision of extracted triples, which is the
ratio of the number of correct triples and the num-
ber of total extracted triples; (2)the number of cor-
rect triples (NoC); (3)the number of correct triples
in the results ranked in top n. The third metric
is crucial for KBP, since most users are only in-
terested in the knowledge facts with high confi-
dences. We compare the extracted triples against
1918
Table 2: The results of our joint frameworks and
the three baselines.
Approach Precision NoC Top 50 Top 100
ME-pl 28.7? 0.8 725? 12 38? 2 75? 4
MultiR-pl 31.0? 0.8 647? 15 39? 2 71? 3
MIML-pl 33.2? 0.6 608? 16 40? 3 74? 5
ME-JE 32.8? 0.7 768? 10 46? 2 90? 3
ME-JEG 34.2? 0.5 757? 8 46? 2 90? 3
ME-JI 34.5? 1.0 784? 9 43? 3 88? 3
ME-JIG 35.7? 1.0 772? 8 43? 3 88? 4
Freebase to compute the precision, which may un-
derestimate the performance since Freebase is in-
complete. Since we do not have exact annotations
for the EL, it is difficult to calculate the exact re-
call. We therefore use NoC instead. We evalu-
ate our framework on the 10 subsets of the testing
dataset and compute their means and standard de-
viations.
5.3 Overall Performance
We are interested to find out: (a)whether the task
benefits from the joint inference i.e., can we col-
lect more and correct facts? Or with a higher pre-
cision? (b) whether the argument type expecta-
tions (explicit and implicit) and global compati-
bility do their jobs as we expected? And, how do
we choose from these components ? (c)whether
the framework can work with other RE models?
(d)whether we can find a suitable approach to
measure the confidence or uncertainty during the
extraction so that users or other applications can
better utilize the extracted KB facts?
Let us first look at the performance of the
baselines and our framework in Table 2 for an
overview. Comparing the three pipeline sys-
tems, we can discover that using the same en-
tity linker, MIML-pl performs the best in precision
with slightly fewer correct triples, while ME-pl
performs the worst. It is not surprising, ME-pl, as
a strong and high-recall baseline, outputs the most
correct triples. As for the results with high confi-
dences, MultiR-pl outputs more correct triples in
the top 50 results than ME-pl, and MIML-pl per-
forms better or comparable than ME-pl in top n
results.
After performing the joint inference, ME-JE
improves ME-pl with 4.1% in precision and 43
more correct triples averagely, and results in bet-
ter performance in top n results. By taking global
compatibilities into consideration, ME-JEG fur-
ther improve the precision to 34.2% in average
with slightly fewer correct triples, indicating that
0
100
200
300
400
500
0.40.50.60.70.80.91
Numb
er of C
orrect
 Triple
s
Precision
 
 
ME?p
l
MultiR
?pl
MIML
?pl
ME?J
EG
ME?J
IG
Figure 3: The numbers of correct triples v.s. the
precisions for different approaches.
both argument type expectations and global com-
patibilities are useful in improving the perfor-
mance: argument type information can help to
select the correct and coherent predictions from
the candidates EL and RE outputs, while global
compatibilities can further prune incorrect triples
that cause disagreements, although a few correct
ones may be incorrectly eliminated. We can also
observe that ME-JIG performs even higher than
ME-JEG in overall precision, but ME-JEG col-
lects more correct triples than ME-JIG in the top
n predictions, showing that explicit type expec-
tations with more accurate type information may
perform better in high confidence results.
Furthermore, even though MultiR-pl and
MIML-pl are based on state-of-the-art RE ap-
proaches, our model (for example, ME-JIG) can
still outperform them in terms of all metrics, with
4.7% higher in precision than MultiR-pl, 2.5%
higher than MIML-pl. Our model can extract 125
more correct triples than MultiR-pl, 164 more
than MIML-pl, and perform better in top n results
as well.
In previous RE tasks, Precision-Recall curves
are mostly used to evaluate the systems? perfor-
mances. In our task, since it is difficult to calculate
the recall exactly, we use the number of correct
triples instead, and plot curves of Precision-NoC
to show the performance of the competitors and
our approaches in more detail. For each value of
NoC, the precision is the average of the ten splits
of the testing dataset.
As shown in Figure 3, our approaches (ME-JEG
and ME-JIG) obtain higher precisions on each
NoC value, and the curves are much smoother than
1919
Table 3: The results of our joint frameworks with
MultiR sentence extractor.
Approach Precision NoC Top 50 Top 100
MultiR-pl 31.0? 0.8 647? 15 39? 2 71? 3
MultiR-JEG 36.9? 0.8 687? 15 46? 2 88? 3
MultiR-JIG 38.5? 0.9 700? 15 45? 2 88? 3
the pipeline systems, indicating that our frame-
work is more suitable for harvesting high quality
knowledge facts. Comparing the two kinds of type
clues, we can see that explicit ones perform better
when the confidence control is high and the num-
ber of correct triples is small, and then the two are
comparable. Since the precision of the triples with
high confidences is crucial for the task of KBP,
we still suggest choosing the explicit ones when
there is a well-defined schema available in the KB,
although implicit type expectations can result in
higher overall precision.
5.4 Adapting MultiR Sentence Extractor into
the Framework
The preliminary relation extractor of our frame-
work is not limited to the MaxEnt
3
extractor. It
can be any sentence level recall-oriented relation
extractors. To further investigate the generaliza-
tion of our joint inference framework, we also
try to fit other sentence level relation extractors
into the framework. Considering that MIML-RE
does not output sentence-level results, we only
adapt MultiR, with both global compatibilities
and explicit/implicit type expectations, named as
MultiR-JEG and MultiR-JIG, respectively. Since
the scores output by the original MultiR are un-
normalized, which are difficult to directly apply to
our framework, we normalize their scores and re-
tune the framework?s parameters accordingly. The
parameters are set to re = 1, el = 32, sp = 16.
As seen in Table 3, MultiR-JEG helps MultiR
obtain about 40 more correct triples in average,
and achieves 5.9% higher in precision, as well
as significant improvements in top n correct pre-
dictions. As for MultiR-JIG, the improvements
are 7.5% in precision and 53 in number of cor-
rect triples. In terms of top n results, the explicit
and implicit type expectations perform compara-
ble. We also observe that our framework improves
MultiR as much as it does to MaxEnt, indicating
our joint framework can generalize well in differ-
ent RE models.
3
http://homepages.inf.ed.ac.uk/
lzhang10/maxent_toolkit.html
01
002
003
004
005
00
0.40.60.81
Numbe
r of Co
rrect T
riples
Precision
 
 
MultiR
?pl
MultiR
?JEG
MultiR
?JIG
Figure 4: The numbers of correct triples v.s. the
precisions for approaches with MultiR extractor.
0.5
0.6
0.7
0.8
0.9
1
0.30.40.50.60.70.80.91
Confid
ence 
Thres
hold
Precision
 
 
ME?p
l
MultiR
?pl
ME?J
IG
MultiR
?JIG
MIML
?pl
ME?J
EG
MultiR
?JEG
Figure 5: The precisions of different models un-
der different confidence thresholds. The error bars
represents the standard deviations of the results.
We further plot Precision-NoC curves for
MultiR-JEG and MultiR-JIG in Figure 4, show-
ing that our framework can result in better perfor-
mance and smoother curves with MultiR extractor.
It is interesting to see that with MultiR extractor,
the two kinds of expectations perform comparably.
5.5 Results with Confidence Estimations
Now, we will investigate the results from another
perspective with the help of confidence estima-
tions. We calculate the precisions of the competi-
tors and our approaches on different confidence
thresholds from 0.5 to 1. The results are summa-
rized in Figure 5. Note that the results across dif-
ferent approaches are not directly comparable, we
put them in the same figure only to save space.
In Figure 5, intuitively, as the confidence thresh-
old goes up, the extraction precisions should
increase, indicating triples with higher confi-
dences are more likely to be correct. However,
1920
lower thresholds tend to result in estimations with
smaller standard derivations due to those preci-
sions are estimated over much more triples than
those with higher thresholds, which means the ran-
domness will be smaller.
On the other hand, our joint frameworks pro-
vide more evidences that can be used to well cap-
ture the reliability of an extraction. For example,
the precisions of Multir-JIG and ME-JIG both stay
around 85% when the confidence is higher than
0.85, with about 120 correct triples, indicating that
by setting a proper threshold, we can obtain con-
siderable amount of high quality knowledge facts
at an acceptable precision, which is crucial for
KBP. However, we cannot harvest such amount of
high quality knowledge facts from the other three
pipeline systems.
6 Conclusions
In this paper, we propose a joint framework for the
task of populating KBs with new knowledge facts,
which performs joint inference on two subtasks,
maximizes their preliminary scores, fulfills the
type expectations of relations and avoids global
incompatibilities with respect to all local predic-
tions to find an optimal assignment. Experimen-
tal results show that our framework can signifi-
cantly eliminate the error propagations in pipeline
systems and outperforms competitive pipeline sys-
tems with state-of-the-art RE models. Regard-
ing the explicit argument type expectations and
the implicit ones, the latter can result in a higher
overall precision, while the former performs bet-
ter in acquiring high quality knowledge facts with
higher confidence control, indicating that if the
KB has a well-defined schema we can use explicit
type requirements for the KBP task, and if not,
our model can still perform well by mining the
implicit ones. Our framework can also generalize
well with other preliminary RE models. Further-
more, we assign extraction confidences to all ex-
tracted facts to facilitate further applications. By
setting a suitable threshold, our framework can
populate high quality reliable knowledge facts to
existing KBs.
For future work, we will address the NIL is-
sue of EL where we currently assume all entities
should be linked to a KB. It would be also inter-
esting to jointly model the two subtasks through
structured learning, instead of joint inference only.
Currently we only use the coherence of extracted
triples and the KB to estimate confidences, which
would be nice to directly model the issue in a joint
model.
Acknowledgments
We would like to thank Heng Ji, Kun Xu, Dong
Wang and Junyang Rao for their helpful discus-
sions and the anonymous reviewers for their in-
sightful comments that improved the work consid-
erably. This work was supported by the National
High Technology R&D Program of China (Grant
No. 2012AA011101, 2014AA015102), National
Natural Science Foundation of China (Grant No.
61272344, 61202233, 61370055) and the joint
project with IBM Research. Any correspondence
please refer to Yansong Feng.
References
ACE. 2004. The automatic content extraction projects.
http://projects.ldc.upenn.edu/ace.
Andrew Carlson, Justin Betteridge, Byran Kisiel, Burr
Settles, Estevam Hruschka Jr., and Tom Mitchell.
2010. Toward an architecture for never-ending lan-
guage learning. In Proceedings of the Conference
on Artificial Intelligence (AAAI), pages 1306?1313.
AAAI Press.
Taylor Cassidy, Zheng Chen, Javier Artiles, Heng Ji,
Hongbo Deng, Lev-Arie Ratinov, Jing Zheng, Jiawei
Han, and Dan Roth. 2011. Entity linking system
description. In TAC2011.
Zheng Chen and Heng Ji. 2011. Collaborative rank-
ing: A case study on entity linking. In Proceedings
of the Conference on Empirical Methods in Natu-
ral Language Processing, EMNLP ?11, pages 771?
781, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
Liwei Chen, Yansong Feng, Songfang Huang, Yong
Qin, and Dongyan Zhao. 2014. Encoding relation
requirements for relation extraction via joint infer-
ence. In Proceedings of the 52nd Annual Meeting
on Association for Computational Linguistics, ACL
2014, pages 818?827, Stroudsburg, PA, USA. Asso-
ciation for Computational Linguistics.
Xiao Cheng and Dan Roth. 2013. Relational inference
for wikification. In EMNLP.
Yejin Choi, Eric Breck, and Claire Cardie. 2006. Joint
extraction of entities and relations for opinion recog-
nition. In Proceedings of the 2006 Conference on
Empirical Methods in Natural Language Process-
ing, EMNLP ?06, pages 431?439, Stroudsburg, PA,
USA. Association for Computational Linguistics.
1921
Mark Dredze, Paul McNamee, Delip Rao, Adam Ger-
ber, and Tim Finin. 2010. Entity disambiguation for
knowledge base population. In Coling2010.
Anthony Fader, Stephen Soderland, and Oren Etzioni.
2011. Identifying relations for open information ex-
traction. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing,
EMNLP ?11, pages 1535?1545, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Radu Florian, Hongyan Jing, Nanda Kambhatla, and
Imed Zitouni. 2006. Factorizing complex mod-
els: A case study in mention detection. In Proceed-
ings of the 21st International Conference on Com-
putational Linguistics and the 44th annual meeting
of the Association for Computational Linguistics,
pages 473?480. Association for Computational Lin-
guistics.
Radu Florian, John F Pitrelli, Salim Roukos, and Imed
Zitouni. 2010. Improving mention detection robust-
ness to noisy input. In Proceedings of the 2010 Con-
ference on Empirical Methods in Natural Language
Processing, pages 335?345. Association for Com-
putational Linguistics.
Xianpei Han and Le Sun. 2011. A generative entity-
mention model for linking entities with knowledge
base. In Proceedings of ACL, HLT ?11, pages 945?
954, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
Xianpei Han, Le Sun, and Jun Zhao. 2011. Collective
entity linking in web text: a graph-based method. In
SIGIR, SIGIR ?11, pages 765?774, New York, NY,
USA. ACM.
Raphael Hoffmann, Congle Zhang, Xiao Ling,
Luke Zettlemoyer, and Daniel S. Weld. 2011.
Knowledge-based weak supervision for information
extraction of overlapping relations. In Proceedings
of the 49th ACL-HLT - Volume 1, HLT ?11, pages
541?550, Stroudsburg, PA, USA. ACL.
Heng Ji, Ralph Grishman, and Hoa Dang. 2011.
Overview of the tac2011 knowledge base population
track. In Proceedings of TAC.
Rohit J. Kate and Raymond J. Mooney. 2010. Joint
entity and relation extraction using card-pyramid
parsing. In Proceedings of the Fourteenth Confer-
ence on Computational Natural Language Learning,
CoNLL ?10, pages 203?212, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Qi Li and Heng Ji. 2014. Incremental joint extrac-
tion of entity mentions and relations. In Proceed-
ings of the 52nd Annual Meeting on Association for
Computational Linguistics, ACL 2014, pages 402?
412, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
Qi Li, Sam Anzaroot, Wen-Pin Lin, Xiang Li, and
Heng Ji. 2011. Joint inference for cross-document
information extraction. In Proceedings of the 20th
ACM International Conference on Information and
Knowledge Management, CIKM ?11, pages 2225?
2228, New York, NY, USA. ACM.
Thomas Lin, Mausam, and Oren Etzioni. 2012.
No noun phrase left behind: Detecting and typ-
ing unlinkable entities. In Proceedings of the 2012
EMNLP-CoNLL, EMNLP-CoNLL ?12, pages 893?
903, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
Fang Liu and Jun Zhao. 2012. Sweat2012: Pattern
based english slot filling system for knowledge base
population at tac 2012. In TAC2012.
Mike Mintz, Steven Bills, Rion Snow, and Dan Ju-
rafsky. 2009. Distant supervision for relation ex-
traction without labeled data. In Proceedings of the
Joint Conference of the 47th Annual Meeting of the
ACL and the 4th IJCNLP of the AFNLP: Volume 2 -
Volume 2, ACL ?09, pages 1003?1011.
Sean Monahan and Dean Carpenter. 2012. Lorify: A
knowledge base from scratch. In TAC2012.
Lev Ratinov, Dan Roth, Doug Downey, and Mike An-
derson. 2011. Local and global algorithms for dis-
ambiguation to wikipedia. In ACL.
Sebastian Riedel, Limin Yao, and Andrew McCallum.
2010. Modeling relations and their mentions with-
out labeled text. In Machine Learning and Knowl-
edge Discovery in Databases, volume 6323 of Lec-
ture Notes in Computer Science, pages 148?163.
Springer Berlin / Heidelberg.
Sebastian Riedel, Limin Yao, Benjamin M. Marlin, and
Andrew McCallum. 2013. Relation extraction with
matrix factorization and universal schemas. In Joint
Human Language Technology Conference/Annual
Meeting of the North American Chapter of the Asso-
ciation for Computational Linguistics (HLT-NAACL
?13), June.
Benjamin Roth, Grzegorz Chrupala, Michael Wiegand,
Mittul Singh, and Dietrich Klakow. 2012. General-
izing from freebase and patterns using cluster-based
distant supervision for tac kbp slotfilling 2012. In
TAC2012.
Prithviraj Sen. 2012. Collective context-aware topic
models for entity disambiguation. In Proceedings
of the 21st International Conference on World Wide
Web, WWW ?12, pages 729?738, New York, NY,
USA. ACM.
Sameer Singh, Sebastian Riedel, Brian Martin, Jiap-
ing Zheng, and Andrew McCallum. 2013. Joint
inference of entities, relations, and coreference. In
Proceedings of the 2013 Workshop on Automated
Knowledge Base Construction, AKBC ?13, pages 1?
6, New York, NY, USA. ACM.
Ang Sun, Xin Wang, Sen Xu, Yigit Kiran, Shakthi
Poornima, Andrew Borthwick, , and Ralph Grish-
man. 2012. Intelius-nyu tac-kbp2012 cold start sys-
tem. In TAC2012.
1922
Mihai Surdeanu, Julie Tibshirani, Ramesh Nallap-
ati, and Christopher D. Manning. 2012. Multi-
instance multi-label learning for relation extraction.
In EMNLP-CoNLL, pages 455?465. ACL.
Suzanne Tamang, Zheng Chen, and Heng Ji. 2012. En-
tity linking system and slot filling validation system.
In TAC2012.
Michael Wick, Sameer Singh, Ari Kobren, and Andrew
McCallum. 2013. Assessing confidence of knowl-
edge base content with an experimental study in en-
tity resolution. In AKBC2013.
Limin Yao, Sebastian Riedel, and Andrew McCallum.
2010. Collective cross-document relation extraction
without labelled data. In Proceedings of EMNLP,
EMNLP ?10, pages 1013?1023, Stroudsburg, PA,
USA. ACL.
Shubin Zhao and Ralph Grishman. 2005. Extracting
relations with integrated information using kernel
methods. In Proceedings of the 43rd Annual Meet-
ing on Association for Computational Linguistics,
ACL ?05, pages 419?426, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Yunhong Zhou, Dennis Wilkinson, Robert Schreiber,
and Rong Pan. 2008. Large-scale parallel collabo-
rative filtering for the netflix prize. In Proceedings
of the 4th International Conference on Algorithmic
Aspects in Information and Management, AAIM
?08, pages 337?348, Berlin, Heidelberg. Springer-
Verlag.
Yiping Zhou, Lan Nie, Omid Rouhani-Kalleh, Fla-
vian Vasile, and Scott Gaffney. 2010. Resolving
surface forms to wikipedia topics. In Proceedings
of the 23rd International Conference on Computa-
tional Linguistics, COLING ?10, pages 1335?1343,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
1923
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 91?99,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Visual Information in Semantic Representation
Yansong Feng and Mirella Lapata
School of Informatics, University of Edinburgh
10 Crichton Street, Edinburgh, EH8 9AB, UK
Y.Feng-4@sms.ed.ac.uk, mlap@inf.ed.ac.uk
Abstract
The question of how meaning might be ac-
quired by young children and represented by
adult speakers of a language is one of the most
debated topics in cognitive science. Existing
semantic representation models are primarily
amodal based on information provided by the
linguistic input despite ample evidence indi-
cating that the cognitive system is also sensi-
tive to perceptual information. In this work we
exploit the vast resource of images and associ-
ated documents available on the web and de-
velop a model of multimodal meaning repre-
sentation which is based on the linguistic and
visual context. Experimental results show that
a closer correspondence to human data can be
obtained by taking the visual modality into ac-
count.
1 Introduction
The representation and modeling of word mean-
ing has been a central problem in cognitive science
and natural language processing. Both disciplines
are concerned with how semantic knowledge is ac-
quired, organized, and ultimately used in language
processing and understanding. A popular tradition
of studying semantic representation has been driven
by the assumption that word meaning can be learned
from the linguistic environment. Words that are sim-
ilar in meaning tend to behave similarly in terms
of their distributions across different contexts. Se-
mantic spacemodels, among which Latent Semantic
Analysis (LSA, Landauer and Dumais 1997) is per-
haps known best, operationalize this idea by captur-
ing word meaning quantitatively in terms of simple
co-occurrence statistics. Each word w is represented
by a k element vector reflecting the local distribu-
tional context of w relative to k context words. More
recently, topic models have been gaining ground as
a more structured representation of word meaning.
In contrast to more standard semantic space mod-
els where word senses are conflated into a single
representation, topic models assume that words ob-
served in a corpus manifest some latent structure ?
word meaning is a probability distribution over a set
of topics (corresponding to coarse-grained senses).
Each topic is a probability distribution over words,
and the content of the topic is reflected in the words
to which it assigns high probability.
Semantic space (and topic) models are extracted
from real language corpora, and thus provide a direct
means of investigating the influence of the statistics
of language on semantic representation. They have
been successful in explaining a wide range of be-
havioral data ? examples include lexical priming,
deep dyslexia, text comprehension, synonym selec-
tion, and human similarity judgments (see Landauer
and Dumais 1997 and the references therein). They
also underlie a large number of natural language
processing (NLP) tasks including lexicon acquisi-
tion, word sense discrimination, text segmentation
and notably information retrieval. Despite their pop-
ularity, these models offer a somewhat impoverished
representation of word meaning based solely on in-
formation provided by the linguistic input.
Many experimental studies in language acquisi-
tion suggest that word meaning arises not only from
exposure to the linguistic environment but also from
our interaction with the physical world. For ex-
ample, infants are from an early age able to form
perceptually-based category representations (Quinn
et al, 1993). Perhaps unsurprisingly, words that re-
fer to concrete entities and actions are among the
first words being learned as these are directly ob-
servable in the environment (Bornstein et al, 2004).
Experimental evidence also shows that children re-
spond to categories on the basis of visual features,
e.g., they generalize object names to new objects of-
ten on the basis of similarity in shape (Landau et al,
1998) and texture (Jones et al, 1991).
In this paper we aim to develop a unified mod-
91
eling framework of word meaning that captures the
mutual dependence between the linguistic and visual
context. This is a challenging task for at least two
reasons. First, in order to emulate the environment
within which word meanings are acquired, we must
have recourse to a corpus of verbal descriptions and
their associated images. Such corpora are in short
supply compared to the large volumes of solely tex-
tual data. Secondly, our model should integrate lin-
guistic and visual information in a single representa-
tion. It is unlikely that we have separate representa-
tions for different aspects of word meaning (Rogers
et al, 2004).
We meet the first challenge by exploiting mul-
timodal corpora, namely collections of documents
that contain pictures. Although large scale corpora
with a one-to-one correspondence between words
and images are difficult to come by, datasets that
contain images and text are ubiquitous. For exam-
ple, online news documents are often accompanied
by pictures. Using this data, we develop a model
that combines textual and visual information to learn
semantic representations. We assume that images
and their surrounding text have been generated by
a shared set of latent variables or topics. Our model
follows the general rationale of topic models ? it is
based upon the idea that documents are mixtures of
topics. Importantly, our topics are inferred from the
joint distribution of textual and visual words. Our
experimental results show that a closer correspon-
dence to human word similarity and association can
be obtained by taking the visual modality into ac-
count.
2 Related Work
The bulk of previous work has focused on models of
semantic representation that are based solely on tex-
tual data. Many of these models represent words as
vectors in a high-dimensional space (e.g., Landauer
and Dumais 1997), whereas probabilistic alterna-
tives view documents as mixtures of topics, where
words are represented according to their likelihood
in each topic (e.g., Steyvers and Griffiths 2007).
Both approaches allow for the estimation of similar-
ity between words. Spatial models compare words
using distance metrics (e.g., cosine), while proba-
bilistic models measure similarity between terms ac-
cording to the degree to which they share the same
topic distributions.
Within cognitive science, the problem of how
words are grounded in perceptual representations
has attracted some attention. Previous modeling ef-
forts have been relatively small-scale, using either
artificial images, or data gathered from a few sub-
jects in the lab. Furthermore, the proposed models
work well for the tasks at hand (e.g., either word
learning or object categorization) but are not de-
signed as a general-purpose meaning representation.
For example, Yu (2005) integrates visual informa-
tion in a computational model of lexical acquisi-
tion and object categorization. The model learns a
mapping between words and visual features from
data provided by (four) subjects reading a children?s
story. In a similar vein, Roy (2002) considers the
problem of learning which words or word sequences
refer to objects in a synthetic image consisting of ten
rectangles. Andrews et al (2009) present a proba-
bilistic model that incorporates perceptual informa-
tion (indirectly) by combining distributional infor-
mation gathered from corpus data with speaker gen-
erated feature norms1 (which are also word-based).
Much work in computer vision attempts to learn
the underlying connections between visual features
and words from examples of images annotated with
description keywords. The aim here is to enhance
image-based applications (e.g., search or retrieval)
by developing models that can label images with
keywords automatically. Most methods discover
the correlations between visual features and words
by introducing latent variables. Standard latent se-
mantic analysis (LSA) and its probabilistic variant
(PLSA) have been applied to this task (Pan et al,
2004; Hofmann, 2001; Monay and Gatica-Perez,
2007). More sophisticated approaches estimate the
joint distribution of words and regional image fea-
tures, whilst treating annotation as a problem of sta-
tistical inference in a graphical model (Blei and Jor-
dan, 2003; Barnard et al, 2002).
Our own work aims to develop a model of se-
mantic representation that takes visual context into
account. We do not model explicitly the correspon-
dence of words and visual features, or learn a map-
ping between words and visual features. Rather,
we develop a multimodal representation of meaning
which is based on visual information and distribu-
tional statistics. We hypothesize that visual features
are crucial in acquiring and representing meaning
1Participants are given a series of object names and for each
object they are asked to name all the properties they can think
of that are characteristic of the object.
92
Michelle Obama fever hits the UK
In the UK on her first
visit as first lady, Michelle
Obama seems to be mak-
ing just as big an im-
pact. She has attracted as
much interest and column
inches as her husband on
this London trip; creating
a buzz with her dazzling outfits, her own schedule
of events and her own fanbase. Outside Bucking-
ham Palace, as crowds gathered in anticipation of
the Obamas? arrival, Mrs Obama?s star appeal was
apparent.
Table 1: Each article in the document collection contains
a document (the title is shown in boldface), and image
with related content.
and conversely, that linguistic information can be
useful in isolating salient visual features. Our model
extracts a semantic representation from large docu-
ment collections and their associated images without
any human involvement. Contrary to Andrews et al
(2009) we use visual features directly without rely-
ing on speaker generated norms. Furthermore, un-
like most work in image annotation, we do not em-
ploy any goldstandard data where images have been
manually labeled with their description keywords.
3 Semantic Representation Model
Much like LSA and the related topic models our
model creates semantic representations from large
document collections. Importantly, we assume that
the documents are paired with images which in turn
describe some of the document?s content. Our ex-
periments make use of news articles which are of-
ten accompanied with images illustrating events, ob-
jects or people mentioned in the text. Other datasets
with similar properties include Wikipedia entries
and their accompanying pictures, illustrated stories,
and consumer photo collections. An example news
article and its associated image is shown in Table 1
(we provide more detail on the database we used in
our experiments in Section 4).
Our model exploits the redundancy inherent in
this multimodal collection. Specifically, we assume
that the images and their surrounding text have been
generated by a shared set of topics. A potential
stumbling block here is the fact that images and
documents represent distinct modalities: images are
commonly described by a continuous feature space
(e.g., color, shape, texture; Barnard et al 2002; Blei
and Jordan 2003), whereas words are discrete. For-
tunately, we can convert the visual features from a
continuous onto a discrete space, thereby rendering
image features more like word units. In the follow-
ing we describe how we do this and then move on to
present an extension of Latent Dirichlet Allocation
(LDA, Blei and Jordan 2003), a topic model that can
be used to represent meaning as a probability distri-
bution over a set of multimodal topics. Finally, we
discuss how word similarity can be measured under
this model.
3.1 Image Processing
A large number of image processing techniques have
been developed in computer vision for extracting
meaningful features which are subsequently used
in a modeling task. For example, a common first
step to all automatic image annotation methods is
partitioning the image into regions, using either an
image segmentation algorithm (such as normalized
cuts; Shi and Malik 2000) or a fixed-grid layout
(Feng et al, 2004). In the first case the image is
represented by irregular regions (see Figure 1(a)),
whereas in the second case the image is partitioned
into smaller scale regions which are uniformly ex-
tracted from a fixed grid (see Figure 1(b)). The ob-
tained regions are further represented by a standard
set of features including color, shape, and texture.
These can be treated as continuous vectors (Blei and
Jordan, 2003) or in quantized form (Barnard et al,
2002).
Despite much progress in image segmentation,
there is currently no automatic algorithm that can
reliably divide an image into meaningful parts. Ex-
tracting features from small local regions is thus
preferable, especially for image collections that are
diverse and have low resolution (this is often the case
for news images). In our work we identify local re-
gions using a difference-of-Gaussians point detector
(see Figure 1(c)). This representation is based on de-
scriptors computed over automatically detected im-
age regions. It provides a much richer (and hopefully
more informative) feature space compared to the
alternative image representations discussed above.
For example, an image segmentation algorithm,
would extract at most 20 regions from the image
in Figure 1; uniform grid segmentation yields 143
93
(a) (b) (c)
Figure 1: Image partitioned into regions of varying granularity using (a) the normalized cut image segmentation algo-
rithm, (b) uniform grid segmentation, and (c) the SIFT point detector.
(11 ? 13) regions, whereas an average of 240 points
(depending on the image content) are detected. A
non-sparse feature representation is critical in our
case, since we usually do not have more than one
image per document.
We compute local image descriptors using the
the Scale Invariant Feature Transform (SIFT) algo-
rithm (Lowe, 1999). Importantly, SIFT descriptors
are designed to be invariant to small shifts in posi-
tion, changes in illumination, noise, and viewpoint
and can be used to perform reliable matching be-
tween different views of an object or scene (Mikola-
jczyk and Schmid, 2003; Lowe, 1999). We further
quantize the SIFT descriptors using the K-means
clustering algorithm to obtain a discrete set of vi-
sual terms (visiterms) which form our visual vo-
cabulary VocV . Each entry in this vocabulary stands
for a group of image regions which are similar
in content or appearance and assumed to origi-
nate from similar objects. More formally, each im-
age I is expressed in a bag-of-words format vector,
[v1,v2, ...,vL], where vi = n only if I has n regions
labeled with vi. Since both images and documents
in our corpus are now represented as bags-of-words,
and since we assume that the visual and textual
modalities express the same content, we can go a
step further and represent the document and its as-
sociated image as a mixture of verbal and visual
words dMix. We will then learn a topic model on this
concatenated representation of visual and textual in-
formation.
3.2 Topic Model
Latent Dirichlet Allocation (Blei et al, 2003; Grif-
fiths et al, 2007) is a probabilistic model of text gen-
eration. LDA models each document using a mix-
ture over K topics, which are in turn characterized
as distributions over words. The words in the docu-
ment are generated by repeatedly sampling a topic
according to the topic distribution, and selecting a
word given the chosen topic. Under this framework,
the problem of meaning representation is expressed
as one of statistical inference: given some data ?
textual and visual words ? infer the latent structure
from which it was generated. Word meaning is thus
modeled as a probability distribution over a set of
latent multimodal topics.
LDA can be represented as a three level hierarchi-
cal Bayesian model. Given a corpus consisting of M
documents, the generative process for a document d
is as follows. We first draw the mixing proportion
over topics ?d from a Dirichlet prior with parame-
ters ?. Next, for each of the Nd words wdn in doc-
ument d, a topic zdn is first drawn from a multino-
mial distribution with parameters ?dn. The probabil-
ity of a word token w taking on value i given that
topic z = j is parametrized using a matrix ? with
bi j = p(w = i|z = j). Integrating out ?d?s and zdn?s,
gives P(D|?,?), the probability of a corpus (or doc-
ument collection):
M
?
d=1
Z
P(?d |?)
(
Nd
?
n=1
?
zdn
P(zdn|?d)P(wdn|zdn,?)
)
d?d
The central computational problem in topic
modeling is to compute the posterior distribu-
tion P(?,z|w,?,?) of the hidden variables given
a document w = (w1,w2, . . . ,wN). Although this
distribution is intractable in general, a variety of ap-
94
proximate inference algorithms have been proposed
in the literature including variational inference
which our model adopts. Blei et al (2003) introduce
a set of variational parameters, ? and ?, and show
that a tight lower bound on the log likelihood of
the probability can be found using the following
optimization procedure:
(??,??) = argmin
?,?
D(q(?,z|?,?)||p(?,z|w,?,?))
Here, D denotes the Kullback-Leibler (KL) diver-
gence between the true posterior and the variational
distribution q(?,z|?,?) defined as: q(?,z|?,?) =
q(?|?)?Nn=1 q(zn|?n), where the Dirichlet parame-
ter ? and the multinomial parameters (?1, . . . ,?N) are
the free variational parameters. Notice that the opti-
mization of parameters (??(w),??(w)) is document-
specific (whereas ? is corpus specific).
Previous applications of LDA (e.g., to docu-
ment classification or information retrieval) typi-
cally make use of the posterior Dirichlet parame-
ters ??(w) associated with a given document. We are
not so much interested in ? as we wish to obtain a
semantic representation for a given word across doc-
uments. We therefore train the LDA model sketched
above on a corpus of multimodal documents {dMix}
consisting of both textual and visual words. We se-
lect the number of topics, K, and apply the LDA al-
gorithm to obtain the ? parameters, where ? repre-
sents the probability of a word wi given a topic z j,
p(wi|z j) = ?i j. The meaning of wi is thus extracted
from ? and is a K-element vector, whose compo-
nents correspond to the probability of wi given each
latent topic assumed to have generated the document
collection.
3.3 Similarity Measures
The ability to accurately measure the similarity or
association between two words is often used as a di-
agnostic for the psychological validity of semantic
representation models. In the topic model described
above, the similarity between two words w1 and w2
can be intuitively measured by the extent to which
they share the same topics (Griffiths et al, 2007).
For example, we may use the KL divergence to mea-
sure the difference between the distributions p and q:
D(p,q) =
K
?
j=1
p j log2
p j
q j
where p and q are shorthand for P(w1|z j)
and P(w2|z j), respectively.
The KL divergence is asymmetric and in many ap-
plications, it is preferable to apply a symmetric mea-
sure such as the Jensen Shannon (JS) divergence.
The latter measures the ?distance? between p and q
through (p+q)2 , the average of p and q:
JS(p,q) =
1
2
[
D(p,
(p+q)
2
)+D(q,
(p+q)
2
)
]
An alternative approach to expressing the similar-
ity between two words is proposed in Griffiths et al
(2007). The underlying idea is that word association
can be expressed as a conditional distribution. If we
have seen word w1, then we can determine the prob-
ability that w2 will be also generated by comput-
ing P(w2|w1). Although the LDA generative model
allows documents to contain multiple topics, here it
is assumed that both w1 and w2 came from a single
topic:
P(w2|w1) =
K
?
z=1
P(w2|z)P(z|w1)
P(z|w1) ? P(w1|z)P(z)
where p(z) is uniform, a single topic is sampled
from the distribution P(z|w1), and an overall esti-
mate is obtained by averaging over all topics K.
Griffiths et al (2007) report results on mod-
eling human association norms using exclu-
sively P(w2|w1). We are not aware of any previous
work that empirically assesses which measure is best
at capturing semantic similarity. We undertake such
an empirical comparison as it is not a priory obvious
how similarity is best modeled under a multimodal
representation.
4 Experimental Setup
In this section we discuss our experimental design
for assessing the performance of the model pre-
sented above. We give details on our training proce-
dure and parameter estimation and present the base-
line method used for comparison with our model.
Data We trained the multimodal topic model on
the corpus created in Feng and Lapata (2008). It
contains 3,361 documents that have been down-
loaded from the BBC News website.2 Each doc-
ument comes with an image that depicts some of
its content. The images are usually 203 pixels wide
2http://news.bbc.co.uk/
95
and 152 pixels high. The average document length
is 133.85 words. The corpus has 542,414 words in
total. Our experiments used a vocabulary of 6,253
textual words. These were words that occurred at
least five times in the whole corpus, excluding
stopwords. The accompanying images were prepro-
cessed as follows. We first extracted SIFT features
from each image (150 on average) which we subse-
quently quantized into a discrete set of visual terms
using K-means. As we explain below, we deter-
mined an optimal value for K experimentally.
Evaluation Our evaluation experiments compared
the multimodal topic model against a standard text-
based topic model trained on the same corpus whilst
ignoring the images. Both models were assessed on
two related tasks, that have been previously used
to evaluate semantic representation models, namely
word association and word similarity.
In order to simulate word association, we used
the human norms collected by Nelson et al (1999).3
These were established by presenting a large num-
ber of participants with a cue word (e.g., rice) and
asking them to name an associate word in response
(e.g.,Chinese, wedding, food, white). For each word,
the norms provide a set of associates and the fre-
quencies with which they were named. We can thus
compute the probability distribution over associates
for each cue. Analogously, we can estimate the de-
gree of similarity between a cue and its associates
using our model (and any of the measures in Sec-
tion 3.3). And consequently examine (using corre-
lation analysis) the degree of linear relationship be-
tween the human cue-associate probabilities and the
automatically derived similarity values. We also re-
port howmany times the word with the highest prob-
ability under the model was the first associate in the
norms. The norms contain 10,127 unique words in
total. Of these, we created semantic representations
for the 3,895 words that appeared in our corpus.
Our word similarity experiment used the Word-
Sim353 test collection (Finkelstein et al, 2002)
which consists of relatedness judgments for word
pairs. For each pair, a similarity judgment (on
a scale of 0 to 10) was elicited from human
subjects (e.g., tiger-cat are very similar, whereas
delay?racism are not). The average rating for each
pair represents an estimate of the perceived sim-
ilarity of the two words. The task varies slightly
from word association. Here, participants are asked
3http://www.usf.edu/Freeassociation.
Figure 2: Performance of multimodal topic model on pre-
dicting word association under varying topics and visual
terms (development set).
to rate perceived similarity rather than generate the
first word that came into their head in response to a
cue word. The collection contains similarity ratings
for 353 word pairs. Of these, we constructed seman-
tic representations for the 254 that appeared in our
corpus. We also evaluated how well model produced
similarities correlate with human ratings. Through-
out this paper we report correlation coefficients us-
ing Pearson?s r.
5 Experimental Results
Model Selection The multimodal topic model has
several parameters that must be instantiated. These
include the quantization of the image features, the
number of topics, the choice of similarity function,
and the values for ? and ?. We explored the pa-
rameter space on held-out data. Specifically, we fit
the parameters for the word association and similar-
ity models separately using a third of the associa-
tion norms and WordSim353 similarity judgments,
respectively. As mentioned in Section 3.1 we used
K-means to quantize the image features into a dis-
crete set of visual terms. We varied K from 250
to 2000.We also varied the number of topics from 25
to 750 for both the multimodal and text-based topic
models. The parameter ? was set to 0.1 and ? was
initialized randomly. The model was trained using
variational Bayes until convergence of its bound on
the likelihood objective. This took 1,000 iterations.
Figure 2 shows how word association perfor-
mance varies on the development set with different
numbers of topics (t) and visual terms (r) according
96
Figure 3: Performance of multimodal topic model on pre-
dicting word similarity under varying topics and visual
terms (development set).
to three similarity measures: KL divergence, JS di-
vergence, and P(w2|w1), the probability of word w2
given w1 (see Section 3.3). Figure 3 shows results on
the development set for the word similarity task. As
far as word association is concerned, we obtain best
results with P(w2|w1), 750 visual terms and 750 top-
ics (r = 0.188). On word similarity, JS performs best
with 500 visual terms and 25 topics (r = 0.374). It is
not surprising that P(w2|w1) works best for word as-
sociation. The measure expresses the associative re-
lations between words as a conditional distribution
over potential response words w2 for cue word w1.
A symmetric function is more appropriate for word
similarity as the task involves measuring the degree
to which to words share some meaning (expressed
as topics in our model) rather than whether a word is
likely to be generated as a response to another word.
These differences also lead to different parametriza-
tions of the semantic space. A rich visual term vo-
cabulary (750 terms) is needed for modeling associ-
ation as broader aspects of word meaning are taken
into account, whereas a sparser more focused repre-
sentation (with 500 visual terms and 25 overall top-
ics) is better at isolating the common semantic con-
tent between two words. We explored the parame-
ter space for the text-based topic model in a sim-
ilar fashion. On the word association task the best
correlation coefficient was achieved with 750 top-
ics and P(w2|w1) (r = 0.139). On word similarity,
the best results were obtained with 75 topics and the
JS divergence (r = 0.309).
Model Word Association Word Similarity
UpperBnd 0.400 0.545
MixLDA 0.123 0.318
TxtLDA 0.077 0.247
Table 2: Model performance on word association and
similarity (test set).
Model Comparison Table 2 summarizes our re-
sults on the test set using the optimal set of pa-
rameters as established on the development set. The
first row shows how well humans agree with each
other on the two tasks (UpperBnd). We estimated
the intersubject correlation using leave-one-out re-
sampling4 (Weiss and Kulikowski, 1991). As can
be seen, in all cases the topic model based on tex-
tual and visual modalities (MixLDA) outperforms
the model relying solely on textual information
(TxtLDA). The differences in performance are sta-
tistically significant (p < 0.05) using a t-test (Cohen
and Cohen, 1983).
Steyvers and Griffiths (2007) also predict word
association using Nelson?s norms and a state-of-the-
art LDA model. Although they do not report correla-
tions, they compute how many times the word with
the highest probability P(w2|w1) under the model
was the first associate in the human norms. Using
a considerably larger corpus (37,651 documents),
they reach an accuracy of 16.15%. Our corpus con-
tains 3,361 documents, the MixLDA model per-
forms at 14.15% and the LDA model at 13.16%. Us-
ing a vector-based model trained on the BNC corpus
(100Mwords), Washtell andMarkert (2009) report a
correlation of 0.167 on the same association data set,
whereas our model achieves a correlation of 0.123.
With respect to word similarity, Marton et al (2009)
report correlations within the range of 0.31?0.54 us-
ing different instantiations of a vector-based model
trained on the BNC with a vocabulary of 33,000
words. Our MixLDA model obtains a correlation
of 0.318 with a vocabulary five times smaller (6,253
words). Although these results are not strictly com-
parable due to the different nature and size of the
training data, they give some indication of the qual-
ity of our model in the context of other approaches
that exploit only the textual modality. Besides, our
intent is not to report the best performance possible,
4We correlated the data obtained from each participant with
the ratings obtained from all other participants and report the
average.
97
GAME, CONSOLE, XBOX, SECOND, SONY, WORLD,
TIME, JAPAN, JAPANESE, SCHUMACHER, LAP, MI-
CROSOFT, ALONSO, RACE, TITLE, WIN, GAMERS,
LAUNCH, RENAULT, MARKET
PARTY, MINISTER, BLAIR, LABOUR, PRIME, LEADER,
GOVERNMENT, TELL, BROW, MP, TONY, SIR, SECRE-
TARY, ELECTION, CONFERENCE, POLICY, NEW, WANT,
PUBLIC, SPEECH
SCHOOL, CHILD, EDUCATION, STUDENT, WORK,
PUPIL, PARENT, TEACHER, GOVERNMENT, YOUNG,
SKILL, AGE, NEED, UNIVERSITY, REPORT, LEVEL,
GOOD, HELL, NEW, SURVEY
Table 3: Most frequent words in three topics learnt from
a corpus of image-document pairs.
but to show that a model of meaning representation
is more accurate when taking visual information into
account.
Table 3 shows some examples of the topics
found by our model, which largely form coher-
ent blocks of semantically related words. In gen-
eral, we observe that the model using image fea-
tures tends to prefer words that visualize easily
(e.g., CONSOLE, XBOX). Furthermore, the visual
modality helps obtain crisper meaning distinctions.
Here, SCHUMACHER is a very probable world for
the ?game? cluster. This is because the Formula One
driver appears as a character in several video games
discussed and depicted in our corpus. For com-
parison the ?game? cluster for the text-based LDA
model contains the words: GAME, USE, INTERNET,
SITE, USE, SET, ONLINE, WEB, NETWORK, MUR-
RAY, PLAY, MATCH, GOOD, WAY, BREAK, TECH-
NOLOGY, WORK, NEW, TIME, SECOND.
We believe the model presented here works bet-
ter than a vanilla text-based topic model for at least
three reasons: (1) the visual information helps cre-
ate better clusters (i.e., conceptual representations)
which in turn are used to measure similarity or as-
sociation; these clusters themselves are amodal but
express commonalities across the visual and textual
modalities; (2) the model is also able to capture per-
ceptual correlations between words. For example,
RED is the most frequent associate for APPLE in Nel-
son?s norms. This association is captured in our vi-
sual features (pictures with apples cluster with pic-
tures showing red objects) even though RED does not
co-occur with APPLE in our data; (3) finally, even in
cases where two words are visually very different in
terms of shape or color (e.g., BANANA and APPLE),
they tend to appear in images with similar structure
(e.g., on tables, in bowls, as being held or eaten by
someone) and thus often share some common ele-
ment of meaning.
6 Conclusion
In this paper we developed a computational model
that unifies visual and linguistic representations of
word meaning. The model learns from natural lan-
guage corpora paired with images under the assump-
tion that visual terms and words are generated by
mixtures of latent topics. We have shown that a
closer correspondence to human data can be ob-
tained by explicitly taking the visual modality into
account in comparison to a model that estimates the
topic structure solely from the textual modality. Be-
yond word similarity and association, the approach
is promising for modeling word learning and cate-
gorization as well as a wide range of priming stud-
ies. Outwith cognitive science, we hope that some
of the work described here might be of relevance
to more applied tasks such as thesaurus acquisition,
word sense disambiguation, multimodal search, im-
age retrieval, and summarization.
Future improvements include developing a non-
parametric version that jointly learns how many vi-
sual terms and topics are optimal. Currently, the size
of the visual vocabulary and the number of topics
are parameters in the model, that must be tuned sep-
arately for different tasks and corpora. Another ex-
tension concerns the creation of visual terms. Our
model assumes that an image is a bag of words. The
assumption is convenient for modeling purposes, but
clearly false in the context of visual processing. Im-
age descriptors found closely to each other are likely
to represent the same object and should form one
term rather than several distinct ones (Wang and
Grimson, 2007). Taking the spatial structure among
visual words into account would yield better topics
and overall better semantic representations. Analo-
gously, we could represent documents by their syn-
tactic structure (Boyd-Graber and Blei, 2009).
References
Andrews, M., G. Vigliocco, and D. Vinson. 2009. In-
tegrating experiential and distributional data to learn
semantic representations. Psychological Review
116(3):463?498.
Barnard, K., P. Duygulu, D. Forsyth, N. de Freitas,
D. Blei, andM. Jordan. 2002. Matching words and pic-
98
tures. Journal of Machine Learning Research 3:1107?
1135.
Blei, D. and M. Jordan. 2003. Modeling annotated data.
In Proceedings of the 26th Annual International ACM
SIGIR Conference. Toronto, ON, pages 127?134.
Blei, D. M., A. Y. Ng, and M. I. Jordan. 2003. Latent
Dirichlet alocation. Journal of Machine Learning Re-
search 3:993?1022.
Bornstein, M. H., L. R. Cote, S. Maital, K. Painter, S.-
Y. Park, and L. Pascual. 2004. Cross-linguistic analy-
sis of vocabulary in young children: Spanish, Dutch,
French, Hebrew, Italian, Korean, and American En-
glish. Child Development 75(4):1115?1139.
Boyd-Graber, J. and D. Blei. 2009. Syntactic topic
models. In Proceedings of the 22nd Conference on
Advances in Neural Information Processing Systems.
MIT, Press, Cambridge, MA, pages 185?192.
Cohen, J. and P. Cohen. 1983. Applied Multiple Regres-
sion/Correlation Analysis for the Behavioral Sciences.
Hillsdale, NJ: Erlbaum.
Feng, S., V. Lavrenko, and R. Manmatha. 2004. Mul-
tiple Bernoulli relevance models for image and video
annotation. In Proceedings of the International Con-
ference on Computer Vision and Pattern Recognition.
Washington, DC, pages 1002?1009.
Feng, Y. and M. Lapata. 2008. Automatic image annota-
tion using auxiliary text information. In Proceedings
of the ACL-08: HLT . Columbus, pages 272?280.
Finkelstein, L., E. Gabrilovich, Y. Matias, E. Rivlin,
Z. Solan, G. Wolfman, and E. Ruppin. 2002. Placing
search in context: The concept revisited. ACM Trans-
actions on Information Systems 20(1):116?131.
Griffiths, T. L., M. Steyvers, and J. B. Tenenbaum. 2007.
Topics in semantic representation. Psychological Re-
view 114(2):211?244.
Hofmann, T. 2001. Unsupervised learning by proba-
bilistic latent semantic analysis. Machine Learning
41(2):177?196.
Jones, S. S., L. B. Smith, and B. Landau. 1991. Ob-
ject properties and knowledge in early lexical learning.
Child Development (62):499?516.
Landau, B., L. Smith, and S. Jones. 1998. Object percep-
tion and object naming in early development. Trends
in Cognitive Science 27:19?24.
Landauer, T. and S. T. Dumais. 1997. A solution to
Plato?s problem: the latent semantic analysis theory
of acquisition, induction, and representation of knowl-
edge. Psychological Review 104(2):211?240.
Lowe, D. 1999. Object recognition from local scale-
invariant features. In Proceedings of International
Conference on Computer Vision. IEEE Computer So-
ciety, pages 1150?1157.
Marton, Y., S. Mohammad, and P. Resnik. 2009. Estimat-
ing semantic distance using soft semantic constraints
in knowledge-source ? corpus hybrid models. In Pro-
ceedings of the 2009 Conference on Empirical Meth-
ods in Natural Language Processing. Singapore, pages
775?783.
Mikolajczyk, K. and C. Schmid. 2003. A performance
evaluation of local descriptors. In Proceedings of the
9th International Conference on Computer Vision and
Pattern Recognition. Nice, France, volume 2, pages
257?263.
Monay, F. and D. Gatica-Perez. 2007. Modeling semantic
aspects for cross-media image indexing. IEEE Trans-
actions on Pattern Analysis and Machine Intelligence
29(10):1802?1817.
Nelson, D. L., C. L. McEvoy, and T.A. Schreiber. 1999.
The university of South Florida word association
norms.
Pan, J., H. Yang, P. Duygulu, and C. Faloutsos. 2004. Au-
tomatic image captioning. In Proceedings of the 2004
International Conference on Multimedia and Expo.
Taipei, pages 1987?1990.
Quinn, P., P. Eimas, and S. Rosenkrantz. 1993. Evidence
for representations of perceptually similar natural cate-
gories by 3-month and 4-month old infants. Perception
22:463?375.
Rogers, T. T., M. A. Lambon Ralph, P. Garrard,
S. Bozeat, J. L. McClelland, J. R. Hodges, and K. Pat-
terson. 2004. Structure and deterioration of semantic
memory: A neuropsychological and computational in-
vestigation. Psychological Review 111(1):205?235.
Roy, D. 2002. Learning words and syntax for a visual de-
scription task. Computer Speech and Language 16(3).
Shi, J. and J. Malik. 2000. Normalized cuts and image
segmentation. IEEE Transactions on Pattern Analysis
and Machine Intelligence 22(8):888?905.
Steyvers, M. and T. Griffiths. 2007. Probabilistic topic
models. In T. Landauer, D. McNamara, S Dennis, and
W Kintsch, editors, A Handbook of Latent Semantic
Analysis, Psychology Press.
Wang, X. and E. Grimson. 2007. Spatial latent Dirichlet
allocation. In Proceedings of the 20th Conference on
Advances in Neural Information Processing Systems.
MI Press, Cambridge, MA, pages 1577?1584.
Washtell, J. and K. Markert. 2009. A comparison of win-
dowless and window-based computational association
measures as predictors of syntagmatic human associa-
tions. In Proceedings of the 2009 Conference on Em-
pirical Methods in Natural Language Processing. Sin-
gapore, pages 628?637.
Weiss, S. M. and C. A. Kulikowski. 1991. Computer Sys-
tems that Learn: Classification and Prediction Meth-
ods from Statistics, Neural Nets, Machine Learning,
and Expert Systems. Morgan Kaufmann, San Mateo,
CA.
Yu, C. 2005. The emergence of links between lexical
acquisition and object categorization: A computational
study. Connection Science 17(3):381?397.
99
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 831?839,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Topic Models for Image Annotation and Text Illustration
Yansong Feng and Mirella Lapata
School of Informatics, University of Edinburgh
10 Crichton Street, Edinburgh EH8 9AB, UK
Y.Feng-4@sms.ed.ac.uk, mlap@inf.ed.ac.uk
Abstract
Image annotation, the task of automatically
generating description words for a picture,
is a key component in various image search
and retrieval applications. Creating image
databases for model development is, however,
costly and time consuming, since the key-
words must be hand-coded and the process
repeated for new collections. In this work
we exploit the vast resource of images and
documents available on the web for develop-
ing image annotation models without any hu-
man involvement. We describe a probabilistic
model based on the assumption that images
and their co-occurring textual data are gener-
ated by mixtures of latent topics. We show that
this model outperforms previously proposed
approaches when applied to image annotation
and the related task of text illustration despite
the noisy nature of our dataset.
1 Introduction
Recent years have witnessed the rapid growth of im-
age collections available for searching and browsing
over the Internet. Although image search engines are
still in their infancy, initial research suggests that the
deployed algorithms are not very accurate (Hawking
et al, 1999). Given a query, search engines retrieve
relevant pictures by analyzing the image caption (if
it exists), textual descriptions found adjacent to the
image, and other text-related factors such as the file
name of the image. However, since they do not an-
alyze the actual content of the images, search en-
gines cannot be used to retrieve pictures from unan-
notated collections. The ability to perform the an-
notation task automatically would be of significant
practical import for many image-based applications.
Besides search and retrieval, other examples include
browsing support (e.g., by clustering images into
groups that are visually and semantically coherent)
and story picturing (i.e., automatically suggesting
images to illustrate text).
Automatic image annotation is a popular task in
computer vision; a large number of approaches have
been proposed in the literature under many distinct
learning paradigms. These range from supervised
classification (Smeulders et al, 2000; Vailaya et al,
2001) to instantiations of the noisy-channel model
(Duygulu et al, 2002), to clustering (Barnard et al,
2002), and methods inspired by information retrieval
(Feng et al, 2004; Lavrenko et al, 2003). Despite
differences in application and formulation, all these
methods essentially attempt to learn the correlation
between image features and words from examples
of annotated images. The Corel database has been
extensively used as a testbed for the development
and evaluation of image annotation models. It is a
collection of stock photographs, divided into themes
(e.g., tigers, sunsets) each of which are associated
with keywords (e.g., sun, sea) that are considered
appropriate descriptors for all images belonging to
the same theme.
Unfortunately, the Corel database is not represen-
tative of the size or content of real-world image col-
lections. It has a small number of themes with many
closely related images which in turn share keyword
descriptions. It is therefore relatively easy to learn
the associations between images and keywords and
do well on annotation and retrieval tasks (Tang and
Lewis, 2007; Westerveld and de Vries, 2003). An
appealing alternative is the use of resources where
images and their annotations co-occur naturally. Ex-
amples include images found in news documents,
consumer photo collections, Wikipedia articles, il-
lustrated stories and so on. The key idea here is to
treat the words in the surrounding text as annota-
tions for the image. These annotations are undoubt-
831
edly noisy, but plenty and cost-free. Moreover, the
collateral text is often longer and more informative
in comparison to the few keywords reserved for each
image in Corel.
In this paper we propose a probabilistic image
annotation model that learns to automatically label
images under such noisy conditions. We use the
database created in Feng and Lapata (2008) which
consists of news articles, images, and their cap-
tions. Our model exploits the redundancy inherent
in this multimodal dataset by assuming that images
and their surrounding text are generated by a shared
set of latent variables or topics. Specifically, we de-
scribe documents and images by a common multi-
modal vocabulary consisting of textual words and
visual terms (visiterms). Due to polysemy and syn-
onymy many words in this vocabulary will refer to
the same underlying concept. Using Latent Dirichlet
Allocation (LDA, Blei and Jordan 2003), a proba-
bilistic model of text generation, we represent visual
and textual meaning jointly as a probability distribu-
tion over a set of topics. Our annotation model takes
these topic distributions into account while finding
the most likely keywords for an image and its asso-
ciated document. We also show how the model can
be straightforwardly modified to perform automatic
text illustration.1 The task is routinely performed by
news writers who often have to search large image
collections in order to find suitable pictures for their
text. Here, the model takes a document as input and
suggests images that match its content. Experimen-
tal results on both tasks bring improvements over
competitive models.
2 Related Work
A variety of learning methods have been applied to
the image annotation task. These generally fall un-
der two broad categories. Supervised methods de-
fine annotation as a classification task, e.g., by as-
suming a one-to-one correspondence between vo-
cabulary words and classes or by grouping several
words into a single class (see Chai and Hung 2008
for an overview). Unsupervised approaches attempt
to discover the underlying connections between vi-
sual features and words, typically by introducing
latent variables. Standard latent semantic analysis
(LSA) and its probabilistic variant (PLSA) have
been applied to this task (Hofmann, 2001; Monay
and Gatica-Perez, 2007; Pan et al, 2004). More so-
phisticated models estimate the joint distribution of
words and regional image features, whilst treating
1We use the terms ?text illustration? and ?story picturing?
interchangeably throughout the paper.
annotation as a problem of statistical inference in a
graphical model (Barnard et al, 2002; Blei and Jor-
dan, 2003; Wang et al, 2009).
Irrespectively of the underlying model or task at
hand, much work has focused how to best represent
the visual and textual modalities in order to exploit
their synergy. Several approaches attempt to render
images more word-like, by reducing the dimension-
ality of the image feature space (Bosch et al, 2008;
Fei-Fei and Perona, 2005) or by learning a single
representation for both visual and textual features
(Monay and Gatica-Perez, 2007; Zhao and Grosky,
2003).
Our own work approaches the image annotation
(and related story picturing) task from a slightly dif-
ferent angle. We train and test our model on im-
ages that contain implicit (and thus noisy) annota-
tions that have not been specifically created for our
task. On account of this, our model has access to
knowledge sources other than the image and its key-
words (i.e., the news article containing the image
we wish to annotate). In Feng and Lapata (2008)
we addressed this problem with a modified ver-
sion of the continuous relevance annotation model
(Lavrenko et al, 2003). Unlike other unsupervised
approaches where a set of latent variables is in-
troduced, each defining a joint distribution on the
space of keywords and image features, the relevance
model captures the joint probability of images and
annotated words directly, without requiring an inter-
mediate clustering stage (i.e., each annotated image
in the training set is treated as a latent variable). We
modified this model so as to exploit the informa-
tion present in the document in two ways. First, in
estimating the conditional probability of a keyword
given an image, we also considered its likelihood in
the collateral document. Secondly, we used an LDA
model (trained on the document collection) to prune
from the model?s output words that are not represen-
tative of the document?s topics.
The proposed approach differs from Feng
and Lapata (2008) in three important respects:
(a) document-based information is an integral part
of our model as we predict caption words given the
image and its accompanying document (b) LDA is
no longer a post-processing step; our model relies on
LDA to infer meaningful topics that capture the co-
occurrence of visual features and words; (c) beyond
image annotation, we show how the same frame-
work can be applied to story picturing (Joshi et al,
2006), a task which has received less attention in the
literature.
In terms of model structure, Blei and Jordan
832
(2003) andMonay and Gatica-Perez (2007) are clos-
est to our work. The first model, known as corre-
spondence LDA (CorrLDA), has been successfully
employed for modeling annotated images in the
Corel domain. CorrLDA also uses the notion of topic
to model the generation of images and their captions.
In this model, the visual modality drives the defini-
tion of the latent space to which the textual modality
is linked. The second model is based on PLSA and
learns a representation similar to ours consisting of
textual and visual features. It is also trained using
captioned images from the Corel database. We work
with noisier and larger datasets. Our model exploits
the captions accompanying the images as well as
their surrounding documents. As a result, we obtain
a similar number of textual and visual words; these
are often imbalanced in the Corel database, where
visual words are nearly 50 times more than textual
words. The different nature of our data dictates the
use of a model where the visual and textual modal-
ity are given equal importance in defining the latent
space.
3 Problem Formulation
In this section we give a brief description of the im-
age database we employ and also define the image
annotation and story picturing tasks we are attempt-
ing here. As mentioned earlier, we use the dataset
created in Feng and Lapata (2008).2 It contains
3,361 articles that have been downloaded from the
BBC News website3. Each article contains a news
image which in turn is associated with a caption.
The images are usually 203 pixels wide and 152 pix-
els high. The average caption length is 5.35 tokens,
and the average document length 133.85 tokens. The
captions vocabulary is 2,167 words and the docu-
ment vocabulary is 6,253. The vocabulary shared
between captions and documents is 2,056 words.
In contrast to the Corel database, this dataset con-
tains more complex images (with many objects) and
has a larger vocabulary (Corel?s vocabulary is ap-
proximately 300 words). An example of an abridged
database entry is shown in Figure 1.
Due to the non-standard nature of the database
we assume that the caption and news article de-
scribe the content of the image either directly or in-
directly. It also follows that we may not be able to
name all objects depicted in the image. Now, given
these constraints our goal is twofold. Firstly, we will
perform image annotation. Our model is trained on
2Available from http://homepages.inf.ed.ac.uk/
s0677528/data.html.
3http://news.bbc.co.uk/
A woman from East
Sussex who bought an
emu egg sold as a nov-
elty food item on a
farm on the Isle of
Wight has managed to
hatch it into a chick. Osborne the emu will grow
Gillian Stone, from to over 6ft tall
Bexhill, who breeds chickens, brought home three large
green emu eggs from a holiday and put them in an incu-
bator in her kitchen. Two turned out to be infertile, but
after 52 days little Osborne hatched
Table 1: Each entry in the BBC News database contains a
document, an image, and its caption (shown in boldface).
document-image-caption tuples like the one shown
in Table 1. During testing, we must infer the cap-
tion for an image. Secondly, we use the same dataset
to perform automatic text illustration. During train-
ing, the model has access to the same collection of
image-caption-document tuples. During testing, we
are given a document and must find the images that
best illustrate it.
4 Image and Document Representation
Words and images represent distinct modalities, im-
ages live in a continuous feature space, whereas
words are discrete. Yet, both modalities on some
level capture the same underlying concepts as they
are used to describe the same objects. A common
first step to all previous methods is the segmenta-
tion of the image into regions, using either a fixed-
grid layout or an image segmentation algorithm. Re-
gions are usually described by a standard set of fea-
tures including color, texture, and shape which are
treated as continuous vectors (e.g., Barnard et al
2002; Blei and Jordan 2003) or in quantized form
(e.g., Duygulu et al 2002). Through this process,
the low-level image features are made to resemble
word-like units.
Here, we go one step further and represent each
image by a bag of visual words, thereby convert-
ing visual features from a continuous onto a discrete
space. In order to do this we use the Scale Invariant
Feature Transform (SIFT) algorithm (Lowe, 1999).
The general idea behind the algorithm is to first
sample an image with the difference-of-Gaussians
point detector at different scales and locations. Im-
portantly, this detector is, to some extent, invariant to
translation, scale, rotation and illumination changes.
Each detected region is represented with a SIFT de-
scriptor which is a histogram of edge directions at
833
different locations. SIFT descriptors are quantized
using the K-means clustering algorithm to obtain a
discrete set of visual terms (visiterms) which form
our visual vocabulary Vocv. Each entry in this vo-
cabulary stands for a group of image regions which
are similar in content or appearance and assumed to
originate from similar objects. More formally, each
image I is expressed in a bag-of-words format vec-
tor, [wv1,wv2, ...,wvL ], where wvi = n only if I has n
regions labeled with vi.
Since visual and textual modalities have now the
same status?they are both represented as bags-of-
words?we can also represent any image-caption-
document tuple jointly as a mixed document dMix.
The underlying assumption is that the two modali-
ties express the same meaning which, as we explain
below, can be operationalized as a probability distri-
bution over a set of topics.
5 Modeling
Latent Dirichlet Allocation For ease of exposi-
tion, we first describe the basics of Latent Dirichlet
Allocation (LDA; Blei et al 2003), a probabilistic
model of text generation and then move on to dis-
cuss our models which make use of probabilities es-
timated by LDA.
LDA can be represented as a three level hierarchi-
cal Bayesian model. Given a corpus consisting of M
documents, Blei et al (2003) define the generative
process for a document d as follows:
1. Choose ?|?? Dir(?)
2. For n ? 1,2, ...,N :
(a) Choose topic zn|??Mult(?)
(b) Choose a word wn|zn,?1:K ?Mult(?zn)
The mixing proportion over topics ? is drawn from
a Dirichlet prior with parameters ? whose role is to
create a smoothed topic distribution. Once ? and ?
are sampled, then each document is generated ac-
cording to the topic proportions z1:K and word prob-
abilities over topics ?. The probability of a docu-
ment d in a corpus is defined as:
P(d|?,?)=
Z
?
P(?|?)
(
N
?
n=1
?
zk
P(zk|?)P(wn|zk,?)
)
d?
Computing the posterior distribution P(?,z|d,?,?)
of the hidden variables given a document is in-
tractable in general. However, a variety of approx-
imate inference algorithms have been proposed in
the literature including variational inference which
our model adopts (Blei et al, 2003). In this case,
training an LDA model on a document collection
will give two sets of parameters, the word proba-
bilities given topics, p(w|z1:K), and the topic pro-
portions given documents, P(z1:K |d). The latter are
document-specific, whereas the former represent the
set of topics learned from the document collection.
Given a trained model, it is possible to do infer-
ence on an unseen document dnew:
p(w|dnew) ?
K
?
k=1
P(w|zk)
?k
?Kj=1 ? j
(1)
where P(w|z1:K) are word probabilities over top-
ics z1:K estimated during model training, and ?1:K
are variational Dirichlet parameters obtained during
inference on the new document (and can be consid-
ered as the posteriors of topic proportions over doc-
uments).
Image Annotation In the standard image annota-
tion setting, a hypothetical model is given an image I
and a set of keywordsW , and must find the subsetWI
(WI ?W ) which appropriately describes image I:
W ?I = argmax
W
P(W |I) (2)
The keywords are usually assumed to be condition-
ally independent on each other, so Equation (2) sim-
plifies to:
W ?I = argmax
W
?
w?W
P(w|I) (3)
Each entry in our database is an image-caption-
document tuple (I,C,D). In this setting, we must
find the subset of keywords WI which appropriately
describe image I with the help of the accompanying
document D:
W ?I = argmax
Wt
P(Wt |I,D) (4)
Here, Wt denotes a set of textual words (we use the
subscript t to discriminate from the visual words
which are not part of the model?s output). We also
assume that the keywords are conditionally indepen-
dent of each other:
W ?I =argmax
Wt
P(Wt |I,D)=argmax
Wt
?
wt?Wt
P(wt |I,D) (5)
Since I and D are represented jointly as the con-
catenation of textual and visual terms, we may intu-
itively simplify the problem and use the mixed doc-
ument representation dMix directly in estimating the
conditional probabilities P(wt |I,D):
P(wt |I,D) ? P(wt |dMix) (6)
834
Substituting Equation (6) into (5) yields:
W ?I ? argmax
Wt
?
wt?Wt
P(wt |dMix) (7)
As mentioned earlier, we assume that the image and
its associated text are generated by a mixture of
latent topics which we infer using LDA. Specifi-
cally, we select the number of topics K and apply
the LDA algorithm to a corpus consisting of doc-
uments {dMix} in order to obtain the multimodal
word distributions over topics P(w|z1:K), and the
estimated posterior of the topic proportions over
documents P(z1:K |dMix). We infer the topic pro-
portions P(z1:K |dMixnew) on a new document-image
pair dMixnew approximately using Equations (1)
and (7):4
W ?I ? argmax
Wt
?
wt?Wt
P(wt |dMix) (8)
= argmax
Wt
?
wt?Wt
K
?
k=1
P(wt |zk)P(zk|dMix)
? argmax
Wt
?
wt?Wt
K
?
k=1
P(wt |zk)
?k
?Kj=1 ? j
where P(wt |zk) are obtained during training, and ?1:K
are inferred on the image-document test pair.
However, note that for an unseen image dI and ac-
companying document dD, the estimated topic pro-
portions are solely based on variational inference
which is an approximate algorithm. In order to ren-
der the model more robust, we smooth the topic pro-
portions P(z1:K |dMix) with probabilities based on a
single modality:
P?(z1:K |dMix) ? (9)
q1P(z1:K |dMix)+q2P(z1:K |dD)+q3P(z1:K |dI)
where P(z1:K |dD) and P(z1:K |dI) are inferred on dD
and dI , respectively, and q1, q2, q3 are smoothing
parameters (which we tune experimentally on held-
out data); q3 is a shorthand for (1?q1?q2).
In sum, calculating P(Wt |I,D) boils down to es-
timating the probabilities P(wt |dMix) according to
Equations (8) and (9) which we obtain using the
LDA model. We train LDA on the document col-
lection {dMix} and use inference to obtain the topic
distributions of unseen image-document pairs. In the
end, we obtain a ranked list of textual words wt , the
n-best of which are the annotations for image I.
4During training, the model has access to all three elements
(I,C,D), so the mixed document dMix is the concatenation of
the visual terms and words in the caption and document. Dur-
ing testing, the model is given an image and its accompanying
document, so dMix contains words based on I and D, but notC.
Text Illustration Previous text illustration models
are based on Corel-like databases with manual im-
age descriptions (Barnard and Forsyth, 2001; Blei
and Jordan, 2003) or instance-based learning using
complex learning schemes (Joshi et al, 2006). Here,
we present a relatively simple model, again under
the topic mixture framework.
Given a test document D and a candidate image
database I1...N with captionsC, we must find the im-
age or images which best describe the document.
We can simply compute the probability of each vi-
sual term in the vocabulary given D by marginaliz-
ing over the document topics z1:K :
P(wv|D) = ?
z1:K
P(wv|zk)P(zk|dD) (10)
where wv is a visual term and P(wv|zk) the probabil-
ity of wv given topic zk (as estimated on the training
set).
Equation (10) delivers a ranked list of visual terms
according to a given document. We could multiply
these probabilities together mirroring Equation (7),
however this is not reliable. In contrast to textual
words, for which we may infer whether they are
linguistically meaningful (e.g., by resorting to their
part of speech), there is no easy way of knowing
which visual words are important. Relying solely on
frequency is not reliable either, as frequent visiterms
may simply represent features common in all images
(e.g., most images have some white color). To avoid
a bias towards frequent but potentially irrelevant vi-
sual words, we output a fixed number of visual terms
and select the image with the highest overlap as the
correct illustration.
6 Experimental Setup
In this section we discuss our experimental design
for assessing the performance of the models pre-
sented above. We give details on our training pro-
cedure and parameter estimation, describe our fea-
tures, and present the baseline methods used for
comparison with our models.
Data We evaluated the image annotation and text
illustration tasks on the dataset described in Sec-
tion 3. Documents and captions were part-of-speech
tagged and lemmatized with Tree Tagger (Schmid,
1994). We excluded from the vocabulary low fre-
quency words (appearing fewer than five times)
and words other than nouns, verbs, and adjectives.
For the image annotation task we follow the data
split used in Feng and Lapata (2008). The training
set contains 2,881 image-caption-document tuples;
240 tuples are reserved for development and 240 for
835
testing. Our text illustration experiments, used 2,881
image-caption-document tuples for training. For the
purposes of simulating a real story picturing engine
environment, we created a large image pool of 450
image-caption pairs and tested on 300 of them.
Model Parameters For each image we ex-
tracted 150 (on average) SIFT features. These were
quantized into a discrete set of visual terms us-
ing K-means. We varied K from 100 to 2,000. We
trained the LDA topic model on the multimodal doc-
ument collection {dMix} and varied the number of
topics from 15 to 1,000. The hyperparameter ? was
initialized to 0.1; the ? probabilities were initial-
ized randomly. The maximum number of iterations
for variational inference was set to 1,000. We tuned
the smoothing parameters q1, q2, and q3 (see Equa-
tion (9)) on the development set. The best values
were q1 = 0.84, q2 = 0.12, and q3 = 0.04 (for both
tasks). As the number of visual terms and topics are
interrelated we exhaustively examined all possible
combinations on the development set. We obtained
best results on image annotation with 1,000 topics
and 750 visual terms. On text illustration the best pa-
rameters were 1,000 topics and 2,000 visual terms.
Baselines For the image annotation experiments,
we compared our model against the following base-
lines. Firstly, we trained a vanilla LDA model on
the document collection without taking the im-
ages into account. This model estimates P(wt |D) =
?Kk=1P(wt |zk)P(zk|D), the probability of textual
word wt given text document D. We assume that the
most probable words are the captions for the accom-
panying image. Our second baseline is the extended
relevance model (Feng and Lapata, 2008) that also
takes the document into account but crucially as-
sumes that the process of generating the images is
independent from the process of generating its key-
words.
We also compared our approach with two
closely related latent variable models (developed for
image-caption pairs), a PLSA-based model (Monay
and Gatica-Perez, 2007) and CorrLDA (Blei and
Jordan, 2003). The former model is an asymmet-
ric version of PLSA; it estimates the topic structure
solely from the textual modality and keeps it fixed
for the visual modality. The technique is similar to
folding-in (Hofmann, 2001), the standard PLSA pro-
cedure for inference in unseen documents and al-
lows modeling an image as a mixture of latent top-
ics that is defined only by one modality (in this
case the caption words). CorrLDA first generates
image regions from a Gaussian multinomial distri-
bution parametrized with Dirichlet priors. Then, for
each annotation word, it uniformly selects a region
from the image and generates a word according to
the topic used to generate that region. We optimized
the parameters for both models on the development
set. For CorrLDA, we followed the mean-field vari-
ational inference strategy proposed in Blei (2004).
The optimal number of topics for PLSA, was 200
(with 2000 visual terms) and for CorrLDA 50.
For the text illustration experiments, the pro-
posed model was compared with three baselines.
The first one is essentially word overlap. We se-
lect the image whose caption has the largest num-
ber of words in common with the test document.
The second one is a straightforward implementa-
tion of the vector space model (Salton and McGill,
1983) where documents and captions are repre-
sented by vectors whose components correspond to
term-document co-occurrences. We followed com-
mon practice in weighting terms by their tf-idf val-
ues, and used the cosine similarity measure to find
the image whose caption is most similar to the
test document. Our third baseline uses a text-based
LDA model to estimate document-caption similar-
ity probabilistically, through topic sharing. The im-
ages most relevant to a document are found by max-
imizing the conditional probability of the candi-
date captions C given the document dD: P(C|dD) =
?
wc?C
?Kk=1P(wc|zk)P(zk|dD) (where wc are the cap-
tion words, P(wc|zk) the conditional distribution of
each wc given a topic zk, and P(zk|dD) the condi-
tional distribution of zk given dD, the document we
wish to illustrate.
Evaluation In the image annotation task we
follow the evaluation methodology proposed in
Duygulu et al (2002). We are given an un-annotated
image I and asked to automatically produce the
n-best keywords. For all models discussed here, we
report results with the top 10 annotation words us-
ing precision, recall and F1. In the text illustration
task, we are given a test document d and a pool
of candidate images I1...N with captions C1...N . The
model is expected to find an image from the can-
didate pool that matches the test document. We use
equation (10) to output a ranked list of MI visual
terms. The image having the highest overlap with
the top 30 visual terms is selected as the illustration
for the test document. All illustration models were
evaluated using top 1 accuracy, which is the percent-
age of successfully matched image-document pairs
in the test set.
836
Model Top 10
Precision Recall F1
CorrLDA 5.33 11.80 7.36
TxtLDA 7.30 16.90 10.20
PLSA 10.26 22.60 14.12
ExtRel 14.70 27.90 19.80
MixLDA 16.30 33.10 21.60
Table 2: Automatic image annotation results.
7 Results
Our results on the image annotation task are summa-
rized in Table 2. Here, we compare our own model
(MixLDA) which is trained on both visual and tex-
tual information against an LDA model based solely
on textual information (TxtLDA), an extended ver-
sion of the Continuous Relevance model that also
exploits collateral document information (ExtRel;
Feng and Lapata 2008), a PLSA model that prior-
itizes the textual over visual modality (Monay and
Gatica-Perez, 2007), and CorrLDA (Blei and Jor-
dan, 2003) which does the opposite. We performed
significance testing on F1 using stratified shuffling
(Noreen, 1989), an instance of assumption-free ap-
proximative randomization testing.
Let us first discuss the performance of TxtLDA
and MixLDA. These two models are closely related
? they both rely on the probabilities P(wt |d) to
generate the image keywords ? save one important
difference. MixLDA uses a concatenated represen-
tation of words and visual features assuming that
the two modalities have equal importance in defin-
ing the latent space, whereas TxtLDA considers only
the textual modality. Our results show that MixLDA
outperforms TxtLDA in terms of precision (by 9%),
recall (by 16.2%). MixLDA improves F1 by 11.4%,
and the difference is significant (p < 0.01).
PLSA significantly (p < 0.01) improves upon
TxtLDA. The key difference is the visual informa-
tion which makes up (to a certain extent) for the
lack of richer textual data. Interestingly, CorrLDA
performs significantly (p < 0.01) worse than both
PLSA and TxtLDA. Recall that in CorrLDA word
topic assignments are drawn from the image regions
which are in turn drawn from a Gaussian distribu-
tion. Although this modeling choice delivers bet-
ter results on the simpler Corel dataset, it does not
seem able to capture the characteristics of our im-
ages which are noisier and more complex. More-
over, CorrLDA assumes that annotation keywords
must correspond to image regions. This assumption
is too restrictive in our setting where a single key-
TxtLDA
Afghanistan, Taleban,
soldier, British, zone,
kill, force, Microsoft,
troop, NATO
police, Burgess, time,
letter, crash, case,
death, operation,
investigation, jail
MixLDA
Afghanistan, troop,
Blair, British, NATO,
helicopter, soldier,
support, operation,
commander
Diana, police, case,
crash, Princess, re-
port, death, inquest,
Paris, Burgess
Caption
Troops need more
Chinook helicopters to
carry out operations
Princess Diana died in
a car crash in Paris in
1997
Figure 1: Annotations generated by the TxtLDA and
MixLDA models. Words in bold face indicate exact
matches. The original captions are in the last row.
word may refer to many objects or persons in an
image (e.g.,the word badminton is used to collec-
tively describe an image depicting players, shuttle-
cocks, and rackets). As an aside, it is interesting to
note, that neither PLSA nor CorrLDA achieve better
results, when modified to take the captions and asso-
ciated documents into account. PLSA scores are in
the same ballpark (see Table 2), whereas CorrLDA
performs worse, F1 decreases by 2%.
The extended relevance model improves consid-
erably upon TxtLDA, CorrLDA, and PLSA but is
significantly worse (p < 0.01) than MixLDA. On
the surface, MixLDA seems similar to ExtRel, both
models take advantage of visual and textual informa-
tion. ExtRel smooths the conditional probability of a
word given an image with the conditional probabil-
ity of the same word given the document and uses an
LDA model (trained on the document collection) to
remove non-topical keywords from the model?s out-
put. MixLDA is conceptually simpler, LDA is the
actual model rather than a post-processing step, and
exploits the synergy between visual and textual in-
formation more directly. Topics are created based on
both modalities which are treated on an equal foot-
ing. Compared to ExtRel, MixLDA improves pre-
cision by 1.6%, recall by 5.2% and the overall F1
by 1.8%.
Figure 1 illustrates examples of annotations gen-
837
Model Accuracy
TxtLDA 31.0
Overlap 31.3
VectorSpace 38.7
MixLDA 57.3
Table 3: Text Illustration results.
erated by TxtLDA and MixLDA for two images. For
comparison, we also show the goldstandard image
captions. Note that TxtLDA fails to generate any
words relating to the objects shown in the image.
It finds primarily words relating to the topics of the
associated articles such as troops and crash. On the
contrary, MixLDA is more successful at identifying
the depicted objects, since it takes visual informa-
tion into account.
Table 3 presents our results on the automatic
text illustration task. Here, we compare our mul-
timodal topic model (MixLDA) against three text-
based baselines, namely word overlap (Overlap)
a standard vector space model (VectorSpace), and
TxtLDA. We examined whether differences in per-
formance are statistically significant using a ?2 test.
As can be seen, MixLDA significantly (p < 0.01)
outperforms these models by a wide margin (accu-
racy is 57.3% for MixLDA vs. 31.0% for TxtLDA,
38.7% for the vector space model, and 31.3% for
word overlap). These results are encouraging given
the simplicity of our model. They also indicate that
substantial mileage can be gained by taking into ac-
count the visual modality.
Figure 2 shows the three best illustrations found
by MixLDA and VectorSpace (incidentally, Overlap
delivered the same ranking as VectorSpace). The im-
ages are presented in ranked order, i.e., the first im-
age was given a higher score than the second one,
etc. The document discusses Smart 1 Probe, a lunar
satellite about to end its mission by crashing onto
the moon?s surface. MixLDA identifies an image de-
picting this satellite. The second best picture is also
relevant, it resembles the moon?s surface. The Vec-
torSpace model does not find any related images, the
first one is a DNA image, the second one depicts
policemen at a crime scene and the third one Ben
Nevis, the highest mountain in the British Isles.
8 Conclusions
In this paper we have presented a probabilistic ap-
proach for automatic image annotation and text il-
lustration. Our model postulates that visual terms
and words are generated by common (hidden) top-
V
ec
to
rS
pa
ce
M
ix
L
D
A
Europe?s lunar satellite, the Smart 1 probe, is
about to end its mission by crashing onto the
Moon?s surface. It will be a spectacular end
for the robot which has spent the past 16 months
testing innovative and miniaturized space tech-
nologies. Smart 1 has also produced detailed
maps of the Moon?s chemical make-up.
Figure 2: Top-3 illustrations for document in bottom row.
ics and is trained on a dataset consisting of images
available on the Internet, their captions, and associ-
ated news articles. The annotations are implicit and
the dataset is representative of the scale, diversity,
and difficulty of real-world image collections. Over-
all, our results show that the model is robust to the
noise inherent in such data. It improves upon com-
petitive approaches that prioritize one modality over
the other or exploit them indirectly. We also show
that with minor modifications the model can be used
to automatically illustrate a document with an appro-
priate image. Our method shows promise for multi-
modal search and image retrieval and other applica-
tions which have been traditionally text-based. An
interesting future direction involves generating ac-
tual sentence descriptions rather than isolated key-
words. Another relevant application is summariza-
tion. Our results suggest that taking visual informa-
tion into account could potentially create more fo-
cused and accurate summaries.
The model presented here could be further im-
proved in two ways. Firstly, we could allow an in-
finite number of topics and develop a nonparamet-
ric version that learns how many topics are optimal.
Secondly, our model is based on word unigrams, and
in this sense takes very little linguistic knowledge
into account. Recent developments in topic model-
ing could potentially rectify this, e.g., by assuming
that each word is generated by a distribution that
combines document-specific topics and parse-tree-
specific syntactic transitions (Boyd-Graber and Blei,
2009).
838
References
Barnard, K., P. Duygulu, D. Forsyth, N. de Freitas,
D. Blei, andM. Jordan. 2002. Matching words and pic-
tures. Journal of Machine Learning Research 3:1107?
1135.
Barnard, K. and D. Forsyth. 2001. Learning the semantics
of words and pictures. In Proceedings of the 8th Inter-
national Conference on Computer Vision. Vancouver,
BC, pages 408?415.
Blei, D. 2004. Probabilistic Models of Text and Images.
Ph.D. thesis, U.C. Berkeley, Division of Computer Sci-
ence.
Blei, D. and M. Jordan. 2003. Modeling annotated data.
In Proceedings of the 26th Annual International ACM
SIGIR Conference. Toronto, ON, pages 127?134.
Blei, D., A. Ng, and M. Jordan. 2003. Latent Dirich-
let alocation. Journal of Machine Learning Research
3:993?1022.
Bosch, A., A. Zisserman, and X. Munoz. 2008. Scene
classification using a hybrid generative/discriminative
approach. IEEE Transactions on Pattern Analysis and
Machine Intelligence 30(4):712?727.
Boyd-Graber, J. and D. Blei. 2009. Syntactic topic
models. In Proceedings of the 22nd Conference on
Advances in Neural Information Processing Systems.
MIT, Press, Cambridge, MA, pages 185?192.
Chai, C. and C. Hung. 2008. Automatically annotating
images with keywords: A review of image annotation
systems. Recent Patents on Computer Science 1:55?
68.
Duygulu, P., K. Barnard, J. de Freitas, and D. Forsyth.
2002. Object recognition as machine translation:
Learning a lexicon for a fixed image vocabulary. In
Proceedings of the 7th European Conference on Com-
puter Vision. Copenhagen, Danemark, pages 97?112.
Fei-Fei, L. and P. Perona. 2005. A Bayesian hierarchi-
cal model for learning natural scene categories. In
Proceedings of the 2005 IEEE Computer Society Con-
ference on Computer Vision and Pattern Recognition.
IEEE Computer Society Washington, DC, volume 2,
pages 524?531.
Feng, S., V. Lavrenko, and R. Manmatha. 2004. Mul-
tiple Bernoulli relevance models for image and video
annotation. In Proceedings of the International Con-
ference on Computer Vision and Pattern Recognition.
Washington, DC, pages 1002?1009.
Feng, Y. and M. Lapata. 2008. Automatic image annota-
tion using auxiliary text information. In Proceedings
of ACL-08: HLT . Columbus, OH, pages 272?280.
Hawking, D., N. Craswell, P. Thistlewaite, and D. Har-
man. 1999. Results and challenges in web search eval-
uation. Computer Networks 31(11):1321?1330.
Hofmann, T. 2001. Unsupervised learning by proba-
bilistic latent semantic analysis. Machine Learning
41(2):177?196.
Joshi, D., J.Z. Wang, and J. Li. 2006. The story picturing
engine?a system for automatic text illustration. ACM
Transactions on Multimedia Computing, Communica-
tions, and Applications 2(1):68?89.
Lavrenko, V., R. Manmatha, and J. Jeon. 2003. A model
for learning the semantics of pictures. In Proceedings
of the 17th Conference on Advances in Neural Infor-
mation Processing Systems. MIT, Press, Cambridge,
MA.
Lowe, D. 1999. Object recognition from local scale-
invariant features. In Proceedings of International
Conference on Computer Vision. IEEE Computer So-
ciety, pages 1150?1157.
Monay, F. and D. Gatica-Perez. 2007. Modeling semantic
aspects for cross-media image indexing. IEEE Trans-
actions on Pattern Analysis and Machine Intelligence
29(10):1802?1817.
Pan, J., H. Yang, P. Duygulu, and C. Faloutsos. 2004. Au-
tomatic image captioning. In Proceedings of the 2004
International Conference on Multimedia and Expo.
Taipei, pages 1987?1990.
Salton, G. and M.J. McGill. 1983. Introduction to Mod-
ern Information Retrieval. McGraw-Hill, New York.
Schmid, H. 1994. Probabilistic part-of-speech tagging us-
ing decision trees. In Proceedings of the International
Conference on New Methods in Language Processing.
Manchester, UK, pages 44?49.
Smeulders, A. W., M. Worring, S. Santini, A. Gupta, and
R. Jain. 2000. Content-based image retrieval at the
end of the early years. IEEE Transactions on Pattern
Analysis and Machine Intelligence 22(12):1349?1380.
Tang, J. and P. H. Lewis. 2007. A study of quality is-
sues for image auto-annotation with the Corel data-set.
IEEE Transactions on Circuits and Systems for Video
Technology 17(3):384?389.
Vailaya, A., M. Figueiredo, A. Jain, and H. Zhang. 2001.
Image classification for content-based indexing. IEEE
Transactions on Image Processing 10:117?130.
Wang, C., D. Blei, and L. Fei-Fei. 2009. Simultaneous
image classification and annotation. In Proceedings of
CVPR. Miami, FL, pages 1903?1910.
Westerveld, T. and A. P. de Vries. 2003. Experimental
evaluation of a generative probabilistic image retrieval
model on ?easy? data. In Proceedings of the SIGIR
Multimedia Information Retrieval Workshop. Toronto,
ON.
Zhao, R. and W. I. Grosky. 2003. Video shot detection
using color anglogram and latent semantic indexing:
From contents to semantics. In B. Furht and O. Mar-
ques, editors, Handbook of Video Databases: Design
and Applications, CRC Press, pages 371?392.
839
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1239?1249,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
How Many Words is a Picture Worth?
Automatic Caption Generation for News Images
Yansong Feng and Mirella Lapata
School of Informatics, University of Edinburgh
10 Crichton Street, Edinburgh EH8 9AB, UK
Y.Feng-4@sms.ed.ac.uk, mlap@inf.ed.ac.uk
Abstract
In this paper we tackle the problem of au-
tomatic caption generation for news im-
ages. Our approach leverages the vast re-
source of pictures available on the web
and the fact that many of them are cap-
tioned. Inspired by recent work in sum-
marization, we propose extractive and ab-
stractive caption generation models. They
both operate over the output of a proba-
bilistic image annotation model that pre-
processes the pictures and suggests key-
words to describe their content. Exper-
imental results show that an abstractive
model defined over phrases is superior to
extractive methods.
1 Introduction
Recent years have witnessed an unprecedented
growth in the amount of digital information avail-
able on the Internet. Flickr, one of the best known
photo sharing websites, hosts more than three bil-
lion images, with approximately 2.5 million im-
ages being uploaded every day.1 Many on-line
news sites like CNN, Yahoo!, and BBC publish
images with their stories and even provide photo
feeds related to current events. Browsing and find-
ing pictures in large-scale and heterogeneous col-
lections is an important problem that has attracted
much interest within information retrieval.
Many of the search engines deployed on the
web retrieve images without analyzing their con-
tent, simply by matching user queries against col-
located textual information. Examples include
meta-data (e.g., the image?s file name and for-
mat), user-annotated tags, captions, and gener-
ally text surrounding the image. As this limits
the applicability of search engines (images that
1http://www.techcrunch.com/2008/11/03/
three-billion-photos-at-flickr/
do not coincide with textual data cannot be re-
trieved), a great deal of work has focused on the
development of methods that generate description
words for a picture automatically. The literature
is littered with various attempts to learn the as-
sociations between image features and words us-
ing supervised classification (Vailaya et al, 2001;
Smeulders et al, 2000), instantiations of the noisy-
channel model (Duygulu et al, 2002), latent vari-
able models (Blei and Jordan, 2003; Barnard et al,
2002; Wang et al, 2009), and models inspired by
information retrieval (Lavrenko et al, 2003; Feng
et al, 2004).
In this paper we go one step further and gen-
erate captions for images rather than individual
keywords. Although image indexing techniques
based on keywords are popular and the method of
choice for image retrieval engines, there are good
reasons for using more linguistically meaningful
descriptions. A list of keywords is often ambigu-
ous. An image annotated with the words blue,
sky, car could depict a blue car or a blue sky,
whereas the caption ?car running under the blue
sky? would make the relations between the words
explicit. Automatic caption generation could im-
prove image retrieval by supporting longer and
more targeted queries. It could also assist journal-
ists in creating descriptions for the images associ-
ated with their articles. Beyond image retrieval, it
could increase the accessibility of the web for vi-
sually impaired (blind and partially sighted) users
who cannot access the content of many sites in
the same ways as sighted users can (Ferres et al,
2006).
We explore the feasibility of automatic caption
generation in the news domain, and create descrip-
tions for images associated with on-line articles.
Obtaining training data in this setting does not re-
quire expensive manual annotation as many ar-
ticles are published together with captioned im-
ages. Inspired by recent work in summarization,
we propose extractive and abstractive caption gen-
1239
eration models. The backbone for both approaches
is a probabilistic image annotation model that sug-
gests keywords for an image. We can then simply
identify (and rank) the sentences in the documents
that share these keywords or create a new caption
that is potentially more concise but also informa-
tive and fluent. Our abstractive model operates
over image description keywords and document
phrases. Their combination gives rise to many
caption realizations which we select probabilisti-
cally by taking into account dependency and word
order constraints. Experimental results show that
the model?s output compares favorably to hand-
written captions and is often superior to extractive
methods.
2 Related Work
Although image understanding is a popular topic
within computer vision, relatively little work has
focused on the interplay between visual and lin-
guistic information. A handful of approaches gen-
erate image descriptions automatically following
a two-stage architecture. The picture is first ana-
lyzed using image processing techniques into an
abstract representation, which is then rendered
into a natural language description with a text gen-
eration engine. A common theme across differ-
ent models is domain specificity, the use of hand-
labeled data, and reliance on background ontolog-
ical information.
For example, He?de et al (2004) generate de-
scriptions for images of objects shot in uniform
background. Their system relies on a manually
created database of objects indexed by an image
signature (e.g., color and texture) and two key-
words (the object?s name and category). Images
are first segmented into objects, their signature is
retrieved from the database, and a description is
generated using templates. Kojima et al (2002,
2008) create descriptions for human activities in
office scenes. They extract features of human mo-
tion and interleave them with a concept hierarchy
of actions to create a case frame from which a nat-
ural language sentence is generated. Yao et al
(2009) present a general framework for generating
text descriptions of image and video content based
on image parsing. Specifically, images are hierar-
chically decomposed into their constituent visual
patterns which are subsequently converted into a
semantic representation using WordNet. The im-
age parser is trained on a corpus, manually an-
notated with graphs representing image structure.
A multi-sentence description is generated using a
document planner and a surface realizer.
Within natural language processing most previ-
ous efforts have focused on generating captions to
accompany complex graphical presentations (Mit-
tal et al, 1998; Corio and Lapalme, 1999; Fas-
ciano and Lapalme, 2000; Feiner and McKeown,
1990) or on using the captions accompanying in-
formation graphics to infer their intended mes-
sage, e.g., the author?s goal to convey ostensible
increase or decrease of a quantity of interest (Elzer
et al, 2005). Little emphasis is placed on image
processing; it is assumed that the data used to cre-
ate the graphics are available, and the goal is to
enable users understand the information expressed
in them.
The task of generating captions for news im-
ages is novel to our knowledge. Instead of relying
on manual annotation or background ontological
information we exploit a multimodal database of
news articles, images, and their captions. The lat-
ter is admittedly noisy, yet can be easily obtained
from on-line sources, and contains rich informa-
tion about the entities and events depicted in the
images and their relations. Similar to previous
work, we also follow a two-stage approach. Us-
ing an image annotation model, we first describe
the picture with keywords which are subsequently
realized into a human readable sentence. The
caption generation task bears some resemblance
to headline generation (Dorr et al, 2003; Banko
et al, 2000; Jin and Hauptmann, 2002) where the
aim is to create a very short summary for a doc-
ument. Importantly, we aim to create a caption
that not only summarizes the document but is also
a faithful to the image?s content (i.e., the caption
should also mention some of the objects or indi-
viduals depicted in the image). We therefore ex-
plore extractive and abstractive models that rely
on visual information to drive the generation pro-
cess. Our approach thus differs from most work in
summarization which is solely text-based.
3 Problem Formulation
We formulate image caption generation as fol-
lows. Given an image I, and a related knowl-
edge database ?, create a natural language descrip-
tion C which captures the main content of the im-
age under ?. Specifically, in the news story sce-
nario, we will generate a caption C for an image I
and its accompanying document D. The training
data thus consists of document-image-caption tu-
1240
Thousands of Tongans have
attended the funeral of King
Taufa?ahau Tupou IV, who
died last week at the age
of 88. Representatives
from 30 foreign countries
watched as the king?s coffin
was carried by 1,000 men
to the official royal burial
ground.
King Tupou, who was 88,
died a week ago.
A Nasa satellite has doc-
umented startling changes
in Arctic sea ice cover be-
tween 2004 and 2005. The
extent of ?perennial? ice
declined by 14%, losing an
area the size of Pakistan
or Turkey. The last few
decades have seen ice cover
shrink by about 0.7% per
year.
Satellite instruments can
distinguish ?old? Arctic
ice from ?new?.
Contaminated Cadbury?s
chocolate was the most
likely cause of an outbreak
of salmonella poisoning,
the Health Protection
Agency has said. About 36
out of a total of 56 cases of
the illness reported between
March and July could be
linked to the product.
Cadbury will increase its
contamination testing levels.
A third of children in the
UK use blogs and social
network websites but two
thirds of parents do not
even know what they
are, a survey suggests.
The children?s charity
NCH said there was ?an
alarming gap? in techno-
logical knowledge between
generations.
Children were found to be
far more internet-wise than
parents.
Table 1: Each entry in the BBC News database contains a document an image, and its caption.
ples like the ones shown in Table 1. During test-
ing, we are given a document and an associated
image for which we must generate a caption.
Our experiments used the dataset created by
Feng and Lapata (2008).2 It contains 3,361 articles
downloaded from the BBC News website3 each of
which is associated with a captioned news image.
The latter is usually 203 pixels wide and 152 pix-
els high. The average caption length is 9.5 words,
the average sentence length is 20.5 words, and
the average document length 421.5 words. The
caption vocabulary is 6,180 words and the docu-
ment vocabulary is 26,795. The vocabulary shared
between captions and documents is 5,921 words.
The captions tend to use half as many words as
the document sentences, and more than 50% of the
time contain words that are not attested in the doc-
ument (even though they may be attested in the
collection).
Generating image captions is a challenging task
even for humans, let alne computers. Journalists
are given explicit instructions on how to write cap-
tions4 and laypersons do not always agree on what
a picture depicts (von Ahn and Dabbish, 2004).
Along with the title, the lead, and section head-
ings, captions are the most commonly read words
2Available from http://homepages.inf.ed.ac.uk/
s677528/data/
3http://news.bbc.co.uk/
4See http://www.theslot.com/captions.html and
http://www.thenewsmanual.net/ for tips on how to write
good captions.
in an article. A good caption must be succinct and
informative, clearly identify the subject of the pic-
ture, establish the picture?s relevance to the arti-
cle, provide context for the picture, and ultimately
draw the reader into the article. It is also worth
noting that journalists often write their own cap-
tions rather than simply extract sentences from the
document. In doing so they rely on general world
knowledge but also expertise in current affairs that
goes beyond what is described in the article or
shown in the picture.
4 Image Annotation
As mentioned earlier, our approach relies on an
image annotation model to provide description
keywords for the picture. Our experiments made
use of the probabilistic model presented in Feng
and Lapata (2010). The latter is well-suited to our
task as it has been developed with noisy, multi-
modal data sets in mind. The model is based on the
assumption that images and their surrounding text
are generated by mixtures of latent topics which
are inferred from a concatenated representation of
words and visual features.
Specifically, images are preprocessed so that
they are represented by word-like units. Lo-
cal image descriptors are computed using the
Scale Invariant Feature Transform (SIFT) algo-
rithm (Lowe, 1999). The general idea behind the
algorithm is to first sample an image with the
difference-of-Gaussians point detector at different
1241
scales and locations. Importantly, this detector is,
to some extent, invariant to translation, scale, ro-
tation and illumination changes. Each detected re-
gion is represented with a SIFT descriptor which
is a histogram of edge directions at different lo-
cations. Subsequently SIFT descriptors are quan-
tized into a discrete set of visual terms via a clus-
tering algorithm such as K-means.
The model thus works with a bag-of-words rep-
resentation and treats each article-image-caption
tuple as a single document dMix consisting of tex-
tual and visual words. Latent Dirichlet Allocation
(LDA, Blei et al 2003) is used to infer the latent
topics assumed to have generated dMix. The ba-
sic idea underlying LDA, and topic models in gen-
eral, is that each document is composed of a prob-
ability distribution over topics, where each topic
represents a probability distribution over words.
The document-topic and topic-word distributions
are learned automatically from the data and pro-
vide information about the semantic themes cov-
ered in each document and the words associated
with each semantic theme. The image annotation
model takes the topic distributions into account
when finding the most likely keywords for an im-
age and its associated document.
More formally, given an image-caption-
document tuple (I,C,D) the model finds the
subset of keywords WI (WI ? W ) which appro-
priately describe I. Assuming that keywords
are conditionally independent, and I, D are
represented jointly by dMix, the model estimates:
W ?I ? argmax
Wt
?
wt?Wt
P(wt |dMix) (1)
= argmax
Wt
?
wt?Wt
K
?
k=1
P(wt |zk)P(zk|dMix)
Wt denotes a set of description keywords (the sub-
script t is used to discriminate from the visual
words which are not part of the model?s output),
K the number of topics, P(wt |zk) the multimodal
word distributions over topics, and P(zk|dMix) the
estimated posterior of the topic proportions over
documents. Given an unseen image-document
pair and trained multimodal word distributions
over topics, it is possible to infer the posterior of
topic proportions over the new data by maximizing
the likelihood. The model delivers a ranked list of
textual words wt , the n-best of which are used as
annotations for image I.
It is important to note that the caption gener-
ation models we propose are not especially tied
to the above annotation model. Any probabilis-
tic model with broadly similar properties could
serve our purpose. Examples include PLSA-based
approaches to image annotation (e.g., Monay
and Gatica-Perez 2007) and correspondence LDA
(Blei and Jordan, 2003).
5 Extractive Caption Generation
Much work in summarization to date focuses on
sentence extraction where a summary is created
simply by identifying and subsequently concate-
nating the most important sentences in a docu-
ment. Without a great deal of linguistic analysis, it
is possible to create summaries for a wide range of
documents, independently of style, text type, and
subject matter. For our caption generation task, we
need only extract a single sentence. And our guid-
ing hypothesis is that this sentence must be max-
imally similar to the description keywords gener-
ated by the annotation model. We discuss below
different ways of operationalizing similarity.
Word Overlap Perhaps the simplest way of
measuring the similarity between image keywords
and document sentences is word overlap:
Overlap(WI,Sd) =
|WI ?Sd |
|WI ?Sd |
(2)
where WI is the set of keywords and Sd a sentence
in the document. The caption is then the sentence
that has the highest overlap with the keywords.
Cosine Similarity Word overlap is admittedly
a naive measure of similarity, based on lexical
identity. We can overcome this by representing
keywords and sentences in vector space (Salton
and McGill, 1983). The latter is a word-sentence
co-occurrence matrix where each row represents
a word, each column a sentence, and each en-
try the frequency with which the word appeared
within the sentence. More precisely matrix cells
are weighted by their tf-idf values. The similarity
of the vectors representing the keywords
??
WI and
document sentence
??
Sd can be quantified by mea-
suring the cosine of their angle:
sim(
??
WI,
??
Sd) =
??
WI ?
??
Sd
|
?????
WI||
??
Sd |
(3)
Probabilistic Similarity Recall that the back-
bone of our image annotation model is a topic
model with images and documents represented as
a probability distribution over latent topics. Un-
der this framework, the similarity between an im-
1242
age and a sentence can be broadly measured by the
extent to which they share the same topic distribu-
tions (Steyvers and Griffiths, 2007). For example,
we may use the KL divergence to measure the dif-
ference between the distributions p and q:
D(p,q) =
K
?
j=1
p j log2
p j
q j
(4)
where p and q are shorthand for the image
topic distribution PdMix and sentence topic distri-
bution PSd , respectively. When doing inference on
the document sentence, we also take its neighbor-
ing sentences into account to avoid estimating in-
accurate topic proportions on short sentences.
The KL divergence is asymmetric and in many
applications, it is preferable to apply a symmet-
ric measure such as the Jensen Shannon (JS) di-
vergence. The latter measures the ?distance? be-
tween p and q through (p+q)2 , the average of p
and q:
JS(p,q) =
1
2
[
D(p,
(p+q)
2
)+D(q,
(p+q)
2
)
]
(5)
6 Abstractive Caption Generation
Although extractive methods yield grammatical
captions and require relatively little linguistic
analysis, there are a few caveats to consider.
Firstly, there is often no single sentence in the doc-
ument that uniquely describes the image?s content.
In most cases the keywords are found in the doc-
ument but interspersed across multiple sentences.
Secondly, the selected sentences make for long
captions (sometimes longer than the average doc-
ument sentence), are not concise and overall not
as catchy as human-written captions. For these
reasons we turn to abstractive caption generation
and present models based on single words but also
phrases.
Word-based Model Our first abstractive model
builds on and extends a well-known probabilistic
model of headline generation (Banko et al, 2000).
The task is related to caption generation, the aim is
to create a short, title-like headline for a given doc-
ument, without however taking visual information
into account. Like captions, headlines have to be
catchy to attract the reader?s attention.
Banko et al (2000) propose a bag-of-words
model for headline generation. It consists of con-
tent selection and surface realization components.
Content selection is modeled as the probability of
a word appearing in the headline given the same
word appearing in the corresponding document
and is independent from other words in the head-
line. The likelihood of different surface realiza-
tions is estimated using a bigram model. They also
take the distribution of the length of the headlines
into account in an attempt to bias the model to-
wards generating concise output:
P(w1,w2, ...,wn) =
n
?
i=1
P(wi ? H|wi ? D) (6)
?P(len(H) = n)
?
n
?
i=2
P(wi|wi?1)
where wi is a word that may appear in head-
line H, D the document being summarized,
and P(len(H) = n) a headline length distribution
model.
The above model can be easily adapted to the
caption generation task. Content selection is now
the probability of a word appearing in the cap-
tion given the image and its associated document
which we obtain from the output of our image an-
notation model (see Section 4). In addition we re-
place the bigram surface realizer with a trigram:
P(w1,w2, ...,wn) =
n
?
i=1
P(wi ?C|I,D) (7)
?P(len(C) = n)
?
n
?
i=3
P(wi|wi?1,wi?2)
where C is the caption, I the image, D the accom-
panying document, and P(wi ? C|I,D) the image
annotation probability.
Despite its simplicity, the caption generation
model in (7) has a major drawback. The content
selection component will naturally tend to ignore
function words, as they are not descriptive of the
image?s content. This will seriously impact the
grammaticality of the generated captions, as there
will be no appropriate function words to glue the
content words together. One way to remedy this
is to revert to a content selection model that ig-
nores the image and simply estimates the prob-
ability of a word appearing in the caption given
the same word appearing in the document. At the
same time we modify our surface realization com-
ponent so that it takes note of the image annotation
probabilities. Specifically, we use an adaptive lan-
guage model (Kneser et al, 1997) that modifies an
1243
n-gram model with local unigram probabilities:
P(w1,w2, ...,wn) =
n
?
i=1
P(wi ?C|wi ? D) (8)
?P(len(C) = n)
?
n
?
i=3
Padap(wi|wi?1,wi?2)
where P(wi ?C|wi ?D) is the probability of wi ap-
pearing in the caption given that it appears in
the document D, and Padap(wi|wi?1,wi?2) the lan-
guage model adapted with probabilities from our
image annotation model:
Padap(w|h) =
?(w)
z(h)
Pback(w|h) (9)
?(w)? (
Padap(w)
Pback(w)
)? (10)
z(h) =?
w
?(w) ?Pback(w|h) (11)
where Pback(w|h) is the probability of w given
the history h of preceding words (i.e., the orig-
inal trigram model), Padap(w) the probability
of w according to the image annotation model,
Pback(w) the probability of w according to the orig-
inal model, and ? a scaling parameter.
Phrase-based Model The model outlined in
equation (8) will generate captions with function
words. However, there is no guarantee that these
will be compatible with their surrounding context
or that the caption will be globally coherent be-
yond the trigram horizon. To avoid these prob-
lems, we turn our attention to phrases which are
naturally associated with function words and can
potentially capture long-range dependencies.
Specifically, we obtain phrases from the out-
put of a dependency parser. A phrase is sim-
ply a head and its dependents with the exception
of verbs, where we record only the head (other-
wise, an entire sentence could be a phrase). For
example, from the first sentence in Table 1 (first
row, left document) we would extract the phrases:
thousands of Tongans, attended, the funeral, King
Taufa?ahau Tupou IV, last week, at the age, died,
and so on. We only consider dependencies whose
heads are nouns, verbs, and prepositions, as these
constitute 80% of all dependencies attested in our
caption data. We define a bag-of-phrases model
for caption generation by modifying the content
selection and caption length components in equa-
tion (8) as follows:
P(?1,?2, ...,?m) ?
m
?
j=1
P(? j ?C|? j ? D) (12)
?P(len(C) =
m
?
j=1
len(? j))
?
?mj=1 len(? j)
?
i=3
Padap(wi|wi?1,wi?2)
Here, P(? j ?C|? j ? D) models the probability of
phrase ? j appearing in the caption given that it also
appears in the document and is estimated as:
P(? j ?C|? j ? D) = ?
w j?? j
P(w j ?C|w j ? D) (13)
where w j is a word in the phrase ? j.
One problem with the models discussed thus
far is that words or phrases are independent of
each other. It is up to the trigram model to en-
force coarse ordering constraints. These may be
sufficient when considering isolated words, but
phrases are longer and their combinations are sub-
ject to structural constraints that are not captured
by sequence models. We therefore attempt to take
phrase attachment constraints into account by es-
timating the probability of phrase ? j attaching to
the right of phrase ?i as:
P(? j|?i)= ?
wi??i
?
w j?? j
p(w j|wi) (14)
=
1
2 ?wi??i
?
w j?? j
{
f (wi,w j)
f (wi,?)
+
f (wi,w j)
f (?,w j)
}
where p(w j|wi) is the probability of a phrase con-
taining word w j appearing to the right of a phrase
containing word wi, f (wi,w j) indicates the num-
ber of times wi and w j are adjacent, f (wi,?) is
the number of times wi appears on the left of any
phrase, and f (?,wi) the number of times it ap-
pears on the right.5
After integrating the attachment probabilities
into equation (12), the caption generation model
becomes:
P(?1,?2, ...,?m)?
m
?
j=1
P(? j ?C|? j ? D) (15)
?
m
?
j=2
P(? j|? j?1)
?P(len(C) = ?mj=1 len(? j))
??
m
?
j=1
len(? j)
i=3 Padap(wi|wi?1,wi?2)
5Equation (14) is smoothed to avoid zero probabilities.
1244
On the one hand, the model in equation (15) takes
long distance dependency constraints into ac-
count, and has some notion of syntactic structure
through the use of attachment probabilities. On
the other hand, it has a primitive notion of caption
length estimated by P(len(C) = ?mj=1 len(? j)) and
will therefore generate captions of the same
(phrase) length. Ideally, we would like the model
to vary the length of its output depending on the
chosen context. However, we leave this to future
work.
Search To generate a caption it is neces-
sary to find the sequence of words that maxi-
mizes P(w1,w2, ...,wn) for the word-based model
(equation (8)) and P(?1,?2, ...,?m) for the
phrase-based model (equation (15)). We rewrite
both probabilities as the weighted sum of their log
form components and use beam search to find a
near-optimal sequence. Note that we can make
search more efficient by reducing the size of the
document D. Using one of the models from Sec-
tion 5, we may rank its sentences in terms of
their relevance to the image keywords and con-
sider only the n-best ones. Alternatively, we could
consider the single most relevant sentence together
with its surrounding context under the assumption
that neighboring sentences are about the same or
similar topics.
7 Experimental Setup
In this section we discuss our experimental design
for assessing the performance of the caption gen-
eration models presented above. We give details
on our training procedure, parameter estimation,
and present the baseline methods used for com-
parison with our models.
Data All our experiments were conducted on
the corpus created by Feng and Lapata (2008),
following their original partition of the data
(2,881 image-caption-document tuples for train-
ing, 240 tuples for development and 240 for test-
ing). Documents and captions were parsed with
the Stanford parser (Klein and Manning, 2003) in
order to obtain dependencies for the phrase-based
abstractive model.
Model Parameters For the image annotation
model we extracted 150 (on average) SIFT fea-
tures which were quantized into 750 visual
terms. The underlying topic model was trained
with 1,000 topics using only content words
(i.e., nouns, verbs, and adjectives) that appeared
no less than five times in the corpus. For all
models discussed here (extractive and abstractive)
we report results with the 15 best annotation key-
words. For the abstractive models, we used a
trigram model trained with the SRI toolkit on a
newswire corpus consisting of BBC and Yahoo!
news documents (6.9 M words). The attachment
probabilities (see equation (14)) were estimated
from the same corpus. We tuned the caption
length parameter on the development set using a
range of [5,14] tokens for the word-based model
and [2,5] phrases for the phrase-based model. Fol-
lowing Banko et al (2000), we approximated the
length distribution with a Gaussian. The scaling
parameter ? for the adaptive language model was
also tuned on the development set using a range
of [0.5,0.9]. We report results with ? set to 0.5.
For the abstractive models the beam size was set
to 500 (with at least 50 states for the word-based
model). For the phrase-based model, we also ex-
perimented with reducing the search scope, ei-
ther by considering only the n most similar sen-
tences to the keywords (range [2,10]), or simply
the single most similar sentence and its neighbors
(range [2,5]). The former method delivered better
results with 10 sentences (and the KL divergence
similarity function).
Evaluation We evaluated the performance of
our models automatically, and also by eliciting hu-
man judgments. Our automatic evaluation was
based on Translation Edit Rate (TER, Snover et al
2006), a measure commonly used to evaluate the
quality of machine translation output. TER is de-
fined as the minimum number of edits a human
would have to perform to change the system out-
put so that it exactly matches a reference transla-
tion. In our case, the original captions written by
the BBC journalists were used as reference:
TER(E,Er) =
Ins+Del+Sub+Shft
Nr
(16)
where E is the hypothetical system output, Er the
reference caption, and Nr the reference length.
The number of possible edits include insertions
(Ins), deletions (Del), substitutions (Sub) and
shifts (Shft). TER is similar to word error rate,
the only difference being that it allows shifts. A
shift moves a contiguous sequence to a different
location within the the same system output and is
counted as a single edit. The perfect TER score
is 0, however note that it can be higher than 1 due
to insertions. The minimum translation edit align-
1245
Model TER AvgLen
Lead sentence 2.12? 21.0
Word Overlap 2.46?? 24.3
Cosine 2.26? 22.0
KL Divergence 1.77?? 18.4
JS Divergence 1.77?? 18.6
Abstract Words 1.11?? 10.0
Abstract Phrases 1.06?? 10.1
Table 2: TER results for extractive, abstractive
models, and lead sentence baseline; ?: sig. dif-
ferent from lead sentence; ?: sig. different from
KL and JS divergence.
ment is usually found through beam search. We
used TER to compare the output of our extractive
and abstractive models and also for parameter tun-
ing (see the discussion above).
In our human evaluation study participants were
presented with a document, an associated image,
and its caption, and asked to rate the latter on two
dimensions: grammaticality (is the sentence flu-
ent or word salad?) and relevance (does it de-
scribe succinctly the content of the image and doc-
ument?). We used a 1?7 rating scale, participants
were encouraged to give high ratings to captions
that were grammatical and appropriate descrip-
tions of the image given the accompanying docu-
ment. We randomly selected 12 document-image
pairs from the test set and generated captions for
them using the best extractive system, and two ab-
stractive systems (word-based and phrase-based).
We also included the original human-authored
caption as an upper bound. We collected ratings
from 23 unpaid volunteers, all self reported native
English speakers. The study was conducted over
the Internet.
8 Results
Table 2 reports our results on the test set us-
ing TER. We compare four extractive models
based on word overlap, cosine similarity, and two
probabilistic similarity measures, namely KL and
JS divergence and two abstractive models based
on words (see equation (8)) and phrases (see equa-
tion (15)). We also include a simple baseline that
selects the first document sentence as a caption
and show the average caption length (AvgLen) for
each model. We examined whether performance
differences among models are statistically signifi-
cant, using the Wilcoxon test.
Model Grammaticality Relevance
KL Divergence 6.42?? 4.10??
Abstract Words 2.08? 3.20?
Abstract Phrases 4.80? 4.96?
Gold Standard 6.39?? 5.55?
Table 3: Mean ratings on caption output elicited
by humans; ?: sig. different from word-
based abstractive system; ?: sig. different from
phrase-based abstractive system.
As can be seen the probabilistic models (KL and
JS divergence) outperform word overlap and co-
sine similarity (all differences are statistically sig-
nificant, p < 0.01).6 They make use of the same
topic model as the image annotation model, and
are thus able to select sentences that cover com-
mon content. They are also significantly better
than the lead sentence which is a competitive base-
line. It is well known that news articles are written
so that the lead contains the most important infor-
mation in a story.7 This is an encouraging result
as it highlights the importance of the visual infor-
mation for the caption generation task. In general,
word overlap is the worst performing model which
is not unexpected as it does not take any lexical
variation into account. Cosine is slightly better
but not significantly different from the lead sen-
tence. The abstractive models obtain the best TER
scores overall, however they generate shorter cap-
tions in comparison to the other models (closer to
the length of the gold standard) and as a result TER
treats them favorably, simply because the number
of edits is less. For this reason we turn to the re-
sults of our judgment elicitation study which as-
sesses in more detail the quality of the generated
captions.
Recall that participants judge the system out-
put on two dimensions, grammaticality and rele-
vance. Table 3 reports mean ratings for the out-
put of the extractive system (based on the KL di-
vergence), the two abstractive systems, and the
human-authored gold standard caption. We per-
formed an Analysis of Variance (ANOVA) to ex-
amine the effect of system type on the generation
task. Post-hot Tukey tests were carried out on the
mean of the ratings shown in Table 3 (for gram-
maticality and relevance).
6We also note that mean length differences are not signif-
icant among these models.
7As a rule of thumb the lead should answer most or all of
the five W?s (who, what, when, where, why).
1246
G: King Tupou, who was 88, died a week ago.
KL: Last year, thousands of Tongans took part in unprece-
dented demonstrations to demand greater democracy
and public ownership of key national assets.
AW : King Toupou IV died at the age of Tongans last week.
AP: King Toupou IV died at the age of 88 last week.
G: Cadbury will increase its contamination testing levels.
KL: Contaminated Cadbury?s chocolate was the most
likely cause of an outbreak of salmonella poisoning,
the Health Protection Agency has said.
AW : Purely dairy milk buttons Easter had agreed to work
has caused.
AP: The 105g dairy milk buttons Easter egg affected by
the recall.
G: Satellite instruments can distinguish ?old? Arctic ice
from ?new?.
KL: So a planet with less ice warms faster, potentially turn-
ing the projected impacts of global warming into real-
ity sooner than anticipated.
AW : Dr less winds through ice cover all over long time
when.
AP: The area of the Arctic covered in Arctic sea ice cover.
G: Children were found to be far more internet-wise than
parents.
KL: That?s where parents come in.
AW : The survey found a third of children are about mobile
phones.
AP: The survey found a third of children in the driving
seat.
Table 4: Captions written by humans (G) and gen-
erated by extractive (KL), word-based abstractive
(AW ), and phrase-based extractive (AP systems).
The word-based system yields the least gram-
matical output. It is significantly worse than the
phrase-based abstractive system (? < 0.01), the
extractive system (? < 0.01), and the gold stan-
dard (? < 0.01). Unsurprisingly, the phrase-based
system is significantly less grammatical than the
gold standard and the extractive system, whereas
the latter is perceived as equally grammatical as
the gold standard (the difference in the means is
not significant). With regard to relevance, the
word-based system is significantly worse than the
phrase-based system, the extractive system, and
the gold-standard. Interestingly, the phrase-based
system performs on the same level with the hu-
man gold standard (the difference in the means is
not significant) and significantly better than the ex-
tractive system. Overall, the captions generated by
the phrase-based system, capture the same content
as the human-authored captions, even though they
tend to be less grammatical. Examples of system
output for the image-document pairs shown in Ta-
ble 1 are given in Table 4 (the first row corresponds
to the left picture (top row) in Table 1, the second
row to the right picture, and so on).
9 Conclusions
We have presented extractive and abstractive mod-
els that generate image captions for news articles.
A key aspect of our approach is to allow both
the visual and textual modalities to influence the
generation task. This is achieved through an im-
age annotation model that characterizes pictures
in terms of description keywords that are subse-
quently used to guide the caption generation pro-
cess. Our results show that the visual information
plays an important role in content selection. Sim-
ply extracting a sentence from the document often
yields an inferior caption. Our experiments also
show that a probabilistic abstractive model defined
over phrases yields promising results. It generates
captions that are more grammatical than a closely
related word-based system and manages to capture
the gist of the image (and document) as well as the
captions written by journalists.
Future extensions are many and varied. Rather
than adopting a two-stage approach, where the im-
age processing and caption generation are carried
out sequentially, a more general model should in-
tegrate the two steps in a unified framework. In-
deed, an avenue for future work would be to de-
fine a phrase-based model for both image annota-
tion and caption generation. We also believe that
our approach would benefit from more detailed
linguistic and non-linguistic information. For in-
stance, we could experiment with features related
to document structure such as titles, headings, and
sections of articles and also exploit syntactic infor-
mation more directly. The latter is currently used
in the phrase-based model by taking attachment
probabilities into account. We could, however, im-
prove grammaticality more globally by generating
a well-formed tree (or dependency graph).
References
Banko, Michel, Vibhu O. Mittal, and Micheael J.
Witbrock. 2000. Headline generation based on
statistical translation. In Proceedings of the 38th
Annual Meeting on Association for Computa-
tional Linguistics. Hong Kong, pages 318?325.
Barnard, Kobus, Pinar Duygulu, David Forsyth,
Nando de Freitas, David Blei, and Michael
Jordan. 2002. Matching words and pictures.
Journal of Machine Learning Research 3:1107?
1135.
Blei, David and Michael Jordan. 2003. Modeling
annotated data. In Proceedings of the 26th An-
1247
nual International ACM SIGIR Conference on
Research and Development in Information Re-
trieval. Toronto, ON, pages 127?134.
Blei, David, Andrew Ng, and Michael Jordan.
2003. Latent Dirichlet alocation. Journal of
Machine Learning Research 3:993?1022.
Corio, Marc and Guy Lapalme. 1999. Generation
of texts for information graphics. In Proceed-
ings of the 7th European Workshop on Natural
Language Generation. Toulouse, France, pages
49?58.
Dorr, Bonnie, David Zajic, and Richard Schwartz.
2003. Hedge trimmer: A parse-and-trim ap-
proach to headline generation. In Proceed-
ings of the HLT-NAACL 2003 Workshop on Text
Summarization. Edmonton, Canada, pages 1?8.
Duygulu, Pinar, Kobus Barnard, Nando de Freitas,
and David Forsyth. 2002. Object recognition as
machine translation: Learning a lexicon for a
fixed image vocabulary. In Proceedings of the
7th European Conference on Computer Vision.
Copenhagen, Denmark, pages 97?112.
Elzer, Stephanie, Sandra Carberry, Ingrid Zuker-
man, Daniel Chester, Nancy Green, , and Seniz
Demir. 2005. A probabilistic framework for rec-
ognizing intention in information graphics. In
Proceedings of the 19th International Confer-
ence on Artificial Intelligence. Edinburgh, Scot-
land, pages 1042?1047.
Fasciano, Massimo and Guy Lapalme. 2000. In-
tentions in the coordinated generation of graph-
ics and text from tabular data. Knowledge In-
formation Systems 2(3):310?339.
Feiner, Steven and Kathleen McKeown. 1990. Co-
ordinating text and graphics in explanation gen-
eration. In Proceedings of National Conference
on Artificial Intelligence. Boston, MA, pages
442?449.
Feng, Shaolei Feng, Victor Lavrenko, and R Man-
matha. 2004. Multiple Bernoulli relevance
models for image and video annotation. In
Proceedings of the International Conference
on Computer Vision and Pattern Recognition.
Washington, DC, pages 1002?1009.
Feng, Yansong and Mirella Lapata. 2008. Au-
tomatic image annotation using auxiliary text
information. In Proceedings of the 46th An-
nual Meeting of the Association of Computa-
tional Linguistics: Human Language Technolo-
gies. Columbus, OH, pages 272?280.
Feng, Yansong and Mirella Lapata. 2010. Topic
models for image annotation and text illustra-
tion. In Proceedings of the 11th Annual Con-
ference of the North American Chapter of the
Association for Computational Linguistics. Los
Angeles, LA.
Ferres, Leo, Avi Parush, Shelley Roberts, and
Gitte Lindgaard. 2006. Helping people with
visual impairments gain access to graphical in-
formation through natural language: The graph
system. In Proceedings of 11th International
Conference on Computers Helping People with
Special Needs. Linz, Austria, pages 1122?1130.
He?de, Patrick, Pierre Allain Moe?llic, Joe?l Bour-
geoys, Magali Joint, and Corinne Thomas.
2004. Automatic generation of natural lan-
guage descriptions for images. In Proceed-
ings of Computer-Assisted Information Re-
trieval (Recherche d?Information et ses Appli-
cations Ordinateur) (RIAO). Avignon, France.
Jin, Rong and Alexander G. Hauptmann. 2002. A
new probabilistic model for title generation. In
Proceedings of the 19th International Confer-
ence on Computational linguistics. Taipei, Tai-
wan, pages 1?7.
Klein, Dan and Christopher D. Manning. 2003.
Accurate unlexicalized parsing. In Proceedings
of the 41st Annual Meeting of the Association
of Computational Linguistics. Sapporo, Japan,
pages 423?430.
Kneser, Reinhard, Jochen Peters, and Dietrich
Klakow. 1997. Language model adaptation
using dynamic marginals. In Proceedings of
5th European Conference on Speech Commu-
nication and Technology. Rhodes, Greece, vol-
ume 4, pages 1971?1974.
Kojima, Atsuhiro, Mamoru Takaya, Shigeki Aoki,
Takao Miyamoto, and Kunio Fukunaga. 2008.
Recognition and textual description of human
activities by mobile robot. In Proceedings of
the 3rd International Conference on Innova-
tive Computing Information and Control. IEEE
Computer Society, Washington, DC, pages 53?
56.
Kojima, Atsuhiro, Takeshi Tamura, and Kunio
Fukunaga. 2002. Natural language description
of human activities from video images based
on concept hierarchy of actions. International
Journal of Computer Vision 50(2):171?184.
Lavrenko, Victor, R. Manmatha, and Jiwoon Jeon.
2003. A model for learning the semantics of
1248
pictures. In Proceedings of the 16th Conference
on Advances in Neural Information Processing
Systems. Vancouver, BC.
Lowe, David G. 1999. Object recognition from
local scale-invariant features. In Proceedings of
International Conference on Computer Vision.
IEEE Computer Society, pages 1150?1157.
Mittal, Vibhu O., Johanna D. Moore, Giuseppe
Carenini, and Steven Roth. 1998. Describing
complex charts in natural language: A caption
generation system. Computational Linguistics
24:431?468.
Monay, Florent and Daniel Gatica-Perez. 2007.
Modeling semantic aspects for cross-media
image indexing. IEEE Transactions on
Pattern Analysis and Machine Intelligence
29(10):1802?1817.
Salton, Gerard and M.J. McGill. 1983. In-
troduction to Modern Information Retrieval.
McGraw-Hill, New York.
Smeulders, Arnols W.M., Marcel Worring, Si-
mone Santini, Amarnath Gupta, and Ramesh
Jain. 2000. Content-based image retrieval at
the end of the early years. IEEE Transactions
on Pattern Analysis and Machine Intelligence
22(12):1349?1380.
Snover, Matthew, Bonnie Dorr, Richard Schwartz,
Linnea Micciulla, and John Makhoul. 2006. A
study of translation edit rate with targeted hu-
man annotation. In Proceedings of the 7th Con-
ference of the Association for Machine Trans-
lation in the Americas. Cambridge, pages 223?
231.
Steyvers, Mark and Tom Griffiths. 2007. Proba-
bilistic topic models. In T. Landauer, D. Mc-
Namara, S Dennis, and W Kintsch, editors, A
Handbook of Latent Semantic Analysis, Psy-
chology Press.
Vailaya, Aditya, Ma?rio A. T. Figueiredo, Anil K.
Jain, and Hong-Jiang Zhang. 2001. Image clas-
sification for content-based indexing. IEEE
Transactions on Image Processing 10:117?130.
von Ahn, Luis and Laura Dabbish. 2004. Labeling
images with a computer game. In ACM Confer-
ence on Human Factors in Computing Systems.
New York, NY, pages 319?326.
Wang, Chong, David Blei, and Li Fei-Fei. 2009.
Simultaneous image classification and annota-
tion. In Proceedings of the International Con-
ference on Computer Vision and Pattern Recog-
nition. Miami, FL, pages 1903?1910.
Yao, Benjamin, Xiong Yang, Liang Lin, Mun Wai
Lee, and Song chun Zhu. 2009. I2t: Image pars-
ing to text description. Proceedings of IEEE (in-
vited for the special issue on Internet Vision) .
1249
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 818?827,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
Encoding Relation Requirements for Relation Extraction via Joint
Inference
Liwei Chen
1
, Yansong Feng
?1
, Songfang Huang
2
, Yong Qin
2
and Dongyan Zhao
1
1
ICST, Peking University, Beijing, China
2
IBM China Research Lab, Beijing, China
chenliwei,fengyansong,zhaodongyan@pku.edu.cn
huangsf,qinyong@cn.ibm.com
Abstract
Most existing relation extraction models
make predictions for each entity pair lo-
cally and individually, while ignoring im-
plicit global clues available in the knowl-
edge base, sometimes leading to conflicts
among local predictions from different en-
tity pairs. In this paper, we propose
a joint inference framework that utilizes
these global clues to resolve disagree-
ments among local predictions. We ex-
ploit two kinds of clues to generate con-
straints which can capture the implicit type
and cardinality requirements of a relation.
Experimental results on three datasets, in
both English and Chinese, show that our
framework outperforms the state-of-the-
art relation extraction models when such
clues are applicable to the datasets. And,
we find that the clues learnt automatically
from existing knowledge bases perform
comparably to those refined by human.
1 Introduction
Identifying predefined kinds of relationship be-
tween pairs of entities is crucial for many knowl-
edge base related applications(Suchanek et al,
2013). In the literature, relation extraction (RE) is
usually investigated in a classification style, where
relations are simply treated as isolated class labels,
while their definitions or background information
are sometimes ignored. Take the relation Capital
as an example, we can imagine that this relation
will expect a country as its subject and a city as
object, and in most cases, a city can be the capital
of only one country. All these clues are no doubt
helpful, for instance, Yao et al (2010) explicitly
modeled the expected types of a relation?s argu-
ments with the help of Freebase?s type taxonomy
and obtained promising results for RE.
?
Yansong Feng is the corresponding author.
However, properly capturing and utilizing such
typing clues are not trivial. One of the hurdles here
is the lack of off-the-shelf resources and such clues
often have to be coded by human experts. Many
knowledge bases do not have a well-defined typing
system, let alne fine-grained typing taxonomies
with corresponding type recognizers, which are
crucial to explicitly model the typing requirements
for arguments of a relation, but rather expensive
and time-consuming to collect. Similarly, the car-
dinality requirements of arguments, e.g., a person
can have only one birthdate and a city can only be
labeled as capital of one country, should be con-
sidered as a strong indicator to eliminate wrong
predictions, but has to be coded manually as well.
On the other hand, most previous relation ex-
tractors process each entity pair (we will use en-
tity pair and entity tuple exchangeably in the rest
of the paper) locally and individually, i.e., the ex-
tractor makes decisions solely based on the sen-
tences containing the current entity pair and ig-
nores other related pairs, therefore has difficulties
to capture possible disagreements among different
entity pairs. However, when looking at the output
of a multi-class relation predictor globally, we can
easily find possible incorrect predictions such as a
university locates in two different cities, two dif-
ferent cities have been labeled as capital for one
country, a country locates in a city and so on.
In this paper, we will address how to derive and
exploit two categories of these clues: the expected
types and the cardinality requirements of a rela-
tion?s arguments, in the scenario of relation extrac-
tion. We propose to perform joint inference upon
multiple local predictions by leveraging implicit
clues that are encoded with relation specific re-
quirements and can be learnt from existing knowl-
edge bases. Specifically, the joint inference frame-
work operates on the output of a sentence level re-
lation extractor as input, derives 5 types of con-
straints from an existing KB to implicitly capture
818
the expected type and cardinality requirements for
a relation?s arguments, and jointly resolve the dis-
agreements among candidate predictions. We for-
malize this procedure as a constrained optimiza-
tion problem, which can be solved by many opti-
mization frameworks. We use integer linear pro-
gramming (ILP) as the solver and evaluate our
framework on English and Chinese datasets. The
experimental results show that our framework per-
forms better than the state-of-the-art approaches
when such clues are applicable to the datasets. We
also show that the automatically learnt clues per-
form comparably to those refined manually.
In the rest of the paper, we first review related
work in Section 2, and in Section 3, we describe
our framework in detail. Experimental setup and
results are discussed in Section 4. We conclude
this paper in Section 5.
2 Related Work
Since traditional supervised relation extraction
methods (Soderland et al, 1995; Zhao and Gr-
ishman, 2005) require manual annotations and are
often domain-specific, nowadays many efforts fo-
cus on semi-supervised or unsupervised methods
(Banko et al, 2007; Fader et al, 2011). Distant
supervision (DS) is a semi-supervised RE frame-
work and has attracted many attentions (Bunescu,
2007; Mintz et al, 2009; Yao et al, 2010; Sur-
deanu et al, 2010; Hoffmann et al, 2011; Sur-
deanu et al, 2012). DS approaches can predict
canonicalized (predefined in KBs) relations for
large amount of data and do not need much hu-
man involvement. Since the automatically gener-
ated training datasets in DS often contain noises,
there are also research efforts focusing on reduc-
ing the noisy labels in the training data (Takamatsu
et al, 2012). To bridge the gaps between the rela-
tions extracted from open information extraction
and the canonicalized relations in KBs, Yao et al
(2012) and Riedel et al (2013) propose a universal
schema which is a union of KB schemas and nat-
ural language patterns, making it possible to in-
tegrate the unlimited set of uncanonicalized rela-
tions in open settings with the relations in existing
KBs.
As far as we know, few works have managed
to take the relation specific requirements for ar-
guments into account, and most existing works
make predictions locally and individually. The
MultiR system allows entity tuples to have more
than one relations, but still predicts each entity
tuple locally (Hoffmann et al, 2011). Surdeanu
et al (2012) propose a two-layer multi-instance
multi-label (MIML) framework to capture the de-
pendencies among relations. The first layer is a
multi-class classifier making local predictions for
single sentences, the output of which are aggre-
gated by the second layer into the entity pair level.
Their approach only captures relation dependen-
cies, while we learn implicit relation backgrounds
from knowledge bases, including argument type
and cardinality requirements. Riedel et al (2013)
propose to use latent vectors to estimate the pref-
erences between relations and entities. These can
be considered as the latent type information of the
relations? arguments, which is learnt from various
data sources. In contrast, our approach learn im-
plicit clues from existing KBs, and jointly opti-
mize local predictions among different entity tu-
ples to capture both relation argument type clues
and cardinality clues. Li et al (2011) and Li et al
(2013) use co-occurring statistics among relations
or events to jointly improve information extrac-
tion performances in ACE tasks, while we mine
existing KBs to collect global clues to solve lo-
cal conflicts and find the optimal aggregation as-
signments, regarding existing knowledge facts. de
Lacalle and Lapata (2013) encode general domain
knowledge as FOL rules in a topic model while
our instantiated constraints are directly operated in
an ILP model. Zhang et al (2013) utilize relation
cardinality to create negative samples for distant
supervision while we use both implicit type clues
and relation cardinality expectations to discover
possible inconsistencies among local predictions.
3 The Framework
Our framework takes a set of entity pairs and their
supporting sentences as its input. We first train
a preliminary sentence level extractor which can
output confidence scores for its predictions, e.g.,
a maximum entropy or logistic regression model,
and use this local extractor to produce local predic-
tions. In order to implicitly capture the expected
type and cardinality requirements for a relation?s
arguments, we derive two kinds of clues from an
existing KB, which are further utilized to discover
the disagreements among local candidate predic-
tions. Our objective is to maximize the overall
confidence of all the selected predictions.
819
3.1 Generating Candidate Relations
Since we will focus on the open domain relation
extraction, we still follow the distant supervision
paradigm to collect our training data guided by
a KB, and train the local extractor accordingly.
Specifically, we train a sentence level extractor us-
ing the maximum entropy model. Given a sen-
tence containing an entity pair, the model will
output the confidence of this sentence represent-
ing certain relationship (from a predefined relation
set) between the entity pair. Formally R repre-
sents the relation set we are working on, T is the
set of entity tuples that we will predict in the test
set.
Keep in mind that our local extractor is trained
on noisy training data, which, we admit, is not
fully reliable. As we observed in a pilot experi-
ment that there is a good chance that the predic-
tions ranked in the second or third may still be
correct, we select top three predictions as the can-
didate relations for each mention in order to intro-
duce more potentially correct output.
On the other hand, we should discard the pre-
dictions whose confidences are too low to be true,
where we set up a threshold of 0.1. For a tuple t,
we obtain its candidate relation set by combining
the candidate relations of all its mentions, and rep-
resent it as R
t
. For a candidate relation r ? R
t
and
a tuple t, we define M
r
t
as all t?s mentions whose
candidate relations contain r. Now the confidence
score of a relation r ? R
t
being assigned to tuple
t can be calculated as:
conf(t, r) =
?
m?M
r
t
MEscore(m, r) (1)
where MEscore(m, r) is the confidence of mention
m representing relation r output by our prelimi-
nary extractor.
Traditionally, both lexical features and syntac-
tic features are used in relation extraction. Lexi-
cal features are the word chains between the sub-
jects and objects in the sentences, while syntactic
features are the dependency paths from the sub-
jects to the objects on the dependency graphs of
the supporting sentences. However, lexical fea-
tures are usually too specific to frequently appear
in the test data, while the reliability of syntactic
features depends heavily on the quality of depen-
dency parsing tools. Generally, we expect more
potentially correct relations to be put into the can-
didate relation set for further consideration. So in
conflict
conflict
conflict
conflict
Capital: 0.5
LargestCity: 0.4
LocationCity: 0.05
1
1->0
1
USA, New York
LocationCity: 0.8
FoundationPlace: 0.15
1
1
New York University, New York
Capital: 0.95
LocationCity: 0.03
1
1->0
USA, Washington D.C.
Nationality: 0.7
BirthPlace: 0.2
1
1
Richard Fuld,USA
Capital: 0.3 1->0
Germany, Washington D.C.
conflict
LocationCountry: 0.5
LocationCity: 0.3
1->0
1
Columbia University, New York
conflict
Figure 1: The different types of disagreements we
will investigate in the candidate relations. The
clues of detecting these inconsistencies can be
learnt from a knowledge base.
addition to lexical and syntactic features, we also
use n-gram features to train our preliminary rela-
tion extraction model. N-gram features are consid-
ered as more ambiguous compared to traditional
lexical and syntactic features, and may introduce
incorrect predictions, thus improving the recall at
the cost of precision.
3.2 Disagreements among the Candidates
The candidate relations we obtained in the pre-
vious subsection inevitably include many incor-
rect predictions. Ideally we should discard those
wrong predictions to produce more accurate re-
sults.
As discussed earlier, we will exploit from the
knowledge base two categories of clues that im-
plicitly capture relations? backgrounds: their ex-
pected argument types and argument cardinalities,
based on which we can discover two categories
of disagreements among the candidate predictions,
summarized as argument type inconsistencies and
violations of arguments? uniqueness, which have
been rarely considered before. We will discuss
them in detail, and describe how to learn the clues
from a KB afterwards.
Implicit Argument Types Inconsistencies:
Generally, the argument types of the correct
predictions should be consistent with each other.
Given a relation, its arguments sometimes are
required to be certain types of entities. For
example, in Figure 1, the relation LargestCity
restricts its subject to be either countries or states,
and its object to be cities. If the predictions
among different entity tuples require the same
entity to belong to different types, we call this
820
an argument type inconsistency. Take <USA,
New York> and <USA, Washington D.C.> as an
example. In Figure 1, <USA, New York> has
a candidate relation LargestCity which restricts
USA to be either countries or states, while <USA,
Washington D.C.> has a prediction LocationCity
which indicates a disagreement in terms of USA?s
type because the latter prediction expects USA to
be an organization located in a city. This warns
that at least one of the two candidate relations is
incorrect.
The previous scenario shows that the subjects
of two candidate relations may disagree with each
other. From Figure 1, we can observe two more
situations: the first one is that the objects of
the two candidate relations are inconsistent with
each other, for example <New York University,
New York> with the prediction LocationCity and
<Columbia University, New York> with the pre-
diction LocationCountry. The second one is
that the subject of one candidate relation do not
agree with another prediction?s object, for exam-
ple <Richard Fuld, USA> with the prediction Na-
tionality and <USA, New York> with the pre-
diction LocationCity. Although we have not as-
signed explicit types to these entities, we can still
exploit the inconsistencies implicitly with the help
of shared entities. Note that the implicit argument
typing clues here mean whether two relations can
share arguments, but NOT enumate what types ex-
plicitly their arguments should have.
We formalize all the relation pairs that disagree
with each other as follows. These relation pairs
can be divided into three subcategories. We repre-
sent the relation pairs (r
i
, r
j
) that are inconsistent
in terms of subjects as C
sr
, the relations pairs that
are inconsistent in terms of objects as C
ro
, the re-
lation pairs that are inconsistent in terms of one?s
subject and the other one?s object as C
rer
.
It is worth mentioning that disagreements in-
side a tuple are also included here. For instance,
an entity tuple <USA, Washington D.C.> in Fig-
ure 1 has two candidate relations, Capital and Lo-
cationCity. These two predictions are inconsistent
with each other with respect to the type of USA.
They implicitly consider USA as ?country? and
?organization?, respectively.
Violations of Arguments? Uniqueness: The
previous categories of disagreements are all based
on the implicit type information of the relations?
arguments, Now we make use of the clues of ar-
gument cardinality requirements. Given a subject,
some relations should have unique objects. For
example, in Figure 1, given USA as the subject of
the relation Capital, we can only accept one pos-
sible object, because there is great chance that a
country only have one capital. On the other hand,
given Washington D.C. as the object of the relation
Capital, we can only accept one subject, since usu-
ally a city can only be the capital of one country
or state. If these are violating in the candidates,
we could know that there may be some incorrect
predictions. We represent the relations expecting
unique objects as C
ou
, and the relations expecting
unique subjects as C
su
.
3.3 Obtaining the Global Clues
Now, the issue is how to obtain the clues used
in the previous subsection. That is, how we de-
termine which relations expect certain types of
subjects, which relations expect certain types of
objects, etc. These knowledge can be definitely
coded by human, or learnt from a KB.
Most existing knowledge bases represent their
knowledge facts in the form of (<subject, rela-
tion, object>) triple, which can be seen as re-
lational facts between entity tuples. Usually the
triples in a KB are carefully defined by experts. It
is rare to find inconsistencies among the triples in
the knowledge base. The clues are therefore learnt
from KBs, and further refined manually if needed.
Given two relations r
1
and r
2
, we query the KB
for all tuples bearing the relation r
1
or r
2
. We use
S
i
and O
i
to represent r
i
?s (i ? {1, 2}) subject set
and object set, respectively. We adopt the point-
wise mutual information (PMI) to estimate the de-
pendency between the argument sets of two rela-
tions:
PMI(A,B) = log
p(A,B)
p(A)p(B)
(2)
where p(A,B) is number of the entities both in
A and B, p(A) and p(B) are the numbers of
the entities in A and B, respectively. For any
pair of relations from R ? R, we calculate four
scores: PMI(S
1
, S
2
), PMI(O
1
, O
2
), PMI(S
1
, O
2
)
and PMI(S
2
, O
1
). To make more stable esti-
mations, we set up a threshold for the PMI. If
PMI(S
1
, S
2
) is lower than the threshold, we will
consider that r
1
and r
2
cannot share a subject.
Things are similar for the other three scores. The
threshold is set to -3 in this paper.
821
We can also learn the uniqueness of arguments
for relations. For each pre-defined relation in R,
we collect all the triples containing this relation,
and count the portion of the triples which only
have one object for each subject, and the por-
tion of the triples which only have one subject
for each object. The relations whose portions are
higher than the threshold will be considered to
have unique argument values. This threshold is
set to 0.8 in this paper.
3.4 Integer Linear Program Formulation
As discussed above, given a set of entity pairs and
their candidate relations output by a preliminary
extractor, our goal is to find an optimal configura-
tion for all those entities pairs jointly, solving the
disagreements among those candidate predictions
and maximizing the overall confidence of the se-
lected predictions. This is an NP-hard optimiza-
tion problem. Many optimization models can be
used to obtain the approximate solutions.
In this paper, we propose to solve the problem
by using an ILP tool, IBM ILOG Cplex
1
. Firstly,
for each tuple t and one of its candidate relations
r, we define a binary decision variable d
r
t
indicat-
ing whether the candidate relation r is selected by
the solver. Our objective is to maximize the total
confidence of all the selected candidates, and the
objective function can be written as:
max
?
t?T ,r?R
t
conf(t, r)d
r
t
+
?
?t,r?R
t
,m?M
r
t
maxMEscore(m, r)d
r
t
where conf(t, r) is the confidence of the tuple t
bearing the candidate relation r. The first compo-
nent is the sum of the original confidence scores of
all the selected candidates, and the second one is
the sum of the maximal mention-level confidence
scores of all the selected candidates. The latter is
designed to encourage the model to select the can-
didates with higher individual mention-level con-
fidence scores.
We add the constraints with respect to the dis-
agreements described in Section 3.2. For the sake
of clarity, we describe the constraints derived from
each scenario of the two categories of disagree-
ments separately.
The subject-relation constraints avoid the dis-
agreements between the predictions of two tuples
1
www.cplex.com
sharing a subject. These constraints can be repre-
sented as:
d
r
t
i
t
i
+ d
r
t
j
t
j
? 1 (3)
?t
i
, t
j
: subj(t
i
) = subj(t
j
) ? (r
t
i
, r
t
j
) ? C
sr
where t
i
and t
j
are two tuples in T , subj(t
i
) is the
subject of t
i
, r
t
i
is a candidate relation of t
i
, r
t
j
is
a candidate relation of t
j
.
The object-relation constraints avoid the incon-
sistencies between the predictions of two tuples
sharing an object. Formally we add the following
constraints:
d
r
t
i
t
i
+ d
r
t
j
t
j
? 1 (4)
?t
i
, t
j
: obj(t
i
) = obj(t
j
) ? (r
t
i
, r
t
j
) ? C
ro
where t
i
? T and t
j
? T are two tuples, obj(t
i
)
is the object of t
i
.
The relation-entity-relation constraints ensure
that if an entity works as subject and object in two
tuples t
i
and t
j
respectively, their relations agree
with each other. The constraints we add are:
d
r
t
i
t
i
+ d
r
t
j
t
j
? 1 (5)
?t
i
, t
j
: obj(t
i
) = subj(t
j
) ? (r
t
i
, r
t
j
) ? C
rer
The object uniqueness constraints ensure that
the relations requiring unique objects do not bear
more than one object given a subject.
?
t?Tuple(r),subj(t)=e
d
r
t
? 1 (6)
?e ? r ? C
ou
where e is an entity, Tuple(r) are the tuples whose
candidate relations contain r.
The subject uniqueness constraints ensure that
given an object, the relations expecting unique
subjects do not bear more than one subject.
?
t?Tuple(r),obj(t)=e
d
r
t
? 1 (7)
?e ? r ? C
su
By adopting ILP, we can combine the local
information including MaxEnt confidence scores
and the implicit relation backgrounds that are em-
bedded into global consistencies of the entity tu-
ples together. After the optimization problem is
solved, we will obtain a list of selected candidate
relations for each tuple, which will be our final
output.
822
4 Experiments
4.1 Datasets
We evaluate our approach on three datasets, in-
cluding two English datasets and one Chinese
dataset.
The first English dataset, Riedel?s dataset, is the
one used in (Riedel et al, 2010; Hoffmann et al,
2011; Surdeanu et al, 2012), with the same split.
It uses Freebase as the knowledge base and New
York Time corpus as the text corpus, including
about 60,000 entity tuples in the training set, and
about 90,000 entity tuples in the testing set.
We generate the second English dataset, DB-
pedia dataset, by mapping the triples in DBpedia
(Bizer et al, 2009) to the sentences in New York
Time corpus. We map 51 different relations to the
corpus and result in about 50,000 entity tuples,
134,000 sentences for training and 30,000 entity
tuples, 53,000 sentences for testing.
For the Chinese dataset, we derive knowledge
facts and construct a Chinese KB from the In-
foboxes of HudongBaike, one of the largest Chi-
nese online encyclopedias. We collect four na-
tional economic newspapers in 2009 as our corpus.
28 different relations are mapped to the corpus and
this results in 60,000 entity tuples, 120,000 sen-
tences for training and 40,000 tuples, 83,000 sen-
tences for testing.
4.2 Baselines and Competitors
The baseline we use in this paper is Mintz++,
which is described in (Surdeanu et al, 2012). It
is a modification of the model proposed by Mintz
et al (2009). The model predicts for each mention
separately, and allows multi-label outputs for an
entity tuple by OR-ing the outputs of its mentions.
As we described in Section 3.1, originally we
select the top three predicted relations as the can-
didates for each mention. In order to investigate
whether it is necessary to use up to three candi-
dates, we implement two variants of our approach,
which select the top one and top two relations as
candidates for each mention, and represented as
ILP-1cand and ILP-2cand, respectively.
We also use two distant supervision approaches
for the comparison. The first one is MultiR (Hoff-
mann et al, 2011), a novel joint model that can
deal with the relation overlap issue. The second
one, MIML-RE (Surdeanu et al, 2012), is one of
the state-of-the-art MIML relation extraction sys-
tems. We tune the models of MultiR and MIML-
RE so that they fit our datasets.
4.3 Overall Performance
First we compare our framework and its vari-
ants with the baseline and the state-of-the-art RE
models. Following previous works, we use the
Precision-Recall curve as the evaluation criterion
in our experiment. The results are summarized
in Figure 2. For the constraints, we first manu-
ally select an average of 20 relation pairs for each
subcategory of the first kind of clues, and all the
relations with unique argument values in R. We
also show how automatically learnt clues perform
in Section 4.5.
Figure 2 shows that compared with the baseline,
our framework performs consistently better in the
DBpedia dataset and the Chinese dataset. Mintz++
proves to be a strong baseline on both datasets. It
tends to result in a high recall, and its weakness of
low precision is perfectly fixed by the ILP model.
Our ILP model and its variants all outperform
Mintz++ in precision in both datasets, indicating
that our approach helps filter out incorrect predic-
tions from the output of MaxEnt model. Com-
pared with MultiR, our framework obtains better
results in both datasets. Especially in the Chinese
dataset, the improvement in precision reaches as
high as 10-16% at the same recall points. Our
framework performs better compared to MIML-
RE in the English dataset. On the Chinese dataset,
our framework outperforms MIML-RE except in
the low-recall portion (<10%) of the P-R curve.
All these results show that embedding the relation
background information into RE can help elim-
inate the wrong predictions and improve the re-
sults.
However, in the Riedel?s dataset, Mintz++, the
MaxEnt relation extractor, does not perform well,
and our framework cannot improve its perfor-
mance. In order to find out the reasons, we manu-
ally investigate the dataset. The top three relations
of this dataset are /location/location/contains,
/people/person/nationality and
/people/person/place lived. About two-thirds of
the entity tuples belongs to these three relations,
and the outputs of the local extractor usually
bias even more to the large relations. What is
worse, we cannot find any clues from the top
three relations because their arguments? types are
too general. Things are similar for many other
823
0 0.1 0.2 0.3 0.4 0.50.2
0.4
0.6
0.8
1
recall
preci
sion
 
 Mintz++ILP?1candILP?2candILP
(a) The DBpedia Dataset
0 0.1 0.2 0.3 0.4 0.5 0.60
0.2
0.4
0.6
0.8
1
recall
preci
sion
 
 Mintz++ILP?1candILP?2candILP
(b) The Riedel?s Dataset
0 0.1 0.2 0.3 0.4 0.5 0.60.3
0.4
0.5
0.6
0.7
0.8
0.9
1
recall
preci
sion
 
 Mintz++ILP?1candILP?2candILP
(c) The Chinese Dataset
0 0.1 0.2 0.3 0.4 0.50.2
0.4
0.6
0.8
1
recall
preci
sion
 
 Mintz++MultiRMIML?REILP
(d) The DBpedia Dataset
0 0.1 0.2 0.3 0.4 0.5 0.60
0.2
0.4
0.6
0.8
1
recall
preci
sion
 
 Mintz++MultiRMIML?REILP
(e) The Riedel?s Dataset
0 0.1 0.2 0.3 0.4 0.5 0.60.3
0.4
0.5
0.6
0.7
0.8
0.9
1
recall
preci
sion
 
 Mintz++MultiRMIML?REILP
(f) The Chinese Dataset
Figure 2: Overall performances of our framework and its variants, the baselines and the state-of-the-art
approaches on the three datasets.
relations in this dataset. Although we may find
some clues any way, they are too few to make
any improvement. Hence, our framework does
not perform well due to the poor performance of
MaxEnt extractor and the lack of clues. To solve
this problem, we think of addressing the selection
preferences between relations and entities pro-
posed in (Riedel et al, 2013), which should be
our future work.
We notice that in all three datasets our variant
ILP-1cand is shorter than Mintz++ in recall, in-
dicating we may incorrectly discard some predic-
tions. Compared to ILP-2cand and original ILP,
ILP-1cand leads to slightly lower precision but
much lower recall, showing that selecting more
candidates may help us collect more potentially
correct predictions. Comparing ILP-2cand and
original ILP, the latter hardly makes any improve-
ment in precision, but is slightly longer in re-
call, indicating using three candidates can still col-
lect some more potentially correct predictions, al-
though the number may be limited.
In order to study how our framework improves
the performances on the DBpedia dataset and the
Chinese dataset, we further investigate the num-
ber of incorrect predictions eliminated by ILP and
the number of incorrect predictions corrected by
ILP. We also examine the number of correct pre-
Table 1: Details of the improvements made by ILP
in the DBpedia and Chinese datasets.
Datasets Incorrect Predictions Wrong Predictioins Correct Predictions
Eliminated Corrected Newly Introduced
DBpedia 268 61 1426
Chinese 1506 14 283
dictions newly introduce by ILP, which were NA
in Mintz++. We summarize the results in Table 1.
The results show that our framework can reduce
the incorrect predictions and introduce more cor-
rect predictions at the same time. We also find
an interesting results: in the DBpedia dataset, ILP
is more likely to introduce correct predictions to
the results, while in the Chinese dataset it tends to
reduce more incorrect predictions, which may be
caused by the differences between performances
of Mintz++ on the two datasets, where it gets a
higher recall on the Chinese dataset.
Following Surdeanu et al (2012), we also list
the peak F1 score (highest F1 score) for each
model in Table 2. Different from (Surdeanu et al,
2012), we use all the entity pairs instead of the
ones with more than 10 mentions. We can observe
that our model obtains the best performance in the
DBpedia dataset and the Chinese dataset. In the
DBpedia dataset, it is 3.6% higher than Mintz++,
824
7.9% higher than MIML-RE and 13.9% higher
than MultiR. In the Chinese dataset, Mintz++,
MultiR and MIML-RE performs similarly in terms
of the highest F1 score, while our model gains
about 8% improvement. In the Riedel?s dataset,
our framework hardly obtains any improvement
compared with Mintz++.
We also investigate the impacts of the con-
straints used in ILP, which are derived based on the
two kinds of clues and can encode relation defini-
tion information into our framework. Experimen-
tal results in Table 2 shows that in the DBpedia
dataset, the highest F1 score increases from 35.2%
to 38.3% with the help of both kinds of clues,
while in the Chinese dataset the improvement is
from 44.4% to 52.8%. In the Riedel?s dataset we
do not see any improvements since there are al-
most no clues. Furthermore, using constraints de-
rived from only one kind of clues can also improve
the performance, but not as well as using both of
them.
4.4 Adapting MultiR Sentence Level
Extractor to Our Framework
The preliminary relation extractor of our optimiza-
tion framework is not limited to the MaxEnt ex-
tractor, and can take any sentence level relation
extractor with confidence scores. We also fit Mul-
tiR?s mention level extractor into our framework.
As shown in Figure 3, in the DBpedia dataset
and the Chinese dataset, in most parts of the curve,
ILP optimized MultiR outperforms original Mul-
tiR. We think the reason is that our framework
make use of global clues to discard the incorrect
predictions. The results are not as high as when
we use MaxEnt as the preliminary extractor. We
think one reason is that MultiR does not perform
well in these two datasets. Furthermore, the confi-
dence scores which MultiR outputs are not nor-
malized to the same scale, which brings us dif-
ficulties in setting up a confidence threshold to
select the candidates. As a result, we only use
the top one result as the candidate since including
top two predictions without thresholding the confi-
dences performs bad, indicating that a probabilis-
tic sentence-level extractor is more suitable for our
framework. We also notice that in the Riedel?s
dataset our framework does not improve the per-
formance significantly, and we have discussed the
reasons in Section 4.3.
( a)
( b)
Figure 4: F1 score v.s. number of relations (used
to introduce the related learnt clues into the ILP
framework) on the DBpedia dataset (a) and the
Chinese dataset (b).
00.
20.4
0.6
0.20.40.60.81
recall
precision
 
 Manual Auto
(a) The DBpedia Dataset
00.2
0.40.6
0.20.40.60.81
recall
precision
 
 Manual Auto
(b) The Chinese Dataset
Figure 5: Performances of manually selected clues
and automatically learnt clues on two datasets.
4.5 Examining the Automatically Learnt
Clues
Now we evaluate the performance of automati-
cally collected clues used in our model. Since
there are almost no clues in the Riedel?s dataset,
we only investigate the other two datasets. We add
clues according to their related relations? propor-
tions in the local predictions. For example, Coun-
try and birthPlace take up about 30% in the local
predictions, we thus add clues that are related to
these two relations, and then move on with new
clues related to other relations according to those
relations? proportions in the local predictions.
As is shown in Figure 4, in both datasets, the
clues related to more local predictions will solve
more inconsistencies, thus are more effective.
Adding the first two relations improves the model
significantly, and as more relations are added, the
825
Table 2: Results of the highest F1 score on all three datasets.
DBpedia Riedel Chinese
Method P(%) R(%) F1(%) P(%) R(%) F1(%) P(%) R(%) F1(%)
Mintz++ 40.2 30.5 34.7 35.3 23.2 27.9 43.3 45.7 44.4
MultiR 60.4 15.3 24.4 32.3 25.1 28.2 53.5 38.2 44.6
MIML-RE 51.3 21.6 30.4 41.5 19.9 26.9 49.2 41.3 44.9
ILP 37.4 39.2 38.3 35.5 23.2 28.0 52.6 52.9 52.8
ILP-No-Constraint 34.1 36.3 35.2 35.3 23.2 28.0 43.3 45.7 44.4
ILP-Type-Inconsistent 36.3 39.2 37.7 35.5 23.2 28.0 49.5 49.0 49.2
ILP-Cardinality 35.3 37.8 36.5 35.4 23.2 28.0 50.3 48.8 49.6
0 0.05 0.1 0.150.6
0.7
0.8
0.9
1
recall
preci
sion
 
 ILP Optimized MultiROriginal MultiR
(a) The DBpedia Dataset
0 0.05 0.1 0.15 0.2 0.25 0.30.3
0.4
0.5
0.6
0.7
0.8
0.9
1
recall
preci
sion
 
 ILP Optimized MultiROriginal MultiR
(b) The Riedel?s Dataset
0 0.1 0.2 0.3 0.40.5
0.6
0.7
0.8
0.9
1
recall
preci
sion
 
 ILP Optimized MultiROriginal MultiR
(c) The Chinese Dataset
Figure 3: The results of original MultiR and ILP optimized MultiR on the three datasets.
performances keep increasing until approaching
the still state. It is worth mentioning that when
sufficient learnt clues are added into the model, the
results are comparable to those based on the clues
refined manually, as shown in Figure 5. This indi-
cates that the clues can be collected automatically,
and further used to examine whether predicted re-
lations are consistent with the existing ones in the
KB, which can be considered as a form of quality
control.
5 Conclusions
In this paper, we make use of the global clues de-
rived from KB to help resolve the disagreements
among local relation predictions, thus reduce the
incorrect predictions and improve the performance
of relation extraction. Two kinds of clues, includ-
ing implicit argument type information and argu-
ment cardinality information of relations are in-
vestigated. Our framework outperforms the state-
of-the-art models if we can find such clues in the
KB. Furthermore, our framework is scalable for
other local sentence level extractors in addition to
the MaxEnt model. Finally, we show that the clues
can be learnt automatically from the KB, and lead
to comparable performance to manually refined
ones.
For future work, we will investigate other kinds
of clues and attempt a joint optimization frame-
work that could host entity disambiguation, rela-
tion extraction and entity linking together. We
will also adopt selection preference between en-
tities and relations since sometimes we may not
find useful clues.
Acknowledgments
We would like to thank Heng Ji, Dong Wang and
Kun Xu for their useful discussions and the anony-
mous reviewers for their helpful comments which
greatly improved the work. This work was sup-
ported by the National High Technology R&D
Program of China (Grant No. 2012AA011101),
National Natural Science Foundation of China
(Grant No. 61272344, 61202233, 61370055) and
the joint project with IBM Research.
References
Michele Banko, Michael J. Cafarella, Stephen Soder-
land, Matt Broadhead, and Oren Etzioni. 2007.
Open information extraction from the web. In Pro-
ceedings of IJCAI, IJCAI?07, pages 2670?2676.
Christian Bizer, Jens Lehmann, Georgi Kobilarov,
S?oren Auer, Christian Becker, Richard Cyganiak,
826
and Sebastian Hellmann. 2009. Dbpedia - a crys-
tallization point for the web of data. Web Semant.,
7:154?165, September.
Razvan C. Bunescu. 2007. Learning to extract rela-
tions from the web using minimal supervision. In
Proceedings of the 45th Annual Meeting of the As-
sociation for Computational Linguistics (ACL07.
Oier Lopez de Lacalle and Mirella Lapata. 2013. Un-
supervised relation extraction with general domain
knowledge. In EMNLP, pages 415?425. ACL.
Anthony Fader, Stephen Soderland, and Oren Etzioni.
2011. Identifying relations for open information ex-
traction. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing,
EMNLP ?11, pages 1535?1545, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Raphael Hoffmann, Congle Zhang, Xiao Ling,
Luke Zettlemoyer, and Daniel S. Weld. 2011.
Knowledge-based weak supervision for information
extraction of overlapping relations. In Proceedings
of the 49th ACL-HLT - Volume 1, HLT ?11, pages
541?550, Stroudsburg, PA, USA. ACL.
Qi Li, Sam Anzaroot, Wen-Pin Lin, Xiang Li, and
Heng Ji. 2011. Joint inference for cross-document
information extraction. In Proceedings of the 20th
ACM International Conference on Information and
Knowledge Management, CIKM ?11, pages 2225?
2228, New York, NY, USA. ACM.
Qi Li, Heng Ji, and Liang Huang. 2013. Joint event
extraction via structured prediction with global fea-
tures. In ACL, pages 73?82. The Association for
Computer Linguistics.
Mike Mintz, Steven Bills, Rion Snow, and Dan Ju-
rafsky. 2009. Distant supervision for relation ex-
traction without labeled data. In Proceedings of the
Joint Conference of the 47th Annual Meeting of the
ACL and the 4th IJCNLP of the AFNLP: Volume 2 -
Volume 2, ACL ?09, pages 1003?1011.
Sebastian Riedel, Limin Yao, and Andrew McCallum.
2010. Modeling relations and their mentions with-
out labeled text. In Machine Learning and Knowl-
edge Discovery in Databases, volume 6323 of Lec-
ture Notes in Computer Science, pages 148?163.
Springer Berlin / Heidelberg.
Sebastian Riedel, Limin Yao, Benjamin M. Marlin, and
Andrew McCallum. 2013. Relation extraction with
matrix factorization and universal schemas. In Joint
Human Language Technology Conference/Annual
Meeting of the North American Chapter of the Asso-
ciation for Computational Linguistics (HLT-NAACL
?13), June.
Stephen Soderland, David Fisher, Jonathan Aseltine,
and Wendy Lehnert. 1995. Crystal inducing a con-
ceptual dictionary. In Proceedings of the 14th IJCAI
- Volume 2, IJCAI?95, pages 1314?1319, San Fran-
cisco, CA, USA. Morgan Kaufmann Publishers Inc.
Fabian Suchanek, James Fan, Raphael Hoffmann, Se-
bastian Riedel, and Partha Pratim Talukdar. 2013.
Advances in automated knowledge base construc-
tion. In SIGMOD Records journal, March.
Mihai Surdeanu, David McClosky, Julie Tibshirani,
John Bauer, Angel X. Chang, Valentin I. Spitkovsky,
and Christopher D. Manning. 2010. A simple dis-
tant supervision approach for the TAC-KBP slot fill-
ing task. In Proceedings of the Third Text Anal-
ysis Conference (TAC 2010), Gaithersburg, Mary-
land, USA, November.
Mihai Surdeanu, Julie Tibshirani, Ramesh Nallap-
ati, and Christopher D. Manning. 2012. Multi-
instance multi-label learning for relation extraction.
In EMNLP-CoNLL, pages 455?465. ACL.
Shingo Takamatsu, Issei Sato, and Hiroshi Nakagawa.
2012. Reducing wrong labels in distant supervi-
sion for relation extraction. In Proceedings of the
50th Annual Meeting of the Association for Compu-
tational Linguistics: Long Papers - Volume 1, ACL
?12, pages 721?729, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.
Limin Yao, Sebastian Riedel, and Andrew McCallum.
2010. Collective cross-document relation extraction
without labelled data. In Proceedings of EMNLP,
EMNLP ?10, pages 1013?1023, Stroudsburg, PA,
USA. ACL.
Limin Yao, Sebastian Riedel, and Andrew McCal-
lum. 2012. Probabilistic databases of universal
schema. In Proceedings of the Joint Workshop on
Automatic Knowledge Base Construction and Web-
scale Knowledge Extraction, AKBC-WEKEX ?12,
pages 116?121, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Xingxing Zhang, Jianwen Zhang, Junyu Zeng, Jun
Yan, Zheng Chen, and Zhifang Sui. 2013. Towards
accurate distant supervision for relational facts ex-
traction. In Proceedings of the 51st Annual Meet-
ing of the Association for Computational Linguis-
tics (Volume 2: Short Papers), pages 810?815, Sofia,
Bulgaria, August. Association for Computational
Linguistics.
Shubin Zhao and Ralph Grishman. 2005. Extracting
relations with integrated information using kernel
methods. In Proceedings of the 43rd Annual Meet-
ing on Association for Computational Linguistics,
ACL ?05, pages 419?426, Stroudsburg, PA, USA.
Association for Computational Linguistics.
827
Proceedings of the 2012 Student Research Workshop, pages 67?72,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Towards Automatic Construction of Knowledge Bases from Chinese Online
Resources
Liwei Chen, Yansong Feng, Yidong Chen, Lei Zou, Dongyan Zhao
Institute of Computer Science and Technology
Peking University
Beijing, China
{clwclw88,fengyansong,chenyidong,zoulei,zhaodongyan}@pku.edu.cn
Abstract
Automatically constructing knowledge bases
from online resources has become a crucial
task in many research areas. Most existing
knowledge bases are built from English re-
sources, while few efforts have been made for
other languages. Building knowledge bases
for Chinese is of great importance on its own
right. However, simply adapting existing tool-
s from English to Chinese yields inferior re-
sults.In this paper, we propose to create Chi-
nese knowledge bases from online resources
with less human involvement.This project will
be formulated in a self-supervised framework
which requires little manual work to extrac-
t knowledge facts from online encyclopedia
resources in a probabilistic view.In addition,
this framework will be able to update the con-
structed knowledge base with knowledge facts
extracted from up-to-date newswire.Currently,
we have obtained encouraging results in our
pilot experiments that extracting knowledge
facts from infoboxes can achieve a high accu-
racy of around 95%, which will be then used
as training data for the extraction of plain web-
pages.
1 Introduction
As the development of world wide web (WWW),
the volume of web data is growing exponentially
in recent years. Most of the data are unstructured,
while a few are manually structured and only a s-
mall part of them are machine-readable. How to
make these data accessible and useable for end user-
s has become a key topic in many research areas,
such as information retrieval, natural language pro-
cessing, semantic web(Tim et al, 2001) and so on.
Among others, constructing knowledge bases (KB)
from web data has been considered as a preliminary
step. However, it is not trivial to extract knowledge
facts from unstructured web data, especially in open
domain, and the accuracy is usually not satisfacto-
ry. On the other hand, with the development of We-
b2.0, there are increasing volume of online encyclo-
pedias which are collectively created by active vol-
unteers, e.g., Wikipedia1. Surprisingly, experiment
evidences show that the confidence of Wikipedia is
even comparable with that of British Encyclopedi-
a (Giles, 2005). Therefore, many efforts have been
made to distill knowledge facts from Wikipedia or
similar resources and further build KBs, for example
YAGO(Suchanek et al, 2007), DBpedia(Bizer et al,
2009) and KOG(Wu and Weld, 2008).
In the literature, most KBs constructed recently
are in English as it takes up an overwhelming major-
ity on the web, while other major languages receives
less attention, for example, Chinese features similar
amounts of web pages with English yet is less fre-
quently studied with regarding to building KBs. Al-
though continuous works have been made to process
English resources, building Chinese KBs is of great
value on its own. To the best of our knowledge, few
efforts have been made to construct a KB in Chi-
nese until now. Despite of necessary special pre-
processings, e.g., word segmentation, for Chinese,
building a Chinese KB from web data is quite differ-
ent from building English ones, since we have lim-
ited resources available in Chinese that are of lower
1http://www.wikipedia.com
67
quality compared to their English counterparts. This
brings more difficulties than that of English. As a
result, the approaches used in English may not work
well in Chinese.
In this paper, we propose a new framework to
build a KB in Chinese from online resources with-
out much human involvement. Since the Chinese
portion of Wikipedia is much smaller than its En-
glish part, we harvest knowledge facts from a Chi-
nese online encyclopedia, HudongBaike2. Hudong-
Baike is the largest Chinese online encyclopedia and
features similar managing rules and writing styles
with Wikipedia. We first obtain knowledge facts by
parsing the infoboxes of HudongBaike. Then we use
these triples as seeds, and adopt the idea of distant
supervision(Mintz et al, 2009; Riedel et al, 2010;
Yao et al, 2010) to extract more facts from other
HudongBaike articles and build a KB accordingly.
Moreover, to make the knowledge base more up-to-
date, we also propose to propagate the KBwith news
events.
The rest of this paper is organized as follows: we
first introduce the related work, and briefly introduce
two online encyclopedias. In Section 4 we describe
our framework in detail. Our current work are dis-
cussed in Section 5. In Section 6 we conclude this
paper.
2 Related Work
KB construction is an important task and has at-
tracted many research efforts from artificial intelli-
gence, information retrieval, natural language pro-
cessing, and so on. Traditional KBs are most-
ly manually created, including WordNet(Stark and
Riesenfeld, 1998), Cyc or OpenCyc(Matuszek et al,
2006), SUMO(Niles and Pease, 2001), and also
some domain-specific ontologies such as GeneOn-
tology3. These KBs achieve a high accuracy since
they are manually built or filtered by domain ex-
perts. However, manually creating KB is a time-
consuming and labor-intensive work, and continu-
ous annotation is required to keep the KB up-to date.
Most of them thus suffers from the coverage issue in
practice.
In recent years, many researchers turn to auto-
2http://www.hudong.com
3http://www.geneontology.org
matically extract knowledge to construct KBs. One
kind of methods extract knowledge facts from gener-
al text corpus. These approaches, such as TextRun-
ner(Banko et al, 2007) and KnowItAll(Etzioni et al,
2004), use rule based information extraction tech-
nologies to extract relations between entity pairs.
Recently, TextRunner is expanded by a life long
learning strategy, which can acquire new facts. An-
other type of approaches aims to automatically de-
rive facts from online encyclopedias. Collectively
created by many volunteers, online encyclopedias
are more reliable than general web pages. They al-
so contain semi-structured knowledge such as hand-
crafted infoboxes. Therefore, the accuracy of the
facts extracted will be higher. Researchers utilize
these semi-structured data resources for knowledge
extraction, for example, YAGO extract facts from in-
foboxes and category names of Wikipedia, and use
WordNet as its taxonomy(Suchanek et al, 2007).
A similar approach is adopted by DBpedia, which
also extract knowledge facts from infoboxes(Bizer
et al, 2009). Unlike YAGO and DBpedia, Kylin us-
es the infoboxes and the Wikipedia pages containing
these infoboxes to build a training set, and use ma-
chine learning methods to extract facts from plain
Wikipedia articles(Wu and Weld, 2007). Although
Kylin achieves a high precision, it is corpus-specific,
which means it can only be used in Wikipedia-like
corpora. It is noticed that all the above works fo-
cus on building an English KB, and few efforts have
been made in building a Chinese one until now.
3 Online Encyclopedia
Wikipedia is known as an accurate online encyclo-
pedia whose accuracy is comparable with Encyclo-
pedia Britannica(Giles, 2005). It?s created by thou-
sands of volunteers around the whole world. Until
now, the English version ofWikipedia has 3,878,200
content pages, making it the largest English on-
line encyclopedia. The Chinese version contains
402,781 content pages, which is much smaller than
the English version.
HudongBaike is the largest Chinese online ency-
clopedia with over 5 million content pages. Similar-
ly with Wikipedia, HudongBaike is also created by
volunteers, and relies on the community to ensure
its quality. Many HudongBaike pages also contains
68
Preprocessed HudongBaike Pages
Extracted Triples HudongBaike Articles
Triples Extracted from Articles
Knowledge Base
 Up-to-Date Data
Semantic Elements 
Propagated KB
Analyzing Infoboxes Cleaning pages
Mapping
Distant supervision
KB construction Semantic Elements Extraction
Propagating KB
Figure 2: The framework of Our project
a hand-crafted summary box, infobox. An infobox
summarizes the knowledge of the corresponding en-
tity. The information in the infobox is reliable since
these are collaboratively crafted by many volunteer-
s. Figure 1 is an example page with an infobox from
HudongBaike, introducing a US general ????
?? (George Marshall).
4 The Framework
In this paper, we formulated the KB construction
task in a semi-supervised learning fashion which re-
quires little manual annotation and supports knowl-
edge propagation by up-to-date feeds. Because
the Chinese part of Wikipedia is relatively smal-
l and may suffer from the coverage problem, we use
HudongBaike to build our KB in this project. In fu-
ture we may merge the Wikipedia part into our KB.
After necessary preprocessings including word seg-
mentation and named entity extraction, we are able
to apply our framework shown in Figure 2.
In general, our framework contains the follow-
ing steps: (1)Extracting knowledge from online
encyclopedia; (2)Linking triples and building KB;
(3)Propagating KB with up-to-date data.
4.1 Entity Relation Extraction
Compared to other resources on the Web, online
encyclopedias contain less noises and feature more
regular structures, thus are considered easier for us
to extract knowledge facts.
Analyzing Infoboxes As mentioned before, many
HudongBaike pages contains an infobox, which
has high accuracy and can be used directly for
relation extraction. We can conveniently parse
these infoboxes into < S,P,O > triples. For
example, from the first entry of this infobox,
we can derive the following triple: < ?
????? , ??? , ???? >(<
GeorgeMarshall, BirthP lace, Uniontown >).
The precision of the extraction is over 95%, and
these triples can form a valuable knowledge source.
Extracting relations with Distant Supervision
Extracting knowledge from infoboxes is efficien-
t and can achieve a high precision. However, many
web pages in HudongBaike do not have infoboxes.
There is much richer knowledge in the main arti-
cles of HudongBaike, which we should also take in-
to consideration.
Extracting knowledge from unstructured articles
is a challenging task. Traditionally, researchers
use manually created templates to extract relation-
s. These templates need lots of human efforts and
are domain-specific. Recent methods trend to re-
ly on machine learning models, which need a large
amount of labeled data. One idea is to utilize the
infoboxes to form the training data set, and train an
extractor to extract relations from the pages with-
out an infobox(Wu and Weld, 2007). However, the
relations extracted from a page are restricted to the
infobox template used by the current page catego-
ry, and their subject must be the entity that this page
describes. For example, when we extract relation-
s from the page of ????? (Charles Yeager,
Ace of US in WWII) which does not contain an in-
fobox, the subject of these relations must be Charles
Yeager, and we can only extract the relation types
listed in infobox template for a military person. As
a result, this method can only be used in online en-
cyclopedias in a Wikipedia style, and the recall will
be relatively low.
Distant supervision is widely used in relation ex-
traction in recent years. It hardly need any manual
work, and can overcome the above problems. It can
be used in any reliable corpus, and doesn?t have the
strict restrictions as previous methods. We adopt its
idea in our framework. The basic assumption of dis-
tant supervision is the sentences containing two en-
69
Figure 1: A HudongBaike page about a US general George Marshall
tities should express the relation between them more
or less. It only needs a reliable seed KB (in the form
of relation triples) and a corpus. Here, we can use
the knowledge facts extracted from infoboxes previ-
ously as the seed KB, and the articles of Hudong-
Baike as text corpus. For each triple in the seed K-
B, we generate positive training data by finding sen-
tences containing both its subject and object in the
corpus. For example, we can map the first entry in
Figure 1 to the sentence 1880?12?31?????
??????? (On December 31th, 1880, Mar-
shall was born in Uniontown). The negative training
data can be generated by randomly select some sen-
tences which contain neither of the subject and the
object. A predictive model such as logistic regres-
sion model is trained with the training data. We can
use the model to give predictions for the relations
in a textual knowledge source. For a HudongBaike
page, we should decide the entity pairs we are in-
terested in. A simple strategy is to select all entity
pairs. But it will be time-consuming, and may suffer
from weak-related entity pairs. So we extract top-
ic entities which have high tfidf weights from this
page, and generate entity pairs under the restriction
that they must contain at least one topic entity. For
each entity pair, we find the sentences which contain
both the subject and object and use the predictive
model to give the possible relations between them
and the confidence of the relations.
However, the predictions of distant supervision
is less accurate than those of supervised method-
s. So we should adopt some heuristics to filter the
relations extracted. An easy strategy is to set up a
threshold for relation confidences to avoid uncertain
relations and improve the precision. We adopt this
method in our project. Furthermore, we can also use
the strategies of Riedel et al (2010) or Yao et al
(2010).
4.2 Knowledge Base Construction
After the relation extraction, we must link the ex-
tracted knowledge triples in order to construct the
knowledge base. In our scenario this linking task can
be formulated as: given a base KB, a bunch of newly
extracted knowledge triples with the sentences de-
scribing them and their contexts, the task of entity
linking aims to link each of the entity mentions in
the plain texts (these sentences mentioned above) to
its corresponding entity in the base KB. At the very
beginning, we initiate a base KB by using the taxon-
omy of HudongBaike thus are able to map relations
between entities into the KB through entity linking.
In online encyclopedias, the synonyms of an en-
tity are represented by redirect links. Synonyms are
important in entity linking because they provide al-
ternative names for entities, and we may miss some
mappings without them. For example, we have an
entity ?????? (United States of America)
in the KB, and an mention ?? (USA) in a piece
of text. Redirect links can tell us that we can create
a mapping between them. Basically, for each men-
tion, we can find matching candidates for them in a
KB through exact matching. However, if we can-
not find an exact match for a mention, we will try
70
fuzzy matching since a mention may not match ex-
actly with its referent entity in KB.
Now we need to solve the entity linking task. Tra-
ditional methods did not exploit global interdepen-
dence between entity linking decisions. We thus
adopt the collective entity linking approach of Han
et al (2011) to solve this problem. This method cap-
tures the rich semantic relatedness knowledge be-
tween entities, and take the interdependence of link-
ing decisions into consideration. They construct a
graph by linking name mentions and candidate enti-
ties in pairwise using the semantic relatedness be-
tween them. Then they use a random walk algo-
rithm on the graph to solve the problem. However,
they did not take the NIL problem into considera-
tion. That is, in entity linking, if the referent enti-
ty of an name mention is not in our KB, it should
be linked to a pseudo entity NIL. In our case, we
should abandon the mapping of the current triple by
deciding whether this entity has been listed in the
KB(Zheng et al, 2010).
4.3 Knowledge base Propagation
Although we can extract millions of relations and
built a KB in previous subsections, it has the same
shortage as most existing KBs: the knowledge ex-
tracted are mostly statical attributes of entities (such
as birthdate or occupation of a person) and can not
describe the latest updates of an entity (such as a
politician is currently visiting a country).
In order to settle this problem, we use the dy-
namical knowledge extracted from up-to-date data
to expand our KB. One possible solution is extract-
ing semantic event elements from online news. In
this project, we will synchronies our KB with a Chi-
nese newspaper, RenMinRiBao (People?s Daily).
5 Current Work
Currently, we have extracted triples from the in-
foboxes of HudongBaike and built the base KB.
Manual evaluation shows that the precision of struc-
tured content extraction is over 95%. Most errors
are caused by the web page?s own mistakes or edit-
ing errors in infoboxes.
To assess the quality of HudongBaike data, in our
preliminary experiments(Yidong et al, 2012), we
extract relation facts from plain HudongBaike arti-
cles without infoboxes in a way similar to Kylin. We
focus on three categories, including ?? (Nation),
?? (Person) and ?? (Actor or Actress). In each
category we select several representative attributes
from its infobox template. We manually annotated
more than 200 testing examples for evaluation: 100
in Person, 33 in Nation and 91 in Actor or Actress.
The results shows that the HudongBaike data can be
used to extract knowledge facts with a high precision
in all three categories: in ?? the average precision
is 79.43%, in ?? it is 78.9%, and in ?? it even
goes up to 90.8%.
Distant Supervision We further adopt the ap-
proach of distant supervision(Mintz et al, 2009) in
a Chinese dataset. We generate a dataset from Ren-
MinRiBao with 10000 sentences, and each sentence
contains at least a pair of entities which correspond
to a knowledge triple in HudongBaike?s infobox ex-
traction. We use 60% of the sentences as training
set and 40% as the testing set. Our experiments
show that when the recall is 10%, we can obtain a
high precision of 87%, which indicates the feasibili-
ty of our model. However, as the recall raises, the
precision drops dramatically. For example, when
the recall is 29% the precision is about 65%. This
can be remedied by adopting more encyclopedia-
specific filtering strategies and assumptions during
the distant supervision modeling.
6 Conclusions
In this project, we proposed a framework to build
KBs in Chinese. It uses the infoboxes of Hudong-
Baike as a seed knowledge base, the articles of
HudongBaike as extra textual resources, adopts the
idea of distant supervision to extract knowledge fact-
s from unstructured data and link the triples to build
a knowledge base. This framework requires lit-
tle manual work, and can be used in other reliable
knowledge resources. Our preliminary experimental
results are encouraging, showing that the Hudong-
Baike provides reasonable resources for building
knowledge bases and the distant supervision fashion
can be adapted to work well in Chinese.
For the next, we will further adapt our frame-
work into a self-training manner. By using higher
threshold for confidence in distant supervision we
can make sure the precision of extracted knowledge
71
is high enough for bootstrapping. Then we put the
extracted knowledge facts into the seed KB, and the
framework will repeat iteratively. On the other hand,
we can extract knowledge facts from other reliable
knowledge resource, such as Wikipedia, academic
literature, and merge knowledge from different re-
sources into one KB. Moreover, we can also make
our KB multilingual by adopting our framework in
other languages.
References
Banko, M., Cafarella, M. J., Soderland, S., Broad-
head, M., and Etzioni, O. (2007). Open informa-
tion extraction from the web. In Proceedings of
IJCAI, IJCAI?07, pages 2670?2676.
Bizer, C., Lehmann, J., Kobilarov, G., Auer, S.,
Becker, C., Cyganiak, R., and Hellmann, S.
(2009). Dbpedia - a crystallization point for the
web of data. Web Semant., 7:154?165.
Etzioni, O., Cafarella, M., Downey, D., Kok, S.,
Popescu, A.-M., Shaked, T., Soderland, S., Weld,
D. S., and Yates, A. (2004). Web-scale informa-
tion extraction in knowitall. In Proceedings of the
13th WWW, WWW ?04, pages 100?110.
Giles, J. (2005). Internet encyclopaedias go head to
head. Nature, 438:900?901.
Han, X., Sun, L., and Zhao, J. (2011). Collective
entity linking in web text: a graph-based method.
In SIGIR, SIGIR ?11, pages 765?774, New York,
NY, USA. ACM.
Matuszek, C., Cabral, J., Witbrock, M., and DeO-
liveira, J. (2006). An introduction to the syntax
and content of cyc. In Proceedings of the 2006
AAAI Spring Symposium.
Mintz, M., Bills, S., Snow, R., and Jurafsky, D.
(2009). Distant supervision for relation extraction
without labeled data. In Proceedings of the Joint
Conference of the 47th Annual Meeting of the A-
CL and the 4th IJCNLP of the AFNLP: Volume 2
- Volume 2, ACL ?09, pages 1003?1011.
Niles, I. and Pease, A. (2001). Towards a standard
upper ontology. In Proceedings of FIOS - Volume
2001, pages 2?9. ACM Press, New York.
Riedel, S., Yao, L., and McCallum, A. (2010).
Modeling relations and their mentions without la-
beled text. In Machine Learning and Knowledge
Discovery in Databases, volume 6323 of Lec-
ture Notes in Computer Science, pages 148?163.
Springer Berlin / Heidelberg.
Stark, M. M. and Riesenfeld, R. F. (1998). Wordnet:
An electronic lexical database. In Proceedings of
11th Eurographics Workshop on Rendering. MIT
Press.
Suchanek, F. M., Kasneci, G., and Weikum, G.
(2007). Yago: a core of semantic knowledge.
In Proceedings of WWW, WWW ?07, pages 697?
706, New York, NY, USA. ACM.
Tim, B.-L., J., H., and O., L. (2001). The semantic
web. Scientific American.
Wu, F. and Weld, D. S. (2007). Autonomously
semantifying wikipedia. In CIKM, CIKM ?07,
pages 41?50, New York, NY, USA. ACM.
Wu, F. and Weld, D. S. (2008). Automatically re-
fining the wikipedia infobox ontology. In WWW,
WWW ?08, pages 635?644, New York, NY, USA.
ACM.
Yao, L., Riedel, S., and McCallum, A. (2010). Col-
lective cross-document relation extraction with-
out labelled data. In Proceedings of EMNLP,
EMNLP ?10, pages 1013?1023, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Yidong, C., Liwei, C., and Kun, X. (2012). Learning
chinese entity attributes from online encyclopedi-
a. In Proceedings of IEKB workshop in APWeb
2012.
Zheng, Z., Li, F., Huang, M., and Zhu, X. (2010).
Learning to link entities with knowledge base. In
HLT-NAACL 2010, pages 483?491, Stroudsburg,
PA, USA.
72
