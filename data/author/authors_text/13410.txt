Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1426?1436,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Relation Extraction with Relation Topics
Chang Wang James Fan Aditya Kalyanpur David Gondek
IBM T. J. Watson Research Lab
19 Skyline Drive, Hawthorne, New York 10532
{wangchan, fanj, adityakal, dgondek}@us.ibm.com
Abstract
This paper describes a novel approach to the
semantic relation detection problem. Instead
of relying only on the training instances for
a new relation, we leverage the knowledge
learned from previously trained relation detec-
tors. Specifically, we detect a new semantic
relation by projecting the new relation?s train-
ing instances onto a lower dimension topic
space constructed from existing relation de-
tectors through a three step process. First, we
construct a large relation repository of more
than 7,000 relations from Wikipedia. Second,
we construct a set of non-redundant relation
topics defined at multiple scales from the re-
lation repository to characterize the existing
relations. Similar to the topics defined over
words, each relation topic is an interpretable
multinomial distribution over the existing re-
lations. Third, we integrate the relation topics
in a kernel function, and use it together with
SVM to construct detectors for new relations.
The experimental results on Wikipedia and
ACE data have confirmed that background-
knowledge-based topics generated from the
Wikipedia relation repository can significantly
improve the performance over the state-of-the-
art relation detection approaches.
1 Introduction
Detecting semantic relations in text is very useful
in both information retrieval and question answer-
ing because it enables knowledge bases to be lever-
aged to score passages and retrieve candidate an-
swers. To extract semantic relations from text, three
types of approaches have been applied. Rule-based
methods (Miller et al, 2000) employ a number of
linguistic rules to capture relation patterns. Feature-
based methods (Kambhatla, 2004; Zhao and Grish-
man, 2005) transform relation instances into a large
amount of linguistic features like lexical, syntactic
and semantic features, and capture the similarity be-
tween these feature vectors. Recent results mainly
rely on kernel-based approaches. Many of them fo-
cus on using tree kernels to learn parse tree struc-
ture related features (Collins and Duffy, 2001; Cu-
lotta and Sorensen, 2004; Bunescu and Mooney,
2005). Other researchers study how different ap-
proaches can be combined to improve the extraction
performance. For example, by combining tree ker-
nels and convolution string kernels, (Zhang et al,
2006) achieved the state of the art performance on
ACE (ACE, 2004), which is a benchmark dataset for
relation extraction.
Although a large set of relations have been iden-
tified, adapting the knowledge extracted from these
relations for new semantic relations is still a chal-
lenging task. Most of the work on domain adapta-
tion of relation detection has focused on how to cre-
ate detectors from ground up with as little training
data as possible through techniques such as boot-
strapping (Etzioni et al, 2005). We take a differ-
ent approach, focusing on how the knowledge ex-
tracted from the existing relations can be reused to
help build detectors for new relations. We believe by
reusing knowledge one can build a more cost effec-
tive relation detector, but there are several challenges
associated with reusing knowledge.
The first challenge to address in this approach is
how to construct a relation repository that has suffi-
1426
cient coverage. In this paper, we introduce a method
that automatically extracts the knowledge charac-
terizing more than 7,000 relations from Wikipedia.
Wikipedia is comprehensive, containing a diverse
body of content with significant depth and grows
rapidly. Wikipedia?s infoboxes are particularly in-
teresting for relation extraction. They are short,
manually-created, and often have a relational sum-
mary of an article: a set of attribute/value pairs de-
scribing the article?s subject.
Another challenge is how to deal with overlap of
relations in the repository. For example, Wikipedia
authors may make up a name when a new relation
is needed without checking if a similar relation has
already been created. This leads to relation duplica-
tion. We refine the relation repository based on an
unsupervised multiscale analysis of the correlations
between existing relations. This method is parame-
ter free, and able to produce a set of non-redundant
relation topics defined at multiple scales. Similar to
the topics defined over words (Blei et al, 2003), we
define relation topics as multinomial distributions
over the existing relations. The relation topics ex-
tracted in our approach are interpretable, orthonor-
mal to each other, and can be used as basis relations
to re-represent the new relation instances.
The third challenge is how to use the relation top-
ics for a relation detector. We map relation instances
in the new domains to the relation topic space, re-
sulting in a set of new features characterizing the
relationship between the relation instances and ex-
isting relations. By doing so, background knowl-
edge from the existing relations can be introduced
into the new relations, which overcomes the limi-
tations of the existing approaches when the training
data is not sufficient. Our work fits in to a class of re-
lation extraction research based on ?distant supervi-
sion?, which studies how knowledge and resources
external to the target domain can be used to im-
prove relation extraction. (Mintz et al, 2009; Jiang,
2009; Chan and Roth, 2010). One distinction be-
tween our approach and other existing approaches is
that we represent the knowledge from distant super-
vision using automatically constructed topics. When
we test on new instances, we do not need to search
against the knowledge base. In addition, our top-
ics also model the indirect relationship between re-
lations. Such information cannot be directly found
from the knowledge base.
The contributions of this paper are three-fold.
Firstly, we extract a large amount of training
data for more than 7,000 semantic relations from
Wikipedia (Wikipedia, 2011) and DBpedia (Auer
et al, 2007). A key part of this step is how we
handle noisy data with little human effort. Sec-
ondly, we present an unsupervised way to con-
struct a set of relation topics at multiple scales.
This step is parameter free, and results in a non-
redundant, multiscale relation topic space. Thirdly,
we design a new kernel for relation detection by
integrating the relation topics into the relation de-
tector construction. The experimental results on
Wikipedia and ACE data (ACE, 2004) have con-
firmed that background-knowledge-based features
generated from the Wikipedia relation repository
can significantly improve the performance over the
state-of-the-art relation detection approaches.
2 Extracting Relations from Wikipedia
Our training data is from two parts: relation in-
stances from DBpedia (extracted from Wikipedia
infoboxes), and sentences describing the relations
from the corresponding Wikipedia pages.
2.1 Collecting the Training Data
Since our relations correspond to Wikipedia infobox
properties, we use an approach similar to that de-
scribed in (Hoffmann et al, 2010) to collect positive
training data instances. We assume that a Wikipedia
page containing a particular infobox property is
likely to express the same relation in the text of
the page. We further assume that the relation is
most likely expressed in the first sentence on the
page which mentions the arguments of the relation.
For example, the Wikipedia page for ?Albert Ein-
stein? contains an infobox property ?alma mater?
with value ?University of Zurich?, and the first sen-
tence mentioning the arguments is the following:
?Einstein was awarded a PhD by the University of
Zurich?, which expresses the relation. When look-
ing for relation arguments on the page, we go be-
yond (sub)string matching, and use link information
to match entities which may have different surface
forms. Using this technique, we are able to collect a
large amount of positive training instances of DBpe-
1427
dia relations.
To get precise type information for the argu-
ments of a DBpedia relation, we use the DBpedia
knowledge base (Auer et al, 2007) and the asso-
ciated YAGO type system (Suchanek et al, 2007).
Note that for every Wikipedia page, there is a cor-
responding DBpedia entry which has captured the
infobox-properties as RDF triples. Some of the
triples include type information, where the subject
of the triple is a Wikipedia entity, and the object
is a YAGO type for the entity. For example, the
DBpedia entry for the entity ?Albert Einstein? in-
cludes YAGO types such as Scientist, Philosopher,
Violinist etc. These YAGO types are also linked
to appropriate WordNet concepts, providing for ac-
curate sense disambiguation. Thus, for any en-
tity argument of a relation we are learning, we ob-
tain sense-disambiguated type information (includ-
ing super-types, sub-types, siblings etc.), which be-
come useful generalization features in the relation
detection model. Given a common noun, we can
also retrieve its type information by checking against
WordNet (Fellbaum, 1998).
2.2 Extracting Rules from the Training Data
We use a set of rules together with their popular-
ities (occurrence count) to characterize a relation.
A rule representing the relations between two ar-
guments has five components (ordered): argument1
type, argument2 type, noun, preposition and verb. A
rule example of ActiveYearsEndDate relation (about
the year that a person retired) is:
person100007846|year115203791|-|in|retire.
In this example, argument1 type is per-
son100007846, argument2 type is year115203791,
both of which are from YAGO type system. The
key words connecting these two arguments are in
(preposition) and retire (verb). This rule does not
have a noun, so we use a ?-? to take the position of
noun. The same relation can be represented in many
different ways. Another rule example characterizing
the same relation is
person100007846|year115203791|retirement|-|announce.
This paper only considers three types of words:
noun, verb and preposition. It is straightforward to
expand or simplify the rules by including more or
removing some word types. The keywords are ex-
tracted from the shortest path on the dependency
Figure 1: A dependency tree example.
tree between the two arguments. A dependency
tree (Figure 1) represents grammatical relations be-
tween words in a sentence. We used a slot grammar
parser (McCord, 1995) to generate the parse tree of
each sentence. Note that there could be multiple
paths between two arguments in the tree. We only
take the shortest path into consideration. The pop-
ularity value corresponding to each rule represents
how many times this rule applies to the given rela-
tion in the given data. Multiple rules can be con-
structed from one relation instance, if multiple argu-
ment types are associated with the instance, or mul-
tiple nouns, prepositions or verbs are in the depen-
dency path.
2.3 Cleaning the Training Data
To find a sentence on the Wikipedia page that is
likely to express a relation in its infobox, we con-
sider the first sentence on the page that mentions
both arguments of the relation. This heuristic ap-
proach returns reasonably good results, but brings in
about 20% noise in the form of false positives, which
is a concern when building an accurate statistical re-
lation detector. To address this issue, we have devel-
oped a two-step technique to automatically remove
some of the noisy data. In the first step, we extract
popular argument types and keywords for each DB-
pedia relation from the given data, and then use the
combinations of those types and words to create ini-
tial rules. Many of the argument types and keywords
introduced by the noisy data are often not very pop-
ular, so they can be filtered out in the first step. Not
all initial rules make sense. In the second step, we
1428
check each rule against the training data to see if that
rule really exists in the training data or not. If it does
not exist, we filter it out. If a sentence does not have
a single rule passing the above procedure, that sen-
tence will be removed. Using the above techniques,
we collect examples characterizing 7,628 DBpedia
relations.
3 Learning Multiscale Relation Topics
An extra step extracting knowledge from the raw
data is needed for two reasons: Firstly, many DB-
pedia relations are inter-related. For example, some
DBpedia relations have a subclass relationship, e.g.
?AcademyAward? and ?Award?; others overlap in
their scope and use, e.g., ?Composer? and ?Artist?;
while some are equivalent, e.g., ?DateOfBirth? and
?BirthDate?. Secondly, a fairly large amount of the
noisy labels are still in the training data.
To reveal the intrinsic structure of the current DB-
pedia relation space and filter out noise, we car-
ried out a correlation analysis of relations in the
training data, resulting in a relation topic space.
Each relation topic is a multinomial distribution
over the existing relations. We adapted diffusion
wavelets (Coifman and Maggioni, 2006) for this
task. Compared to the other well-known topic ex-
traction methods like LDA (Blei et al, 2003) and
LSI (Deerwester et al, 1990), diffusion wavelets can
efficiently extract a hierarchy of interpretable topics
without any user input parameter (Wang and Ma-
hadevan, 2009).
3.1 An Overview of Diffusion Wavelets
The diffusion wavelets algorithm constructs a com-
pressed representation of the dyadic powers of a
square matrix by representing the associated matri-
ces at each scale not in terms of the original (unit
vector) basis, but rather using a set of custom gener-
ated bases (Coifman and Maggioni, 2006). Figure
2 summarizes the procedure to generate diffusion
wavelets. Given a matrix T , the QR (a modified
QR decomposition) subroutine decomposes T into
an orthogonal matrix Q and a triangular matrix R
such that T ? QR, where |Ti,k ? (QR)i,k| < ?
for any i and k. Columns in Q are orthonormal ba-
sis functions spanning the column space of T at the
finest scale. RQ is the new representation of T with
{[?j ]?0} = DWT (T, ?, J)
//INPUT:
//T : The input matrix.
//?: Desired precision, which can be set to a small
number or simply machine precision.
//J : Number of levels (optional).
//OUTPUT:
//[?j ]?0 : extended diffusion scaling functions at
scale j.
?0 = I;
For j = 0 to J ? 1 {
([?j+1]?j , [T 2j ]?j+1?j )? QR([T 2
j ]?j?j , ?);
[?j+1]?0 = [?j+1]?j [?j ]?0 ;
[T 2j+1 ]?j+1?j+1 = ([T
2j ]?j+1?j [?j+1]?j )
2;
}
Figure 2: Diffusion Wavelets construct multiscale repre-
sentations of the input matrix at different scales. QR is a
modified QR decomposition. J is the max step number
(this is optional, since the algorithm automatically ter-
minates when it reaches a matrix of size 1 ? 1). The
notation [T ]?b?a denotes matrix T whose column space isrepresented using basis ?b at scale b, and row space is
represented using basis ?a at scale a. The notation [?b]?a
denotes basis ?b represented on the basis ?a. At an arbi-
trary scale j, we have pj basis functions, and length of
each function is lj . The number of pj is determined by
the intrinsic structure of the given dataset in QR routine.
[T ]?b?a is a pb ? la matrix, and [?b]?a is an la ? pb matrix.
respect to the space spanned by the columns of Q
(this result is based on the matrix invariant subspace
theory). At an arbitrary level j,DWT learns the ba-
sis functions from T 2j using QR. Compared to the
number of basis functions spanning T 2j ?s original
column space, we usually get fewer basis functions,
since some high frequency information (correspond-
ing to the ?noise? at that level) can be filtered out.
DWT then computes T 2j+1 using the low frequency
representation of T 2j and the procedure repeats.
3.2 Constructing Multiscale Relation Topics
Learning Relation Correlations
Assume we have M relations, and the ith of them
is characterized by mi <rule, popularity> pairs. We
use s(a, b) to represent the similarity between the
ath and bth relations. To compute s(a, b), we first
normalize the popularities for each relation, and then
1429
look for the rules that are shared by both relation a
and b. We use the product of corresponding pop-
ularity values to represent the similarity score be-
tween two relations with respect to each common
rule. s(a, b) is set to the sum of such scores over
all common rules. The relation-relation correlation
matrix S is constructed as follows:
S = [
s(1, 1) ? ? ? s(1,M)
? ? ? ? ? ? ? ? ?
s(M, 1) ? ? ? s(M,M)
]
We have more than 200, 000 argument types, tens
of thousands of distinct nouns, prepositions, and
verbs, so we potentially have trillions of distinct
rules. One rule may appear in multiple relations.
The more rules two relations share, the more related
two relations should be. The rules shared across dif-
ferent relations offer us a novel way to model the
correlations between different relations, and further
allow us to create relation topics. The rules can also
be simplified. For example, we may treat argument1,
argument2, noun, preposition and verb separately.
This results in simple rules that only involve in one
argument type or word. The correlations between
relations are then computed only based on one par-
ticular component like argument1, noun, etc.
Theoretical Analysis
Matrix S models the correlations between rela-
tions in the training data. Once S is constructed, we
adapt diffusion wavelets (Coifman and Maggioni,
2006) to automatically extract the basis functions
spanning the original column space of S at multi-
ple scales. The key strength of the approach is that
it is data-driven, largely parameter-free and can au-
tomatically determine the number of levels of the
topical hierarchy, as well as the topics at each level.
However, to apply diffusion wavelets to S, we first
need to show that S is a positive semi-definite ma-
trix. This property guarantees that all eigenvalues
of S are ? 0. Depending on the way we formal-
ize the rules, the methods to validate this property
are slightly different. When we treat argument1,
argument2, noun, preposition and verb separately, it
is straightforward to see the property holds. In The-
orem 1, we show the property also holds when we
use more complicated rules (using the 5-tuple rule
in Section 2.2 as an example in the proof).
Theorem 1. S is a Positive Semi-Denite matrix.
Proof: An arbitrary rule ri is uniquely characterized
by a five tuple: argument1 type| argument2 type|
noun| preposition| verb. Since the number of dis-
tinct argument types and words are constants, the
number of all possible rules is also a constant: R.
If we treat each rule as a feature, then the set of
rules characterizing an arbitrary relation ri can be
represented as a point [p1i , ? ? ? , pRi ] in a latent R di-
mensional rule space, where pji represents the popu-
larity of rule j in relation ri in the given data.
We can verify that the way to compute s(a, b) is
the same as s(a, b) =< [p1a ? ? ? pRa ], [p1b ? ? ? pRb ] >,
where < ?, ? > is the cosine similarity (kernel). It
follows directly from the definition of positive semi-
definite matrix (PSD) that S is PSD (Scho?lkopf and
Smola, 2002).
In our approach, we construct multiscale re-
lation topics by applying DWT to decompose
S/?max(S), where ?max(S) represents the largest
eigenvalue of S. Theorem 2 shows that this decom-
position will converge, resulting in a relation topic
hierarchy with one single topic at the top level.
Theorem 2. Let ?max(S) represent the largest
eigenvalue of matrix S, then DWT (S/?max(S), ?)
produces a set of nested subspaces of the column
space of S, and the highest level of the resulting sub-
space hierarchy is spanned by one basis function.
Proof: From Theorem 1, we know that S is a PSD
matrix. This means ?max(S) ? [0,+?) (all eigen-
values of S are non-negative). This further implies
that ?(S)/?max(S) ? [0, 1], where ?(S) represents
any eigenvalue of S.
The idea underlying diffusion wavelets is based
on decomposing the spectrum of an input matrix
into various spectral bands, spanned by basis func-
tions (Coifman and Maggioni, 2006). Let T =
S/?max(S). In Figure 2, we construct spectral
bands of eigenvalues, whose associated eigenvectors
span the corresponding subspaces. Define dyadic
spatial scales tj as
tj =
j?
t=0
2t = 2j+1 ? 1, j ? 0 .
At each spatial scale, the spectral band is defined as:
?j(T ) = {? ? ?(T ), ?tj ? ?},
1430
where ?(T ) represents any eigenvalue of T , and ? ?
(0, 1) is a pre-defined threshold in Figure 2. We can
now associate with each of the spectral bands a vec-
tor subspace spanned by the corresponding eigen-
vectors:
Vj = ?{?? : ? ? ?(T ), ?tj ? ?}?, j ? 0 .
In the limit, we obtain
lim
j??
Vj = ?{?? : ? = 1}?
That is, the highest level of the resulting subspace
hierarchy is spanned by the eigenvector associated
with the largest eigenvalue of T .
This result shows that the multiscale analysis of
the relation space will automatically terminate at the
level spanned by one basis, which is the most popu-
lar relation topic in the training data.
3.3 High Level Explanation
We first create a set of rules to characterize each in-
put relation. Since these rules may occur in multi-
ple relations, they provide a way to model the co-
occurrence relationship between different relations.
Our algorithm starts with the relation co-occurrence
matrix and then repeatedly applies QR decomposi-
tion to learn the topics at the current level while at
the same time modifying the matrix to focus more on
low-frequency indirect co-occurrences (between re-
lations) for the next level. Running DWT is equiv-
alent to running a Markov chain on the input data
forward in time, integrating the local geometry and
therefore revealing the relevant geometric structures
of the whole data set at different scales. At scale
j, the representation of T 2j+1 is compressed based
on the amount of remaining information and the de-
sired precision. This procedure is illustrated in Fig-
ure 3. In the resulting topic space, instances with
related relations will be grouped together. This ap-
proach may significantly help us detect new rela-
tions, since it potentially expands the information
brought in by new relation instances from making
use of the knowledge extracted from the existing re-
lation repository.
3.4 Benefits
As shown in Figure 3, the topic spaces at different
levels are spanned by a different number of basis
  
 	
   
Proceedings of NAACL-HLT 2013, pages 777?782,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Distant Supervision for Relation Extraction
with an Incomplete Knowledge Base
Bonan Min, Ralph Grishman, Li Wan
New York University
New York, NY 10003
{min,grishman,wanli}
@cs.nyu.edu
Chang Wang, David Gondek
IBM T. J. Watson Research Center
Yorktown Heights, NY 10598
{wangchan,dgondek}
@us.ibm.com
Abstract
Distant supervision, heuristically labeling a
corpus using a knowledge base, has emerged
as a popular choice for training relation ex-
tractors. In this paper, we show that a sig-
nificant number of ?negative? examples gen-
erated by the labeling process are false neg-
atives because the knowledge base is incom-
plete. Therefore the heuristic for generating
negative examples has a serious flaw. Building
on a state-of-the-art distantly-supervised ex-
traction algorithm, we proposed an algorithm
that learns from only positive and unlabeled
labels at the pair-of-entity level. Experimental
results demonstrate its advantage over existing
algorithms.
1 Introduction
Relation Extraction is a well-studied problem
(Miller et al, 2000; Zhou et al, 2005; Kambhatla,
2004; Min et al, 2012a). Recently, Distant Super-
vision (DS) (Craven and Kumlien, 1999; Mintz et
al., 2009) has emerged to be a popular choice for
training relation extractors without using manually
labeled data. It automatically generates training ex-
amples by labeling relation mentions1 in the source
corpus according to whether the argument pair is
listed in the target relational tables in a knowledge
base (KB). This method significantly reduces human
efforts for relation extraction.
The labeling heuristic has a serious flaw. Knowl-
edge bases are usually highly incomplete. For exam-
1An occurrence of a pair of entities with the source sentence.
ple, 93.8% of persons from Freebase2 have no place
of birth, and 78.5% have no nationality (section 3).
Previous work typically assumes that if the argument
entity pair is not listed in the KB as having a re-
lation, all the corresponding relation mentions are
considered negative examples.3 This crude assump-
tion labeled many entity pairs as negative when in
fact some of their mentions express a relation. The
number of such false negative matches even exceeds
the number of positive pairs, by 3 to 10 times, lead-
ing to a significant problem for training. Previous
approaches (Riedel et al, 2010; Hoffmann et al,
2011; Surdeanu et al, 2012) bypassed this problem
by heavily under-sampling the ?negative? class.
We instead deal with a learning scenario where we
only have entity-pair level labels that are either posi-
tive or unlabeled. We proposed an extension to Sur-
deanu et al (2012) that can train on this dataset. Our
contribution also includes an analysis on the incom-
pleteness of Freebase and the false negative match
rate in two datasets of labeled examples generated
by DS. Experimental results on a realistic and chal-
lenging dataset demonstrate the advantage of the al-
gorithm over existing solutions.
2 Related Work
Distant supervision was first proposed by Craven
and Kumlien (1999) in the biomedical domain.
2Freebase is a large collaboratively-edited KB. It is available
at http://www.freebase.com.
3There are variants of labeling heuristics. For example, Sur-
deanu et al (2011) and Sun et al (2011) use a pair < e, v >
as a negative example, when it is not listed in Freebase, but e is
listed with a different v?. These assumptions are also problem-
atic in cases where the relation is not functional.
777
Since then, it has gain popularity (Mintz et al, 2009;
Bunescu and Mooney, 2007; Wu and Weld, 2007;
Riedel et al, 2010; Hoffmann et al, 2011; Sur-
deanu et al, 2012; Nguyen and Moschitti, 2011).
To tolerate noisy labels in positive examples, Riedel
et al (2010) use Multiple Instance Learning (MIL),
which assumes only at-least-one of the relation men-
tions in each ?bag? of mentions sharing a pair of ar-
gument entities which bears a relation, indeed ex-
presses the target relation. MultiR (Hoffmann et
al., 2011) and Multi-Instance Multi-Label (MIML)
learning (Surdeanu et al, 2012) further improve it
to support multiple relations expressed by different
sentences in a bag. Takamatsu et al (2012) mod-
els the probabilities of a pattern showing relations,
estimated from the heuristically labeled dataset.
Their algorithm removes mentions that match low-
probability patterns. Sun et al (2011) and Min et
al. (2012b) also estimate the probablities of patterns
showing relations, but instead use them to relabel ex-
amples to their most likely classes. Their approach
can correct highly-confident false negative matches.
3 Problem Definition
Distant Supervision: Given a KB D (a collection
of relational tables r(e1, e2), in which r?R (R is the
set of relation labels), and < e1, e2 > is a pair of
entities that is known to have relation r) and a cor-
pus C, the key idea of distant supervision is that we
align D to C, label each bag4 of relation mentions
that share argument pair < e1, e2 > with r, other-
wise OTHER. This generates a dataset that has labels
on entity-pair (bag) level. Then a relation extractor
is trained with single-instance learning (by assum-
ing all mentions have the same label as the bag), or
Multiple-Instance Learning (by assuming at-least-
one of the mentions expresses the bag-level label),
or Multi-Instance Multi-Label learning (further as-
suming a bag can have multiple labels) algorithms.
All of these works treat the OTHER class as exam-
ples that are labeled as negative.
The incomplete KB problem: KBs are usually
incomplete because they are manually constructed,
and it is not possible to cover all human knowledge
4A bag is defined as a set of relation mentions sharing the
same entity pair as relation arguments. We will use the terms
bag and entity pair interchangeably in this paper.
nor stay current. We took frequent relations, which
involve an entity of type PERSON, from Freebase
for analysis. We define the incompleteness ?(r) of a
relation r as follows:
?(r) = |{e}|?|{e|?e?,s.t.r(e,e?)?D}||{e}|
?(r) is the percentage of all persons {e} that do
not have an attribute e? (with which r(e, e?) holds).
Table 1 shows that 93.8% of persons have no place
of birth, and 78.5% of them have no nationality.
These are must-have attributes for a person. This
shows that Freebase is highly incomplete.
Freebase relation types Incompleteness
/people/person/education 0.792
/people/person/employment history 0.923
/people/person/nationality* 0.785
/people/person/parents* 0.988
/people/person/place of birth* 0.938
/people/person/places lived* 0.966
Table 1: The incompleteness of Freebase (* are must-
have attributes for a person).
We further investigate the rate of false negative
matches, as the percentage of entity-pairs that are
not listed in Freebase but one of its mentions gen-
erated by DS does express a relation in the tar-
get set of types. We randomly picked 200 unla-
beled bags5 from each of the two datasets (Riedel
et al, 2010; Surdeanu et al, 2012) generated by DS,
and we manually annotate all relation mentions in
these bags. The result is shown in Table 2, along
with a few examples that indicate a relation holds in
the set of false negative matches (bag-level). Both
datasets have around 10% false negative matches in
the unlabeled set of bags. Taking into considera-
tion that the number of positive bags and unlabeled
bags are highly imbalanced (1:134 and 1:37 in the
Riedel and KBP dataset respectively, before under-
sampling the unlabeled class), the number of false
negative matches are 11 and 4 times the number
of positive bags in Reidel and KBP dataset, respec-
tively. Such a large ratio shows false negatives do
have a significant impact on the learning process.
4 A semi-supervised MIML algorithm
Our goal is to model the bag-level label noise,
caused by the incomplete KB problem, in addition
585% and 95.7% of the bags in the Riedel and KBP datasets
have only one relation mention.
778
Dataset
(train-
ing)
# pos-
itive
bags
# positive :
# unlabeled
% are
false
negatives
# positive
: # false
negative
has human
assessment
Examples of false negative mentions
Riedel 4,700 1:134(BD*) 8.5% 1:11.4 no
(/location/location/contains)... in Brooklyn ?s Williamsburg.
(/people/person/place lived) Cheryl Rogowski , a farmer from
Orange County ...
KBP 183,062 1:37(BD*) 11.5% 1:4 yes
(per:city of birth) Juan Martn Maldacena (born September
10, 1968) is a theoretical physicist born in Buenos Aires
(per:employee of)Dave Matthews, from the ABC News, ...
Table 2: False negative matches on the Riedel (Riedel et al, 2010) and KBP dataset (Surdeanu et al, 2012). All
numbers are on bag (pairs of entities) level. BD* are the numbers before downsampling the negative set to 10% and
5% in Riedel and KBP dataset, respectively.
to modeling the instance-level noise using a 3-layer
MIL or MIML model (e.g., Surdeanu et al (2012)).
We propose a 4-layer model as shown in Figure 1.
The input to the model is a list of n bags with a
vector of binary labels, either Positive (P), or Un-
labled (U) for each relation r. Our model can be
viewed as a semi-supervised6 framework that ex-
tends a state-of-the-art Multi-Instance Multi-Label
(MIML) model (Surdeanu et al, 2012). Since the
input to previous MIML models are bags with per-
relation binary labels of either Positive (P) or Neg-
ative (N), we add a set of latent variables ? which
models the true bag-level labels, to bridge the ob-
served bag labels y and the MIML layers. We con-
sider this as our main contribution to the model. Our
hierarchical model is shown in Figure 1.
Figure 1: Plate diagram of our model.
Let i, j be the index in the bag and mention level,
respectively. Following Surdeanu et al (2012), we
model mention-level extraction p(zrij |xij ;wz) and
multi-instance multi-label aggregation p(?ri |zi;wr?)
in the bottom 3 layers. We define:
? r is a relation label. r?R ? {OTHER}, in
which OTHER denotes no relation expressed.
? yri ?{P,U}: r holds for ith bag or the bag is
unlabeled.
6We use the term semi-supervised because the algorithm
uses unlabeled bags but existing solutions requires bags to be
labeled either positive or negative.
? ?ri ?{P,N}: a hidden variable that denotes
whether r holds for the ith bag.
? ? is an observed constant controlling the total
number of bags whose latent label is positive.
We define the following conditional probabilities:
? p(yri |?ri ) =
?
?
?
?
?
?
?
1/2 if yri = P ? ?ri = P ;
1/2 if yri = U ? ?ri = P ;
1 if yri = U ? ?ri = N ;
0 otherwise ;
It encodes the constraints between true bag-
level labels and the entity pair labels in the KB.
? p(?|?) ? N (
?n
i=1
?
r?R ?(?ri ,P )
n , 1k ) where
?(x, y) = 1 if x = y, 0 otherwise. k is a large
number. ? is the fraction of the bags that are
positive. It is an observed parameter that de-
pends on both the source corpus and the KB
used.
Similar to Surdeanu et al (2012), we also define
the following parameters and conditional probabili-
ties (details are in Surdeanu et al (2012)):
? zij?R ? {OTHER}: a latent variable that de-
notes the relation type of the jth mention in the
ith bag.
? xij is the feature representation of the jth rela-
tion mention in the ith bag. We use the set of
features in Surdeanu et al (2012).
? wz is the weight vector for the multi-class rela-
tion mention-level classifier.
? wr? is the weight vector for the rth binary top-
level aggregation classifier (from mention la-
bels to bag-level prediction). We usew? to rep-
resent w1? ,w2? , ...w
|R|
? .
? p(?ri |zi;wr?) ? Bern(f?(wr? , zi)) where f? is
probability produced by the rth top-level clas-
sifier, from the mention-label level to the bag-
label level.
? p(zrij |xij ;wz) ? Multi(fz(wz,xij)) where fz
779
is probability produced by the mention-level
classifier, from the mentions to the mention-
label level.7
4.1 Training
We use hard Expectation-Maximization (EM) algo-
rithm for training the model. Our objective function
is to maximize log-likelihood:
L(wz,w?) = logp(y, ?|x;wz,w?)
= log
?
?
p(y, ?, ?|x;wz,w?)
Since solving it exactly involves exploring an expo-
nential assignment space for ?, we approximate and
iteratively set ?? = arg? max p(?|y, ?,x;wz,w?)
p(?|y, ?,x;wz,w?) ? p(y, ?, ?|x;wz,w?)
= p(y, ?|?,x)p(?|x;wz,w?)
= p(y|?)p(?|?)p(?|x;wz,w?)
Rewriting in log form:
logp(?|y, ?,x;wz,w?)
= logp(y|?) + logp(?|?) + logp(?|x;wz,w?)
=
n
?
i=1
?
r?R
logp(yri |?ri )? k(
n
?
i=1
?
r?R
?(?ri , P )
n ? ?)
2
+
n
?
i=1
?
r?R
logp(?ri |xi;wz,w?) + const
Algorithm 1 Training (E-step:2-11; M-step:12-15)
1: for i = 1, 2 to T do
2: ?ri ? N for all yri = U and r?R
3: ?ri ? P for all yri = P and r?R
4: I = {< i, r > |?ri = N}; I ? = {< i, r > |?ri = P}
5: for k = 0, 1 to ?n? |I ?| do
6: < i?, r? >= argmax<i,r>?I p(?ri |xi;wz,w?)
7: ?r?i? ? P ; I = I\{< i?, r? >}
8: end for
9: for i = 1, 2 to n do
10: z?i = argmaxzi p(zi|?i,xi;wz,w?)
11: end for
12: w?z = argmaxwz
?n
i=1
?|xi|
j=1 logp(zij |xij ,wz)
13: for all r?R do
14: w
r(?)
? = argmaxwr?
?n
i=1 p(?ri |zi,wr?)
15: end for
16: end for
17: return wz,w?
7All classifiers are implemented with L2-regularized logistic
regression with Stanford CoreNLP package.
In the E-step, we do a greedy search (steps 5-8
in algorithm 1) in all p(?ri |xi;wz,w?) and update ?ri
until the second term is maximized. wz , w? are the
model weights learned from the previous iteration.
After fixed ?, we seek to maximize:
logp(?|xi;wz,w?) =
n
?
i=1
logp(?i|xi;wz,w?)
=
n
?
i=1
log
?
zi
p(?i, zi|xi;wz,w?)
which can be solved with an approxi-
mate solution in Surdeanu et al (2012)
(step 9-11): update zi independently with:
z?i = argmaxzi p(zi|?i,xi;wz,w?). More details
can be found in Surdeanu et al (2012).
In the M-step, we retrain both of the mention-
level and the aggregation level classifiers.
The full EM algorithm is shown in algorithm 1.
4.2 Inference
Inference on a bag xi is trivial. For each mention:
z?ij = argzij?R?{OTHER} max p(zij |xij ,wz)
Followed by the aggregation (directly with w?):
yr(?)i = argyri ?{P,N} max p(y
r
i |zi;wr?)
4.3 Implementation details
We implement our model on top of the
MIML(Surdeanu et al, 2012) code base.8 We
use the same mention-level and aggregate-level
feature sets as Surdeanu et al (2012). We adopt
the same idea of using cross validation for the E
and M steps to avoid overfitting. We initialize our
algorithm by sampling 5% unlabeled examples as
negative, in essence using 1 epoch of MIML to
initialize. Empirically it performs well.
5 Experiments
Data set: We use the KBP (Ji et al, 2011)
dataset9 prepared and publicly released by Surdeanu
et al (2012) for our experiment since it is 1) large
and realistic, 2) publicly available, 3) most im-
portantly, it is the only dataset that has associated
human-labeled ground truth. Any KB held-out eval-
uation without manual assessment will be signif-
icantly affected by KB incompleteness. In KBP
8Available at http://nlp.stanford.edu/software/mimlre.shtml
9Available from Linguistic Data Consortium (LDC).
http://projects.ldc.upenn.edu/kbp/data/
780
Figure 2: Performance on the KBP dataset. The figures on the left, middle and right show MIML, Hoffmann, and
Mintz++ compared to the same MIML-Semi curve, respectively. MIML-Semi is shown in red curves (lighter curves in
black and white) while other algorithms are shown in black curves (darker curves in black and white).
dataset, the training bags are generated by mapping
Wikipedia (http://en.wikipedia.org) infoboxes (after
merging similar types following the KBP 2011 task
definition) into a large unlabeled corpus (consisting
of 1.5M documents from the KBP source corpus and
a complete snapshot of Wikipedia). The KBP shared
task provided 200 query named entities with their as-
sociated slot values (in total several thousand pairs).
We use 40 queries as development dataset (dev), and
the rest (160 queries) as evaluation dataset. We set
? = 0.25 by tuning on the dev set and use it in the
experiments. For a fair comparison, we follow Sur-
deanu et al (2012) and begin by downsampling the
?negative? class to 5%. We also set T=8 and use
the following noisy-or (for ith bag) of mention-level
probability to rank predicted types (r) of pairs and
plot the precision-recall curves for all experiments.
Probi(r) = 1?
?
j
(1? p(zij = r|xij ;wz))
Evaluation: We compare our algorithm (MIML-
semi) to three algorithms: 1) MIML (Surdeanu et
al., 2012), the Multiple-Instance Multiple Label al-
gorithm which labels the bags directly with the KB
(y = ?). 2) MultiR (denoted as Hoffmann) (Hoff-
mann et al, 2011), a Multiple-Instance algorithm
that supports overlapping relations. It also imposes
y = ?. 3) Mintz++ (Surdeanu et al, 2012), a vari-
ant of the single-instance learning algorithm (section
3). The first two are stat-of-the-art Multi-Instance
Multi-Label algorithms. Mintz++ is a strong base-
line (Surdeanu et al, 2012) and an improved ver-
sion of Mintz et al (2009). Figure 2 shows that
our algorithm consistently outperforms all three al-
gorithms at almost all recall levels (with the excep-
tion of a very small region in the PR-curve). This
demonstrates that by treating unla-beled data set dif-
ferently and leveraging the missing positive bags,
MIML-semi is able to learn a more accurate model
for extraction. Although the proposed solution is a
specific algorithm, we believe the idea of treating
unlabeled data differently can be incorporated into
any of these algorithms that only use unlabeled data
as negative examples.
6 Conclusion
We show that the distant-supervision labeling pro-
cess generates a significant number of false nega-
tives because the knowledge base is incomplete. We
proposed an algorithm that learns from only positive
and unlabeled bags. Experimental results demon-
strate its advantage over existing algorithms.
Acknowledgments
Supported in part by the Intelligence Advanced Re-
search Projects Activity (IARPA) via Department of
Interior National Business Center contract number
D11PC20154. The U.S. Government is authorized
to reproduce and distribute reprints for Governmen-
tal purposes notwithstanding any copyright annota-
tion thereon. The views and conclusions contained
herein are those of the authors and should not be
interpreted as necessarily representing the official
policies or endorsements, either expressed or im-
plied, of IARPA, DoI/NBC, or the U.S. Government.
781
References
Razvan Bunescu and Raymond Mooney. 2007. Learning
to extract relations from the web using minimal super-
vision. In Proceedings of the 45th Annual Meeting of
the Association for Computational Linguistics.
Mark Craven and Johan Kumlien. 1999. Constructing bi-
ological knowledge bases by extracting information
from text sources. In Proceedings of the Seventh Inter-
national Conference on Intelligent Systems for Molec-
ular Biology.
Raphael Hoffmann, Congle Zhang, Xiao Ling, Luke
Zettlemoyer, and Daniel S. Weld. 2011. Knowledge-
based weak supervision for information extraction of
overlapping relations. In Proceedings of the Annual
Meeting of the Association for Computational Linguis-
tics.
Heng Ji, Ralph Grishman, and Hoa T. Dang. 2011.
Overview of the TAC 2011 knowledge base popula-
tion track. In Proceedings of the Text Analytics Con-
ference.
Jing Jiang and ChengXiang Zhai. 2007. A systematic ex-
ploration of the feature space for relation extraction. In
Proceedings of HLT-NAACL-2007.
Nanda Kambhatla. 2004. Combining lexical, syntactic,
and semantic features with maximum entropy mod-
els for information extraction. In Proceedings of ACL-
2004.
Scott Miller, Heidi Fox, Lance Ramshaw, and Ralph
Weischedel. 2000. A novel use of statistical parsing
to extract information from text. In Proceedings of
NAACL-2000.
Bonan Min, Shuming Shi, Ralph Grishman and Chin-
Yew Lin. 2012a. Ensemble Semantics for Large-scale
Unsupervised Relation Extraction. In Proceedings of
EMNLP-CoNLL 2012.
Bonan Min, Xiang Li, Ralph Grishman and Ang Sun.
2012b. New York University 2012 System for KBP
Slot Filling. In Proceedings of the Text Analysis Con-
ference (TAC) 2012.
Mike Mintz, Steven Bills, Rion Snow, and Daniel Juraf-
sky. 2009. Distant supervision for relation extraction
without labeled data. In Proceedings of the 47th An-
nual Meeting of the Association for Computational
Linguistics.
Truc Vien T. Nguyen and Alessandro Moschitti. 2011.
End-to-end relation extraction using distant supervi-
sion from external semantic repositories. In Proceed-
ings of the 49th Annual Meeting of the Association for
Computational Linguistics: Human Language Tech-
nologies.
Sebastian Riedel, Limin Yao, and Andrew McCallum.
2010. Modeling relations and their mentions without
labeled text. In Proceedings of the European Confer-
ence on Machine Learning and Knowledge Discovery
in Databases (ECML PKDD 10).
Ang Sun, Ralph Grishman, Wei Xu, and Bonan Min.
2011. New York University 2011 system for KBP slot
filling. In Proceedings of the Text Analytics Confer-
ence.
Mihai Surdeanu, Sonal Gupta, John Bauer, David Mc-
Closky, Angel X. Chang, Valentin I. Spitkovsky, and
Christopher D. Manning. 2011. Stanfords distantly-
supervised slot-filling system. In Proceedings of the
Text Analytics Conference.
Mihai Surdeanu, Julie Tibshirani, Ramesh Nallapati,
Christopher D. Manning. 2012. Multi-instance Multi-
label Learning for Relation Extraction. In Proceed-
ings of the 2012 Conference on Empirical Methods in
Natural Language Processing and Natural Language
Learning.
TAC KBP 2011 task definition. 2011. http://nlp
.cs.qc.cuny.edu/kbp/2011/KBP2011 TaskDefinition.pdf
Shingo Takamatsu, Issei Sato, Hiroshi Nakagawa. 2012.
ReducingWrong Labels in Distant Supervision for Re-
lation Extraction. In Proceedings of 50th Annual Meet-
ing of the Association for Computational Linguistics.
Fei Wu and Daniel S. Weld. 2007. Autonomously seman-
tifying wikipedia. In Proceedings of the International
Conference on Information and Knowledge Manage-
ment (CIKM-2007).
Guodong Zhou, Jian Su, Jie Zhang and Min Zhang. 2005.
Exploring various knowledge in relation extraction. In
Proceedings of ACL-2005.
782
Proceedings of the NAACL HLT 2010 First International Workshop on Formalisms and Methodology for Learning by Reading, pages 24?33,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Large Scale Relation Detection?
Chris Welty and James Fan and David Gondek and Andrew Schlaikjer
IBM Watson Research Center ? 19 Skyline Drive ? Hawthorne, NY 10532, USA
{welty, fanj, dgondek, ahschlai}@us.ibm.com
Abstract
We present a technique for reading sentences
and producing sets of hypothetical relations
that the sentence may be expressing. The
technique uses large amounts of instance-level
background knowledge about the relations in
order to gather statistics on the various ways
the relation may be expressed in language, and
was inspired by the observation that half of the
linguistic forms used to express relations oc-
cur very infrequently and are simply not con-
sidered by systems that use too few seed ex-
amples. Some very early experiments are pre-
sented that show promising results.
1 Introduction
We are building a system that learns to read in a new
domain by applying a novel combination of natural
language processing, machine learning, knowledge
representation and reasoning, information retrieval,
data mining, etc. techniques in an integrated way.
Central to our approach is the view that all parts of
the system should be able to interact during any level
of processing, rather than a pipeline view in which
certain parts of the system only take as input the re-
sults of other parts, and thus cannot influence those
results. In this paper we discuss a particular case
of that idea, using large knowledge bases hand in
hand with natural language processing to improve
the quality of relation detection. Ultimately we de-
fine reading as representing natural language text in
? Research supported in part by DARPA MRP Grant
FA8750-09-C0172
a way that integrates background knowledge and in-
ference, and thus are doing the relation detection
to better integrate text with pre-existing knowledge,
however that should not (and does not) prevent us
from using what knowledge we have to influence
that integration along the way.
2 Background
The most obvious points of interaction between NLP
and KR systems are named entity tagging and other
forms of type instance extraction. The second ma-
jor point of interaction is relation extraction, and
while there are many kinds of relations that may
be detected (e.g. syntactic relations such as modi-
fiers and verb subject/object, equivalence relations
like coreference or nicknames, event frame relations
such as participants, etc.), the kind of relations that
reading systems need to extract to support domain-
specific reasoning tasks are relations that are known
to be expressed in supporting knowledge-bases. We
call these relations semantic relations in this paper.
Compared to entity and type detection, extraction
of semantic relations is significantly harder. In our
work on bridging the NLP-KR gap, we have ob-
served several aspects of what makes this task dif-
ficult, which we discuss below.
2.1 Keep reading
Humans do not read and understand text by first rec-
ognizing named entities, giving them types, and then
finding a small fixed set of relations between them.
Rather, humans start with the first sentence and build
up a representation of what they read that expands
and is refined during reading. Furthermore, humans
24
do not ?populate databases? by reading; knowledge
is not only a product of reading, it is an integral part
of it. We require knowledge during reading in order
to understand what we read.
One of the central tenets of our machine reading
system is the notion that reading is not performed on
sentences in isolation. Often, problems in NLP can
be resolved by simply waiting for the next sentence,
or remembering the results from the previous, and
incorporating background or domain specific knowl-
edge. This includes parse ambiguity, coreference,
typing of named entities, etc. We call this the Keep
Reading principle.
Keep reading applies to relation extraction as
well. Most relation extraction systems are imple-
mented such that a single interpretation is forced
on a sentence, based only on features of the sen-
tence itself. In fact, this has been a shortcoming
of many NLP systems in the past. However, when
you apply the Keep Reading principle, multiple hy-
potheses from different parts of the NLP pipeline are
maintained, and decisions are deferred until there is
enough evidence to make a high confidence choice
between competing hypotheses. Knowledge, such
as those entities already known to participate in a
relation and how that relation was expressed, can
and should be part of that evidence. We will present
many examples of the principle in subsequent sec-
tions.
2.2 Expressing relations in language
Due to the flexibility and expressive power of nat-
ural language, a specific type of semantic relation
can usually be expressed in language in a myriad
of ways. In addition, semantic relations are of-
ten implied by the expression of other relations.
For example, all of the following sentences more
or less express the same relation between an actor
and a movie: (1) ?Elijah wood starred in Lord of
the Rings: The Fellowship of the Ring?, (2) ?Lord
of the Rings: The Fellowship of the Ring?s Elijah
Wood, ...?, and(3) ?Elijah Wood?s coming of age
was clearly his portrayal of the dedicated and noble
hobbit that led the eponymous fellowship from the
first episode of the Lord of the Rings trilogy.? No
human reader would have any trouble recognizing
the relation, but clearly this variability of expression
presents a major problem for machine reading sys-
tems.
To get an empirical sense of the variability of nat-
ural language used to express a relation, we stud-
ied a few semantic relations and found sentences
that expressed that relation, extracted simple pat-
terns to account for how the relation is expressed
between two arguments, mainly by removing the re-
lation arguments (e.g. ?Elijah Wood? and ?Lord of
the Rings: The Fellowship of the Ring? above) and
replacing them with variables. We then counted the
number of times each pattern was used to express
the relation, producing a recognizable very long tail
shown in Figure 1 for the top 50 patterns expressing
the acted-in-movie relation in 17k sentences. More
sophisticated pattern generalization (as discussed in
later sections) would significantly fatten the head,
bringing it closer to the traditional 50% of the area
under the curve, but no amount of generalization
will eliminate the tail. The patterns become increas-
ingly esoteric, such as ?The movie Death Becomes
Her features a brief sequence in which Bruce Willis
and Goldie Hawn?s characters plan Meryl Streep?s
character?s death by sending her car off of a cliff
on Mulholland Drive,? or ?The best known Hawk-
sian woman is probably Lauren Bacall, who iconi-
cally played the type opposite Humphrey Bogart in
To Have and Have Not and The Big Sleep.?
2.3 What relations matter
We do not consider relation extraction to be an end
in and of itself, but rather as a component in larger
systems that perform some task requiring interoper-
ation between language- and knowledge-based com-
ponents. Such larger tasks can include question
answering, medical diagnosis, intelligence analysis,
museum curation, etc. These tasks have evaluation
criteria that go beyond measuring relation extraction
results. The first step in applying relation detection
to these larger tasks is analysis to determine what
relations matter for the task and domain.
There are a number of manual and semi-automatic
ways to perform such analysis. Repeating the
theme of this paper, which is to use pre-existing
knowledge-bases as resources, we performed this
analysis using freebase and a set of 20k question-
answer pairs representing our task domain. For each
question, we formed tuples of each entity name in
the question (QNE) with the answer, and found all
25
 0
100
200
300
400
500
600
700
800
900
1000
Figure 1: Pattern frequency for acted-in-movie relation for 17k sentences.
 
0
10
20
30
40
50
60
70
80
Figure 2: Relative frequency for top 50 relations in 20K question-answer pairs.
the relations in the KB connecting the entities. We
kept a count for each relation of how often it con-
nected a QNE to an answer. Of course we don?t ac-
tually know for sure that the relation is the one being
asked, but the intuition is that if the amount of data
is big enough, you will have at least a ranked list of
which relations are the most frequent.
Figure 2 shows the ranking for the top 50 rela-
tions. Note that, even when restricted to the top 50
relations, the graph has no head, it is basically all
tail; The top 50 relations cover about 15% of the do-
main. In smaller, manual attempts to determine the
most frequent relations in our domain, we had a sim-
ilar result. What this means is that supporting even
the top 50 relations with perfect recall covers about
15% of the questions. It is possible, of course, to
narrow the domain and restrict the relations that can
be queried?this is what database systems do. For
reading, however, the results are the same. A read-
ing system requires the ability to recognize hundreds
of relations to have any significant impact on under-
standing.
2.4 Multi-relation learning on many seeds
The results shown in Figure 1 and Figure 2 con-
firmed much of the analysis and experiences we?d
had in the past trying to apply relation extraction in
the traditional way to natural language problems like
26
question answering, building concept graphs from
intelligence reports, semantic search, etc. Either by
training machine learning algorithms on manually
annotated data or by manually crafting finite-state
transducers, relation detection is faced by this two-
fold problem: the per-relation extraction hits a wall
around 50% recall, and each relation itself occurs
infrequently in the data.
This apparent futility of relation extraction led us
to rethink our approach. First of all, the very long
tail for relation patterns led us to consider how to
pick up the tail. We concluded that to do so would
require many more examples of the relation, but
where can we get them? In the world of linked-data,
huge instance-centered knowledge-bases are rapidly
growing and spreading on the semantic web1. Re-
sources like DBPedia, Freebase, IMDB, Geonames,
the Gene Ontology, etc., are making available RDF-
based data about a number of domains. These
sources of structured knowledge can provide a large
number of seed tuples for many different relations.
This is discussed further below.
Furthermore, the all-tail nature of relation cover-
age led us to consider performing relation extraction
on multiple relations at once. Some promising re-
sults on multi-relation learning have already been re-
ported in (Carlson et al, 2009), and the data sources
mentioned above give us many more than just the
handful of seed instances used in those experiments.
The idea of learning multiple relations at once also
fits with our keep reading principle - multiple rela-
tion hypotheses may be annotated between the same
arguments, with further evidence helping to disam-
biguate them.
3 Approach
One common approach to relation extraction is to
start with seed tuples and find sentences that con-
tain mentions of both elements of the tuple. From
each such sentence a pattern is generated using at
minimum universal generalization (replace the tuple
elements with variables), though adding any form of
generalization here can significantly improve recall.
Finally, evaluate the patterns by applying them to
text and evaluating the precision and recall of the tu-
ples extracted by the patterns. Our approach, called
1http://linkeddata.org/
Large Scale Relation Detection (LSRD), differs in
three important ways:
1. We start with a knowledge-base containing a
large number (thousands to millions) of tuples
encoding relation instances of various types.
Our hypothesis is that only a large number of
examples can possibly account for the long tail.
2. We do not learn one relation at a time, but
rather, associate a pattern with a set of relations
whose tuples appear in that pattern. Thus, when
a pattern is matched to a sentence during read-
ing, each relation in its set of associated rela-
tions is posited as a hypothetical interpretation
of the sentence, to be supported or refuted by
further reading.
3. We use the knowledge-base as an oracle to de-
termine negative examples of a relation. As
a result the technique is semi-supervised; it
requires no human intervention but does re-
quire reliable knowledge-bases as input?these
knowledge-bases are readily available today.
Many relation extraction techniques depend on a
prior step of named entity recognition (NER) and
typing, in order to identify potential arguments.
However, this limits recall to the recall of the NER
step. In our approach patterns can match on any
noun phrase, and typing of these NPs is simply an-
other form of evidence.
All this means our approach is not relation extrac-
tion per se, it typically does not make conclusions
about a relation in a sentence, but extracts hypothe-
ses to be resolved by other parts of our reading sys-
tem.
In the following sections, we elaborate on the
technique and some details of the current implemen-
tation.
3.1 Basic pipeline
The two principle inputs are a corpus and a
knowledge-base (KB). For the experiments below,
we used the English Gigaword corpus2 extended
with Wikipedia and other news sources, and IMDB,
DBPedia, and Freebase KBs, as shown. The intent is
2http://www.ldc.upenn.edu/Catalog/
CatalogEntry.jsp?catalogId=LDC2003T05
27
to run against a web-scale corpus and larger linked-
data sets.
Input documents are sentence delimited, tok-
enized and parsed. The technique can benefit dra-
matically from coreference resolution, however in
the experiments shown, this was not present. For
each pair of proper names in a sentence, the names
are looked up in the KB, and if they are related,
a pattern is extracted from the sentence. At min-
imum, pattern extraction should replace the names
with variables. Depending on how patterns are ex-
tracted, one pattern may be extracted per sentence,
or one pattern may be extracted per pair of proper
names in the sentence. Each pattern is associated
with all the relations known in the KB between the
two proper names. If the pattern has been extracted
before, the two are merged by incrementing the as-
sociated relation counts. This phase, called pattern
induction, is repeated for the entire corpus, resulting
in a large set of patterns, each pattern associated with
relations. For each ?pattern, relation? pair, there is a
count of the number of times that pattern appeared
in the corpus with names that are in the relation ac-
cording to the KB.
The pattern induction phase results in positive
counts, i.e. the number of times a pattern appeared
in the corpus with named entities known to be re-
lated in the KB. However, the induction phase does
not exhaustively count the number of times each pat-
tern appears in the corpus, as a pattern may appear
with entities that are not known in the KB, or are not
known to be related. The second phase, called pat-
tern training, goes through the entire corpus again,
trying to match induced patterns to sentences, bind-
ing any noun phrase to the pattern variables. Some
attempt is made to resolve the noun phrase to some-
thing (most obviously, a name) that can be looked
up in the KB, and for each relation associated with
the pattern, if the two names are not in the relation
according to the KB, the negative count for that re-
lation in the matched pattern is incremented. The
result of the pattern training phase is an updated set
of ?pattern, relation? pairs with negative counts.
The following example illustrates the basic pro-
cessing. During induction, this sentence is encoun-
tered:
Tom Cruise and co-star Nicole Kidman
appeared together at the premier.
The proper names ?Tom Cruise? and ?Nicole Kid-
man? are recognized and looked up in the KB. We
find instances in the KB with those names, and the
following relations: coStar(Tom Cruise,
Nicole Kidman); marriedTo(Tom
Cruise, Nicole Kidman). We extract a
pattern p1: ?x and co-star ?y appeared
together at the premier in which all the
names have been replace by variables, and the
associations <p1, costar, 1, 0> and <p1,
marriedTo, 1, 0> with positive counts and
zero negative counts. Over the entire corpus, we?d
expect the pattern to appear a few times and end
up with final positive counts like <p1, coStar,
14, 0> and <p1, marriedTo, 2, 0>, in-
dicating the pattern p1 appeared 14 times in the
corpus between names known to participate in the
coStar relation, and twice between names known
to participate in the marriedTo relation. During
training, the following sentence is encountered that
matches p1:
Tom Hanks and co-star Daryl Hannah ap-
peared together at the premier.
The names ?Tom Hanks? and ?Daryl Hannah?
are looked up in the KB and in this case only
the relation coStar is found between them, so the
marriedTo association is updated with a negative
count: <p1, marriedTo, 2, -1>. Over the
entire corpus, we?d expect the counts to be some-
thing like <p1, costar, 14, -6> and <p1,
marriedTo, 2, -18>.
This is a very simple example and it is difficult to
see the value of the pattern training phase, as it may
appear the negative counts could be collected during
the induction phase. There are several reasons why
this is not so. First of all, since the first phase only
induces patterns between proper names that appear
and are related within the KB, a sentence in the cor-
pus matching the pattern would be missed if it did
not meet that criteria but was encountered before the
pattern was induced. Secondly, for reasons that are
beyond the scope of this paper, having to do with
our Keep Reading principle, the second phase does
slightly more general matching: note that it matches
noun phrases instead of proper nouns.
28
3.2 Candidate-instance matching
An obvious part of the process in both phases is
taking strings from text and matching them against
names or labels in the KB. We refer to the strings in
the sentences as candidate arguments or simply can-
didates, and refer to instances in the KB as entities
with associated attributes. For simplicity of discus-
sion we will assume all KBs are in RDF, and thus
all KB instances are nodes in a graph with unique
identifiers (URIs) and arcs connecting them to other
instances or primitive values (strings, numbers, etc.).
A set of specially designated arcs, called labels, con-
nect instances to strings that are understood to name
the instances. The reverse lookup of entity identi-
fiers via names referred to in the previous section
requires searching for the labels that match a string
found in a sentence and returning the instance iden-
tifier.
This step is so obvious it belies the difficultly of
the matching process and is often overlooked, how-
ever in our experiments we have found candidate-
instance matching to be a significant source of error.
Problems include having many instances with the
same or lexically similar names, slight variations in
spelling especially with non-English names, inflex-
ibility or inefficiency in string matching in KB im-
plementations, etc. In some of our sources, names
are also encoded as URLs. In the case of movie
and book titles-two of the domains we experimented
with-the titles seem almost as if they were designed
specifically to befuddle attempts to automatically
recognize them. Just about every English word is a
book or movie title, including ?It?, ?Them?, ?And?,
etc., many years are titles, and just about every num-
ber under 1000. Longer titles are difficult as well,
since simple lexical variations can prevent matching
from succeeding, e.g. the Shakespeare play, A Mid-
summer Night?s Dream appears often as Midsummer
Night?s Dream, A Midsummer Night Dream, and oc-
casionally, in context, just Dream. When titles are
not distinguished or delimited somehow, they can
confuse parsing which may fail to recognize them as
noun phrases. We eventually had to build dictionar-
ies of multi-word titles to help parsing, but of course
that was imperfect as well.
The problems go beyond the analogous ones in
coreference resolution as the sources and technology
themselves are different. The problems are severe
enough that the candidate-instance matching prob-
lem contributes the most, of all components in this
pipeline, to precision and recall failures. We have
observed recall drops of as much as 15% and preci-
sion drops of 10% due to candidate-instance match-
ing.
This problem has been studied somewhat in the
literature, especially in the area of database record
matching and coreference resolution (Michelson and
Knoblock, 2007), but the experiments presented be-
low use rudimentary solutions and would benefit
significantly from improvements; it is important to
acknowledge that the problem exists and is not as
trivial as it appears at first glance.
3.3 Pattern representation
The basic approach accommodates any pattern rep-
resentation, and in fact we can accommodate non
pattern-based learning approaches, such as CRFs, as
the primary hypothesis is principally concerned with
the number of seed examples (scaling up initial set
of examples is important). Thus far we have only
experimented with two pattern representations: sim-
ple lexical patterns in which the known arguments
are replaced in the sentence by variables (as shown
in the example above), and patterns based on the
spanning tree between the two arguments in a de-
pendency parse, again with the known arguments re-
placed by variables. In our initial design we down-
played the importance of the pattern representation
and especially generalization, with the belief that
very large scale would remove the need to general-
ize. However, our initial experiments suggest that
good pattern generalization would have a signifi-
cant impact on recall, without negative impact on
precision, which agrees with findings in the litera-
ture (Pantel and Pennacchiotti, 2006). Thus, these
early results only employ rudimentary pattern gen-
eralization techniques, though this is an area we in-
tend to improve. We discuss some more details of
the lack of generalization below.
4 Experiment
In this section we present a set of very early proof of
concept experiments performed using drastic simpli-
fications of the LSRD design. We began, in fact, by
29
Relation Prec Rec F1 Tuples Seeds
imdb:actedIn 46.3 45.8 0.46 9M 30K
frb:authorOf 23.4 27.5 0.25 2M 2M
imdb:directorOf 22.8 22.4 0.22 700K 700K
frb:parentOf 68.2 8.6 0.16 10K 10K
Table 1: Precision and recall vs. number of tuples used
for 4 freebase relations.
using single-relation experiments, despite the cen-
trality of multiple hypotheses to our reading system,
in order to facilitate evaluation and understanding of
the technique. Our main focus was to gather data
to support (or refute) the hypothesis that more re-
lation examples would matter during pattern induc-
tion, and that using the KB as an oracle for training
would work. Clearly, no KB is complete to begin
with, and candidate-instance matching errors drop
apparent coverage further, so we intended to explore
the degree to which the KB?s coverage of the relation
impacted performance. To accomplish this, we ex-
amined four relations with different coverage char-
acteristics in the KB.
4.1 Setup and results
The first relation we tried was the acted-in-show
relation from IMDB; for convenience we refer to
it as imdb:actedIn. An IMDB show is a movie,
TV episode, or series. This relation has over 9M
<actor, show> tuples, and its coverage was
complete as far as we were able to determine. How-
ever, the version we used did not have a lot of name
variations for actors. The second relation was the
author-of relation from Freebase (frb:authorOf ),
with roughly 2M <author, written-work>
tuples. The third relation was the director-of-
movie relation from IMDB (imdb:directorOf ), with
700k <director,movie> tuples. The fourth
relation was the parent-of relation from Free-
base (frb:parentOf ), with roughly 10K <parent,
child> tuples (mostly biblical and entertainment).
Results are shown in Table 1.
The imdb:actedIn experiment was performed on
the first version of the system that ran on 1 CPU and,
due to resource constraints, was not able to use more
than 30K seed tuples for the rule induction phase.
However, the full KB (9M relation instances) was
available for the training phase. With some man-
ual effort, we selected tuples (actor-movie pairs) of
popular actors and movies that we expected to ap-
pear most frequently in the corpus. In the other ex-
periments, the full tuple set was available for both
phases, but 2M tuples was the limit for the size of
the KB in the implementation. With these promising
preliminary results, we expect a full implementation
to accommodate up to 1B tuples or more.
The evaluation was performed in decreasing de-
grees of rigor. The imdb:actedIn experiment was run
against 20K sentences with roughly 1000 actor in
movie relations and checked by hand. For the other
three, the same sentences were used, but the ground
truth was generated in a semi-automatic way by re-
using the LSRD assumption that a sentence con-
taining tuples in the relation expresses the relation,
and then spot-checked manually. Thus the evalua-
tion for these three experiments favors the LSRD ap-
proach, though spot checking revealed it is the pre-
cision and not the recall that benefits most from this,
and all the recall problems in the ground truth (i.e.
sentences that did express the relation but were not
in the ground truth) were due to candidate-instance
matching problems. An additional idiosyncrasy in
the evaluation is that the sentences in the ground
truth were actually questions, in which one of the
arguments to the relation was the answer. Since
the patterns were induced and trained on statements,
there is a mismatch in style which also significantly
impacts recall. Thus the precision and recall num-
bers should not be taken as general performance, but
are useful only relative to each other.
4.2 Discussion
The results are promising, and we are continuing the
work with a scalable implementation. Overall, the
results seem to show a clear correlation between the
number of seed tuples and relation extraction recall.
However, the results do not as clearly support the
many examples hypothesis as it may seem. When
an actor and a film that actor starred in are men-
tioned in a sentence, it is very often the case that the
sentence expresses that relation. However, this was
less likely in the case of the parent-of relation, and
as we considered other relations, we found a wide
degree of variation. The borders relation between
two countries, for example, is on the other extreme
from actor-in-movie. Bordering nations often wage
30
war, trade, suspend relations, deport refugees, sup-
port, oppose, etc. each other, so finding the two na-
tions in a sentence together is not highly indicative
of one relation or another. The director-of-movie re-
lation was closer to acted-in-movie in this regard,
and author-of a bit below that. The obvious next step
to gather more data on the many examples hypoth-
esis is to run the experiments with one relation, in-
creasing the number of tuples with each experiment
and observing the change in precision and recall.
The recall results do not seem particularly strik-
ing, though these experiments do not include pat-
tern generalization (other than what a dependency
parse provides) or coreference, use a small corpus,
and poor candidate-instance matching. Further, as
noted above there were other idiosyncrasies in the
evaluation that make them only useful for relative
comparison, not as general results.
Many of the patterns induced, especially for
the acted-in-movie relation, were highly lexical,
using e.g. parenthesis or other punctuation to
signal the relation. For example, a common
pattern was actor-name (movie-name), or
movie-name: actor-name, e.g. ?Leonardo
DiCaprio (Titanic) was considering accepting the
role as Anakin Skywalker,? or ?Titanic: Leonardo
DiCaprio and Kate Blanchett steam up the silver
screen against the backdrop of the infamous disas-
ter.? Clearly patterns like this rely heavily on the
context and typing to work. In general the pattern
?x (?y) is not reliable for the actor-in-movie re-
lation unless you know ?x is an actor and ?y is a
movie. However, some patterns, like ?x appears
in the screen epic ?y is highly indicative
of the relation without the types at all - in fact it is
so high precision it could be used to infer the types
of ?x and ?y if they were not known. This seems
to fit extremely well in our larger reading system,
in which the pattern itself provides one form of evi-
dence to be combined with others, but was not a part
of our evaluation.
One of the most important things to general-
ize in the patterns we observed was dates. If
patterns like, actor-name appears in the
1994 screen epic movie-name could have
been generalized to actor-name appears in
the date screen epic movie-name, re-
call would have been boosted significantly. As it
stood in these experiments, everything but the argu-
ments had to match. Similarly, many relations often
appear in lists, and our patterns were not able to gen-
eralize that away. For example the sentence, ?Mark
Hamill appeared in Star Wars, Star Wars: The Em-
pire Strikes Back, and Star Wars: The Return of the
Jedi,? causes three patterns to be induced; in each,
one of the movies is replaced by a variable in the
pattern and the other two are required to be present.
Then of course all this needs to be combined, so that
the sentence, ?Indiana Jones and the Last Crusade is
a 1989 adventure film directed by Steven Spielberg
and starring Harrison Ford, Sean Connery, Denholm
Elliott and Julian Glover,? would generate a pattern
that would get the right arguments out of ?Titanic
is a 1997 epic film directed by James Cameron and
starring Leonardo DiCaprio, Kate Winslett, Kathy
Bates and Bill Paxon.? At the moment the former
sentence generates four patterns that require the di-
rector and dates to be exactly the same.
Some articles in the corpus were biographies
which were rich with relation content but also with
pervasive anaphora, name abbreviations, and other
coreference manifestations that severely hampered
induction and evaluation.
5 Related work
Early work in semi-supervised learning techniques
such as co-training and multi-view learning (Blum
and Mitchell, 1998) laid much of the ground work
for subsequent experiments in bootstrapped learn-
ing for various NLP tasks, including named entity
detection (Craven et al, 2000; Etzioni et al, 2005)
and document classification (Nigam et al, 2006).
This work?s pattern induction technique also repre-
sents a semi-supervised approach, here applied to
relation learning, and at face value is similar in mo-
tivation to many of the other reported experiments
in large scale relation learning (Banko and Etzioni,
2008; Yates and Etzioni, 2009; Carlson et al, 2009;
Carlson et al, 2010). However, previous techniques
generally rely on a small set of example relation in-
stances and/or patterns, whereas here we explicitly
require a larger source of relation instances for pat-
tern induction and training. This allows us to better
evaluate the precision of all learned patterns across
multiple relation types, as well as improve coverage
31
of the pattern space for any given relation.
Another fundamental aspect of our approach lies
in the fact that we attempt to learn many relations
simultaneously. Previously, (Whitelaw et al, 2008)
found that such a joint learning approach was use-
ful for large-scale named entity detection, and we
expect to see this result carry over to the relation ex-
traction task. (Carlson et al, 2010) also describes
relation learning in a multi-task learning framework,
and attempts to optimize various constraints posited
across all relation classes.
Examples of the use of negative evidence
for learning the strength of associations between
learned patterns and relation classes as proposed
here has not been reported in prior work to our
knowledge. A number of multi-class learning tech-
niques require negative examples in order to prop-
erly learn discriminative features of positive class
instances. To address this requirement, a number of
approaches have been suggested in the literature for
selection or generation of negative class instances.
For example, sampling from the positive instances
of other classes, randomly perturbing known pos-
itive instances, or breaking known semantic con-
straints of the positive class (e.g. positing multiple
state capitols for the same state). With this work,
we treat our existing RDF store as an oracle, and as-
sume it is sufficiently comprehensive that it allows
estimation of negative evidence for all target relation
classes simultaneously.
The first (induction) phase of LSRD is very simi-
lar to PORE (Wang et al, 2007) (Dolby et al, 2009;
Gabrilovich and Markovitch, 2007) and (Nguyen
et al, 2007), in which positive examples were ex-
tracted from Wikipedia infoboxes. These also bear
striking similarity to (Agichtein and Gravano, 2000),
and all suffer from a significantly smaller number of
seed examples. Indeed, its not using a database of
specific tuples that distinguishes LSRD, but that it
uses so many; the scale of the induction in LSRD
is designed to capture far less frequent patterns by
using significantly more seeds
In (Ramakrishnan et al, 2006) the same intu-
ition is captured that knowledge of the structure of
a database should be employed when trying to inter-
pret text, though again the three basic hypotheses of
LSRD are not supported.
In (Huang et al, 2004), a similar phenomenon to
what we observed with the acted-in-movie relation
was reported in which the chances of a protein in-
teraction relation being expressed in a sentence are
already quite high if two proteins are mentioned in
that sentence.
6 Conclusion
We have presented an approach for Large Scale Re-
lation Detection (LSRD) that is intended to be used
within a machine reading system as a source of hy-
pothetical interpretations of input sentences in natu-
ral language. The interpretations produced are se-
mantic relations between named arguments in the
sentences, and they are produced by using a large
knowledge source to generate many possible pat-
terns for expressing the relations known by that
source.
We have specifically targeted the technique at the
problem that the frequency of patterns occurring in
text that express a particular relation has a very long
tail (see Figure 1), and without enough seed exam-
ples the extremely infrequent expressions of the re-
lation will never be found and learned. Further, we
do not commit to any learning strategy at this stage
of processing, rather we simply produce counts, for
each relation, of how often a particular pattern pro-
duces tuples that are in that relation, and how of-
ten it doesn?t. These counts are simply used as ev-
idence for different possible interpretations, which
can be supported or refuted by other components in
the reading system, such as type detection.
We presented some very early results which while
promising are not conclusive. There were many
idiosyncrasies in the evaluation that made the re-
sults meaningful only with respect to other experi-
ments that were evaluated the same way. In addi-
tion, the evaluation was done at a component level,
as if the technique were a traditional relation extrac-
tion component, which ignores one of its primary
differentiators?that it produces sets of hypothetical
interpretations. Instead, the evaluation was done
only on the top hypothesis independent of other evi-
dence.
Despite these problems, the intuitions behind
LSRD still seem to us valid, and we are investing in a
truly large scale implementation that will overcome
the problems discussed here and can provide more
32
valid evidence to support or refute the hypotheses
LSRD is based on:
1. A large number of examples can account for the
long tail in relation expression;
2. Producing sets of hypothetical interpretations
of the sentence, to be supported or refuted by
further reading, works better than producing
one;
3. Using existing, large, linked-data knowledge-
bases as oracles can be effective in relation de-
tection.
References
[Agichtein and Gravano2000] E. Agichtein and L. Gra-
vano. 2000. Snowball: extracting relations from large
plain-text collections. In Proceedings of the 5th ACM
Conference on Digital Libraries, pages 85?94, San
Antonio, Texas, United States, June. ACM.
[Banko and Etzioni2008] Michele Banko and Oren Et-
zioni. 2008. The tradeoffs between open and tradi-
tional relation extraction. In Proceedings of the 46th
Annual Meeting of the Association for Computational
Linguistics.
[Blum and Mitchell1998] A. Blum and T. Mitchell. 1998.
Combining labeled and unlabeled data with co-
training. In Proceedings of the 1998 Conference on
Computational Learning Theory.
[Carlson et al2009] A. Carlson, J. Betteridge, E. R. Hr-
uschka Jr., and T. M. Mitchell. 2009. Coupling semi-
supervised learning of categories and relations. In
Proceedings of the NAACL HLT 2009 Workshop on
Semi-supervised Learning for Natural Language Pro-
cessing.
[Carlson et al2010] A. Carlson, J. Betteridge, R. C.
Wang, E. R. Hruschka Jr., and T. M. Mitchell. 2010.
Coupled semi-supervised learning for information ex-
traction. In Proceedings of the 3rd ACM International
Conference on Web Search and Data Mining.
[Craven et al2000] Mark Craven, Dan DiPasquo, Dayne
Freitag, Andrew McCallum, Tom Mitchell, Kamal
Nigam, and Sean Slattery. 2000. Learning to construct
knowledge bases from the World Wide Web. Artificial
Intelligence, 118(1?2):69?113.
[Dolby et al2009] Julian Dolby, Achille Fokoue, Aditya
Kalyanpur, Edith Schonberg, and Kavitha Srinivas.
2009. Extracting enterprise vocabularies using linked
open data. In Proceedings of the 8th International Se-
mantic Web Conference.
[Etzioni et al2005] Oren Etzioni, Michael Cafarella,
Doug Downey, Ana-Maria Popescu, Tal Shaked,
Stephen Soderland, Daniel S. Weld, and Alexander
Yates. 2005. Unsupervised named-entity extraction
from the web: An experimental study. Artificial Intel-
ligence, 165(1):91?134, June.
[Gabrilovich and Markovitch2007] Evgeniy Gabrilovich
and Shaul Markovitch. 2007. Computing seman-
tic relatedness using wikipedia-based explicit seman-
tic analysis. In IJCAI.
[Huang et al2004] Minlie Huang, Xiaoyan Zhu, Yu Hao,
Donald G. Payan, Kunbin Qu, and Ming Li. 2004.
Discovering patterns to extract protein-protein interac-
tions from full texts. Bioinformatics, 20(18).
[Michelson and Knoblock2007] Matthew Michelson and
Craig A. Knoblock. 2007. Mining heterogeneous
transformations for record linkage. In Proceedings of
the 6th International Workshop on Information Inte-
gration on the Web, pages 68?73.
[Nguyen et al2007] Dat P. Nguyen, Yutaka Matsuo, ,
and Mitsuru Ishizuka. 2007. Exploiting syntactic
and semantic information for relation extraction from
wikipedia. In IJCAI.
[Nigam et al2006] K. Nigam, A. McCallum, , and
T. Mitchell, 2006. Semi-Supervised Learning, chapter
Semi-Supervised Text Classification Using EM. MIT
Press.
[Pantel and Pennacchiotti2006] Patrick Pantel and Marco
Pennacchiotti. 2006. Espresso: Leveraging generic
patterns for automatically harvesting semantic rela-
tions. In Proceedings of the 21st international Confer-
ence on Computational Linguistics and the 44th An-
nual Meeting of the Association For Computational
Linguistics, Sydney, Australia, July.
[Ramakrishnan et al2006] Cartic Ramakrishnan, Krys J.
Kochut, and Amit P. Sheth. 2006. A framework for
schema-driven relationship discovery from unstruc-
tured text. In ISWC.
[Wang et al2007] Gang Wang, Yong Yu, and Haiping
Zhu. 2007. PORE: Positive-only relation extraction
from wikipedia text. In ISWC.
[Whitelaw et al2008] C. Whitelaw, A. Kehlenbeck,
N. Petrovic, , and L. Ungar. 2008. Web-scale named
entity recognition. In Proceeding of the 17th ACM
Conference on information and Knowledge Manage-
ment, pages 123?132, Napa Valley, California, USA,
October. ACM.
[Yates and Etzioni2009] Alexander Yates and Oren Et-
zioni. 2009. Unsupervised methods for determining
object and relation synonyms on the web. Artificial
Intelligence, 34:255?296.
33
Proceedings of the NAACL HLT 2010 First International Workshop on Formalisms and Methodology for Learning by Reading, pages 122?127,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
PRISMATIC: Inducing Knowledge from a Large Scale Lexicalized Relation
Resource?
James Fan and David Ferrucci and David Gondek and Aditya Kalyanpur
IBM Watson Research Lab
19 Skyline Dr
Hawthorne, NY 10532
{fanj, ferrucci, gondek, adityakal}@us.ibm.com
Abstract
One of the main bottlenecks in natural lan-
guage processing is the lack of a comprehen-
sive lexicalized relation resource that contains
fine grained knowledge on predicates. In this
paper, we present PRISMATIC, a large scale
lexicalized relation resource that is automati-
cally created over 30 gb of text. Specifically,
we describe what kind of information is col-
lected in PRISMATIC and how it compares
with existing lexical resources. Our main fo-
cus has been on building the infrastructure and
gathering the data. Although we are still in
the early stages of applying PRISMATIC to
a wide variety of applications, we believe the
resource will be of tremendous value for AI
researchers, and we discuss some of potential
applications in this paper.
1 Introduction
Many natural language processing and understand-
ing applications benefit from the interpretation of
lexical relations in text (e.g. selectional preferences
for verbs and nouns). For example, if one knows that
things being annexed are typically geopolitical enti-
ties, then given the phrase Napoleon?s annexation of
Piedmont, we can infer Piedmont is a geopolitical
entity. Existing linguistic resources such as VerbNet
and FrameNet provide some argument type infor-
mation for verbs and frames. However, since they
are manually built, they tend to specify type con-
straints at a very high level (e.g, Solid, Animate),
?Research supported in part by Air Force Contract FA8750-
09-C-0172 under the DARPA Machine Reading Program
consequently they do not suffice for cases such as
the previous example.
We would like to infer more fine grained knowl-
edge for predicates automatically from a large
amount of data. In addition, we do not want to re-
strict ourselves to only verbs, binary relations, or to
a specific type hierarchy.
In this paper, we present PRISMATIC, a large
scale lexicalized relation resource mined from over
30 gb of text. PRISMATIC is built using a suite of
NLP tools that includes a dependency parser, a rule
based named entity recognizer and a coreference
resolution component. PRISMATIC is composed
of frames which are the basic semantic representa-
tion of lexicalized relation and surrounding context.
There are approximately 1 billion frames in our cur-
rent version of PRISMATIC. To induce knowledge
from PRISMATIC, we define the notion of frame-
cuts, which basically specify a cut or slice operation
on a frame. In the case of the previous Napoleon
annexation example, we would use a noun-phrase
? object type cut to learn the most frequent type
of things being annexed. We believe there are many
potential applications that can utilize PRISMATIC,
such as type inference, relation extraction textual en-
tailment, etc. We discuss some of these applications
in details in section 8.
2 Related Work
2.1 Manually Created Resources
Several lexical resources have been built man-
ually, most notably WordNet (Fellbaum, 1998),
FrameNet(Baker et al, 1998) and VerbNet(Baker et
122
al., 1998). WordNet is a lexical resource that con-
tains individual word synset information, such as
definition, synonyms, antonyms, etc. However, the
amount of predicate knowledge in WordNet is lim-
ited.
FrameNet is a lexical database that describes the
frame structure of selected words. Each frame rep-
resents a predicate (e.g. eat, remove) with a list of
frame elements that constitutes the semantic argu-
ments of the predicate. Different words may map to
the same frame, and one word may map to multiple
frames based on different word senses. Frame ele-
ments are often specific to a particular frame, and
even if two frame elements with the same name,
such as ?Agent?, may have subtle semantic mean-
ings in different frames.
VerbNet is a lexical database that maps verbs to
their corresponding Levin (Levin, 1993) classes, and
it includes syntactic and semantic information of the
verbs, such as the syntactic sequences of a frame
(e.g. NP V NP PP) and the selectional restriction
of a frame argument value must be ANIMATE,
Compared to these resources, in addition to being
an automatic process, PRISMATIC has three major
differences. First, unlike the descriptive knowledge
in WordNet, VerbNet or FrameNet, PRISMATIC of-
fers only numeric knowledge of the frequencies of
how different predicates and their argument values
through out a corpus. The statistical profiles are eas-
ily to produce automatically, and they allow addi-
tional knowledge, such as type restriction (see 8.1),
to be inferred from PRISMATIC easily.
Second, the frames are defined differently. The
frames in PRISMATIC are not abstract concepts
generalized over a set of words. They are defined
by the words in a sentence and the relations between
them. Two frames with different slot values are con-
sidered different even though they may be semanti-
cally similar. For example, the two sentences ?John
loves Mary? and ?John adores Mary? result in two
different frame even though semantically they are
very close. By choosing not to use frame concepts
generalized over words, we avoid the problem of
determining which frame a word belongs to when
processing text automatically. We believe there will
be enough redundancy in a large corpus to produce
valid values for different synonyms and variations.
Third, PRISMATIC only uses a very small set of
slots (see table 1) defined by parser and relation an-
notators to link a frame and its arguments. By using
these slots directly, we avoid the problem of map-
ping parser relations to frame elements.
2.2 Automatically Created Resources
TextRunner (Banko et al, 2007) is an information
extraction system which automatically extracts re-
lation tuples over massive web data in an unsuper-
vised manner. TextRunner contains over 800 mil-
lion extractions (Lin et al, 2009) and has proven
to be a useful resource in a number of important
tasks in machine reading such as hypernym discov-
ery (Alan Ritter and Etzioni, 2009), and scoring in-
teresting assertions (Lin et al, 2009). TextRunner
works by automatically identifying and extracting
relationships using a conditional random field (CRF)
model over natural language text. As this is a rela-
tively inexpensive technique, it allows rapid applica-
tion to web-scale data.
DIRT (Discovering Inference Rules from Text)
(Lin and Pantel, 2001) automatically identifies in-
ference rules over dependency paths which tend to
link the same arguments. The technique consists of
applying a dependency parser over 1 gb of text, col-
lecting the paths between arguments and then cal-
culating a path similarity between paths. DIRT has
been used extensively in recognizing textual entail-
ment (RTE).
PRISMATIC is similar to TextRunner and DIRT
in that it may be applied automatically over mas-
sive corpora. At a representational level it differs
from both TextRunner and DIRT by storing full
frames from which n-ary relations may be indexed
and queried. PRISMATIC differs from TextRun-
ner as it applies a full dependency parser in order
to identify dependency relationships between terms.
In contrast to DIRT and TextRunner, PRISMATIC
also performs co-reference resolution in order to in-
crease coverage for sparsely-occurring entities and
employs a named entity recognizer (NER) and rela-
tion extractor on all of its extractions to better repre-
sent intensional information.
3 System Overview
The PRISMATIC pipeline consists of three phases:
1. Corpus Processing Documents are annotated
123
Figure 1: System Overview
by a suite of components which perform depen-
dency parsing, co-reference resolution, named
entity recognition and relation detection.
2. Frame Extraction Frames are extracted based
on the dependency parses and associated anno-
tations.
3. Frame-Cut Extraction Frame-cuts of interest
(e.g. S-V-O cuts) are identified over all frames
and frequency information for each cut is tabu-
lated.
4 Corpus Processing
The key step in the Corpus Processing stage is the
application of a dependency parser which is used
to identify the frame slots (as listed in Table 1) for
the Frame Extraction stage. We use ESG (McCord,
1990), a slot-grammar based parser in order to fill
in the frame slots. Sentences frequently require co-
reference in order to precisely identify the participat-
ing entity, and so in order to not lose that informa-
tion, we apply a simple rule based co-reference reso-
lution component in this phase. The co-reference in-
formation helps enhance the coverage of the frame-
cuts, which is especially valuable in cases of sparse
data and for use with complex frame-cuts.
A rule based Named Entity Recognizer (NER) is
used to identify the types of arguments in all frame
slot values. This type information is then registered
in the Frame Extraction stage to construct inten-
tional frames.
5 Frame Extraction
Relation Description/Example
subj subject
obj direct object
iobj indirect object
comp complement
pred predicate complement
objprep object of the preposition
mod nprep Bat Cave in Toronto is a tourist attraction.
mod vprep He made it to Broadway.
mod nobj the object of a nominalized verb
mod ndet City?s budget was passed.
mod ncomp Tweet is a word for microblogging.
mod nsubj A poem by Byron
mod aobj John is similar to Steve.
isa subsumption relation
subtypeOf subsumption relation
Table 1: Relations used in a frame and their descriptions
The next step of PRISMATIC is to extract a set of
frames from the parsed corpus. A frame is the basic
semantic unit representing a set of entities and their
relations in a text snippet. A frame is made of a set
of slot value pairs where the slots are dependency
relations extracted from the parse and the values are
the terms from the sentences or annotated types. Ta-
ble 2 shows the extracted frame based on the parse
tree in figure 2.
In order to capture the relationship we are inter-
ested in, frame elements are limited to those that
represent the participant information of a predicate.
Slots consist of the ones listed in table 1. Further-
more, each frame is restricted to be two levels deep
at the most, therefore, a large parse tree may re-
sult in multiple frames. Table 2 shows how two
frames are extracted from the complex parse tree
in figure 2. The depth restriction is needed for two
reasons. First, despite the best efforts from parser
researchers, no parser is perfect, and big complex
parse trees tend to have more wrong parses. By lim-
iting a frame to be only a small subset of a complex
parse tree, we reduce the chance of error parse in
each frame. Second, by isolating a subtree, each
frame focuses on the immediate participants of a
predicate.
Non-parser information may also be included in a
frame. For example, the type annotations of a word
from a named entity recognizer are included, and
such type information is useful for the various ap-
124
Figure 2: The parse tree of the sentence In 1921, Einstein received the Nobel Prize for his original work on the
photoelectric effect.
Frame01
verb receive
subj Einstein
type PERSON / SCIENTIST
obj Nobel prize
mod vprep in
objprep 1921
type YEAR
mod vprep for
objprep Frame02
Frame02
noun work
mod ndet his / Einstein
mod nobj on
objprep effect
Table 2: Frames extracted from Dependency Parse in Fig-
ure 2
plications described in section 8. We also include
a flag to indicate whether a word is proper noun.
These two kinds of information allow us to easily
separate the intensional and the extensional parts of
PRISMATIC.
6 Frame Cut
One of the main reasons for extracting a large
amount of frame data from a corpus is to induce
interesting knowledge patterns by exploiting redun-
dancy in the data. For example, we would like to
learn that things that are annexed are typically re-
gions, i.e., a predominant object-type for the noun-
phrase ?annexation of? is ?Region? where ?Region?
is annotated by a NER. To do this kind of knowledge
induction, we first need to abstract out specific por-
tions of the frame - in this particular case, we need
to isolate and analyze the noun-phrase ? object-
type relationship. Then, given a lot of data, and
frames containing only the above relationship, we
hope to see the frame [noun=?annexation?, prepo-
sition=?of?, object-type=?Region?] occur very fre-
quently.
To enable this induction analysis, we define
frame-cuts, which basically specify a cut or slice op-
eration on a frame. For example, we define an N-P-
OT frame cut, which when applied to a frame only
keeps the noun (N), preposition (P) and object-type
(OT) slots, and discards the rest. Similarly, we de-
fine frame-cuts such as S-V-O, S-V-O-IO, S-V-P-O
etc. (where S - subject, V - verb, O - object, IO -
indirect object) which all dissect frames along dif-
125
ferent dimensions. Continuing with the annexation
example, we can use the V-OT frame cut to learn
that a predominant object-type for the verb ?annex?
is also ?Region?, by seeing lots of frames of the form
[verb=?annex?, object-type=?Region?] in our data.
To make frame-cuts more flexible, we allow them
to specify optional value constraints for slots. For
example, we can define an S-V-O frame cut, where
both the subject (S) and object (O) slot values are
constrained to be proper nouns, thereby creating
strictly extensional frames, i.e. frames containing
data about instances, e.g., [subject=?United States?
verb=?annex? object=?Texas?]. The opposite ef-
fect is achieved by constraining S and O slot val-
ues to common nouns, creating intensional frames
such as [subject=?Political-Entity? verb=?annex?
object=?Region?]. The separation of extensional
from intensional frame information is desirable,
both from a knowledge understanding and an appli-
cations perspective, e.g. the former can be used to
provide factual evidence in tasks such as question
answering, while the latter can be used to learn en-
tailment rules as seen in the annexation case.
7 Data
The corpora we used to produce the initial PRIS-
MATIC are based on a selected set of sources, such
as the complete Wikipedia, New York Times archive
and web page snippets that are on the topics listed in
wikipedia. After cleaning and html detagging, there
are a total of 30 GB of text. From these sources, we
extracted approximately 1 billion frames, and from
these frames, we produce the most commonly used
cuts such as S-V-O, S-V-P-O and S-V-O-IO.
8 Potential Applications
8.1 Type Inference and Its Related Uses
As noted in Section 6, we use frame-cuts to dis-
sect frames along different slot dimensions, and then
aggregate statistics for the resultant frames across
the entire dataset, in order to induce relationships
among the various frame slots, e.g., learn the pre-
dominant types for subject/object slots in verb and
noun phrases. Given a new piece of text, we can
apply this knowledge to infer types for named en-
tities. For example, since the aggregate statistics
shows the most common type for the object of
the verb ?annex? is Region, we can infer from the
sentence ?Napoleon annexed Piedmont in 1859?,
that ?Piedmont? is most likely to be a Region.
Similarly, consider the sentence: ?He ordered a
Napoleon at the restaurant?. A dictionary based
NER is very likely to label ?Napoleon? as a Per-
son. However, we can learn from a large amount
of data, that in the frame: [subject type=?Person?
verb=?order? object type=[?] verb prep=?at? ob-
ject prep=?restaurant?], the object type typically
denotes a Dish, and thus correctly infer the type for
?Napoleon? in this context. Learning this kind of
fine-grained type information for a particular con-
text is not possible using traditional hand-crafted re-
sources like VerbNet or FrameNet. Unlike previ-
ous work in selectional restriction (Carroll and Mc-
Carthy, 2000; Resnik, 1993), PRISMATIC based
type inference does not dependent on a particular
taxonomy or previously annotated training data: it
works with any NER and its type system.
The automatically induced-type information can
also be used for co-reference resolution. For ex-
ample, given the sentence: ?Netherlands was ruled
by the UTP party before Napolean annexed it?, we
can use the inferred type constraint on ?it? (Region)
to resolve it to ?Netherlands? (instead of the ?UTP
Party?).
Finally, typing knowledge can be used for word
sense disambiguation. In the sentence, ?Tom Cruise
is one of the biggest stars in American Cinema?, we
can infer using our frame induced type knowledge
base, that the word ?stars? in this context refers to a
Person/Actor type, and not the sense of ?star? as an
astronomical object.
8.2 Factual Evidence
Frame data, especially extensional data involving
named entities, captured over a large corpus can be
used as factual evidence in tasks such as question
answering.
8.3 Relation Extraction
Traditional relation extraction approach (Zelenko et
al., 2003; Bunescu and Mooney, 2005) relies on the
correct identification of the types of the argument.
For example, to identify ?employs? relation between
?John Doe? and ?XYZ Corporation?, a relation ex-
tractor heavily relies on ?John Doe? being annotated
126
as a ?PERSON? and ?XYZ Corporation? an ?OR-
GANIZATION? since the ?employs? relation is de-
fined between a ?PERSON? and an ?ORGANIZA-
TION?.
We envision PRISMATIC to be applied to rela-
tion extraction in two ways. First, as described in
section 8.1, PRISMATIC can complement a named
entity recognizer (NER) for type annotation. This
is especially useful for the cases when NER fails.
Second, since PRISMATIC has broad coverage of
named entities, it can be used as a database to
check to see if the given argument exist in related
frame. For example, in order to determine if ?em-
ploys? relation exists between ?Jack Welch? and
?GE? in a sentence, we can look up the SVO cut
of PRISMATIC to see if we have any frame that has
?Jack Welch? as the subject, ?GE? as the object and
?work? as the verb, or frame that has ?Jack Welch?
as the object, ?GE? as the subject and ?employs? as
the verb. This information can be passed on as an
feature along with other syntactic and semantic fea-
tures to th relation extractor.
9 Conclusion and Future Work
In this paper, we presented PRISMATIC, a large
scale lexicalized relation resource that is built au-
tomatically over massive amount of text. It provides
users with knowledge about predicates and their ar-
guments. We have focused on building the infras-
tructure and gathering the data. Although we are
still in the early stages of applying PRISMATIC, we
believe it will be useful for a wide variety of AI ap-
plications as discussed in section 8, and will pursue
them in the near future.
References
Stephen Soderland Alan Ritter and Oren Etzioni. 2009.
What is this, anyway: Automatic hypernym discovery.
In Proceedings of the 2009 AAAI Spring Symposium
on Learning by Reading and Learning to Read.
Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998. The berkeley framenet project. In Proceedings
of the 17th international conference on Computational
linguistics, pages 86?90, Morristown, NJ, USA. Asso-
ciation for Computational Linguistics.
Michele Banko, Michael J Cafarella, Stephen Soderl,
Matt Broadhead, and Oren Etzioni. 2007. Open
information extraction from the web. In In Inter-
national Joint Conference on Artificial Intelligence,
pages 2670?2676.
Razvan C. Bunescu and Raymond J. Mooney. 2005. A
shortest path dependency kernel for relation extrac-
tion. In HLT ?05: Proceedings of the conference on
Human Language Technology and Empirical Meth-
ods in Natural Language Processing, pages 724?731,
Morristown, NJ, USA. Association for Computational
Linguistics.
John Carroll and Diana McCarthy. 2000. Word sense
disambiguation using automatically acquired verbal
preferences. Computers and the Humanities Senseval
Special Issue, 34.
Christiane Fellbaum, 1998. WordNet: An Electronic Lex-
ical Database.
Beth Levin, 1993. English Verb Classes and Alterna-
tions: A Preliminary Investigation.
Dekang Lin and Patrick Pantel. 2001. Dirt - discovery
of inference rules from text. In In Proceedings of the
ACM SIGKDD Conference on Knowledge Discovery
and Data Mining, pages 323?328.
Thomas Lin, Oren Etzioni, and James Fogarty. 2009.
Identifying interesting assertions from the web. In
CIKM ?09: Proceeding of the 18th ACM conference on
Information and knowledge management, pages 1787?
1790, New York, NY, USA. ACM.
Michael C. McCord. 1990. Slot grammar: A system
for simpler construction of practical natural language
grammars. In Proceedings of the International Sympo-
sium on Natural Language and Logic, pages 118?145,
London, UK. Springer-Verlag.
Philip Resnik. 1993. Selection and Information:
A Class-Based Approach to Lexical Relationships.
Ph.D. thesis, University of Pennsylvania.
Dmitry Zelenko, Chinatsu Aone, and Anthony
Richardella. 2003. Kernel methods for relation
extraction. J. Mach. Learn. Res., 3:1083?1106.
127
