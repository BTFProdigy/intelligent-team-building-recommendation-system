Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 708?719,
October 25-29, 2014, Doha, Qatar.
c
?2014 Association for Computational Linguistics
Evaluating Neural Word Representations in
Tensor-Based Compositional Settings
Dmitrijs Milajevs
1
Dimitri Kartsaklis
2
Mehrnoosh Sadrzadeh
1
Matthew Purver
1
1
Queen Mary University of London
School of Electronic Engineering
and Computer Science
Mile End Road, London, UK
{d.milajevs,m.sadrzadeh,m.purver}@qmul.ac.uk
2
University of Oxford
Department of Computer Science
Parks Road, Oxford, UK
dimitri.kartsaklis@cs.ox.ac.uk
Abstract
We provide a comparative study be-
tween neural word representations and
traditional vector spaces based on co-
occurrence counts, in a number of com-
positional tasks. We use three differ-
ent semantic spaces and implement seven
tensor-based compositional models, which
we then test (together with simpler ad-
ditive and multiplicative approaches) in
tasks involving verb disambiguation and
sentence similarity. To check their scala-
bility, we additionally evaluate the spaces
using simple compositional methods on
larger-scale tasks with less constrained
language: paraphrase detection and di-
alogue act tagging. In the more con-
strained tasks, co-occurrence vectors are
competitive, although choice of composi-
tional method is important; on the larger-
scale tasks, they are outperformed by neu-
ral word embeddings, which show robust,
stable performance across the tasks.
1 Introduction
Neural word embeddings (Bengio et al., 2006;
Collobert and Weston, 2008; Mikolov et al.,
2013a) have received much attention in the dis-
tributional semantics community, and have shown
state-of-the-art performance in many natural lan-
guage processing tasks. While they have been
compared with co-occurrence based models in
simple similarity tasks at the word level (Levy et
al., 2014; Baroni et al., 2014), we are aware of
only one work that attempts a comparison of the
two approaches in compositional settings (Blacoe
and Lapata, 2012), and this is limited to additive
and multiplicative composition, compared against
composition via a neural autoencoder.
The purpose of this paper is to provide a more
complete picture regarding the potential of neu-
ral word embeddings in compositional tasks, and
meaningfully compare them with the traditional
distributional approach based on co-occurrence
counts. We are especially interested in investi-
gating the performance of neural word vectors in
compositional models involving general mathe-
matical composition operators, rather than in the
more task- or domain-specific deep-learning com-
positional settings they have generally been used
with so far (for example, by Socher et al. (2012),
Kalchbrenner and Blunsom (2013) and many oth-
ers).
In particular, this is the first large-scale study
to date that applies neural word representations in
tensor-based compositional distributional models
of meaning similar to those formalized by Coecke
et al. (2010). We test a range of implementations
based on this framework, together with additive
and multiplicative approaches (Mitchell and Lap-
ata, 2008), in a variety of different tasks. Specif-
ically, we use the verb disambiguation task of
Grefenstette and Sadrzadeh (2011a) and the tran-
sitive sentence similarity task of Kartsaklis and
Sadrzadeh (2014) as small-scale focused experi-
ments on pre-defined sentence structures. Addi-
tionally, we evaluate our vector spaces on para-
phrase detection (using the Microsoft Research
Paraphrase Corpus of Dolan et al. (2005)) and di-
alogue act tagging using the Switchboard Corpus
(see e.g. (Stolcke et al., 2000)).
In all of the above tasks, we compare the neural
word embeddings of Mikolov et al. (2013a) with
two vector spaces both based on co-occurrence
counts and produced by standard distributional
techniques, as described in detail below. The gen-
eral picture we get from the results is that in almost
all cases the neural vectors are more effective than
the traditional approaches.
We proceed as follows: Section 2 provides a
concise introduction to distributional word repre-
sentations in natural language processing. Section
708
3 takes a closer look to the subject of composi-
tionality in vector space models of meaning and
describes the range of compositional operators ex-
amined here. In Section 4 we provide details about
the vector spaces used in the experiments. Our ex-
perimental work is described in detail in Section 5,
and the results are discussed in Section 6. Finally,
Section 7 provides conclusions.
2 Meaning representation
There are several approaches to the representation
of word, phrase and sentence meaning. As nat-
ural languages are highly creative and it is very
rare to see the same sentence twice, any practical
approach dealing with large text segments must
be compositional, constructing the meaning of
phrases and sentences from their constituent parts.
The ideal method would therefore express not
only the similarity in meaning between those con-
stituent parts, but also between the results of their
composition, and do this in ways which fit with
linguistic structure and generalisations thereof.
Formal semantics Formal approaches to the
semantics of natural language have long built
upon the classical idea of compositionality ?
that the meaning of a sentence is a function
of the meanings of its parts (Frege, 1892). In
compositional type-logical approaches, predicate-
argument structures representing phrases and sen-
tences are built from their constituent parts by ?-
reduction within the lambda calculus framework
(Montague, 1970): for example, given a represen-
tation of John as john
?
and sleeps as ?x.sleep
?
(x),
the meaning of the sentence ?John sleeps?
can be constructed as ?x.sleep
?
(x)(john
?
) =
sleep
?
(john
?
). Given a suitable pairing between
words and semantic representations of them, this
method can produce structured sentential repre-
sentations with broad coverage and good gener-
alisability (see e.g. (Bos, 2008)). The above logi-
cal approach is extremely powerful because it can
capture complex aspects of meaning such as quan-
tifiers and their interaction (see e.g. (Copestake et
al., 2005)), and enables inference using well stud-
ied and developed logical methods (see e.g. (Bos
and Gabsdil, 2000)).
Distributional hypothesis However, such for-
mal approaches are less able to express similar-
ity in meaning. We would like to capture the
intuition that while John and Mary are distinct,
they are rather similar to each other (both of them
are humans) and dissimilar to words such as dog,
pavement or idea. The same applies at the phrase
and sentence level: ?dogs chase cats? is similar in
meaning to ?hounds pursue kittens?, but less so to
?cats chase dogs? (despite the lexical overlap).
Distributional methods provide a way to address
this problem. By representing words and phrases
as vectors or tensors in a (usually highly dimen-
sional) vector space, one can express similarity
in meaning via a suitable distance metric within
that space (usually cosine distance); furthermore,
composition can be modelled via suitable linear-
algebraic operations.
Co-occurrence-based word representations
One way to produce such vectorial representa-
tions is to directly exploit Harris (1954)?s intuition
that semantically similar words tend to appear in
similar contexts. We can construct a vector space
in which the dimensions correspond to contexts,
usually taken to be words as well. The word
vector components can then be calculated from
the frequency with which a word has co-occurred
with the corresponding contexts in a window of
words, with a predefined length.
Table 1 shows 5 3-dimensional vectors for the
words Mary, John, girl, boy and idea. The words
philosophy, book and school signify vector space
dimensions. As the vector for John is closer to
Mary than it is to idea in the vector space?a di-
rect consequence of the fact that John?s contexts
are similar to Mary?s and dissimilar to idea?s?we
can infer that John is semantically more similar to
Mary than to idea.
Many variants of this approach exist: perfor-
mance on word similarity tasks has been shown
to be improved by replacing raw counts with
weighted values (e.g. mutual information)?see
(Turney et al., 2010) and below for discussion, and
(Kiela and Clark, 2014) for a detailed comparison.
philosophy book school
Mary 0 10 22
John 4 60 59
girl 0 19 93
boy 0 12 164
idea 10 47 39
Table 1: Word co-occurrence frequencies ex-
tracted from the BNC (Leech et al., 1994).
709
Neural word embeddings Deep learning tech-
niques exploit the distributional hypothesis dif-
ferently. Instead of relying on observed co-
occurrence frequencies, a neural language model
is trained to maximise some objective function re-
lated to e.g. the probability of observing the sur-
rounding words in some context (Mikolov et al.,
2013b):
1
T
T
?
t=1
?
?c?j?c,j 6=0
log p(w
t+j
|w
t
) (1)
Optimizing the above function, for example, pro-
duces vectors which maximise the conditional
probability of observing words in a context around
the target word w
t
, where c is the size of the
training window, and w
1
w
2
, ? ? ?w
T
a sequence of
words forming a training instance. Therefore, the
resulting vectors will capture the distributional in-
tuition and can express degrees of lexical similar-
ity.
This method has an obvious advantage com-
pared to co-occurrence method: since now the
context is predicted, the model in principle can
be much more robust in data sparsity prob-
lems, which is always an important issue for co-
occurrence word spaces. Additionally, neural vec-
tors have also proven successful in other tasks
(Mikolov et al., 2013c), since they seem to en-
code not only attributional similarity (the degree to
which similar words are close to each other), but
also relational similarity (Turney, 2006). For ex-
ample, it is possible to extract the singular:plural
relation (apple:apples, car:cars) using vector sub-
traction:
????
apple ?
?????
apples ?
??
car ?
???
cars
Perhaps even more importantly, semantic relation-
ships are preserved in a very intuitive way:
???
king ?
???
man ?
????
queen ?
?????
woman
allowing the formation of analogy queries similar
to
???
king ?
???
man +
?????
woman = ?, obtaining
????
queen as
the result.
1
Both neural and co-occurrence-based ap-
proaches have advantages over classical formal
approaches in their ability to capture lexical se-
mantics and degrees of similarity; their success at
1
Levy et al. (2014) improved Mikolov et al. (2013c)?s
method of retrieving relational similarities by changing the
underlying objective function.
extending this to the sentence level and to more
complex semantic phenomena, though, depends
on their applicability within compositional mod-
els, which is the subject of the next section.
3 Compositional models
Compositional distributional models represent
meaning of a sequence of words by a vector, ob-
tained by combining meaning vectors of the words
within the sequence using some vector composi-
tion operation. In a general classification of these
models, one can distinguish between three broad
cases: simplistic models which combine word
vectors irrespective of their order or relation to one
another, models which exploit linear word order,
and models which use grammatical structure.
The first approach combines word vectors
by vector addition or point-wise multiplication
(Mitchell and Lapata, 2008)?as this is indepen-
dent of word order, it cannot capture the differ-
ence between the two sentences ?dogs chase cats?
and ?cats chase dogs?. The second approach has
generally been implemented using some form of
deep learning, and captures word order, but not by
necessarily caring about the grammatical structure
of the sentence. Here, one works by recursively
building and combining vectors for subsequences
of words within the sentence using e.g. autoen-
coders (Socher et al., 2012) or convolutional fil-
ters (Kalchbrenner et al., 2014). We do not con-
sider this approach in this paper. This is because,
as mentioned in the introduction, their vectors and
composition operators are task-specific. These are
trained directly to achieve specific objectives in
certain pre-determined tasks. We are interested
in vector and composition operators that work for
any compositional task, and which can be com-
bined with results in linguistics and formal se-
mantics to provide generalisable models that can
canonically extend to complex semantic phenom-
ena. The third (i.e. the grammatical) approach
promises a way to achieve this, and has been in-
stantiated in various ways in the work of Baroni
and Zamparelli (2010),Grefenstette and Sadrzadeh
(2011a), and Kartsaklis et al. (2012).
General framework Formally, we can spec-
ify the vector representation of a word sequence
w
1
w
2
? ? ?w
n
as the vector
??
s =
??
w
1
?
??
w
2
? ? ? ??
??
w
n
,
where ? is a vector operator, such as addition +,
point-wise multiplication , tensor product ?, or
matrix multiplication ?.
710
In the simplest compositional models (the first
approach described above), ? is + or , e.g. see
(Mitchell and Lapata, 2008). Grammar-based
compositional models (the third approach) are
based on a generalisation of the notion of vectors,
known as tensors. Whereas a vector
??
v is an ele-
ment of an atomic vector space V , a tensor z is an
element of a tensor space V ?W ? ? ? ? ? Z. The
number of tensored spaces is referred to by the or-
der of the space. Using a general duality theorem
from multi-linear algebra (Bourbaki, 1989), it fol-
lows that tensors are in one-one correspondence
with multi-linear maps, that is we have:
z ? V ?W?? ? ??Z
?
=
f
z
: V ?W ? ? ? ? ? Z
In such a tensor-based formalism, meanings of
nouns are vectors and meanings of predicates such
as adjectives and verbs are tensors. Meaning of a
string of words is obtained by applying the compo-
sitions of multi-linear map duals of the tensors to
the vectors. For the sake of demonstration, take
the case of an intransitive sentence ?Sbj Verb?;
the meaning of the subject is a vector
??
Sbj ? V
and the meaning of the intransitive verb is a ten-
sor Verb ? V ?W . Meaning of the sentence is
obtained by applying f
V erb
to
??
Sbj, as follows:
??????
Sbj Verb = f
V erb
(
??
Sbj)
By tensor-map duality, the above becomes
equivalent to the following, where composition
has now become the familiar notion of matrix mul-
tiplication, that is ? is ?:
Verb?
??
Sbj
In general and for words with tensors of order
higher than two, ? becomes a generalisation of ?,
referred to by tensor contraction, see e.g. Kartsak-
lis and Sadrzadeh (2013). Since the creation and
manipulation of tensors of order higher than 2 is
difficult, one can work with simplified versions of
tensors, faithful to their underlying mathematical
basis; these have found intuitive interpretations,
e.g. see Grefenstette and Sadrzadeh (2011a), Kart-
saklis and Sadrzadeh (2014). In such cases, ? be-
comes a combination of a range of operations such
as ?, ?, , and +.
Specific models In the current paper we will ex-
periment with a variety of models. In Table 2, we
present these models in terms of their composi-
tion operators and a reference to the main paper in
which each model was introduced. For the sim-
ple compositional models the sentence is a string
of any number of words; for the grammar-based
models, we consider simple transitive sentences
?Sbj Verb Obj? and introduce the following abbre-
viations for the concrete method used to build a
tensor for the verb:
1. Verb is a verb matrix computed using the for-
mula
?
i
???
Sbj
i
?
???
Obj
i
, where
???
Sbj
i
and
???
Obj
i
are
the subjects and objects of the verb across the
corpus. These models are referred to by rela-
tional (Grefenstette and Sadrzadeh, 2011a);
they are generalisations of predicate seman-
tics of transitive verbs, from pairs of individ-
uals to pairs of vectors. The models reduce
the order 3 tensor of a transitive verb to an
order 2 tensor (i.e. a matrix).
2.
?
Verb is a verb matrix computed using the for-
mula
???
Verb ?
???
Verb, where
???
Verb is the distri-
butional vector of the verb. These models are
referred to by Kronecker, which is the term
sometimes used to denote the outer prod-
uct of tensors (Grefenstette and Sadrzadeh,
2011b). This models also reduces the order
3 tensor of a transitive verb to an order 2 ten-
sor.
3. The models of the last five lines of the table
use the so-called Frobenius operators from
categorical compositional distributional se-
mantics (Kartsaklis et al., 2012) to expand
the relational matrices of verbs from order 2
to order 3. The expansion is obtained by ei-
ther copying the dimension of the subject into
the space provided by the third tensor, hence
referred to by Copy-Sbj, or copying the di-
mension of the object in that space, hence re-
ferred to by Copy-Obj; furthermore, we can
take addition, multiplication, or outer product
of these, which are referred to by Frobenius-
Add, Frobenius-Mult, and Frobenius-Outer
(Kartsaklis and Sadrzadeh, 2014).
4 Semantic word spaces
Co-occurrence-based vector space instantiations
have received a lot of attention from the scientific
community (refer to (Kiela and Clark, 2014; Pola-
jnar and Clark, 2014) for recent studies). We in-
stantiate two co-occurrence-based vectors spaces
with different underlying corpora and weighting
schemes.
711
Method Sentence Linear algebraic formula Reference
Addition w
1
w
2
? ? ?w
n
??
w
1
+
??
w
2
+ ? ? ?+
??
w
n
Mitchell and Lapata (2008)
Multiplication w
1
w
2
? ? ?w
n
??
w
1

??
w
2
 ? ? ? 
??
w
n
Mitchell and Lapata (2008)
Relational Sbj Verb Obj Verb (
??
Sbj?
??
Obj) Grefenstette and Sadrzadeh (2011a)
Kronecker Sbj Verb Obj V?erb (
??
Sbj?
??
Obj) Grefenstette and Sadrzadeh (2011b)
Copy object Sbj Verb Obj
??
Sbj (Verb?
??
Obj) Kartsaklis et al. (2012)
Copy subject Sbj Verb Obj
??
Obj (Verb
T
?
??
Sbj) Kartsaklis et al. (2012)
Frob. add. Sbj Verb Obj (
??
Sbj (Verb?
??
Obj)) + (
??
Obj (Verb
T
?
??
Sbj)) Kartsaklis and Sadrzadeh (2014)
Frob. mult. Sbj Verb Obj (
??
Sbj (Verb?
??
Obj)) (
??
Obj (Verb
T
?
??
Sbj)) Kartsaklis and Sadrzadeh (2014)
Frob. outer Sbj Verb Obj (
??
Sbj (Verb?
??
Obj))? (
??
Obj (Verb
T
?
??
Sbj)) Kartsaklis and Sadrzadeh (2014)
Table 2: Compositional methods.
GS11 Our first word space is based on a typ-
ical configuration that has been used in the past
extensively for compositional distributional mod-
els (see below for details), so it will serve as a
useful baseline for the current work. In this vec-
tor space, the co-occurrence counts are extracted
from the British National Corpus (BNC) (Leech et
al., 1994). As basis words, we use the most fre-
quent nouns, verbs, adjectives and adverbs (POS
tags SUBST, VERB, ADJ and ADV in the BNC
XML distribution
2
). The vector space is lemma-
tized, that is, it contains only ?canonical? forms of
words.
In order to weight the raw co-occurrence counts,
we use positive point-wise mutual information
(PPMI). The component value for a target word
t and a context word c is given by:
PPMI(t, c) = max
(
0, log
p(c|t)
p(c)
)
where p(c|t) is the probability of word c given t
in a symmetric window of length 5 and p(c) is the
probability of c overall.
Vector spaces based on point-wise mutual in-
formation (or variants thereof) have been success-
fully applied in various distributional and compo-
sitional tasks; see e.g. Grefenstette and Sadrzadeh
(2011a), Mitchell and Lapata (2008), Levy et al.
(2014) for details. PPMI has been shown to
achieve state-of-the-art results (Levy et al., 2014)
and is suggested by the review of Kiela and Clark
(2014). Our use here of the BNC as a corpus
and the window length of 5 is based on previ-
ous use and better performance of these param-
eters in a number of compositional experiments
(Grefenstette and Sadrzadeh, 2011a; Grefenstette
2
http://www.natcorp.ox.ac.uk/
and Sadrzadeh, 2011b; Mitchell and Lapata, 2008;
Kartsaklis et al., 2012).
KS14 In this variation, we train a vector space
from the ukWaC corpus
3
(Ferraresi et al., 2008),
originally using as a basis the 2,000 content words
with the highest frequency (but excluding a list of
stop words as well as the 50 most frequent content
words since they exhibit low information content).
The vector space is again lemmatized. As context
we consider a 5-word window from either side of
the target word, while as our weighting scheme we
use local mutual information (i.e. point-wise mu-
tual information multiplied by raw counts). In a
further step, the vector space was normalized and
projected onto a 300-dimensional space using sin-
gular value decomposition (SVD).
In general, dimensionality reduction produces
more compact word representations that are robust
against potential noise in the corpus (Landauer and
Dumais, 1997; Sch?utze, 1997). SVD has been
shown to perform well on a variety of tasks similar
to ours (Baroni and Zamparelli, 2010; Kartsaklis
and Sadrzadeh, 2014).
Neural word embeddings (NWE) For our neu-
ral setting, we used the skip-gram model of
Mikolov et al. (2013b) trained with negative sam-
pling. The specific implementation that was tested
in our experiments was a 300-dimensional vec-
tor space learned from the Google News corpus
and provided by the word2vec
4
toolkit. Fur-
thermore, the gensim library (
?
Reh?u?rek and So-
jka, 2010) was used for accessing the vectors.
On the contrary with the previously described co-
3
http://wacky.sslmit.unibo.it/
4
https://code.google.com/p/word2vec/
712
occurrence vector spaces, this version is not lem-
matized.
The negative sampling method improves the ob-
jective function of Equation 1 by introducing neg-
ative examples to the training algorithm. Assume
that the probability of a specific (c, t) pair of words
(where t is a target word and c another word in
the same context with t), coming from the training
data, is denoted as p(D = 1|c, t). The objective
function is then expressed as follows:
?
(c,t)?D
p(D = 1|c, t) (2)
That is, the goal is to set the model parameters in
a way that maximizes the probability of all obser-
vations coming from the training data. Assume
now that D
?
is a set of randomly selected incorrect
(c
?
, t
?
) pairs that do not occur in D, then Equation
2 above can be recasted in the following way:
?
(c,t)?D
p(D = 1|c, t)
?
(c
?
,t
?
)?D
?
p(D = 0|c
?
, t
?
)
(3)
In other words, the model tries to distinguish a tar-
get word t from random draws that come from a
noise distribution. In the implementation we used
for our experiments, c is always selected from
a 5-word window around t. More details about
the negative sampling approach can be found in
(Mikolov et al., 2013b); the note of Goldberg and
Levy (2014) also provides an intuitive explanation
of the underlying setting.
5 Experiments
Our experiments explore the use of the vector
spaces above, together with the compositional op-
erators described in Section 3, in a range of tasks
all of which require semantic composition: verb
sense disambiguation; sentence similarity; para-
phrasing; and dialogue act tagging.
5.1 Disambiguation
We use the transitive verb disambiguation dataset
described in Grefenstette and Sadrzadeh (2011a)
5
.
This dataset consists of ambiguous transitive verbs
together with their arguments, landmark verbs
that identify one of the verb senses, and human
judgements that specify how similar is the disam-
biguated sense of the verb in the given context to
5
This and the sentence similarity dataset are avail-
able at http://www.cs.ox.ac.uk/activities/
compdistmeaning/
one of the landmarks. This is similar to the in-
transitive dataset described in (Mitchell and Lap-
ata, 2008). Consider the sentence ?system meets
specification?; here, meets is the ambiguous tran-
sitive verb, and system and specification are its ar-
guments in this context. Possible landmarks for
meet are satisfy and visit; for this sentence, the
human judgements show that the disambiguated
meaning of the verb is more similar to the land-
mark satisfy and less similar to visit.
The task is to estimate the similarity of the sense
of a verb in a context with a given landmark. To
get our similarity measures, we compose the verb
with its arguments using one of our compositional
models; we do the same for the landmark and then
compute the cosine similarity of the two vectors.
We evaluate the performance by averaging the hu-
man judgements for the same verb, argument and
landmark entries, and calculating the Spearman?s
correlation between the average values and the co-
sine scores. As a baseline, we compare this with
the correlation produced by using only the verb
vector, without composing it with its arguments.
Table 3 shows the results of the experiment.
NWE copy-object composition yields the best cor-
relation with the human judgements, and top per-
formance across all vector spaces and models with
a Spearman ? of 0.456. For the KS14 space, the
best result comes from Frobenius outer (0.350),
Method GS11 KS14 NWE
Verb only 0.212 0.325 0.107
Addition 0.103 0.275 0.149
Multiplication 0.348 0.041 0.095
Kronecker 0.304 0.176 0.117
Relational 0.285 0.341 0.362
Copy subject 0.089 0.317 0.131
Copy object 0.334 0.331 0.456
Frobenius add. 0.261 0.344 0.359
Frobenius mult. 0.233 0.341 0.239
Frobenius outer 0.284 0.350 0.375
Table 3: Spearman ? correlations of models with
human judgements for the word sense disam-
biguation task. The best result (NWE Copy ob-
ject) outperforms the nearest co-occurrence-based
competitor (KS14 Frobenius outer) with a statisti-
cally significant difference (p < 0.05, t-test).
713
while the best operator for the GS11 space is
point-wise multiplication (0.348).
For simple point-wise composition, only mul-
tiplicative GS11 and additive NWE improve over
their corresponding verb-only baselines (but both
perform worse than the KS14 baseline). With
tensor-based composition in co-occurrence based
spaces, copy subject yields lower results than
the corresponding baselines. Other composition
methods, except Kronecker for KS14, improve
over the verb-only baselines. Finally we should
note that, despite the small training corpus, the
GS11 vector space performs comparatively well:
for instance, Kronecker model improves the pre-
viously reported score of 0.28 (Grefenstette and
Sadrzadeh, 2011b).
5.2 Sentence similarity
In this experiment we use the transitive sen-
tence similarity dataset described in Kartsaklis and
Sadrzadeh (2014). The dataset consists of transi-
tive sentence pairs and a human similarity judge-
ment
6
. The task is to estimate a similarity measure
between two sentences. As in the disambiguation
task, we first compose word vectors to obtain sen-
tence vectors, then compute cosine similarity of
them. We average the human judgements for iden-
tical sentence pairs to compute a correlation with
cosine scores.
Table 4 shows the results. Again, the best
performing vector space is KS14, but this time
with addition: the Spearman ? correlation score
with averaged human judgements is 0.732. Addi-
tion was the means for the other vector spaces to
achieve top performance as well: GS11 and NWE
got 0.682 and 0.689 respectively.
None of the models in tensor-based composi-
tion outperformed addition. KS14 performs worse
with tensor-based methods here than in the other
vector spaces. However, GS11 and NWE, except
copy subject for both of them and Frobenius multi-
plication for NWE, improved over their verb-only
baselines.
5.3 Paraphrasing
In this experiment we evaluate our vector spaces
on a mainstream paraphrase detection task.
6
The textual content of this dataset is the same as that of
(Kartsaklis and Sadrzadeh, 2013), the difference is that the
dataset of (Kartsaklis and Sadrzadeh, 2014) has updated hu-
man judgements whereas the previous dataset used the orig-
inal annotations of the intransitive dataset of (Mitchell and
Lapata, 2010).
Method GS11 KS14 NWE
Verb only 0.491 0.602 0.561
Addition 0.682 0.732 0.689
Multiplication 0.597 0.321 0.341
Kronecker 0.581 0.408 0.561
Relational 0.558 0.437 0.618
Copy subject 0.370 0.448 0.405
Copy object 0.571 0.306 0.655
Frobenius add. 0.566 0.460 0.585
Frobenius mult. 0.525 0.226 0.387
Frobenius outer 0.560 0.439 0.622
Table 4: Results for sentence similarity. There
is no statistically significant difference between
KS14 addition and NWE addition (the second best
result).
Specifically, we get classification results on the
Microsoft Research Paraphrase Corpus paraphrase
corpus (Dolan et al., 2005) working in the follow-
ing way: we construct vectors for the sentences
of each pair; if the cosine similarity between the
two sentence vectors exceeds a certain threshold,
the pair is classified as a paraphrase, otherwise as
not a paraphrase. For this experiment and that of
Section 5.4 below, we investigate only the addi-
tion and point-wise multiplication compositional
models, since at their current stage of development
tensor-based models can only efficiently handle
sentences of fixed structure. Nevertheless, the
simple point-wise compositional models still al-
low for a direct comparison of the vector spaces,
which is the main goal of this paper.
For each vector space and model, a number of
different thresholds were tested on the first 2000
pairs of the training set, which we used as a de-
velopment set; in each case, the best-performed
threshold was selected for a single run of our
?classifier? on the test set (1726 pairs). Addition-
ally, we evaluate the NWE model with a lemma-
tized version of the corpus, so that the experimen-
tal setup is maximally similar for all vector spaces.
The results are shown in the first part of Table 5.
Additive NWE gives the highest performance,
with both lemmatized and un-lemmatized versions
outperforming the GS11 and KS14 spaces. In
the un-lemmatized case, the accuracy of our sim-
ple ?classifier? (0.73) is close to state-of-the-art
range. The state-of-the art result (0.77 accuracy
714
Co-occurrence Neural word embeddings
Baseline GS11 KS14 Unlemmatized Lemmatized
Model Accuracy F-Score Accuracy F-Score Accuracy F-Score Accuracy F-Score Accuracy F-Score
MSR addition
0.65 0.75
0.62 0.79 0.70 0.80 0.73 0.82 0.72 0.81
MSR multiplication 0.52 0.58 0.66 0.80 0.42 0.34 0.41 0.36
SWDA addition
0.60 0.58
0.35 0.35 0.40 0.35 0.63 0.60 0.44 0.40
SWDA multiplication 0.32 0.16 0.39 0.33 0.58 0.53 0.43 0.38
Table 5: Results for paraphrase detection (MSR) and dialog act tagging (SWDA) tasks. All top results
significantly outperform corresponding nearest competitors (for accuracy): p < 0.05, ?
2
test.
and 0.84 F-score
7
) by the time of this writing has
been obtained using 8 machine translation metrics
and three constituent classifiers (Madnani et al.,
2012).
The multiplicative model gives lower results
than the additive model across all vector spaces.
The KS14 vector space shows the steadiest per-
formance, with a drop in accuracy of only 0.04
and no drop in F-score, while for the GS11 and
NWE spaces both accuracy and F-score experi-
enced drops by more than 0.20.
5.4 Dialogue act tagging
As our last experiment, we evaluate the word
spaces on a dialogue act tagging task (Stolcke et
al., 2000) over the Switchboard corpus (Godfrey
et al., 1992). Switchboard is a collection of ap-
proximately 2500 dialogs over a telephone line by
500 speakers from the U.S. on predefined topics.
8
The experiment pipeline follows (Milajevs and
Purver, 2014). The input utterances are prepro-
cessed so that the parts of interrupted utterances
are concatenated (Webb et al., 2005). Disfluency
markers and commas are removed from the utter-
ance raw texts. For GS11 and KS14 the utterance
tokens are POS-tagged and lemmatized; for NWE,
we test the vectors in both a lemmatized and an
un-lemmatized version of the corpus.
9
We split
the training and testing utterances as suggested by
Stolcke et al. (2000). Utterance vectors are then
obtained as in the previous experiments; they are
reduced to 50 dimensions using SVD and a k-
nearest-neighbour classifier is trained on these re-
duced utterance vectors (the 5 closest neighbours
by Euclidean distance are retrieved to make a clas-
7
F-scores use the standard definition F = 2(precision ?
recall)/(precision + recall).
8
The dataset and a Python interface to it are available
at http://compprag.christopherpotts.net/
swda.html
9
We use WordNetLemmatizer of the NLTK library
(Bird, 2006).
sification decision). The results are shown in the
second part of Table 5.
Un-lemmatized NWE addition gave the best ac-
curacy (0.63) and F-score (0.60) (averaged over
tag classes), i.e. similar results to (Milajevs and
Purver, 2014)?although note that the dimension-
ality of our NWE vectors is 10 times lower than
theirs. Multiplicative NWE outperformed the cor-
responding model in (Milajevs and Purver, 2014).
In general, addition consistently outperforms mul-
tiplication for all the models. Lemmatization
dramatically lowers tagging accuracy: the lem-
matized GS11, KS14 and NWE models perform
much worse than un-lemmatized NWE, suggest-
ing that morphological features are important for
this task.
6 Discussion
Previous comparisons of co-occurrence-based and
neural word vector representations vary widely
in their conclusions. While Baroni et al. (2014)
conclude that ?context-predicting models obtain
a thorough and resounding victory against their
count-based counterparts?, this seems to contra-
dict, at least at the first consideration, the more
conservative conclusion of Levy et al. (2014) that
?analogy recovery is not restricted to neural word
embeddings [. . . ] a similar amount of relational
similarities can be recovered from traditional dis-
tributional word representations? and the findings
of Blacoe and Lapata (2012) that ?shallow ap-
proaches are as good as more computationally in-
tensive alternatives? on phrase similarity and para-
phrase detection tasks.
It seems clear that neural word embeddings
have an advantage when used in tasks for which
they have been trained; our main questions here
are whether they outperform co-occurrence based
alternatives across the board; and which ap-
proach lends itself better to composition using
general mathematical operators. To partially an-
715
swer this question, we can compare model be-
haviour against the baselines in isolation.
For the disambiguation and sentence similarity
tasks the baseline is the similarity between verbs
only, ignoring the context?see above. For the
paraphrase task, we take the global vector-based
similarity reported in (Mihalcea et al., 2006): 0.65
accuracy and 0.75 F-score. For the dialogue act
tagging task the baseline is the accuracy of the
bag-of-unigrams model in (Milajevs and Purver,
2014): 0.60.
Sections 5.1 and 5.2 show that although the best
choice of vector representation might vary, for
small-scale tasks all methods give fairly compet-
itive results. The choice of compositional oper-
ator seems to be more important and more task-
specific: while a tensor-based operation (Frobe-
nius copy-object) performs best for verb disam-
biguation, the best result for sentence similarity
is achieved by a simple additive model, with all
other compositional methods behaving worse than
the verb-only baseline in the KS14 case. GS11 and
NWE, on the other hand, outperform their base-
lines with a number of compositional methods, al-
though both of them achieve lower performance
than KS14 overall.
Based on only small-scale experiment results,
one could conclude that there is little significant
difference between the two ways of obtaining vec-
tors. GS11 and NWE show similar behaviour in
comparison to their baselines, while it is possible
to tune a co-occurrence based vector space (KS14)
and obtain the best result. Large scale tasks reveal
another pattern: the GS11 vector space, which be-
haves stably on the small scale, drags behind the
KS14 and NWE spaces in the paraphrase detec-
tion task. In addition, NWE consistently yields
best results. Finally, only the NWE space was able
to provide adequate results on the dialogue act tag-
ging task. Table 6 summarizes model performance
with regard to baselines.
7 Conclusion
In this work we compared the performance of two
co-occurrence-based semantic spaces with vectors
learned by a neural network in compositional set-
tings. We carried out two small-scale tasks (word
sense disambiguation and sentence similarity) and
two large-scale tasks (paraphrase detection and di-
alogue act tagging).
Task GS11 KS14 NWE
Disambiguation + + +
Sentence similarity + ? +
Paraphrase ? + +
Dialog act tagging ? ? +
Table 6: Summary of vector space performance
against baselines. General improvement (cases
where more than a half of the models perform bet-
ter) and decrease with regard to a corresponding
baseline is respectively marked by + and ?. A
bold value means that the model gave the best re-
sult in the task.
On small-scale tasks, where the sentence struc-
tures are predefined and relatively constrained,
NWE gives better or similar results to count-based
vectors. Tensor-based composition does not al-
ways outperform simple compositional operators,
but for most of the cases gives results within the
same range.
On large-scale tasks, neural vectors are more
successful than the co-occurrence based alterna-
tives. However, this study does not reveal whether
this is because of their neural nature, or just be-
cause they are trained on a larger amount of data.
The question of whether neural vectors outper-
form co-occurrence vectors therefore requires fur-
ther detailed comparison to be entirely resolved;
our experiments suggest that this is indeed the case
in large-scale tasks, but the difference in size and
nature of the original corpora may be a confound-
ing factor. In any case, it is clear that the neural
vectors of word2vec package perform steadily
off-the-shelf across a large variety of tasks. The
size of the vector space (3 million words) and the
available code-base that simplifies the access to
the vectors, makes this set a good and safe choice
for experiments in the future. Of course, even bet-
ter performances can be achieved by training neu-
ral language models specifically for a given task
(see e.g. Kalchbrenner et al. (2014)).
The choice of compositional operator (tensor-
based or a simple point-wise operation) depends
strongly on the task and dataset: tensor-based
composition performed best with the verb dis-
ambiguation task, where the verb senses depend
strongly on the arguments of the verb. However, it
seems to depend less on the nature of the vectors
itself: in the disambiguation task, tensor-based
716
composition proved best for both co-occurrence-
based and neural vectors; in the sentence similar-
ity task, where point-wise operators proved best,
this was again true across vector spaces.
Acknowledgements
We would like to thank the three anonymous
reviewers for their fruitful comments. Sup-
port by EPSRC grant EP/F042728/1 is grate-
fully acknowledged by Milajevs, Kartsaklis and
Sadrzadeh. Purver is partly supported by Con-
CreTe: the project ConCreTe acknowledges the fi-
nancial support of the Future and Emerging Tech-
nologies (FET) programme within the Seventh
Framework Programme for Research of the Eu-
ropean Commission, under FET grant number
611733.
References
Marco Baroni and Roberto Zamparelli. 2010. Nouns
are vectors, adjectives are matrices: Representing
adjective-noun constructions in semantic space. In
Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing, pages
1183?1193. Association for Computational Linguis-
tics.
Marco Baroni, Georgiana Dinu, and Germ?an
Kruszewski. 2014. Don?t count, predict! a
systematic comparison of context-counting vs.
context-predicting semantic vectors. In Proceedings
of the 52nd Annual Meeting of the Association for
Computational Linguistics, volume 1.
Yoshua Bengio, Holger Schwenk, Jean-S?ebastien
Sen?ecal, Fr?ederic Morin, and Jean-Luc Gauvain.
2006. Neural probabilistic language models. In
Innovations in Machine Learning, pages 137?186.
Springer.
Steven Bird. 2006. NLTK: the natural language
toolkit. In Proceedings of the COLING/ACL on In-
teractive presentation sessions, pages 69?72. Asso-
ciation for Computational Linguistics.
William Blacoe and Mirella Lapata. 2012. A com-
parison of vector-based representations for semantic
composition. In Proceedings of the 2012 Joint Con-
ference on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning, pages 546?556. Association for Compu-
tational Linguistics.
Johan Bos and Malte Gabsdil. 2000. First-order infer-
ence and the interpretation of questions and answers.
Proceedings of Gotelog, pages 43?50.
Johan Bos. 2008. Wide-coverage semantic analy-
sis with boxer. In Johan Bos and Rodolfo Del-
monte, editors, Semantics in Text Processing. STEP
2008 Conference Proceedings, Research in Compu-
tational Semantics, pages 277?286. College Publi-
cations.
N. Bourbaki. 1989. Commutative Algebra: Chapters
1-7. Srpinger Verlag, Berlin/New York.
Bob Coecke, Mehrnoosh Sadrzadeh, and Stephen
Clark. 2010. Mathematical foundations for a com-
positional distributional model of meaning. CoRR,
abs/1003.4394.
Ronan Collobert and Jason Weston. 2008. A unified
architecture for natural language processing: Deep
neural networks with multitask learning. In Pro-
ceedings of the 25th international conference on
Machine learning, pages 160?167. ACM.
Ann Copestake, Dan Flickinger, Carl Pollard, and
Ivan A Sag. 2005. Minimal recursion semantics:
An introduction. Research on Language and Com-
putation, 3(2-3):281?332.
Bill Dolan, Chris Brockett, and Chris Quirk. 2005. Mi-
crosoft research paraphrase corpus. Retrieved May,
29:2013.
Adriano Ferraresi, Eros Zanchetta, Marco Baroni, and
Silvia Bernardini. 2008. Introducing and evaluating
ukWaC, a very large web-derived corpus of English.
In Proceedings of the 4th Web as Corpus Workshop
(WAC-4) Can we beat Google, pages 47?54.
Gottlob Frege. 1892. On sense and reference. Ludlow
(1997), pages 563?584.
John J Godfrey, Edward C Holliman, and Jane Mc-
Daniel. 1992. Switchboard: Telephone speech cor-
pus for research and development. In Acoustics,
Speech, and Signal Processing, 1992. ICASSP-92.,
1992 IEEE International Conference on, volume 1,
pages 517?520. IEEE.
Yoav Goldberg and Omer Levy. 2014. word2vec
Explained: deriving Mikolov et al.?s negative-
sampling word-embedding method. arXiv preprint
arXiv:1402.3722.
Edward Grefenstette and Mehrnoosh Sadrzadeh.
2011a. Experimental support for a categorical com-
positional distributional model of meaning. In Pro-
ceedings of the Conference on Empirical Methods
in Natural Language Processing, pages 1394?1404.
Association for Computational Linguistics.
Edward Grefenstette and Mehrnoosh Sadrzadeh.
2011b. Experimenting with transitive verbs in a Dis-
CoCat. In Proceedings of the GEMS 2011 Work-
shop on GEometrical Models of Natural Language
Semantics, pages 62?66, Edinburgh, UK, July. As-
sociation for Computational Linguistics.
Z.S. Harris. 1954. Distributional structure. Word.
717
Nal Kalchbrenner and Phil Blunsom. 2013. Recurrent
convolutional neural networks for discourse compo-
sitionality. In Proceedings of the Workshop on Con-
tinuous Vector Space Models and their Composition-
ality, pages 119?126, Sofia, Bulgaria, August. Asso-
ciation for Computational Linguistics.
Nal Kalchbrenner, Edward Grefenstette, and Phil Blun-
som. 2014. A convolutional neural network for
modelling sentences. Proceedings of the 52nd An-
nual Meeting of the Association for Computational
Linguistics, June.
Dimitri Kartsaklis and Mehrnoosh Sadrzadeh. 2013.
Prior disambiguation of word tensors for construct-
ing sentence vectors. In Proceedings of the 2013
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNL), pages 1590?1601, Seat-
tle, USA, October. Association for Computational
Linguistics.
Dimitri Kartsaklis and Mehrnoosh Sadrzadeh. 2014. A
study of entanglement in a categorical framework of
natural language. In Proceedings of the 11th Work-
shop on Quantum Physics and Logic (QPL), Kyoto,
Japan, June.
Dimitri Kartsaklis, Mehrnoosh Sadrzadeh, and Stephen
Pulman. 2012. A unified sentence space for
categorical distributional-compositional semantics:
Theory and experiments. In Proceedings of COL-
ING 2012: Posters, pages 549?558, Mumbai, India,
December. The COLING 2012 Organizing Commit-
tee.
Douwe Kiela and Stephen Clark. 2014. A systematic
study of semantic vector space model parameters.
In Proceedings of the 2nd Workshop on Continu-
ous Vector Space Models and their Compositionality
(CVSC), pages 21?30, Gothenburg, Sweden, April.
Association for Computational Linguistics.
T. Landauer and S. Dumais. 1997. A Solution
to Plato?s Problem: The Latent Semantic Analysis
Theory of Acquision, Induction, and Representation
of Knowledge. Psychological Review.
Geoffrey Leech, Roger Garside, and Michael Bryant.
1994. Claws4: the tagging of the british national
corpus. In Proceedings of the 15th conference
on Computational linguistics-Volume 1, pages 622?
628. Association for Computational Linguistics.
Omer Levy, Yoav Goldberg, and Israel Ramat-Gan.
2014. Linguistic regularities in sparse and explicit
word representations. In Proceedings of the Eigh-
teenth Conference on Computational Natural Lan-
guage Learning, Baltimore, Maryland, USA, June.
Association for Computational Linguistics.
Nitin Madnani, Joel Tetreault, and Martin Chodorow.
2012. Re-examining machine translation metrics
for paraphrase identification. In Proceedings of the
2012 Conference of the North American Chapter of
the Association for Computational Linguistics: Hu-
man Language Technologies, pages 182?190. Asso-
ciation for Computational Linguistics.
Rada Mihalcea, Courtney Corley, and Carlo Strappa-
rava. 2006. Corpus-based and knowledge-based
measures of text semantic similarity. In AAAI, vol-
ume 6, pages 775?780.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-
frey Dean. 2013a. Efficient estimation of word
representations in vector space. arXiv preprint
arXiv:1301.3781.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013b. Distributed representa-
tions of words and phrases and their compositional-
ity. In Advances in Neural Information Processing
Systems, pages 3111?3119.
Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.
2013c. Linguistic regularities in continuous space
word representations. In Proceedings of NAACL-
HLT, pages 746?751.
Dmitrijs Milajevs and Matthew Purver. 2014. Inves-
tigating the contribution of distributional semantic
information for dialogue act classification. In Pro-
ceedings of the 2nd Workshop on Continuous Vector
Space Models and their Compositionality (CVSC),
pages 40?47, Gothenburg, Sweden, April. Associa-
tion for Computational Linguistics.
Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. In Proceedings
of ACL-08: HLT, pages 236?244. Association for
Computational Linguistics.
Jeff Mitchell and Mirella Lapata. 2010. Composition
in distributional models of semantics. Cognitive Sci-
ence, 34(8):1388?1439.
Richard Montague. 1970. Universal grammar. Theo-
ria, 36(3):373?398.
Tamara Polajnar and Stephen Clark. 2014. Improv-
ing distributional semantic vectors through context
selection and normalisation. In Proceedings of the
14th Conference of the European Chapter of the As-
sociation for Computational Linguistics, pages 230?
238, Gothenburg, Sweden, April. Association for
Computational Linguistics.
Radim
?
Reh?u?rek and Petr Sojka. 2010. Software
Framework for Topic Modelling with Large Cor-
pora. In Proceedings of the LREC 2010 Workshop
on New Challenges for NLP Frameworks, pages 45?
50, Valletta, Malta, May. ELRA. http://is.
muni.cz/publication/884893/en.
Hinrich Sch?utze. 1997. Ambiguity resolution in natu-
ral language learning. csli. Stanford, CA, 4:12?36.
718
Richard Socher, Brody Huval, Christopher D Manning,
and Andrew Y Ng. 2012. Semantic compositional-
ity through recursive matrix-vector spaces. In Pro-
ceedings of the 2012 Joint Conference on Empiri-
cal Methods in Natural Language Processing and
Computational Natural Language Learning, pages
1201?1211. Association for Computational Linguis-
tics.
Andreas Stolcke, Klaus Ries, Noah Coccaro, Eliza-
beth Shriberg, Rebecca Bates, Daniel Jurafsky, Paul
Taylor, Carol Van Ess-Dykema, Rachel Martin, and
Marie Meteer. 2000. Dialogue act modeling for
automatic tagging and recognition of conversational
speech. Computational Linguistics, 26(3):339?373.
Peter D Turney, Patrick Pantel, et al. 2010. From
frequency to meaning: Vector space models of se-
mantics. Journal of artificial intelligence research,
37(1):141?188.
Peter D Turney. 2006. Similarity of semantic relations.
Computational Linguistics, 32(3):379?416.
Nick Webb, Mark Hepple, and Yorick Wilks. 2005.
Dialogue act classification based on intra-utterance
features. In Proceedings of the AAAI Workshop on
Spoken Language Understanding. Citeseer.
719
Proceedings of the 2nd Workshop on Continuous Vector Space Models and their Compositionality (CVSC) @ EACL 2014, pages 40?47,
Gothenburg, Sweden, April 26-30 2014.
c?2014 Association for Computational Linguistics
Investigating the Contribution of Distributional Semantic Information for
Dialogue Act Classification
Dmitrijs Milajevs
Queen Mary University of London
d.milajevs@qmul.ac.uk
Matthew Purver
Queen Mary University of London
m.purver@qmul.ac.uk
Abstract
This paper presents a series of experiments
in applying compositional distributional
semantic models to dialogue act classifica-
tion. In contrast to the widely used bag-of-
words approach, we build the meaning of
an utterance from its parts by composing
the distributional word vectors using vec-
tor addition and multiplication. We inves-
tigate the contribution of word sequence,
dialogue act sequence, and distributional
information to the performance, and com-
pare with the current state of the art ap-
proaches. Our experiment suggests that
that distributional information is useful for
dialogue act tagging but that simple mod-
els of compositionality fail to capture cru-
cial information from word and utterance
sequence; more advanced approaches (e.g.
sequence- or grammar-driven, such as cat-
egorical, word vector composition) are re-
quired.
1 Introduction
One of the fundamental tasks in automatic dia-
logue processing is dialogue act tagging: labelling
each utterance with a tag relating to its function
in the dialogue and effect on the emerging con-
text: greeting, query, statement etc (see e.g. (Core,
1998)). Although factors such as intonation also
play a role (see e.g. (Jurafsky et al., 1998)), one
of the most important sources of information in
this task is the semantic meaning of an utterance,
and this is reflected in the fact that people use
similar words when they perform similar utterance
acts. For example, utterances which state opinion
(tagged sv in the standard DAMSL schema, see
below) often include words such as ?I think?, ?I
believe?, ?I guess? etc. Hence, a similarity-based
model of meaning ? for instance, a distributional
semantic model ? should provide benefits over
a purely word-based model for dialogue act tag-
ging. However, since utterances generally con-
sist of more than one word, one has to be able
to extend such similarity-based models from sin-
gle words to sentences and/or complete utterances.
Hence, we consider here the application of compo-
sitional distributional semantics for this task.
Here, we extend bag-of-word models com-
mon in previous approaches (Serafin et al., 2003)
with simple compositional distributional opera-
tions (Mitchell and Lapata, 2008) and examine the
improvements gained. These improvements sug-
gest that distributional information does improve
performance, but that more sophisticated compo-
sitional operations such as matrix multiplication
(Baroni and Zamparelli, 2010; Grefenstette and
Sadrzadeh, 2011) should provide further benefits.
The state of the art is a supervised method
based on Recurrent Convolutional Neural Net-
works (Kalchbrenner and Blunsom, 2013). This
method learns both the sentence model and the
discourse model from the same training corpus,
making it hard to understand how much of the
contribution comes from the inclusion of distribu-
tional word meaning, and how much from learn-
ing patterns specific to the corpus at hand. Here,
in contrast, we use an external unlabeled resource
to obtain a model of word meaning, composing
words to obtain representations for utterances, and
rely on training data only for discourse learning
for the tagging task itself.
We proceed as follows. First, we discuss related
work by introducing distributional semantics and
describe common approaches for dialogue act tag-
ging in Section 2. Section 3 proposes several mod-
els for utterance representation based on the bag of
words approach and word vector composition. We
describe the experiment and discuss the result in
Section 4. Finally, Section 5 concludes the work.
40
2 Related work
Distributional semantics The aim of natural
language semantics is to provide logical represen-
tations of meaning for information in textual form.
Distributional semantics is based on the idea that
?You shall know a word by the company it keeps?
(Firth, 1957) ? in other words, the meaning of a
word is related to the contexts it appears in. Fol-
lowing this idea, word meaning can be represented
as a vector where its dimensions correspond to the
usage contexts, usually other words observed to
co-occur, and the values are the co-occurrence fre-
quencies. Such a meaning representation is easy
to build from raw data and does not need rich an-
notation.
Methods based on this distributional hypothe-
sis have recently been applied to many tasks, but
mostly at the word level: for instance, word sense
disambiguation (Zhitomirsky-Geffet and Dagan,
2009) and lexical substitution (Thater et al., 2010).
They exploit the notion of similarity which corre-
lates with the angle between word vectors (Turney
et al., 2010). Compositional distributional seman-
tics goes beyond the word level and models the
meaning of phrases or sentences based on their
parts. Mitchell and Lapata (2008) perform com-
position of word vectors using vector addition and
multiplication operations. The limitation of this
approach is the operator associativity, which ig-
nores the argument order, and thus word order. As
a result, ?John loves Mary? and ?Mary loves John?
get assigned the same meaning.
To capture word order, various approaches
have been proposed. Grefenstette and Sadrzadeh
(2011) extend the compositional approach by us-
ing non-associative linear algebra operators as
proposed in the theoretical work of (Coecke et
al., 2010). Socher et al. (2012) present a recur-
sive technique to build compositional meaning of
phrases from their constituents, where the non-
linear composition operators are learned by Neural
Networks.
Dialogue act tagging There are many ways to
approach the task of dialogue act tagging (Stol-
cke et al., 2000). The most successful approaches
combine intra-utterance features, such as the (se-
quences of) words and intonational contours used,
together with inter-utterance features, such as the
sequence of utterance tags being used previously.
To capture both of these aspects, sequence models
such as Hidden Markov Models are widely used
(Stolcke et al., 2000; Surendran and Levow, 2006).
The sequence of words is an observable variable,
while the sequence of dialogue act tags is a hidden
variable.
However, some approaches have shown com-
petitive results without exploiting features of inter-
utterance context. Webb et al. (2005) concentrate
only on features found inside an utterance, identi-
fying ngrams that correlate strongly with particu-
lar utterance tags, and propose a statistical model
for prediction which produces close to the state of
the art results.
The current state of the art (Kalchbrenner and
Blunsom, 2013) uses Recurrent Convolutional
Neural Networks to achieve high accuracy. This
model includes information about word identity,
intra-utterance word sequence, and inter-utterance
tag sequence, by using a vector space model of
words with a compositional approach. The words
vectors are not based on distributional frequencies
in this case, however, but on randomly initialised
vectors, with the model trained on a specific cor-
pus. This raises several questions: what is the con-
tribution of word sequence and/or utterance (tag)
sequence; and might further gains be made by ex-
ploiting the distributional hypothesis?
As our baseline, we start with an approach
which uses only word information, and excludes
word sequence, tag sequence and word distribu-
tions. Serafin et al. (2003) use Latent Semantic
Analysis for dialogue act tagging: utterances are
represented using a bag-of-words representation
in a word-document matrix. The rows in the ma-
trix correspond to words, the columns correspond
to documents and each cell in the matrix contains
the number of times a word occurs in a document.
Singular Value Decomposition (SVD) is then ap-
plied to reduce the number of rows in the matrix,
with the number of components in the reduced
space set to 50. To predict the tag of an unseen
utterance, the utterance vector is mapped to the re-
duced space and the tag of the closest neighbor is
assigned to it (using cosine similarity as a similar-
ity measure). The reported accuracy on the Span-
ish Call Home corpus for predicting 37 different
utterance tags is 65.36%.
3 Utterance models
In this paper, we investigate the extent to which
distributional representations, word order infor-
41
mation, and utterance order information can im-
prove this basic model, by choosing different ways
to represent an utterance in a vector space. We de-
sign three basic models. The first model is based
directly on the bag-of-words model which serves
as the baseline in our experiment, following (Ser-
afin et al., 2003); and extends this to investigate the
effect of word order information by moving from
word unigrams to bigrams. The second model
investigates distributional information, by calcu-
lating word vector representations from a general
corpus, and obtaining utterance representations by
composing the word vectors using simple opera-
tors. The third model extends this idea to inves-
tigate the role of utterance order information, by
including the information about the previous ut-
terance.
Bag of words The first model represents an ut-
terance as a vector where each component corre-
sponds to a word. The values of vector compo-
nents are the number of times the corresponding
words occured in the utterance. The model is sim-
ilar to (Serafin et al., 2003), but the matrix is trans-
posed. We refer to it as bag of unigrams in Table 1.
However, this bag of words approach does not
preserve any word order information. As it has
been said previously, for the dialogue act tagging
word order may be crucial. Consider these utter-
ances:
? John, are there cookies
? John, there are cookies
One of the utterances is a question (or request)
while the other is a statement. However, the bag
of words model will extract the same vector repre-
sentation for both.
To overcome this problem we also represent an
utterance as a bag of bigrams. When bigrams are
used in place of single words, the utterance rep-
resentation will differ. The question contains the
bigram ?are there?, while the statement contains
the bigram ?there are?.
Simple composition Our second model ex-
ploits the distributional hypothesis, by represent-
ing words not as atomic types (i.e. individual di-
mensions in the utterance matrix, as above), but
as vectors encoding their observed co-occurrence
distributions. We estimate these from a large cor-
pus of general written English (the Google Books
Ngrams corpus ? see below).
However, this raises the question of how to
compose these word vectors into a single repre-
sentation for an utterance. Various approaches to
compositional vector space modelling have been
successfully applied to capture the meaning of a
phrase in a range of tasks (Mitchell and Lapata,
2008; Grefenstette and Sadrzadeh, 2011; Socher
et al., 2013). In this work, we follow (Mitchell and
Lapata, 2008) and apply vector addition and point-
wise multiplication to obtain the vector of an ut-
terance from the words it consists of. This has the
advantage of simplicity and domain-generality, re-
quiring no sentence grammar (problematic for the
non-canonical language in dialogue) or training on
a specific corpus to obtain the appropriate compo-
sitionality operators or associative model; but has
the disadvantage of losing word order information.
The corresponding models are referred as addition
and multiplication in Table 1 and Table 2.
Previous utterance A conversation is a se-
quence of utterances, and the tag of an utter-
ance often depends on the previous utterance
(e.g. answers tend to follow questions). Hid-
den Markov Models (Surendran and Levow, 2006;
Stolcke et al., 2000) are often used to cap-
ture these dependencies; Recurrent Convolutional
Neural Networks (Kalchbrenner and Blunsom,
2013) have been used to simultaneously capture
the intra-utterance sequence of words and the
inter-utterance sequence of dialog tags in a con-
versation.
In this model, we are interested specifically in
the effect of inter-utterance (tag) sequence. We
provide previous addition and previous multipli-
cation models as simple attempts to capture this
phenomenon: the vector of an utterance is the con-
catenation of its vector obtained in the correspond-
ing compositional model (addition or multiplica-
tion) and the vector of the previous utterance.
4 Predicting dialogue acts
4.1 The resources
This section describes the resources we use to
evaluate and compare the proposed models.
Switchboard corpus The Switchboard corpus
(Godfrey et al., 1992) is a corpus of telephone con-
versations on selected topics. It consists of about
2500 conversations by 500 speakers from the U.S.
The conversations in the corpus are labeled with
42 unique dialogue act tags and split to 1115 train
42
A o : Okay. /
A qw : {D So, }
B qy?d: [ [I guess, +
A + : What kind of experience
[ do you, + do you ] have,
then with child care? /
B + : I think, ] + {F uh, }
I wonder if that worked. /
(a) A conversation with interrupted utterances.
A o : Okay.
A qw : So What kind of experience
do you do you have then
with child care?
B qy?d: I guess I think uh I wonder
if that worked.
(b) A preprocessed conversation.
Figure 1: A example of interrupted utterances from Switchboard and their transformation.
and 19 test conversations (Jurafsky et al., 1997;
Stolcke et al., 2000).
In addition to the dialog act tags, utterances
interrupted by the other speaker (and thus split
into two or more parts) have their continuations
marked with a special tag ?+?. Tag prediction of
one part of an interrupted utterance in isolation is
a difficult task even for a human; for example, it
would not be clear why the utterance ?So,? should
be assigned the tag qw (wh-question) in Figure 1a
without the second part ?What kind of experience
do you have [. . . ]?. Following (Webb et al., 2005)
we preprocess Switchboard by concatenating the
parts of an interrupted utterance together, giving
the result the tag of the first part and putting it in
its place in the conversation sequence. We also
remove commas and disfluency markers from the
raw text. Figure 1b illustrates the transformation
we do as preprocessing.
We split the utterances between training and
testing as suggested in (Stolcke et al., 2000).
Google Books Ngram Corpus The Google
Books Ngram Corpus (Lin et al., 2012) is a col-
lection of n-gram frequencies over books written
in 8 languages. The English part of the corpus is
based on more than 4.5 million books and contains
more than four thousand billion tokens. The re-
source provides frequencies of n-grams of length
1 to 5. For our experiments we use 5-grams from
the English part of the resource.
4.2 Word vector spaces
In distributional semantics, the meanings of words
are captured by a vector space model based on a
word co-occurrence matrix. Each row in the ma-
trix represents a target word, and each column rep-
resents a context word; each element in the matrix
is the number of times a target word co-occured
with a corresponding context word. The frequency
counts are typically normalized, or weighted us-
ing tf-idf or log-likelihood ratio to obtain better re-
sults, see (Mitchell and Lapata, 2008; Agirre et al.,
2009) for various approaches. It is also common
to apply dimensionality reduction to get higher
performance (Dinu and Lapata, 2010; Baroni and
Zamparelli, 2010).
As target words we select all the words in our
(Switchboard) training split. As context words
we choose the 3000 most frequent words in the
Google Ngram Corpus, excluding the 100 most
frequent. To obtain co-occurrence frequencies
from ngrams we sum up the frequency of a 5-gram
over the years, treat the word in the middle as a
target, and the other words as its contexts.
For normalization, we experiment with a vec-
tor space based on raw co-occurrences; a vector
space where frequencies are weighted using tf-idf;
and another one with the number of dimensions
reduced to 1000 using Non-negative Matrix Fac-
torization (NMF) (Hoyer, 2004).
We use the NMF and tf-idf implementations
provided by scikit-learn version 0.14 (Pe-
dregosa et al., 2011). For tf-idf, the term vectors
are L
2
normalized. For NMF, NNDSVD initial-
ization (Boutsidis and Gallopoulos, 2008) is used,
and the tolerance value for stopping conditions is
set to 0.001. The co-occurrence matrix is line-
normalized, so the sum of the values in each row
is 1 before applying NMF.
1
4.3 Evaluation
To evaluate these possible models we follow (Ser-
afin et al., 2003). Once we have applied a model
to extract features from utterances and build a vec-
tor space, the dimensionality of the vector space
is reduced using SVD to 50 dimensions. Then a
k-nearest neighbours (KNN) classifier is trained
and used for utterance tag prediction. In contrast
to (Serafin et al., 2003), we use Euclidean dis-
tance as a distance metric and choose the most
1
The co-occurrence matrix and the information about the
software used in the experiment are available at
http://www.eecs.qmul.ac.uk/
?
dm303/cvsc14.html
43
Method Accuracy
(Kalchbrenner and Blunsom, 2013) 0.739
(Webb et al., 2005) 0.719
(Stolcke et al., 2000) 0.710
(Serafin et al., 2003) 0.654
Bag of unigrams 0.602
Bag of bigrams 0.621
Addition 0.639
Multiplication 0.572
Previous addition 0.569
Previous multiplication 0.497
Table 1: Comparison with previous work. Note
that (Serafin et al., 2003) do not use Switchboard
and therefore their results are not directly compa-
rable to others.
frequent label among the 5 closest neighbors.
The SVD and KNN classifier implementations in
scikit-learn are used.
Baseline In our experiments, the bag of uni-
grams model accuracy of 0.602 is lower than the
accuracy of 0.654 reported in (Serafin et al., 2003),
see Table 1. The lower performance may be due
to the differences between Switchboard and Call-
Home37 corpora, in particular the tag distribu-
tion.
2
In CallHome37, 42.7% of utterances are la-
beled with the most frequent dialogue act, while
the figure in Switchboard is 31.5%; the more even
distribution in Switchboard is likely to make over-
all average accuracy levels lower.
Word order As Table 1 shows, the bag of bi-
grams model improves over unigrams. This con-
firms that word order provides important informa-
tion for predicting dialogue act tags.
Distributional models Performance of compo-
sitional distributional models depends both on
compositional operator and weighting. Table 2
demonstrates accuracy of the models. We instan-
tiate 3 vector spaces from Google Ngrams: one
space with raw co-occurrence frequencies, a tf-idf
weighted space and a reduced space using NMF.
Addition outperforms multiplication in our ex-
periments, although for other tasks multiplication
has been shown to perform better (Grefenstette
and Sadrzadeh, 2011; Mitchell and Lapata, 2008).
Lower multiplication performance here might be
2
The CallHome37 corpus is not currently available to us.
Space
Model Raw tf-idf NMF
Addition without SVD 0.592
Addition 0.610 0.639 0.620
Multiplication 0.572 0.568 0.525
Previous addition 0.569
Previous multiplication 0.497
Table 2: Accuracy results for different composi-
tional models and vector spaces.
due to the fact that some utterances are rather long
(for example, more than 70 tokens), and the result-
ing vectors get many zero components.
Selection of the optimal weighting method
could be crucial for overall model performance.
The 3 weighting schemes we use give a broad va-
riety of results; more elaborate weighting and con-
text selection might give higher results.
Figure 2 illustrates dialog tag assignment us-
ing addition and the tf-idf weighted vector space.
As we do not use any inter-utterance features, the
first two statements, which consist only of the
word Okay, got assigned wrong tags. However,
the Wh-question in the conversation got classified
as a Yes-No-question, probably because what did
not influence the classification decision strongly
enough and could have been classified correctly
using only intra-utterance features. Also, the ex-
ample shows how important grammatical features
are: the verb think appears in many different con-
text, and its presence does not indicate a certain
type of an utterance.
In addition, we observed that SVD improves
classification accuracy. The accuracy of KNN
classification without prior dimensionality reduc-
tion drops from 0.610 to 0.592 for vector addition
on the raw vector space.
Utterance sequence To solve the issue of utter-
ances that can be tagged correctly only by consid-
ering inter-utterance features, we included previ-
ous utterance. However, in our experiment, such
inclusion by vector concatenation does not im-
prove tagging accuracy (Table 2). The reason for
this could be that after concatenation the dimen-
sionality of the space doubles, and SVD can not
handle it properly. We evaluated only dimension-
ally reduced spaces because of the memory limit.
44
B**
(b) : Okay.
A b?m (b) : Okay.
B qw (qy): Well what do you think about the idea of uh kids having to do public
service work for a year?
B qy (sd): Do you think it?s a <breathing>
A sv (sv): Well I I think it?s a pretty good idea.
A sv (sd): I think they should either do that or or afford some time to the military
or or helping elderly people.
B aa (aa): Yes
B aa (b) : yes
B % (%) : def
A sv (sv): I I you know I think that we have a bunch of elderly folks in the country
that could use some help
Figure 2: The beginning of the conversation 2151 from the test split of Switchboard. In brackets the
tags predicted using vector addition as a composition method on the tf-idf space are given. We mark
fo o fw " by bc as
**
.
Summary Our accuracy is lower compared to
other work. Webb et al. (2005)?s method, based
only on intra-utterance lexical features, but incor-
porating longer ngram sequences and feature se-
lection, yields accuracy of 0.719. Advanced treat-
ment of both utterance and discourse level features
yields accuracy of 0.739 (Kalchbrenner and Blun-
som, 2013). However, our experiments allow us to
evaluate the contribution of various kinds of infor-
mation: vector spaces based on word bigrams and
on co-occurrence distributions both outperformed
the bag of words approach; but incorporation of
previous utterance information did not.
5 Conclusions and future work
In this work we evaluated the contribution of
word and utterance sequence, and of distributional
information using simple compositional vector
space models, for dialogue act tagging. Our exper-
iments show that information about intra-utterance
word order (ngrams), and information about word
co-occurence distributions, outperforms the bag of
words models, although not competitive with the
state of the art given the simplistic compositional
approach used here. Information about utterance
tag sequence, on the other hand, did not.
The usage of an external, large scale resource
(here, the Google Ngram Corpus) to model word
senses improves the tagging accuracy in compari-
son to the bag of word model, suggesting that the
dialogue act tag of an utterance depends on its se-
mantics.
However, the improvements in performance of
the bag of bigrams model in comparison to bag of
unigrams, and the much higher results of Webb et
al. (2005)?s intra-utterance approach, suggest that
the sequence of words inside an utterance is cru-
cial for the dialogue act tagging task. This sug-
gests that our simplistic approaches to vector com-
position (addition and multiplication) are likely
to be insufficient: more advanced, sequence- or
grammar-driven composition, such as categorical
composition (Coecke et al., 2010), might improve
the tagging accuracy.
In addition, our results show that the perfor-
mance of distributional models depends on many
factors, including compositional operator selec-
tion and weighting of the initial co-occurrence ma-
trix. Our work leaves much scope for improve-
ments in these factors, including co-occurrence
matrix instantiation. For example, the window
size of 2, which we used to obtain co-occurrence
counts, is lower than the usual size of 5 (Dinu and
Lapata, 2010), or the sentence level (Baroni and
Zamparelli, 2010). Word representation in a vec-
tor space using neural networks might improve ac-
curacy as well (Mikolov et al., 2013).
Previous approaches to dialogue act tagging
have shown utterance/tag sequence to be a use-
ful source of information for improved accuracy
(Stolcke et al., 2000). We therefore conclude that
the lower accuracy we obtained using models that
include information about the previous utterance
is due again to our simplistic method of compo-
sition (vector concatenation); models which re-
flect dialogue structure or sequence explicitly are
likely to be more suited. Kalchbrenner and Blun-
som (2013) give one way in which this can be
achieved by learning from a specific corpus, and
the question of possible alternatives and more gen-
eral models remains for future research.
45
Acknowledgments
We thank Mehrnoosh Sadrzadeh for her helpful
advice and valuable discussion. We would like
to thank anonymous reviewers for their effective
comments. Milajevs is supported by the EP-
SRC project EP/J002607/1. Purver is supported
in part by the European Community?s Seventh
Framework Programme under grant agreement no
611733 (ConCreTe).
References
Eneko Agirre, Enrique Alfonseca, Keith Hall, Jana
Kravalova, Marius Pas?ca, and Aitor Soroa. 2009.
A study on similarity and relatedness using distribu-
tional and wordnet-based approaches. In Proceed-
ings of Human Language Technologies: The 2009
Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
pages 19?27. Association for Computational Lin-
guistics.
Marco Baroni and Roberto Zamparelli. 2010. Nouns
are vectors, adjectives are matrices: Representing
adjective-noun constructions in semantic space. In
Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing, pages
1183?1193. Association for Computational Linguis-
tics.
Christos Boutsidis and Efstratios Gallopoulos. 2008.
Svd based initialization: A head start for nonneg-
ative matrix factorization. Pattern Recognition,
41(4):1350?1362.
Bob Coecke, Mehrnoosh Sadrzadeh, and Stephen
Clark. 2010. Mathematical foundations for a com-
positional distributional model of meaning. CoRR,
abs/1003.4394.
Mark Core. 1998. Analyzing and predicting patterns
of damsl utterance tags. In Proceedings of the AAAI
spring symposium on Applying machine learning to
discourse processing.
G. Dinu and M. Lapata. 2010. Measuring distribu-
tional similarity in context. In Proceedings of the
2010 Conference on Empirical Methods in Natural
Language Processing, pages 1162?1172. Associa-
tion for Computational Linguistics.
John R. Firth. 1957. A Synopsis of Linguistic Theory,
1930-1955. Studies in Linguistic Analysis, pages 1?
32.
John J Godfrey, Edward C Holliman, and Jane Mc-
Daniel. 1992. Switchboard: Telephone speech cor-
pus for research and development. In Acoustics,
Speech, and Signal Processing, 1992. ICASSP-92.,
1992 IEEE International Conference on, volume 1,
pages 517?520. IEEE.
Edward Grefenstette and Mehrnoosh Sadrzadeh. 2011.
Experimental support for a categorical composi-
tional distributional model of meaning. In Proceed-
ings of the Conference on Empirical Methods in Nat-
ural Language Processing, pages 1394?1404. Asso-
ciation for Computational Linguistics.
Patrik O Hoyer. 2004. Non-negative matrix factor-
ization with sparseness constraints. The Journal of
Machine Learning Research, 5:1457?1469.
Daniel Jurafsky, Elizabeth Shriberg, and Debra Biasca.
1997. Switchboard swbd-damsl shallow-discourse-
function annotation coders manual, draft 13. Tech-
nical Report 97-02, University of Colorado, Boul-
der. Institute of Cognitive Science.
Daniel Jurafsky, Elizabeth Shriberg, Barbara Fox, and
Traci Curl. 1998. Lexical, prosodic, and syn-
tactic cues for dialog acts. In Proceedings of the
ACL-COLING Workshop on Discourse Relations
and Discourse Markers.
Nal Kalchbrenner and Phil Blunsom. 2013. Recurrent
convolutional neural networks for discourse compo-
sitionality. In Proceedings of the Workshop on Con-
tinuous Vector SpaceModels and their Composition-
ality, pages 119?126, Sofia, Bulgaria, August. Asso-
ciation for Computational Linguistics.
Yuri Lin, Jean-Baptiste Michel, Erez Lieberman Aiden,
Jon Orwant, Will Brockman, and Slav Petrov. 2012.
Syntactic annotations for the Google Books ngram
corpus. In Proceedings of the ACL 2012 System
Demonstrations, pages 169?174. Association for
Computational Linguistics.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-
frey Dean. 2013. Efficient estimation of word
representations in vector space. arXiv preprint
arXiv:1301.3781.
Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. In Proceedings
of ACL-08: HLT, pages 236?244. Association for
Computational Linguistics.
F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,
B. Thirion, O. Grisel, M. Blondel, P. Pretten-
hofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Pas-
sos, D. Cournapeau, M. Brucher, M. Perrot, and
E. Duchesnay. 2011. Scikit-learn: Machine learn-
ing in Python. Journal of Machine Learning Re-
search, 12:2825?2830.
Riccardo Serafin, Barbara Di Eugenio, and Michael
Glass. 2003. Latent semantic analysis for dia-
logue act classification. In Proceedings of the 2003
Conference of the North American Chapter of the
Association for Computational Linguistics on Hu-
man Language Technology: companion volume of
the Proceedings of HLT-NAACL 2003?short papers-
Volume 2, pages 94?96. Association for Computa-
tional Linguistics.
46
Richard Socher, Brody Huval, Christopher D Manning,
and Andrew Y Ng. 2012. Semantic compositional-
ity through recursive matrix-vector spaces. In Pro-
ceedings of the 2012 Joint Conference on Empiri-
cal Methods in Natural Language Processing and
Computational Natural Language Learning, pages
1201?1211. Association for Computational Linguis-
tics.
Richard Socher, John Bauer, Christopher D Manning,
and Andrew Y Ng. 2013. Parsing with composi-
tional vector grammars. In In Proceedings of the
ACL conference. Citeseer.
Andreas Stolcke, Klaus Ries, Noah Coccaro, Eliza-
beth Shriberg, Rebecca Bates, Daniel Jurafsky, Paul
Taylor, Carol Van Ess-Dykema, Rachel Martin, and
Marie Meteer. 2000. Dialogue act modeling for
automatic tagging and recognition of conversational
speech. Computational Linguistics, 26(3):339?373.
Dinoj Surendran and Gina-Anne Levow. 2006. Dialog
act tagging with support vector machines and hidden
markov models. In INTERSPEECH.
Stefan Thater, Hagen F?urstenau, and Manfred Pinkal.
2010. Contextualizing semantic representations us-
ing syntactically enriched vector models. In Pro-
ceedings of the 48th Annual Meeting of the Associa-
tion for Computational Linguistics, ACL ?10, pages
948?957, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Peter D Turney, Patrick Pantel, et al. 2010. From
frequency to meaning: Vector space models of se-
mantics. Journal of artificial intelligence research,
37(1):141?188.
Nick Webb, Mark Hepple, and Yorick Wilks. 2005.
Dialogue act classification based on intra-utterance
features. In Proceedings of the AAAI Workshop on
Spoken Language Understanding. Citeseer.
M. Zhitomirsky-Geffet and I. Dagan. 2009. Bootstrap-
ping distributional feature vector quality. Computa-
tional Linguistics, 35(3):435?461.
47
