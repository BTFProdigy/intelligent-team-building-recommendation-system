Domain Portability in Speech-to-Speech Translation
Alon Lavie, Lori Levin, Tanja Schultz, Chad Langley, Benjamin Han
Alicia Tribble, Donna Gates, Dorcas Wallace and Kay Peterson
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA, USA
alavie@cs.cmu.edu
1. INTRODUCTION
Speech-to-speech translation has made significant advances over
the past decade, with several high-visibility projects (C-STAR, Verb-
mobil, the Spoken Language Translator, and others) significantly
advancing the state-of-the-art. While speech recognition can cur-
rently effectively deal with very large vocabularies and is fairly
speaker independent, speech translation is currently still effective
only in limited, albeit large, domains. The issue of domain porta-
bility is thus of significant importance, with several current research
efforts designed to develop speech-translation systems that can be
ported to new domains with significantly less time and effort than
is currently possible.
This paper reports on three experiments on portability of a speech-
to-speech translation system between semantic domains.1 The ex-
periments were conducted with the JANUS system [5, 8, 12], ini-
tially developed for a narrow travel planning domain, and ported
to the doctor-patient domain and an extended tourism domain. The
experiments cover both rule-based and statistical methods, and hand-
written as well as automatically learned rules. For rule-based sys-
tems, we have investigated the re-usability of rules and other knowl-
edge sources from other domains. For statistical methods, we have
investigated how much additional training data is needed for each
new domain. We are also experimenting with combinations of
hand-written and automatically learned components. For speech
recognition, we have conducted studies of what parameters change
when a recognizer is ported from one domain to another, and how
these changes affect recognition performance.
2. DESCRIPTION OF THE INTERLINGUA
The first two experiments concern the analysis component of our
interlingua-based MT system. The analysis component takes a sen-
tence as input and produces an interlingua representation as output.
We use a task-oriented interlingua [4, 3] based on domain actions.
Examples of domain actions are giving information about the on-
set of a symptom (e.g., I have a headache) or asking a patient
1We have also worked on the issue of portability across languages
via our interlingua approach to translation [3] and on portability of
speech recognition across languages [10].
.
to perform some action (e.g., wiggle your fingers). The interlin-
gua, shown in the example below, has five main components: (1) a
speaker tag such as a: for doctor (agent) and c: for a patient (cus-
tomer), (2) a speech act, in this case, give-information (3)
some concepts (+body-state and+existence), and (4) some
arguments (body-state-spec= andbody-location=), and
(5) some sub-arguments (identifiability=no and
inside=head).
I have a pain in my head.
c:give-information+existence+body-state
(body-state-spec=(pain,identifiability=no),
body-location=(inside=head))
3. EXPERIMENT 1:
EXTENSION OF SEMANTIC GRAMMAR
RULES BY HAND AND BY AUTOMATIC
LEARNING
Experiment 1 concerns extension of the coverage of semantic
grammars in the medical domain. Semantic grammars are based
on semantic constituents such as request information phrases (e.g.,
I was wondering : : : ) and location phrases (e.g., in my right arm)
rather than syntactic constituents such as noun phrases and verb
phrases. In other papers [12, 5], we have described how our mod-
ular grammar design enhances portability across domains. The
portable grammar modules are the cross-domain module, contain-
ing rules for things like greetings, and the shared module, contain-
ing rules for things like times, dates, and locations. Figure 1 shows
a parse tree for the sentence How long have you had this pain? XDM
indicates nodes that were produced by cross-domain rules. MED in-
dicates nodes that were produced by rules from the new medical
domain grammar.
The preliminary doctor-patient grammar focuses on three med-
ical situations: give-information+existence ? giving
information about the existence of a symptom (I have been get-
ting headaches); give-information+onset ? giving infor-
mation about the onset of a symptom (The headaches started three
months ago); and give-information+occurrence ? giv-
ing information about the onset of an instance of the symptoms
(The headaches start behind my ears). Symptoms are expressed
as body-state (e.g., pain), body-object (e.g., rash), and
body-event (e.g., bleeding).
Our experiment on extendibility was based on a hand written
seed grammar that was extended by hand and by automatic learn-
ing. The seed grammar covered the domain actions mentioned
above, but did not cover very many ways to phrase each domain
action. For example, it might have covered The headaches started
[request-information+existence+body-state]::MED
( WH-PHRASES::XDM
( [q:duration=]::XDM ( [dur:question]::XDM ( how long ) ) )
HAVE-GET-FEEL::MED ( GET ( have ) ) you
HAVE-GET-FEEL::MED ( HAS ( had ) )
[super_body-state-spec=]::MED
( [body-state-spec=]::MED
( ID-WHOSE::MED
( [identifiability=]
( [id:non-distant] ( this ) ) )
BODY-STATE::MED ( [pain]::MED ( pain ) ) ) ) )
Figure 1: Parser output with nodes produced by medical and cross-domain grammars.
Seed Extended Learned
IF 37.2 37.2 31.3
Domain Action 37.2 37.2 31.3
Speech Act
Recall 43.3 48.2 49.3
Precision 71.0 75.0 45.8
Concept List
Recall 2.2 10.1 32.5
Precision 12.5 42.2 25.1
Top-Level Arguments
Recall 0.0 7.2 29.6
Precision 0.0 42.2 34.4
Top-Level Values
Recall 0.0 8.3 29.8
Precision 0.0 50.0 39.2
Sub-Level Arguments
Recall 0.0 28.3 14.1
Precision 0.0 48.2 12.6
Sub-level Values
Recall 1.2 28.3 14.1
Precision 6.2 48.2 12.9
Table 1: Comparison of seed grammar, human-extended grammar, and machine-learned grammar on unseen data
three months ago but not I started getting the headaches three months
ago. The seed grammar was extended by hand and by automatic
learning to cover a development set of 133 utterances. The re-
sult was two new grammars, a human-extended grammar and a
machine-learned grammar, referred to as the extended and learned
grammars in Table 1. The two new grammars were then tested on
132 unseen sentences in order to compare generality of the rules.
Results are reported only for 83 of the 132 sentences which were
covered by the current interlingua design. The remaining 49 sen-
tences were not covered by the current interlingua design and were
not scored. Results are shown in Table 1.
The parsed test sentences were scored in comparison to a hand-
coded correct interlingua representation. Table 1 separates results
for six components of the interlingua: speech act, concepts, top-
level arguments, top-level values, sub-level arguments, and sub-
level values, in addition to the total interlingua, and the domain
action (speech act and concepts combined). The components of the
interlingua were described in Section 2.
The scores for the total interlingua and domain action are re-
ported as percent correct. The scores for the six components of the
interlingua are reported as average percent precision and recall. For
example, if the correct interlingua for a sentence has two concepts,
and the parser produces three, two of which are correct and one of
which is incorrect, the precision is 66% and the recall is 100%.
Several trends are reflected in the results. Both the human-ex-
tended grammar and the machine-learned grammar show improved
performance over the seed grammar. However, the human extended
grammar tended to outperform the automatically learned grammar
in precision, whereas the automatically learned grammar tended to
outperform the human extended grammar in recall. This result is to
be expected: humans are capable of formulating correct rules, but
may not have time to analyze the amount of data that a machine can
analyze. (The time spent on the human extended grammar after the
seed grammar was complete was only five days.)
Grammar Induction: Our work on automatic grammar induc-
tion for Experiment 1 is still in preliminary stages. At this point,
we have experimented with completely automatic induction (no in-
teraction with a user)2 of new grammar rules starting from a core
grammar and using a development set of sentences that are not
parsable according to the core grammar. The development sen-
tences are tagged with the correct interlingua, and they do not stray
from the concepts covered by the core grammar ? they only cor-
respond to alternative (previously unseen) ways of expressing the
same set of covered concepts. The automatic induction is based
on performing tree matching between a skeletal tree representation
obtained from the interlingua, and a collection of parse fragments
2Previous work on our project [2] investigated learning of grammar
rules with user interaction.
[give-information+onset+symptom]
[manner=]
[sudden]
suddenly
[symptom-location=]
DETP
DET
POSS
my
BODYLOCATION
BODYFLUID
[urine]
urine
became [adj:symptom-name=]
ADJ-SYMPTOM
FUNCTION-ADJ-VALS [attribute=]
[color_attribute]
colored
[abnormal]
dis
Parse chunk #1 Parse chunk #2 Parse chunk #3
Original interlingua:
give-information+onset+symptom
(symptom-name=(abnormal,attribute=color_attribute),symptom-location=urine,
manner=sudden)
Learned Grammar Rule:
s[give-information+onset+symptom]
( [manner=] [symptom-location=] *+became [adj:symptom-name=] )
Figure 2: A reconstructed parse tree from the Interlingua
that is derived from parsing the new sentence with the core gram-
mar. Extensions to the existing rules are hypothesized in a way that
would produce the correct interlingua representation for the input
utterance.
Figure 2 shows a tree corresponding to an automatically learned
rule. The input to the learning algorithm is the interlingua (shown
in bold boxes in the figure) and three parse chunks (circled in the
figure). The dashed edges are augmented by the learning algorithm.
4. EXPERIMENT 2:
PORTING TO A NEW DOMAIN
USING A HYBRID RULE-BASED AND
STATISTICAL ANALYSIS APPROACH
We are in the process of developing a new alternative analysis
approach for our interlingua-based speech-translation systems that
combines rule-based and statistical methods and we believe inher-
ently supports faster porting into new domains. The main aspects
of the approach are the following. Rather than developing com-
plete semantic grammars for analyzing utterances into our interlin-
gua (either completely manually, or using grammar induction tech-
niques), we separate the task into two main levels. We continue to
develop and maintain rule-based grammars for phrases that corre-
spond to argument-level concepts of our interlingua representation
(e.g., time expressions, locations, symptom-names, etc.). However,
instead of developing grammar rules for assembling the argument-
level phrases into appropriate domain actions, we apply machine
learning and classification techniques [1] to learn these mappings
from a corpus of interlingua tagged utterances. (Earlier work on
this task is reported in [6].)
We believe this approach should prove to be more suitable for
fast porting into new domains for the following reasons. Many of
the required argument-level phrase grammars for a new domain are
likely to be covered by already existing grammar modules, as can
be seen by examining the XDM (cross-domain) nodes in Figure 1.
The remaining new phrase grammars are fairly fast and straightfor-
ward to develop. The central questions, however, are whether the
statistical methods used for classifying strings of arguments into
domain actions are accurate enough, and what amounts of tagged
data are required to obtain reasonable levels of performance. To
assess this last question, we tested the performance of the current
speech-act and concept classifiers for the expanded travel-domain
when trained with increasing amounts of training data. The results
of these experiments are shown in Figure 3. We also report the
performance of the domain-action classification derived from the
combined speech-act and concepts. As can be seen, performance
reaches a relative plateau at around 4000-5000 utterances. We see
these results as indicative that this approach should indeed prove to
be significantly easier to port to new domains. Creating a tagged
database of this order of magnitude can be done in a few weeks,
rather than the months required for complete manual grammar de-
velopment time.
5. EXPERIMENT 3:
PORTING THE SPEECH RECOGNIZER
TO NEW DOMAINS
When the speech recognition components (acoustic models, pro-
nunciation dictionary, vocabulary, and language model) are ported
across domains and languages mainly three types of mismatches
Speech Act Classification Accuracy for 16-fold 
Cross-Validation
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
500 1000 2000 3000 4000 5000 6009
Training Set Size
M
ea
n 
A
cc
ur
ac
y
Concept Sequence Classification Accuracy for 16-
fold Cross-Validation
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
500 1000 2000 3000 4000 5000 6009
Training Set Size
M
ea
n 
A
cc
ur
ac
y
Dialog Act Classification Accuracy for 16-fold 
Cross-Validation
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0.45
0.5
500 1000 2000 3000 4000 5000 6009
Training Set Size
M
ea
n 
A
cc
ur
ac
y
Figure 3: Performance of Speech-Act, Concept, and Domain-Action Classifiers Using Increasing Amounts of Training Data
Baseline Systems WER on Different Tasks [%]
BN (Broadcast News) h4e98 1, all F-conditions 18.5
ESST (scheduling and travel planning domain) 24.3
BN+ESST 18.4
C-STAR (travel planning domain) 20.2
Adaptation!Meeting Recognition
ESST on meeting data 54.1
BN on meeting data 44.2
+ acoustic MAP Adaptation (10h meeting data) 40.4
+ language model interpolation (16 meetings) 38.7
BN+ESST on meeting data 42.2
+ language model interpolation (16 meetings) 39.0
Adaptation! Doctor-Patient Domain
C-STAR on doctor-patient data 34.1
+ language model interpolation ( 34 dialogs) 25.1
Table 2: Recognition Results
occur: (1) mismatches in recording condition; (2) speaking style
mismatches; as well as (3) vocabulary and language model mis-
matches. In the past these problems have mostly been solved by
collecting large amounts of acoustic data for training the acoustic
models and development of the pronunciation dictionary, as well
as large text data for vocabulary coverage and language model cal-
culation. However, especially for highly specialized domains and
conversational speaking styles, large databases cannot always be
provided. Therefore, our research has focused on the problem of
how to build LVCSR systems for new tasks and languages [7, 9]
using only a limited amount of data. In this third experiment we
investigate the results of porting the speech recognition component
of our MT system to different new domains. The experiments and
improvements were conducted with the Janus Speech Recognition
Toolkit JRTk [13].
Table 2 shows the results of porting four baseline speech recog-
nition systems to the doctor-patient domain, and to the meeting do-
main. The four baseline systems are trained on Broadcast News
(BN), English SpontaneousScheduling Task (ESST), combined BN
and ESST, and the travel planning domain of the C-STAR consor-
tium (http://www.c-star.org). The given tasks illustrate
a variety of domain size, speaking styles and recording conditions
ranging from clean spontaneous speech in a very limited domain
(ESST, C-STAR) to highly conversational multi-party speech in an
extremely broad domain (Meeting). As a consequence the error
rates on the meeting data are quite high but using MAP (Maximum
A Posteriori) acoustic model adaptation and language model adap-
tation the error rate can be reduced by about 10.2% relative over the
BN baseline system. With the doctor-patient data the drop in error
rate was less severe which can be explained by the similar speaking
style and recording conditions for C-STAR and doctor-patient data.
Details about the applied recognition engine can be found in [10]
for ESST and [11] for the BN system.
6. ACKNOWLEDGMENTS
The research work reported here was funded in part by the DARPA
TIDES Program and supported in part by the National Science
Foundation under Grant number 9982227. Any opinions, findings
and conclusions or recomendations expressed in this material are
those of the author(s) and do not necessarily reflect the views of the
National Science Foundation (NSF) or DARPA.
7. REFERENCES
[1] W. Daelemans, J. Zavrel, K. van der Sloot, and A. van den
Bosch. TiMBL: Tilburg Memory Based Learner, version 3.0
Reference Guide. Technical Report Technical Report 00-01,
ILK, 2000. Avaliable at http://ilk.kub.nl/ ilk/papers/ilk0001.ps.gz.
[2] M. Gavalda`. Epiphenomenal Grammar Acquisition with
GSG. In Proceedings of the Workshop on Conversational
Systems of the 6th Conference on Applied Natural Language
Processing and the 1st Conference of the North American
Chapter of the Association for Computational Linguistics
(ANLP/NAACL-2000), Seattle, U.S.A, May 2000.
[3] L. Levin, D. Gates, A. Lavie, F. Pianesi, D. Wallace,
T. Watanabe, and M. Woszczyna. Evaluation of a Practical
Interlingua for Task-Oriented Dialogue. In Workshop on
Applied Interlinguas: Practical Applications of Interlingual
Approaches to NLP, Seattle, 2000.
[4] L. Levin, D. Gates, A. Lavie, and A. Waibel. An Interlingua
Based on Domain Actions for Machine Translation of
Task-Oriented Dialogues. In Proceedings of the International
Conference on Spoken Language Processing (ICSLP?98),
pages Vol. 4, 1155?1158, Sydney, Australia, 1998.
[5] L. Levin, A. Lavie, M. Woszczyna, D. Gates, M. Gavalda`,
D. Koll, and A. Waibel. The Janus-III Translation System.
Machine Translation. To appear.
[6] M. Munk. Shallow statistical parsing for machine translation.
Master?s thesis, University of Karlsruhe, Karlsruhe,
Germany, 1999. http://www.is.cs.cmu.edu/papers/
speech/masters-thesis/MS99.munk.ps.gz.
[7] T. Schultz and A. Waibel. Polyphone Decision Tree
Specialization for Language Adaptation. In Proceedings of
the ICASSP, Istanbul, Turkey, 2000.
[8] A. Waibel. Interactive Translation of Conversational Speech.
Computer, 19(7):41?48, 1996.
[9] A. Waibel, P. Geutner, L. Mayfield-Tomokiyo, T. Schultz,
and M. Woszczyna. Multilinguality in Speech and Spoken
Language Systems. Proceedings of the IEEE, Special Issue
on Spoken Language Processing, 88(8):1297?1313, 2000.
[10] A. Waibel, H. Soltau, T. Schultz, T. Schaaf, and F. Metze.
Multilingual Speech Recognition, chapter From Speech Input
to Augmented Word Lattices, pages 33?45. Springer Verlag,
Berlin, Heidelberg, New York, artificial Intelligence edition,
2000.
[11] A. Waibel, H. Yu, H. Soltau, T. Schultz, T. Schaaf, Y. Pan,
F. Metze, and M. Bett. Advances in Meeting Recognition.
Submitted to HLT 2001, January 2001.
[12] M. Woszczyna, M. Broadhead, D. Gates, M. Gavalda`,
A. Lavie, L. Levin, and A. Waibel. A Modular Approach to
Spoken Language Translation for Large Domains. In
Proceedings of Conference of the Association for Machine
Translation in the Americas (AMTA?98), Langhorn, PA,
October 1998.
[13] T. Zeppenfeld, M. Finke, K. Ries, and A. Waibel.
Recognition of Conversational Telephone Speech using the
Janus Speech Engine. In Proceedings of the ICASSP?97,
Mu?nchen, Germany, 1997.
Proceedings of the Human Language Technology Conference of the NAACL, Companion Volume, pages 284?287,
New York City, June 2006. c?2006 Association for Computational Linguistics
SconeEdit: A Text-guided Domain Knowledge Editor 
 
 Alicia Tribble Benjamin Lambert Scott E. Fahlman 
Language Technologies  
Institute 
Language Technologies 
Institute 
Language Technologies  
Institute 
Carnegie Mellon University Carnegie Mellon University Carnegie Mellon University 
Pittsburgh, PA 15213 Pittsburgh, PA 15213 Pittsburgh, PA 15213 
atribble@cs.cmu.edu benlambert@cmu.edu sef@cs.cmu.edu 
 
 
 
 
Abstract 
We will demonstrate SconeEdit, a new tool 
for exploring and editing knowledge bases 
(KBs) that leverages interaction with do-
main texts.  The tool provides an annotated 
view of user-selected text, allowing a user 
to see which concepts from the text are in 
the KB and to edit the KB directly from 
this Text View.  Alongside the Text View, 
SconeEdit provides a navigable KB View 
of the knowledge base, centered on con-
cepts that appear in the text.  This unified 
tool gives the user a text-driven way to ex-
plore a KB and add new knowledge. 
1 Introduction 
We will demonstrate SconeEdit, a new tool for 
exploring and editing knowledge bases that inte-
grates domain text.  SconeEdit expands on the 
function of traditional ontology editors by showing 
the user an interactive text window (Text View) 
where the user can view and edit concepts from the 
knowledge base as highlighted terms in their origi-
nal context.  The Text View augments a traditional 
KB View, allowing the user to leverage existing 
knowledge as well as domain-focused text exam-
ples to perform a variety of knowledge-based 
tasks.   
Consider the task of assessing the quality of a 
knowledge base as a resource for a new AI or natu-
ral language system.  In SconeEdit, a user can view 
the knowledge base alongside a text document 
from the target domain.  SconeEdit searches for 
instances of KB concepts in the text and highlights 
them in the Text View.  Already the user can see a 
concise visual sample of the coverage of the KB 
for this domain. 
Now the user can work with the KB View and 
Text View together to navigate the ontology.  
Double-clicking on a highlighted concept like 
?keyboard? opens a detailed view of that concept 
in the KB View.  Inside the KB View, the user can 
click on the superclass of the keyboard concept to 
see the concept computer input device and all of its 
children.  Next, SconeEdit selectively highlights all 
instances of computer input device in the text.  The 
system uses type inference from the KB to high-
light ?mouse?, ?touchpad?, and ?wireless key-
board.?  If ?scanner? appears in the text but isn?t 
included in the knowledge base, the user can spot 
the omission quickly.   
 
 
Figure 1.  The SconeEdit Interface  
 
284
In this way, domain text is used as a measuring 
tool for coverage of domain knowledge.  Our dem-
onstration allows the user to try SconeEdit and to 
explore the interaction of text and knowledge. 
2 The Knowledge Base 
SconeEdit is a software client to the Scone Knowl-
edge Base System, or simply ?Scone? (Fahlman, 
2005).  Scone is an efficient, open-source knowl-
edge base (KB) system being developed in the 
Language Technologies Institute of Carnegie Mel-
lon University.  Scone is intended to be a practical 
KB system that can be used as a component in a 
wide range of AI and natural language software 
applications.  One of the goals in developing Scone 
is to make it easy to use, especially when adding 
new knowledge. 
The SconeEdit interface makes Scone more us-
able in several ways: the Text View display gives 
the user a convenient and intuitive starting point 
for exploring the knowledge base.  SconeEdit also 
provides an easy way of adding knowledge to the 
KB without learning the formal input language for 
Scone.  This demonstration focuses on the effec-
tiveness of SconeEdit and Scone together, but the 
design principles of SconeEdit are applicable to 
knowledge bases written in other formalisms. 
Figure 1 shows the SconeEdit window with a 
document and KB loaded.  The left side of the in-
terface contains the Text View, and the KB View 
is on the right.  Each of these views is described in 
detail below. 
3 Architecture 
3.1 Text View 
In a traditional ontology browser, the user starts 
looking for concepts of interest by typing words 
and phrases into a search field.  This is the model 
for several existing tools, including the VisDic 
viewer for WordNet (Hor?k and Smr?, 2004), the 
INOH ontology viewer (INOH, 2004), and the 
Gene Ontology viewer presented by Koike and 
Takagi (2004), among others. 
SconeEdit improves on this browsing paradigm 
by giving a user who is unfamiliar with the knowl-
edge base an easy way to start exploring.  Rather 
than generating a series of guesses at what may be 
 
Figure 2.  Excerpt from Text View, with Search 
and Text Tabs 
 
covered by the KB, the user can load natural lan-
guage text into SconeEdit from a file or the system 
clipboard.  We take an article from Xinhuanet 
News Service (Xinhuanet, 2006) as an example.  
Figure 2 shows an excerpt of this text after it has 
been loaded. 
When the text file is loaded, it appears in the 
Text Tab of the Text View pane.  SconeEdit high-
lights all strings that it can identify as concepts 
from the knowledge base. In this example, ?Wash-
ington? is correctly identified as the city, not the 
state.  In many cases the concept may be ambigu-
ous from the string alone.  SconeEdit currently 
uses dynamic programming to highlight the long-
est-matching concept names it can find (see Sec-
tion 5).  More sophisticated disambiguation is a 
priority for our future work.   
The result of highlighting is a concise visual 
representation of what is ?known? about that text.  
The Text View helps a user find relevant knowl-
edge quickly, even in a large general-domain KB.  
Clicking on any highlighted term in the Text View 
brings up a hierarchical representation of that con-
cept in the KB View.  
3.2 KB View 
The KB View contains two tabs: a Graph Tab and 
a List Tab. The Graph Tab displays an excerpt 
from the knowledge base as a network of linked 
concepts with one focus concept in the center.  
When the user clicks on a highlighted concept in 
the Text View, a graph focused on that concept 
appears in the Graph Tab.  Continuing with our 
Xinhuanet example, Figure 3 shows the Graph Tab 
after a user has clicked on ?Washington? in the 
text.  The Graph View now displays concepts that 
are closely related to Washington-Dc in the knowl-
edge base.   
285
  
 
 
 
 
 
 
 
 
 
 
Figure 3.  KB View, Graph Tab of Washington-Dc 
 
 
Figure 4.  KB View, List Tab of City 
 
Clicking on any of these related concepts in the 
Graph Tab moves the focus of the graph to that 
concept. 
The List Tab shows an alternative view of the 
same focus concept.  It displays KB information as 
a set of property lists.  As in the Graph Tab, the 
user can double-click on any concept in the List 
Tab to bring that concept into focus.  When the 
focus concept is densely connected to other con-
cepts in the KB, the List Tab can be easier to inter-
pret than the Graph Tab.  In general, research has 
shown that preference for the list style or graph 
style is personal and varies from user to user 
(Tribble and Ros?, 2006). Figure 4 shows the List 
Tab, focused on the concept City.  
4 Adding Knowledge 
Browsing the knowledge base in this way gives the 
user a detailed, domain-targeted view of its con-
tents.  A natural extension of this paradigm is to 
allow the user to edit the KB while browsing.  For 
example, a user may encounter a concept in the  
 
 
 
 
 
 
 
 
 
 
 
 
Figure 5.  Adding a concept synonym 
 
text that is not present in the knowledge base. 
SconeEdit allows the user to simply click on a 
word in the text to create a new concept in the KB 
(see Figure 5).  To specify where the new concept 
belongs, the user navigates to the appropriate loca-
tion in the KB View (List Tab or Graph Tab). 
The user can also modify an existing KB con-
cept by adding English synonyms.  For example, 
the word ?United States? may be highlighted in a 
text example, while ?U.S.? is not.  To add a syno-
nym for the ?United States? concept, the user 
navigates to this concept in the KB View, and then 
clicks on the text ?U.S.?.  A menu offers the choice 
of adding a synonym to the existing focus concept.  
Figure 5 illustrates this process.   
5 Identifying KB Concepts in Text 
Elements in a Scone knowledge base represent 
specific concepts, rather than words or word 
senses.  Each concept is linked with a list of Eng-
lish names (words or phrases). This association 
between Scone elements and English names is 
many-to-many.  
To map a sentence to the set of concepts that 
appear there, a dynamic-programming alignment is 
performed using the English names in the KB as a 
dictionary.  SconeEdit searches for an alignment 
that covers as much of the input text as possible.  
The result of aligning an input string with concepts 
is a set of triples, each consisting of a concept, an 
offset, and a length.  These triples are used directly 
by the Text Tab to highlight substrings and associ-
ate them with KB concepts. 
Consider the sentence ?Washington, D.C. is a 
city.?  Table 1 shows some example Scone con-
cepts and their English names.  Given a knowledge  
286
Concept Name English Names 
Washington-State ?Washington?, ?Washing-ton State?,  
Washington-Dc ?Washington?, ?Washing-ton, D.C.? 
City ?city? 
Table 1.  Example concepts and their English 
Name lists 
 
base with these concepts, SconeEdit returns the 
alignment: (concept: Washington-DC, offset: 1, 
length: 16) (concept: City, offset: 23, length: 4). 
6 Planned Features 
A single node in the KB could have hundreds or 
thousands of outgoing links.  For readability, the 
browser must select a subset of these links to dis-
play to the user.  We plan to leverage Scone?s rea-
soning ability, along with SconeEdit?s document-
driven design, to select which nodes are likely to 
be relevant to the user in the context of the loaded 
document(s).  For example, a user who views sub-
classes of disease in a medical ontology may be 
presented with thousands of disease types.  If the 
current document loaded into SconeEdit is a 
document about food, Scone may be able to prune 
the subclasses it lists to only food-borne illnesses.  
Another feature we hope to add is better integra-
tion with an entire corpus. The current system al-
lows the user to work with individual documents.  
This could be extended to allow a user to navigate 
to a particular concept in the knowledge base and 
retrieve all documents in a corpus containing that 
concept (in its various forms).  These documents 
could then be used to generate more KB concepts 
of interest. 
7 Related Work 
To the best of our knowledge, existing ontology 
and KB editors and viewers do not specifically 
focus on editing and viewing an ontology or KB in 
the context of natural language text.  Other ontol-
ogy editors such as Prot?g? (Gennari, 2002) and 
OntoEdit (Sure, 2002) offer many features for gen-
erating complex ontologies, but do not provide the 
rich interaction with domain text that is the focus 
of SconeEdit.  The CNet Big Picture (CNet News 
Online, 2000) is one example of a system that does 
link ontology knowledge to text, but the concepts 
in the ontology are limited to a small fixed set. 
Acknowledgements 
This material is based upon work supported by the 
Defense Advanced Research Projects Agency 
(DARPA) under Contract No. NBCHD030010.  
The authors would like to thank Vasco Pedro, Eric 
Nyberg, and Tim Isganitis for their contributions to 
SconeEdit.  
References 
CNet News Online. 2000. The Big Picture, 
http://news.com.com/The+Big+Picture/2030-12_3-
5843390.html. 
Scott E. Fahlman. 2006. Scone User's Manual,  
http://www.cs.cmu.edu/~sef/scone/. 
J. Gennari, M. A. Musen, R. W. Fergerson, W. E. 
Grosso, M. Crubezy, H. Eriksson, N. F. Noy, S. W. 
Tu. 2002. The Evolution of Prot?g?: An Environment 
for Knowledge-Based Systems Development. Inter-
national Journal of Human-Computer Interaction, 
58(1), pp. 89?123. 
Ale? Hor?k and Pavel Smr?. 2004. VisDic -- WordNet 
Browsing and Editing Tool. Proceedings of GWC 
2004, pp. 136?141. 
INOH, 2004. INOH Ontology Viewer Website.  
http://www.inoh.org:8083/ontology-viewer/. 
Asako Koike and Toshishisa Takagi, 2004. 
Gene/protein/family name recognition in biomedical 
literature.  In Proceedings of  BioLINK 2004: Linking 
Biological Literature, Ontologies, and Databases, 
pp. 9-16. 
Alicia Tribble and Carolyn Ros?.  2006. Usable Brows-
ers for Ontological Knowledge Acquisition.  To ap-
pear in Proceedings of CHI-2006.  Montr?al, Canada. 
April 22-27, 2006. 
Xinhuanet. 2006. US accused of blocking approval of 
new UN human rights body. 
http://news.xinhuanet.com/english/2006-
03/02/content_4247159.htm. 
 Y. Sure, M. Erdmann, J. Angele, S. Staab, R. Studer 
and D. Wenke. OntoEdit: Collaborative Ontology 
Engineering for the Semantic Web. In Proceedings of 
the first International Semantic Web Conference 
2002 (ISWC 2002). 
287
Proceedings of the NAACL-HLT 2007 Doctoral Consortium, pages 13?16,
Rochester, April 2007. c?2007 Association for Computational Linguistics
Knowledge-Based Labeling of Semantic Relationships in English 
 
 
Alicia Tribble 
Language Technologies Institute 
Carnegie Mellon University 
Pittsburgh, PA, USA 
atribble@cs.cmu.edu 
 
 
 
 
Abstract 
An increasing number of NLP tasks re-
quire semantic labels to be assigned, not 
only to entities that appear in textual ele-
ments, but to the relationships between 
those entities.  Interest is growing in shal-
low semantic role labeling as well as in 
deep semantic distance metrics grounded 
in ontologies, as each of these contributes 
to better understanding and organization 
of text.  In this work I apply knowledge-
based techniques to identify and explore 
deep semantic relationships in several 
styles of English text: nominal com-
pounds, full sentences in the domain of 
knowledge acquisition, and phrase-level 
labels for images in a collection.  I also 
present work on a graphical tool for ex-
ploring the relationship between domain 
text and deep domain knowledge.  
1 Introduction 
As our command of NLP techniques has grown 
over the decades, the tasks which we can accom-
plish have become more useful and complex: we 
can (to an increasing extent) answer questions, cre-
ate summaries, and even create new knowledge by 
extracting and merging facts from large text cor-
pora.  To make our systems reach their potential on 
these tasks, we need to extend our analysis of text 
into deep semantics, often grounded in world 
knowledge.  
 
  
 
 
  In this work, I explore the semantic relationships 
in several styles of English text using knowledge-
driven NLP techniques as well as a novel graphical 
tool for the navigation of knowledge bases (KBs, 
or ontologies).  
  I begin by describing a system based on aug-
mented LFG-style grammar rules, appropriate for 
the domain-limited sentences that are required for 
knowledge entry by knowledge base engineers.  In 
a subsequent system for interpreting nominal com-
pounds, I rely more heavily on the knowledge al-
ready stored in the knowledge base to guide a 
heuristic search for meaning (Tribble and Fahlman, 
2006).    
  These systems demonstrate how a knowledge 
base can contribute to NLP performance.  During 
development of the systems, knowledge acquisi-
tion and organization became important sub-topics.  
In response I began work on a graphical tool, 
SconeEdit (Tribble, Lambert, and Fahlman, 2006).  
SconeEdit allows users to navigate the semantic 
concepts and relations in a text corpus, guided by 
the rich, grounded features of these concepts in a 
knowledge base.   
  With this interface as a scaffold, future work en-
tails improving the analysis systems for noun com-
pounds and full sentences, and incorporating these 
systems in a comparative evaluation of the graphi-
cal and NLP-based methods for exploring semantic 
relationships in domain-restricted text.  In addition, 
I will use this framework to evaluate a knowledge-
13
based approach for the task of retrieving labeled 
images from a collection.    
2 Semantic Analysis for Knowledge Engi-
neering 
One of the motivating goals of this work is to lev-
erage the power of NLP tools to ease the burden of 
knowledge engineers who develop ontological re-
sources.  By converting English sentences into a 
semantic representation automatically, a system 
provides an intuitive input method for adding new 
knowledge. 
2.1     Knowledge Engineering in Scone 
The context for this work is the Scone Knowledge 
Representation (KR) Project (Fahlman, 2005).  The 
Scone KR System encompasses an inference en-
gine along with a set of upper-level domain on-
tologies.  As with other large KR systems along the 
lines of CYC (Lenat, 1986), knowledge engineers 
create much of the upper-level KB content by 
hand.    
To develop a system that would address the 
needs of these engineers, I collected a corpus of 
English sentences covering the six core structure-
building tasks in Scone: 
 
? Defining a type 
? Adding an instance of a type 
? Defining a relation between types 
? Adding an instance of a relation 
? Defining a new role (HAS-A) relation 
? Instantiating a role-filling relation     
  
2.2 A Grammar-Based System 
The resulting corpus displayed a high degree of 
semantic cohesion, as expected, but with a wide 
degree of syntactic variation.  To transform these 
sentences automatically into the Scone KR, I 
developed a set of semantic interpretation 
functions and added them as callouts in an existing 
LFG-style syntactic grammar.  The resulting 
augmented English grammar is applied to new 
sentences using the LCFlex parser of Ros? and 
Lavie (2000).  In this way, every parse constituent 
can be conditioned on queries to the knowledge 
base, allowing not only flat semantic features (e.g. 
?is the noun animate??) but rich structural 
knowledge (?does this person own a pet??) to be 
applied during the parse. 
The new grammar rules produce output in the 
Scone KR formalism.  As a result, the output can 
be read as the knowledge-grounded meaning of an 
input sentence, and it can also become additional 
input to the Scone inference engine, adding to the 
store of background knowledge or making a new 
query.  However, the appeal of this design is 
limited by the fact that, as in many grammar-based 
systems, the rules themselves are costly to write 
and maintain. 
2.3 Adding Generalization 
For this reason,  I modified the approach and 
examined the effectiveness of a few general 
?preference? rules, based on syntax.  In contrast 
with the grammar system, the search for 
interpretations can now be driven, rather than 
pruned, by domain knowledge.  I tested this 
approach on the interpretation of noun compounds, 
where the lack of syntactic cues requires heavy 
reliance on semantic interpretation  (Tribble and 
Fahlman, 2006).   I found that a majority of 
compounds, even in a new textual domain, could 
be analyzed correctly using the new set of rules 
along with an appropriate domain-specific KB. 
3 A Graphical Tool for Exploring 
Semantic Relationships 
While the cost of grammar writing can be reduced 
with updated algorithms, developing and 
maintaining large knowledge repositories is one of 
the key challenges in knowledge-based NLP: the 
knowledge acquisition ?bottleneck?.  My 
hypothesis is that a natural-language (NL) interface 
is an important tool for easily modifying and 
adding knowledge in a complex KR system like 
Scone; language is an intuitive way for users to 
express what they want from the knowledge base.     
In the course of developing NL tools for the 
Scone Project, I also recognized the need to view 
domain text, domain knowledge, and the semantic 
relationships that they share in a ?snapshot?.  Inte-
grating textual and graphical exploration gives us-
ers a comfortable handle on the knowledge base, 
even when they don?t know exactly what they 
want.    
14
  I designed the SconeEdit knowledge- and text-
browsing tool (Tribble, et al 2006) in response to 
this need.  The tool provides an annotated view of 
text chosen by the user, allowing him to see what 
concepts and vocabulary from the text are 
currently in the KB.  Alongside this Text View, 
SconeEdit provides a navigable snapshot of the 
knowledge base (KB View), centered on concepts 
that appear in the text.  This unified browser 
establishes a principled, coverage-driven way to 
?surf? the KB and add new knowledge.  A 
screenshot of SconeEdit, recently updated to view 
images as well as text, is shown in Figure 1.  
The SconeEdit tool has already been used by 
groups outside the Scone Project, for the purpose 
of qualitatively evaluating knowledge bases for use 
in  new subdomains.  My goal for the conclusion 
of this work is to synergize the lines of research 
described so far, building our English analysis 
tools into the SconeEdit interface.  With the 
resulting tool I can run a detailed evaluation  of my 
English analyzers, as well as shed light on the 
usability of text-based versus graphical knowledge 
entry. 
 
 
 
Figure 1.  Screenshot of SconeEdit, updated to display 
images as well as text. 
 
4 Task-Based Evaluation: Retrieving 
Labeled Images 
To bring this work to bear in a task-based 
evaluation, I have also started developing a system 
for labeled image retrieval.  To retrieve images of 
interest from large collections, traditional systems 
rely on matching between a high-level query and 
low-level image features, or on matching the query 
with an unordered bag-of-words that has been at-
tached to each image.  In current work I am inves-
tigating sentence fragments, which retain some 
syntactic structure, as a useful style of image anno-
tation that is complementary to the current bag-of-
words style.  Analysis of 2,776 image titles 
downloaded from the web establishes that frag-
ment-style labels are intuitive, discriminative, and 
useful. 
These labels can be used to retrieve images from 
a collection in the following way: first, a typed 
query is given to the system (e.g. ?people petting 
their dogs?).  An English analyzer, using im-
provements to the techniques described in Section 
2, produces the Scone semantic representation of 
this query (a semantic graph).  Next, the Scone 
inference engine is used to match the query against 
pre-computed semantic representations of the im-
age labels.  The system retrieves the image whose 
label matches best.  Figure 2 is an example re-
trieved for this query by Google Image Search.     
 
 
 
Figure 2.  Image retrieved by Google Image Search for 
?people petting their dogs?. 
 
4.1 Development Data 
In order to train the functions that measure a 
?match? in the knowledge base, as well as to im-
prove the English-to-Scone analysis, I need train-
ing data in the form of images, their fragment-style 
labels, and one or more query that matches each 
image and its label.  
I collected one corpus of images with their 
fragment-style labels from the publicly available 
collection on Flickr (http://www.flickr.com).  A 
second corpus of fragment-labeled images has 
been provided by one the authors of von Ahn and 
Dabbish (2004).  In many cases, a single image has 
15
multiple fragment-style labels.  To convert this 
data into the format I need, I can use the redundant 
labels as substitute ?queries?, under the assumption 
(which should be validated experimentally) that 
image-retrieval queries often take the form of sen-
tence fragments, as well.    
An evaluation that uses these labels for image 
retrieval will proceed as follows: A subset of the 
labeled images which were not seen or used in 
previous work will be reserved as test data.  Re-
maining images with their labels and queries will 
be used to improve the English-to-Scone analysis 
system and the semantic similarity functions within 
Scone.  Finally, the queries for the test set will be 
submitted to the retrieval system, and system re-
sults will be compared to the ?correct? images 
given by the test set.  Precision and recall can be 
calculated under a variety of conditions, including 
one-image-per-query and several-images-per-
query.  Comparison to shallow techniques for label 
matching, as used with bag-of-words style labels, 
will also be a feature of this evaluation. 
5 Conclusion 
In summary, I have presented a body of work on 
exploring and labeling the deep semantic relation-
ships in English text.  A grammar-based system for 
sentences and a heuristic search system for noun 
compounds explore the role of domain knowledge 
in tools for syntactic and deep semantic analysis.  
In addition, I designed and demonstrated graphical 
tool for exploring rich semantic features in text, 
grounded in a knowledge base or ontology.  The 
tool has been used by our own knowledge engi-
neers as well by other research teams at CMU. 
I will build on this work in the coming months 
as I prepare for two evaluations: a study on the 
usability of natural language and graphical tools 
for navigating a knowledge base, and a task-based 
evaluation on labeled image retrieval.  These 
evaluations should bring closure to the work as a 
contribution in the field of semantic analysis of 
text.   
References  
 
Scott E. Fahlman. 2005.  The Scone User?s Manual.  
http://www.cs.cmu.edu/~sef/scone. 
 
Alicia Tribble, Benjamin Lambert and Scott E. 
Fahlman. 2006. SconeEdit: A Text-Guided Domain 
Knowledge Editor. In Demonstrations of HLT-
NAACL 2006.  New York. 
 
Alicia Tribble and Scott E. Fahlman. 2006. Resolving 
Noun Compounds with Multi-Use Domain Knowl-
edge. In Proceedings of FLAIRS-2006. Melbourne 
Beach, Florida. 
 
Alicia Tribble and Carolyn P. Ros?. 2006. Usable 
Browsers for Knowledge Acquisition. In Proceed-
ings of CHI-2006. Montreal, Quebec. 
 
Carolyn P. Ros? and Alon Lavie. 2001. Balancing Ro-
bustness and Efficiency in Unification-Augmented 
Context-Free Parsers for Large Practical Applica-
tions. In J.C. Junqua and G. Van Noord, eds. Robust-
ness in Language and Speech Technology. Kluwer 
Academic Press. 
 
D. B. Lenat, M. Prakash and M. Shepherd. 1986. Cyc: 
using common sense knowledge to overcome brittle-
ness and knowledge acquisition bottlenecks.. In AI 
Magazine. 6:4. 
 
Luis von Ahn and Laura Dabbish. 2004. Labeling im-
ages with a computer game. In Proceedings of ACM 
CHI (pp 319?326). 
16
IMPROVEMENTS IN NON-VERBAL CUE IDENTIFICATION USING MULTILINGUAL
PHONE STRINGS
Tanja Schultz, Qin Jin, Kornel Laskowski, Alicia Tribble, Alex Waibel
Interactive Systems Laboratories
Carnegie Mellon University
E-mail:
 
tanja,qjin,kornel,atribble,ahw  @cs.cmu.edu
1. INTRODUCTION
Today?s state-of-the-art front-ends for multilingual speech-
to-speech translation systems apply monolingual speech
recognizers trained for a single language and/or accent.
The monolingual speech engine is usually adaptable to an
unknown speaker over time using unsupervised training
methods; however, if the speaker was seen during training,
their specialized acoustic model will be applied, since it
achieves better performance. In order to make full use of
specialized acoustic models in this proposed scenario, it is
necessary to automatically identify the speaker with high
accuracy. Furthermore, monolingual speech recognizers
currently rely on the fact that language and/or accent will
be selected beforehand by the user. This requires the user?s
cooperation and an interface which easily allows for such
selection. Both requirements are awkward and error-prone,
especially when translation services are provided for many
languages using small devices like PDAs or telephones. For
these scenarios, front-ends are desired which automatically
identify the spoken language or accent. We believe that
the automatic identification of an utterance?s non-verbal
cues, such as language, accent and speaker, are necessary to
the successful deployment of speech-to-speech translation
systems.
Currently, approaches based on Gaussian Mixture Models
(GMMs) [1] are the most widely and successfully used
methods for speaker identification. Although GMMs have
been applied successfully to close-speaking microphone
scenarios under matched training and testing conditions,
their performance degrades dramatically under mismatched
conditions. For language and accent identification, phone
recognition together with phone N-gram modeling has been
the most successful approach in the past [2]. More recently,
Kohler introduced an approach for speaker recognition
where a phonotactic N-gram model is used [3].
In [4], we extended Kohler?s approach to accent and lan-
guage identification as well as to speaker identification un-
der mismatched conditions. The term ?mismatched condi-
tion? describes a situation in which the testing conditions,
e.g. microphone distance, are quite different from what had
been seen during training. In that work, we explored a com-
mon framework for the identification of language, accent
and speaker using multilingual phone strings produced by
phone recognizers trained on data from different languages.
In this paper, we propose and evaluate some improvements,
comparing classification accuracy as well as realtime per-
formance in our framework. Furthermore, we investigate
the benefits that are to be drawn from additional phone rec-
ognizers.
2. THE MULTILINGUAL PHONE STRING
APPROACH
The basic idea of the multilingual phone string approach
is to use phone strings produced by different context-
independent phone recognizers instead of traditional
short-term acoustic vectors [6]. For the classification of an
audio segment into one of  classes of a specific non-verbal
cue,  such phone recognizers together with 
phonotactic N-gram models produce an  matrix of
features. A best class estimate is made based solely on this
feature matrix. The process relies on the availability of
 phone recognizers, and the training of  N-gram
models on their output.
By using information derived from phonotactics rather than
directly from acoustics, we expect to cover speaker idiosyn-
crasy and accent-specific pronunciations. Since this infor-
mation is provided from complementary phone recognizers,
we anticipate greater robustness under mismatched condi-
tions. Furthermore, the approach is somewhat language in-
dependent since the recognizers are trained on data from
different languages.
2.1. Phone Recognition
The experiments presented here were conducted using
two versions of phone recognizers borrowed without
modification from the GlobalPhone project [5]. All were
                                            Association for Computational Linguistics.
                         Algorithms and Systems, Philadelphia, July 2002, pp. 101-108.
                          Proceedings of the Workshop on Speech-to-Speech Translation:
trained using our Janus Recognition Toolkit (JRTk).
25
30
35
40
45
50
25 30 35 40 45
Ph
on
em
e 
Er
ro
r R
at
e 
[%
]
Number of Phonemes
CHDE
FR
JA
KR
PO
SPTU
50
Fig. 1. Error rate vs number of phones for the baseline
GlobalPhone phone recognizer set
The first set of phone recognizers, which we refer to as
our baseline, includes recognizers for: Mandarin Chinese
(CH), German (DE), French (FR), Japanese (JA), Croatian
(KR), Portuguese (PO), Spanish (SP) and Turkish (TU).
For each language, the acoustic model consists of a context-
independent 3-state HMM system with 128 Gaussians per
state. The Gaussians are on 13 Mel-scale cepstral coeffi-
cients with first and second order derivatives and power.
Following cepstral mean subtraction, linear discriminant
analysis reduces the input vector to 32 dimensions.
The second set consists of extended phone recognizers,
available in 12 languages. Arabic (AR), Korean (KO),
Russian (RU) and Swedish (SW) are available in this set
in addition to the languages named above for the baseline
set. The 12 new phone recognizers were derived from
an improved generation of context dependent LVCSR
systems which also include vocal tract normalization
(VTLN) for speaker normalization. For decoding, we
used an unsupervised scheme to find the best warp fac-
tor for a test speaker and calculate a viterbi alignment
based on that speaker?s best warp factor. To improve
system speed, we reduced the number of Gaussians per
state from 128 to 16; in addition, the feature dimension
was halved from 32 to 16 using linear discriminant analysis.
Figure 1 shows the phone error rates in relation to the num-
ber of modeled phones for eight languages. The error rate
correlates with the number of phones used to model this lan-
guage. Turkish seems to be an exception to this finding. The
error analysis showed that this is due to a very high substi-
audio
phone string
phone string
	

	

 
	 
  
  
  

Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 121?124,
Prague, June 2007. c?2007 Association for Computational Linguistics
CMU-AT: Semantic Distance and Background Knowledge for Identify-
ing Semantic Relations 
Alicia Tribble 
Language Technologies Institute 
Carnegie Mellon University 
Pittsburgh, PA, USA 
atribble@cs.cmu.edu 
Scott E. Fahlman 
Language Technologies Institute 
Carnegie Mellon University 
Pittsburgh, PA, USA 
sef@cs.cmu.edu 
 
 
Abstract 
This system uses a background knowledge 
base to identify semantic relations between 
base noun phrases in English text, as eva-
luated in SemEval 2007, Task 4.  Training 
data for each relation is converted to state-
ments in the Scone Knowledge Representa-
tion Language.  At testing time a new 
Scone statement is created for the sentence 
under scrutiny, and presence or absence of 
a relation is calculated by comparing the 
total semantic distance between the new 
statement and all positive examples to the 
total distance between the new statement 
and all negative examples. 
   
1 Introduction 
This paper introduces a knowledge-based approach 
to the task of semantic relation classification, as 
evaluated in SemEval 2007, Task 4: ?Classifying 
Relations Between Nominals?.  In Task 4, a full 
sentence is presented to the system, along with the 
WordNet sense keys for two noun phrases which 
appear there and the name of a semantic relation 
(e.g. ?cause-effect?).  The system should return 
?true? if a person reading the sentence would con-
clude that the relation holds between the two la-
beled noun phrases. 
Our system represents a test sentence with a se-
mantic graph, including the relation being tested 
and both of its proposed arguments.  Semantic dis-
tance is calculated between this graph and a set of 
graphs representing the training examples relevant 
to the test sentence.  A near-match between a test 
sentence and a positive training example is evi-
dence that the same relation which holds in the 
example also holds in the test.  We compute se-
mantic distances to negative training examples as 
well, comparing the total positive and negative 
scores in order to decide whether a relation is true 
or false in the test sentence. 
2 Motivation 
Many systems which perform well on related tasks 
use syntactic features of the input sentence, 
coupled with classification by machine learning.  
This approach has been applied to problems like 
compound noun interpretation (Rosario and Hearst 
2001) and semantic role labeling (Gildea and Ju-
rafsky 2002). 
In preparing our system for Task 4, we started 
by applying a similar syntax-based feature analysis 
to the trial data: 140 labeled examples of the rela-
tion ?content-container?.  In 10-fold cross-
validation  with this data we achieved an average f-
score of 70.6, based on features similar to the sub-
set trees used for semantic role labeling in (Mo-
schitti 2004). For classification we applied the up-
dated tree-kernel package (Moschitti 2006), distri-
buted with the svm-light tool (Joachims 1999) for 
learning Support Vector Machines (SVMs). 
Training data for Task 4 is small, compared to 
other tasks where machine learning is commonly 
applied.  We had difficulty finding a combination 
of features which gave good performance in cross-
validation, but which did not result in a separate 
support vector being stored for every training sen-
tence ? a possible indicator of overfitting.  As an 
example, the ratio of support vectors to training 
121
examples for the experiment described above was 
.97, nearly 1-to-1.  
  As a result of this analysis we started work on 
our knowledge-based system, with the goal of us-
ing the two approaches together.  We were also 
motivated by an interest in using relation defini-
tions and background knowledge from WordNet to 
greater advantage.  The algorithm we used in our 
final submission is similar to recent systems which 
discover textual entailment relationships (Haghig-
hi, Ng et al 2005; Zanzotto and Moschitti 2006).  
It gives us a way to encode information from the 
relation definitions directly, in the form of state-
ments in a knowledge representation language.  
The inference rules that are learned by this system 
from training examples are also easier to interpret 
than the models generated by an SVM.  In small-
data applications this can be an advantage.  
3 System Description: A Walk-Through 
The example sentence below is taken (in abbre-
viated form) from the training data for Task 4, Re-
lation 7 ?Content-Container? (Girju, Hearst et al 
2007): 
 
The kitchen holds a cooker. 
 
We convert this positive example into a semantic 
graph by creating a new instance of the relation 
Contains and linking that instance to the WordNet 
term for each labeled argument ("kitch-
en%1:06:00::", "cooker%1:06:00::").  The result is 
shown in Figure 1.  WordNet sense keys (Fellbaum 
1998) have been mapped to a term, a part of 
speech (pos), and a sense number. 
Contains
{relation}
kitchen_n_1
cont iner content
cooker_n_1
 
Figure 1.  Semantic graph for the training example 
"The kitchen holds a cooker".   Arguments are 
represented by a WordNet term, part of speech, 
and sense number. 
 
This graph is instantiated as a statement using 
the Scone Knowledge Representation System, or  
(new-statement {kitchen_n_1} {contains} {cooker_n_1}) 
(new-statement {artifact_n_1} {contains} {artifact_n_1}) 
(new-statement  {whole_n_1}   {contains}  {whole_n_1}) 
Figure 2.  Statements in Scone KR syntax, based 
on generalizing the training example "The kitchen 
holds a cooker". 
 
?Scone? (Fahlman 2005).  Scone gives us a way to 
store, search, and perform inference on graphs like 
the one shown above.  After instantiating the graph 
we generalize it using hypernym information from 
WordNet.  This generates additional Scone state-
ments which are stored in a knowledge base (KB), 
shown in Figure 2.  The first statement in the fig-
ure was generated verbatim from our training sen-
tence.  The remaining statements contain hyper-
nyms of the original arguments. 
For each argument seen in training, we also ex-
tract hypernyms and siblings from WordNet.  For 
the argument kitchen, we extract 101 ancestors 
(artifact, whole, object, etc.) and siblings (struc-
ture, excavation, facility, etc.).  A similar set of 
WordNet entities is extracted for the argument 
cooker.  These entities, with repetitions removed, 
are encoded in a second Scone knowledge base, 
preserving the hierarchical (IS-A) links that come 
from WordNet.  The hierarchy is manually linked 
at the top level into an existing background Scone 
KB where entities like animate, inanimate, person, 
location, and quantity are already defined.   
After using the training data to create these two 
KBs, the system is  ready for a test sentence.  The 
following example is also adapted from SemEval 
Task 4 training data: 
 
     Equipment was carried in a box. 
 
First we convert the sentence to a semantic 
graph, using the same technique as the one de-
scribed above.  The graph is implemented as a new 
Scone statement which includes the WordNet pos 
and sense number for each of the arguments: 
?box_n_1 contains equipment_n_1?. 
Next, using inference operations in Scone, the 
system verifies that the statement conforms to 
high-level constraints imposed by the relation defi-
nition.  If it does, we calculate semantic distances 
between the argument nodes of our test statement 
and the analogous nodes in relevant training state-
ments.  A training statement is relevant if both of 
its arguments are ancestors of the appropriate ar-
122
guments of the test sentence.  In our example, only 
two of the three KB statements from Figure 2 are 
relevant to the test statement ?box contains equip-
ment?: ?whole contains whole? and ?artifact con-
tains artifact?.  The first statement, ?kitchen con-
tains cooker? fails to apply because kitchen is not 
an ancestor of box, and also because cooker is not 
an ancestor of equipment.   
Figure 3 illustrates the distance from ?box con-
tains equipment? to ?whole contains whole?, calcu-
lated as the sum of the distances between box-
whole and equipment-whole.  
Contains
{relation}
box equipment
container content
rtifact artifact
Contains
{relation}
whole whole
container content
Distance = 2
Support = 1/2
Distance = 2
Support = 1/2
 
Figure 3.  Calculating the distance through the 
knowledge base between "equipment contains box" 
and ?whole contains whole?.  Dashed lines indicate 
IS-A links in the knowledge base.   
 
The total number of these relevant, positive 
training statements is an indicator of ?support? for 
the test sentence throughout the training data.  The 
distance between one such statement and the test 
sentence is a measure of the strength of support.  
To reach a verdict, we sum over the inverse dis-
tances to all arguments from positive relevant ex-
amples: in Figure 3, the test statement ?box con-
tains equipment? receives a support score of  (?  + 
? + 1 + 1), or 3.      
Counter-evidence for a test sentence can be cal-
culated in the same way, using relevant negative 
statements.  In our example there are no negative 
training statements, so the total positive support 
score (3) is greater than the counter-evidence score 
(0), and the system verdict is ?true?. 
4 System Components in Detail 
As the detailed example above shows, this system 
is designed around its knowledge bases. The KBs 
provide a consistent framework for representing 
knowledge from a variety of sources as well as for 
calculating semantic distance. 
4.1 Background knowledge 
WordNet-extracted knowledge bases of the type 
described in Section 3 are generated separately for 
each relation.  Average depth of these hierarchies 
is 4; we store only hypernyms of WordNet depth 7 
and above, based on experiments in the literature 
by Nastase, et al (2003; 2006).  
Relation-specific and task-specific knowledge is 
encoded by hand.  For each relation, we examine 
the relation definition and create a set of con-
straints in Scone formalism.  For example, the de-
finition of ?container-contains? includes the fol-
lowing restriction (taken from training data for 
Task 4): There is strong preference against treat-
ing legal entities (people and institutions) as con-
tent. 
In Scone, we encode this preference as a type 
restriction on the container role of any Contains 
relation: (new-is-not-a {container} {potential 
agent}) 
During testing, before calculating semantic dis-
tances, the system checks whether the test state-
ment conforms to all such constraints. 
4.2 Calculating semantic distance 
Semantic distances are calculated between con-
cepts in the knowledge base, rather than through 
WordNet directly.  Distance between two KB en-
tites is calculated by counting the edges along the 
shortest path between them, as illustrated in Figure 
3.  In the current implementation, only ancestors in 
the IS-A hierarchy are considered relevant, so this 
calculation amounts to counting the number of an-
cestors between an argument from the test sentence 
and an argument from a training example.  Quick 
type-checking features which are built into Scone 
allow us to skip the distance calculation for non-
relevant training examples. 
5 Results & Conclusions 
This system performed reasonably well for relation 
3, Product-Producer, outperforming the baseline 
(baseline guesses ?true? for every test sentence).  
Performance for this relation was also higher than 
the average F-score for all comparable groups in 
Task 4 (all groups in class ?B4?).  Average recall 
for this system over all relations was mid-range, 
123
compared to other participating groups.  Average 
precision and average f-score fell below the base-
line and below the average for all comparable 
groups.  These scores are given in Table 1. 
 
Relation  R P F 
1.  Cause-Effect 73.2 54.5 62.5 
2.  Instrument-Agency 76.3 50.9 61.1 
3.  Product-Producer 79.0 71.0 74.8 
4.  Origin-Entity 63.9 54.8 59.0 
5.  Theme-Tool 48.3 53.8 50.9 
6.  Part-Whole 57.7 45.5 50.8 
7.  Content-Container 68.4 59.1 63.4 
Whole test set, not 
divided by relation 
57.1 68.9 62.4 
Average for CMU-AT 66.7 55.7 60.4 
Average for all B4 
systems 
64.4 65.3 63.6   
Baseline: ?alltrue? 100.0   48.5 64.8   
Table 1.  Recall, Precision, and F-scores, separated 
by relation type.  Baseline score is calculated by 
guessing "true" for all test setences. 
 
Analysis of the training data reveals that relation 
3 is the class where target nouns occur most often 
together in nominal compounds and base NPs, with 
little additional syntax to connect them.  While 
other relations included sentences where the targets 
were covered by a single VP, Product-Producer did 
not.  It seems that background knowledge plays a 
larger role in identifying the Producer-Produces 
relationship than it does for other relations.  How-
ever this conclusion is softened by the fact that we 
also spent more time in development and cross-
evaluation for relations 3 and 7, our two best per-
forming relations. 
This system demonstrates a knowledge-based 
framework  that performs very well for certain re-
lations.  Importantly, the system we submitted for 
evaluation did not make use of syntactic features, 
which are almost certainly relevant to this task.  
We are already exploring methods for combining 
the knowledge-based decision process with one 
that uses syntactic evidence as well as corpus sta-
tistics, described in Section 2. 
Acknowledgement 
This work was supported by a generous research 
grant from Cisco Systems, and by the Defense Ad-
vanced Research Projects Agency (DARPA) under 
contract number NBCHD030010.  
References 
Fahlman, S. E. (2005). Scone User's Manual. 
Fellbaum, C. (1998). WordNet An Electronic Lexical 
Database, Bradford Books. 
Gildea, D. and D. Jurafsky (2002). "Automatic labeling 
of semantic roles." Computational Linguistics 28(3): 
245-288. 
Girju, R., M. Hearst, et al (2007). Classification of Se-
mantic Relations between Nominals: Dataset for 
Task 4. SemEval 2007, 4th International Workshop 
on Semantic Evaluations, Prague, Czech Republic. 
Haghighi, A., A. Ng, et al (2005). Robust Textual Infe-
rence via Graph Matching. Human Language Tech-
nology Conference and Conference on Empirical 
Methods in Natural Language Processing, Vancou-
ver, British Columbia, Canada. 
Joachims, T. (1999). Making large-scale SVM learning 
practical. Advances in Kernel Methods - Support 
Vector Learning. B. Sch?lkopf, C. Burges and A. 
Smola. 
Moschitti, A. (2004). A study on Convolution Kernel 
for Shallow Semantic Parsing. proceedings of the 
42nd Conference of the Association for Computa-
tional   Linguistics (ACL-2004). Barcelona, Spain. 
Moschitti, A. (2006). Making tree kernels practical for 
natural language learning. Eleventh International 
Conference on European Association for Computa-
tional Linguistics, Trento, Italy. 
Nastase, V., J. S. Shirabad, et al (2006). Learning noun-
modifier semantic relations with corpus-based and 
Wordnet-based features. 21st National Conference on 
Artificial Intelligence (AAAI-06), Boston, Massa-
chusetts. 
Nastase, V. and S. Szpakowicz (2003). Exploring noun-
modifier semantic relations. IWCS 2003. 
Rosario, B. and M. Hearst (2001). Classifying the se-
mantic relations in Noun Compounds. 2001 Confe-
rence on Empirical Methods in Natural Language 
Processing. 
Zanzotto, F. M. and A. Moschitti (2006). Automatic 
Learning of Textual Entailments with Cross-Pair Si-
milarities. the 21st International Conference on 
Computational Linguistics and 44th Annual Meeting 
of the Association for Computational Linguistics 
(ACL), Sydney, Austrailia. 
124
