Chinese Named Entity Identification Using Class-based
Language Model1
Jian Sun*, Jianfeng Gao, Lei Zhang**, Ming Zhou, Changning Huang
* Beijing University of Posts & Telecommunications, China, jiansun_china@hotmail.com
#Microsoft Research Asia, {jfgao, mingzhou, cnhuang}@microsoft.com Tsinghua University, China
1 This work was done while the author was visiting Microsoft Research Asia
$EVWUDFW
We consider here the problem of Chinese 
named entity (NE) identification using 
statistical language model(LM). In this 
research, word segmentation and NE 
identification have been integrated into a
unified framework that consists of several 
class-based language models. We also adopt a 
hierarchical structure for one of the LMs so 
that the nested entities in organization names 
can be identified. The evaluation on a large test 
set shows consistent improvements. Our 
experiments further demonstrate the 
improvement after seamlessly integrating with 
linguistic heuristic information, cache-based 
model and NE abbreviation identification.
 ,QWURGXFWLRQ
1(LGHQWLILFDWLRQ is the key technique in many
applications such as information extraction, 
question answering, machine translation and so 
on. English NE identification has achieved a 
great success. However, for Chinese, NE 
identification is very different. There is no 
space to mark the word boundary and no 
standard definition of words in Chinese. The 
Chinese NE identification and word 
segmentation are interactional in nature.
This paper presents a unified approach 
that integrates these two steps together using a
class-based LM, and apply Viterbi search to 
select the global optimal solution. The 
class-based LM consists of two sub-models, 
namely the context model and the entity model. 
The context model estimates the probability of 
generating a NE given a certain context, and
the entity model estimates the probability of a 
sequence of Chinese characters given a certain 
kind of NE. In this study, we are interested in 
three kinds of Chinese NE that are most 
commonly used, namely person name (PER), 
location name (LOC) and organization name 
(ORG). We have also adopted a variety of 
approaches to improving the LM. In addition, a 
hierarchical structure for organization LM is 
employed so that the nested PER, LOC in 
ORG can be identified. 
The evaluation is conducted on a large test 
set in which NEs have been manually tagged. 
The experiment result shows consistent 
improvements over existing methods. Our 
experiments further demonstrate the 
improvement after integrating with linguistic 
heuristic information, cache-based model and 
NE abbreviation identification. The precision
of PER, LOC, ORG on the test set is 79.86%, 
80.88%, 76.63%, respectively; and the recall is
87.29%, 82.46%, 56.54%, respectively. 5HODWHG:RUN
Recently, research on English NE 
identification has been focused on the 
machine-learning approaches, including 
hidden Markov model (HMM), maximum 
entropy model, decision tree and 
transformation-based learning, etc. (Bikel et al 
1997; Borthwick et al 1999; Sekine et al 
1998). Some systems have been applied to real 
application.
Research on Chinese NE identification is,
however, still at its early stage. Some 
researches apply methods of English NE 
identification to Chinese. Yu et al(1997) 
applied the HMM approach where the NE 
identification is formulated as a tagging 
problem using Viterbi algorithm. In general, 
current approaches to NE identification (e.g. 
Chen, 1997) usually contain two separate steps: 
word segmentation and NE identification. The 
word segmentation error will definitely lead to 
errors in the NE identification results. Zhang 
(2001) put forward class-based LM for 
Chinese NE identification. We further develop 
this idea with some new features, which leads 
to a new framework. In this framework, we 
integrate Chinese word segmentation and NE 
identification into a unified framework using a 
class-based language model (LM). &ODVVEDVHG/0 IRU1(,GHQWLILFDWLRQ
The n-gram LM is a stochastic model which 
predicts the next word given the previous n-1
words by estimating the conditional probability 
P(w
n
|w1?wn-1). In practice, trigram 
approximation P(wi|wi-2wi-1) is widely used, 
assuming that the word wi depends only on two 
preceding words wi-2 and wi-1. Brown et al(1992) 
put forward and discussed n-gram models 
based on classes of words. In this section, we 
will describe how to use class-based trigram 
model for NE identification. Each kind of NE 
(including PER, LOC and ORG) is defined as a 
class in the model. In addition, we differentiate 
the transliterated person name (FN) from the 
Chinese person name since they have different 
constitution patterns. The four classes of NE 
used in our model are shown in Table 1. All 
other words are also defined as individual 
classes themselves (i.e. one word as one class). 
Consequently, there are _9_+4 classes in our 
model, where _9_ is the size of vocabulary.7DEOH : Classes defined in class-based model7DJ 'HVFULSWLRQ
PN Chinese person name
FN Transliterated person name
LN Location name
ON Organization name 7KH/DQJXDJH0RGHOLQJ
 )RUPXODWLRQ
Given a Chinese character sequence 6=V?VQ, 
the task of Chinese NE identification is to find 
the optimal class sequence &=F?FP (P<=Q) 
that maximizes the probability 3&_6. It can be 
expressed in the equation (1) and we call it 
class-based model.
The class-based model consists of two 
sub-models: the context model 3& and the 
entity model P (S|C). The context model 
indicates the probability of generating a NE
class given a (previous) context. P(C) is a 
priori probability, which is computed 
according to Equation (2):
?
=
--@
P
L LLL FFF3&3 1 12 )|()( (2)
P(C) can be estimated using a NE labeled 
corpus. The entity model can be parameterized 
by Equation (3):
?
=
--
--
@
@
=
P
M MHQGFVWDUWF
PQVWDUWFHQGF
PQ
FVV3
FFVVVV3 FFVV3&63
MM
P
1
11
11
)|]...([
)...|]...]...[...([
)...|...()|(
1 (3)
The entity model estimates the generative 
probability of the Chinese character sequence in
square bracket pair (i.e. starting from FMVWDUW to FMHQG) given the specific NE class.
For different class, we define the different 
entity model.
For the class of PER (including PN and 
FN), the entity model is a FKDUDFWHUEDVHG
trigram model as shown in Equation (4).
?-
-=
--
--
==
=
HQGF
VWDUWFN MNNN
MHQGFVWDUWF
M
M
MM
3(5FVVV3
3(5FVV3
),,|(
)|]...([
12
(4)
where s can be any characters occurred in a 
person name. For example, the generative 
probability of character sequence "?# (Li 
Dapeng) is much larger than that of ??H
(many years) given the PER since " is a 
commonly used family name, and ? and # are 
commonly used first names. The probabilities
can be estimated with the person name list.
For the class of LOC, the entity model is a ZRUGEDVHG trigram model as shown in
Equation (5).
)|(maxarg* 6&3& &=
)|()(maxarg &63&3& ?= (1)
)|]...([ /2&FVV3 MHQGFVWDUWF MM =--
@/2&FZZ_Z3>PD[
/2&F_ZZ3PD[
O
N MNNN:
MO:
?
=
-- ==
=?
(5)
where W = w1?wl is possible segmentation 
result of  character sequence HQGFVWDUWF MM VV -- ... .
For the class of ORG, the construction is 
much more complicated because an ORG often
contain PER and/or LOC. For example, the 
ORG ????N@? ? (Air China 
Corporation) contains the LOC ??? (China).
It is beneficial to such applications as question 
answering, information extraction and so on if 
nested NE can be identified as well . In order to 
identify the nested PER, LOC in ORG 2, we 
adopted class-based LMs for ORG further, in 
which there are three sub models, one is the 
class generative model, and the others are entity 
model: person name model and location name 
model in ORG. Therefore, the entity model of 
ORG is shown in Equation (6) which is almost 
same as Equation (1).
)|]...([ 25*FVV3 MHQGFVWDUWF MM =--
??
??
?
?
?
??
??
?
?
?
=?
=
@
???
?
???
?
=?
==
=@
?
?
=
--
=
--
--
--
N
L MLHQGFVWDUWF
N
L MLLL&
MNHQGFVWDUWF
MN&
MHQGFVWDUWFM&
25*FFVV3
25*FF
F
_3F

25*FFFVV3 25*FFF3
25*F&VV3F&3
LL
MM
MM
1
''
'
1
1
'
'
),'|]...([
max
),'...'|]...([
)|'...'(
max
)],'|]...([)|'([max
(6)
where '...'
1
' NFF& = is the sequence of class 
corresponding to the Chinese character 
sequence.
In addition, if MF is a normal word, 
1)|]...([ =-- MHQGFVWDUWF FVV3 MM .                   (7)
Based on the context model and entity 
models, we can compute the probability 3&_6
2 For simplification, only nested person, location 
names are identified in organization. The nested 
person in location is not identified because of low 
frequency
and can get the optimal class sequence The 
Chinese PER and transliterated PER share the 
same context class model when computing the 
probability. 0RGHOV(VWLPDWLRQ
As discussed in 3.1.1, there are two kinds of 
probabilities to be estimated: P(C) and P(S|C) . 
Both probabilities are estimated using 
Maximum Likelihood Estimation (MLE) with 
the annotated training corpus.
The parser NLPWin3 was used to tag the 
training corpus. As a result, the corpus was 
annotated with NE marks. Four lists were 
extracted from the annotated corpus and each 
list corresponds one NE class. The context 
model 3& was trained with the annotated 
corpus and the four entity models were trained 
with corresponding NE lists. The Figure 1 
shows the training process. (Begin of sentence 
(BOS) and end of sentence (EOS) is added)
NLPWin
Tagged
Sentence
<LOC>b?</LOC>?<PER>??</PER>,$<ORG>???N@?</ORG>X???<LOC>?<LOC>
Context
Class
BOS LN ? PN ,$ ON X??? LN  EOS
LN list b??
FN list ??
ON list ???N@?
ON Class list LN ??N@?
Corresponding 
English
Sentence
<LOC>U.S.</LOC>president 
<PER>Bush</PER> arrived in 
<LOC> P.R. China </LOC> by flight 
No.1 of <ORG>Air China 
Corp.</ORG>
Figure 1:  Example of  Training Process 'HFRGHU
Given a sequence of Chinese characters, the 
decoding process consists of the following 
three steps:6WHS  All possible word segmentations are 
generated using a Chinese lexicon 
containing 120,050 entries. The lexicon is 
only used for segmentation and there is no 
NE tag in it even if one word is PER, LOC or 
3 NLPWin system is a natural language processing 
system developed by Microsoft Research.
ORG. For example, ?? (Beijing) is not 
tagged as LOC.6WHS NE candidates are generated from any 
one or more segmented character strings and 
the corresponding generative probability for 
each candidate is computed using entity 
models described in Equation (4)?(7). 6WHS  Viterbi search is used to select 
hypothesis with the highest probability as 
the best output. Furthermore, in order to 
identify nested named entities, two-pass
Viterbi search is adopted. The inner Viterbi 
search is corresponding to Equation (6) and 
the outer one corresponding to Equation (1). 
After the two-pass searches, the word 
segmentation and the named entities 
(including nested ones) can be obtained. ,PSURYHPHQW
There are some problems with the framework 
of NE identification using the class-based LM.
First, redundant candidates NEs are generated 
in the decoding process, which results in very 
large search space. The second problem is that 
data sparseness will seriously influence the 
performance. Finally, the abbreviation of NEs 
cannot be handled effectively. In the following 
three subsections, we provide solutions to the 
three problems mentioned above. +HXULVWLF,QIRUPDWLRQ
In order to overcome the redundant candidate 
generation problem, the heuristic information 
is introduced into the class-based LM. The 
following resources were used: (1) Chinese 
family name list, containing 373 entries (e.g. ?
(Zhang), _ (Wang)); (2) transliterated name 
character list, containing 618 characters (e.g.?
(shi), S (dun)); and (3) ORG keyword list,
containing 1,355 entries (e.g. ?: (university),@?(corporation)).
The heuristic information is used to 
constrain the generation of NE candidates. For 
PER (PN), only PER candidates beginning with 
the family name is considered. For PER (FN), a 
candidate is generated only if all its composing 
character belongs to the transliterated name 
character list. For ORG, a candidate is excluded 
if it does not contain one ORG keyword.
Here, we do not utilize the LOC keyword to 
generate LOC candidate because of the fact that 
many LOC do not end with keywords.
 &DFKH0RGHO
The cache entity model can address the data 
sparseness problem by adjusting the parameters 
continually as NE identification proceeds. The 
basic idea is to accumulate Chinese character or 
word n-gram so far appeared in the document 
and use them to create a local dynamic entity
model such as )|( 1-LLELFDFKH ZZ3 and 
)( LXQLFDFKH Z3 . We can interpolate the cache 
entity model with the static entity 
LM )...|( 121 -- LLLVWDWLF ZZZZ3 :ZZZ_Z3 LLLFDFKH -- (8)
)....|()1(
)|()(
1121
121
-
-
--+
+=
LLVWDWLF
LLELFDFKHLXQLFDFKH ZZZ3 ZZ3Z3 OO OO
where ]1,0[, 21 ?OO are interpolation weight 
that is determined on the held-out data set. 'HDOLQJZLWK$EEUHYLDWLRQ
We found that many errors result from the 
occurrence of abbreviation of person, location, 
and organization. Therefore, different 
strategies are adopted to deal with 
abbreviations for different kinds of NEs. For 
PER, if Chinese surname is followed by the 
title, then this surname is tagged as PER. For 
example, ??S (President Zuo) is tagged as 
<PER>?</PER> ?S. For LOC, if at least 
two location abbreviations occur consecutive, 
the individual location abbreviation is tagged as 
LOC. For example,?G? (Sino-Japan 
relation) is tagged as <LOC> 
</LOC><LOC>?</LOC> G?. For ORG, if 
organization abbreviation is followed by LOC, 
which is again followed by organization 
keyword, the three units are tagged as one ORG. 
For example,  E ? ? ? ? (Chinese 
Communist Party Committee of Beijing) i s 
tagged as <ORG>E<LOC>??</LOC> ?? </ORG>. At present, we collected 112 
organization abbreviations and 18 location 
abbreviations.
 ([SHULPHQWV
 (YDOXDWLRQ0HWULF
We conduct evaluations in terms of precision (P) 
and recall (R).
1(LGHQWLILHGRIQXPEHU 1(LGHQWLILHGFRUUHFWO\RIQXPEHU3 = (9)
1(DOORIQXPEHU 1(LGHQWLILHGFRUUHFWRIQXPEHU5 = (10)
We also used the F-measure, which is defined 
as a weighted combination of precision and 
recall as Equation (11):
53 53)  +? ??+= EE (11)
where E is the relative weight of precision and 
recall.
There are two differences between MET 
evaluation and ours. First, we include nested 
NE in our evaluation whereas MET does not.
Second, in our evaluation, only NEs with 
correct boundary and type label are considered 
the correct identifications. In MET, the 
evaluation is somewhat flexible. For example, a 
NE may be identified partially correctly if the 
label is correct but the boundary is wrongly 
detected. 'DWD6HWV
The training text corpus contains data from
People?s Daily (Jan.-Jun.1998). It contains
357,544 sentences (about 9,200,000 Chinese 
characters). This corpus includes 104,487
Chinese PER, 51,708 transliterated PER, 
218,904 LOC, and 87,391 ORG. These data 
was obtained after this corpus was parsed with 
NLPWin.
We built the wide coverage test data 
according to the guidelines4 that are just same 
as those of 1999 IEER. The test set (as shown in 
Table 2) contains half a million Chinese 
characters; it is a balanced test set covering 11 
domains. The test set contains 11,844 sentences,
49.84% of the sentences contain at least one NE. 
The number of characters in NE accounts for
8.448% in all Chinese characters.
We can see that the test data is much larger 
than the MET test data and IEER data
4 The difference between IEER?s guidelines and 
ours is that the nested person and location name in 
organization are tagged in our guidelines.
7DEOH: Statistics of Open-Test
Number of NE TokensID Domain
PER LOC ORG
Size
(byte)
1 Army 65 202 25 19k
2 Computer 75 156 171 59k
3 Culture 548 639 85 138k
4 Economy 160 824 363 108k
5 Entertainment 672 575 139 104k
6 Literature 464 707 122 96k
7 Nation 448 1193 250 101k
8 People 1147 912 403 116k
9 Politics 525 1148 218 122k
10 Science 155 204 87 60k
11 Sports 743 1198 628 114k
Total 5002 7758 2491 1037k
 7UDLQLQJ'DWD3UHSDUDWLRQ
The training data produced by NLPWin has 
some noise due to two reasons. First, the NE 
guideline used by NLPWin is different from 
the one we used. For example, in NLPWin, ???(Beijing City) is tagged as <LOC>??
</LOC> ?, whereas ??? should be LOC 
in our definition. Second, there are some errors
in NLPWin results. We utilized 18 rules to 
correct the frequent errors. The following 
shows some examples.
The Table 4 shows the quality of our training 
corpus.
Table 4   Quality of Training Corpus 
NE P (%) R (%) F (%)
PER 61.05 75.26 67.42
LOC 78.14 71.57 74.71
ORG 68.29 31.50 43.11
Total 70.07 66.08 68.02
 ([SHULPHQWV
We conduct incrementally the following four 
experiments:
(1) Class-based LM, we view the results as 
baseline performance;
(2) Integrating heuristic information into (1);
(3) Integrating Cache-based LM with (2);
(4) Integrating NE abbreviation processing 
with (3).
/1/RFDWLRQ.H\ : /1/1O /1Z : 21_b_?_???? : /1?
 &ODVVEDVHG/0%DVHOLQH
Based on the basic class-based models 
estimated with the training data, we can get the 
baseline performance, as is shown in Table 5. 
Comparing Table 4 and Table 5, we found that 
the performance of baseline is better than the 
quality of training data.
Table 5    Baseline Performance
NE P (%) R (%) F (%)
PER 65.70 84.37 73.87
LOC 82.73 76.03 79.24
ORG 56.55 38.56 45.86
Total 72.61 72.44 72.53
 ,QWHJUDWLQJ+HXULVWLF,QIRUPDWLRQ
In this part, we want to see the effects of using 
heuristic information. The results are shown in 
Table 6. In experiments, we found that by 
integrating the heuristic information, we not 
only achieved more efficient decoding, but also 
obtained higher NE identification precision. For 
example, the precision of PER increases from 
65.70% to 77.63%, and precision of ORG 
increases from 56.55% to 81.23%.  The reason 
is that adopting heuristic information reduces 
the noise influence.
However, we noticed that the recall of PER 
and LOC decreased a bit. There are two reasons. 
First, organization names without organization 
ending keywords were not marked as ORG. 
Second, Chinese names without surnames were 
also missed.
Table 6 Results of Heuristic Information Integrated 
into the Class-based LM
NE P (%) R (%) F (%)
PER 77.63 80.89 79.23
LOC 80.05 80.80 80.42
ORG 81.23 36.65 50.51
Total 79.26 73.41 76.23
 ,QWHJUDWLQJ&DFKHEDVHG/0
Table 7 shows the evaluation results after 
cache-based LM was integrated. From Table 6 
and Table 7, we found that almost all the 
precision and recall of PER, LOC, ORG have 
obtained slight improvements.
Table 7   Results of our system
NE P (%) R (%) F (%)
PER 79.12 82.06 80.57
LOC 80.11 81.27 80.69
ORG 79.71 39.89 53.17
Total 79.72 74.58 77.06 ,QWHJUDWLQJ ZLWK 1( $EEUHYLDWLRQ3URFHVVLQJ
In this experiment, we integrated with NE 
abbreviation processing. As shown in Table 8, 
the experiment result indicates that the recall of 
PER, LOC, ORG increased from 82.06%, 
81.27%, 36.65% to 87.29%, 82.46%, 56.54%, 
respectively.
Table 8   Results of our system
NE P (%) R (%) F (%)
PER 79.86 87.29 83.41
LOC 80.88 82.46 81.66
ORG 76.63 56.54 65.07
Total 79.99 79.68 79.83 6XPPDU\
From above data, we observed that (1) the class 
based SLM performs better than the training 
data automatically produced with the parser; (2) 
the distinct improvements is achieved by using 
heuristic information; (3) Furthermore, our 
method of dealing with abbreviation increases 
the recall of NEs.
In addition, the cache-based LM increases 
the performance not so much. The reason is as 
follows: The cache-based LM is based on the 
hypothesis that a word used in the recent past is 
much likely either to be used soon than its 
overall frequency in the language or a 3 -gram 
model would suggest (Kuhn, 1990). However, 
we found that the same NE often vari es its 
morpheme in the same document. For example, 
the same NE  E ? ? ? ? (Chinese 
Communist Party Committee of Beijing),??? ? (Committee of Beijing City), ? ?
(Committee) occur in order.
Furthermore, we notice that the 
segmentation dictionary has an important 
impact on the performance of NE 
identification. We do not think it is better if 
more words are added into dictionary. For 
example, because ??(Chinese) is in our 
dictionary, there is much possibility that ?
(China) in ?? is missed identified.
 (YDOXDWLRQZLWK0(7DQG,((57HVW'DWD
We also evaluated on the MET2 test data and 
IEER test data. The results are shown in Table 
9. The results on MET2 are lower than the 
highest report of MUC7 (PER: Precision 66%, 
Recall 92%; LOC: Precision 89%, Recall 91%; 
ORG: Precision 89%, Recall 88%, 
http://www.itl.nist.gov). We speculate the 
reasons for this in the following. The main 
reason is that our class-based LM was 
estimated with a general domain corpus, which 
is quite different from the domain of MUC 
data. Moreover, we didn?t use a NE dictionary. 
Another reason is that our NE definitions are 
slightly different from MET2.
Table 9 Results on MET2 and IEER
MET2 Data IEER DataNE
P
(%)
R 
(%)
F
(%)
P 
(%)
R 
(%)
F 
(%)
PER 65.86 94.25 77.54 79.38 84.43 81.83
LOC 77.42 89.60 83.07 79.09 80.18 79.63
ORG 88.47 75.33 81.38 88.03 62.30 72.96
Total 77.89 86.09 81.79 80.82 76.78 78.75
 &RQFOXVLRQV 	)XWXUHZRUN
In this research, Chinese word segmentation 
and NE identification has been integrated into 
a framework using class-based language 
models (LM). We adopted a hierarchical 
structure in ORG model so that the nested 
entities in organization names can be identified. 
Another characteristic is that our NE 
identification do not utilize NE dictionary
when decoding.
The evaluation on a large test set shows
consistent improvements. The integration of 
heuristic information improves the precision 
and recall of our system. The cache-based LM 
increases the recall of NE identification to 
some extent. Moreover, some rules dealing 
with abbreviations of NEs have increased 
dramatically the performance. The precision of 
PER, LOC, ORG on the test set is 79.86%, 
80.88%, 76.63%, respectively; and the recall is
87.29%, 82.46%, 56.54%, respectively.
In our future work, we will be focusing 
more on NE coreference using language model. 
Second, we intend to extend our model to 
include the part-of-speech tagging model to 
improve the performance. At present, the 
class-based LM is based on the general domain 
and we may need to fine-tune the model for a 
specific domain. 
ACKNOWLEDGEMENT
I would like to thank Ming Zhou, Jianfeng 
Gao, Changning Huang, Andi Wu, Hang Li
and other colleagues from Microsoft Research 
for their help. And I want to thank especially 
Lei Zhang from Tsinghua University for his 
help in developing the ideas.5HIHUHQFHV
Borthwick. A. (1999) A Maximum Entropy 
Approach to Named Entity Recognition. PhD 
Dissertation
Bikel D., Schwarta R., Weischedel. R. (1997) An 
algorithm that learns what?s in a name. Machine 
Learning 34, pp. 211-231
Brown, P. F., DellaPietra, V. J., deSouza, P. V., Lai, 
J. C., and Mercer, R. L. (1992). Class-based 
n-gram models of natural language. Computational 
Linguistics, 18(4):468--479.
Chinchor. N. (1997) MUC-7 Named Entity Task 
Definition Version 3.5. Available by from 
ftp.muc.saic.com/pub/MUC/MUC7-guidelines
Chen H.H., Ding Y.W., Tsai S.C. and Bian G.W. 
(1997) Description of the NTU System Used for 
MET2
Gao J.F., Goodman J., Li M.J., Lee K.F. (2001)  
Toward a unified Approach to Statistical Language 
Modeling for Chinese. To appear in ACM 
Transaction on Asian Language Processing
Kuhn R., Mori. R.D. (1990) A Cache-Based 
Natural Language Model for Speech Recognition. 
IEEE Transaction on Pattern Analysis and Machine 
Intelligence.Vol.12. No. 6. pp 570-583
Mikheev A., Grover C. and Moens M. (1997)
Description of the LTG System Used for MUC-7
Sekine S., Grishman R. and Shinou H. (1998), ?A 
decision tree method for finding and classifying 
names in Japanese texts?, Proceedings of the Sixth 
Workshop on Very Large Corpora, Canada 
Yu S.H., Bai S.H. and Wu P. (1997) Description of 
the Kent Ridge Digital Labs System Used for 
MUC-7
Zhang L. (2001) Study on Chinese Proofreading 
Oriented Language Modeling, PhD Dissertation
	
						
	
	
	
	

Coling 2010: Poster Volume, pages 1462?1470,
Beijing, August 2010
              Extracting and Ranking Product Features 
                 in Opinion Documents 
Lei  Zhang 
Department of Computer Science 
University of Illinois at Chicago 
lzhang3@cs.uic.edu
Suk Hwan Lim 
Hewlett-Packard Labs 
suk-hwan.lim@hp.com
                           Bing Liu 
Department of Computer Science 
University of Illinois at Chicago 
liub@cs.uic.edu
Eamonn O?Brien-Strain 
Hewlett-Packard Labs 
eob@hpl.hp.com
Abstract
An important task of opinion mining is 
to extract people?s opinions on features 
of an entity. For example, the sentence, 
?I love the GPS function of Motorola 
Droid? expresses a positive opinion on 
the ?GPS function? of the Motorola 
phone. ?GPS function? is the feature. 
This paper focuses on mining features. 
Double propagation is a state-of-the-art 
technique for solving the problem. It 
works well for medium-size corpora. 
However, for large and small corpora, it 
can result in low precision and low re-
call. To deal with these two problems, 
two improvements based on part-whole
and ?no? patterns are introduced to in-
crease the recall. Then feature ranking is 
applied to the extracted feature candi-
dates to improve the precision of the 
top-ranked candidates. We rank feature 
candidates by feature importance which 
is determined by two factors: feature re-
levance and feature frequency. The 
problem is formulated as a bipartite 
graph and the well-known web page 
ranking algorithm HITS is used to find 
important features and rank them high. 
Experiments on diverse real-life datasets 
show promising results. 
1 Introduction 
In recent years, opinion mining or sentiment 
analysis (Liu, 2010; Pang and Lee, 2008) has 
been an active research area in NLP. One task is 
to extract people?s opinions expressed on 
features of entities (Hu and Liu, 2004). For 
example, the sentence, ?The picture of this 
camera is amazing?, expresses a positive 
opinion on the picture of the camera. ?picture?
is the feature. How to extract features from a 
corpus is an important problem. There are 
several studies on feature extraction (e.g., Hu 
and Liu, 2004, Popescu and Etzioni, 2005, 
Kobayashi et al, 2007, Scaffidi et al, 2007, 
Stoyanov and Cardie. 2008, Wong et al, 2008, 
Qiu et al, 2009). However, this problem is far 
from being solved.  
Double Propagation (Qiu et al, 2009) is a 
state-of-the-art unsupervised technique for 
solving the problem. It mainly extracts noun 
features, and works well for medium-size 
corpora. But for large corpora, this method can 
introduce a great deal of noise (low precision), 
and for small corpora, it can miss important 
features. To deal with these two problems, we 
propose a new feature mining method, which 
enhances that in (Qiu et al, 2009). Firstly, two 
improvements based on part-whole patterns and 
?no? patterns are introduced to increase recall. 
Part-whole or meronymy is an important 
semantic relation in NLP, which indicates that 
one or more objects are parts of another object. 
1462
For example, the phrase ?the engine of the car?
contains the part-whole relation that ?engine? is 
part of ?car?. This relation is very useful for 
feature extraction, because if we know one 
object is part of a product class, this object 
should be a feature. ?no? pattern is another 
extraction pattern. Its basic form is the word 
?no? followed by a noun/noun phrase, for 
instance, ?no noise?. People often express their 
short comments or opinions on features using 
this pattern. Both types of patterns can help find 
features missed by double propagation. As for 
the low precision problem, we present a feature 
ranking approach to tackle it. We rank feature 
candidates based on their importance which 
consists of two factors: feature relevance and 
feature frequency. The basic idea of feature 
importance ranking is that if a feature candidate 
is correct and frequently mentioned in a corpus, 
it should be ranked high; otherwise it should be 
ranked low in the final result. Feature frequency 
is the occurrence frequency of a feature in a 
corpus, which is easy to obtain. However, 
assessing feature relevance is challenging. We 
model the problem as a bipartite graph and use 
the well-known web page ranking algorithm 
HITS (Kleinberg, 1999) to find important 
features and rank them high. Our experimental 
results show superior performances. In practical 
applications, we believe that ranking is also 
important for feature mining because ranking 
can help users to discover important features 
from the extracted hundreds of fine-grained 
candidate features efficiently. 
2 Related work 
Hu and Liu (2004) proposed a technique based 
on association rule mining to extract product 
features. The main idea is that people often use 
the same words when they comment on the 
same product features. Then frequent itemsets 
of nouns in reviews are likely to be product fea-
tures while the infrequent ones are less likely to 
be product features. This work also introduced 
the idea of using opinion words to find addi-
tional (often infrequent) features.
   Popescu and Etzioni (2005) investigated the 
same problem. Their algorithm requires that the 
product class is known. The algorithm deter-
mines whether a noun/noun phrase is a feature 
by computing the pointwise mutual information 
(PMI) score between the phrase and class-
specific discriminators, e.g., ?of xx?, ?xx has?,
?xx comes with?, etc., where xx is a product 
class. This work first used part-whole patterns 
for feature mining, but it finds part-whole based 
features by searching the Web. Querying the 
Web is time-consuming. In our method, we use 
predefined part-whole relation patterns to ex-
tract features in a domain corpus. These patterns 
are domain-independent and fairly accurate.  
   Following the initial work in (Hu and Liu 
2004), several researchers have further explored 
the idea of using opinion words in product fea-
ture mining. A dependency based method was 
proposed in (Zhuang et al, 2006) for a movie 
review analysis application. Qiu et al (2009) 
proposed a double propagation method, which 
exploits certain syntactic relations of opinion 
words and features, and propagates through 
both opinion words and features iteratively. The 
extraction rules are designed based on different 
relations between opinion words and features, 
and among opinion words and features them-
selves. Dependency grammar was adopted to 
describe these relations. In (Wang and Wang, 
2008), another bootstrapping method was pro-
posed. In (Kobayashi et al 2007), a pattern min-
ing method was used. The patterns are relations 
between feature and opinion pairs (they call as-
pect-evaluation pairs). The patterns are mined 
from a large corpus using pattern mining. Statis-
tics from the corpus are used to determine the 
confidence scores of the extraction.  
In general information extraction, there are 
two approaches: rule-based and statistical. Early 
extraction systems are mainly based on rules 
(e.g., Riloff, 1993). In statistical methods, the 
most popular models are Hidden Markov Mod-
els (HMM) (Rabiner, 1989), Maximum Entropy 
Models (ME) (Chieu et al, 2002) and Condi-
tional Random Fields (CRF) (Lafferty et al, 
2001). CRF has been shown to be the most ef-
fective method. It was used in (Stoyanov et al, 
2008). However, a limitation of CRF is that it 
only captures local patterns rather than long 
range patterns. It has been shown in (Qiu et al, 
2009) that many feature and opinion word pairs 
have long range dependencies. Experimental 
results in (Qiu et al, 2009) indicate that CRF 
does not perform well.  
Other related works on feature extraction 
mainly use topic modeling to capture topics in 
1463
reviews (Mei et al, 2007). In (Su et al, 2008), 
the authors also proposed a clustering based 
method with mutual reinforcement to identify 
features. However, topic modeling or clustering 
is only able to find some general/rough features, 
and has difficulty in finding fine-grained or pre-
cise features, which is more related to informa-
tion extraction.  
3 The Proposed Method 
As discussed in the introduction section, our 
proposed method deals with the problems of 
double propagation. So let us give a short ex-
planation why double propagation can cause 
problems in large or small corpora. 
 Double propagation assumes that features are 
nouns/noun phrases and opinion words are ad-
jectives. It is shown that opinion words are 
usually associated with features in some ways. 
Thus, opinion words can be recognized by iden-
tified features, and features can be identified by 
known opinion words. The extracted opinion 
words and features are utilized to identify new 
opinion words and new features, which are used 
again to extract more opinion words and fea-
tures. This propagation or bootstrapping process 
ends when no more opinion words or features 
can be found. The biggest advantage of the me-
thod is that it requires no additional resources 
except an initial seed opinion lexicon, which is 
readily available (Wilson et al, 2005, Ding et 
al., 2008). Thus it is domain independent and 
unsupervised, avoiding laborious and time-
consuming work of labeling data for supervised 
learning methods. It works well for medium?
size corpora. But for large corpora, this method 
may extract many nouns/noun phrases which 
are not features. The precision of the method 
thus drops. The reason is that during propaga-
tion, adjectives which are not opinionated will 
be extracted as opinion words, e.g., ?entire? and 
?current?. These adjectives are not opinion 
words but they can modify many kinds of 
nouns/noun phrases, thus leading to extracting 
wrong features. Iteratively, more and more 
noises may be introduced during the process. 
The other problem is that for certain domains, 
some important features do not have opinion 
words modifying them. For example, in reviews 
of mattresses, a reviewer may say ?There is a 
valley on my mattress?, which implies a nega-
tive opinion because ?valley? is undesirable for 
a mattress. Obviously, ?valley? is a feature, but 
?valley? may not be described by any opinion 
adjective, especially for a small corpus. Double 
propagation is not applicable in this situation.  
   To deal with the problem, we propose a novel 
method to mine features, which consists of two 
steps: feature extraction and feature ranking. 
For feature extraction, we still adopt the double 
propagation idea to populate feature candidates. 
But two improvements based on part-whole re-
lation patterns and a ?no? pattern are made to 
find features which double propagation cannot 
find. They can solve part of the recall problem. 
For feature ranking, we rank feature candidates 
by feature importance.        
     A part-whole pattern indicates one object is 
part of another object. For the previous example 
?There is a valley on my mattress?, we can find 
that it contains a part-whole relation between 
?valley? and ?mattress?. ?valley? belongs to 
?mattress?, which is indicated by the preposi-
tion ?on?. Note that ?valley? is not actually a 
part of mattress, but an effect on the mattress. It 
is called a pseudo part-whole relation. For sim-
plicity, we will not distinguish it from an actual 
part-whole relation because for our feature min-
ing task, they have little difference. In this case, 
?noun1 on noun2? is a good indicative pattern 
which implies noun1 is part of noun2. So if we 
know ?mattress? is a class concept, we can infer 
that ?valley? is a feature for ?mattress?. There 
are many phrase or sentence patterns 
representing this type of semantic relation 
which was studied in (Girju et al 2006). Beside 
part-whole patterns, ?no? pattern is another im-
portant and specific feature indicator in opinion 
documents. We introduce these patterns in de-
tail in Sections 3.2 and 3.3. 
   Now let us deal with the first problem: noise. 
With opinion words, part-whole and ?no? pat-
terns, we have three feature indicators at hands, 
but all of them are ambiguous, which means 
that they are not hard rules. We will inevitably 
extract wrong features (also called noises) by 
using them. Pruning noises from feature candi-
dates is a hard task. Instead, we propose a new 
angle for solving this problem: feature ranking. 
The basic idea is that we rank the extracted fea-
ture candidates by feature importance. If a fea-
ture candidate is correct and important, it should 
be ranked high. For unimportant feature or 
1464
noise, it should be ranked low in the final result. 
Ranking is also very useful in practice. In a 
large corpus, we may extract hundreds of fine-
grained features. But the user often only cares 
about those important ones, which should be 
ranked high. We identified two major factors 
affecting the feature importance: one is feature 
relevance and the other is feature frequency. 
Feature relevance: it describes how possible 
a feature candidate is a correct feature. We find 
that there are three strong clues to indicate fea-
ture relevance in a corpus. The first clue is that 
a correct feature is often modified by multiple 
opinion words (adjectives or adverbs). For ex-
ample, in the mattress domain, ?delivery? is 
modified by ?quick? ?cumbersome? and ?time-
ly?. It shows that reviewers put emphasis on the 
word ?delivery?.  Thus we can infer that ?deli-
very? is a possible feature. The second clue is 
that a feature could be extracted by multiple 
part-whole patterns. For example, in the car 
domain, if we find following two phrases, ?the
engine of the car? and ?the car has a big en-
gine?, we can infer that ?engine? is a feature for 
car, because both phrases contain part-whole 
relations to indicate ?engine? is a part of ?car?. 
The third clue is the combination of opinion 
word modification, part-whole pattern extrac-
tion and ?no? pattern extraction. That is, if a 
feature candidate is not only modified by opi-
nion words but also extracted by part-whole or 
?no? patterns, we can infer that it is a feature 
with high confidence. For example, for sentence 
?there is a bad hole in the mattress?, it strongly 
indicates that ?hole? is a feature for a mattress 
because it is modified by opinion word ?bad?
and also in the part-whole pattern. What is 
more, we find that there is a mutual enforce-
ment relation between opinion words, part-
whole and ?no? patterns, and features. If an ad-
jective modifies many correct features, it is 
highly possible to be a good opinion word. Si-
milarly, if a feature candidate can be extracted 
by many opinion words, part-whole patterns, or 
?no? pattern, it is also highly likely to be a cor-
rect feature. This indicates that the Web page 
ranking algorithm HITS is applicable.  
Feature frequency: This is another important 
factor affecting feature ranking. Feature fre-
quency has been considered in (Hu and Liu, 
2004; Blair-Goldensohn et al, 2008). We con-
sider a feature f1 to be more important than fea-
ture f2 if f1 appears more frequently than f2 in 
opinion documents. In practice, it is desirable to 
rank those frequent features higher than infre-
quent features. The reason is that missing a fre-
quently mentioned feature in opinion mining is 
bad, but missing a rare feature is not a big issue.  
   Combining the above factors, we propose a 
new feature mining method. Experiments show 
good results on diverse real-life datasets. 
3.1 Double Propagation 
As we described above, double propagation is 
based on the observation that there are natural 
relations between opinion words and features 
due to the fact that opinion words are often used 
to modify features. Furthermore, it is observed 
that opinion words and features themselves have 
relations in opinionated expressions too (Qiu et 
al., 2009). These relations can be identified via 
a dependency parser (Lin, 1998) based on the 
dependency grammar. The identification of the 
relations is the key to feature extraction. 
Dependency grammar: It describes the de-
pendency relations between words in a sentence. 
After parsed by a dependency parser, words in a 
sentence are linked to each other by a certain 
relation. For a sentence, ?The camera has a 
good lens?, ?good? is the opinion word and 
?lens? is the feature of camera. After parsing, 
we can find that ?good? depends on ?lens? with 
relation mod. Here mod means that ?good? is 
the adjunct modifier for ?lens?. In some cases, 
an opinion word and a feature are not directly 
dependent, but they directly depend on a same 
word. For example, from the sentence ?The lens 
is nice?, we can find that both feature ?lens? and 
opinion word ?nice? depend on the verb ?is?
with the relation s and pred respectively. Here s
means that ?lens? is the surface subject of ?is?
while pred means that ?nice? is the predicate of 
the ?is? clause.    
   In (Qiu et al, 2009), it defines two categories 
of dependency relations to summarize all types 
of dependency relations between two words, 
which are illustrated in Figure 1. Arrows are 
used to represent dependencies. 
Direct relations: It represents that one word 
depends on the other word directly or they both 
depend on a third word directly, shown in (a) 
and (b) of Figure 1. In (a), B depends on A di-
rectly, and in (b) they both directly depend on D.
    Indirect relation: It represents that one word 
1465
depends on the other word through other words 
or they both depend on a third word indirectly. 
For example, in (c) of Figure 1, B depends on A
through D; in (d) of Figure 1, A depends on D
through I1 while B depends on D through I2. For 
some complicated situations, there can be more 
than one I1 or I2.    
Fig.1 Different relations between A and B 
    
      Parsing indirect relations is error-prone for 
Web corpora. Thus we only use direct relation 
to extract opinion words and feature candidates 
in our application. For detailed extraction rules, 
please refer to the paper (Qiu et al, 2009). 
3.2 Part-whole relation 
As we discussed above, a part-whole relation is 
a good indicator for features if the class concept 
word (the ?whole? part) is known. For example, 
the compound nominal ?car hood? contains the 
part-whole relation. If we know ?car? is the 
class concept word, then we can infer that 
?hood? is a feature for car. Part-whole patterns 
occur frequently in text and are expressed by a 
variety of lexico-syntactic structures (Girju et 
al, 2006; Popescu and Etzioni, 2005). There are 
two types of lexico-syntactic structures convey-
ing part-whole relations: unambiguous structure 
and ambiguous structure. The unambiguous 
structure clearly indicates a part-whole relation. 
For example, for sentences ?the camera consists 
of lens, body and power cord.? and ?the bed 
was made of wood?. In these cases, the detec-
tion of the patterns leads to the discovery of real 
part-whole relations. We can easily find features 
of the camera and the bed. Unfortunately, this 
kind of patterns is not very frequent in a corpus. 
However, there are many ambiguous expres-
sions that are explicit but convey part-whole 
relations only in some contexts. For example, 
for two phrases ?valley on the mattress? and 
?toy on the mattress?, ?valley? is a part of ?mat-
tress? whereas ?toy? is not a part of ?mattress?.
Our idea is to use both the unambiguous and 
ambiguous patterns. Although ambiguous pat-
terns may bring some noise, we can rank them 
low in the ranking procedure. The following 
two kinds of patterns are what we have utilized 
for feature extraction.
3.2.1 Phrase pattern 
In this case, the part-whole relation exists in a 
phrase.
NP + Prep + CP:  noun/noun phrase (NP) 
contains the part word and the class concept 
phrase (CP) contains the whole word. They are 
connected by the preposition word (Prep). For 
example, ?battery of the camera? is an instance 
of this pattern where NP (battery) is the part
noun and CP (camera) is the whole noun. For 
our application, we only use three specific pre-
positions: ?of?, ?in? and ?on?.  
CP + with + NP:   likewise, CP is the class 
concept phrase, and NP is the noun/noun phrase. 
They are connected by the word ?with?. Here 
NP is likely to be a feature. For example, in a 
phrase, ?mattress with a cover?, ?cover? is a 
feature for mattress.
NP CP or CP NP: noun/noun phase (NP) 
and class concept phrase (CP) forms a com-
pound word. For example, ?mattress pad?. Here 
?pad? is a feature of ?mattress?. 
3.2.2 Sentence pattern 
In these patterns, the part-whole relation is indi-
cated in a sentence. The patterns contain specif-
ic verbs. The part word and the whole word can 
be found inside noun phrases or prepositional 
phrases which contain specific prepositions. We 
utilize the following patterns in our application. 
   ?CP Verb NP?:  CP is the class concept 
phrase that contains the whole word, NP is the 
noun phrase that contains the part word and the 
verb is restricted and specific. For example, in a 
sentence, ?the phone has a big screen?, we can 
infer that ?screen? is a feature for ?phone?,
which is a class concept. In sentence patterns, 
verbs play an important role. We use indicative 
verbs to find part-whole relations in a sentence, 
A D
A BB
B
A
D
A
D
I1
B
I2
(a) (b) 
(c) (d) 
1466
i.e., ?has?, ?have? ?include? ?contain? ?consist?, 
?comprise? and so on (Girju et al 2006). 
It is worth mentioning that in order to use 
part-whole relations, the class concept word for 
a corpus is needed, which is fairly easy to find 
because the noun with the most frequent occur-
rences in a corpus is always the class concept 
word based on our experiments.  
3.3 ?no? Pattern 
Besides opinion word and part-whole relation, 
?no? pattern is also an important pattern indicat-
ing features in a corpus. Here ?no? represents 
word no.  The basic form of the pattern is ?no? 
word followed by noun/noun phrase. This sim-
ple pattern actually is very useful to feature ex-
traction. It is a specific pattern for product re-
views and forum posts. People often express 
their comments or opinions on features by this 
short pattern. For example, in a mattress domain, 
people always say that ?no noise? and ?no in-
dentation?. Here ?noise? and ?indentation? are 
all features for the mattress. We discover that 
this pattern is frequently used in corpora and a 
very good indicator for features with a fairly 
high precision. But we have to take care of the 
some fixed ?no? expression, like ?no problem?
?no offense?. In these cases, ?problem? and ?of-
fense? should not be regarded as features. We 
have a list of such words, which are manually 
compiled.    
3.4 Bipartite Graph and HITS Algorithm 
Hyperlink-induced topic search (HITS) is a link 
analysis algorithm that rates Web pages. As 
discussed in the introduction section, we can 
apply the HITS algorithm to compute feature 
relevance for ranking.  
   Before illustrating how HITS can be applied 
to our scenario, let us first give a brief 
introduction to HITS. Given a broad search 
query q, HITS sends the query to a search 
engine system, and then collects k (k = 200 in 
the original paper) highest ranked pages, which 
are assumed to be highly relevant to the search 
query. This set is called the root set R; then it 
grows R by including any page pointed to a 
page in R, then forms a base set S. HITS then 
works on the pages in S. It assigns every page in 
S an authority score and a hub score. Let the 
number of pages to be studied be n. We use G = 
(V, E) to denote the (directed) link graph of S. V
is the set of pages (or nodes) and E is the set of 
directed edges (or links). We use L to denote the 
adjacency matrix of the graph.  
??? ?  ?
?????? ?? ? ?
??????????
                 (1) 
Let the authority score of the page i be A(i), and 
the hub score of page i be H(i). The mutual rein-
forcing relationship of the two scores is 
represented as follows: 
                  ???? ? ? ???????????             (2)    
                  ???? ? ? ???????????                (3)
We can write them in a matrix form. We use A
to denote the column vector with all the authori-
ty scores, A = (A(1), A(2), ?, A(n))T, and use H
to denote the column vector with all the hub 
scores, H = (H(1), H(2), ?, H(n))T,
                             ? ?4)                         ???) 
                             ? ? 5)                            ??)
To solve the problem, the widely used method 
is power iteration, which starts with some ran-
dom values for the vectors, e.g., A0 = H0 = (1, 1, 
1, ?1,). It then continues to compute iteratively 
until the algorithm converges.  
   From the formulas, we can see that the author-
ity score estimates the importance of the content 
of the page, and the hub score estimates the val-
ues of its links to other pages. An authority 
score is computed as the sum of the scaled hub 
scores that point to that page. A hub score is the 
sum of the scaled authority scores of the pages 
it points to. The key idea of HITS is that a good 
hub points to many good authorities and a good 
authority is pointed by many good hubs. Thus, 
authorities and hubs have a mutual reinforce-
ment relationship. 
   For our scenario, we have three strong clues 
for features in a corpus: opinion words, part-
whole patterns, and the ?no? pattern. Although 
all these three clues are not hard rules, there 
exist mutual enforcement relations between 
them. If an adjective modify many features, it is 
highly likely to be a good opinion word. If a 
feature candidate is modified by many opinion 
words, it is likely to be a genuine feature. The 
same goes with part-whole patterns, the ?no? 
pattern, or the combination for these three clues. 
This kind of mutual enforcement relation can be 
naturally modeled in the HITS framework.  
1467
Applying the HITS algorithm: Based on the 
key idea of HITS algorithm and feature indica-
tors, we can apply the HITS algorithm to obtain 
the feature relevance ranking. Features act as 
authorities and feature indicators act as hubs. 
Different from the general HITS algorithm, fea-
tures only have authority scores and feature in-
dicators only have hub scores in our case. They 
form a directed bipartite graph, which is illu-
strated in Figure 2. We can run the HITS algo-
rithm on this bipartite graph. The basic idea is 
that if a feature candidate has a high authority 
score, it must be a highly-relevant feature. If a 
feature indicator has a high hub score, it must be 
a good feature indicator. 
Fig. 2 Relations between feature indicators and 
features 
3.5 Feature Ranking 
Although the HITS algorithm can rank features 
by feature relevance, the final ranking is not 
only determined by relevance. As we discussed 
before, feature frequency is another important 
factor affecting the final ranking. It is highly 
desirable to rank those correct and frequent 
features at top because they are more important 
than the infrequent ones in opinion mining (or 
even other applications). With this in mind, we 
put everything together to present the final 
algorithm that we use. We use two steps: 
Step 1:  Compute feature score using HITS 
without considering frequency. Initially, we use 
three feature indicators to populate feature 
candidates, which form a directed bipartite 
graph. Each feature candidate acts as an 
authority node in the graph; each feature 
indicator acts as a hub node. For node s in the 
graph, we let ?? be the hub score and ?? be the 
authority score. Then, we initialize ?? and ?? to 
1 for all nodes in the graph. We update the 
scores of ??  and ??  until they converge using 
power iteration. Finally, we normalize ??  and 
compute the score S for a feature.
Step 2: The final score function considering 
the feature frequency is given in Equation (6).  
? ? ????????????????                       (6)
where ???????  is the frequency count of 
ture?, and S(f) is the authority score of the can-
didate feature f. The idea is to push the frequent 
candidate features up by multiplying the log of 
frequency. Log is taken in order to reduce the 
effect of big frequency count numbers.    
4 Experiments
This section evaluates the proposed method. We 
first describe the data sets, evaluation metrics 
and then the experimental results. We also com-
pare our method with the double propagation
method given in (Qiu et al, 2009).  
4.1 Data Sets 
We used four diverse data sets to evaluate our 
techniques. They were obtained from a com-
mercial company that provides opinion mining 
services. Table 1 shows the domains (based on 
their names) and the number of sentences in 
each data set (?Sent.? means the sentence). The 
data in ?Cars? and ?Mattress? are product re-
views extracted from some online review sites. 
?Phone? and ?LCD? are forum discussion posts 
extracted from some online forum sites. We 
split each review/post into sentences and the 
sentences are POS-tagged using the Brill?s tag-
ger (Brill, 1995). The tagged sentences are the 
input to our system.  
Data  Sets Cars Mattress Phone LCD 
# of Sent. 2223 13233 15168 1783 
                 Table 1.  Experimental data sets 
4.2 Evaluation Metrics 
Besides precision and recall, we adopt the pre-
cision@N metric for experimental evaluation 
(Liu, 2006). It gives the percentage of correct 
features that are among the top N feature candi-
dates in a ranked list. We compare our method?s 
results with those of double propagation which 
ranks extracted candidates only by occurrence 
frequency.  
4.3 Experimental Results 
We first compare our results with double propa-
    Feature Indicators                      Features 
1468
gation on recall and precision for different cor-
pus sizes. The results are presented in Tables 2, 
3, and 4 for the four data sets. They show the 
precision and recall of 1000, 2000, and 3000 
sentences from these data sets. We did not try 
more sentences because manually checking the 
recall and precision becomes prohibitive. Note 
that there are less than 3000 sentences for ?Cars? 
and ?LCD? data sets. Thus, the columns for 
?Cars? and ?LCD? are empty in Table 4. In the 
Tables, ?DP? represents the double propagation 
method; ?Ours? represents our proposed method; 
?Pr? represents precision, and ?Re? represents 
recall. 
    
 Cars Mattress Phone LCD 
Pr Re Pr Re Pr Re Pr Re 
DP 0.79 0.55 0.79 0.54 0.69 0.23 0.68 0.43
Ours 0.78 0.56 0.77 0.64 0.68 0.44 0.66 0.55
Table 2. Results of 1000 sentences 
 Cars Mattress Phone LCD 
Pr Re Pr Re Pr Re Pr Re
DP 0.70 0.65 0.70 0.58 0.67 0.42 0.64 0.52
Ours 0.66 0.69 0.70 0.66 0.70 0.50 0.62 0.56
Table 3. Results of 2000 sentences  
 Cars Mattress Phone LCD
Pr Re Pr Re 
DP 0.65 0.59 0.64 0.48
Ours 0.66 0.67 0.62 0.51
Table 4. Results of 3000 sentences  
From the tables, we can see that for corpora in 
all domains, our method outperforms double 
propagation on recall with only a small loss in 
precision. In data sets for ?Phone? and ?Mat-
tress?, the precisions are even better. We also 
find that with the increase of the data size, the 
recall gap between the two methods becomes 
smaller gradually and the precisions of both me-
thods also drop. However, in this case, feature 
ranking plays an important role in discovering 
important features. 
   Ranking comparison between the two me-
thods is shown in Tables 5, 6, and 7, which give 
the precisions of top 50, 100 and 200 results 
respectively. Note that the experiments reported 
in these tables were run on the whole data sets. 
There were no more results for the ?LCD? data 
beyond top 200 as there were only a limited 
number of features discussed in the data. So the 
column for ?LCD? in Table 7 is empty. We rank 
the extracted feature candidates based on fre-
quency for the double propagation method (DP). 
Using occurrence frequency is the natural way 
to rank features. The more frequent a feature 
occurs in a corpus, the more important it is. 
However, frequency-based ranking assumes the 
extracted candidates are correct features. The 
tables show that our proposed method (Ours) 
outperforms double propagation considerably. 
The reason is that some highly-frequent feature 
candidates extracted by double propagation are 
not correct features. Our method considers the 
feature relevance as an important factor. So it 
produces much better rankings.  
 Cars Mattress Phone LCD 
DP 0.84 0.81 0.64 0.68 
Ours 0.94 0.90 0.76 0.76 
Table 5. Precision at top 50
 Cars Mattress Phone LCD 
DP      0.82      0.80      0.65      0.68 
Ours      0.88      0.85      0.75      0.73 
Table 6. Precision at top 100
 Cars Mattress Phone LCD 
DP      0.75      0.71      0.70  
Ours      0.80      0.79      0.76  
Table 7. Precision at top 200 
5 Conclusion
Feature extraction for entities is an important 
task for opinion mining. The paper proposed a 
new method to deal with the problems of the 
state-of-the-art double propagation method for 
feature extraction. It first uses part-whole and 
?no? patterns to increase recall. It then ranks the 
extracted feature candidates by feature impor-
tance, which is determined by two factors: fea-
ture relevance and feature frequency. The Web 
page ranking algorithm HITS was applying to 
compute feature relevance. Experimental results 
using diverse real-life datasets show promising 
results. In our future work, apart from improv-
ing the current methods, we also plan to study 
the problem of extracting features that are verbs 
or verb phrases.
Acknowledgement 
This work was funded by a HP Labs Innovation 
Research Program Award (CW165044).  
1469
References 
Blair-Goldensohn, Sasha., Kerry, Hannan., Ryan, 
McDonald., Tyler, Neylon., George A. Reis, Jeff, 
Reyna. 2008. Building Sentiment Summarizer for 
Local Service Reviews In Proceedings of the 
Workshop of NLPIX . WWW, 2008
Brill, Eric. 1995. Transformation-Based Error-
Driven Learning and Natural Language 
Processing: a case study in part of speech tagging. 
Computational Linguistics, 1995.   
Chieu, Hai Leong and Hwee-Tou Ng. 2002. Name 
Entity Recognition: a Maximum Entropy Ap-
proach Using Global Information. In Proceedings 
of the 6th Workshop on Very Large Corpora, 
2002.  
Ding, Xiaowen., Bing Liu and Philip S. Yu. 2008. A 
Holistic Lexicon-Based Approach to Opinion 
Mining  In Proceedings of WSDM 2008.
Girju, Roxana., Adriana Badulescu and Dan Moldo-
van. 2006. ?Automatic Discovery of Part-Whole 
Relations?  Computational Linguistics ,32(1):83-
135 2006 
Hu, Mingqin and Bing Liu. 2004. Mining and Sum-
marizing Customer Reviews. In Proceedings of
KDD 2004
Kleinberg, Jon. 1999. ?Authoritative sources in 
hyperlinked environment? Journal of the ACM 46 
(5): 604-632 1999
Kobayashi, Nozomi., Kentaro Inui and Yuji Matsu-
moto. 2007 Extracting Aspect-Evaluation and As-
pect-of Relations in Opinion Mining. In Proceed-
ings of EMNLP, 2007.
Lafferty, John., Andrew McCallum and Fernando 
Pereira. 2001 Conditional Random Fields: Proba-
bilistic Models for Segmenting and Labeling Se-
quence Data.  In Proceedings of ICML, 2001. 
Lin, Dekang. 1998. Dependency-based evaluation of 
MINIPAR. In Proceedings of the Workshop on 
Evaluation of Parsing System at ICLRE 1998.
Liu, Bing. 2006. Web Data Mining: Exploring 
Hyperlinks, contents and usage data. Springer, 
2006.
Liu, Bing. 2010. Sentiment analysis and subjectivity. 
Handbook of Natural Language Processing, 
second edition, 2010. 
Mei, Qiaozhu, Ling Xu, Matthew Wondra, Hang Su 
and ChengXiang Zhai. 2007. Topic Sentiment 
Mixture: Modeling Facets and Opinions in Web-
logs. In Proceedings of WWW, pages 171?180, 
2007.
Pang, Bo., Lillian Lee. 2008. Opinion Mining and 
Sentiment Analysis. Foundations and Trends in 
Information Retrieval  pp. 1-135 2008
Pantel, Patrick., Eric Crestan, Arkady Borkovsky, 
Ana-Maria  Popescu, Vishunu Vyas.  2009. Web-
Scale Distributional Similarity and Entity Set Ex-
pansion.  In Proceedings of. EMNLP, 2009 
Popescu, Ana-Maria and Oren, Etzioni. 2005. Ex-
tracting product features and opinions from re-
views. In Proceedings of EMNLP, 2005.
Qiu, Guang., Bing, Liu., Jiajun Bu and Chun Chen. 
2009. Expanding Domain Sentiment Lexicon 
through Double Propagation. In Proceedings of 
IJCAI 2009.
Rabiner, Lawrenence. 1989. A Tutorial on Hidden 
Markov Models and Selected Applications in 
Speech Recognition. In Proceedings of the IEEE, 
77(2), 1989.
Riloff, Ellen. 1993. Automatically Constructing a 
Dictionary for Information Extraction Tasks. In 
Proceedings of AAAI 1993.
Scaffidi, Christopher., Kevin Bierhoff, Eric Chang, 
Mikhael Felker, Herman Ng and Chun Jin. 2007. 
Red opal: Product-feature Scoring from Reviews. 
In Proceedings of EC 2007
Stoyanov, Veselin and Claire Cardie. 2008. Topic 
Identification for Fine-grained Opinion Analysis. 
In Proceedings of COLING 2008 
Su, Qi., Xinying Xu., Honglei Guo, Zhili Guo, Xian 
Wu, Xiaoxun Zhang, Bin Swen and Zhong Su. 
2008. Hidden Sentiment Association in Chinese 
Web Opinion Mining. In Proceedings of WWW 
2008.
Wang, Bo., Houfeng Wang. 2008.  Bootstrapping 
both Product Features and Opinion Words from 
Chinese Customer Reviews with Cross-Inducing  
In Proceedings of IJCNLP 2008 
Wilson, Theresa., Janyce Wiebe and Paul Hoffmann. 
2005. Recognizing Contextual Polarity in Phrase-
Level Sentiment Analysis. In Proceedings of 
HLT/EMNLP 2005
Wong, Tak-Lam., Wai Lam and Tik-Sun Wong.  
2008. An Unsupervised Framework for Extracting 
and Normalizing Product Attributes from Multiple 
Web Sites In Proceedings of SIGIR 2008
Zhuang, Li., Feng Jing, Xiao-yan Zhu. 2006. Movie 
Review Mining and Summarization. In Proceed-
ings of CIKM 2006
1470
Proceedings of the Demonstrations at the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 9?12,
Gothenburg, Sweden, April 26-30 2014.
c?2014 Association for Computational Linguistics
XLike Project Language Analysis Services
Xavier Carreras
?
, Llu??s Padr
?
o
?
, Lei Zhang
?
, Achim Rettinger
?
, Zhixing Li
1
,
Esteban Garc??a-Cuesta

,
?
Zeljko Agi
?
c
?
, Bo?zo Bekavac
/
, Blaz Fortuna
?
, Tadej
?
Stajner
?
? Universitat Polit`ecnica de Catalunya, Barcelona, Spain.  iSOCO S.A. Madrid, Spain.
/ University of Zagreb, Zagreb, Croatia. ? University of Potsdam, Germany.
? Jo?zef Stefan Institute, Ljubljana, Slovenia. 1 Tsinghua University, Beijing, China.
? Karlsruhe Institute of Technology, Karlsruhe, Germany.
Abstract
This paper presents the linguistic analysis
infrastructure developed within the XLike
project. The main goal of the imple-
mented tools is to provide a set of func-
tionalities supporting the XLike main ob-
jectives: Enabling cross-lingual services
for publishers, media monitoring or de-
veloping new business intelligence appli-
cations. The services cover seven major
and minor languages: English, German,
Spanish, Chinese, Catalan, Slovenian, and
Croatian. These analyzers are provided
as web services following a lightweigth
SOA architecture approach, and they are
publically accessible and shared through
META-SHARE.
1
1 Introduction
Project XLike
2
goal is to develop technology able
to gather documents in a variety of languages and
genres (news, blogs, tweets, etc.) and to extract
language-independent knowledge from them, in
order to provide new and better services to pub-
lishers, media monitoring, and business intelli-
gence. Thus, project use cases are provided by
STA (Slovenian Press Agency) and Bloomberg, as
well as New York Times as an associated partner.
Research partners in the project are Jo?zef Ste-
fan Institute (JSI), Karlsruhe Institute of Technol-
ogy (KIT), Universitat Polit`ecnica de Catalunya
(UPC), University of Zagreb (UZG), and Tsinghua
University (THU). The Spanish company iSOCO
is in charge of integration of all components de-
veloped in the project.
This paper deals with the language technology
developed within the project XLike to convert in-
1
accessible and shared here means that the services are
publicly callable, not that the code is open-source.
http://www.meta-share.eu
2
http://www.xlike.org
put documents into a language-independent rep-
resentation that afterwards enables knowledge ag-
gregation.
To achieve this goal, a bench of linguistic pro-
cessing pipelines is devised as the first step in the
document processing flow. Then, a cross-lingual
semantic annotation method, based on Wikipedia
and Linked Open Data (LOD), is applied. The
semantic annotation stage enriches the linguistic
anaylsis with links to knowledge bases for differ-
ent languages, or links to language independent
representations.
2 Linguistic Analyzers
Apart from basic state-of-the-art tokenizers, lem-
matizers, PoS/MSD taggers, and NE recogniz-
ers, each pipeline requires deeper processors able
to build the target language-independent seman-
tic representantion. For that, we rely on three
steps: dependency parsing, semantic role label-
ing and word sense disambiguation. These three
processes, combined with multilingual ontologi-
cal resouces such as different WordNets and Pred-
icateMatrix (L?opez de la Calle et al., 2014), a
lexical semantics resource combining WordNet,
FrameNet, and VerbNet, are the key to the con-
struction of our semantic representation.
2.1 Dependency Parsing
We use graph-based methods for dependency
parsing, namely, MSTParser
3
(McDonald et al.,
2005) is used for Chinese and Croatian, and
Treeler
4
is used for the other languages. Treeler is
a library developed by the UPC team that imple-
ments several statistical methods for tagging and
parsing.
We use these tools in order to train dependency
parsers for all XLike languages using standard
available treebanks.
3
http://sourceforge.net/projects/mstparser
4
http://treeler.lsi.upc.edu
9
2.2 Semantic Role Labeling
As with syntactic parsing, we are developing SRL
methods with the Treeler library. In order to train
models, we will use the treebanks made available
by the CoNLL-2009 shared task, which provided
data annotated with predicate-argument relations
for English, Spanish, Catalan, German and Chi-
nese. No treebank annotated with semantic roles
exists for Slovene or Croatian. A prototype of
SRL has been integrated in all pipelines (except
the Slovene and Croatian pipelines). The method
implemented follows a pipeline architecture de-
scribed in (Llu??s et al., 2013).
2.3 Word Sense Disambiguation
Word sense disambiguation is performed for all
languages with a publicly available WordNet. This
includes all languages in the project except Chi-
nese. The goal of WSD is to map specific lan-
guages to a common semantic space, in this case,
WN synsets. Thanks to existing connections be-
tween WN and other resources, SUMO and Open-
CYC sense codes are also output when available.
Thanks to PredicateMatrix, the obtained con-
cepts can be projected to FrameNet, achieving a
normalization of the semantic roles produced by
the SRL (which are treebank-dependent, and thus,
not the same for all languages). The used WSD
engine is the UKB (Agirre and Soroa, 2009) im-
plementation provided by FreeLing (Padr?o and
Stanilovsky, 2012).
2.4 Frame Extraction
The final step is to convert all the gathered linguis-
tic information into a semantic representation. Our
method is based on the notion of frame: a seman-
tic frame is a schematic representation of a situ-
ation involving various participants. In a frame,
each participant plays a role. There is a direct cor-
respondence between roles in a frame and seman-
tic roles; namely, frames correspond to predicates,
and participants correspond to the arguments of
the predicate. We distinguish three types of par-
ticipants: entities, words, and frames.
Entities are nodes in the graph connected to
real-world entities as described in Section 3.
Words are common words or concepts, linked to
general ontologies such as WordNet. Frames cor-
respond to events or predicates described in the
document. Figure 1 shows an example sentence,
the extracted frames and their arguments.
It is important to note that frames are a more
general representation than SVO-triples. While
SVO-triples represent a binary relation between
two participants, frames can represent n-ary rela-
tions (e.g. predicates with more than two argu-
ments, or with adjuncts). Frames also allow repre-
senting the sentences where one of the arguments
is in turn a frame (as is the case with plan to make
in the example).
Finally, although frames are extracted at sen-
tence level, the resulting graphs are aggregated
in a single semantic graph representing the whole
document via a very simple coreference resolution
based on detecting named entity aliases and repe-
titions of common nouns. Future improvements
include using an state-of-the-art coreference reso-
lution module for languages where it is available.
3 Cross-lingual Semantic Annotation
This step adds further semantic annotations on top
of the results obtained by linguistic processing.
All XLike languages are covered. The goal is
to map word phrases in different languages into
the same semantic interlingua, which consists of
resources specified in knowledge bases such as
Wikipedia and Linked Open Data (LOD) sources.
Cross-lingual semantic annotation is performed in
two stages: (1) first, candidate concepts in the
knowledge base are linked to the linguistic re-
sources based on a newly developed cross-lingual
linked data lexica, called xLiD-Lexica, (2) next
the candidate concepts get disambiguated based
on the personalized PageRank algorithm by utiliz-
ing the structure of information contained in the
knowledge base.
The xLiD-Lexica is stored in RDF format and
contains about 300 million triples of cross-lingual
groundings. It is extracted from Wikipedia dumps
of July 2013 in English, German, Spanish, Cata-
lan, Slovenian and Chinese, and based on the
canonicalized datasets of DBpedia 3.8 contain-
ing triples extracted from the respective Wikipedia
whose subject and object resource have an equiv-
alent English article.
4 Web Service Architecture Approach
The different language functionalities are imple-
mented following the service oriented architec-
ture (SOA) approach defined in the project XLike.
Therefore all the pipelines (one for each language)
have been implemented as web services and may
10
Figure 1: Graphical representation of frames in the sentence Acme, based in New York, now plans to
make computer and electronic products.
be requested to produce different levels of analy-
sis (e.g. tokenization, lemmatization, NERC, pars-
ing, relation extraction). This approach is very ap-
pealing due to the fact that it allows to treat ev-
ery language independently and execute the whole
language analysis process at different threads or
computers allowing an easier parallelization (e.g.
using external high perfomance platforms such as
Amazon Elastic Compute Cloud EC2
5
) as needed.
Furthermore it also provides independent develop-
ment lifecycles for each language which is crucial
in this type of research projects. Recall that these
web services can be deployed locally or remotely,
maintaining the option of using them in a stand-
alone configuration.
The main structure for each one of the pipelines
is described below:
? Spanish, English, and Catalan: all mod-
ules are based on FreeLing (Padr?o and
Stanilovsky, 2012) and Treeler.
? German: German shallow processing is
based on OpenNLP
6
, Stanford POS tagger
and NE extractor (Toutanova et al., 2003;
Finkel et al., 2005). Dependency parsing,
semantic role labeling, word sense disam-
biguation, and SRL-based frame extraction
are based on FreeLing and Treeler.
? Slovene: Slovene shallow processing is pro-
vided by JSI Enrycher
7
(
?
Stajner et al., 2010),
which consists of the Obeliks morphosyntac-
tic analysis library (Gr?car et al., 2012), the
LemmaGen lemmatizer (Jur?si?c et al., 2010)
and a CRF-based entity extractor (
?
Stajner et
al., 2012). Dependency parsing, word sense
5
http://aws.amazon.com/ec2/
6
http://opennlp.apache.org
7
http://enrycher.ijs.si
disambiguation are based on FreeLing and
Treeler. Frame extraction is rule-based since
no SRL corpus is available for Slovene.
? Croatian: Croatian shallow processing is
based on proprietary tokenizer, POS/MSD-
tagging and lemmatisaton system (Agi?c et
al., 2008), NERC system (Bekavac and
Tadi?c, 2007) and dependency parser (Agi?c,
2012). Word sense disambiguation is based
on FreeLing. Frame extraction is rule-based
since no SRL corpus is available for Croatian.
? Chinese: Chinese shallow and deep process-
ing is based on a word segmentation compo-
nent ICTCLAS
8
and a semantic dependency
parser trained on CSDN corpus. Then, rule-
based frame extraction is performed (no SRL
corpus nor WordNet are available for Chi-
nese).
Each language analysis service is able to pro-
cess thousands of words per second when per-
forming shallow analysis (up to NE recognition),
and hundreds of words per second when produc-
ing the semantic representation based on full anal-
ysis. Moreover, the web service architecture en-
ables the same server to run a different thread for
each client, thus taking advantage of multiproces-
sor capabilities.
The components of the cross-lingual semantic
annotation stage are:
? xLiD-Lexica: The cross-lingual groundings
in xLiD-Lexica are translated into RDF data
and are accessible through a SPARQL end-
point, based on OpenLink Virtuoso
9
as the
back-end database engine.
8
http://ictclas.org/
9
http://virtuoso.openlinksw.com/
11
? Semantic Annotation: The cross-lingual se-
mantic annotation service is based on the
xLiD-Lexica for entity mention recognition
and the JUNG Framework
10
for graph-based
disambiguation.
5 Conclusion
We presented the web service based architecture
used in XLike FP7 project to linguistically ana-
lyze large amounts of documents in seven differ-
ent languages. The analysis pipelines perform ba-
sic processing as tokenization, PoS-tagging, and
named entity extraction, as well as deeper analy-
sis such as dependency parsing, word sense disam-
biguation, and semantic role labelling. The result
of these linguistic analyzers is a semantic graph
capturing the main events described in the docu-
ment and their core participants.
On top of that, the cross-lingual semantic an-
notation component links the resulting linguistic
resources in one language to resources in a knowl-
edge bases in any other language or to language
independent representations. This semantic repre-
sentation is later used in XLike for document min-
ing purposes such as enabling cross-lingual ser-
vices for publishers, media monitoring or devel-
oping new business intelligence applications.
The described analysis services are currently
available via META-SHARE as callable RESTful
services.
Acknowledgments
This work was funded by the European Union
through project XLike (FP7-ICT-2011-288342).
References
?
Zeljko Agi?c, Marko Tadi?c, and Zdravko Dovedan.
2008. Improving part-of-speech tagging accuracy
for Croatian by morphological analysis. Informat-
ica, 32(4):445?451.
?
Zeljko Agi?c. 2012. K-best spanning tree dependency
parsing with verb valency lexicon reranking. In Pro-
ceedings of COLING 2012: Posters, pages 1?12,
Mumbai, India, December. The COLING 2012 Or-
ganizing Committee.
Eneko Agirre and Aitor Soroa. 2009. Personalizing
pagerank for word sense disambiguation. In Pro-
ceedings of the 12th conference of the European
chapter of the Association for Computational Lin-
guistics (EACL-2009), Athens, Greece.
10
Java Universal Network/Graph Framework
http://jung.sourceforge.net/
Bo?zo Bekavac and Marko Tadi?c. 2007. Implementa-
tion of Croatian NERC system. In Proceedings of
the Workshop on Balto-Slavonic Natural Language
Processing (BSNLP2007), Special Theme: Informa-
tion Extraction and Enabling Technologies, pages
11?18. Association for Computational Linguistics.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local informa-
tion into information extraction systems by gibbs
sampling. In Proceedings of the 43rd Annual Meet-
ing on Association for Computational Linguistics
(ACL?05), pages 363?370.
Miha Gr?car, Simon Krek, and Kaja Dobrovoljc. 2012.
Obeliks: statisti?cni oblikoskladenjski ozna?cevalnik
in lematizator za slovenski jezik. In Zbornik Osme
konference Jezikovne tehnologije, Ljubljana, Slove-
nia.
Matjaz Jur?si?c, Igor Mozeti?c, Tomaz Erjavec, and Nada
Lavra?c. 2010. Lemmagen: Multilingual lemmati-
sation with induced ripple-down rules. Journal of
Universal Computer Science, 16(9):1190?1214.
Xavier Llu??s, Xavier Carreras, and Llu??s M`arquez.
2013. Joint arc-factored parsing of syntactic and se-
mantic dependencies. Transactions of the Associa-
tion for Computational Linguistics, 1:219?230.
Maddalen L?opez de la Calle, Egoitz Laparra, and Ger-
man Rigau. 2014. First steps towards a predicate
matrix. In Proceedings of the Global WordNet Con-
ference (GWC 2014), Tartu, Estonia, January. GWA.
Ryan McDonald, Koby Crammer, and Fernando
Pereira. 2005. Online large-margin training of
dependency parsers. In Proceedings of the 43rd
Annual Meeting of the Association for Computa-
tional Linguistics (ACL?05), pages 91?98, Ann Ar-
bor, Michigan, June.
Llu??s Padr?o and Evgeny Stanilovsky. 2012. Freeling
3.0: Towards wider multilinguality. In Proceedings
of the Language Resources and Evaluation Confer-
ence (LREC 2012), Istanbul, Turkey, May. ELRA.
Tadej
?
Stajner, Delia Rusu, Lorand Dali, Bla?z Fortuna,
Dunja Mladeni?c, and Marko Grobelnik. 2010. A
service oriented framework for natural language text
enrichment. Informatica, 34(3):307?313.
Kristina Toutanova, Dan Klein, Christopher D. Man-
ning, and Yoram Singer. 2003. Feature-rich part-of-
speech tagging with a cyclic dependency network.
In Proceedings of the 2003 Conference of the North
American Chapter of the Association for Computa-
tional Lin- guistics on Human Language Technology
(NAACL?03).
Tadej
?
Stajner, Toma?z Erjavec, and Simon Krek.
2012. Razpoznavanje imenskih entitet v slovenskem
besedilu. In In Proceedings of 15th Internation
Multiconference on Information Society - Jezikovne
Tehnologije, Ljubljana, Slovenia.
12
Proceedings of the Demonstrations at the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 13?16,
Gothenburg, Sweden, April 26-30 2014.
c
?2014 Association for Computational Linguistics
Semantic Annotation, Analysis and Comparison:
A Multilingual and Cross-lingual Text Analytics Toolkit
Lei Zhang
Institute AIFB
Karlsruhe Institute of Technology
76128 Karlsruhe, Germany
l.zhang@kit.edu
Achim Rettinger
Institute AIFB
Karlsruhe Institute of Technology
76128 Karlsruhe, Germany
rettinger@kit.edu
Abstract
Within the context of globalization,
multilinguality and cross-linguality for
information access have emerged as issues
of major interest. In order to achieve
the goal that users from all countries
have access to the same information,
there is an impending need for systems
that can help in overcoming language
barriers by facilitating multilingual and
cross-lingual access to data. In this
paper, we demonstrate such a toolkit,
which supports both service-oriented and
user-oriented interfaces for semantically
annotating, analyzing and comparing
multilingual texts across the boundaries
of languages. We conducted an extensive
user study that shows that our toolkit
allows users to solve cross-lingual entity
tracking and article matching tasks more
efficiently and with higher accuracy
compared to the baseline approach.
1 Introduction
Automatic text understanding has been an
unsolved research problem for many years. This
partially results from the dynamic and diverging
nature of human languages, which results in many
different varieties of natural language. These
variations range from the individual level, to
regional and social dialects, and up to seemingly
separate languages and language families.
In recent years there have been considerable
achievements in approaches to computational
linguistics exploiting the information across
languages. This progress in multilingual and
cross-lingual text analytics is largely due
to the increased availability of multilingual
knowledge bases such as Wikipedia, which helps
at scaling the traditionally monolingual tasks
to multilingual and cross-lingual applications.
From the application side, there is a clear need
for multilingual and cross-lingual text analytics
technologies and services.
Text analytics in this work is defined as
three tasks: (i) semantic annotation by linking
entity mentions in the documents to their
corresponding representations in the knowledge
base; (ii) semantic analysis by linking the
documents by topics to the relevant resources in
the knowledge base; (iii) semantic comparison
by measuring semantic relatedness between
documents. While multilingual text analytics
addresses these tasks for multiple languages,
cross-lingual text analytics goes one step beyond,
as it faces these tasks across the boundaries of
languages, i.e., the text to be processed and the
resources in the knowledge base, or the documents
to be compared, are in different languages.
Due to the ever growing richness of its
content, Wikipedia has been increasingly gaining
attention as a precious knowledge base that
contains an enormous number of entities and
topics in diverse domains. In addition, Wikipedia
pages that provide information about the same
concept in different languages are connected
through cross-language links. Therefore, we use
Wikipedia as the central knowledge base.
With the goal of overcoming language barriers,
we would like to demonstrate our multilingual
and cross-lingual text analytics toolkit, which
supports both service-oriented and user-oriented
interfaces for semantically annotating, analyzing
and comparing multilingual texts across the
boundaries of languages.
2 Techniques
In this section, we first present the techniques
behind our toolkit w.r.t. its three components:
semantic annotation (Sec. 2.1), semantic analysis
and semantic comparison (Sec. 2.2).
13
2.1 Wikipedia-based Annotation
The process of augmenting phrases in text with
links to their corresponding Wikipedia articles
(in the sense of Wikipedia-based annotation) is
known as wikification. There is a large body
of work that links phrases in unstructured text
to relevant Wikipedia articles. While Mihalcea
and Csomai (Mihalcea and Csomai, 2007) met
the challenge of wikification by using link
probabilities obtained from Wikipedia?s articles
and by a comparison of features extracted from the
context of the phrases, Milne and Witten (Milne
and Witten, 2008) could improve the wikification
service significantly by viewing wikification even
more as a supervised machine learning task:
Wikipedia is used here not only as a source of
information to point to, but also as training data
to find always the appropriate link.
For multilingual semantic annotation, we
adopted the wikification system in (Milne and
Witten, 2008) and trained it for each language
using the corresponding Wikipedia version. To
perform cross-lingual semantic annotation, we
extended the wikification system by making use of
the cross-language links in Wikipedia to find the
corresponding Wikipedia articles in the different
target languages. More details can be found in our
previous work (Zhang et al., 2013).
2.2 Explicit Semantic Analysis
Explicit Semantic Analysis (ESA) has been
proposed as an approach for semantic modeling
of natural language text (Gabrilovich and
Markovitch, 2006). Based on a given set of
concepts with textual descriptions, ESA defines
the concept-based representation of documents.
Various sources for concept definitions have
been used, such as Wikipedia and Reuters Corpus.
Using the concept-based document representation,
ESA has been successfully applied to compute
semantic relatedness between texts (Gabrilovich
and Markovitch, 2007). In the context of the
cross-language information retrieval (CLIR) task,
ESA has been extended to a cross-lingual setting
(CL-ESA) by mapping the semantic document
representation from a concept space of one
language to an interlingual concept space (Sorg
and Cimiano, 2008).
The semantic analysis and semantic comparison
components of our toolkit are based on CL-ESA
in (Sorg and Cimiano, 2008). The semantic
Figure 2: Architecture of our Toolkit.
analysis component takes as input a document in a
source language and maps it to a high-dimensional
vector in the interlingual concept space, such
that each dimension corresponds to an Wikipedia
article in any target language acting as a
concept. For semantic comparison, the documents
in different languages are first translated into
vectors in the interlingual concept space and then
the cross-lingual semantic relatedness between
the documents in different languages can be
calculated using the standard similarity measure
between the resulting vectors.
3 Implementation
Our multilingual and cross-lingual toolkit is
implemented using a client-server architecture
with communication over HTTP using a XML
schema defined in XLike project
1
. The server
is a RESTful web service and the client user
interface is implemented using Adobe Flex as
both Desktop and Web Applications. The
toolkit can easily be extended or adapted to
switch out the server or client. In this way, it
supports both service-oriented and user-oriented
interfaces for semantically annotating, analyzing
and comparing multilingual texts across the
boundaries of languages. The architecture of our
toolkit is shown in Figure 2.
For all three components, namely semantic
annotation, analysis and comparison, we use
Wikipedia as the central knowledge base. Table 1
shows the statistics of the Wikipedia articles in
English, German, Spanish and French as well as
the cross-language links between the them in these
languages extracted from Wikipedia snapshots of
May 2012
2
, which are used to build our toolkit.
We now describe the user interfaces of these
1
http://www.xlike.org/
2
http://dumps.wikimedia.org/
14
Figure 1: Screenshot of the Semantic Annotation Component of our Toolkit.
English (EN) German (DE) Spanish (ES) French (FR)
#Articles 4,014,643 1,438,325 896,691 1,234,567
(a) Number of articles.
EN-DE EN-ES EN-FR DE-ES DE-FR ES-FR
#Links (?) 721,878 568,210 779,363 295,415 455,829 378,052
#Links (?) 718,401 581,978 777,798 302,502 457,306 370,552
#Links (merged) 722,069 593,571 795,340 307,130 464,628 383,851
(b) Number of cross-language links.
Table 1: Statistics about Wikipedia.
components. Due to the lack of space, we only
show the screenshot of the semantic annotation
component in Figure 1. The semantic annotation
component allows the users to find the entities
in Wikipedia mentioned in the input document.
Given the input document in one language, the
users can select the output language, namely
the language of Wikipedia articles describing
the mentioned entities. In the left pie chart,
the users can see the percentage of Wikipedia
articles in different languages as annotations of the
input document. According to their weights, the
Wikipedia articles in each language are organized
in 3 relevance categories: high, medium and low.
In the middle bar chart, the number of Wikipedia
articles in each language and in each category
is illustrated. The right data grid provides the
Wikipedia article titles with their weights in the
output language and the mentions in the input
document. Clicking an individual title opens
the corresponding Wikipedia article in the output
language. The semantic analysis component
has the similar user interface as the semantic
annotation component. The difference is that the
Wikipedia articles listed in the right data grid are
topically relevant to the input documents instead
of being mentioned as entities. Regarding the user
interface of semantic comparison component, the
main inputs are two documents that might be in
different languages and the output is the semantic
relatedness between them.
4 User Study
We conducted a task-based user study and the
goal is to assess the effectiveness and usability of
our multilingual and cross-lingual text analytics
toolkit. We design two tasks reflecting the real-life
information needs, namely entity tracking and
article matching, to assess the functionality of
our toolkit from different perspectives. The entity
tracking task is to detect mentions of the given
entities in the articles, where the descriptions
of the entities and the articles are in different
languages. Given articles in one language as
context, the article matching task is to find the
most similar articles in another language.
The participants of our user study are 16
volunteers and each of them got both tasks, which
they had to solve in two ways: (1) using a major
online machine translation service as baseline
and (2) using our multilingual and cross-lingual
text analytics toolkit with all the functionality.
For each task, we randomly selected 10 parallel
articles in English, French and Spanish from the
JRC-Acquis parallel corpus
3
. After a survey,
3
http://langtech.jrc.it/JRC-Acquis.
html
15
(a) Avg. successrate per task / method
(b) Avg. time spent per task / method
Figure 3: Evaluation Results of the User Study.
we decided to provide the entity descriptions for
entity tracking task and the context documents
for article matching task in English, which all
participants can speak. Regarding the articles to
be processed, we set up the tasks using Spanish
articles for the participants who do not know
Spanish, and tasks with French articles for the
participants who cannot speak French.
To measure the overall effectiveness of our
toolkit, we have analysed the ratio of tasks that
were completed successfully and correctly and the
time the participants required for the tasks. The
average success rate and time spent per task and
per method are illustrated in Figure 3. For entity
tracking task, we observe that a success rate of
80% was achieved using our toolkit in comparison
with the success rate of 70% yielded by using the
baseline. In addition, there is a significant gap
between the time spent using different methods.
While it took 21.5 minutes on average to solve
the task using the baseline, only 6.75 minutes
were needed when using our toolkit. Regarding
the article matching task, both methods performed
very well. Using our toolkit obtained a slightly
higher success rate of 99% than 94% using the
baseline. The time spent using both methods is not
so different. The participants spent 15.75 minutes
on average using the baseline while 2 minutes less
were needed using our toolkit.
In terms of the user study, our toolkit is
more effective than the baseline for both entity
tracking and article matching tasks. Therefore,
we conclude that our toolkit provides useful
functionality to make searching entities, analyzing
and comparing articles more easily and accurately
in the multilingual and cross-lingual scenarios.
Acknowledgments
This work was supported by the European
Community?s Seventh Framework Programme
FP7-ICT-2011-7 (XLike, Grant 288342).
References
[Gabrilovich and Markovitch2006] Evgeniy
Gabrilovich and Shaul Markovitch. 2006.
Overcoming the Brittleness Bottleneck using
Wikipedia: Enhancing Text Categorization with
Encyclopedic Knowledge. In AAAI, pages
1301?1306.
[Gabrilovich and Markovitch2007] Evgeniy
Gabrilovich and Shaul Markovitch. 2007.
Computing semantic relatedness using
wikipedia-based explicit semantic analysis. In
Proceedings of the 20th international joint
conference on artificial intelligence, volume 6,
page 12.
[Mihalcea and Csomai2007] Rada Mihalcea and
Andras Csomai. 2007. Wikify!: linking documents
to encyclopedic knowledge. In In CIKM ?07:
Proceedings of the sixteenth ACM conference
on Conference on information and knowledge
management, pages 233?242. ACM.
[Milne and Witten2008] David Milne and Ian H.
Witten. 2008. Learning to link with wikipedia.
In Proceedings of the 17th ACM conference on
Information and knowledge management, CIKM
?08, pages 509?518, New York, NY, USA. ACM.
[Sorg and Cimiano2008] P. Sorg and P. Cimiano. 2008.
Cross-lingual Information Retrieval with Explicit
Semantic Analysis. In Working Notes of the Annual
CLEF Meeting.
[Zhang et al.2013] Lei Zhang, Achim Rettinger,
Michael Frber, and Marko Tadic. 2013. A
comparative evaluation of cross-lingual text
annotation techniques. In CLEF, pages 124?135.
16
Proceedings of the ACL 2010 Conference Short Papers, pages 359?364,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Distributional Similarity vs. PU Learning for Entity Set Expansion 
 
 
Xiao-Li  Li 
Institute for Infocomm Research,  
1 Fusionopolis Way #21-01 Connexis 
Singapore 138632 
xlli@i2r.a-star.edu.sg 
Lei Zhang 
University of Illinois at Chicago,  
851 South Morgan Street, Chicago, 
Chicago, IL 60607-7053, USA 
zhang3@cs.uic.edu 
 
Bing Liu 
University of Illinois at Chicago,  
851 South Morgan Street, Chicago, 
Chicago, IL 60607-7053, USA 
liub@cs.uic.edu 
See-Kiong  Ng 
Institute for Infocomm Research,  
1 Fusionopolis Way #21-01 Connexis 
Singapore 138632 
skng@i2r.a-star.edu.sg 
 
Abstract 
Distributional similarity is a classic tech-
nique for entity set expansion, where the 
system is given a set of seed entities of a 
particular class, and is asked to expand the 
set using a corpus to obtain more entities 
of the same class as represented by the 
seeds. This paper shows that a machine 
learning model called positive and unla-
beled learning (PU learning) can model 
the set expansion problem better. Based 
on the test results of 10 corpora, we show 
that a PU learning technique outperformed 
distributional similarity significantly.   
1 Introduction 
The entity set expansion problem is defined as 
follows: Given a set S of seed entities of a partic-
ular class, and a set D of candidate entities (e.g., 
extracted from a text corpus), we wish to deter-
mine which of the entities in D belong to S. In 
other words, we ?expand? the set S based on the 
given seeds. This is clearly a classification prob-
lem which requires arriving at a binary decision 
for each entity in D (belonging to S or not). 
However, in practice, the problem is often solved 
as a ranking problem, i.e., ranking the entities in 
D based on their likelihoods of belonging to S.  
The classic method for solving this problem is 
based on distributional similarity (Pantel et al 
2009; Lee, 1998). The approach works by com-
paring the similarity of the surrounding word 
distributions of each candidate entity with the 
seed entities, and then ranking the candidate enti-
ties using their similarity scores.   
In machine learning, there is a class of semi-
supervised learning algorithms that learns from 
positive and unlabeled examples (PU learning for 
short). The key characteristic of PU learning is 
that there is no negative training example availa-
ble for learning. This class of algorithms is less 
known to the natural language processing (NLP) 
community compared to some other semi-
supervised learning models and algorithms.  
PU learning is a two-class classification mod-
el. It is stated as follows (Liu et al 2002): Given 
a set P of positive examples of a particular class 
and a set U of unlabeled examples (containing 
hidden positive and negative cases), a classifier 
is built using P and U for classifying the data in 
U or future test cases. The results can be either 
binary decisions (whether each test case belongs 
to the positive class or not), or a ranking based 
on how likely each test case belongs to the posi-
tive class represented by P. Clearly, the set ex-
pansion problem can be mapped into PU learning 
exactly, with S and D as P and U respectively. 
This paper shows that a PU learning method 
called S-EM (Liu et al 2002) outperforms distri-
butional similarity considerably based on the 
results from 10 corpora. The experiments in-
volved extracting named entities (e.g., product 
and organization names) of the same type or 
class as the given seeds. Additionally, we also 
compared S-EM with a recent method, called 
Bayesian Sets (Ghahramani and Heller, 2005), 
which was designed specifically for set expan-
sion. It also does not perform as well as PU 
learning. We will explain why PU learning per-
forms better than both methods in Section 5. We 
believe that this finding is of interest to the NLP 
community.  
359
There is another approach used in the Web 
environment for entity set expansion. It exploits 
Web page structures to identify lists of items us-
ing wrapper induction or other techniques. The 
idea is that items in the same list are often of the 
same type. This approach is used by Google Sets 
(Google, 2008) and Boo!Wa! (Wang and Cohen, 
2008). However, as it relies on Web page struc-
tures, it is not applicable to general free texts.  
2 Three Different Techniques  
2.1 Distributional Similarity 
Distributional similarity is a classic technique for 
the entity set expansion problem. It is based on 
the hypothesis that words with similar meanings 
tend to appear in similar contexts (Harris, 1985). 
As such, a method based on distributional simi-
larity typically fetches the surrounding contexts 
for each term (i.e. both seeds and candidates) and 
represents them as vectors by using TF-IDF or 
PMI (Pointwise Mutual Information) values (Lin, 
1998; Gorman and Curran, 2006; Pa?ca et al 
2006; Agirre et al 2009; Pantel et al 2009). Si-
milarity measures such as Cosine, Jaccard, Dice, 
etc, can then be employed to compute the simi-
larities between each candidate vector and the 
seeds centroid vector (one centroid vector for all 
seeds). Lee (1998) surveyed and discussed vari-
ous distribution similarity measures.  
2.2 PU Learning and S-EM 
PU learning is a semi-supervised or partially su-
pervised learning model. It learns from positive 
and unlabeled examples as opposed to the model 
of learning from a small set of labeled examples 
of every class and a large set of unlabeled exam-
ples, which we call LU learning (L and U stand 
for labeled and unlabeled respectively) (Blum 
and Mitchell, 1998; Nigam et al 2000)  
There are several PU learning algorithms (Liu 
et al 2002; Yu et al 2002; Lee and Liu, 2003; Li 
et al 2003; Elkan and Noto, 2008). In this work, 
we used the S-EM algorithm given in (Liu et al 
2002). S-EM is efficient as it is based on na?ve 
Bayesian (NB) classification and also performs 
well. The main idea of S-EM is to use a spy 
technique to identify some reliable negatives 
(RN) from the unlabeled set U, and then use an 
EM algorithm to learn from P, RN and U?RN.  
The spy technique in S-EM works as follows 
(Figure 1): First, a small set of positive examples 
(denoted by SP) from P is randomly sampled 
(line 2). The default sampling ratio in S-EM is s 
= 15%, which we also used in our experiments. 
The positive examples in SP are called ?spies?. 
Then, a NB classifier is built using the set P? SP 
as positive and the set U?SP as negative (line 3, 
4, and 5). The NB classifier is applied to classify 
each u ? U?SP, i.e., to assign a probabilistic 
class label p(+|u) (+ means positive). The proba-
bilistic labels of the spies are then used to decide 
reliable negatives (RN). In particular, a probabili-
ty threshold t is determined using the probabilis-
tic labels of spies in SP and the input parameter l 
(noise level). Due to space constraints, we are 
unable to explain l. Details can be found in (Liu 
et al 2002). t is then used to find RN from U 
(lines 8-10). The idea of the spy technique is 
clear. Since spy examples are from P and are put 
into U in building the NB classifier, they should 
behave similarly to the hidden positive cases in 
U. Thus, they can help us find the set RN.  
Algorithm Spy(P, U, s, l) 
1.  RN ? ?;            // Reliable negative set 
2.  SP ? Sample(P, s%); 
3.  Assign each example in P ? SP the class label +1; 
4.  Assign each example in U ? SP the class label -1; 
5.  C ?NB(P ? S, U?SP); // Produce a NB classifier  
6.  Classify each u ?U?SP using C; 
7.  Decide a probability threshold t using SP and l; 
8.  for each u ?U do 
9.       if its probability p(+|u) < t then 
10.          RN ? RN ? {u}; 
Figure 1. Spy technique for extracting reliable 
negatives (RN) from U. 
Given the positive set P, the reliable negative 
set RN and the remaining unlabeled set U?RN, an 
Expectation-Maximization (EM) algorithm is 
run. In S-EM, EM uses the na?ve Bayesian clas-
sification as its base method. The detailed algo-
rithm is given in (Liu et al 2002). 
2.3 Bayesian Sets 
Bayesian Sets, as its name suggests, is based on 
Bayesian inference, and was designed specifical-
ly for the set expansion problem (Ghahramani 
and Heller, 2005). The algorithm learns from a 
seeds set (i.e., a positive set P) and an unlabeled 
candidate set U. Although it was not designed as 
a PU learning method, it has similar characteris-
tics and produces similar results as PU learning. 
However, there is a major difference. PU learn-
ing is a classification model, while Bayesian Sets 
is a ranking method. This difference has a major 
implication on the results that they produce as we 
will discuss in Section 5.3.  
In essence, Bayesian Sets learns a score func-
360
tion using P and U to generate a score for each 
unlabeled case u ? U. The function is as follows:  
                    
)(
)|(
)(
up
Pup
uscore =  (1) 
where p(u|P) represents how probable u belongs 
to the positive class represented by P. p(u) is the 
prior probability of u. Using the Bayes? rule, eq-
uation (1) can be re-written as:              
               
)()(
),(
)(
Ppup
Pup
uscore =                    (2)  
Following the idea, Ghahramani and Heller 
(2005) proposed a computable score function. 
The scores can be used to rank the unlabeled 
candidates in U to reflect how likely each u ? U 
belongs to P. The mathematics for computing the 
score is involved. Due to the limited space, we 
cannot discuss it here. See (Ghahramani and Hel-
ler, 2005) for details. In (Heller and Ghahramani, 
2006), Bayesian Sets was also applied to an im-
age retrieval application.  
3 Data Generation for Distributional 
Similarity, Bayesian Sets and S-EM 
Preparing the data for distributional similarity is 
fairly straightforward. Given the seeds set S, a 
seeds centroid vector is produced using the sur-
rounding word contexts (see below) of all occur-
rences of all the seeds in the corpus (Pantel et al 
2009). In a similar way, a centroid is also pro-
duced for each candidate (or unlabeled) entity.  
Candidate entities: Since we are interested in 
named entities, we select single words or phrases 
as candidate entities based on their correspond-
ing part-of-speech (POS) tags. In particular, we 
choose the following POS tags as entity indica-
tors ? NNP (proper noun), NNPS (plural proper 
noun), and CD (cardinal number). We regard a 
phrase (could be one word) with a sequence of 
NNP, NNPS and CD POS tags as one candidate 
entity (CD cannot be the first word unless it 
starts with a letter), e.g., ?Windows/NNP 7/CD? 
and ?Nokia/NNP N97/CD? are regarded as two 
candidates ?Windows 7? and ?Nokia N97?. 
Context: For each seed or candidate occurrence, 
the context is its set of surrounding words within 
a window of size w, i.e. we use w words right 
before the seed or the candidate and w words 
right after it. Stop words are removed.  
For S-EM and Bayesian Sets, both the posi-
tive set P (based on the seeds set S) and the unla-
beled candidate set U are generated differently. 
They are not represented as centroids.  
Positive and unlabeled sets: For each seed si ?S, 
each occurrence in the corpus forms a vector as a 
positive example in P. The vector is formed 
based on the surrounding words context (see 
above) of the seed mention. Similarly, for each 
candidate d ? D (see above; D denotes the set of 
all candidates), each occurrence also forms a 
vector as an unlabeled example in U. Thus, each 
unique seed or candidate entity may produce 
multiple feature vectors, depending on the num-
ber of times that it appears in the corpus. 
The components in the feature vectors are 
term frequencies for S-EM as S-EM uses na?ve 
Bayesian classification as its base classifier. For 
Bayesian Sets, they are 1?s and 0?s as Bayesian 
Sets only takes binary vectors based on whether 
a term occurs in the context or not.  
4 Candidate Ranking 
For distributional similarity, ranking is done us-
ing the similarity value of each candidate?s cen-
troid and the seeds? centroid (one centroid vector 
for all seeds). Rankings for S-EM and Bayesian 
Sets are more involved. We discuss them below.  
After it ends, S-EM produces a Bayesian clas-
sifier C, which is used to classify each vector u ? 
U and to assign a probability p(+|u) to indicate 
the likelihood that u belongs to the positive class. 
Similarly, Bayesian Sets produces a score 
score(u) for each u (not a probability).  
Recall that for both S-EM and Bayesian Sets, 
each unique candidate entity may generate mul-
tiple feature vectors, depending on the number of 
times that the candidate entity occurs in the cor-
pus. As such, the rankings produced by S-EM 
and Bayesian Sets are not the rankings of the 
entities, but rather the rankings of the entities? 
occurrences. Since different vectors representing 
the same candidate entity can have very different 
probabilities (for S-EM) or scores (for Bayesian 
Sets), we need to combine them and compute a 
single score for each unique candidate entity for 
ranking.  
To this end, we also take the entity frequency 
into consideration. Typically, it is highly desira-
ble to rank those correct and frequent entities at 
the top because they are more important than the 
infrequent ones in applications. With this in 
mind, we define a ranking method. 
Let the probabilities (or scores) of a candidate 
entity d ? D be Vd = {v1 , v2 ?, vn} for the n fea-
ture vectors of the candidate. Let Md be the me-
dian of Vd. The final score (fs) for d is defined as:  
    )1log()( nMdfs d +?=         (3) 
361
The use of the median of Vd can be justified 
based on the statistical skewness (Neter et al 
1993). If the values in Vd are skewed towards the 
high side (negative skew), it means that the can-
didate entity is very likely to be a true entity, and 
we should take the median as it is also high 
(higher than the mean). However, if the skew is 
towards the low side (positive skew), it means 
that the candidate entity is unlikely to be a true 
entity and we should again use the median as it is 
low (lower than the mean) under this condition.  
Note that here n is the frequency count of 
candidate entity d in the corpus. The constant 1 is 
added to smooth the value. The idea is to push 
the frequent candidate entities up by multiplying 
the logarithm of frequency. log is taken in order 
to reduce the effect of big frequency counts. 
The final score fs(d) indicates candidate d?s 
overall likelihood to be a relevant entity. A high 
fs(d) implies a high likelihood that d is in the 
expanded entity set. We can then rank all the 
candidates based on their fs(d) values.  
5 Experimental Evaluation 
We empirically evaluate the three techniques in 
this section. We implemented distribution simi-
larity and Bayesian Sets. S-EM was downloaded 
from http://www.cs.uic.edu/~liub/S-EM/S-EM-
download.html. For both Bayesian Sets and S-
EM, we used their default parameters. EM in S-
EM ran only two iterations. For distributional 
similarity, we tested TF-IDF and PMI as feature 
values of vectors, and Cosine and Jaccard as si-
milarity measures. Due to space limitations, we 
only show the results of the PMI and Cosine 
combination as it performed the best. This com-
bination was also used in (Pantel et al, 2009). 
5.1 Corpora and Evaluation Metrics 
We used 10 diverse corpora to evaluate the tech-
niques. They were obtained from a commercial 
company. The data were crawled and extracted 
from multiple online message boards and blogs 
discussing different products and services. We 
split each message into sentences, and the sen-
tences were POS-tagged using Brill?s tagger 
(Brill, 1995). The tagged sentences were used to 
extract candidate entities and their contexts. Ta-
ble 1 shows the domains and the number of sen-
tences in each corpus, as well as the three seed 
entities used in our experiments for each corpus. 
The three seeds for each corpus were randomly 
selected from a set of common entities in the ap-
plication domain.  
Table 1. Descriptions of the 10 corpora 
Domains # Sentences Seed Entities 
 Bank 17394 Citi, Chase, Wesabe 
 Blu-ray 7093 S300, Sony, Samsung 
 Car 2095 Honda, A3, Toyota 
 Drug 1504 Enbrel, Hurmia, Methotrexate 
 Insurance 12419 Cobra, Cigna, Kaiser 
 LCD 1733 PZ77U, Samsung, Sony 
 Mattress 13191 Simmons, Serta, Heavenly 
 Phone 14884 Motorola, Nokia, N95 
 Stove 25060 Kenmore, Frigidaire, GE 
 Vacuum 13491 Dc17, Hoover, Roomba 
The regular evaluation metrics for named enti-
ty recognition such as precision and recall are not 
suitable for our purpose as we do not have the 
complete sets of gold standard entities to com-
pare with. We adopt rank precision, which is 
commonly used for evaluation of entity set ex-
pansion techniques (Pantel et al, 2009):  
Precision @ N: The percentage of correct enti-
ties among the top N entities in the ranked list.  
5.2 Experimental Results 
The detailed experimental results for window 
size 3 (w=3) are shown in Table 2 for the 10 cor-
pora. We present the precisions at the top 15-, 
30- and 45-ranked positions (i.e., precisions 
@15, 30 and 45) for each corpus, with the aver-
age given in the last column. For distributional 
similarity, to save space Table 2 only shows the 
results of Distr-Sim-freq, which is the distribu-
tional similarity method with term frequency 
considered in the same way as for Bayesian Sets 
and S-EM, instead of the original distributional 
similarity, which is denoted by Distr-Sim. This 
is because on average, Distr-Sim-freq performs 
better than Distr-Sim. However, the summary 
results of both Distr-Sim-freq and Distr-Sim are 
given in Table 3.  
From Table 2, we observe that on average S-
EM outperforms Distr-Sim-freq by about 12 ? 
20% in terms of Precision @ N. Bayesian-Sets 
is also more accurate than Distr-Sim-freq, but S-
EM outperforms Bayesian-Sets by 9 ? 10%. 
To test the sensitivity of window size w, we 
also experimented with w = 6 and w = 9. Due to 
space constraints, we present only their average 
results in Table 3. Again, we can see the same 
performance pattern as in Table 2 (w = 3): S-EM 
performs the best, Bayesian-Sets the second, and 
the two distributional similarity methods the 
third and the fourth, with Distr-Sim-freq slightly 
better than Distr-Sim.  
362
5.3 Why does S-EM Perform Better? 
From the tables, we can see that both S-EM and 
Bayesian Sets performed better than distribution-
al similarity. S-EM is better than Bayesian Sets. 
We believe that the reason is as follows: Distri-
butional similarity does not use any information 
in the candidate set (or the unlabeled set U). It 
tries to rank the candidates solely through simi-
larity comparisons with the given seeds (or posi-
tive cases). Bayesian Sets is better because it 
considers U. Its learning method produces a 
weight vector for features based on their occur-
rence differences in the positive set P and the 
unlabeled set U (Ghahramani and Heller 2005). 
This weight vector is then used to compute the 
final scores used in ranking. In this way, Baye-
sian Sets is able to exploit the useful information 
in U that was ignored by distributional similarity. 
S-EM also considers these differences in its NB 
classification; in addition, it uses the reliable 
negative set (RN) to help distinguish negative 
and positive cases, which both Bayesian Sets and 
distributional similarity do not do. We believe 
this balanced attempt by S-EM to distinguish the 
positive and negative cases is the reason for the 
better performance of S-EM. This raises an inter-
esting question. Since Bayesian Sets is a ranking 
method and S-EM is a classification method, can 
we say even for ranking (our evaluation is based 
on ranking) classification methods produce better 
results than ranking methods? Clearly, our single 
experiment cannot answer this question. But in-
tuitively, classification, which separates positive 
and negative cases by pulling them towards two 
opposite directions, should perform better than 
ranking which only pulls the data in one direc-
tion. Further research on this issue is needed. 
6 Conclusions and Future Work 
Although distributional similarity is a classic 
technique for entity set expansion, this paper 
showed that PU learning performs considerably 
better on our diverse corpora. In addition, PU 
learning also outperforms Bayesian Sets (de-
signed specifically for the task). In our future 
work, we plan to experiment with various other 
PU learning methods (Liu et al 2003; Lee and 
Liu, 2003; Li et al 2007; Elkan and Noto, 2008) 
on this entity set expansion task, as well as other 
tasks that were tackled using distributional simi-
larity. In addition, we also plan to combine some 
syntactic patterns (Etzioni et al 2005; Sarmento 
et al 2007) to further improve the results.  
Acknowledgements: Bing Liu and Lei Zhang 
acknowledge the support of HP Labs Innovation 
Research Grant 2009-1062-1-A, and would like 
to thank Suk Hwan Lim and Eamonn O'Brien- 
Strain for many helpful discussions.   
Table 2.  Precision @ top N (with 3 seeds, and window size w = 3) 
 Bank Blu-ray Car  Drug Insurance LCD Mattress Phone Stove  Vacuum Avg. 
 Top 15 
Distr-Sim-freq 0.466 0.333 0.800 0.666 0.666 0.400 0.666 0.533 0.666 0.733 0.592
Bayesian-Sets 0.533 0.266 0.600 0.666 0.600 0.733 0.666 0.533 0.800 0.800 0.617
S-EM 0.600 0.733 0.733 0.733 0.533 0.666 0.933 0.533 0.800 0.933 0.720 
 Top 30 
Distr-Sim-freq 0.466 0.266 0.700 0.600 0.500 0.333 0.500 0.466 0.600 0.566 0.499 
Bayesian-Sets 0.433 0.300 0.633 0.666 0.400 0.566 0.700 0.333 0.833 0.700 0.556 
S-EM 0.500 0.700 0.666 0.666 0.566 0.566 0.733 0.600 0.600 0.833 0.643 
 Top 45 
Distr-Sim-freq 0.377 0.288 0.555 0.500 0.377 0.355 0.444 0.400 0.533 0.400 0.422 
Bayesian-Sets 0.377 0.333 0.666 0.555 0.377 0.511 0.644 0.355 0.733 0.600 0.515 
S-EM 0.466 0.688 0.644 0.733 0.533 0.600 0.644 0.555 0.644 0.688 0.620 
Table 3. Average precisions over the 10 corpora of different window size (3 seeds) 
Window-size w = 3   Window-size  w = 6  Window-size  w = 9 
Top Results Top 15 Top 30 Top 45  Top 15 Top 30 Top 45  Top 15 Top 30 Top 45
Distr-Sim 0.579 0.466 0.410  0.553 0.483 0.439  0.519 0.473 0.412 
Distr-Sim-freq 0.592 0.499 0.422  0.553 0.492 0.441  0.559 0.476 0.410 
Bayesian-Sets 0.617 0.556 0.515  0.593 0.539 0.524  0.539 0.522 0.497 
S-EM 0.720 0.643 0.620  0.666 0.606 0.597  0.666 0.620 0.604 
 
363
References  
Agirre, E., Alfonseca, E., Hall, K., Kravalova, J., 
Pasca, M., and Soroa, A. 2009. A study on si-
milarity and relatedness using distributional 
and WordNet-based approaches. NAACL 
HLT.  
Blum, A. and Mitchell, T. 1998. Combining la-
beled and unlabeled data with co-training. In 
Proc. of Computational Learning Theory, pp. 
92?100, 1998. 
Brill, E. 1995. Transformation-Based error-
Driven learning and natural language 
processing: a case study in part of speech 
tagging. Computational Linguistics.  
Bunescu, R. and Mooney, R. 2004. Collective 
information extraction with relational Markov 
Networks. ACL.  
Cheng T., Yan X. and Chang C. K. 2007. Entity-
Rank: searching entities directly and holisti-
cally.  VLDB.  
Chieu, H.L. and Ng, H. Tou. 2002. Name entity 
recognition: a maximum entropy approach 
using global information. In The 6th Work-
shop on Very Large Corpora. 
Downey, D., Broadhead, M. and Etzioni, O. 
2007. Locating complex named entities in 
Web Text. IJCAI.  
Elkan, C. and Noto, K. 2008. Learning classifi-
ers from only positive and unlabeled data. 
KDD, 213-220.  
Etzioni, O., Cafarella, M., Downey. D., Popescu, 
A., Shaked, T., Soderland, S., Weld, D. Yates. 
2005. A. Unsupervised named-entity extrac-
tion from the Web: An Experimental Study. 
Artificial Intelligence, 165(1):91-134.  
Ghahramani, Z and Heller, K.A. 2005. Bayesian 
sets. NIPS.  
Google Sets. 2008.  System and methods for au-
tomatically creating lists. US Patent: 
US7350187, March 25. 
Gorman, J. and Curran, J. R. 2006. Scaling dis-
tributional similarity to large corpora. ACL. 
Harris, Z. Distributional Structure. 1985. In: 
Katz, J. J. (ed.), The philosophy of linguistics. 
Oxford University Press.  
Heller, K. and Ghahramani, Z. 2006. A simple 
Bayesian framework for content-based image 
retrieval. CVPR. 
Isozaki, H. and Kazawa, H. 2002. Efficient sup-
port vector classifiers for named entity recog-
nition. COLING.  
Jiang, J. and Zhai, C. 2006. Exploiting domain 
structure for named entity recognition.  HLT-
NAACL.  
Lafferty J., McCallum A., and Pereira F. 2001. 
Conditional random fields: probabilistic 
models for segmenting and labeling sequence 
data. ICML.  
Lee, L. 1999. Measures of distributional similar-
ity. ACL.  
Lee, W-S. and Liu, B. 2003. Learning with Posi-
tive and Unlabeled Examples Using Weighted 
Logistic Regression. ICML.  
Li, X., Liu, B. 2003. Learning to classify texts 
using positive and unlabeled data, IJCAI. 
Li, X., Liu, B., Ng, S. 2007. Learning to identify 
unexpected instances in the test sSet. IJCAI. 
Lin, D. 1998. Automatic retrieval and clustering 
of similar words. COLING/ACL. 
Liu, B, Lee, W-S, Yu, P. S, and Li, X. 2002. 
Partially supervised text classification. ICML, 
387-394. 
Liu, B, Dai, Y., Li, X., Lee, W-S., and Yu. P. 
2003. Building text classifiers using positive 
and unlabeled examples. ICDM, 179-188. 
Neter, J., Wasserman, W., and Whitmore, G. A. 
1993. Applied Statistics. Allyn and Bacon.  
Nigam, K., McCallum, A., Thrun, S. and Mit-
chell, T. 2000. Text classification from la-
beled and unlabeled documents using EM. 
Machine Learning, 39(2/3), 103?134.  
Pantel, P., Eric Crestan, Arkady Borkovsky, 
Ana-Maria Popescu, Vishnu, Vyas. 2009. 
Web-Scale Distributional similarity and entity 
set expansion, EMNLP.  
Pa?ca, M. Lin, D. Bigham, J. Lifchits, A. Jain, A. 
2006. Names and similarities on the web: fast 
extraction in the fast lane. ACL.  
Sarmento, L., Jijkuon, V. de Rijke, M. and 
Oliveira, E. 2007. ?More like these?: growing 
entity classes from seeds. CIKM. 
Wang, R. C. and Cohen, W. W. 2008. Iterative 
set expansion of named entities using the web. 
ICDM.  
Yu, H., Han, J., K. Chang. 2002. PEBL: Positive 
example based learning for Web page classi-
fication using SVM. KDD, 239-248. 
364
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 575?580,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Identifying Noun Product Features that Imply Opinions 
 
 
Lei Zhang Bing Liu 
University of Illinois at Chicago University of Illinois at Chicago 
851 South Morgan Street 851 South Morgan Street 
Chicago, IL 60607, USA Chicago, IL 60607, USA 
lzhang3@cs.uic.edu liub@cs.uic.edu 
 
 
 
 
 
 
Abstract 
Identifying domain-dependent opinion 
words is a key problem in opinion mining 
and has been studied by several researchers. 
However, existing work has been focused 
on adjectives and to some extent verbs. 
Limited work has been done on nouns and 
noun phrases. In our work, we used the 
feature-based opinion mining model, and we 
found that in some domains nouns and noun 
phrases that indicate product features may 
also imply opinions. In many such cases, 
these nouns are not subjective but objective. 
Their involved sentences are also objective 
sentences and imply positive or negative 
opinions. Identifying such nouns and noun 
phrases and their polarities is very 
challenging but critical for effective opinion 
mining in these domains. To the best of our 
knowledge, this problem has not been 
studied in the literature. This paper proposes 
a method to deal with the problem. 
Experimental results based on real-life 
datasets show promising results. 
1 Introduction 
Opinion words are words that convey positive or 
negative polarities. They are critical for opinion 
mining (Pang et al, 2002; Turney, 2002; Hu and 
Liu, 2004; Wilson et al, 2004; Popescu and 
Etzioni, 2005; Gamon et al, 2005; Ku et al, 2006; 
Breck et al, 2007; Kobayashi et al, 2007; Ding et 
al., 2008; Titov and McDonald, 2008; Pang and 
Lee, 2008; Lu et al, 2009). The key difficulty in 
finding such words is that opinions expressed by 
many of them are domain or context dependent.  
Several researchers have studied the problem of 
finding opinion words (Liu, 2010). The approaches 
can be grouped into corpus-based approaches 
(Hatzivassiloglou and McKeown, 1997; Wiebe, 
2000; Kanayama and Nasukawa, 2006; Qiu et al, 
2009) and dictionary-based approaches (Hu and 
Liu 2004; Kim and Hovy, 2004; Kamps et al, 
2004; Esuli and Sebastiani, 2005; Takamura et al, 
2005; Andreevskaia and Bergler, 2006; Dragut et 
al., 2010). Dictionary-based approaches are 
generally not suitable for finding domain specific 
opinion words as dictionaries contain little domain 
specific information.  
Hatzivassiloglou and McKeown (1997) did the 
first work to tackle the problem for adjectives 
using a corpus. The approach exploits some 
conjunctive patterns, involving and, or, but, either-
or, or neither-nor, with the intuition that the 
conjoining adjectives subject to linguistic 
constraints on the orientation or polarity of the 
adjectives involved. Using these constraints, one 
can infer opinion polarities of unknown adjectives 
based on the known ones. Kanayama and 
Nasukawa (2006) improved this work by using the 
idea of coherency. They deal with both adjectives 
and verbs. Ding et al (2008) introduced the 
concept of feature context because the polarities of 
many opinion bearing words are sentence context 
dependent rather than just domain dependent. Qiu 
et al (2009) proposed a method called double 
propagation that uses dependency relations to 
extract both opinion words and product features. 
575
However, none of these approaches handle nouns 
or noun phrases. Although Zagibalov and Carroll 
(2008) noticed the issue, they did not study it.  
Esuli and Sebastiani (2006) used WordNet to 
determine polarities of words, which can include 
nouns. However, dictionaries do not contain 
domain specific information.  
Our work uses the feature-based opinion mining 
model in (Hu and Liu, 2004) to mine opinions in 
product reviews. We found that in some 
application domains product features which are 
indicated by nouns have implied opinions although 
they are not subjective words.  
This paper aims to identify such opinionated 
noun features. To make this concrete, let us see an 
example from a mattress review: ?Within a month, 
a valley formed in the middle of the mattress.?  
Here ?valley? indicates the quality of the mattress 
(a product feature) and also implies a negative 
opinion. The opinion implied by ?valley? cannot 
be found by current techniques.  
Although Riloff et al (2003) proposed a method 
to extract subjective nouns, our work is very 
different because many nouns implying opinions 
are not subjective nouns, but objective nouns, e.g., 
?valley? and ?hole? on a mattress. Those sentences 
involving such nouns are usually also objective 
sentences. As much of the existing opinion mining 
research focuses on subjective sentences, we 
believe it is high time to study objective words and 
sentences that imply opinions as well. This paper 
represents a positive step towards this direction.  
 Objective words (or sentences) that imply 
opinions are very difficult to recognize because 
their recognition typically requires the 
commonsense or world knowledge of the 
application domain. In this paper, we propose a 
method to deal with the problem, specifically, 
finding product features which are nouns or noun 
phrases and imply positive or negative opinions. 
Our experimental results show promising results. 
2 The Proposed Method  
We start with some observations. For a product 
feature (or feature for short) with an implied 
opinion, there is either no adjective opinion word 
that modifies it directly or the opinion word that 
modify it usually have the same opinion.  
Example 1: No opinion adjective word modifies 
the opinionated product feature (?valley?):  
?Within a month, a valley formed in the middle 
of the mattress.?   
Example 2: An opinion adjective modifies the 
opinionated product feature: 
?Within a month, a bad valley formed in the 
middle of the mattress.?   
Here, the adjective ?bad? modifies ?valley?. It is 
unlikely that a positive opinion word will modify 
?valley?, e.g., ?good valley? in this context. Thus, 
if a product feature is modified by both positive 
and negative opinion adjectives, it is unlikely to be 
an opinionated product feature.  
Based on these examples, we designed the 
following two steps to identify noun product 
features which imply positive or negative opinions: 
1. Candidate Identification: This step determines 
the surrounding sentiment context of each noun 
feature. The intuition is that if a feature occurs 
in negative (respectively positive) opinion 
contexts significantly more frequently than in 
positive (or negative) opinion contexts, we can 
infer that its polarity is negative (or positive). A 
statistical test is used to test the significance. 
This step thus produces a list of candidate 
features with positive opinions and a list of 
candidate features with negative opinions.  
2. Pruning: This step prunes the two lists. The 
idea is that when a noun product feature is 
directly modified by both positive and negative 
opinion words, it is unlikely to be an 
opinionated product feature.  
Basically, step 1 needs the feature-based sentiment 
analysis capability. We adopt the lexicon-based 
approach in (Ding et al 2008) in this work.  
2.1 Feature-Based Sentiment Analysis  
To use the lexicon-based sentiment analysis 
method, we need a list of opinion words, i.e., an 
opinion lexicon. Opinion words are words that 
express positive or negative sentiments. As noted 
earlier, there are also many words whose polarities 
depend on the contexts in which they appear.  
Researchers have compiled sets of opinion 
words for adjectives, adverbs, verbs and nouns 
respectively, called the opinion lexicon. In this 
paper, we used the opinion lexicon complied by 
Ding et al (2008). It is worth mentioning that our 
task is to find nouns which imply opinions in a 
specific domain, and such nouns do not appear in 
any general opinion lexicon.  
576
2.1.1.  Aggregating Opinions on a Feature  
Using the opinion lexicon, we can identify opinion 
polarity expressed on each product feature in a 
sentence. The lexicon based method in (Ding et al 
2008) basically combines opinion words in the 
sentence to assign a sentiment to each product 
feature. The sketch of the algorithm is as follows.  
Given a sentence s which contains a product 
feature f, opinion words in the sentence are first 
identified by matching with the words in the 
opinion lexicon. It then computes an orientation 
score for f. A positive word is assigned the 
semantic orientation (polarity) score of +1, and a 
negative word is assigned the semantic orientation 
score of -1. All the scores are then summed up 
using the following score formula: 
      ,),(
.)(
:
?
???
?
Lwsww i
i
iii fwdis
SOwfscore  (1) 
where wi is an opinion word, L is the set of all 
opinion words (including idioms) and s is the 
sentence that contains the feature f, and dis(wi, f) is 
the distance between feature f and opinion word wi 
in s. wi.SO is the semantic orientation (polarity) of 
word wi. The multiplicative inverse in the formula 
is used to give low weights to opinion words that 
are far away from the feature f. 
If the final score is positive, then the opinion on 
the feature in s is positive. If the score is negative, 
then the opinion on the feature in s is negative.  
2.1.2. Rules of Opinions  
Several language constructs need special handling, 
for which a set of rules is applied (Ding et al, 
2008; Liu, 2010). A rule of opinion is an 
implication with an expression on the left and an 
implied opinion on the right. The expression is a 
conceptual one as it represents a concept, which 
can be expressed in many ways in a sentence.  
Negation rule. A negation word or phrase 
usually reverses the opinion expressed in a 
sentence. Negation words include ?no,? ?not?, etc.  
In this work, we also discovered that when 
applying negation rules, a special case needs extra 
care. For example, ?I am not bothered by the hump 
on the mattress? is a sentence from a mattress 
review. It expresses a neutral feeling from the 
person. However, it also implies a negative opinion 
about ?hump,? which indicates a product feature. 
We call this kind of sentences negated feeling 
response sentences. A sentence like this normally 
expresses the feeling of a person or a group of 
persons towards some items which generally have 
positive or negative connotations in the sentence 
context or the application domain. Such a sentence 
usually consists of four components: a noun 
representing a person or a group of persons (which 
includes personal pronoun and proper noun), a 
negation word, a feeling verb, and a stimulus word. 
Feeling verbs include ?bother,? ?disturb,? ?annoy,? 
etc. The stimulus word, which stimulates the 
feeling, also indicates a feature. In analyzing such 
a sentence, for our purpose, the negation is not 
applied. Instead, we regard the sentence bearing 
the same opinion about the stimulus word as the 
opinion of the feeling verb. These opinion contexts 
will help the statistical test later.  
But clause rule. A sentence containing ?but? 
also needs special treatment. The opinion before 
?but? and after ?but? are usually the opposite to 
each other. Phrases such as ?except that? and 
?except for? behave similarly. 
Deceasing and increasing rules. These rules 
say that deceasing or increasing of some quantities 
associated with opinionated items may change the 
orientations of the opinions. For example, ?The 
drug eased my pain?. Here ?pain? is a negative 
opinion word in the opinion lexicon, and the 
reduction of ?pain? indicates a desirable effect of 
the drug. We have compiled a list of such words, 
which include ?decease?, ?diminish?, ?prevent?, 
?remove?, etc. The basic rules are as follows:   
Decreased Neg ? Positive 
E.g: ?My problem have certainly diminished? 
Decreased Pos ?  Negative 
E.g: ?These tires reduce the fun of driving.? 
Neg and Pos represent respectively a negative 
and a positive opinion word. Increasing rules do 
not change opinion directions (Liu, 2010).   
2.1.3. Handing Context-Dependent Opinions 
As mentioned earlier, context-dependent opinion 
words (only adjectives and adverbs) must be 
determined by its contexts. We solve this problem 
by using the global information rather than only 
the local information in the current sentence. We 
use a conjunction rule. For example, if someone 
writes a sentence like ?This camera is very nice 
and has a long battery life?, we can infer that 
577
?long? is positive for ?battery life? because it is 
conjoined with the positive word ?nice.? This 
discovery can be used anywhere in the corpus.   
2.2 Determining Candidate Noun Product 
Features that Imply Opinions  
Using the sentiment analysis method in section 2.1, 
we can identify opinion sentences for each product 
feature in context, which contains both positive-
opinionated sentences and negative-opinionated 
sentences. We then determine candidate product 
features implying opinions by checking the 
percentage of either positive-opinionated sentences 
or negative-opinionated sentences among all 
opinionated sentences. Through experiments, we 
make an empirical assumption that if either the 
positive-opinionated sentence percentage or the 
negative-opinionated sentence percentage is 
significantly greater than 70%, we regard this noun 
feature as a noun feature implying an opinion. The 
basic heuristic for our idea is that if a noun feature 
is more likely to occur in positive (or negative) 
opinion contexts (sentences), it is more likely to be 
an opinionated noun feature. We use a statistic 
method test for population proportion to perform 
the significant test. The details are as follows. We 
compute the Z-score statistic with one-tailed test. 
 n
pp
ppZ )1( 00
0
?
??
 (2)
 
where p0 is the hypothesized value (0.7 in our 
case), p is the sample proportion, i.e., the 
percentage of positive (or negative) opinions in our 
case, and n is the sample size, which is the total 
number of opinionated sentences that contain the 
noun feature. We set the statistical confidence level 
to 0.95, whose corresponding Z score is -1.64. It 
means that Z score for an opinionated feature must 
be no less than -1.64. Otherwise we do not regard 
it as a feature implying opinion.   
2.3 Pruning Non-Opinionated Features  
Many of candidate noun features with opinions 
may not indicate any opinion. Then, we need to 
distinguish features which have implied opinions 
and normal features which have no opinions, e.g., 
?voice quality? and ?battery life.? For normal 
features, people often can have different opinions. 
For example, for ?voice quality?, people can say 
?good voice quality? or ?bad voice quality.? 
However, for features with context dependent 
opinions, people often have a fixed opinion, either 
positive or negative but not both. With this 
observation in mind, we can detect features with 
no opinion by finding direct modification relations 
using a dependency parser. To be safe, we use only 
two types of direct relations:  
Type1: O  ? O-Dep ? F 
It means O depends on F through a relation O-
Dep. E.g: ?This TV has a good picture quality.? 
Type 2: O ? O-Dep ? H ? F-Dep ? F 
It means both O and F depends on H through 
relation O-Dep and F-Dep respectively. E.g: 
?The springs of the mattress are bad.? 
Here O is an opinion word, O-Dep / F-Dep is a 
dependency relation, which describes a relation 
between words, and includes mod, pnmod, subj, s, 
obj, obj2 and desc (detailed explanations can be 
found in http://www.cs.ualberta.ca/~lindek/ 
minipar.htm). F is a noun feature. H means any 
word. For the first example, given feature ?picture 
quality?, we can extract its modification opinion 
word ?good?. For the second example, given 
feature ?springs?, we can get opinion word ?bad?. 
Here H is the word ?are?. 
Among these extracted opinion words for the 
feature noun, if some belong to the positive 
opinion lexicon and some belong to the negative 
opinion lexicon, we conclude the noun feature is 
not an opinionated feature and is thus pruned.  
3 Experiments  
We conducted experiments using four diverse real-
life datasets of reviews. Table 1 shows the domains 
(based on their names) of the datasets, the number 
of sentences, and the number of noun features. The 
first two datasets were obtained from a commercial 
company that provides opinion mining services, 
and the other two were crawled by us. 
Product Name Mattress    Drug Router Radio 
# Sentences 13191 1541 4308 2306 
# Noun features 326 38 173 222 
Table 1.  Experimental datasets 
    An issue for judging noun features implying 
opinions is that it can be subjective. So for the gold 
standard, a consensus has to be reached between 
the two annotators.  
578
For comparison, we also implemented a baseline 
method, which decides a noun feature?s polarity 
only by its modifying opinion words (adjectives). 
If its corresponding adjective is positive-orientated, 
then the noun feature is positive-orientated. The 
same goes for a negative-orientated noun feature. 
Then using the same techniques in section 2.3 for 
statistical test (in this case, n in equation 2 is the 
total number of sentences containing the noun 
feature) and for pruning, we can determine noun 
features implying opinions from the data corpus.       
Table 2 gives the experimental results. The 
performances are measured using the standard 
evaluation measures of precision and recall. From 
Table 2, we can see that the proposed method is 
much better than the baseline method on both the 
recall and precision. It indicates many noun 
features that imply opinions are not directly 
modified by adjective opinion words. We have to 
determine their polarities based on contexts. 
Product 
Name 
Baseline Proposed Method
Precision Recall Precision Recall 
Mattress 0.35 0.07 0.48 0.82 
Drug 0.40 0.15 0.58 0.88 
Router 0.20 0.45 0.42 0.67 
Radio 0.18 0.50 0.31 0.83 
Table 2. Experimental results for noun features  
    Table 3 and Table 4 give the results of noun 
features implying positive and negative opinions 
separately. No baseline method is used here due to 
its poor results. Because for some datasets, there is 
no noun feature implying a positive/negative 
opinion, their precision and recall are zeros. 
Product Name Precision Recall 
Mattress 0.42 0.95 
Drug 0.33 1.0 
Router 0.43 0.60 
Radio 0.38 0.83 
Table 3. Features implying positive opinions 
Product Name Precision Recall 
Mattress 0.56 0.72 
Drug 0.67 0.86 
Router 0.40 1.00 
Radio 0 0 
Table 4. Features implying negative opinions 
    From Tables 2 - 4, we observe that the precision 
of the proposed method is still low, although the 
recalls are good. To better help the user find such 
words easily, we rank the extracted feature 
candidates. The purpose is to rank correct noun 
features that imply opinions at the top of the list, so 
as to improve the precision of the top-ranked 
candidates. Two ranking methods are used:  
1. rank based on the statistical score Z in equation 
2. We denote this method with Z-rank. 
2. rank based on negative/positive sentence ratio. 
We denote this method with R-rank. 
Tables 5 and 6 show the ranking results. We adopt 
the rank precision, also called the precision@N, 
metric for evaluation. It gives the percentage of 
correct noun features implying opinions at the rank 
position N. Because some domains may not 
contain positive or negative noun features, we 
combine positive and negative candidate features 
together for an overall ranking for each dataset. 
 Mattress Drug Router Radio
Z-rank 0.70 0.60 0.60 0.70 
R-rank 0.60 0.60 0.50 0.40 
Table 5. Experimental results: Precision@10 
 Mattress Drug Router Radio
Z-rank 0.66  0.46 0.53 
R-rank 0.60  0.46 0.40 
     Table 6. Experimental results: Precision@15 
    From Tables 5 and 6, we can see that the 
ranking by statistical value Z is more accurate than 
negative/positive sentence ratio. Note that in Table 
6, there is no result for the Drug dataset because no 
noun features implying opinions were found 
beyond the top 10 results because there are not 
many such noun features in the drug domain. 
4 Conclusions 
This paper proposed a method to identify noun 
product features that imply opinions. Conceptually, 
this work studied the problem of objective nouns 
and sentences with implied opinions. To the best of 
our knowledge, this problem has not been studied 
in the literature. This problem is important because 
without identifying such opinions, the recall of 
opinion mining suffers. Our proposed method 
determines feature polarity not only by opinion 
words that modify the features but also by its 
surrounding context. Experimental results show 
that the proposed method is promising. Our future 
work will focus on improving the precision.    
579
References  
Andreevskaia, A. and S. Bergler. 2006. Mining 
WordNet for fuzzy sentiment: Sentiment tag 
extraction from WordNet glosses. Proceedings of 
EACL 2006. 
Eric Breck, Yejin Choi, and Claire Cardie. 2007. 
Identifying Expressions of Opinion in Context. 
Proceedings of IJCAI 2007. 
Xiaowen Ding, Bing Liu and Philip S. Yu. 2008 A 
Holistic Lexicon-Based Approach to Opinion 
Mining. Proceedings of WSDM 2008. 
Eduard C. Dragut, Clement Yu, Prasad Sistla, and 
Weiyi Meng. 2010. Construction of a sentimental 
word dictionary. In Proceedings of CIKM 
2010.Andrea Esuli and Fabrizio Sebastiani. 2005. 
Determining the Semantic Orientation of Terms 
through Gloss Classification. Proceedings of CIKM 
2005. 
Andrea Esuli and Fabrizio Sebastiani. 2006. 
SentiWorkNet: A Publicly Available Lexical 
Resource for Opinion Mining. Proceedings of LREC 
2006. 
Michael Gamon. 2004. Sentiment Classification on 
Customer Feedback Data: Noisy Data, Large Feature 
Vectors and the Role of Linguistic Analysis. 
Proceedings of COLING 2004. 
Murthy Ganapathibhotla. and Bing Liu. 2008. Mining 
opinions in comparative sentences. Proceedings of 
COLING 2008. 
Vasileios Hatzivassiloglou and Kathleen, McKeown. 
1997. Predicting the Semantic Orientation of 
Adjectives. Proceedings of ACL 1997. 
Minqing Hu and Bing Liu. 2004. Mining and 
Summarizing Customer Reviews. Proceedings of 
KDD 2004. 
Jaap Kamps, Maarten Marx, Robert J, Mokken and 
Maarten de Rijke. 2004. Proceedings of LREC 2004. 
Hiroshi Kanayama, Tetsuya Nasukawa 2006. Fully 
Automatic Lexicon Expansion for Domain-Oriented 
Sentiment Analysis. Proceedings of EMNLP 2006.  
Nozomi Kobayashi, Kentaro Inui, and Yuji Matsumoto. 
2007. Extracting aspect-evaluation and aspect-of 
relations in opinion mining. Proceedings of EMLP 
2007 
Soo-Min Kim and Eduard Hovy. 2004. Determining the 
Sentiment of Opinions. Proceedings of COLING 
2004. 
Lun-Wei Ku,Yu-Ting Liang, and Hsin-Hsi Chen. 2006. 
Opinion extraction, summarization and tracking in 
news and blog corpora. Proceedings of AAAI-CAAW 
2006. 
Bing Liu. 2010. Sentiment analysis and subjectivity. A 
chapter in Handbook of Natural Language 
Processing, Second edition. 
Yue Lu, Chengxiang Zhai, and Neel Sundaresan. 2009. 
Rated aspect summarization of short comments. 
Proceedings of WWW 2009. 
Bo Pang and Lillian Lee. 2008. Opinion Mining and 
sentiment Analysis. Foundations and Trends in 
Information Retrieval 2(1-2), 2008. 
Bo Pang,  Lillian Lee and Shivakumar  Vaithyanathan. 
2002. Thumbs up? Sentiment Classification using 
Machine Learning Techniques. Proceedings of  
EMNLP 2002. 
Ana-Maria Popescu and Oren Etzioni. 2005. Extracting 
Product Features and Opinions from Reviews. 
Proceedings of EMNLP 2005. 
Guang Qiu, Bing Liu, Jiajun  Bu and Chun Chen. 2009. 
Expanding Domain Sentiment Lexicon through 
Double Propagation. Proceedings of IJCAI 2009.  
Ellen Riloff, Janyce Wiebe, and Theresa Wilson. 2003. 
Learning subjective nouns using extraction pattern 
bootstrapping. Proceedings of CoNLL 2003. 
Hiroya Takamura, Takashi Inui and Manabu Okumura. 
2007. Extracting Semantic Orientations of Phrases 
from Dictionary. Proceedings of HLT-NAACL 2007. 
Ivan Titov and Ryan McDonald. 2008. A joint model of 
text and aspect ratings for sentiment summarization. 
In Proceedings of ACL 2008.Peter D. Turney. 2002. 
Thumbs Up or Thumbs Down? Semantic Orientation 
Applied to Unsupervised Classification of Reviews. 
Proceedings of ACL 2002.  
Janyce Wiebe. 2000. Learning Subjective Adjectives 
from Corpora. Proceedings of AAAI 2000. 
Theresa Wilson, Janyce Wiebe, Rebecca Hwa. 2004. 
Just how mad are you? Finding strong and weak 
opinion clauses. Proceedings of AAAI 2004. 
Taras Zagibalov and John Carroll. 2008. Unsupervised 
Classification of Sentiment and Objectivity in 
Chinese Text. Proceedings of IJCNLP 2008.     
 
 
580
