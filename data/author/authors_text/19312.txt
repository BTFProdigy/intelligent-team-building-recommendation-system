A Comparative Evaluation of Data-driven Models in Translation
Selection of Machine Translation
?Yu-Seop Kim ? Jeong-Ho Chang ?Byoung-Tak Zhang
? Ewha Institute of Science and Technology, Ewha Woman?s Univ.
Seoul 120-750 Korea, yskim01@ewha.ac.kr?
? Schools of Computer Science and Engineering, Seoul National Univ.
Seoul 151-742 Korea, {jhchang, btzhang}@bi.snu.ac.kr?
Abstract
We present a comparative evaluation of two
data-driven models used in translation selec-
tion of English-Korean machine translation. La-
tent semantic analysis(LSA) and probabilistic
latent semantic analysis (PLSA) are applied for
the purpose of implementation of data-driven
models in particular. These models are able to
represent complex semantic structures of given
contexts, like text passages. Grammatical rela-
tionships, stored in dictionaries, are utilized in
translation selection essentially. We have used
k-nearest neighbor (k-NN) learning to select an
appropriate translation of the unseen instances
in the dictionary. The distance of instances in
k-NN is computed by estimating the similar-
ity measured by LSA and PLSA. For experi-
ments, we used TREC data(AP news in 1988)
for constructing latent semantic spaces of two
models and Wall Street Journal corpus for eval-
uating the translation accuracy in each model.
PLSA selected relatively more accurate transla-
tions than LSA in the experiment, irrespective
of the value of k and the types of grammatical
relationship.
1 Introduction
Construction of language associated resources
like thesaurus, annotated corpora, machine-
readable dictionary and etc. requires high de-
gree of cost, since they need much of human-
effort, which is also dependent heavily upon hu-
man intuition. A data-driven model, however,
does not demand any of human-knowledge,
knowledge bases, semantic thesaurus, syntac-
tic parser or the like. This model represents
? He is supported by Brain Korea 21 project performed
by Ewha Institute of Science and Technology.
? They are supported by Brain Tech. project controlled
by Korean Ministry of Science and Technology.
latent semantic structure of contexts like text
passages. Latent semantic analysis (LSA) (Lan-
dauer et al, 1998) and probabilistic latent se-
mantic analysis (PLSA) (Hofmann, 2001) fall
under the model.
LSA is a theory and method for extracting
and representing the contextual-usage meaning
of words. This method has been mainly used
for indexing and relevance estimation in infor-
mation retrieval area (Deerwester et al, 1990).
And LSA could be utilized to measure the co-
herence of texts (Foltz et al, 1998). By applying
the basic concept, a vector representation and
a cosine computation, to estimate relevance of
a word and/or a text and coherence of texts,
we could also estimate the semantic similarity
between words. It is claimed that LSA repre-
sents words of similar meaning in similar ways
(Landauer et al, 1998).
Probabilistic LSA (PLSA) is based on proba-
bilistic mixture decomposition while LSA is on a
linear algebra and singular value decomposition
(SVD) (Hofmann, 1999b). In contrast to LSA,
PLSA?s probabilistic variant has a sound statis-
tical foundation and defines a proper generative
model of the data. Both two techniques have
a same idea which is to map high-dimensional
vectors representing text documents, to a lower
dimensional representation, called a latent se-
mantic space (Hofmann, 1999a).
Dagan (Dagan et al, 1999) performed a com-
parative analysis of several similarity measures,
which based mainly on conditional probability
distribution. And the only elements in the dis-
tribution are words, which appeared in texts.
However, LSA and PLSA expressed the latent
semantic structures, called a topic of the con-
text.
In this paper, we comparatively evaluated
these two techniques performed in translation
selection of English-Korean machine transla-
tion. First, we built a dictionary storing tu-
ples representing the grammatical relationship
of two words, like subject-verb, object-verb, and
modifier-modified. Second, with an input tuple,
in which an input word would be translated and
the other would be used as an argument word,
translation is performed by searching the dic-
tionary with the argument word. Third, if the
argument word is not listed in the dictionary,
we used k-nearest neighbor learning method to
determine which class of translation is appro-
priate for the translation of an input word. The
distance used in discovering the nearest neigh-
bors was computed by estimating the similarity
measured on above latent semantic spaces.
In the experiment, we used 1988 AP news
corpus from TREC-7 data (Voorhees and Har-
man, 1998) for building latent semantic spaces
and Wall Street Journal (WSJ) corpus for con-
structing a dictionary and test sets. We ob-
tained 11-20% accuracy improvement, compar-
ing to a simple dictionary search method. And
PLSA has shown that its ability to select an ap-
propriate translation is superior to LSA as an
extent of up to 3%, without regard to the value
of k and grammatical relationship.
In section 2, we discuss two of data-driven
models, LSA and PLSA. Section 3 describes
ways of translation with a grammatical rela-
tion dictionary and k-nearest neighbor learning
method. Experiment is explained in Section 4
and concluding remarks are presented in Section
5.
2 Data-Driven Model
For the data-driven model which does not re-
quire additional human-knowledge in acquiring
information, Latent Semantic Analysis (LSA)
and Probabilistic LSA (PLSA) are applied to es-
timate semantic similarity among words. Next
two subsections will explain how LSA and PLSA
are to be adopted to measuring semantic simi-
larity.
2.1 Latent Semantic Analysis
The basic idea of LSA is that the aggregate of all
the word contexts in which a given word does
and does not appear provides a set of mutual
constraints that largely determines the similar-
ity of meaning of words and sets of words to each
other (Landauer et al, 1998)(Gotoh and Renals,
1997). LSA also extracts and infers relations of
expected contextual usage of words in passages
of discourse. It uses no human-made dictionar-
ies, knowledge bases, semantic thesaurus, syn-
tactic parser or the like. Only raw text parsed
into unique character strings is needed for its
input data.
The first step is to represent the text as a
matrix in which each row stands for a unique
word and each column stands for a text passage
or other context. Each cell contains the occur-
rence frequency of a word in the text passage.
Next, LSA applies singular value decomposi-
tion (SVD) to the matrix. SVD is a form of
factor analysis and is defined as
A = U?V T (1)
,where ? is a diagonal matrix composed of
nonzero eigen values of AAT or ATA, and U
and V are the orthogonal eigenvectors associ-
ated with the r nonzero eigenvalues of AAT and
ATA, respectively. One component matrix (U)
describes the original row entities as vectors of
derived orthogonal factor value, another (V ) de-
scribes the original column entities in the same
way, and the third (?) is a diagonal matrix con-
taining scaling values when the three compo-
nents are matrix-multiplied, the original matrix
is reconstructed.
The singular vectors corresponding to the
k(k ? r) largest singular values are then used
to define k-dimensional document space. Using
these vectors,m?k and n?k matrices Uk and Vk
may be redefined along with k?k singular value
matrix ?k. It is known that Ak = Uk?kV Tk is
the closest matrix of rank k to the original ma-
trix.
LSA can represent words of similar meaning
in similar ways. This can be claimed by the fact
that one compares words with similar vectors as
derived from large text corpora. The term-to-
term similarity is based on the inner products
between two row vectors of A, AAT = U?2UT .
One might think of the rows of U? as defining
coordinates for terms in the latent space. To
calculate the similarity of coordinates, V1 and
V2, cosine computation is used:
cos? = V1 ?V2? V1 ? ? ? V2 ?
(2)
2.2 Probabilistic Latent Semantic
Analysis
Probabilistic latent semantic analysis (PLSA)
is a statistical technique for the analysis of two-
mode and co-occurrence data, and has produced
some meaningful results in such applications
as language modelling (Gildea and Hofmann,
1999) and document indexing in information re-
trieval (Hofmann, 1999b). PLSA is based on
aspect model where each observation of the co-
occurrence data is associated with a latent class
variable z ? Z = {z1, z2, . . . , zK} (Hofmann,
1999a). For text documents, the observation is
an occurrence of a word w ? W in a document
d ? D, and each possible state z of the latent
class represents one semantic topic.
A word-document co-occurrence event,
(d,w), is modelled in a probabilistic way where
it is parameterized as in
P (d,w) =
?
z
P (z)P (d,w|z)
=
?
z
P (z)P (w|z)P (d|z). (3)
Here, w and d are assumed to be condition-
ally independent given a specific z. P (w|z) and
P (d|z) are topic-specific word distribution and
document distribution, respectively. The three-
way decomposition for the co-occurrence data
is similar to that of SVD in LSA. But the ob-
jective function of PLSA, unlike that of LSA,
is the likelihood function of multinomial sam-
pling. And the parameters P (z), P (w|z), and
P (d|z) are estimated by maximization of the
log-likelihood function
L =
?
d?D
?
w?W
n(d,w) logP (d,w), (4)
and this maximization is performed using the
EM algorithm as for most latent variable mod-
els. Details on the parameter estimation are
referred to (Hofmann, 1999a). To compute
the similarity of w1 and w2, P (zk|w1)P (zk|w2)
should be approximately computed with being
derived from
P (zk|w) =
P (zk)P (w|zk)
?
zk?Z P (zk)P (w|zk)
(5)
And we can evaluate similarities with the
low-dimensional representation in the semantic
topic space P (zk|w1) and P (zk|w2).
3 Translation with Grammatical
Relationship
3.1 Grammatical Relationship
We used grammatical relations stored in the
form of a dictionary for translation of words.
The structure of the dictionary is as follows
(Kim and Kim, 1998):
T (Si) =
?
?
?
?
?
?
?
T1 if Cooc(Si, S1)
T2 if Cooc(Si, S2)
. . .
Tn otherwise,
(6)
where Cooc(Si, Sj) denotes grammatical co-
occurrence of source words Si and Sj , which one
means an input word to be translated and the
other means an argument word to be used in
translation, and Tj is the translation result of
the source word. T (?) denotes the translation
process.
Table 1 shows a grammatical relationship dic-
tionary for an English verb Si =?build? and its
object nouns as an input word and an argument
word, respectively. The dictionary shows that
the word ?build? is translated into five different
translated words in Korean, depending on the
context. For example, ?build? is translated into
?geon-seol-ha-da? (?construct?) when its object
noun is a noun ?plant? (=?factory?), into ?che-
chak-ha-da? (?produce?) when co-occurring with
the object noun ?car?, and into ?seol-lip-ha-da?
(?establish?) in the context of object noun ?com-
pany? (Table 2).
One of the fundamental difficulties in co-
occurrence-based approaches to word sense dis-
ambiguation (translation selection in this case)
is the problem of data sparseness or unseen
words. For example, for an unregistered object
noun like ?vehicle? in the dictionary, the correct
translation of the verb cannot be selected us-
ing the dictionary described above. In the next
subsection, we will present k-nearest neighbor
method that resolves this problem.
3.2 k-Nearest Neighbor Learning for
Translation Selection
The similarity between two words on latent se-
mantic spaces is required when performing k-
NN search to select the translation of a word.
The nearest instance of a given word is decided
by selecting a word with the highest similarity
to the given word.
Table 1: Examples of co-occurrence word lists for a verb ?build? in the dictionary
Meaning of ?build? in Korean (Tj) Collocated Object Noun (Sj)
?geon-seol-ha-da? (= ?construct?) plant facility network . . .
?geon-chook-ha-da? (= ?design?) house center housing . . .
?che-chak-ha-da? (= ?produce?) car ship model . . .
?seol-lip-ha-da? (= ?establish?) company market empire . . .
?koo-chook-ha-da? (= ?develop?) system stake relationship . . .
Table 2: Examples of translation of ?build?
source words translated words (in Korean) sense of the verb
?build a plant? ? ?gong-jang-eul geon-seol-ha-da? ?construct?
?build a car? ? ?ja-dong-cha-reul che-chak-ha-da? ?produce?
?build a company? ? ?hoi-sa-reul seol-lip-ha-da? ?establish?
The k-nearest neighbor learning algorithm
(Cover and Hart, 1967)(Aha et al, 1991) as-
sumes all instances correspond to points in the
n-dimensional space Rn. We mapped the n-
dimensional space into the n-dimensional vector
of a word for an instance. The nearest neigh-
bors of an instance are defined in terms of the
standard Euclidean distance.
Then the distance between two instances xi
and xj , D(xi, xj), is defined to be
D(xi, xj) =
?
(a(xi)? a(xj))2 (7)
and a(xi) denotes the value of instance xi, sim-
ilarly to cosine computation between two vec-
tors. Let us consider learning discrete-valued
target functions of the form f : Rn ? V ,
where V is the finite set {v1, . . . , vs}. The k-
nearest neighbor algorithm for approximating a
discrete-valued target function is given in Table
3.
The value f?(xq) returned by this algorithm
as its estimate of f(xq) is just the most com-
mon value of f among the k training examples
nearest to xq.
4 Experiment and Evaluation
4.1 Data for Latent Space and
Dictionary
In the experiment, we used two kinds of cor-
pus data, one for constructing LSA and PLSA
spaces and the other for building a dictionary
containing grammatical relations and a test set.
79,919 texts in 1988 AP news corpus from
TREC-7 data was indexed with a stemming tool
and 19,286 words with the frequency of above 20
Table 3: The k-nearest neighbor learning algo-
rithm.
? Training
? For each training example ?x, f(x)?,
add the example to the list
training examples.
? Classification
? Given a query instance xq to be clas-
sified,
? Let x1, . . . , xk denote the k in-
stances from training examples
that are nearest to xq.
? Return
f?(xq)? argmaxv?V
k
?
i=1
?(v, f(xi)) ,
where ?(a, b) = 1 if a = b and
?(a, b) = 0 otherwise.
are extracted. We built 200 dimensions in SVD
of LSA and 128 latent dimensions of PLSA. The
difference of the numbers was caused from the
degree of computational complexity in learning
phase. Actually, PLSA of 128 latent factors re-
quired 50-fold time as much as LSA hiring 200
eigen-vector space during building latent spaces.
This was caused by 50 iterations which made
the log likelihood maximized. We utilized a sin-
Figure 1: The accuracy ration of verb-object
gle vector lanczos algorithm derived from SVD-
PACK when constructing LSA space. (Berry
et al, 1993). We generated both of LSA and
PLSA spaces, with each word having a vector
of 200 and 128 dimensions, respectively. The
similarity of any two words could be estimated
by performing cosine computation between two
vectors representing coordinates of the words in
the spaces.
Table 4 shows 5 most similar words of ran-
domly selected words from 3,443 examples. We
extracted 3,443 example sentences containing
grammatical relations, like verb-object, subject-
verb and adjective-noun, from Wall Street Jour-
nal corpus of 220,047 sentences and other
newspapers corpus of 41,750 sentences, totally
261,797 sentences. We evaluated the accu-
racy performance of each grammatical rela-
tion. 2,437, 188, and 818 examples were uti-
lized for verb-object, subject-verb, and adjective-
noun, respectively. The selection accuracy was
measured using 5-fold cross validation for each
grammatical relation. Sample sentences of each
grammatical relation were divided into five dis-
joint samples and each sample became a test
sample once in the experiment and the remain-
ing four samples were combined to make up a
collocation dictionary.
4.2 Experimental Result
Table 5 and figure 1-3 show the results of
translation selection with respect to the applied
model and to the value of k. As shown in Table
5, similarity based on data-driven model could
improve the selection accuracy up to 20% as
Figure 2: The accuracy ration of subject-verb
Figure 3: The accuracy ration of adjective-noun
contrasted with the direct matching method.
We could obtain the result that PLSA could
improve the accuracy more than LSA in almost
all cases. The amount of improvement is varied
from -0.12% to 2.96%.
As figure 1-3 show, the value of k had affec-
tion to the translation accuracy in PLSA, how-
ever, not in LSA. From this, we could not de-
clare whether the value of k and translation ac-
curacy have relationship of each other or not
in the data-driven models described in this pa-
per. However, we could also find that the degree
of accuracy was raised in accordance with the
value of k in PLSA. From this, we consequently
inferred that the latent semantic space gener-
ated by PLSA had more sound distribution with
reflection of well-structured semantic structure
than LSA. Only one of three grammatical re-
Table 4: Lists of 5 most semantically similar words for randomly selected words generated from
LSA, and PLSA. The words are stems of original words. The first row of each selected word stands
for the most similar words in LSA semantic space and the second row stands for those in the PLSA
space.
selected words most similar words
plant westinghous isocyan shutdown zinc manur
radioact hanford irradi tritium biodegrad
car buick oldsmobil chevrolet sedan corolla
highwai volkswagen sedan vehicular vehicle
home parapleg broccoli coconut liverpool jamal
memori baxter hanlei corwin headston
business entrepreneur corpor custom ventur firm
digit compat softwar blackston zayr
ship vessel sail seamen sank sailor
destroy frogmen maritim skipper vessel
Table 5: Translation accuracy in various case. The first column stands for each grammatical relation
and the second column stands for the used models, LSA or PLSA. And other three columns stand
for the accuracy ratio (rm) with respect to the value of k. The numbers in parenthesis of the first
column show the translation accuracy ratio of simple dictionary search method (rs). And numbers
in the other parenthesis were obtained by rm ? rs.
grammatical used k = 1 k = 5 k = 10
relations model
verb-object LSA 84.41(1.17) 83.01(1.16) 84.24(1.17)
(71.85) PLSA 84.53(1.18) 85.35(1.19) 86.05(1.20)
subject-verb LSA 83.99(1.11) 84.62(1.11) 84.31(1.11)
(75.93) PLSA 86.85(1.14) 87.49(1.15) 87.27(1.15)
adjective-noun LSA 80.93(1.15) 80.32(1.14) 80.93(1.15)
(70.54) PLSA 80.81(1.15) 82.27(1.17) 82.76(1.17)
lations, subj-verb, showed an exceptional case,
which seemed to be caused by the small size of
examples, 188.
Selection errors taking place in LSA and
PLSA models were caused mainly by the fol-
lowing reasons. First of all, the size of vocab-
ulary should be limited by computation com-
plexity. In this experiment, we acquired below
20,000 words for the vocabulary, which could
not cover a section of corpus data. Second, the
stemming algorithm was not robust for an in-
dexing. For example, ?house? and ?housing? are
regarded as a same word as ?hous?. This fact
brought about hardness in reflecting the seman-
tic structure more precisely. And finally, the
meaning of similar word is somewhat varied in
the machine translation field and the informa-
tion retrieval field. The selectional restriction
tends to depend a little more upon semantic
type like human-being, place and etc., than on
the context in a document.
5 Conclusion
This paper describes a comparative evaluation
of the accuracy performance in translation se-
lection based on data-driven models. LSA and
PLSA were utilized for implementation of the
models, which are mainly used in estimating
similarity between words. And a manually-
built grammatical relation dictionary was used
for the purpose of appropriate translation se-
lection of a word. To break down the data
sparseness problem occurring when the dictio-
nary is used, we utilized similarity measure-
ments schemed out from the models. When an
argument word is not included in the dictionary,
the most k similar words to the word are discov-
ered in the dictionary, and then the meaning of
the grammatically-related class for the majority
of the k words is selected as the translation of
an input word.
We evaluated the accuracy ratio of LSA and
PLSA comparatively and classified the exper-
iments with criteria of the values of k and
the grammatical relations. We acquired up to
20% accuracy improvement, compared to direct
matching to a collocation dictionary. PLSA
showed the ability to select translation better
than LSA, up to 3%. The value of k is strongly
related with PLSA in translation accuracy, not
too with LSA. That means the latent semantic
space of PLSA has more sound distribution of
latent semantics than that of LSA. Even though
longer learning time than LSA, PLSA is benefi-
cial in translation accuracy and distributional
soundness. A distributional soundness is ex-
pected to have better performance as the size
of examples is growing.
However, we should resolve several problems
raised during the experiment. First, a robust
stemming tool should be exploited for more ac-
curate morphology analysis. Second, the opti-
mal value of k should be obtained, according to
the size of examples. Finally, we should discover
more specific contextual information suited to
this type of problem. While simple text could
be used properly in IR, MT should require an-
other type of information.
The data-driven models could be applied to
other sub-fields related with semantics in ma-
chine translation. For example, to-infinitive
phrase and preposition phrase attachment dis-
ambiguation problem can also apply these mod-
els. And syntactic parser could apply the mod-
els for improvement of accurate analysis by us-
ing semantic information generated by the mod-
els.
References
D. Aha, D. Kibler, and M. Albert. 1991.
Instance-based learning algorithms. Machine
Learning, 6:37?66.
M. Berry, T. Do, G. O?Brien, V. Krishna, and
S. Varadhan. 1993. Svdpackc: Version 1.0
user?s guide. Technical Report CS?93?194,
University of Tennessee, Knoxville, TN.
T. Cover and P. Hart. 1967. Nearest neighbor
pattern classification. IEEE Transactions on
Information Theory, 13:21?27.
I. Dagan, L. Lee, and F. Fereira. 1999.
Similarity-based models of word cooccurrence
probabilities. Machine Learning, 34:43?69.
S. Deerwester, S. Dumais, G. Furnas, T. Lan-
dauer, and R. Harshman. 1990. Indexing
by latent semantic analysis. Journal of the
American Society for Information Science,
41:391?407.
P. Foltz, W. Kintsch, and T. Landauer. 1998.
The mesurement of textual coherence with la-
tent semantic analysis. Discourse Processes,
25:285?307.
D. Gildea and T. Hofmann. 1999. Topic based
language models using em. In Proceedings of
the 6th European Conference on Speech Com-
munication and Technology (Eurospeech99).
D. Gotoh and S. Renals. 1997. Document
space models using latent semantic analysis.
In Proceedings of Eurospeech-97, pages 1443?
1446.
T. Hofmann. 1999a. Probabilistic latent se-
mantic analysis. In Proceedings of the Fif-
teenth Conference on Uncertainty in Artifi-
cial Intelligence (UAI?99).
T. Hofmann. 1999b. Probabilistic latent se-
mantic indexing. In Proceedings of the 22th
Annual International ACM SIGIR conference
on Research and Developement in Informa-
tion Retrieval (SIGIR99), pages 50?57.
T. Hofmann. 2001. Unsupervised learning by
probabilistic latent semantic analysis. Ma-
chine Learning Journal, 42(1):177?196.
Y. Kim and Y. Kim. 1998. Semantic implemen-
tation based on extended idiom for english to
korean machine translation. The Asia-Pacific
Association for Machine Translation Journal,
21:23?39.
T. K. Landauer, P. W. Foltz, and D. Laham.
1998. An introduction to latent semantic
analysis. Discourse Processes, 25:259?284.
E. Voorhees and D. Harman. 1998. Overview of
the seventh text retrieval conference (trec-7).
In Proceedings of the Seventh Text REtrieval
Conference (TREC-7), pages 1?24.
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 637?642,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
Training a Korean SRL System with Rich Morphological Features
Young-Bum Kim, Heemoon Chae, Benjamin Snyder and Yu-Seop Kim*
University of Wisconsin-Madison, Hallym University*
{ybkim, hmchae21, bsnyder}@cs.wisc.edu, yskim01@hallym.ac.kr
*
Abstract
In this paper we introduce a semantic role
labeler for Korean, an agglutinative lan-
guage with rich morphology. First, we
create a novel training source by semanti-
cally annotating a Korean corpus contain-
ing fine-grained morphological and syn-
tactic information. We then develop a su-
pervised SRL model by leveraging mor-
phological features of Korean that tend
to correspond with semantic roles. Our
model also employs a variety of latent
morpheme representations induced from a
larger body of unannotated Korean text.
These elements lead to state-of-the-art per-
formance of 81.07% labeled F1, represent-
ing the best SRL performance reported to
date for an agglutinative language.
1 Introduction
Semantic Role Labeling (SRL) is the task of auto-
matically annotating the predicate-argument struc-
ture in a sentence with semantic roles. Ever since
Gildea and Jurafsky (2002), SRL has become an
important technology used in applications requir-
ing semantic interpretation, ranging from infor-
mation extraction (Frank et al, 2007) and ques-
tion answering (Narayanan and Harabagiu, 2004),
to practical problems including textual entailment
(Burchardt et al, 2007) and pictorial communica-
tion systems (Goldberg et al, 2008).
SRL systems in many languages have been
developed as the necessary linguistic resources
become available (Taul?e et al, 2008; Xue and
Palmer, 2009; B?ohmov?a et al, 2003; Kawahara et
al., 2002). Seven languages were the subject of the
CoNLL-2009 shared task in syntactic and seman-
tic parsing (Haji?c et al, 2009). These languages
can be categorized into three broad morphological
types: fusional (4), analytic (2), and one aggluti-
native language.
Paul 
 
 studies 
 
 mathematics 
 
 with 
 
 Jane 
 
 at 
 
 a 
 
 library
Poleun 
 
 doseogwaneseo 
 
Jeingwa 
 
suhageull 
 
 gongbuhanda
Figure 1: English (SVO) and Korean (SOV) words
alignment. The subject, verb, and object are high-
lighted as red, blue, and green, respectively. Also,
prepositions and suffixes are highlighted as purple.
Bj?orkelund et al (2009) report an average la-
beled semantic F1-score of 80.80% across these
languages. The highest performance was achieved
for the analytic language group (82.12%), while
the agglutinative language, Japanese, yielded the
lowest performance (76.30%). Agglutinative lan-
guages such as Japanese, Korean, and Turkish are
computationally difficult due to word-form spar-
sity, variable word order, and the challenge of us-
ing rich morphological features.
In this paper, we describe a Korean SRL system
which achieves 81% labeled semantic F1-score.
As far as we know, this is the highest accuracy
obtained for Korean, as well as any agglutinative
language. Figure 1 displays a English/Korean sen-
tence pair, highlighting the SOV word order of Ko-
rean as well as its rich morphological structure.
Two factors proved crucial in the performance of
our SRL system: (i) The analysis of fine-grained
morphological tags specific to Korean, and (ii) the
use of latent stem and morpheme representations
to deal with sparsity. We incorporated both of
these elements in a CRF (Lafferty et al, 2001) role
labeling model.
Besides the contribution of this model and SRL
system, we also report on the creation and avail-
ability of a new semantically annotated Korean
corpus, covering over 8,000 sentences. We used
this corpus to develop, train, and test our Korean
SRL model. In the next section, we describe the
process of corpus creation in more detail.
637
2 A Semantically Annotated Korean
Corpus
We annotated predicate-argument structure of
verbs in a corpus from the Electronics and
Telecommunications Research Institute of Korea
(ETRI).
1
Our corpus was developed over two
years using a specialized annotation tool (Song et
al., 2012), resulting in more than 8,000 semanti-
cally annotated sentences. As much as possible,
annotations followed the PropBank guidelines for
English (Bonial et al, 2010).
We view our work as building on the efforts of
the Penn Korean PropBank (PKPB).
2
Our corpus
is roughly similar in size to the PKPB, and taken
together, the two Korean corpora now total about
half the size of the Penn English PropBank. One
advantage of our corpus is that it is built on top of
the ETRI Korean corpus, which uses a richer Ko-
rean morphological tagging scheme than the Penn
Korean Treebank. Our experiments will show that
these finer-grained tags are crucial for achieving
high SRL accuracy.
All annotations were performed by two people
working in a team. At first, each annotator as-
signs semantic roles independently and then they
discuss to reduce disagreement of their annotation
results. Initially, the disagreement rate between
two annotators was about 14%. After 4 months
of this process, the disagreement rate fell to 4%
through the process of building annotation rules
for Korean. The underlying ETRI syntactically-
annotated corpus contains the dependency tree
structure of sentences with morpho-syntactic tags.
It includes 101,602 multiple-clause sentences with
21.66 words on average.
We encountered two major difficulties during
annotation. First, the existing Korean frame files
from the Penn Korean PropBank include 2,749
verbs, covering only 13.87% of all the verbs in the
ETRI corpus. Secondly, no Korean PropBanking
guidelines have previously been published, lead-
ing to uncertainty in the initial stages of annota-
tion. These uncertainties were gradually resolved
through the iterative process of resolving inter-
annotator disagreements.
Table 1 shows the semantic roles considered in
our annotated corpus. Although these are based on
the general English PropBank guidelines (Bonial
et al, 2010), they also differ in that we used only
1
http://voice.etri.re.kr/db/db pop.asp?code=88
2
http://catalog.ldc.upenn.edu/LDC2006T03
Roles Definition Rate
ARG0 Agent 10.02%
ARG1 Patient 26.73%
ARG2
Start point /
Benefactive
5.18%
ARG3 Ending point 1.10%
ARGM-ADV Adverbial 1.26%
ARGM-CAU Cause 1.17%
ARGM-CND Condition 0.36%
ARGM-DIR Direction 0.35%
ARGM-DIS Discourse 28.71%
ARGM-EXT Extent 4.50%
ARGM-INS Instrument 1.04%
ARGM-LOC Locative 4.51%
ARGM-MNR Manner 8.72%
ARGM-NEG Negation 0.26%
ARGM-PRD Predication 0.27%
ARGM-PRP Purpose 0.77%
ARGM-TMP Temporal 5.05%
Table 1: Semantic roles in our annotated corpus.
4 numbered arguments from ARG0 to ARG3 in-
stead of 5 numbered arguments. We thus consider
17 semantic roles in total. Four of them are num-
bered roles, describing the essential arguments of
a predicate. The other roles are called modifier
roles that play more of an adjunct role.
We have annotated semantic roles by following
the PropBank annotation guideline (Bonial et al,
2010) and by using frame files of the Penn Korean
PropBank built by Palmer et al (2006). The Prop-
Bank and our corpus are not exactly compatible,
because the former is built on constituency-based
parse trees, whereas our corpus uses dependency
parses.
More importantly, the tagsets of these corpora
are not fully compatible. The PKPB uses much
coarser morpho-syntactic tags than the ETRI
corpus. For example, the PCA tag in PKPB used
for a case suffix covers four different functioning
tags used in our corpus. Using coarser suffix
tags can seriously degrade SRL performance, as
we show in Section 6, where we compare the
performance of our model on both the new corpus
and the older PKPB.
638
3 Previous Work
Korean SRL research has been limited to domesti-
cally published Korean research on small corpora.
Therefore, the most direct precedent to the present
work is a section in Bj?orkelund et al (2009) on
Japanese SRL. They build a classifier consisting
of 3 stages: predicate disambiguation, argument
identification, and argument classification.
They use an L
2
-regularized linear logistic re-
gression model cascaded through these three
stages, achieving F1-score of 80.80% on average
for 7 languages (Catalan, Chinese, Czech, English,
German, Japanese and Spanish). The lowest re-
ported performance is for Japanese, the only ag-
glutinative language in their data set, achieving
F1-score of 76.30%. This result showcases the
computational difficulty of dealing with morpho-
logically rich agglutinative languages. As we dis-
cuss in Section 5, we utilize these same features,
but also add a set of Korean-specific features to
capture aspects of Korean morphology.
Besides these morphological features, we also
employ latent continuous and discrete morpheme
representations induced from a larger body of
unannotated Korean text. As our experiments be-
low show, these features improve performance by
dealing with sparsity issues. Such features have
been useful in a variety of English NLP mod-
els, including chunking, named entity recogni-
tion (Turian et al, 2010), and spoken language un-
derstanding (Anastasakos et al, 2014). Unlike the
English models, we use individual morphemes as
our unit of analysis.
4 Model
For the semantic role task, the input is a sentence
consisting of a sequence of words x = x
1
, . . . , x
n
and the output is a sequence of corresponding se-
mantic tags y = y
1
, . . . , y
n
. Each word con-
sists of a stem and some number of suffix mor-
phemes, and the semantic tags are drawn from the
set {NONE, ARG?, . . . , ARGM-TMP}. We model
the conditional probability p(y|x) using a CRF
model:
Z(x)
?1
x
?
i=1
exp
?
m
?
m
f
m
(y
i?1
, y
i
, x, i),
where f
m
(y
i?1
, y
i
, x, i) are the feature functions.
These feature functions include transition features
that identify the tag bigram (y
i?1
, y
i
), and emis-
sion features that combine the current semantic tag
(y
i
) with instantiated feature templates extracted
from the sentence x and its underlying morpho-
logical and dependency analysis. The function
Z is the normalizing function, which ensures that
p(y|x) is a valid probability distribution. We used
100 iteration of averaged perceptron algorithm to
train the CRF.
5 Features
We detail the feature templates used for our ex-
periments in Table 2. These features are catego-
rized as either general features, Korean-specific
features, or latent morpheme representation fea-
tures. Korean-specific features are built upon the
morphological analysis of the suffix agglutination
of the current word x
i
.
Korean suffixes are traditionally classified into
two groups called Josa and Eomi. Josa is used
to define nominal cases and modify other phrases,
while Eomi is an ending of a verb or an adjective
to define a tense, show an attitude, and connect
or terminate a sentence. Thus, the Eomi and Josa
categorization plays an important role in signaling
semantic roles. Considering the functions of Josa
and Eomi, we expect that numbered roles are rele-
vant to Josa while modifier roles are more closely
related to Eomi. The one exception is adverbial
Josa, making the attached phrase an adverb that
modifies a verb predicate.
For all feature templates, ?A-? or ?P-? are used
respectively to signify that the feature corresponds
to the argument in question (x
i
), or rather is de-
rived from the verbal predicate that the argument
depends on.
General features: We use and modify 18 fea-
tures used for Japanese from the prior work of
Bj?orkelund et al (2009), excluding SENSE, PO-
SITION, and re-ranker features.
? Stem: a stem without any attachment. For
instance, the first word Poleun at the Figure 1
consists of a stem Pol plus Josa eun.
? POS Lv1: the first level (coarse classifi-
cation) of a POS tag such as noun, verb,
adjective, or adverb.
639
Feature Description
A-Stem, P-Stem Stem of an argument and a predicate
A-POS Lv1, P-POS Lv1 Coarse-grained POS of A-Stem and P-Stem
A-POS Lv2, P-POS Lv2 Fine-grained POS of A-Stem and P-Stem
A-Case, P-Case Case of A-Stem and P-Stem
A-LeftmostChildStem Stem of the leftmost child of an argument
A-LeftSiblingStem Stem of the left sibling of an argument
A-LeftSiblingPOS Lv1 Coarse-grained POS of A-LeftSiblingStem
A-LeftSiblingPOS Lv2 Fine-grained POS of A-LeftSiblingStem
A-RightSiblingPOS Lv1 Coarse-grained POS of a stem of the right sibling of an argument
A-RightSiblingPOS Lv2 Fine-grained POS of a stem of the right sibling of an argument
P-ParentStem Stem of a parent of a predicate
P-ChildStemSet Set of stems of children of a predicate
P-ChildPOSSet Lv1 Set of coarse POS of P-ChildStemSet
P-ChildCaseSet Set of cases of P-childStemSet
A-JosaExist If 1, Josa exists in an argument, otherwise 0.
A-JosaClass Linguistic classification of Josa
A-JosaLength Number of morphemes consisting of Josa
A-JosaMorphemes Each morpheme consisting of Josa
A-JosaIdenity Josa of an argument
A-EomiExist If 1, Eomi exists in an argument, otherwise 0.
A-EomiClass Lv1 Linguistic classification of Eomi
A-EomiClass Lv2 Another linguistic classification of Eomi
A-EomiLength Number of morphemes consisting of Eomi
A-EomiMorphemes Each morpheme consisting of Eomi
A-EomiIdentity Eomi of an argument
A-StemRepr Stem representation of an argument
A-JosaRepr Josa representation of an argument
A-EomiRepr Eomi representation of an argument
Table 2: Features used in our SRL experiments. Features are grouped as General, Korean-specific, or
Latent Morpheme Representations. For the last group, we employ three different methods to build them:
(i) CCA, (ii) deep learning, and (iii) Brown clustering.
? POS Lv2: the second level (fine classifica-
tion) of a POS tag. If POS Lv1 is noun, ei-
ther a proper noun, common noun, or other
kinds of nouns is the POS Lv2.
? Case: the case type such as SBJ, OBJ, or
COMP.
The above features are also applied to some depen-
dency children, parents, and siblings of arguments
as shown in Table 2.
Korean-specific features: We have 11 different
kinds of features for the Josa (5) and Eomi (6). We
highlight several below:
? A-JosaExist: an indicator feature checking
any Josa whether or not exists in an argument.
It is set to 1 if any Josa exists, otherwise 0.
? A-JosaClass: the linguistic classification of
Josa with a total of 8 classes. These classes
are adverbial, auxiliary, complemental, con-
nective, determinative, objective, subjective,
and vocative.
? A-JosaLength: the number of morphemes
consisting of Josa. At most five morphemes
are combined to consist of one Josa in our
data set.
? A-JosaMorphemes: Each morpheme com-
posing the Josa.
? A-JosaIdentity: The Josa itself.
? A-EomiClass Lv1: the linguistic classifica-
tion of Eomi with a total of 14 classes. These
14 classes are adverbial, determinative, coor-
dinate, exclamatory, future tense, honorific,
imperative, interrogative, modesty, nominal,
normal, past tense, petitionary, and subordi-
nate.
? A-EomiClass Lv2: Another linguistic classi-
fication of Eomi with a total of 4 classes. The
four classes are closing, connection, prefinal,
and transmutation. The EomiClass Lv1 and
Lv2 are combined to display the characteris-
tic of Eomi such as ?Nominal Transmutation
Eomi?, but not all combinations are possible.
640
Corpus Gen Gen+Kor
Gen+Kor+LMR
CCA Deep Brown All
PKPB 64.83% 75.17% 75.51% 75.43% 75.55% 75.54%
Our annotated corpus 66.88% 80.33% 80.88% 80.84% 80.77% 81.07%
PKPB + our annotated corpus 64.86% 78.61% 79.32% 79.44% 78.91% 79.20%
Table 3: Experimental F1-score results on every experiment. Abbreviation on features are Gen: general
features, Kor: Korean specific features, LMR: latent morpheme representation features.
Latent morpheme representation features: To
alleviate the sparsity, a lingering problem in NLP,
we employ three kinds of latent morpheme repre-
sentations induced from a larger body of unsuper-
vised text data. These are (i) linear continuous rep-
resentation through Canonical Correlation Analy-
sis (Dhillon et al, 2012), (ii) non-linear contin-
uous representation through Deep learning (Col-
lobert and Weston, 2008), and (iii) discrete rep-
resentation through Brown Clustering (Tatu and
Moldovan, 2005).
The first two representations are 50 dimensional
continuous vectors for each morpheme, and the
latter is a set of 256 clusters over morphemes.
6 Experiments and Results
We categorized our experiments by the scenarios
below, and all results are summarized in Table 3.
The F1-score results were investigated for each
scenario. We randomly divided our data into 90%
training and 10% test sets for all scenarios.
For latent morpheme representations, we used
the Donga news article corpus.
3
The Donga cor-
pus contains 366,636 sentences with 25.09 words
on average. The Domain of this corpus cov-
ers typical news articles such as health, entertain-
ment, technology, politics, world and others. We
ran Kokoma Korean morpheme analyzer
4
on each
sentence of the Donga corpus to divide words into
morphemes to build latent morpheme representa-
tions.
1st Scenario: We first tested on general features
in previous work (2nd column in Table 3). We
achieved 64.83% and 66.88% on the PKPB and
our corpus. When the both corpora were com-
bined, we had 64.86%.
2nd Scenario: We then added the Korean-
specific morphological features to signify its ap-
3
http://www.donga.com
4
http://kkma.snu.ac.kr/
propriateness in this scenario. These features in-
creased greatly performance improvements (3rd
column in Table 3). Although both the PKPB
and our corpus had improvements, the improve-
ments were the most notable on our corpus. This
is because PKPB POS tags might be too coarse.
We achieved 75.17%, 80.33%, and 78.61% on the
PKPB, our corpus, and the combined one, respec-
tively.
3rd Scenario: This scenario is to reveal the ef-
fects of the different latent morpheme represen-
tations (4-6th columns in Table 3). These three
representations are from CCA, deep learning, and
Brown clustering. The results gave evidences that
all representations increased the performance.
4th Scenario: We augmented our model with all
kinds of features (the last column in Table 3). We
achieved our best F1-score of 81.07% over all sce-
narios on our corpus.
7 Conclusion
For Korean SRL, we semantically annotated a
corpus containing detailed morphological annota-
tion. We then developed a supervised model which
leverages Korean-specific features and a variety
of latent morpheme representations to help deal
with a sparsity problem. Our best model achieved
81.07% in F1-score. In the future, we will con-
tinue to build our corpus and look for the way to
use unsupervised learning for SRL to apply to an-
other language which does not have available cor-
pus.
Acknowledgments
We thank Na-Rae Han and Asli Celikyilmaz for
helpful discussion and feedback. This research
was supported by the Basic Science Research Pro-
gram of the Korean National Research Foundation
(NRF), and funded by the Korean Ministry of Ed-
ucation, Science and Technology (2010-0010612).
641
References
Tasos Anastasakos, Young-Bum Kim, and Anoop Deo-
ras. 2014. Task specific continuous word represen-
tations for mono and multi-lingual spoken language
understanding. In Proceedings of the IEEE Interna-
tional Conference on Acoustics, Speech, and Signal
Processing (ICASSP).
Anders Bj?orkelund, Love Hafdell, and Pierre Nugues.
2009. Multilingual semantic role labeling. In Pro-
ceedings of the Thirteenth Conference on Compu-
tational Natural Language Learning: Shared Task,
pages 43?48. Association for Computational Lin-
guistics.
Alena B?ohmov?a, Jan Haji?c, Eva Haji?cov?a, and Barbora
Hladk?a. 2003. The prague dependency treebank. In
Treebanks, pages 103?127. Springer.
Claire Bonial, Olga Babko-Malaya, Jinho D Choi, Jena
Hwang, and Martha Palmer. 2010. Propbank an-
notation guidelines. Center for Computational Lan-
guage and Education Research, CU-Boulder.
Aljoscha Burchardt, Nils Reiter, Stefan Thater, and
Anette Frank. 2007. A semantic approach to tex-
tual entailment: system evaluation and task analy-
sis. In Proceedings of the ACL-PASCAL Workshop
on Textual Entailment and Paraphrasing, RTE ?07,
pages 10?15, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Ronan Collobert and Jason Weston. 2008. A unified
architecture for natural language processing: Deep
neural networks with multitask learning. In Pro-
ceedings of the 25th international conference on
Machine learning, pages 160?167. ACM.
Paramveer Dhillon, Jordan Rodu, Dean Foster, and
Lyle Ungar. 2012. Two step cca: A new spec-
tral method for estimating vector models of words.
arXiv preprint arXiv:1206.6403.
Anette Frank, Hans-Ulrich Krieger, Feiyu Xu, Hans
Uszkoreit, Berthold Crysmann, Brigitte J?org, and
Ulrich Sch?afer. 2007. Question answering from
structured knowledge sources. Journal of Applied
Logic, 5(1):20 ? 48. Questions and Answers: Theo-
retical and Applied Perspectives.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic
labeling of semantic roles. Computational linguis-
tics, 28(3):245?288.
Andrew B Goldberg, Xiaojin Zhu, Charles R Dyer,
Mohamed Eldawy, and Lijie Heng. 2008. Easy
as abc?: facilitating pictorial communication via
semantically enhanced layout. In Proceedings of
the Twelfth Conference on Computational Natural
Language Learning, pages 119?126. Association for
Computational Linguistics.
Jan Haji?c, Massimiliano Ciaramita, Richard Johans-
son, Daisuke Kawahara, Maria Ant`onia Mart??, Llu??s
M`arquez, Adam Meyers, Joakim Nivre, Sebastian
Pad?o, Jan
?
St?ep?anek, et al 2009. The conll-2009
shared task: Syntactic and semantic dependencies
in multiple languages. In Proceedings of the Thir-
teenth Conference on Computational Natural Lan-
guage Learning: Shared Task, pages 1?18. Associa-
tion for Computational Linguistics.
Daisuke Kawahara, Sadao Kurohashi, and K?oiti
Hasida. 2002. Construction of a japanese relevance-
tagged corpus. In LREC.
John Lafferty, Andrew McCallum, and Fernando CN
Pereira. 2001. Conditional random fields: Prob-
abilistic models for segmenting and labeling se-
quence data.
Srini Narayanan and Sanda Harabagiu. 2004. Ques-
tion answering based on semantic structures. In
Proceedings of the 20th international conference on
Computational Linguistics, COLING ?04, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Martha Palmer, Shijong Ryu, Jinyoung Choi, Sinwon
Yoon, and Yeongmi Jeon. 2006. Korean propbank.
Linguistic data consortium.
Hye-Jeong Song, Chan-Young Park, Jung-Kuk Lee,
Min-Ji Lee, Yoon-Jeong Lee, Jong-Dae Kim, and
Yu-Seop Kim. 2012. Construction of korean se-
mantic annotated corpus. In Computer Applications
for Database, Education, and Ubiquitous Comput-
ing, pages 265?271. Springer.
Marta Tatu and Dan Moldovan. 2005. A seman-
tic approach to recognizing textual entailment. In
Proceedings of the conference on Human Language
Technology and Empirical Methods in Natural Lan-
guage Processing, pages 371?378. Association for
Computational Linguistics.
Mariona Taul?e, Maria Ant`onia Mart??, and Marta Re-
casens. 2008. Ancora: Multilevel annotated corpora
for catalan and spanish. In LREC.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: a simple and general method
for semi-supervised learning. In Proceedings of the
48th Annual Meeting of the Association for Compu-
tational Linguistics, pages 384?394. Association for
Computational Linguistics.
Nianwen Xue and Martha Palmer. 2009. Adding se-
mantic roles to the chinese treebank. Natural Lan-
guage Engineering, 15(01):143?172.
642
