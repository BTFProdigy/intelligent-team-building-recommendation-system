Development of Indonesian Large Vocabulary Continuous Speech
Recognition System within A-STAR Project
Sakriani Sakti1,2, Eka Kelana3, Hammam Riza4, Shinsuke Sakai1,2
Konstantin Markov1,2, Satoshi Nakamura1,2
1National Institute of Information and Communications Technology, Japan
2ATR Spoken Language Communication Research Laboratories, Japan
3R&D Division, PT Telekomunikasi Indonesia, Indonesia
4Agency for the Assessment and Application of Technology, BPPT, Indonesia
{sakriani.sakti,shinsuke.sakai,konstantin.markov,satoshi.nakamura}@atr.jp,
eka k@telkom.co.id, hammam@iptek.net.id
Abstract
The paper outlines the development of a
large vocabulary continuous speech recog-
nition (LVCSR) system for the Indonesian
language within the Asian speech transla-
tion (A-STAR) project. An overview of the
A-STAR project and Indonesian language
characteristics will be briefly described. We
then focus on a discussion of the develop-
ment of Indonesian LVCSR, including data
resources issues, acoustic modeling, lan-
guage modeling, the lexicon, and accuracy
of recognition. There are three types of In-
donesian data resources: daily news, tele-
phone application, and BTEC tasks, which
are used in this project. They are available in
both text and speech forms. The Indonesian
speech recognition engine was trained using
the clean speech of both daily news and tele-
phone application tasks. The optimum per-
formance achieved on the BTEC task was
92.47% word accuracy.
1 A-STAR Project Overview
The A-STAR project is an Asian consortium that
is expected to advance the state-of-the-art in multi-
lingual man-machine interfaces in the Asian region.
This basic infrastructure will accelerate the devel-
opment of large-scale spoken language corpora in
Asia and also facilitate the development of related
fundamental information communication technolo-
gies (ICT), such as multi-lingual speech translation,
Figure 1: Outline of future speech-technology ser-
vices connecting each area in the Asian region
through network.
multi-lingual speech transcription, and multi-lingual
information retrieval.
These fundamental technologies can be applied to
the human-machine interfaces of various telecom-
munication devices and services connecting Asian
countries through the network using standardized
communication protocols as outlined in Fig. 1. They
are expected to create digital opportunities, improve
our digital capabilities, and eliminate the digital di-
vide resulting from the differences in ICT levels in
each area. The improvements to borderless commu-
nication in the Asian region are expected to result
in many benefits in everyday life including tourism,
business, education, and social security.
The project was coordinated together by the Ad-
vanced Telecommunication Research (ATR) and the
National Institute of Information and Communica-
tions Technology (NICT) Japan in cooperation with
several research institutes in Asia, such as the Na-
tional Laboratory of Pattern Recognition (NLPR) in
China, the Electronics and Telecommunication Re-
search Institute (ETRI) in Korea, the Agency for the
Assessment and Application Technology (BPPT)
in Indonesia, the National Electronics and Com-
puter Technology Center (NECTEC) in Thailand,
the Center for Development of Advanced Comput-
ing (CDAC) in India, the National Taiwan Univer-
sity (NTU) in Taiwan. Partners are still being sought
for other languages in Asia.
More details about the A-STAR project can be
found in (Nakamura et al, 2007).
2 Indonesian Language Characteristic
The Indonesian language, or so-called Bahasa In-
donesia, is a unified language formed from hun-
dreds of languages spoken throughout the Indone-
sian archipelago. Compared to other languages,
which have a high density of native speakers, In-
donesian is spoken as a mother tongue by only 7%
of the population, and more than 195 million people
speak it as a second language with varying degrees
of proficiency. There are approximately 300 eth-
nic groups living throughout 17,508 islands, speak-
ing 365 native languages or no less than 669 di-
alects (Tan, 2004). At home, people speak their own
language, such as Javanese, Sundanese or Balinese,
even though almost everybody has a good under-
standing of Indonesian as they learn it in school.
Although the Indonesian language is infused with
highly distinctive accents from different ethnic lan-
guages, there are many similarities in patterns across
the archipelago. Modern Indonesian is derived from
the literary of the Malay dialect. Thus, it is closely
related to the Malay spoken in Malaysia, Singapore,
Brunei, and some other areas.
Unlike the Chinese language, it is not a tonal
language. Compared with European languages, In-
donesian has a strikingly small use of gendered
words. Plurals are often expressed by means of word
repetition. It is also a member of the agglutina-
tive language family, meaning that it has a complex
range of prefixes and suffixes, which are attached to
base words. Consequently, a word can become very
long.
More details on Indonesian characteristics can be
found in (Sakti et al, 2004).
3 Indonesian Phoneme Set
The Indonesian phoneme set is defined based on In-
donesian grammar described in (Alwi et al, 2003).
A full phoneme set contains 33 phoneme symbols in
total, which consists of 10 vowels (including diph-
thongs), 22 consonants, and one silent symbol. The
vowel articulation pattern of the Indonesian lan-
guage, which indicates the first two resonances of
the vocal tract, F1 (height) and F2 (backness), is
shown in Fig. 2.
 
 
High
 
 
 
          
Mid
 
 
                     
Low
 
i
 
Front        Central      Back 
e
 
e2
 
u
 
o 
a
 
Figure 2: Articulatory pattern of Indonesian vowels.
It consists of vowels, i.e., /a/ (like ?a? in ?father?),
/i/ (like ?ee? in ?screen?), /u/ (like ?oo? in ?soon?),
/e/ (like ?e? in ?bed?), /e2/ (a schwa sound, like ?e?
in ?learn?), /o/ (like ?o? in ?boss?), and four diph-
thongs, /ay/, /aw/, /oy/ and /ey/. The articulatory pat-
tern for Indonesian consonants can be seen in Table
1.
4 Indonesian Data Resources
Three types of Indonesian data resources available
in both text and speech forms were used here. The
first two resources were developed or processed by
the R&D Division of PT Telekomunikasi Indone-
sia (R&D TELKOM) in collaboration with ATR as
continuation of the APT project (Sakti et al, 2004),
while the third one was developed by ATR under the
A-STAR project in collaboration with BPPT. They
are described in the following.
Table 1: Articulatory pattern of Indonesian consonants.
Bilabial Labiodental Dental/Alveolar Palatal Velar Glotal
Plosives p, b t, d k, g
Affricates c, j
Fricatives f s, z sy kh h
Nasal m n ny ng
Trill r
Lateral l
Semivowel w y
4.1 Text Data
The three text corpora are:
1. Daily News Task
There is already a raw source of Indonesian
text data, which has been generated by an In-
donesian student (Tala, 2003). The source is a
compilation from ?KOMPAS? and ?TEMPO?,
which are currently the largest and most widely
read Indonesian newspaper and magazine. It
consists of more than 3160 articles with about
600,000 sentences. R&D TELKOM then fur-
ther processed them to generate a clean text
corpus.
2. Telephone Application Task
About 2500 sentences from the telephone
application domain were also generated by
R&D TELKOM, and were derived from some
daily dialogs from telephone services, includ-
ing tele-home security, billing information ser-
vices, reservation services, status tracking of
e-Government services, and also hearing im-
paired telecommunication services (HITSs).
3. BTEC Task
The ATR basic travel expression corpus
(BTEC) has served as the primary source
for developing broad-coverage speech transla-
tion systems (Kikui et al, 2003). The sen-
tences were collected by bilingual travel ex-
perts from Japanese/English sentence pairs in
travel domain ?phrasebooks?. BTEC has also
been translated into several languages includ-
ing French, German, Italian, Chinese and Ko-
rean. Under the A-STAR project, there are also
plans to collect synonymous sentences from the
different languages of the Asian region. ATR
has currently successfully collected an Indone-
sian version of BTEC tasks, which consists of
160,000 sentences (with about 20,000 unique
words) of a training set and 510 sentences of a
test set with 16 references per sentence. There
are examples of BTEC English sentences and
synonymous Indonesian sentences in Table 2.
Table 2: Examples of English-Indonesian bilingual
BTEC sentences.
English Indonesian
Good Evening Selamat Malam
I like strong coffee Saya suka kopi yang kental
Where is the boarding Di manakah pintu
gate? keberangkatan berada?
How much is this? Harganya berapa?
Thank you Terima kasih
4.2 Speech Data
The three speech corpora are:
1. Daily News Task
From the text data of the news task described
above, we selected phonetically-balanced sen-
tences, then recorded the speech utterances.
Details on the phonetically-balanced sentences,
the recording set-up, speaker criteria, and
speech utterances are described in what fol-
lows:
? Phonetically-Balanced Sentences
We selected phonetically-balanced sen-
tences using the greedy search algorithm
(Zhang and S.Nakamura, 2003), resulting
in 3168 sentences in total (see Table 3).
Table 3: Number of phonetically-balanced sentences
resulting from greedy search algorithm.
Phone # Units # Sentences
Monophones 33 6
Left Biphones 809 240
Right Biphones 809 242
Triphones 9667 2978
Total 3168
? Recording Set-Up
Speech recording was done by R&D
TELKOM in Bandung, Java, Indonesia. It
was conducted in parallel for both clean
and telephone speech, recorded at respec-
tive sampling frequency of 16 kHz and 8
kHz. The system configuration is outlined
in Fig. 3.
Sennheizer
microphone
Microphone
pre-amplifier
& ADC
PC
Phone
PABX
R&D TELKOM
building
Phone USB
Microphone
pre-amplifier
& ADC
USB
Sound-proofed room
Figure 3: Recording set-up.
? Speaker Criteria
The project will require a lot of time,
money, and resources to collect all of
the possible languages and dialects of
the tribes recognized in Indonesia. In
this case, R&D TELKOM only focused
on the major ethnic accents in Bandung
area where the actual telecommunication
services will be implemented. Four
main accents were selected, including:
Batak, Javanese, Sundanese, and standard
Indonesian (no accent) with appropriate
distributions as outlined in Fig. 4. Both
genders are evenly distributed and the
speakers? ages are also distributed as out-
lined in Fig. 5. The largest percentage is
those aged 20-35 years who are expected
to use the telecommunication services
more often.
Figure 4: Accent distribution of 400 speakers in
daily news and telephone application tasks.
Figure 5: Age distribution of 400 speakers in daily
news and telephone application tasks.
? Speech Utterances
The total number of speakers was 400
(200 males and 200 females). Each
speaker uttered 110 sentences resulting
in a total of 44,000 speech utterances or
about 43.35 hours of speech.
2. Telephone Application Task
The utterances in speech of 2500 telephone
application sentences were recorded by R&D
TELKOM in Bandung, Indonesia using the
same recording set-up as that for the news task
corpus. The total number of speakers, as well
as appropriate distributions for age and accent,
were also kept the same. Each speaker uttered
100 sentences resulting in a total of 40,000 ut-
terances (36.15 hours of speech).
3. BTEC Task
From the test set of the BTEC text data pre-
viously described, 510 sentences of one refer-
ence were selected and the recordings of speech
were then done by ATR in Jakarta, Indone-
sia. BPPT helped to evaluate the preliminary
recordings. For this first version, we only se-
lected speakers who spoke standard Indonesian
(no accent). There were 42 speakers (20 males
and 22 females) and each speaker uttered the
same 510 BTEC sentences, resulting in a total
of 21,420 utterances (23.4 hours of speech).
5 Indonesian Speech Recognizer
The Indonesian LVCSR system was developed us-
ing the ATR speech recognition engine. The clean
speech of both daily news and telephone applica-
tion tasks were used as the training data, while the
BTEC task was used as an evaluation test set. More
details on the parameter set-up, acoustic modeling,
language modeling, pronunciation dictionary and
recognition accuracy will be described in the follow-
ing.
5.1 Parameter Set-up
The experiments were conducted using feature ex-
traction parameters, which were a sampling fre-
quency of 16 kHz, a frame length of a 20-ms Ham-
ming window, a frame shift of 10 ms, and 25 dimen-
sional MFCC features (12-order MFCC, ? MFCC
and ? log power).
5.2 Segmentation Utterances
Segmented utterances according to labels are usu-
ally used as a starting point in speech recognition
systems for training speech models. Automatic seg-
mentation is mostly used since it is efficient and less
time consuming. It is basically produced by forced
alignment given the transcriptions. In this case, we
used an available Indonesian phoneme-based acous-
tic model developed using the English-Indonesian
cross language approach (Sakti et al, 2005).
5.3 Acoustic Modeling
Three states were used as the initial HMM for each
phoneme. A shared state HMnet topology was then
obtained using a successive state splitting (SSS)
training algorithm based on the minimum descrip-
tion length (MDL) optimization criterion (Jitsuhiro
et al, 2004). Various MDL parameters were eval-
uated, resulting in context-dependent triphone sys-
tems having different version of total states. i.e.,
1,277 states, 1,944 states and 2,928 states. All tri-
phone HMnets were also generated with three dif-
ferent versions of Gaussian mixture components per
state, i.e., 5, 10, and 15 mixtures.
5.4 Language Modeling
Word bigram and trigram language models were
trained using the 160,000 sentences of the BTEC
training set, yielding a trigram perplexity of 67.0 and
an out-of-vocabulary (OOV) rate of 0.78% on the
510 sentences of the BTEC test set. This high per-
plexity could be due to agglutinative words in the
Indonesian language.
5.5 Pronunciation Dictionary
About 40,000 words from an Indonesian pronun-
ciation dictionary were manually developed by In-
donesian linguists and this was owned by R&D
TELKOM. This was derived from the daily news
and telephone application text corpora, which con-
sisted of 30,000 original Indonesian words plus
8,000 person and place names and also 2,000 of for-
eign words. Based on these pronunciations, we then
included additional words derived from the BTEC
sentences.
5.6 Recognition Accuracy
The performance of the Indonesian speech recog-
nizer with different versions of total states and Gaus-
sian mixture components per state is graphically
depicted in Fig. 6. On average, they achieved
92.22% word accuracy. The optimum performance
was 92.47% word accuracy at RTF=0.97 (XEON 3.2
GHz), which was obtained by the model with 1.277
total states and 15 Gaussian mixture components per
state.
Figure 6: Recognition accuracy of Indonesian
LVCSR on BTEC test set.
6 Conclusion
We have presented the results obtained from the pre-
liminary stages of an Indonesian LVCSR system.
The optimum performance achieved was 92.47%
word accuracy at RTF=0.97. A future development
will be to implement it on a real speech-to-speech
translation system using computer terminals (tablet
PCs). To further refine the system, speaker adap-
tation as well as environmental or noise adaptation
needs to be done in the near future.
References
H. Alwi, S. Dardjowidjojo, H. Lapoliwa, and A.M. Moe-
liono. 2003. Tata Bahasa Baku Bahasa Indonesia (In-
donesian Grammar). Balai Pustaka, Jakarta, Indone-
sia.
T. Jitsuhiro, T. Matsui, and S. Nakamura. 2004. Au-
tomatic generation of non-uniform HMM topologies
based on the MDL criterion. IEICE Trans. Inf. & Syst.,
E87-D(8):2121?2129.
G. Kikui, E. Sumita, T. Takezawa, and S. Yamamoto.
2003. Creating corpora for speech-to-speech trans-
lation. In Proc. EUROSPEECH, pages 381?384,
Geneva, Switzerland.
S. Nakamura, E. Sumita, T. Shimizu, S. Sakti, S. Sakai,
J. Zhang, A. Finch, N. Kimura, and Y. Ashikari. 2007.
A-star: Asia speech translation consortium. In Proc.
ASJ Autumn Meeting, page to appear, Yamanashi,
Japan.
S. Sakti, P. Hutagaol, A. Arman, and S. Nakamura. 2004.
Indonesian speech recognition for hearing and speak-
ing impaired people. In Proc. ICSLP, pages 1037?
1040, Jeju, Korea.
S. Sakti, K. Markov, and S.Nakamura. 2005. Rapid de-
velopment of initial indonesian phoneme-based speech
recognition using cross-language approach. In Proc.
Oriental COCOSDA, pages 38?43, Jakarta, Indonesia.
F. Tala. 2003. A Study of Stemming Effects on Infor-
mation Retrieval in Bahasa Indonesia. Ph.D. thesis,
The Information and Language System (ILPS) Group,
Informatics Institute, University of Amsterdam, Ams-
terdam, Netherland.
J. Tan. 2004. Bahasa indonesia: Between faqs and facts.
http://www.indotransnet.com/article1.html.
J. Zhang and S.Nakamura. 2003. An efficient algorithm
to search for a minimum sentence set for collecting
speech database. In Proc. ICPhS, pages 3145?3148,
Barcelona, Spain.
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 1124?1132, Dublin, Ireland, August 23-29 2014.
Discriminative Language Models as a Tool for
Machine Translation Error Analysis
Koichi Akabe Graham Neubig Sakriani Sakti Tomoki Toda Satoshi Nakamura
Graduate School of Information Science
Nara Institute of Science and Technology
8916-5 Takayama-cho, Ikoma-shi, Nara, Japan
{akabe.koichi.zx8, neubig, ssakti, tomoki, s-nakamura}@is.naist.jp
Abstract
In this paper, we propose a new method for effective error analysis of machine translation (MT)
systems. In previous work on error analysis of MT, error trends are often shown by frequency.
However, if we attempt to perform a more detailed analysis based on frequently erroneous word
strings, the word strings also often occur in correct translations, and analyzing these correct sen-
tences decreases the overall efficiency of error analysis. In this paper, we propose the use of
regularized discriminative language models (LMs) to allow for more focused MT error analysis.
In experiments, we demonstrate that our method is more efficient than frequency-based analysis,
and examine differences across systems, language pairs, and evaluation measures. 1
1 Introduction
Accuracy of Statistical Machine Translation (SMT) systems is continually increasing, but systems are
now more complex than ever before. As a result, not all effects of making modifications to a system are
known without actually making the modification and generating translations. Therefore, in the process
of developing an SMT system, it is common to evaluate actual translations to identify problems to make
improvements. This process is time consuming, as it is often necessary to analyze a large number of
translations to get an overall grasp of the system?s error trends. In addition, many sentences will contain
no errors, or only errors from the long tail that are not representative of the system as a whole. On the
other hand, if we are able to detect and rank important errors automatically, we will likely be able to find
representative errors of the SMT system more efficiently.
Previous work has proposed methods for automatic error analysis of MT systems based on automati-
cally separating errors into classes and sorting these classes by frequency (Vilar et al., 2006; Popovic and
Ney, 2011). These classes cover common mistakes of MT systems, e.g. conjugation, reordering, word
deletion, and insertion. This makes it possible to view overall error trends, but when the goal of analysis
is to identify errors to make some concrete improvement to the system, it is often necessary to perform a
more focused analysis, looking at actual errors made by a particular language pair or system. We show
examples of errors types that are informative, but are language- or task-specific, and not covered by pre-
vious methods in Figure 1. In this example, the type given by more standard error typologies is indicated
by ?Traditional type,? but we would prefer a more detailed analysis such as ?Fine-grained type,? would
allows us to take specific steps to fix the machine translation system (such as ensuring that Wikipedia
titles are not punctuated, or normalizing full-width characters to half-width). These fine-grained types
are difficult to conceive without actually observing the MT system output, but if we are able to group ac-
tual errors into fine-grained classes based on, for example, lexical clues, this sort of analysis will become
possible and more efficient.
Previous research on improving the efficiency of error analysis has generally focused on grouping error
types by frequency, but try to apply such frequency-based techniques to individual errors, selected errors
1Our implementation is available open-source at https://github.com/vbkaisetsu/dlm-analyzer
This work is licensed under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
1124
Src ?? ?? ??
Ref the academic exchange agreement
MT academic exchange agreement .
Traditional type Insertion error
Fine-grained type Insertion error (unneeded period)
Src ?? ? ?? ? ???? ? ???? ? ? ?? ?? ?
Ref prince kakugyoho -lrb- 1075 - 1105 -rrb- ninna-ji monzeki
MT imperial prince kakugyo -lrb-???? - 1105 -rrb- : ninna-ji temple ruins
Traditional type Replacement error or Unknown word
Fine-grained type Unknown word (number) or Half-/Full-width error
Figure 1: Example of errors in Japanese to English translation, classified into traditional, or more fine-
grained and useful classes.
1-gram 2-gram
the 61 (BOS) the 42
, 47 . (EOS) 41
and 43 , and 32
of 42 of the 27
: 42 in the 21
Table 1: Frequently occurring erroneous n-grams
are often dominated by frequently occurring linguistic phenomena that are not necessarily indicative
of translation errors. To show examples of this problem, in Table 1 we provide a list of erroneous n-
grams that were produced by an MT system (described in Section 4.1) but not contained in the respective
references. From this table, we can see that frequently occurring erroneous n-grams are simply n-grams
that frequently occur in English, and because of this we cannot discover characteristic errors of the system
for improvement just from this information.
In this paper, we propose a new method that uses regularized discriminative LMs to solve the above
problem. Discriminative LMs are LMs trained to fix common output errors of a particular system. From
the viewpoint of error analysis, if we train a discriminative LM using n-gram features and examine the
weights learned by this model, n-grams with large negative or positive weights will be indicative of pat-
terns that are over- or under-produced by the MT system. Because the weights are specifically trained to
fix errors, it is likely that these patterns will be more informative than mistakes that are simply frequently
occurring. We can also use a number of features of discriminative LMs to perform a more focused and
efficient analysis. For example, if we perform training with L1 regularization, many features will be
removed and only important patterns will remain in the model. Additionally, we can focus on specific
varieties of errors by changing the evaluation measure used for training the LMs.
In our experiments, we validate the effectiveness of error analysis based on discriminative LMs. We
perform a manual evaluation of the n-gram patterns discovered by random selection, by frequency-based
analysis, and by the proposed method. As a result, the proposed method is more effective at identifying
errors than other methods.
2 Discriminative Language Models
In this section, we first introduce the discriminative LM used in our method. As a target for our analysis,
we have input sentences F = {F
1
, . . . , F
K
}, n-best outputs ?E = { ?E
1
, . . . ,
?
E
K
} of an MT system, and
reference translations R = {R
1
, . . . , R
K
}. Discriminative LMs define feature vectors ?(E
i
) for each
candidate in ?E
k
= {E
1
, E
2
, . . . , E
I
}, and calculate inner products w ? ?(E
i
) as scores.
To train the weight vector w, we first calculate evaluation scores of all candidates using a sentence-
level evaluation measure EV such as BLEU+1 (Lin and Och, 2004) given the reference sentence R
k
.
1125
We choose the sentence with the highest evaluation EV as an oracle E?
k
. Oracles are chosen for each
n-best, and we train w so that the oracle?s score becomes higher than the other candidates.
2.1 Structured Perceptron
While there are a number of methods for training discriminative LMs, we follow Roark et al. (2007)
in using the structured perceptron as a simple and effective method for LM training. The structured
perceptron is a widely used on-line learning method that examines one training instance and updates
the weight vector using the difference between feature vectors generated from the oracle E? and the
hypothesis ?E calculated by the current model. For each iteration, w is updated using the difference
between E? and ?E. If ?E is equal to E?, the difference becomes 0, so no update is performed. This
process is run for all F sequentially, and iterated until weights converge or we reach a fixed iteration
limit N . We show the above procedure in Algorithm 1.
Algorithm 1 Structured perceptron training of the discriminative LM
for n = 1 to N do
for all ?E ? ?E do
E
?
? arg max
E?
?E
EV (E)
?
E ? arg max
E?
?E
w ? ?(E)
w ? w + ?(E
?
)? ?(
?
E)
end for
end for
2.2 Learning Sparse Discriminative LMs
While the structured perceptron is a simple and effective method for learning discriminative LMs, it also
has no bias towards reducing the number of features used in the model. However, if we add a bias towards
learning smaller models, we can keep only salient features (Tsuruoka et al., 2009).
In our work, we use L1 regularization to add this bias. L1 regularization gives a penalty to w pro-
portional to the L1 norm ?w?
1
=
?
i
|w
i
|, pushing a large number of elements in w to 0, so ineffective
features are removed from the model.
To train L1 regularized discriminative LMs, we use the forward-backward splitting (FOBOS) algo-
rithm proposed by Duchi and Singer (2009). FOBOS splits update and regularization, and lazily calcu-
lates the regularization upon using the weight to improve efficiency.
2.3 Features of Discriminative LMs
In the LM, we used the following three features:
1. System score feature ?
s
: As our goal is fixing the output of the system, we add this feature to allow
a default ordering of n-bests by score.
2. n-gram feature?
n
: We add a binary feature counting the frequency of eachn-gram in the hypothesis.
The weights of these features will be the main target of our analysis.
3. Hypothesis length feature ?
l
: If the evaluation measure has a penalty for the number of words, this
allows us to adjust it.
In this work, we do not use other features, but our method theoretically allows for addition of other
features such as POS tags or syntactic information, which could also potentially be used as a target for
analysis.
3 Discriminative LMs for Error Analysis
In this section, we describe how to incorporate information from discriminative LMs into manual error
analysis.
1126
Error types
Replacement (Context dependent)
(Context independent)
Insertion
Deletion
Reordering
Conjugation
Polarity
Unknown words
Table 2: Error categories for annotation
Src ? ??? ? ? ?? ?
Ref kyo-chan -lrb- city bus -rrb-
MT <s> kyoto chan -lrb- kyoto city bus -rrb- </s>
Rules SYMP ( x0:SYM SYMP ( NP ( NN ( ??? ) NN ( ???? ) ) x1:SYM ) )?
x0 ?kyoto? ?city? ?bus? x1
Eval Insertion error
Src ?? ?? ?? 13 ?
Ref there are 13 open patents .
MT <s> the number of public patent 13 cases </s>
Rules NP ( NP ( x0:NN x1:NN ) NN ( ???? ) )? ?number? ?of? x0 x1
NN ( ???? )? ?public?
Eval Context-dependent replacement error
Figure 2: Example of the evaluation sheet. Boxed words are chosen n-grams.
3.1 Focused Error Analysis of MT output
We first define the following general framework for focused analysis of errors in MT output. Using this,
we can find error trends of chosen n-grams:
1. Automatically choose potentially erroneous n-grams in the MT output.
2. Select one or more 1-best translations that contain each chosen n-gram.
3. Show selected translations to an annotator with the selected n-gram highlighted.
4. The annotator looks at the indicated n-gram, and marks whether or not by examining the n-gram
whether they were able to identify an error in the MT output. If the answer is ?yes,? the annotator
additionally indicates which variety of error was found according to Table 2.
A part of the actual evaluation sheet is shown in Fig. 2. The first four rows are the input, and the final
row is the annotator?s evaluation.
3.2 Selection of Target n-grams
We can think of the following three methods for choosing potentially erroneous n-grams:
Random: n-grams that are selected randomly. This corresponds to the standardmethod of error analysis,
where sentences are randomly sampled and analyzed.
1127
Sent Words
English Japanese
Train 330k 5.91M 6.09M
Dev 1166 24.3k 26.8k
Test 1160 26.7k 28.5k
Table 3: Data size of KFTT
Frequency: n-grams that are most frequently over-generated (occur in the hypothesis, but not in the
references). This corresponds to a focused version of the frequency-based automatic error analysis
methods of Vilar et al. (2006) and Popovic and Ney (2011).
LM: n-grams that have the lowest weight according to the discriminative LM. This is our proposed
method.
In particular, for discriminative LMs, n-gram features that have large positive or negative weights
indicate n-grams that are under-generated or over-generated by the system. Therefore, by examining
high-weighted or low-weighted n-grams, it is likely that we will be able to get a grasp of the system
mistakes. When performing actual evaluation, we want to analyze n-grams with 1-best translations.
Almost high-weighted n-grams are only contained in oracle translations, and not contained in 1-best
translation. Therefore, we use low-weighted n-grams for evaluation. If the discriminative LM is properly
trained, low-weighted n-grams will often correspond to actual errors.
3.3 System Comparison
When developing MT systems, it is common to not only evaluate a single system, but also compare
multiple systems, such as when comparing a new system with baselines.
To do this in the current work, we create discriminative LMs from n-bests generated by multiple
translation systems, and choose representative n-grams using the proposed method. Then we examine
the selected n-grams in context and then compare the result of this analysis.
4 Experiments
We evaluate the effectiveness of our method by performing a manual evaluation over three translation
systems, two translation directions, and two evaluation measures.
4.1 Experiment Setup
For each MT system, we use Japanese-English data from the KFTT (Neubig, 2011) as a corpus. The size
of the corpus is shown in Table 3. In our experiment, we use a forest-to-string (f2s) system trained using
the Travatar toolkit (Neubig, 2013) for single system evaluation. For system comparison, we compare the
above f2s system with a phrase based (pbmt) system and a hierarchical phrase based (hiero) system
built using Moses (Koehn et al., 2007).
The f2s system is built using Nile2 for making word alignments, and syntax trees generated with
Egret3. pbmt andhiero are built usingGIZA++ (Och andNey, 2003) for word alignments. Each system
is optimized using MERT (Och, 2003) with BLEU (Papineni et al., 2002) as an evaluation measure. For
single system evaluation, we also use the reordering-oriented evaluation metric RIBES (Isozaki et al.,
2010) as additional metric for training the discriminative LM.
For training discriminative LMs, our method uses the structured perceptron with 100 iterations and
FOBOS for L1 regularization as described in Section 2.2. The regularization factor is chosen from the
range 10?6-10?2 to give the highest performance on the KFTT test data.
LMs are trained using 500-bests from each MT system and features described in Section 2.3. We use
1-grams to 3-grams as n-gram features.
2http://code.google.com/p/nile/
3http://code.google.com/p/egret-parser/
1128
System BLEU(dev) BLEU(test)
Original LM applied Original LM applied
pbmt 0.2929 0.3521 0.2460 0.2485
hiero 0.2953 0.3859 0.2616 0.2562
f2s 0.2958 0.3887 0.2669 0.2676
Table 4:  Translation accuracy of each system, without LMs and with LMs
Method Ja? En En? Ja
Random 0.46 0.37
Frequency 0.30 0.31
LM 0.55 0.48
Table 5: Precision of top 30 n-grams that select errors in both directions
We show translation accuracies of each system before and after training in Table 4. From this table, we
can see that the LM increases the accuracy of all dev data, but it does not necessarily have a large effect
for the test data. The main reason for this is because the development set used to train the LM is relatively
small, at only 1166 sentences. However, as our goal in this paper is to perform error analysis on set of
data which we already have parallel references (in this case, the development set), the generalization
ability of the model is not necessarily fundamental to our task at hand. We directly identify the ability to
identify errors in the next section.
4.2 Evaluation of Error Identification Ability
This section evaluates the ability of our method to identify errors in MT output. As we are proposing
our method as a tool for manual analysis of MT output, it is necessary to perform manual evaluation to
ensure that our method is identifying locations that are actually erroneous according to human subjective
evaluation. To measure the accuracy of each method, we perform an evaluation as described in Section
3.1 and use the precision of selectedn-grams (the percentage of selectedn-grams for which then annotator
indicated that an error actually existed) as our evaluation measure. The annotator is an MT specialist who
is proficient in English and Japanese. The order of the evaluation sentences is shuffled so the annotator
can not determine which method was responsible for choosing each n-gram.
0 10 20 30 40 50 60 70 80 90 1000.0
0.5
1.0
# of selected n-grams
Precisi
on
FrequencyLMRandom
Figure 3: Precision of n-grams that select errors (Japanese to English)
We show the precision results for each number of selected n-grams over three methods for Japanese-
English translation in Fig. 3, and the precision of the top 30 n-grams in both directions in Table 5. From
1129
n-gram Weight Examples
-rrb- of -7.50950 Src ?? ?? ? ?? ? ? ? ? ? ? ??? ?? ? ? ? ?? ? ????
? ? ? ? ? ? ? ? ? ? ? ? ? ? ?? ? ?
Ref his achievements were evaluated by emperor go-daigo , and he was awarded the
letter -lrb- ? -rrb- , which came from the emperor ?s real name takaharu -lrb-
?? -rrb- , so he changed the letter in his name from ??? ? to ??? ? .
MT <s> it is regarded as a valor in the fall of the bakufu , and was the first character of
takaharu , imina -lrb-???? -rrb- of emperor godaigo , and changed his name
to takauji . </s>
Eval Reordering error
<s> the first -6.55510 (Only contained in other candidates in n-bests)
senior -6.52024 Src ?? ? ? ?? ?? ? ?? ? ? ? ?? ?? ? ?? ?? ?? ? ?
?? ?
Ref kyoryukai-this organization consists of teachers of junior high , high , and other
schools who are ryukoku university graduates .
MT <s> graduates of?? association - ryukoku university , and is a organization con-
sisting of teachers such as senior . </s>
Eval Context independent replacement error
the ko clan -6.52021 Src ?? ? ? ? ? ? ?? ? ? ? ??? ? ?? ? ?? ? ? ? ? ?
? ? ?
Ref in this fighting , takewakamaru , the son of takauji ?s concubine , was killed .
MT <s> on this occasion , was killed during the confusion??? , the son of a concu-
bine of the ko clan . </s>
Eval Context dependent replacement error
foundation of -6.50773 Src ?? ?? ? ? ? ? ? ? ? ?? ?? ? ? ? ? ?? ? ? ? ?
? ? ? ?? ? ?? ?
Ref the family name comes from the fact that the kujo family lived in kujo-den , which
was located in kyoto kujo and said to have been built by fujiwara no mototsune .
MT <s> the origin of the family name that lived in kujo dono , which was located in
kyoto kujo is said to be a foundation of fujiwara no mototsune . </s>
Eval Context dependent replacement error
Table 6: Top 5 erroneous n-grams learned by the discriminative LM and examples. Boxes on MT indi-
cates the selected n-gram, and boxes in Src and Ref indicate the corresponding words.
these results, we can see that each method is able to detect erroneous n-grams, but the proposed method
achieves a precision that outperforms other methods.
To demonstrate why this is the case, in Table 6 we show examples, in context, of potentially erroneous
n-grams chosen by our proposed method. Compared to the baseline n-grams in Table 1, we can see that
these n-grams are not limited to frequently occurring n-grams in English, and are more likely to have a
high probability of indicating actual errors.
In addition, to give a better idea of the prominence of the selected n-grams, in Table 7, we show
the mean number of locations of the KFTT test data that contain the top 100 n-grams selected by each
method. We can see that randomly selected n-grams are rarely contained in the separate test set, while
the proposed method tends to select n-grams that are more frequent than random, and thus have a better
chance of generalizing.
4.3 Effect of Evaluation Measure Choice
We can also hypothesize that by varying the evaluation measure used in training the LM, we can select
different varieties of errors for analysis. To test this, we compare analysis results obtained using one
1130
Method Ja? En En? Ja
Random 1.1 1.5
Frequency 381.0 432.6
LM 6.2 14.0
Table 7: Mean number of occurrences of selected n-grams in the test set
Type +BLEU +RIBES
Actual Error 0.55 0.41
Replacement (Context dependent) 0.36 0.30
(Context independent) 0.15 0
Insertion 0.17 0.25
Deletion 0.18 0.10
Reordering 0.14 0.27
Conjugation 0 0.08
Polarity 0 0
Unknown words 0 0
Table 8: Error statistics found when optimizing different metrics. Bold indicates the higher score.
LM optimized with BLEU and another with RIBES, which is a reordering-oriented evaluation metric.
We show a breakdown of the identified errors in Table 8. From this table, we can see that the BLEU-
optimized LM is able to detect more deletion errors than the RIBES-optimized LM. This is a natural result,
as the BLEU metric puts a heavier weight on the brevity penalty assigned to shorter translations. On the
other hand, the RIBES-optimized LM detects more reordering errors than the BLEU-optimized LM. The
RIBES metric is sensitive to reordering errors, and thus reordering errors will cause larger decreases in
RIBES. From this experiment, we can see that it is possible to focus on different error types by using
different metrics in the optimization of the LM.
4.4 Result of System Comparison
Finally, we examine whether discriminative LMs allow us to grasp characteristic errors for system com-
parison. Similarly with single system analysis, we generated the top 30 potentially erroneous n-grams
for pbmt, hiero, and f2s in two directions, and evaluated them manually. The result is listed in Table
9. From this table, we can see that pbmt and hiero count reordering errors as one of the three most
frequent types, while f2s does not, especially for English to Japanese. This is consistent with common
knowledge that syntactic information can be used to improve reordering accuracy. We can also see in-
sertion is a problem when translating into English, and conjugation is a problem when translating into
morphologically-rich Japanese. While these are only general trends, they largely match with intuition,
even after analysis of only the top 30 n-grams.
5 Conclusion
In this paper, we proposed a new method for efficiently analyzing the output of MT systems using L1
regularized discriminative LMs, and evaluate its effectiveness. As a result, weights trained by discrim-
inative LMs are more effective at identifying errors than n-grams chosen either randomly or by error
frequency. This indicates that our method allows an MT system engineer to inspect fewer sentences in
the course of identifying characteristic errors of the MT system.
The overall framework of using discriminative LMs in error analysis opens up a number of directions
for future work, and there are a number of additional points we plan to analyze in the future. For example,
while it is clear that the proposedmethod allows errors to be identifiedmore efficiently, it is still necessary
to quantify the overall benefit of having an MT expert use the result of this error analysis to improve
1131
Type Ja? En En? Ja
pbmt hiero f2s pbmt hiero f2s
Actual Error 0.58 0.60 0.55 0.81 0.64 0.48
Replacement (Context dependent) 0.41 0.33 0.36 0.10 0.17 0.52
(Context independent) 0.03 0.08 0.15 0.55 0.03 0.12
Insertion 0.26 0.22 0.17 0.06 0.13 0.15
Deletion 0.10 0.09 0.18 0.07 0.14 0.06
Reordering 0.13 0.28 0.14 0.19 0.32 0.04
Conjugation 0.07 0 0 0.04 0.20 0.12
Polarity 0 0 0 0 0.01 0
Unknown words 0 0 0 0 0 0
Table 9: Error statistics of three systems with in both directions. Bold scores are the top 3 most occuring
error types in each system.
an MT system. In addition, we plan on examining the effect of using larger training data for the LM,
incorporating different features based on POS patterns or syntactic features, and using more sophisticated
training methods.
References
John Duchi and Yoram Singer. 2009. Efficient online and batch learning using forward backward splitting. In
Journal of Machine Learning Research, volume 10.
Hideki Isozaki, Tsutomu Hirao, Kevin Duh, Katsuhito Sudoh, and Hajime Tsukada. 2010. Automatic evaluation
of translation quality for distant language pairs. In Proc. EMNLP, pages 944?952.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke
Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, and Evan
Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In Proc. ACL, pages 177?180.
Chin-Yew Lin and Franz Josef Och. 2004. Orange: a method for evaluating automatic evaluation metrics for
machine translation. In Proc. COLING, pages 501?507.
Graham Neubig. 2011. The Kyoto free translation task. http://www.phontron.com/kftt.
Graham Neubig. 2013. Travatar: A forest-to-string machine translation engine based on tree transducers. In Proc.
ACL.
Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models.
Computational Linguistics, 29(1):19?51.
Franz Josef Och. 2003. Minimum error rate training in statistical machine translation. In Proc. ACL.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. BLEU: a method for automatic evaluation
of machine translation. In Proc. ACL, pages 311?318.
Maja Popovic and Hermann Ney. 2011. Towards automatic error analysis of machine translation output. In
Computational Linguistics, pages 657?688.
Brian Roark, Murat Saraclar, and Michael Collins. 2007. Discriminative n-gram language modeling. Computer
Speech & Language, 21(2):373?392.
Yoshimasa Tsuruoka, Jun?ichi Tsujii, and Sophia Ananiadou. 2009. Stochastic gradient descent training for l1-
regularized log-linear models with cumulative penalty. In Proc. ACL, pages 477?485.
David Vilar, Jia Xu, Luis Fernando D?Haro, and Hermann Ney. 2006. Error analysis of statistical machine trans-
lation output. In Proc. LREC, pages 697?702.
1132
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 1706?1717, Dublin, Ireland, August 23-29 2014.
Reinforcement Learning of Cooperative Persuasive Dialogue Policies
using Framing
Takuya Hiraoka, Graham Neubig, Sakriani Sakti, Tomoki Toda, Satoshi Nakamura
Nara Institute of Science and Technology (NAIST), Nara, Japan
{takuya-h,neubig,ssakti,tomoki,s-nakamura}@is.naist.jp
Abstract
In this paper, we apply reinforcement learning for automatically learning cooperative persuasive
dialogue system policies using framing, the use of emotionally charged statements common in
persuasive dialogue between humans. In order to apply reinforcement learning, we describe a
method to construct user simulators and reward functions specifically tailored to persuasive dia-
logue based on a corpus of persuasive dialogues between human interlocutors. Then, we evaluate
the learned policy and the effect of framing through experiments both with a user simulator and
with real users. The experimental evaluation indicates that applying reinforcement learning is
effective for construction of cooperative persuasive dialogue systems which use framing.
1 Introduction
With the basic technology supporting dialogue systems maturing, there has been more interest in recent
years about dialogue systems that move beyond the traditional task-based or chatter bot frameworks. In
particular there has been increasing interest in dialogue systems that engage in persuasion or negotiation
(Georgila and Traum, 2011; Georgila, 2013; Paruchuri et al., 2009; Heeman, 2009; Mazzotta and de
Rosis, 2006; Mazzotta et al., 2007; Nguyen et al., 2007; Guerini et al., 2003). We concern ourselves
with cooperative persuasive dialogue systems (Hiraoka et al., 2013), which try to satisfy both the user
and system goals. For these types of systems, creating a system policy that both has persuasive power
and is able to ensure that the user is satisfied is the key to the system?s success.
In recent years, reinforcement learning has gained much attention in the dialogue research community
as an approach for automatically learning optimal dialogue policies. The most popular framework for
reinforcement learning in dialogue models is based on Markov decision processes (MDP) and partially
observable Markov decision processes (POMDP). In these frameworks, the system gets a reward repre-
senting the degree of success of the dialogue. Reinforcement learning enables the system to learn a policy
maximizing the reward. Traditional reinforcement learning requires thousands of dialogues, which are
difficult to collect with real users. Therefore, a user simulator which simulates the behavior of real users
is used for generating training dialogues. Most research in reinforcement learning for dialogue system
policies has been done in slot-filling dialogue, where the system elicits information required to provide
appropriate services for the user (Levin et al., 2000; Williams and Young, 2007).
There is also ongoing research on applying reinforcement learning to persuasion and negotiation
dialogues, which are different from slot-filling dialogue (Georgila and Traum, 2011; Georgila, 2013;
Paruchuri et al., 2009; Heeman, 2009). In slot-filling dialogue, the system is required to perform the
dialogue to achieve the user goal, eliciting some information from a user to provide an appropriate ser-
vice. A reward corresponding to the achievement of the user?s goal is given to the system. In contrast,
in persuasive dialogue, the system convinces the user to take some action achieving the system goal.
Thus, in this setting, reward corresponding to the achievement of both the user?s and the system?s goal is
given to the system. The importance of each goal will vary depending on the use case of the system. For
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer
are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/
1706
example, a selfish system could be rewarded with an emphasis on only achievement of the system goal,
and a cooperative system could be rewarded with an emphasis on achievement of both of the goals. In
addition, negotiation dialogue could be considered as a kind of the persuasive dialogue where the user
also tries to convince the system to achieve the user?s goal.
In this paper, our research purpose is learning better policies for cooperative persuasive dialogue sys-
tems using framing. We focus on learning a policy that tries to satisfy both the user and system goals. In
particular, two elements in this work set it apart from previous works:
? We introduce framing (Irwin et al., 2013), which is known to be important for persuasion and a
key concept of this paper, as a system action. Framing uses emotionally charged words to explain
particular alternatives. In the context of research that applies reinforcement learning to persuasive
(or negotiation) dialogue, this is the first work that considers framing as a system action.
? We use a human-to-human persuasive dialogue corpus of Hiraoka et al. (2014) to train predictive
models for achievement of a human persuadee?s and a human persuader?s goals, and introduce these
models to reward calculation to enable the system to learn a policy reflecting knowledge of human
persuasion.
To achieve our research purpose, we construct a POMDP where the reward function and user simulator
are learned from a corpus of human persuasive dialogue. We define system actions based on framing and
general dialogue acts. In addition, the system dialogue state (namely, belief state) is defined for tracking
the system?s rewards. Then, we evaluate the effect of framing and learning a system policy. Experimental
evaluation is done through a user simulator and real users.
2 Reinforcement learning
Reinforcement learning is a machine learning technique for learning a system policy. The policy is a
mapping function from a dialogue state to a particular system action. In reinforcement learning, the
policy is learned by maximizing the reward function. Reinforcement learning is often applied to models
based on the framework of MDP or POMDP.
In this paper, we follow a POMDP-based approach. A POMDP is defined as a tuple
?S,A, P,R,O,Z, ?, b
0
? where S is the set of states (representing different contexts) which the system
may be in (the system?s world), A is the set of actions of the system, P : S ?A ? P (S,A) is the set of
transition probabilities between states after taking an action, R : S?A ? ? is the reward function, O is
a set of observations that the system can receive about the world, Z is a set of observation probabilities
Z : S ? A ? Z(S,A), and ? a discount factor weighting longterm rewards. At any given time step i
the world is in some unobserved state s
i
? S. Because s
i
is not known exactly, we keep a distribution
over states called a belief state b, thus b(s
i
) is the probability of being in state s
i
, with initial belief state
b
0
. When the system performs an action ?
i
? A based on b, following a policy pi : S ? A, it receives
a reward r
i
(s
i
, ?
i
) ? ? and transitions to state s
i+1
according to P (s
i+1
|s
i
, ?
i
) ? P . The system then
receives an observation o
i+1
according to P (o
i+1
|s
i+1
, ?
i
). The quality of the policy pi followed by the
agent is measured by the expected future reward also called Q-function, Q
pi
: S ?A ? ?.
In this framework, it is critical to be able to learn a good policy function. In order to do so, we use
Neural fitted Q Iteration (Riedmiller, 2005) for learning the system policy. Neural fitted Q Iteration is an
offline value-based method, and optimizes the parameters to approximate the Q-function. Neural fitted
Q Iteration repeatedly performs 1) sampling training experience using a POMDP through interaction and
2) training a Q-function approximator using training experience. Neural fitted Q Iteration uses a multi-
layered perceptron as the Q-function approximator. Thus, even if the Q-function is complex, Neural
fitted Q Iteration can approximate the Q-function better than using a linear approximation function
1
.
3 Persuasive dialogue corpus
In this section, we give a brief overview of Hiraoka et al. (2014)?s persuasive dialogue corpus between
human participants that we will use to estimate the models described in later sections.
1
In a preliminary experiment, we found that Neural fitted Q Iteration had high performance compared to using the linear
approximation of the Q-function in this domain.
1707
Table 1: The beginning of a dialogue from the cor-
pus (translated from Japanese)
Speaker Transcription GPF Tag
Cust Well, I am looking for a camera, PROPQ
do you have camera B?
Sales Yes, we have camera B. ANSWER
Sales Did you already take a look at
it somewhere? PROPQ
Cust Yes. On the Internet. ANSWER
Sales It is very nice. Don?t you think? PROPQ
Cust Yes, that?s right, yes. INFORM
Table 2: Sytem and user?s GPF tags
Inform Answer Question PropQ
SetQ Commisive Directive
Table 3: An example of positive framing
(Camera A is) able to achieve performance
of comparable single-lens cameras
and can fit in your pocket, this is a point.
3.1 Outline of persuasive dialogue corpus
As a typical example of persuasive dialogue, the corpus consists of dialogues between a salesperson
(persuader) and customer (persuadee). The salesperson attempts to convince the customer to purchase a
particular product (decision) from a number of alternatives (decision candidates). This type of dialogue
is defined as ?sales dialogue.? More concretely, the corpus assumes a situation where the customer is in
an appliance store looking for a camera, and the customer must decide which camera to purchase from 5
alternatives.
Prior to recording, the salesperson is given the description of the 5 cameras and instructed to try to
convince the customer to purchase a specific camera (the persuasive target). In this corpus, the persuasive
target is camera A, and this persuasive target is invariant over all subjects. The customer is also instructed
to select one preferred camera from the catalog of the cameras, and choose one aspect of the camera that
is particularly important in making their decision (the determinant). During recording, the customer and
the salesperson converse and refer to the information in the camera catalog as support for their dialogues.
The customer can close the dialogue whenever they want, and choose to buy a camera, not buy a camera,
or reserve their decision for a later date.
The corpus includes a role-playing dialogue with participants consisting of 3 salespeople from 30 to
40 years of age and 19 customers from 20 to 40 years of age. All salespeople have experience working
in an appliance store. The total number of dialogues is 34, and the total time is about 340 minutes. Table
1 show an example transcript of the beginning of one dialogue. Further examples are shown in Table 8
in the appendix.
3.2 Annotated dialogue acts
Each utterance is annotated with two varieties of tags, the first covering dialogue acts in general, and the
rest covering framing.
As a tag set to represent traditional dialogue acts, we use the general-purpose functions (GPF) defined
by the ISO international standard for dialogue act annotation (ISO24617-2, 2010). All annotated GPF
tags are defined to be one of the tags in this set (Table 2).
More relevant to this work is the framing annotation. Framing uses emotionally charged words to
explain particular alternatives. It has been suggested that humans generally evaluate decision candidates
by selecting based on several determinants weighted by the user?s preference, and that framing is an
effective way of increasing persuasive power. This corpus focuses on negative/positive framing (Irwin
et al., 2013; Mazzotta and de Rosis, 2006), with negative framing using negative words and positive
framing using positive words.
In the corpus, framing is defined as a tuple ?a, p, r? where a represents the target alternative, p takes
value NEG if the framing is negative, and POS if the framing is positive, and r represents whether the
framings contains a reference to the persuadees preferred determinant (for example, the performance or
price of a camera), taking the value TRUE if contained, and FALSE if not contained. The user?s preferred
determinant is annotated based on the results of a questionnaire.
Table 3 shows an example of positive framing (p=POS) about the performance of Camera A (a=A). In
this example, the customer answered that his preference is the price of camera, and this utterance does
1708
Figure 1: Dynamic Bayesian network of the user simulator. Each node represents a variable, and each
edge represents a probabilistic dependency. The system cannot observe the shaded variables.
not contain any description of price. Thus, r=NO is annotated. Further examples of positive and negative
framing are shown in Tables 9 and 10 in the appendix.
In this paper, we re-perform annotation of the framing tags and evaluate inter-annotator agreement,
which is slightly improved from Hiraoka et al. (2014). Two annotators are given the description and
examples of tags (e.g. what a positive word is), and practice with these manuscripts prior to annotation.
In corpus annotation, at first, each annotator independently chooses the framing sentences. Then, framing
tags are independently annotated to all utterances chosen by the two annotators. The inter-annotator
agreement of framing polarity is 96.9% (kappa=0.903).
4 User simulator
In this section, we describe a statistical dialogue model for the user (customer in Section 3). This model
is used to simulate the system?s conversational partner in applying reinforcement learning.
The user simulator estimates two aspects of the conversation:
1. The user?s general dialogue act.
2. Whether the preferred determinant has been conveyed to the user (conveyed preferred determinant;
CPD).
The users? general dialogue act is represented by using GPF. For example, in Table 1, PROPQ, ANSWER,
and INFORM appear as the user?s dialogue act. In our research, the user simulator chooses one GPF
described in Table 2 or None representing no response at each turn. CPD represents that the user
has been convinced that the determinant in the persuader?s framing satisfies the user?s preference. For
example, in Table 3, the ?performance? is contained in the clerk?s positive framing for camera A. If the
persuadee is convinced that the decision candidate satisfies his/her preference based on this framing,
we say that CPD has occurred (r=YES)
2
. In our research, the user simulator models CPD for each of
the 5 cameras. This information is required to calculate reward described in the following Section 5.1.
Specifically, GPF and CPD are used for calculating naturalness and persuasion success, which are part
of the reward function.
The user simulator is based on an order one Markov chain, and Figure 1 shows its dynamic Bayesian
network. The user?s GPF G
t+1
user
and CPD C
t+1
alt
at turn t + 1 are calculated by the following equations.
P (G
t+1
user
|G
t
user
, F
t
sys
, G
t
sys
, S
alt
) (1)
P (C
t+1
alt
|C
t
alt
, F
t
sys
, G
t
sys
, S
alt
) (2)
G
t
sys
represents the system GPF at time t. F
t
sys
represents the system framing at t. These two variables
correspond to system actions, and are explained in Section 5.2. G
t
user
represents the user?s GPF at t.
C
t
alt
represents the CPD at t. S
alt
represents the users?s original evaluation of the alternatives. In our
2
Note that the persuader does not necessarily know if r=YES because the persuader is not certain of the user?s preferred
determinants.
1709
research, this is the camera that the user selected as a preferred camera at the beginning of the dialogue
3
.
We use the persuasive dialogue corpus described in Section 3 for training the user simulator, considering
the customer in the corpus as the user and the salesperson in the corpus as the system. In addition, we
use logistic regression for learning Equations (1) and (2).
5 Learning cooperative persuasion policies
Now that we have introduced the user model, we describe the system?s dialogue management. In par-
ticular, we describe the reward, system action, and belief state, which are required for reinforcement
learning.
5.1 Reward
We follow Hiraoka et al. (2014) in defining a reward function according to three factors: user satisfac-
tion, system persuasion success, and naturalness. As described in Section 1, we focus on developing
cooperative persuasive dialogue systems. Therefore, the system must perform dialogue to achieve both
the system and user goals. In our research, we define three elements of the reward function as follows:
Satisfaction The user?s goal is represented by subjective user satisfaction. The reason why we use
satisfaction is that the user?s goal is not necessarily clear for the system (and system creator) in
persuasive dialogue. For example, some users may want the system to recommend appropriate
alternatives, while some users may want the system not to recommend, but only give information
upon the user?s request. As the goal is different for each user, we use abstract satisfaction as a
measure, and leave it to each user how to evaluate achievement of the goal.
Persuasive success The system goal is represented by persuasion success. Persuasion success represents
whether the persuadee finally chooses the persuasive target (in this paper, camera A) at the end of
the dialogue. Persuasion success takes the value SUCCESS when the customer decides to purchase
the persuasive target at the end of dialogue, and FAILURE otherwise.
Naturalness In addition, we use naturalness as one of the rewards. This factor is known to enhance the
learned policy performance for real users (Meguro et al., 2011).
The reward at each turn t is calculated with the following equation
4
.
r
t
= (Sat
t
user
+ PS
t
sys
+ N
t
)/3 (3)
Sat
t
user
represents a 5 level score of the user?s subjective satisfaction (1: Not satisfied?3: Neutral?
5: Satisfied) at turn t scaled into the range between 0 and 1. PS
t
sys
represents persuasion success (1:
SUCCESS?0: FAILURE) at turn t. N
t
represents bi-gram likelihood of the dialogue between system and
user at turn t as follows.
N
t
= P (F
t
sys
, G
t
sys
, G
t
user
|F
t?1
sys
, G
t?1
sys
, G
t?1
user
) (4)
In our research, Sat and PS are calculated with a predictive model constructed from the human per-
suasion dialogue corpus described in Section 3. In constructing these predictive models, the persuasion
results (i.e. persuasion success and persuadee?s satisfaction) at the end of dialogue are given as the su-
pervisory signal, and the dialogue features in Table 4 are given as the input. In the reward calculation,
the dialogue features used by the predictive model are calculated by information generated from the dia-
logue of the user simulator and the system. Table 4 shows all features used for reward calculation at each
turn
5
. Note that, for the calculating TOTAL TIME, average speaking time corresponding to speakers and
dialogue acts is added at each turn.
3
Preliminary experiments indicated that the user behaved differently depending on the first selection of the camera, thus we
introduce this variable to the user simulator.
4
We also optimized the policy in the case where the reward (Equation (3)) is given only when dialogue is closed. However,
the convergence of the learning was much longer, and the performance was relatively bad.
5
Originally, there are more dialogue features for the predictive model. However as in previous research, we choose signifi-
cant dialogue features by step-wise feature selection (Terrell and Bilge, 2012).
1710
Table 4: Features for calculating reward. These
features are also used as the system belief state.
Sat
user
Frequency of system commisive
Frequency of system question
PS
sys
Total time
C
alt
(for each 6 cameras)
S
alt
(for each 6 cameras)
N System and user current GPF
System and user previous GPF
System framing
Table 5: System framing. Pos represents positive
framing and Neg represents negative framing. A, B,
C, D, E represent camera names.
Pos A Pos B Pos C Pos D Pos E None
Neg A Neg B Neg C Neg D Neg E
Table 6: System action.
<None, ReleaseTurn> <None, CloseDialogue>
<Pos A, Inform> <Pos A, Answer>
<Neg A, Inform> <Pos B, Inform>
<Pos B, Answer> <Pos E, Inform>
<None, Inform> <None, Answer>
<None, Question> <None, Commissive>
<None, Directive>
5.2 Action
The system?s action ?F
sys
, G
sys
? is a framing/GPF pair. These pairs represent the dialogue act of the
salesperson, and are required for reward calculation (Section 5.1). There are 11 types of framing (Table
5), and 9 types of GPF which are expanded by adding RELEASETURN and CLOSEDIALOGUE to the
original GPF sets (Table 2). The number of all possible GPF/framing pairs is 99, and some pairs have not
appeared in the original corpus. Therefore, we reduce the number of actions by filtering. We construct
a unigram model of the salesperson?s dialogue acts P (F
sales
, G
sales
) from the original corpus, then
exclude pairs for which the likelihood is below 0.005
6
. As a result, the 13 pairs shown in Table 6
remained
7
. We use these pairs as the system actions.
5.3 Belief state
The current system belief state is represented by the features used for reward calculation (Table 4) and
the reward calculated at previous turn. Namely, the features for the reward calculation and calculated
reward are also used as the next input of the system policy. Note that the system cannot directly observe
C
alt
, thus the system estimates it through the dialogue by using the following equation.
P (
?
C
t+1
alt
|
?
C
t
alt
, F
t
sys
, G
t
sys
, S
alt
) (5)
where
?
C
t+1
alt
represents the estimated CPD at t + 1.
?
C
t
alt
represents the estimated CPD at t. The other
variables are the same as those in Equation (2). In contrast, we assume that the system can observe
G
user
and S
alt
. G
user
is not usually observable because traditional dialogue systems have automatic
speech recognition/Spoken language understanding errors. However, in this work, we use Wizard of Oz
in place of automatic speech recognition/Spoken language understanding (Section 6.2). Thus, we can
ignore these factors
8
.
6 Experimental evaluation
In this section, we describe the evaluation of the proposed method for learning cooperative persuasive
dialogue policies. Especially, we focus on examining how the learned policy with framing is effective
for persuasive dialogue. The evaluation is done both using a user simulator and real users.
6
We chose this threshold by trying values from 0.001 to 0.01 with incrementation of 0.001. We select the threshold that
resulted in the number of actions closest to previous work (Georgila, 2013).
7
Cameras C and D are not popular, and don?t appear frequently in the human persuasive dialogue corpus, and are therefore
excluded in filtering.
8
In addition to this reason, the G
user
is not so essential to our research (GPF is general dialogue act), and we want to focus
the CPD. This is the other reason that we assume that G
user
is observable.
1711
Figure 2: Average reward of each system. Error bars represents 95% confidence intervals. Rew repre-
sents the reward, Sat represents the user satisfaction, PS represents persuasion success, and Nat represents
naturalness.
6.1 Policy learning and evaluation using the user simulator
For evaluating the effectiveness of framing and learning the policy through the user simulator, we prepare
the following 3 policies.
Random A baseline where the action is randomly output from all possible actions.
NoFraming A baseline where the action is output based on the policy which is learned using only
GPFs. For constructing the actions, we remove actions whose framing is not None from the actions
described in Section 5.2. The policy is a greedy policy, and selects the action with the highest Q-
value.
Framing The proposed method where the action is output based on the policy learned with all actions
described in Section 5.2 including framing. The policy is also a greedy policy.
For learning the policy, we use Neural fitted Q Iteration (Section 2). For applying Neural fitted Q
Iteration, we use the Pybrain library (Schaul et al., 2010). We set the discount factor ? of learning to 0.9,
and the number of nodes in the hidden layer of the neural network for approximating the Q-function to
the sum of number of belief states and actions (i.e. Framing: 53, NoFraming: 47). The policy in learning
is the ?-greedy policy (? = 0.3). These conditions follow the default Pybrain settings. We consider 50
dialogues as one epoch, and update the parameters of the neural network at each epoch. Learning is
finished when number of epochs reaches 200 (10000 dialogues), and the policy with the highest average
reward is used for evaluation.
We evaluate the system on the basis of average reward per dialogue with the user simulator. For
calculating average reward, 1000 dialogues are performed with each policy.
Experimental results (Figure 2) indicate that 1) performance is greatly improved by learning and 2)
framing is somewhat effective for the user simulator. Learned policies (Framing, NoFraming) get a
higher reward than Random. Particularly, both of the learned policies better achieve user satisfaction than
Random. On the other hand, only Framing is able to achieve better persuasion success than Random.
This result indicates that framing is effective for persuasive success. In contrast, naturalness of Framing
is not improved from Random. One of the reasons for this is that variance of Nat is smaller than those
of the other factors, and the optimization algorithm favored the other two factors which had a higher
variance.
6.2 Real user evaluation based on Wizard of Oz
To test whether the gains shown on the user simulator will carry over to an actual dialogue scenario, we
perform an experiment with real human users. In addition to the policies described in Section 6.1, we
add the following policy.
Human An oracle where the action is output based on human selection. In this research, the first author
(who has no formal sales experience, but experience of about 1 year in analysis of camera sales
dialogue) selects the action.
1712
Figure 3: The experimental environment based on Wizard of Oz. The rectangle represents information,
and the cylinder represents a system module. The information flow (dashed line) in the experiment
through the user simulator is also shown for comparison.
Experimental evaluation is conducted, based on the Wizard of Oz framework. In the experiment, the
wizard plays the salesperson, and the evaluator plays the customer. Dialogue is performed between the
wizard and the evaluator. The wizard and evaluator are divided by a partition, and the evaluator cannot
see or detect what the wizard is doing. The evaluator selects his/her preferred camera from the catalog
before starting evaluation. Then, the evaluator starts the dialogue with the wizard who is obeying one
of the policies (Figure 3). In particular, dialogue between wizard and evaluator proceeds based on the
following steps.
1. The evaluator talks to the wizard using the mic. In this step, the evaluators can close the dialogue if
they want.
2. The wizard listens to the evaluator?s utterance, translating the utterance into the appropriate G
user
.
Then, the wizard inputs G
user
to the policy module.
3. The policy module decides action sequences (F
sys
, G
sys
) based on G
user
, then outputs the action to
the utterance database module. This module is constructed from the camera sales corpus (Section
3).
4. The utterance database module searches for similar sentences that match the history of input actions
and G
user
so far, then outputs the top 6 similar utterances to the wizard.
5. The wizard generates the system utterance (Text) using the retrieved sentences. The wizard selects
one sentence which best matches the context
9
. If the wizard determines the sentence is hard to
understand, the wizard can correct the sentence to be more natural.
6. The wizard inputs the system utterance to text-to-speech, then waits for the next evaluator utterance
(back to step 1).
Finally, the evaluator answers the following questionnaire for calculating the evaluation measures in
Section 5.1.
Satisfaction The evaluator?s subjective satisfaction defined as a 5 level score of customer satisfaction
(1: Not satisfied?3: Neutral?5: Satisfied).
Final decision The camera that the customer finally wants to buy.
We use SofTalk (cncc, 2010) as text-to-speech software.
Evaluation criteria are basically same to those of previous section (described in Section 5.1). Note
that in the previous section, Sat
user
and PS
sys
are estimated from the simulated dialogue. In contrast
to the previous section, Sat
user
and PS
sys
are calculated from the result of the real user?s questionnaire
9
Note that the wizard is not allowed to create the utterance with complete freedom, and selects an utterance from the
utterance database even when Human policy is used.
1713
Figure 4: Evaluation results for real users. Error bars represent 95% confidence intervals. Rew represents
the reward, Sat represents the user satisfaction, PS represents persuasion success, and Nat represents
naturalness.
Table 7: Part of a dialogue between Framing and an evaluator (translated from Japanese)
Speaker Transcription Fra GPF
Wiz Which pictures do you want to take? Far or near? None QUESTION
Wiz Camera B has 20x zoom, and this is good. Pos B ANSWER
Wiz How about it? RELEASET
Eva I think B sounds good. ANSWER
Wiz Yes, B is popular with zoom, Pos B INFORM
Wiz But, A has extremely good performance.
Camera A has almost the same parts as a single lens camera,
and is more reasonably priced than a single lens-camera. Pos A ANSWER
Wiz How about it? RELEASET
(described in the previous paragraph)
10
based on the definition of Sat
user
and Sat
user
in Section 6.1. The
naturalness is automatically calculated by the system, in the same manner as described in the previous
section. Finally, reward is calculated considering Sat
user
, PS
sys
and naturalness according to Equation
3.
Participants consist of 13 evaluators (3 female, 10 male) and one wizard. Evaluators perform one
dialogue with the wizard obeying each policy (a total of 4 dialogues) in random order.
Experimental results (Figure 4) indicate that framing is effective in persuasive dialogues with real
users, and that the reward of Framing is higher than NoFraming and Random, and almost equal to
Human. In addition, the score of NoFraming is almost equal to Random. This indicates that despite the
fact that it performed relatively well in the simulation experiment, NoFraming is not an effective policy
for real users. In addition, the score of NoFraming is lower than the score given by the user simulator.
In particular, persuasion success is drastically decreased. This indicates that framing is important for
persuasion.
We can see that some features in human persuasive dialogue appear in the dialogue between users
and the wizard using the Framing policy. An example of a typical dialogue of Framing is shown in
Table 7. The first feature is that the system also recommends camera B when the system does positive
framing of camera A, which is the persuasive target. This feature was found by Hiraoka et al. (2014) to
be an indicator of persuasion success in the camera sales corpus. The second feature is that the system
asks the user about the user?s profile at the first stage of the dialogue. This feature is often found when
user satisfaction is high. The second feature also appeared in the dialogue with NoFraming. However,
NoFraming does not use framing, and asks the user to make a decision (DIRECTIVE). An example
utterance from the DIRECTIVE class is ?Please, decide (which camera you want to buy) after seeing the
catalog?.
Considering the evaluation result of the previous section, we can see that Sat and PS differ between
the user simulator and the real users (p < .05). While the general trend of showing improvements for
10
Note that, though systems estimate the satisfaction and evaluator?s decision at each turn for the belief state, the human
evaluator answers the questionnaire only when the dialogue is closed.
1714
satisfaction and persuasive success is identical in Figures 2 and 4, the systems are given excessively high
Sat in simulation. In addition, systems (especially Framing) are given underestimated PS in simulation.
One of the reasons for this is that the property of dialogue features for the predictive model for reward
differs from previous research (Hiraoka et al., 2014). In this paper, dialogue features for the predictive
model are calculated at each turn. In addition, persuasion success and user satisfaction are successively
calculated at each turn. In contrast, in previous research, the predictive model was constructed with
dialogue features calculated at end of the dialogue. Therefore, it is not guaranteed that the predictive
model estimates appropriate persuasion success and user satisfaction at each turn. Another reason is
that the simulator is not sufficiently accurate to use for reflecting real user?s behavior. Compared to
other works (Meguro et al., 2010; Misu et al., 2012), we are using a relatively small sized corpus for
training the user simulator. Therefore, the user simulator cannot be trained to accurately imitate real user
behavior. Improving the user simulator is an important challenge for future work.
7 Related work
There are a number of related works that apply reinforcement learning to persuasion and negotiation
dialogue. Georgila and Traum (2011) apply reinforcement learning to negotiation dialogue using user
simulators divided into three types representing individualist, collectivist, and altruist. Dialogue between
a florist and a grocer are assumed as an example of negotiation dialogue. In addition, Georgila (2013)
also applies reinforcement learning to two-issue negotiation dialogue where participants have a party,
and decide both the date and food type. A handcrafted user simulator is used for learning the policy
of each participant. Heeman (2009) models negotiation dialogue, assuming a furniture layout task, and
Paruchuri et al. (2009) model negotiation dialogue, assuming the dialogue between a seller and buyer.
Our research differs from these in three major ways. The first is that we use framing, positive or
negative statements about the particular item, which is known to be important for persuasion (Irwin et
al., 2013). By considering framing, the system has the potential to be more persuasive. While there is
one previous example of persuasive dialogue using framing (Mazzotta et al., 2007), this system does not
use an automatically learned policy, relying on handcrafted rules. In contrast, in our research, we apply
reinforcement learning to learn the system policy automatically.
In addition, in these previous works, rewards and belief states are defined with heuristics. In contrast,
in our research, reward is defined on the basis of knowledge of human persuasive dialogue. In particular,
we calculate the reward and belief state using the predictive model of Hiraoka et al. (2014) for estimating
persuasion success and user satisfaction using dialogue features. In the real world, it is unclear what
factors are important for achieving the dialogue goal in many persuasive situations. By considering these
predictions as knowledge of human persuasion, the system can identify the important factors in human
persuasion and can track the achievement of the goal based on these.
Finally, these works do not evaluate the learned policy, or evaluate only in simulation. In contrast, we
evaluate the learned policy with real users.
8 Conclusion
We apply reinforcement learning for learning cooperative persuasive dialogue system policies using
framing. In order to apply reinforcement learning, a user simulator and reward function is constructed
based on a human persuasive dialogue corpus. Then, we evaluate the learned policy and effect of fram-
ing using a user simulator and real users. Experimental evaluation indicates that applying reinforcement
learning is effective for construction of cooperative persuasive dialogue systems that use framing.
In the future, we plan to construct a fully automatic persuasive dialogue system using framing. In this
research, automatic speech recognition, spoken language understanding and natural language generation
are performed by a human Wizard. We plan to implement these modules and evaluate system perfor-
mance. In addition, in this research, corpus collection and evaluation are done in a role-playing situation.
Therefore, we plan to evaluate the system policies in a more realistic situation. We also plan to consider
non-verbal information (Nouri et al., 2013) for estimating persuasive success and user satisfaction.
1715
References
cncc. 2010. SofTalk. http://www35.atwiki.jp/softalk/.
Kallirroi Georgila and David Traum. 2011. Reinforcement learning of argumentation dialogue policies in negoti-
ation. Proceedings of INTERSPEEECH.
Kallirroi Georgila. 2013. Reinforcement learning of two-issue negotiation dialogue policies. Proceedings of the
SIGDIAL.
Marco Guerini, Oliviero Stock, and Massimo Zancanaro. 2003. Persuasion model for intelligent interfaces.
Proceedings of the IJCAI Workshop on Computational Models of Natural Argument.
Peter A. Heeman. 2009. Representing the reinforcement learning state in a negotiation dialogue. Proceedings of
ASRU.
Takuya Hiraoka, Yuki Yamauchi, Graham Neubig, Sakriani Sakti, Tomoki Toda, and Satoshi Nakamura. 2013.
Dialogue management for leading the conversation in persuasive dialogue systems. Proceedings of ASRU.
Takuya Hiraoka, Graham Neubig, Sakriani Sakti, Tomoki Toda, and Satoshi Nakamura. 2014. Construction and
analysis of a persuasive dialogue corpus. Proceedings of IWSDS.
Levin Irwin, Sandra L. Schneider, and Gary J. Gaeth. 2013. All frames are not created equal: A typology and
critical analysis of framing effects. Organizational behavior and human decision processes 76.2.
ISO24617-2, 2010. Language resource management-Semantic annotation frame work (SemAF), Part2: Dialogue
acts. ISO.
Esther Levin, Roberto Pieraccini, and Wieland Eckert. 2000. A stochastic model of human-machine interaction
for learning dialog strategies. Proceedings of ICASSP.
Irene Mazzotta and Fiorella de Rosis. 2006. Artifices for persuading to improve eating habits. AAAI Spring
Symposium: Argumentation for Consumers of Healthcare.
Irene Mazzotta, Fiorella de Rosis, and Valeria Carofiglio. 2007. PORTIA: a user-adapted persuasion system in the
healthy-eating domain. Intelligent Systems.
Toyomi Meguro, Ryuichiro Higashinaka, Yasuhiro Minami, and Kohji Dohsaka. 2010. Controlling listening-
oriented dialogue using partially observable Markov decision processes. Proceedings of COLING.
Toyomi Meguro, Yasuhiro Minami, Ryuichiro Higashinaka, and Kohji Dohsaka. 2011. Wizard of oz evaluation of
listening-oriented dialogue control using pomdp. Proceedings of ASRU.
Teruhisa Misu, Kallirroi Georgila, Anton Leuski, and David Traum. 2012. Reinforcement learning of question-
answering dialogue policies for virtual museum guides. Proceedings of the 13th Annual Meeting of SigDial.
Hien Nguyen, Judith Masthoff, and Pete Edwards. 2007. Persuasive effects of embodied conversational agent
teams. Proceedings of HCI.
Elnaz Nouri, Sunghyun Park, Stefan Scherer, Jonathan Gratch, Peter Carnevale, Louis-Philippe Morency, and
David Traum. 2013. Prediction of strategy and outcome as negotiation unfolds by using basic verbal and
behavioral features. Proceedings of INTERSPEECH.
Praveen Paruchuri, Nilanjan Chakraborty, Roie Zivan, Katia Sycara, Miroslav Dudik, and Geoff Gordon. 2009.
POMDP based negotiation modeling. Proceedings of the first MICON.
Martin Riedmiller. 2005. Neural fitted Q iteration - first experiences with a data efficient neural reinforcement
learning method. Machine Learning: ECML.
Tom Schaul, Justin Bayer, Daan Wierstra, Yi Sun, Martin Felder, Frank Sehnke, Thomas R?uckstie?, and J?urgen
Schmidhuber. 2010. Pybrain. The Journal of Machine Learning Research.
Allison Terrell and Mutlu Bilge. 2012. A regression-based approach to modeling addressee backchannels. Pro-
ceedings of the 13th Annual Meeting of SIGDIAL.
Jason D. Williams and Steve Young. 2007. Scaling POMDPs for spoken dialog management. IEEE Transactions
on Audio, Speech, and Language Processing.
1716
Appendix
Table 8: The summary of one dialogue in the corpus (translated from Japanese)
Speaker Transcription GPF Tag
Customer Hello. INFORM
Customer I?m looking for a camera for traveling. Do you have any recommendations? PROPQ
Clerk What kind of pictures do you want to take? SETQ
Customer Well, I?m the member of a tennis club,
and want to take a picture of landscapes or tennis. ANSWER
Clerk O.K. You want the camera which can take both far and near. Don?t you? PROPQ
Clerk Well, have you used a camera before? PROPQ
Customer I have used a digital camera. But the camera was cheap and low resolution. ANSWER
Clerk I see. I see. Camera A is a high resolution camera.
A has extremely good resolution compared with other cameras.
Although this camera does not have a strong zoom,
its sensor is is almost the same as a single-lens camera. INFORM
Customer I see. INFORM
Clerk For a single lens camera,
buying only the lens can cost 100 thousand yen.
Compared to this, this camera is a bargain. INFORM
Customer Ah, I see. INFORM
Customer But, it?s a little expensive. right? PROPQ
Customer Well, I think, camera B is good at price. INFORM
Clerk Hahaha, yes, camera B is reasonably priced. ANSWER
Clerk But its performance is low compared with camera A. INFORM
Customer If I use the two cameras will I be able to tell the difference? PROPQ
Clerk Once you compare the pictures taken by these cameras,
you will understand the difference immediately.
The picture itself is very high quality.
But, camera B and E are lower resolution,
and the picture is a little bit lower quality. ANSWER
Customer Is there also difference in normal size pictures? PROPQ
Clerk Yes, whether the picture is small or large, there is a difference ANSWER
Customer Considering A has single-lens level performance, it is surely reasonable. INFORM
Clerk I think so too. INFORM
Clerk The general price of a single-lens is about 100 or 200 thousand yen.
Considering these prices, camera A is a good choice. INFORM
Customer Certainly, I?m interested in this camera. INFORM
Clerk Considering its performance, it is a bargain. INFORM
Customer I think I?ll go home, compare the pictures, and think a little more. COMMISIVE
Clerk I see. Thank you. DIRECTIVE
Table 9: Example positive framing of a salesperson?s utterance ?a
i
= B, p
i
= POS, r
i
= YES?. In this
example, the customer has indicated price as the preferred determinant.
Hahaha, yes, camera B is reasonably priced.
Table 10: Example negative framing of a salesperson?s utterance ?a
i
= B, p
i
= NEG, r
i
= NO?. In this
example, the customer has indicated price as the preferred determinant.
But, considering the long term usage, you might care about picture quality.
You might change your mind if you only buy a small camera (Camera B).
1717
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 128?132,
Gothenburg, Sweden, April 26-30 2014.
c?2014 Association for Computational Linguistics
Acquiring a Dictionary of Emotion-Provoking Events
Hoa Trong Vu
?,?
, Graham Neubig
?
, Sakriani Sakti
?
, Tomoki Toda
?
, Satoshi Nakamura
?
?
Graduate School of Information Science, Nara Institute of Science and Technology
8916-5 Takayama-cho, Ikoma-shi, Nara, Japan
?
Vietnam National University, University of Engineering and Technology
E3 Building - 144 Xuan Thuy Street, Cau Giay, Hanoi, Vietnam
Abstract
This paper is concerned with the discov-
ery and aggregation of events that provoke
a particular emotion in the person who
experiences them, or emotion-provoking
events. We first describe the creation of a
small manually-constructed dictionary of
events through a survey of 30 subjects.
Next, we describe first attempts at auto-
matically acquiring and aggregating these
events from web data, with a baseline from
previous work and some simple extensions
using seed expansion and clustering. Fi-
nally, we propose several evaluation meas-
ures for evaluating the automatically ac-
quired events, and perform an evaluation
of the effectiveness of automatic event ex-
traction.
1 Introduction
?You look happy today, did something good hap-
pen?? This is a natural question in human dia-
logue, and most humans could think of a variety of
answers, such as ?I met my friends? or ?I passed a
test.? In this work, we concern ourselves with cre-
ating resources that answer this very question, or
more formally ?given a particular emotion, what
are the most prevalent events (or situations, con-
texts) that provoke it??
1
Information about these
emotion-provoking events is potentially useful for
emotion recognition (recognizing emotion based
on events mentioned in a dialogue), response gen-
eration (providing an answer to emotion-related
questions), and answering social-science related
questions (discovering events that affect the emo-
tion of a particular segment of the population).
1
This is in contrast to existing sentiment lexicons (Riloff
et al., 2003; Valitutti, 2004; Esuli and Sebastiani, 2006; Ve-
likovich et al., 2010; Mohammad and Turney, 2013), which
only record the sentiment orientation of particular words
(such as ?meet? or ?friend?), which, while useful, are less dir-
ectly connected to the emotions than the events themselves.
While there is very little previous research on
this subject, one previous work of note by Tok-
uhisa et al. (2008) focused on emotion-provoking
events purely from the viewpoint of emotion re-
cognition. They used large corpus of examples
collected from the Web using manual patterns to
build a k-nearest-neighbors emotion classifier for
dialog systems and found that the classifier sig-
nificantly outperforms baseline methods. This
method provides both an inspiration and a baseline
for our work, but still lacks in that it makes no
attempt to measure the quality of the extracted
events, aggregate similar events, or rank events by
prevalence, all essential factors when attempting
to use extracted events for applications other than
simple emotion recognition.
In this paper, we describe work on creat-
ing prevalence-ranked dictionaries of emotion-
provoking events through both manual labor and
automatic information extraction. To create a
manual dictionary of events, we perform a sur-
vey asking 30 participants to describe events that
caused them to feel a particular emotion, and
manually cleaned and aggregated the results into
a ranked list. Next, we propose several methods
for extracting events automatically from large data
from the Web, which will allow us to increase the
coverage over the smaller manually created dic-
tionary. We start with Tokuhisa et al. (2008)?s pat-
terns as a baseline, and examine methods for im-
proving precision and coverage through the use of
seed expansion and clustering. Finally, we dis-
cuss evaluation measures for the proposed task,
and perform an evaluation of the automatically ex-
tracted emotion-provoking events. The acquired
events will be provided publicly upon acceptance
of the paper.
2 Manual Creation of Events
In order to create a small but clean set of gold-
standard data for each emotion, we first performed
128
Emotions Words
happiness happy, glad
sadness sad, upset
anger angry, irritated
fear afraid, scared
surprise surprised, astonished
disgust disgusted, terrible
Table 2: Seed words for each emotion.
a survey on emotion-provoking events. We did so
by asking a total of 30 subjects (a mixture of male
and female from 20-40 years of age) to write down
five events that provoke each of five emotions:
happiness, sadness, anger, fear, and surprise. As
these events created according to this survey still
have a large amount of lexical variation, we manu-
ally simplify them to their core and merge together
events that have similar meanings.
Finally, for each emotion we extract all the
events that are shared by more than one person. It
should be noted that this will not come anywhere
close to covering the entirety of human emotion,
but as each event is shared by at least two people
in a relatively small sample, any attempt to create
a comprehensive dictionary of emotion-provoking
events should at least be able to cover the pairs in
this collection. We show the most common three
events for each emotion in Table 1.
3 Automatic Extraction of Events
We also performed experiments attempting to
automatically extract and aggregate events from
Web data. As a starting point, we follow Tokuhisa
et al. (2008) in defining a single reliable pattern as
a starting point for event extraction:
I am EMOTION that EVENT
As this pattern is a relatively reliable indicator that
the event is correct, most events extracted by this
pattern will actually be emotion-provoking events.
For instance, this pattern will be matched with the
sentence ?I am happy that my mother is feeling
better?, in which my mother is feeling better cer-
tainly causes happiness.
For the EMOTION placeholder, we take into ac-
count 6 emotions - happiness, sadness, anger, fear,
disgust, and surprise - argued by Ekman (1992) to
be the most basic. We manually create a short list
of words that can be inserted into the above pattern
appropriately, as shown in Table 2.
For the EVENT placeholder, we allow any string
of words, but it is necessary to choose the scope
of the string that is referring to the emotion-
provoking event. To this end, we use a syntactic
parser and set a hard restriction that all events must
be a subtree having root tag S and containing at
least one noun phrase and one verb phrase.
Given these two restrictions, these patterns
provide us with high quality event-emotion pairs,
but the method is still lacking in two respects, lack
of coverage and lack of ability to aggregate sim-
ilar events. As both of these are essential to cre-
ating a high-quality and non-redundant dictionary
of events, we make two simple extensions to the
extraction process as follows.
3.1 Pattern Expansion
Pattern expansion, or bootstrapping algorithms are
widely used in the information extraction field
(Ravichandran and Hovy, 2002). In particular Es-
presso (Pantel and Pennacchiotti, 2006) is known
as a state-of-the-art pattern expansion algorithm
widely used in acquiring relationships between
entities. We omit the details of the algorithm
for space concerns, but note that applying the al-
gorithm to our proposed task is relatively straight-
forward, and allows us to acquire additional pat-
terns that may be matched to improve the cover-
age over the single seed pattern. We do, however,
make two changes to the algorithm. The first is
that, as we are interested in extracting events in-
stead of entities, we impose the previously men-
tioned restriction of one verb phrase and one noun
phrase over all events extracted by the patterns.
The second is that we perform normalization of
events to reduce their variability, namely removing
all function words, replacing proper nouns with
special symbol, and lemmatizing words.
3.2 Grouping events
The second improvement we perform is group-
ing the extracted events together. Grouping has a
number of potential practical advantages, as noted
frequently in previous work (Becker et al., 2011).
The first is that by grouping similar events to-
gether, we can relieve sparsity issues to some
extent by sharing statistics among the events in
a single group. The second is that aggregating
events together allows humans to browse the lists
more efficiently by reducing the number of re-
dundant entries. In preliminary experiments, we
attempted several clustering methods and even-
129
Emotions Events
happiness meeting friends going on a date getting something I want
sadness someone dies/gets sick someone insults me people leave me alone
anger someone insults me someone breaks a promise someone is too lazy
fear thinking about the future taking a test walking/driving at night
surprise seeing a friend unexpectedly someone comes to visit receiving a gift
Table 1: The top three events for each emotion.
tually settled on hierarchical agglomerative clus-
tering and the single-linkage criterion using co-
sine similarity as a distance measure (Gower and
Ross, 1969). Choosing the stopping criterion for
agglomerative clustering is somewhat subjective,
in many cases application dependent, but for the
evaluation in this work, we heuristically choose
the number of groups so the average number of
events in each group is four, and leave a further
investigation of the tuning to future work.
4 Evaluation Measures
Work on information extraction typically uses ac-
curacy and recall of the extracted information as
an evaluation measure. However, in this work, we
found that it is difficult to assign a clear-cut dis-
tinction between whether an event provokes a par-
ticular emotion or not. In addition, recall is diffi-
cult to measure, as there are essentially infinitely
many events. Thus, in this section, we propose two
new evaluation measures to measure the precision
and recall of the events that we recovered in this
task.
To evaluate the precision of the events extrac-
ted by our method, we focus on the fact that an
event might provoke multiple emotions, but usu-
ally these emotions can be ranked in prominence
or appropriateness. This is, in a way, similar to the
case of information retrieval, where there may be
many search results, but some are more appropri-
ate than others. Based on this observation, we fol-
low the information retrieval literature (Voorhees,
1999) in adapting mean reciprocal rank (MRR) as
an evaluation measure of the accuracy of our ex-
traction. In our case, one event can have multiple
emotions, so for each event that the system out-
puts, we ask an annotator to assign emotions in
descending order of prominence or appropriate-
ness, and assess MRR with respect to these ranked
emotions.
2
We also measure recall with respect to the
2
In the current work we did not allow annotators to assign
?ties? between the emotions, but this could be accommodated
in the MRR framework.
manually created dictionary described in Section
2, which gives us an idea of what percent of com-
mon emotions we were able to recover. It should
be noted that in order to measure recall, it is ne-
cessary to take a matching between the events out-
put by the system and the events in the previously
described list. While it would be ideal to do this
automatically, this is difficult due to small lexical
variations between the system output and the list.
Thus, for the current work we perform manual
matching between the system hypotheses and the
references, and hope to examine other ways of
matching in future work.
5 Experiments
In this section, we describe an experimental eval-
uation of the accuracy of automatic extraction of
emotion-provoking events.
5.1 Experimental Setup
We use Twitter
3
as a source of data, as it is it
provides a massive amount of information, and
also because users tend to write about what they
are doing as well as their thoughts, feelings and
emotions. We use a data set that contains more
than 30M English tweets posted during the course
of six weeks in June and July of 2012. To remove
noise, we perform a variety of preprocessing, re-
moving emoticons and tags, normalizing using
the scripts provided by Han and Baldwin (2011),
and Han et al. (2012). CoreNLP
4
was used to
get the information about part-of-speech, syntactic
parses, and lemmas.
We prepared four systems for comparison. As a
baseline, we use a method that only uses the ori-
ginal seed pattern mentioned in Section 3 to ac-
quire emotion-provoking events. We also evalu-
ate expansions to this method with clustering, with
pattern expansion, and with both.
We set a 10 iteration limit on the Espresso al-
gorithm and after each iteration, we add the 20
3
http://www.twitter.com
4
http://nlp.stanford.edu/software/
corenlp.shtml
130
Methods MRR Recall
Seed 46.3 (?5.0) 4.6 (?0.5)
Seed + clust 57.2 (?7.9) 8.5 (?0.9)
Espresso 49.4 (?2.8) 8.0 (?0.5)
Espresso + clust 71.7 (?2.9) 15.4 (?0.8)
Table 3: MRR and recall of extracted data (with
standard deviation for 3 annotators).
most reliable patterns to the pattern set, and in-
crease the seed set by one third of its size. These
values were set according to a manual inspection
of the results for several settings, before any eval-
uation was performed.
We examine the utility of each method accord-
ing to the evaluation measures proposed in Sec-
tion 4 over five emotions, happiness, sadness, an-
ger, fear, and surprise.
5
To measure MRR and
recall, we used the 20 most frequent events or
groups extracted by each method for these five
emotions, and thus all measures can be interpreted
as MRR@20 and recall@20. As manual annota-
tion is required to calculate both measures, we ac-
quired results for 3 annotators and report the aver-
age and standard deviation.
5.2 Experimental Results
The results are found in Table 3. From these res-
ults we can see that clustering the events causes a
significant gain on both MRR and recall, regard-
less of whether we use Espresso or not. Looking
at the results for Espresso, we see that it allows for
small boost in recall when used on its own, due
to the fact that the additional patterns help recover
more instances of each event, making the estimate
of frequency counts more robust. However, Es-
presso is more effective when used in combination
with clustering, showing that both methods are
capturing different varieties of information, both
of which are useful for the task.
In the end, the combination of pattern expansion
and clustering achieves an MRR of 71.7% and re-
call of 15.4%. While the MRR could be deemed
satisfactory, the recall is still relatively low. One
reason for this is that due to the labor-intensive
manual evaluation, it is not realistic to check many
more than the top 20 extracted events for each
emotion, making automatic evaluation metrics the
top on the agenda for future work.
5
We exclude disgust, as the seed only matched 26 times
over entire corpus, not enough for a reasonable evaluation.
Emotions MRR Recall
happiness 93.9 23.1
sadness 76.9 10.0
anger 76.5 14.0
fear 48.3 24.3
surprise 59.6 0.0
Table 4: Average MRR and recall by emotion for
the Espresso + clustering method.
However, even without considering this, we
found that the events extracted from Twitter
were somewhat biased towards common, everyday
events, or events regarding love and dating. On the
other hand, our annotators produced a wide vari-
ety of events including both everyday events, and
events that do not happen every day, but leave a
particularly strong impression when encountered.
This can be seen particularly in the accuracy and
recall results by emotion for the best system shown
in Table 4. We can see that for some emotions we
achieved recall approaching 25%, but for surprise
we didn?t manage to extract any of the emotions
created by the annotators at all, instead extracting
more mundane events such as ?surprised I?m not
fat yet? or ?surprised my mom hasn?t called me
yet.? Covering the rare, but important events is an
interesting challenge for expansions to this work.
6 Conclusion and Future Work
In this paper we described our work in creat-
ing a dictionary of emotion-provoking events, and
demonstrated results for four varieties of auto-
matic information extraction to expand this dic-
tionary. As this is the first attempt at acquiring dic-
tionaries of emotion-provoking events, there are
still many future directions that deserve further in-
vestigation. As mentioned in the experimental dis-
cussion, automatic matching for the evaluation of
event extraction, and ways to improve recall over
rarer but more impressive events are necessary.
There are also many improvements that could be
made to the extraction algorithm itself, including
more sophisticated clustering and pattern expan-
sion algorithms. Finally, it would be quite interest-
ing to use the proposed method as a tool for psy-
chological inquiry, including into the differences
between events that are extracted from Twitter and
other media, or the differences between different
demographics.
131
References
Hila Becker, Mor Naaman, and Luis Gravano. 2011.
Beyond trending topics: Real-world event identific-
ation on Twitter. In Proceedings of the Fifth Inter-
national AAAI Conference on Weblogs and Social
Media (ICWSM11).
Paul Ekman. 1992. An argument for basic emotions.
Cognition & Emotion, 6(3-4):169?200.
Andrea Esuli and Fabrizio Sebastiani. 2006. Senti-
wordnet: A publicly available lexical resource for
opinion mining. In In Proceedings of the 5th Con-
ference on Language Resources and Evaluation,
pages 417?422.
John C Gower and GJS Ross. 1969. Minimum span-
ning trees and single linkage cluster analysis. Ap-
plied statistics, pages 54?64.
Bo Han and Timothy Baldwin. 2011. Lexical normal-
isation of short text messages: makn sens a #twitter.
In Proceedings of the 49th Annual Meeting of the
Association for Computational Linguistics: Human
Language Technologies - Volume 1, HLT ?11, pages
368?378.
Bo Han, Paul Cook, and Timothy Baldwin. 2012.
Automatically constructing a normalisation diction-
ary for microblogs. In Proceedings of the 2012 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning, pages 421?432, Jeju Island, Korea,
July. Association for Computational Linguistics.
Saif M Mohammad and Peter D Turney. 2013. Crowd-
sourcing a word?emotion association lexicon. Com-
putational Intelligence, 29(3):436?465.
Patrick Pantel and Marco Pennacchiotti. 2006. Es-
presso: leveraging generic patterns for automatic-
ally harvesting semantic relations. In Proceedings
of the 21st International Conference on Computa-
tional Linguistics and the 44th annual meeting of the
Association for Computational Linguistics, ACL-44,
pages 113?120.
Deepak Ravichandran and Eduard Hovy. 2002. Learn-
ing surface text patterns for a question answering
system. In Proceedings of the 40th Annual Meeting
on Association for Computational Linguistics, ACL
?02, pages 41?47.
Ellen Riloff, Janyce Wiebe, and Theresa Wilson. 2003.
Learning subjective nouns using extraction pattern
bootstrapping. In Proceedings of the seventh confer-
ence on Natural language learning at HLT-NAACL
2003-Volume 4, pages 25?32. Association for Com-
putational Linguistics.
Ryoko Tokuhisa, Kentaro Inui, and Yuji Matsumoto.
2008. Emotion classification using massive ex-
amples extracted from the web. In Proceedings
of the 22nd International Conference on Computa-
tional Linguistics - Volume 1, COLING ?08, pages
881?888.
Ro Valitutti. 2004. Wordnet-affect: an affective ex-
tension of wordnet. In In Proceedings of the 4th In-
ternational Conference on Language Resources and
Evaluation, pages 1083?1086.
Leonid Velikovich, Sasha Blair-Goldensohn, Kerry
Hannan, and Ryan McDonald. 2010. The viability
of web-derived polarity lexicons. In Human Lan-
guage Technologies: The 2010 Annual Conference
of the North American Chapter of the Association
for Computational Linguistics, pages 777?785.
Ellen M Voorhees. 1999. The trec-8 question an-
swering track report. In Proceedings of TREC,
volume 99, pages 77?82.
132
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 551?556,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Optimizing Segmentation Strategies for Simultaneous Speech Translation
Yusuke Oda Graham Neubig Sakriani Sakti Tomoki Toda Satoshi Nakamura
Graduate School of Information Science
Nara Institute of Science and Technology
Takayama, Ikoma, Nara 630-0192, Japan
{oda.yusuke.on9, neubig, ssakti, tomoki, s-nakamura}@is.naist.jp
Abstract
In this paper, we propose new algorithms
for learning segmentation strategies for si-
multaneous speech translation. In contrast
to previously proposed heuristic methods,
our method finds a segmentation that di-
rectly maximizes the performance of the
machine translation system. We describe
two methods based on greedy search and
dynamic programming that search for the
optimal segmentation strategy. An experi-
mental evaluation finds that our algorithm
is able to segment the input two to three
times more frequently than conventional
methods in terms of number of words,
while maintaining the same score of auto-
matic evaluation.
1
1 Introduction
The performance of speech translation systems
has greatly improved in the past several years,
and these systems are starting to find wide use in
a number of applications. Simultaneous speech
translation, which translates speech from the
source language into the target language in real
time, is one example of such an application. When
translating dialogue, the length of each utterance
will usually be short, so the system can simply
start the translation process when it detects the end
of an utterance. However, in the case of lectures,
for example, there is often no obvious boundary
between utterances. Thus, translation systems re-
quire a method of deciding the timing at which
to start the translation process. Using estimated
ends of sentences as the timing with which to start
translation, in the same way as a normal text trans-
lation, is a straightforward solution to this problem
(Matusov et al, 2006). However, this approach
1
The implementation is available at
http://odaemon.com/docs/codes/greedyseg.html.
impairs the simultaneity of translation because the
system needs to wait too long until the appearance
of a estimated sentence boundary. For this reason,
segmentation strategies, which separate the input
at appropriate positions other than end of the sen-
tence, have been studied.
A number of segmentation strategies for simul-
taneous speech translation have been proposed in
recent years. F?ugen et al (2007) and Bangalore et
al. (2012) propose using prosodic pauses in speech
recognition to denote segmentation boundaries,
but this method strongly depends on characteris-
tics of the speech, such as the speed of speaking.
There is also research on methods that depend on
linguistic or non-linguistic heuristics over recog-
nized text (Rangarajan Sridhar et al, 2013), and it
was found that a method that predicts the location
of commas or periods achieves the highest perfor-
mance. Methods have also been proposed using
the phrase table (Yarmohammadi et al, 2013) or
the right probability (RP) of phrases (Fujita et al,
2013), which indicates whether a phrase reorder-
ing occurs or not.
However, each of the previously mentioned
methods decides the segmentation on the basis
of heuristics, so the impact of each segmenta-
tion strategy on translation performance is not di-
rectly considered. In addition, the mean number
of words in the translation unit, which strongly af-
fects the delay of translation, cannot be directly
controlled by these methods.
2
In this paper, we propose new segmentation al-
gorithms that directly optimize translation perfor-
mance given the mean number of words in the
translation unit. Our approaches find appropri-
ate segmentation boundaries incrementally using
greedy search and dynamic programming. Each
boundary is selected to explicitly maximize trans-
2
The method using RP can decide relative frequency of
segmentation by changing a parameter, but guessing the
length of a translation unit from this parameter is not trivial.
551
lation accuracy as measured by BLEU or another
evaluation measure.
We evaluate our methods on a speech transla-
tion task, and we confirm that our approaches can
achieve translation units two to three times as fine-
grained as other methods, while maintaining the
same accuracy.
2 Optimization Framework
Our methods use the outputs of an existing ma-
chine translation system to learn a segmentation
strategy. We define F = {f
j
: 1 ? j ? N},
E = {e
j
: 1 ? j ? N} as a parallel corpus
of source and target language sentences used to
train the segmentation strategy. N represents the
number of sentences in the corpus. In this work,
we consider sub-sentential segmentation, where
the input is already separated into sentences, and
we want to further segment these sentences into
shorter units. In an actual speech translation sys-
tem, these sentence boundaries can be estimated
automatically using a method like the period es-
timation mentioned in Rangarajan Sridhar et al
(2013). We also assume the machine translation
system is defined by a function MT (f) that takes
a string of source words f as an argument and re-
turns the translation result
?
e.
3
We will introduce individual methods in the fol-
lowing sections, but all follow the general frame-
work shown below:
1. Decide the mean number of words ? and the
machine translation evaluation measure EV
as parameters of algorithm. We can use an
automatic evaluation measure such as BLEU
(Papineni et al, 2002) as EV . Then, we cal-
culate the number of sub-sentential segmen-
tation boundaries K that we will need to in-
sert into F to achieve an average segment
length ?:
K := max
(
0,
?
?
f?F |f |
?
?
?N
)
. (1)
2. Define S as a set of positions in F in which
we will insert segmentation boundaries. For
example, if we will segment the first sentence
after the third word and the third sentence af-
ter the fifth word, then S = {?1, 3? , ?3, 5?}.
3
In this work, we do not use the history of the language
model mentioned in Bangalore et al (2012). Considering this
information improves the MT performance and we plan to
include this in our approach in future work.
Figure 1: Concatenated translation MT (f ,S).
Based on this representation, choose K seg-
mentation boundaries in F to make the set
S
?
that maximizes an evaluation function ?
as below:
S
?
:= arg max
S?{S
?
:|S
?
|=K}
?(S;F , E , EV,MT ).
(2)
In this work, we define ? as the sum of the
evaluation measure for each parallel sentence
pair ?f
j
,e
j
?:
?(S) :=
N
?
j=1
EV (MT (f
j
,S), e
j
), (3)
where MT (f ,S) represents the concatena-
tion of all partial translations {MT (f
(n)
)}
given the segments S as shown in Figure 1.
Equation (3) indicates that we assume all
parallel sentences to be independent of each
other, and the evaluation measure is calcu-
lated for each sentence separately. This lo-
cality assumption eases efficient implementa-
tion of our algorithm, and can be realized us-
ing a sentence-level evaluation measure such
as BLEU+1 (Lin and Och, 2004).
3. Make a segmentation model M
S
?
by treating
the obtained segmentation boundaries S
?
as
positive labels, all other positions as negative
labels, and training a classifier to distinguish
between them. This classifier is used to de-
tect segmentation boundaries at test time.
Steps 1. and 3. of the above procedure are triv-
ial. In contrast, choosing a good segmentation ac-
cording to Equation (2) is difficult and the focus
of the rest of this paper. In order to exactly solve
Equation (2), we must perform brute-force search
over all possible segmentations unless we make
some assumptions about the relation between the
? yielded by different segmentations. However,
the number of possible segmentations is exponen-
tially large, so brute-force search is obviously in-
tractable. In the following sections, we propose 2
552
I ate lunch but she left
Segments already selected at the k-th iteration
? = 0.5 ? = 0.8
(k+1)-th segment
? = 0.7
Figure 2: Example of greedy search.
Algorithm 1 Greedy segmentation search
S
?
? ?
for k = 1 to K do
S
?
? S
?
?
{
arg max
s

?S
?
?(S
?
? {s})
}
end for
return S
?
methods that approximately search for a solution
to Equation (2).
2.1 Greedy Search
Our first approximation is a greedy algorithm that
selects segmentation boundaries one-by-one. In
this method, k already-selected boundaries are left
unchanged when deciding the (k+1)-th boundary.
We find the unselected boundary that maximizes ?
and add it to S:
S
k+1
= S
k
?
{
arg max
s

?S
k
?(S
k
? {s})
}
. (4)
Figure 2 shows an example of this process for a
single sentence, and Algorithm 1 shows the algo-
rithm for calculating K boundaries.
2.2 Greedy Search with Feature Grouping
and Dynamic Programming
The method described in the previous section
finds segments that achieve high translation per-
formance for the training data. However, because
the translation system MT and evaluation mea-
sureEV are both complex, the evaluation function
? includes a certain amount of noise. As a result,
the greedy algorithm that uses only ? may find a
segmentation that achieves high translation perfor-
mance in the training data by chance. However,
these segmentations will not generalize, reducing
the performance for other data.
We can assume that this problem can be solved
by selecting more consistent segmentations of the
training data. To achieve this, we introduce a con-
straint that all positions that have similar charac-
teristics must be selected at the same time. Specif-
ically, we first group all positions in the source
I ate lunch but she left
PRP VBD NN CC PRP VBD
I ate an apple and an orange
PRP VBD DT NN CC DT NN
WORD:
 POS:
WORD:
 POS:
Group
PRP+VBD
Group
NN+CC
Group
DT+NN
Figure 3: Grouping segments by POS bigrams.
sentences using features of the position, and intro-
duce a constraint that all positions with identical
features must be selected at the same time. Figure
3 shows an example of how this grouping works
when we use the POS bigram surrounding each
potential boundary as our feature set.
By introducing this constraint, we can expect
that features which have good performance over-
all will be selected, while features that have rela-
tively bad performance will not be selected even if
good performance is obtained when segmenting at
a specific location. In addition, because all posi-
tions can be classified as either segmented or not
by evaluating whether the corresponding feature is
in the learned feature set or not, it is not necessary
to train an additional classifier for the segmenta-
tion model when using this algorithm. In other
words, this constraint conducts a kind of feature
selection for greedy search.
In contrast to Algorithm 1, which only selected
one segmentation boundary at once, in our new
setting there are multiple positions selected at one
time. Thus, we need to update our search algo-
rithm to handle this setting. To do so, we use
dynamic programming (DP) together with greedy
search. Algorithm 2 shows ourGreedy+DP search
algorithm. Here, c(?;F) represents the number
of appearances of ? in the set of source sentences
F , and S(F ,?) represents the set of segments de-
fined by both F and the set of features ?.
The outer loop of the algorithm, like Greedy,
iterates over all S of size 1 to K. The inner loop
examines all features that appear exactly j times
in F , and measures the effect of adding them to
the best segmentation with (k ? j) boundaries.
2.3 Regularization by Feature Count
Even after we apply grouping by features, it
is likely that noise will still remain in the less
frequently-seen features. To avoid this problem,
we introduce regularization into the Greedy+DP
algorithm, with the evaluation function ? rewrit-
553
Algorithm 2 Greedy+DP segmentation search
?
0
? ?
for k = 1 to K do
for j = 0 to k ? 1 do
?
?
? {? : c(?;F) = k ? j ? ?
Workshop on Computational Linguistics and Clinical Psychology: From Linguistic Signal to Clinical Reality, pages 88?96,
Baltimore, Maryland USA, June 27, 2014.
c?2014 Association for Computational Linguistics
Linguistic and Acoustic Features for Automatic Identification of Autism
Spectrum Disorders in Children?s Narrative
Hiroki Tanaka, Sakriani Sakti, Graham Neubig, Tomoki Toda, Satoshi Nakamura
Graduate School of Information Science, Nara Institute of Science and Technology
{hiroki-tan, ssakti, neubig, tomoki, s-nakamura}@is.naist.jp
Abstract
Autism spectrum disorders are develop-
mental disorders characterised as deficits
in social and communication skills, and
they affect both verbal and non-verbal
communication. Previous works measured
differences in children with and without
autism spectrum disorders in terms of
linguistic and acoustic features, although
they do not mention automatic identifi-
cation using integration of these features.
In this paper, we perform an exploratory
study of several language and speech fea-
tures of both single utterances and full nar-
ratives. We find that there are charac-
teristic differences between children with
autism spectrum disorders and typical de-
velopment with respect to word categories,
prosody, and voice quality, and that these
differences can be used in automatic clas-
sifiers. We also examine the differences
between American and Japanese children
and find significant differences with re-
gards to pauses before new turns and lin-
guistic cues.
1 Introduction
Autism spectrum disorders (ASD) are develop-
mental disorders, first described by Kanner and
Asperger in 1943 and 1944 respectively (Kanner,
1943; Asperger, 1944). The American Psychi-
atric Association defines the two characteristics of
ASD as: 1) persistent deficits in social communi-
cation and social interaction across multiple con-
texts, and 2) restricted, repetitive patterns of be-
havior, interests, or activities (American Psychi-
atric Association, 2013). In particular, the former
deficits in social communication are viewed as the
most central characteristic of ASD. Thus, quanti-
fying the degree of social communication skills is
a necessary component of understanding the na-
ture of ASD, creating systems for automatic ASD
screening, and early intervention methods such as
social skills training and applied behaviour analy-
sis (Wallace et al., 1980; Lovaas et al., 1973).
There are a number of studies finding differ-
ences between people with ASD and people with
typical development (TD). In terms of deficits in
social communication, there have been reports de-
scribing atypical usage of gestures (Ashley and
Inge-Marie, 2010), frequency of eye-contact and
laughter (Geraldine et al., 1990), prosody (Mc-
Cann and Peppe, 2003; Rhea et al., 2005), voice
quality (Asgari et al., 2013), delay responses
(Heeman et al., 2010), and unexpected words
(Rouhizadeh et al., 2013). In this paper, we par-
ticularly focus on the cues of ASD that appear in
children?s language and speech
In the case of language, Newton et al. (2009)
analyze blogs of people with ASD and TD, and
found that people with ASD have larger variation
of usage of words describing social processes, al-
though there are no significant differences in other
word categories. In the case of speech, people with
ASD tend to have prosody that differs from that
of their peers (Kanner, 1943), although McCann
and Peppe (2003) note that prosody in ASD is an
under-researched area and that where research has
been undertaken, findings often conflict. Since
then, there have been various studies analyzing
and modeling prosody in people with ASD (Daniel
et al., 2012; Kiss et al., 2013; Santen et al., 2013;
Van et al., 2010). For example, Kiss et al. (2012)
find several significant differences in the pitch
characteristics of ASD, and report that automatic
classification utilizing these features achieves ac-
curacy well above chance level. To our knowl-
edge, there is no previous work integrating both
language and speech features to identify differ-
ences between people with ASD and TD. How-
ever, it has been noted that differences in person-
88
ality traits including introversion/extroversion can
be identified using these features (Mairesse et al.,
2007).
In this paper, we perform a comprehensive anal-
ysis of language and speech features mentioned in
previous works, as well as novel features specific
to this work. In addition, while previous works an-
alyzed differences between people with ASD and
TD, we additionally investigate whether it is possi-
ble to automatically distinguish between children
with ASD or TD using both language and speech
features and a number of classification methods.
We focus on narratives, where the children serving
as our subjects tell a memorable story to their par-
ent (Davis et al., 2004). Here, the use of narrative
allows us to consider not only single-sentence fea-
tures, but also features considering interaction as-
pects between the child and parent such as pauses
before new turns and overall narrative-specific fea-
tures such as words per minute and usage of un-
expected words. Given this setting, we perform
a pilot study examining differences between chil-
dren with ASD and TD, the possibilities of auto-
matic classification between ASD and TD, and the
differences between American and Japanese chil-
dren.
2 Data Description
As a target for our analysis, we first collected a
data set of interactions between Japanese children
and their parents. In collecting the data, we fol-
lowed the procedure used in the creation of the
USC Rachel corpus (Mower et al., 2011). The data
consists of four sessions: doh (free play), jenga (a
game), narrative, and natural conversation. The
first child-parent interaction is free play with the
parent. The child and parent are given play doh,
Mr. Potato Head, and blocks. The second child-
parent interaction is a jenga game. Jenga is a game
in which the participants must remove blocks, one
at a time, from a tower. The game ends when the
tower falls. The third child-parent interaction is a
narrative task. The child and parent are asked to
explain stories in which they experienced a mem-
orable emotion. The final child-parent interaction
is a natural conversation without a task. These
child-parent interactions are recorded and will en-
able comparison of the child?s interaction style and
communication with their parent. Each session
continues for 10 minutes. During interaction, a pin
microphone and video camera record the speech
and video of the child and the parent.
In this paper, we use narrative data of four chil-
dren with ASD (male: 3, female: 1) and two
children with TD (male: 1, female: 1) as an ex-
ploratory study. The intelligence quotient (IQ) for
all subjects is above 70, which is often used as
a threshold for diagnosis of intellectual disabil-
ity. Each subject?s age and diagnosis as ASD/TD
is provided in Table 1. In the narrative session,
each child and parent speaks ?a memorable story?
for 5 minutes in turn, and the listener responds to
the speaker?s story by asking questions. After 5
minutes, the experimenter provides directions to
change the turn.
Table 1: Subjects? age and diagnosis
Subject A1 A2 A3 A4 T1 T2
Age 10 10 10 13 10 12
Diagnosis ASD ASD ASD ASD TD TD
In this paper, we analyze the child-speaking turn
of the narrative session in which the parent re-
sponds to the child?s utterances. All utterances are
transcribed based on USC Rachel corpus manual
(Mower et al., 2011) to facilitate comparison with
this existing corpus. In the transcription manual, if
the speaker pauses for more than one second, the
speech is transcribed as separate utterances. In this
paper, we examine two segment levels, the first
treating each speech segment independently, and
the second handling a whole narrative as the tar-
get. When handling each segment independently,
we use a total of 116 utterances for both children
with ASD and TD.
3 Single Utterance Level
In this section, we describe language and speech
features and analysis of these characteristics to-
wards automatic classification of utterances based
on whether they were spoken by children with
ASD or TD. We hypothesize that based on the fea-
tures extracted from the speech signal we are ca-
pable to classify children with ASD and TD on a
speech segment level, as well as on narrative level
after temporally combining all the segment-based
decisions.
3.1 Feature Extraction
We extract language and speech features based
on those proposed by (Mairesse et al., 2007) and
89
(Hanson, 1995). Extracted features are summa-
rized in Table 2. We also add one feature not cov-
ered in previous work counting the number of oc-
currences of laughter.
Table 2: Description of language and speech fea-
tures.
Language Features
Words per sentence (WPS)
General descriptor Words with more than 6 letters
Occurrences of laughter
Sentence structure
Percentage of pronouns, conjunctions,
negations, quantifiers, numbers
Psychological proc.
Percentage of words describing social,
affect, cognitive, perceptual,
and biological
Personal concerns
Percentage of words describing work,
achievement, leisure, and home
Paralinguistic
Percentage of assent,
disfluencies, and fillers
Speech Features
Pitch Statistics of sd and cov
Intensity Statistics of sd and cov
Speech rate Words per voiced second
Amplitude of a3
Voice quality Difference of the h1 and the h2
Difference of the h1 and the a3
3.1.1 Language Features
We use the linguistic inquiry and word count
(LIWC) (Pennebaker et al., 2007), which is a tool
to categorize words, to extract language features.
Because a Japanese version of LIWC is not avail-
able and there is no existing similar resource for
Japanese, we implement the following procedures
to automatically establish correspondences be-
tween LIWC categories and transcribed Japanese
utterances. First, we use Mecab
1
for part-of-
speech tagging in Japanese utterances, translate
each word into English using the WWWJDIC
2
dictionary, and finally determine the LIWC cate-
gory corresponding to the English word. Among
the language features described in Table 2, we
calculate sentence structures, psychological pro-
cesses, and personal concerns using LIWC, and
other features using Mecab. Here, we do not
consider language-dependent features and subcat-
egories of LIWC.
1
https://code.google.com/p/mecab/
2
http://www.edrdg.org/cgi-bin/wwwjdic/wwwjdic?1C
3.1.2 Speech Features
For speech feature extraction, we use the Snack
sound toolkit
3
. Here, we consider fundamental
frequency, power, and voice quality, which are ef-
fective features according to previous works (Mc-
Cann and Peppe, 2003; Hanson, 1995). We do
not extract mean values of fundamental frequency
and power because those features are strongly re-
lated to individuality. Thus, we extract statistics
of standard deviation (fsd, psd) and coefficient of
variation (fcov, pcov) for fundamental frequency
and power. We calculate speech rate, which is a
feature dividing the number of words by the num-
ber of voiced seconds. Voice quality is also com-
puted using: the amplitude of the third formant
(a3), the difference between the first harmonic and
the second harmonic (h1h2), and the difference
between the first harmonic and the third formant
(h1a3) (Hanson, 1995).
3.1.3 Projection Normalization
For normalization, we simply project all feature
values to a range of [0, 1], where 0 corresponds
to the smallest observed value and 1 to the largest
observed value across all utterances. For utterance
i, we define the value of the jth feature as v
ij
and
define p
ij
=
v
ij
?min
j
max
j
?min
j
, where p
ij
is the feature
value after normalisation.
3.2 Characteristics of Language and Speech
Features
In this section, we report the result of a t-test, prin-
cipal component analysis, factor analysis, and de-
cision tree using the normalised features. We use
R
4
for statistical analysis.
Table 3 shows whether utterances of children
with ASD or TD have a greater mean on the cor-
responding feature. The results indicate that the
children with ASD more frequently use words
with more than 6 letters (e.g. complicated words),
assent (e.g. ?uh-huh,? or ?un? in Japanese), and
fillers (e.g. ?umm,? or ?eh? in Japanese) signif-
icantly more than the children with TD. In con-
trast, the children with TD more frequently use the
words words categorized as social (e.g. friend), af-
fect (e.g. enjoy), and cognitive (e.g. understand)
significantly more than the children with ASD. In
addition, there are differences in terms of funda-
mental frequency variations and voice quality (e.g.
3
http://www.speech.kth.se/snack/
4
http://www.r-project.org
90
Table 3: Difference of mean values between ASD and TD based on language and speech features from
children?s utterances. Each table cell notes which of the two classes has the greater mean on the corre-
sponding feature (*: p < 0.01, **: p < 0.005).
WPS 6 let. laughter adverb pronoun conjunctions negations quantifiers numbers social
- ASD* - - - - - - - TD**
affect cognitive perceptual biological relativity work achievement leisure home assent
TD** TD* - - - - - - - ASD**
nonfluent fillers fsd fcov psd pcov speech rate a3 h1h2 h1a3
- ASD* TD** TD* - - - - - ASD**
h1a3). In particular, we observe that the children
with ASD tend to use monotonous intonation as
reported in (Kanner, 1943). We do not confirm a
significant differences in other features.
Next, we use principal component analysis and
factor analysis to find features that have a large
contribution based on large variance values. As
a result of principal component analysis, features
about fundamental frequency, power, and h1a3
have large variance in the first component, and the
feature counting perceptual words also has large
value in the second component. To analyze a dif-
ferent aspect of principal component analysis with
rotated axes, we use factor analysis with the vari-
max rotation method. Figure 1 shows the result of
factor analysis indicating that features regarding
fundamental frequency and power have large vari-
ance. In addition, other features such as speech
rate, a3, and h1a3 also have large variance. Here,
we can see that for features such as statistics of
fundamental frequency (fsd and fcov) and power
(psd and pcov), the correlation coefficient between
these features are over 80% (p < 0.01). For cor-
related features, we use only standard deviation in
the following sections.
We also analyze important features to distin-
guish between children with ASD and TD by us-
ing a decision tree. Figure 2 shows the result of a
decision tree with 10 leaves indicating that speech
features fill almost all of the leaves (e.g. fsd is a
most useful feature to distinguish between ASD
and TD). In terms of the language features, we
confirm that WPS and perceptual words are im-
portant for classification.
3.3 Classification
In this section, we examine the possibility of au-
tomatic identification of whether an utterance be-
longs to a speaker with ASD or TD. Based on
the previous analysis, we prepare the following
Figure 1: Factor analysis with varimax rotation
method. First and second factors are indicated.
feature sets: 1) language features (Language), 2)
speech features (Speech), 3) all features (All), 4)
important features according to the t-test, princi-
pal component analysis, factor analysis, and de-
cision tree (Selected), 5) important features ac-
cording to the t-test that are not highly correlated
(T-Uncor). The feature set of T-Uncor is as fol-
lows: 6 let., social, affect, cognitive, fillers, as-
sent, fed, and h1a3. We also show the chance
rate, which is a baseline of 50% because the num-
ber of utterances in each group is the same, and
measure accuracy with 10-fold cross-validation
and leave-one-speaker-out cross-validation using
naive Bayes (NB) and support vector machines
with a linear kernel (SVM). In the case of leave-
one-speaker-out cross-validation, we use T-Uncor
because the number of utterances without one
speaker is too small to train using high dimen-
sional feature sets.
Table 4 shows the result indicating that accu-
91
racies with almost all feature sets and classifiers
are over 65%. The SVM with Selected achieves
the best performance for the task of 10-fold cross-
validation, and The SVM with T-Uncor achieves
66.7% for the task of leave-one-speaker-out. The
accuracy for the task of leave-one-speaker-out on
each speaker A1 to T2 is as follows: 78%, 60%,
53%, 51%, 82%, and 78%.
Table 4: Accuracy using Naive Bayes and SVM
classifiers. The p-value of the t-test is measured
compared to baseline (chance rate) (?: p < 0.1, *:
p < 0.01)
Feature set Accuracy [%]
Baseline NB SVM
Language 62.2? 70.3*
Speech 57.6 67.6*
All 50.0 65.0? 68.8*
Selected 67.4* 71.9*
T-Uncor 67.8? 68.1?
Per-Speaker 50.0 65.5? 66.7?
4 Narrative Level
In this section, we focus on the features of en-
tire narratives, which allows us to examine other
features of child-parent interaction for a better un-
derstanding of ASD and classification in children
with ASD and TD. Each following subsection de-
scribes the procedure of feature extraction and
analysis of characteristics at the narrative level.
We consider pauses before new turns and unex-
pected words, which are mentioned in previous
works, as well as words per minute.
4.1 Pauses Before New Turns
Heeman et al., (2010) reported that children with
ASD tend to delay responses to their parent more
than children with TD in natural conversation. In
this paper, we examine whether a similar result is
found in interactive narrative. We denote values
of pauses before new turns as time between the
end of the parent?s utterance and the start of the
child?s utterance. We do not consider overlap of
utterances. We test goodness of fit of pauses to a
gamma and an exponential distribution based on
(Theodora et al., 2013), because the later is a spe-
cial case of gamma with a unity shape parameter,
using the Kolmogorov-Smirnov test.
Figure 3 shows a fitting of pauses to gamma
or exponential distributions, and we select a bet-
2 4 6 8 10
0
.
0
0
.
1
0
.
2
0
.
3
0
.
4
0
.
5
Pauses before new turns (sec)
E
x
p
o
n
e
n
t
i
a
l
/
G
a
m
m
a
 
p
r
o
b
a
b
i
l
i
t
y
 
v
a
l
u
e
s
0
.
0
0
.
1
0
.
2
0
.
3
0
.
4
0
.
5
0
.
0
0
.
1
0
.
2
0
.
3
0
.
4
0
.
5
0
.
0
0
.
1
0
.
2
0
.
3
0
.
4
0
.
5
0
.
0
0
.
1
0
.
2
0
.
3
0
.
4
0
.
5
0
.
0
0
.
1
0
.
2
0
.
3
0
.
4
0
.
5
TD
ASD
Figure 3: Gamma/Exponential pause distributions
with parameters computed using Maximum Like-
lihood Estimation (MLE) for children with ASD
and TD.
ter fitted distribution. All subjects significantly fit
(p > 0.6). As shown in Figure 3, we confirm that
children with ASD tend to delay responses to their
parent compared with children with TD. To reflect
this information in our following experiments in
automatic identification of ASD in narrative, we
extract the expectation value of the exponential
distribution
Heeman et al., (2010) also reported the rela-
tionship of the parent?s previous utterance?s type
(question or non-question) and the child?s pauses.
We examine the relationship between the parent?s
previous question?s type and pauses before new
turns. For each of the children?s utterances, we
label the parent?s utterance that directly precedes
as either ?open question,? ?closed question,? or
?non-question?, and we calculate pause latency.
Closed-questions are those which can be answered
by a simple ?yes? or ?no,? while open-questions
are those which require more thought and more
than a simple one-word answer. As shown in Table
5, children with ASD tend to delay responses to
their parent to a greater extent than children with
TD. We found no difference between open and
closed questions, although a difference between
questions and non-questions is observed. These
results are consistent with those of previous work
(Heeman et al., 2010) in terms of differences be-
tween questions and non-questions.
92
|fsd < 0.366375
fcov < 0.308899
WPS < 0.0543478
fcov < 0.204553
psd < 0.306304
pcov < 0.46429
fsd < 0.513756
perceptual < 0.07
pcov < 0.230634
a
t
t
a a
t
a
t
a
a
Figure 2: Decision tree with 10 leaves (a: ASD, t: TD).
Table 5: Relationship of pauses before new turns
and parents? question types. The mean value and
standard deviation are shown.
Question type TD ASD
Closed-question 0.47 (0.46) 1.61 (1.87)
Open-question 0.43 (0.34) 1.76 (1.51)
Non-question 0.95 (1.18) 2.60 (3.64)
4.2 Words Per Minute
We analyze words per minute (WPM) in children
with ASD and TD to clarify the relationship be-
tween ASD and frequency of speech. We use a
total of 5 minutes of data in each narrative, and
thus the total number of words are divided by 5 to
calculate WPM. Table 6 shows the result. The data
in this table indicates that some children with ASD
have a significantly lower speaking rate than oth-
ers with TD, but it is not necessarily the case that
ASD will result in a low speaking rate such as the
case of Asperger?s syndrome (Asperger, 1944).
4.3 Unexpected Words
Characteristics of ASD include deficits in social
communication, and these deficits affect inappro-
Table 6: Mean value of words per minute.
Subj. Averaged WPM
A1 18.25
A2 86.75
A3 23.75
A4 115.5
T1 99.25
T2 103.5
priate usage of words (Rouhizadeh et al., 2013).
We evaluate these unexpected words using two
measures, term frequency-inverse document fre-
quency (TF-IDF) and log odds ratio. We use
the following formulation to calculate TF-IDF for
each child?s narrative i and each word in that nar-
rative j, where c
ij
is the count of word j in narra-
tive i. f
j
is the number of narratives from the full
data of child narratives containing that word j, and
D is the total number of narratives (Rouhizadeh et
al., 2013).
tf ? idf
ij
= (1 + log c
ij
) log
D
f
j
The log odds ratio, another measure used in in-
93
formation retrieval and extraction tasks, is the ratio
between the odds of a particular word, j, appear-
ing in a child?s narrative, i. Letting the probabil-
ity of a word appearing in a narrative be p
1
and
the probability of that word appearing in all other
narratives be p
2
, we can express the odds ratio as
follows:
odds ratio =
odds(p
1
)
odds(p
2
)
=
p
1
/(1? p
1
)
p
2
/(1? p
2
)
A large TF-IDF and log odds score indicates
that the word j is very specific to the narrative
i, which in turn suggests that the word might be
unexpected or inappropriate. In addition, because
the overall amount of data included in the narra-
tives is too small to robustly analyze these statis-
tics for all words, we also check for the presence
of each word in Japanese WordNet
5
and deter-
mine that if it exists in WordNet it is likely a com-
mon (expected) word. Table 7 shows the result
of TF-IDF, log odds ratio, and their summation,
and we confirm that there is no difference between
children with ASD and TD. This result is differ-
ent from that of previous work (Rouhizadeh et al.,
2013). The children in that study were all telling
the same story, and one possible explanation for
this is due to the fact that in this work we do
not use language-constricted data such as narrative
retelling, and thus differences due to individuality
are more prevalent.
Table 7: TF-IDF, log odds ratio, and their summa-
tion.
Subj. TF-IDF Log-odds T+L
A1 0.50 1.01 1.52
A2 0.58 0.49 1.08
A3 0.66 1.23 1.89
A4 0.66 0.31 0.96
T1 0.74 0.49 1.23
T2 0.62 0.44 1.06
4.4 Classification
In this section, we examine the possibility of auto-
matic classification of whether an interactive nar-
rative belongs to children with ASD or TD. Be-
cause of the total number of subjects is small (n=4
for ASD, n=2 for TD), we perform classification
5
http://www.omomimi.com/wnjpn/
with a K-NN classifier with K=1 nearest neigh-
bour. As features, we compute the features men-
tioned in Section 3.1, and use the average over all
utterances as the features for the entire narrative.
Finally, we use pauses before new turns (expecta-
tion value of the exponential distribution), WPM,
TF-IDF, log odds ratio, 6 let., social, affect, cogni-
tive, assent, fillers, fsd, h1a3, and calculate accu-
racy with leave-one-speaker-out cross-validation.
As a result, we achieved an accuracy of 100%
in classification between ASD and TD on the full-
narrative level, which shows that these features
are effective to some extent to distinguish children
with ASD and TD. However, with only a total of 6
children, our sample size is somewhat small, and
thus experiments with a larger data set will be nec-
essary to draw more firm conclusions.
5 Data Comparison
As all our preceding experiments have been per-
formed on data for Japanese child-parent pairs, it
is also of interest to compare these results with
data of children and parents from other cultures.
In particular, we refer to the USC Rachel corpus
(Mower et al., 2011) (the subjects are nine chil-
dren with ASD) for comparison. Using the USC
Rachel corpus, there is a report mentioning the re-
lationship of parent?s and child?s linguistic infor-
mation and pauses before new turns (Theodora et
al., 2013). In this paper, we follow this work us-
ing Japanese data. The USC Rachel corpus in-
cludes a session of child-parent interaction, and
the same transcription standard is used. We ex-
tract pauses before new turns, and short and long
pauses are differentiated based on the 70th per-
centile of latency values for each child individu-
ally. We investigate the relationship between the
parent and child?s language information based on
features used in Section 3.1, and short and long
pauses.
Table 8 and 9 show significantly greater mean
values performed using bootstrap significance
testing on the means of the two pause types. By
observing the values in the table, we can see
that the trends are similar for both American and
Japanese children. However, in terms of WPS,
there is a difference. The American ASD chil-
dren have greater means for WPS in the case of
long pauses, while Japanese children have greater
means for WPS in the case of short pauses. We
analyze these differences in detail.
94
Table 8: In the case of USC Rachel corpus, boot-
strap on difference of means between short (S) and
long (L) pauses based on linguistic features from
child?s and parent?s utterances (?: p < 0.1, *: p <
0.01). Each table cell notes which of the two types
of pauses has greater mean on the corresponding
feature.
Subj.
Child Parent
WPS conj. affect nonflu. adverb cogn. percept.
S1 L* L* S* - L* L* L*
S2 L* L* S? L* L* L* L*
S3 L* L? - S? L* L* L*
S4 - - - L* L* L* L*
S5 L? - - - L* L* L*
S6 L* - S* - L* L* -
S7 L? - S? - L? - -
S8 L* - - - L* L* L*
S9 - - - S? L* L* L*
Table 9: Bootstrap for pause differences in the
Japanese corpus.
Subj.
Child Parent
WPS conj. affect nonflu. adverb cogn. percept.
A1 S* - - - S* L* -
A2 S? - S* - L* L* L*
A3 S? - - - L* L* L*
A4 S* - - - - - -
In the Japanese corpus, we observe that WPS is
larger in the case of short pauses. As we noticed
that the child often utters only a single word for
responses that follow a long pause, we analyzed
the content of these single word utterances. As
shown in Figure 4, for example, A1 tends to use
a word related to assent when latency is long, and
A4 tends to use a word related to filler, assent or
others when latency is long. Though there are in-
dividual differences, we confirm that the Japanese
children with ASD examined in this study tend
to delay their responses before uttering one word.
These characteristics may be related to the parent?s
question types and the child?s cognitive process,
and thus we need to examine these possibilities in
detail.
6 Conclusion
In this work, we focused on differentiation of chil-
dren with ASD and TD in terms of social com-
munication, particularly focusing on language and
speech features. Using narrative data, we exam-
ined several features on both the single utterance
A1 A2 A3 A4
Others
Laugh
Filler
Assent
Subject
P
e
r
c
e
n
t
a
g
e
 
o
f
 
o
n
e
?
w
o
r
d
 
r
e
s
p
o
n
c
e
s
0
.
0
0
.
2
0
.
4
0
.
6
0
.
8
1
.
0
Figure 4: The language category of one-word re-
sponses in the case of a long pause.
level and the narrative level. We examined fea-
tures mentioned in a number of previous works, as
well as a few novel features. We confirmed about
70% accuracy in an evaluation over single utter-
ances, and some narrative features also proved to
have a correlation with ASD.
For future directions, we plan to perform larger
scale experiments to examine the potential of these
features for automated ASD screening. Given the
results of this, we plan to move to applications in-
cluding the development of dialogue systems for
automatic ASD screening and social skills train-
ing.
Acknowledgments
We would like to thank the participants, children
and their parents, in this study. We also thank
Dr. Hidemi Iwasaka for his advice and support as
clinician in pediatrics. A part of this study was
conducted in Signal Analysis and Interpretation
Laboratory (SAIL), University of Southern Cali-
fornia. This study is supported by JSPS KAKEN
24240032.
References
American Psychiatric Association. 2013. The Diag-
nostic and Statistical Manual of Mental Disorders:
DSM 5.
Asgari, Meysam, Alireza Bayestehtashk, and Izhak
Shafran. 2013. Robust and Accurate Featuers for
Detecting and Diagnosing Autism Spectrum Disor-
ders. Proceedings of Interspeech, 191?194.
95
Asperger, H.. 1944. Die ,,Autistischen Psychopathen?
im Kindesalter. European Archives of Psychiatry
and Clinical Neuroscience, 117: 76?136.
Bone, D., Black, M. P., Lee, C. C., Williams, M.
E., Levitt, P., Lee, S., and Narayanan, S.. 2012.
Spontaneous-Speech Acoustic-Prosodic Features of
Children with Autism and the Interacting Psycholo-
gist. Proceedings of Interspeech.
Chaspari, T., Gibson, D. B., Lee, C.-C., and Narayanan,
S. S. 2013. Using physiology and language cues for
modeling verbal response latencies of children with
ASD. Proceedings of ICASSP, 3702?3706.
Davis, Megan, Kerstin Dautenhahn, CL Nehaniv, and
SD Powell. 2004. Towards an Interactive Sys-
tem Facilitating Therapeutic Narrative Elicitation in
Autism. Proceedings of NILE.
Dawson, Geraldine, Deborah Hill, Art Spencer, Larry
Galpert, and Linda Watson.. 1990. Affective ex-
changes between young autistic children and their
mothers. Journal of Abnormal Child Psychology,
18: 335?345.
de Marchena, A. and Inge-Marie E.. 2010. Conversa-
tional gestures in autism spectrum disorders: asyn-
chrony but not decreased frequency. Autism Re-
search, 3: 311?322.
Hanson M. H.. 1995. Glottal characteristics of female
speakers. Harvard University, Ph.D. dissertation.
Heeman, P. A., Lunsford, R., Selfridge, E., Black, L.,
and Van Santen, J.. 2010. Autism and interactional
aspects of dialogue. Proceedings of SIGDIAL, 249?
252.
Kanner, L.. 1943. Autistic disturbances of affective
contact. Nervous Child, 2: 217?250.
Kiss, G. and van Santen, J. P. H.. 2013. Estimating
Speaker-Specific Intonation Patterns Using the Lin-
ear Alignment Model. Proceedings of Interspeech
354?358.
Kiss, G., van Santen, J. P. H., Prud?hommeaux, E. T.,
and Black, L. M.. 2012. Quantitative Analysis of
Pitch in Speech of Children with Neurodevelopmen-
tal Disorders. Proceedings of Interspeech.
Lovaas, O Ivar, Robert Koegel, James Q Simmons, and
Judith Stevens Long. 1973. Some generalisation
and follow-up measures on autistic children in be-
haviour therapy. Journal of Applied Behavior Anal-
ysis, 6: 131?166.
Mairesse, Francois, Marilyn A Walker, Matthias R
Mehl, and Roger K Moore. 2007. Using Linguis-
tic cues for the automatic recognition of personality
in conversation and text. Journal of Artificial Intel-
ligence Research, 30: 457?500.
McCann, J. and Sue, P.. 2003. Prosody in autism
spectrum disorders: a critical review. International
Journal of Language & Communication Disorders,
38(4): 325?350.
Mower, E., Black, M. P., Flores, E., Williams, M., and
Narayanan, S.. 2011. Rachel: Design of an emo-
tionally targeted interactive agent for children with
autism. Proceedings of IEEE ICME, 1?6.
Newton, A. T., Kramer, A. D. I., and McIntosh, D. N..
2009. Autism online: a comparison of word usage
in bloggers with and without autism spectrum disor-
ders. Proceedings of SIGCHI, 463?466.
Paul, Rhea, Amy Augustyn, Ami Klin, and Fred R
Volkmar. 2005. Perception and production of
prosody by speakers with autism spectrum disor-
ders. Journal of Autism and Developmental Disor-
ders, 35: 205?220.
Pennebaker, James W, Martha E Francis, and Roger J
Booth. 2005. Linguistic inquiry and word count:
LIWC [Computer software] Austin, TX: liwc. net.
Rouhizadeh Masoud, Prud?hommeaux Emily, Roark
Brian, and van Santen Jan. 2013. Distributional se-
mantic models for the evaluation of disordered lan-
guage. Proceedings of NAACL-HLT, 709?714.
Santen, Jan PH, Richard W Sproat, and Alison Pres-
manes Hill. 2013. Quantifying repetitive speech
in autism spectrum disorders and language impair-
ment. Autism Research, 6: 372?383.
Sharda, Megha, T Padma Subhadra, Sanchita Sahay,
Chetan Nagaraja, Latika Singh, Ramesh Mishra,
Amit Sen, Nidhi Singhal, Donna Erickson, and Nan-
dini C Singh. 2010. Sounds of melody?Pitch pat-
terns of speech in autism. Neuroscience letters, 478:
42?45.
Van Santen, Jan PH, Emily T Prud?hommeaux, Lois
M Black, and Margaret Mitchell. 2010. Compu-
tational prosodic markers for autism. Autism, 14:
215?236.
Wallace, Charles J, Connie J Nelson, Robert Paul
Liberman, Robert A Aitchison, David Lukoff, John
P Elder, and Chris Ferris. 1980. A review and cri-
tique of social skills training with schizophrenic pa-
tients. Schizophrenia Bulletin, 6:42?63.
96
Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 34?42,
October 25, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Rule-based Syntactic Preprocessing
for Syntax-based Machine Translation
Yuto Hatakoshi, Graham Neubig, Sakriani Sakti, Tomoki Toda, Satoshi Nakamura
Nara Institute of Science and Technology
Graduate School of Information Science
Takayama, Ikoma, Nara 630-0192, Japan
{hatakoshi.yuto.hq8,neubig,ssakti,tomoki,s-nakamura}@is.naist.jp
Abstract
Several preprocessing techniques using
syntactic information and linguistically
motivated rules have been proposed to im-
prove the quality of phrase-based machine
translation (PBMT) output. On the other
hand, there has been little work on similar
techniques in the context of other trans-
lation formalisms such as syntax-based
SMT. In this paper, we examine whether
the sort of rule-based syntactic preprocess-
ing approaches that have proved beneficial
for PBMT can contribute to syntax-based
SMT. Specifically, we tailor a highly suc-
cessful preprocessing method for English-
Japanese PBMT to syntax-based SMT,
and find that while the gains achievable are
smaller than those for PBMT, significant
improvements in accuracy can be realized.
1 Introduction
In the widely-studied framework of phrase-based
machine translation (PBMT) (Koehn et al., 2003),
translation probabilities between phrases consist-
ing of multiple words are calculated, and trans-
lated phrases are rearranged by the reordering
model in the appropriate target language order.
While PBMT provides a light-weight framework
to learn translation models and achieves high
translation quality in many language pairs, it does
not directly incorporate morphological or syntac-
tic information. Thus, many preprocessing meth-
ods for PBMT using these types of information
have been proposed. Methods include preprocess-
ing to obtain accurate word alignments by the divi-
sion of the prefix of verbs (Nie?en and Ney, 2000),
preprocessing to reduce the errors in verb conju-
gation and noun case agreement (Avramidis and
Koehn, 2008), and many others. The effectiveness
of the syntactic preprocessing for PBMT has been
supported by these and various related works.
In particular, much attention has been paid to
preordering (Xia and McCord, 2004; Collins et
al., 2005), a class of preprocessing methods for
PBMT. PBMT has well-known problems with lan-
guage pairs that have very different word order,
due to the fact that the reordering model has dif-
ficulty estimating the probability of long distance
reorderings. Therefore, preordering methods at-
tempt to improve the translation quality of PBMT
by rearranging source language sentences into an
order closer to that of the target language. It?s of-
ten the case that preordering methods are based
on rule-based approaches, and these methods have
achieved great success in ameliorating the word
ordering problems faced by PBMT (Collins et al.,
2005; Xu et al., 2009; Isozaki et al., 2010b).
One particularly successful example of rule-
based syntactic preprocessing is Head Finalization
(Isozaki et al., 2010b), a method of syntactic pre-
processing for English to Japanese translation that
has significantly improved translation quality of
English-Japanese PBMT using simple rules based
on the syntactic structure of the two languages.
The most central part of the method, as indicated
by its name, is a reordering rule that moves the
English head word to the end of the corresponding
syntactic constituents to match the head-final syn-
tactic structure of Japanese sentences. Head Final-
ization also contains some additional preprocess-
ing steps such as determiner elimination, parti-
cle insertion and singularization to generate a sen-
tence that is closer to Japanese grammatical struc-
ture.
In addition to PBMT, there has also recently
been interest in syntax-based SMT (Yamada and
Knight, 2001; Liu et al., 2006), which translates
using syntactic information. However, few at-
tempts have been made at syntactic preprocessing
for syntax-based SMT, as the syntactic informa-
tion given by the parser is already incorporated
directly in the translation model. Notable excep-
34
tions include methods to perform tree transforma-
tions improving correspondence between the sen-
tence structure and word alignment (Burkett and
Klein, 2012), methods for binarizing parse trees to
match word alignments (Zhang et al., 2006), and
methods for adjusting label sets to be more ap-
propriate for syntax-based SMT (Hanneman and
Lavie, 2011; Tamura et al., 2013). It should be
noted that these methods of syntactic preprocess-
ing for syntax-based SMT are all based on auto-
matically learned rules, and there has been little in-
vestigation of the manually-created linguistically-
motivated rules that have proved useful in prepro-
cessing for PBMT.
In this paper, we examine whether rule-based
syntactic preprocessing methods designed for
PBMT can contribute anything to syntax-based
machine translation. Specifically, we examine
whether the reordering and lexical processing of
Head Finalization contributes to the improvement
of syntax-based machine translation as it did for
PBMT. Additionally, we examine whether it is
possible to incorporate the intuitions behind the
Head Finalization reordering rules as soft con-
straints by incorporating them as a decoder fea-
ture. As a result of our experiments, we demon-
strate that rule-based lexical processing can con-
tribute to improvement of translation quality of
syntax-based machine translation.
2 Head Finalization
Head Finalization is a syntactic preprocessing
method for English to Japanese PBMT, reducing
grammatical errors through reordering and lexi-
cal processing. Isozaki et al. (2010b) have re-
ported that translation quality of English-Japanese
PBMT is significantly improved using a transla-
tion model learned by English sentences prepro-
cessed by Head Finalization and Japanese sen-
tences. In fact, this method achieved the highest
results in the large scale NTCIR 2011 evaluation
(Sudoh et al., 2011), the first time a statistical ma-
chine translation (SMT) surpassed rule-based sys-
tems for this very difficult language pair, demon-
strating the utility of these simple syntactic trans-
formations from the point of view of PBMT.
2.1 Reordering
The reordering process of Head Finalization uses
a simple rule based on the features of Japanese
grammar. To convert English sentence into
John hit a ball
John hita ball
NN VBD DT NN
NP
VP
NP
S
VBD
VP
NP
S
DT NN
NP
NN
Original English
Head Final English
Add Japanese Particles
John hita ballva0 va2
Singularize, 
Eliminate Determiners
John hita ballva0 va2
Reordering
Figure 1: Head Finalization
Japanese word order, the English sentence is first
parsed using a syntactic parser, and then head
words are moved to the end of the corresponding
syntactic constituents in each non-terminal node
of the English syntax tree. This helps replicate
the ordering of words in Japanese grammar, where
syntactic head words come after non-head (depen-
dent) words.
Figure 1 shows an example of the application
of Head Finalization to an English sentence. The
head node of the English syntax tree is connected
to the parent node by a bold line. When this node
is the first child node, we move it behind the de-
pendent node in order to convert the English sen-
tence into head final order. In this case, moving
the head node VBD of black node VP to the end of
this node, we can obtain the sentence ?John a ball
hit? which is in a word order similar to Japanese.
2.2 Lexical Processing
In addition to reordering, Head Finalization con-
ducts the following three steps that do not affect
word order. These steps do not change the word
35
ordering, but still result in an improvement of
translation quality, and it can be assumed that the
effect of this variety of syntactic preprocessing is
not only applicable to PBMT but also other trans-
lation methods that do not share PBMT?s problems
of reordering such as syntax-based SMT. The three
steps included are as follows:
1. Pseudo-particle insertion
2. Determiner (?a?, ?an?, ?the?) elimination
3. Singularization
The motivation for the first step is that in con-
trast to English, which has relatively rigid word
order and marks grammatical cases of many noun
phrases according to their position relative to the
verb, Japanese marks the topic, subject, and object
using case marking particles. As Japanese parti-
cles are not found in English, Head Finalization
inserts ?pseudo-particles? to prevent a mistransla-
tion or lack of particles in the translation process.
In the pseudo-particle insertion process (1), we in-
sert the following three types of pseudo-particles
equivalent to Japanese case markers ?wa? (topic),
?ga? (subject) or ?wo? (object).
? va0: Subject particle of the main verb
? va1: Subject particle of other verbs
? va2: Object particle of any verb
In the example of Figure 1, we insert the topic par-
ticle va0 behind of ?John?, which is a subject of a
verb ?hit? and object particle va2 at the back of
object ?ball.?
Another source of divergence between the two
languages stems from the fact that Japanese does
not contain determiners or makes distinctions be-
tween singular and plural by inflection of nouns.
Thus, to generate a sentence that is closer to
Japanese, Head Finalization eliminates determin-
ers (2) and singularizes plural nouns (3) in addi-
tion to the pseudo-particle insertion.
In Figure 1, we can see that applying these
three processes to the source English sentence re-
sults in the sentence ?John va0 (wa) ball va2 (wo)
hit? which closely resembles the structure of the
Japanese translation ?jon wa bo-ru wo utta.?
3 Syntax-based Statistical Machine
Translation
Syntax-based SMT is a method for statistical
translation using syntactic information of the sen-
tence (Yamada and Knight, 2001; Liu et al., 2006).
By using translation patterns following the struc-
ture of linguistic syntax trees, syntax-based trans-
lations often makes it possible to achieve more
grammatical translations and reorderings com-
pared with PBMT. In this section, we describe
tree-to-string (T2S) machine translation based on
synchronous tree substitution grammars (STSG)
(Graehl et al., 2008), the variety of syntax-based
SMT that we use in our experiments.
T2S captures the syntactic relationship between
two languages by using the syntactic structure of
parsing results of the source sentence. Each trans-
lation pattern is expressed as a source sentence
subtree using rules including variables. The fol-
lowing example of a translation pattern include
two noun phrases NP
0
and NP
1
, which are trans-
lated and inserted into the target placeholders X
0
and X
1
respectively. The decoder generates the
translated sentence in consideration of the proba-
bility of translation pattern itself and translations
of the subtrees of NP
0
and NP
1
.
S((NP
0
) (VP(VBD hit) (NP
1
)))
? X
0
wa X
1
wo utta
T2S has several advantages over PBMT. First,
because the space of translation candidates is re-
duced using the source sentence subtree, it is often
possible to generate translations that are more ac-
curate, particularly with regards to long-distance
reordering, as long as the source parse is correct.
Second, the time to generate translation results is
also reduced because the search space is smaller
than PBMT. On the other hand, because T2S gen-
erates translation results using the result of auto-
matic parsing, translation quality highly depends
on the accuracy of the parser.
4 Applying Syntactic Preprocessing to
Syntax-based Machine Translation
In this section, we describe our proposed method
to apply Head Finalization to T2S translation.
Specifically, we examine two methods for incor-
porating the Head Finalization rules into syntax-
based SMT: through applying them as preprocess-
ing step to the trees used in T2S translation, and
36
through adding reordering information as a feature
of the translation patterns.
4.1 Syntactic Preprocessing for T2S
We applied the two types of processing shown in
Table 1 as preprocessing for T2S. This is similar
to preprocessing for PBMTwith the exception that
preprocessing for PBMT results in a transformed
string, and preprocessing for T2S results in a trans-
formed tree. In the following sections, we elabo-
rate on methods for applying these preprocessing
steps to T2S and some effects expected therefrom.
Table 1: Syntactic preprocessing applied to T2S
Preprocessing Description
Reordering Reordering based on Japanese
typical head-final grammatical
structure
Lexical Processing Pseudo-particle insertion, deter-
miner elimination, singulariza-
tion
4.1.1 Reordering for T2S
In the case of PBMT, reordering is used to change
the source sentence word order to be closer to
that of the target, reducing the burden on the rel-
atively weak PBMT reordering models. On the
other hand, because translation patterns of T2S
are expressed by using source sentence subtrees,
the effect of reordering problems are relatively
small, and the majority of reordering rules spec-
ified by hand can be automatically learned in a
well-trained T2S model. Therefore, preordering
is not expected to cause large gains, unlike in the
case of PBMT.
However, it can also be thought that preordering
can still have a positive influence on the translation
model training process, particularly by increasing
alignment accuracy. For example, training meth-
ods for word alignment such as the IBM or HMM
models (Och and Ney, 2003) are affected by word
order, and word alignment may be improved by
moving word order closer between the two lan-
guages. As alignment accuracy plays a important
role in T2S translation (Neubig and Duh, 2014), it
is reasonable to hypothesize that reordering may
also have a positive effect on T2S. In terms of the
actual incorporation with the T2S system, we sim-
ply follow the process in Figure 1, but output the
reordered tree instead of only the reordered termi-
nal nodes as is done for PBMT.
John hit a ball
NN VBD DT NN
NP
VP
NP
S
Original English
NN VBD NN VA
NP
VP
NP
S
VA
John hit ball va2va0
Lexical Processing
Figure 2: A method of applying Lexical Process-
ing
4.1.2 Lexical Processing for T2S
In comparison to reordering, Lexical Processing
may be expected to have a larger effect on T2S,
as it will both have the potential to increase align-
ment accuracy, and remove the burden of learning
rules to perform simple systematic changes that
can be written by hand. Figure 2 shows an ex-
ample of the application of Lexical Processing to
transform not strings, but trees.
In the pseudo-particle insertion component,
three pseudo particles ?va0,? ?va1,? and ?va2? (as
shown in Section 2.2) are added in the source En-
glish syntax tree as terminal nodes with the non-
terminal node ?VA?. As illustrated in Figure 2, par-
ticles are inserted as children at the end of the cor-
responding NP node. For example, in the figure
the topic particle ?va0? is inserted after ?John,?
subject of the verb ?hit,? and the object particle
?va2? is inserted at the end of the NP for ?ball,?
the object.
In the determiner elimination process, terminal
nodes ?a,? ?an,? and ?the? are eliminated along
with non-terminal node DT. Determiner ?a? and
its corresponding non-terminal DT are eliminated
in the Figure 2 example.
Singularization, like in the processing for
PBMT, simply changes plural noun terminals to
their base form.
4.2 Reordering Information as Soft
Constraints
As described in section 4.1.1, T2S work well on
language pairs that have very different word order,
but is sensitive to alignment accuracy. On the other
hand, we know that in most cases Japanese word
order tends to be head final, and thus any rules that
do not obey head final order may be the result of
bad alignments. On the other hand, there are some
cases where head final word order is not applica-
ble (such as sentences that contain the determiner
37
?no,? or situations where non-literal translations
are necessary) and a hard constraint to obey head-
final word order could be detrimental.
In order to incorporate this intuition, we add
a feature (HF-feature) to translation patterns that
conform to the reordering rules of Head Final-
ization. This gives the decoder ability to discern
translation patterns that follow the canonical re-
ordering patterns in English-Japanese translation,
and has the potential to improve translation quality
in the T2S translation model.
We use the log-linear approach (Och, 2003) to
add the Head Finalization feature (HF-feature). As
in the standard log-linear model, a source sen-
tence f is translated into a target language sen-
tence e, by searching for the sentence maximizing
the score:
?
e = arg max
e
w
T
? h(f ,e). (1)
where h(f , e) is a feature function vector. w is
a weight vector that scales the contribution from
each feature. Each feature can take any real value
which is useful to improve translation quality, such
as the log of the n-gram language model proba-
bility to represent fluency, or lexical/phrase trans-
lation probability to capture the word or phrase-
wise correspondence. Thus, if we can incorporate
the information about reordering expressed by the
Head Finalization reordering rule as a features in
this model, we can learn weights to inform the de-
coder that it should generally follow this canonical
ordering.
Figure 3 shows a procedure of Head Finaliza-
tion feature (HF-feature) addition. To add the
HF-feature to translation patterns, we examine
the translation rules, along with the alignments
between target and source terminals and non-
terminals. First, we apply the Reordering to the
source side of the translation pattern subtree ac-
cording to the canonical head-final reordering rule.
Second, we examine whether the word order of the
reordered translation pattern matches with that of
the target translation pattern for which the word
alignment is non-crossing, indicating that the tar-
get string is also in head-final word order. Finally,
we set a binary feature (h
HF
(f , e) = 1) if the tar-
get word order obeys the head final order. This
feature is only applied to translation patterns for
which the number of target side words is greater
than or equal to two.
VP
VBD NP
hit x0:NP
x0 wo
Source side of
translation pattern
Target side of
translation pattern
VP
NP VBD
hitx0:NP
1. Apply Reordering to 
source translation pattern
2. Add HF-feature
if word alignment is 
non-crossing
utta
Word alignment 
x0 woTarget side of
translation pattern
utta
Reordered
translation pattern
Figure 3: Procedure of HF-feature addition
Table 2: The details of NTCIR7
Dataset Lang Words Sentences
Average
length
train
En 99.0M 3.08M 32.13
Ja 117M 3.08M 37.99
dev
En 28.6k 0.82k 34.83
Ja 33.5k 0.82k 40.77
test
En 44.3k 1.38k 32.11
Ja 52.4k 1.38k 37.99
5 Experiment
In our experiment, we examined how much each
of the preprocessing steps (Reordering, Lexical
Processing) contribute to improve the translation
quality of PBMT and T2S. We also examined the
improvement in translation quality of T2S by the
introduction of the Head Finalization feature.
5.1 Experimental Environment
For our English to Japanese translation experi-
ments, we used NTCIR7 PATENT-MT?s Patent
corpus (Fujii et al., 2008). Table 2 shows the
details of training data (train), development data
(dev), and test data (test).
As the PBMT and T2S engines, we used the
Moses (Koehn et al., 2007) and Travatar (Neubig,
2013) translation toolkits with the default settings.
38
Enju (Miyao and Tsujii, 2002) is used to parse En-
glish sentences and KyTea (Neubig et al., 2011) is
used as a Japanese tokenizer. We generated word
alignments using GIZA++ (Och and Ney, 2003)
and trained a Kneser-Ney smoothed 5-gram LM
using SRILM (Stolcke et al., 2011). Minimum
Error Rate Training (MERT) (Och, 2003) is used
for tuning to optimize BLEU. MERT is replicated
three times to provide performance stability on test
set evaluation (Clark et al., 2011).
We used BLEU (Papineni et al., 2002) and
RIBES (Isozaki et al., 2010a) as evaluation mea-
sures of translation quality. RIBES is an eval-
uation method that focuses on word reordering
information, and is known to have high correla-
tion with human judgement for language pairs that
have very different word order such as English-
Japanese.
5.2 Result
Table 3 shows translation quality for each com-
bination of HF-feature, Reordering, and Lexical
Processing. Scores in boldface indicate no sig-
nificant difference in comparison with the con-
dition that has highest translation quality using
the bootstrap resampling method (Koehn, 2004)
(p < 0.05).
For PBMT, we can see that reordering plays an
extremely important role, with the highest BLEU
and RIBES scores being achieved when using Re-
ordering preprocessing (line 3, 4). Lexical Pro-
cessing also provided a slight performance gain
for PBMT.When we applied Lexical Processing to
PBMT, BLEU and RIBES scores were improved
(line 1 vs 2), although this gain was not significant
when Reordering was performed as well.
Overall T2S without any preprocessing
achieved better translation quality than all con-
ditions of PBMT (line 1 of T2S vs line 1-4 of
PBMT). In addition, BLEU and RIBES score of
T2S were clearly improved by Lexical Processing
(line 2, 4, 6, 8 vs line 1, 3, 5, 7), and these scores
are the highest of all conditions. On the other
hand, Reordering and HF-Feature addition had no
positive effect, and actually tended to slightly hurt
translation accuracy.
5.3 Analysis of Preprocessing
With regards to PBMT, as previous works on
preordering have already indicated, BLEU and
RIBES scores were significantly improved by Re-
ordering. In addition, Lexical Processing also con-
Table 5: Optimized weight of HF-feature in each
condition
HF-feature Reordering
Word Weight of
Processing HF-feature
+ - - -0.00707078
+ - + 0.00524676
+ + - 0.156724
+ + + -0.121326
tributed to improve translation quality of PBMT
slightly. We also investigated the influence
that each element of Lexical Processing (pseudo-
particle insertion, determiner elimination, singu-
larization) had on translation quality, and found
that the gains were mainly provided by particle
insertion, with little effect from determiner elim-
ination or singularization.
Although Reordering was effective for PBMT,
it did not provide any benefit for T2S. This in-
dicates that T2S can already conduct long dis-
tance word reordering relatively correctly, and
word alignment quality was not improved as much
as expected by closing the gap in word order be-
tween the two languages. This was verified by a
subjective evaluation of the data, finding very few
major reordering issues in the sentences translated
by T2S.
On the other hand, Lexical Processing func-
tioned effectively for not only PBMT but also T2S.
When added to the baseline, lexical processing on
its own resulted in a gain of 0.57 BLEU, and 0.99
RIBES points, a significant improvement, with
similar gains being seen in other settings as well.
Table 4 demonstrates a typical example of the
improvement of the translation result due to Lex-
ical Processing. It can be seen that translation
performance of particles (indicated by underlined
words) was improved. The underlined particle is
in the direct object position of the verb that corre-
sponds to ?comprises? in English, and thus should
be given the object particle ?? wo? as in the refer-
ence and the system using Lexical Processing. On
the other hand, in the baseline system the genitive
?? to? is generated instead due to misaligned par-
ticles being inserted in an incorrect position in the
translation rules.
5.4 Analysis of Feature Addition
Our experimental results indicated that translation
quality is not improved by HF-feature addition
(line 1-4 vs line 5-8). We conjecture that the rea-
son why HF-feature did not contribute to an im-
39
Table 3: Translation quality by combination of HF-feature, Reordering, and Lexical Processing. Bold
indicates results that are not statistically significantly different from the best result (39.60 BLEU in line
4 and 79.47 RIBES in line 2).
ID
PBMT T2S
HF-feature Reordering Lexical Processing BLEU RIBES BLEU RIBES
1 - - - 32.11 69.06 38.94 78.48
2 - - + 33.16 70.19 39.51 79.47
3 - + - 37.62 77.56 38.44 78.48
4 - + + 37.77 77.71 39.60 79.26
5 + - - ? ? 38.74 78.33
6 + - + ? ? 39.29 79.23
7 + + - ? ? 38.48 78.44
8 + + + ? ? 39.38 79.21
Table 4: Improvement of translation results due to Lexical Processing
Source another connector 96 , which is matable with this cable connector 90 , comprises a plurality of
male contacts 98 aligned in a row in an electrically insulative housing 97 as shown in the figure .
Reference ????????????????????????????????????
?????????????????????????????????
- Lexical Processing ???????????????????????????????????
???????????????????????????????????
???
+ Lexical Processing ???????????????????????????????????
???????????????????????????????????
??
provement in translation quality is that the reorder-
ing quality achieved by T2S translation was al-
ready sufficiently high, and the initial feature led
to confusion in MERT optimization.
Table 5 shows the optimized weight of the HF
feature in each condition. From this table, we can
see that in two of the conditions positive weights
are learned, and in two of the conditions negative
weights are learned. This indicates that there is no
consistent pattern of learning weights that corre-
spond to our intuition that head-final rules should
receive higher preference.
It is possible that other optimization methods,
or a more sophisticated way of inserting these fea-
tures into the translation rules could help alleviate
these problems.
6 Conclusion
In this paper, we analyzed the effect of applying
syntactic preprocessing methods to syntax-based
SMT. Additionally, we have adapted reordering
rules as a decoder feature. The results showed
that lexical processing, specifically insertion of
pseudo-particles, contributed to improving trans-
lation quality, and it was effective as preprocessing
for T2S.
It should be noted that this paper, while demon-
strating that the simple rule-based syntactic pro-
cessing methods that have been useful for PBMT
can also contribute to T2S in English-Japanese
translation, more work is required to ensure that
this will generalize to other settings. A next step in
our inquiry is the generalization of these results to
other proposed preprocessing techniques and other
language pairs. In addition, we would like to try
two ways described below. First, it is likely that
other tree transformations, for example changing
the internal structure of the tree by moving chil-
dren to different nodes, would help in cases where
it is common to translate into highly divergent syn-
tactic structures between the source and target lan-
guages. Second, we plan to investigate other ways
of incorporating the preprocessing rules as a soft
constraints, such as using n-best lists or forests to
enode many possible sentence interpretations.
References
Eleftherios Avramidis and Philipp Koehn. 2008. En-
riching morphologically poor languages for statisti-
cal machine translation. In Annual Meeting of the
40
Association for Computational Linguistics (ACL),
pages 763?770.
David Burkett and Dan Klein. 2012. Transforming
trees to improve syntactic convergence. In Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP), pages 863?872.
Jonathan H Clark, Chris Dyer, Alon Lavie, and Noah A
Smith. 2011. Better hypothesis testing for statisti-
cal machine translation: Controlling for optimizer
instability. In Annual Meeting of the Association for
Computational Linguistics (ACL), pages 176?181.
Michael Collins, Philipp Koehn, and Ivona Ku?cerov?a.
2005. Clause restructuring for statistical machine
translation. In Annual Meeting of the Association
for Computational Linguistics (ACL), pages 531?
540.
Atsushi Fujii, Masao Utiyama, Mikio Yamamoto,
Takehito Utsuro, Terumasa Ehara, Hiroshi Echizen-
ya, and Sayori Shimohata. 2008. Overview of the
patent translation task at the NTCIR-7 workshop. In
Proceedings of the 7th NTCIR Workshop Meeting,
pages 389?400.
Jonathan Graehl, Kevin Knight, and Jonathan May.
2008. Training tree transducers. Computational
Linguistics, pages 391?427.
Greg Hanneman and Alon Lavie. 2011. Automatic
category label coarsening for syntax-based machine
translation. In Workshop on Syntax and Structure in
Statistical Translation, pages 98?106.
Hideki Isozaki, Tsutomu Hirao, Kevin Duh, Katsuhito
Sudoh, and Hajime Tsukada. 2010a. Automatic
evaluation of translation quality for distant language
pairs. In Conference on Empirical Methods in Natu-
ral Language Processing (EMNLP), pages 944?952.
Hideki Isozaki, Katsuhito Sudoh, Hajime Tsukada, and
Kevin Duh. 2010b. Head finalization: A simple
reordering rule for SOV languages. In Proceedings
of the Joint Fifth Workshop on Statistical Machine
Translation and MetricsMATR, pages 244?251.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In North
American Chapter of the Association for Computa-
tional Linguistics, pages 48?54.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, et al. 2007. Moses: Open source
toolkit for statistical machine translation. In Annual
Meeting of the Association for Computational Lin-
guistics (ACL), pages 177?180.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Conference on
Empirical Methods in Natural Language Processing
(EMNLP), pages 388?395.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-
to-string alignment template for statistical machine
translation. In Annual Meeting of the Association
for Computational Linguistics (ACL), pages 609?
616.
Yusuke Miyao and Jun?ichi Tsujii. 2002. Maximum
entropy estimation for feature forests. In Proceed-
ings of the second international conference on Hu-
man Language Technology Research, pages 292?
297.
Graham Neubig and Kevin Duh. 2014. On the ele-
ments of an accurate tree-to-string machine transla-
tion system. In Annual Meeting of the Association
for Computational Linguistics (ACL), pages 143?
149.
Graham Neubig, Yosuke Nakata, and Shinsuke Mori.
2011. Pointwise prediction for robust, adaptable
japanese morphological analysis. In Annual Meet-
ing of the Association for Computational Linguistics
(ACL), pages 529?533.
Graham Neubig. 2013. Travatar: A forest-to-string
machine translation engine based on tree transduc-
ers. Annual Meeting of the Association for Compu-
tational Linguistics (ACL), page 91.
Sonja Nie?en and Hermann Ney. 2000. Improving
SMT quality with morpho-syntactic analysis. In
Proceedings of the 18th conference on Computa-
tional linguistics-Volume 2, pages 1081?1085.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational linguistics, pages 19?51.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Annual Meet-
ing of the Association for Computational Linguistics
(ACL), pages 160?167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Annual Meet-
ing of the Association for Computational Linguistics
(ACL), pages 311?318.
Andreas Stolcke, Jing Zheng, Wen Wang, and Victor
Abrash. 2011. SRILM at sixteen: Update and out-
look. In IEEE Automatic Speech Recognition and
Understanding Workshop (ASRU), page 5.
Katsuhito Sudoh, Kevin Duh, Hajime Tsukada,
Masaaki Nagata, Xianchao Wu, Takuya Matsuzaki,
and Jun ?ichi Tsujii. 2011. NTT-UT statistical ma-
chine translation in NTCIR-9 PatentMT. In Pro-
ceedings of NTCIR, pages 585?592.
Akihiro Tamura, Taro Watanabe, Eiichiro Sumita, Hi-
roya Takamura, and Manabu Okumura. 2013. Part-
of-speech induction in dependency trees for statisti-
cal machine translation. In Annual Meeting of the
Association for Computational Linguistics (ACL),
pages 841?851.
41
Fei Xia and Michael McCord. 2004. Improving a sta-
tistical mt system with automatically learned rewrite
patterns. In International Conference on Computa-
tional Linguistics (COLING), page 508.
Peng Xu, Jaeho Kang, Michael Ringgaard, and Franz
Och. 2009. Using a dependency parser to improve
smt for subject-object-verb languages. In North
American Chapter of the Association for Computa-
tional Linguistics, pages 245?253.
Kenji Yamada and Kevin Knight. 2001. A syntax-
based statistical translation model. In Annual Meet-
ing of the Association for Computational Linguistics
(ACL), pages 523?530.
Hao Zhang, Liang Huang, Daniel Gildea, and Kevin
Knight. 2006. Synchronous binarization for ma-
chine translation. In North American Chapter of the
Association for Computational Linguistics, pages
256?263.
42
