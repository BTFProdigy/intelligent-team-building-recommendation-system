Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 324?333,
Singapore, 6-7 August 2009.
c
?2009 ACL and AFNLP
Deriving lexical and syntactic expectation-based measures
for psycholinguistic modeling via incremental top-down parsing
Brian Roark
?
Asaf Bachrach
?
Carlos Cardenas
?
and Christophe Pallier
?
?
Center for Spoken Language Understanding, Oregon Health & Science University
?
INSERM-CEA Cognitive Neuroimaging Unit, Gif sur Yvette, France
?
MIT
roark@cslu.ogi.edu asafbac@gmail.com cardenas@mit.edu christophe@pallier.org
Abstract
A number of recent publications have
made use of the incremental output of
stochastic parsers to derive measures of
high utility for psycholinguistic modeling,
following the work of Hale (2001; 2003;
2006). In this paper, we present novel
methods for calculating separate lexical
and syntactic surprisal measures from a
single incremental parser using a lexical-
ized PCFG. We also present an approx-
imation to entropy measures that would
otherwise be intractable to calculate for a
grammar of that size. Empirical results
demonstrate the utility of our methods in
predicting human reading times.
1 Introduction
Assessment of linguistic complexity has played
an important role in psycholinguistics and neu-
rolinguistics for a long time, from the use of
mean length of utterance and related scores in
child language development (Klee and Fitzgerald,
1985), to complexity scores related to reading dif-
ficulty in human sentence processing studies (Yn-
gve, 1960; Frazier, 1985; Gibson, 1998). Opera-
tionally, such linguistic complexity scores are de-
rived via deterministic manual (human) annotation
and scoring algorithms of language samples. Nat-
ural language processing has been employed to
automate the extraction of such measures (Sagae
et al, 2005; Roark et al, 2007), which can have
high utility in terms of reduction of time required
to annotate and score samples. More interest-
ingly, however, novel data driven methods are be-
ing increasingly employed in this sphere, yield-
ing language sample characterizations that require
NLP in their derivation. For example, scores
derived from variously estimated language mod-
els have been used to evaluate and classify lan-
guage samples associated with neurodevelopmen-
tal or neurodegenerative disorders (Roark et al,
2007; Solorio and Liu, 2008; Gabani et al, 2009),
as well as within general studies of human sen-
tence processing (Hale, 2001; 2003; 2006). These
scores cannot feasibly be derived by hand, but
rather rely on large-scale statistical models and
structured inference algorithms to be derived. This
is quickly becoming an important application of
NLP, making possible new methods in the study
of human language processing in both typical and
impaired populations.
The use of broad-coverage parsing for psy-
cholinguistic modeling has become very popular
recently. Hale (2001) suggested a measure (sur-
prisal) derived from an Earley (1970) parser us-
ing a probabilistic context-free grammar (PCFG)
for psycholinguistic modeling; and in later work
(Hale, 2003; 2006) he suggested an alternate
parser-derived measure (entropy reduction) that
may also account for some human sentence pro-
cessing performance. Recent work continues to
advocate surprisal in particular as a very use-
ful measure for predicting processing difficulty
(Boston et al, 2008a; Boston et al, 2008b; Dem-
berg and Keller, 2008; Levy, 2008), and the mea-
sure has been derived using a variety of incre-
mental (left-to-right) parsing strategies, includ-
ing an Earley parser (Boston et al, 2008a), the
Roark (2001) incremental top-down parser (Dem-
berg and Keller, 2008), and an n-best version of
the Nivre et al (2007) incremental dependency
parser (Boston et al, 2008a; 2008b). Deriving
such measures by hand, even for a relatively lim-
ited set of stimuli, is not feasible, hence parsing
plays a critical role in this developing psycholin-
guistic enterprise.
There is no single measure that can account for
all of the factors influencing human sentence pro-
cessing performance, and some of the most recent
work on using parser-derived measures for psy-
cholinguistic modeling has looked to try to de-
rive multiple, complementary measures. One of
324
the key distinctions being looked at is syntactic
versus lexical expectations (Gibson, 2006). For
example, in Demberg and Keller (2008), trials
were run deriving surprisal from the Roark (2001)
parser under two different conditions: fully lex-
icalized parsing, and fully unlexicalized parsing
(to pre-terminal part-of-speech tags). Boston et
al. (2008a) capture a similar distinction by mak-
ing use of an unlexicalized PCFG within an Ear-
ley parser and a fully lexicalized unlabeled depen-
dency parser (Nivre et al, 2007). As Demberg and
Keller (2008) point out, fully unlexicalized gram-
mars ignore important lexico-syntactic informa-
tion when deriving the ?syntactic? expectations,
such as subcategorization preferences of particular
verbs, which are generally accepted to impact syn-
tactic expectations in human sentence processing
(Garnsey et al, 1997). Demberg and Keller argue,
based on their results, for unlexicalized surprisal
instead of lexicalized surprisal. Here we present a
novel method for deriving separate syntactic and
lexical surprisal measures from a fully lexicalized
incremental parser, to allow for rich probabilistic
grammars to be used to derive either measure, and
demonstrate the utility of this method versus that
of Demberg and Keller in empirical trials.
The use of large-scale lexicalized grammars
presents a problem for using an Earley parser to
derive surprisal or for the calculation of entropy as
Hale (2003; 2006) defines it, because both meth-
ods require matrix inversion of a matrix with di-
mensionality the size of the non-terminal set. With
very large lexicalized PCFGs, the size of the non-
terminal set is too large for tractable matrix in-
version. The use of an incremental, beam-search
parser provides a tractable approximation to both
measures. Incremental top-down and left-corner
parsers have been shown to effectively (and effi-
ciently) make use of non-local features from the
left-context to yield very high accuracy syntactic
parses (Roark, 2001; Henderson, 2003; Collins
and Roark, 2004), and we will use such rich mod-
els to derive our scores.
In addition to teasing apart syntactic and lexical
surprisal (defined explicitly in ?3), we present an
approximation to the full entropy that Hale (2003;
2006) used to define the entropy reduction hypoth-
esis. Such an entropy measure is derived via a pre-
dictive step, advancing the parses independently
of the input, as described in ?3.3. We also present
syntactic and lexical alternatives for this measure,
and demonstrate the utility of making such a dis-
tinction for entropy as well as surprisal.
The purpose of this paper is threefold. First,
to present a careful and well-motivated decompo-
sition of lexical and syntactic expectation-based
measures from a given lexicalized PCFG. Sec-
ond, to explicitly document methods for calculat-
ing these and other measures from a specific in-
cremental parser. And finally, to present some em-
pirical validation of the novel measures from real
reading time trials. We modified the Roark (2001)
parser to calculate the discussed measures
1
, and
the empirical results in ?4 show several things,
including: 1) using a fully lexicalized parser to
calculate syntactic surprisal and entropy provides
higher predictive utility for reading times than
these measures calculated via unlexicalized pars-
ing (as in Demberg and Keller); and 2) syntactic
entropy is a useful predictor of reading time.
2 Notation and preliminaries
A probabilistic context-free grammar (PCFG)
G = (V, T, S
?
, P, ?) consists of a set of non-
terminal variables V ; a set of terminal items
(words) T ; a special start non-terminal S
?
? V ;
a set of rule productions P of the form A ? ?
for A ? V , ? ? (V ? T )
?
; and a function ?
that assigns probabilities to each rule in P such
that for any given non-terminal symbol X ? V ,
?
?
?(X ? ?) = 1.
For a given rule A ? ? ? P , let the func-
tion RHS return the right-hand side of the rule, i.e.,
RHS(A ? ?) = ?. Without loss of generality, we
will assume that for every rule A ? ? ? P , one
of two cases holds: either RHS(A ? ?) ? T or
RHS(A ? ?) ? V
?
. That is, the right-hand side
sequences consist of either (1) exactly one termi-
nal item, or (2) zero or more non-terminals.
Let W ? T
n
be a terminal string of length n,
i.e., W = W
1
. . .W
n
and |W | = n. Let W [i, j]
denote the substring beginning at word W
i
and
ending at word W
j
of the string. Then W
|W |
is the
last word in the string, and W [1, |W |] is the string
as a whole. Adjacent strings represent concate-
nation, i.e., W [1, i]W [i+1, j] = W [1, j]. Thus
W [1, i]w represents the string where W
i+1
= w.
We can define a ?derives? relation (denoted?
G
for a given PCFG G) as follows: ?A? ?
G
???
if and only if A ? ? ? P . A string W ? T
?
is in the language of a grammar G if and only
if S
?
+
?
G
W , i.e., a sequence of one or more
derivation steps yields the string from the start
1
The parser version will be made publicly available.
325
non-terminal. A leftmost derivation begins with
S
?
and each derivation step replaces the leftmost
non-terminal A in the yield with some ? such that
A ? ? ? P . For a leftmost derivation S
?
?
?
G
?,
where ? ? (V ? T )
?
, the sequence of deriva-
tion steps that yield ? can be represented as a
tree, with the start symbol S
?
at the root, and the
?yield? sequence ? at the leaves of the tree. A
complete tree has only terminal items in the yield,
i.e., ? ? T
?
; a partial tree has some non-terminal
items in the yield. With a leftmost derivation, the
yield ? = ?? partitions into an initial sequence
of terminals ? ? T
?
followed by a sequence of
non-terminals ? ? V
?
. For a complete derivation,
? = ; for a partial derivation ? ? V
+
, i.e., one or
more non-terminals. Let T (G,W [1, i]) be the set
of complete trees with W [1, i] as the yield of the
tree, given PCFG G.
A leftmost derivation D consists of a sequence
of |D| steps. Let D
i
represent the i
th
step in
the derivation D, and D[i, j] represent the subse-
quence of steps in D beginning with D
i
and end-
ing with D
j
. Note that D
|D|
is the last step in
the derivation, and D[1, |D|] is the derivation as
a whole. Each step D
i
in the derivation is a rule
in G, i.e., D
i
? P for all i. The probability of the
derivation and the corresponding tree is:
?(D) =
m
?
i=1
?(D
i
) (1)
Let D(G,W [1, i]) be the set of all possible left-
most derivations D (with respect to G) such that
RHS(D
|D|
) = W
i
. These are the set of partial left-
most derivations whose last step used a production
with terminal W
i
on the right-hand side. The pre-
fix probability of W [1, i] with respect to G is
PrefixProb
G
(W [1, i]) =
?
D?D(G,W [1,i])
?(D) (2)
From this prefix probability, we can calculate the
conditional probability of each word w ? T in the
terminal vocabulary, given the preceding sequence
W [1, i] as follows:
P
G
(w | W [1, i]) =
PrefixProb
G
(W [1, i]w)
P
w
?
?T
PrefixProb
G
(W [1, i]w
?
)
=
PrefixProb
G
(W [1, i]w)
PrefixProb
G
(W [1, i])
(3)
This, in fact, is precisely the conditional proba-
bility that is used for language modeling for such
applications as speech recognition and machine
translation, which was the motivation for various
syntactic language modeling approaches (Jelinek
and Lafferty, 1991; Stolcke, 1995; Chelba and Je-
linek, 1998; Roark, 2001).
As with language modeling, it is important to
model the end of the string as well, usually with
an explicit end symbol, e.g., </s>. For a string
W [1, i], we can calculate its prefix probability as
shown above. To calculate its complete probabil-
ity, we must sum the probabilities over the set of
complete trees T (G,W [1, i]). In such a way, we
can calculate the conditional probability of ending
the string with </s> given W [1, i] as follows:
P
G
(</s> | W [1, i]) =
?
D?T (G,W [1,i])
?(D)
PrefixProb
G
(W [1, i])
(4)
2.1 Incremental top-down parsing
In this section, we review relevant details of
the Roark (2001) incremental top-down parser,
as configured for use here. As presented in
Roark (2004), the probabilities in the PCFG are
smoothed so that the parser is guaranteed not to
fail due to garden pathing, despite following a
beam search strategy. Hence there is always a non-
zero prefix probability as defined in Eq. 2.
The parser follows a top-down leftmost deriva-
tion strategy. The grammar is factored so that ev-
ery production has either a single terminal item on
the right-hand side or is of the form A ? B A-B,
where A,B ? V and the factored A-B category
can expand to any sequence of children categories
of A that can follow B. This factorization of n-
ary productions continues to nullary factored pro-
ductions, i.e., the end of the original production
A ? B
1
. . . B
n
is signaled with an empty produc-
tion A-B
1
-. . . -B
n
? .
The parser maintains a set of possible connected
derivations, weighted via the PCFG. It uses a beam
search, whereby the highest scoring derivations
are worked on first, and derivations that fall out-
side of the beam are discarded. The reader is re-
ferred to Roark (2001; 2004) for specifics about
the beam search.
The model conditions the probability of each
production on features extracted from the par-
tial tree, including non-local node labels such as
parents, grandparents and siblings from the left-
context, as well as c-commanding lexical items.
Hence this is a lexicalized grammar, though the
incremental nature precludes a general head-first
strategy, rather one that looks to the left-context
for c-commanding lexical items.
To avoid some of the early prediction of struc-
ture, the version of the Roark parser that we used
326
performs an additional grammar transformation
beyond the simple factorization already described
? a selective left-corner transform of left-recursive
productions (Johnson and Roark, 2000). In the
transformed structure, slash categories are used to
avoid predicting left-recursive structure until some
explicit indication of modification is present, e.g.,
a preposition.
The final step in parsing, following the last word
in the string, is to ?complete? all non-terminals
in the yield of the tree. All of these open non-
terminals are composite factored categories, such
as S-NP-VP, which are ?completed? by rewriting
to . The probability of these  productions is what
allows for the calculation of the conditional prob-
ability of ending the string, shown in Eq. 4.
One final note about the size of the non-terminal
set and the intractability of exact inference for
such a scenario. The non-terminal set not only
includes the original atomic non-terminals of the
grammar, but also any categories created by gram-
mar factorization (S-NP) or the left-corner trans-
form (NP/NP). Additionally, however, to remain
context-free, the non-terminal set must include
categories that incorporate non-local features used
by the statistical model into their label, includ-
ing parents, grandparents and sibling categories in
the left-context, as well as c-commanding lexical
heads. These non-local features must be made lo-
cal by encoding them in the non-terminal labels,
leading to a very large non-terminal set and in-
tractable exact inference. Heavy smoothing is re-
quired when estimating the resulting PCFG. The
benefit of such a non-terminal set is a rich model,
which enables a more peaked statistical distribu-
tion around high quality syntactic structures and
thus more effective pruning of the search space.
The fully connected left-context produced by top-
down derivation strategies provides very rich fea-
tures for the stochastic parsing models. See Roark
(2001; 2004) for discussion of these issues.
We now turn to measures that can be derived
from the parser which may be of use for psycholin-
guistic modeling.
3 Parser and grammar derived measures
3.1 Surprisal
The surprisal at word W
i
is the negative log prob-
ability of W
i
given the preceding words. Using
prefix probabilities, this can be calculated as:
S
G
(W
i
) = ? log
PrefixProb
G
(W [1, i])
PrefixProb
G
(W [1, i? 1])
(5)
Substituting equation 2 into this, we get
S
G
(W
i
) = ? log
?
D?D(G,W [1,i])
?(D)
?
D?D(G,W [1,i?1])
?(D)
(6)
If we are using a beam-search parser, some of the
derivations are pruned away. Let B(G,W [1, i]) ?
D(G,W [1, i]) be the set of derivations in the
beam. Then the surprisal can be approximated as
S
G
(W
i
) ? ? log
?
D?B(G,W [1,i])
?(D)
?
D?B(G,W [1,i?1])
?(D)
(7)
Any pruning in the beam search will result in a de-
ficient probability distribution, i.e., a distribution
that sums to less than 1. Roark?s thesis (2001)
showed that the amount of probability mass lost
for this particular approach is very low, hence this
provides a very tight bound on the actual surprisal
given the model.
3.2 Lexical and Syntactic surprisal
High surprisal scores result when the prefix proba-
bility at word W
i
is low relative to the prefix prob-
ability at word W
i?1
. Sometimes this is due to the
identity of W
i
, i.e., it is a surprising word given
the context. Other times, it may not be the lexical
identity of the word so much as the syntactic struc-
ture that must be created to integrate the word into
the derivations. One would like to tease surprisal
apart into ?syntactic surprisal? versus ?lexical sur-
prisal?, which would capture this intuition of the
lexical versus syntactic dimensions to the score.
Our solution to this has the beneficial property of
producing two scores whose sum equals the origi-
nal surprisal score.
The original surprisal score is calculated via
sets of partial derivations at the point when each
word W
i
is integrated into the syntactic structure,
D(G,W [1, i]). We then calculate the ratio from
point to point in sequence. To tease apart the lexi-
cal and syntactic surprisal, we will consider sets of
partial derivations immediately before each word
W
i
is integrated into the syntactic structure, i.e.,
D[1, |D|?1] for D ? D(G,W [1, i]). Recall that
the last derivation move for every derivation in the
set is from the POS-tag to the lexical item. Hence
the sequence of derivation moves that excludes the
last one includes all structure except the word W
i
.
Then the syntactic surprisal is calculated as:
SynS
G
(W
i
) = ? log
P
D?D(G,W [1,i])
?(D[1, |D|?1])
P
D?D(G,W [1,i?1])
?(D)
(8)
327
and the lexical surprisal is calculated as:
LexS
G
(W
i
) = ? log
P
D?D(G,W [1,i])
?(D)
P
D?D(G,W [1,i])
?(D[1, |D|?1])
(9)
Note that the numerator of SynS
G
(W
i
) is the de-
nominator of LexS
G
(W
i
), hence they sum to form
total surprisal S
G
(W
i
). As with total surprisal,
these measures can be defined either for the full
set D(G,W [1, i]) or for a pruned beam of deriva-
tions B(G,W [1, i]) ? D(G,W [1, i]).
Finally, we replicated the Demberg and Keller
(2008) ?unlexicalized? surprisal by replacing ev-
ery lexical item in the training corpus with its
POS-tag, and then parsing the POS-tags of the lan-
guage samples rather than the words. This differs
from our syntactic surprisal by having no lexical
conditioning events for rule probabilities, and by
having no ambiguity about the POS-tag of the lex-
ical items in the string. We will refer to the result-
ing surprisal measure as ?POS surprisal? to distin-
guish it from our syntactic surprisal measure.
3.3 Entropy
Entropy scores of the sort advocated by Hale
(2003; 2006) involve calculation over the set of
complete derivations consistent with the set of par-
tial derivations. Hale performs this calculation
efficiently via matrix inversion, which explains
the use of relatively small-scale grammars with
tractably sized non-terminal sets. Such methods
are not tractable for the kinds of richly condi-
tioned, large-scale PCFGs that we advocate using
here. At each word in the string, the Roark (2001)
top-down parser provides access to the weighted
set of partial analyses in the beam; the set of com-
plete derivations consistent with these is not im-
mediately accessible, hence additional work is re-
quired to calculate such measures.
Let H(D) be the entropy over a set of deriva-
tions D, calculated as follows:
H(D) = ?
X
D?D
?(D)
P
D
?
?D
?(D
?
)
log
?(D)
P
D
?
?D
?(D
?
)
(10)
If the set of derivations D = D(G,W [1, i])
is a set of partial derivations for string W [1, i],
then H(D) is a measure of uncertainty over the
partial derivations, i.e., the uncertainty regarding
the correct analysis of what has already been pro-
cessed. This can be calculated directly from the
existing parser operations. If the set of derivations
are the complete derivations consistent with the set
of partial derivations ? complete derivations that
could occur over the set of possible continuations
of the string ? then this is a measure of the un-
certainty about what is yet to come. We would
like measures that can capture this distinction be-
tween (a) uncertainty of what has already been
processed (?current ambiguity?) versus (b) uncer-
tainty of what is yet to be processed (?predictive
entropy?). In addition, as with surprisal, we would
like to tease apart the syntactic uncertainty versus
lexical uncertainty.
To calculate the predictive entropy after word
sequence W [1, i], we modify the parser as fol-
lows: the parser extends the set of partial deriva-
tions to include all possible next words (the entire
vocabulary plus </s>), and calculates the entropy
over that set. This measure is calculated from just
one additional word beyond the current word, and
hence is an approximation to Hale?s conditional
entropy of grammatical continuations, which is
over complete derivations. We will denote this as
H
1
G
(W [1, i]) and calculate it as follows:
H
1
G
(W [1, i]) = H(
?
w?T?{</s>}
D(G,W [1, i]w)) (11)
This is performing a predictive step that the base-
line parser does not perform, extending the parses
to all possible next words.
Unlike surprisal, entropy does not decompose
straightforwardly into syntactic and lexical com-
ponents that sum to the original composite mea-
sure. To tease apart entropy due to syntactic un-
certainty versus that due to lexical uncertainty, we
can define the set of derivations up to the pre-
terminal (POS-tag) non-terminals as follows. Let
S(D) = {D[1, |D|?1] : D ? D}, i.e., the set of
derivations achieved by removing the last step of
all derivations inD. Then we can calculate a ?syn-
tactic? H
1
G
as follows:
SynH
1
G
(W [1, i]) = H(
[
w?T?{</s>}
S(D(G,W [1, i]w))) (12)
Finally, ?lexical? H
1
G
is defined in terms of the
conditional probabilities derived from prefix prob-
abilities as defined in Eq. 3.
LexH
1
G
(W [1, i]) =
?
X
w?T?{</s>}
P
G
(w | W [1, i]) logP
G
(w | W [1, i]) (13)
As a practical matter, these values are calculated
within the Roark parser as follows. A ?dummy?
word is created that can be assigned every POS-
tag, and the parser extends from the current state to
this dummy word. (The beam threshold is greatly
328
expanded to allow for many possible extensions.)
Then every word in the vocabulary is substituted
for the word, and the appropriate probabilities cal-
culated over the beam. Finally, the actual next
word is substituted, the beam threshold is reduced
to the actual working threshold, and the requisite
number of analyses are advanced to continue pars-
ing the string. This represents a significant amount
of additional work for the parser ? particularly for
vocabulary sizes that we currently use, on the or-
der of tens of thousands of words.
As with surprisal, we can calculate an ?unlex-
icalized? version of the measure by training and
parsing just to POS-tags. We will refer to this sort
of entropy as ?POS entropy?.
4 Empirical validation
4.1 Subjects and stimuli
In order to test the psycholinguistic relevance of
the different measures produced by the parser, we
conducted a word by word reading experiment.
23 native speakers of English read 4 short texts
(mean length: 883.5 words, 49.25 sentences). The
texts were the written versions of narratives used
in a parallel fMRI experiment making use of the
same parser derived measures and whose results
will be published in a different paper (Bachrach et
al., 2009). The narratives contained a high density
of syntactically complex structures (in the form of
sentential embeddings, relative clauses and other
non-local dependencies) but were constructed so
as to appear highly natural. The modified version
of the Roark parser, trained on the Brown Cor-
pus section of the Penn Treebank (Marcus et al,
1993), was used to parse the different narratives
and produce the word by word measures.
4.2 Procedure
Each narrative was presented line by line (cer-
tain sentences required more than one line) on a
computer screen (Dell Optiplex 755 running Win-
dows XP Professional) using Linger 2.88
2
. Each
line contained 11.5 words on average. Each word
would appear in its relative position on the screen.
The subject would then be required to push a key-
board button to advance to the next word. The
original word would then disappear and the fol-
lowing word appear in the subsequent position on
the screen. After certain sentences a comprehen-
sion question would appear on the screen (10 per
narrative). This was done in order to encourage
2
http://tedlab.mit.edu/?dr/Linger/readme.html
subjects to pay attention and to provide data for a
post-hoc evaluation of comprehension. After each
narrative, subjects were instructed to take a short
break (2 minutes on average).
4.3 Data analysis
The log (base 10) of the reaction times were ana-
lyzed using a linear mixed effects regression anal-
ysis implemented in the language R (Bates et al,
2008). Reaction times longer than 1500 ms and
shorter than 150 ms (raw) were excluded from the
analysis (4.8% of total data). Since button press la-
tencies inferior to 150 ms must have been planned
prior to the presentation of the word, we consid-
ered that they could not reflect stimulus driven ef-
fects. Data from the first and last words on each
line were discarded.
The combined data from the 4 narratives was
first modeled using a model which included or-
der of word in the narrative
3
, word length, parser-
derived lexical surprisal, unigram frequency, bi-
gram probability, syntactic surprisal, lexical en-
tropy, syntactic entropy and mean number of
parser derivation steps as numeric regressors. We
also included the unlexicalized POS variants of
syntactic surprisal and entropy, along the lines of
Demberg and Keller (2008), as detailed in ? 3.
Table 1 presents the correlations between these
mean-centered measures.
In addition, we modeled word class
(open/closed) as a categorical factor in order
to assess interaction between class and the vari-
ables of interest, since such an interaction has
been observed in the case of frequency (Bradley,
1983). Finally, the random effect part of the
model included intercepts for subjects, words and
sentences. We report significant effects at the
threshold p < .05.
Given the presence of significant interactions
between lexical class (open/closed) and a number
of the variables of interests, we decided to split
the data set into open and closed class words and
model these separately (linear mixed effects with
the same numeric variables as in the full model).
In order to evaluate the usefulness of splitting
total surprisal into lexical and syntactic compo-
nents we compared, using a likelihood ratio test,
a model where lexical and syntactic surprisal are
modeled as distinct regressors to a model where a
single regressor equal to their sum (total surprisal)
3
This is a regressor to control for the trend of subjects to
read faster later in the narrative.
329
Predictor SynH LexH SynS LexS Freq Bgrm PosS PosH Step WLen
Syntactic Entropy (SynH) 1.00 -0.26 0.00 0.24 -0.24 0.20 0.02 0.55 -0.05 0.18
Lexical Entropy (LexH) -0.26 1.00 0.01 -0.40 0.43 -0.38 -0.03 0.02 0.11 -0.29
Syntactic Surprisal (SynS) 0.00 0.01 1.00 -0.12 0.08 0.18 0.77 0.21 0.38 -0.03
Lexical Surprisal (LexS) 0.24 -0.40 -0.12 1.00 -0.81 0.87 -0.10 -0.20 -0.35 0.64
Unigram Frequency (Freq) -0.24 0.43 0.08 -0.81 1.00 -0.69 0.02 0.18 0.31 -0.72
Bigram Probability (Bgrm) 0.20 -0.38 0.18 0.87 -0.69 1.00 0.11 -0.11 -0.16 0.56
POS Surprisal (PosS) 0.02 -0.03 0.77 -0.10 0.02 0.11 1.00 0.22 0.32 0.02
POS Entropy (PosH) 0.55 0.02 0.21 -0.20 0.18 -0.11 0.22 1.00 0.16 -0.11
Derivation steps (Step) -0.05 0.11 0.38 -0.35 0.31 -0.16 0.32 0.16 1.00 -0.24
Word Length (WLen) 0.18 -0.29 -0.03 0.64 -0.72 0.56 0.02 -0.11 -0.24 1.00
Table 1: Correlations between (mean-centered) predictors. Note that unigram frequencies were represented as logs, other
scores as negative logs, hence the sign of the correlations.
was included. If the larger model provides a sig-
nificantly better fit than the smaller model, this
provides evidence that distinguishing between lex-
ical and syntactic contributions to surprisal is rel-
evant. Since total entropy is not a sum of syntactic
and lexical entropy, an analogous test would not
be valid in that case.
4.4 Results
All subjects successfully answered the com-
prehension questions (92.8% correct responses,
S.D.=5.1). In the full model, we observed signifi-
cant main effects of word class as well as of lexical
surprisal, bigram probability, unigram frequency,
syntactic entropy, POS entropy and of order in the
narrative. Syntactic surprisal, lexical entropy and
number of steps had no significant effect. Word
length also had no significant main effect but inter-
acted significantly with word class (open/closed).
Word class also interacted significantly with lexi-
cal surprisal, unigram frequency and syntactic sur-
prisal.
The presence of these interactions led us to
construct models restricted to open and closed
class items respectively. The estimated parame-
ters are reported in Table 2. Reading time for open
class words showed significant effects of unigram
frequency, syntactic surprisal, syntactic entropy,
POS entropy and order within the narrative. The
positive effect of length approached significance.
Reading time for closed class words exhibited sig-
nificant effects of lexical surprisal, bigram prob-
ability, syntactic entropy and order in the narra-
tive. Length had a non-significant negative effect,
thus explaining the interaction observed in the full
model.
The models with separate lexical and syntac-
tic surprisal performed better than models includ-
ing combined surprisal. For open class words, the
Akaike?s information criterion (AIC) was -54810
for the combined model and -54819 for the inde-
pendent model (likelihood ratio test comparing the
Estimate Std. Error t-value
Open-class
(Intercept) 2.40?10
+00
2.39?10
?02
100.4*
Lexical Surprisal -1.99?10
?04
7.28?10
?04
-0.3
Word Length 8.97?10
?04
4.62?10
?04
1.9
Bigram 4.18?10
?04
5.27?10
?04
0.8
Unigram Freq -2.43?10
?03
1.20?10
?03
-2.0*
Derivation Steps -1.17?10
?03
9.02?10
?04
-1.3
Syntactic Entropy 2.55?10
?03
6.19?10
?04
4.1*
Lexical Entropy 3.96?10
?04
6.68?10
?04
0.6
Syntactic Surprisal 3.28?10
?03
9.71?10
?04
3.4*
Order in narrative -1.43?10
?05
4.34?10
?06
-3.3*
POS Surprisal -6.84?10
?04
8.11?10
?04
-0.8
POS Entropy 1.47?10
?03
6.05?10
?04
2.4*
Closed-class
(Intercept) 2.42?10
+00
2.32?10
?02
104.3*
Lexical Surprisal 2.02?10
?03
7.84?10
?04
2.6*
Word Length -1.87?10
?03
1.13?10
?03
-1.7
Bigram 1.19?10
?03
4.94?10
?04
2.4*
Unigram Freq 1.69?10
?03
2.67?10
?03
0.6
Derivation Steps 3.01?10
?04
5.09?10
?04
0.6
Syntactic Entropy 3.15?10
?03
5.05?10
?04
6.2*
Lexical Entropy 1.83?10
?04
8.63?10
?04
0.2
Syntactic Surprisal 3.00?10
?04
8.35?10
?04
0.4
Order in narrative -1.33?10
?05
3.99?10
?06
-3.3*
POS Surprisal -6.46?10
?04
6.81?10
?04
-0.9
POS Entropy 6.63?10
?04
5.04?10
?04
1.3
Table 2: Estimated effects from mixed effects models on
open and closed items (stars denote significance at p<.05)
two, nested, models: ?
2
(1)=10.7, p<.001). For
closed class items, combined model?s AIC was -
61467 and full model?s AIC was -61469 (likeli-
hood ratio test: ?
2
(1)=3.54, p=0.06).
4.5 Discussion
Our results demonstrate the relevance of model-
ing psycholinguistic processes using an incremen-
tal probabilistic parser, and the utility of the novel
measures presented here. Of particular interest
are: the significant effects of our syntactic en-
tropy measure; the independent contributions of
lexical surprisal, bigram probability and unigram
frequency; and the differences between the pre-
dictions of the lexicalized parsing model and the
unlexicalized (POS) parsing model.
The effect of entropy, or uncertainty regarding
330
the upcoming input independent of the surprise
of that input, has been observed in non-linguistic
tasks (Hyman, 1953; Bestmann et al, 2008) but
to our knowledge has not been quantified before
in the context of sentence processing. The use-
fulness of computational modeling is particularly
evident in the case of entropy given the absence of
any subjective procedure for its evaluation
4
. The
results argue in favor of a predictive parsing archi-
tecture (Van Berkum et al, 2005). The approach
to entropy here differs from the one described in
Hale (2006) in a couple of ways. First, as dis-
cussed above, the calculation procedure is differ-
ent ? we focus on extending the derivations with
just one word, rather than to all possible complete
derivations. Second, and most importantly, Hale
emphasizes entropy reduction (or the gain in in-
formation, given an input, regarding the rest of the
sentence) as the correlate of cognitive cost while
here we are interested in the amount of entropy it-
self (and not the size of change).
Interestingly, we observed only an effect of syn-
tactic entropy, not lexical entropy. Recent ERP
work has demonstrated that subjects do form spe-
cific lexical predictions in the context of sentence
processing (Van Berkum et al, 2005; DeLong et
al., 2005) and so we suspect that the absence of
lexical entropy effect might be partly due to sparse
data. Lexical surprisal and entropy were calcu-
lated using the internal state of a parser trained
on the relatively small Brown corpus. Lexical en-
tropy showed no significant effect while lexical
surprisal affected only closed class words. This
pattern of results might be due to the sparseness
of the relevant information in such a small corpus
(e.g., verb/object preferences) and the relevance of
extra-textual dimensions (world knowledge, con-
textual information) to lexical-specific prediction.
Closed class words are both more frequent (and
hence better sampled) and are less sensitive to
world knowledge, yet are often determined by the
grammatical context.
Demberg and Keller (2008) made use of the
same parsing architecture used here to compute a
syntactic surprisal measure, but used an unlexical-
ized parser (down to POS-tags rather than words)
for this score. Their ?lexicalized? surprisal is
equivalent to our total surprisal (lexical surprisal
+ syntactic surprisal), while their POS surprisal is
4
The Cloze procedure (Taylor, 1953) is one way to derive
probabilities that could be used to calculate entropy, though
this procedure is usually conducted with lexical elicitation,
which would make syntactic entropy calculations difficult.
derived from a completely different model. In con-
trast, our approach achieves lexical and syntactic
measures from the same model. In order to eval-
uate the difference between the two approaches
we added unlexicalized POS surprisal calculated
along the lines of that paper to our model, along
with an unlexicalized POS entropy from the same
model. We found no effect of unlexicalized POS
surprisal
5
and a significant (but relatively small)
effect of unlexicalized POS entropy. While syn-
tactic surprisal was correlated with POS surprisal
(see Table 1) and syntactic entropy correlated with
POS entropy, the fact that our syntactic measures
still had a significant effect suggests that lexical
information contributes towards the formation of
syntactic expectations.
While the effect of surprisal calculated by an
incremental top down parser has been already
demonstrated (Demberg and Keller, 2008), our re-
sults argue for a distinction between the effect
of lexical surprisal and that of syntactic surprisal
without requiring unlexicalized parsing of the sort
that Demberg and Keller advocate. It is important
to keep in mind that this distinction between types
of prediction (and as a consequence, prediction er-
ror) is not equivalent to the one drawn in the tradi-
tional cognitive science modularity debate, which
has focused on the source of these predictions. We
found a positive effect of syntactic surprisal in the
case of open class words. The absence of an effect
for closed class words remains to be explained.
We quantified word specific surprisal using 3
sources: the parser?s internal state (lexical sur-
prisal); probability given the preceding word (neg-
ative log bigram probability); and the unigram fre-
quency of the word in a large corpus
6
. As can
be observed in Table 1, these three measures are
highly correlated
7
. This is the consequence of the
smoothing in the estimation procedure but also re-
lates to a more general fact about language use:
overall, more frequent words are also words more
expected to appear in a specific context (Anderson
and Schooler, 1991). Despite these strong corre-
lations, the three measures produced independent
5
We also ran the model including unlexicalized POS sur-
prisal without our syntactic surprisal or syntactic entropy, and
in this condition the unlexicalized POS surprisal measure had
a nearly significant effect (t = 1.85), which is consistent with
the results in Boston et al (2008a) and Demberg and Keller
(2008).
6
The unigram frequencies came from the HAL corpus
(Lund and Burgess, 1996). All other statistical models were
estimated from the Brown Corpus.
7
Unigram frequencies were represented as logs, the others
as negative logs, hence the sign of the correlations.
331
effects. Unigram frequency had a significant effect
for open class words while bigram probability and
lexical surprisal each had an effect on reading time
of closed class items. Bigram probability has been
often found to affect reading time using eye move-
ment measures. This is the first study to demon-
strate an additional effect of contextual surprisal
given the preceding sentential context (lexical sur-
prisal). Demberg and Keller found no effect for
surprisal once bigram and unigram probabilities
were included in the model but, importantly, they
did not distinguish lexical and syntactic surprisal,
rather ?lexicalized? and ?unlexicalized? surprisal.
5 Summary
We have presented novel methods for teasing apart
syntactic and lexical surprisal from a fully lexi-
calized parser, as well as for extending the oper-
ation of a predictive parser to capture novel en-
tropy measures that are also shown to be rele-
vant to psycholinguistic modeling. Such auto-
matic methods provide psycholinguistically rele-
vant measures that are intractable to calculate by
hand. The empirical validation presented here
demonstrated that the new measures ? particularly
syntactic entropy and syntactic surprisal ? have
high utility for modeling human reading time data.
Our approach to calculating syntactic surprisal,
based on fully lexicalized parsing, provided sig-
nificant effects, while the POS-tag based (unlexi-
calized) surprisal ? of the sort used in Boston et
al. (2008a) and Demberg and Keller (2008) ? did
not provide a significant effect in our trials. Fur-
ther, we showed an effect of lexical surprisal for
closed class words even when combined with uni-
gram and bigram probabilities in the same model.
This work contributes to the important, develop-
ing enterprise of leveraging data-driven NLP ap-
proaches to derive new measures of high utility for
psycholinguistic and neuropsychological studies.
Acknowledgments
Thanks to Michael Collins, John Hale and Shravan
Vasishth for valuable discussions about this work.
This research was supported in part by NSF Grant
#BCS-0826654. Any opinions, findings, conclu-
sions or recommendations expressed in this publi-
cation are those of the authors and do not neces-
sarily reflect the views of the NSF.
References
J.R. Anderson and L.J. Schooler. 1991. Reflections of
the environment in memory. Psychological Science,
2(6):396?408.
A. Bachrach, B. Roark, A. Marantz, S. Whitfield-
Gabrieli, C. Cardenas, and J.D.E. Gabrieli. 2009.
Incremental prediction in naturalistic language pro-
cessing: An fMRI study. In preparation.
D. Bates, M. Maechler, and B. Dai, 2008. lme4: Linear
mixed-effects models using S4 classes. R package
version 0.999375-20.
S. Bestmann, L.M. Harrison, F. Blankenburg, R.B.
Mars, P. Haggard, and K.J. Friston. 2008. Influence
of uncertainty and surprise on human corticospinal
excitability during preparation for action. Current
Biology, 18:775?780.
M. Ferrara Boston, J.T. Hale, R. Kliegl, U. Patil, and
S. Vasishth. 2008a. Parsing costs as predictors
of reading difficulty: An evaluation using the Pots-
dam sentence corpus. Journal of Eye Movement Re-
search, 2(1):1?12.
M. Ferrara Boston, J.T. Hale, R. Kliegl, and S. Va-
sishth. 2008b. Surprising parser actions and read-
ing difficulty. In Proceedings of ACL-08:HLT, Short
Papers, pages 5?8.
D.C. Bradley. 1983. Computational Distinctions of
Vocabulary Type. Indiana University Linguistics
Club, Bloomington.
C. Chelba and F. Jelinek. 1998. Exploiting syntactic
structure for language modeling. In Proceedings of
ACL-COLING, pages 225?231.
M.J. Collins and B. Roark. 2004. Incremental parsing
with the perceptron algorithm. In Proceedings of
ACL, pages 111?118.
K.A. DeLong, T.P. Urbach, and M. Kutas. 2005. Prob-
abilistic word pre-activation during language com-
prehension inferred from electrical brain activity.
Nature Neuroscience, 8(8):1117?1121.
V. Demberg and F. Keller. 2008. Data from eye-
tracking corpora as evidence for theories of syntactic
processing complexity. Cognition, 109(2):193?210.
J. Earley. 1970. An efficient context-free parsing algo-
rithm. Communications of the ACM, 6(8):451?455.
L. Frazier. 1985. Syntactic complexity. In D.R.
Dowty, L. Karttunen, and A.M. Zwicky, editors,
Natural Language Parsing. Cambridge University
Press, Cambridge, UK.
K. Gabani, M. Sherman, T. Solorio, and Y. Liu.
2009. A corpus-based approach for the prediction
of language impairment in monolingual English and
Spanish-English bilingual children. In Proceedings
of NAACL-HLT.
S.M. Garnsey, N.J. Pearlmutter, E. Myers, and M.A.
Lotocky. 1997. The contributions of verb bias and
plausibility to the comprehension of temporarily am-
biguous sentences. Journal of Memory and Lan-
guage, 37(1):58?93.
332
E. Gibson. 1998. Linguistic complexity: locality of
syntactic dependencies. Cognition, 68(1):1?76.
E. Gibson. 2006. The interaction of top-down and
bottom-up statistics in the resolution of syntactic
category ambiguity. Journal of Memory and Lan-
guage, 54(3):363?388.
J.T. Hale. 2001. A probabilistic Earley parser as a
psycholinguistic model. In Proceedings of the 2nd
meeting of NAACL.
J.T. Hale. 2003. The information conveyed by words
in sentences. Journal of Psycholinguistic Research,
32(2):101?123.
J.T. Hale. 2006. Uncertainty about the rest of the sen-
tence. Cognitive Science, 30(4):643?672.
J. Henderson. 2003. Inducing history representations
for broad coverage statistical parsing. In Proceed-
ings of HLT-NAACL, pages 24?31.
R. Hyman. 1953. Stimulus information as a determi-
nant of reaction time. Journal of Experimental Psy-
chology: General, 45(3):188?96.
F. Jelinek and J. Lafferty. 1991. Computation of
the probability of initial substring generation by
stochastic context-free grammars. Computational
Linguistics, 17(3):315?323.
M. Johnson and B. Roark. 2000. Compact non-left-
recursive grammars using the selective left-corner
transform and factoring. In Proceedings of COL-
ING, pages 355?361.
T. Klee and M.D. Fitzgerald. 1985. The relation be-
tween grammatical development and mean length
of utterance in morphemes. Journal of Child Lan-
guage, 12:251?269.
R. Levy. 2008. Expectation-based syntactic compre-
hension. Cognition, 106(3):1126?1177.
K. Lund and C. Burgess. 1996. Producing
high-dimensional semantic spaces from lexical co-
occurrence. Behavior Research Methods, Instru-
ments, & Computers, 28:203?208.
M.P. Marcus, B. Santorini, and M.A. Marcinkiewicz.
1993. Building a large annotated corpus of En-
glish: The Penn Treebank. Computational Linguis-
tics, 19(2):313?330.
J. Nivre, J. Hall, J. Nilsson, A. Chanev, G. Eryigit,
S. K?ubler, S. Marinov, and E. Marsi. 2007. Malt-
parser: A language-independent system for data-
driven dependency parsing. Natural Language En-
gineering, 13(2):95?135.
B. Roark, M. Mitchell, and K. Hollingshead. 2007.
Syntactic complexity measures for detecting mild
cognitive impairment. In Proceedings of BioNLP
Workshop at ACL, pages 1?8.
B. Roark. 2001. Probabilistic top-down parsing
and language modeling. Computational Linguistics,
27(2):249?276.
B. Roark. 2004. Robust garden path parsing. Natural
Language Engineering, 10(1):1?24.
K. Sagae, A. Lavie, and B. MacWhinney. 2005. Au-
tomatic measurement of syntactic development in
child langugage. In Proceedings of ACL, pages 197?
204.
T. Solorio and Y. Liu. 2008. Using language models
to identify language impairment in Spanish-English
bilingual children. In Proceedings of BioNLP Work-
shop at ACL, pages 116?117.
A. Stolcke. 1995. An efficient probabilistic context-
free parsing algorithm that computes prefix proba-
bilities. Computational Linguistics, 21(2):165?202.
W.L. Taylor. 1953. Cloze procedure: A new tool
for measuring readability. Journalism Quarterly,
30:415?433.
J.J.A. Van Berkum, C.M. Brown, P. Zwitserlood,
V.Kooijman, and P. Hagoort. 2005. Anticipat-
ing upcoming words in discourse: Evidence from
ERPs and reading times. Learning and Memory,
31(3):443?467.
V.H. Yngve. 1960. A model and an hypothesis for lan-
guage structure. Proceedings of the American Philo-
sophical Society, 104:444?466.
333
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1189?1198,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Complexity Metrics in an Incremental Right-corner Parser
Stephen Wu Asaf Bachrach? Carlos Cardenas? William Schuler?
Department of Computer Science, University of Minnesota
? Unit de Neuroimagerie Cognitive INSERM-CEA
? Department of Brain & Cognitive Sciences, Massachussetts Institute of Technology
? University of Minnesota and The Ohio State University
swu@cs.umn.edu ?asaf@mit.edu ?cardenas@mit.edu ?schuler@ling.ohio-state.edu
Abstract
Hierarchical HMM (HHMM) parsers
make promising cognitive models: while
they use a bounded model of working
memory and pursue incremental hypothe-
ses in parallel, they still achieve parsing
accuracies competitive with chart-based
techniques. This paper aims to validate
that a right-corner HHMM parser is also
able to produce complexity metrics, which
quantify a reader?s incremental difficulty
in understanding a sentence. Besides
defining standard metrics in the HHMM
framework, a new metric, embedding
difference, is also proposed, which tests
the hypothesis that HHMM store elements
represents syntactic working memory.
Results show that HHMM surprisal
outperforms all other evaluated metrics
in predicting reading times, and that
embedding difference makes a significant,
independent contribution.
1 Introduction
Since the introduction of a parser-based calcula-
tion for surprisal by Hale (2001), statistical tech-
niques have been become common as models of
reading difficulty and linguistic complexity. Sur-
prisal has received a lot of attention in recent lit-
erature due to nice mathematical properties (Levy,
2008) and predictive ability on eye-tracking move-
ments (Demberg and Keller, 2008; Boston et al,
2008a). Many other complexity metrics have
been suggested as mutually contributing to reading
difficulty; for example, entropy reduction (Hale,
2006), bigram probabilities (McDonald and Shill-
cock, 2003), and split-syntactic/lexical versions of
other metrics (Roark et al, 2009).
A parser-derived complexity metric such as sur-
prisal can only be as good (empirically) as the
model of language from which it derives (Frank,
2009). Ideally, a psychologically-plausible lan-
guage model would produce a surprisal that would
correlate better with linguistic complexity. There-
fore, the specification of how to encode a syntac-
tic language model is of utmost importance to the
quality of the metric.
However, it is difficult to quantify linguis-
tic complexity and reading difficulty. The two
commonly-used empirical quantifications of read-
ing difficulty are eye-tracking measurements and
word-by-word reading times; this paper uses read-
ing times to find the predictiveness of several
parser-derived complexity metrics. Various fac-
tors (i.e., from syntax, semantics, discourse) are
likely necessary for a full accounting of linguis-
tic complexity, so current computational models
(with some exceptions) narrow the scope to syn-
tactic or lexical complexity.
Three complexity metrics will be calculated in
a Hierarchical Hidden Markov Model (HHMM)
parser that recognizes trees in right-corner form
(the left-right dual of left-corner form). This type
of parser performs competitively on standard pars-
ing tasks (Schuler et al, 2010); also, it reflects
plausible accounts of human language processing
as incremental (Tanenhaus et al, 1995; Brants and
Crocker, 2000), as considering hypotheses proba-
bilistically in parallel (Dahan and Gaskell, 2007),
as bounding memory usage to short-term mem-
ory limits (Cowan, 2001), and as requiring more
memory storage for center-embedding structures
than for right- or left-branching ones (Chomsky
and Miller, 1963; Gibson, 1998). Also, unlike
most other parsers, this parser preserves the arc-
eager/arc-standard ambiguity of Abney and John-
1189
son (1991). Typical parsing strategies are arc-
standard, keeping all right-descendants open for
subsequent attachment; but since there can be an
unbounded number of such open constituents, this
assumption is not compatible with simple mod-
els of bounded memory. A consistently arc-eager
strategy acknowledges memory bounds, but yields
dead-end parses. Both analyses are considered in
right-corner HHMM parsing.
The purpose of this paper is to determine
whether the language model defined by the
HHMM parser can also predict reading times ?
it would be strange if a psychologically plausi-
ble model did not also produce viable complex-
ity metrics. In the course of showing that the
HHMM parser does, in fact, predict reading times,
we will define surprisal and entropy reduction in
the HHMM parser, and introduce a third metric
called embedding difference.
Gibson (1998; 2000) hypothesized two types
of syntactic processing costs: integration cost, in
which incremental input is combined with exist-
ing structures; and memory cost, where unfinished
syntactic constructions may incur some short-term
memory usage. HHMM surprisal and entropy
reduction may be considered forms of integra-
tion cost. Though typical PCFG surprisal has
been considered a forward-looking metric (Dem-
berg and Keller, 2008), the incremental nature of
the right-corner transform causes surprisal and en-
tropy reduction in the HHMM parser to measure
the likelihood of grammatical structures that were
hypothesized before evidence was observed for
them. Therefore, these HHMM metrics resemble
an integration cost encompassing both backward-
looking and forward-looking information.
On the other hand, embedding difference is
designed to model the cost of storing center-
embedded structures in working memory. Chen,
Gibson, and Wolf (2005) showed that sentences
requiring more syntactic memory during sen-
tence processing increased reading times, and it
is widely understood that center-embedding incurs
significant syntactic processing costs (Miller and
Chomsky, 1963; Gibson, 1998). Thus, we would
expect for the usage of the center-embedding
memory store in an HHMM parser to correlate
with reading times (and therefore linguistic com-
plexity).
The HHMM parser processes syntactic con-
structs using a bounded number of store states,
defined to represent short-term memory elements;
additional states are utilized whenever center-
embedded syntactic structures are present. Simi-
lar models such as Crocker and Brants (2000) im-
plicitly allow an infinite memory size, but Schuler
et al (2008; 2010) showed that a right-corner
HHMM parser can parse most sentences in En-
glish with 4 or fewer center-embedded-depth lev-
els. This behavior is similar to the hypothesized
size of a human short-term memory store (Cowan,
2001). A positive result in predicting reading
times will lend additional validity to the claim
that the HHMM parser?s bounded memory cor-
responds to bounded memory in human sentence
processing.
The rest of this paper is organized as fol-
lows: Section 2 defines the language model of the
HHMM parser, including definitions of the three
complexity metrics. The methodology for evalu-
ating the complexity metrics is described in Sec-
tion 3, with actual results in Section 4. Further dis-
cussion on results, and comparisons to other work,
are in Section 5.
2 Parsing Model
This section describes an incremental parser in
which surprisal and entropy reduction are sim-
ple calculations (Section 2.1). The parser uses a
Hierarchical Hidden Markov Model (Section 2.2)
and recognizes trees in a right-corner form (Sec-
tion 2.3 and 2.4). The new complexity metric, em-
bedding difference (Section 2.5), is a natural con-
sequence of this HHMM definition. The model
is equivalent to previous HHMM parsers (Schuler,
2009), but reorganized into 5 cases to clarify the
right-corner structure of the parsed sentences.
2.1 Surprisal and Entropy in HMMs
Hidden Markov Models (HMMs) probabilistically
connect sequences of observed states ot and hid-
den states qt at corresponding time steps t. In pars-
ing, observed states are words; hidden states can
be a conglomerate state of linguistic information,
here taken to be syntactic.
The HMM is an incremental, time-series struc-
ture, so one of its by-products is the prefix prob-
ability, which will be used to calculate surprisal.
This is the probability that that words o1..t have
been observed at time t, regardless of which syn-
tactic states q1..t produced them. Bayes? Law and
Markov independence assumptions allow this to
1190
be calculated from two generative probability dis-
tributions.1
Pre(o1..t)=
?
q1..t
P(o1..t q1..t) (1)
def=
?
q1..t
t
?
?=1
P?A(q? | q??1)?P?B(o? | q? ) (2)
Here, probabilities arise from a Transition
Model (?A) between hidden states and an Ob-
servation Model (?B) that generates an observed
state from a hidden state. These models are so
termed for historical reasons (Rabiner, 1990).
Surprisal (Hale, 2001) is then a straightforward
calculation from the prefix probability.
Surprisal(t) = log2
Pre(o1..t?1)
Pre(o1..t)
(3)
This framing of prefix probability and surprisal in
a time-series model is equivalent to Hale?s (2001;
2006), assuming that q1..t ? Dt, i.e., that the syn-
tactic states we are considering form derivations
Dt, or partial trees, consistent with the observed
words. We will see that this is the case for our
parser in Sections 2.2?2.4.
Entropy is a measure of uncertainty, defined as
H(x) = ?P(x) log2 P(x). Now, the entropy Ht
of a t-word string o1..t in an HMM can be written:
Ht =
?
q1..t
P(q1..t o1..t) log2 P(q1..t o1..t) (4)
and entropy reduction (Hale, 2003; Hale, 2006) at
the tth word is then
ER(ot) = max(0, Ht?1 ? Ht) (5)
Both of these metrics fall out naturally from the
time-series representation of the language model.
The third complexity metric, embedding differ-
ence, will be discussed after additional back-
ground in Section 2.5.
In the implementation of an HMM, candidate
states at a given time qt are kept in a trel-
lis, with step-by-step backpointers to the highest-
probability q1..t?1.2 Also, the best qt are often kept
in a beam Bt, discarding low-probability states.
1Technically, a prior distribution over hidden states,
P(q0), is necessary. This q0 is factored and taken to be a de-
terministic constant, and is therefore unimportant as a proba-
bility model.
2Typical tasks in an HMM include finding the most likely
sequence via the Viterbi algorithm, which stores these back-
pointers to maximum-probability previous states and can
uniquely find the most likely sequence.
This mitigates the problems of large state spaces
(e.g., that of all possible grammatical derivations).
Since beams have been shown to perform well
(Brants and Crocker, 2000; Roark, 2001; Boston
et al, 2008b), complexity metrics in this paper
are calculated on a beam rather than over all (un-
bounded) possible derivations Dt. The equations
above, then, will replace the assumption q1..t ?Dt
with qt?Bt.
2.2 Hierarchical Hidden Markov Models
Hidden states q can have internal structure; in Hi-
erarchical HMMs (Fine et al, 1998; Murphy and
Paskin, 2001), this internal structure will be used
to represent syntax trees and looks like several
HMMs stacked on top of each other. As such, qt
is factored into sequences of depth-specific vari-
ables ? one for each of D levels in the HMM hi-
erarchy. In addition, an intermediate variable ft is
introduced to interface between the levels.
qt
def= ?q1t . . . qDt ? (6)
ft
def= ?f1t . . . fDt ? (7)
Transition probabilities P?A(qt | qt?1) over com-
plex hidden states qt are calculated in two phases:
? Reduce phase. Yields an intermediate
state ft, in which component HMMs may ter-
minate. This ft tells ?higher? HMMs to hold
over their information if ?lower? levels are in
operation at any time step t, and tells lower
HMMs to signal when they?re done.
? Shift phase. Yields a modeled hidden state qt,
in which unterminated HMMs transition, and
terminated HMMs are re-initialized from
their parent HMMs.
Each phase is factored according to level-
specific reduce and shift models, ?F and ?Q:
P?A(qt|qt?1) =
?
ft
P(ft|qt?1)?P(qt|ft qt?1) (8)
def=
?
f1..Dt
D
?
d=1
P?F(f
d
t |fd+1t qdt?1qd?1t?1 )
? P?Q(qdt |fd+1t fdt qdt?1qd?1t ) (9)
with fD+1t and q0t defined as constants. Note that
only qt is present at the end of the probability cal-
culation. In step t, ft?1 will be unused, so the
marginalization of Equation 9 does not lose any
information.
1191
. . .
. . .
. . .
. . .
f3t?1
f2t?1
f1t?1
q1t?1
q2t?1
q3t?1
ot?1
f3t
f2t
f1t
q1t
q2t
q3t
ot
(a) Dependency structure in the HHMM
parser. Conditional probabilities at a node are
dependent on incoming arcs.
d=1
d=2
d=3
word
t=1 t=2 t=3 t=4 t=5 t=6 t=7 t=8
the
engineers
pulled
off an
engineering
trick
? ? ? ? ? ? ?
? ? vbd
VBD/PRT
? ? ?
dt
NP/NN
S/VP
S/VP
S/NP
S/NN
S/NN S
(b) HHMM parser as a store whose elements at each time step are listed
vertically, showing a good hypothesis on a sample sentence out of many
kept in parallel. Variables corresponding to qdt are shown.
S
NP
DT
the
NN
engineers
VP
VBD
VBD
pulled
PRT
off
NP
DT
an
NN
NN
engineering
NN
trick
(c) A sample sentence in CNF.
S
S/NN
S/NN
S/NP
S/VP
NP
NP/NN
DT
the
NN
engineers
VBD
VBD/PRT
VBD
pulled
PRT
off
DT
an
NN
engineering
NN
trick
(d) The right-corner transformed version of (c).
Figure 1: Various graphical representations of HHMM parser operation. (a) shows probabilistic depen-
dencies. (b) considers the qdt store to be incremental syntactic information. (c)?(d) demonstrate the
right-corner transform, similar to a left-to-right traversal of (c). In ?NP/NN? we say that NP is the active
constituent and NN is the awaited.
The Observation Model ?B is comparatively
much simpler. It is only dependent on the syntac-
tic state at D (or the deepest active HHMM level).
P?B(ot | qt)
def= P(ot | qDt ) (10)
Figure 1(a) gives a schematic of the dependency
structure of Equations 8?10 for D = 3. Evalua-
tions in this paper are done with D = 4, following
the results of Schuler, et al (2008).
2.3 Parsing right-corner trees
In this HHMM formulation, states and dependen-
cies are optimized for parsing right-corner trees
(Schuler et al, 2008; Schuler et al, 2010). A sam-
ple transformation between CNF and right-corner
trees is in Figures 1(c)?1(d).
Figure 1(b) shows the corresponding store-
element interpretation3 of the right corner tree
in 1(d). These can be used as a case study to
see what kind of operations need to occur in an
3This is technically a pushdown automoton (PDA), where
the store is limited to D elements. When referring to direc-
tions (e.g., up, down), PDAs are typically described opposite
of the one in Figure 1(b); here, we push ?up? instead of down.
HHMM when parsing right-corner trees. There
is one unique set of HHMM state values for each
tree, so the operations can be seen on either the
tree or the store elements.
At each time step t, a certain number of el-
ements (maximum D) are kept in memory, i.e.,
in the store. New words are observed input, and
the bottom occupied element (the ?frontier? of the
store) is the context; together, they determine what
the store will look like at t+1. We can characterize
the types of store-element changes by when they
happen in Figures 1(b) and 1(d):
Cross-level Expansion (CLE). Occupies a new
store element at a given time step. For exam-
ple, at t=1, a new store element is occupied
which can interact with the observed word,
?the.? At t = 3, an expansion occupies the
second store element.
In-level Reduction (ILR). Completes an active
constituent that is a unary child in the right-
corner tree; always accompanied by an in-
level expansion. At t= 2, ?engineers? com-
pletes the active NP constituent; however, the
1192
level is not yet complete since the NP is along
the left-branching trunk of the tree.
In-level Expansion (ILE). Starts a new active
constituent at an already-occupied store ele-
ment; always follows an in-level reduction.
With the NP complete in t= 2, a new active
constituent S is produced at t=3.
In-level Transition (ILT). Transitions the store
to a new state in the next time step at the same
level, where the awaited constituent changes
and the active constituent remains the same.
This describes each of the steps from t=4 to
t=8 at d=1 .
Cross-level Reduction (CLR). Vacates a store
element on seeing a complete active con-
stituent. This occurs after t = 4; ?off?
completes the active (at depth 2) VBD con-
stituent, and vacates store element 2. This
is accompanied with an in-level transition at
depth 1, producing the store at t=5. It should
be noted that with some probability, complet-
ing the active constituent does not vacate the
store element, and the in-level reduction case
would have to be invoked.
The in-level/cross-level ambiguity occurs in the
expansion as well as the reduction, similar to Ab-
ney and Johnson?s arc-eager/arc-standard compo-
sition strategies (1991). At t=3, another possible
hypothesis would be to remain on store element
1 using an ILE instead of a CLE. The HHMM
parser, unlike most other parsers, will preserve this
in-level/cross-level ambiguity by considering both
hypotheses in parallel.
2.4 Reduce and Shift Models
With the understanding of what operations need to
occur, a formal definition of the language model is
in order. Let us begin with the relevant variables.
A shift variable qdt at depth d and time step t is
a syntactic state that must represent the active and
awaited constituents of right-corner form:
qdt
def= ?gAqdt , g
W
qdt
? (11)
e.g., in Figure 1(b), q12=?NP,NN?=NP/NN. Each g is
a constituent from the pre-right-corner grammar,
G.
Reduce variables f are then enlisted to ensure
that in-level and cross-level operations are correct.
fdt
def= ?kfdt , gfdt ? (12)
First, kfdt is a switching variable that differenti-
ates between ILT, CLE/CLR, and ILE/ILR. This
switching is the most important aspect of fdt , so
regardless of what gfdt is, we will use:
? fdt ? F0 when kfdt =0, (ILT/no-op)
? fdt ? F1 when kfdt =1, (CLE/CLR)
? fdt ? FG when kfdt ? G. (ILE/ILR)
Then, gfdt is used to keep track of a completely-
recognized constituent whenever a reduction oc-
curs (ILR or CLR). For example, in Figure 1(b),
after time step 2, an NP has been completely rec-
ognized and precipitates an ILR. The NP gets
stored in gf13 for use in the ensuing ILE instead
of appearing in the store-elements.
This leads us to a specification of the reduce and
shift probability models. The reduce step happens
first at each time step. True to its name, the re-
duce step handles in-level and cross-level reduc-
tions (the second and third case below):
P?F(f
d
t | fd+1t qdt?1qd?1t?1 )
def=
{
if fd+1t 6?FG : Jfdt =0K
if fd+1t ?FG, fdt ? F1 : P??F-ILR,d(fdt | qdt?1 qd?1t?1 )
if fd+1t ?FG, fdt ? FG : P??F-CLR,d(fdt | qdt?1 qd?1t?1 )
(13)
with edge cases q0t and fD+1t defined as appropri-
ate constants. The first case is just store-element
maintenance, in which the variable is not on the
?frontier? and therefore inactive.
Examining ?F-ILR,d and ?F-CLR,d, we see that
the produced fdt variables are also used in the ?if?
statement. These models can be thought of as
picking out a fdt first, finding the matching case,
then applying the probability models that matches.
These models are actually two parts of the same
model when learned from trees.
Probabilities in the shift step are also split into
cases based on the reduce variables. More main-
tenance operations (first case) accompany transi-
tions producing new awaited constituents (second
case below) and expansions producing new active
constituents (third and fourth case):
P?Q(q
d
t | fd+1t fdt qdt?1qd?1t )
def=
?
?
?
?
?
if fd+1t 6?FG : Jqdt = qdt?1K
if fd+1t ?FG, fdt ? F0 : P??Q-ILT,d(qdt | fd+1t qdt?1 qd?1t )
if fd+1t ?FG, fdt ? F1 : P??Q-ILE,d(qdt | fdt qdt?1 qd?1t )
if fd+1t ?FG, fdt ?FG : P??Q-CLE,d(qdt | qd?1t )
(14)
1193
FACTOR DESCRIPTION EXPECTED
Word order in
narrative
For each story, words were indexed. Subjects would tend to read faster later in a story. negative
slope
Reciprocal
length
Log of the reciprocal of the number of letters in each word. A decrease in the reciprocal
(increase in length) might mean longer reading times.
positive
slope
Unigram
frequency
A log-transformed empirical count of word occurrences in the Brown Corpus section of
the Penn Treebank. Higher frequency should indicate shorter reading times.
negative
slope
Bigram
probability
A log-transformed empirical count of two-successive-word occurrences, with Good-
Turing smoothing on words occuring less than 10 times.
negative
slope
Embedding
difference
Amount of change in HHMM weighted-average embedding depth. Hypothesized to in-
crease with larger working memory requirements, which predict longer reading times.
positive
slope
Entropy
reduction
Amount of decrease in the HHMM?s uncertainty about the sentence. Larger reductions
in uncertainty are hypothesized to take longer.
positive
slope
Surprisal ?Surprise value? of a word in the HHMM parser; models were trained on the Wall Street
Journal, sections 02?21. More surprising words may take longer to read.
positive
slope
Table 1: A list of factors hypothesized to contribute to reading times. All data was mean-centered.
A final note: the notation P??(? | ?) has been used
to indicate probability models that are empirical,
trained directly from frequency counts of right-
corner transformed trees in a large corpus. Alter-
natively, a standard PCFG could be trained on a
corpus (or hand-specified), and then the grammar
itself can be right-corner transformed (Schuler,
2009).
Taken together, Equations 11?14 define the
probabilistic structure of the HHMM for parsing
right-corner trees.
2.5 Embedding difference in the HHMM
It should be clear from Figure 1 that at any time
step while parsing depth-bounded right-corner
trees, the candidate hidden state qt will have a
?frontier? depth d(qt). At time t, the beam of
possible hidden states qt stores the syntactic state
(and a backpointer) along with its probability,
P(o1..t q1..t). The average embedding depth at a
time step is then
?EMB(o1..t) =
?
qt?Bt
d(qt) ?
P(o1..t q1..t)
?
q?t?Bt P(o1..t q
?
1..t)
(15)
where we have directly used the beam notation.
The embedding difference metric is:
EmbDiff(o1..t) = ?EMB(o1..t) ? ?EMB(o1..t?1)
(16)
There is a strong computational correspondence
between this definition of embedding difference
and the previous definition of surprisal. To see
this, we rewrite Equations 1 and 3:
Pre(o1..t)=
?
qt?Bt
P(o1..t q1..t) (1?)
Surprisal(t) = log2 Pre(o1..t?1) ? log2 Pre(o1..t)
(3?)
Both surprisal and embedding difference include
summations over the elements of the beam, and
are calculated as a difference between previous
and current beam states.
Most differences between these metrics are rel-
atively inconsequential. For example, the dif-
ference in order of subtraction only assures that
a positive correlation with reading times is ex-
pected. Also, the presence of a logarithm is rel-
atively minor. Embedding difference weighs the
probabilities with center-embedding depths and
then normalizes the values; since the measure is
a weighted average of embedding depths rather
than a probability distribution, ?EMB is not always
less than 1 and the correspondence with Kullback-
Leibler divergence (Levy, 2008) does not hold, so
it does not make sense to take the logs.
Therefore, the inclusion of the embedding
depth, d(qt), is the only significant difference
between the two metrics. The result is a met-
ric that, despite numerical correspondence to sur-
prisal, models the HHMM?s hypotheses about
memory cost.
3 Evaluation
Surprisal, entropy reduction, and embedding dif-
ference from the HHMM parser were evaluated
against a full array of factors (Table 1) on a cor-
pus of word-by-word reading times using a linear
mixed-effects model.
1194
The corpus of reading times for 23 native En-
glish speakers was collected on a set of four nar-
ratives (Bachrach et al, 2009), each composed of
sentences that were syntactically complex but con-
structed to appear relatively natural. Using Linger
2.88, words appeared one-by-one on the screen,
and required a button-press in order to advance;
they were displayed in lines with 11.5 words on
average.
Following Roark et al?s (2009) work on the
same corpus, reading times above 1500 ms (for
diverted attention) or below 150 ms (for button
presses planned before the word appeared) were
discarded. In addition, the first and last word of
each line on the screen were removed; this left
2926 words out of 3540 words in the corpus.
For some tests, a division between open- and
closed-class words was made, with 1450 and 1476
words, respectively. Closed-class words (e.g., de-
terminers or auxiliary verbs) usually play some
kind of syntactic function in a sentence; our evalu-
ations used Roark et al?s list of stop words. Open
class words (e.g., nouns and other verbs) more
commonly include new words. Thus, one may ex-
pect reading times to differ for these two types of
words.
Linear mixed-effect regression analysis was
used on this data; this entails a set of fixed effects
and another of random effects. Reading times y
were modeled as a linear combination of factors
x, listed in Table 1 (fixed effects); some random
variation in the corpus might also be explained by
groupings according to subject i, word j, or sen-
tence k (random effects).
yijk = ?0 +
m
X
?=1
??xijk? + bi + bj + bk + ? (17)
This equation is solved for each of m fixed-
effect coefficients ? with a measure of confidence
(t-value = ??/SE(??), where SE is the standard er-
ror). ?0 is the standard intercept to be estimated
along with the rest of the coefficients, to adjust for
affine relationships between the dependent and in-
dependent variables. We report factors as statisti-
cally significant contributors to reading time if the
absolute value of the t-value is greater than 2.
Two more types of comparisons will be made to
see the significance of factors. First, a model of
data with the full list of factors can be compared
to a model with a subset of those factors. This is
done with a likelihood ratio test, producing (for
mixed-effects models) a ?21 value and correspond-
ing probability that the smaller model could have
produced the same estimates as the larger model.
A lower probability indicates that the additional
factors in the larger model are significant.
Second, models with different fixed effects can
be compared to each other through various infor-
mation criteria; these trade off between having
a more explanatory model vs. a simpler model,
and can be calculated on any model. Here, we
use Akaike?s Information Criterion (AIC), where
lower values indicate better models.
All these statistics were calculated in R, using
the lme4 package (Bates et al, 2008).
4 Results
Using the full list of factors in Table 1, fixed-effect
coefficients were estimated in Table 2. Fitting the
best model by AIC would actually prune away
some of the factors as relatively insignificant, but
these smaller models largely accord with the sig-
nificance values in the table and are therefore not
presented.
The first data column shows the regression on
all data; the second and third columns divide the
data into open and closed classes, because an eval-
uation (not reported in detail here) showed statis-
tically significant interactions between word class
and 3 of the predictors. Additionally, this facil-
itates comparison with Roark et al (2009), who
make the same division.
Out of the non-parser-based metrics, word order
and bigram probability are statistically significant
regardless of the data subset; though reciprocal
length and unigram frequency do not reach signif-
icance here, likelihood ratio tests (not shown) con-
firm that they contribute to the model as a whole.
It can be seen that nearly all the slopes have been
estimated with signs as expected, with the excep-
tion of reciprocal length (which is not statistically
significant).
Most notably, HHMM surprisal is seen here to
be a standout predictive measure for reading times
regardless of word class. If the HHMM parser is
a good psycholinguistic model, we would expect
it to at least produce a viable surprisal metric, and
Table 2 attests that this is indeed the case. Though
it seems to be less predictive of open classes, a
surprisal-only model has the best AIC (-7804) out
of any open-class model. Considering the AIC
on the full data, the worst model with surprisal
1195
FULL DATA OPEN CLASS CLOSED CLASS
Coefficient Std. Err. t-value Coefficient Std. Err. t-value Coefficient Std. Err. t-value
(Intcpt) -9.340?10?3 5.347?10?2 -0.175 -1.237?10?2 5.217?10?2 -0.237 -6.295?10?2 7.930?10?2 -0.794
order -3.746?10?5 7.808?10?6 -4.797? -3.697?10?5 8.002?10?6 -4.621? -3.748?10?5 8.854?10?6 -4.232?
rlength -2.002?10?2 1.635?10?2 -1.225 9.849?10?3 1.779?10?2 0.554 -2.839?10?2 3.283?10?2 -0.865
unigrm -8.090?10?2 3.690?10?1 -0.219 -1.047?10?1 2.681?10?1 -0.391 -3.847?10+0 5.976?10+0 -0.644
bigrm -2.074?10+0 8.132?10?1 -2.551? -2.615?10+0 8.050?10?1 -3.248? -5.052?10+1 1.910?10+1 -2.645?
embdiff 9.390?10?3 3.268?10?3 2.873? 2.432?10?3 4.512?10?3 0.539 1.598?10?2 5.185?10?3 3.082?
etrpyrd 2.753?10?2 6.792?10?3 4.052? 6.634?10?4 1.048?10?2 0.063 4.938?10?2 1.017?10?2 4.857?
srprsl 3.950?10?3 3.452?10?4 11.442? 2.892?10?3 4.601?10?4 6.285? 5.201?10?3 5.601?10?4 9.286?
Table 2: Results of linear mixed-effect modeling. Significance (indicated by ?) is reported at p < 0.05.
(Intr) order rlngth ungrm bigrm emdiff entrpy
order .000
rlength -.006 -.003
unigrm .049 .000 -.479
bigrm .001 .005 -.006 -.073
emdiff .000 .009 -.049 -.089 .095
etrpyrd .000 .003 .016 -.014 .020 -.010
srprsl .000 -.008 -.033 -.079 .107 .362 .171
Table 3: Correlations in the full model.
(AIC=-10589) outperformed the best model with-
out it (AIC=-10478), indicating that the HHMM
surprisal is well worth including in the model re-
gardless of the presence of other significant fac-
tors.
HHMM entropy reduction predicts reading
times on the full dataset and on closed-class
words. However, its effect on open-class words is
insignificant; if we compare the model of column
2 against one without entropy reduction, a likeli-
hood ratio test gives ?21 = 0.0022, p = 0.9623
(the smaller model could easily generate the same
data).
The HHMM?s average embedding difference
is also significant except in the case of open-
class words ? removing embedding difference on
open-class data yields ?21 = 0.2739, p = 0.6007.
But what is remarkable is that there is any signifi-
cance for this metric at all. Embedding difference
and surprisal were relatively correlated compared
to other predictors (see Table 3), which is expected
because embedding difference is calculated like
a weighted version of surprisal. Despite this, it
makes an independent contribution to the full-data
and closed-class models. Thus, we can conclude
that the average embedding depth component af-
fects reading times ? i.e., the HHMM?s notion of
working memory behaves as we would expect hu-
man working memory to behave.
5 Discussion
As with previous work on large-scale parser-
derived complexity metrics, the linear mixed-
effect models suggest that sentence-level factors
are effective predictors for reading difficulty ? in
these evaluations, better than commonly-used lex-
ical and near-neighbor predictors (Pollatsek et al,
2006; Engbert et al, 2005). The fact that HHMM
surprisal outperforms even n-gram metrics points
to the importance of including a notion of sentence
structure. This is particularly true when the sen-
tence structure is defined in a language model that
is psycholinguistically plausible (here, bounded-
memory right-corner form).
This accords with an understated result of
Boston et al?s eye-tracking study (2008a): a
richer language model predicts eye movements
during reading better than an oversimplified one.
The comparison there is between phrase struc-
ture surprisal (based on Hale?s (2001) calculation
from an Earley parser), and dependency grammar
surprisal (based on Nivre?s (2007) dependency
parser). Frank (2009) similarly reports improve-
ments in the reading-time predictiveness of unlexi-
calized surprisal when using a language model that
is more plausible than PCFGs.
The difference in predictivity due to word class
is difficult to explain. One theory may be that
closed-class words are less susceptible to random
effects because there is a finite set of them for
any language, making them overall easier to pre-
dict via parser-derived metrics. Or, we could note
that since closed-class words often serve grammat-
ical functions in addition to their lexical content,
they contribute more information to parser-derived
measures than open-class words. Previous work
with complexity metrics on this corpus (Roark et
al., 2009) suggests that these explanations only ac-
count for part of the word-class variation in the
performance of predictors.
1196
Further comparsion to Roark et al will show
other differences, such as the lesser role of word
length and unigram frequency, lower overall cor-
relations between factors, and the greater predic-
tivity of their entropy metric. In addition, their
metrics are different from ours in that they are de-
signed to tease apart lexical and syntactic contri-
butions to reading difficulty. Their notion of en-
tropy, in particular, estimates Hale?s definition of
entropy on whole derivations (2006) by isolating
the predictive entropy; they then proceed to define
separate lexical and syntactic predictive entropies.
Drawing more directly from Hale, our definition
is a whole-derivation metric based on the condi-
tional entropy of the words, given the root. (The
root constituent, though unwritten in our defini-
tions, is always included in the HHMM start state,
q0.)
More generally, the parser used in these evalu-
ations differs from other reported parsers in that
it is not lexicalized. One might expect for this
to be a weakness, allowing distributions of prob-
abilities at each time step in places not licensed
by the observed words, and therefore giving poor
probability-based complexity metrics. However,
we see that this language model performs well
despite its lack of lexicalization. This indicates
that lexicalization is not a requisite part of syntac-
tic parser performance with respect to predicting
linguistic complexity, corroborating the evidence
of Demberg and Keller?s (2008) ?unlexicalized?
(POS-generating, not word-generating) parser.
Another difference is that previous parsers have
produced useful complexity metrics without main-
taining arc-eager/arc-standard ambiguity. Results
show that including this ambiguity in the HHMM
at least does not invalidate (and may in fact im-
prove) surprisal or entropy reduction as reading-
time predictors.
6 Conclusion
The task at hand was to determine whether the
HHMM could consistently be considered a plau-
sible psycholinguistic model, producing viable
complexity metrics while maintaining other char-
acteristics such as bounded memory usage. The
linear mixed-effects models on reading times val-
idate this claim. The HHMM can straightfor-
wardly produce highly-predictive, standard com-
plexity metrics (surprisal and entropy reduction).
HHMM surprisal performs very well in predicting
reading times regardless of word class. Our for-
mulation of entropy reduction is also significant
except in open-class words.
The new metric, embedding difference, uses the
average center-embedding depth of the HHMM
to model syntactic-processing memory cost. This
metric can only be calculated on parsers with an
explicit representation for short-term memory el-
ements like the right-corner HHMM parser. Re-
sults show that embedding difference does predict
reading times except in open-class words, yielding
a significant contribution independent of surprisal
despite the fact that its definition is similar to that
of surprisal.
Acknowledgments
Thanks to Brian Roark for help on the reading
times corpus, Tim Miller for the formulation of
entropy reduction, Mark Holland for statistical in-
sight, and the anonymous reviewers for their input.
This research was supported by National Science
Foundation CAREER/PECASE award 0447685.
The views expressed are not necessarily endorsed
by the sponsors.
References
Steven P. Abney and Mark Johnson. 1991. Memory
requirements and local ambiguities of parsing strate-
gies. J. Psycholinguistic Research, 20(3):233?250.
Asaf Bachrach, Brian Roark, Alex Marantz, Susan
Whitfield-Gabrieli, Carlos Cardenas, and John D.E.
Gabrieli. 2009. Incremental prediction in naturalis-
tic language processing: An fMRI study.
Douglas Bates, Martin Maechler, and Bin Dai. 2008.
lme4: Linear mixed-effects models using S4 classes.
R package version 0.999375-31.
Marisa Ferrara Boston, John T. Hale, Reinhold Kliegl,
U. Patil, and Shravan Vasishth. 2008a. Parsing costs
as predictors of reading difficulty: An evaluation us-
ing the Potsdam Sentence Corpus. Journal of Eye
Movement Research, 2(1):1?12.
Marisa Ferrara Boston, John T. Hale, Reinhold Kliegl,
and Shravan Vasishth. 2008b. Surprising parser ac-
tions and reading difficulty. In Proceedings of ACL-
08: HLT, Short Papers, pages 5?8, Columbus, Ohio,
June. Association for Computational Linguistics.
Thorsten Brants and Matthew Crocker. 2000. Prob-
abilistic parsing and psychological plausibility. In
Proceedings of COLING ?00, pages 111?118.
1197
Evan Chen, Edward Gibson, and Florian Wolf. 2005.
Online syntactic storage costs in sentence com-
prehension. Journal of Memory and Language,
52(1):144?169.
Noam Chomsky and George A. Miller. 1963. Intro-
duction to the formal analysis of natural languages.
In Handbook of Mathematical Psychology, pages
269?321. Wiley.
Nelson Cowan. 2001. The magical number 4 in short-
term memory: A reconsideration of mental storage
capacity. Behavioral and Brain Sciences, 24:87?
185.
Matthew Crocker and Thorsten Brants. 2000. Wide-
coverage probabilistic sentence processing. Journal
of Psycholinguistic Research, 29(6):647?669.
Delphine Dahan and M. Gareth Gaskell. 2007. The
temporal dynamics of ambiguity resolution: Evi-
dence from spoken-word recognition. Journal of
Memory and Language, 57(4):483?501.
Vera Demberg and Frank Keller. 2008. Data from eye-
tracking corpora as evidence for theories of syntactic
processing complexity. Cognition, 109(2):193?210.
Ralf Engbert, Antje Nuthmann, Eike M. Richter, and
Reinhold Kliegl. 2005. SWIFT: A dynamical model
of saccade generation during reading. Psychological
Review, 112:777?813.
Shai Fine, Yoram Singer, and Naftali Tishby. 1998.
The hierarchical hidden markov model: Analysis
and applications. Machine Learning, 32(1):41?62.
Stefan L. Frank. 2009. Surprisal-based comparison be-
tween a symbolic and a connectionist model of sen-
tence processing. In Proc. Annual Meeting of the
Cognitive Science Society, pages 1139?1144.
Edward Gibson. 1998. Linguistic complexity: Local-
ity of syntactic dependencies. Cognition, 68(1):1?
76.
Edward Gibson. 2000. The dependency locality the-
ory: A distance-based theory of linguistic complex-
ity. In Image, language, brain: Papers from the first
mind articulation project symposium, pages 95?126.
John Hale. 2001. A probabilistic earley parser as a
psycholinguistic model. In Proceedings of the Sec-
ond Meeting of the North American Chapter of the
Association for Computational Linguistics, pages
159?166, Pittsburgh, PA.
John Hale. 2003. Grammar, Uncertainty and Sentence
Processing. Ph.D. thesis, Cognitive Science, The
Johns Hopkins University.
John Hale. 2006. Uncertainty about the rest of the
sentence. Cognitive Science, 30(4):609?642.
Roger Levy. 2008. Expectation-based syntactic com-
prehension. Cognition, 106(3):1126?1177.
Scott A. McDonald and Richard C. Shillcock. 2003.
Low-level predictive inference in reading: The influ-
ence of transitional probabilities on eye movements.
Vision Research, 43(16):1735?1751.
George Miller and Noam Chomsky. 1963. Finitary
models of language users. In R. Luce, R. Bush,
and E. Galanter, editors, Handbook of Mathematical
Psychology, volume 2, pages 419?491. John Wiley.
Kevin P. Murphy and Mark A. Paskin. 2001. Lin-
ear time inference in hierarchical HMMs. In Proc.
NIPS, pages 833?840, Vancouver, BC, Canada.
Joakim Nivre. 2007. Inductive dependency parsing.
Computational Linguistics, 33(2).
Alexander Pollatsek, Erik D. Reichle, and Keith
Rayner. 2006. Tests of the EZ Reader model:
Exploring the interface between cognition and eye-
movement control. Cognitive Psychology, 52(1):1?
56.
Lawrence R. Rabiner. 1990. A tutorial on hid-
den Markov models and selected applications in
speech recognition. Readings in speech recognition,
53(3):267?296.
Brian Roark, Asaf Bachrach, Carlos Cardenas, and
Christophe Pallier. 2009. Deriving lexical and
syntactic expectation-based measures for psycholin-
guistic modeling via incremental top-down parsing.
Proceedings of the 2009 Conference on Empirical
Methods in Natural Langauge Processing, pages
324?333.
Brian Roark. 2001. Probabilistic top-down parsing
and language modeling. Computational Linguistics,
27(2):249?276.
William Schuler, Samir AbdelRahman, Tim
Miller, and Lane Schwartz. 2008. Toward a
psycholinguistically-motivated model of language.
In Proceedings of COLING, pages 785?792,
Manchester, UK, August.
William Schuler, Samir AbdelRahman, TimMiller, and
Lane Schwartz. 2010. Broad-coverage incremen-
tal parsing using human-like memory constraints.
Computational Linguistics, 36(1).
William Schuler. 2009. Parsing with a bounded
stack using a model-based right-corner transform.
In Proceedings of the North American Association
for Computational Linguistics (NAACL ?09), pages
344?352, Boulder, Colorado.
Michael K. Tanenhaus, Michael J. Spivey-Knowlton,
Kathy M. Eberhard, and Julie E. Sedivy. 1995. In-
tegration of visual and linguistic information in spo-
ken language comprehension. Science, 268:1632?
1634.
1198
