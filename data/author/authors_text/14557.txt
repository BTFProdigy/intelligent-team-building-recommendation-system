Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 1173?1181,
Beijing, August 2010
 A Character-Based Joint Model for Chinese Word Segmentation
Kun Wang and Chengqing Zong 
National Laboratory of Pattern Recognition 
Institute of Automation, Chinese Academy of Science
{kunwang,cqzong}@nlpr.ia.ac.cn 
Keh-Yih Su 
Behavior Design Corporation
 
Kysu@bdc.com.tw 
 
Abstract 
The character-based tagging approach 
is a dominant technique for Chinese 
word segmentation, and both discrimi-
native and generative models can be 
adopted in that framework. However, 
generative and discriminative charac-
ter-based approaches are significantly 
different and complement each other. 
A simple joint model combining the 
character-based generative model and 
the discriminative one is thus proposed 
in this paper to take advantage of both 
approaches. Experiments on the Sec-
ond SIGHAN Bakeoff show that this 
joint approach achieves 21% relative 
error reduction over the discriminative 
model and 14% over the generative one. 
In addition, closed tests also show that 
the proposed joint model outperforms 
all the existing approaches reported in 
the literature and achieves the best F-
score in four out of five corpora. 
1 Introduction 
Chinese word segmentation (CWS) plays an 
important role in most Chinese NLP applica-
tions such as machine translation, information 
retrieval and question answering. Many statis-
tical methods for CWS have been proposed in 
the last two decades, which can be classified as 
either word-based or character-based. The 
word-based approach regards the word as the 
basic unit, and the desired segmentation result 
is the best word sequence found by the search 
process. On the other hand, the character-based 
approach treats the word segmentation task as 
a character tagging problem. The final segmen-
tation result is thus indirectly generated ac-
cording to the tag assigned to each associated 
character. Since the vocabulary size of possible 
character-tag-pairs is limited, the character-
based models can tolerate out-of-vocabulary 
(OOV) words and have become the dominant 
technique for CWS in recent years. 
On the other hand, statistical approaches can 
also be classified as either adopting a genera-
tive model or adopting a discriminative model. 
The generative model learns the joint probabil-
ity of the given input and its associated label 
sequence, while the discriminative model 
learns the posterior probability directly. Gen-
erative models often do not perform well be-
cause they make strong independence assump-
tions between features and labels. However, 
(Toutanova, 2006) shows that generative mod-
els can also achieve very similar or better per-
formance than the corresponding discrimina-
tive models if they have a structure that avoids 
unrealistic independence assumptions.  
In terms of the above dimensions, methods 
for CWS can be classified as:  
1) The word-based generative model (Gao et 
al., 2003; Zhang et al, 2003), which is a well-
known approach and has been used in many 
successful applications;  
2) The word-based discriminative model 
(Zhang and Clark, 2007), which generates 
word candidates with both word and character 
features and is the only word-based model that 
adopts the discriminative approach? 
3) The character-based discriminative model 
(Xue, 2003; Peng et al, 2004; Tseng et al, 
2005; Jiang et al, 2008), which has become 
the dominant method as it is robust on OOV 
words and is capable of handling a range of 
different features, and it has been adopted in 
many previous works;  
1173
4) The character-based generative model 
(Wang et al, 2009), which adopts a character-
tag-pair-based n-gram model and achieves 
comparable results with the popular character-
based discriminative model. 
In general, character-based models are much 
more robust on OOV words than word-based 
approaches do, as the vocabulary size of char-
acters is a closed set (versus the open set of 
that of words). Furthermore, among those 
character-based approaches, the generative 
model and the discriminative one complement 
each other in handling in-vocabulary (IV) 
words and OOV words. Therefore, a character-
based joint model is proposed to combine them. 
This proposed joint approach has achieved 
good balance between IV word recognition 
and OOV word identification. The experiments 
of closed tests on the second SIGHAN Bakeoff 
(Emerson, 2005) show that the joint model 
significantly outperforms the baseline models 
of both generative and discriminative ap-
proaches. Moreover, statistical significance 
tests also show that the joint model is signifi-
cantly better than all those state-of-the-art sys-
tems reported in the literature and achieves the 
best F-score in four of the five corpora tested. 
2 Character-Based Models for CWS 
The goal of CWS is to find the corresponding 
word sequence for a given character sequence. 
Character-based model is to find out the corre-
sponding tags for given character sequence. 
2.1 Character-Based Discriminative Model 
The character-based discriminative model 
(Xue, 2003) treats segmentation as a tagging 
problem, which assigns a corresponding tag to 
each character. The model is formulated as: 
1
1 1 1 1 2
1 1
( ) ( , ) (
n n
n n k n k
k k
k k
P t c P t t c P t c? ?
= =
= ?? ? 2 )k +        (1) 
Where tk is a member of {Begin, Middle, End, 
Single} (abbreviated as B, M, E and S from 
now on) to indicate the corresponding position 
of character ck in its associated word. For ex-
ample, the word ???? (Beijing City)? will 
be assigned with the corresponding tags as: ??
/B (North) ?/M (Capital) ?/E (City)?.  
Since this tagging approach treats characters 
as basic units, the vocabulary size of those 
possible character-tag-pairs is limited. There-
fore, this method is robust to OOV words and 
could possess a high recall of OOV words 
(ROOV). Although the dependency between ad-
jacent tags/labels can be addressed, the de-
pendency between adjacent characters within a 
word cannot be directly modeled under this 
framework. Lower recall of IV words (RIV) is 
thus usually accompanied (Wang et al, 2009).  
In this work, the character-based discrimina-
tive model is implemented by adopting the fea-
ture templates given by (Ng and Low, 2004), 
but excluding those ones that are forbidden by 
the closed test regulation of SIGHAN (e.g., 
Pu(C0): whether C0 is a punctuation). Those 
feature templates adopted are listed below: 
1
1 1
( ) ( 2, 1,0,1, 2);
( ) ( 2, 1,0,1);
( )
n
n n
a C n
b C C n
c C C
+
?
= ? ?
= ? ?  
For example, when we consider the third 
character ??? in the sequence ???????, 
template (a) results in the features as following: 
C-2=?, C-1=?, C0=?, C1=?, C2=?, and tem-
plate (b) generates the features as: C-2C-1=??, 
C-1C0=??, C0C1=??, C1C2=??, and tem-
plate (c) gives the feature C-1C1=??. 
2.2 Character-Based Generative Model 
To incorporate the dependency between adja-
cent characters in the character-based approach, 
(Wang et al, 2009) proposes a character-based 
generative model. In this approach, word wi is 
first replaced with its corresponding sequence 
of [character, tag] (denoted as [c, t]), where tag 
is the same as that adopted in the above char-
acter-based discriminative model. With this 
representation, this model can be expressed as:  
 
1 1 1 1
1 1 1 1
( ) ([ , ] )
( [ , ] ) ([ , ] ) ( )
m n n n
n n n n
P w c P c t c
P c c t P c t P c
?
= ?                    (2) 
Since 1 1( [ , ] ) 1
n nP c c t ?  and  is the same for 
various candidates, only should be 
considered. It can be further simplified with 
Markov Chain assumption as: 
1( )
nP c
([ ,P c 1] )
nt
 11
1
([ , ] ) ([ , ] [ , ] ).
n
n
i i k
i
P c t P c t c t ??
=
?? i                     (3) 
Compared with the character-based dis-
criminative model, this generative model keeps 
the capability to handle OOV words because it 
also regards the character as basic unit. In ad-
dition, the dependency between adjacent 
1174
? Gold and Discriminative Tag: M Generative Trigram Tag: E 
Tag probability:  B/0.0333 E/0.2236 M/0.7401 S/0.0030 
Feature 
Tag C-2 C-1 C0 C1 C2 C-2C-1 C-1C0 C0C1 C1C2 C-1C1
B -1.4375 0.1572 0.0800 0.2282 0.7709 0.2741 0.0000 0.0000 -0.6718 0.0000
E 1.3558 0.1910 0.7229 -1.2696 -0.5970 0.0049 0.0921 0.0000 0.8049 0.0000
M 1.1071 -0.5527 -0.3174 2.9422 0.4636 -0.1708 0.0000 0.0000 -0.9700 0.0000
S -1.0254 0.2046 -0.4856 -1.9008 -0.6375 0.0000 0.0000 0.0000 0.8368 0.0000
? Gold and Discriminative Tag: E Generative Trigram Tag: S 
Tag probability:  B/0.0009 E/0.8138 M/0.0012 S/0.1841 
Feature 
Tag C-2 C-1 C0 C1 C2 C-2C-1 C-1C0 C0C1 C1C2 C-1C1
B 0.3586 0.4175 0.0000 -0.7207 0.4626 0.0085 0.0000 0.0000 0.0000 0.0000
E 0.3666 0.0687 4.5381 2.8300 -0.0846 0.0000 0.0000 -1.0279 0.6127 0.0000
M -0.5657 -0.4330 1.8847 0.0000 -0.0918 0.0000 0.0000 0.0000 0.0000 0.0000
S -0.1595 -0.0532 2.7360 1.8223 -0.2862 -0.0024 0.0000 1.0494 0.7113 0.0000
Table 1: The corresponding lambda weight of features for ????? in the sentence ?[?] [?] [?] [???] 
[?] [?] [?] [?]?. In the Feature column and Tag row, the value is the corresponding lambda weight for 
the feature and tag under ME framework. The meanings of those features are explained in Section 2.1. 
 
characters is now directly modeled. This will 
give sharper preference when the history of 
assignment is given. Therefore, this approach 
not only holds robust IV performance but also 
achieves comparable results with the discrimi-
native model. However, the OOV performance 
of this approach is still lower than that of the 
discriminative model (see in Table 5), which 
would be discussed in the next section. 
3 Problems with the Character-Based 
Generative Model 
The character-based generative model can 
handle the dependency between adjacent char-
acters and thus performs well on IV words. 
However, this generative trigram model is de-
rived under the second order Markov Chain 
assumption. Future character context (i.e., C1 
and C2) is thus not utilized in the model when 
the tag of the current character (i.e., t0) is de-
termined. Nevertheless, the future context 
would help to select the correct tag when the 
associated trigram has not been observed in the 
training-set, which is just the case for those 
OOV words. In contrast, the discriminative 
one could get help from the future context in 
this case. The example given in the next para-
graph clearly shows the above situation. 
At the sentence ??(that) ?(place) ?(of) ?
??(street sleeper) ?(only) ?(have) ?(some) 
?(person) (There are only some street sleepers 
in that place)? in the CITYU corpus, ??/B?
/M?/E(street sleeper)? is observed to be an 
OOV word, while ?? /B? /E(sleep on the 
street)? is an IV word, where the associated tag 
of each character is given after the slash sym-
bol. The character-based generative model 
wrongly splits ????? into two words ??/B
?/E? and ??/S (person)?, as the associated 
trigram for ????? is not seen in the training 
set. However, the discriminative model gives 
the correct result for ??/M? and the dominant 
features come from its future context ??? and 
???. Similarly, the future context ??? helps 
to give the correct tag to ??/E?. Table 1 gives 
the corresponding lambda feature weights (un-
der the Maximum Entropy (ME) (Ratnaparkhi, 
1998) framework) for ????? in the dis-
criminative model. It shows that in the column 
of ?C1? below ???, the lambda value associ-
ated with the correct tag ?M? is 2.9422, which 
is the highest value in that column and is far 
greater than that of the wrong tag ?E? (i.e., -
1.2696) assigned by the generative model. 
Which indicates that the future feature ?C1? is 
the most useful feature for tagging ???. 
The above example shows the character-
based generative model fails to handle some 
OOV words such as ????? because this ap-
proach cannot utilize future context when it is 
indeed required. However, the future context 
for the generative model scanning from left to 
right is just its past context when it scans from 
right to left. It is thus expected that this kind of 
1175
errors will be fixed if we let the model scans 
from both directions, and then combine their 
results. Unfortunately, it is observed that these 
two scanning modes share over 90% of their 
errors. For example, in CITYU corpus, the 
left-to-right scan generates 1,958 wrong words 
and the right-to-left scan results 1,947 ones, 
while 1,795 of them are the same. Similar be-
havior can also be observed on other corpora. 
To find out what are the problems, 10 errors 
that are similar to ????? are selected to ex-
amine. Among those errors, only one of them 
is fixed, and ????? still cannot be correctly 
segmented. Having analyzed the scores of the 
model scanning from both directions, we found 
that the original scores (from left-to-right scan) 
at the stages ??? and ??? indeed get better if 
the model scans from right-to-left. However, 
the score at the stage ??? deteriorates because 
the useful feature ??? (a past non-adjacent 
character for ??? when scans form right-to-
left) still cannot be utilized when the past con-
text ???? as a whole is unseen, when the re-
lated probabilities are estimated via modified 
Kneser-Ney smoothing (Chen and Goodman, 
1998) technique. 
Two scanning modes seem not complement-
ing each other, which is out of our original ex-
pectation. However, we found that the charac-
ter-based generative model and the discrimina-
tive one complement each other much more 
than the two scanning modes do. It is observed 
that these two approaches share less than 50% 
of their errors. For example, in CITYU corpus, 
the generative approach generates 1,958 wrong 
words and the discriminative one results 2,338 
ones, while only 835 of them are the same. 
The statistics of the remaining errors re-
sulted from the generative model and the dis-
criminative model is shown in Table 2. As 
shown in the table, it can be seen that the gen-
erative model and the discriminative model 
complement each other on handling IV words 
and OOV words (In the ?IV Errors? column, 
the number of ?G+D-? is much more than the 
?G-D+?, while the behavior is reversed in the 
?OOV Errors? column). 
4 Proposed Joint Model 
Since the performance of both IV words and 
OOV words are important for real applications, 
IV Errors OOV Errors 
G+D- G-D+ G-D- G+D- G-D+ G-D-
12,027 4,723 7,481 2,384 6,139 3,975
Table 2: Statistics for remaining errors of the char-
acter-based generative model and the discriminative 
one on the second SIGHAN Bakeoff (?G+D-? in 
the ?IV Errors? column means that the generative 
model segments the IV words correctly but the dis-
criminative one gives wrong results. The meanings 
of other abbreviations are similar with this one.). 
we need to combine the strength from both 
models. Among various combining methods, 
log-linear interpolation combination is a sim-
ple but effective one (Bishop, 2006). Therefore, 
the following character-based joint model is 
proposed, and a parameter ?  is used to weight 
the generative model in a cross-validation set. 
 
1
2
2
2
( ) log( ([ , ] [ , ] ))
(1 ) log( ( ))
k
k k
k
k k
Score t P c t c t
P t c
?
?
?
?
+
?
= ?
+ ? ?
k
           (4) 
Where tk indicates the corresponding position 
of character ck, and (0.0 1.0)? ?? ?  is the 
weight for the generative model. Score(tk) will 
be used during searching the best sequence. It 
can be seen that these two models are inte-
grated naturally as both are character-based. 
Generally speaking, if the ?G(or D)+? has a 
strong preference on the desired candidate, but 
the ?D(or G)-? has a weak preference on its 
top-1 incorrect candidate, then this combining 
method would correct most ?G+D- (also  G-
D+)? errors. On the other hand, the advantage 
of combining two models would vanish if the 
?G(or D)+? has a weak preference while the 
?D(or G)-? has a strong preference over their 
top-1 candidates. In our observation, these two 
models meet this requirement quite well. 
5 Weigh Various Features Differently 
For a given observation, intuitively each 
feature should be trained only once under the 
ME framework and its associated weight will 
be automatically learned from the training cor-
pus. However, when we repeat the work of 
(Jiang et al, 2008), which reports to achieve 
the state-of-art performance in the data-sets 
that we adopt, it has been found that some fea-
tures (e.g., C0) are unnoticeably trained several 
times in their model (which are implicitly gen-
erated from different feature templates used in 
the paper). For example, the feature C0 actually 
1176
Corpus Abbrev. Encoding Training Size(Words/Type)
Test Size 
(Words/Type) OOV Rate
Academia Sinica (Taipei) AS Unicode/Big5 5.45M/141K 122K/19K 0.046 
City University of Hong Kong CITYU Unicode/Big5 1.46M/69K 41K/9K 0.074 
Microsoft Research (Beijing) MSR Unicode/CP936 2.37M/88K 107K/13K 0.026 
PKU(ucvt.) Unicode/CP936 1.1M/55K 104K/13K 0.058 Peking University 
PKU(cvt.) Unicode/CP936 1.1M/55K 104K/13K 0.035 
Table 3: Corpus statistics for the second SIGHAN Bakeoff 
 
appears twice, which is generated from two 
different templates Cn (with n=0, generates C0) 
and [C0Cn] (used in (Jiang et al, 2008), with 
n=0, generates [C0C0]). The meanings of fea-
tures are illustrated in Section 2.1. Those re-
petitive features also include [C-1C0] and 
[C0C1], which implicitly appear thrice. And it 
is surprising to discover that its better perform-
ance is mainly due to this implicit feature repe-
tition but the authors do not point out this fact. 
As all the features adopted in (Jiang et al, 
2008) possess binary values, if a binary feature 
is repeated n times, then it should behave like a 
real-valued feature with its value to be ?n?, at 
least in principle. Inspired by the above dis-
covery, accordingly, we convert all the binary-
value features into their corresponding real-
valued features. After having transformed bi-
nary features into their corresponding real-
valued ones, the original discriminative model 
is re-trained under the ME framework. 
This new implementation, which would be 
named as the character-based discriminative-
plus model, just weights various features dif-
ferently before conducting ME training. Af-
terwards, it is further combined with the gen-
erative trigram model, and is called the charac-
ter-based joint-plus model. 
6 Experiments 
The corpora provided by the second SIGHAN 
Bakeoff (Emerson, 2005) were used in our ex-
periments. The statistics of those corpora are 
shown in Table 3. 
Note that the PKU corpus is a little different 
from others. In the training set, Arabic num-
bers and English characters are in full-width 
form occupying two bytes. However, in the 
testing set, these characters are in half-width 
form occupying only one byte. Most research-
ers in the SIGHAN Bakeoff competition per-
formed a conversion before segmentation 
(Xiong et al, 2009). In this work, we conduct 
the tests on both unconverted (ucvt.) case and 
converted (cvt.) case. After the conversion, the 
OOV rate of converted corpus is obviously 
lower than that of unconverted corpus. 
To fairly compare the proposed approach 
with previous works, we only conduct closed 
tests1. The metrics Precision (P), Recall (R), 
F-score (F) (F=2PR/(P+R)), Recall of OOV 
(ROOV) and Recall of IV (RIV) are used to 
evaluate the results. 
6.1 Character-Based Generative Model 
and Discriminative Model 
As shown in (Wang et al, 2009), the character-
based generative trigram model significantly 
exceeds its related bigram model and performs 
the same as its 4-gram model. Therefore,  SRI 
Language Modeling  Toolkit2 (Stolcke, 2002) 
is used to train the trigram model with modi-
fied Kneser-Ney smoothing (Chen and Good-
man, 1998). Afterwards, a beam search de-
coder is applied to find out the best sequence. 
For the character-based discriminative 
model, the ME Package3 given by Zhang Le is 
used to conduct the experiments. Training was 
done with Gaussian prior 1.0 and 300, 150 it-
erations for AS and other corpora respectively.  
Ta
                                                
ble 5 gives the segmentation results of both 
the character-based generative model and the 
discriminative model. From the results, it can 
be seen that the generative model achieves 
comparable results with the discriminative one 
and they outperform each other on different 
corpus. However, the generative model ex-
ceeds the discriminative one on RIV (0.973 vs. 
0.956) but loses on ROOV (0.511 vs. 0.680). It 
illustrates that they complement each other. 
 
1 According to the second Sighan Bakeoff regulation, the 
closed test could only use the training data directly pro-
vided. Any other data or information is forbidden, includ-
ing the knowledge of characters set, punctuation set, etc. 
2 http://www.speech.sri.com/projects/srilm/ 
3 http://homepages.inf.ed.ac.uk/lzhang10/maxent_toolkit.html 
1177
Joint model performance on Development sets
0.9300
0.9400
0.9500
0.9600
0.9700
0.9800
0.9900
0.0
0
0.1
0
0.2
0
0.3
0
0.4
0
0.5
0
0.6
0
0.7
0
0.8
0
0.9
0
1.0
0
alpha
F-
sc
or
e
AS
CITYU
MSR
PKU
 
Figure 1: Development sets performance of Charac-
ter-based joint model. 
Corpus Set Words  OOV Num OOV Rate
Development 17,243 445 0.026  AS 
Testing 122,610 5,308/5,311 0.043/0.043
Development 17,324 355 0.020 MSR 
Testing 106,873 2,829/2,833 0.026/0.027
Development 12,075 537 0.044 CITYU 
Testing 40,936 3,028/3,034 0.074/0.074
Development 13,576 532 0.039 
Testing (ucvt.) 104,372 6,006/6,054 0.058/0.058PKU 
Testing (cvt.) 104,372 3,611/3,661 0.035/0.035
Table 4: Corpus statistics for Development sets and 
Testing sets. A ?/? separates the OOV number (or 
OOV rate) with respect to the original training sets 
and the new training sets. 
6.2 Character-Based Joint Model 
For the character-based joint model, a devel-
opment set is required to obtain the weight ?  
for its associated generative model. A small 
portion of each original training corpus is thus 
extracted as the development set and the re-
maining data is regarded as the new training-
set, which is used to train two new parameter-
sets for both generative and discriminative 
models associated.  
The last 2,000, 600, 400, and 300 sentences 
for AS, MSR, CITYU, and PKU are extracted 
from the original training corpora as their cor-
responding development sets. The statistics for 
new data sets are shown in Table 4. It can be 
seen that the variation of the OOV rate could 
be hardly noticed. The F-scores of the joint 
model, versus different ? , evaluated on four 
development sets are shown in Figure 1. It can 
be seen that the curves are not sharp but flat 
near the top, which indicates that the character-
based joint model is not sensitive to the ?  
value selected. From those curves, the best 
suitable ?  for AS, CITYU, MSR and PKU are 
found to be 0.30, 0.60, 0.60 and 0.60, respec-
Corpus Model R P F ROOV RIV
tively. Those alpha values will then be adopted 
to conduct the experiments on the testing sets. 
G 0.958 0.938 0.948 0.518 0.978
D 0 0.946 0  0.967.955 .951 0.707 
D-Plus 0.960 0.948 0.954 0.680 0.973
J 0.962 0.950 0.956 0.679 0.975
AS 
J-Plus 0.963 0.949 0.956 0.652 0.977
G 0.951 0.937 0.944 0.609 0.978
D 0.941 0.944 0.942 0.708 0.959
D-Plus 0.951 0.952 0.952 0.720 0.970
J 0.957 0.951 0.954 0.691 0.979
CITYU
J-Plus 0.959 0.952 0.956 0.700 0.980
G 0.974 0.967 0.970 0.561 0.985
D 0.957 0.962 0.960 0.719 0.964
D-Plus 0.965 0.967 0.966 0.675 0.973
J 0.974 0.971 0.972 0.659 0.983
MSR 
J-Plus 0.975 0.970 0.972 0.632 0.984
G 0.929 0.933 0.931 0.435 0.959
D 0.922 0.941 0.932 0.620 0.941
D-Plus 0.934 0.949 0.941 0.649 0.951
J 0.935 0.946 0.941 0.561 0.958
PKU 
(ucvt.) 
J-Plus 0.937 0.947 0.942 0.556 0.960
G 0.952 0.951 0.952 0.503 0.968
D 0.940 0.951 0.946 0.685 0.949
D-Plus 0.949 0.958 0.953 0.674 0.958
J 0.954 0.958 0.956 0.616 0.966
PKU 
(cvt.) 
J-Plus 0.955 0.958 0.957 0.610 0.967
G 0.953 0.946 0.950 0.511 0.973
D 0.944 0.950 0.947 0.680 0.956
D-Plus 0.952 0.955 0.953 0.676 0.965
J 0.957 0.955 0.956 0.633 0.971
Overall
J-Plus 0.958 0.955 0.957 0.621 0.973
Table 5: ent e
based m n t G  
ificantly outperforms both the character-
ba
 Segm
odels o
ation r sults of various character-
he second SI HAN Bakeoff, the
generative trigram model (G), the discriminative 
model (D), the discriminative-plus model (D-Plus), 
the joint model (J) and the joint-plus model (J-Plus). 
 
As shown in Table 5, the joint model sig-
n
sed generative model and the discriminative 
one in F-score on all the testing corpora. Com-
pared with the generative approach, the joint 
model increases the overall ROOV from 0.510 to 
0.633, with the cost of slightly degrading the 
overall RIV from 0.973 to 0.971. This shows 
that the joint model holds the advantage of the 
generative model on IV words. Compared with 
the discriminative model, the proposed joint 
model improves the overall RIV from 0.956 to 
0.971, with the cost of degrading the overall 
ROOV from 0.680 to 0.633. It clearly shows that 
the joint model achieves a good balance be-
tween IV words and OOV words and achieves 
the best F-scores obtained so far (21% relative 
error reduction over the discriminative model 
and 14% over the generative model). 
1178
6.3 Weigh Various Features Differently 
Inspired by (Jiang et al, 2008), we set the real-
d 
 
Although Table 5 has shown that the proposed 
all the 
value of C0 to be 2.0, the value of C-1C0 an
C0C1 to be 3.0, and the values of all other fea-
tures to be 1.0 for the character-based dis-
criminative-plus model. Although it seems rea-
sonable to weight those closely relevant fea-
tures more (C0 should be the most relevant fea-
ture for assigning tag t0), both implementations 
seem to be equal if their corresponding 
lambda-values are also updated accordingly. 
However, Table 5 shows that this new dis-
criminative-plus implementation (D-Plus) sig-
nificantly outperforms the original one (overall 
F-score is raised from 0.947 to 0.953) when 
both of them adopt real-valued features. It is 
not clear how this change makes the difference. 
Similar improvements can be observed with 
two other ME packages. One anonymous re-
viewer pointed out that the duplicated features 
should not make difference if there is no regu-
larization. However, we found that the dupli-
cated features would improve the performance 
whether we give Gaussian penalty or not. 
Afterwards, this new implementation and 
the generative trigram model are further com-
bined (named as the joint-plus model). Table 5 
shows that this joint-plus model also achieves 
better results compared with the discrimina-
tive-plus model, which illustrates that our joint 
approach is an effective and robust method for 
CWS. However, compared with the original 
joint model, the new joint-plus approach does 
not show much improvement, regardless of the 
significant improvement made by the discrimi-
native-plus model, as the additional benefit 
generated by the discriminative-plus model has 
already covered by the generative approach 
(Among the 6,965 error words corrected by the 
discriminative-plus model, 6,292 (90%) of 
them are covered by the generative model). 
7 Statistical Significance Tests 
joint (joint-plus) model outperforms 
baselines mentioned above, we want to know 
if the difference is statistically significant 
enough to make such a claim. Since there is 
only one testing set for each training corpus, 
the bootstrapping technique (Zhang et al, 2004) 
is adopted to conduct the tests: Giving an  
Models  
A B 
AS CITYU MSR PKU (ucvt.) 
PKU
(cvt.)
G D <  ~ >  ~ >  
D-Plus G >  >  <  >  >  
D-Plus D >  >  >  >  >  
J G >  >  >  >  >  
J D >  >  >  >  >  
J-Plus G >  >  >  >  >  
J-Plus D-Plus >  >  >  ~ >  
J-Plus J ~ >  ~ >  >  
Table 6 atistic sign anc est F- e 
 v er-b d m ls. 
f T0) will 
be generated by repeatedly re-sampling data 
eas-
 the dis-
he confi-
 
the pro-
po
e-
ng 
d. 
tegory 
includes (Asahara et al, 2005) (denoted as 
: St al ific e t of scor
among arious charact ase ode
testing-set T0, additional M-1 new testing-sets 
T0,?,TM-1 (each with the same size o
from T0. Then, we will have a total of M 
testing-sets (M=2000 in our experiments). 
7.1 Comparisons with Baselines 
We then follow (Zhang et al, 2004) to m
ure the 95% confidence interval for
crepancy between two models. If t
dence interval does not include the origin point,
we then claim that system A is significantly 
different from system B. Table 6 gives the re-
sults of significant tests among various models 
mentioned above. In this table, ?>? means that 
system A is significantly better than B, where 
as ?<? denotes that system A is significantly 
worse than B, and ?~? indicates that these two 
systems are not significantly different. 
As shown in Table 6, the proposed joint 
model is significantly better than the two base-
line models on all corpora. Similarly, 
sed joint-plus model also significantly out-
performs the generative model and the dis-
criminative-plus model on all corpora except 
on the PKU(ucvt.). The comparison shows that 
the proposed joint (also joint-plus) model in-
deed exceeds each of its component models. 
7.2 Comparisons with Previous Works 
The above comparison mainly shows the sup
riority of the proposed joint model amo
those approaches that have been implemente
However, it would be interesting to know if the 
joint (and joint-plus) model also outperforms 
those previous state-of-the-art systems.  
The systems that performed best for at least 
one corpus in the second SIGHAN Bakeoff are 
first selected for comparison. This ca
1179
A-sets. In-
st
th
                                                
sahara05) and (Tseng et al, 2005) 4  
(Tseng05). (Asahara et al, 2005) achieves the 
best result in the AS corpus, and (Tseng et al, 
2005) performs best in the remaining three 
corpora. Besides, those systems that are re-
ported to exceed the above two systems are 
also selected. This category includes (Zhang et 
al., 2006) (Zhang06), (Zhang and Clark, 2007) 
(Z&C07) and (Jiang et al, 2008) (Jiang08). 
They are briefly summarized as follows. 
(Zhang et al, 2006) is based on sub-word tag-
ging and uses a confidence measure method to 
combine the sub-word CRF (Lafferty et al, 
2001) and rule-based models. (Zhang and 
Clark, 2007) uses perceptron (Collins, 2002) to 
generate word candidates with both word and 
character features. Last, (Jiang et al, 2008)5  
adds repeated features implicitly based on (Ng 
and Low, 2004). All of the above models, ex-
cept (Zhang and Clark, 2007), adopt the char-
acter-based discriminative approach. 
All the results of the systems mentioned 
above are shown in Table 7. Since the systems 
are not re-implemented, we cannot generate 
paired samples from those M testing
ead, we calculate the 95% confidence inter-
val of the joint (also joint-plus) model. After-
wards, those systems can be compared with 
our proposed models. If the F-score of system 
B does not fall within the 95% confidence in-
terval of system A (joint or joint-plus), then 
they are statistically significantly different. 
Table 8 gives the results of significant tests 
for those systems mentioned in this section. It 
shows that both our joint-plus model and joint 
model exceed (or are comparable to) almost all 
e state-of-the-art systems across all corpora, 
except (Zhang and Clark, 2007) at PKU(ucvt.). 
In that special case, (Zhang and Clark, 2007) 
 
4 We are not sure whether (Asahara et al, 2005) and 
(Tseng et al, 2005) performed a conversion before seg-
mentation in PKU corpus. In this paper, we followed 
previous works, which cited and compared with them. 
5 The data for (Jiang et al, 2008) given at Table 7 are 
different from what were reported at their paper. In the 
communication with the authors, it is found that the script 
for evaluating performance, provided by the SIGHAN 
Bakeoff, does not work correctly in their platform. After 
the problem is fixed, the re-evaluated real performances 
reported here deteriorate from their original version. 
Please see the announcement in Jiang?s homepage 
(http://mtgroup.ict.ac.cn/~jiangwenbin/papers/error_corre
ction.pdf). 
Corpus
Participants AS CITYU MSR 
PKU 
(ucvt.) 
PKU
(cvt.)
Asahara05 0.952 0.941 0.958 N/A 0.941
Tseng05 0.947 0.943 0.964 N/A 0.950
Zhang06 0.951 0.951 0.971 N/A 0.951
Z&C07 0.946 0.951 0.972 0.945 N/A
Jiang08 0.953 0.948 0.966 0.937 N/A
Our Joint 0.956 0.954 0.972 0.941 0.956
Our Joint-Plus 0.956 0.956 0.972 0.942 0.957
Table 7: Compari r  p u
the-art sy
sons of F-sco e with revio s 
state-of- stems. 
Systems 
A B 
AS CITYU MSR (ucvt.)
PKU 
 (cvt.)
PKU
Asahara05 > > > N/A > 
Tseng05 > > > N/A > 
Zhang06 > ~ ~ N/A > 
Z&C07 > > ~ < N/A
J 
Jiang08 > > > > N/A
Asahara05 > > > N/A > 
Tseng05 > > > N/A > 
Zhang06 > > ~ N/A > 
Z&C07 > > ~ < N/A
J-Plus
Jiang08 ~ > > > N/A
Table al s ific e te of r 
f-the  syst s. 
outpe he jo -plu model by .3%  
 and 0.5%, re-
ne, 
e two models complement 
dling IV words and OOV 
e-
nomenon.  
8: Statistic ign anc st  F-score fo
previous state-o -art em
rforms t int s  0  on
F- score (0.4% for the joint model). However, 
our joint-plus model exceeds it more over AS 
and CITYU corpora by 1.0%
spectively (1.0% and 0.3% for the joint model). 
Thus, it is fair to say that both our joint model 
and joint-plus model are superior to the state-
of-the-art systems reported in the literature. 
8 Conclusion 
From the error analysis of the character-based 
generative model and the discriminative o
we found that thes
each other on han
words. To take advantage of these two ap-
proaches, a joint model is thus proposed to 
combine them. Experiments on the Second 
SIGHAN Bakeoff show that the joint model 
achieves 21% error reduction over the dis-
criminative model (14% over the generative 
model). Moreover, closed tests on the second 
SIGHAN Bakeoff corpora show that this joint 
model significantly outperforms all the state-
of-the-art systems reported in the literature. 
Last, it is found that weighting various fea-
tures differently would give better result. How-
ever, further study is required to find out the 
true reason for this strange but interesting ph
1180
A Generic-Beam-Search code and 
o Ms. Nanyan Kuo for 
eric-Beam-Search code.  
m
optimum 
entation. In Proceedings of 
GHAN Workshop on Chinese Lan-
St
Th
Jia
W
Jo
Hw
Fu
ction using conditional random fields. 
Ad
MNLP, pages 
Hu
ld Word Segmenter 
Ku d Keh-Yih Su, 2009. 
Yi
scriminative 
Ni
ssing, 8 (1). pages 
Hu
Second 
Ru
2006. Subword-based Tagging for Con-
Yi  
 scores: How much im-
Yu
f ACL, pages 840-847, 
cknowledgement 
The authors extend sincere thanks to Wenbing 
Jiang for his helps with our experiments. Also, 
we thank Behavior Design Corporation for 
using their
show special thanks t
her helps with the Gen
The research work has been partially funded 
by the Natural Science Foundation of China 
under Grant No. 60975053, 90820303 and 
60736014, the National Key Technology R&D 
Program under Grant No. 2006BAH03B02, 
and also the Hi-Tech Research and Develop-
ent Program (?863? Program) of China under 
Grant No. 2006AA010108-4 as well. 
References 
Masayuki Asahara, Kenta Fukuoka, Ai Azuma, 
Chooi-Ling Goh, Yotaro Watanabe, Yuji Ma-
tsumoto and Takashi Tsuzuki, 2005. Combina-
tion of machine learning methods for 
Chinese word segm
the Fourth SI
guage Processing, pages 134?137, Jeju, Korea. 
Christopher M. Bishop, 2006. Pattern recognition 
and machine learning. New York: Springer  
anley F. Chen and Joshua Goodman, 1998. An 
empirical study of smoothing techniques for lan-
guage modeling. Technical Report TR-10-98, 
Harvard University Center for Research in 
Computing Technology. 
Michael Collins, 2002. Discriminative training 
methods for hidden markov models: theory and 
experiments with perceptron algorithms. In Pro-
ceedings of EMNLP, pages 1-8, Philadelphia. 
omas Emerson, 2005. The second international 
Chinese word segmentation bakeoff. In Proceed-
ings of the Fourth SIGHAN Workshop on Chi-
nese Language Processing, pages 123-133. 
nfeng Gao, Mu Li and Chang-Ning Huang, 2003. 
Improved Source-Channel Models for Chinese 
Word Segmentation. In Proceedings of ACL, 
pages 272-279. 
enbin Jiang, Liang Huang, Qun Liu and Yajuan 
Lu, 2008. A Cascaded Linear Model for Joint 
Chinese Word Segmentation and Part-of-Speech 
Tagging. In Proceedings of ACL, pages 897-904. 
hn Lafferty, Andrew McCallum and Fernando 
Pereira, 2001. Conditional Random Fields: Prob-
abilistic Models for Segmenting and Labeling 
Sequence Data. In Proceedings of ICML, pages 
282-289. 
ee Tou Ng and Jin Kiat Low, 2004. Chinese 
part-of-speech tagging: one-at-a-time or all-at-
once? word-based or character-based. In Pro-
ceedings of EMNLP, pages 277-284. 
chun Peng, Fangfang Feng and Andrew 
McCallum, 2004. Chinese segmentation and new 
word dete
In Proceedings of COLING, pages 562?568. 
wait Ratnaparkhi, 1998. Maximum entropy 
models for natural language ambiguity resolu-
tion. University of Pennsylvania. 
Andreas Stolcke, 2002. SRILM-an extensible lan-
guage modeling toolkit. In Proceedings of the 
International Conference on Spoken Language 
Processing, pages 311-318. 
Kristina Toutanova, 2006. Competitive generative 
models with structure learning for NLP classifi-
cation tasks. In Proceedings of E
576-584, Sydney, Australia. 
ihsin Tseng, Pichuan Chang, Galen Andrew, 
Daniel Jurafsky and Christopher Manning, 2005. 
A Conditional Random Fie
for Sighan Bakeoff 2005. In Proceedings of the 
Fourth SIGHAN Workshop on Chinese Lan-
guage Processing, pages 168-171. 
n Wang, Chengqing Zong an
Which is more suitable for Chinese word seg-
mentation, the generative model or the discrimi-
native one? In Proceedings of PACLIC, pages 
827-834, Hong Kong, China. 
ng Xiong, Jie Zhu, Hao Huang and Haihua Xu, 
2009. Minimum tag error for di
training of conditional random fields. Informa-
tion Sciences, 179 (1-2). pages 169-179. 
anwen Xue, 2003. Chinese Word Segmentation 
as Character Tagging. Computational Linguistics 
and Chinese Language Proce
29-48. 
aping Zhang, Hongkui Yu, Deyi Xiong and Qun 
Liu, 2003. HHMM-based Chinese lexical ana-
lyzer ICTCLAS. In Proceedings of the 
SIGHAN Workshop on Chinese Language Proc-
essing, pages 184?187. 
iqiang Zhang, Genichiro Kikui and Eiichiro 
Sumita, 
fidence-dependent Chinese Word Segmentation. 
In Proceedings of the COLING/ACL, pages 961-
968, Sydney, Australia. 
ng Zhang, Stephan Vogel and Alex Waibel, 2004.
Interpreting BLEU/NIST
provement do we need to have a better system. 
In Proceedings of LREC, pages 2051?2054. 
e Zhang and Stephen Clark, 2007. Chinese Seg-
mentation with a Word-Based Perceptron Algo-
rithm. In Proceedings o
Prague, Czech Republic. 
1181
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 398?408, Dublin, Ireland, August 23-29 2014.
 Dynamically Integrating Cross-Domain Translation Memory into 
Phrase-Based Machine Translation during Decoding 
 
 
Kun Wang?        Chengqing Zong?        Keh-Yih Su? 
?National Laboratory of Pattern Recognition, Institute of Automation, 
Chinese Academy of Sciences, Beijing, China 
?Institute of Information Science, Academia Sinica, Taiwan 
?
{kunwang, cqzong}@nlpr.ia.ac.cn 
?
kysu@iis.sinica.edu.tw 
 
 
Abstract 
Our previous work focuses on combining translation memory (TM) and statistical machine translation 
(SMT) when the TM database and the SMT training set are the same. However, the TM database will 
deviate from the SMT training set in the real task when time goes by. In this work, we concentrate on 
the task when the TM database and the SMT training set are different and even from different domains. 
Firstly, we dynamically merge the matched TM phrase-pairs into the SMT phrase table to meet the real 
application. Secondly, we propose an improved integrated model to distinguish the original and the new-
ly-added phrase-pairs. Thirdly, a simple but effective TM adaptation method is adopted to favor the 
consistent translations in cross-domain test. Our experiments have shown that merging the TM phrase-
pairs achieves significant improvements. Furthermore, the proposed approaches are significantly better 
than the TM, the SMT and previous integration works for both in-domain and cross-domain tests. 
1 Introduction 
Since the translation memory (TM) system and the statistical machine translation (SMT) system com-
plement each other in those matched sub-segments and unmatched sub-segments (Wang et al., 2013), 
combining them can improve the output quality significantly, especially when high-similarity fuzzy 
matches are available. Therefore, combining TM and SMT is drawing more and more attention in re-
cent years (He et al., 2010a; 2010b; 2011; Koehn and Senellart, 2010; Zhechev and van Genabith, 
2010; Ma et al., 2011; Dara et al., 2013; Wang et al., 2013). 
Those previous works on combining TM and SMT can be classified into four categories: (1) select-
ing the better translation sentence from TM and SMT (He et al., 2010a; 2010b; Dara et al., 2013); (2) 
incorporating TM matched sub-segments into SMT in a pipelined manner (Koehn and Senellart, 2010; 
He et al., 2011; Ma et al., 2011); (3) only enhancing the SMT phrase table with new TM phrase-pairs 
(Bi?ici and Dymetman, 2008; Simard and Isabelle, 2009); and (4) incorporating the associated TM 
information with each source phrase to guide the SMT decoding (Wang et al., 2013). 
However, all previous works mentioned above only focus on the case in which the TM database and 
the SMT training set share the same data-set. Nonetheless, in real applications, the TM database will 
deviate from the SMT training set when time goes by, because the TM database will be dynamically 
enlarged when more translations are generated by the human translator. Therefore, this paper will con-
centrate on a more realistic case, in which the TM database and the SMT training set are different and 
even from different domains. 
When the TM database and the SMT training set share the same data-set, the integrated model 
(Wang et al., 2013) can avoid the drawbacks of the pipeline approaches and outperforms the other ap-
proaches significantly. However, this integrated model only refers to the TM information but not 
adopts the matched TM phrase-pairs as candidates during decoding. Therefore, many TM phrase-pairs 
cannot be covered by the SMT phrase table when the TM database and the SMT training set are dif-
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer 
are added by the organizers. Licence details: http://creativecommons.org/licenses/by/4.0/ 
398
ferent. It is thus impossible to generate those unseen TM target phrases. This problem would even get 
worse when the TM database and the SMT training set are from different domains. 
To make the integrated model meet the real application, we dynamically merge the matched TM 
phrase-pairs into the SMT phrase table. In addition, an improved integrated model is proposed to dis-
tinguish the original SMT phrase-pairs and the newly-added ones extracted from TM. Furthermore, a 
simple but effective TM adaptation method is adopted to favor the consistent translation in cross-
domain test. To our best knowledge, this is the first unified framework for integrating TM into SMT 
during decoding when the TM database and the SMT training set are different (even from different 
domains). 
On the TM database which consists of Chinese?English computer technical documents, our experi-
ments have shown that merging the matched TM phrase-pairs achieves significant improvement when 
the fuzzy match score is above 0.5. Besides, the proposed approaches are significantly better than ei-
ther the SMT or the TM systems for both the in-domain and the cross-domain tests when the fuzzy 
match score is above 0.4. Furthermore, the proposed approaches also outperform previous integration 
works significantly in all test conditions. 
2 Integrated Model 
Wang et al. (2013) incorporated the TM information into the phrase-based SMT, and re-defined the 
translation problem as: 
 ?          ( |                         )  
Where   denotes the given source sentence,   is a corresponding target translation, and  ? is the final 
result; [                       ]  is the associated information of the best TM sentence-pairs; 
     and      are the corresponding TM source and target sentences, respectively;      denotes its 
corresponding fuzzy match score (from 0 to 1);     is the monolingual alignment information between 
  and     ; and      denotes the bilingual word alignment information between      and     . 
With the TM information, this problem can be simplified to: 
  ?        { (  ?
 | ? ( )
 ( ))  ?        ? ( )  (  |    ) 
 
   }  (1) 
Where  ? ( ) and   ? denote the k-th associated source and target phrases, respectively;     ? ( ) and 
     ?( ) are the corresponding TM source and target phrases associated with the given source phrase 
 ? ( ) (total K phrases without insertion).   is the corresponding TM target phrase matching status for 
the current target candidate   ?, which reflects the quality of the given candidate;    is the linking sta-
tus vector of  ? ( ) (the aligned source phrase, within  ? ( )
 ( )  of   ?), which indicates the matching and 
linking status in the source side (and is closely related to the matching status of the target side).      
is uniformly divided into ten fuzzy match intervals and the index   specifies the corresponding interval. 
In Equation (1), the first factor is just the typical phrase-based SMT model, and the second factor 
 (  |    ) is the information derived from the TM sentence pair. Afterwards, the factor  (  |    ) 
was further derived with TM matching status as follows: 
  (  |    )  {
 (    |                          )
  (    |                     )
  (    |                )
} (2) 
Where the first factor reflects the TM content matching status, the second factor is the relationship 
between various TM target phrases, and the third factor is the reordering information implied by TM. 
Equation (2) is adopted to guide the SMT decoding, and is denoted as the integrated Model-III in 
(Wang et al., 2013) (also called Model-III in this paper thereafter). 
For space limitation, only those features which are also adopted in our additional introduced proba-
bility factor (to be specified later) will be briefly introduced here: 
Target Phrase Content Matching Status (TCM): It indicates the content matching status between   ? 
and      ?( ) , and reflects the quality of   ? . It is a member of {Same, High, Low, NA (Not-
Applicable)}. 
399
Source Phrase Content Matching Status (SCM): It indicates the content matching status between 
 ? ( )  and     ? ( ) , and affects the matching status of   ?  and      ?( )  greatly. It is a member of 
{Same, High, Low, NA}. 
Number of Linking Neighbors (NLN): Usually, the context of a source phrase would affect its target 
translation. The more similar the context is, the more likely that the translation is the same. NLN is 
adopted to measure the context similarity. 
3 Proposed Approaches 
3.1 Merging the TM Phrase-Pairs 
Since all TM phrase-pairs are only referred while re-scoring the SMT candidates in Model-III, they are 
not regarded as candidates during decoding. When the TM database and the SMT training set are the 
same, this restriction is reasonable because the SMT phrase table can cover all the continuous TM 
phrase pairs within the phrase length limit. However, this would not be true when the TM database 
and the SMT training set are different. Therefore, the SMT phrase table should be further enhanced 
with those matched new TM phrase pairs in this case.  
According to their relations with the SMT phrase table, TM phrase pairs can be classified into three 
different categories: (1) the whole TM phrase-pair can be found in the original SMT phrase table; (2) 
only TM source phrase exists in the original SMT phrase table, but its corresponding target phrase 
does not; (3) even TM source phrase cannot be found in the original SMT phrase table. Since the first 
category has been covered by the original SMT phrase table, only the phrase-pairs from the second 
and the third categories should be added into the SMT phrase table dynamically for each input sen-
tence. To distinguish those newly added phrase-pairs from the original SMT phrase-pairs, we use eight 
additional feature weights    for the translation probability (lexical and phrase transfer in both direc-
tions) and two more feature weights for the phrase penalty (details will be specified later in Section 4). 
The above approach is inspired by the work of (Bi?ici and Dymetman, 2008). However, there are 
three differences between our approach and theirs. Firstly, we add all those matched TM phrase-pairs 
(include all associated sub-phrase pairs), while Bi?ici and Dymetman (2008) only added the longest 
matched one; Secondly, we add all the possible TM target phrase-pairs for a given TM source phrase 
while they extracted only one TM target phrase regardless of the existence of multiple TM target can-
didates; Lastly, we use different feature weights to distinguish those newly added TM phrase-pairs 
from the original SMT phrase-pairs, while they treated them equally. 
3.2 Distinguishing the TM Phrase-Pairs 
As mentioned in Section 3.1, we need to merge those TM matched phrase pairs into the SMT phrase 
table when the TM database and the SMT training set are different. However, the original integrated 
Model-III does not distinguish the newly added TM phrase-pairs from those original SMT phrase-
pairs in  (  |    ). Therefore, we introduce two new features Source Phrase Origin (SPO) and 
Target Phrase Origin (TPO), which are a member of {Original, Newly-Added}, to the original Mod-
el-III in (Wang et al., 2013) to favor the newly added TM phrase-pairs, and re-derive  (  |    ) as 
follows (assume that TPO is only dependent on SPO, NLN and  ): 
 
 (  |    ) 
  ([               ] |[                       ]   ) 
 
{
 
 
 (    |                          )
  (    |                     )
  (    |                )
  (    |           ) }
 
 
 
(2) 
The additional factor  (    |           ) in the above equation is added to handle those newly 
added TM phrase-pairs. This would be the proposed Distinguishing Model. For the phrases from the 
original SMT phrase table, both the SPO and TPO features would be ?Original?; for the phrases from 
the second category mentioned in Section 3.1, the SPO would be ?Original? but the TPO would be 
?Newly-Added?; for the phrases from the third category, both the SPO and TPO features would be 
?Newly-Added?. 
400
3.3 TM Adaptation 
In real applications, the TM database is usually not big enough to train an SMT system when it is ap-
plied to a special technical domain other than the news domain. Besides, many professional translators 
do not want to expose the whole TM database to the SMT system providers (Cancedda, 2012). In this 
situation, we will be forced to first train an SMT model on an out domain (usually the news domain) 
which possesses a lot of training data, and then fix the obtained phrase-based SMT model. Afterwards, 
we incorporate it on line with an additional TM database which is from another in domain. 
To simulate the above scenario, we will thus train our integrated model on the out domain. However, 
we have a domain-mismatch problem for this cross-domain test. Generally, in the technical domain, 
which is suitable for TM application, the translations (especially for technical terms) are much more 
consistent than that in the news domain. That is, the same source phrase in various places tends to 
have exactly the same translation in technical domains. Therefore, when we use Distinguishing Model 
to perform forced decoding, the obtained results would possess different statistics among the in-
domain development set and the out-domain training set. For example, at interval [0.9, 1.0), when 
SCM is ?Same?, 94.6% of TCM are ?Same? in the development set (in), while this ratio is only 65.1%  
in the training set (out). Therefore, the factor  (    |                          ) from the 
test set will possess a different probability distribution in comparison with that from the training set. 
However, the development set is not big enough (only a few hundreds sentence-pairs at each interval) 
to re-train all TM factors of the proposed model. Therefore, we simply add the following h1 feature to 
reflect the tendency of having high translation consistency in the development set: 
  ( ?  ?  ) {
                              
                                                         
 
Where  ? and  ? denote the source phrase, the target candidate, respectively. 
Furthermore, various source synonyms might generate the same translation (Zhu et al., 2013). 
Therefore, even SCM?Same, we still favor the SMT phrase-pair candidate which exactly matches TM 
target phrase. For example, if source words are synonyms such as ???? (want) and ??? (want), ??
?? (if) and ??? (if), ???? (at once) and ???? (at once), the target translations would be the same. 
Therefore, the issue of having high translation consistency in the technical domain is also applied. We 
thus further add the following h2 feature to reflect the tendency of having high translation consistency 
in this case (?High? and ?Low? are grouped into ?Other? for the SCM): 
  ( ?  ?  ) {
                               
                                                          
 
Afterwards, the associated feature weights are tuned on the development set. 
4 Experiments 
4.1 Experimental Setup 
We use the same TM data-set adopted by Wang et al. (2013), which is a Chinese?English TM data-
base consisting of computer technical documents. It includes about 267k sentence pairs. All the exper-
iments are conducted around this TM data-set. To compare the performances under different condi-
tions, the same development set and the test set will be shared by both in-domain and cross-domain 
tests. Since the associated SMT training-set and TM database will vary under different experimental 
configurations, they will be specified later in each sub-section. 
In this work, the translation memory system (denoted as TM) and the phrase-based machine transla-
tion system (denoted as SMT) are adopted as our two baseline systems. Following (Wang et al., 2013), 
for TM, the word-based fuzzy match score is adopted as the similarity measure; also, for the phrase-
based SMT system, the same Moses toolkit (Koehn et al., 2007) and the same set of following features 
are adopted: the phrase translation model, the language model, the distance-based reordering model, 
the lexicalized reordering model and the word penalty. The system configurations are as follows: GI-
ZA++ (Och and Ney, 2003) is used to obtain the bidirectional word alignments. Afterwards, ?intersec-
tion? refinement (Koehn et al., 2003) is adopted to extract phrase-pairs. We use SRI Language Model 
401
toolkit (Stolcke, 2002) to train a 5-gram model with modified Kneser-Ney smoothing (Kneser and Ney, 
1995; Chen and Goodman, 1998) on the target-side (English) training corpus. All the feature weights 
and the weight for each probability factor are tuned on the development set with minimum-error-rate 
training (MERT) (Och, 2003). The maximum phrase length is set to 7 in our experiments. 
To compare our proposed models with those state-of-the-art methods, we re-implement two XML-
Markup approaches (Koehn and Senellart, 2010; and the upper bound version of (Ma et al, 2011)) and 
the Model-III (Wang et al., 2013) as three baseline systems, and denote them as Koehn-10, Ma-11-U 
and Model-III, respectively. Similar to (Wang et al., 2013), we only re-implement the XML-Markup 
method used in (Ma et al, 2011), but not their discriminative learning method. 
Following (Wang et al., 2013), we also train the TCM, LTC and CPM factors in the SMT training 
set with cross-fold translation. Since the TPO factor (conditioning on NLN and Distinguishing Model) 
is based on Model-III, we first use Model-III to generate the desired results on the development set via 
forced decoding, and then generate the training samples of TPO factor for Distinguishing Model.  
In this work, the translation performance is measured with case-insensitive BLEU-4 score (Papineni 
et al., 2002) and TER score (Snover et al., 2006). Statistical significance tests are conducted with re-
sampling (1,000 times) approach (Koehn, 2004) in 95% confidence level. 
4.2 In-Domain Translation Results 
In the in-domain test, the original TM dataset is first randomly divided into two parts. The first part is 
then adopted as the new TM database, while the second part is adopted as the SMT training set. The 
detailed corpus statistics is shown in Table 1. Since the TM database is different from that adopted in 
(Wang et al., 2013), the statistics shown in Table 2 at each interval is also different from theirs.  
All matched TM phrase-pairs are extracted according to the word alignment generated from the 
phrase-based SMT system. Since there are not enough samples to estimate the translation probabilities 
for those newly added TM phrase-pairs, we use the following method to assign the translation proba-
bilities. For those TM phrase-pairs that only their source phrases exist in the original SMT phrase table 
(the second category mentioned in Section 3.1), as their source phrases have already existed in the 
SMT phrase table, there is at least one associated target phrase in the original SMT phrase table. For 
each new TM phrase-pair, we thus directly assign the maximum probability among its associated orig-
inal target phrases to it. For those TM phrase-pairs that even their source phrase cannot be found in the 
original SMT phrase table (the third category), as there is no corresponding phrase-pair in the original 
SMT phrase table, we will simply assign probability ?1.0? (this value is not important as its associated 
weight will be tuned later) as their four translation probabilities. To distinguish those newly added 
phrase-pairs from the original SMT phrase-pairs, we use eight additional feature weights for the trans-
lation probability and two more feature weights for the phrase penalty. 
To evaluate the effectiveness of adding TM phrase-pairs, we compare the cases of whether merging 
TM phrase-pairs or not for both SMT and Model-III. Table 3 and Table 4 give the translation results in 
BLEU and TER, respectively. ?SMT? and ?Model-III? denote that we do not merge the TM phrase-
pairs into the SMT phrase table during decoding. That is, they only use the original SMT phrase table. 
  #Sentences #Chn. Words #Chn. VOC. #Eng. Words #Eng. VOC. 
New TM Database 130,953 1,808,992 30,164 1,811,413 30,807 
SMT Training Set 130,953 1,814,524 29,792 1,815,615 30,516 
Table 1: Corpus Statistics for In-Domain Tests 
Intervals 
[0.9, 
1.0) 
[0.8, 
0.9) 
[0.7, 
0.8) 
[0.6, 
0.7) 
[0.5, 
0.6) 
[0.4, 
0.5) 
[0.3, 
0.4) 
(0.0, 
0.3) 
(0.0, 
1.0) 
#Sentences 147 255 244 355 488 514 419 154 2,576 
#Words 2,431 3,438 3,299 4,674 6,125 7,525 7,082 4,074 38,648 
W/S 16.5 13.5 13.5 13.2 12.6 14.6 16.9 26.5 15.0 
Table 2: Corpus Statistics for In-Domain Test-Set (W/S: the average #words per sentence) 
402
?SMT+? and ?Model-III+? mean that we merge the TM phrase-pairs into the SMT phrase table dynam-
ically. In these tables, ?+? indicates that those newly added TM phrase-pairs significantly improve the 
translation results (?SMT? vs. ?SMT+?, ?Model-III? vs. ?Model-III+?, and ?Model-III? vs. ?Distin-
guishing?). 
It can be seen that adding TM phrase-pairs significantly improve the translation results when the 
fuzzy match score is above 0.5 (comparing SMT with SMT+, and Model-III with Model-III+). For ex-
ample, at interval [0.9, 1.0), those added TM phrase-pairs significantly improve the SMT system from 
63.65 to 73.55, and Model-III from 80.69 to 86.40. However, if Model-III+ is compared with Model-III, 
the improvements from merging the TM phrase-pairs get less when the fuzzy match score decreases, 
because the matched TM parts are fewer at low fuzzy match intervals. 
Also, with the same original SMT phrase table, Model-III exceeds the SMT system at each interval.  
For example, at interval [0.9, 1.0), the TM information significantly improve the translation result 
from 63.65 to 80.69. It thus shows that the TM information is very useful. However, it is still worse 
than the TM in TER (13.32 vs. 10.42). On the other hand, although Model-III has greatly exceeded the 
SMT at each interval, Model-III+ still significantly outperforms Model-III at most intervals. Therefore, 
the benefit of utilizing TM information and the benefit of adding TM phrase-pairs are not covered by 
each other and can be jointly enjoyed. Take the interval [0.9, 1.0) as an example, the TM information 
first improve the translation results from 63.65 (SMT) to 80.69 (Model-III), and then the added TM 
phrase-pairs further boosts it to 86.40 (Model-III+). 
Besides, Table 3 and Table 4 also present the translation results of our other two baselines (Koehn-
10 and Ma-11-U), and the proposed Distinguishing Model. Scores marked with  ?*?  indicate  that  
they are significantly better (p < 0.05) than both the TM and the SMT+ baselines, and those marked 
with ?#? are significantly better (p < 0.05) than Koehn-10. Scores marked with ?$? are significantly 
better than Model-III+. The bold entries are the best result at each interval. 
Intervals TM SMT SMT+ Model-III Model-III+ Distinguishing Koehn-10 Ma-11-U 
[0.9, 1.0) 79.89 63.65  73.55 + 80.69  86.40 +*# 86.69 +*# 82.21 67.58 
[0.8, 0.9) 72.65 60.75  74.04 + 78.95 * 83.35 +*# 83.44 +*# 79.50 * 67.03 
[0.7, 0.8) 59.59 60.57  65.52 + 68.55 * 71.37 +*# 72.06 +*# 67.52 62.60 
[0.6, 0.7) 41.57 53.38  56.14 + 55.61 # 57.75 +*# 58.73 +*#$ 51.83 56.74 
[0.5, 0.6) 25.17 45.60  46.95 + 47.40 # 48.39 +*# 48.27 *# 39.08 47.94 
[0.4, 0.5) 14.62 41.81  42.03  42.60 # 42.30 # 43.04 *#$ 31.60 42.93 
[0.3, 0.4) 7.50 35.95  35.49  36.10 # 35.31 # 35.34 # 25.25 36.58 
(0.0, 0.3) 4.94 32.64  33.22  33.45 # 33.23 # 33.23 # 23.70 33.10 
(0.0, 1.0) 31.11 46.68  49.41 + 51.00 *# 52.26 +*# 52.56 +*#$ 44.28 48.91 
Table 3: In-Domain Translation Results (BLEU). Scores marked with ?+? indicates that those newly 
added TM phrase-pairs significantly (p < 0.05) improve the translation results (?SMT? vs. ?SMT+?, 
?Model-III? vs. ?Model-III+?, and ?Model-III? vs. ?Distinguishing?). Scores marked with ?*? are sig-
nificantly better (p < 0.05) than both TM and SMT+ systems, and those marked with ?#? are signifi-
cantly better (p < 0.05) than Koehn-10. Scores marked with ?$? are significantly better  (p < 0.05) than 
Model-III+ (?Model-III+? vs. ?Distinguishing?) 
Intervals TM SMT SMT+ Model-III Model-III+ Distinguishing Koehn-10 Ma-11-U 
[0.9, 1.0) 10.42  27.14  17.64 + 13.32  8.76 +*# 8.22 +*# 12.95 23.94 
[0.8, 0.9) 16.07  28.73  17.66 + 14.69 * 10.46 +*# 10.49 +*# 14.72 * 23.83 
[0.7, 0.8) 28.68  29.47  24.99 + 22.01 * 20.15 +*# 19.33 +*# 23.96 27.43 
[0.6, 0.7) 48.59  33.76  31.53 + 31.57 # 29.77 +*# 28.95 +*#$ 36.89 30.98 
[0.5, 0.6) 63.13  40.57  39.00 + 38.79 # 38.00 *# 38.51 # 47.08 38.44 
[0.4, 0.5) 74.02  44.09  43.66  42.84 *# 43.43 # 42.88 *#$ 55.35 42.31 
[0.3, 0.4) 81.09  50.00  50.63  50.04 # 50.70 # 50.90 # 63.28 48.83 
(0.0, 0.3) 84.34  55.58  56.66  54.68 # 55.96 *# 55.96 *# 68.00 54.51 
(0.0, 1.0) 58.58  40.88  38.55 + 37.26 *# 36.47 +*# 36.28 +*# 45.63 38.73 
Table 4: In-Domain Translation Results (TER). The marks are the same as that in Table 3. 
403
In comparison with the TM and the SMT+ systems, Model-III+ is significantly better than both of 
them in either BLEU or TER scores when the fuzzy match score is above 0.5; also, Distinguishing 
Model outperforms both the TM and the SMT+ systems in either BLEU or TER scores when the fuzzy 
match score is above 0.4. Furthermore, the improvements from both Model-III+ and Distinguishing 
Model get less when the fuzzy match score decreases, as the TM information is less reliable at low 
fuzzy match intervals. 
Across all intervals (the last row in the table), Distinguishing Model not only achieves the best 
BLEU score (52.56), but also gets the best TER score (36.28). At those intervals when the fuzzy 
match score is above 0.4, Model-III+ and Distinguishing Model are the best two in either BLEU or 
TER scores. Besides, Distinguishing Model slightly exceeds Model-III+ at most intervals. However, 
both Model-III+ and Distinguishing Model achieve significant improvements over the TM and the 
SMT+. 
Compared with previous works, it can be seen that both Model-III+ and Distinguishing Model sig-
nificantly outperform Koehn-10 in either BLEU or TER scores at all intervals, and are significantly 
better than Model-III when the fuzzy match score is above 0.6. Furthermore, the proposed approaches 
(both Model-III+ and Distinguishing Model) achieve a much better TER score than the TM system 
does at the interval [0.9, 1.0); while Model-III and Koehn-10 are worse than the TM system at this 
interval. Also, both Model-III+ and Distinguishing Model exceed Ma-11-U at most intervals. There-
fore, it can be concluded that the proposed models outperform previous approaches significantly in 
this scenario. 
To further verify the proposed approaches in this case, we swap the TM database and the SMT 
training set and re-run the experiments. Similar and significant improvements are still observed: both 
Model-III+ and the Distinguishing Model achieve significant improvements over the TM and the 
SMT+. All those results have shown that the proposed approaches are robust. 
In real environments, the SMT training set and the TM database could be the same before transla-
tion projects starts. However, the TM database will gradually deviate from the SMT training set while 
the translation task progresses.  Nonetheless, our experiments have shown that the proposed Distin-
guishing Model is effective even when the TM database and the SMT training set are totally different 
(which would be the extreme case for real applications). Therefore, it can be concluded that this pro-
posed approach is robust. 
4.3 Cross-Domain Translation Results 
To evaluate the cross domain performance, we adopt the news corpora about computer and science 
from CWMT09 (Liu and Zhao, 2009) as the SMT training set, and adopt the whole TM dataset as the 
TM database. The SMT training set includes about 404k bilingual sentence-pairs (which includes 
about 9M Chinese words and 8.7M English words). Corpus statistics is shown in Table 5. Since the 
TM database and the test set (also the development set) are the same as that in (Wang et al., 2013), the 
statistics at each interval is the same as theirs but different from Table 2. 
The training procedure is the same as that mentioned in the last sub-section. Table 6 and Table 7 
present the translation results of TM, SMT, SMT+, two baselines (Koehn-10 and Model-III), and three 
proposed approaches (Model-III+, Distinguishing and Adaptation). The Adaptation approach means 
that we add two consistent related features based on Distinguishing Model (Section 3.3). All the for-
mats are the same as that adopted in Table 3 and Table 4. Besides, scores marked by ?&? are signifi-
cantly better than Distinguishing Model. 
Comparing the TM with the SMT, the performance of in-domain TM significantly exceeds that of 
out-domain SMT. Since the fuzzy match intervals are divided according to the TM database, the trans-
lation result of the SMT system at interval [0.8, 0.9) even slightly outperforms that at interval [0.9, 
1.0). Besides, adding TM phrase-pairs significantly improves the translation results when the fuzzy 
match score is above 0.5 (SMT vs. SMT+, and Model-III vs. Model-III+). Furthermore, the benefit of 
utilizing TM information and the benefit of adding TM phrase-pairs are not covered by each other, and 
can be jointly enjoyed. Furthermore, compared with TM, SMT, SMT+ and Model-III, both Model-III+ 
and Distinguishing Model achieve better translation results when the fuzzy match score is above 0.4. 
All observed trends are similar to that in the last sub-section. 
 
404
   #Sentences #Chn. Words #Chn. VOC. #Eng. Words #Eng. VOC. 
TM Database 261,906 3,623,516 43,112 3,627,028 44,221 
SMT Training Set 404,172 9,007,614 102,073 8,737,801 107,883 
Table 5: Corpus Statistics for Cross-Domain Tests 
Intervals TM SMT SMT+ Model-III Model-III+ Distinguishing Adaptation Koehn-10 
[0.9, 1.0) 81.31 30.87 64.74 + 64.79 82.28 + 83.19 +*$ 84.89 *#$& 81.52 
[0.8, 0.9) 73.25 31.94 60.13 + 61.91 74.21 + 74.72 +* 79.78 *#$& 76.47 * 
[0.7, 0.8) 63.62 30.63 51.64 + 51.44 62.94 + 63.32 + 67.74 *$& 67.12 *$& 
[0.6, 0.7) 43.64 28.95 39.94 + 38.28 46.28 +* 46.46 +* 49.49 *$& 48.47 * 
[0.5, 0.6) 27.37 27.61 32.49 + 28.85 34.50 +* 34.87 +* 37.12 *#$& 35.25 * 
[0.4, 0.5) 15.43 27.16 27.35 27.30 # 27.47 # 27.82 # 28.80 *#$& 25.10 
[0.3, 0.4) 8.24 23.85 22.66 23.81 # 22.41 # 22.41 # 22.95 # 20.72 
(0.0, 0.3) 4.13 24.64 24.25 24.24 # 23.65 # 24.12 # 24.31 # 18.79 
(0.0, 1.0) 40.17 28.30 40.59 + 40.47 47.37 +* 47.70 +*#$ 49.79 *#$& 47.09 * 
Table 6: Cross-Domain Translation Results (BLEU). The marks are the same as that in Table 3. Be-
sides, scores marked by ?$? are significantly better  (p < 0.05) than Model-III+, and those marked by 
?&? are significantly better than ?Distinguishing? (?Adaptation? vs. ?Distinguishing?). 
Intervals TM SMT SMT+ Model-III Model-III+ Distinguishing Adaptation Koehn-10 
[0.9, 1.0) 9.79 54.54 27.07 + 27.09 11.81 + 11.01 + 9.58 #$& 13.51 
[0.8, 0.9) 16.21 52.86 29.33 + 28.04 17.13 + 17.47 + 13.80 *#$& 17.29 
[0.7, 0.8) 27.79 52.42 36.48 + 35.56 27.07 + 26.40 +$ 23.04 *$& 24.31 *$& 
[0.6, 0.7) 46.40 54.74 47.39 + 48.06 41.13 +* 40.36 +*$ 37.45 *#$& 40.16 * 
[0.5, 0.6) 62.59 57.18 53.08 + 56.78 51.77 +* 51.60 +* 48.08 *#$& 51.57 
[0.4, 0.5) 73.93 57.19 56.57 57.19 # 56.82 # 56.53 # 54.42 *#$& 61.32 
[0.3, 0.4) 79.86 60.62 61.16 61.35 # 61.31 # 61.31 # 60.33 #$& 68.82 
(0.0, 0.3) 85.31 63.62 62.81 62.22 # 63.04 # 62.07 # 61.87 # 74.85 
(0.0, 1.0) 50.51 56.42 46.89 + 47.38 # 41.63 +*# 41.27 +*#$ 38.87 *#$& 43.95 * 
Table 7: Cross-Domain Translation Results (TER). The marks are the same as that in Table 6. 
However, both Model-III+ and Distinguishing Model are worse than Koehn-10 at some high fuzzy 
match intervals. The reason is that the TM factors are trained on the news domain but the test set is 
from computer technical domain. Therefore, it is not strange that the Adaptation approach achieves the 
best translation results at all intervals in either BLEU or TER when the fuzzy match score is above 0.4. 
At most intervals, the Adaptation approach significantly outperforms Koehn-10 in either BLEU or 
TER, especially for the high fuzzy match intervals such as [0.9, 1.0) and [0.8, 0.9). Furthermore, the 
Adaptation approach achieves better TER than the TM system and Koehn-10 at intervals [0.9, 1.0) and 
[0.8, 0.9). All obtained results have shown that the Adaptation approach is effective and robust for 
cross-domain test. Moreover, it can be seen that the h1 feature (mentioned in Section 3.3) is more ef-
fective than the h2 feature. 
5 Related Work 
According to the way of combination, those previous works can be classified into four categories (as 
specified in Section 1). The first category uses a classifier (or a re-ranker) to judge whether TM or 
SMT gives a better translation sentence, and then delivers the better one to the post-editor (He et al., 
2010a; He et al., 2010b; Dara et al., 2013). Since the outputs of SMT and TM are not merged but only 
re-ranked, the possible improvement resulted from those approaches is quite limited. 
The second category incorporates TM matched parts into the SMT input sentence in a pipelined 
manner (Koehn and Senellart, 2010; Zhechev and van Genabith, 2010; He et al., 2011; Ma et al., 
2011). These approaches usually translate the sentence in two stages: (1) first determine whether the 
405
extracted TM sentence pair should be adopted or not, and then merge the relevant translations of 
matched parts into the input sentence; (2) then force the SMT system to only translate those un-
matched parts at decoding. There are three drawbacks for this kind of pipeline approaches (Wang et al., 
2013). Firstly, whether those matched parts should be adopted or not is determined at the sentence lev-
el. Secondly, they select only one TM target phrase before decoding. Thirdly, they do not utilize the 
SMT probabilistic information for the matched parts. 
The third category mainly adds the longest matched TM phrase pairs into the SMT phrase table 
(Bi?ici and Dymetman, 2008; Simard and Isabelle, 2009), and associates them with a fixed large prob-
ability value to favor the TM target phrase. However, they only add one aligned target phrase for each 
matched source phrase and did not distinguish the original and the newly-added phrase-pairs. 
The last category incorporates the associated TM information of each source phrase into the SMT 
during decoding (Wang et al., 2013). This category can avoid the drawbacks of the pipeline approach-
es, and thus achieves superior results when the TM database and the SMT training set are the same. 
However, they only refer to the TM information and do not regard the TM phrase-pairs as candidates 
during decoding. Therefore, the superiority of this approach disappears when the TM database and the 
SMT training set are different, because many TM phrase-pairs cannot be found in the original SMT 
phrase table in this case. 
Our approach combines the strength of both the third and the last categories. During decoding, the 
associated TM information is referred to re-score the SMT candidates. At the same time, all matched 
TM phrase-pairs are dynamically merged into the phrase table. Moreover, this is the first unified 
framework for integrating TM into SMT at decoding when the TM database and the SMT training set 
are different. Although some previous works of the second and third categories can be also applied 
when the TM database and the SMT training set are different, they did not explicitly focus on and test 
this case.  
Last, since the example-based machine translation (EBMT, [Nagao, 1984]) is similar to that of us-
ing TM, some approaches (Watanabe and Sumita, 2003; Smith and Clark, 2009; Dandapat et al., 2011; 
2012; Phillips, 2011) also combined EBMT with SMT. It would be interesting to compare our ap-
proaches with theirs in the future. 
6 Conclusion 
Combining TM and SMT can greatly improve the translation performance and reduce human post-
editing effort. In comparison with those previous approaches, our work makes the following contribu-
tions: 
(1) Dynamically merge the matched TM phrase-pairs into the SMT phrase table to meet the real ap-
plication;  
(2) Propose an improved integrated model to distinguish the original SMT phrase-pairs from the 
newly-added ones extracted from TM;  
(3) Adopt a simple but effective TM adaptation method to favor the consistent translation in cross-
domain test. 
This is the first work adopting a unified framework to integrate the TM information into the SMT 
model during decoding when the TM database and the SMT training set are different. On the TM da-
tabase which consists of Chinese?English computer technical documents, our experiments have shown 
that merging the TM phrase-pairs achieves significant improvements when the fuzzy match score is 
above 0.5. Furthermore, the proposed approaches are significantly better than either the SMT or the 
TM systems for both the in-domain and the cross-domain tests. Last, the proposed approaches outper-
form previous works significantly in all test conditions. 
Acknowledgements 
This research work was partially funded by the Natural Science Foundation of China under Grant No. 
61333018, the Hi-Tech Research and Development Program (?863? Program) of China under Grant 
No. 2012AA011101, the Key Project of Knowledge Innovation Program of Chinese Academy of Sci-
ences under Grant No. KGZD-EW-501, and Toshiba (China) R&D Center. 
 
406
Reference 
Ergun Bi?ici and Marc Dymetman. 2008. Dynamic translation memory: using statistical machine translation to 
improve translation memory fuzzy matches. In Proceedings of the 9th International Conference on Intelligent 
Text Processing and Computational Linguistics (CICLing 2008), pages 454?465. 
Nicola Cancedda. 2012. Private Access to Phrase Tables for Statistical Machine Translation. In Proceedings of 
the 50th Annual Meeting of the Association for Computational Linguistics (ACL), pages 23?27. 
Stanley F. Chen and Joshua Goodman. 1998. An empirical study of smoothing techniques for language modeling. 
Technical Report TR-10-98, Harvard University Center for Research in Computing Technology. 
Chiang, David. 2005. A hierarchical phrase-based model for statistical machine translation, In Proceedings of 
the 43rd Annual Meeting on Association for Computational Linguistics (ACL), pages 263?270. 
Sandipan Dandapat, Sara Morrissey, Andy Way, and Mikel L Forcada. 2011. Using example-based MT to sup-
port statistical MT when translating homogeneous data in resource-poor settings, In Proceedings of the 15th 
Annual Meeting of the European Association for Machine Translation (EAMT 2011), pages 201?208. 
Sandipan Dandapat, Sara Morrissey, Andy Way, and Joseph Van Genabith. 2012. Combining EBMT, SMT, TM 
and IR technologies for quality and scale, In Proceedings of the Joint Workshop on Exploiting Synergies be-
tween Information Retrieval and Machine Translation (ESIRMT) and Hybrid Approaches to Machine Trans-
lation (HyTra), pages 48?58. 
Aswarth Dara, Sandipan Dandapat, Declan Groves, and Josef van Genabith. TMTprime: a recommender system 
for MT and TM integration. In Proceedings of the NAACL HLT 2013 Demonstration Session, pages 10?13. 
Yifan He, Yanjun Ma, Josef van Genabith and Andy Way, 2010a. Bridging SMT and TM with translation rec-
ommendation. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics 
(ACL), pages 622?630. 
Yifan He, Yanjun Ma, Andy Way, and Josef Van Genabith. 2010b. Integrating N-best SMT outputs into a TM 
system, In Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), 
pages 374?382. 
Yifan He, Yanjun Ma, Andy Way and Josef van Genabith. 2011. Rich linguistic features for translation memory-
inspired consistent translation. In Proceedings of the Thirteenth Machine Translation Summit, pages 456?463. 
Reinhard Kneser and Hermann Ney. 1995. Improved backing-off for m-gram language modeling. In Proceed-
ings of the IEEE International Conference on Acoustics, Speech and Signal Processing, pages 181?184. 
Philipp Koehn. 2004. Statistical significance tests for machine translation evaluation. In Proceedings of the 2004 
Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 388?395, Barcelona, 
Spain. 
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke 
Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer and Ond?ej Bojar. 2007. Moses: Open source 
toolkit for statistical machine translation. In Proceedings of the ACL 2007 Demo and Poster Sessions, pages 
177?180. 
Philipp Koehn, Franz Josef Och and Daniel Marcu. 2003. Statistical phrase-based translation. In Proceedings of 
the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on 
Human Language Technology, pages 48?54. 
Philipp Koehn and Jean Senellart. 2010. Convergence of translation memory and statistical machine translation. 
In AMTA Workshop on MT Research and the Translation Industry, pages 21?31. 
Liu, Qun and Hongmei Zhao. 2009. Report on CWMT2009 MT Translation Evaluation. In Proceedings of the 
5th China Workshop on Machine Translation (CWMT2009), pages 1?31, Nanjing, China. 
Yanjun Ma, Yifan He, Andy Way and Josef van Genabith. 2011. Consistent translation using dis-criminative 
learning: a translation memory-inspired approach. In Proceedings of the 49th Annual Meeting of the Associa-
tion for Computational Linguistics (ACL), pages 1239?1248, Portland, Oregon. 
Makoto Nagao, 1984. A framework of a mechanical translation between Japanese and English by anal-ogy prin-
ciple. In: Banerji, Alick Elithorn and  Ran-an (ed). Artifiical and Human Intelligence: Edited Review Papers 
Presented at the International NATO Symposium on Artificial and Human Intelligence. North-Holland, Am-
sterdam, 173?180. 
407
Franz Josef Och. 2003. Minimum error rate training in statistical machine translation. In Proceedings of the 41st 
Annual Meeting of the Association for Computational Linguistics (ACL), pages 160?167. 
Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. 
Computational Linguistics, 29 (1). pages 19?51. 
Kishore Papineni, Salim Roukos, Todd Ward and Wei-Jing Zhu. 2002. BLEU: a method for automatic evalua-
tion of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Computational 
Linguistics (ACL), pages 311?318. 
Aaron B. Phillips, 2011. Cunei: open-source machine translation with relevance-based models of each transla-
tion instance. Machine Translation, 25 (2). pages 166-177. 
Michel Simard and Pierre Isabelle. 2009. Phrase-based machine translation in a computer-assisted translation 
environment. In Proceedings of the Twelfth Machine Translation Summit (MT Summit XII), pages 120?127. 
James Smith and Stephen Clark. 2009. EBMT for SMT: a new EBMT-SMT hybrid. In Proceedings of the 3rd 
International Workshop on Example-Based Machine Translation (EBMT'09), pages 3?10, Dublin, Ireland. 
Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea Micciulla and John Makhoul. 2006. A study of transla-
tion edit rate with targeted human annotation. In Proceedings of Association for Ma-chine Translation in the 
Americas (AMTA-2006), pages 223?231. 
Andreas Stolcke. 2002. SRILM-an extensible language modeling toolkit. In Proceedings of the International 
Conference on Spoken Language Processing, pages 311?318. 
Taro Watanabe, Eiichiro Sumita. 2003. Example-based decoding for statistical machine translation, In Proceed-
ing of Machine Translation Summit IX, pages 410?417. 
Kun Wang, Chengqing Zong and Keh-Yih Su, 2013. Integrating translation memory into phrase-based machine 
translation during decoding. In Proceedings of the 51st Annual Meeting of the Association for Computational 
Linguistics (ACL), pages 11?21. 
Ventsislav Zhechev and Josef van Genabith. 2010. Seeding statistical machine translation with translation 
memory output through tree-based structural alignment. In Proceedings of the 4th Workshop on Syntax and 
Structure in Statistical Translation, pages 43?51. 
Xiaoning Zhu, Zhongjun He, Hua Wu, Haifeng Wang, Conghui Zhu, and Tiejun Zhao. 2013. Improving pivot-
based statistical machine translation using random walk. In Proceedings of the 2013 Conference on Empirical 
Methods in Natural Language Processing, pages 524?534. 
408
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 656?666, Dublin, Ireland, August 23-29 2014.
Knowledge Sharing via Social Login: Exploiting Microblogging Service
for Warming up Social Question Answering Websites
Yang Xiao
1
, Wayne Xin Zhao
2
, Kun Wang
1
and Zhen Xiao
1
1
School of Electronics Engineering and Computer Science, Peking University, China
2
School of Information, Renmin University of China, China
{xiaoyangpku, batmanfly}@gmail.com
{wangkun, xiaozhen}@net.pku.edu.cn
Abstract
Community Question Answering (CQA) websites such as Quora are widely used for users to get
high quality answers. Users are the most important resource for CQA services, and the awareness
of user expertise at early stage is critical to improve user experience and reduce churn rate.
However, due to the lack of engagement, it is difficult to infer the expertise levels of newcomers.
Despite that newcomers expose little expertise evidence in CQA services, they might have left
footprints on external social media websites. Social login is a technical mechanism to unify
multiple social identities on different sites corresponding to a single person entity. We utilize the
social login as a bridge and leverage social media knowledge for improving user performance
prediction in CQA services. In this paper, we construct a dataset of 20,742 users who have
been linked across Zhihu (similar to Quora) and Sina Weibo. We perform extensive experiments
including hypothesis test and real task evaluation. The results of hypothesis test indicate that
both prestige and relevance knowledge on Weibo are correlated with user performance in Zhihu.
The evaluation results suggest that the social media knowledge largely improves the performance
when the available training data is not sufficient.
1 Introduction
One of the main challenges for social startup websites is how to gain a considerable number of users
quickly. A growing number of social startups outsource sign-up process to existing social networking
services. They allow users to log in to the services using their existing social media accounts. For exam-
ple, Quora allows users to log in with their Google, Twitter or Facebook accounts based on the OpenID
technology. Lots of startup web services benefit from the huge number of users and rich relationships
accumulated by social network sites. Social login helps the newborn web services to collect crowds of
users in a short time. Moreover, startup web services can gain reliable profiles through social login. It
also offers a convenient mechanism for users to surf the web using a unified social identity (e.g., Twit-
ter account). For example, by the end of 2013, there are about 600,000 web services including mobile
applications using social login offered by Sina Weibo.
When we go beyond simple import of profiles and consider the general problem of leveraging knowl-
edge from social media, many subtasks arise. One of them is how to incorporate data from social media
and startup web service to better predict user performance. In this paper, we take the largest social based
question answering service Zhihu in China, which closely resembles Quora, as the testbed. Different
from traditional CQA sites such as Baidu Zhidao, Zhihu have more prominent social features, which
supports login with Sina Weibo accounts. Although Zhihu grows quickly and attracts more and more
users, about 85% of the users answer fewer than 10 questions and 60% of the users answer fewer than 4
questions in our dataset, which is a large sample of Zhihu.
Previously, many studies have been proposed to improve expertise ranking on CQA services. Link
analysis based approaches (Jurczyk and Agichtein, 2007; Zhang et al., 2007) exploit the question-
answering relationships to construct a graph and run PageRank or HITS on the graph. Jeon et al. (2006)
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer
are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/
656
propose a method based on the non-textual behaviors. Moreover, co-training model (Bian et al., 2009)
jointly infers answer quality and user expertise. Liu et al. (2011) formalize expertise ranking as a com-
petition game with the insight that the best answerer beats other answerers in the same question thread.
However, the above studies highly rely on the history data, which might not work well for newcomers or
users with few answering records. For startup services, many users may not accumulate sufficient data to
support the reliable estimation for their expertise levels. Indeed, the importance of newcomers has been
noted in related studies, and it has been shown that the effective evaluation of users? performance at an
early stage significantly affects the overall development of QA services (Nam et al., 2009; Sung et al.,
2013).
In this paper, we propose a method that incorporates social media and social startup data to predict
newcomers? performance. This problem is technically challenging due to the heterogeneous charac-
teristics across websites. Given a user, we hypothesize that her capability of contributing high quality
answers is dependent on her prestige and relevance. The more contents a user publishes on an area and
the higher prestige a user has on social media sites, the higher likelihood that user can offer high quality
answers. Thus, the first goal is to precisely measure the relevance between question and a user?s tweets.
Owing to the short question length and noisy tweet content, this problem brings technical challenges.
We make use of user-annotated tags and adopt a translation based model to improve relevance estima-
tion. For prestige, a straightforward way is to use the standard graph based ranking algorithm, however,
Zhihu users have very sparse links on Weibo and the standard PageRank algorithm does not work well
on sparse graphs. To address it, we add virtual links to alleviate the sparsity problem by finding available
paths on a large Weibo graph. Furthermore, we propose a performance biased random walk algorithm
and naturally incorporates Zhihu performance history as the supervised information.
We carefully construct a dataset of 20,742 users who have been linked across Zhihu and Weibo, which
represent the social startup and the social media site respectively. We first conduct Spearman correlation
test for these two hypotheses. Our results have shown that prestige in Weibo has a strong correlation with
overall performance in Zhihu. For the performance in question level, we have found that the relevance
of Weibo contents is also significantly correlated with answer quality in Zhihu. Based on these findings,
we further incorporate the extracted prestige and relevance knowledge into the existing framework for
user performance prediction. To simulate the process of history data accumulation, we also conduct
experiments with the varying observed number of answers. The experiment results suggest that the
borrowed social media knowledge, i.e., prestige and relevance information in Weibo, largely improves
the performance when the available training data is not sufficient. Interestingly, we have found that even
individual prestige feature can achieve very competitive results.
Although our approach is tested on a joint combination of Weibo and Zhihu, it is equally applicable
to other knowledge sharing startup web services. The flexibility of our approach lies in that we identify
two important and general types of knowledge that are easy to leverage from external social media sites.
The rest of this paper is organized as follows. The construction of the dataset collection and the
problem formulation are given in Section 2 and 3 respectively. Section 4 presents the detailed feature
engineering and is followed by the experiment part in Section 5. Finally, the related work and conclusions
are given in Section 6 and 7 respectively.
2 Construction of the Dataset Collection
We focus on a popular social question answering website, Zhihu as the studied service. We select Sina
Weibo, the largest Chinese microblogging service as the external website to help improve user exper-
tise estimation task in Zhihu. We exploit the social login mechanism to identify the same user across
these two platforms: if a user logs in Zhihu with her Weibo account, her Zhihu profile will contain the
corresponding Weibo account link. This approach accurately links users across websites.
Zhihu dataset. Zhihu
1
is a social based question answering site in China, which is similar to Quora in
terms of overall design and service. Zhihu has three major components: users, questions, and topics.
Users on Zhihu can ask and answer questions, furthermore, they can comment on or vote for answers.
1
http://www.zhihu.com
657
Each question is usually assigned with a small set of topic tags by the asker and opens a discussion thread
consisting of candidate answers. Topics are represented as tags and organized in a directed acyclic graph
where a child topic can have multiple parent topics.
Zhihu was founded in January 2011, and we obtain the data between January 2011 and November
2013 via a Web crawler. The dataset contains 266,672 users, 819,125 questions and 2,730,013 answers.
These questions are associated with 44,333 topic tags. Since the aim is to examine whether knowledge
extracted from Weibo is helpful to improve tasks in Zhihu, we only keep the users who explicitly use
social login and get 136,002 cross-site users, which roughly covers 50% of the users in our dataset. For
a robust evaluation, we further remove users who have answered fewer than ten questions. Finally, we
obtain a total of 20,742 users and summarize the data statistics in Table 1.
#users #topics #questions #answers
20,742 44,333 335,145 883,373
Table 1: Basic statistics of Zhihu dataset for linked users.
Weibo Dataset. Sina Weibo is the largest Chinese microblogging service which has about 500 million
registered users by the end of 2012. We have crawled all the detailed information of these 20,742 linked
users, including tweets, followers, and following links. These users are indeed active on Weibo and have
posted 21,121,955 tweets in total. In later sections, we will adopt the PageRank algorithm to estimate the
prestige scores of these linked users, thus we need a dense following graph for reliable estimation. By
using these linked users as seeds, we further crawl their followings and followers as well as the following
links between all the crawled users. Finally, we obtain 253,361,449 edges between 1,322,425 users. Note
that we only use these 20,742 linked users for further study, and the rest are only used to help compute
more accurate PageRank scores.
In what follows, we refer to a user who has both a Weibo account and a Zhihu account in our dataset
as a linked user.
3 Problem Formulation
Users are the most valuable resource in community question answering (CQA) services. Discovering
users? expertise at an early stage is important to improve the service quality. A typical task on CQA
services is to predict users? performance or expertise: given a question, it aims to estimate the user
expertise level and identify experts who can provide good answers to this question.
Borrowing the ideas from information retrieval, we solve the performance prediction task via the
learning to rank framework (Liu, 2009). Formally, we assume that there are a set of m questions (i.e.,
queries)Q = {q
(1)
, q
(2)
, q
(3)
, ...q
(m)
}. A question is associated with a set of n
(i)
answers {a
(i)
1
, ..., a
(i)
n
(i)
}
provided by n
(i)
users {u
(i)
1
, ..., u
(i)
n
(i)
} respectively. For each user, let y
(i)
j
denote the performance score
of user u
(i)
j
with respect to query q
(i)
. A higher value of y
(i)
j
indicates better performance for query
q
(i)
. In our work, we instantiate the performance score by the number of votes that a user receives on a
question. A feature vector x
(i)
j
is constructed based on a pair of question and user (q
(i)
, u
(i)
j
). The aim
of the learning task is to derive a ranking function f such that, for each feature vector x
(i)
j
, it outputs a
prediction score f(x
(i)
j
) for the performance of user u
(i)
j
on the question q
(i)
. With this function, when a
new question comes, we can predict who will be competent at it.
For prediction tasks, the answer information {a
(i)
1
, ..., a
(i)
n
(i)
} is not available during training. Besides
users? accumulated history data on Zhihu, external knowledge from Weibo is available to help construct
the query-user feature vector. We assume that the studied Zhihu users have already been linked to
the corresponding Weibo accounts, and we can obtain their Weibo information, including tweets and
followings/followers. The key of the learning to rank framework is how to derive effective features. In
our task, we consider two types of features, i.e., Zhihu features and Weibo features. Our focus in this
paper is how to leverage microblogging information for improving CQA service, i.e., how to incorporate
knowledge from Weibo as features into the learning to rank framework.
658
4 Feature Engineering
In this section, we discuss how to derive effective features from both Zhihu and Weibo. In particular, we
mainly study how to leverage Weibo knowledge for the current task.
4.1 Weibo features
In our work, we focus on two types of Weibo features: prestige and relevance. For prestige, it aims to
capture the social status of a user. In our setting, it refers to the status or authority level of a user on online
social networks (Anderson et al., 2012). We hypothesize that a user is likely to have similar status levels
across multiple online communities, thus the prestige scores of Zhihu users can be roughly estimated
based on the rich link information of Weibo. The second type of knowledge we consider is relevance.
A user is more likely to be an expert on an area that she is interested in, and Weibo provides a good
platform to identify users? interests. Since Weibo and Zhihu are text based websites, we hypothesize that
a user will show similar interests on these two medias.
Prestige. Prestige features aim to capture the status of one user. Status characteristic theory posits
that one with higher status characteristic is expected to perform better in the group task (Oldmeadow
et al., 2003). Prestige estimation has been a classical problem in both web graph analysis and social
networking analysis (Easley and Kleinberg, 2012). We are motivated by previous study on authority
ranking in Twitter (Kwak et al., 2010), which utilizes the following relations as the evidence of authority.
A straightforward way is to run standard PageRank algorithm on the Weibo subgraph consisting of these
20,742 linked users. However, the subgraph of these linked users is very sparse, each linked user has
only about 5 out-links to other linked users on average. Such a sparse graph will not produce meaningful
ranking results.
Our solution is to add virtual links between linked users. Let N (N = 20, 742) denote the number
of linked users andM
N?N
denote the transition matrix based on the graph of these linked users. Given
two users u
i
and u
j
, we check whether there is a directed path between them on our large Weibo graph.
Recall that we have 253,361,449 edges between 1,322,425 users in Weibo dataset. We run the breadth-
first search algorithm to find the shortest path between two linked users. If there exists a directed path
between two linked users, we add a virtual link between them and set the weight to the reciprocal of the
shortest path length, i.e., I(i, j) =
1
len(u
i
?u
j
)
, where len(u
i
? u
j
) denotes the length of the shortest
path between u
i
and u
j
. In this way, we have M
ij
=
I(i,j)?
k
I(i,k)
. By adding virtual links, we obtain a more
dense graph of these linked users. Formally, the standard PageRank algorithm (Brin and Page, 1998) can
be formulated as:
r
(n+1)
= ? ?M
T
? r
(n)
+ (1? ?) ? y (1)
where ? is the damping factor usually set to 0.85 and y is the restart probability vector usually set to be
uniform (Yan et al., 2012). When the algorithm converges, we can obtain the stationary distribution of
users (i.e., r) as the prestige scores.
The above method assumes that users have same restart probability, which may not be true in reality.
Since we are considering improving Zhihu service quality, we incorporate users? history data from Zhihu
as supervised information. The main idea is that instead of using a uniform restart distribution y, we use
a performance biased restart distribution in Eq. 1. We set the restart probability of a user to her average
vote ratio based on the questions she has answered. Formally, we set y
u
= Average(
?
q
#vote(q,u)?
v
#vote(q,v)
),
where #vote(q, u) denotes the number of votes user u receives on question q and
?
v
#vote(q, v)
denotes the total number of votes that all users receive on question q. We do not use other measures such
as best answer ratio because we assume that the history window is very limited and our proposed method
provides more robust estimation. Let us further explain the idea. At the beginning of each iteration,
each user is assigned to her performance score estimated based on Zhihu data: the more competent she
is, the larger score she has. During the iteration, each user begins to collect authority evidence from
her incoming neighbors on the Weibo graph. The final score is indeed a trade-off between her own
performance on Zhihu and her authority on Weibo.
659
There are also other measures to consider, e.g., the follower number and the times of being retweeted.
In our experiments, we have tried these variants and found that no one is more effective than the above
method.
Relevance. Intuitively, a user is more likely to be an expert on an area that she is interested in. In
the setting of Zhihu, a user tends to perform better on the topics that are more relevant to her interests.
Status characteristic theory also conveys that task relevance is an important factor which affects one?s
performance (Oldmeadow et al., 2003). Weibo provides a good platform to infer users? interests, which
is helpful to derive relevance scores.
We formulate relevance estimation as an information retrieval task. Let V denote a term vocabu-
lary and w denote a word in V . Note that we take the union of the Weibo vocabulary and Zhihu vo-
cabulary. The interest of a user u is modeled as a multinomial distribution over the terms in V , i.e.,
?
u
= {?
u
w
}
w?V
. Given a question q, we also model it as a multinomial distribution over the terms in V ,
i.e., ?
q
= {?
q
w
}
w?V
. Following (Zhai, 2008), the relevance score between question q and user u can be
estimated by the negative Kullback-Leibler divergence between ?
q
and ?
u
:
Rel(q, u) = ?KL(?
q
, ?
u
) = ?
?
w?V
p(w|?
q
) log
p(w|?
q
)
p(w|?
u
)
(2)
We first estimate ?
q
. The straightforward way is to estimate ?
q
based on the question text. However, the
question text is usually short and noisy, which does not yield good results in our experiments. Recall a
question is associated with a small set of user-annotated topic tags, and tags are good semantic indicators
of the question. A topic tag usually indexes a considerable amount of questions, and we can use tags to
leverage semantics from the indexed questions. Formally, we adopt the translation based model (Zhai,
2008) to estimate the question model:
?
q
w
?
?
t?q
p(w, t|q) =
?
t?q
p(w|t)p(t|q) (3)
where p(w|t) is the translation probability from a tag to a term, and p(t|q) is the empirical distribution of
tag t in question q. Here we make an independent assumption: given a tag, the question is independent
of a word, i.e., p(w|t, q) = p(w|t). The procedure can be interpreted as follows: sample a tag from the
question and then compute the probability of translating the tag into a specific word. We estimate the tag-
term translation probability as p(w|t) =
#(w,t)+1?
w
?
?V
#(w
?
,t)+|V|
, where #(w, t) denotes the term frequency
of w in the question text that tag t indexes. We use the additive-one smoothing.
We also try to incorporate the question text into the above estimation formula. However, it does
not result in any improvement. The main reason is that the question words may be too specific, as
a comparison, tags provide a general level of semantics, which is more effective to identify expertise
areas.
Next, we estimate user interest model ?
u
. We consider aggregating all the tweets of a user as a ?doc-
ument?, and then estimate the document-term probability as ?
u
w
=
#(w,u)+1?
w
?
?V
#(w
?
,u)+|V|
, where #(w, u)
denotes the term frequency of w in the aggregated document of user u.
4.2 Zhihu features
Now we describe the features extracted from Zhihu, and we refer to them as baseline features since we
take the performance of them as a base reference. We summarize these features in Table 2.
These features have been extensively tested to be very effective by previous related studies (Song et
al., 2010; Liu et al., 2011), which represent the state-of-art of the current task.
Summary. We have considered two general types of knowledge in social media which are potential to
improve user expertise estimation in Zhihu. It is easy to see that our approach can be equally applicable
to other third-party websites which is text based and contain manually annotated tags.
660
Features Abbr Formulas
Number of Best Answers NBA ?
Number of Answers NA ?
Number of Received Votes NV ?
Average Number of Votes AVA ?
Smoothed Average number of Votes SAVA SAVA(u) =
?
q
?(v(q,u))
NA(u)
, ?(x) =
1
1+e
(?x)
Best Answer Ratio BAR BAR(u) =
NBA(u)
NA(u)
Smoothed Best Answer Ratio SBAR SBAR(u) =
BAR(u)?NA(u)+BAR
avg
?NA
avg
NA
avg
+NA(u)
Average Answer Length AAL ?
Table 2: List of baseline features with corresponding abbreviations and formulas. Here u denotes a Zhihu
user.
5 Experiment
Questions with fewer than five answerers do not receive much attention, and we only keep questions
which involve at least six users. In this way, we have obtained a total of 25,262 questions. The number
of votes is used as the measure of answer quality. The question threads are sorted by the post time, and
we can simulate the cold-start phenomenon to examine the performance of different methods. We split
the dataset into a training set and a test set by question threads with the ratio of 3:1. The ?history? data
of a user is put into the training set and the rest is treated as test data. We further vary the size of ?history
data? that can be used for performance prediction in three levels, i.e., at most 3, 5, and 10 ?historical?
question threads have been observed for a given user.
5.1 Hypothesis Testing
In this part, we first examine the fundamental hypotheses of our work: whether Weibo knowledge is
potentially effective to improve the performance of tasks in Zhihu. We conduct significance test to
examine the correlation between user features extracted from Weibo and user performance in Zhihu. We
adopt the Spearman?s rank correlation coefficient as the test measure. For a sample of size n, the n raw
scores X
i
, Y
i
are converted to ranks x
i
, y
i
, and the Spearman correlation coefficient ? is computed as
? = 1 ?
6
?
i
d
2
i
n(n
2
?1)
, where d
i
= x
i
? y
i
. The Spearman?s coefficient ? lies in the interval [?1, 1], and a
value of ?+1? or ?-1? indicates a perfect, positive or negative Spearman correlation.
Test of prestige. In our test, the overall performance of a user is estimated by the average vote counts she
receives per answer, and the prestige level of a user is estimated by her PageRank score on the original
Weibo following graph with a uniform restart probability. With these two measures, it is straightforward
to generate two rankings of users, either by user prestige level or by user performance. However, it is
noted that ? is usually very sensitive when the sample size is too large, and it is difficult to obtain robust
correlation values in this case. To better capture the overall correlation patterns, we group users according
to their prestige levels and examine the correlation degree in the group level. We sort users according to
their PageRank scores in a descending order, and split users equally into 100 buckets. The correlation
value between performance and PageRank is ? = 0.5617 at the significance level of 9.879e
?10
, which
indicates there is a strong correlation between performance and prestige.
Test of relevance. Different from prestige, relevance is defined to be question specific, so we cannot
perform global correlation analysis. We perform the correlation analysis in the question level. For
each question, we have two rankings of involved users: the relevance ranking and the question-specific
performance ranking. Let ? denote the correlation coefficient between the relevance ranking and the
performance ranking for a given question. Formally, given a question, we have the null hypothesis H
0
being ?? is zero?, whereas H
1
being ?? is not zero?. If H
0
is rejected, we can conclude that prestige
in Weibo is correlated with users? performance on Zhihu for the given question. Our experiments have
shown that 14.48% of the questions rejected the H
0
hypothesis at the confidence level of 0.9.
661
5.2 Evaluation metrics
In the above, we have shown that prestige and relevance knowledge extracted from Weibo are correlated
with user performance in Zhihu. Next we are going a step further to examine the feasibility of using
these external features to improve user performance ranking in CQA service. In this paper, we consider
studying this problem in two aspects: in the first case, we only focus on the user who provides the best
answer; while in the second case, we focus on the overall ranking of all engaged answerers in a given
question thread. By following previous studies (Song et al., 2010; Deng et al., 2012), we adopt traditional
evaluation metrics in information retrieval for evaluating user performance prediction in CQA services.
Best answer prediction. Our first task is to predict which user will provide the best answer given a
question. The user who has received the maximum vote counts in a question thread will be labeled as
relevant and the rest will be treated as non-relevant. Then we can adopt the widely used relevance metrics
Precision at rank n (P@n) and Mean Reciprocal Rank (MRR).
Top expert recommendation. Unlike best answer prediction, top expert recommendation aims to pro-
vide a short list of candidate experts given a question. By following the study (Liu et al., 2011), we use
nDCG (normalized Discounted Cumulative Gain) as the evaluation metrics. Let vote(i) denote the vote
counts of the answer ranked at i in a system output. To reduce the effects of large outliers, we set the
gain value for an answer with the vote counts v to be log(v + 1). The metrics are formally defined as
follows:
DCG@n =
n
?
i=1
log(vote(i) + 1)
log(i+ 1)
(4)
maxDCG@n =
n
?
i=1
log(vote
?
(i) + 1)
log(i+ 1)
(5)
nDCG@n =
DCG@n
maxDCG@n
(6)
where vote
?
denotes the vote counts list of the ideal ranking system, i.e., the answer list is sorted by vote
counts in the descending order.
Similar to query-specific information retrieval tasks, all our experiments are question specific. For
a system, we evaluate its performance of each question and then average all the results as the final
performance.
5.3 Results
As studied in Section 3, the above two tasks can be formulated as the learning to rank problem. Following
previous work (Song et al., 2010), we adopt SVMRank as the ranking model and implement SVMRank
using the tool package SVMLight
2
. We use the linear kernel for SVMRank, and report the results in
Table 3 and Table 4.
We refer to the system with all Zhihu features as Baseline. We use two ways to compute prestige
features: P+UniformG denotes the system which implements the standard PageRank algorithm with
uniform restart probability, while P+HisG denotes the system which implements the biased PageRank
algorithm with users? history performance on Zhihu as the restart probability. Rel denotes the system
with only relevance features and Baseline+Weibo denotes the system with all the features.
Analysis of baseline results. The baseline system is built with all Zhihu features, which are estimated
using history data, and it is natural to see that the performance of the baseline system improves with
the increasing of the history data. Recall that all the question threads in our dataset contain more than
six answers, indeed, 36.3% of them contain more than ten answers. A random algorithm to guess the
best answer can only achieve a poor P@1 value of 11.07%. Results in Table 3 and Table 4 show that
our baseline is competitive even on long question threads. In our experiments, the system performance
begins to stay stable when the history window is set to ten question threads since quite a few users have
engaged in fewer than ten question threads.
2
http://svmlight.joachims.org
662
History Window Size Systems NDCG@1 NDCG@3 NDCG@5
NULL P+UniformG 0.510 0.555 0.621
Rel. 0.360 0.434 0.519
?3 question threads (B)aseline 0.508 0.582 0.656
P+HisG 0.550 0.596 0.658
B.+Weibo 0.580 0.617 0.676
vs. B.
+14.17%
??
+6.01%
???
+3.05%
???
?5 question threads (B)aseline 0.509 0.578 0.658
P+HisG 0.556 0.603 0.668
B.+Weibo 0.589 0.625 0.687
vs. B.
+15.72%
???
+8.13%
???
+4.41%
???
?10 question threads (B)aseline 0.534 0.602 0.671
P+HisG 0.568 0.616 0.679
B.+Weibo 0.595 0.637 0.696
vs. B.
+11.42% +5.81% +3.73%
?
Table 3: Overall ranking performance with varying history window sizes. ?*?, ?**?, ?***? indicate the
improvement is significant at the level of 0.1, 0.05 and 0.01 respectively.
Analysis of the effect of Weibo features. We now incorporate Weibo features and check whether they
can help improve the system performance. In Table 3 and Table 4, we present the improvement ratios
over baselines with the incorporation of Weibo features. We can see that Weibo features yield a large
improvement over the baseline system, especially when the size of history window is small, i.e., ?3
question threads. This indicates the effectiveness of Weibo features on alleviating the cold-start problem
in Zhihu. When we have more history data, i.e.,?10 question threads, the improvement becomes smaller.
It is noteworthy that the single prestige feature (i.e., P+UniformG and P+HisG) achieves good per-
formance. Especially, P+HisG obtains very competitive results compared with the baseline system.
P+HisG naturally combines history data on Zhihu and prestige information on Weibo, which largely
improves the standard prestige estimation method P+UniformG. As a comparison, the relevance feature
is not that effective but still improves the overall performance a bit. These findings indicate that the
incorporation of social media data can be a very promising way to improve the tasks of startup services.
History Window Size Systems MRR P@1 P@3
NULL P+UniformG 0.457 0.261 0.544
Rel. 0.353 0.157 0.404
?3 question threads (B)aseline 0.474 0.263 0.589
P+HisG 0.498 0.303 0.604
B.+Weibo 0.516 0.323 0.624
vs. B.
+8.86%
???
+22.81%
??
+5.94%
??
?5 question threads (B)aseline 0.478 0.271 0.590
P+HisG 0.501 0.303 0.613
B.+Weibo 0.521 0.327 0.627
vs. B.
+9.00%
???
+20.66%
???
+6.27%
???
?10 question threads (B)aseline 0.494 0.286 0.612
P+HisG 0.514 0.316 0.627
B.+Weibo 0.530 0.332 0.643
vs. B.
+7.29% +16.08% +5.07%
Table 4: Best answer prediction performance with varying history window sizes. ?*?, ?**?, ?***?
indicate the improvement is significant at the level of 0.1, 0.05 and 0.01 respectively.
663
6 Related Work
Our task is built on community question and answering site and researchers have studied CQA from
many perspectives. One perspective focuses on user expertise estimation. Generally, there are two prin-
ciple methods for expertise ranking, interaction graph analysis and interest modeling. Interaction graph
based methods (Jurczyk and Agichtein, 2007; Zhang et al., 2007) construct a graph using interaction(e.g.,
asking and answering) behavior, and rank users using some generalization of PageRank (Brin and Page,
1998) or HITS (Kleinberg, 1999). Interest modeling methods characterize users? interests using ques-
tion category (Guo et al., 2008) or latent topic modeling (Liu et al., 2005). There are also methods that
combine both interest modeling and graph structure (Zhou et al., 2012; Yang et al., 2013) to rank users.
Another research perspective on question answering service is quality prediction including answer qual-
ity prediction (Harper et al., 2008; Shah and Pomerantz, 2010; Severyn and Moschitti, 2012; Severyn
et al., 2013) and question quality prediction (Anderson et al., 2012). However, since the methods men-
tioned above are based on the history data, the system will experience the cold start problem. Our work
explore to what extent can external features help relieve the problem.
This work is also concerned with mining across heterogeneous social networks. Recently, many re-
searches focus on mapping accounts from different sites to one single identity (Zafarani and Liu, 2013;
Liu et al., 2013; Kong et al., 2013). By utilizing these recent studies on linking users across communities,
our work can be extended to larger scale datasets. From another perspective, cross-domain recommen-
dation has also been widely studied. Zhang et al. (Zhang and Pennacchiotti, 2013a; Zhang and Pennac-
chiotti, 2013b) explore how Facebook profiles can help boost product recommendation on e-commerce
site. Previous work (Zhang et al., 2014) analyze user novelty seeking traits on social network and e-
commerce site, which can be used to personalized recommendation and targeted advertisement. Dif-
ferent from simply borrowing user?s profiles or psychological traits, our work integrates user footprints
from heterogenous social networks and captures performance related characteristics more precisely.
7 Conclusion
In this paper, we take the initiative attempt to leverage social media knowledge for improving the social
startup service. We carefully construct a dataset of 20,742 users who have been linked across Zhihu and
Weibo, which are social startup and external social media websites respectively. We hypothesize that a
user with higher prestige and more relevant Weibo contents to a question is more likely to have better
performance.
We first carefully construct testing experiments for these two hypotheses. Our results indicate that
prestige in Weibo has strong correlation with overall performance in Zhihu. For question specific per-
formance, we have found that relevance between questions and a user?s tweets also correlates with user
performance on Zhihu. Based on these findings, we further add prestige and relevance knowledge into
existing user performance prediction framework. The experiment results show that prestige and rele-
vance information in Weibo largely improve the performance when the available training data is not suf-
ficient. Moreover, individual prestige feature achieves very competitive results. Our approach is equally
applicable to other knowledge sharing web services with appropriate external social media information.
Acknowledgements
The authors would like to thank the anonymous reviewers for their comments. This work is supported
by the National Grand Fundamental Research 973 Program of China under Grant No.2014CB340405
and the National Natural Science Foundation of China (Grant No.61170056). The contact author is Zhen
Xiao.
References
Ashton Anderson, Daniel Huttenlocher, Jon Kleinberg, and Jure Leskovec. 2012. Discovering value from com-
munity activity on focused question answering sites: a case study of stack overflow. In Proceedings of the 18th
ACM SIGKDD international conference on Knowledge discovery and data mining, pages 850?858. ACM.
664
Jiang Bian, Yandong Liu, Ding Zhou, Eugene Agichtein, and Hongyuan Zha. 2009. Learning to recognize reliable
users and content in social media with coupled mutual reinforcement. In Proceedings of the 18th international
conference on World wide web, pages 51?60. ACM.
Sergey Brin and Lawrence Page. 1998. The anatomy of a large-scale hypertextual web search engine. Computer
networks and ISDN systems, 30(1):107?117.
Hongbo Deng, Jiawei Han, Michael R Lyu, and Irwin King. 2012. Modeling and exploiting heterogeneous
bibliographic networks for expertise ranking. In Proceedings of the 12th ACM/IEEE-CS joint conference on
Digital Libraries, pages 71?80. ACM.
David Easley and Jon Kleinberg. 2012. Networks, crowds, and markets: Reasoning about a highly connected
world.
Jinwen Guo, Shengliang Xu, Shenghua Bao, and Yong Yu. 2008. Tapping on the potential of q&a community by
recommending answer providers. In Proceedings of the 17th ACM conference on Information and knowledge
management, pages 921?930. ACM.
F Maxwell Harper, Daphne Raban, Sheizaf Rafaeli, and Joseph A Konstan. 2008. Predictors of answer quality in
online q&a sites. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, pages
865?874. ACM.
Jiwoon Jeon, W Bruce Croft, Joon Ho Lee, and Soyeon Park. 2006. A framework to predict the quality of answers
with non-textual features. In Proceedings of the 29th annual international ACM SIGIR conference on Research
and development in information retrieval, pages 228?235. ACM.
Pawel Jurczyk and Eugene Agichtein. 2007. Discovering authorities in question answer communities by using
link analysis. In Proceedings of the sixteenth ACM conference on Conference on information and knowledge
management, pages 919?922. ACM.
Jon M Kleinberg. 1999. Authoritative sources in a hyperlinked environment. Journal of the ACM (JACM),
46(5):604?632.
Xiangnan Kong, Jiawei Zhang, and Philip S Yu. 2013. Inferring anchor links across multiple heterogeneous
social networks. In Proceedings of the 22nd ACM international conference on Conference on information &
knowledge management, pages 179?188. ACM.
Haewoon Kwak, Changhyun Lee, Hosung Park, and Sue Moon. 2010. What is twitter, a social network or a news
media? In Proceedings of the 19th international conference on World wide web, pages 591?600. ACM.
Xiaoyong Liu, W Bruce Croft, and Matthew Koll. 2005. Finding experts in community-based question-answering
services. In Proceedings of the 14th ACM international conference on Information and knowledge management,
pages 315?316. ACM.
Jing Liu, Young-In Song, and Chin-Yew Lin. 2011. Competition-based user expertise score estimation. In
Proceedings of the 34th international ACM SIGIR conference on Research and development in Information
Retrieval, pages 425?434. ACM.
Jing Liu, Fan Zhang, Xinying Song, Young-In Song, Chin-Yew Lin, and Hsiao-Wuen Hon. 2013. What?s in a
name?: An unsupervised approach to link users across communities. In Proceedings of the Sixth ACM Interna-
tional Conference on Web Search and Data Mining, WSDM ?13.
Tie-Yan Liu. 2009. Learning to rank for information retrieval. Foundations and Trends in Information Retrieval,
3(3):225?331.
Kevin Kyung Nam, Mark S Ackerman, and Lada A Adamic. 2009. Questions in, knowledge in?: a study of naver?s
question answering community. In Proceedings of the SIGCHI conference on human factors in computing
systems, pages 779?788. ACM.
Julian Oldmeadow, Michael Platow, Margaret Foddy, and Donna Anderson. 2003. Self-categorization, status, and
social influence. Social Psychology Quarterly, 66(2):138?152.
Aliaksei Severyn and Alessandro Moschitti. 2012. Structural relationships for large-scale learning of answer
re-ranking. In Proceedings of the 35th international ACM SIGIR conference on Research and development in
information retrieval, pages 741?750. ACM.
Aliaksei Severyn, Massimo Nicosia, and Alessandro Moschitti. 2013. Learning adaptable patterns for passage
reranking. CoNLL-2013, page 75.
665
Chirag Shah and Jefferey Pomerantz. 2010. Evaluating and predicting answer quality in community qa. In
Proceedings of the 33rd international ACM SIGIR conference on Research and development in information
retrieval, pages 411?418. ACM.
Young-In Song, Jing Liu, Tetsuya Sakai, Xin-Jing Wang, Guwen Feng, Yunbo Cao, Hisami Suzuki, and Chin-Yew
Lin. 2010. Microsoft research asia with redmond at the ntcir-8 community qa pilot task. In NTCIR-8.
Juyup Sung, Jae-Gil Lee, and Uichin Lee. 2013. Booming up the long tails: Discovering potentially contributive
users in community-based question answering services.
Rui Yan, Mirella Lapata, and Xiaoming Li. 2012. Tweet recommendation with graph co-ranking. In Proceedings
of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume 1, pages
516?525. Association for Computational Linguistics.
Liu Yang, Minghui Qiu, Swapna Gottipati, Feida Zhu, Jing Jiang, Huiping Sun, and Zhong Chen. 2013. Cqarank:
jointly model topics and expertise in community question answering. In Proceedings of the 22nd ACM interna-
tional conference on Conference on information & knowledge management, pages 99?108. ACM.
Reza Zafarani and Huan Liu. 2013. Connecting users across social media sites: a behavioral-modeling approach.
In Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining,
pages 41?49. ACM.
ChengXiang Zhai. 2008. Statistical language models for information retrieval. Synthesis Lectures on Human
Language Technologies, pages 1?141.
Yongzheng Zhang and Marco Pennacchiotti. 2013a. Predicting purchase behaviors from social media. In Pro-
ceedings of the 22nd international conference on World Wide Web, pages 1521?1532. International World Wide
Web Conferences Steering Committee.
Yongzheng Zhang and Marco Pennacchiotti. 2013b. Recommending branded products from social media. In
Proceedings of the 7th ACM conference on Recommender systems, pages 77?84. ACM.
Jun Zhang, Mark S Ackerman, and Lada Adamic. 2007. Expertise networks in online communities: structure and
algorithms. In Proceedings of the 16th international conference on World Wide Web, pages 221?230. ACM.
Fuzheng Zhang, Nicholas Jing Yuan, Defu Lian, and Xing Xie. 2014. Mining novelty-seeking trait across het-
erogeneous domains. In Proceedings of the 23rd international conference on World wide web, pages 373?384.
International World Wide Web Conferences Steering Committee.
Guangyou Zhou, Siwei Lai, Kang Liu, and Jun Zhao. 2012. Topic-sensitive probabilistic model for expert finding
in question answer communities. In Proceedings of the 21st ACM international conference on Information and
knowledge management, pages 1662?1666. ACM.
666
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 11?21,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Integrating Translation Memory into Phrase-Based 
Machine Translation during Decoding 
 
 
Kun Wang?        Chengqing Zong?        Keh-Yih Su? 
?National Laboratory of Pattern Recognition, Institute of Automation, 
Chinese Academy of Sciences, Beijing, China 
?Behavior Design Corporation, Taiwan 
?
{kunwang, cqzong}@nlpr.ia.ac.cn, 
?
kysu@bdc.com.tw 
 
  
 
Abstract 
Since statistical machine translation (SMT) 
and translation memory (TM) complement 
each other in matched and unmatched regions, 
integrated models are proposed in this paper to 
incorporate TM information into phrase-based 
SMT. Unlike previous multi-stage pipeline 
approaches, which directly merge TM result 
into the final output, the proposed models refer 
to the corresponding TM information associat-
ed with each phrase at SMT decoding. On a 
Chinese?English TM database, our experi-
ments show that the proposed integrated Mod-
el-III is significantly better than either the 
SMT or the TM systems when the fuzzy match 
score is above 0.4. Furthermore, integrated 
Model-III achieves overall 3.48 BLEU points 
improvement and 2.62 TER points reduction 
in comparison with the pure SMT system. Be-
sides, the proposed models also outperform 
previous approaches significantly.  
1 Introduction 
Statistical machine translation (SMT), especially 
the phrase-based model (Koehn et al, 2003), has 
developed very fast in the last decade. For cer-
tain language pairs and special applications, 
SMT output has reached an acceptable level, es-
pecially in the domains where abundant parallel 
corpora are available (He et al, 2010). However, 
SMT is rarely applied to professional translation 
because its output quality is still far from satis-
factory. Especially, there is no guarantee that a 
SMT system can produce translations in a con-
sistent manner (Ma et al, 2011). 
In contrast, translation memory (TM), which 
uses the most similar translation sentence (usual-
ly above a certain fuzzy match threshold) in the 
database as the reference for post-editing, has 
been widely adopted in professional translation 
field for many years (Lagoudaki, 2006). TM is 
very useful for repetitive material such as updat-
ed product manuals, and can give high quality 
and consistent translations when the similarity of 
fuzzy match is high. Therefore, professional 
translators trust TM much more than SMT. 
However, high-similarity fuzzy matches are 
available unless the material is very repetitive. 
In general, for those matched segments1, TM 
provides more reliable results than SMT does. 
One reason is that the results of TM have been 
revised by human according to the global context, 
but SMT only utilizes local context. However, 
for those unmatched segments, SMT is more re-
liable. Since TM and SMT complement each 
other in those matched and unmatched segments, 
the output quality is expected to be raised signif-
icantly if they can be combined to supplement 
each other. 
In recent years, some previous works have in-
corporated TM matched segments into SMT in a 
pipelined manner (Koehn and Senellart, 2010; 
Zhechev and van Genabith, 2010; He et al, 2011; 
Ma et al, 2011). All these pipeline approaches 
translate the sentence in two stages. They first 
determine whether the extracted TM sentence 
pair should be adopted or not. Most of them use 
fuzzy match score as the threshold, but He et al 
(2011) and Ma et al (2011) use a classifier to 
make the judgment. Afterwards, they merge the 
relevant translations of matched segments into 
the source sentence, and then force the SMT sys-
tem to only translate those unmatched segments 
at decoding. 
There are three obvious drawbacks for the 
above pipeline approaches. Firstly, all of them 
determine whether those matched segments 
                                                 
1 We mean ?sub-sentential segments? in this work. 
11
should be adopted or not at sentence level. That 
is, they are either all adopted or all abandoned 
regardless of their individual quality. Secondly, 
as several TM target phrases might be available 
for one given TM source phrase due to insertions, 
the incorrect selection made in the merging stage 
cannot be remedied in the following translation 
stage. For example, there are six possible corre-
sponding TM target phrases for the given TM 
source phrase ???4 ?5 ??6? (as shown in 
Figure 1) such as ?object2 that3 is4 associated5?, 
and ?an1 object2 that3 is4 associated5  with6?, etc. 
And it is hard to tell which one should be adopt-
ed in the merging stage. Thirdly, the pipeline 
approach does not utilize the SMT probabilistic 
information in deciding whether a matched TM 
phrase should be adopted or not, and which tar-
get phrase should be selected when we have mul-
tiple candidates. Therefore, the possible im-
provements resulted from those pipeline ap-
proaches are quite limited. 
On the other hand, instead of directly merging 
TM matched phrases into the source sentence, 
some approaches (Bi?ici and Dymetman, 2008; 
Simard and Isabelle, 2009) simply add the long-
est matched pairs into SMT phrase table, and 
then associate them with a fixed large probability 
value to favor the corresponding TM target 
phrase at SMT decoding. However, since only 
one aligned target phrase will be added for each 
matched source phrase, they share most draw-
backs with the pipeline approaches mentioned 
above and merely achieve similar performance. 
To avoid the drawbacks of the pipeline ap-
proach (mainly due to making a hard decision 
before decoding), we propose several integrated 
models to completely make use of TM infor-
mation during decoding. For each TM source 
phrase, we keep all its possible corresponding 
target phrases (instead of keeping only one of 
them). The integrated models then consider all 
corresponding TM target phrases and SMT pref-
erence during decoding. Therefore, the proposed 
integrated models combine SMT and TM at a 
deep level (versus the surface level at which TM 
result is directly plugged in under previous pipe-
line approaches). 
On a Chinese?English computer technical 
documents TM database, our experiments have 
shown that the proposed Model-III improves the 
translation quality significantly over either the 
pure phrase-based SMT or the TM systems when 
the fuzzy match score is above 0.4. Compared 
with the pure SMT system, the proposed inte-
grated Model-III achieves 3.48 BLEU points im-
provement and 2.62 TER points reduction over-
all. Furthermore, the proposed models signifi-
cantly outperform previous pipeline approaches. 
2 Problem Formulation 
Compared with the standard phrase-based ma-
chine translation model, the translation problem 
is reformulated as follows (only based on the 
best TM, however, it is similar for multiple TM 
sentences): 
  (1) 
Where  is the given source sentence to be trans-
lated,  is the corresponding target sentence and  
is the final translation;  
are the associated information of the best TM 
sentence-pair;  and  denote the corre-
sponding TM sentence pair;  denotes its 
associated fuzzy match score (from 0.0 to 1.0); 
 is the editing operations between  and ; 
and  denotes the word alignment between 
 and . 
Let  and  denote the k-th associated 
source phrase and target phrase, respectively. 
Also,  and  denote the associated source 
phrase sequence and the target phrase sequence, 
respectively (total  phrases without insertion). 
Then the above formula (1) can be decomposed 
as below: 
 
(2) 
Afterwards, for any given source phrase , 
we can find its corresponding TM source phrase 
 and all possible TM target phrases (each 
of them is denoted by ) with the help of 
corresponding editing operations  and word 
alignment . As mentioned above, we can 
have six different possible TM target phrases for 
the TM source phrase ??? 4 ? 5 ?? 6?. This 
??0                    ?1  ??2  ??3  ??4  ?5  ??6  ?7
??0  ?1  ??2  ?3  ??4             ??5  ?6  ??7  ?8
gets0  n1  obj ct2  that3  is4  associated5  with6  the7  annotation8  label9  .10
Source
TM Source
TM Target
 
Figure 1: Phrase Mapping Example 
12
is because there are insertions around the directly 
aligned TM target phrase. 
In the above Equation (2), we first segment the 
given source sentence into various phrases, and 
then translate the sentence based on those source 
phrases. Also,  is replaced by , as they 
are actually the same segmentation sequence. 
Assume that the segmentation probability 
 is a uniform distribution, with the corre-
sponding TM source and target phrases obtained 
above, this problem can be further simplified as 
follows: 
 
(3) 
Where  is the corresponding TM phrase 
matching status for , which is a vector consist-
ing of various indicators (e.g., Target Phrase 
Content Matching Status, etc., to be defined lat-
er), and reflects the quality of the given candi-
date;  is the linking status vector of  (the 
aligned source phrase of  within ), and indi-
cates the matching and linking status in the 
source side (which is closely related to the status 
in the target side); also,  indicates the corre-
sponding TM fuzzy match interval specified later.  
In the second line of Equation (3), we convert 
the fuzzy match score  into its correspond-
ing interval , and incorporate all possible com-
binations of TM target phrases. Afterwards, we 
select the best one in the third line. Last, in the 
fourth line, we introduce the source matching 
status and the target linking status (detailed fea-
tures would be defined later). Since we might 
have several possible TM target phrases , 
the one with the maximum score will be adopted 
during decoding. 
The first factor  in the above for-
mula (3) is just the typical phrase-based SMT 
model, and the second factor  (to be 
specified in the Section 3) is the information de-
rived from the TM sentence pair. Therefore, we 
can still keep the original phrase-based SMT 
model and only pay attention to how to extract 
useful information from the best TM sentence 
pair to guide SMT decoding. 
3 Proposed Models 
Three integrated models are proposed to incorpo-
rate different features as follows: 
3.1 Model-I 
In this simplest model, we only consider Target 
Phrase Content Matching Status (TCM) for . 
For , we consider four different features at the 
same time: Source Phrase Content Matching 
Status (SCM), Number of Linking Neighbors 
(NLN), Source Phrase Length (SPL), and Sen-
tence End Punctuation Indicator (SEP). Those 
features will be defined below.  is 
then specified as: 
 
All features incorporated in this model are speci-
fied as follows: 
TM Fuzzy Match Interval (z): The fuzzy match 
score (FMS) between source sentence  and TM 
source sentence  indicates the reliability of 
the given TM sentence, and is defined as (Sikes, 
2007): 
 
Where  is the word-based 
Levenshtein Distance (Levenshtein, 1966) be-
tween  and . We equally divide FMS into 
ten fuzzy match intervals such as: [0.9, 1.0), [0.8, 
0.9) etc., and the index  specifies the corre-
sponding interval. For example, since the fuzzy 
match score between  and  in Figure 1 is 
0.667, then . 
Target Phrase Content Matching Status 
(TCM): It indicates the content matching status 
between   and , and reflects the quality 
of . Because  is nearly perfect when FMS 
is high, if the similarity between    and  
is high, it implies that the given  is possibly a 
good candidate. It is a member of {Same, High, 
Low, NA (Not-Applicable)}, and is specified as: 
(1) If  is not null: 
(a) if , ; 
(b) else if , ; 
(c) else, ; 
(2) If  is null, ; 
Here  is null means that either there is no 
corresponding TM source phrase  or 
there is no corresponding TM target phrase 
13
 aligned with . In the example of 
Figure 1, assume that the given  is ??? 5  
? 6  ?? 7? and  is ?object that is associated?. 
If  is ?object2 that3 is4 associated5?, 
; if  is ?an1 object2 that3 
is4 associated5?, . 
Source Phrase Content Matching Status 
(SCM): Which indicates the content matching 
status between  and , and it affects 
the matching status of  and  greatly. 
The more similar  is to , the more 
similar   is to . It is a member of {Same, 
High, Low, NA} and is defined as: 
(1) If  is not null: 
(a) if , ; 
(b) else if , 
; 
(c) else, ; 
(2) If  is null, ; 
Here  is null means that there is no corre-
sponding TM source phrase  for the giv-
en source phrase . Take the source phrase  
 ??? 5 ? 6 ?? 7? in Figure 1 for an ex-
ample, since its corresponding  is ??? 4 
? 5 ?? 6?, then . 
Number of Linking Neighbors (NLN): Usually, 
the context of a source phrase would affect its 
target translation. The more similar the context 
are, the more likely that the translations are the 
same. Therefore, this NLN feature reflects the 
number of matched neighbors (words) and it is a 
vector of <x, y>. Where ?x? denotes the number 
of matched source neighbors; and ?y? denotes 
how many those neighbors are also linked to tar-
get words (not null), which also affects the TM 
target phrase selection. This feature is a member 
of {<x, y>: <2, 2>, <2, 1>, <2, 0>, <1, 1>, <1, 0>, 
<0, 0>}. For the source phrase ??? 5 ? 6 ??
7? in Figure 1, the corresponding TM source 
phrase is ??? 4 ? 5 ?? 6? . As only their 
right neighbors ??8? and ??7? are matched, and 
??7? is aligned with ?.10?, NLN will be <1, 1>. 
Source Phrase Length (SPL): Usually the long-
er the source phrase is, the more reliable the TM 
target phrase is. For example, the corresponding 
 for the source phrase with 5 words 
would be more reliable than that with only one 
word. This feature denotes the number of words 
included in , and is a member of {1, 2, 3, 4, 
?5}. For the case ??? 5 ? 6 ?? 7?, SPL will 
be 3.  
Sentence End Punctuation Indicator (SEP): 
Which indicates whether the current phrase is a 
punctuation at the end of the sentence, and is a 
member of {Yes, No}. For example, the SEP for 
??? 5 ? 6 ?? 7? will be ?No?. It is intro-
duced because the SCM and TCM for a sen-
tence-end-punctuation are always ?Same? re-
gardless of other features. Therefore, it is used to 
distinguish this special case from other cases. 
3.2 Model-II 
As Model-I ignores the relationship among vari-
ous possible TM target phrases, we add two fea-
tures TM Candidate Set Status (CSS) and Long-
est TM Candidate Indicator (LTC) to incorporate 
this relationship among them.  Since CSS is re-
dundant after LTC is known, we thus ignore it 
for evaluating TCM probability in the following 
derivation: 
 
The two new features CSS and LTC adopted in 
Model-II are defined as follows: 
TM Candidate Set Status (CSS): Which re-
stricts the possible status of , and is a 
member of {Single, Left-Ext, Right-Ext, Both-Ext, 
NA}. Where ?Single? means that there is only 
one  candidate for the given source 
phrase ; ?Left-Ext? means that there are 
multiple  candidates, and all the candi-
dates are generated by extending only the left 
boundary; ?Right-Ext? means that there are mul-
tiple  candidates, and all the candidates 
are generated by only extending to the right; 
?Both-Ext? means that there are multiple  
candidates, and the candidates are generated by 
extending to both sides; ?NA? means that 
 is null. 
For ??? 4 ? 5 ?? 6? in Figure 1, the 
linked TM target phrase is ?object2 that3 is4 asso-
ciated5?, and there are 5 other candidates by ex-
tending to both sides. Therefore, 
. 
Longest TM Candidate Indicator (LTC): 
Which indicates whether the given  is the 
longest candidate or not, and is a member of 
{Original, Left-Longest, Right-Longest, Both-
Longest, Medium, NA}. Where ?Original? means 
that the given  is the one without exten-
sion; ?Left-Longest? means that the given 
14
 is only extended to the left and is the 
longest one; ?Right-Longest? means that the giv-
en  is only extended to the right and is 
the longest one; ?Both-Longest? means that the 
given  is extended to both sides and is the 
longest one; ?Medium? means that the given 
 has been extended but not the longest 
one; ?NA? means that  is null. 
For  ?object2 that3 is4 associated5? in 
Figure 1, ; for  ?an1 ob-
ject2 that3 is4 associated5?, ; 
for the longest  ?an1 object2 that3 is4 as-
sociated5 with6 the7?, . 
3.3 Model-III 
The abovementioned integrated models ignore 
the reordering information implied by TM. 
Therefore, we add a new feature Target Phrase 
Adjacent Candidate    Relative   Position   
Matching    Status (CPM) into Model-II and 
Model-III is given as: 
 
We assume that CPM is independent with SPL 
and SEP, because the length of source phrase 
would not affect reordering too much and SEP is 
used to distinguish the sentence end punctuation 
with other phrases.  
The new feature CPM adopted in Model-III is 
defined as: 
Target Phrase Adjacent Candidate Relative 
Position Matching Status (CPM): Which indi-
cates the matching status between the relative 
position of 
 
and the relative position of  
. It checks if  are 
positioned in the same order with 
, and reflects the quality of 
ordering the given target candidate . It is a 
member of {Adjacent-Same, Adjacent-Substitute, 
Linked-Interleaved, Linked-Cross, Linked-
Reversed, Skip-Forward, Skip-Cross, Skip-
Reversed, NA}. Recall that 
 
is always right ad-
jacent to , then various cases are defined as 
follows: 
(1) If both  and  are not null: 
(a) If  is on the right of  
and they are also adjacent to each other: 
i. If the right boundary words of  and 
 are the same, and the left 
boundary words of  and  are 
the same, ; 
ii. Otherwise, ; 
(b) If  is on the right of  
but they are not adjacent to each other, 
; 
(c) If  is not on the right of 
: 
i. If there are cross parts between  
and , ; 
ii. Otherwise, ; 
(2) If   is null but  is not null, 
then find the first which is 
not null (  starts from 2)2: 
(a) If  is on the right of , 
; 
(b) If  is not on the right of 
: 
i. If there are cross parts between  
and , ; 
ii. Otherwise, . 
(3) If  is null, . 
In Figure 1, assume that ,  and 
 are ?gets an?, ?object that is associat-
ed with? and ?gets0 an1?, respectively. For 
 ?object2 that3 is4 associated5?, because 
 is on the right of  and they are 
adjacent pair, and both boundary words (?an? 
and ?an1?; ?object? and ?object2?) are matched, 
; for  ?an1 object2 
that3 is4 associated5?, because there are cross 
parts ?an1? between  and , 
. On the other hand, as-
sume that ,  and  are ?gets?, ?ob-
ject that is associated with? and ?gets0?, respec-
tively. For  ?an1 object2 that3 is4 associ-
ated5?, because  and  are adja-
cent pair, but the left boundary words of  and 
 (?object? and ?an1?) are not matched, 
; for  ?object2 
that3 is4 associated5?, because  is on the 
right of  but they are not adjacent pair, 
therefore, . One more 
example, assume that ,  and  are 
?the annotation label?, ?object that is associated 
with? and ?the7 annotation8 label9?, respectively. 
For  ?an1 object2 that3 is4 associated5?, 
because  is on the left of , and 
there are no cross parts, .  
                                                 
2 It can be identified by simply memorizing the index of 
nearest non-null  during search. 
15
4 Experiments 
4.1 Experimental Setup 
Our TM database consists of computer domain 
Chinese-English translation sentence-pairs, 
which contains about 267k sentence-pairs. The 
average length of Chinese sentences is 13.85 
words and that of English sentences is 13.86 
words. We randomly selected a development set 
and a test set, and then the remaining sentence 
pairs are for training set. The detailed corpus sta-
tistics are shown in Table 1. Furthermore, devel-
opment set and test set are divided into various 
intervals according to their best fuzzy match 
scores. Corpus statistics for each interval in the 
test set are shown in Table 2.  
For the phrase-based SMT system, we adopted 
the Moses toolkit (Koehn et al, 2007). The sys-
tem configurations are as follows: GIZA++ (Och 
and Ney, 2003) is used to obtain the bidirectional 
word alignments. Afterwards, ?intersection? 3 
refinement (Koehn et al, 2003) is adopted to ex-
tract phrase-pairs. We use the SRI Language 
Model toolkit (Stolcke, 2002) to train a 5-gram 
model with modified Kneser-Ney smoothing 
(Kneser and Ney, 1995; Chen and Goodman, 
1998) on the target-side (English) training corpus. 
All the feature weights and the weight for each 
probability factor (3 factors for Model-III) are 
tuned on the development set with minimum-
error-rate training (MERT) (Och, 2003). The 
maximum phrase length is set to 7 in our exper-
iments. 
In this work, the translation performance is 
measured with case-insensitive BLEU-4 score 
(Papineni et al, 2002) and TER score (Snover et 
al., 2006). Statistical significance test is conduct-
ed with re-sampling (1,000 times) approach 
(Koehn, 2004) in 95% confidence level. 
4.2 Cross-Fold Translation 
To estimate the probabilities of proposed models, 
the corresponding phrase segmentations for bi-
lingual sentences are required. As we want to 
check what actually happened during decoding in 
the real situation, cross-fold translation is used to 
obtain the corresponding phrase segmentations. 
We first extract 95% of the bilingual sentences as 
a new training corpus to train a SMT system. 
Afterwards, we generate the corresponding 
phrase segmentations for the remaining 5% bi-
                                                 
3 ?grow-diag-final? and ?grow-diag-final-and? are also test-
ed. However, ?intersection? is the best option in our exper-
iments, especially for those high fuzzy match intervals.  
lingual sentences with Forced Decoding (Li et 
al., 2000; Zollmann et al, 2008; Auli et al, 2009; 
Wisniewski et al, 2010), which searches the best 
phrase segmentation for the specified output. 
Having repeated the above steps 20 times4, we 
obtain the corresponding phrase segmentations 
for the SMT training data (which will then be 
used to train the integrated models). 
Due to OOV words and insertion words, not 
all given source sentences can generate the de-
sired results through forced decoding. Fortunate-
ly, in our work, 71.7% of the training bilingual 
sentences can generate the corresponding target 
results. The remaining 28.3% of the sentence 
pairs are thus not adopted for generating training 
samples. Furthermore, more than 90% obtained 
source phrases are observed to be less than 5 
words, which explains why five different quanti-
zation levels are adopted for Source Phrase 
Length (SPL) in section 3.1. 
4.3 Translation Results 
After obtaining all the training samples via cross-
fold translation, we use Factored Language 
Model toolkit (Kirchhoff et al, 2007) to estimate 
the probabilities of integrated models with Wit-
ten-Bell smoothing (Bell et al, 1990; Witten et 
al., 1991) and Back-off method. Afterwards, we 
incorporate the TM information  for 
each  phrase  at  decoding.   All  experiments  are 
                                                 
4  This training process only took about 10 hours on our 
Ubuntu server (Intel 4-core Xeon 3.47GHz, 132 GB of 
RAM).  
  Train Develop Test 
#Sentences 261,906 2,569 2,576 
#Chn. Words 3,623,516 38,585 38,648 
#Chn. VOC. 43,112 3,287 3,460 
#Eng. Words 3,627,028 38,329 38,510 
#Eng. VOC. 44,221 3,993 4,046 
Table 1: Corpus Statistics 
Intervals #Sentences #Words W/S 
[0.9, 1.0) 269 4,468 16.6 
[0.8, 0.9) 362 5,004 13.8 
[0.7, 0.8) 290 4,046 14.0 
[0.6, 0.7) 379 4,998 13.2 
[0.5, 0.6) 472 6,073 12.9 
[0.4, 0.5) 401 5,921 14.8 
[0.3, 0.4) 305 5,499 18.0 
(0.0, 0.3) 98 2,639 26.9 
(0.0, 1.0) 2,576 38,648 15.0 
Table 2: Corpus Statistics for Test-Set 
16
Intervals TM SMT Model-I Model-II Model-III Koehn-10 Ma-11 Ma-11-U 
[0.9, 1.0) 81.31 81.38 85.44  * 86.47  *# 89.41  *# 82.79 77.72 82.78 
[0.8, 0.9) 73.25 76.16 79.97  * 80.89  * 84.04  *# 79.74  * 73.00 77.66 
[0.7, 0.8) 63.62 67.71 71.65  * 72.39  * 74.73  *# 71.02  * 66.54 69.78 
[0.6, 0.7) 43.64 54.56 54.88    # 55.88  *# 57.53  *# 53.06 54.00 56.37 
[0.5, 0.6) 27.37 46.32 47.32  *# 47.45  *# 47.54  *# 39.31 46.06 47.73 
[0.4, 0.5) 15.43 37.18 37.25    # 37.60    # 38.18  *# 28.99 36.23 37.93 
[0.3, 0.4) 8.24 29.27 29.52    # 29.38    # 29.15    # 23.58 29.40 30.20 
(0.0, 0.3) 4.13 26.38 25.61    # 25.32    # 25.57    # 18.56 26.30 26.92 
(0.0, 1.0) 40.17 53.03 54.57  *# 55.10  *# 56.51  *# 50.31 51.98 54.32 
Table 3: Translation Results (BLEU%). Scores marked by ?*? are significantly better (p < 0.05) than both TM 
and SMT systems, and those marked by ?#? are significantly better (p < 0.05) than Koehn-10. 
Intervals TM SMT Model-I Model-II Model-III Koehn-10 Ma-11 Ma-11-U 
[0.9, 1.0) 9.79 13.01 9.22      # 8.52    *# 6.77    *# 13.01 18.80 11.90 
[0.8, 0.9) 16.21 16.07 13.12  *# 12.74  *# 10.75  *# 15.27 20.60 14.74 
[0.7, 0.8) 27.79 22.80 19.10  *# 18.58  *# 17.11  *# 21.85 25.33 21.11 
[0.6, 0.7) 46.40 33.38 32.63    # 32.27  *# 29.96  *# 35.93 35.24 31.76 
[0.5, 0.6) 62.59 39.56 38.24  *# 38.77  *# 38.74  *# 47.37 40.24 38.01 
[0.4, 0.5) 73.93 47.19 47.03    # 46.34  *# 46.00  *# 56.84 48.74 46.10 
[0.3, 0.4) 79.86 55.71 55.38    # 55.44    # 55.87    # 64.55 55.93 54.15 
(0.0, 0.3) 85.31 61.76 62.38    # 63.66    # 63.51    # 73.30 63.00 60.67 
(0.0, 1.0) 50.51 35.88 34.34  *# 34.18  *# 33.26  *# 40.75 38.10 34.49 
Table 4: Translation Results (TER%). Scores marked by ?*? are significantly better (p < 0.05) than both TM and 
SMT systems, and those marked by ?#? are significantly better (p < 0.05) than Koehn-10. 
conducted using the Moses phrase-based decoder 
(Koehn et al, 2007). 
Table 3 and 4 give the translation results of 
TM, SMT, and three integrated models in the test 
set. In the tables, the best translation results (ei-
ther in BLEU or TER) at each interval have been 
marked in bold. Scores marked by ?*? are signif-
icantly better (p < 0.05) than both the TM and 
the SMT systems. 
It can be seen that TM significantly exceeds 
SMT at the interval [0.9, 1.0) in TER score, 
which illustrates why professional translators 
prefer TM rather than SMT as their assistant tool. 
Compared with TM and SMT, Model-I is signif-
icantly better than the SMT system in either 
BLEU or TER when the fuzzy match score is 
above 0.7; Model-II significantly outperforms 
both the TM and the SMT systems in either 
BLEU or TER when the fuzzy match score is 
above 0.5; Model-III significantly exceeds both 
the TM and the SMT systems in either BLEU or 
TER when the fuzzy match score is above 0.4. 
All these improvements show that our integrated 
models have combined the strength of both TM 
and SMT.  
However, the improvements from integrated 
models get less when the fuzzy match score de-
creases. For example, Model-III outperforms 
SMT 8.03 BLEU points at interval [0.9, 1.0), 
while the advantage is only 2.97 BLEU points at 
interval [0.6, 0.7). This is because lower fuzzy 
match score means that there are more un-
matched parts between  and ; the output of 
TM is thus less reliable. 
Across all intervals (the last row in the table), 
Model-III not only achieves the best BLEU score 
(56.51), but also gets the best TER score (33.26). 
If intervals are evaluated separately, when the 
fuzzy match score is above 0.4, Model-III out-
performs both Model-II and Model-I in either 
BLEU or TER. Model-II also exceeds Model-I in 
either BLEU or TER. The only exception is at 
interval [0.5, 0.6), in which Model-I achieves the 
best TER score. This might be due to that the 
optimization criterion for MERT is BLEU rather 
than TER in our work. 
4.4 Comparison with Previous Work 
In order to compare our proposed models with 
previous work, we re-implement two XML-
Markup approaches: (Koehn and Senellart, 2010) 
and (Ma et al 2011), which are denoted as 
Koehn-10 and Ma-11, respectively. They are 
selected because they report superior perfor-
mances in the literature. A brief description of 
them is as follows: 
17
Source 
?? 0 ?? 1 ? 2 ?? 3 ?? 4 ?5 internet6 explorer7 ? 8 ?? 9 internet10 ?? 11 ??? 12 
? 13 ? 14 ?? 15 ?16 ?? 17 ? 18 ? 19 ?? 20 ?? 21 ?? 22 ?23 
Reference 
if0 you1 disable2 this3 policy4 setting5 ,6 internet7 explorer8 does9 not10 check11 the12 internet13 
for14 new15 versions16 of17 the18 browser19 ,20 so21 does22 not23 prompt24 users25 to26 install27 
them28 .29 
TM 
Source 
?? 0 ? 1 ?? 2 ? 3 ?? 4 ?? 5 ?6 internet7 explorer8 ? 9 ?? 10 internet11 ?? 12 ??
? 13 ? 14 ? 15 ?? 16 ?17 ?? 18 ? 19 ? 20 ?? 21 ?? 22 ?? 23 ?24 
TM 
Target 
if0 you1 do2 not3 configure4 this5 policy6 setting7 ,8 internet9 explorer10 does11 not12 check13 the14 
internet15 for16 new17 versions18 of19 the20 browser21 ,22 so23 does24 not25 prompt26 users27 to28 
install29 them30 .31 
TM 
Alignment 
0-0 1-3 2-4 3-5 4-6 5-7 6-8 7-9 8-10 9-11 11-15 13-21 14-19 15-17 16-18 17-22 18-23 19-24 
21-26 22-27 23-29 24-31 
SMT 
if you disable this policy setting , internet explorer does not prompt users to install internet for 
new versions of the browser .    [Miss 7 target words: 9~12, 20~21, 28; Has one wrong permuta-
tion] 
Koehn-10 
if you do you disable this policy setting , internet explorer does not check the internet for new 
versions of the browser , so does not prompt users to install them .    [Insert two spurious target 
words] 
Ma-11 
if you disable this policy setting , internet explorer does not prompt users to install internet for 
new versions of the browser .    [Miss 7 target words: 9~12, 20~21, 28; Has one wrong permuta-
tion] 
Model-I 
if you disable this policy setting , internet explorer does not prompt users to install new ver-
sions of the browser , so does not check the internet .    [Miss 2 target words: 14, 28; Has one 
wrong permutation] 
Model-II 
if you disable this policy setting , internet explorer does not prompt users to install new ver-
sions of the browser , so does not check the internet .    [Miss 2 target words: 14, 28; Has one 
wrong permutation] 
Model-III 
if you disable this policy setting , internet explorer does not check the internet for new versions 
of the browser , so does not prompt users to install them .    [Exactly the same as the reference] 
Figure 2: A Translation Example at Interval [0.9, 1.0] (with FMS=0.920) 
Koehn et al (2010) first find out the un-
matched parts between the given source sentence 
and TM source sentence. Afterwards, for each 
unmatched phrase in the TM source sentence, 
they replace its corresponding translation in the 
TM target sentence by the corresponding source 
phrase in the input sentence, and then mark the 
substitution part. After replacing the correspond-
ing translations of all unmatched source phrases 
in the TM target sentence, an XML input sen-
tence (with mixed TM target phrases and marked 
input source phrases) is thus obtained. The SMT 
decoder then only translates the un-
matched/marked source phrases and gets the de-
sired results. Therefore, the inserted parts in the 
TM target sentence are automatically included. 
They use fuzzy match score to determine wheth-
er the current sentence should be marked or not; 
and their experiments show that this method is 
only effective when the fuzzy match score is 
above 0.8. 
Ma et al (2011) think fuzzy match score is not 
reliable and use a discriminative learning method 
to decide whether the current sentence should be 
marked or not. Another difference between Ma-
11 and Koehn-10 is how the XML input is con-
structed. In constructing the XML input sentence, 
Ma-11 replaces each matched source phrase in 
the given source sentence with the corresponding 
TM target phrase. Therefore, the inserted parts in 
the TM target sentence are not included. In Ma?s 
another paper (He et al, 2011), more linguistic 
features for discriminative learning are also add-
ed. In our work, we only re-implement the XML-
Markup method used in (He et al, 2011; Ma et al 
2011), but do not implement the discriminative 
learning method. This is because the features 
adopted in their discriminative learning are com-
plicated and difficult to re-implement. However, 
the proposed Model-III even outperforms the 
upper bound of their methods, which will be dis-
cussed later.  
Table 3 and 4 give the translation results of 
Koehn-10 and Ma-11 (without the discriminator). 
Scores marked by ?#? are significantly better (p 
< 0.05) than Koehn-10. Besides, the upper bound 
of (Ma et al 2011) is also given in the tables, 
which is denoted as Ma-11-U. We calculate this 
18
upper bound according to the method described 
in (Ma et al, 2011). Since He et al, (2011) only 
add more linguistic features to the discriminative 
learning method, the upper bound of (He et al, 
2011) is still the same with (Ma et al, 2011); 
therefore, Ma-11-U applies for both cases. 
It is observed that Model-III significantly ex-
ceeds Koehn-10 at all intervals. More important-
ly, the proposed models achieve much better 
TER score than the TM system does at interval 
[0.9, 1.0), but Koehn-10 does not even exceed 
the TM system at this interval. Furthermore, 
Model-III is much better than Ma-11-U at most 
intervals. Therefore, it can be concluded that the 
proposed models outperform the pipeline ap-
proaches significantly.  
Figure 2 gives an example at interval [0.9, 1.0), 
which shows the difference among different sys-
tem outputs. It can be seen that ?you do? is re-
dundant for Koehn-10, because they are inser-
tions and thus are kept in the XML input. How-
ever, SMT system still inserts another ?you?, 
regardless of ?you do? has already existed. This 
problem does not occur at Ma-11, but it misses 
some words and adopts one wrong permutation. 
Besides, Model-I selects more right words than 
SMT does but still puts them in wrong positions 
due to ignoring TM reordering information. In 
this example, Model-II obtains the same results 
with Model-I because it also lacks reordering 
information. Last, since Model-III considers both 
TM content and TM position information, it 
gives a perfect translation. 
5 Conclusion and Future Work 
Unlike the previous pipeline approaches, which 
directly merge TM phrases into the final transla-
tion result, we integrate TM information of each 
source phrase into the phrase-based SMT at de-
coding. In addition, all possible TM target 
phrases are kept and the proposed models select 
the best one during decoding via referring SMT 
information. Besides, the integrated model con-
siders the probability information of both SMT 
and TM factors. 
The experiments show that the proposed 
Model-III outperforms both the TM and the SMT 
systems significantly (p < 0.05) in either BLEU 
or TER when fuzzy match score is above 0.4. 
Compared with the pure SMT system, Model-III 
achieves overall 3.48 BLEU points improvement 
and 2.62 TER points reduction on a Chinese?
English TM database. Furthermore, Model-III 
significantly exceeds all previous pipeline ap-
proaches. Similar improvements are also ob-
served on the Hansards parts of LDC2004T08 
(not shown in this paper due to space limitation). 
Since no language-dependent feature is adopted, 
the proposed approaches can be easily adapted 
for other language pairs. 
Moreover, following the approaches of 
Koehn-10 and Ma-11 (to give a fair comparison), 
training data for SMT and TM are the same in 
the current experiments. However, the TM is 
expected to play an even more important role 
when the SMT training-set differs from the TM 
database, as additional phrase-pairs that are un-
seen in the SMT phrase table can be extracted 
from TM (which can then be dynamically added 
into the SMT phrase table at decoding time). Our 
another study has shown that the integrated mod-
el would be even more effective when the TM 
database and the SMT training data-set are from 
different corpora in the same domain (not shown 
in this paper). In addition, more source phrases 
can be matched if a set of high-FMS sentences, 
instead of only the sentence with the highest 
FMS, can be extracted and referred at the same 
time. And it could further raise the performance. 
Last, some related approaches (Smith and 
Clark, 2009; Phillips, 2011) combine SMT and 
example-based machine translation (EBMT) 
(Nagao, 1984). It would be also interesting to 
compare our integrated approach with that of 
theirs. 
 
Acknowledgments 
 
The research work has been funded by the Hi-
Tech Research and Development Program 
(?863? Program) of China under Grant No. 
2011AA01A207, 2012AA011101, and 
2012AA011102 and also supported by the Key 
Project of Knowledge Innovation Program of 
Chinese Academy of Sciences under Grant 
No.KGZD-EW-501.  
The authors would like to thank the anony-
mous reviewers for their insightful comments 
and suggestions. Our sincere thanks are also ex-
tended to Dr. Yanjun Ma and Dr. Yifan He for 
their valuable discussions during this study.  
References  
Michael Auli, Adam Lopez, Hieu Hoang and Philipp 
Koehn, 2009. A systematic analysis of translation 
model search spaces. In Proceedings of the Fourth 
Workshop on Statistical Machine Translation, pag-
es 224?232. 
19
Timothy C. Bell, J.G. Cleary and Ian H. Witten, 1990. 
Text compression: Prentice Hall, Englewood Cliffs, 
NJ. 
Ergun Bi?ici and Marc Dymetman. 2008. Dynamic 
translation memory: using statistical machine trans-
lation to improve translation memory fuzzy match-
es. In Proceedings of the 9th International Confer-
ence on Intelligent Text Processing and Computa-
tional Linguistics (CICLing 2008), pages 454?465. 
Stanley F. Chen and Joshua Goodman. 1998. An em-
pirical study of smoothing techniques for language 
modeling. Technical Report TR-10-98, Harvard 
University Center for Research in Computing 
Technology. 
Yifan He, Yanjun Ma, Josef van Genabith and Andy 
Way, 2010. Bridging SMT and TM with transla-
tion recommendation. In Proceedings of the 48th 
Annual Meeting of the Association for Computa-
tional Linguistics (ACL), pages 622?630. 
Yifan He, Yanjun Ma, Andy Way and Josef van 
Genabith. 2011. Rich linguistic features for transla-
tion memory-inspired consistent translation. In 
Proceedings of the Thirteenth Machine Translation 
Summit, pages 456?463. 
Reinhard Kneser and Hermann Ney. 1995. Improved 
backing-off for m-gram language modeling. In 
Proceedings of the IEEE International Conference 
on Acoustics, Speech and Signal Processing, pages 
181?184. 
Katrin Kirchhoff, Jeff A. Bilmes and Kevin Duh. 
2007. Factored language models tutorial. Technical 
report, Department of Electrical Engineering, Uni-
versity of Washington, Seattle, Washington, USA.  
Philipp Koehn. 2004. Statistical significance tests for 
machine translation evaluation. In Proceedings of 
the 2004 Conference on Empirical Methods in 
Natural Language Processing (EMNLP), pages 
388?395, Barcelona, Spain. 
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris 
Callison-Burch, Marcello Federico, Nicola Bertoldi, 
Brooke Cowan, Wade Shen, Christine Moran, 
Richard Zens, Chris Dyer and Ond?ej Bojar. 2007. 
Moses: Open source toolkit for statistical machine 
translation. In Proceedings of the ACL 2007 Demo 
and Poster Sessions, pages 177?180. 
Philipp Koehn, Franz Josef Och and Daniel Marcu. 
2003. Statistical phrase-based translation. In Pro-
ceedings of the 2003 Conference of the North 
American Chapter of the Association for Computa-
tional Linguistics on Human Language Technology, 
pages 48?54. 
Philipp Koehn and Jean Senellart. 2010. Convergence 
of translation memory and statistical machine 
translation. In AMTA Workshop on MT Research 
and the Translation Industry, pages 21?31. 
Elina Lagoudaki. 2006. Translation memories survey 
2006: Users? perceptions around tm use. In Pro-
ceedings of the ASLIB International Conference 
Translating and the Computer 28, pages 1?29. 
Qi Li, Biing-Hwang Juang, Qiru Zhou, and Chin-Hui 
Lee. 2000. Automatic verbal information verifica-
tion for user authentication. IEEE transactions on 
speech and audio processing, Vol. 8, No. 5, pages 
1063?6676. 
Vladimir Iosifovich Levenshtein. 1966. Binary codes 
capable of correcting deletions, insertions, and re-
versals. Soviet Physics Doklady, 10 (8). pages 707?
710. 
Yanjun Ma, Yifan He, Andy Way and Josef van 
Genabith. 2011. Consistent translation using dis-
criminative learning: a translation memory-inspired 
approach. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 1239?1248, Portland, Oregon. 
Makoto Nagao, 1984. A framework of a mechanical 
translation between Japanese and English by anal-
ogy principle. In: Banerji, Alick Elithorn and  Ran-
an (ed). Artifiical and Human Intelligence: Edited 
Review Papers Presented at the International 
NATO Symposium on Artificial and Human Intelli-
gence. North-Holland, Amsterdam, 173?180. 
Franz Josef Och. 2003. Minimum error rate training 
in statistical machine translation. In Proceedings of 
the 41st Annual Meeting of the Association for 
Computational Linguistics, pages 160?167. 
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment 
models. Computational Linguistics, 29 (1). pages 
19?51. 
Kishore Papineni, Salim Roukos, Todd Ward and 
Wei-Jing Zhu. 2002. BLEU: a method for automat-
ic evaluation of machine translation. In Proceed-
ings of the 40th Annual Meeting of the Association 
for Computational Linguistics (ACL), pages 311?
318. 
Aaron B. Phillips, 2011. Cunei: open-source machine 
translation with relevance-based models of each 
translation instance. Machine Translation, 25 (2). 
pages 166-177. 
Richard Sikes. 2007, Fuzzy matching in theory and 
practice. Multilingual, 18(6):39?43. 
Michel Simard and Pierre Isabelle. 2009. Phrase-
based machine translation in a computer-assisted 
translation environment. In Proceedings of the 
Twelfth Machine Translation Summit (MT Summit 
XII), pages 120?127. 
James Smith and Stephen Clark. 2009. EBMT for 
SMT: a new EBMT-SMT hybrid. In Proceedings 
of the 3rd International Workshop on Example-
20
Based Machine Translation (EBMT'09), pages 3?
10, Dublin, Ireland. 
Matthew Snover, Bonnie Dorr, Richard Schwartz, 
Linnea Micciulla and John Makhoul. 2006. A 
study of translation edit rate with targeted human 
annotation. In Proceedings of Association for Ma-
chine Translation in the Americas (AMTA-2006), 
pages 223?231. 
Andreas Stolcke. 2002. SRILM-an extensible lan-
guage modeling toolkit. In Proceedings of the In-
ternational Conference on Spoken Language Pro-
cessing, pages 311?318. 
Guillaume Wisniewski, Alexandre Allauzen and 
Fran?ois Yvon, 2010. Assessing phrase-based 
translation models with oracle decoding. In Pro-
ceedings of the 2010 Conference on Empirical 
Methods in Natural Language Processing, pages 
933?943. 
Ian H. Witten and Timothy C. Bell. 1991. The zero-
frequency problem: estimating the probabilities of 
novel events in adaptive test compression. IEEE 
Transactions on Information Theory, 37(4): 1085?
1094, July. 
Ventsislav Zhechev and Josef van Genabith. 2010. 
Seeding statistical machine translation with transla-
tion memory output through tree-based structural 
alignment. In Proceedings of the 4th Workshop on 
Syntax and Structure in Statistical Translation, 
pages 43?51. 
Andreas Zollmann, Ashish Venugopal, Franz Josef 
Och and Jay Ponte, 2008. A systematic comparison 
of phrase-based, hierarchical and syntax-
augmented statistical MT. In Proceedings of the 
22nd International Conference on Computational 
Linguistics (Coling 2008), pages 1145?1152. 
 
 
21
 A Character-Based Joint Model 
for CIPS-SIGHAN Word Segmentation Bakeoff 2010 
Kun Wang and Chengqing Zong 
National Laboratory of Pattern Recognition 
Institute of Automation, Chinese Academy of Science
{kunwang,cqzong}@nlpr.ia.ac.cn 
Keh-Yih Su 
Behavior Design Corporation
 
kysu@bdc.com.tw 
 
Abstract 
This paper presents a Chinese Word 
Segmentation system for the closed track 
of CIPS-SIGHAN Word Segmentation 
Bakeoff 2010. This system adopts a 
character-based joint approach, which 
combines a character-based generative 
model and a character-based discrimina-
tive model. To further improve the cross-
domain performance, we use an addi-
tional semi-supervised learning proce-
dure to incorporate the unlabeled corpus. 
The final performance on the closed 
track for the simplified-character text 
shows that our system achieves compa-
rable results with other state-of-the-art 
systems. 
1 Introduction 
The character-based tagging approach (Xue, 
2003) has become the dominant technique for 
Chinese word segmentation (CWS) as it can tol-
erate out-of-vocabulary (OOV) words. In the last 
few years, this method has been widely adopted 
and further improved in many previous works 
(Tseng et al, 2005; Zhang et al, 2006; Jiang et 
al., 2008). Among various character-based tag-
ging approaches, the character-based joint model 
(Wang et al, 2010) achieves a good balance be-
tween in-vocabulary (IV) words recognition and 
OOV words identification. 
In this work, we adopt the character-based 
joint model as our basic system, which combines 
a character-based discriminative model and a 
character-based generative model. The genera-
tive module holds a robust performance on IV 
words, while the discriminative module can 
handle the extra features easily and enhance the 
OOV words segmentation. However, the per-
formance of out-of-domain text is still not satis-
factory as that of in-domain text, while few pre-
vious works have paid attention to this problem. 
To further improve the performance of the ba-
sic system in out-of-domain text, we use a semi-
supervised learning procedure to incorporate the 
unlabeled corpora of Literature (Unlabeled-A) 
and Computer (Unlabeled-B). The final results 
show that our system performs well on all four 
testing-sets and achieves comparable segmenta-
tion results with other participants. 
2 Our system 
2.1 Character-Based Joint Model 
The character-based joint model in our system 
contains two basic components:  
? The character-based discriminative model.  
? The character-based generative model. 
The character-based discriminative model 
(Xue, 2003) is based on a Maximum Entropy 
(ME) framework (Ratnaparkhi, 1998) and can be 
formulated as follows: 
2
1 1 1 2
1
( ) ( ,
n
n n k
k k k
k
P t c P t t c +? ?
=
?? )  (1) 
Where tk is a member of {Begin, Middle, End, 
Single} (abbreviated as B, M, E and S from now 
on) to indicate the corresponding position of 
character ck in its associated word. For example, 
the word ???? (Beijing City)? will be as-
signed with the corresponding tags as: ?? /B 
(North) ?/M (Capital) ?/E (City)?.  
This discriminative module can flexibly in-
corporate extra features and it is implemented 
with the ME package1 given by Zhang Le. All 
training experiments are done with Gaussian 
prior 1.0 and 200 iterations. 
The character-based generative module is a 
character-tag-pair-based trigram model (Wang et 
al., 2009) and can be expressed as below: 
1
1
1
([ , ] ) ([ , ] [ , ] ).
n
n
i i
i
P c t P c t c t ??
=
?? 2i  (2) 
In our experiments, SRI Language Modeling  
Toolkit2 (Stolcke, 2002) is used to train the gen-
erative trigram model with modified Kneser-Ney 
smoothing (Chen and Goodman, 1998). 
The character-based joint model combines the 
above discriminative module and the generative 
module with log-linear interpolation as follows: 
1
2
2
1 2
( ) log( ([ , ] [ , ] ))
(1 ) log( ( , ))
k
k k
k
k k k
Score t P c t c t
P t t c
?
?
?
?
+
? ?
= ?
+ ? ?
k
 (3) 
Where the parameter (0.0 1.0)? ?? ?  is the 
weight for the generative model. Score(tk) will 
be directly used during searching the best se-
quence. We set an empirical value ( 0.3? = ) to 
this model as there is no development-set for 
various domains. 
2.2 Features 
In this work, the feature templates adopted in the 
character-based discriminative model are very 
simple and are listed below: 
1
1 1
2 1 0 1
( ) ( 2, 1,0,12);
( ) ( 2, 1,0,1);
( ) ;
( ) ( ) ( ) ( ) ( ) ( )
n
n n
a C n
b C C n
c C C
d T C T C T C T C T C
+
?
? ?
= ? ?
= ? ?
2
 
In the above templates, Cn represents a char-
acter and the index n indicates the position. For 
example, when we consider the third character 
??? in the sequence ???????, template (a) 
results in the features as following: C-2=?, C-1=
?, C0=?, C1=?, C2=?, and template (b) gen-
erates the features as: C-2C-1=??, C-1C0=??, 
                                                 
1 http://homepages.inf.ed.ac.uk/lzhang10/maxent_toolkit.html 
2 http://www.speech.sri.com/projects/srilm/ 
C0C1=??, C1C2=??, and template (c) gives 
the feature C-1C1=??.  
Template (d) is the feature of character type. 
Five types classes are defined: dates (???, ???, 
???, the Chinese character for ?year?, ?month? 
and ?day? respectively) represents class 0; for-
eign alphabets represent class 1; Arabic and 
Chinese numbers represent class 2; punctuation 
represents class 3 and other characters represent 
class 4. For example, when we consider the 
character ??? in the sequence ?????Q?, 
the feature T C  will 
be set to ?20341?. 
2 1 0 1 2( ) ( ) ( ) ( ) ( )T C T C T C T C? ?
When training the character-based discrimina-
tive module, we convert all the binary features 
into real-value features, and set the real-value of 
C0 to be 2.0, the value of C-1C0 and C0C1 to be 
3.0, and the values of all other features to be 1.0. 
This method sounds a little strange because it is 
equal to duplicate some features for the maxi-
mum entropy training. However, it effectively 
improves the performance in our previous works. 
2.3 Restrictions in constructing lattice 
As the closed track allows the participants to use 
the character type information, we add some re-
strictions to our system when constructing the 
character-tag lattice. When we consider a char-
acter in the sequence, the type information of 
both the previous and the next character would 
be taken into account. The restrictions are list as 
follows: 
z If the previous, the current and the next 
characters are all English or numbers, we 
would fix the current tag to be ?M?; 
z If the previous and the next characters are 
both English or numbers, while the current 
character is a connective symbol such as ?-?, 
?/?, ?_?, ?\? etc., we would also fix the cur-
rent tag to be ?M?; 
z Otherwise, all four tags {B, E, M, S} would 
be given to the current character. 
It is shown that in the Computer domain these 
simple restrictions not only greatly reduce the 
number of words segmented, but also speed up 
the system. 
Domain Mark OOV Rate R P F1 ROOV RIV 
Literature A 0.069 0.937 0.937 0.937 0.652 0.958 
Computer B 0.152 0.941 0.940 0.940 0.757 0.974 
Medicine C 0.110 0.930 0.917 0.923 0.674 0.961 
Finance D 0.087 0.957 0.956 0.957 0.813 0.971 
Table 1: Official segmentation results of our system. 
Algorithm 1: Semi-Supervised Learning 
Given: 
z Labeled training corpus: L 
z Unlabeled training corpus: U  
1: Use L to train a segmenter S ;  0
2: Use S  to segment the unlabeled corpus U  
and then get labeled corpus U ; 
0
0
3: for i  to K  do = 1
4: Add U  to L and get a new corpus Li;i?1
Use Li to train a new segmenter Si; 5: 
6: Use Si to segment the unlabeled corpus 
 and then get labeled corpus Ui; U
7:     if convergence criterion meets 
8:          break 
8: end for 
Output: the last segmenter S  K
 
2.4 Semi-Supervised Learning 
In the last decade, Chinese word segmentation 
has been improved significantly and gets a high 
precision rate in performance. However, the per-
formance for out-of-domain text is still unsatis-
factory at the present. Also, few works have paid 
attention to the cross-domain problem in Chi-
nese word segmentation task so far. 
Self-training and Co-training are two simple 
semi-supervised learning methods to incorporate 
unlabeled corpus (Zhu, 2006). In this work, we 
use an iterative self-training method to incorpo-
rate the unlabeled data. A segmenter is first 
trained with the labeled corpus. Then this seg-
menter is used to segment the unlabeled data. 
Then the predicted data is added to the original 
training corpus as a new training-set. The seg-
menter will be re-trained and the procedure re-
peated. To simplify the task, we fix the weight  
0.3? =  for the generative module of our joint 
model in the training iterations. The procedure is 
shown in Algorithm 1. The iterations will not be 
ended until the similarity of two segmentation 
results Ui?1 and Ui reach a certain level. Here we 
used F-score to measure the similarity between 
?1 and Ui: treat Ui?1 as the benchmark, Ui as a 
testing-set. From our observation, this method 
converges quickly in only 3 or 4 iterations for 
both Literature and Computer corpora. 
Ui
3 Experiments and Discussion 
3.1 Results 
In this CIPS-SIGHAN bakeoff, we only partici-
pate the closed track for simplified-character text. 
There are two kinds of training corpora:  
z Labeled corpus from News Domain 
z Unlabeled corpora from Literature Do-
main (Unlabeled-A) and Computer Do-
main (Unlabeled-B). 
Also, the testing corpus covers four domains: 
Literature (Testing-A), Computer (Testing-B), 
Medicine (Testing-C) and Finance (Testing-D). 
As there are only two unlabeled corpora for 
Domain A and B, we thus adopt different strate-
gies for each testing-set: 
z Testing-A: Character-Based Joint Model 
with semi-supervised learning, training 
on Labeled corpus and Unlabeled-A; 
z Testing-B: Character-Based Joint Model 
with semi-supervised learning, training 
on Labeled corpus and Unlabeled-B; 
z Testing-C and D: Character-Based Joint 
Model, training on Labeled corpus; 
Table 1 shows that our system achieves F-
scores for various testing-sets: 0.937 (A), 0.940 
(B), 0.923 (C) and 0.957 (D), which are compa-
rable with other systems. Among those four test-
ing domains, our system performs unsatisfactor-
ily on Testing-C (Medicine) even the OOV rate 
of this domain is not the highest. There are pos-
sible reasons for this result: (1) Semi-supervised 
learning is not conducted for this domain; (2) the 
statistical property between News and Medicine 
are significantly different. 
Domain Model F1 ROOV 
J + R + S 0.937 0.652 
J + S 0.937 0.646 
J + R 0.936 0.646 
A 
J 0.936 0.642 
J + R + S 0.940 0.757 
J + S 0.931 0.721 
J + R 0.938 0.744 
B 
J 0.927 0.699 
J + R 0.923 0.674 C 
J 0.923 0.674 
J + R 0.957 0.813 
D 
J 0.954 0.786 
Table 2: Performance of various approaches 
J: Baseline, the character-based joint model 
R: Adding restrictions in constructing lattice 
S: Conduct Semi-Supervised Learning 
 
3.2 Discussion 
The aim of restrictions in constructing lattice is 
to improve the performance of English and nu-
merical expressions, both of which appear fre-
quently in Computer and Finance domain. 
Therefore, the improvements gained from these 
restrictions are significantly in these two do-
mains (as shown in Table 2). 
Besides, the adopted semi-supervised learning 
procedure improves the performance in Domain 
A and B., but the improvement is not significant. 
Semi-supervised learning aims to incorporate 
large amounts of unlabeled data. However, the 
size of unlabeled corpora provided here is too 
small. The semi-supervised learning procedure is 
expected to be more effective if a large amount 
of unlabeled data is available. 
4 Conclusion 
Our system is based on a character-based joint 
model, which combines a generative module and 
a discriminative module. In addition, we applied 
a semi-supervised learning method to the base-
line approach to incorporate the unlabeled cor-
pus. Our system achieves comparable perform-
ance with other participants. However, cross-
domain performance is still not satisfactory and 
further study is needed. 
Acknowledgement 
The research work has been partially funded by  
the Natural Science Foundation of China under 
Grant No. 60975053, 90820303 and 60736014, 
the National Key Technology R&D Program 
under Grant No. 2006BAH03B02, and also the 
Hi-Tech Research and Development Program 
(?863? Program) of China under Grant No. 
2006AA010108-4 as well. 
References 
Stanley F. Chen and Joshua Goodman, 1998. An em-
pirical study of smoothing techniques for language 
modeling. Technical Report TR-10-98, Harvard 
University Center for Research in Computing 
Technology. 
Wenbin Jiang, Liang Huang, Qun Liu and Yajuan Lu, 
2008. A Cascaded Linear Model for Joint Chinese 
Word Segmentation and Part-of-Speech Tagging. 
In Proceedings of ACL, pages 897-904. 
Adwait Ratnaparkhi, 1998. Maximum entropy mod-
els for natural language ambiguity resolution. Uni-
versity of Pennsylvania. 
Andreas Stolcke, 2002. SRILM-an extensible lan-
guage modeling toolkit. In Proceedings of the In-
ternational Conference on Spoken Language Proc-
essing, pages 311-318. 
Huihsin Tseng, Pichuan Chang, Galen Andrew, 
Daniel Jurafsky and Christopher Manning, 2005. A 
Conditional Random Field Word Segmenter for 
Sighan Bakeoff 2005. In Proceedings of the Fourth 
SIGHAN Workshop on Chinese Language Process-
ing, pages 168-171. 
Kun Wang, Chengqing Zong and Keh-Yih Su, 2009. 
Which is more suitable for Chinese word segmen-
tation, the generative model or the discriminative 
one? In Proceedings of the 23rd Pacific Asia Con-
ference on Language, Information and Computa-
tion (PACLIC23), pages 827-834. 
Kun Wang, Chengqing Zong and Keh-Yih Su, 2010. 
A Character-Based Joint Model for Chinese Word 
Segmentation. To appear in COLING 2010. 
Nianwen Xue, 2003. Chinese Word Segmentation as 
Character Tagging. Computational Linguistics and 
Chinese Language Processing, 8 (1). pages 29-48. 
Ruiqiang Zhang, Genichiro Kikui and Eiichiro 
Sumita, 2006. Subword-based Tagging for Confi-
dence-dependent Chinese Word Segmentation. In 
Proceedings of the COLING/ACL, pages 961-968. 
Xiaojin Zhu, 2006. Semi-supervised learning litera-
ture survey. Technical Report 1530, Computer Sci-
ences, University of Wisconsin-Madison. 
