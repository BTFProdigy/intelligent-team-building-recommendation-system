Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 620?628,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Unsupervised Approaches for Automatic Keyword Extraction Using
Meeting Transcripts
Feifan Liu, Deana Pennell, Fei Liu and Yang Liu
Computer Science Department
The University of Texas at Dallas
Richardson, TX 75080, USA
{ffliu,deana,feiliu,yangl}@hlt.utdallas.edu
Abstract
This paper explores several unsupervised ap-
proaches to automatic keyword extraction
using meeting transcripts. In the TFIDF
(term frequency, inverse document frequency)
weighting framework, we incorporated part-
of-speech (POS) information, word clustering,
and sentence salience score. We also evalu-
ated a graph-based approach that measures the
importance of a word based on its connection
with other sentences or words. The system
performance is evaluated in different ways, in-
cluding comparison to human annotated key-
words using F-measure and a weighted score
relative to the oracle system performance, as
well as a novel alternative human evaluation.
Our results have shown that the simple un-
supervised TFIDF approach performs reason-
ably well, and the additional information from
POS and sentence score helps keyword ex-
traction. However, the graph method is less
effective for this domain. Experiments were
also performed using speech recognition out-
put and we observed degradation and different
patterns compared to human transcripts.
1 Introduction
Keywords in a document provide important infor-
mation about the content of the document. They
can help users search through information more effi-
ciently or decide whether to read a document. They
can also be used for a variety of language process-
ing tasks such as text categorization and informa-
tion retrieval. However, most documents do not
provide keywords. This is especially true for spo-
ken documents. Current speech recognition system
performance has improved significantly, but there
is no rich structural information such as topics and
keywords in the transcriptions. Therefore, there is
a need to automatically generate keywords for the
large amount of written or spoken documents avail-
able now.
There have been many efforts toward keyword ex-
traction for text domain. In contrast, there is less
work on speech transcripts. In this paper we fo-
cus on one speech genre ? the multiparty meeting
domain. Meeting speech is significantly different
from written text and most other speech data. For
example, there are typically multiple participants
in a meeting, the discussion is not well organized,
and the speech is spontaneous and contains disflu-
encies and ill-formed sentences. It is thus ques-
tionable whether we can adopt approaches that have
been shown before to perform well in written text
for automatic keyword extraction in meeting tran-
scripts. In this paper, we evaluate several differ-
ent keyword extraction algorithms using the tran-
scripts of the ICSI meeting corpus. Starting from
the simple TFIDF baseline, we introduce knowl-
edge sources based on POS filtering, word cluster-
ing, and sentence salience score. In addition, we
also investigate a graph-based algorithm in order to
leverage more global information and reinforcement
from summary sentences. We used different per-
formance measurements: comparing to human an-
notated keywords using individual F-measures and
a weighted score relative to the oracle system per-
formance, and conducting novel human evaluation.
Experiments were conducted using both the human
transcripts and the speech recognition (ASR) out-
620
put. Overall the TFIDF based framework seems to
work well for this domain, and the additional knowl-
edge sources help improve system performance. The
graph-based approach yielded worse results, espe-
cially for the ASR condition, suggesting further in-
vestigation for this task.
2 Related Work
TFIDF weighting has been widely used for keyword
or key phrase extraction. The idea is to identify
words that appear frequently in a document, but do
not occur frequently in the entire document collec-
tion. Much work has shown that TFIDF is very ef-
fective in extracting keywords for scientific journals,
e.g., (Frank et al, 1999; Hulth, 2003; Kerner et al,
2005). However, we may not have a big background
collection that matches the test domain for a reli-
able IDF estimate. (Matsuo and Ishizuka, 2004) pro-
posed a co-occurrence distribution based method us-
ing a clustering strategy for extracting keywords for
a single document without relying on a large corpus,
and reported promising results.
Web information has also been used as an ad-
ditional knowledge source for keyword extraction.
(Turney, 2002) selected a set of keywords first and
then determined whether to add another keyword hy-
pothesis based on its PMI (point-wise mutual infor-
mation) score to the current selected keywords. The
preselected keywords can be generated using basic
extraction algorithms such as TFIDF. It is impor-
tant to ensure the quality of the first selection for the
subsequent addition of keywords. Other researchers
also used PMI scores between each pair of candidate
keywords to select the top k% of words that have
the highest average PMI scores as the final keywords
(Inkpen and Desilets, 2004).
Keyword extraction has also been treated as a
classification task and solved using supervised ma-
chine learning approaches (Frank et al, 1999; Tur-
ney, 2000; Kerner et al, 2005; Turney, 2002; Tur-
ney, 2003). In these approaches, the learning al-
gorithm needs to learn to classify candidate words
in the documents into positive or negative examples
using a set of features. Useful features for this ap-
proach include TFIDF and its variations, position of
a phrase, POS information, and relative length of a
phrase (Turney, 2000). Some of these features may
not work well for meeting transcripts. For exam-
ple, the position of a phrase (measured by the num-
ber of words before its first appearance divided by
the document length) is very useful for news article
text, since keywords often appear early in the doc-
ument (e.g., in the first paragraph). However, for
the less well structured meeting domain (lack of ti-
tle and paragraph), these kinds of features may not
be indicative. A supervised approach to keyword ex-
traction was used in (Liu et al, 2008). Even though
the data set in that study is not very big, it seems that
a supervised learning approach can achieve reason-
able performance for this task.
Another line of research for keyword extrac-
tion has adopted graph-based methods similar to
Google?s PageRank algorithm (Brin and Page,
1998). In particular, (Wan et al, 2007) attempted
to use a reinforcement approach to do keyword ex-
traction and summarization simultaneously, on the
assumption that important sentences usually contain
keywords and keywords are usually seen in impor-
tant sentences. We also find that this assumption also
holds using statistics obtained from the meeting cor-
pus used in this study. Graph-based methods have
not been used in a genre like the meeting domain;
therefore, it remains to be seen whether these ap-
proaches can be applied to meetings.
Not many studies have been performed on speech
transcripts for keyword extraction. The most rel-
evant work to our study is (Plas et al, 2004),
where the task is keyword extraction in the mul-
tiparty meeting corpus. They showed that lever-
aging semantic resources can yield significant per-
formance improvement compared to the approach
based on the relative frequency ratio (similar to
IDF). There is also some work using keywords for
other speech processing tasks, e.g., (Munteanu et
al., 2007; Bulyko et al, 2007; Wu et al, 2007; De-
silets et al, 2002; Rogina, 2002). (Wu et al, 2007)
showed that keyword extraction combined with se-
mantic verification can be used to improve speech
retrieval performance on broadcast news data. In
(Rogina, 2002), keywords were extracted from lec-
ture slides, and then used as queries to retrieve rel-
evant web documents, resulting in an improved lan-
guage model and better speech recognition perfor-
mance of lectures. There are many differences be-
tween written text and speech ? meetings in par-
ticular. Thus our goal in this paper is to investi-
621
gate whether we can successfully apply some exist-
ing techniques, as well as propose new approaches
to extract keywords for the meeting domain. The
aim of this study is to set up some starting points for
research in this area.
3 Data
We used the meetings from the ICSI meeting data
(Janin et al, 2003), which are recordings of naturally
occurring meetings. All the meetings have been
transcribed and annotated with dialog acts (DA)
(Shriberg et al, 2004), topics, and extractive sum-
maries (Murray et al, 2005). The ASR output for
this corpus is obtained from a state-of-the-art SRI
conversational telephone speech system (Zhu et al,
2005), with a word error rate of about 38.2% on
the entire corpus. We align the human transcripts
and ASR output, then map the human annotated DA
boundaries and topic boundaries to the ASR words,
such that we have human annotation of these infor-
mation for the ASR output.
We recruited three Computer Science undergradu-
ate students to annotate keywords for each topic seg-
ment, using 27 selected ICSI meetings.1 Up to five
indicative key words or phrases were annotated for
each topic. In total, we have 208 topics annotated
with keywords. The average length of the topics
(measured using the number of dialog acts) among
all the meetings is 172.5, with a high standard devi-
ation of 236.8. We used six meetings as our devel-
opment set (the same six meetings as the test set in
(Murray et al, 2005)) to optimize our keyword ex-
traction methods, and the remaining 21 meetings for
final testing in Section 5.
One example of the annotated keywords for a
topic segment is:
? Annotator I: analysis, constraints, template
matcher;
? Annotator II: syntactic analysis, parser, pattern
matcher, finite-state transducers;
? Annotator III: lexicon, set processing, chunk
parser.
Note that these meetings are research discussions,
and that the annotators may not be very familiar with
1We selected these 27 meetings because they have been used
in previous work for topic segmentation and summarization
(Galley et al, 2003; Murray et al, 2005).
the topics discussed and often had trouble deciding
the important sentences or keywords. In addition,
limiting the number of keywords that an annotator
can select for a topic also created some difficulty.
Sometimes there are more possible keywords and
the annotators felt it is hard to decide which five are
the most topic indicative. Among the three annota-
tors, we notice that in general the quality of anno-
tator I is the poorest. This is based on the authors?
judgment, and is also confirmed later by an indepen-
dent human evaluation (in Section 6).
For a better understanding of the gold standard
used in this study and the task itself, we thoroughly
analyzed the human annotation consistency. We re-
moved the topics labeled with ?chitchat? by at least
one annotator, and also the digit recording part in
the ICSI data, and used the remaining 140 topic seg-
ments. We calculated the percentage of keywords
agreed upon by different annotators for each topic,
as well as the average for all the meetings. All of the
consistency analysis is performed based on words.
Figure 1 illustrates the annotation consistency over
different meetings and topics. The average consis-
tency rate across topics is 22.76% and 5.97% among
any two and all three annotators respectively. This
suggests that people do not have a high agreement
on keywords for a given document. We also notice
that the two person agreement is up to 40% for sev-
eral meetings and 80% for several individual top-
ics, and the agreement among all three annotators
reaches 20% and 40% for some meetings or topics.
This implies that the consistency depends on topics
(e.g., the difficulty or ambiguity of a topic itself, the
annotators? knowledge of that topic). Further studies
are needed for the possible factors affecting human
agreement. We are currently creating more annota-
tions for this data set for better agreement measure
and also high quality annotation.
4 Methods
Our task is to extract keywords for each of the topic
segments in each meeting transcript. Therefore, by
?document?, we mean a topic segment in the re-
mainder of this paper. Note that our task is different
from keyword spotting, where a keyword is provided
and the task is to spot it in the audio (along with its
transcript).
The core part of keyword extraction is for the sys-
622
0
0.2
0.4
0.6
0.8
1
0 30 60 90 120
3 agree
2 agree
0
0.1
0.2
0.3
0.4
0.5
1 3 5 7 9 11 13 15 17 19 21 23 25 27
3 agree
2 agree
Figure 1: Human annotation consistency across differ-
ent topics (upper graph) and meetings (lower graph). Y-
axis is the percent of the keywords agreed upon by two or
three annotators.
tem to assign an importance score to a word, and
then pick the top ranked words as keywords. We
compare different methods for weight calculation in
this study, broadly divided into the following two
categories: the TFIDF framework and the graph-
based model. Both are unsupervised learning meth-
ods.2 In all of the following approaches, when se-
lecting the final keywords, we filter out any words
appearing on the stopword list. These stopwords are
generated based on the IDF values of the words us-
ing all the meeting data by treating each topic seg-
ment as a document. The top 250 words from this
list (with the lowest IDF values) were used as stop-
words. We generated two different stopword lists for
human transcripts and ASR output respectively. In
addition, in this paper we focus on performing key-
word extraction at the single word level, therefore
no key phrases are generated.
2Note that by unsupervised methods, we mean that no data
annotated with keywords is needed. These methods do require
the use of some data to generate information such as IDF, or
possibly a development set to optimize some parameters or
heuristic rules.
4.1 TFIDF Framework
(A) Basic TFIDF weighting
The term frequency (TF) for a word wi in a doc-
ument is the number of times the word occurs in the
document. The IDF value is:
IDFi = log(N/Ni)
whereNi denotes the number of the documents con-
taining word wi, and N is the total number of the
documents in the collection. We also performed L2
normalization for the IDF values when combining
them with other scores.
(B) Part of Speech (POS) filtering
In addition to using a stopword list to remove
words from consideration, we also leverage POS in-
formation to filter unlikely keywords. Our hypothe-
sis is that verb, noun and adjective words are more
likely to be keywords, so we restrict our selection to
words with these POS tags only. We used the TnT
POS tagger (Brants, 2000) trained from the Switch-
board data to tag the meeting transcripts.
(C) Integrating word clustering
One weakness of the baseline TFIDF is that it
counts the frequency for a particular word, without
considering any words that are similar to it in terms
of semantic meaning. In addition, when the docu-
ment is short, the TF may not be a reliable indicator
of the importance of the word. Our idea is therefore
to account for the frequency of other similar words
when calculating the TF of a word in the document.
For this, we group all the words into clusters in an
unsupervised fashion. If the total term frequency
of all the words in one cluster is high, it is likely
that this cluster contributes more to the current topic
from a thematic point of view. Thus we want to as-
sign higher weights to the words in this cluster.
We used the SRILM toolkit (Stolcke, 2002) for
automatic word clustering over the entire docu-
ment collection. It minimizes the perplexity of the
induced class-based n-gram language model com-
pared to the original word-based model. Using the
clusters, we then adjust the TF weighting by inte-
grating with the cluster term frequency (CTF):
TF CTF (wi) = TF (wi)??(
P
wl?Ci,wl 6=wi freq(wl))
where the last summation component means the to-
tal term frequency of all the other words in this docu-
ment that belong to the same clusterCi as the current
623
word wi. We set parameter ? to be slightly larger
than 1. We did not include stopwords when adding
the term frequencies for the words in a cluster.
(D) Combining with sentence salience score
Intuitively, the words in an important sentence
should be assigned a high weight for keyword ex-
traction. In order to leverage the sentence infor-
mation, we adjust a word?s weight by the salience
scores of the sentences containing that word. The
sentence score is calculated based on its cosine sim-
ilarity to the entire meeting. This score is often used
in extractive summarization to select summary sen-
tences (Radev et al, 2001). The cosine similarity
between two vectors, D1 and D2, is defined as:
sim(D1, D2) =
?
i t1it2i??
i t21i ?
??
i t22i
where ti is the term weight for a word wi, for which
we use the TFIDF value.
4.2 Graph-based Methods
For the graph-based approach, we adopt the itera-
tive reinforcement approach from (Wan et al, 2007)
in the hope of leveraging sentence information for
keyword extraction. This algorithm is based on the
assumption that important sentences/words are con-
nected to other important sentences/words.
Four graphs are created: one graph in which sen-
tences are connected to other sentences (S-S graph),
one in which words are connected to other words
(W-W graph), and two graphs connecting words to
sentences with uni-directional edges (W-S and S-W
graphs). Stopwords are removed before the creation
of the graphs so they will be ineligible to be key-
words.
The final weight for a word node depends on its
connection to other words (W-W graph) and other
sentences (W-S graph); similarly, the weight for
a sentence node is dependent on its connection to
other sentences (S-S graph) and other words (S-W
graph). That is,
u = ?UTu+ ?W? T v
v = ?V T v + ?W Tu
where u and v are the weight vectors for sentence
and word nodes respectively, U, V,W, W? represent
the S-S, W-W, S-W, and W-S connections. ? and ?
specify the contributions from the homogeneous and
the heterogeneous nodes. The initial weight is a uni-
form one for the word and sentence vector. Then
the iterative reinforcement algorithm is used until
the node weight values converge (the difference be-
tween scores at two iterations is below 0.0001 for all
nodes) or 5,000 iterations are reached.
We have explored various ways to assign weights
to the edges in the graphs. Based on the results on
the development set, we use the following setup in
this paper:
? W-W Graph: We used a diagonal matrix for
the graph connection, i.e., there is no connec-
tion among words. The self-loop values are
the TFIDF values of the words. This is also
equivalent to using an identity matrix for the
word-word connection and TFIDF as the initial
weight for each vertex in the graph. We investi-
gated other strategies to assign a weight for the
edge between two word nodes; however, so far
the best result we obtained is using this diago-
nal matrix.
? S-W and W-S Graphs: The weight for an
edge between a sentence and a word is the TF
of the word in the sentence multiplied by the
word?s IDF value. These weights are initially
added only to the S-W graph, as in (Wan et al,
2007); then that graph is normalized and trans-
posed to create the W-S graph.
? S-S Graph: The sentence node uses a vector
space model and is composed of the weights of
those words connected to this sentence in the
S-W graph. We then use cosine similarity be-
tween two sentence vectors.
Similar to the above TFIDF framework, we also
use POS filtering for the graph-based approach. Af-
ter the weights for all the words are determined, we
select the top ranked words with the POS restriction.
5 Experimental Results: Automatic
Evaluation
Using the approaches described above, we com-
puted weights for the words and then picked the top
five words as the keywords for a topic. We chose five
keywords since this is the number of keywords that
624
human annotators used as a guideline, and it also
yielded good performance in the development set.
To evaluate system performance, in this section we
use human annotated keywords as references, and
compare the system output to them. The first metric
we use is F-measure, which has been widely used
for this task and other detection tasks. We compare
the system output with respect to each human anno-
tation, and calculate the maximum and the average
F-scores. Note that our keyword evaluation is word-
based. When human annotators choose key phrases
(containing more than one word), we split them into
words and measure the matching words. Therefore,
when the system only generates five keywords, the
upper bound of the recall rate may not be 100%. In
(Liu et al, 2008), a lenient metric is used which ac-
counts for some inflection of words. Since that is
highly correlated with the results using exact word
match, we report results based on strict matching in
the following experiments.
The second metric we use is similar to Pyramid
(Nenkova and Passonneau, 2004), which has been
used for summarization evaluation. Instead of com-
paring the system output with each individual hu-
man annotation, the method creates a ?pyramid?
using all the human annotated keywords, and then
compares system output to this pyramid. The pyra-
mid consists of all the annotated keywords at dif-
ferent levels. Each keyword has a score based on
how many annotators have selected this one. The
higher the score, the higher up the keyword will be in
the pyramid. Then we calculate an oracle score that
a system can obtain when generating k keywords.
This is done by selecting keywords in the decreas-
ing order in terms of the pyramid levels until we
obtain k keywords. Finally for the system hypoth-
esized k keywords, we compute its score by adding
the scores of the keywords that match those in the
pyramid. The system?s performance is measured us-
ing the relative performance of the system?s pyramid
scores divided by the oracle score.
Table 1 shows the results using human transcripts
for different methods on the 21 test meetings (139
topic segments in total). For comparison, we also
show results using the supervised approach as in
(Liu et al, 2008), which is the average of the 21-
fold cross validation. We only show the maximum
F-measure with respect to individual annotations,
since the average scores show similar trend. In ad-
dition, the weighted relative scores already accounts
for the different annotation and human agreement.
Methods F-measure weighted relative score
TFIDF 0.267 0.368
+ POS 0.275 0.370
+ Clustering 0.277 0.367
+ Sent weight 0.290 0.404
Graph 0.258 0.364
Graph+POS 0.277 0.380
Supervised 0.312 0.401
Table 1: Keyword extraction results using human tran-
scripts compared to human annotations.
We notice that for the TFIDF framework, adding
POS information slightly helps the basic TFIDF
method. In all the meetings, our statistics show that
adding POS filtering removed 2.3% of human anno-
tated keywords from the word candidates; therefore,
this does not have a significant negative impact on
the upper bound recall rate, but helps eliminate un-
likely keyword candidates. Using word clustering
does not yield a performance gain, most likely be-
cause of the clustering technique we used ? it does
clustering simply based on word co-occurrence and
does not capture semantic similarity properly.
Combining the term weight with the sentence
salience score improves performance, supporting the
hypothesis that summary sentences and keywords
can reinforce each other. In fact we performed an
analysis of keywords and summaries using the fol-
lowing two statistics:
(1) k = Psummary(wi)Ptopic(wi)
where Psummary(wi) and Ptopic(wi) represent the
the normalized frequency of a keyword wi in the
summary and the entire topic respectively; and
(2) s = PSsummaryPStopic
where PSsummary represents the percentage of the
sentences containing at least one keyword among all
the sentences in the summary, and similarly PStopic
is measured using the entire topic segment. We
found that the average k and s are around 3.42 and
6.33 respectively. This means that keywords are
625
more likely to occur in the summary compared to the
rest of the topic, and the chance for a summary sen-
tence to contain at least one keyword is much higher
than for the other sentences in the topic.
For the graph-based methods, we notice that
adding POS filtering also improves performance,
similar to the TFIDF framework. However, the
graph method does not perform as well as the TFIDF
approach. Comparing with using TFIDF alone, the
graph method (without using POS) yielded worse re-
sults. In addition to using the TFIDF for the word
nodes, information from the sentences is used in the
graph method since a word is linked to sentences
containing this word. The global information in the
S-S graph (connecting a sentence to other sentences
in the document) is propagated to the word nodes.
Unlike the study in (Wan et al, 2007), this infor-
mation does not yield any gain. We did find that the
graph approach performed better in the development
set, but it seems that it does not generalize to this test
set.
Compared to the supervised results, the TFIDF
approach is worse in terms of the individual maxi-
mum F-measure, but achieves similar performance
when using the weighted relative score. However,
the unsupervised TFIDF approach is much simpler
and does not require any annotated data for train-
ing. Therefore it may be easily applied to a new
domain. Again note that these results used word-
based selection. (Liu et al, 2008) investigated
adding bigram key phrases, which we expect to
be independent of these unigram-based approaches
and adding bigram phrases will yield further per-
formance gain for the unsupervised approach. Fi-
nally, we analyzed if the system?s keyword ex-
traction performance is correlated with human an-
notation disagreement using the unsupervised ap-
proach (TFIDF+POS+Sent weight). The correla-
tion (Spearman?s ? value) between the system?s
F-measure and the three-annotator consistency on
the 27 meetings is 0.5049 (p=0.0072). This indi-
cates that for the meetings with a high disagreement
among human annotators, it is also challenging for
the automatic systems.
Table 2 shows the results using ASR output for
various approaches. The performance measure is
the same as used in Table 1. We find that in gen-
eral, there is a performance degradation compared
to using human transcripts, which is as expected.
We found that only 59.74% of the human annotated
keywords appear in ASR output, that is, the upper
bound of recall is very low. The TFIDF approach
still outperforms the graph method. Unlike on hu-
man transcripts, the addition of information sources
in the TFIDF approach did not yield significant per-
formance gain. A big difference from the human
transcript condition is the use of sentence weight-
ing ? adding it degrades performance in ASR, in
contrast to the improvement in human transcripts.
This is possibly because the weighting of the sen-
tences is poor when there are many recognition er-
rors from content words. In addition, compared to
the supervised results, the TFIDF method has sim-
ilar maximum F-measure, but is slightly worse us-
ing the weighted score. Further research is needed
for the ASR condition to investigate better modeling
approaches.
Methods F-measure weighted relative score
TFIDF 0.191 0.257
+ POS 0.196 0.259
+ Clustering 0.196 0.259
+ Sent weigh 0.178 0.241
Graph 0.173 0.223
Graph+POS 0.183 0.233
Supervised 0.197 0.269
Table 2: Keyword extraction results using ASR output.
6 Experimental Results: Human
Evaluation
Given the disagreement among human annotators,
one question we need to answer is whether F-
measure or even the weighted relative scores com-
pared with human annotations are appropriate met-
rics to evaluate system-generated keywords. For
example, precision measures among the system-
generated keywords how many are correct. How-
ever, this does not measure if the unmatched system-
generated keywords are bad or acceptable. We
therefore performed a small scale human evaluation.
We selected four topic segments from four differ-
ent meetings, and gave output from different sys-
tems to five human subjects. The subjects ranged
in age from 22 to 63, and all but one had only basic
knowledge of computers. We first asked the eval-
626
uators to read the entire topic transcript, and then
presented them with the system-generated keywords
(randomly ordered by different systems). For com-
parison, the keywords annotated by our three hu-
man annotators were also included without reveal-
ing which sets of keywords were generated by a
human and which by a computer. Because there
was such disagreement between annotators regard-
ing what made good keywords, we instead asked our
evaluators to mark any words that were definitely
not keywords. Systems that produced more of these
rejected words (such as ?basically? or ?mmm-hm?)
are assumed to be worse than those containing fewer
rejected words. We then measured the percentage of
rejected keywords for each system/annotator. The
results are shown in Table 3. Not surprisingly, the
human annotations rank at the top. Overall, we find
human evaluation results to be consistent with the
automatic evaluation metrics in terms of the ranking
of different systems.
Systems Rejection rate
Annotator 2 8%
Annotator 3 19%
Annotator 1 25%
TFIDF + POS 28%
TFIDF 30%
Table 3: Human evaluation results: percentage of the re-
jected keywords by human evaluators for different sys-
tems/annotators.
Note this rejection rate is highly related to the re-
call/precision measure in the sense that it measures
how many keywords are acceptable (or rejected)
among the system generated ones. However, instead
of comparing to a fixed set of human annotated key-
words (e.g., five) and using that as a gold standard
to compute recall/precision, in this evaluation, the
human evaluator may have a larger set of accept-
able keywords in their mind. We also measured the
human evaluator agreement regarding the accepted
or bad keywords. We found that the agreement on
a bad keyword among five, four, and three human
evaluator is 10.1%, 14.8%, and 10.1% respectively.
This suggests that humans are more likely to agree
on a bad keyword selection compared to agreement
on the selected keywords, as discussed in Section 3
(even though the data sets in these two analysis are
not the same). Another observation from the human
evaluation is that sometimes a person rejects a key-
word from one system output, but accepts that on
the list from another system. We are not sure yet
whether this is the inconsistency from human evalu-
ators or whether the judgment is based on a word?s
occurrence with other provided keywords and thus
some kind of semantic coherence. Further investi-
gation on human evaluation is still needed.
7 Conclusions and Future Work
In this paper, we evaluated unsupervised keyword
extraction performance for the meeting domain, a
genre that is significantly different from most pre-
vious work. We compared several different ap-
proaches using the transcripts of the ICSI meeting
corpus. Our results on the human transcripts show
that the simple TFIDF based method is very compet-
itive. Adding additional knowledge such as POS and
sentence salience score helps improve performance.
The graph-based approach performs less well in this
task, possibly because of the lack of structure in
this domain. We use different performance measure-
ments, including F-measure with respect to individ-
ual human annotations and a weighted metric rela-
tive to the oracle system performance. We also per-
formed a new human evaluation for this task and our
results show consistency with the automatic mea-
surement. In addition, experiments on the ASR out-
put show performance degradation, but more impor-
tantly, different patterns in terms of the contributions
of information sources compared to using human
transcripts. Overall the unsupervised approaches are
simple but effective; however, system performance
compared to the human performance is still low,
suggesting more work is needed for this domain.
For the future work, we plan to investigate dif-
ferent weighting algorithms for the graph-based ap-
proach. We also need a better way to decide the
number of keywords to generate instead of using a
fixed number. Furthermore, since there are multiple
speakers in the meeting domain, we plan to incor-
porate speaker information in various approaches.
More importantly, we will perform a more rigorous
human evaluation, and also use extrinsic evaluation
to see whether automatically generated keywords fa-
cilitate tasks such as information retrieval or meeting
browsing.
627
Acknowledgments
This work is supported by NSF award IIS-0714132.
Any opinions expressed in this work are those of the
authors and do not necessarily reflect the views of
NSF.
References
T. Brants. 2000. TnT ? a statistical part-of-speech tagger.
In Proceedings of the 6th Applied NLP Conference.
S. Brin and L. Page. 1998. The anatomy of a large-scale
hypertextual web search engine. Computer Networks
and ISDN Systems, 30.
I. Bulyko, M. Ostendorf, M. Siu, T. Ng, A. Stolcke, and
O. Cetin. 2007. Web resources for language modeling
in conversational speech recognition. ACM Transac-
tions on Speech and Language Processing, 5:1?25.
A. Desilets, B.D. Bruijn, and J. Martin. 2002. Extracting
keyphrases from spoken audio documents. In Infor-
mation Retrieval Techniques for Speech Applications,
pages 339?342.
E. Frank, G.W. Paynter, I.H. Witten, C. Gutwin, and C.G.
Nevill-Manning. 1999. Domain-specific keyphrase
extraction. In Proceedings of IJCAI, pages 688?673.
M. Galley, K. McKeown, E. Fosler-Lussier, and H. Jing.
2003. Discourse segmentation of multi-party conver-
sation. In Proceedings of ACL.
A. Hulth. 2003. Improved automatic keyword extraction
given more linguistic knowledge. In Proceedings of
EMNLP, pages 216?223.
D. Inkpen and A. Desilets. 2004. Extracting
semantically-coherent keyphrases from speech. Cana-
dian Acoustics Association, 32:130?131.
A. Janin, D. Baron, J. Edwards, D. Ellis, G . Gelbart,
N. Morgan, B. Peskin, T. Pfau, E. Shriberg, A. Stolcke,
and C. Wooters. 2003. The ICSI meeting corpus. In
Proceedings of ICASSP.
Y.H. Kerner, Z. Gross, and A. Masa. 2005. Automatic
extraction and learning of keyphrases from scientific
articles. In Computational Linguistics and Intelligent
Text Processing, pages 657?669.
F. Liu, F. Liu, and Y. Liu. 2008. Automatic keyword
extraction for the meeting corpus using supervised ap-
proach and bigram expansion. In Proceedings of IEEE
SLT.
Y. Matsuo and M. Ishizuka. 2004. Keyword extraction
from a single document using word co-occurrence sta-
tistical information. International Journal on Artifi-
cial Intelligence, 13(1):157?169.
C. Munteanu, G. Penn, and R. Baecker. 2007. Web-
based language modeling for automatic lecture tran-
scription. In Proceedings of Interspeech.
G. Murray, S. Renals, J. Carletta, and J. Moore. 2005.
Evaluating automatic summaries of meeting record-
ings. In Proceedings of ACL 2005 MTSE Workshop,
pages 33?40.
A. Nenkova and R. Passonneau. 2004. Evaluating con-
tent selection in summarization: the pyramid method.
In Proceedings of HLT/NAACL.
L. Plas, V. Pallotta, M. Rajman, and H. Ghorbel. 2004.
Automatic keyword extraction from spoken text. a
comparison of two lexical resources: the EDR and
WordNet. In Proceedings of the LREC.
D. Radev, S. Blair-Goldensohn, and Z. Zhang. 2001. Ex-
periments in single and multi-document summariza-
tion using MEAD. In Proceedings of The First Docu-
ment Understanding Conference.
I. Rogina. 2002. Lecture and presentation tracking in an
intelligent meeting room. In Proceedings of ICMI.
E. Shriberg, R. Dhillon, S. Bhagat, J. Ang, and H. Carvey.
2004. The ICSI meeting recorder dialog act (MRDA)
corpus. In Proceedings of SIGDial Workshop, pages
97?100.
A. Stolcke. 2002. SRILM ? An extensible language
modeling toolkit. In Proceedings of ICSLP, pages
901?904.
P.D. Turney. 2000. Learning algorithms for keyphrase
extraction. Information Retrieval, 2:303?336.
P.D. Turney. 2002. Mining the web for lexical knowl-
edge to improve keyphrase extraction: Learning from
labeled and unlabeled data. In National Research
Council, Institute for Information Technology, Techni-
cal Report ERB-1096.
P.D. Turney. 2003. Coherent keyphrase extraction via
web mining. In Proceedings of IJCAI, pages 434?439.
X. Wan, J. Yang, and J. Xiao. 2007. Towards an iter-
ative reinforcement approach for simultaneous docu-
ment summarization and keyword extraction. In Pro-
ceedings of ACL, pages 552?559.
C.H. Wu, C.L. Huang, C.S. Hsu, and K.M. Lee. 2007.
Speech retrieval using spoken keyword extraction and
semantic verification. In Proceedings of IEEE Region
10 Conference, pages 1?4.
Q. Zhu, A. Stolcke, B. Chen, and N. Morgan. 2005.
Using MLP features in SRI?s conversational speech
recognition system. In Proceedings of Interspeech.
628
Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 261?264,
Suntec, Singapore, 4 August 2009.
c?2009 ACL and AFNLP
From Extractive to Abstractive Meeting Summaries: Can It Be Done by
Sentence Compression?
Fei Liu and Yang Liu
Computer Science Department
The University of Texas at Dallas
Richardson, TX 75080, USA
{feiliu, yangl}@hlt.utdallas.edu
Abstract
Most previous studies on meeting summariza-
tion have focused on extractive summariza-
tion. In this paper, we investigate if we can
apply sentence compression to extractive sum-
maries to generate abstractive summaries. We
use different compression algorithms, includ-
ing integer linear programming with an addi-
tional step of filler phrase detection, a noisy-
channel approach using Markovization for-
mulation of grammar rules, as well as hu-
man compressed sentences. Our experiments
on the ICSI meeting corpus show that when
compared to the abstractive summaries, using
sentence compression on the extractive sum-
maries improves their ROUGE scores; how-
ever, the best performance is still quite low,
suggesting the need of language generation for
abstractive summarization.
1 Introduction
Meeting summaries provide an efficient way for people
to browse through the lengthy recordings. Most cur-
rent research on meeting summarization has focused on
extractive summarization, that is, it extracts important
sentences (or dialogue acts) from speech transcripts, ei-
ther manual transcripts or automatic speech recogni-
tion (ASR) output. Various approaches to extractive
summarization have been evaluated recently. Popular
unsupervised approaches are maximum marginal rele-
vance (MMR), latent semantic analysis (LSA) (Mur-
ray et al, 2005a), and integer programming (Gillick et
al., 2009). Supervised methods include hidden Markov
model (HMM), maximum entropy, conditional ran-
dom fields (CRF), and support vector machines (SVM)
(Galley, 2006; Buist et al, 2005; Xie et al, 2008;
Maskey and Hirschberg, 2006). (Hori et al, 2003) used
a word based speech summarization approach that uti-
lized dynamic programming to obtain a set of words to
maximize a summarization score.
Most of these summarization approaches aim for
selecting the most informative sentences, while less
attempt has been made to generate abstractive sum-
maries, or compress the extracted sentences and merge
them into a concise summary. Simply concatenating
extracted sentences may not comprise a good sum-
mary, especially for spoken documents, since speech
transcripts often contain many disfluencies and are re-
dundant. The following example shows two extractive
summary sentences (they are from the same speaker),
and part of the abstractive summary that is related to
these two extractive summary sentences. This is an ex-
ample from the ICSI meeting corpus (see Section 2.1
for more information on the data).
Extractive summary sentences:
Sent1: um we have to refine the tasks more and more which
of course we haven?t done at all so far in order to avoid this
rephrasing
Sent2: and uh my suggestion is of course we we keep the
wizard because i think she did a wonderful job
Corresponding abstractive summary:
the group decided to hire the wizard and continue with the
refinement...
In this paper, our goal is to answer the question if
we can perform sentence compression on an extrac-
tive summary to improve its readability and make it
more like an abstractive summary. Compressing sen-
tences could be a first step toward our ultimate goal
of creating an abstract for spoken documents. Sen-
tence compression has been widely studied in language
processing. (Knight and Marcu, 2002; Cohn and Lap-
ata, 2009) learned rewriting rules that indicate which
words should be dropped in a given context. (Knight
and Marcu, 2002; Turner and Charniak, 2005) applied
the noisy-channel framework to predict the possibil-
ities of translating a sentence to a shorter word se-
quence. (Galley and McKeown, 2007) extended the
noisy-channel approach and proposed a head-driven
Markovization formulation of synchronous context-
free grammar (SCFG) deletion rules. Unlike these ap-
proaches that need a training corpus, (Clarke and La-
pata, 2008) encoded the language model and a variety
of linguistic constraints as linear inequalities, and em-
ployed the integer programming approach to find a sub-
set of words that maximize an objective function.
Our focus in this paper is not on new compression al-
gorithms, but rather on using compression to bridge the
gap of extractive and abstractive summarization. We
use different automatic compression algorithms. The
first one is the integer programming (IP) framework,
where we also introduce a filler phrase (FP) detection
261
module based on the Web resources. The second one
uses the SCFG that considers the grammaticality of the
compressed sentences. Finally, as a comparison, we
also use human compression. All of these compressed
sentences are compared to abstractive summaries. Our
experiments using the ICSI meeting corpus show that
compressing extractive summaries can improve human
readability and the ROUGE scores against the refer-
ence abstractive summaries.
2 Sentence Compression of Extractive
Summaries
2.1 Corpus
We used the ICSI meeting corpus (Janin et al, 2003),
which contains naturally occurring meetings, each
about an hour long. All the meetings have been tran-
scribed and annotated with dialogue acts (DAs), top-
ics, abstractive and extractive summaries (Shriberg et
al., 2004; Murray et al, 2005b). In this study, we use
the extractive and abstractive summaries of 6 meetings
from this corpus. These 6 meetings were chosen be-
cause they have been used previously in other related
studies, such as summarization and keyword extraction
(Murray et al, 2005a). On average, an extractive sum-
mary contains 76 sentences
1
(1252 words), and an ab-
stractive summary contains 5 sentences (111 words).
2.2 Compression Approaches
2.2.1 Human Compression
The data annotation was conducted via Amazon Me-
chanical Turk
2
. Human annotators were asked to gen-
erate condensed version for each of the DAs in the ex-
tractive summaries. The compression guideline is sim-
ilar to (Clarke and Lapata, 2008). The annotators were
asked to only remove words from the original sentence
while preserving most of the important meanings, and
make the compressed sentence as grammatical as pos-
sible. The annotators can leave the sentence uncom-
pressed if they think no words need to be deleted; how-
ever, they were not allowed to delete the entire sen-
tence. Since the meeting transcripts are not as readable
as other text genres, we may need a better compression
guideline for human annotators. Currently we let the
annotators make their own judgment what is an appro-
priate compression for a spoken sentence.
We split each extractive meeting summary sequen-
tially into groups of 10 sentences, and asked 6 to 10
online workers to compress each group. Then from
these results, another human subject selected the best
annotation for each sentence. We also asked this hu-
man judge to select the 4-best compressions. However,
in this study, we only use the 1-best annotation result.
We would like to do more analysis on the 4-best results
in the future.
1
The extractive units are DAs. We use DAs and sentences
interchangeably in this paper when there is no ambiguity.
2
http://www.mturk.com/mturk/welcome
2.2.2 Filler Phrase Detection
We define filler phrases (FPs) as the combination of
two or more words, which could be discourse markers
(e.g., I mean, you know), editing terms, as well as some
terms that are commonly used by human but without
critical meaning, such as, ?for example?, ?of course?,
and ?sort of?. Removing these fillers barely causes any
information loss. We propose to use web information
to automatically generate a list of filler phrases and fil-
ter them out in compression.
For each extracted summary sentence of the 6 meet-
ings, we use it as a query to Google and examine the top
N returned snippets (N is 400 in our experiments). The
snippets may not contain all the words in a sentence
query, but often contain frequently occurring phrases.
For example, ?of course? can be found with high fre-
quency in the snippets. We collect all the phrases that
appear in both the extracted summary sentences and the
snippets with a frequency higher than three. Then we
calculate the inverse sentence frequency (ISF) for these
phrases using the entire ICSI meeting corpus. The ISF
score of a phrase i is:
isf
i
=
N
N
i
where N is the total number of sentences and N
i
is the
number of sentences containing this phrase. Phrases
with low ISF scores mean that they appear in many oc-
casions and are not domain- or topic-indicative. These
are the filler phrases we want to remove to compress
a sentence. The three phrases we found with the low-
est ISF scores are ?you know?, ?i mean? and ?i think?,
consistent with our intuition.
We also noticed that not all the phrases with low
ISF scores can be taken as FPs (?we are? would be a
counter example). We therefore gave the ranked list of
FPs (based on ISF values) to a human subject to select
the proper ones. The human annotator crossed out the
phrases that may not be removable for sentence com-
pression, and also generated simple rules to shorten
some phrases (such as turning ?a little bit? into ?a bit?).
This resulted in 50 final FPs and about a hundred sim-
plification rules. Examples of the final FPs are: ?you
know?, ?and I think?, ?some of?, ?I mean?, ?so far?, ?it
seems like?, ?more or less?, ?of course?, ?sort of?, ?so
forth?, ?I guess?, ?for example?. When using this list
of FPs and rules for sentence compression, we also re-
quire that an FP candidate in the sentence is considered
as a phrase in the returned snippets by the search en-
gine, and its frequency in the snippets is higher than a
pre-defined threshold.
2.2.3 Compression Using Integer Programming
We employ the integer programming (IP) approach in
the same way as (Clarke and Lapata, 2008). Given an
utterance S = w
1
, w
2
, ..., w
n
, the IP approach forms a
compression of this utterance only by dropping words
and preserving the word sequence that maximizes an
objective function, defined as the sum of the signifi-
262
cance scores of the consisting words and n-gram prob-
abilities from a language model:
max ? ?
n
?
i=1
y
i
? Sig(w
i
)
+ (1 ? ?) ?
n?2
?
i=0
n?1
?
j=i+1
n
?
k=j+1
x
ijk
? P (w
k
|w
i
, w
j
)
where y
i
and x
ijk
are two binary variables: y
i
= 1
represents that word w
i
is in the compressed sentence;
x
ijk
= 1 represents that the sequence w
i
, w
j
, w
k
is in the compressed sentence. A trade-off parameter
? is used to balance the contribution from the signif-
icance scores for individual words and the language
model scores. Because of space limitation, we omit-
ted the special sentence beginning and ending symbols
in the formula above. More details can be found in
(Clarke and Lapata, 2008). We only used linear con-
straints defined on the variables, without any linguistic
constraints.
We use the lp solve toolkit.
3
The significance score
for each word is its TF-IDF value (term frequency ?
inverse document frequency). We trained a language
model using SRILM
4
on broadcast news data to gen-
erate the trigram probabilities. We empirically set ? as
0.7, which gives more weight to the word significance
scores. This IP compression method is applied to the
sentences after filler phrases (FPs) are filtered out. We
refer to the output from this approach as ?FP + IP?.
2.2.4 Compression Using Lexicalized Markov
Grammars
The last sentence compression method we use is the
lexicalized Markov grammar-based approach (Galley
and McKeown, 2007) with edit word detection (Char-
niak and Johnson, 2001). Two outputs were generated
using this method with different compression rates (de-
fined as the number of words preserved in the com-
pression divided by the total number of words in the
original sentence).
5
We name them ?Markov (S1)? and
?Markov (S2)? respectively.
3 Experiments
First we perform human evaluation for the compressed
sentences. Again we use the Amazon Mechanical Turk
for the subjective evaluation process. For each extrac-
tive summary sentence, we asked 10 human subjects to
rate the compressed sentences from the three systems,
as well as the human compression. This evaluation was
conducted on three meetings, containing 244 sentences
in total. Participants were asked to read the original
sentence and assign scores to each of the compressed
sentences for its informativeness and grammaticality
respectively using a 1 to 5 scale. An overall score is
calculated as the average of the informativeness and
grammaticality scores. Results are shown in Table 1.
3
http://www.geocities.com/lpsolve
4
http://www.speech.sri.com/projects/srilm/
5
Thanks to Michel Galley to help generate these output.
For a comparison, we also include the ROUGE-1 F-
scores (Lin, 2004) of each system output against the
human compressed sentences.
Approach Info. Gram. Overall R-1 F (%)
Human 4.35 4.38 4.37 -
Markov (S1) 3.64 3.79 3.72 88.76
Markov (S2) 2.89 2.76 2.83 62.99
FP + IP 3.70 3.95 3.82 85.83
Table 1: Human evaluation results. Also shown is the
ROUGE-1 (unigram match) F-score of different sys-
tems compared to human compression.
We can see from the table that as expected, the hu-
man compression yields the best performance on both
informativeness and grammaticality. ?FP + IP? and
?Markov (S1)? approaches also achieve satisfying per-
formance under both evaluation metrics. The relatively
low scores for ?Markov (S2)? output are partly due to
its low compression rate (see Table 2 for the length in-
formation). As an example, we show below the com-
pressed sentences from human and systems for the first
sentence in the example in Sec 1.
Human: we have to refine the tasks in order to avoid
rephrasing
Markov (S1): we have to refine the tasks more and more
which we haven?t done in order to avoid this rephrasing
Markov (S2): we have to refine the tasks which we haven?t
done order to avoid this rephrasing
FP + IP: we have to refine the tasks more and more which
we haven?t done to avoid this rephrasing
Since our goal is to answer the question if we can
use sentence compression to generate abstractive sum-
maries, we compare the compressed summaries, as
well as the original extractive summaries, against the
reference abstractive summaries. The ROUGE-1 re-
sults along with the word compression ratio for each
compression approach are shown in Table 2. We can
see that all of the compression algorithms yield bet-
ter ROUGE score than the original extractive sum-
maries. Take Markov (S2) as an example. The recall
rate dropped only 8% (from the original 66% to 58%)
when only 53% words in the extractive summaries are
preserved. This demonstrates that it is possible for the
current sentence compression systems to greatly con-
dense the extractive summaries while preserving the
desirable information, and thus yield summaries that
are more like abstractive summaries. However, since
the abstractive summaries are much shorter than the ex-
tractive summaries (even after compression), it is not
surprising to see the low precision results as shown in
Table 2. We also observe some different patterns be-
tween the ROUGE scores and the human evaluation
results in Table 1. For example, Markov (S2) has the
highest ROUGE result, but worse human evaluation
score than other methods.
To evaluate the length impact and to further make
263
All Sent. Top Sent.
Approach Word ratio (%) P(%) R(%) F(%) P(%) R(%) F(%)
Original extractive summary 100 7.58 66.06 12.99 29.98 34.29 31.83
Human compression 65.58 10.43 63.00 16.95 34.35 37.39 35.79
Markov (S1) 67.67 10.15 61.98 16.41 34.24 36.88 35.46
Markov (S2) 53.28 11.90 58.14 18.37 32.23 34.96 33.49
FP + IP 76.38 9.11 59.85 14.78 31.82 35.62 33.57
Table 2: Compression ratio of different systems and ROUGE-1 scores compared to human abstractive summaries.
the extractive summaries more like abstractive sum-
maries, we conduct an oracle experiment: we compute
the ROUGE score for each of the extractive summary
sentences (the original sentence or the compressed sen-
tence) against the abstract, and select the sentences
with the highest scores until the number of selected
words is about the same as that in the abstract.
6
The
ROUGE results using these selected top sentences are
shown in the right part of Table 2. There is some dif-
ference using all the sentences vs. the top sentences
regarding the ranking of different compression algo-
rithms (comparing the two blocks in Table 2).
From Table 2, we notice significant performance im-
provement when using the selected sentences to form a
summary. These results indicate that, it may be possi-
ble to convert extractive summaries to abstractive sum-
maries. On the other hand, this is an oracle result since
we compare the extractive summaries to the abstract for
sentence selection. In the real scenario, we will need
other methods to rank sentences. Moreover, the current
ROUGE score is not very high. This suggests that there
is a limit using extractive summarization and sentence
compression to form abstractive summaries, and that
sophisticated language generation is still needed.
4 Conclusion
In this paper, we attempt to bridge the gap between ex-
tractive and abstractive summaries by performing sen-
tence compression. Several compression approaches
are employed, including an integer programming based
framework, where we also introduced a filler phrase de-
tection module, the lexicalized Markov grammar-based
approach, as well as human compression. Results show
that, while sentence compression provides a promising
way of moving from extractive summaries toward ab-
stracts, there is also a potential limit along this direc-
tion. This study uses human annotated extractive sum-
maries. In our future work, we will evaluate using auto-
matic extractive summaries. Furthermore, we will ex-
plore the possibility of merging compressed extractive
sentences to generate more unified summaries.
References
A. Buist, W. Kraaij, and S. Raaijmakers. 2005. Automatic
summarization of meeting data: A feasibility study. In
Proc. of CLIN.
6
Thanks to Shasha Xie for generating these results.
E. Charniak and M. Johnson. 2001. Edit detection and pars-
ing for transcribed speech. In Proc. of NAACL.
J. Clarke and M. Lapata. 2008. Global inference for sentence
compression: An integer linear programming approach.
Journal of Artificial Intelligence Research, 31:399?429.
T. Cohn and M. Lapata. 2009. Sentence compression as tree
transduction. Journal of Artificial Intelligence Research.
M. Galley and K. McKeown. 2007. Lexicalized markov
grammars for sentence compression. In Proc. of
NAACL/HLT.
M. Galley. 2006. A skip-chain conditional random field
for ranking meeting utterances by importance. In Proc.
of EMNLP.
D. Gillick, K. Riedhammer, B. Favre, and D. Hakkani-Tur.
2009. A global optimization framework for meeting sum-
marization. In Proc. of ICASSP.
C. Hori, S. Furui, R. Malkin, H. Yu, and A. Waibel. 2003.
A statistical approach to automatic speech summarization.
Journal on Applied Signal Processing, 2003:128?139.
A. Janin, D. Baron, J. Edwards, D. Ellis, G. Gelbart, N. Mor-
gan, B. Peskin, T. Pfau, E. Shriberg, A. Stolcke, and
C. Wooters. 2003. The ICSI meeting corpus. In Proc.
of ICASSP.
K. Knight and D. Marcu. 2002. Summarization beyond
sentence extraction: A probabilistic approach to sentence
compression. Artificial Intelligence, 139:91?107.
C. Lin. 2004. Rouge: A package for automatic evaluation
of summaries. In Proc. of ACL Workshop on Text Summa-
rization Branches Out.
S. Maskey and J. Hirschberg. 2006. Summarizing speech
without text using hidden markov models. In Proc. of
HLT/NAACL.
G. Murray, S. Renals, and J. Carletta. 2005a. Extractive
summarization of meeting recordings. In Proc. of INTER-
SPEECH.
G. Murray, S. Renals, J. Carletta, and J. Moore. 2005b. Eval-
uating automatic summaries of meeting recordings. In
Proc. of ACL 2005 MTSE Workshop.
E. Shriberg, R. Dhillon, S. Bhagat, J. Ang, and H. Carvey.
2004. The ICSI meeting recorder dialog act (MRDA)
corpus. In Proc. of SIGdial Workshop on Discourse and
Dialogue.
J. Turner and E. Charniak. 2005. Supervised and unsuper-
vised learning for sentence compression. In Proc. of ACL.
S. Xie, Y. Liu, and H. Lin. 2008. Evaluating the effective-
ness of features and sampling in extractive meeting sum-
marization. In Proc. of IEEE Workshop on Spoken Lan-
guage Technology.
264
Proceedings of the 9th SIGdial Workshop on Discourse and Dialogue, pages 80?83,
Columbus, June 2008. c?2008 Association for Computational Linguistics
What Are Meeting Summaries? An Analysis of Human Extractive
Summaries in Meeting Corpus
Fei Liu, Yang Liu
Erik Jonsson School of Engineering and Computer Science
The University of Texas at Dallas
Richardson, TX, USA
{feiliu,yangl}@hlt.utdallas.edu
Abstract
Significant research efforts have been devoted to
speech summarization, including automatic ap-
proaches and evaluation metrics. However, a fun-
damental problem about what summaries are for the
speech data and whether humans agree with each
other remains unclear. This paper performs an anal-
ysis of human annotated extractive summaries us-
ing the ICSI meeting corpus with an aim to examine
their consistency and the factors impacting human
agreement. In addition to using Kappa statistics and
ROUGE scores, we also proposed a sentence dis-
tance score and divergence distance as a quantitative
measure. This study is expected to help better define
the speech summarization problem.
1 Introduction
With the fast development of recording and storage tech-
niques in recent years, speech summarization has re-
ceived more attention. A variety of approaches have
been investigated for speech summarization, for exam-
ple, maximum entropy, conditional random fields, latent
semantic analysis, support vector machines, maximum
marginal relevance (Maskey and Hirschberg, 2003; Hori
et al, 2003; Buist et al, 2005; Galley, 2006; Murray et
al., 2005; Zhang et al, 2007; Xie and Liu, 2008). These
studies used different domains, such as broadcast news,
lectures, and meetings. In these approaches, different in-
formation sources have been examined from both text and
speech related features (e.g., prosody, speaker activity,
turn-taking, discourse).
How to evaluate speech summaries has also been stud-
ied recently, but so far there is no consensus on eval-
uation yet. Often the goal in evaluation is to develop
an automatic metric to have a high correlation with hu-
man evaluation scores. Different methods have been used
in the above summarization research to compare system
generated summaries with human annotation, such as F-
measure, ROUGE, Pyramid, sumACCY (Lin and Hovy,
2003; Nenkova and Passonneau, 2004; Hori et al, 2003).
Typically multiple reference human summaries are used
in evaluation in order to account for the inconsistency
among human annotations.
While there have been efforts on speech summariza-
tion approaches and evaluation, some fundamental prob-
lems are still unclear. For example, what are speech sum-
maries? Do humans agree with each other on summary
extraction? In this paper, we focus on the meeting do-
main, one of the most challenging speech genre, to an-
alyze human summary annotation. Meetings often have
several participants. Its speech is spontaneous, contains
disfluencies, and lacks structure. These all post new chal-
lenges to the consensus of human extracted summaries.
Our goal in this study is to investigate the variation of
human extractive summaries, and help to better under-
stand the gold standard reference summaries for meet-
ing summarization. This paper aims to answer two key
questions: (1) How much variation is there in human ex-
tractive meeting summaries? (2) What are the factors
that may impact interannotator agreement? We use three
different metrics to evaluate the variation among human
summaries, including Kappa statistic, ROUGE score, and
a new proposed divergence distance score to reflect the
coherence and quality of an annotation.
2 Corpus Description
We use the ICSI meeting corpus (Janin et al, 2003) which
contains 75 naturally-occurred meetings, each about an
hour long. All of them have been transcribed and anno-
tated with dialog acts (DA) (Shriberg et al, 2004), top-
ics, and abstractive and extractive summaries in the AMI
project (Murray et al, 2005).
We selected 27 meetings from this corpus. Three anno-
tators (undergraduate students) were recruited to extract
summary sentences on a topic basis using the topic seg-
ments from the AMI annotation. Each sentence corre-
sponds to one DA annotated in the corpus. The annota-
tors were told to use their own judgment to pick summary
sentences that are informative and can preserve discus-
sion flow. The recommended percentages for the selected
summary sentences and words were set to 8.0% and
16.0% respectively. Human subjects were provided with
both the meeting audio files and an annotation Graphi-
80
cal User Interface, from which they can browse the man-
ual transcripts and see the percentage of the currently se-
lected summary sentences and words.
We refer to the above 27 meetings Data set I in this
paper. In addition, some of our studies are performed
based on the 6 meeting used in (Murray et al, 2005),
for which we have human annotated summaries using 3
different guidelines:
? Data set II: summary annotated on a topic basis. This is
a subset of the 27 annotated meetings above.
? Data set III: annotation is done for the entire meeting
without topic segments.
? Data set IV: the extractive summaries are from the AMI
annotation (Murray et al, 2005).
3 Analysis Results
3.1 Kappa Statistic
Kappa coefficient (Carletta, 1996) is commonly used
as a standard to reflect inter-annotator agreement. Ta-
ble 1 shows the average Kappa results, calculated for
each meeting using the data sets described in Section 2.
Compared to Kappa score on text summarization, which
is reported to be 0.38 by (Mani et al, 2002) on a set
of TREC documents, the inter-annotator agreement on
meeting corpus is lower. This is likely due to the dif-
ference between the meeting style and written text.
Data Set I II III IV
Avg-Kappa 0.261 0.245 0.335 0.290
Table 1: Average Kappa scores on different data sets.
There are several other observations from Table 1.
First, comparing the results for Data Set (II) and (III),
both containing six meetings, the agreement is higher
for Data Set (III). Originally, we expected that by di-
viding the transcript into several topics, human subjects
can focus better on each topic discussed during the meet-
ing. However, the result does not support this hypoth-
esis. Moreover, the Kappa result of Data Set (III) also
outperforms that of Data Set (IV). The latter data set is
from the AMI annotation, where they utilized a different
annotation scheme: the annotators were asked to extract
dialog acts that are highly relevant to the given abstrac-
tive meeting summary. Contrary to our expectation, the
Kappa score in this data set is still lower than that of Data
Set (III), which used a direct sentence extraction scheme
on the whole transcript. This suggests that even using
the abstracts as a guidance, people still have a high varia-
tion in extracting summary sentences. We also calculated
the pairwise Kappa score between annotations in differ-
ent data sets. The inter-group Kappa score is much lower
than those of the intragroup agreement, most likely due
to the different annotation specifications used in the two
different data sets.
3.2 Impacting Factors
We further analyze inter-annotator agreement with re-
spect to two factors: topic length and meeting partic-
ipants. All of the following experiments are based on
Data Set (I) with 27 meetings.
We computed Kappa statistic for each topic instead of
the entire meeting. The distribution of Kappa score with
respect to the topic length (measured using the number of
DAs) is shown in Figure 1. When the topic length is less
than 100, Kappa scores vary greatly, from -0.065 to 1.
Among the entire range of different topic lengths, there
seems no obvious relationship between the Kappa score
and the topic length (a regression from the data points
does not suggest a fit with an interpretable trend).
-0.2
0
0.2
0.4
0.6
0.8
1
1.2
0 200 400 600 800 1000 1200 1400
Topic length
-0.2
0
0.2
0.4
0.6
0.8
1
1.2
0 200 400 600 800 1000 1200 1400
Topic length
K
ap
pa
 s
co
re
Figure 1: Relationship between Kappa score and topic length.
Using the same Kappa score for each topic, we also in-
vestigated its relationship with the number of speakers in
that topic. Here we focused on the topic segments longer
than a threshold (with more than 60 DAs) as there seems
to be a wide range of Kappa results when the topic is
short (in Figure 1). Table 2 shows the average Kappa
score for these long topics, using the number of speak-
ers in the topic as the variable. We notice that when the
speaker number varies from 4 to 7, kappa scores grad-
ually decrease with the increasing of speaker numbers.
This phenomenon is consistent with our intuition. Gener-
ally the more participants are involved in a conversation,
the more discussions can take place. Human annotators
feel more ambiguity in selecting summary sentences for
the discussion part. The pattern does not hold for other
speaker numbers, namely, 2, 3, and 8. This might be due
to a lack of enough data points, and we will further ana-
lyze this in the future research.
# of speakers # of topics Avg Kappa score
2 2 0.204
3 6 0.182
4 26 0.29
5 26 0.249
6 33 0.226
7 19 0.221
8 7 0.3
Table 2: Average Kappa score with respect to the number of
speakers after removing short topics.
3.3 ROUGE Score
ROUGE (Lin and Hovy, 2003) has been adopted as
a standard evaluation metric in various summarization
tasks. It is computed based on the n-gram overlap be-
tween a summary and a set of reference summaries.
Though the Kappa statistics can measure human agree-
ment on sentence selection, it does not account for the
fact that different annotators choose different sentences
81
that are similar in content. ROUGE measures the word
match and thus can compensate this problem of Kappa.
Table 3 shows the ROUGE-2 and ROUGE-SU4 F-
measure results. For each annotator, we computed
ROUGE scores using other annotators? summaries as ref-
erences. For Data Set (I), we present results for each an-
notator, since one of our goals is to evaluate the qual-
ity of different annotator?s summary annotation. The low
ROUGE scores suggest the large variation among human
annotations. We can see from the table that annotator
1 has the lowest ROUGE score and thus lowest agree-
ment with the other two annotators in Data Set (I). The
ROUGE score for Data Set (III) is higher than the others.
This is consistent with the result using Kappa statistic:
the more sentences two summaries have in common, the
more overlapped n-grams they tend to share.
ROUGE-2 ROUGE-SU4
Annotator 1 0.407 0.457
data (I) Annotator 2 0.421 0.471
Annotator 3 0.433 0.483
data (III) 2 annotators 0.532 0.564
data (IV) 3 annotators 0.447 0.484
Table 3: ROUGE F-measure scores for different data sets.
3.4 Sentence Distance and Divergence Scores
From the annotation, we notice that the summary sen-
tences are not uniformly distributed in the transcript, but
rather with a clustering or coherence property. However,
neither Kappa coefficient nor ROUGE score can rep-
resent such clustering tendency of meeting summaries.
This paper attempts to develop an evaluation metric to
measure this property among different human annotators.
For a sentence i selected by one annotator, we define a
distance score di to measure its minimal distance to sum-
mary sentences selected by other annotators (distance be-
tween two sentences is represented using the difference
of their sentence indexes). di is 0 if more than one anno-
tator have extracted the same sentence as summary sen-
tence. Using the annotated summaries for the 27 meet-
ings in Data Set (I), we computed the sentence distance
scores for each annotator. Figure 2 shows the distribution
of the distance score for the 3 annotators. We can see
that the distance score distributions for the three annota-
tors differ. Intuitively, small distance scores mean better
coherence and more consistency with other annotators?
results. We thus propose a mechanism to quantify each
annotator?s summary annotation by using a random vari-
able (RV) to represent an annotator?s sentence distance
scores.
When all the annotators agree with each other, the RV
d will take a value of 0 with probability 1. In general,
when the annotators select sentences close to each other,
the RV d will have small values with high probabilities.
Therefore we create a probability distribution Q for the
ideal situation where the annotators have high agreement,
and use this to quantify the quality of each annotation. Q
is defined as:
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0.45
0.5
0 1 2 3 4 5 6 7 8 9 10 >10
Distance Score
Pe
rc
en
ta
ge
Annotator 1 Annotator 2 Annotator 3
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0.45
0.5
0 1 2 3 4 5 6 7 8 9 10 >10
Distance Score
Pe
rc
en
ta
ge
Annotator 1 Annotator 2 Annotator 3
Figure 2: Percentage distribution of the summary sentence dis-
tance scores for the 3 annotators in Data Set (I).
Q(i) =
?
?
?
?
?
(dmax ? i + 1) ? q i 6= 0
1 ? ?dmaxi=1 Q(i)
= 1 ? dmax?(dmax+1)2 ? q i = 0
where dmax denotes the maximum distance score based
on the selected summary sentences from all the annota-
tors. We assign linearly decreasing probabilities Q(i) for
different distance values i (i > 0) in order to give more
credit to sentences with small distance scores. The rest
of the probability mass is given to Q(0). The parame-
ter q is small, such that the probability distribution Q can
approximate the ideal situation.
For each annotator, the probability distribution P is de-
fined as:
P (i) =
{
wi?fi
P
i wi?fi
i ? Dp
0 otherwise
where Dp is the set of the possible distance values for this
annotator, fi is the frequency for a distance score i, and
wi is the weight assigned to that distance (wi is i when
i 6= 0; w0 is p). We use parameter p to vary the weighting
scale for the distance scores in order to penalize more for
the large distance values.
Using the distribution P for each annotator and the
ideal distribution Q, we compute their KL-divergence,
called the Divergence Distance score (DD-score):
DD =
?
i
P (i) log P (i)Q(i)
We expect that the smaller the score is, the better the sum-
mary is. In the extreme case, if an annotator?s DD-score
is equal to 0, it means that all of this annotator?s extracted
sentences are selected by other annotators.
Figure 3 shows the DD-score for each annotator cal-
culated using Data Set (I), with varying q parameters.
Our experiments showed that the scale parameter p in the
annotator?s probability distribution only affects the abso-
lute value of the DD-score for the annotators, but does
not change the ranking of each annotator. Therefore we
simply set p = 10 when reporting DD-scores. Figure 3
shows that different weight scale q does not impact the
ranking of the annotators either. We observe in Figure 3,
annotator 1 has the highest DD score to the desirable dis-
tribution. We found this is consistent with the cumulative
distance score obtained from the distance score distribu-
tion, where annotator 1 has the least cumulative frequen-
cies for all the distance values greater than 0. This is
82
also consistent with the ROUGE scores, where annotator
1 has the lowest ROUGE score. These suggest that the
DD-score can be used to quantify the consistency of an
annotator with others.
0
5
10
15
20
25
7 8 9 10 11 12 13 14 15 16 17 18 19 20 21
-log(q)
 D
iv
er
ge
nc
e 
D
is
ta
nc
e
Sc
or
e
Annotator 1
Annotator 2
Annotator 3
0
5
10
15
20
25
7 8 9 10 11 12 13 14 15 16 17 18 19 20 21
-log(q)
 D
iv
er
ge
nc
e
Di
st
an
ce
 S
co
re
Annotator 1
Annotator 2
Annotator 3
Figure 3: Divergence distance score when varying parameter q
in the ideal distribution Q.
We also investigated using the sentence distance scores
to improve the human annotation quality. Our hypothe-
sis is that those selected summary sentences with high
distance scores do not contain crucial information of
the meeting content and thus can be removed from the
reference summary. To verify this, for each annota-
tor, we removed the summary sentences with distance
scores greater than some threshold, and then computed
the ROUGE score for the newly generated summary by
comparing to other two summary annotations that are
kept unchanged. The ROUGE-2 scores when varying the
threshold is shown in Figure 4. No threshold in the X-
axis means that no sentence is taken out from the human
summary. We can see from the figure that the removal
of sentences with high distance scores can result in even
better F-measure scores. This suggests that we can delete
the incoherent human selected sentences while maintain-
ing the content information in the summary.
0.38
0.39
0.4
0.41
0.42
0.43
0.44
0.45
0.46
3 4 5 6 7 8 no threshold
Threshold of Distance Score
F-
sc
or
e
Annotator 1 Annotator 2 Annotator 3
0.38
0.39
0.4
0.41
0.42
0.43
0.44
0.45
0.46
3 4 5 6 7 8 no threshold
Threshold of Distance Score
F-
sc
or
e
Annotator 1 Annotator 2 Annotator 3
Figure 4: ROUGE-2 score after removing summary sentences
with a distance score greater than a threshold.
4 Conclusion
In this paper we conducted an analysis about human an-
notated extractive summaries using a subset of the ICSI
meeting corpus. Different measurements have been used
to examine interannotator agreement, including Kappa
coefficient, which requires exact same sentence selection;
ROUGE, which measures the content similarity using n-
gram match; and our proposed sentence distance scores
and divergence, which evaluate the annotation consis-
tency based on the sentence position. We find that the
topic length does not have an impact on the human agree-
ment using Kappa, but the number of speakers seems to
be correlated with the agreement. The ROUGE score and
the divergence distance scores show some consistency
in terms of evaluating human annotation agreement. In
addition, using the sentence distance score, we demon-
strated that we can remove some poorly chosen sentences
from the summary to improve human annotation agree-
ment and preserve the information in the summary. In
our future work, we will explore other factors, such as
summary length, and the speaker information for the se-
lect summaries. We will also use a bigger data set for a
more reliable conclusion.
Acknowledgments
The authors thank University of Edinburgh for sharing the an-
notation on the ICSI meeting corpus. This research is supported
by NSF award IIS-0714132. The views in this paper are those
of the authors and do not represent the funding agencies.
References
A. H. Buist, W. Kraaij, and S. Raaijmakers. 2005. Automatic
summarization of meeting data: A feasibility study. In Proc.
of the 15th CLIN conference.
J. Carletta. 1996. Assessing agreement on classification tasks:
the kappa statistic. Computational Linguistics, 22(2):249?
254.
M. Galley. 2006. A skip-chain conditional random field
for ranking meeting utterances by importance. In Proc. of
EMNLP, pages 364?372.
C. Hori, T. Hori, and S. Furui. 2003. Evaluation methods for
automatic speech summarization. In Proc. of Eurospeech,
pages 2825?2828.
A. Janin, D. Baron, J. Edwards, D. Ellis, D. Gelbart, N. Morgan,
B. Peskin, T. Pfau, E. Shriberg, A. Stolcke, and C. Wooters.
2003. The ICSI meeting corpus. In Proc. of ICASSP.
C. Y. Lin and E. Hovy. 2003. Automatic evaluation of sum-
maries using n-gram co-occurrence statistics. In Proc. of
HLT?NAACL.
I. Mani, G. Klein, D. House, L. Hirschman, T. Firmin, and
B. Sundheim. 2002. Summac: a text summarization eval-
uation. Natural Language Engineering, 8:43?68.
S. Maskey and J. Hirschberg. 2003. Automatic summariza-
tion of broadcast news using structural features. In Proc. of
EUROSPEECH, pages 1173?1176.
G. Murray, S. Renals, J. Carletta, and J. Moore. 2005. Evalu-
ating automatic summaries of meeting recordings. In Proc.
of the ACL Workshop on Intrinsic and Extrinsic Evaluation
Measures for Machine Translation.
A. Nenkova and R. Passonneau. 2004. Evaluating content se-
lection in summarization: The pyramid method. In Proc. of
HLT-NAACL.
E. Shriberg, R. Dhillon, S. Bhagat, J. Ang, and H. Carvey. 2004.
The ICSI meeting recorder dialog act (MRDA) corpus. In
Proc. of 5th SIGDial Workshop, pages 97?100.
S. Xie and Y. Liu. 2008. Using corpus and knowledge-based
similarity measure in maximum marginal relevance for meet-
ing summarization. In Proc. of ICASSP.
J. Zhang, H. Chan, P. Fung, and L. Cao. 2007. A compara-
tive study on speech summarization of broadcast news and
lecture speech. In Proc. of Interspeech.
83
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 884?894, Dublin, Ireland, August 23-29 2014.
A Step Towards Usable Privacy Policy:
Automatic Alignment of Privacy Statements
Fei Liu Rohan Ramanath Norman Sadeh Noah A. Smith
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213, USA
{feiliu, rrohan, sadeh, nasmith}@cs.cmu.edu
Abstract
With the rapid development of web-based services, concerns about user privacy have height-
ened. The privacy policies of online websites, which serve as a legal agreement between service
providers and users, are not easy for people to understand and therefore offer an opportunity for
natural language processing. In this paper, we consider a corpus of these policies, and tackle the
problem of aligning or grouping segments of policies based on the privacy issues they address.
A dataset of pairwise judgments from humans is used to evaluate two methods, one based on
clustering and another based on a hidden Markov model. Our analysis suggests a five-point gap
between system and median-human levels of agreement with a consensus annotation, of which
half can be closed with bag of words representations and half requires more sophistication.
1 Introduction
Privacy policies are legal documents, authored by privacy lawyers to protect the interests of companies
offering services through the web. According to a study conducted by McDonald and Cranor (2008), if
every internet user in the U.S. read the privacy notice of each new website she visited, it would take the
nation 54 billion hours annually to read privacy policies. It is not surprising that they often go unread
(Federal Trade Commission, 2012).
Users, nonetheless, might do well to understand the implications of agreeing to a privacy policy, and
might make different choices if they did. Researchers in the fields of internet privacy and security have
made various attempts to standardize the format of privacy notices, so that they are easier to understand
and to allow the general public to have better control of their personal information. An early effort is the
Platform for Privacy Preferences Project (P3P), which defines a machine-readable language that enables
the websites to explicitly declare their intended use of personal information (Cranor, 2002). Many other
studies primarily focus on the qualitative perspective of policies and use tens of carefully selected privacy
notices. For example, Kelley et al. (2010) proposed a ?nutrition label? approach that formalizes the
privacy policy into a standardized table format. Breaux et al. (2014) map privacy requirements encoded
in text to a formal logic, in order to detect conflicts in requirements and trace data flows (e.g., what data
might be collected, to whom the data will be transferred and for what purposes).
The need for automatically or semi-automatically generating simple, easy-to-digest privacy summaries
is further exacerbated by the emergence of the mobile Web and the Internet of Things, with early efforts
in this area including the use of static analysis to identify sensitive data flows in mobile apps (Lin et al.,
2012) and the development of mobile app privacy profiles (Liu et al., 2014).
Increased automation for such efforts motivates our interest in privacy policies as a text genre for NLP,
with the general goal of supporting both user-oriented tools that interpret policies and studies of the
contents of policies by legal scholars.
In this paper, we start with a corpus of 1,010 policies collected from widely-used websites (Ramanath
et al., 2014),
1
and seek to automatically align segments of policies. We believe this is a worthwhile first
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
1
http://www.usableprivacy.org/data
884
Amazon.com Privacy Notice
...
What About Cookies?
Cookies are unique identifiers that we transfer to your device to enable
our systems to recognize your device and to provide features such as
1-Click purchasing, Recommended for You, personalized advertisements
on other Web sites...
...Because cookies allow you to take advantage of some of Amazon.com?s
essential features, we recommend that you leave them turned on. For
instance, if you block or otherwise reject our cookies, you will not be
able to add items to your Shopping Cart, proceed to Checkout, or use any
Amazon.com products and services that require you to Sign in...
Walmart Privacy Policy
...
Information We Collect
...We use ?cookies? to recognize you as you use or return to our sites.
This is done so that we can provide a continuous and more personalized
shopping experience for you. A cookie is a small text file that a website
or email may save to your browser and store on your hard drive...
Your Choices
...You may exercise choices related to our online operations and adver-
tising. For instance, you can choose to browse our websites without ac-
cepting cookies. Please know that cookies allow us to recognize you from
page to page, and they support your transactions with us. Without cookies
enabled, you will still be able to browse our websites, but will not be able
to complete a purchase or take advantage of certain website features...
Table 1: Example privacy statements from Amazon.com (left) and Walmart.com (right). The statements
are concerned with the websites? cookie policy. The top-most level section subtitles are shown in bold.
step toward interpretation of the documents of direct interest here, and also that automatic alignment of
a large set of similarly-constructed documents might find application elsewhere.
Consider the example in Table 1, where we show privacy statements from Amazon.com
2
and Wal-
mart.com.
3
These statements are concerned with the usage of cookies?small data files transferred by a
website to the user?s computer hard drive?often used for tracking a user?s browsing behavior. Cookies
are one issue among many that are addressed by privacy policies; by aligning segments by issue, across
policies, we can begin to understand the range of policy approaches for each issue.
We contribute pairwise annotations of segment pairs drawn from different policies, for use in evalu-
ating the quality of alignments, an analysis of the inter-annotator reliability, and an experimental assess-
ment of three alignment methods, one based on clustering and two based on a hidden Markov model.
This paper?s results refine the findings of Ramanath et al. (2014). Our key finding is that these unsuper-
vised methods reach far better agreement with the consensus of crowdworkers than originally estimated,
and that the gap between these methods and the ?median? crowdworker is about half due to the greedy
nature of such methods and about half due to the bag of words representation.
2 Privacy Dataset and Annotations
For completeness, we review the corpus of privacy policies presented by Ramanath et al. (2014), and
then present the new annotations created for evaluation of alignment.
2.1 Corpus
We collected 1,010 unique privacy policy documents from the top websites ranked by Alexa.com.
4
These
policies were collected during a period of six weeks during December 2013 and January 2014. They are a
snapshot of privacy policies of mainstream websites covering fifteen of Alexa.com?s seventeen categories
(Table 2).
5
Finding a website?s policy is not trivial. Though many well-regulated commercial websites provide a
?privacy? link on their homepages, not all do. We found university websites to be exceptionally unlikely
to provide such a link. Even once the policy?s URL is identified, extracting the text presents the usual
challenges associated with scraping documents from the web. Since every site is different in its place-
ment of the document (e.g., buried deep within the website, distributed across several pages, or mingled
together with Terms of Service) and format (e.g., HTML, PDF, etc.), and since we wish to preserve as
much document structure as possible (e.g., section labels), full automation was not a viable solution.
2
https://www.amazon.com/gp/help/customer/display.html?nodeId=468496
3
http://corporate.walmart.com/privacy-security/walmart-privacy-policy
4
http://www.alexa.com
5
The ?Adult? category was excluded; the ?World? category was excluded since it contains mainly popular websites in
different languages, and we opted to focus on policies in English in this first stage of research, though multilingual policy
analysis presents interesting challenges for future work.
885
Sections Paragraphs Sections Paragraphs
Category Count Length Count Length Category Count Length Count Length
Arts 11.1 254.8 39.2 72.1 Recreation 11.9 218.8 38.5 67.4
Business 10.0 244.2 37.6 65.1 Reference 9.7 179.4 26.2 66.3
Computers 10.5 213.4 34.4 65.4 Regional 10.2 207.7 36.0 59.1
Games 10.0 244.1 34.9 70.1 Science 8.7 155.0 22.1 61.0
Health 9.9 228.2 32.4 69.4 Shopping 11.9 213.9 39.3 64.8
Home 11.6 201.5 32.4 72.0 Society 9.8 230.8 32.6 69.3
Kids and Teens 9.6 231.5 32.3 68.6 Sports 10.1 217.1 29.1 75.6
News 10.3 248.4 35.5 72.4 Average 10.4 221.9 34.1 68.0
Table 2: Fifteen website categories, average number of sections and paragraphs per document in that
category, and average length in word tokens.
We therefore crowdsourced the privacy policy document collection using Amazon Mechanical Turk.
For each website, we created a HIT in which a worker was asked to copy and paste the following privacy
policy-related information into text boxes: (i) privacy policy URL; (ii) last updated date (or effective
date) of the current privacy policy; (iii) privacy policy full text; and (iv) the section subtitles in the
top-most layer of the privacy policy. To identify the privacy policy URL, workers were encouraged to
go to the website and search for the privacy link. Alternatively, they could form a search query using
the website name and ?privacy policy? (e.g., ?Amazon.com privacy policy?) and search in the returned
results for the most appropriate privacy policy URL. Each HIT was completed by three workers, paid
$0.05, for a total cost of $380 (including Amazon?s surcharge). After excluding dupliates, the dataset
contains 1,010 unique documents.
6
Given the privacy policy full text and the section subtitles, we partition the full privacy document into
different sections, delimited by the section subtitles. To generate paragraphs, we break the sections by
lines, and consider each line as a paragraph. We require a paragraph to end with a period, if not, it will
be concatenated with the next paragraph. Using this partition scheme, sections contain 12 sentences on
average; and paragraphs contain 4 sentences on average. More statistics are presented in Table 2.
2.2 Pairwise Annotations
Ramanath et al. (2014) described an evaluation method in which pairs of privacy policy sections were
annotated by crowdworkers.
7
A sample of section pairs from different policies was drawn, stratified
by cosine similarity of unigram tfidf vectors. In a single task, a crowdworker was asked whether two
sections broadly discussed the same topic. The question was presented alongside three answer options,
essentially a strong yes, a yes, and a no. In that initial exploration, each item was annotated at least three
times, and up to fifteen, until an absolute majority was reached.
The annotations conducted for this study were done somewhat differently. Our motivations were to
enable a more careful exploration of inter-annotator agreement, which was complicated in the earlier
work by the variable number of annotations per pair, from three to fifteen. We also sought to explore a
more fine-grained problem at the paragraph level.
We sampled 1,000 document pairs from each of the 15 categories, then generated pairs (separately of
sections and of paragraphs) by choosing one at random from each document. In total, 1,278,204 section
pairs and 7,968,487 paragraph pairs were produced. These pairs were stratified by cosine similarity
intervals: [0, 0.25], (0.25, 0.5], (0.5, 0.75], (0.75, 1], as in Ramanath et al. (2014). We sampled 250 pairs
from each interval, resulting in 1,000 pairs each of sections and paragraphs.
These pairs were annotated on Amazon Mechanical Turk. The crowdworkers were instructed to care-
fully read the privacy statements and answer a ?yes/no? question, indicating whether the two texts are
discussing the same privacy issue or not. Several key privacy issues are provided as examples, including
6
Note that different websites may be covered by the same privacy policy provided by the parent company. For example,
espn.go.com, abc.go.com, and marvel.com are all covered under the Walt Disney privacy policy.
7
Another evaluation, based on text selected by humans in a separate, unrelated task, was also explored. Because such an
evaluation seems less broadly applicable, we did not pursue it here.
886
Sections Paragraphs
Cosine similarity: [0, .25] (.25, .5] (.5, .75] (.75, 1] All [0, .25] (.25, .5] (.5, .75] (.75, 1] All
5 workers agree 36.4 12.4 28.0 85.2 40.5 42.4 12.0 32.8 77.6 41.2
4 workers agree 42.8 42.4 42.0 13.6 35.2 39.6 36.8 35.6 17.6 32.4
3 workers agree 20.8 45.2 30.0 1.2 24.3 18.0 51.2 31.6 4.8 26.4
Consensus-yes 4.4 45.2 87.2 99.2 59.0 9.2 66.0 88.8 98.0 65.5
Consensus-no 95.6 54.8 12.8 0.8 41.0 90.8 34.0 11.2 2.0 34.5
Table 3: Inter-annotator agreement of section and paragraph pairs.
collection of personal information, sharing of information with third parties, cookies and other tracking
techniques, data security, children policies, and contact of the websites. To encourage the crowdwork-
ers to carefully read the privacy statements, we also asked them to copy and paste 1?3 keywords from
each section/paragraph, before answering the question.
8
Each section/paragraph pair was judged by five
crowdworkers and was rewarded $0.05. In total, $550 was spent on the annotations.
On average, it took a crowdworker 2.15 minutes to complete a section pair and 1.67 minutes for a
paragraph pair. Interestingly, although a section is roughly three times the length of a paragraph (see
Table 2), the time spent on annotation is not proportional to the text length.
In Table 3, we present the inter-annotator agreement results for section and paragraph pairs, broken
down by cosine-similarity bin and by the majority answer. 75.7% (73.6%) of section (paragraph) pairs
were agreed upon by four or more out of five annotators. Unsurprisingly, disagreement is greatest in
the (.25, .5] similarity bin. Cosine similarity is a very strong predictor of the consensus answer (Pearson
correlation 0.72 for section pairs, 0.67 for paragraphs, on this stratified sample).
Ramanath et al. (2014) considered only sections. A different method was used to obtain consensus
annotations; we simply kept adding annotators to a pair until consensus was reached. For a fair compar-
ison with the new data, we calculated pairwise agreement among three annotators per item, randomly
selected if there were more than three to choose from. On the old section-level data, this was 60.5%; on
the new data, it was 71.3% (using five annotators). Although a controlled experiment in the task setup
was not conducted, we take this as a sign that our binary question with keywords led to a higher quality
set of annotations than the three-way question in the older data. Our experiments in this paper use only
the new data.
2.3 Discussion
We had expected higher agreement at the paragraph level, since paragraphs are shorter, presumably easier
to read and compare, and presumably more focused on a smaller number of issues. This was not borne
out empirically, though a slightly different analysis presented in ?4.2 suggests that, among crowdworkers
who completed ten or more tasks, paragraphs were easier to agree on.
Privacy policies are generally written by attorneys with expertise in privacy law, though there are
automatic generation solutions available that allow a non-expert to quickly fill in a template to create a
policy document.
9
Example 1 in Table 4 shows a case of very high text overlap (five out of five annotators
agreed on a ?yes? answer for this pair). While this kind of localized alignment is not our aim here, we
believe that such ?boilerplate? text, to the extent that it occurs in large numbers of policies, will make
automatic alignment easier.
A case where annotators seem not to have understood, or not taken care to read carefully, is illustrated
by Example 2 in Table 4. Both sections describe ?opt-out? options for unsubscribing from mailing lists
that send promotional messages, though the first is more generally about ?communications? and the
second only addresses email. Three out of five crowdworkers labeled this example with?no.? Achieving
better consensus might require more careful training of annotators about a predefined set of concepts at
the right granularity.
8
We have not used these keywords for any other purpose.
9
For example: http://www.rendervisionsconsulting.com/blog/wp-content/uploads/2011/09/
Privacy-policy-solutions-list_rvc.pdf
887
Example 1 Example 2
Policy excerpt from Urban Outfitters website:
To serve you better, we may combine information you give us online, in
our stores or through our catalogs. We may also combine that information
with publicly available information and information we receive from or
cross-reference with our Select Partners and others. We use that com-
bined information to enhance and personalize the shopping experience of
you and others with us, to communicate with you about our products and
events that may be of interest to you, and for other promotional purposes.
Policy excerpt from Williams-Sonoma website:
To serve you better and improve our performance, we may combine
information you give us online, in our stores or through our catalogs. We
may also combine that information with publicly available information
and information we receive from or cross-reference with select partners
and others. By combining this information we are better able to com-
municate with you about our products, special events and promotional
purposes and to personalize your shopping experience.
Policy excerpt from IKEA website:
What if I prefer not to receive communications from IKEA? If you prefer
not to receive product information or promotions from us by U.S. Mail,
please click here. To unsubscribe from our email list, please follow the
opt-out instructions at the bottom of the email you received, or click here
and update your profile by deselecting ?Please send me: Inspirational
emails and updates.?
Policy excerpt from Neiman Marcus website:
Emails. You will receive promotional emails from us only if you have
asked to receive them. If you do not want to receive email from Neiman
Marcus or its affiliates you can click on the ?Manage Your Email
Preferences? link at the bottom of any email communication sent by us.
Choose ?Unsubscribe? at the bottom of the page that opens. Please allow
us 3 business days from when the request was received to complete the
removal, as some of our promotion s may already have been in process
before you submitted your request.
Table 4: Privacy policy excerpts. Example 1 (a pair of paragraphs) illustrates the likely use of boilerplate;
identical text is marked in gray. Example 2 shows a pair of sections where our intuitions disagree with
the annotations.
3 Problem Formulation
Given a collection of privacy policy documents and assuming each document consists of a sequence
of naturally-occurring text segments (e.g., sections or paragraphs), our goal is to automatically group
the text segments that address the same privacy issue, without pre-specifying the set of such issues.
We believe this exemplifies many scenarios where a collection of documents follow a similar content
paradigm, such as legal documents and, in some cases, scientific literature. Our interest in algorithms
that characterize each individual document?s parts in the context of the corpus is inspired by biological
sequence alignment in computational biology (Durbin et al., 1998).
In our experiments, we consider a hidden Markov model (HMM) that captures local transitions be-
tween topics. The motivation for the HMM is that privacy policies might tend to order issues similarly,
e.g., the discussion on ?sharing information to third parties? appears to often follow the discussion of
?personal information collection.? If each of these corresponds to an HMM state, then the regularity in
ordering is captured by the transition distribution, and each state is characterized by its emission dis-
tribution over words. In this section, we discuss the HMM and two estimation procedures based on
Expectation-Maximization (EM) and variational Bayesian (VB) inference.
3.1 Hidden Markov Model
Assume we have a sequence of observed text segments
10
O = [O
1
, O
2
, ..., O
T
], and each O
t
represents
a text segment in a privacy document (t ? {1, 2, ..., T}). We denote O
t
= [O
1
t
, O
2
t
, ..., O
N
t
t
], where each
O
j
t
corresponds to a word token in the tth text segment; N
t
is the total number of word tokens in the
segment; T represents the total number of segments in the observation sequence. Each text segment O
t
is associated with a hidden state S
t
(S
t
? {1, 2, . . . ,K}, where K is the total number of states). Given
an observation sequence O, our goal is to decode the corresponding hidden state sequence S.
We employ a first-order hidden Markov model where the next state depends only on the previous state.
A notable difference from the familiar HMM used in NLP (e.g., as used for part-of-speech tagging) is that
we allow multiple observation symbols to be emitted from each hidden state. Each symbol corresponds
to a word token in the text segment. Hence the likelihood for a single document can be written as:
L(?, ?) =
?
S?{1,...,K}
T
p(O,S | ?, ?) =
?
S?{1,...,K}
T
T+1
?
t=1
?
S
t
|S
t?1
N
t
?
j=1
?
O
j
t
|S
t
(1)
10
We use segments to refer abstractly to either sections or paragraphs. In any given instantiation, one or the other is used,
never a blend.
888
E-step:
Forward pass: ?
1
(?) = 1; ?
t
(k) =
?
K
k
?
=1
?
t?1
(k
?
) ? ?
k|k
?
?
?
N
t
j=1
?
O
j
t
|k
, ?t ? {2, . . . , T}, ?k ? {1, . . . ,K}(2)
Backward pass: ?
T+1
(?) = 1; ?
t
(k) =
?
K
k
?
=1
?
k
?
|k
?
?
N
t
j=1
?
O
j
t
|k
?
? ?
t+1
(k
?
), ?t ? {T, . . . , 1}, ?k ? {1, . . . ,K}
(3)
Likelihood: p(O | ?, ?) = p(O
1,
O
2
, ..., O
T
| ?, ?) =
?
K
k=1
?
t
(k) ? ?
t
(k) (for any t) (4)
Posteriors: ?
t
(k) = p(S
t
= k | O, ?, ?) =
?
t
(k) ? ?
t
(k)
p(O | ?, ?)
(5)
Pair posteriors: ?
t
(k, k
?
) = p(S
t
= k, S
t+1
= k
?
| O, ?, ?) =
?
t
(k) ? ?
k
?
|k
?
(
?
N
t+1
j=1
?
O
j
t+1
|k
?
)
? ?
t+1
(k
?
)
p(O | ?, ?)
(6)
M-step (in EM):
Transitions: ?
k
?
|k
=
?
T
t=1
?
t
(k, k
?
)
?
T
t=1
?
K
k
??
=1
?
t
(k, k
??
)
; Emissions: ?
v|k
=
?
T
t=1
?
t
(k) ?
?
N
t
j=1
1{O
j
t
= v}
?
T
t=1
?
t
(k) ?N
t
(7)
Variational update (in VB):
?
k
?
|k
=
exp ?
(
?
T
t=1
?
t
(k, k
?
) + ?
)
exp ?
(
?
T
t=1
?
K
k
??
=1
?
t
(k, k
??
) + ? ?K
)
; ?
v|k
=
exp ?
(
?
T
t=1
?
t
(k) ?
?
N
t
j=1
1{O
j
t
= v}+ ?
?
)
exp ?
(
?
T
t=1
?
t
(k) ?N
t
+ ?
?
? V
)
(8)
Table 5: Equations for parameter estimation of the HMM with multiple emissions at each state and a
single sequence. K is the number of states, V is the emission vocabulary size, and T is the length of the
sequence in sections. ?(?) is the digamma function.
?
k
?
|k
denotes the probability of transitioning to state k
?
given that the preceding state is k. ?
v|k
denotes
the probability that a particular symbol emitted during a visit to state k is the word v. As in standard
treatments, we assume an extra final state at the end of the sequence that emits a stop symbol.
Ramanath et al. (2014) considered three variants of the HMM, with different constraints on the tran-
sitions, such as a ?strict forward? variant that orders the states and only allows transition to ?later? states
than the current one. In the evaluation against direct human judgments, they found a slight benefit from
such constraints, but they increased performance variance considerably. Here we only consider an un-
constrained HMM.
3.2 EM and VB
We consider two estimation methods, neither novel. Both are greedy hillclimbing methods that locally
optimize functions based on likelihood under the HMM.
The first method is EM, adapted for the multiple emission case; the equations for the E-step (forward-
backward algorithm and subsequent posterior calculations) and the M-step are shown in Table 5.
We also consider Bayesian inference, which seeks to marginalize out the parameter values, since we
are really only interested in the assignment of sections to hidden states. Further, Bayesian inference
has been found favorable on small datasets (Gao and Johnson, 2008). We assume symmetric Dirichlet
priors on the transition and emission distributions, parameterized respectively by ? = 1 and ?
?
= 0.1.
We apply mean-field variational approximate inference as described by Beal (2003), which amounts to
an EM-like procedure. The E-step is identical to EM, and the M-step involves a transformation of the
expected counts, shown in Table 5. (We also explored Gibbs sampling; performance was less stable but
generally similar; for clarity we do not report the results here.)
889
3.3 Implementation Details
In modeling, the vocabulary excludes 429 stopwords,
11
words whose document frequency is less than
ten, and a set of terms specific to website privacy polices: privacy, policy, personal, information, service,
web, site, website, com, and please. After lemmatizing, the vocabulary contains V = 2,876 words.
We further exclude sections and paragraphs that contain less than 10 words. Many of these are not
meaningful statements, e.g., ?return to top.? This results in 9,935 sections and 27,594 paragraphs in the
experiments.
During estimation, we concatenate all segments into a single sequence, delimited by a special bound-
ary symbol. This does not affect the outcome (due to the first-order conditions; it essentially conflates
?start? and ?stop? states), but gave some efficiency gains in our implementation.
EM or VB iterations continue until one of two stopping criteria is met: either 100 iterations have
passed, or the relative change in log-likelihood (or the variational bound in the case of VB) falls below
10
?4
; this consistently happens within forty iterations.
After estimating parameters, we decode using the Viterbi algorithm.
4 Experiments
Our experiments compare three methods for aligning segments of privacy policies, at both the paragraph
and the section level:
? A greedy divising clustering algorithm, as implemented in CLUTO.
12
The algorithm performs a
sequence of bisections until the desired number of clusters is reached. In each step, a cluster is
selected and partitioned so as to optimize the clustering criterion. CLUTO demonstrated robust
performance in several related NLP tasks (Zhong and Ghosh, 2005; Lin and Bilmes, 2011; Chen et
al., 2011).
? The Viterbi state assignment from the EM-estimated HMM. We report averaged results over ten
runs, with random initialization.
? The Viterbi state assignment after VB inference, using the mean field parameters. We report aver-
aged results over ten runs, with random initialization.
Our evaluation metrics are precision, recall, and F -score on the identification of section or paragraph
pairs annotated ?yes.?
4.1 Results
In Figure 1, we present performance of different algorithms using a range of hidden state values K ?
{1, 2, . . . , 20}. The top row shows precision, recall and F -scores on section pairs, the bottom row on
paragraph pairs.
The algorithms mostly perform similarly. At the section level, we find the clustering algorithm to
perform better in terms of F -score than the HMM with larger K; at K = 10 the two are very close.
13
CLUTO?s best performance, 85%, was achieved by K = 14.
At the paragraph level, the HMMs outperform clustering in the K ? [5, 15) range, and this is where
the peak F -score is obtained (87%). We do not believe these differences among algorithms are espe-
cially important, noting only that the HMM?s advantage is that it does not require pairwise similarity
calculations between all section pairs.
890
Figure 1: Performance results against pairwise annotations when using different number of hidden states
K ? {1, . . . , 20}. The top row is at the section level, the bottom row at the paragraph level.
4.2 Upper Bounds
How do these automatic alignment methods compare with the levels of agreement reached among crowd-
workers? We consider the agreement rate of each method, at varying values of K, with the majority vote
of the annotators. Note that this is distinct from the positive-match?focused precision, recall, and F -
score measures presented in ?4.1. For each crowdworker who completed ten tasks or more, and therefore
for whom we have hope of a reliable estimate, we calculated her agreement rate with the majority. For
sections, this set included 65 out of 162 crowdworkers; for paragraphs, 76 out of 197.
In Figure 2 we show the three quartile points for this agreement measure, across the pool of ten-or-
more-item crowdworkers, in comparison to the various automatic methods. For sections, our systems
perform on par with the 25% of crowdworkers just below the median. For paragraphs, which show a
generally higher level of agreement among this subset of crowdworkers, our systems are on par with the
lowest 25% of workers. We take all of this to suggest that there is room for improvement in methods
overall.
Given the observation in ?2 that cosine similarity of two segments? tfidf vectors is a very strong pre-
dictor of human agreement on whether they are about the same issue, we also consider a threshold on
cosine similarity for deciding whether a pair is about the same issue. This is not a complete solution to
the problem of alignment, since pairwise scores only provide groupings if they are coupled with a tran-
sitivity constraint. The clustering and HMM methods can be understood as greedy approximations to
such an approach. We therefore view cosine similarity thresholding as an upper bound for bag of words
representations on the pairwise evaluation task. Figure 2 includes agreement levels for oracle cosine
similarity thresholding.
14
11
http://www.lextek.com/manuals/onix/stopwords1.html
12
http://glaros.dtc.umn.edu/gkhome/cluto/cluto/overview
13
Ramanath et al. (2014) only considered K = 10 and found a K = 10 HMM to outperform clustering at the section level;
the scores reported there, on the earlier dataset, are much lower and not comparable to those reported here. There are numerous
differences between the setup here and the earlier one. The most important, we believe, are the improved quality of the dataset
and greater care given to preprocessing, most notably the pruning of documents and vocabulary, in the present experiments.
14
For comparison with the results in ?4.1, we found that, for sections, oracle thresholding (at 0.3) achieved F -score of 0.87,
and for paragraphs, oracle thresholding (at 0.2) achieved 0.90.
891
Figure 2: Agreement rates, as compared to crowdworkers and a cosine similarity oracle.
Taken together, this analysis suggests that?in principle?an automated approach based on word-level
similarity could close about half of the gap between our methods and median crowdworkers, and further
gains would require more sophisticated representations or similarity measures.
5 Related Work
There has been little work on applying NLP to privacy policies. Some have sought to parse privacy
policies into machine-readable representations (Brodie et al., 2006) or extract sub-policies from larger
documents (Xiao et al., 2012). Machine learning has been applied to assess certain attributes of policies
(Costante et al., 2012; Costante et al., 2013), e.g., compliance of privacy policies to legal regulations
(Krachina et al., 2007) or simple categorical questions about privacy policies (Ammar et al., 2012; Zim-
meck and Bellovin, 2014).
Our alignment-style analysis is motivated by an expectation that many policies will address similar
issues,
15
such as collection of a user?s contact, location, health, and financial information, sharing with
third parties, and deletion of data. This expectation is supported by recommendation by privacy experts
(Gellman, 2014) and policymakers (Federal Trade Commission, 2012); in the financial services sector,
the Gramm-Leach-Bliley Act requires these institutions to address a specific set of issues. Sadeh et al.
(2013) describe our larger research initiative to incorporate automation into privacy policy analysis.
Methodologically, the HMM used above is very similar to extensive previous uses of HMMs for POS
tagging (Merialdo, 1994), including with Bayesian inference (Goldwater and Griffiths, 2007; Johnson,
2007; Gao and Johnson, 2008). Bayesian topic models (Blei et al., 2003) are a related set of techniques,
and future exploration might consider their use in automatically discovering document sections (Eisen-
stein and Barzilay, 2008), rather than fixing section or paragraph boundaries.
6 Conclusion
This paper presents an exploration of alignment-by-paragraph and -section of website privacy policies.
We contribute an improved annotated dataset for pairwise evaluation of automatic methods and an explo-
ration of clustering and HMM-based alignment methods. Our results show that these algorithms achieve
agreement on par with the lower half of crowdworkers, with about half of the difference from the median
due to the bag of words representation and half due to the inherent greediness of the methods.
Acknowledgments
The authors gratefully acknowledge helpful comments from Lorrie Cranor, Joel Reidenberg, Florian
Schaub, and several anonymous reviewers. This research was supported by NSF grant SaTC-1330596.
15
Personal communication, Joel Reidenberg.
892
References
Waleed Ammar, Shomir Wilson, Norman Sadeh, and Noah A. Smith. 2012. Automatic categorization of privacy
policies: A pilot study. Technical Report CMU-LTI-12-019, Carnegie Mellon University.
Matthew J. Beal. 2003. Variational Algorithms for Approximate Bayesian Inference. Ph.D. thesis, Gatsby Com-
putational Neuroscience unit, University College London.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan. 2003. Latent Dirichlet allocation. Journal of Machine
Learning Research.
Travis D. Breaux, Hanan Hibshi, and Ashwini Rao. 2014. Eddy, A formal language for specifying and analyzing
data flow specifications for conflicting privacy requirements. Requirements Engineering Journal.
Carolyn A. Brodie, Clare-Marie Karat, and John Karat. 2006. An empirical study of natural language parsing
of privacy policy rules using the SPARCLE policy workbench. In Proceedings of the Second Symposium on
Usable Privacy and Security (SOUPS).
Harr Chen, Edward Benson, Tahira Naseem, and Regina Barzilay. 2011. In-domain relation discovery with meta-
constraints via posterior regularization. In Proceedings of ACL-HLT.
Elisa Costante, Yuanhao Sun, Milan Petkovi?c, and Jerry den Hartog. 2012. A machine learning solution to assess
privacy policy completeness. In Proceedings of the ACM Workshop on Privacy in the Electronic Society.
Elisa Costante, Jerry Hartog, and Milan Petkovi. 2013. What websites know about you. In Roberto Pietro, Javier
Herranz, Ernesto Damiani, and Radu State, editors, Data Privacy Management and Autonomous Spontaneous
Security, volume 7731 of Lecture Notes in Computer Science, pages 146?159. Springer Berlin Heidelberg.
Lorrie Faith Cranor. 2002. Web Privacy with P3P. O?Reilly & Associates.
Richard Durbin, Sean R. Eddy, Anders Krogh, and Graeme Mitchison. 1998. Biological Sequence Analysis:
Probabilistic Models of Proteins and Nucleic Acids. Cambridge University Press.
Jacob Eisenstein and Regina Barzilay. 2008. Bayesian unsupervised topic segmentation. In Proceedings of ACL.
Federal Trade Commission. 2012. Protecting consumer privacy in an era of rapid change: Recom-
mendations for businesses and policymakers. Available at http://www.ftc.gov/reports/
protecting-consumer-privacy-era-rapid-change-recommendations-businesses-policymakers.
Jianfeng Gao and Mark Johnson. 2008. A comparison of Bayesian estimators for unsupervised Hidden Markov
model POS taggers. In Proceedings of EMNLP.
Robert Gellman. 2014. Fair information practices: a basic history (v. 2.11). Available at http://www.
bobgellman.com/rg-docs/rg-FIPShistory.pdf.
Sharon Goldwater and Thomas L. Griffiths. 2007. A fully Bayesian approach to unsupervised part-of-speech
tagging. In Proceedings of ACL.
Mark Johnson. 2007. Why doesnt EM find good HMM POS-taggers? In Proceedings of EMNLP?CoNLL.
Patrick Gage Kelley, Lucian Cesca, Joanna Bresee, and Lorrie Faith Cranor. 2010. Standardizing privacy notices:
An online study of the nutrition label approach. In Proceedings of CHI.
Olga Krachina, Victor Raskin, and Katrina Triezenberg. 2007. Reconciling privacy policies and regulations:
Ontological semantics perspective. In Michael J. Smith and Gavriel Salvendy, editors, Human Interface and the
Management of Information. Interacting in Information Environments, pages 730?739. Springer.
Hui Lin and Jeff Bilmes. 2011. A class of submodular functions for document summarization. In Proceedings of
ACL.
Jialiu Lin, Shahriyar Amini, Jason I. Hong, Norman Sadeh, Janne Lindqvist, and Joy Zhang. 2012. Expectation
and purpose: Understanding users? mental models of mobile app privacy through crowdsourcing. In Proceed-
ings of ACM International Joint Conference on Pervasive and Ubiquitous Computing (UbiComp).
Bin Liu, Jialiu Lin, and Norman Sadeh. 2014. Reconciling mobile app privacy and usability on smartphones:
Could user privacy profiles help? In Proceedings of WWW.
Aleecia M. McDonald and Lorrie Faith Cranor. 2008. The cost of reading privacy policies. I/S: A Journal of Law
and Policy for the Information Society.
893
Bernard Merialdo. 1994. Tagging English text with a probabilistic model. Computational Linguistics, 20(2):155?
171.
Rohan Ramanath, Fei Liu, Norman Sadeh, and Noah A. Smith. 2014. Unsupervised alignment of privacy policies
using hidden Markov models. In Proceedings of ACL.
Norman Sadeh, Alessandro Acquisti, Travis Breaux, Lorrie Cranor, Aleecia McDonald, Joel Reidenberg, Noah
Smith, Fei Liu, Cameron Russel, Florian Schaub, and Shomir Wilson. 2013. The usable privacy policy project:
Combining crowdsourcing, machine learning and natural language processing to semi-automatically answer
those privacy questions users care about. Technical Report CMU-ISR-13-119, Carnegie Mellon University.
Xusheng Xiao, Amit Paradkar, Suresh Thummalapenta, and Tao Xie. 2012. Automated extraction of security
policies from natural-language software documents. In Proceedings of the ACM SIGSOFT 20th International
Symposium on the Foundations of Software Engineering.
Shi Zhong and Joydeep Ghosh. 2005. Generative model-based document clustering: a comparative study. Knowl-
edge and Information Systems, 8(3):374?384.
Sebastian Zimmeck and Steven M. Bellovin. 2014. Privee: An architecture for automatically analyzing web
privacy policies. In Proceedings of the 23rd USENIX Security Symposium.
894
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 490?500,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Document Summarization via Guided Sentence Compression
Chen Li1, Fei Liu2, Fuliang Weng2, Yang Liu1
1 Computer Science Department, The University of Texas at Dallas
Richardson, Texas 75080, USA
2 Research and Technology Center, Robert Bosch LLC
Palo Alto, California 94304, USA
{chenli,yangl@hlt.utdallas.edu}
{fei.liu, fuliang.weng@us.bosch.com}
Abstract
Joint compression and summarization has
been used recently to generate high quality
summaries. However, such word-based joint
optimization is computationally expensive. In
this paper we adopt the ?sentence compression
+ sentence selection? pipeline approach for
compressive summarization, but propose to
perform summary guided compression, rather
than generic sentence-based compression. To
create an annotated corpus, the human anno-
tators were asked to compress sentences while
explicitly given the important summary words
in the sentences. Using this corpus, we train
a supervised sentence compression model us-
ing a set of word-, syntax-, and document-
level features. During summarization, we use
multiple compressed sentences in the inte-
ger linear programming framework to select
salient summary sentences. Our results on the
TAC 2008 and 2011 summarization data sets
show that by incorporating the guided sen-
tence compression model, our summarization
system can yield significant performance gain
as compared to the state-of-the-art.
1 Introduction
Automatic summarization can be broadly divided
into two categories: extractive and abstractive sum-
marization. Extractive summarization focuses on
selecting the salient sentences from the document
collection and concatenating them to form a sum-
mary; while abstractive summarization is generally
considered more difficult, involving sophisticated
techniques for meaning representation, content plan-
ning, surface realization, etc., and the ?true abstrac-
tive summarization remains a researcher?s dream?
(Radev et al, 2002).
There has been a surge of interest in recent
years on generating compressed document sum-
maries as a viable step towards abstractive sum-
marization. These compressive summaries often
contain more information than sentence-based ex-
tractive summaries since they can remove insignif-
icant sentence constituents and make space for more
salient information that is otherwise dropped due to
the summary length constraint. Two general strate-
gies have been used for compressive summarization.
One is a pipeline approach, where sentence-based
extractive summarization is followed or proceeded
by sentence compression (Knight and Marcu, 2000;
Lin, 2003; Zajic et al, 2007; Wang et al, 2013).
Another line of work uses joint compression and
summarization. They have been shown to achieve
promising performance (Daume?, 2006; Martins and
Smith, 2009; Berg-Kirkpatrick et al, 2011; Chali
and Hasan, 2012; Almeida and Martins, 2013; Qian
and Liu, 2013). One popular approach for such joint
compression and summarization is via integer lin-
ear programming (ILP). However, since words are
the units in the optimization framework, solving this
ILP problem can be expensive.
In this study, we use the pipeline compression
and summarization method because of its compu-
tational efficiency. Prior work using such pipeline
methods simply uses generic sentence-based com-
pression for each sentence in the documents, no mat-
ter whether compression is done before or after sum-
mary sentence extraction. We propose to use sum-
490
mary guided compression combined with ILP-based
sentence selection for summarization in this paper.
We create a compression corpus for this purpose.
Using human summaries for a set of documents, we
identify salient words in the sentences. During anno-
tation, the human annotators are given these salient
words and asked to generate compressed sentences.
We expect such ?guided? sentence compression is
beneficial for the pipeline compression and summa-
rization task. In addition, previous research on joint
modeling for compression and summarization sug-
gested that the labeled extraction and compression
data sets would be helpful for learning a better joint
model (Daume?, 2006; Martins and Smith, 2009).
We hope that our work on this guided compression
will also be of benefit to the future joint modeling
studies.
Using our created compression data, we train
a supervised compression model using a variety
of word-, sentence-, and document-level features.
During summarization, we generate multiple com-
pression candidates for each sentence, and use the
ILP framework to select compressed summary sen-
tences. In addition, we also propose to apply a pre-
selection step to select some important sentences,
which can both speed up the summarization system
and improve performance. We evaluate our pro-
posed summarization approach on the TAC 2008
and 2011 data sets using the standard ROUGE met-
ric (Lin, 2004). Our results show that by incorporat-
ing a guided sentence compression model, our sum-
marization system can yield significant performance
gain as compared to the state-of-the-art reported re-
sults.
2 Related Work
Summarization research has seen great development
over the last fifty years (Nenkova and McKeown,
2011). Compared to the abstractive counterpart, ex-
tractive summarization has received considerable at-
tention due to its clear problem formulation ? to ex-
tract a set of salient and non-redundant sentences
from the given document set. Both unsupervised and
supervised approaches have been explored for sen-
tence selection. The supervised approaches include
the Bayesian classifier (Kupiec et al, 1995), max-
imum entropy (Osborne, 2002), skip-chain condi-
tional random fields (CRF) (Galley, 2006), discrim-
inative reranking (Aker et al, 2010), among others.
The extractive summary sentence selection prob-
lem can also be formulated in an optimization
framework. Previous approaches include the inte-
ger linear programming (ILP) and submodular func-
tions, which are used to solve the optimization prob-
lem. In particular, Gillick et al (2009) proposed
a concept-based ILP approach for summarization.
Li et al (2013) improved it by using supervised
stragety to estimate concept weight in ILP frame-
work. In (Lin and Bilmes, 2010), the authors model
the sentence selection problem as maximizing a sub-
modular function under a budget constraint. A
greedy algorithm is proposed to efficiently approxi-
mate the solution to this NP-hard problem.
Compressive summarization receives increasing
attention in recent years, since it offers a viable
step towards abstractive summarization. The com-
pressed summaries can be generated through a joint
model of the sentence selection and compression
processes, or through a pipeline approach that in-
tegrates a generic sentence compression model with
a summary sentence pre-selection or post-selection
step.
Many studies explore the joint sentence compres-
sion and selection setting. Martins and Smith (2009)
jointly perform sentence extraction and compression
by solving an ILP problem; Berg-Kirkpatrick et al
(2011) propose an approach to score the candidate
summaries according to a combined linear model
of extractive sentence selection and compression.
They train the model using a margin-based objec-
tive whose loss captures the final summary qual-
ity. Woodsend and Lapata (2012) present a method
where the summary?s informativeness, succinctness,
and grammaticality are learned separately from data
but optimized jointly using an ILP setup; Yoshikawa
et al (2012) incorporate semantic role information
in the ILP model; Chali and Hasan (2012) investi-
gate three strategies in compressive summarization:
compression before extraction, after extraction, or
joint compression and extraction in one global op-
timization framework. These joint models offer a
promise for high quality summaries, but they often
have high computational cost. Qian and Liu (2013)
propose a graph-cut based method that improves the
speed of joint compression and summarization.
491
The pipeline approach, where sentence-based ex-
tractive summarization is followed or proceeded by
sentence compression, is also popular. Knight and
Marcu (2000) utilize the noisy channel and deci-
sion tree method to perform sentence compression;
Lin (2003) shows that pure syntactic-based com-
pression may not improve the system performance;
Zajic et al (2007) compare two sentence compres-
sion approaches for multi-document summarization,
including a ?parse-and-trim? and a noisy-channel ap-
proach; Galanis and Androutsopoulos (2010) use
the maximum entropy model to generate the candi-
date compressions by removing the branches from
the source sentences; Liu and Liu (2013) couple
the sentence compression and extraction approaches
for summarizing the spoken documents; Wang et al
(2013) design a series of learning-based compres-
sion models built on parse trees, and integrate them
in query-focused multi-document summarization.
Prior studies often rely heavily on the generic sen-
tence compression approaches (McDonald, 2006;
Nomoto, 2007; Clarke and Lapata, 2008; Thadani
and McKeown, 2013) for compressing the sentences
in the documents, yet a generic compression system
may not be the best fit for the summarization pur-
pose.
In this paper, we adopt the pipeline-based com-
pressive summarization framework, but propose a
novel guided compression method that is catered to
the summarization task. We expect this approach
to take advantage of the efficient pipeline process-
ing while producing satisfying results as the joint
models. We train a supervised guided compression
model to produce n-best compressions for each sen-
tence, and use an ILP formulation to select the best
set of summary sentences. In addition, we pro-
pose to apply a sentence pre-selection step to fur-
ther accelerate the processing and enhance the per-
formance.
3 Guided Compression Corpus
The goal of guided sentence compression is to create
compressed sentences that are grammatically cor-
rect and contain the important information that we
would like to preserve in the final summary. Fol-
lowing the compression literature (Clarke and Lap-
ata, 2008), the compression task is defined as a word
Original Sentence:
The gas leak was contained Monday afternoon , nearly 18
hours after it was reported , Statoil spokesman Oeivind
Reinertsen said .
Compression A:
The gas leak was contained
Compression B:
The gas leak was contained Monday afternoon
Compression C:
The gas leak was contained nearly 18 hours after it was
reported
Table 1: Example sentence and three compressions.
deletion problem, that is, the human annotators (and
also automatic compression systems) are allowed to
only remove words from the original sentence to
form a compression. The key difference between
our proposed guided compression with generic sen-
tence compression is that, we provide guidance to
the human compression process by specifying a set
of ?important words? that we wish to keep for each
sentence. We expect this kind of summary oriented
compression would benefit the ultimate summariza-
tion task. Take the sentence shown in Table 1 as an
example. For generic sentence compression, there
may be multiple ?good? human compressions for this
sentence, such as those listed in the table. Without
guidance, a human annotator (or automatic system)
is likely to use option A or B; however, if ?18 hours?
appears in the summary, then we want to provide this
guidance in the compression process, hence option
C may be the best compression choice. This guided
compression therefore avoids removing the salient
words that are important to the final summary.
To generate the guided compression corpus, we
use the TAC 2010 data set1 that was used for
the multi-document summarization task. There are
46 topics. Each has 10 news documents, and
also four human-created abstractive reference sum-
maries. Since annotating all the sentences in this
data set is time consuming and some sentences are
not very important for the summarization task, we
choose a set of sentences that are highly related to
the human abstracts for annotation. We compare
each sentence with the four human abstracts using
the ROUGE-2 metric (Lin, 2004), and the sentences
1http://www.nist.gov/tac/2010/
492
Original Sentence:
He said Vietnam veterans are presumed to have been ex-
posed to Agent Orange and veterans with any of the 10 dis-
eases is presumed to have contracted it from the exposure ,
without individual proof .
Guided Compression:
Vietnam veterans are presumed to have been exposed to
Agent Orange.
Original Sentence:
The province has limited the number of trees to be chopped
down in the forest area in northwest Yunnan and has stopped
building sugar factories in the Xishuangbanna region to
preserve the only tropical rain forest in the country located
there .
Guided Compression:
province has stopped building sugar factories in the
Xishuangbanna region to preserve tropical rain forest.
Table 2: Example original sentences and their guided
compressions. The ?guiding words? are italicized and
marked in red.
with the highest scores are selected.
In annotation, human annotators are provided
with important ?guiding words? (highlighted in the
annotation interface) that we want to preserve in the
sentences. We calculate the word overlap between a
sentence and each of those sentences in the human
abstracts, and use a set of heuristic rules to deter-
mine the ?guiding words? in a sentence: the longest
consecutive word overlaps (greater than 2 words) in
each sentence pair are first selected; the rest overlaps
that contain 2 or more words (excluding the stop-
words) are also selected. We suggest the human an-
notators to use their best judgment to keep the guid-
ing words as many as possible while compressing
the sentence.
We use the Amazon Mechanical Turk (AMT) for
data annotation2. In total, we select 1,150 sentences
from the TAC news documents. They are grouped
into about 230 human intelligence tasks (HITs) with
5 sentences in each HIT. A sentence was compressed
by 3 human annotatorsand we select the shortest
candidate as the goldstandard compression for each
sentence. In Table 2, we show two example sen-
tences, their guiding words (bold), and the human
compressions. The first example shows that giving
up some guiding words is acceptable, since more
2http://www.mturk.com
unnecessary words will be included in order to ac-
commodate all the guiding words; the second ex-
ample shows that the guided compression can lead
to more aggressive word deletions since the con-
stituents that are not important to the summary will
be deleted even though they contain salient informa-
tion by themselves.
For our compression corpus, which contains
1,150 sentences and their guided compressions, the
average compression rate, as measured by the per-
centage of dropped words, is about 50%. This com-
pression ratio is higher compared to other generic
sentence compression corpora, in which the word
deletion rate ranges from 24% to 34% depending
on different text genres and annotation guidelines
(Clarke and Lapata, 2008; Liu and Liu, 2009). This
suggests that the annotators can remove words more
aggressively when they are provided with a limited
set of guiding words.
4 Summarization System
Our summarization system consists of three key
components: we train a supervised guided compres-
sion model using our created compression data, with
a variety of features.then we use this model to gener-
ate n-best compressions for each sentence; we feed
the multiple compressed sentences to the ILP frame-
work to select the best summary sentences. In ad-
dition, we propose a sentence pre-selection step that
can both speed up the summarization system and im-
prove the performance.
4.1 Guided Sentence Compression
Sentence compression has been explored in previous
studies using both supervised and unsupervised ap-
proaches, including the noisy-channel and decision
tree model (Knight and Marcu, 2000; Turner and
Charniak, 2005), discriminative learning (McDon-
ald, 2006), integer linear programming (Clarke and
Lapata, 2008; Thadani and McKeown, 2013), con-
ditional random fields (CRF) (Nomoto, 2007; Liu
and Liu, 2013), etc. In this paper, we employ the
CRF-based compression approach due to its proved
performance and its flexibility to integrate differ-
ent levels of discriminative features. Under this
framework, sentence compression is formulated as
a sequence labeling problem, where each word is
493
labeled as either ?0? (retained) or ?1? (removed).
We develop different levels of features to capture
word-specific characteristics, sentence related infor-
mation, and document level importance. Most of the
features are extracted based only on the sentence to
be compressed. However, we introduce a few doc-
ument level features. These are designed to cap-
ture the word and sentence significance within the
given document collection and are thus expected to
be more summary related.
Word and sentence features:
? Word n-grams: identity of the current word
and two words before and after, as well as all
the bigrams and trigrams that can be formed by
the adjacent words and the current word.
? POS n-grams: same as the word n-grams, but
use the part-of-speech tags instead.
? Named entity tags: binary features represent-
ing whether the current word is a person, loca-
tion, or temporal expression. We use the Stan-
ford CoreNLP tools3 for named entity tagging.
? Stopwords: whether the current word is a stop-
word or not.
? Conjunction features: (1) conjunction of the
current word with its relative position in the
sentence; (2) conjunction of the NER tag with
its relative position.
? Syntactic features: We obtain the syntactic
parsing tree using the Berkeley Parser (Petrov
and Klein, 2007), then obtain the following fea-
tures: (1) the last sentence constituent tag in
the path from the root to the word; (2) depth:
length of the path starting from the root node
to the word; (3) normalized depth: depth di-
vided by the longest path in the parsing tree;
(4) whether the word is under an SBAR node;
(5) depth and normalized depth of the SBAR
node if the word is under an SBAR node;
? Dependency features: We employ the
Penn2Malt toolkit 4 to convert the parse re-
sult from the Berkeley parser to the depen-
dency parsing tree, and use these dependency
3http://nlp.stanford.edu/software/corenlp.shtml
4http://stp.lingfil.uu.se/?nivre/research/Penn2Malt.html
features: (1) dependency relations such as
?AMOD? (adjective modifier), ?NMOD? (noun
modifier), etc. (2) whether the word has a child,
left child, or right child in the dependency tree.
Document-level features:
? Sentence salience score: We use a simple re-
gression model to estimate a salience score for
each sentence (more details in Section 4.3),
which represents the importance of the sen-
tence in the document. This score is discretized
into four binary features according to the aver-
age sentence salience.
? Unigram document frequency: this is the
current word?s document frequency based on
the 10 documents associated with each topic.
? Bigram document frequency: document fre-
quency for the two bigrams, the current word
and its previous or next word.
Some of the above features were employed in re-
lated sentence compression studies (Nomoto, 2007;
Liu and Liu, 2013). In addition to these features, we
explored other related features, including the abso-
lute position of the current word, whether the word
appears in the corresponding topic title and descrip-
tions, conjunction of the syntactic tag with the tree
depth, etc.; however, these features did not lead to
improved performance. We train the CRF model
with the Pocket CRF toolkit5 using the guided com-
pression corpus collected in Section 3. During sum-
marization, we apply the model to a given sentence
to generate its n-best guided compressions and use
them in the following summarization step.
4.2 Summary Sentence Selection
The sentence selection process is similar to the stan-
dard sentence-based extractive summarization, ex-
cept that the input to the selection module is a list
of compressed sentences in our work. Many extrac-
tive summarization approaches can be applied for
this purpose. In this work, we choose the integer
linear programming (ILP) method, specifically, the
concept-based ILP framework introduced in (Gillick
5http://sourceforge.net/projects/pocket-crf-1/
494
et al, 2009), mainly because it yields best perfor-
mance in the TAC evaluation tasks. This ILP ap-
proach aims to extract sentences that can cover as
many important concepts as possible, while ensuring
the summary length is within a given constraint. We
follow the study in (Gillick et al, 2009) to use word
bi-grams as concepts, and assign a weight to each
bi-gram using its document frequency in the given
document collection for a test topic. Two differences
are between our ILP setup and that in (Gillick et al,
2009). First, since we use multiple compressions
for one sentence, we need to introduce an additional
constraint: for each sentence, only one of the n-best
compressions may be included in the summary. Sec-
ond, we optimize a joint score of the concept cover-
age and the sentence salience. The formal ILP for-
mulation is shown below:
max
?
i
wici +
?
j
vj
?
k
sjk (1)
s.t.
?
k
sjk ? 1?j (2)
sjkOcci jk ? ci (3)
?
jk
sjkOcci jk ? ci (4)
?
jk
ljksjk ? L (5)
ci ? {0, 1} ?i (6)
sjk ? {0, 1} ?j, k (7)
where ci and sjk are binary variables indicating the
presence of a concept and a sentence respectively;
sjk denotes the kth candidate compression of the
jth sentence; wi represents the weight of the con-
cept; vj is the sentence salience score of the jth
sentence, predicted using a regression model (Sec-
tion 4.3), and all of its compressed candidates share
this value. (1) is the new objective function we use
that combines the coverage of the concepts and the
sentence salience scores. (2) represents our addi-
tional constraint, which requires that for each sen-
tence j, only one candidate compression will be cho-
sen. Occi jk represents the occurrence of concept i
in the sentence sjk. Inequalities (3) and (4) associate
the sentences and the concepts. Constraint (5) con-
trols the summary length, as measured by the total
number of words in the summary. We use an open
source ILP solver6.
4.3 Sentence Pre-selection
The above ILP method can offer an exact solution
to the defined objective function. However, ILP is
computationally expensive when the formulation in-
volves large quantities of variables, i.e, when we
have many sentences and a large number of candi-
date compressions for each sentence. We therefore
propose to apply a sentence pre-selection step be-
fore the compression. This kind of selection step
has been used in previous ILP-based summarization
systems (Berg-Kirkpatrick et al, 2011; Gillick et al,
2009). In this work, we propose to use a simple su-
pervised support vector regression (SVR) model (Ng
et al, 2012) to predict a salience score for each sen-
tence and select the top ranked sentences for further
processing (compression and summarization).
To train the SVR model, the target value for each
sentence is the ROUGE-2 score between the sen-
tence and the four human abstracts (this same value
is used for sentence selection in corpus annotation
(Section 3)). We employ three commonly used fea-
tures: (1) sentence position in the document; (2) sen-
tence length as indicated by a binary feature: it takes
the value of 0 if the number of words in the sentence
is greater than 50 or less than 10, otherwise the fea-
ture value is 1; (3) interpolated n-gram document
frequency as introduced in (Ng et al, 2012), which
is a weighted linear combination of the document
frequency of the unigrams and bigrams contained in
the sentence:
f(s) =
?
?
wu?S
DF (wu) + (1? ?)
?
wb?S
DF (wb)
|S|
where wu and wb represent the unigrams and bi-
grams contained in the sentence S; ? is a balancing
factor; |S| denotes the number of words in the sen-
tence.
The SVR model was trained using the SVMlight
toolkit7. Using this model, we can predict a salience
score (Vj in Eq 1) for each sentence and only select
the top n sentences and supply them to the compres-
sion and summarization steps. In practice, using a
fixed n may not be a good choice since the number
6http://www.gnu.org/software/glpk/
7http://svmlight.joachims.org/
495
of sentences varies greatly for different topics. We
therefore set n heuristically based on the total num-
ber of sentencesm for each topic: n=15 ifm > 150;
n=10 if m < 100; n=0.1 ?m otherwise.
5 Experimental Results
5.1 Experimental Setup
For our experiments, we use the standard TAC data
sets8, which have been used in the NIST competi-
tions and in other summarization studies. In par-
ticular, we used the TAC 2010 data set for creating
the guided compression corpus and training the SVR
pre-selection model, the TAC 2009 data set as devel-
opment set for parameter tuning, and the TAC 2008
and 2011 data sets as the test set for reporting the
final summarization results.
We compare our pipeline summarization sys-
tem against three recent studies, which have re-
ported some of the highest published results on this
task. Berg-Kirkpatrick et al (2011) introduce a
joint model for sentence extraction and compres-
sion. The model is trained using a margin-based ob-
jective whose loss captures the end summary qual-
ity; Woodsend and Lapata (2012) learn individ-
ual summary aspects from data, e.g., informative-
ness, succinctness, grammaticality, stylistic writ-
ing conventions, and jointly optimize the outcome
in an integer linear programming framework. Ng
et al (2012) exploit category-specific information
for multi-document summarization. In addition to
the three previous studies, we also report the best
achieved results in the TAC competitions.
5.2 Summarization Results
In Table 3 and Table 4, we present the results of our
system and the aforementioned summarization stud-
ies. We use the ROUGE evaluation metrics (Lin,
2004), with R-2 measuring the bigram overlap be-
tween the system and reference summaries and R-
SU4 measuring the skip-bigram with the maximum
gap length of 4. ?Our System? uses the pipeline
setting including the three components described in
Section 4. We use the SVR-based approach to pre-
select a set of sentences from the document set; these
sentences are further fed to the guided compression
module that produces n-best compressions for each
8http://www.nist.gov/tac/data/index.html
System R-2 R-SU4 CompR
TAC?08 Best System 11.03 13.96 n/a
(Berg-Kirkpatrick et al, 2011) 11.70 14.38 n/a
(Woodsend et al, 2012) 11.37 14.47 n/a
Our System 12.35? 15.27? 43.06%
Our System w/o Pre-selection 12.02 14.98 55.69%
Our System w/ Generic Comp 10.88 13.79 30.90%
Table 3: Results on the TAC 2008 data set. ?Our Sys-
tem? uses the SVR-based sentence pre-selection + guided
compression + ILP-based summary sentence selection.
?Our System w/ Generic Comp? uses the pre-selection +
generic compression + ILP summary sentence selection
setting. ?CompR? represents the compression ratio, i.e.,
percentage of dropped words. ? represents our system
outperforms the best previous result at the 95% signifi-
cance level.
System R-2 R-SU4 CompR
TAC?11 Best System 13.44 16.51 n/a
(Ng et al, 2012) 13.93 16.83 n/a
Our System 14.40 16.89 39.90%
Our System w/o Pre-selection 13.74 16.5 53.81%
Our System w/ Generic Comp 13.08 16.23 30.10%
Table 4: Results on the TAC 2011 data set. The systems
use the same settings as for the TAC 2008 data set.
sentence; the ILP-based framework is then used to
select the summary sentences from these compres-
sions.
We can see from the table that in general, our sys-
tem achieves considerably better results compared to
the state-of-the-art on both the TAC 2008 and 2011
data sets. On the TAC 2008 data set, our system out-
performs the best reported result at the 95% signifi-
cance level; on the TAC 2011 data set, our system
also yields considerable performance gain though
not exceed the 95% significance level. In the fol-
lowing, we show more detailed analysis to study the
effect of different system parameters.
With or without sentence pre-selection. First
we evaluate the impact of sentence pre-selection
step. In Table 3 and Table 4, we include the
results when this step is not used (?Our System
w/o Pre-selection?). That is, all of the sentences
in the documents (excluding those containing less
than 5 words) are compressed and used in the ILP-
496
based summary sentence selection module. We can
see that although sentence pre-selection removes
some sentences from consideration in the later sum-
marization step, it actually significantly improves
system performance. In the TAC 2008 data set,
each topic contains averagely 210 sentences; while
the pre-selection step chooses 13 sentences among
them. These numbers are 185 and 12 for the TAC
2011 data set. Table 5 shows the average running
time of each topic in TAC 2011 data for the two sys-
tems, with or without the pre-selection step. Here
we fix the number of compressions to 100 in both
cases for fair comparison. We can see the selec-
tion step greatly accelerates the system processing.
When applying the pre-selection step, fewer sen-
tences are used in the compression and summariza-
tion, this means we are able to use more compres-
sion candidates for each sentence (considering the
complexity of ILP module). Using the TAC 2009
as development set, we tuned the number of can-
didate compressions generated for each sentence.
Without pre-selection, we used the 100-best candi-
dates generated from the compression model; with
pre-selection, we are able to increase the number
to 200-best candidate compressions and still main-
tain reasonable computational cost. These are the
numbers used in the results in Table 3 and 4. Us-
ing more compressions helps improve summariza-
tion performance. We also notice that the compres-
sion ratios are quite different when using sentence
pre-selection vs. not. This suggests that in the im-
portant sentences (those are kept after pre-selection),
there is more summary related information and thus
the compression model keeps more words in them
(lower compression ratio).
System
Compressed Number of Running
Sentences Compressions Time (sec)
w/o Pre-selection 185 100 3.9
w/ Pre-selection 12 100 0.85
Table 5: Average running time of our system, w/ or w/o
the sentence pre-selection step. Experiments conducted
on the TAC 2011 data set. Running time refers only to
the execution time of the ILP module for each topic.
Number of compression candidates. This pa-
rameter (denoted as n) also impacts system perfor-
mance. Figure 1 shows the R-2 scores of the two
systems (with and without the sentence pre-selection
step) when using different number of compressions
for each sentence. In general, we find that the R-2
scores do not change much when n is large enough.
For example, the ?with pre-selection? system can
achieve relatively stable R-2 scores on the TAC 2008
data set (ranging from 12.2 to 12.4) when m is
greater than 140; similarly, the R-2 scores on the
TAC 2011 data is over 14.2 when m is greater than
100. Without the pre-selection step, the scores are
less stable in regard to the changing of the m value,
since the large amount of sentences plus a high vol-
ume of the compression candidates may incur huge
computational cost to the ILP solver. This is also the
reason that in Figure 1, for the system without pre-
selection, we only vary n from 1 to 100. In general,
we also notice that given more compression candi-
dates, the R-2 score is still improving, as indicated
by Figure 1. The improved performance of ?with
pre-selection? over ?without pre-selection? is partly
because fewer sentences are used and thus we are
able to increase the number of compression candi-
dates for these sentences in the ILP sentence extrac-
tion module.
Quality of sentence compression training data.
In order to illustrate the contribution of our
summary-guided sentence compression component,
we train a generic sentence compression model
and use this in our compression and summariza-
tion pipeline. The generic compression model was
trained using the Edinburgh sentence compression
corpus (Clarke and Lapata, 2008), which contains
1370 sentences collected from news articles. This
data set has been widely used in other summariza-
tion studies (Martins and Smith, 2009). Each sen-
tence has 3 compressions and we choose the short-
est compression as the reference. The average com-
pression rate of this corpus is about 28%, lower than
that in our summary guided compression data. Note
that in generic sentence compression, we only use
those word and sentence features described in Sec-
tion 4.1, not the document-level features since they
are not available for the Edinburgh data set. Results
of our system using the generic compression model
(with sentence pre-selection) are shown in the last
row of Table 3 and Table 4. We can see that the sys-
tem with this generic compression model performs
497
 11
 11.5
 12
 12.5
 13
 13.5
 14
 14.5
 15
 0  20  40  60  80  100
RO
UG
E-
2
# Compression Candidates
TAC 2011
TAC 2008
 11
 11.5
 12
 12.5
 13
 13.5
 14
 14.5
 15
 0  50  100  150  200  250
RO
UG
E-
2
# Compression Candidates
TAC 2011
TAC 2008
 11
 11.5
 12
 12.5
 13
 13.5
 14
 14.5
 15
 0  20  40  60  80  100
RO
UG
E-
2
# Compression Candidates
TAC 2011
TAC 2008
 11
 11.5
 12
 12.5
 13
 13.5
 14
 14.5
 15
 0  50  100  150  200  250
RO
UG
E-
2
# Compression Candidates
TAC 2011
TAC 2008
Figure 1: R-2 scores of the two systems (without and
with the sentence pre-selection step) when using differ-
ent number of compressions for each sentence.
worse than ours, and is also inferior to the TAC best
performing system on both data sets, which signi-
fies the importance of our proposed summary guided
sentence compression approach. We can also see
there is a difference in the compression ratio in the
system generated compressions when using differ-
ent compression corpora to train the compression
models. The resulting compression ratio patterns are
consistent with those in the training data, that is, us-
ing our guided compression corpus our system com-
pressed sentences more aggressively.
Learning curve of guided compression. Since
we use a supervised compression model, we further
consider the relationship between the summarization
performance and the number of sentence pairs used
for training the guided compression model. In to-
tal, there are 1150 training sentence pairs in our cor-
pus. We incrementally add 100 sentence pairs each
time and plot the learning curve in Figure 2. In
the compression step, we generate only the 1-best
compression candidate in order to remove the im-
pact caused by the downstream summary sentence
selection module. As seen from Figure 2, increasing
the compression training data generally improves
summarization performance, although there are also
fluctuations. When adding more training sentence
pairs, the system performance is likely to further in-
crease.
 10.5
 11
 11.5
 12
 12.5
 200  400  600  800 1000 1200
RO
UG
E-
2
# Sentence Pairs in the Training Set
TAC 2011
TAC 2008
Figure 2: ROUGE-2 scores when using different number
of sentences to train the guided compression model.
6 Conclusion and Future Work
In this paper, we propose a pipeline summariza-
tion approach that combines a novel guided com-
pression model with ILP-based summary sentence
selection. We create a guided compression cor-
pus, where the human annotators were explicitly in-
formed about the important summary words during
the compression annotation. We then train a super-
vised compression model to capture the guided com-
pression process using a set of word-, sentence-, and
document-level features. We conduct experiments
on the TAC 2008 and 2011 summarization data sets
and show that by incorporating the guided sentence
compression model, our summarization system can
yield significant performance gain as compared to
the state-of-the-art. In future, we would like to
further explore the reinforcement relationship be-
tween keywords and summaries (Wan et al, 2007),
improve the readability of the sentences generated
from the guided compression system, and report re-
sults using multiple evaluation metrics (Nenkova et
al., 2007; Louis and Nenkova, 2012) as well as per-
forming human evaluations.
498
Acknowledgments
Part of this work was done during the first au-
thor?s internship in Bosch Research and Technol-
ogy Center. The work is also partially supported
by NSF award IIS-0845484 and DARPA Contract
No. FA8750-13-2-0041. Any opinions, findings,
and conclusions or recommendations expressed are
those of the author and do not necessarily reflect the
views of the funding agencies.
References
Ahmet Aker, Trevor Cohn, and Robert Gaizauskas. 2010.
Multi-document summarization using a* search and
discriminative training. In Proceedings of EMNLP.
Miguel B. Almeida and Andre F. T. Martins. 2013. Fast
and robust compressive summarization with dual de-
composition and multi-task learning. In Proceedings
of ACL.
Taylor Berg-Kirkpatrick, Dan Gillick, and Dan Klein.
2011. Jointly learning to extract and compress. In
Proceedings of ACL.
Yllias Chali and Sadid A. Hasan. 2012. On the effective-
ness of using sentence compression models for query-
focused multi-document summarization. In Proceed-
ings of COLING.
James Clarke and Mirella Lapata. 2008. Global infer-
ence for sentence compression an integer linear pro-
gramming approach. Journal of Artificial Intelligence
Research.
Hal Daume?. 2006. Practical structured learning tech-
niques for natural language processing. Ph.D. thesis,
University of Southern California.
Dimitrios Galanis and Ion Androutsopoulos. 2010. An
extractive supervised two-stage method for sentence
compression. In Human Language Technologies: The
2010 Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics.
Michel Galley. 2006. A skip-chain conditional random
field for ranking meeting utterances by importance. In
Proceedings of EMNLP.
Dan Gillick, Benoit Favre, Dilek Hakkani-Tur, Berndt
Bohnet, Yang Liu, and Shasha Xie. 2009. The icsi/utd
summarization system at tac 2009. In Proceedings of
TAC.
Kevin Knight and Daniel Marcu. 2000. Statistics-based
summarization - step one: Sentence compression.
Julian Kupiec, Jan Pedersen, and Francine Chen. 1995.
A trainable document summarizer. In Proceedings of
SIGIR.
Chen Li, Xian Qian, and Yang Liu. 2013. Using super-
vised bigram-based ilp for extractive summarization.
In Proceedings of ACL.
Hui Lin and Jeff Bilmes. 2010. Multi-document sum-
marization via budgeted maximization of submodular
functions. In Proceedings of NAACL.
Chin-Yew Lin. 2003. Improving summarization perfor-
mance by sentence compression - A pilot study. In
Proceeding of the Sixth International Workshop on In-
formation Retrieval with Asian Language.
Chin-Yew Lin. 2004. Rouge: a package for automatic
evaluation of summaries. In Proceedings of ACL.
Fei Liu and Yang Liu. 2009. From extractive to abstrac-
tive meeting summaries: Can it be done by sentence
compression? In Proceedings of ACL-IJCNLP.
Fei Liu and Yang Liu. 2013. Towards abstractive speech
summarization: Exploring unsupervised and super-
vised approaches for spoken utterance compression.
IEEE Transactions on Audio, Speech, and Language
Processing.
Annie Louis and Ani Nenkova. 2012. Automati-
cally assessing machine summary content with a gold-
standard. Computational Linguistics.
Andre F. T. Martins and Noah A. Smith. 2009. Summa-
rization with a joint model for sentence extraction and
compression. In Proceedings of the ACL Workshop
on Integer Linear Programming for Natural Language
Processing.
Ryan McDonald. 2006. Discriminative sentence com-
pression with soft syntactic evidence. In Proceedings
of EACL.
Ani Nenkova and Kathleen McKeown. 2011. Automatic
summarization. Foundations and Trends in Informa-
tion Retrieval.
Ani Nenkova, Rebecca Passonneau, and Kathleen McK-
eown. 2007. The pyramid method: Incorporating hu-
man content selection variation in summarization eval-
uation. ACM Transactions on Speech and Language
Processing.
Jun-Ping Ng, Praveen Bysani, Ziheng Lin, Min-Yen Kan,
and Chew-Lim Tan. 2012. Exploiting category-
specific information for multi-document summariza-
tion. In Proceedings of COLING.
Tadashi Nomoto. 2007. Discriminative sentence com-
pression with conditional random fields. Information
Processing and Management.
Miles Osborne. 2002. Using maximum entropy for sen-
tence extraction. In Proceedings of the ACL-02 Work-
shop on Automatic Summarization.
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In Proceedings of HLT-
NAACL.
499
Xian Qian and Yang Liu. 2013. Fast joint compression
and summarization via graph cuts. In Proceedings of
EMNLP.
Dragomir R. Radev, Eduard Hovy, and Kathleen McKe-
own. 2002. Introduction to the special issue on sum-
marization. In Computational Linguistics.
Kapil Thadani and Kathleen McKeown. 2013. Sentence
compression with joint structural inference. In Pro-
ceedings of CoNLL.
Jenine Turner and Eugene Charniak. 2005. Supervised
and unsupervised learning for sentence compression.
In Proceedings of ACL.
Xiaojun Wan, Jianwu Yang, and Jianguo Xiao. 2007. To-
wards an iterative reinforcement approach for simulta-
neous document summarization and keyword extrac-
tion. In Proceedings of ACL.
Lu Wang, Hema Raghavan, Vittorio Castelli, Radu Flo-
rian, and Claire Cardie. 2013. A sentence com-
pression based framework to query-focused multi-
document summarization. In Proceedings of ACL.
Kristian Woodsend and Mirella Lapata. 2012. Multiple
aspect summarization using integer linear program-
ming. In Proceedings of EMNLP-CoNLL.
Katsumasa Yoshikawa, Tsutomu Hirao, Ryu Iida, and
Manabu Okumura. 2012. Sentence compression with
semantic role constraints. In Proceedings of ACL.
David Zajic, Bonnie J. Dorr, Jimmy Lin, and Richard
Schwartz. 2007. Multi-candidate reduction: Sentence
compression as a tool for document summarization
tasks. In Information Processing and Management.
500
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 691?701,
October 25-29, 2014, Doha, Qatar. c?2014 Association for Computational Linguistics
Improving Multi-documents Summarization by Sentence Compression
based on Expanded Constituent Parse Trees
Chen Li1, Yang Liu1, Fei Liu2, Lin Zhao3, Fuliang Weng3
1 Computer Science Department, The University of Texas at Dallas
Richardson, TX 75080, USA
2 School of Computer Science, Carnegie Mellon University
Pittsburgh, PA 15213, USA
3 Research and Technology Center, Robert Bosch LLC
Palo Alto, California 94304, USA
{chenli,yangl@hlt.utdallas.edu}
{feiliu@cs.cmu.edu}
{lin.zhao,fuliang.weng@us.bosch.com}
Abstract
In this paper, we focus on the problem
of using sentence compression techniques
to improve multi-document summariza-
tion. We propose an innovative sentence
compression method by considering every
node in the constituent parse tree and de-
ciding its status ? remove or retain. In-
teger liner programming with discrimina-
tive training is used to solve the problem.
Under this model, we incorporate various
constraints to improve the linguistic qual-
ity of the compressed sentences. Then we
utilize a pipeline summarization frame-
work where sentences are first compressed
by our proposed compression model to ob-
tain top-n candidates and then a sentence
selection module is used to generate the
final summary. Compared with state-of-
the-art algorithms, our model has simi-
lar ROUGE-2 scores but better linguistic
quality on TAC data.
1 Introduction
Automatic summarization can be broadly divided
into two categories: extractive and abstractive
summarization. Extractive summarization focuses
on selecting salient sentences from the document
collection and concatenating them to form a sum-
mary; while abstractive summarization is gener-
ally considered more difficult, involving sophisti-
cated techniques for meaning representation, con-
tent planning, surface realization, etc.
There has been a surge of interest in recent years
on generating compressed document summaries as
a viable step towards abstractive summarization.
These compressive summaries often contain more
information than sentence-based extractive sum-
maries since they can remove insignificant sen-
tence constituents and make space for more salient
information that is otherwise dropped due to the
summary length constraint. Two general strate-
gies have been used for compressive summariza-
tion. One is a pipeline approach, where sentence-
based extractive summarization is followed or pro-
ceeded by sentence compression (Lin, 2003; Zajic
et al., 2007; Vanderwende et al., 2007; Wang et al.,
2013). Another line of work uses joint compres-
sion and summarization. Such methods have been
shown to achieve promising performance (Daume?,
2006; Chali and Hasan, 2012; Almeida and Mar-
tins, 2013; Qian and Liu, 2013), but they are typi-
cally computationally expensive.
In this study, we propose an innovative sen-
tence compression model based on expanded con-
stituent parse trees. Our model uses integer lin-
ear programming (ILP) to search the entire space
of compression, and is discriminatively trained.
It is built based on the discriminative sentence
compression model from (McDonald, 2006) and
(Clarke and Lapata, 2008), but our method uses
an expanded constituent parse tree rather than only
the leaf nodes in previous work. Therefore we
can extract rich features for every node in the con-
stituent parser tree. This is an advantage of tree-
based compression technique (Knight and Marcu,
2000; Galley and McKeown, 2007; Wang et al.,
2013). Similar to (Li et al., 2013a), we use a
pipeline summarization framework where multi-
ple compression candidates are generated for each
pre-selected important sentence, and then an ILP-
691
based summarization model is used to select the
final compressed sentences. We evaluate our pro-
posed method on the TAC 2008 and 2011 data
sets using the standard ROUGE metric (Lin, 2004)
and human evaluation of the linguistic quality.
Our results show that using our proposed sentence
compression model in the summarization system
can yield significant performance gain in linguis-
tic quality, without losing much performance on
the ROUGE metric.
2 Related Work
Summarization research has seen great develop-
ment over the last fifty years (Nenkova and McKe-
own, 2011). Compared to the abstractive counter-
part, extractive summarization has received con-
siderable attention due to its clear problem for-
mulation: to extract a set of salient and non-
redundant sentences from the given document
set. Both unsupervised and supervised approaches
have been explored for sentence selection. Su-
pervised approaches include the Bayesian classi-
fier (Kupiec et al., 1995), maximum entropy (Os-
borne, 2002), skip-chain CRF (Galley, 2006), dis-
criminative reranking (Aker et al., 2010), among
others. The extractive summary sentence selec-
tion problem can also be formulated in an opti-
mization framework. Previous methods include
using integer linear programming (ILP) and sub-
modular functions to solve the optimization prob-
lem (Gillick et al., 2009; Li et al., 2013b; Lin and
Bilmes, 2010).
Compressive summarization receives increas-
ing attention in recent years, since it offers a vi-
able step towards abstractive summarization. The
compressed summaries can be generated through a
joint model of the sentence selection and compres-
sion processes, or through a pipeline approach that
integrates a sentence compression model with a
summary sentence pre-selection or post-selection
step.
Many studies have explored the joint sentence
compression and selection setting. Martins and
Smith (2009) jointly performed sentence extrac-
tion and compression by solving an ILP prob-
lem. Berg-Kirkpatrick et al. (2011) proposed an
approach to score the candidate summaries ac-
cording to a combined linear model of extrac-
tive sentence selection and compression. They
trained the model using a margin-based objec-
tive whose loss captures the final summary qual-
ity. Woodsend and Lapata (2012) presented an-
other method where the summary?s informative-
ness, succinctness, and grammaticality are learned
separately from data but optimized jointly using an
ILP setup. Yoshikawa et al. (2012) incorporated
semantic role information in the ILP model.
Our work is closely related with the pipeline
approach, where sentence-based extractive sum-
marization is followed or proceeded by sentence
compression. There have been many studies on
sentence compression, independent of the summa-
rization task. McDonald (2006) firstly introduced
a discriminative sentence compression model to
directly optimize the quality of the compressed
sentences produced. Clarke and Lapata (2008)
improved the above discriminative model by us-
ing ILP in decoding, making it convenient to
add constraints to preserve grammatical structure.
Nomoto (2007) treated the compression task as
a sequence labeling problem and used CRF for
it. Thadani and McKeown (2013) presented an
approach for discriminative sentence compression
that jointly produces sequential and syntactic rep-
resentations for output text. Filippova and Altun
(2013) presented a method to automatically build
a sentence compression corpus with hundreds of
thousands of instances on which deletion-based
compression algorithms can be trained.
In addition to the work on sentence compres-
sion as a stand-alone task, prior studies have also
investigated compression for the summarization
task. Knight and Marcu (2000) utilized the noisy
channel and decision tree method to perform sen-
tence compression in the summarization task. Lin
(2003) showed that pure syntactic-based compres-
sion may not significantly improve the summariza-
tion performance. Zajic et al. (2007) compared
two sentence compression approaches for multi-
document summarization, including a ?parse-and-
trim? and a noisy-channel approach. Galanis and
Androutsopoulos (2010) used the maximum en-
tropy model to generate the candidate compres-
sions by removing branches from the source sen-
tences. Woodsend and Lapata (2010) presented a
joint content selection and compression model for
single-document summarization. They operated
over a phrase-based representation of the source
document which they obtained by merging infor-
mation from PCFG parse trees and dependency
graphs. Liu and Liu (2013) adopted the CRF-
based sentence compression approach for summa-
692
rizing spoken documents. Unlike the word-based
operation, some of these models e.g (Knight and
Marcu, 2000; Siddharthan et al., 2004; Turner
and Charniak, 2005; Galanis and Androutsopou-
los, 2010; Wang et al., 2013), are tree-based ap-
proaches that operate on the parse trees and thus
the compression decision can be made for a con-
stituent, instead of a single word.
3 Sentence Compression Method
Sentence compression is a task of producing a
summary for a single sentence. The compressed
sentence should be shorter, contain important con-
tent from the original sentence, and be grammat-
ical. In some sense, sentence compression can
be described as a ?scaled down version of the
text summarization problem? (Knight and Marcu,
2002). Here similar to much previous work on
sentence compression, we just focus on how to re-
move/select words in the original sentence without
using operation like rewriting sentence.
3.1 Discriminative Compression Model by
ILP
McDonald (2006) presented a discriminative com-
pression model, and Clarke and Lapata (2008) im-
proved it by using ILP for decoding. Since our
proposed method is based upon this model, in
the following we briefly describe it first. Details
can be found in (Clarke and Lapata, 2008). In
this model, the following score function is used
to evaluate each compression candidate:
s(x, y) =
|y|
?
j=2
s(x, L(y
j?1
), L(y
j
)) (1)
where x = x
1
x
2
, ..., x
n
represents an original sen-
tence and y = y
1
y
2
, ..., y
m
denotes a compressed
sentence. Because the sentence compression prob-
lem is defined as a word deletion task, y
j
must oc-
cur in x. Function L(y
i
) ? [1...n] maps word y
i
in
the compression to the word index in the original
sentence x. Note that L(y
i
) < L(y
i+1
) is required,
that is, each word in x can only occur at most
once in compression y. In this model, a first or-
der Markov assumption is used for the score func-
tion. Decoding this model is to find the combina-
tion of bigrams that maximizes the score function
in Eq (1). Clarke and Lapata (2008) introduced the
following variables and used ILP to solve it:
?
i
=
{
1 if x
i
is in the compression
0 otherwise
?i ? [1..n]
?
i
=
{
1 if x
i
starts the compression
0 otherwise
?i ? [1..n]
?
i
=
{
1 if x
i
ends the compression
0 otherwise
?i ? [1..n]
?
ij
=
{
1 if x
i
, x
j
are in the compression
0 otherwise
?i ? [1..n ? 1]?j ? [i + 1..n]
Using these variables, the objective function can
be defined as:
max z =
n
?
i=1
?
i
? s(x, 0, i)
+
n?1
?
i=1
n
?
j=i+1
?
ij
? s(x, i, j)
+
n
?
i=1
?
i
? s(x, i, n + 1) (2)
The following four basic constraints are used to
make the compressed result reasonable:
n
?
i=1
?
i
= 1 (3)
?
j
? ?
j
?
j
?
i=1
?
ij
= 0 ?j ? [1..n] (4)
?
i
?
n
?
j=i+1
?
ij
? ?
i
= 0 ?i ? [1..n] (5)
n
?
i=1
?
i
= 1 (6)
Formula (3) and (6) denote that exactly one
word can begin or end a sentence. Formula (4)
means if a word is in the compressed sentence, it
must either start the compression or follow another
word; formula (5) represents if a word is in the
693
compressed sentence, it must either end the sen-
tence or be followed by another word.
Furthermore, discriminative models are used for
the score function:
s(x, y) =
|y|
?
j=2
w ? f(x, L(y
j?1
), L(y
j
)) (7)
High dimensional features are used and their cor-
responding weights are trained discriminatively.
Above is the basic supervised ILP formula-
tion for sentence compression. Linguistically and
semantically motivated constraints can be added
in the ILP model to ensure the correct grammar
structure in the compressed sentence. For exam-
ple, Clarke and Lapata (2008) forced the introduc-
ing term of prepositional phrases and subordinate
clauses to be included in the compression if any
word from within that syntactic constituent is also
included, and vice versa.
3.2 Compression Model based on Expanded
Constituent Parse Tree
In the above ILP model, variables are defined for
each word in the sentence, and the task is to pre-
dict each word?s status. In this paper, we propose
to adopt the above ILP framework, but operate di-
rectly on the nodes in the constituent parse tree,
rather than just the words (leaf nodes in the tree).
This way we can remove or retain a chunk of the
sentence rather than isolated words, which we ex-
pect can improve the readability and grammar cor-
rectness of the compressed sentences.
The top part of Fig1 is a standard constituent
parse tree. For some levels of the tree, the nodes
at that same level can not represent a sentence. We
extend the parse tree by duplicating non-POS con-
stituents so that leaf nodes (words and their corre-
sponding POS tags) are aligned at the bottom level
as shown in bottom of as Fig1. In the example tree,
the solid lines represent relationship of nodes from
the original parse tree, the long dot lines denote the
extension of the duplication nodes from the up-
per level to the lower level, and the nodes at the
same level are connected (arrowed lines) to repre-
sent that is a sequence. Based on this expanded
constituent parse tree, we can consider every level
as a ?sentence? and the tokens are POS tags and
parse tree labels. We apply the above compression
model in Section 3.1 on every level to decide every
node?s status in the final compressed sentence. In
order to make the compressed parsed tree reason-
able, we model the relationship of nodes between
PRP/
I 
VBP/ 
am 
DT/  
a 
NN/ 
worker 
IN/ 
from 
NNP/
USA
NP IN 
PRP 
PRP 
PRP 
DT/ 
the
 
NNP/
USA
 
IN/ 
from 
NN/ 
worker 
PP 
NP 
PRP/ 
I 
DT/ 
the
S 
VP NP 
VBP NP 
NP PP 
DT NN 
VBP 
VBP 
S 
VP NP 
VBP/ 
am 
NP 
NP 
DT/ 
a 
Figure 1: A regular constituent parse tree and its
Expanded constituent tree.
adjacent levels as following: if the parent node is
labeled as removed, all of its children will be re-
moved; one node will retain if at least one of its
children is kept.
Therefore, the objective function in the new ILP
formulation is:
max z =
height
?
l=1
(
n
l
?
i=1
?
l
i
? s(x, 0, l
i
)
+
n
l
?1
?
i=1
n
l
?
j=i+1
?
l
ij
? s(x, l
i
, l
j
)
+
n
l
?
i=1
?
l
i
? s(x, l
i
, n
l
+ 1) ) (8)
where height is the depth for a parse tree (starting
from level 1 for the tree), and n
l
means the length
of level l (for example, n
5
= 6 in the example
in Fig1). Then every level will have a set of pa-
rameters ?l
i
, ?
l
i
, ?
l
i
, and ?l
ij
, and the corresponding
constraints as shown in Formula (3) to (6). The re-
lationship between nodes from adjacent levels can
be expressed as:
?
l
i
? ?
(l+1)
j
(9)
?
l
i
?
?
?
(l+1)
j
(10)
in which node j at level (l+1) is the child of node
694
i at level l. In addition, 1 ? l ? height ? 1,
1 ? i ? n
l
and 1 ? j ? n
l+1
.
3.3 Linguistically Motivated Constraints
In our proposed model, we can jointly decide the
status of every node in the constituent parse tree
at the same time. One advantage is that we can
add constraints based on internal nodes or rela-
tionship in the parse tree, rather than only using
the relationship based on words. In addition to
the constraints proposed in (Clarke and Lapata,
2008), we introduce more linguistically motivated
constraints to keep the compressed sentence more
grammatically correct. The following describes
the constraints we used based on the constituent
parse tree.
? If a node?s label is ?SBAR?, its parent?s label
is ?NP? and its first child?s label is ?WHNP? or
?WHPP? or ?IN?, then if we can find a noun
in the left siblings of ?SBAR?, this subordi-
nate clause could be an attributive clause or
appositive clause. Therefore the found noun
node should be included in the compression
if the ?SBAR? is also included, because the
node ?SBAR? decorates the noun. For exam-
ple, the top part of Fig 2 is part of expanded
constituent parse tree of sentence ?Those who
knew David were all dead.? The nodes in el-
lipse should share the same status.
? If a node?s label is ?SBAR?, its parent?s label
is ?VP? and its first child?s label is ?WHNP?,
then if we can find a verb in the left siblings
of ?SBAR?, this subordinate clause could be
an objective clause. Therefore, the found
verb node should be included in the compres-
sion if the ?SBAR? node is also included, be-
cause the node ?SBAR? is the object of that
verb. An example is shown in the bottom part
of Fig 2. The nodes in ellipse should share the
same status.
? If a node?s label is ?SBAR?, its parent?s
label is ?VP? and its first child?s label is
?WHADVP?, then if the first leaf for this node
is a wh-word (e.g., ?where, when, why?) or
?how?, this clause may be an objective clause
(when the word is ?why, how, where?) or at-
tributive clause (when the word is ?where?) or
adverbial clause (when the word is ?when?).
Therefore, similar to above, if a verb or noun
is found in the left siblings of ?SBAR?, the
VBD/ 
knew 
NNP/ 
David 
NP 
DT 
DT 
DT 
VP 
WP 
WP/ 
who 
DT/ 
Those 
VBD 
S 
NP 
PRP/ 
he 
PRP/ 
    I 
VBP/ 
believe  
PRP VBP 
 WP/ 
what 
WHNP S PRP 
 
VBP 
VBD/ 
said 
SBAR 
VP NP 
NP VP WP PRP 
 
VBP 





















NP 
SBAR 
WHNP 
WHNP 
 
S 
Figure 2: Expanded constituent parse tree for ex-
amples.
found verb or noun node should be included
in the compression if the ?SBAR? node is also
included.
? If a node?s label is ?SBAR? and its parent?s la-
bel is ?ADJP?, then if we can find a ?JJ?, ?JJR?,
or ?JJS? in the left siblings of ?SBAR?, the
?SBAR? node should be included in the com-
pression if the found ?JJ?, ?JJR? or ?JJS? node
is also included because the node ?SBAR? is
decorated by the adjective.
? The node with a label of ?PRN? can be re-
moved without other constraints.
We also include some other constraints based on
the Stanford dependency parse tree. Table 1 lists
the dependency relations we considered.
? For type I relations, the parent and child node
with those relationships should have the same
value in the compressed result (both are kept
or removed).
? For type II relations, if the child node in
those relations is retained in the compressed
sentence, the parent node should be also re-
tained.
695
Dependency Relation Example
prt: phrase verb particle They shut down the station. prt(shut,down)
prep: prepositional modifier He lives in a small village. prep(lives,in)
I pobj: object of a preposition I sat on the chair. pobj(on,chair)
nsubj: nominal subject The boy is cute. nsubj(cute,boy)
cop: copula Bill is big. cop(big,is)
partmod: participial modifier Truffles picked during the spring are tasty. partmod(truffles,picked)
II nn: noun compound modifier Oil price futures. nn(futures,oil)
acomp: adjectival complement She looks very beautiful. acomp(looks,beautiful)
pcomp: prepositional complement He felt sad after learning that tragedy. pcomp(after,learning)
III ccomp: clausal complement I am certain that he did it. ccomp(certain,did)
tmod: temporal modifier Last night I swam in the pool. tmod(swam,night)
Table 1: Some dependency relations used for extra constraints. All the examples are from (Marneffe and
Manning, 2002)
? For type III relations, if the parent node in
these relations is retained, the child node
should be kept as well.
3.4 Features
So far we have defined the decoding process
and related constraints used in decoding. These
all rely on the score function s(x, y) = w ?
f(x, L(y
j?1
), L(y
j
)) for every level in the con-
stituent parse tree. We included all the features in-
troduced in (Clarke and Lapata, 2008) (those fea-
tures are designed for leaves). Table 2 lists the
additional features we used in our system.
General Features for Every Node
1. individual node label and concatenation of a pair of
nodes
2. distance of two nodes at the same level
3. is the node at beginning or end at that level?
4. do the two nodes have the same parent?
5. if two nodes do not have the same parent, then is the left
node the rightmost child of its parent? is the right node the
leftmost child of its parent?
6. combination of parent label if the node pair are not
under the same parent
7. number of node?s children: 1/0/>1
8. depth of nodes in the parse tree
Extra Features for Leaf nodes
1. word itself and concatenation of two words
2. POS and concatenation of two words? POS
3. whether the word is a stopword
4. node?s named entity tag
5. dependency relationship between two leaves
Table 2: Features used in our system besides those
used in (Clarke and Lapata, 2008).
3.5 Learning
To learn the feature weights during training, we
perform ILP decoding on every sentence in the
training set, to find the best hypothesis for each
node in the expanded constituent parse tree. If
the hypothesis is incorrect, we update the feature
weights using the structured perceptron learning
strategy (Collins, 2002). The reference label for
every node in the expanded constituent parse tree
is obtained automatically from the bottom to the
top of the tree. Since every leaf node (word) is
human annotated (removed or retain), we annotate
the internal nodes as removed if all of its children
are removed. Otherwise, the node is annotated as
retained.
During perceptron training, a fixed learning rate
is used and parameters are averaged to prevent
overfitting. In our experiment, we observe sta-
ble convergence using the held-out development
corpus, with best performance usually obtained
around 10-20 epochs.
4 Summarization System
Similar to (Li et al., 2013a), our summarization
system is , which consists of three key compo-
nents: an initial sentence pre-selection module
to select some important sentence candidates; the
above compression model to generate n-best com-
pressions for each sentence; and then an ILP sum-
marization method to select the best summary sen-
tences from the multiple compressed sentences.
The sentence pre-selection model is a simple su-
pervised support vector regression (SVR) model
that predicts a salience score for each sentence and
selects the top ranked sentences for further pro-
cessing (compression and summarization). The
target value for each sentence during training is
the ROUGE-2 score between the sentence and the
human written abstracts. We use three common
features: (1) sentence position in the document;
(2) sentence length; and (3) interpolated n-gram
document frequency as introduced in (Ng et al.,
2012).
The final sentence selection process follows the
696
ILP method introduced in (Gillick et al., 2009).
Word bi-grams are used as concepts, and their doc-
ument frequency is used as weights. Since we use
multiple compressions for one sentence, an addi-
tional constraint is used: for each sentence, only
one of its n-best compressions may be included in
the summary.
For the compression module, using the ILP
method described above only finds the best com-
pression result for a given sentence. To generate
n-best compression candidates, we use an iterative
approach ? we add one more constraints to prevent
it from generating the same answer every time af-
ter getting one solution.
5 Experimental Results
5.1 Experimental Setup
Summarization Data For summarization experi-
ments, we use the standard TAC data sets1, which
have been used in the NIST competitions. In par-
ticular, we used the TAC 2010 data set as train-
ing data for the SVR sentence pre-selection model,
TAC 2009 data set as development set for parame-
ter tuning, and the TAC 2008 and 2011 data as the
test set for reporting the final summarization re-
sults. The training data for the sentence compres-
sion module in the summarization system is sum-
mary guided compression corpus annotated by (Li
et al., 2013a) using TAC2010 data. In the com-
pression module, for each word we also used its
document level feature.2
Compression Data We also evaluate our com-
pression model using the data set from (Clarke
and Lapata, 2008). It includes 82 newswire arti-
cles with manually produced compression for each
sentence. We use the same partitions as (Martins
and Smith, 2009), i.e., 1,188 sentences for training
and 441 for testing.
Data Processing We use Stanford CoreNLP
toolkit3 to tokenize the sentences, extract name en-
tity tags, and generate the dependency parse tree.
Berkeley Parser (Petrov et al., 2006) is adopted
to obtain the constituent parse tree for every sen-
tence and POS tag for every token. We use Pocket
1http://www.nist.gov/tac/data/index.html
2Document level features for a word include information
such as the word?s document frequency in a topic. These
features cannot be extracted from a single sentence, as in the
standard sentence compression task, and are related to the
document summarization task.
3http://nlp.stanford.edu/software/corenlp.shtml
CRF4 to implement the CRF sentence compres-
sion model. SVMlight5 is used for the summary
sentence pre-selection model. Gurobi ILP solver6
does all ILP decoding.
5.2 Summarization Results
We compare our summarization system against
four recent studies, which have reported some of
the highest published results on this task. Berg-
Kirkpatrick et al. (2011) introduced a joint model
for sentence extraction and compression. Wood-
send and Lapata (2012) learned individual sum-
mary aspects from data, e.g., informativeness, suc-
cinctness, grammaticalness, stylistic writing con-
ventions, and jointly optimized the outcome in
an ILP framework. Ng et al. (2012) exploited
category-specific information for multi-document
summarization. Almeida and Martins (2013) pro-
posed compressive summarization method by dual
decomposition and multi-task learning. Our sum-
marization framework is the same as (Li et al.,
2013a), except they used a CRF-based compres-
sion model. In addition to the four previous stud-
ies, we also report the best achieved results in the
TAC competitions.
Table 3 shows the summarization results of our
method and others. The top part contains the re-
sults for TAC 2008 data and bottom part is for
TAC 2011 data. We use the ROUGE evaluation
metrics (Lin, 2004), with R-2 measuring the bi-
gram overlap between the system and reference
summaries and R-SU4 measuring the skip-bigram
with the maximum gap length of 4. In addition,
we evaluate the linguistic quality (LQ) of the sum-
maries for our system and (Li et al., 2013a).7 The
linguistic quality consists of two parts. One eval-
uates the grammar quality within a sentence. For
this, annotators marked if a compressed sentence
is grammatically correct. Typical grammar errors
include lack of verb or subordinate clause. The
other evaluates the coherence between sentences,
including the order of sentences and irrelevant sen-
tences. We invited 3 English native speakers to do
this evaluation. They gave every compressed sen-
tence a grammar score and a coherence score for
4http://sourceforge.net/projects/pocket-crf-1/
5http://svmlight.joachims.org/
6http://www.gurobi.com
7We chose to evaluate the linguistic quality for this system
because of two reasons: one is that we have an implementa-
tion of that method; the other more important one is that it
has the highest reported ROUGE results among the compared
methods.
697
System R-2 R-SU4 Gram Cohere
TAC?08 Best System 11.03 13.96 n/a n/a
(Berg-Kirk et al., 2011) 11.70 14.38 n/a n/a
(Woodsend et al., 2012) 11.37 14.47 n/a n/a
(Almeida et al.,2013) 12.30 15.18 n/a n/a
(Li et al., 2013a) 12.35 15.27 3.81 3.41
Our System 12.23 15.47 4.29 4.11
TAC?11 Best System 13.44 16.51 n/a n/a
(Ng et al., 2012) 13.93 16.83 n/a n/a
(Li et al., 2013a) 14.40 16.89 3.67 3.32
Our System 14.04 16.67 4.18 4.07
Table 3: Summarization results on the TAC 2008
and 2011 data sets.
each topic. The score is scaled and ranges from 1
(bad) to 5 (good). Therefore, in table 3, the gram-
mar score is the average score for each sentence
and coherence score is the average for each topic.
We measure annotators? agreement in the follow-
ing way: we consider the scores from each anno-
tator as a distribution and we find that these three
distributions are not statistically significantly dif-
ferent each other (p > 0.05 based on paired t-test).
We can see from the table that in general, our
system achieves better ROUGE results than most
previous work except (Li et al., 2013a) on both
TAC 2008 and TAC 2011 data. However, our
system?s linguistic quality is better than (Li et
al., 2013a). The CRF-based compression model
used in (Li et al., 2013a) can not well model the
grammar. Particularly, our results (ROUGE-2) are
statistically significantly (p < 0.05) higher than
TAC08 Best system, but are not statistically signif-
icant compared with (Li et al., 2013a) (p > 0.05).
The pattern is similar in TAC 2011 data. Our result
(R-2) is statistically significantly (p < 0.05) better
than TAC11 Best system, but not statistically (p >
0.05) significantly different from (Li et al., 2013a).
However, for the grammar and coherence score,
our results are statistically significantly (p < 0.05)
than (Li et al., 2013a). All the above statistics are
based on paired t-test.
5.3 Compression Results
The results above show that our summarization
system is competitive. In this section we focus
on the evaluation of our proposed compression
method. We compare our compression system
against four other models. HedgeTrimmer in Dorr
et al. (2003) applied a variety of linguistically-
motivated heuristics to guide the sentences com-
System C Rate (%) Uni-F1 Rel-F1
HedgeTrimmer 57.64 0.64 0.50
McDonald (2006) 70.95 0.77 0.55
Martins (2009) 71.35 0.77 0.56
Wang (2013) 68.06 0.79 0.59
Our System 71.19 0.77 0.58
Table 4: Sentence compression results. The hu-
man compression rate of the test set is 69%.
pression; McDonald (2006) used the output of two
parsers as features in a discriminative model that
decomposes over pairs of consecutive words; Mar-
tins and Smith (2009) built the compression model
in the dependency parse and utilized the relation-
ship between the head and modifier to preserve the
grammar relationship; Wang et al. (2013) devel-
oped a novel beam search decoder using the tree-
based compression model on the constituent parse
tree, which could find the most probable compres-
sion efficiently.
Table 4 shows the compression results of vari-
ous systems, along with the compression ratio (C
Rate) of the system output. We adopt the com-
pression metrics as used in (Martins and Smith,
2009) that measures the macro F-measure for the
retained unigrams (Uni-F1), and the one used
in (Clarke and Lapata, 2008) that calculates the
F1 score of the grammatical relations labeled by
(Briscoe and Carroll, 2002) (Rel-F1). We can see
that our proposed compression method performs
well, similar to the state-of-the-art systems.
To evaluate the power of using the expanded
parse tree in our model, we conducted another ex-
periment where we only consider the bottom level
of the constituent parse tree. In some sense, this
could be considered as the system in (Clarke and
Lapata, 2008). Furthermore, we use two differ-
ent setups: one uses the lexical features (about the
words) and the other does not. Table 5 shows the
results using the data in (Clarke and Lapata, 2008).
For a comparison, we also include the results us-
ing the CRF-based compression model (the one
used in (Nomoto, 2007; Li et al., 2013a)). We
report results using both the automatically calcu-
lated compression metrics and the linguistic qual-
ity score. Three English native speaker annotators
were asked to judge two aspects of the compressed
sentence compared with the gold result: one is the
content that looks at whether the important words
are kept and the other is the grammar score which
evaluates the sentence?s readability. Each of these
698
two scores ranges from 1(bad) to 5(good).
Table 5 shows that when using lexical features,
our system has statistically significantly (p < 0.05)
higher Grammar value and content importance
value than the CRF and the leaves only system.
When no lexical features are used, default system
can achieve statistically significantly (p < 0.01)
higher results than the CRF and the leaves only
system.
We can see that using the expanded parse tree
performs better than using the leaves only, espe-
cially when lexical features are not used. In ad-
dition, we observe that our proposed compression
method is more generalizable than the CRF-based
model. When our system does not use lexical
features in the leaves, it achieves better perfor-
mance than the CRF-based model. This is impor-
tant since such a model is more robust and may be
used in multiple domains, whereas a model rely-
ing on lexical information may suffer more from
domain mismatch. From the table we can see our
proposed tree based compression method consis-
tently has better linguistic quality. On the other
hand, the CRF compression model is the most
computationally efficient one among these three
compression methods. It is about 200 times faster
than our model using the expanded parse tree. Ta-
ble 6 shows some examples using different meth-
ods.
System C Rate(%) Uni-F1 Rel-F1 Gram Imp
Using lexical features
CRF 79.98 0.80 0.51 3.9 4.0
ILP(I) 80.54 0.79 0.57 4.0 4.2
ILP(II) 79.90 0.80 0.57 4.2 4.4
No lexical features
CRF 77.75 0.78 0.51 3.35 3.5
ILP(I) 77.77 0.78 0.56 3.7 3.9
ILP(II) 77.78 0.80 0.58 4.1 4.2
Table 5: Sentence compression results: effect of
lexical features and expanded parse tree. ILP(I)
represents the system using only bottom nodes in
constituent parse tree. ILP(II) is our system. Imp
means the content importance value.
6 Conclusion
In this paper, we propose a discriminative ILP sen-
tence compression model based on the expanded
constituent parse tree, which aims to improve the
linguistic quality of the compressed sentences in
the summarization task. Linguistically motivated
constraints are incorporated to improve the sen-
tence quality. We conduct experiments on the TAC
Using lexical features
Source:
Apart from drugs, detectives believe money is laun-
dered from a variety of black market deals involving
arms and high technology.
Human compress:
detectives believe money is laundered from a variety of
black market deals.
CRF result :
Apart from drugs detectives believe money is laundered
from a black market deals involving arms and technol-
ogy.
ILP(I) Result:
detectives believe money is laundered from a variety of
black deals involving arms.
ILP(II) Result:
detectives believe money is laundered from black mar-
ket deals.
No lexical features
Source:
Mrs Allan?s son disappeared in May 1989, after a party
during his back packing trip across North America.
Human compress:
Mrs Allan?s son disappeared in 1989, after a party dur-
ing his trip across North America.
CRF result :
Mrs Allan?s son disappeared May 1989, after during his
packing trip across North America.
ILP(I) Result:
Mrs Allan?s son disappeared in May, 1989, after a party
during his packing trip across North America .
ILP(II) Result:
Mrs Allan?s son disappeared in May 1989, after a party
during his trip.
Table 6: Examples of original sentences and their
compressed sentences from different systems.
2008 and 2011 summarization data sets and show
that by incorporating this sentence compression
model, our summarization system can yield signif-
icant performance gain in linguistic quality with-
out losing much ROUGE results. The analysis
of the compression module also demonstrates its
competitiveness, in particular the better linguistic
quality and less reliance on lexical cues.
Acknowledgments
We thank the anonymous reviewers for their de-
tailed and insightful comments on earlier drafts
of this paper. The work is also partially sup-
ported by NSF award IIS-0845484 and DARPA
Contract No. FA8750-13-2-0041. Any opinions,
findings, and conclusions or recommendations ex-
pressed are those of the authors and do not neces-
sarily reflect the views of the funding agencies.
699
References
Ahmet Aker, Trevor Cohn, and Robert Gaizauskas.
2010. Multi-document summarization using a*
search and discriminative training. In Proceedings
of EMNLP.
Miguel B. Almeida and Andre F. T. Martins. 2013.
Fast and robust compressive summarization with
dual decomposition and multi-task learning. In Pro-
ceedings of ACL.
Taylor Berg-Kirkpatrick, Dan Gillick, and Dan Klein.
2011. Jointly learning to extract and compress. In
Proceedings of ACL.
Ted Briscoe and John Carroll. 2002. Robust accurate
statistical annotation of general text. In Proceedings
of LREC.
Yllias Chali and Sadid A. Hasan. 2012. On the effec-
tiveness of using sentence compression models for
query-focused multi-document summarization. In
Proceedings of COLING.
James Clarke and Mirella Lapata. 2008. Global in-
ference for sentence compression an integer linear
programming approach. Journal of Artificial Intelli-
gence Research.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and exper-
iments with perceptron algorithms. In Proceedings
of EMNLP.
Hal Daume?. 2006. Practical structured learning tech-
niques for natural language processing. Ph.D. the-
sis, University of Southern California.
Bonnie Dorr, David Zajic, and Richard Schwartz.
2003. Hedge trimmer: A parse-and-trim approach
to headline generation. In Proceedings of NAACL.
Katja Filippova and Yasemin Altun. 2013. Overcom-
ing the lack of parallel data in sentence compression.
In Proceedings of EMNLP.
Dimitrios Galanis and Ion Androutsopoulos. 2010. An
extractive supervised two-stage method for sentence
compression. In Proceedings of NAACL.
Michel Galley and Kathleen McKeown. 2007. Lexi-
calized markov grammars for sentence compression.
In Processings of NAACL.
Michel Galley. 2006. A skip-chain conditional random
field for ranking meeting utterances by importance.
In Proceedings of EMNLP.
Dan Gillick, Benoit Favre, Dilek Hakkani-Tur, Berndt
Bohnet, Yang Liu, and Shasha Xie. 2009. The
icsi/utd summarization system at tac 2009. In Pro-
ceedings of TAC.
Kevin Knight and Daniel Marcu. 2000. Statistics-
based summarization - step one: Sentence compres-
sion. In Proceedings of AAAI.
Kevin Knight and Daniel Marcu. 2002. Summariza-
tion beyond sentence extraction: A probabilistic ap-
proach to sentence compression. Artificial Intelli-
gence, 139(1):91?107.
Julian Kupiec, Jan Pedersen, and Francine Chen. 1995.
A trainable document summarizer. In Proceedings
of SIGIR.
Chen Li, Fei Liu, Fuliang Weng, and Yang Liu. 2013a.
Document summarization via guided sentence com-
pression. In Proceedings of the EMNLP.
Chen Li, Xian Qian, and Yang Liu. 2013b. Using su-
pervised bigram-based ilp for extractive summariza-
tion. In Proceedings of ACL.
Hui Lin and Jeff Bilmes. 2010. Multi-document sum-
marization via budgeted maximization of submodu-
lar functions. In Proceedings of NAACL.
Chin-Yew Lin. 2003. Improving summarization per-
formance by sentence compression - A pilot study.
In Proceeding of the Sixth International Workshop
on Information Retrieval with Asian Language.
Chin-Yew Lin. 2004. Rouge: a package for automatic
evaluation of summaries. In Proceedings of ACL.
Fei Liu and Yang Liu. 2013. Towards abstractive
speech summarization: Exploring unsupervised and
supervised approaches for spoken utterance com-
pression. IEEE Transactions on Audio, Speech, and
Language Processing.
Marie-Catherine de Marneffe and Christopher D Man-
ning. 2002. Stanford typed dependencies manual.
Andre F. T. Martins and Noah A. Smith. 2009. Sum-
marization with a joint model for sentence extrac-
tion and compression. In Proceedings of the ACL
Workshop on Integer Linear Programming for Natu-
ral Language Processing.
Ryan McDonald. 2006. Discriminative sentence com-
pression with soft syntactic evidence. In Proceed-
ings of EACL.
Ani Nenkova and Kathleen McKeown. 2011. Auto-
matic summarization. Foundations and Trends in
Information Retrieval.
Jun-Ping Ng, Praveen Bysani, Ziheng Lin, Min-Yen
Kan, and Chew-Lim Tan. 2012. Exploiting
category-specific information for multi-document
summarization. In Proceedings of COLING.
Tadashi Nomoto. 2007. Discriminative sentence com-
pression with conditional random fields. Informa-
tion Processing and Management.
Miles Osborne. 2002. Using maximum entropy for
sentence extraction. In Proceedings of the ACL-02
Workshop on Automatic Summarization.
700
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and
interpretable tree annotation. In Proceedings of
COLING-ACL.
Xian Qian and Yang Liu. 2013. Fast joint compression
and summarization via graph cuts. In Proceedings
of EMNLP.
Advaith Siddharthan, Ani Nenkova, and Kathleen
McKeown. 2004. Syntactic simplification for im-
proving content selection in multi-document sum-
marization. In Proceedings of Coling.
Kapil Thadani and Kathleen McKeown. 2013. Sen-
tence compression with joint structural inference. In
Proceedings of CoNLL.
Jenine Turner and Eugene Charniak. 2005. Super-
vised and unsupervised learning for sentence com-
pression. In Proceedings of ACL.
Lucy Vanderwende, Hisami Suzuki, Chris Brockett,
and Ani Nenkova. 2007. Beyond sumbasic: Task-
focused summarization with sentence simplification
and lexical expansion. Information Processing &
Management.
Lu Wang, Hema Raghavan, Vittorio Castelli, Radu Flo-
rian, and Claire Cardie. 2013. A sentence com-
pression based framework to query-focused multi-
document summarization. In Proceedings of ACL.
Kristian Woodsend and Mirella Lapata. 2010. Auto-
matic generation of story highlights. In Proceedings
of ACL.
Kristian Woodsend and Mirella Lapata. 2012. Mul-
tiple aspect summarization using integer linear pro-
gramming. In Proceedings of EMNLP-CoNLL.
Katsumasa Yoshikawa, Tsutomu Hirao, Ryu Iida, and
Manabu Okumura. 2012. Sentence compression
with semantic role constraints. In Proceedings of
ACL.
David Zajic, Bonnie J. Dorr, Jimmy Lin, and Richard
Schwartz. 2007. Multi-candidate reduction: Sen-
tence compression as a tool for document summa-
rization tasks. In Information Processing and Man-
agement.
701
Proceedings of NAACL-HLT 2013, pages 1152?1162,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
A Participant-based Approach for Event Summarization Using
Twitter Streams
Chao Shen1, Fei Liu2, Fuliang Weng2, Tao Li1
1School of Computing and Information Sciences, Florida International University
Miami, Florida 33199, USA
2Research and Technology Center, Robert Bosch LLC
Palo Alto, California 94304, USA
{cshen001, taoli}@cs.fiu.edu
{fei.liu, fuliang.weng}@us.bosch.com
Abstract
Twitter offers an unprecedented advantage on
live reporting of the events happening around
the world. However, summarizing the Twit-
ter event has been a challenging task that was
not fully explored in the past. In this paper,
we propose a participant-based event summa-
rization approach that ?zooms-in? the Twit-
ter event streams to the participant level, de-
tects the important sub-events associated with
each participant using a novel mixture model
that combines the ?burstiness? and ?cohesive-
ness? properties of the event tweets, and gen-
erates the event summaries progressively. We
evaluate the proposed approach on different
event types. Results show that the participant-
based approach can effectively capture the
sub-events that have otherwise been shadowed
by the long-tail of other dominant sub-events,
yielding summaries with considerably better
coverage than the state-of-the-art.
1 Introduction
Twitter has increasingly become a critical source of
information. People report the events they are ex-
periencing or publish comments on a wide variety
of events happening around the world, ranging from
the unexpected natural disasters, regional riots, to
many scheduled events, such as sports games, po-
litical debates, local festivals, and even academic
conferences. The Twitter data streams thus cover
a broad range of events and broadcast these in-
formation in a live manner. Event summarization
in this paper aims to generate a representative and
concise textual description of the scheduled events
that are being lively reported on Twitter, providing
people with an alternative means of observing the
world beyond the traditional journalism. Specifi-
cally, we investigate scheduled events of different
types, including six of the NBA (National Basket-
ball Association) sports games and a representative
conference event, namely the Apple CEO?s keynote
speech in the Apple Worldwide Developers Confer-
ence (WWDC 2012)1. All these events have excited
great discussion among the Twitter community.
Summarizing the Twitter event is a challenging
task that has yet been fully explored in the past.
Most previous summarization studies focus on the
well-formatted news documents, as driven by the
annual DUC2 and TAC3 evaluations. In contrast,
the Twitter messages (a.k.a., tweets) are very short
and noisy, containing nonstandard terms such as ab-
breviations, acronyms, emoticons, etc. (Liu et al,
2011b; Liu et al, 2012; Eisenstein, 2013). The
noisy contents also cause great difficulties to the tra-
ditional NLP tools such as NER and dependency
parser (Ritter et al, 2011; Foster et al, 2011), lim-
iting the possibility of applying finer-grained event
analysis tools. In nature, the event tweets are closely
associated with the timeline and are drastically dif-
ferent from a static collection of news documents.
The tweets converge into text streams that pulse
along the timeline and cluster around the important
moments or sub-events. These ?sub-events? are of
crucial importance since they represent a surge of in-
terest from the Twitter audience and the correspond-
1https://developer.apple.com/wwdc/
2http://duc.nist.gov/
3http://www.nist.gov/tac/
1152
Figure 1: Example Twitter event stream (upper) and par-
ticipant stream (lower). Event stream contains tweets
related to an NBA basketball game (Spurs vs Thunder)
scheduled on May 31, 2012; participant stream contains
tweets corresponding to the player Russell Westbrook in
team Thunder. X-axis denotes the timeline and y-axis
represents the number of tweets per 10-second interval.
ing key information must be reflected in the event
summary. As such, event summarization research
has been focusing on developing accurate sub-event
detection systems and generating text descriptions
that can best summarize the sub-events in a progres-
sive manner (Chakrabarti and Punera, 2011; Nichols
et al, 2012; Zubiaga et al, 2012).
In Figure 1, we show an example Twitter event
stream and one of its ?participant? streams. The
event stream contains all the tweets related to an
NBA basketball game Spurs vs Thunder; while
the participant stream contains only tweets corre-
sponding to the player Russell Westbrook in this
game. Previous research on event summarization
focuses on identifying the important moments from
the coarse-level event stream. This may yield sev-
eral side effects: first, the spike patterns are not
clearly identifiable from the overall event stream,
though they are more clearly seen if we ?zoom-in? to
the participant level; second, it is arguable whether
the important sub-events can be accurately detected
based solely on the tweet volume change; third, a
popular participant or sub-event can elicit huge vol-
ume of tweets which dominant the event discussion
and shield less prominent sub-events. For example,
in the NBA games, discussions about the key players
(e.g., ?LeBron James?, ?Kobe Bryant?) can heavily
shadow other important participants or sub-events,
resulting in an event summary with repetitive de-
scriptions about the dominant players.
In this work, we propose a novel participant-
based event summarization approach, which dynam-
ically identifies the participants from data streams,
then ?zooms-in? the event stream to participant
level, detects the important sub-events related to
each participant using a novel time-content mixture
model, and generates the event summary progres-
sively by concatenating the descriptions of the im-
portant sub-events. Results show that the mixture
model-based sub-event detection approach can effi-
ciently incorporate the ?burstiness? and ?cohesive-
ness? of the participant streams, and the participant-
based event summarization can effectively capture
the sub-events that have otherwise been shadowed
by the long-tail of other dominant sub-events, yield-
ing summaries with considerably better coverage
than the state-of-the-art approach.
2 Related Work
Mining Twitter for event information has received
increasing attention in recent years. Many research
studies focus on identifying the trending events from
Twitter and providing a concise and dynamic visual-
ization of the information. The identified events are
often represented using a set of keywords. (Petro-
vic et al, 2010) proposed an algorithm based on
locality-sensitive hashing for detecting new events
from a stream of Twitter posts. (O?Connor et al,
2010; Becker et al, 2011b; Becker et al, 2011a;
Weng et al, 2011) proposed demo systems to dis-
play the event-related themes and popular tweets,
allowing the users to navigate through their topic
of interest. (Zhao et al, 2011) described an effort
to perform data collection and event recognition de-
spite various limits to the free access of Twitter data.
(Diao et al, 2012) integrated both temporal infor-
mation and users? personal interests for bursty topic
detection from the microblogs. (Ritter et al, 2012)
described an open-domain event-extraction and cat-
egorization system, which extracts an open-domain
calendar of significant events from Twitter.
With the identified events of interest, there is an
ever-increasing demand for event summarization,
which distills the huge volume of Twitter discus-
sions into a concise and representative textual de-
scription of the events. Many studies start with
the text summarization approaches that have been
shown to perform well on the news documents and
1153
develop adaptations to fit these methods to a col-
lection of event tweets. (Sharifi et al, 2010b) pro-
posed a graph-based phrase reinforcement algorithm
to build a one-sentence summary from a collection
of topic tweets. (Sharifi et al, 2010a; Inouye and
Kalita, 2011) presented a hybrid TF-IDF approach
to extract one- or multiple-sentence summary for
each topic. (Liu et al, 2011a) proposed to use
the concept-based ILP framework for summarizing
the Twitter trending topics, using both tweets and
the webpages linked from the tweets as input text
sources. (Harabagiu and Hickl, 2011) introduced a
generative framework that incorporates event struc-
ture and user behavior information in summarizing
multiple microblog posts related to the same topic.
Regarding summarizing the data streams, (Mar-
cus et al, 2011) introduced a ?TwitInfo? system to
visually summarize and track the events on Twit-
ter. They proposed an automatic peak detection and
labeling algorithm for the social streams. (Taka-
mura et al, 2011) proposed a summarization model
based on the facility location problem, which gener-
ates summary for a stream of short documents along
the timeline. (Chakrabarti and Punera, 2011) pro-
posed an event summarization algorithm based on
learning an underlying hidden state representation
of the event via hidden Markov models. (Louis and
Newman, 2012) presented a method for summariz-
ing a collection of tweets related to a business. The
proposed procedure aggregates tweets into subtopic
clusters which are then ranked and summarized
by a few representative tweets from each cluster.
(Nichols et al, 2012; Zubiaga et al, 2012) focused
on real-time event summarization, which detects the
sub-events by identifying those moments where the
tweet volume has increases sharply, then uses var-
ious weighting schemes to perform tweet selection
and finally generates the event summary.
Our work is different from the above research
studies in three folds: first, we propose to ?zoom-
in? the Twitter event streams to the participant
level, which allows us to clearly identify the im-
portant sub-events associated with each participant
and generate a balanced event summary with com-
prehensive coverage of all the important sub-events;
second, we propose a novel time-content mixture
model approach for sub-event detection, which ef-
fectively leverages the ?burstiness? and ?cohesive-
ness? of the event tweets and accurately detects
the participant-level sub-events. Third, we evalu-
ate the participant-based event summarization sys-
tem on different event types and demonstrate that the
proposed approach outperforms the state-of-the-art
method by a considerable margin.
3 Participant-based Event Summarization
We propose a novel participant-centered event sum-
marization approach that consists of three key com-
ponents: (1) ?Participant Detection? dynamically
identifies the event participants and divides the
entire event stream into a number of participant
streams (Section 3.1); (2) ?Sub-event Detection? in-
troduces a novel time-content mixture model ap-
proach to identify the important sub-events associ-
ated with each participant; these ?participant-level
sub-events? are then merged along the timeline to
form a set of ?global sub-events?4, which capture
all the important moments in the event stream (Sec-
tion 3.2); (3) ?Summary Tweet Extraction? extracts
the representative tweets from the global sub-events
and forms a comprehensive coverage of the event
progress (Section 3.3).
3.1 Participant Detection
We define event participants as the entities that play
a significant role in shaping the event progress. ?Par-
ticipant? is a general concept to denote the event
participating persons, organizations, product lines,
etc., each of which can be captured by a set of
correlated proper nouns. For example, the NBA
player ?LeBron Raymone James? can be represented
by {LeBron James, LeBron, LBJ, King James, L.
James}, where each proper noun represents a unique
mention of the participant. In this work, we automat-
ically identify the proper nouns from tweet streams,
filter out the infrequent ones using a threshold ?,
and cluster them into individual event participants.
This process allows us to dynamically identify the
key participating entities and provide a full-coverage
for these participants in the event summary.
4We use ?participant sub-events? and ?global sub-events?
respectively to represent the important moments happened on
the participant-level and on the entire event-level. A ?global
sub-event? may consist of one or more ?participant sub-events?.
For example., the ?steal? action in the basketball game typically
involves both the defensive and offensive players, and can be
generated by merging the two participant-level sub-events.
1154
We formulate the participant detection in a hier-
archical agglomerative clustering framework. The
CMU TweetNLP tool (Gimpel et al, 2011) was used
for proper noun tagging. The proper nouns (a.k.a.,
mentions) are grouped into clusters in a bottom-up
fashion. Two mentions are considered similar if they
share (1) lexical resemblance, and (2) contextual
similarity. For example, in the following two tweets
?Gotta respect Anthony Davis, still rocking the uni-
brow?, ?Anthony gotta do something about that uni-
brow?, the two mentions Anthony Davis and An-
thony are referring to the same participant and they
share both character overlap (?anthony?) and con-
text words (?unibrow?, ?gotta?). We use sim(ci, cj)
to represent the similarity between two mentions ci
and cj , defined as:
sim(ci, cj) = lex sim(ci, cj)?cont sim(ci, cj)
where the lexical similarity (lex sim(?)) is defined
as a binary function representing whether a mention
ci is an abbreviation, acronym, or part of another
mention cj , or if the character edit distance between
the two mentions is less than a threshold ?5:
lex sim(ci, cj)=
?
?
?
1 ci(cj) is part of cj(ci)
1 EditDist(ci, cj) < ?
0 Otherwise
We define the context similarity (cont sim(?)) of
two mentions as the cosine similarity between their
context vectors ~vi and ~vj . Note that on the tweet
stream, two temporally distant tweets can be very
different even though they are lexically similar, e.g.,
two slam dunk shots performed by the same player
at different time points are different. We there-
fore restrain the context to a segment of the tweet
stream |Sk| and then take the weighted average of
the segment-based similarity as the final context
similarity. To build the context vector, we use term
frequency (TF) as the term weight and remove all the
stopwords. We use |D| to represent the total tweets
in the event stream.
cont sim|Sk|(ci, cj) = cos(~vi, ~vj)
cont sim(ci, cj) =
?
k
|Sk|
|D|
? cont sim|Sk|(ci, cj)
5? was empirically set as 0.2?min{|ci|, |cj |}
t w
Wz? |D|
? ? ? ?'K B
Figure 2: Plate notation of the mixture model.
Similarity between two clusters of mentions are de-
fined as the maximum possible similarity between a
pair of mentions, each from one cluster:
sim(Ci, Cj) = max
ci?Ci,cj?Cj
sim(ci, cj)
We perform bottom-up agglomerative clustering on
the mentions until a stopping threshold ? has been
reached for sim(Ci, Cj). The clustering approach
naturally groups the frequent proper nouns into par-
ticipants. The participant streams are then formed
by gathering the tweets that contain one or more
mentions in the participant cluster.
3.2 Mixture Model-based Sub-event Detection
A sub-event corresponds to a topic that emerges
from the data stream, being intensively discussed
during a short period, and then gradually fades away.
The tweets corresponding to a sub-event thus de-
mand not only ?temporal burstiness? but also a cer-
tain degree of ?lexical cohesiveness?. To incorporate
both the time and content aspects of the sub-events,
we propose a mixture model approach for sub-event
detection. Figure 2 shows the plate notation.
In the proposed model, each tweet d in the data
stream D is generated from a topic z, weighted by
piz . Each topic is characterized by both its content
and time aspects. The content aspect is captured by
a multinomial distribution over the words, param-
eterized by ?; while the time aspect is character-
ized by a Gaussian distribution, parameterized by ?
and ?, with ? represents the average time point that
the sub-event emerges and ? determines the duration
of the sub-event. These distributions bear similari-
ties with the previous work (Hofmann, 1999; Allan,
2002; Haghighi and Vanderwende, 2009). In addi-
tion, there are often background or ?noise? topics
that are being constantly discussed over the entire
1155
event evolvement process and do not present the de-
sired ?burstiness? property. We use a uniform dis-
tribution U(tb, te) to model the time aspect of these
?background? topics, with tb and te being the event
beginning and end time points. The content aspect
of a background topic is modeled by similar multi-
nomial distribution, parameterized by ??. We use the
maximum likelihood parameter estimation. The data
likelihood can be represented as:
L(D) =
?
d?D
?
z
{pizpz(td)
?
w?d
pz(w)}
where pz(td) models the timestamp of tweet d under
the topic z; pz(w) corresponds to the word distribu-
tion in topic z. They are defined as:
pz(td) =
{
N(td;?z, ?z) if z is a sub-event topic
U(tb, te) if z is background topic
pz(w) =
{
p(w; ?z) if z is a sub-event topic
p(w; ??z) if z is background topic
where both p(w; ?z) and p(w; ??z) are multinomial
distributions over the words. Initially, we assume
there are K sub-event topics and B background top-
ics and use the EM algorithm for model fitting. The
EM equations are listed below:
E-step:
p(zd = j) ?
?
?
?
pijN(d;?j , ?j)
?
w?d
p(w; ?j) if j <= K
pijU(tb, te)
?
w?d
p(w; ??j) else
M-step:
pij ?
?
d
p(zd = j)
p(w; ?j) ?
?
d
p(zd = j)? c(w, d)
p(w; ??j) ?
?
d
p(zd = j)? c(w, d)
?j =
?
d p(zd = j)? td
?K
j=1
?
d p(zd = j)
?2j =
?
d p(zd = j)? (td ? ?j)
2
?K
j=1
?
d p(zd = j)
To process the data stream D, we divide the data
into 10-second bins and process each bin at a time.
The peak time of a sub-event was determined as
the bin that has the most tweets related to this sub-
event. During EM initialization, the number of sub-
event topics K was empirically decided by scanning
through the data stream and examine tweets in ev-
ery 3-minute stream segment. If there was a spike6,
we add a new sub-event to the model and use the
tweets in this segment to initialize the value of ?,
?, and ?. Initially, we use a fixed number of back-
ground topics with B = 4. A topic re-adjustment
was performed after the EM process. We merge two
sub-events in a data stream if they (1) locate closely
in the timeline, with peaks times within a 2-minute
window; and (2) share similar word distributions:
among the top-10 words with highest probability in
the word distributions, there are over 5 words over-
lap. We also convert the sub-event topics to back-
ground topics if their ? values are greater than a
threshold ?7. We then re-run the EM to obtain the
updated parameters. The topic re-adjustment pro-
cess continues until the number of sub-events and
background topics do not change further.
We obtain the ?participant sub-events? by ap-
plying this sub-event detection approach to each of
the participant streams. The ?global sub-events?
are obtained by merging the participant sub-events
along the timeline. We merge two participant sub-
events into a global sub-event if (1) their peaks are
within a 2-minute window, and (2) the Jaccard simi-
larity (Lee, 1999) between their associated tweets is
greater than a threshold (set to 0.1 empirically). The
tweets associated with each global sub-event are the
ones with p(z|d) greater than a threshold ?, where z
is one of the participant sub-events and ? was set to
0.7 empirically. After the sub-event detection pro-
cess, we obtain a set of global sub-events and their
associated event tweets.8
3.3 Summary Tweet Extraction
We extract a representative tweet from each of the
global sub-events and concatenate them to form an
informative event summary. Note that our goal in
this work is to identify all the important moments
6We use the algorithm described in (Marcus et al, 2011) as
a baseline and ad hoc spike detection algorithm.
7? was set to 5 minutes in our experiments.
8We empirically set some threshold values in the topic re-
adjustment and sub-event merging process. In future, we would
like to explore more principled way of parameter selection.
1156
Event Date Duration #Tweets
Lakers vs Okc 05/19/2012 3h10m 218,313
N Celtics vs 76ers 05/23/2012 3h30m 245,734
B Celtics vs Heat 05/30/2012 3h30m 345,335
A Spurs vs Okc 05/31/2012 3h 254,670
Heat vs Okc (1) 06/12/2012 3h30m 331,498
Heat vs Okc (2) 06/21/2012 3h30m 332,223
Apple?s WWDC?12 Conf. 06/11/2012 3h30m 163,775
Table 1: Statistics of the data set, including six NBA bas-
ketball games and the WWDC 2012 conference event.
for event summarization, but not on proposing new
methods for tweet selection. We thus use the Hybrid
TF-IDF approach (Sharifi et al, 2010a; Liu et al,
2011a) to extract the representative sentences from
a collection of tweets. In this approach, each tweet
was considered as a sentence. The sentences were
ranked according to the average TF-IDF score of the
consisting words; top weighted sentences were it-
eratively extracted, while excluding those that have
high cosine similarity with the existing summary
sentences. (Inouye and Kalita, 2011) showed the
Hybrid TF-IDF approach performs constantly better
than the phrase reinforcement algorithm and other
traditional summarization systems.
4 Data Corpus
We evaluate the proposed event summarization ap-
proach on six NBA basketball games and a repre-
sentative conference event, namely the Apple CEO?s
keynote speech in the Apple Worldwide Develop-
ers Conference (WWDC 2012)9. We use the het-
erogeneous event types to verify that the proposed
approach can robustly and efficiently produce sum-
maries on different event streams. The tweet streams
corresponding to these events are collected using
the Twitter Streaming API10 with pre-defined key-
word set. For NBA games, we use the team names,
first name and last name of the players and head
coaches as keywords for retrieving the event tweets;
for the WWDC conference, the keyword set contains
about 20 terms related to the Apple event, such as
?wwdc?, ?apple?, ?mac?, etc. We crawl the tweets
in real-time when these scheduled events are taking
place; nevertheless, certain non-event tweets could
be mis-included due to the broad coverage of the
used keywords. During preprocessing, we filter out
9https://developer.apple.com/wwdc/
10https://dev.twitter.com/docs/streaming-apis
Time Action (Sub-event) Score
9:22 Chris Bosh misses 10-foot two point shot 7-2
9:22 Serge Ibaka defensive rebound 7-2
9:11 Kevin Durant makes 15-foot two point shot 9-2
8:55 Serge Ibaka shooting foul (Shane Battier draws 9-2
the foul)
8:55 Shane Battier misses free throw 1 of 2 9-2
8:55 Miami offensive team rebound 9-2
8:55 Shane Battier makes free throw 2 of 2 9-3
Table 2: An example clip of the play-by-play live cov-
erage of an NBA game (Heat vs Okc). ?Time? corre-
sponds to the minutes left in the current quarter of the
game; ?Score? shows the score between the two teams.
the tweets containing URLs, non-English tweets,
and retweets since they are less likely containing
new information regarding the event progress. Ta-
ble 1 shows statistics of the event tweets after the
filtering process. In total, there are over 1.8 million
tweets used in the event summarization experiments.
We use the play-by-play live coverage collected
from the ESPN11 and MacRumors12 websites as ref-
erence, which provide detailed descriptions of the
NBA and WWDC events as they unfold. Table 2
shows an example clip of the play-by-play descrip-
tions of an NBA game. Ideally, each item in the live
coverage descriptions may correspond to a sub-event
in the tweet streams, but in reality, not all actions
would attract enough attention from the Twitter au-
dience. We use a human annotator to manually filter
out the actions that did not lead to any spike in the
corresponding participant stream. The rest items are
projected to the participant and event streams as the
goldstandard sub-events. The projection was man-
ually performed since the ?game clock? associated
with the goldstandard (first column in Table 2) does
not align well with the ?wall clock? due to the game
rules such as timeout and halftime rest. To evalu-
ate the participant detection performance, we ask the
annotator to manually group the proper noun men-
tions into clusters, each cluster corresponds to a par-
ticipant. The mentions that do not correspond to any
participant are discarded. The goldstandard event
summaries are generated by manually selecting one
representative tweet from each of the groundtruth
global sub-events. We choose not to use the play-
by-play descriptions as reference summaries since
their vocabulary is rather limited and do not overlap
with the tweet language.
11http://espn.go.com/nba/scoreboard
12http://www.macrumorslive.com/archive/wwdc12/
1157
Example Participants - NBA game
westbrook, russell westbrook
stephen jackson, steven jackson, jackson
james, james harden, harden
ibaka, serge ibaka
oklahoma city thunder, oklahoma
gregg popovich, greg popovich, popovich
kevin durant, kd, durant
thunder, okc, #okc, okc thunder, #thunder
Example Participants - WWDC Conference
macbooks, mbp, macbook pro, macbook air,...
google maps, google, apple maps
wwdc, apple wwdc, #wwdc
os, mountain, os x mountain, os x
iphone 4s, iphone 3gs, iphone
Table 3: Example participants automatically detected
from the NBA game Spurs vs Okc (2012-5-31) and the
WWDC?12 conference.
5 Experimental Results
We evaluate the participant-based event summariza-
tion in a cascaded fashion and present results for
each of the three components, including the par-
ticipant detection (Section 5.1), sub-event detection
(Section 5.2), and quantitative and qualitative evalu-
ation of example event summaries (Section 5.3).
5.1 Participant Detection Results
In Table 3, we show example participants that were
automatically detected by the proposed hierarchical
agglomerative clustering approach. We note that the
clusters include various mentions of the same event
participant, e.g., ?gregg popovich?, ?greg popovich?,
and ?popovich? are both referring to the head coach
of the team Spurs; ?macbooks?, ?macbook pro?,
?mbp? are referring to a line of products from Apple.
Quantitatively, we evaluate the participant detection
results on both participant- and mention-level. As-
sume the system-detected and the goldstandard par-
ticipant clusters are Ts and Tg respectively. We de-
fine a correct participant as a system detected par-
ticipant with more than half of its associated men-
tions are included in a goldstandard participant (re-
ferred to as the hit participant). As a result, we
can define the participant-level precision and recall
as below:
participant-prec = #correct-participants/|Ts|
participant-recall = #hit-participants/|Tg|
Note that a correct participant may include incor-
rect mentions, and that more than one correct par-
Figure 3: Participant detection performance. The upper
figures represent the participant-level precision and re-
call scores; while the lower figures represent the mention-
level precision and recall. X-axis corresponds to the six
NBA games and the WWDC conference.
ticipants may correspond to the same hit participant,
both of which are undesired. In the latter case, we
use representative participant to refer to the cor-
rect participant which contains the most mentions
in the hit participant. In this way, we build a 1-
to-1 mapping from the detected participants to the
groundtruth participants. Next, we define correct
mentions as the union of the overlapping mentions
between all pairs of representative and hit partici-
pants. Then we calculate the mention-level precision
and recall as the number of correct mentions divided
by the total mentions in the system or goldstandard
participant clusters.
Figure 3 shows the participant- and mention-level
precision and recall scores. We experimented with
different similarity measures for the agglomerative
clustering approach13. The ?global context? means
that the context vectors are created from the entire
data stream; this may not perform well since dif-
ferent participants can share similar global context.
E.g., the terms ?shot?, ?dunk?, ?rebound? can ap-
pear in the context of any NBA players and are not
13The stopping threshold ? was set to 0.15, local context
length is 3 minutes, and frequency threshold ? was set to 200.
1158
Participant-level Sub-event Detection Global Sub-event Detection
Event
#P #S
Spike MM
#S
Spike Participant + Spike Participant + MM
R P F R P F R P F R P F R P F
Lakers vs Okc 9 65 0.75 0.31 0.44 0.71 0.39 0.50 48 0.67 0.38 0.48 0.94 0.19 0.32 0.88 0.40 0.55
Celtics vs 76ers 10 88 0.52 0.39 0.45 0.53 0.43 0.47 60 0.65 0.51 0.57 0.72 0.18 0.29 0.78 0.39 0.52
Celtics vs Heat 14 152 0.53 0.29 0.37 0.50 0.38 0.43 67 0.57 0.41 0.48 0.97 0.21 0.35 0.91 0.28 0.43
Spurs vs Okc 12 98 0.78 0.46 0.58 0.84 0.57 0.68 81 0.41 0.42 0.41 0.88 0.35 0.50 0.91 0.54 0.68
Heat vs Okc (1) 15 123 0.75 0.27 0.40 0.72 0.35 0.47 85 0.41 0.47 0.44 0.94 0.20 0.33 0.96 0.34 0.50
Heat vs okc (2) 13 153 0.74 0.36 0.48 0.76 0.43 0.55 92 0.41 0.33 0.37 0.88 0.21 0.34 0.87 0.38 0.53
WWDC?12 10 56 0.64 0.14 0.23 0.59 0.33 0.42 43 0.53 0.26 0.35 0.77 0.14 0.24 0.70 0.31 0.43
Average 12 105 0.67 0.32 0.42 0.66 0.41 0.50 68 0.52 0.40 0.44 0.87 0.21 0.34 0.86 0.38 0.52
Table 4: Sub-event detection results on both participant and the event streams. ?Spike? corresponds to the spike
detection algorithm proposed in (Marcus et al, 2011); ?MM? represents our proposed time-content mixture model
approach. ?#P? and ?#S? list the number of participants and sub-events in each event stream.
discriminative enough. We found that adding the
lexical similarity measure greatly boosted the clus-
tering performance, especially on the mention-level,
and that combining the lexical similarity with the lo-
cal context is even more helpful for some events.
We notice that two events (celtics vs 76ers and
celtics vs heat) yield relatively low precision on both
participant- and mention-level. Taking a close look
at the data, we found that these two events acciden-
tally co-occurred with other popular events, namely
the TV program ?American Idol? finale and the NBA
Draft. The keyword based data crawler thus includes
many noisy tweets in the event streams, leading to
some false participants being detected.
5.2 Sub-event Detection Results
We compare our proposed time-content mixture
model (noted as ?MM?) against the spike detection
algorithm proposed in (Marcus et al, 2011) (noted
as ?Spike?) . The spike algorithm is based on the
tweet volume change. It uses 10 seconds as a time
unit, calculates the tweet arrival rate in each unit,
and identifies the rates that are significantly higher
than the mean tweet rate. For these rate spikes, the
algorithm finds the local maximum of tweet rate and
identify a window surrounding the local maximum.
We tune the parameter of the ?Spike? approach (set
? = 4) so that it yields similar recall values as the
mixture model approach. We then apply the ?MM?
and ?Spike? approaches to both the participant and
event streams and evaluate the sub-event detection
performance. Results are shown in Table 4. A sys-
tem detected sub-event is considered to match the
goldstandard sub-event if its peak time is within a
2-minute window of the goldstandard.
We first apply the ?Spike? and ?MM? approach to
the participant streams. The participant streams on
which we cannot detect any meaningful sub-events
have been excluded, the resulting number of partic-
ipants are listed in Table 4 and denoted as ?#P?.
In general, we found the ?MM? approach can per-
form better since it inherently incorporates both the
?burstiness? and ?lexical cohesiveness? of the event
tweets, while the ?Spike? approach relies solely on
the ?burstiness? property. Note that although we di-
vide the entire event stream into participant streams,
some key participants still own huge amount of dis-
cussion and the spike patterns are not always clearly
identifiable. The time-content mixture model gains
advantages in these cases.
We apply three settings to detect global sub-
events on the data streams. ?Spike? directly ap-
plies the spike algorithm on the entire event stream;
the ?Participant + Spike? and ?Participant + MM?
approaches first perform sub-event detection on the
participant streams and then merge the detected sub-
events along the timeline to generate global sub-
events. Note that there are fewer goldstandard
sub-events (?#S?) on the global streams since each
global sub-event may correspond to one or multiple
participant-level sub-events. Because of the averag-
ing effect, spike patterns on the entire event stream
is less obvious than those on the participant streams.
As a result, few spikes have been detected on the
event stream using the ?Spike? algorithm, which
leads to low recall as compared to other participant-
based approaches. It also indicates that, by dividing
the entire event stream into participant streams, we
have a better chance of identifying the sub-events
that have otherwise been shadowed by the domi-
nant sub-events or participants. The two participant-
based methods yield similar recall but ?Participant
1159
+ Spike? yields slightly worse precision, since it is
very sensitive to the spikes on the participant-level,
leading to the rise of false alarms. The ?Participant +
MM? approach is much better in precision, which is
consistent to our findings on the participant streams.
5.3 Summarization Results
Summarization evaluation has been a longstanding
issue in the literature (Nenkova and Mckeown, 2011;
Liu and Liu, 2010). There are even less studies fo-
cusing on evaluating the event summaries generated
from data streams. Since the summary annotation
takes quite some effort, we sample a 10-minute seg-
ment from each of the seven event streams and ask
a human annotator to select representative tweets
for each segment. We then compare the system
summaries against the manual summaries using the
ROUGE-1 (Lin, 2004) metric. The quantitative re-
sults and qualitative analysis are presented in Table 5
and Table 6 respectively. Note that the ROUGE
scores are based solely on the n-gram overlap be-
tween the system and reference summaries, which
may not be the most appropriate measure for eval-
uating the Twitter event summaries. However, we
do notice that the accurate sub-event detection per-
formance can successfully translate into a gain of
the ROUGE scores. Qualitatively, the participant-
based event summarization approach focus more on
extracting tweets associated with the targeted partic-
ipants, which could lead to better text coherence.
6 Conclusion and Future Work
In this work, we made an initial attempt to gen-
erate event summaries using Twitter data streams.
We proposed a participant-based event summariza-
tion approach which ?zooms-in? the Twitter event
streams to the participant level, detects the impor-
tant sub-events associated with each participant us-
ing a novel mixture model that incorporates both the
?burstiness? and ?cohesiveness? of tweets, and gen-
erates the event summaries progressively. Results
show that the proposed approach can effectively cap-
ture the sub-events that have otherwise been shad-
owed by the long-tail of other dominant sub-events,
yielding summaries with considerably better cover-
age. Without loss of generality, we report results
on the entire event streams, though the proposed ap-
proach can well be applied in an online fashion.
Event Method R(%) P(%) F(%)
NBA Average
Spike 14.73 23.24 16.87
Participant + Spike 54.60 14.65 22.40
Participant + MM 54.36 23.06 31.53
WWDC Conf.
Spike 26.58 39.62 31.82
Participant + Spike 49.37 25.16 33.33
Participant + MM 42.77 31.73 36.07
Table 5: ROUGE-1 scores of summarization
Method Summary
Manual
Good drive for durant
Pretty shot by Duncan
Good 3 point tony parker
Nice move westbrook
Good shot Westbrook
Spike
Game 3. Spurs vs. OKC
Okc and spurs game.
Participant
+ Spike
OKLAHOMA CITY THUNDER vs san antonio
spurs!! YA
I hope okc win the series. Ill hate too see the heat
play San Antonio
we aint in San Antonio anymore.
NBA: SA 0 OKC 8, 9:11 1st.#TeamOkc
San antonio spurs for 21 consecutive win? #nba
Somebody Should Stop Tim Duncan.
Pass the damn ball Westbrook
Good 3 pointer tony parker!
Participant
+ MM
Tim Duncan shot is so precise
Tim Duncan is gettin started
Good 3 pointer tony parker!
Sefalosa guarding tony parker. Good fucking move
coach brooks
Westbrook = 2 Fast 2 Furious
Niggas steady letting Tim Duncan shoot
Westbrook mid range shot is automatic
Table 6: Example summaries for an event segment. Par-
ticipants are marked using italicized text.
There are many challenges left in this line of re-
search. Having a standardized evaluation metric for
event summaries is one of them. In the current work,
we employed ROUGE-1 for summary evaluation,
since it has been shown to correlate well with the hu-
man judgements on noisy text genres (Liu and Liu,
2010). We would like to explore other evaluation
metrics (e.g., ROUGE-2, -SU4, Pyramid (Nenkova
et al, 2007)) and the human evaluation in future.
We will also explore better ways of integrating the
sub-event detection and summarization approaches.
Acknowledgments
Part of this work was done during the first author?s
internship in Bosch Research and Technology Cen-
ter. The work is also partially supported by NSF
grants DMS-0915110 and HRD-0833093.
1160
References
James Allan. 2002. Topic detection and tracking: Event-
based information organization. Kluwer Academic
Publishers Norwell, MA, USA.
Hila Becker, Feiyang Chen, Dan Iter, Mor Naaman, and
Luis Gravano. 2011a. Automatic identification and
presentation of twitter content for planned events. In
Proceedings of the Fifth International AAAI Confer-
ence on Weblogs and Social Media (ICWSM), pages
655?656.
Hila Becker, Mor Naaman, and Luis Gravano. 2011b.
Beyond trending topics: Real-world event identifica-
tion on twitter. In Proceedings of the Fifth Interna-
tional AAAI Conference on Weblogs and Social Media
(ICWSM), pages 438?441.
Deepayan Chakrabarti and Kunal Punera. 2011. Event
summarization using tweets. In Proceedings of the
Fifth International AAAI Conference on Weblogs and
Social Media (ICWSM), pages 66?73.
Qiming Diao, Jing Jiang, Feida Zhu, and Ee-Peng Lim.
2012. Finding bursty topics from microblogs. In Pro-
ceedings of the 50th Annual Meeting of the Association
for Computational Linguistics (ACL), pages 536?544.
Jacob Eisenstein. 2013. What to do about bad language
on the internet. In Proceedings of the 2013 Confer-
ence of the North American Chapter of the Associa-
tion for Computational Linguistics: Human Language
Technologies (NAACL/HLT).
Jennifer Foster, Ozlem Cetinoglu, Joachim Wagner,
Joseph Le Roux, Stephen Hogan, Joakim Nivre,
Deirdre Hogan, and Josef van Genabith. 2011. #hard-
toparse: POS tagging and parsing the twitterverse. In
Proceedings of the AAAI Workshop on Analyzing Mi-
crotext, pages 20?25.
Kevin Gimpel, Nathan Schneider, Brendan O?Connor,
Dipanjan Das, Daniel Mills, Jacob Eisenstein, Michael
Heilman, Dani Yogatama, Jeffrey Flanigan, and
Noah A. Smith. 2011. Part-of-speech tagging for twit-
ter: Annotation, features, and experiments. In Pro-
ceedings of the 49th Annual Meeting of the Associa-
tion for Computational Linguistics: Human Language
Technologies (ACL-HLT), pages 42?47.
Aria Haghighi and Lucy Vanderwende. 2009. Explor-
ing content models for multi-Document summariza-
tion. In Proceedings of Human Language Technolo-
gies: The 2009 Annual Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics (NAACL), pages 362?370.
Sanda Harabagiu and Andrew Hickl. 2011. Relevance
modeling for microblog summarization. In Proceed-
ings of the Fifth International AAAI Conference on We-
blogs and Social Media (ICWSM), pages 514?517.
Thomas Hofmann. 1999. Probabilistic latent semantic
analysis. In Proceedings of Uncertainty in Artificial
Intelligence (UAI).
David Inouye and Jugal K. Kalita. 2011. Compar-
ing twitter summarization algorithms for multiple post
summaries. In Proceedings of 2011 IEEE Third Inter-
national Conference on Social Computing, pages 290?
306.
Lillian Lee. 1999. Measures of distributional similarity.
In Proceedings of the 37th Annual Meeting of the As-
sociation for Computational Linguistics (ACL), pages
25?32.
Chin-Yew Lin. 2004. ROUGE: A package for automatic
evaluation of summaries. In Workshop on Text Sum-
marization Branches Out.
Feifan Liu and Yang Liu. 2010. Exploring correlation
between ROUGE and human evaluation on meeting
summaries. IEEE Transactions on Audio, Speech, and
Language Processing, 18(1):187?196.
Fei Liu, Yang Liu, and Fuliang Weng. 2011a. Why is
?SXSW? trending? Exploring multiple text sources
for twitter topic summarization. In Proceedings of the
ACL Workshop on Language in Social Media (LSM),
pages 66?75.
Fei Liu, Fuliang Weng, Bingqing Wang, and Yang Liu.
2011b. Insertion, deletion, or substitution? Normal-
izing text messages without pre-categorization nor su-
pervision. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguistics
(ACL), pages 71?76.
Fei Liu, Fuliang Weng, and Xiao Jiang. 2012. A broad-
coverage normalization system for social media lan-
guage. In Proceedings of the 50th Annual Meeting of
the Association for Computational Linguistics (ACL),
pages 1035?1044.
Annie Louis and Todd Newman. 2012. Summarization
of business-related tweets: A concept-based approach.
In Proceedings of the 24th International Conference
on Computational Linguistics (COLING).
Adam Marcus, Michael S. Bernstein, Osama Badar,
David R. Karger, Samuel Madden, and Robert C.
Miller. 2011. Twitinfo: Aggregating and visualizing
microblogs for event exploration. In Proceedings of
the SIGCHI Conference on Human Factors in Com-
puting Systems, pages 227?236.
Ani Nenkova and Kathleen Mckeown. 2011. Automatic
summarization. Foundations and Trends in Informa-
tion Retrieval, 5(2?3):103?233.
Ani Nenkova, Rebecca Passonneau, and Kathleen Mcke-
own. 2007. The pyramid method: Incorporating hu-
man content selection variation in summarization eval-
uation. ACM Transactions on Speech and Language
Processing, 4(2).
1161
Jeffrey Nichols, Jalal Mahmud, and Clemens Drews.
2012. Summarizing sporting events using twitter. In
Proceedings of the 2012 ACM Interntional Conference
on Intelligent User Interfaces (IUI), pages 189?198.
Brendan O?Connor, Michel Krieger, and David Ahn.
2010. TweetMotif: Exploratory search and topic sum-
marization for twitter. In Proceedings of the Fourth
International AAAI Conference on Weblogs and Social
Media (ICWSM), pages 384?385.
Sasa Petrovic, Miles Osborne, and Victor Lavrenko.
2010. Streaming first story detection with application
to twitter. In Proceedings of the 2010 Annual Con-
ference of the North American Chapter of the Associ-
ation for Computational Linguistics (NAACL), pages
181?189.
Alan Ritter, Sam Clark, Mausam, and Oren Etzioni.
2011. Named entity recognition in tweets: An exper-
imental study. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing
(EMNLP), pages 1524?1534.
Alan Ritter, Mausam, Oren Etzioni, and Sam Clark.
2012. Open domain event extraction from twitter. In
Proceedings of the 18th ACM SIGKDD International
Conference on Knowledge Discovery and Data Min-
ing, pages 1104?1112.
Beaux Sharifi, Mark-Anthony Hutton, and Jugal K.
Kalita. 2010a. Experiments in microblog summariza-
tion. In Proceedings of the 2010 IEEE Second Interna-
tional Conference on Social Computing, pages 49?56.
Beaux Sharifi, Mark-Anthony Hutton, and Jugal K.
Kalita. 2010b. Summarizing microblogs automati-
cally. In Proceedings of the 2010 Annual Conference
of the North American Chapter of the Association for
Computational Linguistics (NAACL), pages 685?688.
Hiroya Takamura, Hikaru Yokono, and Manabu Oku-
mura. 2011. Summarizing a document stream. In
Proceedings of the 33rd European Conference on Ad-
vances in Information Retrieval (ECIR), pages 177?
188.
Jui-Yu Weng, Cheng-Lun Yang, Bo-Nian Chen, Yen-Kai
Wang, and Shou-De Lin. 2011. Imass: An intelli-
gent microblog analysis and summarization system. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies (ACL-HLT), pages 133?138.
Siqi Zhao, Lin Zhong, Jehan Wickramasuriya, and Venu
Vasudevan. 2011. Human as real-time sensors of so-
cial and physical events: A case study of twitter and
sports games. Technical Report TR0620-2011, Rice
University and Motorola Labs.
Arkaitz Zubiaga, Damiano Spina, Enrique Amigo?, and
Julio Gonzalo. 2012. Towards real-time summariza-
tion of scheduled events from twitter streams. In Pro-
ceedings of the 23rd ACM Conference on Hypertext
and Social Media, pages 319?320.
1162
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 71?76,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Insertion, Deletion, or Substitution? Normalizing Text Messages without
Pre-categorization nor Supervision
Fei Liu1 Fuliang Weng2 Bingqing Wang3 Yang Liu1
1Computer Science Department, The University of Texas at Dallas
2Research and Technology Center, Robert Bosch LLC
3School of Computer Science, Fudan University
{feiliu, yangl}@hlt.utdallas.edu1
fuliang.weng@us.bosch.com2, wbq@fudan.edu.cn3
Abstract
Most text message normalization approaches
are based on supervised learning and rely on
human labeled training data. In addition, the
nonstandard words are often categorized into
different types and specific models are de-
signed to tackle each type. In this paper,
we propose a unified letter transformation ap-
proach that requires neither pre-categorization
nor human supervision. Our approach mod-
els the generation process from the dictionary
words to nonstandard tokens under a sequence
labeling framework, where each letter in the
dictionary word can be retained, removed, or
substituted by other letters/digits. To avoid
the expensive and time consuming hand label-
ing process, we automatically collected a large
set of noisy training pairs using a novel web-
based approach and performed character-level
alignment for model training. Experiments on
both Twitter and SMS messages show that our
system significantly outperformed the state-
of-the-art deletion-based abbreviation system
and the jazzy spell checker (absolute accuracy
gain of 21.69% and 18.16% over jazzy spell
checker on the two test sets respectively).
1 Introduction
Recent years have witnessed the explosive growth
of text message usage, including the mobile phone
text messages (SMS), chat logs, emails, and sta-
tus updates from the social network websites such
as Twitter and Facebook. These text message col-
lections serve as valuable information sources, yet
the nonstandard contents within them often degrade
2gether (6326) togetha (919) tgthr (250) togeda (20)
2getha (1266) togather (207) t0gether (57) toqethaa (10)
2gthr (178) togehter (94) togeter (49) 2getter (10)
2qetha (46) togethor (29) tagether (18) 2gtr (6)
Table 1: Nonstandard tokens originated from ?together?
and their frequencies in the Edinburgh Twitter corpus.
the existing language processing systems, calling
the need of text normalization before applying the
traditional information extraction, retrieval, senti-
ment analysis (Celikyilmaz et al, 2010), or sum-
marization techniques. Text message normalization
is also of crucial importance for building text-to-
speech (TTS) systems, which need to determine pro-
nunciation for nonstandard words.
Text message normalization aims to replace the
non-standard tokens that carry significant mean-
ings with the context-appropriate standard English
words. This is a very challenging task due to the
vast amount and wide variety of existing nonstan-
dard tokens. We found more than 4 million dis-
tinct out-of-vocabulary tokens in the English tweets
of the Edinburgh Twitter corpus (see Section 2.2).
Table 1 shows examples of nonstandard tokens orig-
inated from the word ?together?. We can see that
some variants can be generated by dropping let-
ters from the original word (?tgthr?) or substitut-
ing letters with digit (?2gether?); however, many
variants are generated by combining the letter in-
sertion, deletion, and substitution operations (?to-
qethaa?, ?2gthr?). This shows that it is difficult to
divide the nonstandard tokens into exclusive cate-
gories.
Among the literature of text normalization
71
(for text messages or other domains), Sproat et
al. (2001), Cook and Stevenson (2009) employed the
noisy channel model to find the most probable word
sequence given the observed noisy message. Their
approaches first classified the nonstandard tokens
into various categories (e.g., abbreviation, stylistic
variation, prefix-clipping), then calculated the pos-
terior probability of the nonstandard tokens based
on each category. Choudhury et al (2007) de-
veloped a hidden Markov model using hand anno-
tated training data. Yang et al (2009), Pennell and
Liu (2010) focused on modeling word abbreviations
formed by dropping characters from the original
word. Toutanova and Moore (2002) addressed the
phonetic substitution problem by extending the ini-
tial letter-to-phone model. Aw et al (2006), Kobus
et al (2008) viewed the text message normalization
as a statistical machine translation process from the
texting language to standard English. Beaufort et
al. (2010) experimented with the weighted finite-
state machines for normalizing French SMS mes-
sages. Most of the above approaches rely heavily
on the hand annotated data and involve categorizing
the nonstandard tokens in the first place, which gives
rise to three problems: (1) the labeled data is very
expensive and time consuming to obtain; (2) it is
hard to establish a standard taxonomy for categoriz-
ing the tokens found in text messages; (3) the lack of
optimized way to integrate various category-specific
models often compromises the system performance,
as confirmed by (Cook and Stevenson, 2009).
In this paper, we propose a general letter trans-
formation approach that normalizes nonstandard to-
kens without categorizing them. A large set of noisy
training word pairs were automatically collected via
a novel web-based approach and aligned at the char-
acter level for model training. The system was tested
on both Twitter and SMS messages. Results show
that our system significantly outperformed the jazzy
spell checker and the state-of-the-art deletion-based
abbreviation system, and also demonstrated good
cross-domain portability.
2 Letter Transformation Approach
2.1 General Framework
Given a noisy text message T , our goal is to nor-
malize it into a standard English word sequence S.
b - - - - d a y f - o t o z
h u b b i e
(1) birthday --> bday
(2) photos --> fotoz
(4) hubby --> hubbie
b i r t h d a y
p h o t o s
h u b b y
s o m e 1 - -
(6) someone --> some1
s o m e o n e
n u t h i n -
(3) nothing --> nuthin
n o t h i n g
4 - - e v a -
(5) forever --> 4eva
f o r e v e r
Figure 1: Examples of nonstandard tokens generated by
performing letter transformation on the dictionary words.
Under the noisy channel model, this is equivalent to
finding the sequence S? that maximizes p(S|T ):
S? = argmaxS p(S|T ) = argmaxS(
?
i
p(Ti|Si))p(S)
where we assume that each non-standard token Ti
is dependent on only one English word Si, that is,
we are not considering acronyms (e.g., ?bbl? for
?be back later?) in this study. p(S) can be cal-
culated using a language model (LM). We formu-
late the process of generating a nonstandard token
Ti from dictionary word Si using a letter transfor-
mation model, and use the model confidence as the
probability p(Ti|Si). Figure 1 shows several exam-
ple (word, token) pairs1. To form a nonstandard to-
ken, each letter in the dictionary word can be labeled
with: (a) one of the 0-9 digits; (b) one of the 26 char-
acters including itself; (c) the null character ?-?; (d)
a letter combination. This transformation process
from dictionary words to nonstandard tokens will be
learned automatically through a sequence labeling
framework that integrates character-, phonetic-, and
syllable-level information.
In general, the letter transformation approach will
handle the nonstandard tokens listed in Table 2 yet
without explicitly categorizing them. Note for the
tokens with letter repetition, we first generate a set
of variants by varying the repetitive letters (e.g. Ci =
{?pleas?, ?pleeas?, ?pleaas?, ?pleeaas?, ?pleeeaas?}
for Ti = {?pleeeaas?}), then select the maximum
posterior probability among all the variants:
p(Ti|Si) = max
T?i?Ci
p(T?i|Si)
1The ideal transform for example (5) would be ?for? to ?4?.
But in this study we are treating each letter in the English word
separately and not considering the phrase-level transformation.
72
(1) abbreviation tgthr, weeknd, shudnt
(2) phonetic sub w/- or w/o digit 4got, sumbody, kulture
(3) graphemic sub w/- or w/o digit t0gether, h3r3, 5top, doinq
(4) typographic error thimg, macam
(5) stylistic variation betta, hubbie, cutie
(6) letter repetition pleeeaas, togtherrr
(7) any combination of (1) to (6) luvvvin, 2moro, m0rnin
Table 2: Nonstandard tokens that can be processed by the
unified letter transformation approach.
2.2 Web based Data Collection w/o Supervision
We propose to automatically collect training data
(annotate nonstandard words with the corresponding
English forms) using a web-based approach, there-
fore avoiding the expensive human annotation. We
use the Edinburgh Twitter corpus (Petrovic et al,
2010) for data collection, which contains 97 mil-
lion Twitter messages. The English tweets were
extracted using the TextCat language identification
toolkit (Cavnar and Trenkle, 1994), and tokenized
into a sequence of clean tokens consisting of letters,
digits, and apostrophe.
For the out-of-vocabulary (OOV) tokens consist-
ing of letters and apostrophe, we form n Google
queries for each of them in the form of either
?w1 w2 w3? OOV or OOV ?w1 w2 w3?, where w1
to w3 are consecutive context words extracted from
the tweets that contain this OOV. n is set to 6 in this
study. The first 32 returned snippets for each query
are parsed and the words in boldface that are differ-
ent from both the OOV and the context words are
collected as candidate normalized words. Among
them, we further select the words that have longer
common character sequence with the OOV than with
the context words, and pair each of them with the
OOV to form the training pairs. For the OOV tokens
consisting of both letters and digits, we use simple
rules to recover possible original words. These rules
include: 1 ? ?one?, ?won?, ?i?; 2 ? ?to?, ?two?,
?too?; 3 ? ?e?; 4 ? ?for?, ?fore?, ?four?; 5 ? ?s?;
6 ? ?b?; 8 ? ?ate?, ?ait?, ?eat?, ?eate?, ?ight?,
?aight?. The OOV tokens and any resulting words
from the above process are included in the noisy
training pairs. In addition, we add 932 word pairs
of chat slangs and their normalized word forms col-
lected from InternetSlang.com that are not covered
by the above training set.
These noisy training pairs were further expanded
and purged. We apply the transitive rule on these
initially collected training pairs. For example, if the
two pairs ?(cause, cauz)? and ?(cauz, coz)? are in the
data set, we will add ?(cause, coz)? as another train-
ing pair. We remove the data pairs whose word can-
didate is not in the CMU dictionary. We also remove
the pairs whose word candidate and OOV are simply
inflections of each other, e.g., ?(headed, heading)?,
using a set of rules. In total, this procedure generated
62,907 training word pairs including 20,880 unique
candidate words and 46,356 unique OOVs.2
2.3 Automatic Letter-level Alignment
Given a training pair (Si, Ti) consisting of a word Si
and its nonstandard variant Ti, we propose a proce-
dure to align each letter in Si with zero, one, or more
letters/digits in Ti. First we align the letters of the
longest common sequence between the dictionary
word and the variant (which gives letter-to-letter cor-
respondence in those common subsequences). Then
for the letter chunks in between each of the obtained
alignments, we process them based on the following
three cases:
(a) (many-to-0): a chunk in the dictionary word
needs to be aligned to zero letters in the variant.
In this case, we map each letter in the chunk to
?-? (e.g., ?birthday? to ?bday?), obtaining letter-
level alignments.
(b) (0-to-many): zero letters in the dictionary word
need to be aligned to a letter/digit chunk in the
variant. In this case, if the first letter in the
chunk can be combined with the previous letter
to form a digraph (such as ?wh? when aligning
?sandwich? to ?sandwhich?), we combine these
two letters. The remaining letters, or the entire
chunk when the first letter does not form a di-
graph with the previous letter, are put together
with the following aligned letter in the variant.
(c) (many-to-many): non-zero letters in the dictio-
nary word need to be aligned to a chunk in the
variant. Similar to (b), the first letter in the vari-
ant chunk is merged with the previous alignment
if they form a digraph. Then we map the chunk
in the dictionary word to the chunk in the vari-
ant as one alignment, e.g., ?someone? aligned to
?some1?.
2Please contact the first author for the collected word pairs.
73
The (b) and (c) cases above generate chunk-level
(with more than one letter) alignments. To elimi-
nate possible noisy training pairs, such as (?you?,
?haveu?), we keep all data pairs containing digits,
but remove the data pairs with chunks involving
three letters or more in either the dictionary word or
the variant. For the chunk alignments in the remain-
ing pairs, we sequentially align the letters (e.g., ?ph?
aligned to ?f-?). Note that for those 1-to-2 align-
ments, we align the single letter in the dictionary
word to a two-letter combination in the variant. We
limit to the top 5 most frequent letter combinations,
which are ?ck?, ?ey?, ?ie?, ?ou?, ?wh?, and the pairs
involving other combinations are removed.
After applying the letter alignment to the col-
lected noisy training word pairs, we obtained
298,160 letter-level alignments. Some example
alignments and corresponding word pairs are:
e ? ? ? (have, hav) q ? k (iraq, irak)
e ? a (another, anotha) q ? g (iraq, irag)
e? 3 (online, 0nlin3) w?wh (watch, whatch)
2.4 Sequence Labeling Model for P (Ti|Si)
For a letter sequence Si, we use the conditional ran-
dom fields (CRF) model to perform sequence tag-
ging to generate its variant Ti. To train the model,
we first align the collected dictionary word and its
variant at the letter level, then construct a feature
vector for each letter in the dictionary word, using
its mapped character as the reference label. This la-
beled data set is used to train a CRF model with L-
BFGS (Lafferty et al, 2001; Kudo, 2005). We use
the following features:
? Character-level features
Character n-grams: c?1, c0, c1, (c?2 c?1),
(c?1 c0), (c0 c1), (c1 c2), (c?3 c?2 c?1),
(c?2 c?1 c0), (c?1 c0 c1), (c0 c1 c2), (c1 c2 c3).
The relative position of character in the word.
? Phonetic-level features
Phoneme n-grams: p?1, p0, p1, (p?1 p0),
(p0 p1). We use the many-to-many letter-
phoneme alignment algorithm (Jiampojamarn
et al, 2007) to map each letter to multiple
phonemes (1-to-2 alignment). We use three bi-
nary features to indicate whether the current,
previous, or next character is a vowel.
? Syllable-level features
Relative position of the current syllable in the
word; two binary features indicating whether
the character is at the beginning or the end of
the current syllable. The English hyphenation
dictionary (Hindson, 2006) is used to mark all
the syllable information.
The trained CRF model can be applied to any En-
glish word to generate its variants with probabilities.
3 Experiments
We evaluate the system performance on both Twitter
and SMS message test sets. The SMS data was used
in previous work (Choudhury et al, 2007; Cook and
Stevenson, 2009). It consists of 303 distinct non-
standard tokens and their corresponding dictionary
words. We developed our own Twitter message test
set consisting of 6,150 tweets manually annotated
via the Amazon Mechanical Turk. 3 to 6 turkers
were required to convert the nonstandard tokens in
the tweets to the standard English words. We extract
the nonstandard tokens whose most frequently nor-
malized word consists of letters/digits/apostrophe,
and is different from the token itself. This results
in 3,802 distinct nonstandard tokens that we use as
the test set. 147 (3.87%) of them have more than
one corresponding standard English words. Similar
to prior work, we use isolated nonstandard tokens
without any context, that is, the LM probabilities
P (S) are based on unigrams.
We compare our system against three approaches.
The first one is a comprehensive list of chat slangs,
abbreviations, and acronyms collected by Internet-
Slang.com; it contains normalized word forms for
6,105 commonly used slangs. The second is the
word-abbreviation lookup table generated by the su-
pervised deletion-based abbreviation approach pro-
posed in (Pennell and Liu, 2010). It contains
477,941 (word, abbreviation) pairs automatically
generated for 54,594 CMU dictionary words. The
third is the jazzy spell checker based on the Aspell
algorithm (Idzelis, 2005). It integrates the phonetic
matching algorithm (DoubleMetaphone) and Leven-
shtein distance that enables the interchanging of two
adjacent letters, and changing/deleting/adding of let-
ters. The system performance is measured using the
n-best accuracy (n=1,3). For each nonstandard to-
ken, the system is considered correct if any of the
corresponding standard words is among the n-best
output from the system.
74
System Accuracy
Twitter (3802 pairs) SMS (303 pairs)
1-best 3-best 1-best 3-best
InternetSlang 7.94 8.07 4.95 4.95
(Pennell et al 2010) 20.02 27.09 21.12 28.05
Jazzy Spell Checker 47.19 56.92 43.89 55.45
LetterTran (Trim) 57.44 64.89 58.09 70.63
LetterTran (All) 59.15 67.02 58.09 70.96
LetterTran (All) + Jazzy 68.88 78.27 62.05 75.91
(Choudhury et al 2007) n/a n/a 59.9 n/a
(Cook et al 2009) n/a n/a 59.4 n/a
Table 3: N-best performance on Twitter and SMS data
sets using different systems.
Results of system accuracies are shown in Ta-
ble 3. For the system ?LetterTran (All)?, we first
generate a lookup table by applying the trained CRF
model to the CMU dictionary to generate up to
30 variants for each dictionary word.3 To make
the comparison more meaningful, we also trim our
lookup table to the same size as the deletion ta-
ble, namely ?LetterTran (Trim)?. The trimming was
performed by selecting the most frequent dictionary
words and their generated variants until the length
limit is reached. Word frequency information was
obtained from the entire Edinburgh corpus. For both
the deletion and letter transformation lookup tables,
we generate a ranked list of candidate words for each
nonstandard token, by sorting the combined score
p(Ti|Si)?C(Si), where p(Ti|Si) is the model con-
fidence and C(Si) is the unigram count generated
from the Edinburgh corpus (we used counts instead
of unigram probability P (Si)). Since the string sim-
ilarity and letter switching algorithms implemented
in jazzy can compensate the letter transformation
model, we also investigate combining it with our ap-
proach, ?LetterTran(All) + Jazzy?. In this configura-
tion, we combine the candidate words from both sys-
tems and rerank them according to the unigram fre-
quency; since the ?LetterTran? itself is very effective
in ranking candidate words, we only use the jazzy
output for tokens where ?LetterTran? is not very
confident about its best candidate ((p(Ti|Si)?C(Si)
is less than a threshold ? = 100).
We notice the accuracy using the InternetSlang
list is very poor, indicating text message normal-
ization is a very challenging task that can hardly
3We heuristically choose this large number since the learned
letter/digit insertion, substitution, and deletion patterns tend to
generate many variants for each dictionary word.
be tackled by using a hand-crafted list. The dele-
tion table has modest performance given the fact
that it covers only deletion-based abbreviations and
letter repetitions (see Section 2.1). The ?Letter-
Tran? approach significantly outperforms all base-
lines even after trimming. This is because it han-
dles different ways of forming nonstandard tokens
in an unified framework. Taking the Twitter test
set for an example, the lookup table generated by
?LetterTran? covered 69.94% of the total test to-
kens, and among them, 96% were correctly normal-
ized in the 3-best output, resulting in 67.02% over-
all accuracy. The test tokens that were not covered
by the ?LetterTrans? model include those generated
by accidentally switching and inserting letters (e.g.,
?absolotuely? for ?absolutely?) and slangs (?addy?
or ?address?). Adding the output from jazzy com-
pensates these problems and boosts the 1-best ac-
curacy, achieving 21.69% and 18.16% absolute per-
formance gain respectively on the Twitter and SMS
test sets, as compared to using jazzy only. We also
observe that the ?LetterTran? model can be easily
ported to the SMS domain. When combined with
the jazzy module, it achieved 62.05% 1-best accu-
racy, outperforming the domain-specific supervised
system in (Choudhury et al, 2007) (59.9%) and
the pre-categorized approach by (Cook and Steven-
son, 2009) (59.4%). Regarding different feature cat-
egories, we found the character-level features are
strong indicators, and using phonetic- and syllabic-
level features also slightly benefits the performance.
4 Conclusion
In this paper, we proposed a generic letter trans-
formation approach for text message normaliza-
tion without pre-categorizing the nonstandard to-
kens into insertion, deletion, substitution, etc. We
also avoided the expensive and time consuming hand
labeling process by automatically collecting a large
set of noisy training pairs. Results in the Twitter
and SMS domains show that our system can signif-
icantly outperform the state-of-the-art systems and
have good domain portability. In the future, we
would like to compare our method with a statistical
machine translation approach performed at the let-
ter level, evaluate the system using sentences by in-
corporating context word information, and consider
many-to-one letter transformation in the model.
75
5 Acknowledgments
The authors thank Deana Pennell for sharing the
look-up table generated using the deletion-based ab-
breviation approach. Thank Sittichai Jiampojamarn
for providing the many-to-many letter-phoneme
alignment data sets and toolkit. Part of this work
was done while Fei Liu was working as a research
intern in Bosch Research and Technology Center.
References
AiTi Aw, Min Zhang, Juan Xiao, and Jian Su. 2006. A
phrase-based statistical model for sms text normaliza-
tion. In Proceedings of the COLING/ACL, pages 33?
40.
Richard Beaufort, Sophie Roekhaut, Louise-Ame?lie
Cougnon, and Ce?drick Fairon. 2010. A hybrid
rule/model-based finite-state framework for normaliz-
ing sms messages. In Proceedings of the ACL, pages
770?779.
William B. Cavnar and John M. Trenkle. 1994. N-gram-
based text categorization. In Proceedings of Third An-
nual Symposium on Document Analysis and Informa-
tion Retrieval, pages 161?175.
Asli Celikyilmaz, Dilek Hakkani-Tur, and Junlan Feng.
2010. Probabilistic model-based sentiment analysis of
twitter messages. In Proceedings of the IEEE Work-
shop on Spoken Language Technology, pages 79?84.
Monojit Choudhury, Rahul Saraf, Vijit Jain, Animesh
Mukherjee, Sudeshna Sarkar, and Anupam Basu.
2007. Investigation and modeling of the structure of
texting language. International Journal on Document
Analysis and Recognition, 10(3):157?174.
Paul Cook and Suzanne Stevenson. 2009. An unsuper-
vised model for text messages normalization. In Pro-
ceedings of the NAACL HLT Workshop on Computa-
tional Approaches to Linguistic Creativity, pages 71?
78.
Matthew Hindson. 2006. En-
glish language hyphenation dictionary.
http://www.hindson.com.au/wordpress/2006/11/11/english-
language-hyphenation-dictionary/.
Mindaugas Idzelis. 2005. Jazzy: The java open source
spell checker. http://jazzy.sourceforge.net/.
Sittichai Jiampojamarn, Grzegorz Kondrak, and Tarek
Sherif. 2007. Applying many-to-many alignments
and hidden markov models to letter-to-phoneme con-
version. In Proceedings of the HLT/NAACL, pages
372?379.
Catherine Kobus, Franc?ois Yvon, and Ge?raldine
Damnati. 2008. Normalizing sms: Are two metaphors
better than one? In Proceedings of the COLING, pages
441?448.
Taku Kudo. 2005. CRF++: Yet another CRF took kit.
http://crfpp.sourceforge.net/.
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional random fields: Probabilistic mod-
els for segmenting and labeling sequence data. In Pro-
ceedings of the ICML, pages 282?289.
Deana L. Pennell and Yang Liu. 2010. Normalization
of text messages for text-to-speech. In Proceedings of
the ICASSP, pages 4842?4845.
Sasa Petrovic, Miles Osborne, and Victor Lavrenko.
2010. The edinburgh twitter corpus. In Proceedings
of the NAACL HLT Workshop on Computational Lin-
guistics in a World of Social Media, pages 25?26.
Richard Sproat, Alan W. Black, Stanley Chen, Shankar
Kumar, Mari Ostendorf, and Christopher Richards.
2001. Normalization of non-standard words. Com-
puter Speech and Language, 15(3):287?333.
Kristina Toutanova and Robert C. Moore. 2002. Pronun-
ciation modeling for improved spelling correction. In
Proceedings of the ACL, pages 144?151.
Dong Yang, Yi cheng Pan, and Sadaoki Furui. 2009.
Automatic chinese abbreviation generation using con-
ditional random field. In Proceedings of the NAACL
HLT, pages 273?276.
76
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 1035?1044,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
A Broad-Coverage Normalization System for Social Media Language
Fei Liu Fuliang Weng Xiao Jiang
Research and Technology Center
Robert Bosch LLC
{fei.liu, fuliang.weng}@us.bosch.com
{fixed-term.xiao.jiang}@us.bosch.com
Abstract
Social media language contains huge amount
and wide variety of nonstandard tokens, cre-
ated both intentionally and unintentionally by
the users. It is of crucial importance to nor-
malize the noisy nonstandard tokens before
applying other NLP techniques. A major
challenge facing this task is the system cov-
erage, i.e., for any user-created nonstandard
term, the system should be able to restore the
correct word within its top n output candi-
dates. In this paper, we propose a cognitively-
driven normalization system that integrates
different human perspectives in normalizing
the nonstandard tokens, including the en-
hanced letter transformation, visual priming,
and string/phonetic similarity. The system
was evaluated on both word- and message-
level using four SMS and Twitter data sets.
Results show that our system achieves over
90% word-coverage across all data sets (a
10% absolute increase compared to state-of-
the-art); the broad word-coverage can also
successfully translate into message-level per-
formance gain, yielding 6% absolute increase
compared to the best prior approach.
1 Introduction
The amount of user generated content has increased
drastically in the past few years, driven by the pros-
perous development of the social media websites
such as Twitter, Facebook, and Google+. As of June
2011, Twitter has attracted over 300 million users
and produces more than 2 billion tweets per week
(Twitter, 2011). In a broader sense, Twitter mes-
sages, SMS messages, Facebook updates, chat logs,
Emails, etc. can all be considered as ?social text?,
which is significantly different from the traditional
news text due to the informal writing style and the
conversational nature. The social text serves as a
very valuable information source for many NLP ap-
plications, such as the information extraction (Ritter
et al, 2011), retrieval (Subramaniam et al, 2009),
summarization (Liu et al, 2011a), sentiment analy-
sis (Celikyilmaz et al, 2010), etc. Yet existing sys-
tems often perform poorly in this domain due the
to extensive use of the nonstandard tokens, emoti-
cons, incomplete and ungrammatical sentences, etc.
It is reported that the Stanford named entity recog-
nizer (NER) experienced a performance drop from
90.8% to 45.8% on tweets (Liu et al, 2011c); the
part-of-speech (POS) tagger and dependency parser
degraded 12.2% and 20.65% respectively on tweets
(Foster et al, 2011). It is therefore of great impor-
tance to normalize the social text before applying the
standard NLP techniques. Text normalization is also
crucial for building robust text-to-speech (TTS) sys-
tems, which need to determine the pronunciations
for nonstandard words in the social text.
The goal of this work is to automatically con-
vert the noisy nonstandard tokens observed in the
social text into standard English words. We aim
for a robust text normalization system with ?broad
coverage?, i.e., for any user-created nonstandard to-
ken, the system should be able to restore the correct
word within its top n candidates (n = 1, 3, 10...).
This is a very challenging task due to two facts:
first, there exists huge amount and a wide variety
of nonstandard tokens. (Liu et al, 2011b) found
more than 4 million distinct out-of-vocabulary to-
kens in the Edinburgh Twitter corpus (Petrovic et
al., 2010); second, the nonstandard tokens consist
1035
2gether (6326) togetha (919) tgthr (250) togeda (20)
2getha (1266) togather (207) t0gether (57) toqethaa (10)
2gthr (178) togehter (94) togeter (49) 2getter (10)
u (3240535) ya (460963) yo (252274) yaa (17015)
yaaa (7740) yew (7591) yuo (467) youz (426)
yoooooou (186) youy (105) yoiu (128) yoooouuuu (82)
Table 1: Nonstandard tokens and their frequencies in the
Edinburgh Twitter corpus. The corresponding standard
words are ?together? and ?you?, respectively.
of a mixture of both unintentional misspellings and
intentionally-created tokens for various reasons1, in-
cluding the needs for speed, ease of typing (Crystal,
2009), sentiment expressing (e.g., ?coooool? (Brody
and Diakopoulos, 2011)), intimacy and social pur-
pose (Thurlow, 2003), etc., making it even harder to
decipher the social messages. Table 1 shows some
example nonstandard tokens.
Existing spell checkers and normalization sys-
tems rely heavily on lexical/phonetic similarity to
select the correct candidate words. This may not
work well since a good portion of the correct words
lie outside the specified similarity threshold (e.g.,
(tomorrow, ?tmrw?)2), yet the number of candidates
increases dramatically as the system strives to in-
crease the coverage by enlarging the threshold. (Han
and Baldwin, 2011) reported an average of 127 can-
didates per nonstandard token with the correct-word
coverage of 84%. The low coverage score also en-
forces an undesirable performance ceiling for the
candidate reranking approaches. Different from pre-
vious work, we tackle the text normalization prob-
lem from a cognitive-sensitive perspective and in-
vestigate the human rationales for normalizing the
nonstandard tokens. We argue that there exists a set
of letter transformation patterns that humans use to
decipher the nonstandard tokens. Moreover, the ?vi-
sual priming? effect may play an important role in
human comprehension of the noisy tokens. ?Prim-
ing? represents an implicit memory effect. For ex-
ample, if a person reads a list of words including the
word table, and is later asked to complete a word
starting with tab-, it is very likely that he answers
table since the person is primed.
In this paper, we propose a broad-coverage nor-
malization system by integrating three human per-
1For this reason, we will use the term ?nonstandard tokens?
instead of ?ill-formed tokens? throughout the paper.
2We use the form (standard word, ?nonstandard token?) to
denote an example nonstandard token and its corresponding
standard word.
spectives, including the enhanced letter transforma-
tion, visual priming, and the string and phonetic
similarity. For an arbitrary nonstandard token, the
three subnormalizers each suggest their most con-
fident candidates from a different perspective. The
candidates can then be heuristically combined or
reranked using a message-level decoding process.
We evaluate the system on both word- and message-
level using four SMS and Twitter data sets. Results
show that our system can achieve over 90% word-
coverage with limited number of candidates and the
broad word-coverage can be successfully translated
into message-level performance gain. In addition,
our system requires no human annotations, therefore
can be easily adapted to different domains.
2 Related work
Text normalization, in its traditional sense, is the
first step of a speech synthesis system, where the
numbers, dates, acronyms, etc. found in the real-
world text were converted into standard dictionary
words, so that the system can pronounce them cor-
rectly. Spell checking plays an important role in this
process. (Church and Gale, 1991; Mays et al, 1991;
Brill and Moore, 2000) proposed to use the noisy
channel framework to generate a list of corrections
for any misspelled word, ranked by the correspond-
ing posterior probabilities. (Sproat et al, 2001) en-
hanced this framework by calculating the likelihood
probability as the chance of a noisy token and its as-
sociated tag being generated by a specific word.
With the rapid growth of SMS and social me-
dia content, text normalization system has drawn in-
creasing attention in the recent decade, where the
focus is on converting the noisy nonstandard tokens
in the informal text into standard dictionary words.
(Choudhury et al, 2007) modeled each standard En-
glish word as a hidden Markov model (HMM) and
calculated the probability of observing the noisy-
token under each of the HMM models; (Cook and
Stevenson, 2009) calculated the sum of the probabil-
ities of a noisy token being generated by a specific
word and a word formation process; (Beaufort et al,
2010) employed the weighted finite-state machines
(FSMs) and rewriting rules for normalizing French
SMS; (Pennell and Liu, 2010) focused on tweets cre-
ated by handsets and developed a CRF tagger for
deletion-based abbreviation. The text normalization
problem was also tackled under the machine transla-
1036
tion (MT) or speech recognition (ASR) framework.
(Aw et al, 2006) adapted a phrase-based MT model
for normalizing SMS and achieved satisfying per-
formance. (Kobus et al, 2008) showed that using a
statistical MT system in combination with an anal-
ogy of the ASR system improved performance in
French SMS normalization. (Pennell and Liu, 2011)
proposed a two-phase character-level MT system for
expanding the abbreviations into standard text.
Recent work also focuses on normalizing the
Twitter messages, which is generally considered a
more challenging task. (Han and Baldwin, 2011) de-
veloped classifiers for detecting the ill-formed words
and generated corrections based on the morpho-
phonemic similarity. (Liu et al, 2011b) proposed
to normalize the nonstandard tokens without explic-
itly categorizing them. (Xue et al, 2011) adopted
the noisy-channel framework and incorporated or-
thographic, phonetic, contextual, and acronym ex-
pansion factors in calculating the likelihood proba-
bilities. (Gouws et al, 2011) revealed that different
populations exhibit different shortening styles.
Most of the above systems limit their processing
scope to certain categories (e.g., deletion-based ab-
breviations, misspellings) or require large-scale hu-
man annotated corpus for training, which greatly
hinders the scalability of the system. In this paper,
we propose a novel cognitively-driven text normal-
ization system that robustly tackle both the unin-
tentional misspellings and the intentionally-created
noisy tokens. We propose a global context-based
approach to purify the automatically collected train-
ing data and learn the letter transformation pat-
terns without human supervision. We also propose
a cognitively-grounded ?visual priming? approach
that leverages the ?priming? effect to suggest the
candidate words. By integrating different perspec-
tives, our system can successfully mimic the hu-
man rationales and yield broad word-coverage on
both SMS and Twitter messages. To the best of our
knowledge, we are the first to integrate these human
perspectives in the text normalization system.
3 Broad-Coverage Normalization System
In this section, we describe our broad-coverage nor-
malization system, which consists of four key com-
ponents. For a standard/nonstandard token, three
subnormalizers each suggest their most confident
b - - - - d a y f - o t o z
h u b b i e
(1) birthday --> bday
(2) photos --> fotoz
(4) hubby --> hubbie
b i r t h d a y
p h o t o s
h u b b y
s o m e 1 - -
(6) someone --> some1
s o m e o n e
n u t h i n -
(3) nothing --> nuthin
n o t h i n g
4 - - e v a -
(5) forever --> 4eva
f o r e v e r
Figure 1: Examples of nonstandard tokens generated by
performing letter transformation on the dictionary words.
candidates from a different perspective3: ?Enhanced
Letter Transformation? automatically learns a set
of letter transformation patterns and is most effec-
tive in normalizing the intentionally created non-
standard tokens through letter insertion, repetition,
deletion, and substitution (Section 3.1); ?Visual
Priming? proposes candidates based on the visual
cues and a primed perspective (Section 3.2); ?Spell
Checker? corrects the misspellings (Section 3.3).
The fourth component, ?Candidate Combination?
introduces various strategies to combine the candi-
dates with or without the local context (Section 3.4).
Note that it is crucial to integrate different human
perspectives so that the system is flexible in pro-
cessing both unintentional misspellings and various
intentionally-created noisy tokens.
3.1 Enhanced Letter Transformation
Given a noisy token ti seen in the text, the letter
transformation subnormalizer produces a list of cor-
rection candidates si under the noisy channel model:
s? = argmaxsip(si|ti) = argmaxsip(ti|si)p(si)
where we assume each nonstandard token ti is de-
pendent on only one English word si, that is, we
are not considering acronyms (e.g., ?bbl? for ?be
back later?) in this study. p(si) can be calculated
as the unigram count from a background corpus. We
formulate the process of generating a nonstandard
token ti from the dictionary word si using a letter
transformation model, and use the model confidence
as the probability p(ti|si). Figure 1 shows several
example (word, token) pairs.
To form a nonstandard token, each letter in the
dictionary word can be labeled with: (a) one of the
0-9 digits; (b) one of the 26 characters including it-
self; (c) the null character ?-?; (d) a letter combi-
nation4. This transformation process from dictio-
3For the dictionary word, we allow the subnormalizers to
either return the word itself or candidates that are the possibly
intended words in the given context (e.g., (with, ?wit?)).
4The set of letter combinations used in this work are {ah, ai,
aw, ay, ck, ea, ey, ie, ou, te, wh}
1037
nary words to nonstandard tokens will be learned
by a character-level sequence labeling system us-
ing the automatically collected (word, token) pairs.
Next, we create a large lookup table by applying the
character-level labeling system to the standard dic-
tionary words and generate multiple variations for
each word using the n-best labeling output, the la-
beling confidence is used as p(ti|si). During testing,
we search this lookup table to find the best candidate
words for the nonstandard tokens. For tokens with
letter repetition, we first generate a set of variants
by varying the repetitive letters (e.g. Ci = {?pleas?,
?pleeas?, ?pleaas?, ?pleeaas?, ?pleeeaas?} for ti =
{?pleeeaas?}), then select the maximum posterior
probability among all the variants:
p(ti|si) = max
t?i?Ci
p(t?i|si)
Different from the work in (Liu et al, 2011b), we
enhanced the letter transformation process with two
novel aspects: first, we devise a set of phoneme-,
syllable-, morpheme- and word-boundary based fea-
tures that effectively characterize the formation pro-
cess of the nonstandard tokens; second, we propose
a global context-aware approach to purify the auto-
matically collected training (word, token) pairs, re-
sulting system yielded similar performance but with
only one ninth of the original data. We name this
subnormalizer ?Enhanced Letter Transformation?.
3.1.1 Context-Aware Training Pair Selection
Manual annotation of the noisy nonstandard to-
kens takes a lot of time and effort. (Liu et al, 2011b)
proposed to use Google search engine to automati-
cally collect large amount of training pairs. Yet the
resulting (work, token) pairs are often noisy, con-
taining pairs such as (events, ?ents?), (downtown,
?downto?), etc. The ideal training data should con-
sist of the most frequent nonstandard tokens paired
with the corresponding corrections, so that the sys-
tem can learn from the most representative letter
transformation patterns.
Motivated by research on word sense disambigua-
tion (WSD) (Mihalcea, 2007), we hypothesize the
nonstandard token and the standard word share a lot
of common terms in their global context. For exam-
ple, ?luv? and ?love? share ?i?, ?you?, ?u?, ?it?, etc.
among their top context words. Based on this find-
ing, we propose to filter out the low-quality train-
ing pairs by evaluating the global contextual simi-
larity between the word and token. To the best of
our knowledge, we are the first to explore this global
contextual similarity for the text normalization task.
Given a noisy (word, token) pair, we construct
two context vectors vi and vj by collecting the
most frequent terms appearing before or after the
work/token. We consider two terms on each side
of the word/token as context and restrict the vector
length to the top 100 terms. The frequency informa-
tion were calculated using a large background cor-
pus; stopwords were not excluded from the context
vector. The contextual similarity of the (word, to-
ken) pair is defined as the cosine similarity between
the context vectors vi and vj :
ContextSim(vi, vj) =
Pn
k=1 wi,k ? wj,kqPn
k=1 w
2
i,k ?
qPn
k=1 w
2
j,k
where wi,k is the weight of term tk within the con-
text of term ti. The term weights are defined using a
normalized TF-IDF method:
wi,k =
TFi,k
TFi
? log(
N
DFk
)
where TFi,k is the count of term tk appearing within
the context of term ti; TFi is the total count of ti in
the corpus. TFi,kTFi is therefore the relative frequency
of tk appearing in the context of ti; log( NDFk ) de-
notes the inverse document frequency of tk, calcu-
lated as the logarithm of total tweets (N ) divided by
the number of tweets containing tk.
To select the most representative (word, token)
pairs for training, we rank the automatically col-
lected 46,288 pairs by the token frequency, filter
out pairs whose contextual similarity lower than a
threshold ? (set empirically at 0.0003), and retain
only the top portion (5,000 pairs) for experiments.
3.1.2 Character-level Sequence Labeling
For a dictionary word si, we use the conditional
random fields (CRF) model to perform character-
level labeling to generate its variant ti. In the train-
ing stage, we align the collected (word, token) pairs
at the character level (Liu et al, 2011b), then con-
struct a feature vector for each letter of the dictio-
nary word, using its mapped character as the ref-
erence label. This aligned data set is used to train
a CRF model (Lafferty et al, 2001; Kudo, 2005)
1038
Character a d v e r t i s e m e n t s
Phoneme AE D V ER ER T AY Z M AH N T S
Phoneme boundary O O O B1 L1 O O O O O O O O O
Syllable boundary B L B I L B I I L B I I I L
Morpheme boundary B I I I I I I I L B I I L U
Word boundary B I I I I I I I I I I I I L
Table 2: Example boundary tags for word ?advertise-
ments? on the phoneme-, syllable-, morpheme-, and
word-level, labeled with the ?BILOU? encoding scheme.
with L-BFGS optimization. We use the charac-
ter/phoneme n-gram and binary vowel features as in
(Liu et al, 2011b), but develop a set of boundary
features to effectively characterize the letter trans-
formation process.
We notice that in creating the nonstandard tokens,
humans tend to drop certain letter units from the
word or replace them with other letters. For exam-
ple, in abbreviating ?advertisements? to ?ads?, hu-
mans may first break the word into smaller units
?ad-ver-tise-ment-s?, then drop the middle parts.
This also conforms with the word construction the-
ory where a word is composed of smaller units and
construction rules. Based on this assumption, we
decompose the dictionary words on the phoneme-,
syllable-, morpheme-, and word-level5 and use the
?BILOU? tagging scheme (Ratinov and Roth, 2009)
to represent the unit boundary, where ?BILOU?
stands for B(egin), I(nside), L(ast), O(utside), and
U(nit-length) of the corresponding unit6. Example
?BILOU? boundary tags were shown in Table 2.
On top of the boundary tags, we develop a set of
conjunction features to accurately pinpoint the cur-
rent character position. We consider conjunction
features formed by concatenating character position
in syllable and current syllable position in the word
(e.g., conjunction feature ?L B? for the letter ?d? in
Table 2). A similar set of features are also devel-
oped on morpheme level. We consider conjunction
of character/vowel feature and their boundary tags
on the syllable/morpheme/word level; conjunction
of phoneme and phoneme boundary tags, and ab-
solute position of current character within the corre-
5Phoneme decomposition is generated using the (Jiampo-
jamarn et al, 2007) algorithm to map up to two letters to
phonemes (2-to-2 alignment); syllable boundary acquired by
the hyphenation algorithm (Liang, 1983); morpheme boundary
determined by toolkit Morfessor 1.0 (Creutz and Lagus, 2005).
6For phoneme boundary, we use ?B1? and ?L1? to represent
two different characters aligned to one phoneme and ?B2?, ?L2?
represent same characters aligned to one phoneme.
sponding syllable/morpheme/word.
We use the aforementioned features to train the
CRF model, then apply the model on dictionary
words si to generate multiple variations ti for each
word. When a nonstandard token is seen during test-
ing, we apply the noisy channel to generate a list of
best candidate words: s? = argmaxsip(ti|si)p(si).
3.2 Visual Priming Approach
A second key component of the broad-coverage nor-
malization system is a novel ?Visual Priming? sub-
normalizer. It is built on a cognitively-driven ?prim-
ing? effect, which has not been explored by other
studies yet was shown to be effective across all our
data sets.
?Priming?7 is an implicit memory effect caused
by spreading neural networks (Tulving and Stark,
1982). As an example, in the word-stem comple-
tion task, participants are given a list of study words,
and then asked to complete word ?stems? consisting
of first 3 letters. A priming effect is observed when
participants complete stems with words on the study
list more often than with the novel words. The study
list activates parts of the human brain right before
the stem completion task, later when a word stem is
seen, less additional activation is needed for one to
choose a word from the study list.
We argue that the ?priming? effect may play an
important role in human comprehension of the noisy
tokens. A person familiarized with the ?social talk?
is highly primed with the most commonly used
words; later when a nonstandard token shows only
minor visual cues or visual stimulus, it can still be
quickly recognized by the person. In this process,
the first letter or first few letters of the word serve
as a very important visual stimulus. Based on this
assumption, we introduce the ?priming? subnormal-
izer based only on the word frequency and the minor
visual stimulus. Concretely, this approach proposes
candidate words based on the following equation:
V isualPrim(si|ti) =
len(LCS(ti, si))
len(ti)
? log(TF (si))
Where TF (si) is the term frequency of si as in the
background social text corpus; log(TF (si)) primes
the system with the most common words in the so-
cial text; LCS(?) means the longest common char-
acter subsequence; len(?) denotes the length of the
7http://en.wikipedia.org/wiki/Priming (psychology)
1039
character sequence. Together len(LCS(ti,si))len(ti) pro-
vides the minor visual stimulus from ti. Note that
the first character has been shown to be a crucial vi-
sual cue for the brain to understand jumbled words
(Davis, ), we therefore consider as candidates only
those words si that start with the same character as
ti. In the case that the nonstandard token ti starts
with a digit (e.g., ?2moro?), we use the mostly likely
corresponding letter to search the candidates (those
starting with letter ?t?). This setting also effectively
reduces the candidate search space.
The ?visual priming? subnormalizer promotes the
candidate words that are frequently used in the so-
cial talk and also bear visual similarity with the
given noisy token. It slightly deviates from the tradi-
tional ?priming? notion in that the frequency infor-
mation were acquired from the global corpus rather
than from the prior context. This approach also in-
herently follows the noisy channel framework, with
p(ti|si) represents the visual stimulus and p(si) be-
ing the logarithm of frequency. The candidate words
are ranked by s? = argmaxsiV isualPrim(si|ti).
We show that the ?priming? subnormalizer is robust
across data sets abide its simplistic representation.
3.3 Spell Checker
The third subnormalizer is the spell checker, which
combines the string and phonetic similarity algo-
rithms and is most effective in normalizing the mis-
spellings. We use the Jazzy spell checker (Idzelis,
2005) that integrates the DoubleMetaphone phonetic
matching algorithm and the Levenshtein distance us-
ing the near-miss strategy, which enables the in-
terchange of two adjacent letters, and the replac-
ing/deleting/adding of letters.
3.4 Candidate Combination
Each of the three subnormalizers is a stand-alone
system and can suggest corrections for the nonstan-
dard tokens. Yet we show that each subnormal-
izer mimics a different perspective that humans use
to decode the nonstandard tokens, as a result, our
broad-coverage normalization system is built by in-
tegrating candidates from the three subnormalizers
using various strategies.
For a noisy token seen in the informal text, the
most convenient way of system combination is to
harvest up to n candidates from each of the sub-
normalizers, and use the pool of candidates (up to
3n) as the system output. This sets an upper bound
for other candidate combination strategies, and we
name this approach ?Oracle?.
A second combination strategy is to give higher
priority to candidates from high-precision subsys-
tems. Both ?Letter Transformation? and ?Spell
Checker? have been shown to have high precision in
suggesting corrections (Liu et al, 2011b), while ?Vi-
sual Priming? may not yield high precision due to
its definition. We therefore take the top-3 candidates
from each of the ?Letter Tran.? and ?Spell Checker?
subsystems, but put candidates from ?Letter Tran.?
ahead of ?Spell Checker? if the confidence of the
best candidate is greater than a threshold ? and vice
versa. The list of candidates is then compensated us-
ing the ?Visual Priming? output until the total num-
ber reaches n. We name this approach ?Word-level?
combination since no message-level context infor-
mation is involved.
Based on the ?Word-level? combination output,
we can further rerank all the candidates using a
message-level Viterbi decoding process (Pennell and
Liu, 2011) where the local context information is
used to select the best candidate. This approach is
named ?Message-level? combination.
4 Experiments
4.1 Experimental Setup
We use four SMS and Twitter data sets to evaluate
the system effectiveness. Statistics of these data sets
are summarized in Table 3. Data set (1) to (3) are
used for word-level evaluation; data set (4) for both
word- and message-level evaluation. In Table 3, we
also present the number of distinct nonstandard to-
kens found in each data set, and notice that only a
small portion of the nonstandard tokens correspond
to multiple standard words. We calculate the dic-
tionary coverage of the manually annotated words
since this sets an upper bound for any normaliza-
tion system. We use the Edinburgh Twitter corpus
(Petrovic et al, 2010) as the background corpus for
frequency calculation, and a dictionary containing
82,324 words.8 The nonstandard tokens may consist
of both numbers/characters and apostrophe.
8The dictionary is created by combining the CMU (CMU,
2007) and Aspell (Atkinson, 2006) dictionaries and dropping
words with frequency < 20 in the background corpus. ?rt? and
all single characters except ?a? and ?i? are excluded.
1040
Index Domain Time Period #Msgs
#Uniq Nonstan. %Nonstan. Tkns %Dict cov.
Reference
Tokens w/ Multi-cands of cands
(1) SMS Around 2007 n/a 303 1.32% 100% (Choudhury et al, 2007)
(2) Twitter Nov 2009 ? Feb 2010 6150 3802 3.87% 99.34% (Liu et al, 2011)
(3) SMS/Twitter Aug 2009 4660 2040 2.41% 96.84% (Pennell and Liu, 2011)
(4) Twitter Aug 2010 ? Oct 2010 549 558 2.87% 99.10% (Han and Baldwin, 2011)
Table 3: Statistics of different SMS and Twitter data sets.
The goal of word-level normalization is to convert
the list of distinct nonstandard tokens into standard
words. For each nonstandard token, the system is
considered correct if any of the corresponding stan-
dard words is among the n-best output from the sys-
tem. We adopt this word-level n-best accuracy to
make our results comparable to other state-of-the-art
systems. On message-level, we evaluate the 1-best
system output using precision, recall, and f-score,
calculated respective to the nonstandard tokens.
4.2 Word-level Results
The word-level results are presented in Table 4, 5,
and 6, evaluated on data set (1), (2), (3) respectively.
We present the n-best accuracy (n = 1, 3, 10, 20) of
the system as well as the ?Oracle? results generated
by pooling the top-20 candidates from each of the
three subnormalizers. The best prior results on the
data sets are also included in the tables.
We notice that the broad-coverage system outper-
forms all other systems on the reported data sets.
It achieves about 90% word-level accuracy on data
set (1) and (2) with the top-10 candidates (an aver-
age 10% performance gain compared to (Liu et al,
2011b)). This is of crucial importance to a normal-
ization system, since the high accuracy and limited
number of candidates will enable more sophisticated
reranking or supervised learning techniques to se-
lect the best candidate. We also observe the ?Ora-
cle? system has averagely only 5% gap to the dic-
tionary coverage. A detailed analysis shows that the
human annotators perform many semantic/grammar
corrections as well as inconsistent annotations, e.g.,
(sleepy, ?zzz?), (disliked, ?unliked?). These are out
of the capabilities of the current text normalization
system and partly explains the remaining 5% gap.
Regarding the subnormalizer performance, the
spell checker yields only 50% to 60% accuracy on
all data sets, indicating that the vast amount of the
intentionally created nonstandard tokens can hardly
be tackled by a system relies solely on the lexi-
cal/phonetic similarity. The ?Visual Priming? sub-
SMS Dataset Word Level Accuracy (%)
(303 pairs) 1-best 3-best 10-best 20-best Oracle
Jazzy Spell Checker 43.89 55.45 56.77 56.77 n/a
Visual Priming 54.13 74.92 84.82 87.13 n/a
Enhanced Letter Tran. 61.06 74.92 80.86 82.51 n/a
Broad-Cov. System 64.36 80.20 89.77 91.75 94.06
(Pennell et al, 2011)? 60.39 74.58 75.57 75.57 n/a
(Liu et al, 2011) 62.05 75.91 81.19 81.19 n/a
(Cook et al, 2009) 59.4 n/a 83.8 87.8 n/a
(Choudhury et al, 2007)? 59.9 n/a 84.3 88.7 n/a
Table 4: Word-level results on data set (1). ? denotes
system requires human annotations for training.
Twitter Dataset Word Level Accuracy (%)
(3802 pairs) 1-best 3-best 10-best 20-best Oracle
Jazzy Spell Checker 47.19 56.92 59.13 59.18 n/a
Visual Priming 54.34 70.59 80.83 84.74 n/a
Enhanced Letter Tran. 61.05 70.07 74.04 74.75 n/a
Broad-Cov. System 69.81 82.51 92.24 93.79 95.71
(Liu et al, 2011) 68.88 78.27 80.93 81.17 n/a
Table 5: Word-level results on data set (2).
normalizer performs surprisingly well and shows ro-
bust performance across all data sets. A minor side-
effect is that the candidates were restricted to have
the same first letter with the noisy token, this sets
the upper bound of the approach to 89.77%, 92.45%,
and 93.51%, respectively on data set (1), (2), and (3).
Compared to other subnormalizers, the ?Enhanced
Letter Tran.? is effective at normalizing intention-
ally created tokens and has better precision regard-
ing its top candidate (n = 1). We demonstrate the
context-aware training pair selection results in Fig-
ure 2, by plotting the learning curve using different
amounts of training data, ranging from 1,000 (word,
token) pairs to the total 46,288 pairs. We notice that
the system can effectively learn the letter transfor-
mation patterns from a small number of high quality
training pairs. The final system was trained using the
top 5,000 pairs and the lookup table was created by
generating 50 variations for each dictionary word.
4.3 Message-level Results
The goal of message-level normalization is to re-
place each occurrence of the nonstandard token with
the candidate word that best fits the local context.
1041
SMS/Twitter Dataset Word Level Accuracy (%)
(2404 pairs) 1-best 3-best 10-best 20-best Oracle
Jazzy Spell Checker 39.89 46.51 48.54 48.67 n/a
Visual Priming 54.12 68.59 78.83 83.11 n/a
Enhanced Letter Tran. 57.65 67.18 71.01 71.88 n/a
Broad-Cov. System 64.39 78.29 86.56 88.69 91.60
(Pennell et al, 2011)? 37.40 n/a n/a 72.38 n/a
Table 6: Word-level results on data set (3). ? denotes
system requires human annotations for training.
64666870727476
1K
2K
5K
10K
20K
All (~45K)
Amoun
t of Tra
ining P
airs
Nonstandard Token Cov. (%)
Rando
m Sele
ction
Contex
t-aware
 Select
ion
Figure 2: Learning curve of the enhanced letter transfor-
mation system using random training pair selection or the
context-aware approach. Evaluated on data set (2).
We use the word-level ?Broad-Cov. System? for
candidate suggestion and the Viterbi algorithm for
message-level decoding. The system is evaluated on
data set (4) and results shown in Table 7. Following
research in (Han and Baldwin, 2011), we focus on
the the normalization task and assume perfect non-
standard token detection.
The ?Word-level w/o Context? results are gen-
erated by replacing each nonstandard token using
the 1-best word-level candidate. Although the re-
placement process is static, it results in 70.97% f-
score due to the high performance of the word-level
system. We explore two language models (LM)
for the Viterbi decoding process. First, a bigram
LM is trained using the Edinburgh Twitter corpus
(53,794,549 English tweets) with the SRILM toolkit
(Stolcke, 2002) and Kneser-Ney smoothing; second,
we retrieve the bigram probabilities from the Mi-
crosoft Web N-gram API (Wang et al, 2010) since
this represents a more comprehensive web-based
corpus. During decoding, we use the ?VisualPrim?
score as the emission probability, since this score
best fits the log scale and applies to all candidates.
For the Twitter LM, we apply a scaling factor of
0.5 to the ?VisualPrim? score to make it compara-
ble in scale to the LM probabilities. We use the 3-
best word-level candidates for Viterbi decoding. In
addition, we add the commonly used corrections for
Twitter Dataset Message-level P/R/F
(549 Tweets) Precision (%) Recall (%) F-score (%)
Word-level w/o Context 75.69 66.81 70.97
w/ Context
Web LM 79.12 77.11 78.10
Twitter LM 84.13 78.38 81.15
(Han and Baldwin, 2011)? 75.30 75.30 75.30
Table 7: Message-level results on data set (4). ? denotes
system requires human annotations for training.
16 single-characters, e.g., for ?r?, ?c?, we add ?are?,
?see? to the candidate list if they are not already pre-
sented. A default ?VisualPrim? score (? = 25) is
used for these candidates. As seen from Table 7,
both Web LM and Twitter LM achieve better perfor-
mance than the best prior results, with Twitter LM
outperforms the Web LM, yielding a f-score of 81%.
This shows that a vanilla Viterbi decoding process is
able to outperform the fine-tuned supervised system
given competitive word-level candidates. In future,
we will investigate other comprehensive message-
level candidate reranking process.
5 Conclusion
In this paper, we propose a broad-coverage normal-
ization system for the social media language with-
out using the human annotations. It integrates three
key components: the enhanced letter transformation,
visual priming, and string/phonetic similarity. The
system was evaluated on both word- and message-
level using four SMS and Twitter data sets. We show
that our system achieves over 90% word-coverage
across all data sets and the broad word-coverage can
be successfully translated into message-level perfor-
mance gain. We observe that the social media is an
emotion-rich language, therefore future normaliza-
tion system will need to address various sentiment-
related expressions, such as emoticons (?:d?, ?X-
8?), interjections (?bwahaha?, ?brrrr?), acronyms
(?lol?, ?lmao?), etc., whether and how these expres-
sions should be normalized is an unaddressed issue
and worths future investigation.
Acknowledgments
We thank the three anonymous reviewers for their
insightful comments and valuable input. We thank
Prof. Yang Liu, Deana Pennell, Bo Han, and Prof.
Tim Baldwin for sharing the annotated data and the
useful discussions. Part of this work was done while
Xiao Jiang was a research intern in Bosch Research.
1042
References
Kevin Atkinson. 2006. Gnu aspell. http://aspell.net/.
AiTi Aw, Min Zhang, Juan Xiao, and Jian Su. 2006. A
phrase-based statistical model for sms text normaliza-
tion. In Proceedings of COLING/ACL, pages 33?40.
Richard Beaufort, Sophie Roekhaut, Louise-Ame?lie
Cougnon, and Ce?drick Fairon. 2010. A hybrid
rule/model-based finite-state framework for normaliz-
ing sms messages. In Proceedings of ACL, pages 770?
779.
Eric Brill and Robert C. Moore. 2000. An improved
error model for noisy channel spelling correction. In
Proceedings of ACL.
Samuel Brody and Nicholas Diakopoulos. 2011.
Cooooooooooooooollllllllllllll!!!!!!!!!!!!!! Using word
lengthening to detect sentiment in microblogs. In Pro-
ceedings of EMNLP, pages 562?570.
Asli Celikyilmaz, Dilek Hakkani-Tur, and Junlan Feng.
2010. Probabilistic model-based sentiment analysis of
twitter messages. In Proceedings of the IEEE Work-
shop on Spoken Language Technology, pages 79?84.
Monojit Choudhury, Rahul Saraf, Vijit Jain, Animesh
Mukherjee, Sudeshna Sarkar, and Anupam Basu.
2007. Investigation and modeling of the structure of
texting language. International Journal on Document
Analysis and Recognition, 10(3):157?174.
Kenneth W. Church and William A. Gale. 1991. Prob-
ability scoring for spelling correction. Statistics and
Computing, 1:93?103.
CMU. 2007. The cmu pronouncing dictionary.
http://www.speech.cs.cmu.edu/cgi-bin/cmudict.
Paul Cook and Suzanne Stevenson. 2009. An unsuper-
vised model for text messages normalization. In Pro-
ceedings of the NAACL HLT Workshop on Computa-
tional Approaches to Linguistic Creativity, pages 71?
78.
Mathias Creutz and Krista Lagus. 2005. Unsupervised
morpheme segmentation and morphology induction
from text corpora using morfessor 1.0. In Computer
and Information Science, Report A81, Helsinki Uni-
versity of Technology.
David Crystal. 2009. Txtng: The gr8 db8. Oxford Uni-
versity Press.
Matt Davis. Reading jumbled texts. http://www.mrc-
cbu.cam.ac.uk/personal/matt.davis/Cmabrigde/.
Jennifer Foster, Ozlem Cetinoglu, Joachim Wagner,
Joseph Le Roux, Stephen Hogan, Joakim Nivre,
Deirdre Hogan, and Josef van Genabith. 2011. #hard-
toparse: POS tagging and parsing the twitterverse. In
Proceedings of the AAAI Workshop on Analyzing Mi-
crotext, pages 20?25.
Stephan Gouws, Donald Metzler, Congxing Cai, and Ed-
uard Hovy. 2011. Contextual bearing on linguistic
variation in social media. In Proceedings of the ACL
Workshop on Language in Social Media, pages 20?29.
Bo Han and Timothy Baldwin. 2011. Lexical normalisa-
tion of short text messages: Makn sens a #twitter. In
Proceedings of ACL, pages 368?378.
Mindaugas Idzelis. 2005. Jazzy: The java open source
spell checker. http://jazzy.sourceforge.net/.
Sittichai Jiampojamarn, Grzegorz Kondrak, and Tarek
Sherif. 2007. Applying many-to-many alignments
and hidden markov models to letter-to-phoneme con-
version. In Proceedings of HLT/NAACL, pages 372?
379.
Catherine Kobus, Franc?ois Yvon, and Ge?raldine
Damnati. 2008. Normalizing sms: Are two metaphors
better than one? In Proceedings of COLING, pages
441?448.
Taku Kudo. 2005. CRF++: Yet another CRF took kit.
http://crfpp.sourceforge.net/.
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional random fields: Probabilistic mod-
els for segmenting and labeling sequence data. In Pro-
ceedings of ICML, pages 282?289.
Franklin Mark Liang. 1983. Word hy-phen-a-tion by
com-put-er. In PhD Dissertation, Stanford University.
Fei Liu, Yang Liu, and Fuliang Weng. 2011a. Why
is ?sxsw? trending? Exploring multiple text sources
for twitter topic summarization. In Proceedings of the
ACL Workshop on Language in Social Media (LSM),
pages 66?75.
Fei Liu, Fuliang Weng, Bingqing Wang, and Yang Liu.
2011b. Insertion, deletion, or substitution? Normal-
izing text messages without pre-categorization nor su-
pervision. In Proceedings of ACL, pages 71?76.
Xiaohua Liu, Shaodian Zhang, Furu Wei, and Ming
Zhou. 2011c. Recognizing named entities in tweets.
In Proceedings of ACL, pages 359?367.
Eric Mays, Fred J. Damerau, and Robert L. Mercer.
1991. Context based spelling correction. Information
Processing and Management: An International Jour-
nal, 27(5):517?522.
Rada Mihalcea. 2007. Using wikipedia for auto-
matic word sense disambiguation. In Proceedings of
NAACL, pages 196?203.
Deana L. Pennell and Yang Liu. 2010. Normalization
of text messages for text-to-speech. In Proceedings of
ICASSP, pages 4842?4845.
Deana L. Pennell and Yang Liu. 2011. A character-
level machine translation approach for normalization
of sms abbreviations. In Proceedings of the 5th Inter-
national Joint Conference on Natural Language Pro-
cessing, pages 974?982.
Sasa Petrovic, Miles Osborne, and Victor Lavrenko.
2010. The edinburgh twitter corpus. In Proceedings
1043
of the NAACL HLT Workshop on Computational Lin-
guistics in a World of Social Media, pages 25?26.
Lev Ratinov and Dan Roth. 2009. Design challenges
and misconceptions in named entity recognition. In
Proceedings of CoNLL, pages 147?155.
Alan Ritter, Sam Clark, Mausam, and Oren Etzioni.
2011. Named entity recognition in tweets: An experi-
mental study. In Proceedings of EMNLP.
Richard Sproat, Alan W. Black, Stanley Chen, Shankar
Kumar, Mari Ostendorf, and Christopher Richards.
2001. Normalization of non-standard words. Com-
puter Speech and Language, 15(3):287?333.
Andreas Stolcke. 2002. SRILM ? An extensible lan-
guage modeling toolkit. In Proceedings of ICSLP,
pages 901?904.
L. Venkata Subramaniam, Shourya Roy, Tanveer A.
Faruquie, and Sumit Negi. 2009. A survey of types
of text noise and techniques to handle noisy text. In
Proceedings of AND.
Crispin Thurlow. 2003. Generation txt? the sociolin-
guistics of young people?s text-messaging. Discourse
Analysis Online.
Endel Tulving and Daniel L. Schacter; Heather A. Stark.
1982. Priming effects in word fragment comple-
tion are independent of recognition memory. Journal
of Experimental Psychology: Learning, Memory and
Cognition, 8(4).
Twitter. 2011. http://en.wikipedia.org/wiki/Twitter.
Kuansan Wang, Christopher Thrasher, Evelyne Viegas,
Xiaolong Li, and Bo june (Paul) Hsu. 2010. An
overview of microsoft web n-gram corpus and appli-
cations. In Proceedings of NAACL-HLT, pages 45?48.
Zhenzhen Xue, Dawei Yin, and Brian D. Davison. 2011.
Normalizing microtext. In Proceedings of the AAAI
Workshop on Analyzing Microtext, pages 74?79.
1044
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 605?610,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Unsupervised Alignment of Privacy Policies using Hidden Markov Models
Rohan Ramanath Fei Liu Norman Sadeh Noah A. Smith
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213, USA
{rrohan,feiliu,sadeh,nasmith}@cs.cmu.edu
Abstract
To support empirical study of online pri-
vacy policies, as well as tools for users
with privacy concerns, we consider the
problem of aligning sections of a thousand
policy documents, based on the issues they
address. We apply an unsupervised HMM;
in two new (and reusable) evaluations, we
find the approach more effective than clus-
tering and topic models.
1 Introduction
Privacy policy documents are verbose, often eso-
teric legal documents that many people encounter
as clients of companies that provide services on
the web. McDonald and Cranor (2008) showed
that, if users were to read the privacy policies of
every website they access during the course of a
year, they would end up spending a substantial
amount of their time doing just that and would
often still not be able to answer basic questions
about what these policies really say. Unsurpris-
ingly, many people do not read them (Federal
Trade Commission, 2012).
Such policies therefore offer an excellent op-
portunity for NLP tools that summarize or ex-
tract key information that (i) helps users under-
stand the implications of agreeing to these poli-
cies and (ii) helps legal analysts understand the
contents of these policies and make recommenda-
tions on how they can be improved or made more
clear. Past applications of NLP have sought to
parse privacy policies into machine-readable rep-
resentations (Brodie et al, 2006) or extract sub-
policies from larger documents (Xiao et al, 2012).
Machine learning has been applied to assess cer-
tain attributes of policies (Costante et al, 2012;
Ammar et al, 2012; Costante et al, 2013; Zim-
meck and Bellovin, 2013).
This paper instead analyzes policies in aggre-
gate, seeking to align sections of policies. This
task is motivated by an expectation that many poli-
cies will address similar issues,
1
such as collec-
tion of a user?s contact, location, health, and fi-
nancial information, sharing with third parties, and
deletion of data. This expectation is supported
by recommendation by privacy experts (Gellman,
2014) and policymakers (Federal Trade Commis-
sion, 2012); in the financial services sector, the
Gramm-Leach-Bliley Act requires these institu-
tions to address a specific set of issues. Aligning
policy sections is a first step toward our aforemen-
tioned summarization and extraction goals.
We present the following contributions:
? A new corpus of over 1,000 privacy policies
gathered from widely used websites, manually
segmented into subtitled sections by crowdwork-
ers (?2).
? An unsupervised approach to aligning the policy
sections based on the issues they discuss. For
example, sections that discuss ?user data on the
company?s server? should be grouped together.
The approach is inspired by the application of
hidden Markov models to sequence alignment in
computational biology (Durbin et al, 1998; ?3).
? Two reusable evaluation benchmarks for the re-
sulting alignment of policy sections (?4). We
demonstrate that our approach outperforms na??ve
methods (?5).
Our corpus and benchmarks are available at
http://usableprivacy.org/data.
2 Data Collection
We collected 1,010 unique privacy policy
documents from the top websites ranked by
Alexa.com.
2
These policies were collected during
a period of six weeks during December 2013 and
January 2014. They are a snapshot of privacy
policies of mainstream websites covering fifteen
1
Personal communication, Joel Reidenberg.
2
http://www.alexa.com
605
Business Computers Games Health
Home News Recreation Shopping
Arts Kids and Teens Reference Regional
Science Society Sports
Table 1: Fifteen website categories provided by Alexa.com.
We collect privacy policies from the top 100 websites in each.
of Alexa.com?s seventeen categories (Table 1).
3
Finding a website?s policy is not trivial. Though
many well-regulated commercial websites provide
a ?privacy? link on their homepages, not all do.
We found university websites to be exceptionally
unlikely to provide such a link. Even once the pol-
icy?s URL is identified, extracting the text presents
the usual challenges associated with scraping doc-
uments from the web. Since every site is differ-
ent in its placement of the document (e.g., buried
deep within the website, distributed across several
pages, or mingled together with Terms of Service)
and format (e.g., HTML, PDF, etc.), and since we
wish to preserve as much document structure as
possible (e.g., section labels), full automation was
not a viable solution.
We therefore crowdsourced the privacy policy
document collection using Amazon Mechanical
Turk. For each website, we created a HIT in
which a worker was asked to copy and paste the
following privacy policy-related information into
text boxes: (i) privacy policy URL; (ii) last up-
dated date (or effective date) of the current privacy
policy; (iii) privacy policy full text; and (iv) the
section subtitles in the top-most layer of the pri-
vacy policy. To identify the privacy policy URL,
workers were encouraged to go to the website and
search for the privacy link. Alternatively, they
could form a search query using the website name
and ?privacy policy? (e.g., ?Amazon.com privacy
policy?) and search in the returned results for the
most appropriate privacy policy URL. Given the
privacy policy full text and the section subtitles,
we partition the full privacy document into differ-
ent sections, delimited by the section subtitles. A
privacy policy is then converted into XML.
Each HIT was completed by three workers, paid
$0.05, for a total cost of $380 (including Ama-
zon?s surcharge).
3
The ?Adult? category was excluded; the ?World? cate-
gory was excluded since it contains mainly popular websites
in different languages, and we opted to focus on policies in
English in this first stage of research, though mulitlingual pol-
icy analysis presents interesting challenges for future work.
3 Approach
Given the corpus of privacy policies described in
?2, we designed a model to efficiently infer an
alignment of policy sections. While we expect that
different kinds of websites will likely address dif-
ferent privacy issues, we believe that many poli-
cies will discuss roughly the same set of issues.
Aligning the policies is a first step in a larger effort
to (i) automatically analyze policies to make them
less opaque to users and (ii) support legal experts
who wish to characterize the state of privacy on-
line and make recommendations (Costante et al,
2012; Ammar et al, 2012; Costante et al, 2013).
We are inspired by multiple sequence alignment
methods in computational biology (Durbin et al,
1998) and by Barzilay and Lee (2004), who de-
scribed a hidden Markov model (HMM) for doc-
ument content where each state corresponds to a
distinct topic and generates sentences relevant to
that topic according to a language model. We
estimate an HMM-like model on our corpus, ex-
ploiting similarity across privacy policies to the
extent it is evident in the data. In our formula-
tion, each hidden state corresponds to an issue or
topic, characterized by a distribution over words
and bigrams appearing in privacy policy sections
addressing that issue. The transition distribution
captures tendencies of privacy policy authors to
organize these sections in similar orders, though
with some variation.
The generative story for our model is as follows.
Let S denote the set of hidden states.
1. Choose a start state y
1
from S according to the
start-state distribution.
2. For t = 1, 2, . . ., until y
t
is the stopping state:
(a) Sample the tth section of the document by
drawing a bag of terms, o
t
, according to the
emission multinomial distribution for state y
t
.
Note the difference from traditional HMMs, in
which a single observation symbol is drawn
at each time step. o
t
is generated by repeat-
edly sampling from a distribution over terms
that includes all unigrams and bigrams except
those that occur in fewer than 5% of the doc-
uments and in more than 98% of the docu-
ments. This filtering rule was designed to
eliminate uninformative stopwords as well as
company-specific terms (e.g., the name of the
company).
4
4
The emission distributions are not a proper language
606
Websites with Unique privacy Unique privacy Ave. sections Ave. tokens
Category privacy URL policies policies w/ date per policy per policy
Arts 94 80 72 11.1 (? 3.8) 2894 (? 1815)
Business 100 95 75 10.1 (? 4.9) 2531 (? 1562)
Computers 100 78 62 10.7 (? 4.9) 2535 (? 1763)
Games 92 80 51 10.2 (? 4.9) 2662 (? 2267)
Health 92 86 57 10.0 (? 4.4) 2325 (? 1891)
Home 100 84 68 11.5 (? 3.8) 2493 (? 1405)
Kids and Teens 96 86 62 10.3 (? 4.5) 2683 (? 1979)
News 96 91 68 10.7 (? 3.9) 2588 (? 2493)
Recreation 98 97 67 11.9 (? 4.5) 2678 (? 1421)
Reference 84 86 55 9.9 (? 4.1) 2002 (? 1454)
Regional 98 91 72 11.2 (? 4.2) 2557 (? 1359)
Science 71 75 49 9.2 (? 4.1) 1705 (? 1136)
Shopping 100 99 84 12.0 (? 4.1) 2683 (? 1154)
Society 96 94 65 10.2 (? 4.6) 2505 (? 1587)
Sports 96 62 38 10.9 (? 4.0) 2222 (? 1241)
Average 94.2 85.6 63.0 10.7 (? 4.3) 2471 (? 1635)
Table 2: Statistics of each website category, including (i) the number of websites with an identified privacy policy link; (ii)
number of unique privacy policies in each category (note that in rare cases, multiple unique privacy policies were identified
for the same website, e.g., a website that contains links to both new and old versions of its privacy policy); (iii) number of
websites with an identified privacy modification date; (iv) average number of sections per policy; (v) average number of tokens
per policy.
(b) Sample the next state, y
t+1
, according to the
transition distribution over S.
This model can nearly be understood as a hid-
den semi-Markov model (Baum and Petrie, 1966),
though we treat the section lengths as observable.
Indeed, our model does not even generate these
lengths, since doing so would force the states to
?explain? the length of each section, not just its
content. The likelihood function for the model is
shown in Figure 1.
The parameters of the model are almost iden-
tical to those of a classic HMM (start state dis-
tribution, emission distributions, and transition
distributions), except that emissions are char-
acterized by multinomial rather than a cate-
gorical distributions. These are learned us-
ing Expectation-Maximization, with a forward-
backward algorithm to calculate marginals (E-
step) and smoothed maximum likelihood estima-
tion for the M-step (Rabiner, 1989). After learn-
ing, the most probable assignment of a policy?s
sections to states can be recovered using a variant
of the Viterbi algorithm.
We consider three HMM variants. ?Vanilla? al-
lows all transitions. The other two posit an order-
ing on the states S = {s
1
, s
2
, . . . , s
K
}, and re-
strict the set of transitions that are possible, impos-
ing bias on the learner. ?All Forward? only allows
models (e.g., a bigram may be generated by as many as three
draws from the emission distribution: once for each unigram
it contains and once for the bigram).
s
k
to transition to {s
k
, s
k+1
, . . . , s
K
}. ?Strict For-
ward? only allows s
k
to transition to s
k
or s
k+1
.
4 Evaluation
Developing a gold-standard alignment of privacy
policies would either require an interface that al-
lows each annotator to interact with the entire cor-
pus of previously aligned documents while read-
ing the one she is annotating, or the definition (and
likely iterative refinement) of a set of categories
for manually labeling policy sections. These were
too costly for us to consider, so we instead pro-
pose two generic methods to evaluate models
for sequence alignment of a collection of docu-
ments with generally similar content. Though our
model (particularly the restricted variants) treats
the problem as one of alignment, our evaluations
consider groupings of policy sections. In the se-
quel, a grouping on a set X is defined as a collec-
tion of subsets X
i
? X; these may overlap (i.e.,
there might be x ? X
i
?X
j
) and need not be ex-
haustive (i.e., there might be x ? X \
?
i
X
i
).
4.1 Evaluation by Human QA
This study was carried out as part of a larger col-
laboration with legal scholars who study privacy.
In that work, we have formulated a set of nine mul-
tiple choice questions about a single policy that
ask about collection of contact, location, health,
and financial information, sharing of each with
607
Ppi,?,? (?yt,ot?
n
t=1
| ?`
t
?
n
t=1
) = pi(y
1
)
n
?
t=1
(
`
t
?
i=1
?(o
t,i
| y
i
)
)
?(y
t+1
| y
t
)
Figure 1: The likelihood function for the alignment model (one privacy policy). y
t
is the hidden state for the tth section, o
t
is
the bag of unigram and bigram terms observed in that section, and `
t
is the size of the bag. Start-state, emission, and transition
distributions are denoted respectively by pi, ?, and ?. y
n+1
is the silent stopping state.
third parties, and deletion of data.
5
The questions
were inspired primarily by the substantive interest
of these domain experts?not by this particular al-
gorithmic study.
For thirty policies, we obtained answers from
each of six domain experts who were not involved
in designing the questions. For the purposes of this
study, the experts? answers are not important. In
addition to answering each question for each pol-
icy, we also asked each expert to copy and paste
the text of the policy that contains the answer.
Experts were allowed to select as many sections
for each question as they saw fit, since answering
some questions may require synthesizing informa-
tion from different sections.
For each of the nine questions, we take the
union of all policy sections that contain text se-
lected by any annotator as support for her answer.
This results in nine groups of policy sections,
which we call answer-sets denoted A
1
, . . . , A
9
.
Our method allows these to overlap (63% of the
sections in any A
i
occurred in more than one A
i
),
and they are not exhaustive (since many sections
of the policies were not deemed to contain answers
to any of the nine questions by any expert).
Together, these can be used as a gold standard
grouping of policy sections, against which we can
compare our system?s output. To do this, we define
the set of section pairs that are grouped together
in answer sets, G = |{?a, b? | ?A
i
3 a, b}|, and
a similar set of pairs H from a model?s grouping.
From these sets, we calculate estimates of preci-
sion (|G ?H|/|H|) and recall (|G ?H|/|G|).
One shortcoming of this approach, for which
the second evaluation seeks to compensate, is that
a very small, and likely biased, subset of the policy
sections is considered.
4.2 Evaluation by Direct Judgment
We created a separate gold standard of judgments
of pairs of privacy policy sections. The data se-
lected for judgment was a sample of pairs stratified
5
The questions are available in an online appendix at
http://usableprivacy.org/data.
by a simple measure of text similarity. We derived
unigram tfidf vectors for each section in each of
50 randomly sampled policies per category. We
then binned pairs of sections by cosine similarity
(into four bins bounded by 0.25, 0.5, and 0.75).
We sampled 994 section pairs uniformly across the
15 categories? four bins each.
Crowdsourcing was used to determine, for each
pair, whether the two sections should be grouped
together. A HIT consisted of a pair of policy sec-
tions and a multiple choice question, ?After read-
ing the two sections given below, would you say
that they broadly discuss the same topic?? The
possible answers were:
1. Yes, both the sections essentially convey the
same message in a privacy policy.
2. Although, the sections do not convey the same
message, the broadly discuss the same topic.
(For ease of understanding, some examples of
content on ?the same topic? were included.)
3. No, the sections discuss two different topics.
The first two options were considered a ?yes? for
the majority voting and for defining a gold stan-
dard. Every section-pair was annotated by at least
three annotators (as many as 15, increased until
an absolute majority was reached). Turkers with
an acceptance rate greater than 95% with an ex-
perience of at least 100 HITs were allowed and
paid $0.03 per annotation. The total cost includ-
ing some initial trials was $130. 535 out of the
994 pairs were annotated to be similar in topic. An
example is shown in Figure 2.
As in ?4.1, we calculate precision and recall on
pairs. This does not penalize the model for group-
ing together a ?no? pair; we chose it nonetheless
because it is interpretable.
5 Experiment
In this section, we evaluate the three HMM vari-
ants described in ?3, and two baselines, using the
methods in ?4. All of the methods require the
specification of the number of groups or hidden
states, which we fix to ten, the average number of
sections per policy.
608
Section 5 of classmates.com:
[46 words] . . . You may also be required to use a password to access certain pages on the Services where certain
types of your personal information can be changed or deleted. . . . [113 words]
Section 2 of 192.com:
[50 words] . . . This Policy sets out the means by which You can have Your Personal Information removed from
the Service. 192.com is also committed to keeping Personal Information of users of the Service secure and only to
use it for the purposes set out in this Policy and as agreed by You. . . . [24 words]
Figure 2: Selections from sections that discuss the issue of ?deletion of personal information? and were labeled as discussing
the same issue by crowdworkers. Both na??ve grouping and LDA put them in two different groups, but the Strict Forward variant
of our model correctly groups them together.
Precision Recall F
1
Mean S.D. Mean S.D. Mean S.D.
Clust. 0.63 ? 0.30 ? 0.40 ?
LDA 0.56 0.03 0.20 0.05 0.29 0.06
Vanilla 0.62 0.04 0.41 0.04 0.49 0.03
All F. 0.63 0.03 0.47 0.12 0.53 0.06
Strict F. 0.62 0.05 0.46 0.18 0.51 0.07
Clust. 0.62 ? 0.23 ? 0.34 ?
LDA 0.57 0.03 0.18 0.01 0.28 0.02
Vanilla 0.57 0.01 0.30 0.03 0.39 0.02
All F. 0.58 0.02 0.32 0.06 0.41 0.04
Strict F. 0.58 0.03 0.32 0.14 0.40 0.08
Table 3: Evaluation by human QA (above) and direct judg-
ment (below), aggregated across ten independent runs where
appropriate (see text). Vanilla, All F(orward), and Strict
F(orward) are three variants of our HMM.
Baselines. Our first baseline is a greedy divisive
clustering algorithm
6
to partition the policy sec-
tions into ten clusters. In this method, the de-
sired K-way clustering solution is computed by
performing a sequence of bisections. The imple-
mentation uses unigram features and cosine simi-
larity. Our second baseline is latent Dirichlet alo-
cation (LDA; Blei et al, 2003), with ten topics and
online variational Bayes for inference (Hoffman et
al., 2010).
7
To more closely match our models,
LDA is given access to the same unigram and bi-
gram tokens.
Results. Table 3 shows the results. For LDA
and the HMM variants (which use random initial-
ization), we report mean and standard deviation
across ten independent runs. All three variants
of the HMM improve over the baselines on both
tasks, in terms of F
1
. In the human QA evalu-
ation, this is mostly due to recall improvements
(i.e., more pairs of sections relevant to the same
policy question were grouped together).
The three variants of the model performed sim-
ilarly on average, though Strict Forward had very
high variance. Its maximum performance across
6
As implemented in CLUTO, http://glaros.dtc.
umn.edu/gkhome/cluto/cluto/overview
7
As implemented in gensim (
?
Reh?u?rek and Sojka, 2010).
ten runs was very high (67% and 53% F
1
on the
two tasks), suggesting the potential benefits of
good initialization or model selection.
6 Conclusion
We considered the task of aligning sections of
a collection of roughly similarly-structured legal
documents, based on the issues they address. We
introduced an unsupervised model for this task
along with two new (and reusable) evaluations.
Our experiments show the approach to be more ef-
fective than clustering and topic models. The cor-
pus and evaluation data have been made available
at http://usableprivacy.org/data . In
future work, policy section alignments will be
used in automated analysis to extract useful infor-
mation for users and privacy scholars.
Acknowledgments
The authors gratefully acknowledge helpful com-
ments from Lorrie Cranor, Joel Reidenberg, Flo-
rian Schaub, and several anonymous reviewers.
This research was supported by NSF grant SaTC-
1330596.
References
Waleed Ammar, Shomir Wilson, Norman Sadeh, and
Noah A. Smith. 2012. Automatic categorization of
privacy policies: A pilot study. Technical Report
CMU-LTI-12-019, Carnegie Mellon University.
Regina Barzilay and Lillian Lee. 2004. Catching the
drift: Probabilistic content models, with applications
to generation and summarization. In Proc. of HLT-
NAACL.
Leonard E. Baum and Ted Petrie. 1966. Statistical
inference for probabilistic functions of finite state
Markov chains. Annals of Mathematical Statistics,
37:1554?1563.
David M Blei, Andrew Y Ng, and Michael I Jordan.
2003. Latent Dirichlet alocation. the Journal of
machine Learning research, 3:993?1022.
609
Carolyn A. Brodie, Clare-Marie Karat, and John Karat.
2006. An empirical study of natural language pars-
ing of privacy policy rules using the SPARCLE pol-
icy workbench. In Proc. of the Symposium on Us-
able Privacy and Security.
Elisa Costante, Yuanhao Sun, Milan Petkovi?c, and
Jerry den Hartog. 2012. A machine learning solu-
tion to assess privacy policy completeness. In Proc.
of the ACM Workshop on Privacy in the Electronic
Society.
Elisa Costante, Jerry Hartog, and Milan Petkovi.
2013. What websites know about you. In Roberto
Pietro, Javier Herranz, Ernesto Damiani, and Radu
State, editors, Data Privacy Management and Au-
tonomous Spontaneous Security, volume 7731 of
Lecture Notes in Computer Science, pages 146?159.
Springer Berlin Heidelberg.
Richard Durbin, Sean R. Eddy, Anders Krogh, and
Graeme Mitchison. 1998. Biological Sequence
Analysis: Probabilistic Models of Proteins and Nu-
cleic Acids. Cambridge University Press.
Federal Trade Commission. 2012. Protecting con-
sumer privacy in an era of rapid change: Recom-
mendations for businesses and policymakers.
Robert Gellman. 2014. Fair information prac-
tices: a basic history (v. 2.11). Available at
http://www.bobgellman.com/rg-docs/
rg-FIPShistory.pdf.
Matthew D Hoffman, David M Blei, and Francis R
Bach. 2010. Online learning for latent Dirichlet al
location. In NIPS.
Aleecia M. McDonald and Lorrie Faith Cranor. 2008.
The cost of reading privacy policies. I/S: A Journal
of Law and Policy for the Information Society, 4(3).
Lawrence Rabiner. 1989. A tutorial on hidden Markov
models and selected applications in speech recogni-
tion. Proceedings of the IEEE, 77(2):257?286.
Radim
?
Reh?u?rek and Petr Sojka. 2010. Software frame-
work for topic modelling with large corpora. In
Proc. of the LREC Workshop on New Challenges for
NLP Frameworks.
Xusheng Xiao, Amit Paradkar, Suresh Thum-
malapenta, and Tao Xie. 2012. Automated ex-
traction of security policies from natural-language
software documents. In Proc. of the ACM SIGSOFT
International Symposium on the Foundations of
Software Engineering.
Sebastian Zimmeck and Steven M. Bellovin. 2013.
Machine learning for privacy policy.
610
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 506?513,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
Zhou qiaoli: A divide-and-conquer strategy for  
semantic dependency parsing 
 
 
Qiaoli Zhou Ling Zhang Fei Liu Dongfeng 
Cai 
Guiping 
Zhang 
Knowledge Engineering  
Research Center Shenyang Aerospace University 
No.37 Daoyi South Avenue 
Shenyang, Liaoning, China 
Zhou_qiao_li@
hotmail.com 
710138892@qq.
com 
fei_l2011@
163.com 
caidf@vip.16
3.com 
zgp@ge-
soft.com 
 
 
 
 
 
 
Abstract 
We describe our SemEval2012 shared Task 5 
system in this paper. The system includes 
three cascaded components: the tagging se-
mantic role phrase, the identification of se-
mantic role phrase, phrase and frame semantic 
dependency parsing. In this paper, semantic 
role phrase is tagged automatically based on 
rules, and takes Conditional Random Fields 
(CRFs) as the statistical identification model 
of semantic role phrase. A projective graph-
based parser is used as our semantic depend-
ency parser. Finally, we gain Labeled At-
tachment Score (LAS) of 61.84%, which 
ranked the first position. At present, we gain 
the LAS of 62.08%, which is 0.24% higher 
than that ranked the first position in the task 5. 
1 System Architecture  
To solve the problem of low accuracy of long dis-
tance dependency parsing, this paper proposes a 
divide-and-conquer strategy for semantic depend-
ency parsing. Firstly, Semantic Role (SR) phrase in 
a sentence are identified; next, SR phrase can be 
replaced by their head or SR of head. Therefore, 
the original sentence is divided into two kinds of 
parts, which can be parsed separately. The first 
kind is SR phrase parsing; the second kind is  
parsing the sentence in which the SR phrases are 
replaced by their head or SR of head. Finally, the 
paper takes graph-based parser as the semantic de-
pendency parser for all parts. They are described in 
Section 2 and Section 4. Their experimental results 
are shown in Section5. Section 6 gives our conclu-
sion and future work. 
2 SR Phrase Tagging and Frame  
To identify SR phrase, SR phrase of train corpus 
are tagged. SR phrase is tagged automatically 
based on rules in this paper. A phrase of the sen-
tence is called Semantic Role phrase (SR phrase) 
when the parent of only one word of this phrase is 
out of this phrase. The word with the parent out of 
the phrase is called Head of Phrase (HP). The 
shortest SR phrase is one word, while the longest 
SR phrase is a part of the sentence. In this paper, 
the new sequence in which phrases are replaced by 
their head or SR of head is defined as the frame. In 
this paper, firstly, SR phrases of the sentence are 
identified; secondly, the whole sentence is divided 
into SR phrases and frame; thirdly, SR phrase and 
frame semantic dependency are parsed; finally, the 
dependency parsing results of all components are 
combined into the dependency parsing result of the 
whole sentence. 
SR of HP is used as the type of this phrase. Only 
parts of types of SR phrases are tagged. In this pa-
per, the tagged SR phrases are divided into two 
506
types: Main Semantic Role (MSR) phrase and 
Preposition Semantic Role (PSR) phrase. 
2.1 MSR Phrase Tagging  
In this paper, MSR phrase includes: OfPart, agent, 
basis, concerning, content, contrast, cost, existent, 
experiencer, isa, partner, patient, possession, pos-
sessor, relevant, scope and whole. MSR phrase 
tagging rules are shown in figure1&2. 
  
Figure1: Tagging Rule of the Last Word of MSR Phrase 
Figure 1 shows the rule for identification of the 
last word of MSR phrase. If the SR of the current 
word is MSR and its POS is not VV, VE, VC or 
VA, it is the last word of phrase. 
As shown in the figure 2, the first word of 
phrase is found based on the last word of phrase. 
The child with the longest distance from the last 
word of phrase is used as the current word, and if 
the current word has no child, it is the first word of 
phrase; otherwise, the child of the current word is 
found recursively. If the first word of phrase POS 
is preposition and punctuation, and its parent is the 
last word, the word following the first word serves 
as the first word of phrase. 
 
Figure2: Tagging Rule of the First Word of MSR Phrase 
 
 
Figure3: Example of the Tagging MSR Phrase 
As shown in the figure 3, the first column is 
word ID and the seventh column is parent ID of 
word. SR of ID40 is content, so ID40 is the last 
word of phrase. Its children include ID39 and ID37, 
thus ID37 with the longest distance from ID40 is 
the current word. The child of ID37 is ID33, the 
child of ID33 is ID32, ID32 has no child, and ID32 
is the first word of SR phrase. 
The tagged result in the above figure 3 is as fol-
lows: ?/CC ?/VC ??/VV content[ ??/JJ ?
? /NN ? /CC ?? /NR ? /ETC ?? /NN ?
/DEG ??/NN ??/NN ]  
Input: wi: word index (ID) in a given sentence. 
           N: the number of words. 
          Mi: MSR list. 
          Vi: POS tags list 
Output: the last word ID of MSR phrase 
Function: Findmainsemanticword(wi): return word 
ID when wi of semantic belongs to Mi. 
Otherwise return 0. 
Function: FindPOSword(wi): return true when wi 
of POS tagging not belongs to Vi. Oth-
erwise return 0. 
Function Findlastword(wi) 
For i?1 to N do begin 
             If (Findmainsemanticword(wi)&& 
FindPOSword(wi)) 
               { 
                   return wi; 
} 
else { 
                          i++; 
} 
       end 
return 0; 
29  ?  ?  CC  CC  _  30  aux-depend  _  _ 
30  ?  ?  VC  VC  _  58 s-succession  _  _ 
31  ?? ??  VV  VV  _  54  s-succession _  _ 
32  ??  ??  JJ   JJ  _  33  d-attribute  _  _ 
33  ??  ??  NN  NN  _  37  s-coordinate  _  _ 
34  ?  ?  CC  CC  _  37  aux-depend  _  _ 
35  ??  ??  NR  NR  _  37  d-member  _  _ 
36  ?  ?  ETC  ETC  _  35  aux-depend  _  _ 
37  ??  ??  NN  NN  _  40  d-genetive  _  _ 
38  ?  ?  DEG  DEG  _  37  aux-depend  _  _ 
39  ??  ??  NN  NN  _  40  s-coordinate  _ _ 
40  ?? ??  NN  NN  _  31  content  _  _ 
Input: Lword: the last word ID of MSR phrase. 
Output: Fword: the first word ID of MSR phrase. 
Function: Findmaxlenchild (w): return child ID 
with the longest distance from w when w 
has child. Otherwise returns 0. 
Fuction: FindPOSword(w): return POS of w. 
Fuction:Findparent(w): return parent ID of w. 
Function Findfirstword(Lword) 
     If(Findmaxlenchild (Lword)= =0) 
      { 
         return Lword; 
} 
Else { 
Fword=Findmaxlenchildword(Lword); 
If(findPOSword(Fword)==P||  
findPOSword(Fword)= =PU) 
{ 
    If (findparent(Fword)= =Lword) 
        Return Fword +1; 
} 
Findfirstword(Fword); 
} 
507
After phrases are tagged, a new sequence gener-
ated by replacing the phrase with HP is called 
MSR frame. 
MSR frame: ?/CC ?/VC ??/VV ??/NN  
Example of sentences with nested phrases: 
?/P ??/JJ ??/NN ?/PU ??/NT exis-
tent[ ? /P ?? /NR ?? /NN ?? /VV con-
tent[ ??/NN ] ?/DEC ??/NN ???/NN ] 
?/AD ?/VE ?????/CD ?/M  
After phrases are tagged, a new sequence gener-
ated by replacing the phrase with HP is called 
MSR frame. 
MSR frame: ?/P ??/JJ ??/NN ?/PU ??
/NT???/NN ?/AD ?/VE ?????/CD ?
/M 
2.2 PSR Phrase Tagging  
In this paper, SR phrase containing preposition is 
defined as PSR phrase. If the POS tags of the cur-
rent word is Preposition (P), the first word and the 
last word of PSR phrase are found based on the 
current word. PSR phrase tagging rule as figure 4 
& 5. 
 
Figure 4: Tagging Rule of the First Word of PSR Phrase 
As shown in the figure 4, the child with the 
longest distance from the current word is the first 
word of phrase. If the prep has no child, then it is 
PSR phrase. 
As shown in the figure 5, firstly, the parent of 
the prep is found; next, the parent is taken as the 
current word, and the child with the longest dis-
tance from the current word is found recursively. If 
no child is found, the current word is the last word 
of PSR phrase. If preposition of SR is root or par-
ent of preposition is root, and proposition is PSR. 
If ID of preposition is larger than ID of parent of 
preposition, and preposition is PSR. 
 
Figure5: Tagging Rule of the Last Word of PSR Phrase 
 
 
Figure6: Example of the Tagging PSR Phrase 
As shown in the figure6, ID4 is prep, and it has 
no child, so the first word is ID4. The parent of 
Input: Pword: the word ID that word POS tags is P. 
Output: Fword: the first word ID of PSR phrase. 
Function: Findmaxlenchildword(w): return word ID 
with the longest distance from w when w 
has child. Otherwise returns 0. 
Function Findfirstword(Pword) 
        If(Findmaxlenchildword(Pword)= =0) 
          { 
             return Pword; 
} 
Else { 
return Fwrod= 
 Findmaxlenchildword(Pword); 
} 
Input: Pword: the word ID that word POS tags is P. 
Output: Lword: the last word ID of PSR phrase. 
Function: Findmaxchild (w): return word ID that 
length is max with w when w has child. 
Otherwise return 0. 
Function: Findparent (w): return word ID when w of 
parent is not root. Otherwise return 0.  
Function: Findroot(w): return 1 when w of semantic 
role is root. Other wise return 0. 
Function Findlastword(Pword) 
Var cword: parent ID 
     If(Findparentsword(Pword)= =0|| 
 findroot(Pword)= =1)  { 
             return Pword; 
} 
else { cword=Findparent (Pword) ) 
 If(Pword>cword){ 
return Pword; 
} 
else { 
                   if(Findmaxchild (cword)= =0) { 
                               return cword; 
} 
else{  
Lword= 
Findmaxchild (cword); 
Findlastword(Lword); 
} 
                           } 
}
1  ??  ??  NN  NN  _  2  j-agent  _  _ 
2  ??  ??  NN  NN  _  3  r-patient  _  _ 
3  ??  ??  NN  NN  _  11  agent  _  _ 
4  ?  ?  P P  _ 5  prep-depend  _ first word 
5  ??  ??  VV  VV  _  11 duration _ head_ 
6  ?? ?? NR  NR _ 8  d-genetive  _ _ 
7  ?? ?? NN  NN _  8 r-patient _ _ 
8  ??  ?? NN  NN _ 9 d-host _  _ 
9  ??  ?? NN  NN _ 5 patient  _  _ 
10  ?  ? LC  LC  _ 5  aux-depend _ last word_ 
11  ??  ?? VV VV  _  0  ROOT _  _ 
12  ?  ?  AS  AS  _ 11 aspect  _  _ 
13 ??  ?? JJ  JJ  _ 14 d-attribute  _  _ 
14  ?? ??  NN NN  _  11 content  _  _ 
15  ?  ?  PU  PU  _ 11  PU  _  _ 
508
ID4 is ID5, the child with the longest distance from 
ID5 is ID10, and ID10 with no child is the last 
word of phrase. 
The tagged result in the above figure 6 is as fol-
lows: ??/NN ??/NN ??/NN duration[?/P 
??/VV ??/NR ??/NN ??/NN ?/LC] ?
?/VV ?/AS ??/JJ ??/NN ?/PU 
The position of HP in PSR phrase is not fixed. 
After phrases are tagged, a new sequence gener-
ated by replacing the phrase with SR of HP is 
called PSR frame. 
PSR frame: ??/NN ??/NN ??/NN dura-
tion/duration ?? /VV ? /AS ?? /JJ ??
/NN ?/PU 
Examples of sentences with nested phrases: 
s-cause[ ??/P ??/NR s-purpose[ ?/P ?
?/VV ???/NT ]  ?/MSP ??/VV ??/VV 
?/DT ?/M ??/NN ??/NN ],/PU ??/AD 
?? /NN ?? /NN ? /VV ? /VV ????
/VV ?/PU 
PSR frame: s-cause/s-cause ,/PU ??/AD ??
/NN ??/NN ?/VV ?/VV ????/VV ?/PU 
2.3 SR Phrase Tagging Performance 
If the parent of only one word of the tagged phrase 
is out of this phrase, this phrase is tagged correctly. 
If each word in the generated frame has one parent 
(i.e. words out of the phrase are dependent on HP 
instead of other words of the phrase), the frame is 
correct. 
 Phrase Frame 
MSR 99.99% 100% 
PSR 99.98% 99.70% 
Table 1. Tagging Performance (P-score) 
 
As shown in the table 1, tagging results were of 
very high accuracy. The wrong results were not 
contained in phrase and frame train corpus of de-
pendency parsing. 
3 SR Phrase Identification  
In this paper, we divide SR phrase into two classes: 
Max SR phrase and Base SR phrase. Max SR 
phrase refers to SR phrase is not included in any 
other SR phrase in a sentence. Base SR phrase re-
fers to SR phrase does not include any other SR 
phrase in a SR phrase. Therefore, MSR phrase is 
divided into two classes: Max MSR (MMSR) 
phrase and Base MSR (BMSR) phrase. PSR phrase 
was divided into two classes: Max PSR (MPSR) 
phrase and Base PSR (BPSR) phrase. 
3.1 MMSR Phrase Identification based on 
Cascaded Conditional Random Fields 
Reference (Qiaoli Zhou, 2010) is selected as our 
approach of MMSR phrase identification. The 
MMSR identifying process is conceptually very 
simple. The MMSR identification first performs 
identifying BMSR phrase, and converts the identi-
fied phrase to head. It then performs identifying for 
the updated sequence and converts the newly rec-
ognized phrases into head. The identification re-
peats this process until the whole sequence has no 
phrase, and the top-level phrase are the MMSR 
phrases. A common approach to the phrase identi-
fication problem is to convert the problem into a 
sequence tagging task by using the ?BIEO? (B for 
beginning, I for inside, E for ending, and O for 
outside) representation. If the phrase has one word, 
the tag is E. This representation enables us to use 
the linear chain CRF model to perform identifying, 
since the task is simply assigning appropriate la-
bels to sequence. 
There are two differences between our feature 
set and Qiaoli (2010)?s: 
1) We use dependency direction of word as iden-
tification feature, while Qiaoli (2010) did not 
use. 
2) We do not use scoring algorithm which is used 
by Qiaoli (2010). 
Direction Unigrams D-3,D-2 ,D-1 , D0 , D+1 ,D+2 ,D+3
Direction Bigrams D-2D-1, D-1D0, D0D+1, D+1D+2,  
Word & Direction W0D0
Table 2. Feature Templates of MMSR Phrase 
 
Table 2 is additional new feature templates 
based on Qiaoli (2010). W represents a word, and 
D represents dependency direction of the word. 
With this approach, nested MSR phrases are identi-
fied, and the top-level MSR phrase is the MMSR 
that we obtained. 
corpus P R F 
dev 81.41% 75.40% 78.29% 
test 81.23% 73.04% 76.92% 
Table 3.  MMSR Identification Performance 
509
3.2 BMSR Phrase Identification based on 
CRFs  
We use the tag set ?BIEO? the same as that used 
for MMSR identification. 
Word Unigrams W-3, W-2, W-1, W0, W+1, W+2, W+3
Word Bigrams 
W-3W-2, W-2W-1, W-1W0, W0W+1, 
W+1W+2, W+2W+3
POS Unigrams P-3 , P-2, P-1, P0, P+1, P+2, P+3
POS Bigrams 
P-3P-2, P-2P-1, P-1P0, P0P+1,  
P+1P+2, P+2P+3
Word_X X0
Word_Y Y0
Word_D D0
Word_S S-3, S-2 , S-1 , S0, S+1, S+2, S+3
Word & POS W-1P-1, W0P0, W+1P+1
Word & Word_X W-3X0
Word & Word_D 
W0D0, W-3W-2D0, W-2W-1D0,  
W-1W0D0, W0W+1D0, W+1W+2D0, 
W+2W+3D0
Word & Word_S W-1S-1, W0S0, W+1S+1, W+2S+2
Word_X & Word_Y X0Y0
POS & Word_D 
P0D0, P-3P-2D0, P-2P-1D0, P-1P0D0, 
P0P+1D0, P+1P+2D0, P+2P+3D0
POS & Word_S 
P-1S-1, P-2S-2, P-3S-3, P0S0, 
 P+1S+1, P+2S+2, P+3S+3
Word_D & Word_S 
D-1S-1, D-2S-2, D-3S-3, D0S0, 
 D+1S+1, D+2S+2, D+3S+3
Word & POS & 
Word_D 
W-1P-1D0, W0P0D0, W+1P+1D0
Word & POS & 
Word_D & Word_S 
W-3P-3D-3S-3, W-2P-2D-2S-2,  
W-1P-1D-1S-1, W0P0D0S0, W1P1D1S1, 
W2P2D2S2, W3P3D3S3
Table 4. Feature Templates of BMSR Phrase 
 
In table 4, ?W? represents a word, ?P? repre-
sents the part-of-speech of the word, ?X? repre-
sents the fourth word following the current word, 
?Y? represents the fifth word following the current 
word, ?D? represents the dependency direction of 
the current word, and ?S? represents the paired 
punctuation feature. ?S? consists of ?RLIO? (R for 
the right punctuation, L for the left punctuation, I 
for the part between the paired punctuation and O 
for outside). 
 
corpus P R F 
dev 79.32% 80.65% 79.98% 
test 79.22% 79.96% 79.59% 
Table 5.  BMSR Identification Performance (F-score) 
3.3 MPSR Phrase Identification Based on 
Collection  
Reference (Dongfeng, 2011) is selected as our ap-
proach of MPSR phrase identification. The posi-
tion of HP in PSR phrase is not fixed. Not only 
PSR phrase is identified, but also PSR phrase type 
is identified.  
There are two major differences between our 
feature set and Dongfeng (2011)?s: 
1) We take the PSR phrase type (the SR of HP) 
as tag.  
2)  We use ?S-type? represents that the PSR 
phrase is the single preposition. ?Type? represents 
SR of the preposition. 
For example: ???/NN location [?/P ??
/NR ??/NR] ??/VV 
O|W POS
Dongfeng 
(2011) Tag 
Our Tag 
*|??? NN O O 
*|? P O O 
?|?? NR I I 
?|?? NR E Location-E
?|?? VV N N 
Table 6. Example of PSR Phrase Tag Set  
 
In table 6, Dongfeng(2011) takes ?E? as the tag 
of last word of PSR phrase, but we take ?Location-
E? as the tag of last word of PSR phrase  (Location 
is type of  PSR phrase). 
With this approach, nested PSR phrases are 
identified, and the top-level PSR phrase is the 
MPSR that we obtained. 
corpus MPSR phrase MPSR phrase & type
dev 84.00% 54.23% 
test 83.78% 51.60% 
Table 7. MPSR Identification Performance (F-score) 
3.4 Combined Identification of MSR Phrase 
and PSR Phrase 
Identification process: MSR phrase and PSR 
phrase are respectively identified in one sentence, 
and the results are combined in accordance with 
this rule: if phrases are nested, only the top-level 
phrase is tagged; if phrases are same, only the PSR 
510
phrase is tagged; if phrases are overlapped, only 
PSR phrase is tagged. 
There are two combinations in this paper:  
1) MMSR phrase and MPSR phrase combined 
result is defined as MMMP phrase. For exam-
ple as follow (?[ ]?represents MMSR, 
?{}?represents MPSR): 
Example A: [ ??/NN ] ?/VC [ ??/VV ?
?/NR ?/DEC ?/CD ?/M ??/JJ ??/NN ?
?/NN ] ?/PU ??/DT ?/M ?/VE [ ??/CD 
?/M ??/NN ??/NN ?/PU ???/CD ?/M 
??/NN ??/NN ] ??/VV location{ ?/P ?
/DT ?/M ??/NN ?/LC } ?/PU  
MMMP  frame: [ ??/NN ] ?/VC ??/NN ?
/PU ??/DT ?/M ?/VE ??/NN ??/VV 
location/location ?/PU 
2) BMSR phrase and MPSR phrase combined 
result is defined as BMMP phrase. 
Example B: [ ??/NN ] ?/VC ??/VV [ ??
/NR ] ?/DEC ?/CD ?/M ??/JJ ??/NN ?
?/NN ?/PU ??/DT ?/M ?/VE [ ??/CD ?
/M ??/NN ??/NN ?/PU ???/CD ?/M ?
?/NN ??/NN ] ??/VV location{ ?/P ?/DT 
?/M ??/NN ?/LC } ?/PU 
BMMP  frame: ??/NN ?/VC ??/VV ??
/NR ?/DEC ?/CD ?/M ??/JJ ??/NN ??
/NN ?/PU ??/DT ?/M ?/VE ??/NN ??
/VV location/location ?/PU 
corpus phrase P R F 
BMMP 79.48% 81.60% 80.53%
dev 
MMMP 80.00% 76.79% 78.36%
BMMP 80.14% 82.48% 81.30%
test 
MMMP 80.19% 78.53% 79.35%
Table 8.  Combination Phrase Identification 
Performance 
3.5 Phrase and Frame Length Distribution   
We count phrases, frame and Original Sentence 
(OS) length distribution in training set and dev set. 
 BMMP MMMP MMSR BMSR OS 
[0,5) 80.07% 71.36% 75.36% 85.74% 9.07%
[5,10) 16.15% 21.63% 18.93% 12.33% 8.30%
[10,20) 3.35% 6.13% 5.05% 1.80% 17.23%
20? 0.43% 0.88% 0.66% 0.13% 65.40%
Table 9.  Length Distribution of Phrases and OS 
 
Table 9 shows, about 95% of phrases have less 
than 10 words, but about 65% of OS has more than 
20 words. 
 BMMP MMMP MMSR BMSR OS 
[0,5) 16.00% 18.70% 16.43% 14.36% 9.07%
[5,10) 18.87% 24.91% 19.41% 14.11% 8.30%
[10,20) 34.26% 35.42% 33.94% 30.68% 17.23%
20? 30.87% 20.97% 30.22% 40.85% 65.40%
Table 10.  Length Distribution of Frames and OS 
 
Table 10 shows, about 70% of frames have less 
than 20 words, especially 80% of MMMP frame 
has less than 20 words, but about 65% of OS has 
more than 20 words. 
 BMMP MMMP BMSR MMSR OS 
phrase 3.07 3.83 2.53 3.44 30.07
frame 16.00 13.21 19.16 15.79 30.07
Table 11. Average Length 
 
We count phrases, frame and Original Sentence 
(OS) Average Length (AL) in training set and dev 
set. Table 11 shows phrase of AL accounted for 
10% of OS of AL, and frame of AL accounted for 
50% of OS of AL. The AL shows that the semantic 
dependency paring unit length of OS is greatly re-
duced after dividing an original sentence into SR 
phrases and frame.  
As shown in tables 9, 10 and 11, the length dis-
tribution indicates that the divide-and-conquer 
strategy reduces the complexity of sentences sig-
nificantly. 
4 Semantic Dependency Parsing  
Graph-based parser is selected as our basic seman-
tic dependency parser. It views the semantic de-
pendency parsing as problem of finding maximum 
spanning trees (McDonald, 2006) in directed 
graphs. In this paper, phrase and frame semantic 
dependency parsing result was obtained by Graph-
based parser. Training set of phrase comes from 
phrases, and training set of frame comes from 
frames. 
5 Experiments  
5.1 Direction of Identification  
511
Dependency direction serves as feature of SR 
phrase identification, so we need to identify de-
pendency direction of word. We use tag set is {B, 
F}, B represents backward dependence, F repre-
sents forward dependence. The root?s dependency 
direction in sentence is B. Dependency direction 
identification p-score has reached 94.87%. 
Word Unigrams W-4, W-3, W-2, W-1, W0, W+1,  
W+ 2, W+ 3, W+ 4
Word Bigrams W-3W-2, W-2W-1, W-1W0, W0W+1, 
W+1W+2, W+2W+3
Word Trigrams W-1W 0W+1
Word Four-grams W-2W-1W0 W +1, W0W+1W+2W+3
Word Five-grams W- 4W-3W-2W-1W0,  
W0W+1W+2W+3W+ 4
POS Unigrams P-4, P-3, P-2, P-1, P0, P+1, P+2, P+3, P+ 4
POS Bigrams P-3P-2, P-2P-1, P-1P0, P0P+1, 
 P+1P+2, P +2P+3
POS Trigrams P-1P0P+1
POS Four-grams P-2P-1P0P+1, P0P+1P+2P+3
POS Five-grams P-4P-3P-2P-1P0, P0P+1P+2P+3P+4
Word & POS W-2 P-2, W-1P-1, W0P0, W+1P+1, 
W+2P+2
Table 12.  Feature Templates of Dependency Direction 
In table12, w represents word, p represents POS. 
5.2 System and Model  
For a sentence for which phrases has been identi-
fied, if phrases can be identified, then the whole 
sentence semantic dependency parsing result is 
obtained by phrase parsing model and frame pars-
ing model. Therefore, in this paper, the sentence is 
divided into the following types based on the 
phrase identification results: (1) SentMMMP indi-
cates MMSR phrase and MPSR phrase identified 
in a sentence; (2) SentBMMP indicates BMSR 
phrase and MPSR phrase identified in a sentence; 
(3) SentMMSR indicates only MMSR phrase iden-
tified in a sentence; (4) SentMPSR indicates only 
MPSR phrase identified in a sentence; (5) 
SentBMSR indicates only BMSR phrase identified 
in a sentence; (6) SentNone indicates no phrase 
identified in a sentence. 
Sentence type Phrase parsing Model 
Frame parsing
Model 
SentMMMP MMMP phrase MMMP frame
SentBMMP BMMP phrase BMMP frame
SentMMSR MMSR phrase MMSR frame
SentMPSR MPSR phrase MPSR frame 
SentBMSR BMSR phrase BMSR frame
SentNone Sentence model 
Table 13.  Type of Sentence and Parsing Model 
Table 13 shows types of sentence, and parsing 
models for every type of sentence. For example, 
parsing SentMMMP needs MMMP phrase parsing 
model and MMMP frame paring model 
The corpus contains the sentence type deter-
mined by the phrase identification strategy. 
Strategy of phrase 
identification Sentence type in the corpus
Strategy MMMP SentMMMP, SentMMSR, SentMPSR, SentNone 
Strategy BMMP SentBMMP, SentMPSR, SentBMSR, SentNone 
Strategy BMSR SentBMSR, SentNone 
Table 14.  Sentence Types in the Corpus 
 
As shown in table 14, Strategy MMMP indicates 
that MMMP phrase in the corpus was identified, 
and sentences in the corpus were divided into 
SentMMMP, SentMMSR, SentMPSR and Sent-
None. Strategy BMMP indicates that BMMP 
phrase in the corpus was identified, and sentences 
in the corpus were divided into SentBMMP, 
SentBMSR, SentMPSR and SentNone. Strategy 
BMSR indicates that BMSR phrase in the corpus 
was identified, and sentences in the corpus were 
divided into SentBMSR and SentNone. 
5.3 Comparative Experiments  
In this paper, we carry out comparative experi-
ments of parsing for the test set by 3 systems. 
1) System1 represents strategy MMMP in the 
table 14. 
2) System2 represents strategy BMMP in the ta-
ble 14. 
3) System3 represents strategy BMSR in the table 
14. 
 Dev Test 
G-parser 62.31% 61.68% 
System1(MMMP) 61.98% 61.84% 
System2(BMMP) 62.7% 62.08% 
System3(BMSR) 62.22% 61.15% 
Table 15.  Comparative Experiments 
 
As shown in the table 15, system2 result is more 
accurate than system1, because BMMP phrase 
identification is more accurate than MMMP as 
shown in the table 8. Although, BMSR phrase 
identification is more accurate than MMMP phrase 
as shown in the table 5 & 8, system 3 result is less 
accurate than systm1. Compared with BMSR iden-
512
tification, MMMP identification reduces the com-
plexity of sentences significantly, because the table 
11 shows that the AL of MMMP frame is about 
30% less than that of BMSR frame. G-parser is 
graph-based parser (Wangxiang Che, 2008). 
6 Conclusion and Future Work  
To solve the problem of low accuracy of long dis-
tance dependency parsing, this paper proposes a 
divide-and-conquer strategy for semantic depend-
ency parsing. We present our SemEval2012 shared 
Task 5 system which is composed of three cas-
caded components: the tagging of SR phrase, the 
identification of Semantic-role- phrase and seman-
tic dependency parsing.  
Divide-and-conquer strategy is influenced by 
two factors: one is identifying the type of phrase 
will greatly reduce the sentence complexity; the 
other is phrase identifying precision results in cas-
caded errors. The topic of this evaluation is seman-
tic dependency parsing, and word and POS contain 
less semantic information. If we can make seman-
tic label on words, then it will be more helpful for 
semantic dependency parsing. In the future, we 
will study how to solve the long distance depend-
ency parsing problem. 
Acknowledgments 
The authors would like to thank the reviewers for 
their helpful comments. This work was supported 
by National Natural Science Foundation of China 
(NSFC) via grant 61073123 and Natural Science 
Foundation of Liaoning province via grant 
20102174. 
References  
Dongfeng Cai, Ling Zhang, Qiaoli Zhou and Yue Zhao. 
A Collocation Based Approach for Prepositional 
Phrase Identification. IEEE NLPKE, 2011. 
McDonald, Ryan. 2006. Discriminative Learning and 
Spanning Tree Algorithms for Dependency Parsing. 
Ph.D. thesis, University of Pennsylvania. 
Guiping Zhang, Wenjing Lang, Qiaoli Zhou and Dong-
feng Cai. 2010. Identification of Maximal-Length 
Noun Phrases Based on Maximal-Length Preposition 
Phrases in Chinese, 2010 International Conference 
on Asian Language Processing, pages 65-68. 
Qiaoli Zhou, Wenjing Lang, Yingying Wang, Yan 
Wang, Dongfeng Cai. 2010.  The SAU Report for the 
1st CIPS-SIGHAN-ParsEval-2010, Proceedings of 
the First CIPS-SIGHAN Joint Conference on Chi-
nese Language Processing, pp:304-311. 
Wanxiang Che, Zhenghua Li, Yuxuan Hu, Yongqiang 
Li,Bing Qin, Ting Liu, and Sheng Li. 2008. A cas-
caded syntactic and semantic dependency parsing 
system. In CoNLL-2008. 
513
Proceedings of the Workshop on Language in Social Media (LSM 2011), pages 66?75,
Portland, Oregon, 23 June 2011. c?2011 Association for Computational Linguistics
Why is ?SXSW? trending? Exploring Multiple Text Sources for
Twitter Topic Summarization
Fei Liu1 Yang Liu1 Fuliang Weng2
1Computer Science Department, The University of Texas at Dallas
2Research and Technology Center, Robert Bosch LLC
{feiliu, yangl}@hlt.utdallas.edu1
fuliang.weng@us.bosch.com2
Abstract
User-contributed content is creating a surge on
the Internet. A list of ?buzzing topics? can
effectively monitor the surge and lead people
to their topics of interest. Yet a topic phrase
alone, such as ?SXSW?, can rarely present
the information clearly. In this paper, we
propose to explore a variety of text sources
for summarizing the Twitter topics, includ-
ing the tweets, normalized tweets via a ded-
icated tweet normalization system, web con-
tents linked from the tweets, as well as inte-
gration of different text sources. We employ
the concept-based optimization framework for
topic summarization, and conduct both au-
tomatic and human evaluation regarding the
summary quality. Performance differences are
observed for different input sources and types
of topics. We also provide a comprehensive
analysis regarding the task challenges.
1 Introduction
User contributed content has become a major source
of information in the Web 2.0 era. People follow
their topics of interest, share their experience or
opinions on a variety of interactive platforms, in-
cluding forums, blogs, microblogs, social network-
ing sites, etc. To keep track of the trends online
and suggest topics of interest to the general public,
many leading websites provide a ?buzzing? service
by publishing the current most popular topics on
their entrance page and update them regularly, such
as the ?popular now? column on Bing.com, ?trend-
ing topics? on Twitter.com, ?trending now? on Ya-
hoo.com, Google Trends, and so forth. Often pop-
ular topics are in the form of a list of keywords or
phrases1. Take Twitter.com as an example. Clicking
on a trending topic phrase will return a set of relevant
Twitter posts (tweets) or web pages. Nonetheless,
whether this is a convenient way for users to navi-
gate through the popular topic information is still ar-
guable. For example, when ?SXSW? was listed as a
trending topic, it seems difficult to understand at the
first glance. A condensed topic summary would be
extremely helpful for the users before diving into the
massive search results to figure out what this topic
phrase is about and why it is trending. In this paper,
our goal is to generate a short text summary for any
given topic phrase. Note that the proposed approach
is not limited to trending topics, but can be applied
to arbitrary Twitter topics.
There are a lot of differences between tweets and
traditional written text that has been widely used
for automatic summarization. In Table 1, we show
example tweets for the topic ?SXSW?. The tweets
were extracted by searching the Twitter site using
the topic phrase as a query. We also provide an ex-
cerpt of the linked web content to help understand
the topic. The tweets present some unique charac-
teristics:
? All tweets are limited to 140 characters. Some
tweets are news headlines from the official me-
dia, others are generated by users with vari-
ous degrees of familiarity with the social me-
dia. The resulting tweets can be very different
regarding the text quality and word usage.
1They are referred to as topic phrases hereafter, with no dis-
tinction between keywords and key phrases.
66
Twitter Topic: ?SXSW?
Twts
I wish I could go to SXSW... I will, one day!
http://sxsw.com/
RT @user123: SXSW Film
Round-Up: Documentaries http://bit.ly/fg033b
@user456 yo.whats good,i met u at sxsw, talkin
bout that feature.I was gonna see about sending
u a few beats.u lookin for only original?
The South by Southwest (SXSW) Conferences
Web & Festivals offer the unique convergence of
Cont original music, independent films, and
emerging technologies...(http://sxsw.com/)
Table 1: Example tweets and an excerpt of the linked web
content for Twitter topic ?SXSW?.
? Tweets lack structure information, contain var-
ious ill-formed sentences and grammatical er-
rors. There are lots of noisy nonstandard to-
kens, such as abbreviations (?feelin? for ?feel-
ing?), substitutions (?Pr1mr0se? for ?Prim-
rose?), emoticons, etc.
? Twitter invented its own markup language.
?@user? is used to reply to a specific user or
call for attentions. The hashtag ?#topic? aims
to assign a topic label to the tweet, and is fre-
quently employed by the twitter users.
? Tweets frequently contain embedded URLs
that direct users to other online content, such
as news web pages, blogs, organization home-
pages (Wu et al, 2011). According to Twitter?s
news release in September 2010 (Rao, 2010),
25% of tweets contain an URL. These linked
web pages provide a much richer source of in-
formation than is possible in the 140-character
tweet.
These Twitter-specific characteristics may pose
challenges to the automatic summarization systems
for identifying the essential information. In this pa-
per, we focus on two such characteristics that are
not studied in previous literature, the web content
link and the non-standard tokens in tweets. Specif-
ically, we ask two questions: (1) Is the web content
linked from the tweets useful for summarization?
Can we integrate different text sources, including
the tweets and linked web pages, to generate more
informative Twitter topic summaries? (2) what is
the effect of nonstandard tokens on summarization
performance? Will the summaries be improved if
the noisy tweets were pre-normalized into standard
English sentences? We investigate these two ques-
tions under a concept-based summarization frame-
work using integer linear programming (ILP). We
utilize text input that has various quality and is orig-
inated from multiple sources, and thoroughly ana-
lyze the resulting summaries using both automatic
and human evaluation metrics.
2 Related Work
There is not much previous work on summarizing
the Twitter topics. Most previous summarization lit-
erature focused on the written text domain, as driven
by the annual evaluation tracks of the DUC (Doc-
ument Understanding Conference) and TAC (Text
Analysis Conference). To some extent, Twitter topic
summarization is related to spoken document sum-
marization, since both tasks deal with the conver-
sational text that is contributed by multiple par-
ticipants and contains lots of ill-formed sentences,
colloquial expressions, nonstandard word tokens or
high word error rate, etc. To summarize the spo-
ken text, (Zechner, 2002) aimed to address prob-
lems related to disfluencies, extraction units, cross-
speaker coherence, etc. (Maskey and Hirschberg,
2005; Murray et al, 2006; Galley, 2006; Xie et
al., 2008; Liu and Liu, 2010a) incorporated lexical,
structural, speaker, and discourse cues to generate
textual summaries for broadcast news and meeting
conversations.
For microblog summarization, (Sharifi et al,
2010a) proposed a phrase reinforcement (PR) algo-
rithm to summarize the Twitter topic in one sen-
tence. The algorithm builds a word graph using the
topic phrase as the root node; each word node is
weighted in proportion to its distance to the root and
the corresponding phrase frequency. The summary
sentence is selected as one of the highest weighted
paths in the graph. (Sharifi et al, 2010b; Inouye,
2010) introduced a hybrid TF-IDF approach to ex-
tract one- or multiple-sentence summary for each
topic. Sentences were ranked according to the av-
erage TF-IDF score of the consisting words; top
weighted sentences were iteratively extracted, but
excluding those that have high cosine similarity with
the existing summary sentences. They showed the
Hybrid TF-IDF approach performs constantly bet-
67
ter than the PR algorithm and other traditional sum-
marization systems. Our approach of summarizing
the Twitter topics is different from the above stud-
ies in that, we focus on exploring richer informa-
tion sources (such as the online web content) and in-
vestigating effect of non-standard tokens. There are
also studies working on visualizing Twitter topics
by identifying a set of topic phrases and presenting
the related tweets to users (O?Connor et al, 2010;
Marcus et al, 2011). Our proposed approach can be
beneficial to these systems by providing informative
topic summaries generated from rich text sources.
3 Data Collection
We collected 5,537 topic phrases and the reference
topic descriptions by crawling the Twitter.com and
WhatTheTrend.com simultaneously during the pe-
riod of Aug 22th, 2010 to Oct 30th, 2010 (about 70
days). The Twitter API was queried every 5 min-
utes for the current top ten trending topics. For each
of these topics, a search query was submitted to the
Twitter Search API to retrieve only English tweets
related to this topic. If any tweet contains embedded
URLs linked to the other web pages, the contents
of these web pages were retrieved. For each topic,
we limit the maximum number of retrieved tweets to
5,000 and webpages to 100. An example is shown in
Table 1 for a topic phrase, some related tweets, and
an excerpt of the linked webpage. WhatTheTrend
API provides short topic descriptions contributed
and constantly updated by the Twitter users. There
is also a manually assigned category tag for each
topic phrase. We found the top categories among
the collected topics are ?Entertainment (29.26%)?,
?Sports (25.58%)?, and ?Meme (15.69%, pointless
babble)?. We divided the collected topics into two
groups: the general topics (e.g., ?Chilean miners?,
?MTV VMA?) and the hashtag topics that start with
the ?#? (e.g., ?#top10rappers?, ?#octoberwish?).
To generate reference summaries for the Twit-
ter topics, two human annotators were asked to
pick the topic descriptions/sentences (collected from
WhatTheTrend.com) that are appropriate and valu-
able to be included in the summary. This is per-
formed on a selected set of 1,511 topics with both
trending duration and number of tweets greater than
our predefined thresholds. For each of the topic sen-
tences, we ask the annotators to label its category:
(1) the sentence is a general description of the topic;
(2) the sentence is trying to explain why the topic is
trending; (3) it is hard to tell the difference. Over-
all, the two annotators have good agreement (Kappa
= 0.67) regarding whether or not to include a sen-
tence in the summary. Among the selected summary
sentences, 22.58% of them were assigned with con-
flicting purpose tags such as (1) or (2). To form
a reference summary, we concatenate all the topic
sentences selected by both annotators. Since some
reference descriptions are simply repetition of oth-
ers with very minor changes, we reduce the dupli-
cates by iteratively removing the oldest sentences if
all the consisting words are covered by the remain-
ing sentence collection, until no sentence can be re-
moved. On average, the reference summary for gen-
eral and hashtag topics contains 44 and 40 words
respectively.
4 Summarization System
For each of the topic phrases, our goal is to gener-
ate a short textual summary that can best convey the
main ideas of the topic contents. We explore and
compare multiple text sources as summarization in-
put, including the user-contributed tweets, web con-
tents linked from the tweets, as well as combination
of the two sources. The concept-based optimization
approach (Gillick et al, 2009; Xie et al, 2009; Mur-
ray et al, 2010) was employed for selecting informa-
tive summary sentences and minimizing the redun-
dancy. Note that our focus of this paper is not devel-
oping new summarization systems, but rather utiliz-
ing and integrating different text sources for gener-
ating more informative Twitter topic summaries.
4.1 Concept-based Optimization Framework
Concept-based summarization approach first ex-
tracts a set of important concepts for each topic, then
selects a collection of sentences that can cover as
many important concepts as possible, while within
the specified length limit. This idea is realized us-
ing the integer linear programming-based (ILP) op-
timization framework, with objective function set to
maximize the sum of the weighted concepts:
max
?
i
wici
68
where ci is a binary variable indicating whether the
concept i is covered by the summary; wi is the
weight assigned to ci.
We enforce two sets of length constraints to the
summary: sentence- or word-based. Sentence con-
straint requires the total number of selected sum-
mary sentences to not to exceed a length limit L1;
while word constraint requires the total words of
selected sentences not to exceed length limit L2.
These two constraints are:
?
j
sj < L1 or
?
j
ljsj < L2
where sj is a binary variable indicating whether sen-
tence j was selected in the summary; lj represents
the number of words in sj .
Further, we connect concept i with sentence j us-
ing two sets of constraints. For all the sentences that
contain concept i, if any sentence was selected in
the summary, the concept i should be covered by the
summary; reversely, if concept i was covered by the
summary, at least one of the sentences containing
concept i should be selected.
?i ci ?
?
j
oijsj
?i, j ci ? oijsj
where the binary variable oij is used to indicate
whether concept i exists in sentence j.
The concepts are selected by extracting n-grams
(n=1, 2, 3) from the input documents corresponding
to each topic. Similar to (Xie et al, 2009), we re-
move (1) n-grams that appear only once in the docu-
ments; (2) n-grams that have a consisting word with
inverse document frequency (IDF) value lower than
a threshold; (3) n-grams that are enclosed by higher
order n-grams with the same frequency. These fil-
ters are designed to exclude insignificant n-grams
from the concept set. The IDF scores were calcu-
lated from a large background corpus corresponding
to the input text source, using individual sentences
or tweets as pseudo-documents; words with low IDF
scores (such as stopwords) tend to appear in many
sentences and therefore should be removed from the
concept set. We assign a weight wi to an n-gram
concept as follows:
wi = tf(ngrami)? n?max
j
idf(wij)
where tf(ngrami) is the term frequency of ngrami
in the input document of the topic; n denotes the
order of ngrami; wij are the consisting words of
ngrami; idf(wij) represents IDF value of word
wij . This approach aims to extract n-grams that ap-
pear frequently in each topic, but do not appear fre-
quently in a large background corpus. The weights
are also biased towards longer n-grams since they
carry more information.
4.2 Summarization Input
In this section, we explore different text sources
as input to the summarization system. Different
from previous studies that take input from a sin-
gle text source, we propose to utilize both the
user-contributed tweets and the linked web con-
tents for Twitter topic summarization, since these
two sources provide very different text quality and
may contain complementary information regarding
the topic. These text sources also pose great chal-
lenges to the summarization system: the tweets are
short and extremely noisy; while the online contents
linked from the tweets may have vastly different lay-
outs and contain a variety of information.
4.2.1 Original Tweets
As shown in Table 1, the initially collected tweets
are very noisy. They are passed through a set of pre-
processors to remove non-ascii characters, HTML
special characters, URLs, emoticons, punctuation
marks, retweet tags (RT @user), etc. We also re-
move the reply (@) and hashtag (#) tokens that do
not carry important syntactic roles (such as in the
subject or object position) by using a set of regular
expressions. These preprocessed tweets are sorted
by date and taken as the first input source to the sum-
marization system (denoted by ?OrigTweets?).
4.2.2 Normalized Tweets
The original tweets contain various nonstandard
word tokens. In Table 2, we list the possible to-
ken categories and corresponding examples. We hy-
pothesize that normalizing these nonstandard tokens
into standard English words and using the normal-
ized tweets as input can help boost the summariza-
tion performance.
We developed a twitter message normalization
system based on the noisy-channel framework and
a proposed letter transformation model (Liu et al,
69
Category Example
(1) abbreviation tgthr, weeknd, shudnt
(2) phonetic sub w/- or w/o digit 4got, sumbody, kulture
(3) graphemic sub w/- or w/o digit t0gether, h3r3, 5top, doinq
(4) typographic error thimg, macam
(5) stylistic variation betta, hubbie, cutie
(6) letter repetition pleeeaas, togtherrr
(7) any combination of (1) to (6) luvvvin, 2moro, m0rnin
Table 2: Nonstandard token categories and examples.
2011). Given a noisy tweet T , our goal is to nor-
malize it into a standard English word sequence S.
Under the noisy channel model, this is equivalent to
finding the sequence S? that maximizes p(S|T ):
S? = argmaxS p(S|T ) = argmaxS(
?
i
p(Ti|Si))p(S)
where we assume that each non-standard token Ti
is dependent on only one English word Si, that is,
we are not considering acronyms (e.g., ?bbl? for ?be
back later?) in this study. p(S) can be calculated
using a language model (LM). We formulate the
process of generating a nonstandard token Ti from
dictionary word Si using a letter transformation
model, and use the model confidence as the prob-
ability p(Ti|Si). This transformation process will be
learned automatically through a sequence labeling
framework. To form a nonstandard token, each let-
ter in the dictionary word can be labeled with: (a)
one of the 0-9 digits; (b) one of the 26 characters
including itself; (c) the null character ?-?; (d) a let-
ter combination. We integrate character-, phonetic-,
and syllable-level features in the model that can ef-
fectively characterize the formation process of non-
standard tokens. In general, the letter transforma-
tion approach will handle the nonstandard tokens
listed in Table 2 yet without explicitly categorizing
them. The proposed system also achieved robust
performance using the automatically collected train-
ing word pairs. On a test set of 3,802 distinct non-
standard tokens collected from Twitter, our system
achieved 68.88% 1-best normalization word accu-
racy and 78.27% 3-best accuracy.
We identify the nonstandard tokens that need to
be normalized using the following criteria: (1) it is
not in the CMU dictionary2; (2) it does not contain
capitalized letter; (3) it appears infrequently in the
2http://www.speech.cs.cmu.edu/cgi-bin/cmudict
topic (less than a threshold); (4) it is not a popular
chat acronyms (such as ?lol?, ?omg?); (5) it contains
letters/digits/apostrophe, but should not be numbers
only. These criteria are designed to avoid normaliz-
ing the named entities, frequently appearing out-of-
vocabulary terms (such as ?itunes?), chat acronyms,
usernames, and hashtags. The selected nonstandard
tokens in the original tweets will be replaced by the
system generated 1-best candidate word. Note that
we do not discriminate the context when replacing
each nonstandard token. This will be addressed in
the future work. We use these normalized tweets as
a second source of summarization input and name
them ?NormTweets?.
4.2.3 Linked Web Contents
For each Twitter topic, we collect a set of web
pages linked by the topic tweets and use them as
another source of summarization input. For each
topic, we select up to n (n = 10) URLs that appear
most frequently in the topic tweets and infrequently
across different Twitter topics. This scheme is sim-
ilar to the TF-IDF measure. This way we can se-
lect the salient URLs for each topic while avoiding
the spam URLs. The contents of these URLs were
collected and only distinct web pages were retained.
We use an HTML parser3 to extract the textual con-
tents, and perform sentence segmentation (Reynar
and Ratnaparkhi, 1997) on the parsed web pages.
All the pages corresponding to the same topic were
sorted by the date they were first cited in the tweets.
These web pages were taken as another input text
source for the summarization system, denoted as
?Web?.
4.2.4 Combining Tweets and Web Contents
We expect that taking advantage of both tweets
and linked web contents would benefit the topic
summarization system. Consolidating the distinct
text sources may help boost the weight of key con-
cepts and eliminate the spam information. As a pre-
liminary study, we investigate concatenating either
the original tweets or the normalized tweets with
the linked web pages as input to the concept-based
summarization system. This results in two inputs
?Web + OrigTweets? and ?Web + NormTweets?. We
will explore other ways of combining the two text
3http://jericho.htmlparser.net/docs/index.html
70
sources in future work.
5 Experiments
5.1 Experimental Setup
Among the collected topics, we select 500 general
topics (such as ?Chilean miners?) and 50 hashtag
topics (such as ?#octoberwish?, ?#wheniwasakid?)
for experimentation. On average, a general topic
contains 1673 tweets and 3.43 extracted linked web
pages; while a hashtag topic contains 3316 tweets
but does not have meaningful linked web pages.
The concept-based optimization system was con-
figured to extract a collection of sentences/tweets
for each topic, using either the sentence- or word-
constraint (denoted as ?#Sent? and ?#Word?). We
opt to set individual length constraint for each topic
rather than using a uniform length limit for all the
topics, since the topics can be very different in
length and duration. We use the number of sen-
tences/words in the reference summary as the sen-
tence/word constraint for each topic. Note that in
practice this reference summary length information
may not be available. We use the length constraints
obtained from the reference summary in this ex-
ploratory study, since our focus is to first evaluate if
twitter trending summarization is feasible, and what
are the effects of different information sources and
non-standard tokens. For a comparison to our ap-
proach, we implement the Hybrid TF-IDF approach
in (Sharifi et al, 2010b; Inouye, 2010) as a baseline
using ?OrigTweets? as input. For the baseline, the
summary length is altered according to the sentence-
or word-constraint. The last summary tweet is cut in
the middle if it exceeds the word limit.
The ROUGE-1 F-scores (Lin, 2004) are used to
measure the n-gram (n=1) overlap between the sys-
tem summaries and reference summaries. Since the
ROUGE scores may not correlate well with the hu-
man judgments (Liu and Liu, 2010b), we also per-
formed human evaluation by asking annotators to
score both the system and reference summaries re-
garding the linguistic quality and content respon-
siveness, in the hope this will benefit future research
in this direction.
5.2 Automatic Evaluation
We present the results (ROUGE-1 F-measure) for
the general topics in Table 3. ROUGE-2 and
General Topics R-1 F(%) RefSum
Input Source Render #Sent #Word Cov(%)
OrigTweets
Orig 29.53 30.21 94.81
Norm 29.41 30.21 94.81
NormTweets Norm 29.69 30.35 94.60
Web 24.32 25.07 63.74
Web + OrigTweets 29.58 30.44 95.37
Web + NormTweets 29.66 30.54 95.16
OrigTweets
(Sharifi et al, 2010b) 24.37 25.68 94.81
Table 3: ROUGE-1 F-measure and reference summary
coverage scores for general topics.
ROUGE-4 scores show similar trends and thus are
not presented. Five different text sources were ex-
ploited as the system inputs, as described in Sec-
tion 4.2. To measure the quality of the input for
summarization, we also include reference summary
coverage score in the table, defined as the percent-
age of words in the reference summary that are cov-
ered by the input text source. When using tweets
as input, we also investigate whether we should ap-
ply tweet normalization before or after the summa-
rization process, that is ?pre-normalization? (using
?NormTweets? as input), or ?post-normalization?
(using ?OrigTweets? as input, and rendering the nor-
malized summary tweets).
Compared to the Hybrid TF-IDF approach (Shar-
ifi et al, 2010b; Inouye, 2010), our system per-
forms significantly better (p < 0.05) according
to the paired t-test; however, we also notice the
ROUGE scores are lower compared to summariza-
tion in other text domains. This indicates that Twit-
ter topic summarization is very challenging. Com-
paring the two constraints used in the concept-based
optimization framework, we found that the word
constraint performs constantly better for the gen-
eral topics. This is natural since the word constraint
tightly bounds the length of the system output, while
the sentence constraint is relatively loose. For the
different sources, we notice using linked web pages
alone yields worse summarization performance, as
well as lower reference summary coverage; how-
ever, when combined with the tweets, there is a
slight increase in the coverage scores, and some-
times improved summarization results. This sug-
gests that the linked web pages can contain extra
71
useful information for generating summaries. Re-
garding normalization, results show that the ?pre-
normalization? (using normalized tweets as input)
can generally improve the summary tweet selec-
tion. For general topics, the best performance was
achieved by combining the normalized tweets and
linked web pages as input source and using the
word-level constraint.
Hashtag Topics R-1 F(%) RefSum
Input Source Render #Sent #Word Cov(%)
OrigTweets
Orig 9.08 7.19 93.93
Norm 9.09 7.16 93.93
NormTweets Norm 9.35 7.14 93.71
OrigTweets
(Sharifi et al, 2010b) 7.03 7.72 93.93
Table 4: ROUGE-1 F-measure and reference summary
coverage scores for hashtag topics.
Results for hashtag topics were shown in Table
4 using tweets as input (there are no linked web-
pages for these topics). We notice the reference cov-
erage scores are satisfying, yet the system output
barely matches the reference summaries (very low
ROUGE-1 scores). Looking at the reference and
system generated summaries for the hashtag top-
ics, we found the system output is more specific
(e.g., ?#octoberwish everything goes well.?), while
the reference summaries are often very general (e.g.,
?people tweeting about their wishes for October.?).
The human annotators also noted that most hashtag
topics (such as ?#octoberwish?, ?#wheniwasakid?)
are self-explainable and may require special atten-
tion to redefine an appropriate summary. Using
sentence constraints yields better performance than
word-based one, with larger performance difference
than that for the general topics. We found the
word-constraint summaries tend to include tweets
that are very short and noisy. Our system with
sentence-based length constraint also significantly
outperforms the Hybrid TF-IDF approach (Sharifi
et al, 2010b; Inouye, 2010). For hashtag topics,
the best performance was achieved using the ?pre-
normalization? with sentence constraint.
For an analysis, we generate oracle system per-
formance by using the reference summaries to ex-
tract a set of unweighted concepts to use in the ILP
optimization framework for sentences/tweets selec-
tion. This results in 61.76% ROUGE-1 F-score for
the general topics and 40.34% for the hashtag topics,
indicating abundant space for future improvement.
We also notice that though there is some perfor-
mance gain using normalized tweets and linked web
contents, the improvement is not statistically signifi-
cant as compared to using the original tweets. Upon
closer examination, we found the normalization sys-
tem replaced 1.08% and 1.8% of the total word to-
kens for the general and hashtag topics respectively;
these tokens spread in 13.12% and 16.85% of the
total tweets. The relatively small percentage of the
normalized tokens partly explains the marginal per-
formance gain when using the normalized tweets as
input. Similarly for linked web content, though it
contains some sentences that can provide more de-
tails of the topic, but they can also take more space
in the summary as compared to the short and con-
densed tweets. Therefore using the combined tweets
and linked webpages does not significantly outper-
form using just the tweets.
5.3 Human Evaluation
General Hashtag
Tweet Web Ref Tweet Ref
Gram. 3.13 3.42 4.52 3.04 4.24
NRedun. 3.93 4.64 4.30 4.82 3.62
Clarity 4.07 3.91 4.77 4.06 4.60
Focus 3.64 3.03 4.75 3.22 4.72
Content 2.82 2.55 n/a 2.60 n/a
ExtraInfo n/a 2.63 n/a n/a n/a
Table 5: Linguistic quality, content coverage, and useful-
ness scores judged by human assessors.
We ask two human annotators to manually evalu-
ate the system and reference summaries regarding
the readability and content coverage. Readability
includes grammaticality, non-redundancy, referen-
tial clarity, and focus; content coverage was eval-
uated for system summaries against the reference
summary. The annotators were also asked to rate
the ?Web? summaries regarding whether they pro-
vided extra useful topic information on top of the
?Tweet? summary. 50 general topics and 25 hash-
tag topics were randomly selected for assessment.
The ?Tweet? and ?Web? summaries were generated
using the original tweets and linked web pages with
word constraint for general topics, and sentence con-
straint for hashtag topics. Each of the assessors was
72
General Topic: ?3PAR?
RefSum
Dell Inc. and Hewlett-Packard Co. are both bidding for storage device maker 3Par Inc.
3Par jumped 21 percent after Hewlett- Packard Co. offered $30 a share for the company.
TweetSum
Dell ups 3Par offer yet again, to $27 per share
Dell Raises 3par Offer to Match HP Bid
Dell Matches HP?s Offer for 3Par, Boosting Bid to $1.8 Billion
WebSum
Dell Matches HP?s $27 Offer, Is Accepted by 3PAR.
3PAR has accepted an increased acquisition offer from Dell of US$27 per share, matching
Hewlett-Packard?s earlier raised bid.
Hashtag Topic: ?#wheniwasakid?
RefSum
when i was a kid.... people are sharing there best (good or bad) memories from childhood.
People reminise the wonderful times about being a kid.
TweetSum
#whenIwasakid getting wasted meant eating all the ice cream and candy you could until you puked!
#whenIWasAKid Apple & Blackberry were fruits not phones.
Table 6: Example system and reference summaries for both general and hashtag topics.
asked to judge all the summaries and assign a score
for each criterion on a 1 to 5 Likert scale (5 being
the best quality). The average scores of the two as-
sessors were presented in Table 5.
For general topics, the ?Web? summaries outper-
form the ?Tweet? summaries on both grammatical-
ity and non-redundancy, confirming the advantage
of using the high-quality linked web pages. The
referential clarity and focus scores of the ?Web?
summaries are not very high, since the summary
sentences were extracted simultaneously from sev-
eral web pages, and the system subjects to simi-
lar challenges as in multi-document summarization.
The content coverage scores of both system sum-
maries seem to correlate well with the ROUGE-1
F-measure, with a higher score for ?Tweet? sum-
maries. The assessors also rated that 48% of the
?Web? summaries contain ?Somewhat Useful? ex-
tra topic information, and 21% are ?Very Useful?.
Note that this could be just because of the inherent
difference of the two summaries, regardless of the
input source, but in general we believe the linked
web pages (such as the news documents) can pro-
vide more detailed and coherent stories as compared
to the 140-character tweets. For hashtag topics, the
?Tweet? summaries yield worse grammaticality and
focus scores, but have very high non-redundancy
score. On the contrary, the reference summaries
often contain redundant information. The content
match score between the system and reference sum-
maries (2.6) does not seem to reflect the ROUGE
scores. We hypothesize that even though the speci-
ficity of the two summaries is different, the asses-
sors may still think the system summaries match the
reference ones to some extent. A larger scale human
evaluation is needed to study the correlation between
human and automatic evaluation.
5.4 Discussions
We show an example of reference and system gen-
erated summaries for a general and a hashtag topic
in Table 6, and summarize some challenges for this
summarization task below:
? Gold standard summaries are difficult and
time-consuming to obtain. The reference de-
scriptions from WhatTheTrend.com were cre-
ated by Twitter users, which vary a lot in
word usage and would be unavoidably biased
to the information available in Twitter. The
user-contributed descriptions may also contain
spam descriptions, repetitions, nonstandard to-
kens, etc. It would be better to have a con-
cise non-redundant sentence collection for de-
veloping future summarization systems. In
particular, hashtag topics need special atten-
tion. They account for 40% of the total trend-
ing topics in 2010 according to the statistics
in WhatTheTrend.com4. Yet there still lacks
standard definition regarding a good hashtag
summary. From the example topic ?#wheni-
wasakid? in Table 6, we can see they are very
different in nature from general topics, thus fu-
ture efforts are needed to define an appropriate
summary.
4http://yearinreview.whatthetrend.com/
73
? Evaluation issues. Word based evaluation
measures will rarely consider semantic relat-
edness between concepts, or name entity vari-
ations, such as ?Hewlett-Packard? vs. ?HP?,
?Dell ups 3Par offer? vs. ?Dell Raises 3par
Offer?, etc. When comparing the system
summaries with short human-written reference
summaries, the word overlap varies a lot for
different human summarizers.
? Dynamically changing topics/events. Some
general topics are related to events that are con-
stantly changing. Take the ?3PAR? topic in
Table 6 as an example, where two companies
take turns to raise the bid for 3Par Inc. A good
topic summary should be able to develop a se-
ries of sub-events and show the topic evolving
process.
6 Conclusion
In this paper, we proposed to explore a variety of text
sources for summarizing the Twitter topics. We em-
ployed the concept-based optimization framework
with multiple input text sources to generate the sum-
maries. We conducted both automatic and human
evaluation regarding the summary quality. Better
performance is observed when using the normalized
tweets as input, indicating special treatment should
be performed before feeding the noisy tweets to the
summarization system. We also found the linked
web contents can provide extra useful topic infor-
mation. In future work, we will compare our sys-
tem with other dedicated microblog summarization
systems, as well as address some of the challenges
identified in this study.
Acknowledgments
This work is partly supported by NSF award IIS-
0845484. Any opinions expressed in this work are
those of the authors and do not necessarily reflect the
views of NSF.
References
Michel Galley. 2006. A skip-chain conditional random
field for ranking meeting utterances by importance. In
Proc. of EMNLP.
Dan Gillick, Korbinian Riedhammer, Benoit Favre, and
Dilek Hakkani-Tu?r. 2009. A global optimization
framework for meeting summarization. In Proc. of
ICASSP.
David Inouye. 2010. Multiple post microblog summa-
rization. REU Research Final Report.
Chin-Yew Lin. 2004. ROUGE: A package for automatic
evaluation of summaries. In Workshop on Text Sum-
marization Branches Out.
Fei Liu and Yang Liu. 2010a. Exploring speaker char-
acteristics for meeting summarization. In Proc. of IN-
TERSPEECH.
Feifan Liu and Yang Liu. 2010b. Exploring correlation
between ROUGE and human evaluation on meeting
summaries. IEEE Transactions on Audio, Speech, and
Language Processing, 18(1):187?196.
Fei Liu, Fuliang Weng, Bingqing Wang, and Yang Liu.
2011. Insertion, deletion, or substitution? Normaliz-
ing text messages without pre-categorization nor su-
pervision. In Proc. of ACL-HLT.
Adam Marcus, Michael S. Bernstein, Osama Badar,
David R. Karger, Samuel Madden, and Robert C.
Miller. 2011. TwitInfo: Aggregating and visualizing
microblogs for event exploration. In Proc. of CHI.
Sameer Maskey and Julia Hirschberg. 2005. Compar-
ing lexical, acoustic/prosodic, structural and discourse
features for speech summarization. In Proc. of Eu-
rospeech.
Gabriel Murray, Steve Renals, Jean Carletta, and Johanna
Moore. 2006. Incorporating speaker and discourse
features into speech summarization. In Proc. of HLT-
NAACL.
Gabriel Murray, Giuseppe Carenini, and Raymond Ng.
2010. Interpretation and transformation for abstract-
ing conversations. In Proc. of NAACL.
Brendan O?Connor, Michel Krieger, and David Ahn.
2010. Tweetmotif: Exploratory search and topic sum-
marization for twitter. In Proc. of the International
AAAI Conference on Weblogs and Social Media.
Leena Rao. 2010. Twitter seeing 90 mil-
lion tweets per day, 25 percent contain links.
http://techcrunch.com/2010/09/14/twitter-seeing-90-
million-tweets-per-day/.
Jeffrey C. Reynar and Adwait Ratnaparkhi. 1997. A
maximum entropy approach to identifying sentence
boundaries. In Proc. of the Fifth Conference on Ap-
plied Natural Language Processing.
Beaux Sharifi, Mark-Anthony Hutton, and Jugal Kalita.
2010a. Summarizing microblogs automatically. In
Proc. of HLT/NAACL.
Beaux Sharifi, Mark-Anthony Hutton, and Jugal K.
Kalita. 2010b. Experiments in microblog summariza-
tion. In Proc. of IEEE Second International Confer-
ence on Social Computing.
74
Shaomei Wu, Jake M. Hofman, Winter A. Mason, and
Duncan J. Watts. 2011. Who says what to whom on
twitter. In Proc. of WWW.
Shasha Xie, Yang Liu, and Hui Lin. 2008. Evaluating
the effectiveness of features and sampling in extractive
meeting summarization. In Proc. of IEEE Workshop
on Spoken Language Technology.
Shasha Xie, Benoit Favre, Dilek Hakkani-Tu?r, and Yang
Liu. 2009. Leveraging sentence weights in a concept-
based optimization framework for extractive meeting
summarization. In Proc. of INTERSPEECH.
Klaus Zechner. 2002. Automatic summarization of
open-domain multiparty dialogues in diverse genres.
Computational Linguistics, 28(4):447?485.
75
