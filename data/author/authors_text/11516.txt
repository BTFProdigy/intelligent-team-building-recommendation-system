Evaluating and Integrating Treebank Parsers on a Biomedical Corpus
Andrew B. Clegg and Adrian J. Shepherd
School of Crystallography
Birkbeck College
University of London
London WC1E 7HX, UK
{a.clegg,a.shepherd}@mail.cryst.bbk.ac.uk
Abstract
It is not clear a priori how well parsers
trained on the Penn Treebank will parse
significantly different corpora without
retraining. We carried out a compet-
itive evaluation of three leading tree-
bank parsers on an annotated corpus
from the human molecular biology do-
main, and on an extract from the Penn
Treebank for comparison, performing a
detailed analysis of the kinds of errors
each parser made, along with a quan-
titative comparison of syntax usage be-
tween the two corpora. Our results sug-
gest that these tools are becoming some-
what over-specialised on their training
domain at the expense of portability, but
also indicate that some of the errors en-
countered are of doubtful importance for
information extraction tasks.
Furthermore, our inital experiments
with unsupervised parse combination
techniques showed that integrating the
output of several parsers can ameliorate
some of the performance problems they
encounter on unfamiliar text, providing
accuracy and coverage improvements,
and a novel measure of trustworthiness.
Supplementary materials are available
at http://textmining.cryst.bbk.
ac.uk/acl05/.
1 Introduction
The availability of large-scale syntactically-
annotated corpora in general, and the Penn Tree-
bank1 (PTB; Marcus et al, 1994) in particular, has
enabled the field of stochastic parsing to advance
rapidly over the course of the last 10-15 years.
However, the newspaper English which makes up
the bulk of the PTB is only one of many dis-
tinct genres of writing in the Anglophone world,
and certainly not the only domain where poten-
tial natural-language processing (NLP) applica-
tions exist that would benefit from robust and re-
liable syntactic analysis. Due to the massive glut
of published literature, the biomedical sciences in
general, and molecular biology in particular, con-
stitute one such domain, and indeed much atten-
tion has been focused recently on NLP in this area
(Shatkay and Feldman, 2003; Cohen and Hunter,
2004).
Unfortunately, annotated corpora of a large
enough size to retrain stochastic parsers on do not
exist in this domain, and are unlikely to for some
time. This is partially due to the same differences
of vocabulary and usage that set biomedical En-
glish apart from the Wall Street Journal in the first
place; these differences necessitate the input of
both biological and linguistic knowledge on bio-
logical corpus annotation projects (Kulick et al,
2004), and thus require a wider variety of annota-
tor skills than general-English projects. For exam-
ple, 5? (pronounced ?five-prime?) is an adjective in
molecular biology, but p53 is a noun; amino acid
1http://www.cis.upenn.edu/~treebank/
is an adjective-noun sequence2 but cadmium chlo-
ride is a pair of nouns. These tagging decisions
would be hard to make correctly without biologi-
cal background knowledge, as would the preposi-
tional phrase attachment decisions in Figure 1.
Although it is intuitively apparent that there
are differences between newspaper English and
biomedical English, and that these differences
are quantifiable enough for biomedical writing
to be characterised as a sublanguage of En-
glish (Friedman et al, 2002), the performance of
conventionally-trained parsers on data from this
domain is to a large extent an open question.
Nonetheless, papers have begun to appear which
employ treebank parsers on biomedical text, es-
sentially untested (Xiao et al, 2005). Recently,
however, the GENIA project (Kim et al, 2003)
and the Mining the Bibliome project (Kulick et al,
2004) have begun producing small draft corpora of
biomedical journal paper abstracts with PTB-style
syntactic bracketing, as well as named-entity and
part-of-speech (POS) tags. These are not currently
on a scale appropriate for retraining parsers (com-
pare the ?50,000 words in the GENIA Treebank
to the ?1,000,000 in the PTB; but see also Sec-
tion 7.2) but can provide a sound basis for empiri-
cal performance evaluation and analysis. A collec-
tion of methods for performing such an analysis,
along with several interesting results and an inves-
tigation into techniques for narrowing the perfor-
mance gap, is presented here.
1.1 Motivation
We undertook this project with the intention of
addressing several questions. Firstly, in order to
deploy existing parsing technologies in a bioin-
formatics setting, the biomedical NLP commu-
nity needs a comprehensive assessment of perfor-
mance ? which parser(s) to choose, what accuracy
each should be expected to achieve etc., along with
information about the different situations in which
each parser can be expected to perform well or
poorly. Secondly, assuming there is a performance
deficit, can any simple steps be taken to mitigate
it? Thirdly, what engineering issues arise from the
2According to some annotators at least; others tag amino
as a noun, although one would not speak of *an amino, *some
amino or *several aminos.
idiosyncracies of biomedical text?
The differences discovered in the behaviour of
each parser, either between domains or between
different software versions on the same domain,
will also be of interest to those in the computa-
tional linguistics community who are involved in
parser design. These values will give a compara-
tive index of the flexibility of each parsing model
on being presented with out-of-domain data, and
may help parser developers to detect signs of over-
training or, analogously, ?over-design? for one nar-
row genre of English. It is hoped that our findings
can assist those better equipped than ourselves in
properly investigating these phenomena, and that
our analysis of the problems encountered can shed
new light on the thorny problem of parser evalua-
tion.
Finally, several questions arise from the use of
multiple parsers on the same corpus that are of
both theoretical and practical interest. Does agree-
ment between several parsers indicate that a sen-
tence has been parsed correctly, or do they tend to
make the same mistakes? How best can the output
of an ensemble of parsers be integrated, in order
to boost performance above that of the best sin-
gle member? And what additional information can
be gleaned from comparing the opinions of sev-
eral parsers that can help make sense of unfamiliar
text?
2 Evaluation methodologies
We initially chose to rate the parsers in our as-
sessment by several different means which can
be grouped into two broad classes: constituent-
and lineage-based. While Sampson and Babarczy
(2003) showed that there is a limited degree of cor-
relation between the per-sentence scores assigned
by the two methods, they are independent enough
that a fuller picture of parser competence can be
built up by combining them and thus sidestep-
ping the drawbacks of either approach. However,
overall performance scores designed for competi-
tively evaluating parsers do not provide much in-
sight into the aetiology of errors and anomalies, so
we developed a third approach based on produc-
tion rules that enabled us to mine the megabytes
of syntactic data for enlightening results more ef-
a. [ This protein ] [ binds the DNA [ by the TATA box [ on its minor groove. ]2 ]1 ]
b. [ This protein ] [ binds the DNA [ by the TATA box ]1 [ at its C-terminal domain. ]2 ]
Figure 1: These two sentences are biologically clear but syntactically ambiguous. Only the knowledge
that the C-terminal domain is part of a protein, whereas the TATA box and minor groove are parts of
DNA, allows a human to interpret them correctly, by attaching the prepositional phrases 1 and 2 at the
right level.
fectively. All the Perl scoring routines we wrote
are available from our website.
2.1 Constituent-based assessment
Most evaluations of parser performance are based
upon three primary measures: labelled constituent
precision and recall, and number of crossing
brackets per sentence. Calculation of these scores
for each sentence is straightforward. Each con-
stituent in a candidate parse is treated as a tu-
ple ?lbound,LABEL,rbound?, where lbound and
rbound are the indices of the first and last words
covered by the constituent. Precision is the pro-
portion of candidate constituents that are correct
and is calculated as follows:
P =
# true positives
# true positives + # false positives
Recall is the proportion of constituents from the
gold standard that are in the candidate parse:
R =
# true positives
# true positives + # false negatives
The crossing brackets score is reached by count-
ing the number of constituents in the candidate
parse that overlap with at least one constituent in
the gold standard, in such a way that one is not a
subsequence of the other.
Although this scoring system is in wide use, it
is not without its drawbacks. Most obviously, it
gives no credit for partial matches, for example
when a constituent in one parse covers most of
the same words as the other but is truncated or ex-
tended at one or both ends. Indeed, one can imag-
ine situations where a long constituent is truncated
at one end and extended at the other compared to
the gold standard; this would incur a penalty un-
der each of the above metrics even though some
or even most of the words in the constituent were
correctly categorised. One can of course suggest
modifications for these measures designed to ac-
count for particular situations like these, although
not without losing some of their elegance. The
same is true for label mismatches, where a con-
stituent?s boundaries are correct but its category is
wrong.
More fundamentally, it could be argued that by
taking as it were horizontal slices through the syn-
tax tree, these measures lose important informa-
tion about the ability of a parser to recreate the
gross grammatical structure of a sentence. The
height of a given constituent in the tree, and the
details of its ancestors and descendants, are not
directly taken into account, and it is surely the
case that these broader phenomena are at least as
important as the extents of individual constituents
in affecting meaning. However, constituent-based
measures are not without specific advantages too.
These include the ease with which they can be bro-
ken down into scores per label to give an impres-
sion of a parser?s performance on particular kinds
of constituent, and the straightforward message
they deliver about whether a badly-performing
parser is tending to over-generate (low precision),
under-generate (low recall) or mis-generate (high
crossing brackets).
2.2 Lineage-based assessment
In contrast to this horizontal-slice philosophy,
Sampson and Babarczy (2003) advocate a verti-
cal view of the syntax tree. By walking up the
tree structure from the immediate parent of a given
word until the top node is reached, and adding
each label encountered to the end of a list, a ?lin-
eage? representing the word?s ancestry can be re-
trieved. Boundary symbols are inserted into this
lineage before the highest constituent that begins
on the word, and after the highest constituent that
ends on the word, if such conditions apply; this al-
lows potential ambiguities to be avoided, so that
the tree as a whole has one and only one corre-
sponding set of ?lineage strings? (see Figure 2).
Using dynamic programming, a Levenshtein
edit distance can be calculated between each
word?s lineage strings in the candidate parse and
the gold standard, by determining the smallest
number of symbol insertions, deletions and substi-
tutions required to transform one of the strings into
the other. The leaf-ancestor (LA) metric, a simi-
larity score ranging between 0 (total parse failure)
and 1 (exact match), is then calculated by taking
into account the lengths of the two lineages:
LA = 1?
dist(lineage1, lineage2)
len(lineage1)+ len(lineage2)
The per-word score can then be averaged over a
sentence or a whole corpus in order to arrive at an
overall performance indicator. Besides avoiding
some of the limitations of constituent-based eval-
uation discussed above, one major advantage of
this approach is that it can provide a word-by-word
measure of parser performance, and thus draw at-
tention easily to those regions of a sentence which
have proved problematic (see Section 6.2 for an
example). The algorithm can be made more sen-
sitive to near-matches between phrasal categories
by tuning the cost incurred for a substitution be-
tween similar labels, e.g. those for ?singular noun?
and ?proper noun?, rather than adhering to the uni-
form edit cost dictated by the standard Levenshtein
scheme. In order to avoid over-complicating this
study, however, we chose to keep the standard
penalty of 1 for each insertion, deletion or substi-
tution.
One drawback to leaf-ancestor evaluation is that
although it scores each word (sentence, corpus)
between 0 and 1, and these scores are presented
here as percentages for readability, it is mislead-
ing to think of them as percentages of correctness
in the same way that one would regard constituent
precision and recall. Indeed, the very fact that
it results in a single score means that it reveals
less at first glance about the broad classes of er-
rors that a parser is making than precision, recall
and crossing brackets do. Another possible objec-
tion is that since an error high in the tree will af-
fect many words, the system implicitly gives most
weight to the correct determination of those fea-
tures of a sentence which are furthest from be-
ing directly observable. One might argue, how-
ever, that since a high-level attachment error can
grossly perturb the structure of the tree and thus
the interpretation of the sentence, this is a perfectly
valid approach; it is certainly complementary to
the uniform scoring scheme described in the previ-
ous section, where every mistake is weighted iden-
tically.
2.3 Production-based assessment
In order to properly characterise the kinds of errors
that occurred in each parse, and to help elucidate
the differences between multiple corpora and be-
tween each parser?s behaviour on each corpus, we
developed an additional scoring process based on
production rules. A production rule is a syntactic
operation that maps from a parent constituent in a
syntax tree to a list of daughter constituents and/or
POS tags, of the general form:
LABELp ? LABEL1 . . .LABELn
For example, the rule that maps from the top-
most constituent in Figure 2 to its daughters would
be S ? NP VP. A production is the application
of a production rule at a particular location in the
sentence, and can be expressed as:
LABELp(lbound,rbound)? LABEL1 . . .LABELn
Production precision and recall can be calcu-
lated as in a normal labelled constituent-based as-
sessment, except that a proposed production is a
true positive if and only if there exists a production
in the gold standard with the same parent label and
boundaries, and the same daughter labels in the
same order. (The respective widths of the daughter
constituents, where applicable, are not taken into
account, only their labels and order; any errors of
width in the daughters are detected when they are
tested as parents themselves.)
Furthermore, as an aid to the detection and anal-
ysis of systematic errors, we developed a heuristic
SNP
NN
TCF-1
NN
mRNA
VP
VBD
was
VP
VBN
expressed
ADVP
RB
uniquely
PP
IN
in
NP
NN
T
NNS
lymphocytes
Figure 2: Skipping the POS tag, the lineage string for uniquely is: [ ADVP ] VP VP S . The left and
right boundary markers record the fact that the ADVP constituent both starts and ends with this word.
for finding the closest-matching candidate produc-
tions PRODc1 . . .PRODcm in a parse, in each case
where a production PRODg in the gold standard is
not exactly matched in the parse.
1. First, the heuristic looks for productions with
correct boundaries and parent labels, but in-
correct daughters. The corresponding pro-
duction rules are returned.
2. Failing that, it looks for productions with
correct boundaries and daughters, preserving
the order of the daughters, but with incorrect
parent labels. The corresponding production
rules are returned.
3. Failing that, it looks for productions with cor-
rect boundaries but incorrect parent labels
and daughters. The corresponding produc-
tion rules are returned.
4. Failing that, it looks for all extensions and
truncations of the production (boundary mod-
ifications such that there is at least one word
from PRODg still covered) with correct par-
ent and daughter labels and daughter order,
keeping only those that are closest in width to
PRODg (minimum number of extensions and
truncations). The meta-rules EXT ALLMATCH
and/or TRUNC ALLMATCH as appropriate are
returned.
5. Failing that, it looks for all extensions
and truncations of the production where
the parent label is correct but the daugh-
ters are incorrect, keeping only those
that are closest in width to PRODg. The
meta-rules EXT PARENTMATCH and/or
TRUNC PARENTMATCH are returned.
6. If no matches are found in any of these
classes, a null result is returned.
Note that in some cases, m production rules of
the same class may be returned, for example when
the closest matches in the parse are two produc-
tions with the correct parent label, one of which is
one word longer than PRODg, and one of which
is one word shorter. It is also conceivable that
multiple productions with the same parent or same
daughters could occupy the same location in the
sentence without branching, although it seems un-
likely that this would occur apart from in patho-
logically bad parses. In any ambiguous cases, no
attempt is made to decided which is the ?real? clos-
est match; all m matches are returned, but they are
downweighted so that each counts as 1/m of an
error when error frequencies are calculated. In no
circumstances are matches from different classes
returned.
The design of this procedure reflects our re-
quirements for a tool to facilitate the diagnosis and
summarisation of parse errors. We wanted to be
able to answer questions like ?given that parser
A has a low recall for NP ? NN NN productions,
what syntactic structures is it generating in their
place? Why might this be so? And what effect
might these errors have on the interpretation of the
sentence?? Accordingly, as the heuristic casts the
net further and further to find the closest match
for a production PRODg, the classes to which it
assigns errors become broader and broader. Any
match at stages 1?3 is not simply recorded as a
substitution error, but a substitution for a particu-
lar incorrect production rule. However, matches at
stages 4 and 5 do not make a distinction between
different magnitudes of truncation and extension,
and at stage 5 the information about the daugh-
ters of incorrect productions is discarded. This al-
lowed us to identify broad trends in the data even
where the correspondences between the gold stan-
dard and the parses were weak, yet nonetheless
recover detailed substitution information akin to
confusion matrices where possible.
Similar principles guided the decision not to
consider extensions and truncations with different
parent labels as potential loose matches, in order to
avoid uninformative matches to productions else-
where in the syntax tree. In practice, the matches
returned by the heuristic accounted for almost all
of the significant systematic errors suffered by the
parsers (see Section 6) ? null matches were in-
frequent enough in general that their presence in
larger numbers on certain production rules was it-
self useful from an explanatory point of view.
2.4 Alternative approaches
Several other proposed solutions to the evalua-
tion problem exist, and it is an ongoing and con-
tinually challenging field of research. Suggested
protocols based on grammatical or dependency
relations (Crouch et al, 2002), head projection
(Ringger et al, 2004), alternative edit distance
metrics (Roark, 2002) and various other schemes
have been suggested. Many of these alterna-
tive methodologies, however, suffer from one or
more disadvantages, such as specificity to one par-
ticular grammatical formalism (e.g. head-driven
phrase structure grammar) or one class of parser
(e.g. partial parsers), or a requirement for a spe-
cific manually-prepared evaluation corpus in a
non-treebank format. In addition, none of them
deliver the richness of information supplied by
production-based assessment, particularly in com-
bination with the other methods outlined above.
3 Comparing the corpora
The gold standard data for our experiments was
drawn from the GENIA Treebank3, a beta-stage
corpus of 200 abstracts drawn randomly from
the MEDLINE database4 with the search terms
?human?, ?blood cell? and ?transcription factor?.
These abstracts have been annotated with POS
tags, named entity classes and boundaries5, and
syntax trees which broadly follow the conventions
of the PTB. Some manual editing was required
to correct annotation errors and remove sentences
with uncorrectable errors, leaving 1757 sentences
(45406 tokens) in the gold standard. All errors
were reported to the GENIA group.
For comparison purposes, we used the standard
set-aside test set from the PTB, section 23. This
consists of 56684 words in 2416 sentences.
To gain insight into the differences between the
two corpora, we ran several tests of the grammati-
cal composition of each. For consistency with the
parser evaluation results, we stripped the follow-
ing punctuation tokens from the corpora before
gathering these statistics: period, comma, semi-
colon, colon, and double-quotes (whether they
were expressed as a single double-quotes charac-
ter, or pairs of opening or closing single-quotes).
We also removed any super-syntactic information
such as grammatical function suffixes, pruned any
tree branches that did not contain textual termi-
nals (e.g. traces), and deleted any duplicated
constituents ? that is, constituents with only one
daughter that has the same label.
3.1 Sentence length and complexity
Having performed these pre-processing steps, we
counted the distributions of sentence lengths (in
3http://www-tsujii.is.s.u-tokyo.ac.jp/
GENIA/topics/Corpus/GTB.html
4http://www.pubmed.org/
5The named entity annotations are supplied in a separate
file which was discarded.
words) and sentence complexities, using the num-
ber of constituents, not counting POS tags, as
a simple measure of complexity ? although of
course one can imagine various other ways to
gauge complexity (mean tree depth, maximum
tree depth, constituents per word etc.). The results
are shown in Figure 3, and reveal an unexpected
level of correlation. Apart from a few sparse in-
stances at the right-hand tails of the two GENIA
distributions, and a single-constituent spike on the
PTB complexity distribution (due to one-phrase
headings like STOCK REPORT.), the two corpora
have broadly similar distributions of word count
and constituent count. The PTB has slightly more
mass on the short end of the length scale, but
GENIA does not have a corresponding number of
longer sentences. This ran contrary to our initial
intuition that newspaper English would tend to be
composed predominantly of shorter and simpler
sentences than biological English.
3.2 Constituent and production rule usage
Next, we counted the frequency with which each
constituent label appears in each corpus. The re-
sults are shown in Figure 4. The distributions are
reasonably similar between the two corpora, with
the most obvious difference being that GENIA
uses noun phrases more often, by just over six per-
centage points. This may reflect the fact that much
of the text in GENIA describes interactions be-
tween multiple biological entities at the molecular
and cellular levels; conjunction phrases are three
times as frequent in GENIA too, although this is
not obvious from the chart as the numbers are so
low in each corpus.
One surprising result is revealed by looking
at Table 1 which shows production rule usage
across the corpora. Although GENIA uses slightly
more productions per sentence on average, it uses
marginally fewer distinct production rules per sen-
tence, and considerably fewer overall ? 62% of the
number of rules used in the PTB, despite being
73% of the size in sentences. These figures, along
with the significantly different rankings and fre-
quencies of the actual rules themselves (Table 2),
demonstrate that there are important syntactic dif-
ferences between the corpora, despite the simi-
larities in length, complexity and constituent us-
age. Such differences are invisible to conventional
constituent-based analysis.
The comparative lack of syntactic diversity in
GENIA may seem counter-intuitive, since biolog-
ical language seems at first glance dense and dif-
ficult. However, it must be remembered that the
text in GENIA consists only of abstracts, which
are tailored to the purpose of communicating a
few salient points in a short passage, and tend
to be composed in a somewhat formulaic man-
ner. They are written in a very restricted regis-
ter, compared to the range of registers that may
be present in one issue of a newspaper ? news ar-
ticles, lifestyle features, opinion pieces, financial
reports and letters will be delivered in very dif-
ferent voices. Also, some of the apparent com-
plexity of biomedical texts is illusory, stemming
from the unfamiliar vocabulary, and furthermore,
a distinction must be made between syntactic and
semantic complexity. Consider a phrase like iron-
sulphur cluster assembly transcription factor, the
name of a family of DNA-binding proteins, which
is a semantically-complex concept expressed in a
syntactically-simple form ? essentially just a se-
ries of nouns.
4 Evaluating the parsers
The parsers chosen for this evaluation were those
described originally in Collins (1999), Charniak
(1999) and Bikel (2002). These were selected be-
cause they are up-to-date (having last been up-
dated in 2002, 2003 and 2004 respectively), highly
regarded by the computational linguistics com-
munity, and importantly, free to use and modify
for academic research. Since part of our moti-
vation was to detect signs of over-specialisation
on the PTB, we assessed the current (0.9.9) and
previous (0.9.8) versions of the Bikel parser in-
dividually. The current version was invoked
with the new bikel.properties settings file,
which enables parameter pruning (Bikel, 2004),
whereas the previous version used the original
collins.properties settings which were de-
signed to emulate the Collins parser model 2 (see
below). The same approach was attempted with
the Charniak parser, but the latest version (re-
leased February 2005) suffered from fatal errors
 
 

 

 

 

 

 

 

 
	
 

 
 


 


 


 
 

 

 

 

 

 

 

 

 
	
 

 
 


 

 




 









