Proceedings of the 12th Conference of the European Chapter of the ACL, pages 701?709,
Athens, Greece, 30 March ? 3 April 2009. c?2009 Association for Computational Linguistics
Unsupervised Methods for Head Assignments
Federico Sangati, Willem Zuidema
Institute for Logic, Language and Computation
University of Amsterdam, the Netherlands
{f.sangati,zuidema}@uva.nl
Abstract
We present several algorithms for assign-
ing heads in phrase structure trees, based
on different linguistic intuitions on the role
of heads in natural language syntax. Start-
ing point of our approach is the obser-
vation that a head-annotated treebank de-
fines a unique lexicalized tree substitution
grammar. This allows us to go back and
forth between the two representations, and
define objective functions for the unsu-
pervised learning of head assignments in
terms of features of the implicit lexical-
ized tree grammars. We evaluate algo-
rithms based on the match with gold stan-
dard head-annotations, and the compar-
ative parsing accuracy of the lexicalized
grammars they give rise to. On the first
task, we approach the accuracy of hand-
designed heuristics for English and inter-
annotation-standard agreement for Ger-
man. On the second task, the implied lex-
icalized grammars score 4% points higher
on parsing accuracy than lexicalized gram-
mars derived by commonly used heuris-
tics.
1 Introduction
The head of a phrasal constituent is a central
concept in most current grammatical theories and
many syntax-based NLP techniques. The term is
used to mark, for any nonterminal node in a syn-
tactic tree, the specific daughter node that fulfills
a special role; however, theories and applications
differ widely in what that special role is supposed
to be. In descriptive grammatical theories, the
role of the head can range from the determinant of
agreement or the locus of inflections, to the gover-
nor that selects the morphological form of its sis-
ter nodes or the constituent that is distributionally
equivalent to its parent (Corbett et al, 2006).
In computational linguistics, heads mainly
serve to select the lexical content on which the
probability of a production should depend (Char-
niak, 1997; Collins, 1999). With the increased
popularity of dependency parsing, head annota-
tions have also become a crucial level of syntac-
tic information for transforming constituency tree-
banks to dependency structures (Nivre et al, 2007)
or richer syntactic representations (e.g., Hocken-
maier and Steedman, 2007).
For the WSJ-section of the Penn Treebank, a set
of heuristic rules for assigning heads has emerged
from the work of (Magerman, 1995) and (Collins,
1999) that has been employed in a wide variety of
studies and proven extremely useful, even in rather
different applications from what the rules were
originally intended for. However, the rules are
specific to English and the treebank?s syntactic an-
notation, and do not offer much insights into how
headedness can be learned in principle or in prac-
tice. Moreover, the rules are heuristic and might
still leave room for improvement with respect to
recovering linguistic head assignment even on the
Penn WSJ corpus; in fact, we find that the head-
assignments according to the Magerman-Collins
rules correspond only in 85% of the cases to de-
pendencies such as annotated in PARC 700 De-
pendency Bank (see section 5).
Automatic methods for identifying heads are
therefore of interest, both for practical and more
fundamental linguistic reasons. In this paper we
investigate possible ways of finding heads based
on lexicalized tree structures that can be extracted
from an available treebank. The starting point
of our approach is the observation that a head-
annotated treebank (obeying the constraint that ev-
ery nonterminal node has exactly one daughter
marked as head) defines a unique lexicalized tree
substitution grammar (obeying the constraint that
every elementary tree has exactly one lexical an-
chor). This allows us to go back and forth between
701
the two representations, and define objective func-
tions for the unsupervised learning of head assign-
ments in terms of features of the implicit Lexical-
ized Tree Substitution Grammars.
Using this grammar formalism (LTSGs) we will
investigate which objective functions we should
optimize for recovering heads. Should we try to
reduce uncertainty about the grammatical frames
that can be associated with a particular lexical
item? Or should we assume that linguistic head
assignments are based on the occurrence frequen-
cies of the productive units they imply?
We present two new algorithms for unsuper-
vised recovering of heads ? entropy minimization
and a greedy technique we call ?familiarity max-
imization? ? that can be seen as ways to opera-
tionalize these last two linguistic intuitions. Both
algorithms are unsupervised, in the sense that they
are trained on data without head annotations, but
both take labeled phrase-structure trees as input.
Our work fits well with several recent ap-
proaches aimed at completely unsupervised learn-
ing of the key aspects of syntactic structure: lex-
ical categories (Schu?tze, 1993), phrase-structure
(Klein and Manning, 2002; Seginer, 2007),
phrasal categories (Borensztajn and Zuidema,
2007; Reichart and Rappoport, 2008) and depen-
dencies (Klein and Manning, 2004).
For the specific task addressed in this paper ?
assigning heads in treebanks ? we only know of
one earlier paper: Chiang and Bikel (2002). These
authors investigated a technique for identifying
heads in constituency trees based on maximiz-
ing likelihood, using EM, under a Tree Insertion
Grammar (TIG)model1. In this approach, headed-
ness in some sense becomes a state-split, allowing
for grammars that more closely match empirical
distributions over trees. The authors report some-
what disappointing results, however: the automat-
ically induced head-annotations do not lead to sig-
nificantly more accurate parsers than simple left-
most or rightmost head assignment schemes2.
In section 2 we define the grammar model we
will use. In section 3 we describe the head-
assignment algorithms. In section 4, 5 and 6 we
1The space over the possible head assignments that these
authors consider ? essentially regular expressions over CFG
rules ? is more restricted than in the current work where we
consider a larger ?domain of locality?.
2However, the authors? approach of using EM for induc-
ing latent information in treebanks has led to extremely ac-
curate constituency parsers, that neither make use of nor pro-
duce headedness information; see (Petrov et al, 2006)
then describe our evaluations of these algorithms.
2 Lexicalized Tree Grammars
In this section we define Lexicalised Tree Substi-
tution Grammars (LTSGs) and show how they can
be read off unambiguously from a head-annotated
treebank. LTSGs are best defined as a restriction
of the more general Probabilistic Tree Substitution
Grammars, which we describe first.
2.1 Tree Substitution Grammars
A tree substitution grammar (TSG) is a 4-tuple
?Vn, Vt, S, T ? where Vn is the set of nonterminals;
Vt is the set of of terminals; S ? Vn is the start
symbol; and T is the set of elementary trees, hav-
ing root and internal nodes in Vn and leaf nodes in
Vn?Vt. Two elementary trees ? and ? can be com-
bined by means of the substitution operation ? ??
to produce a new tree, only if the root of ? has the
same label of the leftmost nonterminal leaf of ?.
The combined tree corresponds to ? with the left-
most nonterminal leaf replaced with ?. When the
tree resulting from a series of substitution opera-
tions is a complete parse tree, i.e. the root is the
start symbol and all leaf nodes are terminals, we
define the sequence of the elementary trees used
as a complete derivation.
A probabilistic TSG defines a probabilistic
space over the set of elementary trees: for every
? ? T , P (?) ? [0, 1] and
?
? ?:r(? ?)=r(?) P (?
?) =
1, where r(?) returns the root node of ? . Assum-
ing subsequent substitutions are stochastically in-
dependent, we define the probability of a deriva-
tion as the product of the probability of its elemen-
tary trees. If a derivation d consists of n elemen-
tary trees ?1 ? ?2 ? . . . ? ?n, we have:
P (d) =
n?
i=1
P (?i) (1)
Depending on the set T of elementary trees, we
might have different derivations producing the
same parse tree. For any given parse tree t, we
define ?(t) as the set of its derivations licensed by
the grammar. Since any derivation d ? ?(t) is a
possible way to construct the parse tree, we will
compute the probability of a parse tree as the sum
of the probabilities of its derivations:
P (t) =
?
d??(t)
?
??d
P (?) (2)
702
Lexicalized Tree Substitution Grammars are de-
fined as TSGs with the following contraint on the
set of elementary trees T : every ? in T must have
at least one terminal (the lexical anchor) among
its leaf nodes. In this paper, we are only con-
cerned with single-anchored LTSGs, in which all
elementary trees have exactly one lexical anchor.
Like TSGs, LTSGs have a weak generative ca-
pacity that is context-free; but whereas PTSGs are
both probabilistically and in terms of strong gen-
erative capacity richer than PCFGs (Bod, 1998),
LTSG are more restricted (Joshi and Schabes,
1991). This limits the usefulness of LTSGs for
modeling the full complexity of natural language
syntax; however, computationally, LTSGs have
many advantages over richer formalisms and for
the current purposes represent a useful compro-
mise between linguistic adequacy and computa-
tional complexity.
2.2 Extracting LTSGs from a head-annotated
corpus
In this section we will describe a method for as-
signing to each word token that occurs in the cor-
pus a unique elementary tree. This method de-
pends on the annotation of heads in the treebank,
such as for instance provided for the Penn Tree-
bank by the Magerman-Collins head-percolation
rules. We adopt the same constraint as used in this
scheme, that each nonterminal node in every parse
tree must have exactly one of its children anno-
tated as head. Our method is similar to (Chiang,
2000), but is even simpler in ignoring the distinc-
tion between arguments and adjuncts (and thus the
sister-adjunction operation). Figure 1 shows an
example parse tree enriched with head-annotation:
the suffix -H indicates that the specific node is the
head of the production above it.
S
NP
NNP
Ms.
NNP-H
Haag
VP-H
V-H
plays
NP
NNP-H
Elianti
Figure 1: Parse tree of the sentence ?Ms. Haag
plays Elianti? annotated with head markers.
Once a parse tree is annotated with head mark-
ers in such a manner, we will be able to extract
for every leaf its spine. Starting from each lexical
production we need to move upwards towards the
root on a path of head-marked nodes until we find
the first internal node which is not marked as head
or until we reach the root of the tree. In the ex-
ample above, the verb of the sentence ?plays? is
connected through head-marked nodes to the root
of the tree. In this way we can extract the 4 spines
from the parse tree in figure 1, as shown in fig-
ure 2.
NNP
Ms.
NP
NNP-H
Haag
S-H
VP-H
V-H
plays
NP
NNP-H
Elianti
Figure 2: The lexical spines of the tree in fig. 1.
It is easy to show that this procedure yields a
unique spine for each of its leaves, when applied
to a parse tree where all nonterminals have a single
head-daughter and all terminals are generated by a
unary production. Having identified the spines, we
convert them to elementary trees, by completing
every internal node with the other daughter nodes
not on the spine. In this way we have defined a
way to obtain a derivation of any parse tree com-
posed of lexical elementary trees. The 4 elemen-
tary trees completed from the previous paths are in
figure 3 with the substitution sites marked with ?.
NNP
Ms.
NP
NNP? NNP-H
Haag
S-H
NP? VP-H
V-H
plays
NP?
NP
NNP-H
Elianti
Figure 3: The extracted elementary trees.
3 Head Assignment Algorithms
We investigate two novel approaches to automat-
ically assign head dependencies to a training cor-
pus where the heads are not annotated: entropy
minimization and familiarity maximization. The
baselines for our experiments will be given by the
Magerman and Collins scheme together with the
random, the leftmost daughter, and the rightmost
daughter-based assignments.
703
3.1 Baselines
The Magerman-Collins scheme, and very similar
versions, are well-known and described in detail
elsewhere (Magerman, 1995; Collins, 1999; Ya-
mada and Matsumoto, 2003); here we just men-
tion that it is based on a number of heuristic rules
that only use the labels of nonterminal nodes and
the ordering of daughter nodes. For instance if the
root label of a parse tree is S, the head-percolation
scheme will choose to assign the head marker to
the first daughter from the left, labeled with TO.
If no such label is present, it will look for the first
IN. If no IN is found, it will look for the first VP,
and so on. We used the freely available software
?Treep? (Chiang and Bikel, 2002) to annotate the
Penn WSJ treebank with heads.
We consider three other baselines, that are ap-
plicable to other treebanks and other languages as
well: RANDOM, where, for every node in the tree-
bank, we choose a random daughter to be marked
as head; LEFT, where the leftmost daughter is
marked; and RIGHT, where the rightmost daughter
is marked.
3.2 Minimizing Entropy
In this section we will describe an entropy based
algorithm, which aims at learning the simplest
grammar fitting the data. Specifically, we take a
?supertagging? perspective (Bangalore and Joshi,
1999) and aim at reducing the uncertainty about
which elementary tree (supertag) to assign to a
given lexical item. We achieve this by minimizing
an objective function based on the general defini-
tion of entropy in information theory.
The entropy measure that we are going to de-
scribe is calculated from the bag of lexicalized el-
ementary trees T extracted from a given training
corpus of head annotated parse trees. We define
Tl as a discrete stochastic variable, taking as val-
ues the elements from the set of all the elementary
trees having l as lexical anchor {?l1 , ?l2 , . . . , ?ln}.
Tl thus takes n possible values with specific prob-
abilities; its entropy is then defined as:
H(Tl) = ?
n?
i=1
p(?li) log2 p(?li) (3)
The most intuitive way to assign probabilities to
each elementary tree is considering its relative fre-
quency in T . If f(?) is the frequency of the frag-
ment ? and f(l) is the total frequency of fragments
with l as anchor we will have:
p(?lj ) =
f(?lj )
f(lex(?lj ))
=
f(?lj )
n?
i=1
f(?li))
(4)
We will then calculate the entropy H(T ) of our
bag of elementary trees by summing the entropy of
each single discrete stochastic variable Tl for each
choice of l:
H(T ) =
|L |?
l=1
H(Tl) (5)
In order to minimize the entropy, we apply a
hill-climbing strategy. The algorithm starts from
an already annotated tree-bank (for instance using
the RANDOM annotator) and iteratively tries out
a random change in the annotation of each parse
tree. Only if the change reduces the entropy of the
entire grammar it is kept. These steps are repeated
until no further modification which could reduce
the entropy is possible. Since the entropy measure
is defined as the sum of the function p(?) log2 p(?)
of each fragment ? , we do not need to re-calculate
the entropy of the entire grammar, when modify-
ing the annotation of a single parse tree. In fact:
H(T ) = ?
|L |?
l=1
n?
i=1
p(?li) log2 p(?li)
= ?
|T |?
j=1
p(?j) log2 p(?j)
(6)
For each input parse tree under consideration,
the algorithm selects a non-terminal node and tries
to change the head annotation from its current
head-daughter to a different one. As an example,
considering the parse tree of figure 1 and the inter-
nal node NP (the leftmost one), we try to annotate
its leftmost daughter as the new head. When con-
sidering the changes that this modification brings
on the set of the elementary trees T , we understand
that there are only 4 elementary trees affected, as
shown in figure 4.
After making the change in the head annotation,
we just need to decrease the frequencies of the old
trees by one unit, and increase the ones of the new
trees by one unit. The change in the entropy of our
grammar can therefore be computed by calculat-
ing the change in the partial entropy of these four
704
NP
NNP NNP
Haag
NNP
Ms.
NP
NNP
Ms.
NNP
NNP
Haag
?h ?d ? ?h ?
?
d
Figure 4: Lexical trees considered in the EN-
TROPY algorithm when changing the head ass-
ingnment from the second NNP to the first NNP
of the leftmost NP node of figure 1. ?h is the old
head tree; ?d the old dependent tree; ? ?d the new
dependent tree; ? ?h the new head tree.
elementary trees before and after the change. If
such change results in a lower entropy of the gram-
mar, the new annotation is kept, otherwise we go
back to the previous annotation. Although there is
no guarantee our algorithm finds the global min-
imum, it is very efficient and succeeds in drasti-
cally minimize the entropy from a random anno-
tated corpus.
3.3 Maximizing Familiarity
The main intuition behind our second method is
that we like to assign heads to a tree t in such
a way that the elementary trees that we can ex-
tract from t are frequently observed in other trees
as well. That is, we like to use elementary trees
which are general enough to occur in many possi-
ble constructions.
We start with building the bag of all one-anchor
lexicalized elementary trees from the training cor-
pus, consistent with any annotation of the heads.
This operation is reminiscent of the extraction of
all subtrees in Data-Oriented Parsing (Bod, 1998).
Fortunately, and unlike DOP, the number of possi-
ble lexicalised elementary trees is not exponential
in sentence length n, but polynomial: it is always
smaller than n2 if the tree is binary branching.
Next, for each node in the treebank, we need
to select a specific lexical anchor, among the ones
it dominates, and annotate the nodes in the spine
with head annotations. Our algorithm selects the
lexical anchor which maximizes the frequency of
the implied elementary tree in the bag of elemen-
tary trees. In figure 5, algorithm 1 (right) gives the
pseudo-code for the algorithm, and the tree (left)
shows an example of its usage.
3.4 Spine and POS-tag reductions
The two algorithms described in the previous two
sections are also evaluated when performing two
possible generalization operations on the elemen-
tary trees, which can be applied both alone or in
combination:
? in the spine reduction, lexicalized trees are
transformed to their respective spines. This
allows to merge elementary trees that are
slightly differing in argument structures.
? in the POStag reduction, every lexical item
of every elementary tree is replaced by its
POStag category. This allows for merging el-
ementary trees with the same internal struc-
ture but differing in lexical production.
4 Implementation details
4.1 Using CFGs for TSG parsing
When evaluating parsing accuracy of a given
LTSG, we use a CKY PCFG parser. We will
briefly describe how to set up an LTSG parser us-
ing the CFG formalism. Every elementary tree
in the LTSG should be treated by our parser as
a unique block which cannot be further decom-
posed. But to feed it to a CFG-parser, we need
to break it down into trees of depth 1. In order to
keep the integrity of every elementary tree we will
assign to its internal node a unique label. We will
achieve this by adding ?@i? to each i-th internal
node encountered in T .
Finally, we read off a PCFG from the elemen-
tary trees, assigning to each PCFG rule a weight
proportional to the weight of the elementary tree it
is extracted from. In this way the PCFG is equiv-
alent to the original LTSG: it will produce exactly
the same derivation trees with the same probabil-
ities, although we would have to sum over (expo-
nentially) many derivations to obtain the correct
probabilities of a parse tree (derived tree). We ap-
proximate parse probability by computing the n-
best derivations and summing over the ones that
yield the same parse tree (by removing the ?@i?-
labels). We then take the parse tree with highest
probability as best parse of the input sentence.
4.2 Unknown words and smoothing
We use a simple strategy to deal with unknown
words occurring in the test set. We replace all the
words in the training corpus occurring once, and
all the unknown words in the test set, with a spe-
cial *UNKNOWN* tag. Moreover we replace all
the numbers in the training and test set with a spe-
cial *NUMBER* tag.
705
Algorithm 1: MaximizeFamiliarity(N)
Input: a non-terminal node N of a parsetree.
begin
L = null;MAX = ?1;
foreach leaf l underN do
?Nl = lex. tree rooted in N and anchored in l;
F = frequency of ?Nl ;
if F > MAX then
L = l;MAX = F ;
Mark all nodes in the path fromN to L with heads;
foreach substitution siteNi of ?NL do
MaximizeFamiliarity(Ni);
end
Figure 5: Left: example of a parse tree in an instantiation of the ?Familiarity? algorithm. Each arrow,
connecting a word to an internal node, represents the elementary tree anchored in that word and rooted
in that internal node. Numbers in parentheses give the frequencies of these trees in the bag of subtrees
collected from WSJ20. The number below each leaf gives the total frequency of the elementary trees
anchored in that lexical item. Right: pseudo-code of the ?Familiarity? algorithm.
Even with unknown words treated in this way,
the lexicalized elementary trees that are extracted
from the training data are often too specific to
parse all sentences in the test set. A simple strat-
egy to ensure full coverage is to smooth with the
treebank PCFG. Specifically, we add to our gram-
mars all CFG rules that can be extracted from the
training corpus and give them a small weight pro-
portional to their frequency3. This in general will
ensure coverage, i.e. that all the sentences in the
test set can be successfully parsed, but still priori-
tizing lexicalized trees over CFG rules4.
4.3 Corpora
The evaluations of the different models were car-
ried out on the Penn Wall Street Journal corpus
(Marcus et al, 1993) for English, and the Tiger
treebank (Brants et al, 2002) for German. As gold
standard head annotations corpora, we used the
Parc 700 Dependency Bank (King et al, 2003) and
the Tiger Dependency Bank (Forst et al, 2004),
which contain independent reannotations of ex-
tracts of the WSJ and Tiger treebanks.
5 Results
We evaluate the head annotations our algorithms
find in two ways. First, we compare the head
annotations to gold standard manual annotations
3In our implementation, each CFG rule frequency is di-
vided by a factor 100.
4In this paper, we prefer these simple heuristics over more
elaborate techniques, as our goal is to compare the merits of
the different head-assignment algorithms.
of heads. Second, we evaluate constituency pars-
ing performance using an LTSG parser (trained
on the various LTSGs), and a state-of-the-art
parser (Bikel, 2004).
5.1 Gold standard head annotations
Table 1 reports the performance of different al-
gorithms against gold standard head annotations
of the WSJ and the Tiger treebank. These an-
notations were obtained by converting the depen-
dency structures of the PARC corpus (700 sen-
tences from section 23) and the Tiger Dependency
Bank (2000 sentences), into head annotations5.
Since the algorithm doesn?t guarantee that the re-
covered head annotations always follow the one-
head-per-node constraint, when evaluating the ac-
curacy of head annotations of different algorithms,
we exclude the cases in which in the gold cor-
pus no head or multiple heads are assigned to the
daughters of an internal node6, as well as cases in
which an internal node has a single daughter.
In the evaluation against gold standard de-
pendencies for the PARC and Tiger dependency
banks, we find that the FAMILIARITY algorithm
when run with POStags and Spine conversion ob-
tains around 74% recall for English and 69% for
German. The different scores of the RANDOM as-
signment for the two languages can be explained
5This procedure is not reported here for reasons of space,
but it is available for other researchers (together with the ex-
tracted head assignments) at http://staff.science.
uva.nl/?fsangati.
6After the conversion, the percentage of incorrect heads
in PARC 700 is around 9%; in Tiger DB it is around 43%.
706
by their different branching factors: trees in the
German treebank are typically more flat than those
in the English WSJ corpus. However, note that
other settings of our two annotation algorithms do
not always obtain better results than random.
When focusing on the Tiger results, we ob-
serve that the RIGHT head assignment recall is
much better than the LEFT one. This result is in
line with a classification of German as a predomi-
nantly head-final language (in contrast to English).
More surprisingly, we find a relatively low recall
of the head annotation in the Tiger treebank, when
compared to a gold standard of dependencies for
the same sentences as given by the Tiger depen-
dency bank. Detailed analysis of the differences
in head assigments between the two approaches
is left for future work; for now, we note that our
best performing algorithm approaches the inter-
annotation-scheme agreement within only 10 per-
centage points7.
5.2 Constituency Parsing results
Table 2 reports the parsing performances of our
LTSG parser on different LTSGs extracted from
the WSJ treebank, using our two heuristics to-
gether with the 4 baseline strategies (plus the re-
sult of a standard treebank PCFG). The parsing re-
sults are computed on WSJ20 (WSJ sentences up
to length 20), using sections 02-21 for training and
section 22 for testing.
We find that all but one of the head-assignment
algorithms lead to LTSGs that without any fine-
tuning perform better than the treebank PCFG. On
this metric, our best performing algorithm scores
4 percentage points higher than the Magerman-
Collins annotation scheme (a 19% error reduc-
tion). The poor results with the RIGHT assign-
ment, in contrast with the good results with the
LEFT baseline (performing even better than the
Magerman-Collins assignments), are in line with
the linguistic tradition of listing English as a pre-
dominantly head-initial language. A surprising
result is that the RANDOM-assignment gives the
7We have also used the various head-assignments to con-
vert the treebank trees to dependency structures, and used
these in turn to train a dependency parser (Nivre et al, 2005).
Results from these experiments confirm the ordering of the
various unsupervised head-assignment algorithms. Our best
results, with the FAMILIARITY algorithm, give us an Unla-
beled Attachment Score (UAS) of slightly over 50% against
a gold standard obtained by applying the Collins-Magerman
rules to the test set. This is much higher than the three base-
lines, but still considerably worse than results based on su-
pervised head-assignments.
best performing LTSG among the baselines. Note,
however, that this strategy leads to much wield-
ier grammars; with many more elementary trees
than for instance the left-head assignment, the
RANDOM strategy is apparently better equipped
to parse novel sentences. Both the FAMILIAR-
ITY and the ENTROPY strategy are at the level of
the random-head assignment, but do in fact lead to
much more compact grammars.
We have also used the same head-enriched tree-
bank as input to a state-of-the-art constituency
parser8 (Bikel, 2004), using the same training and
test set. Results, shown in table 3, confirm that
the differences in parsing success due to differ-
ent head-assignments are relatively minor, and that
even RANDOM performs well. Surprisingly, our
best FAMILIARITY algorithm performs as well as
the Collins-Magerman scheme.
LFS UFS |T|
PCFG 78.23 82.12 -
RANDOM 82.70 85.54 64k
LEFT 80.05 83.21 46k
Magerman-Collins 79.01 82.67 54k
RIGHT 73.04 77.90 49k
FAMILIARITY 84.44 87.22 42k
ENTROPY-POStags 82.81 85.80 64k
FAMILIARITY-Spine 82.67 85.35 47k
ENTROPY-POStags-Spine 82.64 85.55 64k
Table 2: Parsing accuracy on WSJ20 of the LTSGs
extracted from various head assignments, when
computing the most probable derivations for ev-
ery sentence in the test set. The Labeled F-Score
(LFS) and unlabeled F-Score (UFS) results are re-
ported. The final column gives the total number of
extracted elementary trees (in thousands).
LFS UFS
Magerman-Collins 86.20 88.35
RANDOM 84.58 86.97
RIGHT 81.62 84.41
LEFT 81.13 83.95
FAMILIARITY-POStags 86.27 88.32
FAMILIARITY-POStags-Spine 85.45 87.71
FAMILIARITY-Spine 84.41 86.83
FAMILIARITY 84.28 86.53
Table 3: Evaluation on WSJ20 of various head as-
signments on Bikel?s parser.
8Although we had to change a small part of the code,
since the parser was not able to extract heads from an en-
riched treebank, but it was only compatible with rule-based
assignments. For this reason, results are reported only as a
base of comparison.
707
Gold = PARC 700 % correct
Magerman-Collins 84.51
LEFT 47.63
RANDOM 43.96
RIGHT 40.70
FAMILIARITY-POStags-Spine 74.05
FAMILIARITY-POStags 51.10
ENTROPY-POStags-Spine 43.23
FAMILIARITY-Spine 39.68
FAMILIARITY 37.40
Gold = Tiger DB % correct
Tiger TB Head Assignment? 77.39
RIGHT 52.59
RANDOM 38.66
LEFT 18.64
FAMILIARITY-POStags-Spine 68.88
FAMILIARITY-POStags 41.74
ENTROPY-POStags-Spine 37.99
FAMILIARITY 26.08
FAMILIARITY-Spine 22.21
Table 1: Percentage of correct head assignments against gold standard in Penn WSJ and Tiger.
? The Tiger treebank already comes with built-in head labels, but not for all categories. In this case the
score is computed only for the internal nodes that conform to the one head per node constraint.
6 Conclusions
In this paper we have described an empirical inves-
tigation into possible ways of enriching corpora
with head information, based on different linguis-
tic intuitions about the role of heads in natural lan-
guage syntax. We have described two novel algo-
rithms, based on entropy minimization and famil-
iarity maximization, and several variants of these
algorithms including POS-tag and spine reduction.
Evaluation of head assignments is difficult, as
no widely agreed upon gold standard annotations
exist. This is illustrated by the disparities between
the (widely used) Magerman-Collins scheme and
the Tiger-corpus head annotations on the one
hand, and the ?gold standard? dependencies ac-
cording to the corresponding Dependency Banks
on the other. We have therefore not only evalu-
ated our algorithms against such gold standards,
but also tested the parsing accuracies of the im-
plicit lexicalized grammars (using three different
parsers). Although the ordering of the algorithms
on performance on these various evaluations is dif-
ferent, we find that the best performing strategies
in all cases and for two different languages are
with variants of the ?familiarity? algorithm.
Interestingly, we find that the parsing results are
consistently better for the algorithms that keep the
full lexicalized elementary trees, whereas the best
matches with gold standard annotations are ob-
tained by versions that apply the POStag and spine
reductions. Given the uncertainty about the gold
standards, the possibility remains that this reflects
biases towards the most general headedness-rules
in the annotation practice rather than a linguisti-
cally real phenomenon.
Unsupervised head assignment algorithms can
be used for the many applications in NLP where
information on headedness is needed to convert
constituency trees into dependency trees, or to
extract head-lexicalised grammars from a con-
stituency treebank. Of course, it remains to be
seen which algorithm performs best in any of these
specific applications. Nevertheless, we conclude
that among currently available approaches, i.e.,
our two algorithms and the EM-based approach of
(Chiang and Bikel, 2002), ?familiarity maximiza-
tion? is the most promising approach for automatic
assignments of heads in treebanks.
From a linguistic point of view, our work can
be seen as investigating ways in which distribu-
tional information can be used to determine head-
edness in phrase-structure trees. We have shown
that lexicalized tree grammars provide a promis-
ing methodology for linking alternative head as-
signments to alternative dependency structures
(needed for deeper grammatical structure, includ-
ing e.g., argument structure), as well as to alterna-
tive derivations of the same sentences (i.e. the set
of lexicalized elementary trees need to derive the
given parse tree). In future work, we aim to extend
these results by moving to more expressive gram-
matical formalisms (e.g., tree adjoining grammar)
and by distinguishing adjuncts from arguments.
Acknowledgments We gratefully acknowledge
funding by the Netherlands Organization for Sci-
entific Research (NWO): FS is funded through
a Vici-grant ?Integrating Cognition? (277.70.006)
to Rens Bod and WZ through a Veni-grant ?Dis-
covering Grammar? (639.021.612). We thank
Rens Bod, Yoav Seginer, Reut Tsarfaty and
three anonymous reviewers for helpful comments,
Thomas By for providing us with his dependency
bank and Joakim Nivre and Dan Bikel for help in
adapting their parsers to work with our data.
708
References
S. Bangalore and A.K. Joshi. 1999. Supertagging: An
approach to almost parsing. Computational Linguis-
tics, 25(2):237?265.
D.M. Bikel. 2004. Intricacies of Collins? Parsing
Model. Computational Linguistics, 30(4):479?511.
R. Bod. 1998. Beyond Grammar: An experience-
based theory of language. CSLI, Stanford, CA.
G. Borensztajn, and W. Zuidema. 2007. Bayesian
Model Merging for Unsupervised Constituent La-
beling and Grammar Induction. Technical Report,
ILLC.
S. Brants, S. Dipper, S. Hansen, W. Lezius, and
G. Smith. 2002. The TIGER treebank. In Proceed-
ings of the Workshop on Treebanks and Linguistic
Theories, Sozopol.
T. By. 2007. Some notes on the PARC 700 dependency
bank. Natural Language Engineering, 13(3):261?
282.
E. Charniak. 1997. Statistical parsing with a context-
free grammar and word statistics. In Proceedings of
the fourteenth national conference on artificial intel-
ligence, Menlo Park. AAAI Press/MIT Press.
D. Chiang and D.M. Bikel. 2002. Recovering
latent information in treebanks. Proceedings of
the 19th international conference on Computational
linguistics-Volume 1, pages 1?7.
D. Chiang. 2000. Statistical parsing with an
automatically-extracted tree adjoining grammar. In
Proceedings of the 38th Annual Meeting of the ACL.
M. Collins. 1999. Head-Driven Statistical Models for
Natural Language Parsing. Ph.D. thesis, University
of Pennsylvania.
G. Corbett, N. Fraser, and S. McGlashan, editors.
2006. Heads in Grammatical Theory. Cambridge
University Press.
M. Forst, N. Bertomeu, B. Crysmann, F. Fouvry,
S. Hansen-Schirra, and V. Kordoni. 2004. To-
wards a dependency-based gold standard for Ger-
man parsers.
J. Hockenmaier and M. Steedman. 2007. CCGbank:
A corpus of ccg derivations and dependency struc-
tures extracted from the penn treebank. Comput.
Linguist., 33(3):355?396.
A.K. Joshi and Y. Schabes. 1991. Tree-adjoining
grammars and lexicalized grammars. Technical re-
port, Department of Computer & Information Sci-
ence, University of Pennsylvania.
T. King, R. Crouch, S. Riezler, M. Dalrymple, and
R. Kaplan. 2003. The PARC 700 dependency bank.
D. Klein and C.D. Manning. 2002. A generative
constituent-context model for improved grammar in-
duction. In Proceedings of the 40th Annual Meeting
of the ACL.
D. Klein and C.D. Manning. 2004. Corpus-based
induction of syntactic structure: models of depen-
dency and constituency. In Proceedings of the 42nd
Annual Meeting of the ACL.
D.M. Magerman. 1995. Statistical decision-tree mod-
els for parsing. In Proceedings of the 33rd Annual
Meeting of the ACL.
M.P. Marcus, B. Santorini, and M.A. Marcinkiewicz.
1993. Building a large annotated corpus of En-
glish: The Penn Treebank. Computational Linguis-
tics, 19(2).
J. Nivre and J. Hall. 2005. MaltParser: A Language-
Independent System for Data-Driven Dependency
Parsing. In Proceedings of the Fourth Workshop
on Treebanks and Linguistic Theories (TLT2005),
pages 137?148.
J. Nivre, J. Hall, S. Ku?bler, R. McDonald, J. Nils-
son,S. Riedel, and D. Yuret. 2007. The conll 2007
shared task on dependency parsing. In Proc. of the
CoNLL 2007 Shared Task., June.
J. Nivre. 2007. Inductive Dependency Parsing. Com-
putational Linguistics, 33(2).
S. Petrov, L. Barrett, R. Thibaux, and D. Klein.
2006. Learning accurate, compact, and interpretable
tree annotation. In Proceedings ACL-COLING?06,
pages 443?440.
R. Reichart and A. Rappoport. 2008. Unsupervised
Induction of Labeled Parse Trees by Clustering with
Syntactic Features. In Proceedings Coling.
H. Schu?tze. 1993. Part-of-speech induction from
scratch. In Proceedings of the 31st annual meeting
of the ACL.
Y. Seginer 2007. Learning Syntactic Structure. Ph.D.
thesis, University of Amsterdam.
H. Yamada, and Y. Matsumoto. 2003. Statistical De-
pendency Analysis with Support Vector Machines.
In Proceedings of the Eighth International Work-
shop on Parsing Technologies. Nancy, France.
709
Proceedings of the 11th International Conference on Parsing Technologies (IWPT), pages 238?241,
Paris, October 2009. c?2009 Association for Computational Linguistics
A generative re-ranking model for dependency parsing
Federico Sangati, Willem Zuidema and Rens Bod
Institute for Logic, Language and Computation
University of Amsterdam
Science Park 904, 1098 XH Amsterdam, The Netherlands
{f.sangati,zuidema,rens.bod}@uva.nl
Abstract
We propose a framework for dependency
parsing based on a combination of dis-
criminative and generative models. We
use a discriminative model to obtain a k-
best list of candidate parses, and subse-
quently rerank those candidates using a
generative model. We show how this ap-
proach allows us to evaluate a variety of
generative models, without needing differ-
ent parser implementations. Moreover, we
present empirical results that show a small
improvement over state-of-the-art depen-
dency parsing of English sentences.
1 Introduction
Probabilistic generative dependency models de-
fine probability distributions over all valid depen-
dency structures, and thus provide a useful inter-
mediate representation that can be used for many
NLP tasks including parsing and language mod-
eling. In recent evaluations of supervised de-
pendency parsing, however, generative approaches
are consistently outperformed by discriminative
models (Buchholz et al, 2006; Nivre et al,
2007), which treat the task of assigning the cor-
rect structure to a given sentence as a classifica-
tion task. In this category we include both transi-
tion based (Nivre and Hall , 2005) and graph based
parsers (McDonald, 2006).
In this paper, we explore a reranking approach
that combines a generative and a discrimative
model and tries to retain the strengths of both.
The idea of combining these two types of models
through re-ranking is not new, although it has been
mostly explored in constituency parsing (Collins
et al, 2002). This earlier work, however, used the
generative model in the first step, and trained the
discriminative model over its k-best candidates. In
this paper we reverse the usual order of the two
models, by employing a generative model to re-
score the k-best candidates provided by a discrim-
inative model. Moreover, the generative model of
the second phase uses frequency counts from the
training set but is not trained on the k-best parses
of the discriminative model.
The main motivation for our approach is that
it allows for efficiently evaluating many gener-
ative models, differing from one another on (i)
the choice of the linguistic units that are gener-
ated (words, pairs of words, word graphs), (ii) the
generation process (Markov process, top-down,
bottom-up), and (iii) the features that are consid-
ered to build the event space (postags/words, dis-
tance). Although efficient algorithms exist to cal-
culate parse forests (Eisner, 1996a), each choice
gives rise to different parser instantiations.
1.1 A generative model for re-ranking
In our re-ranking perspective, all the generative
model has to do is to compute the probability of
k pre-generated structures, and select the one with
maximum probability. In a generative model, ev-
ery structure can be decomposed into a series of
independent events, each mapped to a correspond-
ing conditioning event. As an example, if a gener-
ative model chooses D as the right dependent of a
certain word H , conditioned uniquely on their rel-
ative position, we can define the event as D is the
right dependent of H , and the conditioning event
as H has a right dependent.
As a preprocessing step, every sentence struc-
ture in the training corpus is decomposed into a se-
ries of independent events, with their correspond-
ing conditioning events. During this process, our
model updates two tables containing the frequency
of events and their conditioning counterparts.
In the re-ranking phase, a given candidate struc-
ture can be decomposed into independent events
(e1, e2, . . . , en) and corresponding conditioning
events (c1, c2, . . . , cn) as in the training phase.
238
The probability of the structure can then be cal-
culated as
n?
i=1
f(ei)
f(ci) (1)
where f(x) returns the frequency of x previously
stored in the tables.
It is important to stress the point that the only
specificity each generative model introduces is in
the way sentence structures are decomposed into
events; provided a generic representation for the
(conditioning) event space, both training phase
and probability calculation of candidate structures
can be implemented independently from the spe-
cific generative model, through the implementa-
tion of generic tables of (conditioning) events.
In this way the probabilities of candidate struc-
tures are exact probabilities, and do not suf-
fer from possible approximation techniques that
parsers often utilize (i.e., pruning). On the other
hand the most probable parse is selected from the
set of the k candidates generated by the discrimi-
native model, and it will equal with the most prob-
able parse among all possible structures, only for
sufficiently high k.
2 MST discriminative model
In order to generate a set of k-candidate struc-
tures for every test sentence, we use a state-of-
the-art discriminative model (McDonald, 2006).
This model treats every dependency structure as
a set of word-dependent relations, each described
by a high dimensional feature representation. For
instance, if in a certain sentence word i is the
head of word j, v(i, j) is the vector describing
all the features of such relation (i.e., labels of the
two words, their postag, and other information
including words in between them, and ancestral
nodes). During the training phase the model learns
a weight vector w which is then used to find the
best dependency structure y for a given test sen-
tence x. The score that needs to be maximized is
defined as?(i,j)?y w ?v(i, j), and the best candi-
date is called the maximum spanning tree (MST).
Assuming we have the weight vector and we
only consider projective dependency structures,
the search space can be efficiently computed by
using a dynamic algorithm on a compact repre-
sentation of the parse forest (Eisner, 1996a). The
training phase is more complex; for details we re-
fer to (McDonald, 2006). Roughly, the model em-
ploys a large-margin classifier which iterates over
the structures of the training corpus, and updates
the weight vector w trying to keep the score of the
correct structure above the scores of the incorrect
ones by an amount which is proportional to how
much they differ in accuracy.
3 Generative model
3.1 Eisner model
As a generative framework we have chosen to use
a variation of model C in (Eisner, 1996a). In
this approach nodes are generated recursively in
a top-down manner starting from the special sym-
bol EOS (end of sentence). At any given node, left
and right children are generated as two separate
Markov sequences of nodes1, each conditioned on
ancestral and sibling information (which, for now,
we will simply refer to as context).
One of the relevant variations with respect to
the original model is that in our version the direc-
tion of the Markov chain sequence is strictly left
to right, instead of the usual inside outwards.
More formally, given a dependency structure T ,
and any of its node N , the probability of generat-
ing the fragment T (N) of the dependency struc-
ture rooted in N is defined as:
P (T (N)) =
L?
l=1
P (N2l)|context) ? P (T (N2l))
?
R?
r=1
P (N3r)|context) ? P (T (N3r)) (2)
where L and R are the number of left and right
children of N in T (L,R > 0), N2l is the left
daughter of N at position l in T (analogously for
right daughters). The probability of the entire de-
pendency structure T is computed as P (T (EOS)).
In order to illustrate how a dependency struc-
ture can be decomposed into events, we present
in table 1 the list of events and the correspond-
ing conditioning events extracted from the depen-
dency structure illustrated in figure 1. In this sim-
ple example, each node is identified with its word,
and the context is composed of the direction with
respect to the head node, the head node, and the
previously chosen daughter (or NONE if it is the
first). While during the training phase the event
tables are updated with these events, in the test
phase they are looked-up to compute the structure
probability, as in equation 1.
1Every sequence ends with the special symbol EOC.
239
NObama
V
won
Dthe Jpresidential
N
election
EOS
Figure 1: Dependency tree of the sentence
?Obama won the presidential election?.
3.2 Model extension
In equation 2 we have generically defined the
probability of choosing a daughter D based on
specific features associated with D and the con-
text in which it occurs. In our implementation,
this probability is instantiated as in equation 3.
The specific features associated with D are: the
distance2 dist(H,D) between D and its head H ,
the flag term(D) which specifies whether D has
more dependents, and the lexical and postag repre-
sentation of D. The context in which D occurs is
defined by features of the head node H , the previ-
ously chosen sister S, the grandparent G, and the
direction dir (left or right).
Equation 3 is factorized in four terms, each em-
ploying an appropriate backoff reduction list re-
ported in descending priority3.
P (D|context) = (3)
P (dist(H,D), term(D), word(D), tag(D)|H,S,G, dir) =
P (tag(D)|H,S,G, dir)
reduction list:
wt(H), wt(S), wt(G), dir
wt(H), wt(S), t(G), dir{ wt(H), t(S), t(G), dir
t(H), wt(S), t(G), dir
t(H), t(S), t(G), dir
? P (word(D)|tag(D), H, S,G, dir)
reduction list: wt(H), t(S), dirt(H), t(S), dir
? P (term(D)|word(D), tag(D), H, S,G, dir)
reduction list: tag(D), wt(H), t(S), dirtag(D), t(H), t(S), dir
? P (dist(P,D)|term(D), word(D), tag(D), H, S,G, dir)
reduction list: word(D), tag(D), t(H), t(S), dirtag(D), t(H), t(S), dir
2In our implementation distance values are grouped in 4
categories: 1, 2, 3? 6, 7??.
3In the reduction lists, wt(N) stands for the string in-
corporating both the postag and the word of N , and t(N)
stands for its postag. This second reduction is never applied
to closed class words. All the notation and backoff parame-
ters are identical to (Eisner, 1996b), and are not reported here
for reasons of space.
4The counts are extracted from a two-sentence corpus
which also includes ?Obama lost the election.?
Events Freq. Conditioning Events Freq.won L EOS NONE 1 L EOS NONE 2EOC L EOS won 1 L EOS won 1EOC R EOS NONE 2 R EOS NONE 2Obama L won NONE 1 L won NONE 1EOC L won Obama 1 L won Obama 1election R won NONE 1 R won NONE 1EOC R won election 1 R won election 1EOC L Obama NONE 2 L Obama NONE 2EOC R Obama NONE 2 R Obama NONE 2the L election NONE 2 L election NONE 2presidential L election the 1 L election the 2EOC L election presidential 1 L election presidential 1EOC R election NONE 2 R election NONE 2EOC L the NONE 2 L the NONE 2EOC R the NONE 2 R the NONE 2EOC L presidential NONE 1 L presidential NONE 1EOC R presidential NONE 1 R presidential NONE 1
Table 1: Events occurring when generating the de-
pendency structure in figure 1, for the event space
(dependent | direction, head, sister). According to
the reported frequency counts4, the structure has a
associated probability of 1/4.
4 Results
In our investigation, we have tested our model
on the Wall Street Journal corpus (Marcus et al,
1993) with sentences up to 40 words in length,
converted to dependency structures. Although
several algorithms exist to perform such a conver-
sion (Sangati and Zuidema, 2008), we have fol-
lowed the scheme in (Collins, 1999). Section 2-21
was used as training, and section 22 as test set.
The MST discriminative parser was provided with
the correct postags of the words in the test set, and
it was run in second-order5 and projective mode.
Results are reported in table 2, as unlabeled attach-
ment score (UAS). The MST dependency parser
obtains very high results when employed alone
(92.58%), and generates a list of k-best-candidates
which can potentially achieve much better results
(an oracle would score above 95% when selecting
from the first 5-best, and above 99% from the first
1000-best). The decrease in performance of the
generative model, as the number of the candidate
increases, suggests that its performance would be
lower than a discriminative model if used alone.
On the other hand, our generative model is able to
select better candidates than the MST parser, when
their number is limited to a few dozens, yielding a
maximum accuracy for k = 7 where it improves
accuracy on the discriminative model by a 0.51%
(around 7% error reduction).
5The features of every dependency relation include infor-
mation about the previously chosen sister of the dependent.
240
k-best Oracle best Oracle worst Reranked
1 92.58 92.58 92.58
2 94.22 88.66 92.89
3 95.05 87.04 93.02
4 95.51 85.82 93.02
5 95.78 84.96 93.02
6 96.02 84.20 93.06
7 96.23 83.62 93.09
8 96.40 83.06 93.02
9 96.54 82.57 92.97
10 96.64 82.21 92.96
100 98.48 73.30 92.32
1000 99.34 64.86 91.47 91.00%92.00%93.00%94.00%
95.00%96.00%97.00%98.00%99.00%
100.00%
1 2 3 4 5 6 7 8 9 10 100 1000
Oracle-BestRerankedMST
Figure 2: UAS accuracy of the MST discriminative and re-ranking parser on section 22 of the WSJ.
Oracle best: always choosing the best result in the k-best, Oracle worst: always choosing the worst,
Reranked: choosing the most probable candidate according to the generative model.
5 Conclusions
We have presented a general framework for depen-
dency parsing based on a combination of discrim-
inative and generative models. We have used this
framework to evaluate and compare several gener-
ative models, including those of Eisner (1996) and
some of their variations. Consistently with earlier
results, none of these models performs better than
the discriminative baseline when used alone. We
have presented an instantiation of this framework
in which our newly defined generative model leads
to an improvement of the state-of-the-art parsing
results, when provided with a limited number of
best candidates. This result suggests that discrim-
inative and generative model are complementary:
the discriminative model is very accurate to filter
out ?bad? candidates, while the generative model
is able to further refine the selection among the
few best candidates. In our set-up it is now pos-
sible to efficiently evaluate many other generative
models and identify the most promising ones for
further investigation. And even though we cur-
rently still need the input from a discriminative
model, our promising results show that pessimism
about the prospects of probabilistic generative de-
pendency models is premature.
Acknowledgments We gratefully acknowledge
funding by the Netherlands Organization for
Scientific Research (NWO): FS and RB are
funded through a Vici-grant ?Integrating Cogni-
tion? (277.70.006) to RB, and WZ through a Veni-
grant ?Discovering Grammar? (639.021.612) of
NWO. We also thank 3 anonymous reviewers for
useful comments.
References
S. Buchholz, and E. Marsi. 2006. CoNLL-X Shared
Task on Multilingual Dependency Parsing. In Proc.
of the 10th CoNLL Conference, pp. 149?164.
M. Collins. 1999. Head-Driven Statistical Models for
Natural Language Parsing. Ph.D. thesis, University
of Pennsylvania.
M. Collins, N. Duffy, and F. Park. 2002. New Ranking
Algorithms for Parsing and Tagging: Kernels over
Discrete Structures, and the Voted Perceptron. In In
Proceedings of the ACL 2002, pp. 263?270.
J. Eisner. 1996a. Three New Probabilistic Models for
Dependency Parsing: An Exploration. In Proc. of
the 16th International Conference on Computational
Linguistics (COLING-96), pp. 340?345.
J. Eisner. 1996b. An Empirical Comparison of Proba-
bility Models for Dependency Grammar. Technical
Report number IRCS-96-11, Univ. of Pennsylvania.
M.P. Marcus, B. Santorini, and M.A. Marcinkiewicz.
1993. Building a large annotated corpus of English:
The Penn Treebank. In Computational Linguistics,
19(2), pp. 313?330.
R. McDonald. 2006. Discriminative Learning and
Spanning Tree Algorithms for Dependency Parsing.
Ph.D. thesis, University of Pennsylvania.
J. Nivre and J. Hall. 2005. MaltParser: A Language-
Independent System for Data-Driven Dependency
Parsing. In Proc. of the Fourth Workshop on Tree-
banks and Linguistic Theories, pp. 137?148.
J. Nivre, J. Hall, S. Ku?bler, R. McDonald, J. Nils-
son,S. Riedel, and D. Yuret. 2007. The CONLL
2007 shared task on dependency parsing. In Proc.
of the CoNLL 2007 Shared Task Session, pp. 915?
932.
F. Sangati and W. Zuidema. 2008. Unsupervised
Methods for Head Assignments. In Proc. of the
EACL 2009 Conference, pp. 701?709.
241
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 84?95,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Accurate Parsing with Compact Tree-Substitution Grammars: Double-DOP
Federico Sangati and Willem Zuidema
Institute for Logic, Language and Computation
University of Amsterdam
Science Park 904, 1098 XH Amsterdam, The Netherlands
{f.sangati,zuidema}@uva.nl
Abstract
We present a novel approach to Data-Oriented
Parsing (DOP). Like other DOP models, our
parser utilizes syntactic fragments of arbitrary
size from a treebank to analyze new sentences,
but, crucially, it uses only those which are
encountered at least twice. This criterion al-
lows us to work with a relatively small but
representative set of fragments, which can be
employed as the symbolic backbone of sev-
eral probabilistic generative models. For pars-
ing we define a transform-backtransform ap-
proach that allows us to use standard PCFG
technology, making our results easily replica-
ble. According to standard Parseval metrics,
our best model is on par with many state-of-
the-art parsers, while offering some comple-
mentary benefits: a simple generative proba-
bility model, and an explicit representation of
the larger units of grammar.
1 Introduction
Data-oriented Parsing (DOP) is an approach to
wide-coverage parsing based on assigning structures
to new sentences using fragments of variable size
extracted from a treebank. It was first proposed by
Scha in 1990 and formalized by Bod (1992), and
preceded many developments in statistical parsing
(e.g., the ?treebank grammars? of Charniak 1997)
and linguistic theory (e.g., the current popularity
of ?constructions?, Jackendoff 2002). A rich lit-
erature on DOP has emerged since, yielding state-
of-the-art results on the Penn treebank benchmark
test (Bod, 2001; Bansal and Klein, 2010) and in-
spiring developments in related frameworks includ-
ing tree kernels (Collins and Duffy, 2002), reranking
(Charniak and Johnson, 2005) and Bayesian adaptor
and fragment grammars (e.g., Johnson et al, 2007;
O?Donnell et al, 2009; Cohn et al, 2010). By for-
malizing the idea of using large fragments of earlier
language experience to analyze new sentences, DOP
captures an important property of language cogni-
tion that has shaped natural language. It therefore
complements approaches that have focused on prop-
erties like lexicalization or incrementality, and might
bring supplementary strengths in other NLP tasks.
Early versions of DOP (e.g., Bod et al, 2003)
aimed at extracting all subtrees of all trees in the
treebank. The total number of constructions, how-
ever, is prohibitively large for non-trivial treebanks:
it grows exponentially with the length of the sen-
tences, yielding the astronomically large number of
approximately 1048 for section 2-21 of the Penn
WSJ corpus. These models thus rely on a big sample
of fragments, which inevitably includes a substan-
tial portion of overspecialized constructions. Later
DOP models have used the Goodman transforma-
tion (Goodman, 1996, 2003) to obtain a compact
representation of all fragments in the treebank (Bod,
2003; Bansal and Klein, 2010). In this case the
grammatical constructions are no longer explicitly
represented, and substantial engineering effort is
needed to optimally tune the models and make them
efficient.
In this paper we present a novel DOP model
(Double-DOP) in which we extract a restricted yet
representative subset of fragments: those recurring
at least twice in the treebank. The explicit represen-
tation of the fragments allows us to derive simple
84
ways of estimating probabilistic models on top of the
symbolic grammar. This and other implementation
choices aim at making the methodology transparent
and easily replicable. The accuracy of Double-DOP
is well within the range of state-of-the-art parsers
currently used in other NLP-tasks, while offering the
additional benefits of a simple generative probability
model and an explicit representation of grammatical
constructions.
The contributions of this paper are summarized as
follows: (i) we describe an efficient tree-kernel algo-
rithm which allows us to extract all recurring frag-
ments, reducing the set of potential elementary units
from the astronomical 1048 to around 106. (ii) We
implement and compare different DOP estimation
techniques to induce a probability model (PTSG)
on top of the extracted symbolic grammar. (iii)
We present a simple transformation of the extracted
fragments into CFG-rules that allows us to use off-
the-shelf PCFG parsing and inference. (iv) We in-
tegrate Double-DOP with recent state-splitting ap-
proaches (Petrov et al, 2006), yielding an even more
accurate parser and a better understanding of the re-
lation between DOP and state-splitting.
The rest of the paper is structured as follows. In
section 2 we describe the symbolic backbone of the
grammar formalism that we will use for parsing.
In section 3 we illustrate the probabilistic exten-
sion of the grammar, including our transformation
of PTSGs to PCFGs that allows us to use a standard
PCFG parser, and a different transform that allows
us to use a standard implementation of the inside-
outside algorithm. In section 4 we present the ex-
perimental setup and the results.
2 The symbolic backbone
The basic idea behind DOP is to allow arbitrarily
large fragments from a treebank to be the elemen-
tary units of production of the grammar. Fragments
can be combined through substitution to obtain the
phrase-structure tree of a new sentence. Figure 1
shows an example of a complete syntactic tree ob-
tained by combining three elementary fragments. As
in previous work, two fragments fi and fj can be
combined (fi ? fj) only if the leftmost substitution
site X? in fi has the same label as the root node of
fj ; in this case the resulting tree will correspond to
fi with fj replacing X . The DOP formalism is dis-
cussed in detail in e.g., Bod et al (2003).
S
NP? VP
VBD
wore
NP? ?
NP
DT
The
NNP
Free
NNP
French
?
NP
JJ
black
NN
arm
NNS
bands
?
S
NP
DT
The
NNP
Free
NNP
French
VP
VBD
wore
NP
JJ
black
NN
arm
NNS
bands
Figure 1: An example of a derivation of a complete syn-
tactic structure (below) obtained combining three ele-
mentary fragments (above) by means of the substitution
operation ?. Substitution sites are marked with ?.
2.1 Finding Recurring Fragments
The first step to build a DOP model is to define its
symbolic grammar, i.e. the set of elementary frag-
ments in the model. In the current work we explic-
itly extract a subset of fragments from the training
treebank. To limit the fragment set size, we use a
simple but heretofore unexplored constraint: we ex-
tract only those fragments that occur two or more
times in the treebank1. Extracting this particular
set of fragments is not trivial, though: a naive ap-
proach that filters a complete table of fragments to-
gether with their frequencies fails because that set, in
a reasonably sized treebank, is astronomically large.
Instead, we use a dynamic programming algorithm
based on tree-kernel techniques (Collins and Duffy,
2001; Moschitti, 2006; Sangati et al, 2010).
The algorithm iterates over every pair of trees in
1More precisely we extract only the largest shared fragments
for all pairs of trees in the treebank. All subtrees of these ex-
tracted fragments necessarily also occur at least twice, but they
are only explicitly represented in our extracted set if they hap-
pen to form a largest shared fragment from another pair of trees.
Hence, if a large tree occurs twice in the treebank the algorithm
will extract from this pair only the full tree as a fragment and
not all its (exponentially many) subtrees.
85
S
NP
PRP
I
VP
VBP
say
SBAR
S
NP
PRP
they
VP
VBP
are
ADJP
JJ
ready
.
.
S
NP
NNS
Analysts
VP
VBP
say
SBAR
S
NP
NNP
USAir
VP
VBZ
has
NP
JJ
great
NN
promise
.
.
S NP PRP VP VBP SBAR S NP PRP VP VBP ADJP JJ .S ? ?NP ? ?NNSVP ? ?VBP ? ?SBAR ?S ? ?NP ? ?NNPVP ?VBZNPJJ ?NN. ?? ?
Figure 2: Left: example of two trees sharing a single maximum fragment, circled in the two trees. Right: the chart
M which is used in the dynamic algorithm to extract all maximum fragments shared between the two trees. The
highlighted cells in the chart are the ones which contribute to extract the shared fragment. The marked cells are those
for which the corresponding nodes in the two tree have equivalent labels but differ in their lists of child nodes.
the treebank to look for common fragments. Fig-
ure 2 shows an example of a pair of trees ??,?? be-
ing compared. The algorithm builds a chartM with
one column for every indexed non-terminal node ?i
in ?, and one row for every indexed non-terminal
node ?j in ?. Each cellM?i, j? identifies a set of in-
dices corresponding to the largest fragment in com-
mon between the two trees starting from ?i and ?j .
This set is empty if ?i and ?j differ in their labels,
or they don?t have the same list of child nodes. Oth-
erwise (if both the labels and the lists of children
match) the set is computed recursively as follows:
M?i, j? = {?i} ??
? ?
c={1,2,...,|ch(?)|}
M?ch(?i, c), ch(?j , c)?
?
? (1)
where ch(?) returns the indices of ??s children, and
ch(?, c) the index of its cth child.
After filling the chart, the algorithm extracts the
set of recurring fragments, and stores them in a ta-
ble to keep track of their counts. This is done by
converting back each fragment implicitly defined in
every cell-set2, and filtering out those that are prop-
erly contained in others.
In a second pass over the treebank, exact counts
are obtained for each fragment in the extracted set.
2A cell-set containing a single index corresponds to the frag-
ment including the node with that index together with all its
children.
Parse trees in the training corpus are not necessarily
covered entirely by recurring fragments; to ensure
coverage, we also include in the symbolic backbone
of our Double-DOP model all PCFG-productions
not included in the set of extracted fragments.
2.2 Comparison with previous DOP work
Explicit grammars The number of recurring frag-
ments in our symbolic grammar, extracted from
the training sections of the Penn WSJ treebank3, is
around 1 million, and thus is significantly lower than
previous work extracting explicit fragments (e.g.,
Bod, 2001, used more than 5 million fragments up
to depth 14).
When looking at the extracted fragments we ask
if we could have predicted which fragments occur
twice or more. Figure 3 attempts to tackle this ques-
tion by reporting some statistics on the extracted
fragments. The majority of fragments are rather
small with a limited number of words or substitution
sites in the frontier. Yet, there is a significant por-
tion of fragments, in the tail of the distribution, with
more than 10 words or substitution sites. Since the
space of all fragments with such characteristics is
enormously large, selecting big recurring fragments
using random sampling technique is like finding a
needle in a haystack. Hence, random sampling pro-
cesses (like Bod, 2001), will tend to represent fre-
3This is after the treebank has been preprocessed. See also
section 4.
S
NP
PRP
I
VP
VBP
say
SBAR
S
NP
PRP
they
VP
VBP
are
ADJP
JJ
ready
.
.
S
NP
NNS
Analysts
VP
VBP
say
SBAR
S
NP
NNP
USAir
VP
VBZ
has
NP
JJ
great
NN
promise
.
.
S NP PRP VP VBP SBAR S NP PRP VP VBP ADJP JJ .S ? ?NP ? ?NNSVP ? ?VBP ? ?SBAR ?S ? ?NP ? ?NNPVP ?VBZNPJJ ?NN. ?? ?
Figure 2: Left: example of two trees sharing a single maximum fragment, circled in the two trees. Right: the chart
M which is used in the dynamic algorithm to extract all maximum fragments shared between the two trees. The
highlighted cells in the chart are the ones which contribute to extract the shared fragment. The marked cells are those
for which the corresponding nodes in the two tree have equivalent labels but differ in their lists of child nodes.
the treebank to look for common fragments. Fig-
ure 2 shows an example of a pair of trees ??,?? be-
ing compared. The algorithm builds a chartM with
one column for every indexed non-terminal node ?i
in ?, and one row for every indexed non-terminal
node ?j in ?. Each cellM?i, j? identifies a set of in-
dices corresponding to the largest fragment in com-
mon between the two trees starting from ?i and ?j .
This set is empty if ?i and ?j differ in their labels,
or they don?t have the same list of child nodes. Oth-
erwise (if both the labels and the lists of children
match) the set is computed recursively as follows:
M?i, j? = {?i} ??
? ?
c={1,2,...,|ch(?)|}
M?ch(?i, c), ch(?j , c)?
?
? (1)
where ch(?) returns the indices of ??s children, and
ch(?, c) the index of its cth child.
After filling the chart, the algorithm extracts the
set of recurring fragments, and stores them in a ta-
ble to keep track of their counts. This is done by
converting back each fragment implicitly defined in
every cell-set2, and filtering out those that are prop-
erly contained in others.
In a second pass over the treebank, exact counts
are obtained for each fragment in the extracted set.
2A cell-set containing a single index corresponds to the frag-
ment including the node with that index together with all its
children.
Parse trees in the training corpus are not necessarily
covered entirely by recurring fragments; to ensure
coverage, we also include in the symbolic backbone
of our Double-DOP model all PCFG-productions
not included in the set of extracted fragments.
2.2 Comparison with previous DOP work
Explicit grammars The number of recurring frag-
ments in our symbolic grammar, extracted from
the training sections of the Penn WSJ treebank3, is
around 1 million, and thus is significantly lower than
previous work extracting explicit fragments (e.g.,
Bod, 2001, used more than 5 million fragments up
to depth 14).
When looking at the extracted fragments we ask
if we could have predicted which fragments occur
twice or more. Figure 3 attempts to tackle this ques-
tion by reporting some statistics on the extracted
fragments. The majority of fragments are rather
small with a limited number of words or substitution
sites in the frontier. Yet, there is a significant por-
tion of fragments, in the tail of the distribution, with
more than 10 words or substitution sites. Since the
space of all fragments with such characteristics is
enormously large, selecting big recurring fragments
using rando sa pling technique is like finding a
needle in a haystack. Hence, rando sa pling pro-
cesses (like Bod, 2001), ill tend to represent fre-
3This is after the treebank has been preprocessed. See also
section 4.
S NP PRP VP VBP SBAR S NP PRP VP VBP ADJP JJ .S ? ?NP ? ?NNSVP ? ?VBP ? ?SBAR ?S ? ?NP ? ?NNPVP ?VBZNPJJ ?NN. ??
Figure 2: Left: t trees sharing a single maximum fragment, ircled in the two trees. Rig t: t e chart
M which is us i t ic algorithm to extract all maximum fragments shared b tween the two trees. The
highlighted cells in the chart are the ones which contribute to extract the shared fragment. The marked cells are those
for which the corresponding nodes in the two tree have equivalent labels but differ in their lists of child nodes.
the treebank to look for common fragments. Fig-
ure 2 shows an example of a pair of trees ??, ?? be-
i g c mpared. The algorithm builds a chartM with
one column fo every ind xed non-terminal node ?i
in ?, and one row for every ndexed non-terminal
node ?j in ?. Each cellM?i, j? identifies a set of in-
dices corresponding to the l gest agment in com-
mon between the two trees starting from ?i and ?j .
This set is empty if ?i and ?j differ i th ir labels,
or they don?t have the same list of child nodes. Oth-
erwise (if both the labels and the lists of children
match) the set is computed recursively as follows:
M?i, j? = {?i} ??
? ?
c={1,2,...,|ch(?)|}
M?ch(?i, c), ch(?j , c)?
?
? (1)
where ch(?) returns the indices of ??s children, and
ch(?, c) the index of its c child.
After filling the chart, the algorithm extracts the
set of recurring fragments, and stores them in a ta-
ble to keep track of their counts. This is done by
converting back each fragment implicitly defined in
every cell-set2, and filtering out those that are prop-
erly contained in others.
In a second pass over the treebank, exact counts
are obtained for each fragment in the extracted set.
2A cell-set containing a single index corresponds to the frag-
ment including the node with that index together with all its
children.
Parse trees in the training corpus are not necessarily
covered entirely by recurring fragments; to ensure
c verage, we also include in the symb lic backbone
of o r Double-DOP model all PCFG-productions
not included in the set of extracted fragments.
2.2 Comparison with previous DOP work
Explicit g am ars The numb r of recurring frag-
ments in our symbolic grammar, extracted from
the training sections of the Penn WSJ treebank3, is
around 1 million, and thus is significantly lower than
previous work extracti g explicit fragments (e.g.,
Bod, 2001, used more than 5 million fragments up
to d pth 14).
When looking at th extra ted fragments we ask
if we could have predicted which fragments occur
twice or more. Figure 3 attempts to tackle this ques-
tion by reporting some statistics on the extracted
fragments. The majority of fragments are rather
small with a limited number of words or substitution
sites in the frontier. Yet, there is a significant por-
tion of fragments, in the tail of the distribution, with
more than 10 words or substitution sites. Since the
space of all fragments with such characteristics is
enormously large, selecting big recurring fragments
using random sampling technique is like finding a
needle in a haystack. Hence, random sampling pro-
cesses (like Bod, 2001), will tend to represent fre-
3This is after the treebank has been preprocessed. See also
section 4.
86
quent recurring constructions such as from NP to
NP or whether S or not, together with infrequent
overspecialized fragments like from Houston to NP,
while missing large generic constructions such as
everything you always wanted to know about NP but
were afraid to ask. These large constructions are
excluded completely by models that only allow ele-
mentary trees up to a certain depth (typically 4 or 5)
into the symbolic grammar (Zollmann and Sima?an,
2005; Zuidema, 2007; Borensztajn et al, 2009), or
only elementary trees with exactly one lexical an-
chor (Sangati and Zuidema, 2009).
100101102103104105106  0
 
10
 
20
 
30
 
40
 
50
Number of Fragments
Depth
 / Wo
rds / S
ubstit
ution 
SitesDepth Words
Subst
itutio
n Site
s
Figure 3: Distribution of the recurring fragments types
according to several features: depth, number of words,
and number of substitution sites. Their corresponding
curves peak at 4 (depth), 1 (words), and 4 (substitution
sites).
Implicit grammars Goodman (1996, 2003) de-
fined a transformation for some versions of DOP to
an equivalent PCFG-based model, with the number
of rules extracted from each parse tree linear in the
size of the trees. This transform, representing larger
fragments only implicitly, is used in most recent
DOP parsers (e.g., Bod, 2003; Bansal and Klein,
2010). Bod has promoted the Goodman transform as
the solution to the computational challenges of DOP
(e.g., Bod, 2003); it?s important to realize, how-
ever, that the resulting grammars are still very large:
WSJ sections 2-21 yield about 2.5 million rules in
the basic version of Goodman?s transform. More-
over, the transformed grammars differ from untrans-
formed DOP grammars in that larger fragments are
no longer explicitly represented. Rather, informa-
tion about their frequency is distributed over many
CFG-rules: if a construction occurs n times and con-
tains m context-free productions, Goodman?s trans-
form uses the weights of 7nm +m rules to encode
this fact. Thus, the information that the idiomatic
fragment (PP (IN ?out?) (PP (IN ?of?) (NP (NN
?town?))))) occurs 3 times in WSJ sections 2-21, is
distributed over 132 rules. This way, an attractive
feature of DOP, viz. the explicit representation of
the ?productive units? of language, is lost4.
In addition, grammars that implicitly encode all
fragments found in a treebank are strongly biased to
over-represent big constructions: the great majority
of the entire set of fragments belongs in fact to the
largest tree in the treebank5. DOP models relying on
Goodman?s transform, need therefore to counteract
this tendency. Bansal and Klein (2010), for instance,
rely on a sophisticated tuning technique to correctly
adjust the weights of the rules in the grammar. In
our Double-DOP approach, instead, the number of
fragments extracted from each tree varies much less
(it ranges between 4 and 1,759). This comparison is
shown in figure 4.
3 The probabilistic model
Like CFG grammars, our symbolic model produces
extremely many parse trees for a given test sentence.
We therefore need to disambiguate between the pos-
sible parses by means of a probability model that as-
signs probabilities to fragments, and defines a proper
distribution over the set of possible full parse trees.
For every nonterminal X in the treebank we have:
?
f?FX
p(f) = 1 (2)
where FX is the set of fragments in our sym-
bolic grammar rooted in X . A derivation d =
f1, f2, . . . , fn of t is a sequence of the fragments that
through left-most substitution produces t. The prob-
ability of a derivation is computed as the product of
4Bansal and Klein (2010) address this issue for contigu-
ous constructions by extending the Goodman transform with
a ?Packed Graph Encoding? for fragments that ?bottom out in
terminals?. However, constructions with variable slots, such as
whether S or not, are left unchanged.
5In fact, the number of extracted fragments increase expo-
nentially with the size of the tree.
87
510102103105101010201050
0
1?104
2?104
3?104
4?104
Number of fragments
Rank
 of tre
e from
 train
 set
Recu
rring 
fragm
ents
All fr
agme
nts
Figure 4: Number of fragments extracted from each tree
in sections 2-21 of the WSJ treebank, when considering
all-fragments (dotted line) and recurring-fragments (solid
line). Trees on the x-axis are ranked according to the
number of fragments. Note the double logarithmic scale
on the y-axis.
the probability of each of its fragments.
P (d) =
?
f?d
p(f) (3)
In section 3.2 we describe ways of obtaining dif-
ferent probability distributions over the fragments in
our grammar. In the following section we assume a
given probabilistic model, and illustrate how to use
standard PCFG parsing.
3.1 Parsing
It is possible to define a simple transform of our
probabilistic fragment grammar, such that off-the-
shelf parsers can be used. In order to perform
the PTSG/PCFG conversion, every fragment in our
grammar must be mapped to a CFG rule which will
keep the same probability as the original fragment.
The corresponding rule will have as the left hand
side the root of the fragment and as the right hand
side its yield, i.e., a sequence of terminals and non-
terminals (substitution sites).
It might occur that several fragments are mapped
to the same CFG rule6. These are interesting cases
of syntactic ambiguity as shown in figure 5. In order
to resolve this problem we need to map each am-
biguous fragment to two unique CFG rules chained
6In our binarized treebank we have 31,465 fragments types
that are ambiguous in this sense.
by a unique artificial node, as shown at the bottom
of the same figure. To the first CFG rule in the chain
we assign the probability of the fragment, while the
second will receive probability 1, so the product
gives back the original probability. The ambiguous
and unambiguous PTSG/PCFG mappings need to be
stored in a table, in order to convert back the com-
pressed CFG derivations to the original PTSG model
after parsing.
Such a transformed PCFG will generate the same
derivations as the original PTSG grammar with iden-
tical probabilities. In our experiment we use a stan-
dard PCFG parser to produce a list of k-best Viterbi
derivations. These, in turn, will be used to maximize
possible objectives as described in section 3.3.
VP
VBD NP
NP
DT NN
PP
IN
?with?
NP
VP
VBD NP
DT NN
PP
IN
?with?
NP
m m
VP
NODE@7276
VP
NODE@7277
NODE@7276
VBD DT NN ?with? NP
NODE@7277
VBD DT NN ?with? NP
Figure 5: Above: example of 2 ambiguous fragments
mapping to the same CFG rule VP ? VBD DT NN
?with? NP. The first fragment occurs 5 times in the train-
ing treebank, (e.g. in the sentence was an executive with
a manufacturing concern) while the second fragment oc-
curs 4 times (e.g. in the sentence began this campaign
with such high hopes). Below: the two pairs of CFG rules
that are used to map the two fragments to separate CFG
derivations.
3.2 Inducing probability distributions
Relative Frequency Estimate (RFE) The sim-
plest way to assign probabilities to fragments is to
make them proportional to their counts7 in the train-
ing set. When enforcing equation 2, that gives the
7We refer to the counts of each fragment as returned by our
extraction algorithm in section 2.1.
88
Relative Frequency Estimate (RFE):
pRFE(f) =
count(f)?
f ??Froot(f) count(f ?)
(4)
Unlike RFE for PCFGs, however, the RFE for
PTSGs has no clear probabilistic interpretation. In
particular, it does not yield the maximum likelihood
solution, and when used as an estimator for an all-
fragments grammar, it is strongly biased since it as-
signs the great majority of the probability mass to
big fragments (Johnson, 2002). As illustrated in fig-
ure 4 this bias is much weaker when restricting the
set of fragments with our approach. Although this
does not solve all theoretical issues, it makes RFE a
reasonable first choice again.
Equal Weights Estimate (EWE) Various other
ways of choosing the weights of a DOP grammar
have been worked out. The best empirical results
have been reported by Bod (2003) with the EWE
proposed by Goodman (2003). Goodman defined it
for grammars in the Goodman transform, but for ex-
plicit grammars it becomes:
wEWE(f) =
?
t?TB
count(f, t)
|{f ? ? t}| (5)
pEWE(f) =
wEWE(f)?
f ??Froot(f) wEWE(f ?)
(6)
where the first sum is over all parse trees t in the tree-
bank (TB), count(f, t) gives the number of times
fragment f occurs in t, and |{f ? ? t}| is the total
number of subtrees of t that were included in the
symbolic grammar.
Maximum Likelihood (ML) For reestimation,
we can aim at maximizing the likelihood (ML) of
the treebank. For this, it turns out that we can de-
fine another transformation of our PTSG, such that
we can apply standard Inside-Outside algorithm for
PCFGs (Lari and Young, 1990). The original ver-
sion of IO is defined over string rewriting PCFGs,
and maximizes the likelihood of the training set con-
sisting of plain sentences. Reestimation shifts prob-
ability mass between alternative parse trees for a
sentence. In contrast, our grammars consist of frag-
ments of various size, and our training set consists
of parse trees. Reestimation here shifts probability
mass between alternative derivations for a parse tree.
Our transformation approach is illustrated with an
example in figure 6. In step (b) the fragments in
the grammar as well as the original parse trees in
the treebank are ?flattened? into bracket notation. In
step (c) each fragment is transformed into a CFG
rule in the transformed meta-grammar, whose right-
hand side is constituted by the bracket notation of
the fragment. Each substitution site X? is raised to
a meta-nonterminal X ?, and all other symbols, in-
cluding parentheses, become meta-terminals. The
left-hand side of the rule is constituted by the origi-
nal root symbol R of the fragment raised to a meta-
nonterminal R?.
The resulting PCFG generates trees in bracket no-
tation, and we can run an of-the-shelf inside-outside
algorithm by presenting it parse trees from the train
corpus in bracket notation8. In the experiments that
we report below we used the RFE from section 3, to
generate the initial weights for the grammar.
(a)
S
A? B
y
?
A
x =
S
A
x
B
y
(b) ( S A? ( B y ) ) ? ( A x ) = ( S ( A x ) ( B y ) )
(c) S?? ( S A? ( B y ) ) ? A?? ( A x ) =
S?
( S A?
( A x )
( B y ) )
(d) ( S ( A x ) ( B y ) )
Figure 6: Rule and tree transforms that turn PTSG rees-
timation into PCFG reestimation; (a) a derivation of the
sentence x y through successive substitutions of elemen-
tary trees from a PTSG; (b) the same elementary trees
and resulting parse tree in bracket notation; (c) an equiva-
lent derivation with the meta-grammar, where the original
substitution sites reappear as meta-nonterminals (marked
with a prime) and all other symbols as meta-terminals;
(d) the yield of the derivation in c.
8However, the results with inside-outside reported in this pa-
per were obtained with an earlier version of our code that uses
an equivalent but special-purpose implementation.
89
3.3 Maximizing Objectives
MPD The easiest objective in parsing, is to se-
lect the most probable derivation (MPD), obtained
by maximizing equation 3.
MPP A DOP grammar can often generate the
same parse tree t through different derivations
D(t) = d1, d2, . . . dm. The probability of t is there-
fore obtained by summing the probabilities of all its
possible derivations.
P (t) =
?
d?D(t)
p(d) =
?
d?D(t)
?
f?d
p(f) (7)
An intuitive objective for a parser is to select, for
a given sentence, the parse tree with highest proba-
bility according to equation 7, i.e., the most probable
parse (MPP): unfortunately, identifying the MPP is
computationally intractable (Sima?an, 1996). How-
ever, we can approximate the MPP by deriving a list
of k-best derivations, summing up the probabilities
of those resulting in the same parse tree, and select
the tree with maximum probability.
MCP, MRS Following Goodman (1998), Sima?an
(1999, 2003), and others, we also consider other
objectives, in particular, the max constituent parse
(MCP), and the max rule sum (MRS).
MCP maximizes a weighted average of the ex-
pected labeled recall L/NC and (approximated) la-
beled precision L/NG under the given posterior dis-
tribution, where L is the number of correctly labeled
constituents, NC the number of constituents in the
correct tree, and NG the number of constituents in
the guessed tree. Recall is easy to maximize since
the estimated NC is constant. L/NC can be in fact
maximized in:
t? = argmax
t
?
lc?t
P (lc) (8)
where lc ranges over all labeled constituents in t
and P (lc) is the marginalized probability of all the
derivation trees in the grammar yielding the sentence
under consideration which contains lc.
Precision, instead, is harder because the denom-
inator NG depends on the chosen guessed tree.
Goodman (1998) proposes to look at another metric
which is strongly correlated with precision, which is
the mistake rate (NG?L)/NC that we want to min-
imize. We combine recall with mistake rate through
linear interpolation:
t? = argmax
t
E( LNC
? ?NG ? LNC
) (9)
= argmax
t
?
lc?t
P (lc)? ?(1? P (lc)) (10)
where 10 is obtained from 9 assuming NC constant,
and the optimal level for ? has to be evaluated em-
pirically.
Unlike MPP, the MCP can be calculated effi-
ciently using dynamic programming techniques over
the parse forest. However, in line with the aims of
this paper to produce an easily reproducible imple-
mentation of DOP, we developed an accurate ap-
proximation of the MCP using a list of k-best deriva-
tions, such as those that can be obtained with an off-
the-shelf PCFG parser.
We do so by building a standard CYK chart,
where every cell corresponds to a specific span in
the test sentence. We store in each cell the proba-
bility of seeing every label in the grammar yielding
the corresponding span, by marginalizing the prob-
abilities of all the parse trees in the obtained k-best
derivations that contains that label covering the same
span. We then compute the Viterbi-best parse maxi-
mizing equation 10.
We implement max rule sum (MRS) in a similar
way, but do not only keep track of labels in every
cell, but of each CFG rule that span the specific yield
(see also Sima?an, 1999, 2003). We haven?t im-
plemented the max rule product (MRP) where pos-
teriors are multiplied instead of added (Petrov and
Klein, 2007; Bansal and Klein, 2010).
4 Experimental Setup
In order to build and test our Double-DOP model9,
we employ the Penn WSJ Treebank (Marcus et al,
1993). We use sections 2-21 for training, section 24
for development and section 23 for testing.
Treebank binarization We start with some pre-
processing of the treebank, following standard prac-
9The software produced for running our model is publicly
available and included in the supplementary material to this pa-
per. To the best of our knowledge this is the first DOP software
released that can be used to parse the WSJ PTB.
90
S
NP|S
NP|S@NNP|NP
DT|NP
The
NNP|NP
Free
NNP|NP
French
VP|S
VBD|VP
wore
NP|VP
NP|VP@NN|NP
JJ|NP
black
NN|NP
arm
NNS|NP
bands
Figure 7: The binarized version of the tree in figure 1,
with H=1 and P=1.
tice in WSJ parsing. We remove traces and func-
tional tags. We apply a left binarization of the train-
ing treebank as in Matsuzaki et al (2005) and Klein
and Manning (2003), setting the horizontal history
H=1 and the parent labeling P=1. This means that
when a node has more than 2 children, the ith child
(for i ? 3) is conditioned on child i ? 1. Moreover
the labels of all non-lexical nodes are enriched with
the labels of their parent node. Figure 7 shows the
binarized version of the tree structure in figure 1.
Unknownwords We replace words appearing less
than 5 times in the training data by one of 50 un-
known word categories based on the presence of lex-
ical features as implemented in Petrov (2009). In
some of the experiments we also perform a smooth-
ing over the lexical elements assigning low counts
( = 0.01) to open-class ?words, PoS-tags? pairs not
encountered in the training corpus10.
Fragment extraction We extract the symbolic
grammar and fragment frequencies from this prepro-
cessed treebank as explained in section 2. This is
the the most time-consuming step (around 160 CPU
hours11).
In the extracted grammar we have in total
1,029,342 recurring fragments and 17,768 unseen
CFG rules. We test several probability distributions
over the fragments (section 3.2) and various maxi-
mization objectives (section 3.3).
10A PoS-tag is an open class if it rewrites to at least 50 differ-
ent words in the training corpus. A word is an open class word
if it has been seen only with open-class PoS-tags.
11Although our code could still be optimized further, it does
already allow for running the job on M CPUs in parallel, reduc-
ing the time required by a factor M (10 hours with 16-CPUs).
86.086.587.087.588.0  
0
 
0.5
 
1 1.1
5
 
1.5
 
2
F1 / Recall / Precision (%)
?M
ax Co
nst. P
arse
Max 
Rule 
Sum
Max 
Proba
ble Pa
rse
Max 
Proba
ble D
erivat
ion
Precis
ion (M
CP)
F1 sc
ore (M
CP)
Recal
l (MCP
)
Figure 8: Double-DOP results on the development sec-
tion (? 40) with different maximizing objectives.
Parsing We convert our PTSG into a PCFG (sec-
tion 3.1) and use Bitpar12 for parsing. For approx-
imating MPP and other objectives we marginalize
probabilities from the 1,000 best derivations.
4.1 Results
We start by presenting in figure 8 the results we ob-
tain on the development set (section 24). Here we
compare the maximizing objectives presented in sec-
tion 3.3, using RFE to obtain the probability distri-
bution over the fragments. We conclude that, em-
pirically, MCP for ? = 1.15, is the best choice to
maximize F1, followed by MRS, MPP, and MPD.
We also compare the various estimators presented
in section 3.2, on the same development set, keep-
ing MCP with ? = 1.15 as the maximizing objec-
tive. We find that RFE is the best estimator (87.2
F113) followed by EWE (86.8) and ML (86.6). Our
best results with ML are obtained when removing
fragments occurring less than 6 times (apart from
CFG-rules) and when stopping at the second iter-
ation. This filtering is done in order to limit the
number of big fragments in the grammar. It is well
known that IO for DOP tends to assign most of the
probability mass to big fragments, quickly overfit-
ting the training data. It is surprising that EWE and
ML perform worse than RFE, in contrast to earlier
findings (Bod, 2003).
12http://www.ims.uni-stuttgart.de/tcl/
SOFTWARE/BitPar.html
13We computed F1 scores with EvalB (http://nlp.cs.
nyu.edu/evalb/) using parameter file new.prm.
91
 
80
 
81
 
82
 
83
 
84
 
85
 
86
 
87
 
88  1
 
10 2
0
 
50
 
1001
04105106107
F1
Number of fragments
Fragm
ent fr
equen
cy thr
eshol
d
F1
Doub
le-DO
P gra
mmar
 size
Num
ber of
 PCFG
 rules
Figure 9: Performance (on the development set) and size
of Double-DOP when considering only fragments whose
occurring frequency in the training treebank is above a
specific threshold (x-axis). In all cases, all PCFG-rules
are included in the grammars. For instance, at the right-
hand side of the plot a grammar is evaluated which in-
cluded only 6754 fragments with a frequency > 100 as
well as 39227 PCFG rules.
We also investigate how a further restriction on
the set of extracted fragments influences the perfor-
mance of our model. In figure 9 we illustrate the
performance of Double-DOP when restricting the
grammar to fragments having frequencies greater
than 1, 2, . . . , 100. We can notice a rather sharp
decrease in performance as the grammar becomes
more and more compact.
Next, we present some results on various Double-
DOP grammars extracted from the same training
treebank after refining it using the Berkeley state-
splitting model14 (Petrov et al, 2006; Petrov and
Klein, 2007). In total we have 6 increasingly refined
versions of the treebank, corresponding to the 6 cy-
cles of the Berkeley model. We observe in figure 10
that our grammar is able to benefit from the state
splits for the first four levels of refinement, reaching
the maximum score at cycle 4, where we improve
over our base model. For the last two data points, the
treebank gets too refined, and using Double-DOP
model on top of it, no longer improves accuracy.
We have also compared our best Double-DOP
14We use the Berkeley grammar labeler following the base
settings for the WSJ: trees are right-binarized, H=0, and
P=0. Berkeley parser package is available at http://code.
google.com/p/berkeleyparser/
 
74
 
76
 
78
 
80
 
82
 
84
 
86
 
88
 
90
 
92
 
1
 
2
 
3
 
4
 
5
 
6
F1
Berke
ley gr
amma
r/tree
bank 
refine
ment 
level
Berke
ley M
RP
Berke
ley M
PD
Doub
le-DO
P
Doub
le-DO
P Lex
 smoo
th
Figure 10: Comparison on section 24 between the per-
formance of Double-DOP (using RFE and MCP with
? = 1.15, H=0, P=0) and Berkeley parser on different
stages of refinement of the treebank/grammar.
base model and the Berkeley parser on per-category
performance. Here we observe an interesting trend:
the Berkeley parser outperforms Double-DOP on
very frequent categories, while Double-DOP per-
forms better on infrequent ones. A detailed com-
parison is included in table 1.
Finally, in table 2 we present our results on the
test set (section 23). Our best model (according to
the best settings on the development set) performs
slightly worse than the one by Bansal and Klein
(2010) when trained on the original corpus, but out-
performs it (and the version of their model with
additional refinements) when trained on the refined
version, in particular for the exact match score.
5 Conclusions
We have described Double-DOP, a novel DOP ap-
proach for parsing, which uses all constructions re-
curring at least twice in a treebank. This method-
ology is driven by the linguistic intuition that con-
structions included in the grammar should prove to
be reusable in a representative corpus.
The extracted set of fragments is significantly
smaller than in previous approaches. Moreover con-
structions are explicitly represented, which makes
them potentially good candidates as semantic or
translation units to be used in other applications.
Despite earlier reported excellent results with
DOP parsers, they are almost never used in other
92
Category % F1 F1label in gold Berkeley Double-DOPNP 41.42 91.4 89.5VP 20.46 90.6 88.6S 13.38 90.7 87.6PP 12.82 85.5 84.1SBAR 3.47 86.0 82.1ADVP 3.36 82.4 81.0ADJP 2.32 68.0 67.3QP 0.98 82.8 84.6WHNP 0.88 94.5 92.0WHADVP 0.33 92.8 91.9PRN 0.32 83.0 77.9NX 0.29 9.50 7.70SINV 0.28 90.3 88.1SQ 0.14 82.1 79.3FRAG 0.10 26.4 34.3SBARQ 0.09 84.2 88.2X 0.06 72.0 83.3NAC 0.06 54.6 88.0WHPP 0.06 91.7 44.4CONJP 0.04 55.6 66.7LST 0.03 61.5 33.3UCP 0.03 30.8 50.0INTJ 0.02 44.4 57.1
Table 1: Comparison of the performance (per-category
F1 score) on the development set between the Berkeley
parser and the best Double-DOP model.
NLP tasks: where other successful parsers often fea-
ture as components of machine translation, semantic
role labeling, question-answering or speech recogni-
tion systems, DOP is conspicuously absent in these
neighboring fields (but for a possible application of
closely related formalisms see, e.g., Yamangil and
Shieber, 2010). The reasons for this are many, but
most important are probably the computational inef-
ficiency of many instances of the approach, the lack
of downloadable software and the difficulties with
replicating some of the key results.
In this paper we have addressed all three obsta-
cles: our efficient algorithm for identifying the re-
current fragments in a treebank runs in polynomial
time. The transformation to PCFGs that we define
allows us to use a standard PCFG parser, while re-
taining the benefit of explicitly representing larger
fragments. A different transform also allows us to
run the popular inside-outside algorithm. Although
IO results are slightly worse than with the naive
relative frequency estimate, it is important to es-
tablish that the standard method for dealing with
latent information (i.e., the derivations of a given
parse) is not the best choice in this case. We expect
that other re-estimation methods, for instance Vari-
test (? 40) test (all)
Parsing Model F1 EX F1 EX
PCFG Baseline
PCFG (H=1, P=1) 77.6 17.2 76.5 15.9
PCFG (H=1, P=1) Lex smooth. 78.5 17.2 77.4 16.0
FRAGMENT-BASED PARSERS
Zuidema (2007)* 83.8 26.9 - -
Cohn et al (2010) MRS 85.4 27.2 84.7 25.8
Post and Gildea (2009) 82.6 - - -
Bansal and Klein (2010) MCP 88.5 33.0 87.6 30.8
Bansal and Klein (2010) MCP 88.7 33.8 88.1 31.7
+ Additional Refinement
THIS PAPER
Double-DOP 87.7 33.1 86.8 31.0
Double-DOP Lex smooth. 87.9 33.7 87.0 31.5
Double-DOP-Sp 88.8 35.9 88.2 33.8
Double-DOP-Sp Lex smooth. 89.7 38.3 89.1 36.1
REFINEMENT-BASED PARSERS
Collins (1999) 88.6 - 88.2 -
Petrov and Klein (2007) 90.6 39.1 90.1 37.1
Table 2: Summary of the results of different parsers
on the test set (sec 23). Double-DOP experiments use
RFE, MCP with ? = 1.15, H=1, P=1; those on state-
splitting (Double-DOP-Sp) use Berkeley cycle 4, H=0,
P=0. Results from Petrov and Klein (2007) already in-
clude smoothing which is performed similarly to our
smoothing technique (see section 4). (* Results on a de-
velopment set, with sentences up to length 20.)
ational Bayesian techniques, could be formulated in
the same manner.
Finally, the availability of our programs, as well
as the third party software that we use, also ad-
dresses the replicability issue. Where some re-
searchers in the field have been skeptical of the DOP
approach to parsing, we believe that our independent
development of a DOP parser adds credibility to the
idea that an approach that uses very many large sub-
trees, can lead to very accurate parsers.
Acknowledgments
We gratefully acknowledge funding by the
Netherlands Organization for Scientific Research
(NWO): FS is funded through a Vici-grant ?Inte-
grating Cognition? (277.70.006) to Rens Bod, and
WZ through a Veni-grant ?Discovering Grammar?
(639.021.612). We also thank Rens Bod, Gideon
Borensztajn, Jos de Bruin, Andreas van Cranen-
burgh, Phong Le, Remko Scha, Khalil Sima?an and
the anonymous reviewers for very useful comments.
93
References
Mohit Bansal and Dan Klein. 2010. Simple, accu-
rate parsing with an all-fragments grammar. In
Proceedings of the 48th Annual Meeting of the
ACL, pages 1098?1107. Association for Compu-
tational Linguistics, Uppsala, Sweden.
Rens Bod. 1992. A computational model of lan-
guage performance: Data oriented parsing. In
Proceedings COLING?92 (Nantes, France), pages
855?859. Association for Computational Linguis-
tics, Morristown, NJ.
Rens Bod. 2001. What is the minimal set of frag-
ments that achieves maximal parse accuracy? In
Proceedings of the ACL. Morgan Kaufmann, San
Francisco, CA.
Rens Bod. 2003. An efficient implementation of a
new DOP model. In Proceedings of the tenth con-
ference on European chapter of the Association
for Computational Linguistics - Volume 1, EACL
?03, pages 19?26. Association for Computational
Linguistics, Morristown, NJ, USA.
Rens Bod, Khalil Sima?an, and Remko Scha. 2003.
Data-Oriented Parsing. University of Chicago
Press, Chicago, IL, USA.
Gideon Borensztajn, Willem Zuidema, and Rens
Bod. 2009. Children?s Grammars Grow More
Abstract with Age?Evidence from an Automatic
Procedure for Identifying the Productive Units of
Language. Topics in Cognitive Science, 1(1):175?
188.
Eugene Charniak. 1997. Statistical parsing with a
context-free grammar and word statistics. In Pro-
ceedings of the Fourteenth National Conference
on Artificial Intelligence, pages 598?603. AAAI
Press/MIT Press.
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-fine n-best parsing and maxent discriminative
reranking. In Proc. 43nd Meeting of Association
for Computational Linguistics (ACL 2005).
Trevor Cohn, Phil Blunsom, and Sharon Goldwa-
ter. 2010. Inducing tree-substitution grammars.
Journal of Machine Learning Research, 11:3053?
3096.
Michael Collins and Nigel Duffy. 2001. Convolu-
tion Kernels for Natural Language. In Thomas G.
Dietterich, Suzanna Becker, and Zoubin Ghahra-
mani, editors, NIPS, pages 625?632. MIT Press.
Michael Collins and Nigel Duffy. 2002. New rank-
ing algorithms for parsing and tagging: Kernels
over discrete structures, and the voted percep-
tron. In Proceedings of 40th Annual Meeting of
the ACL, pages 263?270. Association for Compu-
tational Linguistics, Philadelphia, Pennsylvania,
USA.
Michael J. Collins. 1999. Head-Driven Statistical
Models for Natural Language Parsing. Ph.D. the-
sis, University of Pennsylvania.
Joshua Goodman. 1996. Efficient algorithms for
parsing the DOP model. In Proceedings of
the Conference on Empirical Methods in Natural
Language Processing, pages 143?152.
Joshua Goodman. 2003. Efficient parsing of DOP
with PCFG-reductions. In Bod et al (2003).
Joshua T. Goodman. 1998. Parsing inside-out.
Ph.D. thesis, Harvard University, Cambridge,
MA, USA.
Ray Jackendoff. 2002. Foundations of Language.
Oxford University Press, Oxford, UK.
Mark Johnson. 2002. The dop estimation method is
biased and inconsistent. Computational Linguis-
tics, 28:71?76.
Mark Johnson, Thomas L. Griffiths, and Sharon
Goldwater. 2007. Adaptor grammars: A frame-
work for specifying compositional nonparametric
bayesian models. In Advances in Neural Informa-
tion Processing Systems, volume 16, pages 641?
648.
Dan Klein and Christopher D. Manning. 2003. Ac-
curate unlexicalized parsing. In ACL ?03: Pro-
ceedings of the 41st Annual Meeting on ACL,
pages 423?430. Association for Computational
Linguistics, Morristown, NJ, USA.
K. Lari and S. J. Young. 1990. The estimation
of stochastic context-free grammars using the
inside-outside algorithm. Computer Speech and
Language, 4:35?56.
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and
Beatrice Santorini. 1993. Building a Large An-
notated Corpus of English: The Penn Treebank.
Computational Linguistics, 19(2):313?330.
94
Takuya Matsuzaki, Yusuke Miyao, and Jun?ichi
Tsujii. 2005. Probabilistic cfg with latent anno-
tations. In ACL ?05: Proceedings of the 43rd
Annual Meeting on ACL, pages 75?82. Associa-
tion for Computational Linguistics, Morristown,
NJ, USA.
Alessandro Moschitti. 2006. Efficient Convolution
Kernels for Dependency and Constituent Syntac-
tic Trees. In ECML, pages 318?329. Machine
Learning: ECML 2006, 17th European Confer-
ence on Machine Learning, Proceedings, Berlin,
Germany.
Timothy J. O?Donnell, Noah D. Goodman, and
Joshua B. Tenenbaum. 2009. Fragment Gram-
mars: Exploring Computation and Reuse in Lan-
guage. Technical Report MIT-CSAIL-TR-2009-
013, MIT.
Slav Petrov. 2009. Coarse-to-Fine Natural Lan-
guage Processing. Ph.D. thesis, University of
California at Bekeley, Berkeley, CA, USA.
Slav Petrov, Leon Barrett, Romain Thibaux, and
Dan Klein. 2006. Learning accurate, compact,
and interpretable tree annotation. In ACL-44:
Proceedings of the 21st International Conference
on Computational Linguistics and the 44th an-
nual meeting of the ACL, pages 433?440. Associ-
ation for Computational Linguistics, Morristown,
NJ, USA.
Slav Petrov and Dan Klein. 2007. Improved infer-
ence for unlexicalized parsing. In Human Lan-
guage Technologies 2007: The Conference of the
North American Chapter of the ACL; Proceedings
of the Main Conference, pages 404?411. Asso-
ciation for Computational Linguistics, Rochester,
New York.
Matt Post and Daniel Gildea. 2009. Bayesian learn-
ing of a tree substitution grammar. In Proceed-
ings of the ACL-IJCNLP 2009 Conference Short
Papers, pages 45?48. Association for Computa-
tional Linguistics, Suntec, Singapore.
Federico Sangati and Willem Zuidema. 2009. Unsu-
pervised Methods for Head Assignments. In Pro-
ceedings of the 12th Conference of the European
Chapter of the ACL (EACL 2009), pages 701?
709. Association for Computational Linguistics,
Athens, Greece.
Federico Sangati, Willem Zuidema, and Rens Bod.
2010. Efficiently extract recurring tree fragments
from large treebanks. In Proceedings of the
Seventh conference on International Language
Resources and Evaluation (LREC?10). European
Language Resources Association (ELRA), Val-
letta, Malta.
Remko Scha. 1990. Taaltheorie en taaltechnolo-
gie: competence en performance. In Q. A. M.
de Kort and G. L. J. Leerdam, editors, Com-
putertoepassingen in de Neerlandistiek, LVVN-
jaarboek, pages 7?22. Landelijke Vereniging van
Neerlandici, Almere. [Language theory and
language technology: Competence and Perfor-
mance] in Dutch.
Khalil Sima?an. 1996. Computational complexity
of probabilistic disambiguation by means of tree-
grammars. In Proceedings of the 16th conference
on Computational linguistics, pages 1175?1180.
Association for Computational Linguistics, Mor-
ristown, NJ, USA.
Khalil Sima?an. 1999. Learning Efficient Disam-
biguation. Ph.D. thesis, Utrecht University and
University of Amsterdam.
Khalil Sima?an. 2003. On maximizing metrics for
syntactic disambiguation. In Proceedings of the
International Workshop on Parsing Technologies
(IWPT?03).
Elif Yamangil and Stuart M. Shieber. 2010.
Bayesian synchronous tree-substitution grammar
induction and its application to sentence compres-
sion. In Proceedings of the 48th Annual Meeting
of the ACL, ACL ?10, pages 937?947. Associa-
tion for Computational Linguistics, Stroudsburg,
PA, USA.
Andreas Zollmann and Khalil Sima?an. 2005.
A consistent and efficient estimator for data-
oriented parsing. Journal of Automata, Lan-
guages and Combinatorics, 10(2/3):367?388.
Willem Zuidema. 2007. Parsimonious Data-
Oriented Parsing. In Proceedings of the 2007
Joint Conference on Empirical Methods in Nat-
ural Language Processing and Computational
Natural Language Learning (EMNLP-CoNLL),
pages 551?560. Association for Computational
Linguistics, Prague, Czech Republic.
95
Proceedings of the ACL 2010 Student Research Workshop, pages 19?24,
Uppsala, Sweden, 13 July 2010.
c
?2010 Association for Computational Linguistics
A probabilistic generative model for an intermediate
constituency-dependency representation
Federico Sangati
Institute for Logic, Language and Computation
University of Amsterdam, the Netherlands
f.sangati@uva.nl
Abstract
We present a probabilistic model exten-
sion to the Tesni`ere Dependency Structure
(TDS) framework formulated in (Sangati
and Mazza, 2009). This representation in-
corporates aspects from both constituency
and dependency theory. In addition, it
makes use of junction structures to handle
coordination constructions. We test our
model on parsing the English Penn WSJ
treebank using a re-ranking framework.
This technique allows us to efficiently test
our model without needing a specialized
parser, and to use the standard evalua-
tion metric on the original Phrase Struc-
ture version of the treebank. We obtain
encouraging results: we achieve a small
improvement over state-of-the-art results
when re-ranking a small number of candi-
date structures, on all the evaluation met-
rics except for chunking.
1 Introduction
Since its origin, computational linguistics has
been dominated by Constituency/Phrase Structure
(PS) representation of sentence structure. How-
ever, recently, we observe a steady increase in
popularity of Dependency Structure (DS) for-
malisms. Several researchers have compared
the two alternatives, in terms of linguistic ade-
quacy (Nivre, 2005; Schneider, 2008), practical
applications (Ding and Palmer, 2005), and eval-
uations (Lin, 1995).
Dependency theory is historically accredited to
Lucien Tesni`ere (1959), although the relation of
dependency between words was only one of the
various key elements proposed to represent sen-
tence structures. In fact, the original formulation
incorporates the notion of chunk, as well as a spe-
cial type of structure to represent coordination.
The Tesni`ere Dependency Structure (TDS) rep-
resentation we propose in (Sangati and Mazza,
2009), is an attempt to formalize the original work
of Tesni`ere, with the intention to develop a simple
but consistent representation which combines con-
stituencies and dependencies. As part of this work,
we have implemented an automatic conversion
1
of
the English Penn Wall Street Journal (WSJ) tree-
bank into the new annotation scheme.
In the current work, after introducing the key
elements of TDS (section 2), we describe a first
probabilistic extension to this framework, which
aims at modeling the different levels of the repre-
sentation (section 3). We test our model on parsing
the WSJ treebank using a re-ranking framework.
This technique allows us to efficiently test our sys-
tem without needing a specialized parser, and to
use the standard evaluation metric on the original
PS version of the treebank. In section 3.4 we also
introduce new evaluation schemes on specific as-
pects of the new TDS representation which we will
include in the results presented in section 3.4.
2 TDS representation
It is beyond the scope of this paper to provide an
exhaustive description of the TDS representation
of the WSJ. It is nevertheless important to give the
reader a brief summary of its key elements, and
compare it with some of the other representations
of the WSJ which have been proposed. Figure 1
shows the original PS of a WSJ tree (a), together
with 3 other representations: (b) TDS, (c) DS
2
,
and (d) CCG (Hockenmaier and Steedman, 2007).
1
staff.science.uva.nl/
?
fsangati/TDS
2
The DS representation is taken from the conversion pro-
cedure used in the CoNLL 2007 Shared Task on dependency
parsing (Nivre et al, 2007). Although more elaborate rep-
resentation have been proposed (de Marneffe and Manning,
2008; Cinkov?a et al, 2009) we have chosen this DS repre-
sentation because it is one of the most commonly used within
the CL community, given that it relies on a fully automatic
conversion procedure.
19
(a) (b)
NP
NP
NNS
activities
SBAR
WHNP
WDT
that
S
VP
VBP
encourage
,
,
VBP
promote
CC
or
VBP
advocate
NP
NN
abortion
activities
that encourage , promote or advocate
abortion
N
N J
V
J
V
J
V
J
V
N
N
(c) (d)
NNS
activities
WDTthat VBPencourage ,, VBPpromote
CC
or
VBP
advocate NNabortion
S[dcl]
S[dcl]
NP
NP[nb]/NThe Nrule
S[dcl]\NP(S\NP)/(S\NP)also S[dcl]\NP(S[dcl]\NP)/NPprohibits NPNP
Nfunding
NP\NP(NP\NP)/NPfor NPNP
Nactivities
NP\NP(NP\NP)/(S[dcl]\NP)that S[dcl]\NP(S[dcl]\NP)/NP(S[dcl]\NP)/NPencourage (S[dcl]\NP)/NP[conj]
,
,
(S[dcl]\NP)/NP(S[dcl]\NP)/NPpromote (S[dcl]\NP)/NP[conj]conjor (S[dcl]\NP)/NPadvocate
NP
Nabortion
.
.
Figure 1: Four different structure representations, derived from a sentence of the WSJ treebank (section
00, #977). (a) PS (original), (b) CCG, (c) DS, (d) TDS.
Words and Blocks In TDS, words are di-
vided in functional words (determiners, preposi-
tions, etc.) and content words (verbs, nouns, etc.).
Blocks are the basic elements (chunks) of a struc-
ture, which can be combined either via the depen-
dency relation or the junction operation. Blocks
can be of two types: standard and junction blocks.
Both types may contain any sequence of func-
tional words. Standard blocks (depicted as black
boxes) represent the elementary chunks of the
original PS, and include exactly one content word.
Coordination Junction blocks (depicted as yel-
low boxes) are used to represent coordinated struc-
tures. They contain two or more blocks (con-
juncts) possibly coordinated by means of func-
tional words (conjunctions). In Figure 1(d) the
yellow junction block contains three separate stan-
dard blocks. This representation allows to cap-
ture the fact that these conjuncts occupy the same
role: they all share the relativizer ?that?, they all
depend on the noun ?activities?, and they all gov-
ern the noun ?abortion?. In Figure 1(a,c), we can
notice that both PS and DS do not adequately rep-
resent coordination structures: the PS annotation
is rather flat, avoiding to group the three verbs in a
unique unit, while in the DS the last noun ?abor-
tion? is at the same level of the verbs it should
be a dependent of. On the other hand, the CCG
structure of Figure 1(d), properly represents the
coordination. It does so by grouping the first three
verbs in a unique constituent which is in turn bi-
narized in a right-branching structure. One of the
strongest advantages of the CCG formalism, is
that every structure can be automatically mapped
to a logical-form representation. This is one rea-
son why it needs to handle coordinations properly.
Nevertheless, we conjecture that this representa-
tion of coordination might introduce some diffi-
culties for parsing: it is very hard to capture the
relation between ?advocate? and ?abortion? since
they are several levels away in the structure.
Categories and Transference There are 4 dif-
ferent block categories, which are indicated with
little colored bricks (as well as one-letter abbrevi-
ation) on top and at the bottom of the correspond-
ing blocks: verbs (red, V), nouns (blue, N), ad-
verbs (yellow, A), and adjectives (green, J). Every
block displays at the bottom the original category
determined by the content word (or the original
category of the conjuncts if it is a junction struc-
ture), and at the top, the derived category which
relates to the grammatical role of the whole block
in relation to the governing block. In several cases
we can observe a shift in the categories of a block,
from the original to the derived category. This
phenomenon is called transference and often oc-
curs by means of functional words in the block. In
Figure 1(b) we can observe the transference of the
junction block, which has the original category of
a verb, but takes the role of an adjective (through
the relativizer ?that?) in modifying the noun ?activ-
ities?.
20
P (S) = P
BGM
(S) ? P
BEM
(S) ? P
WFM
(S) (1)
P
BGM
(S) =
?
B ? dependentBlocks(S)
P (B|parent(B), direction(B), leftSibling(B)) (2)
P
BEM
(S) =
?
B ? blocks(S)
P (elements(B)|derivedCat(B)) (3)
P
WFM
(S) =
?
B ? standardBlocks(S)
P (cw(B)|cw(parent(B)), cats(B), fw(B), context(B)) (4)
Table 1: Equation (1) gives the likelihood of a structure S as the product of the likelihoods of generating
three aspects of the structure, according to the three models (BGM, BEM, WFM) specified in equations
(2-4) and explained in the main text.
3 A probabilistic Model for TDS
This section describes the probabilistic generative
model which was implemented in order to dis-
ambiguate TDS structures. We have chosen the
same strategy we have described in (Sangati et al,
2009). The idea consists of utilizing a state of the
art parser to compute a list of k-best candidates of
a test sentence, and evaluate the new model by us-
ing it as a reranker. How well does it select the
most probable structure among the given candi-
dates? Since no parser currently exists for the TDS
representation, we utilize a state of the art parser
for PS trees (Charniak, 1999), and transform each
candidate to TDS. This strategy can be considered
a first step to efficiently test and compare different
models before implementing a full-fledged parser.
3.1 Model description
In order to compute the probability of a given TDS
structure, we make use of three separate proba-
bilistic generative models, each responsible for a
specific aspect of the structure being generated.
The probability of a TDS structure is obtained by
multiplying its probabilities in the three models, as
reported in the first equation of Table 2.
The first model (equation 2) is the Block Gen-
eration Model (BGM). It describes the event of
generating a block B as a dependent of its parent
block (governor). The dependent block B is identi-
fied with its categories (both original and derived),
and its functional words, while the parent block is
characterized by the original category only. More-
over, in the conditioning context we specify the
direction of the dependent with respect to the par-
ent
3
, and its adjacent left sister (null if not present)
specified with the same level of details of B. The
model applies only to dependent blocks
4
.
The second model (equation 3) is the Block Ex-
pansion Model (BEM). It computes the probabil-
ity of a generic block B of known derived cate-
gory, to expand to the list of elements it is com-
posed of. The list includes the category of the
content word, in case the expansion leads to a
standard block. In case of a junction structure, it
contains the conjunctions and the conjunct blocks
(each identified with its categories and its func-
tional words) in the order they appear. Moreover,
all functional words in the block are added to the
list
5
. The model applies to all blocks.
The third model (equation 4) is the Word Fill-
ing Model (WFM), which applies to each stan-
dard block B of the structure. It models the event
of filling B with a content word (cw), given the
content word of the governing block, the cate-
gories (cats) and functional words (fw) of B, and
further information about the context
6
in which B
occurs. This model becomes particularly interest-
3
A dependent block can have three different positions
with respect to the parent block: left, right, inner. The first
two are self-explanatory. The inner case occurs when the de-
pendent block starts after the beginning of the parent block
but ends before it (e.g. a nice dog).
4
A block is a dependent block if it is not a conjunct. In
other words, it must be connected with a line to its governor.
5
The attentive reader might notice that the functional
words are generated twice (in BGM and BEM). This deci-
sion, although not fully justified from a statistical viewpoint,
seems to drive the model towards a better disambiguation.
6
context(B) comprises information about the grandpar-
ent block (original category), the adjacent left sibling block
(derived category), the direction of the content word with re-
spect to its governor (in this case only left and right), and the
absolute distance between the two words.
21
ing when a standard block is a dependent of a junc-
tion block (such as ?abortion? in Figure 1(d)). In
this case, the model needs to capture the depen-
dency relation between the content word of the
dependent block and each of the content words be-
longing to the junction block
7
.
3.2 Smoothing
In all the three models we have adopted a smooth-
ing techniques based on back-off level estima-
tion as proposed by Collins (1999). The different
back-off estimates, which are listed in decreasing
levels of details, are interpolated with confidence
weights
8
derived from the training corpus.
The first two models are implemented with two
levels of back-off, in which the last is a constant
value (10
?6
) to make the overall probability small
but not zero, for unknown events.
The third model is implemented with three lev-
els of back-off: the last is set to the same con-
stant value (10
?6
), the first encodes the depen-
dency event using both pos-tags and lexical infor-
mation of the governor and the dependent word,
while the second specifies only pos-tags.
3.3 Experiment Setup
We have tested our model on the WSJ section of
Penn Treebank (Marcus et al, 1993), using sec-
tions 02-21 as training and section 22 for testing.
We employ the Max-Ent parser, implemented by
Charniak (1999), to generate a list of k-best PS
candidates for the test sentences, which are then
converted into TDS representation.
Instead of using Charniak?s parser in its origi-
nal settings, we train it on a version of the corpus
in which we add a special suffix to constituents
which have circumstantial role
9
. This decision is
based on the observation that the TDS formalism
well captures the argument structure of verbs, and
7
In order to derive the probability of this multi-event we
compute the average between the probabilities of the single
events which compose it.
8
Each back-off level obtains a confidence weight which
decreases with the increase of the diversity of the context
(?(C
i
)), which is the number of separate events occurring
with the same context (C
i
). More formally if f(C
i
) is the
frequency of the conditioning context of the current event,
the weight is obtained as f(C
i
)/(f(C
i
) ? ? ? ?(C
i
)); see
also (Bikel, 2004). In our model we have chosen ? to be
5 for the first model, and 50 for the second and the third.
9
Those which have certain function tags (e.g. ADV, LOC,
TMP). The full list is reported in (Sangati and Mazza, 2009).
It was surprising to notice that the performance of this slightly
modified parser (in terms of F-score) is only slightly lower
than how it performs out-of-the-box (0.13%).
we believe that this additional information might
benefit our model.
We then applied our probabilistic model to re-
rank the list of available k-best TDS, and evalu-
ate the selected candidates using several metrics
which will be introduced next.
3.4 Evaluation Metrics for TDS
The re-ranking framework described above, al-
lows us to keep track of the original PS of each
TDS candidate. This provides an implicit advan-
tage for evaluating our system, viz. it allows us to
evaluate the re-ranked structures both in terms of
the standard evaluation benchmark on the original
PS (F-score) as well as on more refined metrics
derived from the converted TDS representation.
In addition, the specific head assignment that the
TDS conversion procedure performs on the origi-
nal PS, allows us to convert every PS candidate to
a standard projective DS, and from this represen-
tation we can in turn compute the standard bench-
mark evaluation for DS, i.e. unlabeled attachment
score
10
(UAS) (Lin, 1995; Nivre et al, 2007).
Concerning the TDS representation, we have
formulated 3 evaluation metrics which reflect the
accuracy of the chosen structure with respect to the
gold structure (the one derived from the manually
annotated PS), regarding the different components
of the representation:
Block Detection Score (BDS): the accuracy of de-
tecting the correct boundaries of the blocks in the
structure
11
.
Block Attachment Score (BAS): the accuracy
of detecting the correct governing block of each
block in the structure
12
.
Junction Detection Score (JDS): the accuracy of
detecting the correct list of content-words com-
posing each junction block in the structure
13
.
10
UAS measures the percentage of words (excluding punc-
tuation) having the correct governing word.
11
It is calculated as the harmonic mean between recall and
precision between the test and gold set of blocks, where each
block is identified with two numerical values representing the
start and the end position (punctuation words are discarded).
12
It is computed as the percentage of words (both func-
tional and content words, excluding punctuation) having the
correct governing block. The governing block of a word, is
defined as the governor of the block it belongs to. If the block
is a conjunct, its governing block is computed recursively as
the governing block of the junction block it belongs to.
13
It is calculated as the harmonic mean between recall and
precision between the test and gold set of junction blocks ex-
pansions, where each expansion is identified with the list of
content words belonging to the junction block. A recursive
junction structure expands to a list of lists of content-words.
22
F-Score UAS BDS BAS JDS
Charniak (k = 1) 89.41 92.24 94.82 89.29 75.82
Oracle Best F-Score (k = 1000) 97.47 96.98 97.03 95.79 82.26
Oracle Worst F-Score (k = 1000) 57.04 77.04 84.71 70.10 43.01
Oracle Best JDS (k = 1000) 90.54 93.77 96.20 90.57 93.55
PCFG-reranker (k = 5) 89.03 92.12 94.86 88.94 75.88
PCFG-reranker (k = 1000) 83.52 87.04 92.07 82.32 69.17
TDS-reranker (k = 5) 89.65 92.33 94.77 89.35 76.23
TDS-reranker (k = 10) 89.10 92.11 94.58 88.94 75.47
TDS-reranker (k = 100) 86.64 90.24 93.11 86.34 69.60
TDS-reranker (k = 500) 84.94 88.62 91.97 84.43 65.30
TDS-reranker (k = 1000) 84.31 87.89 91.42 83.69 63.65
Table 2: Results of Charniak?s parser, the TDS-reranker, and the PCFG-reranker according to several
evaluation metrics, when the number k of best-candidates increases.
Figure 2: Left: results of the TDS-reranking model according to several evaluation metrics as in Table 2.
Right: comparison between the F-scores of the TDS-reranker and a vanilla PCFG-reranker (together
with the lower and the upper bound), with the increase of the number of best candidates.
3.5 Results
Table 2 reports the results we obtain when re-
ranking with our model an increasing number of
k-best candidates provided by Charniak?s parser
(the same results are shown in the left graph of
Figure 2). We also report the results relative to a
PCFG-reranker obtained by computing the prob-
ability of the k-best candidates using a standard
vanilla-PCFG model derived from the same train-
ing corpus. Moreover, we evaluate, by means of an
oracle, the upper and lower bound of the F-Score
and JDS metric, by selecting the structures which
maximizes/minimizes the results.
Our re-ranking model performs rather well for
a limited number of candidate structures, and out-
performs Charniak?s model when k = 5. In this
case we observe a small boost in performance for
the detection of junction structures, as well as for
all other evaluation metrics, except for the BDS.
The right graph in Figure 2 compares the F-
score performance of the TDS-reranker against the
PCFG-reranker. Our system consistently outper-
forms the PCFG model on this metric, as for UAS,
and BAS. Concerning the other metrics, as the
number of k-best candidates increases, the PCFG
model outperforms the TDS-reranker both accord-
ing to the BDS and the JDS.
Unfortunately, the performance of the re-
ranking model worsens progressively with the in-
crease of k. We find that this is primarily due to
the lack of robustness of the model in detecting the
block boundaries. This suggests that the system
might benefit from a separate preprocessing step
which could chunk the input sentence with higher
accuracy (Sang et al, 2000). In addition the same
module could detect local (intra-clausal) coordina-
tions, as illustrated by (Marin?ci?c et al, 2009).
23
4 Conclusions
In this paper, we have presented a probabilistic
generative model for parsing TDS syntactic rep-
resentation of English sentences. We have given
evidence for the usefulness of this formalism: we
consider it a valid alternative to commonly used
PS and DS representations, since it incorporates
the most relevant features of both notations; in ad-
dition, it makes use of junction structures to repre-
sent coordination, a linguistic phenomena highly
abundant in natural language production, but of-
ten neglected when it comes to evaluating parsing
resources. We have therefore proposed a special
evaluation metrics for junction detection, with the
hope that other researchers might benefit from it
in the future. Remarkably, Charniak?s parser per-
forms extremely well in all the evaluation metrics
besides the one related to coordination.
Our parsing results are encouraging: the over-
all system, although only when the candidates are
highly reliable, can improve on Charniak?s parser
on all the evaluation metrics with the exception of
chunking score (BDS). The weakness on perform-
ing chunking is the major factor responsible for
the lack of robustness of our system. We are con-
sidering to use a dedicated pre-processing module
to perform this step with higher accuracy.
Acknowledgments The author gratefully ac-
knowledge funding by the Netherlands Organiza-
tion for Scientific Research (NWO): this work is
funded through a Vici-grant ?Integrating Cogni-
tion? (277.70.006) to Rens Bod. We also thank
3 anonymous reviewers for very useful comments.
References
Daniel M. Bikel. 2004. Intricacies of Collins? Parsing
Model. Comput. Linguist., 30(4):479?511.
Eugene Charniak. 1999. A Maximum-Entropy-
Inspired Parser. Technical report, Providence, RI,
USA.
Silvie Cinkov?a, Josef Toman, Jan Haji?c, Krist?yna
?
Cerm?akov?a, V?aclav Klime?s, Lucie Mladov?a,
Jana
?
Sindlerov?a, Krist?yna Tom?s?u, and Zden?ek
?
Zabokrtsk?y. 2009. Tectogrammatical Annotation
of the Wall Street Journal. The Prague Bulletin of
Mathematical Linguistics, (92).
Michael J. Collins. 1999. Head-Driven Statistical
Models for Natural Language Parsing. Ph.D. the-
sis, University of Pennsylvania.
Marie-Catherine de Marneffe and Christopher D. Man-
ning. 2008. The Stanford Typed Dependencies
Representation. In Coling 2008: Proceedings of the
workshop on Cross-Framework and Cross-Domain
Parser Evaluation, pages 1?8, Manchester, UK.
Yuan Ding and Martha Palmer. 2005. Machine Trans-
lation Using Probabilistic Synchronous Dependency
Insertion Grammars. In Proceedings of the 43rd An-
nual Meeting of the Association for Computational
Linguistics (ACL?05), pages 541?548.
Julia Hockenmaier and Mark Steedman. 2007. CCG-
bank: A Corpus of CCG Derivations and Depen-
dency Structures Extracted from the Penn Treebank.
Computational Linguistics, 33(3):355?396.
Dekang Lin. 1995. A Dependency-based Method for
Evaluating Broad-Coverage Parsers. In In Proceed-
ings of IJCAI-95, pages 1420?1425.
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and
Beatrice Santorini. 1993. Building a Large Anno-
tated Corpus of English: The Penn Treebank. Com-
putational Linguistics, 19(2):313?330.
Domen Marin?ci?c, Matja?z Gams, and Toma?z
?
Sef. 2009.
Intraclausal Coordination and Clause Detection as a
Preprocessing Step to Dependency Parsing. In TSD
?09: Proceedings of the 12th International Confer-
ence on Text, Speech and Dialogue, pages 147?153,
Berlin, Heidelberg. Springer-Verlag.
Joakim Nivre, Johan Hall, Sandra K?ubler, Ryan Mc-
donald, Jens Nilsson, Sebastian Riedel, and Deniz
Yuret. 2007. The CoNLL 2007 Shared Task on
Dependency Parsing. In Proceedings of the CoNLL
Shared Task Session of EMNLP-CoNLL 2007, pages
915?932, Prague, Czech Republic.
Joakim Nivre. 2005. Dependency Grammar and De-
pendency Parsing. Technical report, V?axj?o Univer-
sity: School of Mathematics and Systems Engineer-
ing.
Erik F. Tjong Kim Sang, Sabine Buchholz, and Kim
Sang. 2000. Introduction to the CoNLL-2000
Shared Task: Chunking. In Proceedings of CoNLL-
2000 and LLL-2000, Lisbon, Portugal.
Federico Sangati and Chiara Mazza. 2009. An English
Dependency Treebank `a la Tesni`ere. In The 8th In-
ternational Workshop on Treebanks and Linguistic
Theories, pages 173?184, Milan, Italy.
Federico Sangati, Willem Zuidema, and Rens Bod.
2009. A generative re-ranking model for depen-
dency parsing. In Proceedings of the 11th In-
ternational Conference on Parsing Technologies
(IWPT?09), pages 238?241, Paris, France, October.
Gerold Schneider. 2008. Hybrid long-distance func-
tional dependency parsing. Ph.D. thesis, University
of Zurich.
Lucien Tesni`ere. 1959. El?ements de syntaxe struc-
turale. Editions Klincksieck, Paris.
24
Proceedings of the ACL 2010 Student Research Workshop, pages 37?42,
Uppsala, Sweden, 13 July 2010.
c?2010 Association for Computational Linguistics
How spoken language corpora can refine
current speech motor training methodologies
Daniil Umanski, Niels O. Schiller
Leiden Institute for Brain and Cognition
Leiden University, The Netherlands
daniil.umanski@gmail.com
N.O.Schiller@hum.leidenuniv.nl
Federico Sangati
Institute for Logic,
Language and Computation
University of Amsterdam, the Netherlands
f.sangati@uva.nl
Abstract
The growing availability of spoken lan-
guage corpora presents new opportunities
for enriching the methodologies of speech
and language therapy. In this paper, we
present a novel approach for construct-
ing speech motor exercises, based on lin-
guistic knowledge extracted from spoken
language corpora. In our study with the
Dutch Spoken Corpus, syllabic inventories
were obtained by means of automatic syl-
labification of the spoken language data.
Our experimental syllabification method
exhibited a reliable performance, and al-
lowed for the acquisition of syllabic tokens
from the corpus. Consequently, the syl-
labic tokens were integrated in a tool for
clinicians, a result which holds the poten-
tial of contributing to the current state of
speech motor training methodologies.
1 Introduction
Spoken language corpora are often accessed by
linguists, who need to manipulate specifically de-
fined speech stimuli in their experiments. How-
ever, this valuable resource of linguistic informa-
tion has not yet been systematically applied for
the benefit of speech therapy methodologies. This
is not surprising, considering the fact that spoken
language corpora have only appeared relatively re-
cently, and are still not easily accessible outside
the NLP community. Existing applications for
selecting linguistic stimuli, although undoubtedly
useful, are not based on spoken language data,
and are generally not designed for utilization by
speech therapists per se (Aichert et al, 2005). As
a first attempt to bridge this gap, a mechanism is
proposed for utilizing the relevant linguistic in-
formation to the service of clinicians. In coor-
dination with speech pathologists, the domain of
speech motor training was identified as an appro-
priate area of application. The traditional speech
motor programs are based on a rather static inven-
tory of speech items, and clinicians do not have
access to a modular way of selecting speech tar-
gets for training.
Therefore, in this project, we deal with develop-
ing an interactive interface to assist speech thera-
pists with constructing individualized speech mo-
tor practice programs for their patients. The prin-
cipal innovation of the proposed system in re-
gard to existing stimuli selection applications is
twofold: first, the syllabic inventories are derived
from spoken word forms, and second, the selec-
tion interface is integrated within a broader plat-
form for conducting speech motor practice.
2 Principles of speech motor practice
2.1 Speech Motor Disorders
Speech motor disorders (SMD) arise from neuro-
logical impairments in the motor systems involved
in speech production. SMD include acquired and
developmental forms of dysarthria and apraxia of
speech. Dysarthria refers to the group of disor-
ders associated with weakness, slowness and in-
ability to coordinate the muscles used to produce
speech (Duffy, 2005). Apraxia of speech (AOS)
is referred to the impaired planning and program-
ming of speech (Ziegler , 2008). Fluency dis-
orders, namely stuttering and cluttering, although
not always classified as SMD, have been exten-
sively studied from the speech motor skill perspec-
tive (Van Lieshout et al, 2001).
2.2 Speech Motor Training
The goal of speech therapy with SMD patients is
establishing and maintaining correct speech mo-
tor routines by means of practice. The process of
learning and maintaining productive speech mo-
tor skills is referred to as speech motor training.
37
An insightful design of speech motor training ex-
ercises is crucial in order to achieve an optimal
learning process, in terms of efficiency, retention,
and transfer levels (Namasivayam, 2008).
Maas et al (2008) make the attempt to relate find-
ings from research on non-speech motor learning
principles to the case of speech motor training.
They outline a number of critical factors in the de-
sign of speech motor exercises. These factors in-
clude the training program structure, selection of
speech items, and the nature of the provided feed-
back.
It is now generally agreed that speech motor exer-
cises should involve simplified speech tasks. The
use of non-sense syllable combinations is a gener-
ally accepted method for minimizing the effects of
higher-order linguistic processing levels, with the
idea of tapping as directly as possible to the motor
component of speech production (Smits-Bandstra
et al, 2006) .
2.3 Selection of speech items
The main considerations in selecting speech items
for a specific patient are functional relevance and
motor complexity. Functional relevance refers
to the specific motor, articulatory or phonetic
deficits, and consequently to the treatment goals
of the patient. For example, producing correct
stress patterns might be a special difficulty for one
patient, while producing consonant clusters might
be challenging for another. Relative motor com-
plexity of speech segments is much less defined in
linguistic terms than, for example, syntactic com-
plexity (Kleinow et al, 2000). Although the part-
whole relationship, which works well for syntactic
constructions, can be applied to syllabic structures
as well (e.g., ?flake? and ?lake?), it may not be the
most suitable strategy.
However, in an original recent work, Ziegler
presented a non-linear probabilistic model of
the phonetic code, which involves units from a
sub-segmental level up to the level of metrical
feet (Ziegler , 2009). The model is verified on
the basis of accuracy data from a large sample of
apraxic speakers, and thus provides a quantitive
index of a speech segment?s motor complexity.
Taken together, it is evident that the task of se-
lecting sets of speech items for an individualized,
optimal learning process is far from obvious, and
much can be done to assist the clinicians with go-
ing through this step.
3 The role of the syllable
The syllable is the primary speech unit used in
studies on speech motor control (Namasivayam,
2008). It is also the basic unit used for con-
structing speech items in current methodologies
of speech motor training (Kent, 2000). Since
the choice of syllabic tokens is assumed to affect
speech motor learning, it would be beneficial to
have access to the syllabic inventory of the spoken
language. Besides the inventory of spoken sylla-
bles, we are interested in the distribution of sylla-
bles across the language.
3.1 Syllable frequency effects
The observation that syllables exhibit an exponen-
tial distribution in English, Dutch and German has
led researchers to infer the existence of a ?men-
tal syllabary? component in the speech production
model (Schiller et al, 1996). Since this hypothesis
assumes that production of high frequency sylla-
bles relies on highly automated motor gestures, it
bears direct consequences on the utility of speech
motor exercises. In other words, manipulating syl-
lable sets in terms of their relative frequency is ex-
pected to have an effect on the learning process of
new motor gestures. This argument is supported
by a number of empirical findings. In a recent
study, Staiger et al report that syllable frequency
and syllable structure play a decisive role with re-
spect to articulatory accuracy in the spontaneous
speech production of patients with AOS (Staiger
et al, 2008). Similarly, (Laganaro, 2008) con-
firms a significant effect of syllable frequency on
production accuracy in experiments with speakers
with AOS and speakers with conduction aphasia.
3.2 Implications on motor learning
In that view, practicing with high-frequency sylla-
bles could promote a faster transfer of skills to ev-
eryday language, as the most ?required? motor ges-
tures are being strengthened. On the other hand,
practicing with low-frequency syllables could po-
tentially promote plasticity (or ?stretching? ) of the
speech motor system, as the learner is required to
assemble motor plans from scratch, similar to the
process of learning to pronounce words in a for-
eign language. In the next section, we describe
our study with the Spoken Dutch Corpus, and il-
lustrate the performed data extraction strategies.
38
4 A study with the Spoken Dutch Corpus
The Corpus Gesproken Nederlands (CGN) is a
large corpus of spoken Dutch
1
. The CGN con-
tains manually verified phonetic transcriptions of
53,583 spoken forms, sampled from a wide vari-
ety of communication situations. A spoken form
reports the phoneme sequence as it was actually
uttered by the speaker as opposed to the canonical
form, which represents how the same word would
be uttered in principle.
4.1 Motivation for accessing spoken forms
In contrast to written language corpora, such as
CELEX (Baayenet al, 1996), or even a corpus
like TIMIT (Zue et al, 1996), in which speak-
ers read prepared written material, spontaneous
speech corpora offer an access to an informal, un-
scripted speech on a variety of topics, including
speakers from a range of regional dialects, age and
educational backgrounds.
Spoken language is a dynamic, adaptive, and gen-
erative process. Speakers most often deviate from
the canonical pronunciation, producing segment
reductions, deletions, insertions and assimilations
in spontaneous speech (Mitterer, 2008). The work
of Greenberg provides an in-depth account on the
pronunciation variation in spoken English. A de-
tailed phonetic transcription of the Switchboard
corpus revealed that the spectral properties of
many phonetic elements deviate significantly from
their canonical form (Greenberg, 1999).
In the light of the apparent discrepancy between
the canonical forms and the actual spoken lan-
guage, it becomes apparent that deriving syllabic
inventories from spoken word forms will approxi-
mate the reality of spontaneous speech production
better than relying on canonical representations.
Consequently, it can be argued that clinical ap-
plications will benefit from incorporating speech
items which optimally converge with the ?live? re-
alization of speech.
4.2 Syllabification of spoken forms
The syllabification information available in the
CGN applies only to the canonical forms of words,
and no syllabification of spoken word forms exists.
The methods of automatic syllabification have
been applied and tested exclusively on canonical
word forms (Bartlett, 2007). In order to obtain
the syllabic inventory of spoken language per se,
1
(see http://lands.let.kun.nl/cgn/)
a preliminary study on automatic syllabification
of spoken word forms has been carried out. Two
methods for dealing with the syllabification task
were proposed, the first based on an n-gram model
defined over sequences of phonemes, and the sec-
ond based on statistics over syllable units. Both
algorithms accept as input a list of possible seg-
mentations of a given phonetic sequence, and re-
turn the one which maximizes the score of the spe-
cific function they implement. The list of possible
segmentations is obtained by exhaustively gener-
ating all possible divisions of the sequence, satis-
fying the condition of keeping exactly one vowel
per segment.
4.3 Syllabification Methods
The first method is a reimplementation of the work
of (Schmid et al, 2007). The authors describe the
syllabification task as a tagging problem, in which
each phonetic symbol of a word is tagged as ei-
ther a syllable boundary (?B?) or as a non-syllable
boundary (?N?). Given a set of possible segmenta-
tions of a given word, the aim is to select the one,
viz. the tag sequence
?
b
n
1
, which is more proba-
ble for the given phoneme sequence p
n
1
, as shown
in equation (1). This probability in equations (3)
is reduced to the joint probability of the two se-
quences: the denominator of equation (2) is in fact
constant for the given list of possible syllabifica-
tions, since they all share the same sequence of
phonemes. Equation (4) is obtained by introduc-
ing a Markovian assumption of order 3 in the way
the phonemes and tags are jointly generated
?
b
n
1
= argmax
b
n
1
P (b
n
1
|p
n
1
) (1)
= argmax
b
n
1
P (b
n
1
, p
n
1
)/P (p
n
1
) (2)
= argmax
b
n
1
P (b
n
1
, p
n
1
) (3)
= argmax
b
n
1
n+1
?
i=1
P (b
i
, p
i
|b
i?1
i?3
, p
i?1
i?3
) (4)
The second syllabification method relies on
statistics over the set of syllables unit and bi-
gram (bisegments) present in the training corpus.
Broadly speaking, given a set of possible segmen-
tations of a given phoneme sequence, the algo-
rithm, selects the one which maximizes the pres-
ence and frequency of its segments.
39
Corpus
Phonemes Syllables
Boundaries Words Boundaries Words
CGN Dutch 98.62 97.15 97.58 94.99
CELEX Dutch 99.12 97.76 99.09 97.70
CELEX German 99.77 99.41 99.51 98.73
CELEX English 98.86 97.96 96.37 93.50
Table 1: Summary of syllabification results on canonical word forms.
4.4 Results
The first step involved the evaluation of the two
algorithms on syllabification of canonical word
forms. Four corpora comprising three different
languages (English, German, and Dutch) were
evaluated: the CELEX2 corpora (Baayenet al,
1996) for the three languages, and the Spoken
Dutch Corpus (CGN). All the resources included
manually verified syllabification transcriptions. A
10-fold cross validation on each of the corpora was
performed to evaluate the accuracy of our meth-
ods. The evaluation is presented in terms of per-
centage of correct syllable boundaries
2
, and per-
centage of correctly syllabified words.
Table 1 summarizes the obtained results. For the
CELEX corpora, both methods produce almost
equally high scores, which are comparable to the
state of the art results reported in (Bartlett, 2007).
For the Spoken Dutch Corpus, both methods
demonstrate quite high scores, with the phoneme-
level method showing an advantage, especially
with respect to correctly syllabified words.
4.5 Data extraction
The process of evaluating syllabification of spo-
ken word forms is compromised by the fact that
there exists no gold annotation for the pronuncia-
tion data in the corpus. Therefore, the next step
involved applying both methods on the data set
and comparing the two solutions. The results re-
vealed that the two algorithms agree on 94.29%
of syllable boundaries and on 90.22% of whole
word syllabification. Based on the high scores re-
ported for lexical word forms syllabification, an
agreement between both methods most probably
implies a correct solution. The ?disagreement? set
can be assumed to represent the class of ambigu-
ous cases, which are the most problematic for au-
tomatic syllabification. As an example, consider
2
Note that recall and precision coincide since the number
of boundaries (one less than the number of vowels) is con-
stant for different segmentations of the same word.
the following pair of possible syllabification, on
which the two methods disagree: ?bEl-kOm-pjut?
vs ?bEl-kOmp-jut?
3
.
Motivated by the high agreement score, we have
applied the phoneme-based method on the spo-
ken word forms in the CGN, and compiled a syl-
labic inventory. In total, 832,236 syllable tokens
were encountered in the corpus, of them 11,054
unique syllables were extracted and listed. The
frequencies distribution of the extracted syllabary,
as can be seen in Figure 1, exhibits an exponential
curve, a result consistent with earlier findings re-
ported in (Schiller et al, 1996). According to our
statistics, 4% of unique syllable tokens account for
80% of all extracted tokens, and 10% of unique
syllables account for 90% respectively. For each
extracted syllable, we have recorded its structure,
frequency rank, and the articulatory characteristics
of its consonants. Next, we describe the speech
items selection tool for clinicians.
Figure 1: Syllable frequency distribution over the
spoken forms in the Dutch Spoken Corpus.
The x-axis represents 625 ranked frequency bins.
The y-axis plots the total number of syllable to-
kens extracted for each frequency bin.
3
A manual evaluation of the disagreement set revealed a
clear advantage for the phoneme-based method
40
5 An interface for clinicians
In order to make the collected linguistic informa-
tion available for clinicians, an interface has been
built which enables clinicians to compose individ-
ual training programs. A training program con-
sists of several training sessions, which in turn
consists of a number of exercises. For each ex-
ercise, a number of syllable sets are selected, ac-
cording to the specific needs of the patient. The
main function of the interface, thus, deals with
selection of customized syllable sets, and is de-
scribed next. The rest of the interface deals with
the different ways in which the syllable sets can
be grouped into exercises, and how exercises are
scheduled between treatment sessions.
5.1 User-defined syllable sets
The process starts with selecting the number of
syllables in the current set, a number between one
and four. Consequently, the selected number of
?syllable boxes? appear on the screen. Each box
allows for a separate configuration of one syllable
group. As can be seen in Figure 2, a syllable box
contains a number of menus, and a text grid at the
bottom of the box.
Figure 2: A snapshot of the part of the interface
allowing configuration of syllable sets
Here follows the list of the parameters which the
user can manipulate, and their possible values:
? Syllable Type
4
? Syllable Frequency
5
4
CV, CVC, CCV, CCVC, etc.
5
Syllables are divided in three rank groups - high,
medium, and low frequency.
? Voiced - Unvoiced consonant
6
? Manner of articulation
7
? Place of articulation
8
Once the user selects a syllable type, he/she can
further specify each consonant within that syllable
type in terms of voiced/unvoiced segment choice
and manner and place of articulation. For the sake
of simplicity, syllable frequency ranks have been
divided in three rank groups. Alternatively, the
user can bypass this criterion by selecting ?any?.
As the user selects the parameters which define the
desired syllable type, the text grid is continuously
filled with the list of syllables satisfying these cri-
teria, and a counter shows the number of syllables
currently in the grid.
Once the configuration process is accomplished,
the syllables which ?survived? the selection will
constitute the speech items of the current exercise,
and the user proceeds to select how the syllable
sets should be grouped, scheduled and so on.
6 Final remarks
6.1 Future directions
A formal usability study is needed in order to
establish the degree of utility and satisfaction with
the interface. One question which demands inves-
tigation is the degrees of choice that the selection
tool should provide. With too many variables
and hinges of choice, the configuration process
for each patient might become complicated and
time consuming. Therefore, a usability study
should provide guidelines for an optimal design
of the interface, so that its utility for clinicians is
maximized.
Furthermore, we plan to integrate the proposed
interface within an computer-based interactive
platform for speech therapy. A seamless integra-
tion of a speech items selection module within
biofeedback games for performing exercises with
these items seems straight forward, as the selected
items can be directly embedded (e.g., as text
symbols or more abstract shapes) in the graphical
environment where the exercises take place.
6
when applicable
7
for a specific consonant. Plosives, Fricatives, Sonorants
8
for a specific consonant. Bilabial, Labio-Dental, Alveo-
lar, Post-Alveolar, Palatal, Velar, Uvular, Glottal
41
Acknowledgments
This research is supported with the ?Mosaic? grant
from The Netherlands Organisation for Scientific
Research (NWO). The authors are grateful for
the anonymous reviewers for their constructive
feedback.
References
Aichert, I., Ziegler, W. 2004. Syllable frequency and
syllable structure in apraxia of speech. Brain and
Language, 88, 148-159.
Aichert, I., Marquardt, C., Ziegler, W. 2005. Fre-
quenzen sublexikalischer Einheiten des Deutschen:
CELEX-basierte Datenbanken. Neurolinguistik, 19,
55-81
Baayen R.H., Piepenbrock R. and Gulikers L. 1996.
CELEX2. Linguistic Data Consortium, Philadel-
phia.
Bartlett, S. 2007. Discriminative approach to auto-
matic syllabication. Masters thesis, Departmentof-
Computing Science, University of Alberta.
Duffy, J.R 2005. Motor speech disorder: Substrates,
Differential Diagnosis, and Management. (2nd Ed.)
507-524. St. Louis, MO: Elsevier Mosby
Greenberg, S. 1999. Speaking in shorthanda syllable-
centric perspective for understanding pronunciation
variation. Speech Comm., 29(2-4):159-176
Kent, R. 2000. Research on speech motor control
and its disorders, a review and prospectives. Speech
Comm., 29(2-4):159-176 J.
Kleinow, J., Smith, A. 2000. Inuences of length and
syntactic complexity on the speech motor stability
of the uent speech of adults who stutter. Journal
of Speech, Language, and Hearing Research, 43,
548559.
Laganaro, M. 2008. Is there a syllable frequency effect
in aphasia or in apraxia of speech or both? Aphasi-
ology, Volume 22, Number 11, November 2008 , pp.
1191-1200(10)
Maas, E., Robin, D.A., Austermann Hula, S.N., Freed-
man, S.E., Wulf, G., Ballard, K.J., Schmidt, R.A.
2008. Principles of Motor Learning in Treatment
of Motor Speech Disorders American Journal of
Speech-Language Pathology, 17, 277-298.
Mitterer, H. 2008. How are words reduced in sponta-
neous speech? In A. Botinis (Ed.), Proceedings of
the ISCA Tutorial and Research Workshop on Ex-
perimental Linguistics (pages 165-168). University
of Athens.
Namasivayam, A.K., van Lieshout, P. 2008. Investi-
gating speech motor practice and learning in people
who stutter Journal of Fluency Disorders 33 (2008)
3251
Schiller, N. O., Meyer, A. S., Baayen, R. H., Levelt, W.
J. M. 1996. A Comparison of Lexeme and Speech
Syllables in Dutch. Journal of Quantitative Linguis-
tics, 3, 8-28.
Schmid H., M?obius B. and Weidenkaff J. 2007. Tag-
ging Syllable Boundaries With Joint N-Gram Mod-
els. Proceedings of Interspeech-2007 (Antwerpen),
pages 2857-2860.
Smits-Bandstra, S., DeNil, L. F., Saint-Cyr, J. 2006.
Speech and non-speech sequence skill learning in
adults who stutter. Journal of Fluency Disorders,
31,116136.
Staiger, A., Ziegler, W. 2008. Syllable frequency and
syllable structure in the spontaneous speech produc-
tion of patients with apraxia of speech. Aphasiol-
ogy, Volume 22, Number 11, November 2008 , pp.
1201-1215(15)
Tjaden, K. 2000. Exploration of a treatment technique
for prosodic disturbance following stroke training.
Clinical Linguistics and Phonetics 2000, Vol. 14,
No. 8, Pages 619-641
Riley, J., Riley, G. 1995. Speech motor improvement
program for children who stutter. In C.W. Stark-
weather, H.F.M. Peters (Eds.), Stuttering (pp.269-
272) New York: Elsevier
Van Lieshout, P. H. H. M. 2001. Recent developments
in studies of speech motor control in stuttering. In B.
Maassen, W. Hulstijn, R. D. Kent, H. F. M. Peters,
P. H. H. M. Van Lieshout (Eds.), Speech motor con-
trol in normal and disordered speech(pp. 286290).
Nijmegen, The Netherlands:Vantilt.
Ziegler W. 2009. Modelling the architecture of pho-
netic plans: Evidence from apraxia of speech. Lan-
guage and Cognitive Processes 24, 631 - 661
Ziegler W. 2008. Apraxia of speech. In: Goldenberg
G, Miller B (Eds.), Handbook of Clinical Neurology,
Vol. 88 (3rd series), pp. 269 - 285. Elsevier. London
Zue, V.W. and Seneff, S. 1996. ?Transcription and
alignment of the TIMIT database. In Recent Re-
search Towards Advanced Man-Machine Interface
Through Spoken Language. H. Fujisaki (ed.), Am-
sterdam: Elsevier, 1996, pp. 515-525.
42
Transactions of the Association for Computational Linguistics, 1 (2013) 111?124. Action Editor: David Chiang.
Submitted 10/2012; Revised 2/2013; Published 5/2013. c?2013 Association for Computational Linguistics.
Incremental Tree Substitution Grammar
for Parsing and Sentence Prediction
Federico Sangati and Frank Keller
Institute for Language, Cognition, and Computation
School of Informatics, University of Edinburgh
10 Crichton Street, Edinburgh EH8 9AB, UK
federico.sangati@gmail.com keller@inf.ed.ac.uk
Abstract
In this paper, we present the first incremental
parser for Tree Substitution Grammar (TSG).
A TSG allows arbitrarily large syntactic frag-
ments to be combined into complete trees;
we show how constraints (including lexical-
ization) can be imposed on the shape of the
TSG fragments to enable incremental process-
ing. We propose an efficient Earley-based al-
gorithm for incremental TSG parsing and re-
port an F-score competitive with other incre-
mental parsers. In addition to whole-sentence
F-score, we also evaluate the partial trees that
the parser constructs for sentence prefixes;
partial trees play an important role in incre-
mental interpretation, language modeling, and
psycholinguistics. Unlike existing parsers, our
incremental TSG parser can generate partial
trees that include predictions about the up-
coming words in a sentence. We show that
it outperforms an n-gram model in predicting
more than one upcoming word.
1 Introduction
When humans listen to speech, the input becomes
available gradually as the speech signal unfolds.
Reading happens in a similarly gradual manner
when the eyes scan a text. There is good evidence
that the human language processor is adapted to this
and works incrementally, i.e., computes an interpre-
tation for an incoming sentence on a word-by-word
basis (Tanenhaus et al, 1995; Altmann and Kamide,
1999). Also language processing systems often deal
with speech as it is spoken, or text as it is being
typed. A dialogue system should start interpreting
a sentence while it is being spoken, and a question
answering system should start retrieving answers be-
fore the user has finished typing the question.
Incremental processing is therefore essential both
for realistic models of human language processing
and for NLP applications that react to user input
in real time. In response to this, a number of in-
cremental parsers have been developed, which use
context-free grammar (Roark, 2001; Schuler et al,
2010), dependency grammar (Chelba and Jelinek,
2000; Nivre, 2007; Huang and Sagae, 2010), or tree-
adjoining grammar (Demberg et al, 2014). Typical
applications of incremental parsers include speech
recognition (Chelba and Jelinek, 2000; Roark, 2001;
Xu et al, 2002), machine translation (Schwartz
et al, 2011; Tan et al, 2011), reading time modeling
(Demberg and Keller, 2008), or dialogue systems
(Stoness et al, 2004). Another potential use of incre-
mental parsers is sentence prediction, i.e., the task
of predicting upcoming words in a sentence given a
prefix. However, so far only n-gram models and clas-
sifiers have been used for this task (Fazly and Hirst,
2003; Eng and Eisner, 2004; Grabski and Scheffer,
2004; Bickel et al, 2005; Li and Hirst, 2005).
In this paper, we present an incremental parser for
Tree Substitution Grammar (TSG). A TSG contains
a set of arbitrarily large tree fragments, which can be
combined into new syntax trees by means of a sub-
stitution operation. An extensive tradition of parsing
with TSG (also referred to as data-oriented parsing)
exists (Bod, 1995; Bod et al, 2003), but none of the
existing TSG parsers are incremental. We show how
constraints can be imposed on the shape of the TSG
fragments to enable incremental processing. We pro-
pose an efficient Earley-based algorithm for incre-
mental TSG parsing and report an F-score competi-
tive with other incremental parsers.
111
TSG fragments can be arbitrarily large and can
contain multiple lexical items. This property enables
our incremental TSG parser to generate partial parse
trees that include predictions about the upcoming
words in a sentence. It can therefore be applied di-
rectly to the task of sentence prediction, simply by
reading off the predicted items in a partial tree. We
show that our parser outperforms an n-gram model
in predicting more than one upcoming word.
The rest of the paper is structured as follows. In
Section 2, we introduce the ITSG framework and re-
late it to the original TSG formalism. Section 3 de-
scribes the chart-parser algorithm, while Section 4
details the experimental setup and results. Sections 5
and 6 present related work and conclusions.
2 Incremental Tree Substitution Grammar
The current work is based on Tree Substitu-
tion Grammar (TSG, Schabes 1990; for a recent
overview see Bod et al 2003). A TSG is composed
of (i) a set of arbitrarily large fragments, usually ex-
tracted from an annotated phrase-structure treebank,
and (ii) the substitution operation by means of which
fragments can be combined into complete syntactic
analyses (derivations) of novel sentences.
Every fragment?s node is either a lexical node
(word), a substitution site (a non-lexical node in the
yield of the structure),1 or an internal node. An inter-
nal node must always keep the same daughter nodes
as in the original tree. For an example of a binarized2
tree and a fragment extracted from it see Figure 1.
A TSG derivation is constructed in a top-down
generative process starting from a fragment in the
grammar rooted in S (the unique non-lexical node
all syntactic analysis are rooted in). A partial deriva-
tion is extended by subsequently introducing more
fragments: if X is the left-most substitution site in
the yield of the current partial derivation, a fragment
1For example nodes NP, VP, S@ are the substitution sites of
the right fragment in Figure 1.
2The tree is right-binarized via artificial nodes with @ sym-
bols, as explained in Section 4.1. The original tree is
S
.
?.?
VP
VP
VBN
?disclosed?
VBD
?were?
NP
NNS
?Terms?
S
S@
S@
.
?.?
VP
VP
VBN
?disclosed?
VBD
?were?
NP
NNS
?Terms?
S
S@
S@VP
VPVBD
?were?
NP
Figure 1: An example of a binarized2 parse tree and a
lexicalized fragment extracted from it.
rooted in X is chosen from the grammar and sub-
stituted into it. When there are no more substitution
sites (all nodes in the yield are lexical items) the gen-
erative process terminates.
2.1 Incrementality
In this work we are interested in defining an incre-
mental TSG (in short ITSG). The new generative
process, while retaining the original mechanism for
combining fragments (by means of the substitution
operation), must ensure a way for deriving syntactic
analyses of novel sentences in an incremental man-
ner, i.e., one word at the time from left to right. More
precisely, at each stage of the generative process, the
partially derived structure must be connected (as in
standard TSG) and have a prefix of the sentence at
the beginning of its yield. A partial derivation is con-
nected if it has tree shape, i.e., all the nodes are dom-
inated by a common root node (which does not nec-
essarily have to be the root node of the sentence).
For instance, the right fragment in Figure 1 shows a
possible way of starting a standard TSG derivation
which does not satisfy the incrementality constraint:
the partial derivation has a substitution site as the
first element in its yield.
In order to achieve incrementality while maintain-
ing connectedness, we impose one further constraint
on the type of fragments which are allowed in an
ITSG: each fragment should be lexicalized, i.e., con-
tain at least one word (lexical anchor) at the first or
the second position in its yield. Allowing more than
one substitution site at the beginning of a fragment?s
yield would lead to a violation of the incrementality
requirement (as will become clear in Section 2.2).
112
The generative process starts with a fragment an-
chored in the first word of the sentence being gener-
ated. At each subsequent step, a lexicalized fragment
is introduced (by means of the substitution opera-
tion) to extend the current partial derivation in such
a way that the prefix of the yield of the partial struc-
ture is lengthened by one word (the lexical anchor of
the fragment being introduced). The lexicalization
constraint allows a fragment to have multiple lexical
items, not necessarily adjacent to one another. This
is useful to capture the general ability of TSG to pro-
duce in one single step an arbitrarily big syntactic
construction ranging from phrasal verbs (e.g., ask
someone out), to parallel constructions (e.g., either
X or Y), and idiomatic expressions (e.g., took me
to the cleaners). For an example of a fragment with
multiple lexical anchors see the fragment in the mid-
dle of Figure 2.
2.2 Symbolic Grammar
An ITSG is a tuple ?N ,L ,F ,4,5,?, where N
and L are the set of non-lexical and lexical nodes
respectively, F is a collection of lexicalized frag-
ments, 4 and 5 are two variants of the substitution
operation (backward and forward) used to combine
fragments into derivations, and  is the stop opera-
tion which terminates the generative process.
Fragments A fragment f ?F belongs to one of
the three sets Finit ,F Xlex,FYsub:
? An initial fragment ( finit) has the lexical anchor
in the first position of the yield, being the initial
word of a sentence (the left-most lexical node
of the parse tree from which it was extracted).
? A lex-first fragment ( f Xlex) has the lexical anchor
(non sentence-initial) in the first position of the
yield, and is rooted in X .3
? A sub-first fragment ( f Ysub) has the lexical an-
chor in the second position of its yield, and a
substitution site Y in the first.
Fringes We will use fringes (Demberg et al,
2014) as a compressed representation of fragments,
3A fragment can be both an initial and a lex-first fragment
(e.g., if the lexical anchor is a proper noun). We will make use
of two separate instances of the same fragment in the two sets.
NP
NNS
?Terms?
5 S
S@
S@
.
?.?
VP
VPVBD
?were?
NP
4 VP
VBN
?disclosed?
S
Figure 2: An example of an ITSG derivation yielding the
tree on the left side of Figure 1. The second and third frag-
ment are introduced by means of forward and backward
substitution, respectively.
in which the internal structure is replaced by a trian-
gle (a or ) and only the root and the yield are vis-
ible. It is possible in a grammar that multiple frag-
ments map to the same fringe; we will refer to those
as ambiguous fringes. We use both vertical (a, e.g.,
in Figure 3 and 4) and horizontal () fringe nota-
tion. The latter is used for describing the states in our
chart-parsing algorithm in Section 3. For instance,
the horizontal fringe representation of the right frag-
ment in Figure 1 is S  NP ?were? VP S@.
Incremental Derivation An incremental deriva-
tion is a sequence of lexicalized fragments
? f1, f2, . . . , fn? which, combined together in the
specified order, give rise to a complete parse tree
(see Figure 2 for an example). The first fragment f1
being introduced in the derivation must be an initial
fragment, and its lexical anchor constitutes the one-
word prefix of the sentence being generated. Sub-
sequent fragments are introduced by means of the
substitution operation, which has two variants: back-
ward substitution (4), which is used to substitute
lex-first fragments into the partial derivation gener-
ated so far, and forward substitution (5), which is
used to substitute sub-first fragments into the partial
derivation. After a number of fragments are intro-
duced, a stop operation () may terminate the gen-
erative process.
Operations The three ITSG operations take place
under specific conditions within an incremental
derivation, as illustrated in Figure 3 and explained
hereafter. At a given stage of the generative process
(after an initial fragment has been inserted), the con-
nected partial structure may or may not have sub-
113
Partial Structure Operation Accepted Fragment Resulting Structure Terminated
Y
`1 lex. . . `i X ?. . .
4
(backward) X
`i+1 ?. . .
Y
`1 lex. . . `i+1 ?. . . ?. . .
NO
Y
`1 lex. . . `i
5
(forward) X
Y `i+1 ?. . .
X
`1 lex. . . `i `i+1 ?. . .
NO
Y
`1 lex. . . `n

(stop) ?
Y #
?
`1 lex. . . `n #
YES
Figure 3: Schemata of the three ITSG operations. All tree structures (partial structure and fragments) are represented
in a compact notation, which displays only the root nodes and the yields. The i-th words in the structure?s yield is
represented as `i, while ? and ? stands for any (possibly empty) sequence of words and substitution sites.
stitution sites present in its yield. In the first case,
a backward substitution (4) must take place in the
following generative step: if X is the left-most sub-
stitution site, a new fragment of type f Xlex is chosen
from the grammar and substituted into X . If the par-
tially derived structure has no substitution site (all
the nodes in its yield are lexical nodes) and it is
rooted in Y , two possible choices exist: either the
generative process terminates by means of the stop
operation (Y ), or the generative process contin-
ues. In the latter case a forward substitution (5) is
performed: a new f Ysub fragment is chosen from the
grammar, and the partial structure is substituted into
the left-most substitution site Y of the fragment.4
Multiple Derivations As in TSG, an ITSG may
be able to generate the same parse tree in multiple
ways: multiple incremental derivations yielding the
same tree. Figure 4 shows one such example.
Generative Capacity It is useful to clarify the dif-
ference between ITSG and the more general TSG
formalism in terms of generative capacity. Although
both types of grammar make use of the substitu-
tion operation to combine fragments, an ITSG im-
poses more constraints on (i) the type of fragments
which are allowed in the grammar (initial, lex-first,
4A stop operation can be viewed as a forward substitution
when using an artificial sub-first fragment ?Y # (stop frag-
ment), where # is an artificial lexical node indicating the termi-
nation of the sentence. For simplicity, stop fragments are omit-
ted in Figure 2 and 4 and Y is attached to the stop symbol (Y ).
S
S@
S@
.
?.?
VP
VP
VBN
?disclosed?
VBD
?were?
NP
NNS
?Terms?
NP
?Terms?
5 S
NP ?were? VP ?.?
4 VP
?disclosed?
 S
S
?Terms? S@
4 S@
?were? VP ?.?
4 VP
?disclosed?
 S
Figure 4: Above: an example of a set of fragments ex-
tracted from the tree in Figure 1. Below: two incremental
derivations that generate it. Colors (and lines strokes) in-
dicate which derivation fragments belong to.
and sub-first fragments), and (ii) the generative pro-
cess with which fragments are combined (incremen-
tally left to right instead of top-down). If we com-
pare a TSG and an ITSG on the same set of (ITSG-
compatible) fragments, then there are cases in which
the TSG can generate more tree structures than the
ITSG.
In the following, we provide a more formal char-
acterization of the strong and weak generative power
114
S
X?a?
X
?c?X
X
?b?
S
X
?c?X
?c?X
?c?X
?b?
?a?
Figure 5: Left: an example of a CFG with left recursion.
Right: one of the structures the CFG can generate.
of ITSG with respects to context-free grammar
(CFG) and TSG. (However, a full investigation of
this issue is beyond the scope of this paper.) We can
limit our analysis to CFG, as TSG is strongly equiv-
alent to CFG. The weak equivalence between ITSG
and CFG is straightforward: for any CFG there is
a way to produce a weakly equivalent grammar in
Greibach Normal Form in which any production has
a right side beginning with a lexical item (Aho and
Ullman, 1972). The grammar that results from this
transformation is an ITSG which uses only back-
ward substitutions.
Left-recursion seems to be the main obstacle for
strong equivalence between ITSG and CFG. As an
example, the left side of Figure 5 shows a CFG that
contains a left-recursive rule. The types of structures
this grammar can generate (such as the one given on
the right side of the same figure) are characterized by
an arbitrarily long chain of rules that can intervene
before the second word of the string, ?b?, is gener-
ated. Given the incrementality constraints, there is
no ITSG that can generate the same set of struc-
tures that this CFG can generate. However, it may
be possible to circumvent this problem by applying
the left-corner transform (Rosenkrantz and Lewis,
1970; Aho and Ullman, 1972) to generate an equiv-
alent CFG without left-recursive rules.
2.3 Probabilistic Grammar
In the generative process presented above there are
a number of choices which are left open, i.e., which
fragment is being introduced at a specific stage of
a derivation, and when the generative process ter-
minates. A symbolic ITSG can be equipped with
a probabilistic component which deals with these
choices. A proper probability model for ITSG needs
to define three probability distributions over the
three types of fragments in the grammar, such that:
?
finit?Finit
P( finit) = 1 (1)
?
f Xlex?FXlex
P( f Xlex) = 1 (?X ?N ) (2)
P(Y )+ ?
fYsub?FYsub
P( f Ysub) = 1 (?Y ?N ) (3)
The probability that an ITSG generates a specific
derivation d is obtained by multiplying the probabil-
ities of the fragments taking part in the derivation:
P(d) =?
f?d
P( f ) (4)
Since the grammar may generate a tree t via multiple
derivations D(t) = d1,d2, . . . ,dm, the probability of
the parse tree is the sum of the probabilities of the
ITSG derivations in D(t):
P(t) = ?
d?D(t)
P(d) = ?
d?D(t)
?
f?d
P( f ) (5)
3 Probabilistic ITSG Parser
We introduce a probabilistic chart-parsing algorithm
to efficiently compute all possible incremental de-
rivations that an ITSG can generate given an input
sentence (presented one word at the time). The pars-
ing algorithm is an adaptation of the Earley algo-
rithm (Earley, 1970) and its probabilistic instantia-
tion (Stolcke, 1995).
3.1 Parsing Chart
A TSG incremental derivation is represented in the
chart as a sequence of chart states, i.e., a path.
For a given fringe in an incremental derivation,
there will be one or more states in the chart, depend-
ing on the length of the fringe?s yield. This is be-
cause we need to keep track of the extent to which
the yield of each fringe has been consumed within
a derivation as the sentence is processed incremen-
tally.5 At the given stage of the derivation, the states
offer a compact representation over the partial struc-
tures generated so far.
5A fringe (state) may occur in multiple derivations (paths):
for instance in Figure 4 the two derivations will correspond to
two separate paths that will converge to the same fringe (state).
115
Start(`0) X  `0?
0 : 0X ?`0? [?,?,?]
?= ? = P(X  `0?) ? = ?(1 : 0X  `0 ??)
Backward Substitution(`i)i : kX ??Y ? [?,?,?] Y  `i?
i : iY ?`i? [??,??,??]
?? += ? ?P(Y  `i?) ?? = P(Y  `i?)
Forward Substitution(`i)i : 0Y ?? [?,?,?] X Y `i?
i : 0X Y ? `i? [??,??,??]
?? += ? ?P(X Y `i?)?? += ? ?P(X Y `i?) ?
+= ?? ?P(X Y `i?)
Completioni : jY  ` j?? [?,?,?] j : kX ??Y ? [??,??,??]
i : kX ?Y ?? [???,???,???]
??? += ?? ? ?
??? += ?? ? ?
? += ??? ? ??
?? += ??? ? ???
Scan(`i) i : kX ?? `i? [?,?,?]
i+1 : kX ?`i ?? [??,??,??]
?? = ??? = ? ? = ??
Stop(#) n : 0Y ?? [?= ?,?] ?Y #
n : 0?Y ?# [??,??,??]
?? = ?? = ? ?P(Y ) ?? = 1? = P(Y )
Figure 6: Chart operations with forward (?), inner (?),
and outer (?) probabilities.
Each state is composed of a fringe and some ad-
ditional information which keeps track of where the
fringe is located within a path. A chart state can be
generally represented as
i : kX ??? (6)
where X ?? is the state?s fringe, Greek letters are
(possibly empty) sequences of words and substitu-
tion sites, and ? is a placeholder indicating to which
extent the fragment?s yield has been consumed: all
the elements in the yield preceding the dot have
been already accepted. Finally, i and k are indices
of words in the input sentence:
? i signifies that the current state is introduced
after the first i words in the sentence have
been scanned. All states in the chart will be
grouped according to this index, and will con-
stitute state-set i.
? k indicates that the fringe associated with the
current state was first introduced in the chart
after the first k words in the input sentence had
been scanned. The index k is therefore called
the start index.
For instance when generating the first incremental
derivation in Figure 4, the parser will pass through
state 1 : 1S  NP ? ?were? VP ?.? indicating that
the second fringe is introduced right after the parser
has scanned the first word in the sentence and before
having scanned the second word.
3.2 Parsing Algorithm
We will first introduce the symbolic part of the
parsing algorithm, and then discuss its probabilistic
component in Section 3.3. In line with the generative
process illustrated in Section 2.2, the parser operates
on the chart states in order to keep track of all pos-
sible ITSG derivations as new words are fed in. It
starts by reading the first word `0 and introducing
new states to state-set 0 in the chart, those mapping
to initial fragments in the grammar with `0 as lexi-
cal anchor. At a given stage, after i words have been
scanned, the parser reads the next word (`i) and in-
troduces new states in state-sets i and i+1 by apply-
ing specific operations on states present in the chart,
and fringes in the grammar.
Parser Operations The parser begins with the
start operation just described, and continues with a
cycle of four operations for every word in the input
sentence `i (for i ? 0). The order of the four opera-
tions is the following: completion, backward substi-
tution (4), forward substitution (5), and scan. When
there are no more words in input, the parser termi-
nates with a stop operation. We will now describe
the parser operations (see Figure 6 for their formal
definition), ignoring the probabilities for now.
Start(`0): For every initial fringe in the grammar
anchored in `0, the parser inserts a (scan) state for
that fringe in the state-set 0.
116
Backward Substitution(`i) applies to acceptor
states, i.e., those with a substitution site following
the dot, say X . For each acceptor state in state-set i,
and any lex-first fringe in the grammar rooted in X
and anchored in `i, the parser inserts a (scan) state
for that fringe in state-set i.
Forward Substitution(`i) applies to donor
states, i.e., those that have no elements following
the dot and with start index 0. For each donor state
in state-set i, rooted in Y , and any sub-first fringe in
the grammar with Y as the left-most element in its
yield, the parser inserts a (scan) state for that fringe
in state-set i, with the dot placed after Y .
Completion applies to complete states, i.e., those
with no elements following the dot and with start
index j > 0. For every complete state in state-set i,
rooted in Y , with starting index j, and every acceptor
state in set j with Y following the dot, the parser
inserts a copy of the acceptor state in state-set i, and
advances the dot.
Scan(`i) applies to scan states, i.e., those with a
word after the dot. For every scan state in state-set
i having `i after the dot, the parser inserts a copy of
that state in state-set (i+1), and advances the dot.
Stop(#) is a special type of forward substitution
and applies to donor states, but only when the input
word is the terminal symbol #. For every donor state
in state-set n (the length of the sentence), if the root
of the fringe?s state is Y , the parser introduces a stop
state whose fringe is a stop fringe with Y as the left
most substitution site.
Comparison with the Earley Algorithm It is
useful to clarify the differences between the pro-
posed ITSG parsing algorithm and the original Ear-
ley algorithm. Primarily, the ITSG parser is based
on a left-right processing order, whereas the Ear-
ley algorithm uses a top-down generative process.
Moreover, our parser presupposes a restricted in-
ventory of fragments in the grammar (the ones al-
lowed by an ITSG) as opposed to the general CFG
rules allowed by the Earley algorithm. In particular,
the Backward Substitution operation is more limited
than the corresponding Prediction step of the Earley
algorithm: only lex-first fragments can be introduced
using Backward Substitution, and therefore left re-
cursion (allowed by the Earley algorithm) is not pos-
sible here.6 This restriction is compensated for by
the existence of the Forward Substitution operation,
which has no analog in the Earley algorithm.7 The
worst case complexity of Earley algorithm is domi-
nated by the Completion operation which is identical
to that in our parser, and therefore the original total
time complexity applies, i.e., O(l3) for an input sen-
tence of length l, and O(n3) in terms of the number
of non-lexical nodes n in the grammar.
Derivations Incremental (partial) derivations are
represented in the chart as (partial) paths along
states. Each state can lead to one or more succes-
sors, and come from one or more antecedents. Scan
is the only operation which introduces, for every
scan state, a new single successor state (which can
be of any of the four types) in the following state-
set. Complete states may lead to several states within
the current state-set, which may belong to any of the
four types. An acceptor state may lead to a number
of scan states via backward substitution (depending
on the number of lex-first fringes that can combine
with it). Similarly, a donor state may lead to a num-
ber of scan states via forward substitution.
After i words have been scanned, we can retrieve
(partial) paths from the chart. This is done in a back-
ward direction starting from scan states in state-set i
all the way back to the initial states. This is possible
since all the operations are reversible, i.e., given a
state it is possible to retrieve its antecedent state(s).
As an example, consider the ITSG grammar con-
sisting of the fragments in Figure 7 and the two de-
rivations of the same parse tree in the same figure;
Figure 7 represents the parsing chart of the same
grammar, containing the two corresponding paths.
3.3 Probabilistic Parser
In the probabilistic version of the parser, each fringe
in the grammar has a given probability, such that
Equations (1)?(3) are satisfied.8 In the probabilistic
chart, every state i : kX??? is decorated with three
6This further simplifies the probabilistic version of our
parser, as there is no need to resort to the probabilistic reflex-
ive, transitive left-corner relation described by Stolcke (1995).
7This operation would violate Earley?s top-down constraint;
donor states are in fact the terminal states in Earley algorithm.
8The probability of an ambiguous fringe is the marginal
probability of the fragments mapping to it.
117
0 ? ?Terms?
S | 0NP? ?Terms? [1/2, 1/2, 1]|| 0S? ?Terms? S@ [1/2, 1/2, 1]
1 ? ?were?
S | 0SNP ? ?were? V P ?.? [1/2, 1/2, 1]|| 1S@? ?were? V P ?.? [1/2, 1, 1/2]
4 || 0S ?Terms? ? S@ [1/2, 1/2, 1] ? || S@ ?were? V P ?.? [1]
5 | 0NP ?Terms? ? [1/2, 1/2, 1] | SNP ?were? V P ?.? [1]
2 ? ?disclosed?
S 2V P? ?disclosed? [1, 1, 1]
4 | 0SNP ?were? ? V P ?.? [1/2, 1/2, 1] ??|| 1S@ ?were? ? V P ?.? [1/2, 1, 1/2] ??? V P ?disclosed? [1]
3 ? ?.?
S | 0SNP ?were? V P ? ?.? [1/2, 1/2, 1]|| 1S@ ?were? V P ? ?.? [1/2, 1, 1/2]
C 2V P ?disclosed? ? [1, 1, 1] | **|| ***
4 ? #
S 0?S ? # [1, 1, 1]
 || 0S ?Terms? S@ ? [1/2, 1/2, 1]| 0SNP ?were? V P ?.? ? [1/2, 1/2, 1] ?S # [1]C || 1S@ ?were? V P ?.? ? [1/2, 1, 1/2] || *
Figure 7: The parsing chart of the two derivations in Figure 4. Blue states or fringes (also marked with |) are the ones in
the first derivation, red (||) in the second, and yellow (no marks) are the ones in common. Each state-set is represented
as a separate block in the chart, headed by the state-set index and the next word. Each row maps to a chart operation
(specified in the first column, with S and C standing for ?scan? and ?complete? respectively) and follows the same
notation of figure 6. Symbols ? are used as state placeholders.
probabilities [?,?,?] as shown in the chart example
in Figure 7.
? The forward probability ? is the marginal prob-
ability of all the paths starting with an initial
state, scanning all initial words in the sentence
until `i?1 included, and passing through the
current state.
? The inner probability ? is the marginal proba-
bility of all the paths passing through the state
k : kX  ???, scanning words `k, . . . , `i?1 and
passing through the current state.
? The outer probability ? is the marginal prob-
ability of all the paths starting with an initial
state, scanning all initial words in the sentence
until `k?1 included, passing through the current
state, and reaching a stop state.
Forward (?) and inner (?) probabilities are propa-
gated while filling the chart incrementally, whereas
outer probabilities (?) are back-propagated from the
stop states, for which ? = 1 (see Figure 6). These
probabilities are used for computing prefix and sen-
tence probabilities, and for obtaining the most prob-
able partial derivation (MPD) of a prefix, the MPD
of a sentence, its minimum risk parse (MRP), and to
approximate its most probable parse (MPP).
Prefix probabilities are obtained by summing over
the forward probabilities of all scan states in state-set
i having `i after the dot:9
P(`0, . . . , `i) = ?
state si:kX??`i?
?(s) (7)
3.4 Most Probable Derivation (MPD)
The Most Probable (partial) Derivation (MPD) can
be obtained from the chart by backtracking the
Viterbi path. Viterbi forward and inner probabilities
9Sentence probability is obtained by marginalizing the for-
ward probabilities of the stop states in the last state-set n.
118
(??,??) are propagated as standard forward and in-
ner probabilities except that summation is replaced
by maximization, and the probability of an ambigu-
ous fringe is the maximum probability among all the
fragments mapping into it (instead of the marginal
one). The Viterbi partial path for the prefix `0, . . . , `i
can then be retrieved by backtracking from the scan
state in state-set i with max ??: for each state, the
most probable preceding state is retrieved, i.e., the
state among its antecedents with maximum ??. The
Viterbi complete path of a sentence can be obtained
by backtracking the Viterbi path from the stop state
with max ??. Given a Viterbi path, it is possible to
obtain the corresponding MPD. This is done by re-
trieving the associated sequence of fragments10 and
connecting them.
3.5 Most Probable Parse (MPP)
According to Equation (5), if we want to compute
the MPP we need to retrieve all possible derivations
of the current sentence, sum up the probabilities of
those generating the same tree, and returning the
tree with max marginal probability. Unfortunately
the number of possible derivations grows exponen-
tially with the length of the sentence, and computing
the exact MPP is NP-hard (Sima?an, 1996). In our
implementation, we approximate the MPP by per-
forming this marginalization over the Viterbi-best
derivations obtained from all stop states in the chart.
3.6 Minimum Risk Parse (MRP)
MPD and MPP aim at obtaining the structure of a
sentence which is more likely as a whole under the
current probabilistic model. Alternatively, we may
want to focus on the single components of a tree
structures, e.g., CFG rules covering a certain span of
the sentence, and search for the structure which has
the highest number of correct constituents, as pro-
posed by Goodman (1996). Such structure is more
likely to obtain higher results according to standard
parsing evaluations, as the objective being maxi-
mized is closely related to the metric used for eval-
uation (recall/precision on the number of correct la-
beled constituents).
10For each scan state in the path, we obtain the fragment in
the grammar that maps into the state?s fringe. For ambiguous
fringes the most probable fragment that maps into it is selected.
In order to obtain the minimum risk parse (MRP)
we utilize both inner (?) and outer (?) probabilities.
The product of these two probabilities equals the
marginal probability of all paths generating the en-
tire current sentence and passing through the current
state. We can therefore compute the probability of a
fringe f = X ??? covering a specific span [s, t] of
the sentence:
P( f , [s, t]) = ?(t : s f?) ??(t : s f?) (8)
We can then compute the probability of each frag-
ment spanning [s, t],11 and the probability P(r, [s, t])
of a CFG-rule r spanning [s, t].12 Finally the MRP is
computed as
MRP = argmax
T ?r?T P(r, [s, t]) (9)
4 Experiments
For training and evaluating the ITSG parser, we em-
ploy the Penn WSJ Treebank (Marcus et al, 1993).
We use sections 2?21 for training, section 22 and 24
for development and section 23 for testing.
4.1 Grammar Extraction
Following standard practice, we start with some pre-
processing of the treebank. After removing traces
and functional tags, we apply right binarization on
the training treebank (Klein and Manning, 2003),
with no horizontal and vertical conditioning. This
means that when a node X has more than two chil-
dren, new artificial constituents labeled X@ are cre-
ated in a right recursive fashion (see Figure 1).13 We
then replace words appearing less than five times in
the training data by one of 50 unknown word cate-
gories based on the presence of lexical features as
described in Petrov (2009).
Fragment Extraction In order to equip the gram-
mar with a representative set of lexicalized frag-
ments, we use the extraction algorithm of Sangati
11For an ambiguous fringe, the spanning probability of each
fragment mapping into it is the fraction of the fringe?s spanning
probability with respect to the marginal fringe probability.
12Marginalizing the probabilities of all fragments having r
spanning [s, t].
13This shallow binarization (H0V1) was used based on gold
coverage of the unsmoothed grammar (extracted from the train-
ing set) on trees in section 22: H0V1 binarization results on a
coverage of 88.0% of the trees, compared to 79.2% for H1V1.
119
et al (2010) which finds maximal fragments recur-
ring twice or more in the training treebank. To en-
sure better coverage, we additionally extract one-
word fragments from each training parse tree: for
each lexical node ` in the parse tree we percolate
up till the root node, and for every encountered in-
ternal node X0,X1, . . . ,Xi we extract the lexicalized
fragment whose spine is Xi ? Xi?1 ? . . .? X0 ? `,
and where all the remaining children of the inter-
nal nodes are substitution sites (see for instance the
right fragment in Figure 1). Finally, we remove all
fragments which do not comply with the restrictions
presented in Section 2.1.14
For each extracted fragment we keep track of its
frequency, i.e., the number of times it occurs in the
training corpus. Each fragment?s probability is then
derived according to its relative frequency in the
corresponding set of fragments ( finit , f Xlex, f Ysub), so
that equations(1)?(3) are satisfied. The final gram-
mar consists of 2.2M fragments mapping to 2.0M
fringes.
Smoothing Two types of smoothing are per-
formed over the grammar?s fragments: Open class
smoothing adds simple CFG rewriting rules to the
grammar for open-class15 ?PoS,word? pairs not en-
countered in the training corpus, with frequency
10?6. Initial fragments smoothing adds each lex-first
fragment f to the initial fragment set with frequency
10?2 ? freq( f ).16
All ITSG experiments we report used exhaustive
search (no beam was used to prune the search space).
4.2 Evaluation
In addition to standard full-sentence parsing results,
we propose a novel way of evaluating our ITSG on
partial trees, i.e., those that the parser constructs for
sentence prefixes. More precisely, for each prefix of
the input sentence (length two words or longer) we
compute the parsing accuracy on the minimal struc-
ture spanning that prefix. The minimal structure is
obtained from the subtree rooted in the minimum
14The fragment with no lexical items, and those with more
than one substitution site at the beginning of the yield.
15A PoS belongs to the open class if it rewrites to at least 50
different words in the training corpus. A word belongs to the
open class if it has been seen only with open-class PoS tags.
16The parameters were tuned on section 24 of the WSJ.
common ancestor of the prefix nodes, after pruning
those nodes not yielding any word in the prefix.
As observed in the example derivations of Fig-
ure 4, our ITSG generates partial trees for a given
prefix which may include predictions about unseen
parts of the sentence. We propose three new mea-
sures for evaluating sentence prediction:17
Word prediction PRD(m): For every prefix of
each test sentence, if the model predicts m? ? m
words, the prediction is correct if the first m pre-
dicted words are identical to the m words following
the prefix in the original sentence.
Word presence PRS(m): For every prefix of each
test sentence, if the model predicts m? ? m words,
the prediction is correct if the first m predicted words
are present, in the same order, in the words following
the prefix in the original sentence (i.e., the predicted
word sequence is a subsequence of the sequence of
words following the prefix).18
Longest common subsequence LCS: For every
prefix of each test sentence, it computes the longest
common subsequence between the sequence of pre-
dicted words (possibly none) and the words follow-
ing the prefix in the original sentence.
Recall and precision can be computed in the usual
way for these three measures. Recall is the total
number (over all prefixes) of correctly predicted
words (as defined by PRD(m), PRS(m), or LCS)
over the total number of words expected to be pre-
dicted (according to m), while precision is the num-
ber of correctly predicted words over the number of
words predicted by the model.
We compare the ITSG parser with the incremental
parsers of Schuler et al (2010) and Demberg et al
(2014) for full-sentence parsing, with the Roark
(2001) parser19 for full-sentence and partial pars-
17We also evaluated our ITSG model using perplexity; the
results obtained were substantially worse than those obtained
using Roark?s parsers.
18Note that neither PRD(m) nor PRS(m) correspond to word
error rate (WER). PRD requires the predicted word sequence to
be identical to the original sequence, while PRS only requires
the predicted words to be present in the original. In contrast,
WER measures the minimum number of substitutions, inser-
tions, and deletions needed to transform the predicted sequence
into the original sequence.
19Apart from reporting the results in Roark (2001), we also
run the latest version of Roark?s parser, used in Roark et al
(2009), which has higher results compared to the original work.
120
R P F1
Demberg et al (2014) 79.4 79.4 79.4
Schuler et al (2010) 83.4 83.7 83.5
Roark (2001) 86.6 86.5 86.5
Roark et al (2009) 87.7 87.5 87.6
ITSG (MPD) 81.5 83.5 82.5
ITSG (MPP) 81.6 83.6 82.6
ITSG (MRP) 82.6 85.8 84.1
ITSG Smoothing (MPD) 83.0 83.5 83.2
ITSG Smoothing (MPP) 83.2 83.6 83.4
ITSG Smoothing (MRP) 83.9 85.6 84.8
Table 1: Full-sentence parsing results for sentences in the
test set of length up to 40 words.
ing, and with a language model built using SRILM
(Stolcke, 2002) for sentence prediction. We used a
standard 3-gram model trained on the sentences of
the training set using the default setting and smooth-
ing (Kneser-Ney) provided by the SRILM pack-
age. (Higher n-gram model do not seem appropriate,
given the small size of the training corpus.) For ev-
ery prefix in the test set we compute the most prob-
able continuation predicted by the n-gram model.20
4.3 Results
Table 1 reports full-sentence parsing results for our
parser and three comparable incremental parsers
from the literature. While Roark (2001) obtains the
best results, the ITSG parser without smoothing per-
forms on a par with Schuler et al (2010), and out-
performs Demberg et al (2014).21 Adding smooth-
ing results in a gain of 1.2 points F-score over the
Schuler parser. When we compare the different pars-
ing objectives of the ITSG parser, MRP is the best
one, followed by MPP and MPD.
Incremental Parsing The graphs in Figure 8 com-
pare the ITSG and Roark?s parser on the incremental
parsing evaluation, when parsing sentences of length
10, 20, 30 and 40. The performance of both models
declines as the length of the prefix increases, with
Roark?s parser outperforming the ITSG parser on
average, although the ITSG parser seems more com-
20We used a modified version of a script by Nathaniel Smith
available at https://github.com/njsmith/pysrilm.
21Note that the scores reported by Demberg et al (2014) are
for TAG structures, not for the original Penn Treebank trees.
2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20Prefix Length
86
88
90
92
94
96
F-sc
ore
Roark (last)ITSG Smooth. (MPD)oark et al (2009)
2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20Prefix Length
86
88
90
92
94
96
F-sc
ore
Roark (last)ITSG Smooth. (MPD)SG Smooth. (MPD)
F-s
co
re 2 3 4 5 6 7 8 9 109192
9394
9596
9798
99
2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 208586
878889
909192
939495
9697
2 4 6 8 10 12 14 16 18 20 22 24 26 28 307880
828486
889092
949698
2 4 6 8 10 12 14 16 18 20 22 24 26 28 30 32 34 36 38 407880
828486
889092
949698
Prefix Length
Figure 8: Partial parsing results for sentences of length
10, 20, 30, and 40 (from upper left to lower right).
petitive when parsing prefixes for longer (and there-
fore more difficult) sentences.
Sentence Prediction Table 2 compares the sen-
tence prediction results of the ITSG and the lan-
guage model (SRILM). The latter is outperforming
the former when predicting the next word of a pre-
fix, i.e. PRD(1), whereas ITSG is better than the lan-
guage model at predicting a single future word, i.e.
PRS(1). When more than one (consecutive) word
is considered, the SRILM model exhibits a slightly
better recall while ITSG achieves a large gain in pre-
cision. This illustrates the complementary nature of
the two models: while the language model is better
at predicting the next word, the ITSG predicts future
words (rarely adjacent to the prefix) with high con-
fidence (89.4% LCS precision). However, it makes
predictions for only a small number of words (5.9%
LCS recall). Examples of sentence predictions can
be found in Table 3.
5 Related Work
To the best of our knowledge, there are no other in-
cremental TSG parsers in the literature. The parser
of Demberg et al (2014) is closely related, but uses
tree-adjoining grammar, which includes both sub-
stitution and adjunction. That parser makes predic-
tions, but only for upcoming structure, not for up-
coming words, and thus cannot be used directly
for sentence prediction. The incremental parser of
Roark (2001) uses a top-down algorithm and works
121
ITSG SRILM
Correct R P Correct R P
PRD(1) 4,637 8.7 12.5 11,430 21.5 21.6
PRD(2) 864 1.7 13.9 2,686 5.3 5.7
PRD(3) 414 0.9 20.9 911 1.9 2.1
PRD(4) 236 0.5 23.4 387 0.8 1.0
PRS(1) 34,831 65.4 93.9 21,954 41.2 41.5
PRS(2) 4,062 8.0 65.3 5,726 11.3 12.2
PRS(3) 1,066 2.2 53.7 1,636 3.4 3.8
PRS(4) 541 1.2 53.7 654 1.4 1.7
LCS 44,454 5.9 89.4 92,587 12.2 18.4
Table 2: Sentence prediction results.
Prefix Shares of UAL , the parent PRD(3) PRS(3)
ITSG company of United Airlines , ? ?
SRILM company , which is the ? ?
Goldstd of United Airlines , were extremely active all day
Friday .
Prefix PSE said it expects to report earnings of $ 1.3
million to $ 1.7 million , or 14
ITSG cents a share , ? +
SRILM % to $ UNK ? ?
Goldstd cents to 18 cents a share .
Table 3: Examples comparing sentence predictions for
ITSG and SRILM (UNK: unknown word).
on the basis of context-free rules. These are aug-
mented with a large number of non-local fea-
tures (e.g., grandparent categories). Our approach
avoids the need for such additional features, as
TSG fragments naturally contain non-local informa-
tion. Roark?s parser outperforms ours in both full-
sentence and incremental F-score (see Section 4),
but cannot be used for sentence prediction straight-
forwardly: to obtain a prediction for the next word,
we would need to compute an argmax over the
whole vocabulary, then iterate this for each word af-
ter that (the same is true for the parsers of Schuler
et al, 2010 and Demberg et al, 2014). Most in-
cremental dependency parsers use a discriminative
model over parse actions (Nivre, 2007), and there-
fore cannot predict upcoming words either (but see
Huang and Sagae 2010).
Turning to the literature on sentence prediction,
we note that ours is the first attempt to use a parser
for this task. Existing approaches either use n-gram
models (Eng and Eisner, 2004; Bickel et al, 2005) or
a retrieval approach in which the best matching sen-
tence is identified from a sentence collection given a
set of features (Grabski and Scheffer, 2004). There
is also work combining n-gram models with lexical
semantics (Li and Hirst, 2005) or part-of-speech in-
formation (Fazly and Hirst, 2003).
In the language modeling literature, more sophis-
ticated models than simple n-gram models have
been developed in the past few years, and these
could potentially improve sentence prediction. Ex-
amples include syntactic language models which
have applied successfully for speech recognition
(Chelba and Jelinek, 2000; Xu et al, 2002) and ma-
chine translation (Schwartz et al, 2011; Tan et al,
2011), as well as discriminative language models
(Mikolov et al, 2010; Roark et al, 2007). Future
work should evaluate these approaches against the
ITSG model proposed here.
6 Conclusions
We have presented the first incremental parser for
tree substitution grammar. Incrementality is moti-
vated by psycholinguistic findings, and by the need
for real-time interpretation in NLP. We have shown
that our parser performs competitively on both full
sentence and sentence prefix F-score. We also intro-
duced sentence prediction as a new way of evaluat-
ing incremental parsers, and demonstrated that our
parser outperforms an n-gram model in predicting
more than one upcoming word.
The performance of our approach is likely to im-
prove by implementing better binarization and more
advanced smoothing. Also, our model currently con-
tains no conditioning on lexical information, which
is also likely to yield a performance gain. Finally,
future work could involve replacing the relative fre-
quency estimator that we use with more sophisti-
cated estimation schemes.
Acknowledgments
This work was funded by EPSRC grant
EP/I032916/1 ?An integrated model of syntac-
tic and semantic prediction in human language
processing?. We are grateful to Brian Roark for
clarifying correspondence and for guidance in using
his incremental parser. We would also like to thank
Katja Abramova, Vera Demberg, Mirella Lapata,
Andreas van Cranenburgh, and three anonymous
reviewers for useful comments.
122
References
Alfred V. Aho and Jeffrey D. Ullman. 1972. The
theory of parsing, translation, and compiling.
Prentice-Hall, Inc., Upper Saddle River, NJ.
Gerry T. M. Altmann and Yuki Kamide. 1999. Incre-
mental interpretation at verbs: Restricting the do-
main of subsequent reference. Cognition, 73:247?
264.
Steffen Bickel, Peter Haider, and Tobias Scheffer.
2005. Predicting sentences using n-gram lan-
guage models. In Proceedings of the Conference
on Human Language Technology and Empirical
Methods in Natural Language Processing, pages
193?200. Vancouver.
Rens Bod. 1995. The problem of computing the
most probable tree in data-oriented parsing and
stochastic tree grammars. In Proceedings of the
7th Conference of the European Chapter of the
Association for Computational Linguistics, pages
104?111. Association for Computer Linguistics,
Dublin.
Rens Bod, Khalil Sima?an, and Remko Scha. 2003.
Data-Oriented Parsing. University of Chicago
Press, Chicago, IL.
Ciprian Chelba and Frederick Jelinek. 2000. Struc-
tured language modeling. Computer Speech and
Language, 14:283?332.
Vera Demberg and Frank Keller. 2008. Data from
eye-tracking corpora as evidence for theories
of syntactic processing complexity. Cognition,
101(2):193?210.
Vera Demberg, Frank Keller, and Alexander Koller.
2014. Parsing with psycholinguistically moti-
vated tree-adjoining grammar. Computational
Linguistics, 40(1). In press.
Jay Earley. 1970. An efficient context-free pars-
ing algorithm. Communications of the ACM,
13(2):94?102.
John Eng and Jason M. Eisner. 2004. Radiology
report entry with automatic phrase completion
driven by language modeling. Radiographics,
24(5):1493?1501.
Afsaneh Fazly and Graeme Hirst. 2003. Testing
the efficacy of part-of-speech information in word
completion. In Proceedings of the EACL Work-
shop on Language Modeling for Text Entry Meth-
ods, pages 9?16. Budapest.
Joshua Goodman. 1996. Parsing algorithms and
metrics. In Proceedings of the 34th Annual Meet-
ing on Association for Computational Linguistics,
pages 177?183. Association for Computational
Linguistics, Santa Cruz.
Korinna Grabski and Tobias Scheffer. 2004. Sen-
tence completion. In Proceedings of the 27th An-
nual International ACM SIR Conference on Re-
search and Development in Information Retrieval,
pages 433?439. Sheffield.
Liang Huang and Kenji Sagae. 2010. Dynamic pro-
gramming for linear-time incremental parsing. In
Proceedings of the 48th Annual Meeting of the
Association for Computational Linguistics, pages
1077?1086. Association for Computational Lin-
guistics, Uppsala.
Dan Klein and Christopher D. Manning. 2003. Ac-
curate unlexicalized parsing. In Proceedings of
the 41st Annual Meeting on Association for Com-
putational Linguistics, pages 423?430. Associa-
tion for Computational Linguistics, Sapporo.
Jianhua Li and Graeme Hirst. 2005. Semantic
knowledge in a word completion task. In Pro-
ceedings of the 7th International ACM SIGAC-
CESS Conference on Computers and Accessibil-
ity, pages 121?128. Baltimore.
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and
Beatrice Santorini. 1993. Building a large anno-
tated corpus of english: The penn treebank. Com-
putational Linguistics, 19(2):313?330.
Tomas Mikolov, Martin Karafiat, Jan Cernocky, and
Sanjeev. 2010. Recurrent neural network based
language model. In Proceedings of the 11th
Annual Conference of the International Speech
Communication Association, pages 2877?2880.
Florence.
Joakim Nivre. 2007. Incremental non-projective
dependency parsing. In Proceedings of Human
Language Technologies: The Annual Conference
of the North American Chapter of the Associa-
tion for Computational Linguistics, pages 396?
403. Association for Computational Linguistics,
Rochester.
123
Slav Petrov. 2009. Coarse-to-Fine Natural Lan-
guage Processing. Ph.D. thesis, University of
California at Bekeley, Berkeley, CA.
Brian Roark. 2001. Probabilistic top-down parsing
and language modeling. Computational Linguis-
tistics, 27:249?276.
Brian Roark, Asaf Bachrach, Carlos Cardenas, and
Christophe Pallier. 2009. Deriving lexical and
syntactic expectation-based measures for psy-
cholinguistic modeling via incremental top-down
parsing. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing,
pages 324?333. Association for Computational
Linguistics, Singapore.
Brian Roark, Murat Saraclar, and Michael Collins.
2007. Discriminative n-gram language modeling.
Computer Speech and Language, 21(2):373?392.
D. J. Rosenkrantz and P. M. Lewis. 1970. Deter-
ministic left corner parsing. In Proceedings of
the 11th Annual Symposium on Switching and Au-
tomata Theory, pages 139?152. IEEE Computer
Society, Washington, DC.
Federico Sangati, Willem Zuidema, and Rens Bod.
2010. Efficiently extract recurring tree fragments
from large treebanks. In Nicoletta Calzolari,
Khalid Choukri, Bente Maegaard, Joseph Mar-
iani, Jan Odijk, Stelios Piperidis, Mike Rosner,
and Daniel Tapias, editors, Proceedings of the 7th
InternationalConference on Language Resources
and Evaluation. European Language Resources
Association, Valletta, Malta.
Yves Schabes. 1990. Mathematical and Computa-
tional Aspects of Lexicalized Grammars. Ph.D.
thesis, University of Pennsylvania, Philadelphia,
PA.
William Schuler, Samir AbdelRahman, Tim Miller,
and Lane Schwartz. 2010. Broad-coverage pars-
ing using human-like memory constraints. Com-
putational Linguististics, 36(1):1?30.
Lane Schwartz, Chris Callison-Burch, William
Schuler, and Stephen Wu. 2011. Incremental syn-
tactic language models for phrase-based transla-
tion. In Proceedings of the 49th Annual Meeting
of the Association for Computational Linguistics:
Human Language Technologies, Volume 1, pages
620?631. Association for Computational Linguis-
tics, Portland, OR.
Khalil Sima?an. 1996. Computational complexity
of probabilistic disambiguation by means of tree-
grammars. In Proceedings of the 16th Confer-
ence on Computational Linguistics, pages 1175?
1180. Association for Computational Linguistics,
Copenhagen.
Andreas Stolcke. 1995. An efficient probabilis-
tic context-free parsing algorithm that computes
prefix probabilities. Computational Linguistics,
21(2):165?201.
Andreas Stolcke. 2002. SRILM ? an extensible lan-
guage modeling toolkit. In Proceedings Interna-
tional Conference on Spoken Language Process-
ing, pages 257?286. Denver, CO.
Scott C. Stoness, Joel Tetreault, and James Allen.
2004. Incremental parsing with reference inter-
action. In Frank Keller, Stephen Clark, Matthew
Crocker, and Mark Steedman, editors, Proceed-
ings of the ACL Workshop Incremental Parsing:
Bringing Engineering and Cognition Together,
pages 18?25. Association for Computational Lin-
guistics, Barcelona.
Ming Tan, Wenli Zhou, Lei Zheng, and Shaojun
Wang. 2011. A large scale distributed syntac-
tic, semantic and lexical language model for ma-
chine translation. In Proceedings of the 49th
Annual Meeting of the Association for Compu-
tational Linguistics: Human Language Technolo-
gies, Volume 1, pages 201?210. Association for
Computational Linguistics, Portland, OR.
Michael K. Tanenhaus, Michael J. Spivey-
Knowlton, Kathleen M. Eberhard, and Julie C.
Sedivy. 1995. Integration of visual and linguistic
information in spoken language comprehension.
Science, 268:1632?1634.
Peng Xu, Ciprian Chelba, and Frederick Jelinek.
2002. A study on richer syntactic dependencies
for structured language modeling. In Proceedings
of the 40th Annual Meeting on Association for
Computational Linguistics, pages 191?198. As-
sociation for Computational Linguistics, Philadel-
phia.
124
