Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 1370?1381, Dublin, Ireland, August 23-29 2014.
Hybrid Grammars for Discontinuous Parsing
Mark-Jan Nederhof
School of Computer Science
University of St Andrews
KY16 9SX, UK
Heiko Vogler
Department of Computer Science
Technische Universit?at Dresden
D-01062 Dresden, Germany
Abstract
We introduce the concept of hybrid grammars, which are extensions of synchronous grammars,
obtained by coupling of lexical elements. One part of a hybrid grammar generates linear struc-
tures, another generates hierarchical structures, and together they generate discontinuous struc-
tures. This formalizes and generalizes some existing mechanisms for dealing with discontinuous
phrase structures and non-projective dependency structures. Moreover, it allows us to separate
the degree of discontinuity from the time complexity of parsing.
1 Introduction
Discontinuous phrases occur frequently in languages with relatively free word order, and adequate de-
scription of their structure requires special care (Kathol and Pollard, 1995; M?uller, 2004). Even for
languages such as English, with a relatively rigid word order, there is a clear need for discontinuous
structures (McCawley, 1982; Stucky, 1987).
Early treebanks for English (Marcus et al., 1993) have often represented discontinuity in a way that
makes it tempting to ignore it altogether, certainly for the purposes of parsing, whereas recent approaches
tend to represent discontinuity in a more overt form, sometimes after transformation of existing treebanks
(Choi and Palmer, 2010; Evang and Kallmeyer, 2011). In many modern treebanks, discontinuous struc-
tures have been given a prominent status (B?ohmov?a et al., 2000).
Classes of trees without discontinuity can be specified as the sets of parse trees of context-free gram-
mars (CFGs). Somewhat larger classes can be specified by tree substitution grammars (Sima?an et al.,
1994) and regular tree grammars (Brainerd, 1969; G?ecseg and Steinby, 1997). Practical parsers for these
three formalisms have running time O(n
3
), where n is the length of the input sentence. Discontinuous
structures go beyond their strong generative capacity however. Similarly, non-projective dependency
structures cannot be obtained by traditional dependency grammars. See (Rambow, 2010) for discussion
of the relation between constituent and dependency structures and see (Maier and Lichte, 2009) for a
comparison of discontinuity and non-projectivity.
One way to solve the above problems has been referred to as pseudo-projectivity, i.e. a parser produces
a projective structure, which in a second phase is transformed into a non-projective structure (Kahane
et al., 1998; McDonald and Pereira, 2006; Nivre and Nilsson, 2005). In particular, this may involve
lifting, whereby one end point of a dependency link moves across a path of nodes. A related idea for
discontinuous phrase structure is the reversible splitting conversion of (Boyd, 2007). See also (Johnson,
2002; Campbell, 2004; Gabbard et al., 2006).
As shown by (Nivre, 2009), the second phase of pseudo-projective dependency parsing can be inter-
leaved with the first, by replacing the usual one-way input tape by an additional stack, or buffer. Where
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
1370
non-topmost positions from the parsing stack are moved back to the buffer, input positions are effectively
swapped and non-projective dependency structures arise.
Tree adjoining grammars (TAGs) can describe strictly larger classes of word order phenomena than
CFGs (Rambow and Joshi, 1997). TAG parsers have a time complexity of O(n
6
) (Vijay-Shankar and
Joshi, 1985). However, the derived trees they generate are still continuous. Although their derivation
trees may be argued to be discontinuous, these by themselves are not normally the desired syntactic
structures. It was argued by (Becker et al., 1991) that further additions to TAGs are needed to obtain
adequate descriptions of scrambling phenomena.
An alternative is proposed by (Kallmeyer and Kuhlmann, 2012): a transformation is added that turns a
derivation tree of a (lexicalized) TAG into a non-projective dependency structure. A very similar mech-
anism is used to obtain non-projective dependency structures using linear context-free rewriting systems
(LCFRSs) (Kuhlmann, 2013) that are lexicalized. In a LCFRS the synthesis of strings is normally spec-
ified by yield functions associated with rules. By an additional interpretation of the templates of these
yield functions in the algebra of dependency trees (with the overt lexical items as roots), the LCFRS
generates both strings and (possibly non-projective) dependency structures.
However, the running time of LCFRS parsers is generally very high, still polynomial in the sentence
length, but with a degree determined by properties of the grammar; difficulties involved in running
LCFRS parsers for natural languages are described by (Kallmeyer and Maier, 2013).
It follows from the above that there is considerable freedom in the design of parsers that produce
discontinuous structures for given input sentences. One can distinguish between two main issues. The
first is the formalism that guides the parsing of the input. This determines a class of input (string)
languages, which can be that of the context-free languages, or tree adjoining languages, etc. We assume
parsing with any of these formalisms results in derivations of some sort. The second main issue is the
mechanism that translates such derivations into discontinuous structures.
This leads to a number of open questions that are all related. First, what is, or should be, the division
of labor between the parser producing the derivations and the mechanism turning those derivations into
discontinuous structures? If we want to achieve high degrees of discontinuity in the output structures,
should the formalism for the input language be much more powerful than, say, context-free? Or can
highly discontinuous structures be obtained equally well through ordinary CFGs in combination with an
advanced mechanism producing discontinuous structures out of derivations?
Second, how should one approach the problem of finding the grammar (and grammar class) for the
input language and the mapping from derivations to structures if the only thing that is given is a treebank?
A third question is which formalisms are suitable to formally describe mappings from derivations to
discontinuous structures. Lastly, can we characterize the classes of output (tree-)languages for various
combinations of input grammars and derivation-to-structure mappings?
In this paper we provide one possible answer to these questions by a new type of formalism, which we
call hybrid grammars. Such a grammar consists of a string grammar and a tree grammar. Derivations are
coupled so as to achieve synchronous rewriting. The input string language and the output tree language
are thereby straightforwardly defined. Different from synchronous grammars (Shieber and Schabes,
1990; Satta and Peserico, 2005) is that occurrences of terminal symbols are also coupled. Thereby
the linear order of the symbols in a derived string imposes an order on the coupled symbols in the
synchronously derived tree; this allows a straightforward specification of a discontinuous structure.
One can define a hybrid grammar consisting of a simple macro grammar (Fischer, 1968) and a simple
context-free tree grammar (Rounds, 1970), but various other combinations of a string grammar and a tree
grammar are possible as well. Due to lack of space we will here concentrate on only one kind of hybrid
grammar, namely that consisting of a LCFRS as string grammar and a form of definite clause program as
tree grammar. We will show that hybrid grammars that induce (finite) sets of hybrid trees can always be
constructed, even if the allowable derivations are severely restricted, and we discuss experiments. Lastly,
a negative result will be given, which shows that a certain linguistic phenomenon cannot be handled if
the string grammar is too restricted.
We cast our definitions in terms of hybrid trees, of which discontinuous phrase structures and non-
1371
projective dependency structures are special cases.
1
Thereby the generality of the framework is demon-
strated.
2 Preliminaries
Let N = {0, 1, 2, . . .} and N
+
= N \ {0}. For each n ? N
+
, we let [n] stand for the set {1, . . . , n}, and
we let [0] stand for ?. We write [n]
0
to denote [n] ? {0}. We fix an infinite list x
1
, x
2
, . . . of pairwise
distinct variables. We let X = {x
1
, x
2
, x
3
, . . .} and X
k
= {x
1
, . . . , x
k
} for each k ? N.
A ranked set ? is a set of symbols, associated with a rank function assigning a number rk
?
(?) ? N
to each symbol ? ? ?. A ranked alphabet is a ranked set with a finite number of symbols. We let ?
(k)
denote {? ? ? | rk
?
(?) = k}.
The following definitions were inspired by (Seki and Kato, 2008). The sets of terms and sequence-
terms (s-terms) over ranked set ?, with variables in some set Y ? X , are denoted by T
?
(Y ) and T
?
?
(Y ),
respectively, and defined inductively as follows:
(i) Y ? T
?
(Y ),
(ii) if k ? N, ? ? ?
(k)
and s
i
? T
?
?
(Y ) for each i ? [k], then ?(s
1
, . . . , s
k
) ? T
?
(Y ), and
(iii) if n ? N and t
i
? T
?
(Y ) for each i ? [n], then ?t
1
, . . . , t
n
? ? T
?
?
(Y ).
We let T
?
?
and T
?
stand for T
?
?
(?) and T
?
(?) respectively. Throughout this paper, we use variables such
as s and s
i
for s-terms and variables such as t and t
i
for terms. The justification for using s-terms as
defined here is that they provide the required flexibility for dealing with both strings (? = ?
(0)
) and
unranked trees (? = ?
(1)
), in combination with derivational nonterminals.
Concatenation of s-terms is given by ?t
1
, . . . , t
n
? ? ?t
n+1
, . . . , t
n+m
? = ?t
1
, . . . , t
n+m
?. Sequences
such as s
1
, . . . , s
k
or x
1
, . . . , x
k
will typically be abbreviated to s
1,k
or x
1,k
, respectively. For ? ? ?
(0)
we sometimes abbreviate ?() to ?.
In examples we also abbreviate ?t
1
, . . . , t
n
? to t
1
? ? ? t
n
, that is, omitting the angle brackets and com-
mas. Moreover, we sometimes abbreviate ?(??) to ?. Whether ? then stands for ?(??) or for ?() depends
on whether ? ? ?
(1)
or ? ? ?
(0)
, which will be clear from the context.
Subterms in terms or s-terms are identified by positions; these can be formalized by a suitable refine-
ment of the familiar notion of Gorn address. The set of all positions in term t or in s-term s is denoted
by pos(t) or pos(s), respectively. The subset of pos(t) consisting of all positions where the label is in
some set ? ? ? is denoted by pos
?
(t).
3 Hybrid trees
The purpose of this section is to unify existing notions of non-projective dependency structures and
discontinuous phrase structures, formalized using s-terms.
We fix an alphabet ? = ?
(1)
and a subset ? ? ?. A hybrid tree over (?,?) is a pair h = (s,?
s
),
where s ? T
?
?
and?
s
is a total order on pos
?
(s). In words, a hybrid tree combines hierarchical structure,
in the form of an s-term over the full alphabet ?, with a linear structure, which can be seen as a string
over ? ? ?. This string will be denoted by str(h).
For discontinuous phrase structures, the elements of ? would typically represent lexical items, and
the elements of ? \ ? would typically represent syntactic categories. For non-projective dependency
structures, ? would be equal to ?. Simple examples of discontinuous phrase structures are presented in
Figures 1 and 2.
4 Basic grammatical formalisms
The concept of hybrid grammars is illustrated in Section 5, by coupling a class of string grammars and a
class of tree grammars.
1
Moreover, we need to avoid any confusion with the term ?discontinuous tree? from (Bunt, 1996), which is characterized
by the notion of ?context daughter?, which is absent from our framework. The term ?hybrid tree? was used before by (Lu et
al., 2008), also for a mixture of a tree structure and a linear structure, generated by a probabilistic model. However, the linear
?surface? structure was obtained by a simple left-to-right tree traversal, whereas a meaning representation was obtained by a
slightly more flexible traversal of the same tree. The emphasis in the current paper is rather on separating the linear structure
from the tree structure.
1372
VP
V
hat gearbeitet
ADV
schnell
hat schnell gearbeitet
Figure 1: Hybrid tree for German ?[...]
hat schnell gearbeitet? (?[...] has worked
quickly?), after (Seifert and Fischer, 2004).
The bottom line indicates the word order in
German. (Alternative analyses exist that do not
require discontinuity; we make no claim the
structure above is the most adequate.)
S
a S
a S
a b
b
b
aaa bb b
Figure 2: Abstract representation of cross-
serial dependencies in Dutch (Bresnan et al.,
1982).
4.1 Linear context-free rewriting systems
Much as in (Vijay-Shanker et al., 1987), we define a linear context-free rewriting system (LCFRS) as a
tuple G = (N,S,?, P ), where N is a ranked alphabet of nonterminals, S ? N
(1)
is the start symbol,
? = ?
(0)
is a ranked alphabet of terminals (? ?N = ?), and P is a finite set of rules, each of the form:
A
0
(s
1,k
0
)? ?A
1
(x
1,m
1
), A
2
(x
m
1
+1,m
2
), . . . , A
n
(x
m
n?1
+1,m
n
)? (1)
where n ? N, A
i
? N
(k
i
)
for each i ? [n]
0
, and m
i
=
?
j:1?j?i
k
j
for each i ? [n], and s
j
? T
?
?
(X
m
n
)
for each j ? [k
0
]. In words, the right-hand side is an s-term consisting of nonterminals A
i
(i ? [n]), with
distinct variables as arguments; there are m
n
variables altogether, which is the sum of the ranks k
i
of all
A
i
(i ? [n]). The left-hand side is an occurrence of A
0
with each argument being a string of variables
and terminals. Furthermore, we demand that each x
j
(j ? [m
n
]) occurs exactly once in the left-hand
side. The largest rank of any nonterminal is called the fanout of the grammar.
A rule instance is obtained by choosing a rule of the above form, and consistently substituting variables
with s-terms in T
?
?
(which are strings due to the terminals having rank 0). The language induced is the
set of s-terms s such that ?S(s)? ?
?
G
??, where?
G
is the ?derives? relation that uses rule instances. For
given s, the set of all LCFRS derivations ?S(s)? ?
?
G
?? (in compact tabular form) can be obtained in
polynomial time in the length of s (Seki et al., 1991).
Example 1
An example of a LCFRS is presented on the S(x
1
x
3
x
2
x
4
) ? A(x
1
, x
2
) B(x
3
, x
4
)
A(ax
1
,bx
2
) ? A(x
1
, x
2
)
A(??, ??) ? ??
B(cx
1
,dx
2
) ? B(x
1
, x
2
)
B(??, ??) ? ??
right. Terminals are lower case bold letters and
nonterminals are upper-case italic letters. All
derived strings are of the form a
m
c
n
b
m
d
n
with
m,n ? N. The linguistic relevance lies in cross-
serial dependencies in Swiss German (Shieber,
1985).
4.2 Definite clause programs
In this section we describe a particular kind of definite clause programs. Our definition is inspired by
(Deransart and Ma?uszynski, 1985), which investigated the relation between logic programs and attribute
grammars, together with the ?syntactic single use requirement? from (Giegerich, 1988). The values
produced are trees (or to be more precise s-terms).
1373
A simple definite clause program (sDCP) is a tuple G = (N,S,?, P ), where N is a ranked alphabet
of nonterminals and ? = ?
(1)
is a ranked alphabet of terminals.
2
Moreover, each nonterminal A ? N
has a fixed i-rank (the number of inherited arguments) and a fixed s-rank (the number of synthesized
arguments), denoted by i-rk(A) and s-rk(A), respectively, satisfying i-rk(A) + s-rk(A) = rk
N
(A). In
our notation, the inherited arguments precede the synthesized arguments. The start symbol S has only
one argument, which is synthesized, i.e. rk
N
(S) = s-rk(S) = 1 and i-rk(S) = 0.
A rule is of the form:
A
0
(x
(0)
1,k
0
, s
(0)
1,k
?
0
)? ?A
1
(s
(1)
1,k
1
, x
(1)
1,k
?
1
), . . . , A
n
(s
(n)
1,k
n
, x
(n)
1,k
?
n
)? (2)
where n ? N, k
i
= i-rk(A
i
) and k
?
i
= s-rk(A
i
), for i ? [n]
0
. The set of variables occurring in the lists
x
(0)
1,k
0
and x
(i)
1,k
?
i
(i ? [n]) equals X
m
, where m = k
0
+
?
i?[n]
k
?
i
. In other words, every variable from X
m
occurs exactly once in all these lists together. This is where values ?enter? the rule. Further, the s-terms
in s
(0)
1,k
?
0
and s
(i)
1,k
i
(i ? [n]) are in T
?
?
(X
m
) and together contain each variable in X
m
exactly once. This
is where values are combined and ?exit? the rule.
The ?derives? relation?
G
and other relevant notation are defined as for LCFRSs (where the s-terms
in arguments are now trees due to the terminals having rank 1). If the rules in a derivation are given, then
the relevant rule instances are uniquely determined, and can be computed in linear time in the size of
the derivation, provided the sDCP contains no cycles. The existence of cycles is decidable, as we know
from the literature on attribute grammars. There are sufficient conditions for absence of cycles, such as
the grammar being L-attributed (Bochmann, 1976). In this article, we will assume that sDCPs contain
no cycles.
Example 2
An example of a sDCP is presented S(x
2
) ? A(x
1
) B(x
1
, x
2
)
A(a A(x
1
) b) ? A(x
1
)
A(??) ? ??
B(x
1
, c B(x
2
) d) ? B(x
1
, x
2
)
B(x
1
, x
1
) ? ??
on the right, where the first argument of
B is inherited and all other arguments
are synthesized. A derived s-term is
e.g. c B(c B(a A(??) b) d) d.
5 Hybrid grammars
We couple derivations in two grammars in a way similar to how this is commonly done for synchronous
grammars, namely by indexed symbols. However, we apply the mechanism not only to derivational
nonterminals but also to terminals.
Let ? be a ranked alphabet. We define the ranked set I(?) = {?
u
| ? ? ?, u ? N
+
}, with rk
I(?)
(?
u
)
= rk
?
(?). Let ? be another ranked alphabet (? ? ? = ?) and Y ? X , with X as in Section 2. We let
I
?
?,?
(Y ) be the set of all s-terms s ? T
?
I(?)??
(Y ) in which each index occurs at most once.
For an s-term s, let ind(s) be the set of all indices occurring in s. The deindexing function D removes
all indices from an s-term s ? I
?
?,?
(Y ) to obtain D(s) ? T
?
???
(Y ). The set I
?,?
(Y ) ? T
I(?)??
(Y ) of
terms with indexed symbols is defined much as above. We let I
?
?,?
= I
?
?,?
(?) and I
?,?
= I
?,?
(?).
A LCFRS/sDCP hybrid grammar (HG) is a tuple G = ((N
1
, S
1
,?), (N
2
, S
2
,?), P ), subject to the
following restrictions. The objects ? and ? are ranked alphabets with ? = ?
(0)
and ? = ?
(1)
. As mere
sets of symbols, we demand ? ? ? but the rank functions associated with ? and ? differ. Let ? be the
ranked alphabet ? \ ?, with rk
?
(?) = 1 for ? ? ?.
The hybrid rules in P are of the form [?
1
, ?
2
] where ?
1
has the form in Equation (1) of an LCFRS
rule except that s
i
? I
?
?,?
(X
m
n
) (i ? [k
0
]) and A
i
? I(N
1
) (i ? [n]) and each index in ?
1
occurs
exactly once, and ?
2
has the form in Equation (2) of a sDCP rule except that the s-terms in s
(0)
1,k
?
0
and
s
(i)
1,k
i
(i ? [n]) are in I
?
?,?
(X
m
) and A
i
? I(N
2
) (i ? [n]) and each index in ?
2
occurs exactly once. We
require that ind(?
1
) = ind(?
2
) and each index either couples a pair of identical terminals or couples a
pair of (possibly distinct) nonterminals.
2
The term ?simple? here has a more restrictive meaning than the term with the same name in (Deransart and Ma?uszynski,
1985).
1374
Let P
1
and P
2
be the sets of all D(?
1
) and D(?
2
), respectively, of some hybrid rule [?
1
, ?
2
]. Then we
refer to the LCFRS (N
1
, S
1
,?, P
1
) and the sDCP (N
2
, S
2
,?, P
2
) as the first and second components,
respectively, of G.
In order to define the ?derives? relation?
G
, we need rule instantiation as before, in combination with
reindexing, which is a common notion for synchronous grammars. This allows specification of a set of
pairs [s
1
, s
2
] ? I
?
?,?
? I
?
?,?
which are such that [?S
1
1
(s
1
)?, ?S
1
2
(s
2
)?] ?
?
G
[??, ??]. For each such pair
we can construct a hybrid tree (s,?
s
) over (?,?), where s = D(s
2
), and ?
s
is defined as follows. If
there is a combination of positions p
1
, p
?
1
, p
2
, p
?
2
such that at p
1
in s
1
we find the same label as at p
2
in
s
2
(this label must then be in I(?)), and at p
?
1
in s
1
we find the same label as at p
?
2
in s
2
, and p
1
occurs
to the left of p
?
1
, then p
2
?
s
p
?
2
. The language induced by G is defined as the set of all such hybrid trees.
Given an input string, the desired hybrid trees can be effectively enumerated. To be exact, after
construction of the parse table by a LCFRS parser, which takes polynomial time in the length of the
string, synchronous derivations can be enumerated. Extracting a single derivation from the table requires
linear time in the size of that derivation. Given a derivation, an s-term can be constructed in linear time
in the size of that derivation, applying sDCP rules in the second component. This s-term, in combination
with the input string and the indices linking the two is then easily extended to a hybrid tree as outlined
above.
Example 3
The hybrid tree
[VP(x
1
x
2
x
3
)? V
1
(x
1
, x
3
) ADV
2
(x
2
),VP(VP(x
1
x
2
))? V
1
(x
1
) ADV
2
(x
2
)]
[V(h
1
, g
2
)? ??,V(V(h
1
g
2
))? ??]
[ADV(s
1
)? ??,ADV(ADV(s
1
))? ??]
in Figure 1 is ob-
tained by the HG
on the right. (All
arguments in the
second component are synthesized.) We derive:
[VP
1
(h
2
s
3
g
4
),VP
1
(VP(V(h
2
g
4
) ADV(s
3
)))]?
[V
1
(h
2
, g
4
) ADV
5
(s
3
),V
1
(V(h
2
g
4
)) ADV
5
(ADV(s
3
))]?
[ADV
5
(s
3
), ADV
5
(ADV(s
3
))]? [??, ??]
Note that in the LCFRS that
[VP(x
1
)? V
1
(x
1
), VP(VP(x
1
))? V
1
(x
1
)]
[V(h
1
x
1
g
2
)? ADV
3
(x
1
), V(V(h
1
g
2
) x
1
)? ADV
3
(x
1
)]
[ADV(s
1
)? ??, ADV(ADV(s
1
))? ??]
is the first component of the HG
above, nonterminal V has rank 2.
On the right is an alternative HG
deriving the same hybrid tree, but
now with all LCFRS nonterminals having rank 1, by which we obtain a syntactic variant of a CFG. Yet
another HG for the same hybrid tree will be discussed in the next section, where we will see that the first
and second components can be disconnected even further, departing from the traditional way of LCFRS
parsing.
Example 4
Hybrid trees as in Figure 2
[A(x
1
x
2
)? S
1
(x
1
, x
2
), A(x
1
)? S
1
(x
1
)]
[S(a
1
x
1
,b
2
x
2
)? S
3
(x
1
, x
2
), S(S(a
1
x
1
b
2
)? S
3
(x
1
)]
[S(??, ??)? ??, S(??)? ??]
can be obtained by the HG on
the right.
6 Grammar induction
We define a recursive partitioning of a string s = ?
1
? ? ??
n
as a tree whose nodes are labeled with
subsets of [n]. The root is labeled with [n]. Each leaf is labeled with a single element of [n]. Each
internal node is labeled with the union of the labels of its children, which furthermore must be disjoint.
We say a subset of [n] has fanout k if k is the smallest number such that it can be written as the union of
k sets of consecutive numbers.
1375
A derivation of an LCFRS relates straightforwardly to a recursive partitioning. Consider for example
the derivation of string h s g by the LCFRS that is the first component of the first HG in Example 3.
The root would be labeled {1, 2, 3}, with children labeled {1, 3} and {2}. The node labeled {1, 3} has
children labeled {1} and {3}. The fanout of {1, 3} is 2, whereas it is 1 for all other node labels. One
may also extract a recursive partitioning directly from a hybrid tree, by associating each node with the
set of positions of terminals that it dominates. For example, Figure 1 gives rise to the same recursive
partitioning as the one mentioned above.
One central observation of this paper is that for any hybrid tree h = (s,?
s
) and any recursive par-
titioning of str(h), not necessarily extracted from h, we can construct a hybrid grammar G allowing a
derivation of h, and moreover, the first (LCFRS) component of that derivation parses str(h) according to
the given recursive partitioning. This observation holds for both dependency structures and constituent
structures. The proof for dependency structures is quite technical however, and requires that the second
(sDCP) component of a hybrid grammar has rules with inherited arguments. For lack of space, we can
only give an outline for constituent structures, or in other words, we consider only input hybrid trees over
(?,?) where labels from ? occur exclusively at the leaves. In the resulting hybrid grammars, all sDCP
rules will have only synthesized arguments.
The intuition is the following. For each node of the given recursive partitioning, the numbers in its
label correspond to leaves of s, for the given hybrid tree h = (s,?
s
). There is a smallest number of
maximal disjoint subtrees in s that together contain all those leaves and no others. If we now relate a
parent node of the recursive partitioning to its child nodes, then we see that the relevant disjoint subtrees
in s for the children can be combined to give the relevant disjoint subtrees for the parent, possibly adding
further internal nodes. This process can be expressed in terms of a hybrid rule. Each pair consisting of
a hybrid tree and a recursive partitioning gives rise to a number of hybrid rules. For a collection of such
pairs, we can combine all the rules into a hybrid grammar.
Example 5 Consider again the hybrid tree in Figure 1, in combination with a recursive partitioning
whose root has children labeled {1, 2} and {3}. The relevant disjoint subtrees for {1, 2} are hat and
ADV(schnell) and for {3} there is the subtree gearbeitet. (In a real-world grammar we would have
parts of speech occurring above all the words.) An appropriate hybrid rule that both respects the recursive
partitioning (by the first component LCFRS rule) and puts together relevant parts of the hybrid tree (by
the second component sDCP rule) would be of the form:
[A(x
1
x
2
)? B
1
(x
1
) C
2
(x
2
), A(VP(V(x
1
x
3
)x
2
))? B
1
(x
1
, x
2
) C
2
(x
3
)]
Here A, B and C should to be chosen to be consistent with neighboring nodes in the recursive partition-
ing, to be discussed next. An alternative recursive partitioning whose root has children labeled {1, 3}
and {2} leads to the first hybrid rule in Example 3 (apart from nonterminal names).
We have experimented with two ways of naming nonterminals in the derived hybrid rules. The first
encodes the list of labels of the roots of the relevant disjoint subtrees. In the above example, we would
have a name such as ?hat,ADV? for A. For fanout greater than 1, the locations of the ?gaps? are ex-
plicitly indicated. For example, we might have ?hat, gap, gearbeitet?. We will call this strict labeling.
The second, and less precise, way is to replace lists of labels of siblings by a single name of the form
children-of(X), where X is the label of the parent. We will call this child labeling.
Because our construction of hybrid grammars works for all recursive partitionings, there is no need to
limit ourselves to those extracted directly from the hybrid trees. Moreover, a given recursive partitioning
can be transformed into a similar but different one in which fanout is restricted to some given value
k ? 1. One possible procedure is to start at the root. If the label J of the present node is a singleton,
then we stop. Otherwise, we search breadth-first through the subtree of the present node to identify a
descendant such that both its label J
?
and J \J
?
have fanout not exceeding k. (It is easy to see such a node
always exists: ultimately breadth-first search will reach the leaves, which are labeled with singletons.)
The present node is now given two children, the first is the node labeled J
?
that we identified above, and
the second is a copy of the present subtree, but with J
?
subtracted from the label of every node. (Nodes
1376
labeled with the empty set are removed, and if a node has the same label as its parent then the two are
collapsed.) We repeat the procedure for both children recursively. Note that with k = 1, we can induce
a ?CFG/sDCP? hybrid grammar, that is, with the first component having fanout 1.
Example 6
The recursive partition-
{1, 2, 3, 5, 6, 7}
{1, 3, 6, 7}
{1, 6}
{1} {6}
{3, 7}
{3} {7}
{2, 5}
{2} {5}
=?
{1, 2, 3, 5, 6, 7}
{3, 7}
{3} {7}
{1, 2, 5, 6}
{1, 6}
{1} {6}
{2, 5}
{2} {5}
Figure 3: Transformation of recursive partitioning to restrict fanout to 2.
ing in the left half of Fig-
ure 3 has a node labeled
{1, 3, 6, 7}, with fanout 3.
With J = {1, 2, 3, 5, 6, 7}
and k = 2, one possible
choice for J
?
is {3, 7}, as
then both J
?
and J \ J
?
=
{1, 2, 5, 6} have fanout not
exceeding 2. This leads to
the partitioning in the right
half of the figure. Because now all node labels have fanout not exceeding 2, recursive traversal will make
no further changes. Other valid choices for J
?
would be {2} and {5}. Not a valid choice for J
?
would be
{1, 6}, as J \ {1, 6} = {2, 3, 5, 7}, which has fanout 3.
Our procedure ensures that subsequent grammar induction leads to binary grammars. Note that this
contrasts with binarization algorithms (G?omez-Rodr??guez and Satta, 2009; G?omez-Rodr??guez et al.,
2009) that are applied after a grammar is obtained. Unlike (van Cranenburgh, 2012), our objective is
not to obtain a ?coarse? grammar for the purpose of coarse-to-fine parsing.
In experiments we also considered the right-branching partitioning, whose internal node labels are
{m,m+ 1, . . . , n}, with children labeled {m} and {m+ 1, . . . , n}. Similarly, there is a left-branching
recursive partitioning. In this way, we can induce a ?FA/sDCP? hybrid grammar, with the first component
having finite-state power, which means we can parse in linear time.
7 Experiments
The theory developed above shows that hybrid grammars allow considerable flexibility in the first com-
ponent, leading to a wide range of different time complexities of parsing while, at least potentially, the
same kinds of discontinuous structures can be obtained. We have run experiments to measure what
impact different choices of the first component have on recall/precision and the degree of discontinuity.
The training data consisted of the first 7000 trees of the TIGER treebank (Brants et al., 2004). From
these, recursive partitionings were straightforwardly obtained, and transformed for different values of k.
Also the left-branching and right-branching recursive partitionings were considered. Hybrid grammars
were then extracted using strict or child labeling. Probabilities of rules were determined by relative
frequency estimation, without any smoothing techniques.
Test sentences were taken from the next 500 trees, excluding sentences of length greater than 20 and
those where a single tree did not span the entire sentence, leaving 324 sentences. Parsing was on (gold
standard) parts of speech rather than words. All punctuation was ignored. Labeled recall, precision and
F-measure were computed on objects each consisting of the label of a node and a sequence of pairs of
input positions delimiting substrings covered by that node. The algorithms were implemented in Python
and the experiments were carried out on a desktop with four 3.1GHz Intel Core i5 CPUs.
Results are reported in Table 1. The choice of k = 1 can be seen as a baseline, the first component
then being restricted to context-free power. Note that k = 1, 2, 3 imply parsing complexities O(n
3
),
O(n
6
), O(n
9
), respectively.
In the case of strict labeling, the change from k = 1 to k = 2 leads to significant changes in running
time, but that from k = 2 to k = 3 less so, which can be explained by the smaller number of constituents
that have two gaps, compared to those with zero or one gap. There was no significant change, neither in
running time nor in F-measure, for values of k greater than 4, and therefore these values were omitted
1377
here. Note that for k = ? one would obtain the conventional technique of discontinuous parsing using
LCFRSs. For the right-branching recursive partitionings, the running time is significantly higher than
that for the left-branching ones, although it is linear-time in both cases. This is due to the directional bias
of the implemented parsing strategy. In order to allow a straightforward comparison we have taken the
same parsing strategy in all cases. Note the large number of parse failures for the right-branching and
left-branching partitionings, which is explained by the large number of very specific nonterminals.
Child labeling leads to much smaller
fail R P F1 # gaps secs
strict labeling
k = 1 16 73.0 70.4 71.2 0.0075 442
k = 2 12 73.1 70.7 71.4 0.0111 2,580
k = 3 12 73.1 70.7 71.4 0.0121 2,942
k = 4 12 73.1 70.7 71.4 0.0127 2,828
r-branch 151 65.6 62.4 63.2 0.0118 775
l-branch 266 82.0 78.9 79.5 0.0124 24
child labeling
k = 1 4 74.3 74.2 73.9 0.0120 939
k = 2 4 75.0 75.1 74.7 0.0125 58,164
r-branch 15 73.1 73.0 72.6 0.0117 319
l-branch 56 75.7 76.6 75.7 0.0114 183
Table 1: Number of parse failures, recall, precision, F-
measure, average number of gaps per constituent, and run-
ning time.
numbers of nonterminals, and thereby
also to more ambiguity, and as a re-
sult the increase from time complexity
O(n
3
) to O(n
6
) is more noticeable in
terms of the actual running time. There-
fore carrying out the experiment for k ?
3 was outside our reach. Surprisingly,
the right-branching partitioning performed
very well in this case, with a relatively low
number of parse failures, F-measure com-
peting with k = 1, 2, 3, 4 and strict label-
ing, although it is clearly worse than that
with k = 1, 2 and child labeling, and run-
ning time smaller than in the case of any of
the hybrid grammars where the first com-
ponent has power beyond that of finite au-
tomata.
Child labeling generally gave better F-measure than strict labeling (ignoring strict labeling and left-
branching partitioning, where the many parse failures distort the recall and precision). This seems to be
due to the more accurate parameter estimation that was possible for the smaller numbers of rules obtained
with child labeling.
The differences in F-measure are relatively small for varying k. This can be explained by the relatively
small portion of discontinuous structures in the test set. We have looked closer at discontinuity in the
test set in two ways. First, we measured the average number of gaps per constituent, which in the gold
standard was 0.0171. None of the hybrid grammars came close to achieving this, but we do observe
that more discontinuity is obtained for higher values of k. Secondly, we reran the experiments for only
the 75 sentences out of the aforementioned 324 where the gold structure had at least one discontinuous
phrase. For this smaller set, F1 increases from 59.5 (k = 1) to 61.9 (k = 2, 3, 4) for strict labeling, and
it increases from 64.4 (k = 1) to 66.5 (k = 2) for child labeling. This suggests that with higher k, the
additional discontinuous structures found have at least some overlap with those of the gold standard. Note
again that there is no a priori bound on the fanout of produced hybrid trees, even when the first component
has finite-state power, but the ability to abstract away from discontinuous structures in the training set
seems to be enhanced if the first component is more powerful. This is consistent with observations made
by (van Cranenburgh, 2012).
8 Limitations
The theory from Section 6 does not necessarily mean that any language of hybrid trees can be induced
by a HG whose first-component LCFRS has arbitrarily low fanout. We illustrate this by means of the
language of hybrid trees generated by the HG of Example 4, in which the LCFRS has fanout 2. No
CFG/sDCP grammar in fact exists for the same language, or in other words, the fanout of the first-
component LCFRS cannot be reduced to 1, regardless of how we choose the second-component sDCP.
For a proof, assume that a CFG/sDCP grammar does exist. Letm be the maximum number of members
in the right-hand side of any CFG rule. Let k be the maximum rank of any nonterminal in the second-
component sDCP. Now consider a CFG/sDCP derivation for a hybrid tree with yield a
n
b
n
, where n ?
1378
2 ? k ?m. In a top-down traversal, identify the first CFG nonterminal occurrence that covers a substring
of the input string that has a length smaller than or equal to n/2 and greater than k. This substring
may contain occurrences of a and of b, but because its length is at most n/2, there will not be any pair
consisting of an occurrence of a and an occurrence of b that are both part of that substring, and that
have a common parent labeled S in the hybrid tree. This means that more than k tree fragments or tree
nodes with missing child nodes are involved, which translate to more than k synthesized or inherited
arguments, contradicting the assumptions.
9 Conclusions
We have presented hybrid grammars as a novel framework for describing languages of discontinuous
syntactic structures. This framework sheds light on the relation between various existing techniques, but
it also offers potential for development of novel techniques. Much of what we have shown is merely
an illustration of particular instances of this framework. For example, next to the hybrid grammars
discussed here, we can consider those with macro grammars as first component, or simple context-
free tree grammars as second component. Many variations exist on the illustrated grammar induction
technique. For example, next to our strict labeling and child labeling, one can consider approaches using
latent variables, combined with expectation-maximization.
Acknowledgments
We thank the anonymous reviewers for many helpful comments.
References
T. Becker, A.K. Joshi, and O. Rambow. 1991. Long-distance scrambling and Tree Adjoining Grammars. In Fifth
EACL, pages 21?26.
G.V. Bochmann. 1976. Semantic evaluation from left to right. Communications of the ACM, 19(2):55?62.
A. B?ohmov?a, J. Haji?c, E. Haji?cov?a, and B. Hladk?a. 2000. The Prague dependency treebank: A tree-level anno-
tation scenario. In A. Abeill?e, editor, Treebanks: Building and using syntactically annotated corpora, pages
103?127. Kluwer, Dordrecht.
A. Boyd. 2007. Discontinuity revisited: An improved conversion to context-free representations. In Linguistic
Annotation Workshop, at ACL 2007, pages 41?44.
W.S. Brainerd. 1969. Tree generating regular systems. Information and Control, 14:217?231.
S. Brants, S. Dipper, P. Eisenberg, S. Hansen-Schirra, E. K?onig, W. Lezius, C. Rohrer, G. Smith, and H. Uszkoreit.
2004. TIGER: Linguistic interpretation of a German corpus. Research on Language and Computation, 2:597?
620.
J. Bresnan, R.M. Kaplan, S. Peters, and A. Zaenen. 1982. Cross-serial dependencies in Dutch. Linguistic Inquiry,
13(4):613?635.
H. Bunt. 1996. Formal tools for describing and processing discontinuous constituency structure. In H. Bunt and
A. van Horck, editors, Discontinuous Constituency, pages 63?84. Mouton de Gruyter.
R. Campbell. 2004. Using linguistic principles to recover empty categories. In 42nd Annual Meeting of the ACL,
pages 645?652.
J.D. Choi and M. Palmer. 2010. Robust constituent-to-dependency conversion for English. In Ninth International
Workshop on Treebanks and Linguistic Theories, pages 55?66.
P. Deransart and J. Ma?uszynski. 1985. Relating logic programs and attribute grammars. Journal of Logic Pro-
gramming, 2:119?155.
K. Evang and L. Kallmeyer. 2011. PLCFRS parsing of English discontinuous constituents. In 12th International
Conference on Parsing Technologies, pages 104?116.
1379
M.J. Fischer. 1968. Grammars with macro-like productions. In IEEE Conference Record of 9th Annual Sympo-
sium on Switching and Automata Theory, pages 131?142.
R. Gabbard, S. Kulick, and M. Marcus. 2006. Fully parsing the Penn Treebank. In Human Language Technology
Conference of the NAACL, Main Conference, pages 184?191.
F. G?ecseg and M. Steinby. 1997. Tree languages. In G. Rozenberg and A. Salomaa, editors, Handbook of Formal
Languages, Vol. 3, chapter 1, pages 1?68. Springer, Berlin.
R. Giegerich. 1988. Composition and evaluation of attribute coupled grammars. Acta Informatica, 25:355?423.
C. G?omez-Rodr??guez and G. Satta. 2009. An optimal-time binarization algorithm for linear context-free rewriting
systems with fan-out two. In 47th ACL and 4th International Joint Conference on Natural Language Processing
of the AFNLP, pages 985?993.
C. G?omez-Rodr??guez, M. Kuhlmann, G. Satta, and D. Weir. 2009. Optimal reduction of rule length in linear
context-free rewriting systems. In Human Language Technologies: The 2009 Annual Conference of the North
American Chapter of the ACL, pages 539?547.
M. Johnson. 2002. A simple pattern-matching algorithm for recovering empty nodes and their antecedents. In
40th ACL, pages 136?143.
S. Kahane, A. Nasr, and O. Rambow. 1998. Pseudo-projectivity, a polynomially parsable non-projective depen-
dency grammar. In 36th ACL and 17th International Conference on Computational Linguistics, volume 1, pages
646?652.
K. Kallmeyer and M. Kuhlmann. 2012. A formal model for plausible dependencies in lexicalized tree adjoining
grammar. In Eleventh International Workshop on Tree Adjoining Grammar and Related Formalisms, pages
108?116.
L. Kallmeyer and W. Maier. 2013. Data-driven parsing using probabilistic linear context-free rewriting systems.
Computational Linguistics, 39(1):87?119.
A. Kathol and C. Pollard. 1995. Extraposition via complex domain formation. In 33rd ACL, pages 174?180.
M. Kuhlmann. 2013. Mildly non-projective dependency grammar. Computational Linguistics, 39(2):355?387.
W. Lu, H.T. Ng, W.S. Lee, and L.S. Zettlemoyer. 2008. A generative model for parsing natural language to
meaning representations. In Conference on Empirical Methods in Natural Language Processing, pages 783?
792.
W. Maier and T. Lichte. 2009. Characterizing discontinuity in constituent treebanks. In P. de Groote, M. Egg,
and L. Kallmeyer, editors, 14th Conference on Formal Grammar, volume 5591 of Lecture Notes in Artificial
Intelligence, Bordeaux, France.
M.P. Marcus, B. Santorini, and M.A. Marcinkiewicz. 1993. Building a large annotated corpus of English: The
Penn treebank. Computational Linguistics, 19(2):313?330.
J.D. McCawley. 1982. Parentheticals and discontinuous constituent structure. Linguistic Inquiry, 13(1):91?106.
R. McDonald and F. Pereira. 2006. Online learning of approximate dependency parsing algorithms. In 11th
EACL, pages 81?88.
S. M?uller. 2004. Continuous or discontinuous constituents? a comparison between syntactic analyses for con-
stituent order and their processing systems. Research on Language and Computation, 2:209?257.
J. Nivre and J. Nilsson. 2005. Pseudo-projective dependency parsing. In 43rd ACL, pages 99?106.
J. Nivre. 2009. Non-projective dependency parsing in expected linear time. In Joint Conference of the 47th ACL
and the 4th International Joint Conference on Natural Language Processing of the AFNLP, pages 351?359.
O. Rambow and A.K. Joshi. 1997. A formal look at dependency grammars and phrase structure grammars with
special consideration of word-order phenomena. In L. Wenner, editor, Recent Trends in Meaning-Text Theory.
John Benjamin.
O. Rambow. 2010. The simple truth about dependency and phrase structure representations: An opinion piece.
In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL,
Main Conference, pages 337?340.
1380
W.C. Rounds. 1970. Mappings and grammars on trees. Mathematical Systems Theory, 4:257?287.
G. Satta and E. Peserico. 2005. Some computational complexity results for synchronous context-free grammars.
In Human Language Technology Conference and Conference on Empirical Methods in Natural Language Pro-
cessing, pages 803?810.
S. Seifert and I. Fischer. 2004. Parsing string generating hypergraph grammars. In H. Ehrig, G. Engels, F. Parisi-
Presicce, and G. Rozenberg, editors, 2nd International Conference on Graph Transformations, volume 3256 of
Lecture Notes in Computer Science, pages 352?267. Springer-Verlag.
H. Seki and Y. Kato. 2008. On the generative power of multiple context-free grammars and macro grammars.
IEICE Transactions on Information and Systems, E91-D:209?221.
H. Seki, T. Matsumura, M. Fujii, and T. Kasami. 1991. On multiple context-free grammars. Theoretical Computer
Science, 88:191?229.
S.M. Shieber and Y. Schabes. 1990. Synchronous tree-adjoining grammars. In Papers presented to the 13th
International Conference on Computational Linguistics, volume 3, pages 253?258.
S.M. Shieber. 1985. Evidence against the context-freeness of natural language. Linguistics and Philosophy,
8(3):333?343.
K. Sima?an, R. Bod, S. Krauwer, and R. Scha. 1994. Efficient disambiguation by means of stochastic tree
substitution grammars. In International Conference on New Methods in Language Processing, pages 50?58.
S. Stucky. 1987. Configurational variation in English. In G.J. Huck and A.E. Ojeda, editors, Discontinuous
Constituency, volume 20 of Syntax and Semantics, pages 377?404. Academic Press.
A. van Cranenburgh. 2012. Efficient parsing with linear context-free rewriting systems. In 13th EACL, pages
460?470.
K. Vijay-Shankar and A.K. Joshi. 1985. Some computational properties of tree adjoining grammars. In 23rd ACL,
pages 82?93.
K. Vijay-Shanker, D.J. Weir, and A.K. Joshi. 1987. Characterizing structural descriptions produced by various
grammatical formalisms. In 25th ACL, pages 104?111.
1381
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1058?1066,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Efficient Inference Through Cascades of Weighted Tree Transducers
Jonathan May and Kevin Knight
Information Sciences Institute
University of Southern California
Marina del Rey, CA 90292
{jonmay,knight}@isi.edu
Heiko Vogler
Technische Universita?t Dresden
Institut fu?r Theoretische Informatik
01062 Dresden, Germany
heiko.vogler@tu-dresden.de
Abstract
Weighted tree transducers have been pro-
posed as useful formal models for rep-
resenting syntactic natural language pro-
cessing applications, but there has been
little description of inference algorithms
for these automata beyond formal founda-
tions. We give a detailed description of
algorithms for application of cascades of
weighted tree transducers to weighted tree
acceptors, connecting formal theory with
actual practice. Additionally, we present
novel on-the-fly variants of these algo-
rithms, and compare their performance
on a syntax machine translation cascade
based on (Yamada and Knight, 2001).
1 Motivation
Weighted finite-state transducers have found re-
cent favor as models of natural language (Mohri,
1997). In order to make actual use of systems built
with these formalisms we must first calculate the
set of possible weighted outputs allowed by the
transducer given some input, which we call for-
ward application, or the set of possible weighted
inputs given some output, which we call backward
application. After application we can do some in-
ference on this result, such as determining its k
highest weighted elements.
We may also want to divide up our problems
into manageable chunks, each represented by a
transducer. As noted by Woods (1980), it is eas-
ier for designers to write several small transduc-
ers where each performs a simple transformation,
rather than painstakingly construct a single com-
plicated device. We would like to know, then,
the result of transformation of input or output by
a cascade of transducers, one operating after the
other. As we will see, there are various strate-
gies for approaching this problem. We will con-
sider offline composition, bucket brigade applica-
tion, and on-the-fly application.
Application of cascades of weighted string
transducers (WSTs) has been well-studied (Mohri,
1997). Less well-studied but of more recent in-
terest is application of cascades of weighted tree
transducers (WTTs). We tackle application of WTT
cascades in this work, presenting:
? explicit algorithms for application of WTT cas-
cades
? novel algorithms for on-the-fly application of
WTT cascades, and
? experiments comparing the performance of
these algorithms.
2 Strategies for the string case
Before we discuss application of WTTs, it is help-
ful to recall the solution to this problem in the WST
domain. We recall previous formal presentations
of WSTs (Mohri, 1997) and note informally that
they may be represented as directed graphs with
designated start and end states and edges labeled
with input symbols, output symbols, and weights.1
Fortunately, the solution for WSTs is practically
trivial?we achieve application through a series
of embedding, composition, and projection oper-
ations. Embedding is simply the act of represent-
ing a string or regular string language as an iden-
tity WST. Composition of WSTs, that is, generat-
ing a single WST that captures the transformations
of two input WSTs used in sequence, is not at all
trivial, but has been well covered in, e.g., (Mohri,
2009), where directly implementable algorithms
can be found. Finally, projection is another triv-
ial operation?the domain or range language can
be obtained from a WST by ignoring the output or
input symbols, respectively, on its arcs, and sum-
ming weights on otherwise identical arcs. By em-
bedding an input, composing the result with the
given WST, and projecting the result, forward ap-
plication is accomplished.2 We are then left with
a weighted string acceptor (WSA), essentially a
weighted, labeled graph, which can be traversed
1We assume throughout this paper that weights are in
R+ ? {+?}, that the weight of a path is calculated as the
product of the weights of its edges, and that the weight of a
(not necessarily finite) set T of paths is calculated as the sum
of the weights of the paths of T .
2For backward applications, the roles of input and output
are simply exchanged.
1058
A B
a : a / 1 a : a / 1
C
(a) Input string ?a a? embedded in an
identity WST
E
a : b / . 1 a : a / . 9
b : a / . 5 D
a : b / . 4
a : a / . 6
b : a / . 5
b : b / . 5
b : b / . 5
(b) first WST in cascade
a : c / . 6
b
: c / . 7
F
a : d / . 4
b
: d / . 3
(c) second WST in cascade
E F
a : c / . 0 7
a : c / . 5 4
b
: c / . 6 5
b
: d / . 3 5
D F
a : c / . 2 8
a : c / . 3 6
b
: c / . 6 5
b
: d / . 3 5
a : d / . 3 6
a : d / . 0 3 a : d / . 2 4
a : d / . 1 2
(d) Offline composition approach:
Compose the transducers
A D
B D C
D
a : b / . 1
B E
a : a / . 9
C
E
(e) Bucket brigade approach:
Apply WST (b) to WST (a)
A D F
B D F C
D F
d / . 0 3
c
/ . 0 7
B E F
c / . 5 4
C
E F
c
/ . 5 4
c / . 3 6
c
/ . 2 8
c
/ . 0 7
d / . 3 6
d / . 0 3
d / . 3 6
d / . 1 2
d / . 2 4
(f) Result of offline or bucket application
after projection
A D F
B D F C
D F
d / . 0 3
B E F
c
/ . 5 4
C
E F
c / . 3 6
c
/ . 2 8
c
/ . 0 7
d / . 3 6
d / . 1 2
d / . 2 4
(g) Initial on-the-fly
stand-in for (f)
A D F
B D F C
D F
d / . 0 3
B E F
c / . 5 4
C E F
c / . 3 6
c
/ . 2 8
c
/ . 0 7
d / . 3 6
d / . 1 2
d / . 2 4
(h) On-the-fly stand-in after exploring
outgoing edges of state ADF
A D F
B D F C
D F
d / . 0 3
B E F
c / . 5 4
C
E Fc / . 3 6
c
/ . 2 8
c
/ . 0 7
d / . 3 6
d / . 1 2
d / . 2 4
(i) On-the-fly stand-in after best path has been found
Figure 1: Three different approaches to application through cascades of WSTs.
by well-known algorithms to efficiently find the k-
best paths.
Because WSTs can be freely composed, extend-
ing application to operate on a cascade of WSTs
is fairly trivial. The only question is one of com-
position order: whether to initially compose the
cascade into a single transducer (an approach we
call offline composition) or to compose the initial
embedding with the first transducer, trim useless
states, compose the result with the second, and so
on (an approach we call bucket brigade). The ap-
propriate strategy generally depends on the struc-
ture of the individual transducers.
A third approach builds the result incrementally,
as dictated by some algorithm that requests in-
formation about it. Such an approach, which we
call on-the-fly, was described in (Pereira and Ri-
ley, 1997; Mohri, 2009; Mohri et al, 2000). If
we can efficiently calculate the outgoing edges of
a state of the result WSA on demand, without cal-
culating all edges in the entire machine, we can
maintain a stand-in for the result structure, a ma-
chine consisting at first of only the start state of
the true result. As a calling algorithm (e.g., an im-
plementation of Dijkstra?s algorithm) requests in-
formation about the result graph, such as the set of
outgoing edges from a state, we replace the current
stand-in with a richer version by adding the result
of the request. The on-the-fly approach has a dis-
tinct advantage over the other two methods in that
the entire result graph need not be built. A graphi-
cal representation of all three methods is presented
in Figure 1.
3 Application of tree transducers
Now let us revisit these strategies in the setting
of trees and tree transducers. Imagine we have a
tree or set of trees as input that can be represented
as a weighted regular tree grammar3 (WRTG) and
a WTT that can transform that input with some
weight. We would like to know the k-best trees the
WTT can produce as output for that input, along
with their weights. We already know of several
methods for acquiring k-best trees from a WRTG
(Huang and Chiang, 2005; Pauls and Klein, 2009),
so we then must ask if, analogously to the string
case, WTTs preserve recognizability4 and we can
form an application WRTG. Before we begin, how-
ever, we must define WTTs and WRTGs.
3.1 Preliminaries5
A ranked alphabet is a finite set ? such that ev-
ery member ? ? ? has a rank rk(?) ? N. We
call ?(k) ? ?, k ? N the set of those ? ? ?
such that rk(?) = k. The set of variables is de-
notedX = {x1, x2, . . .} and is assumed to be dis-
joint from any ranked alphabet used in this paper.
We use ? to denote a symbol of rank 0 that is not
in any ranked alphabet used in this paper. A tree
t ? T? is denoted ?(t1, . . . , tk) where k ? 0,
? ? ?(k), and t1, . . . , tk ? T?. For ? ? ?(0) we
3This generates the same class of weighted tree languages
as weighted tree automata, the direct analogue of WSAs, and
is more useful for our purposes.
4A weighted tree language is recognizable iff it can be
represented by a wrtg.
5The following formal definitions and notations are
needed for understanding and reimplementation of the pre-
sented algorithms, but can be safely skipped on first reading
and consulted when encountering an unfamiliar term.
1059
write ? ? T? as shorthand for ?(). For every set
S disjoint from ?, let T?(S) = T??S , where, for
all s ? S, rk(s) = 0.
We define the positions of a tree
t = ?(t1, . . . , tk), for k ? 0, ? ? ?(k),
t1, . . . , tk ? T?, as a set pos(t) ? N? such that
pos(t) = {?} ? {iv | 1 ? i ? k, v ? pos(ti)}.
The set of leaf positions lv(t) ? pos(t) are those
positions v ? pos(t) such that for no i ? N,
vi ? pos(t). We presume standard lexicographic
orderings < and ? on pos.
Let t, s ? T? and v ? pos(t). The label of t
at position v, denoted by t(v), the subtree of t at
v, denoted by t|v, and the replacement at v by s,
denoted by t[s]v, are defined as follows:
1. For every ? ? ?(0), ?(?) = ?, ?|? = ?, and
?[s]? = s.
2. For every t = ?(t1, . . . , tk) such that
k = rk(?) and k ? 1, t(?) = ?, t|? = t,
and t[s]? = s. For every 1 ? i ? k and
v ? pos(ti), t(iv) = ti(v), t|iv = ti|v, and
t[s]iv = ?(t1, . . . , ti?1, ti[s]v, ti+1, . . . , tk).
The size of a tree t, size (t) is |pos(t)|, the car-
dinality of its position set. The yield set of a tree
is the set of labels of its leaves: for a tree t, yd (t)
= {t(v) | v ? lv(t)}.
Let A and B be sets. Let ? : A ? T?(B)
be a mapping. We extend ? to the mapping ? :
T?(A)? T?(B) such that for a ?A, ?(a) = ?(a)
and for k ? 0, ? ? ?(k), and t1, . . . , tk ? T?(A),
?(?(t1, . . . , tk)) = ?(?(t1), . . . , ?(tk)). We indi-
cate such extensions by describing ? as a substi-
tution mapping and then using ? without further
comment.
We use R+ to denote the set {w ? R | w ? 0}
and R?+ to denote R+ ? {+?}.
Definition 3.1 (cf. (Alexandrakis and Bozapa-
lidis, 1987)) A weighted regular tree grammar
(WRTG) is a 4-tuple G = (N,?, P, n0) where:
1. N is a finite set of nonterminals, with n0 ? N
the start nonterminal.
2. ? is a ranked alphabet of input symbols, where
? ?N = ?.
3. P is a tuple (P ?, pi), where P ? is a finite set
of productions, each production p of the form
n ?? u, n ? N , u ? T?(N), and pi : P ? ? R+
is a weight function of the productions. We will
refer to P as a finite set of weighted produc-
tions, each production p of the form n
pi(p)
??? u.
A production p is a chain production if it is
of the form ni
w
?? nj , where ni, nj ? N .6
6In (Alexandrakis and Bozapalidis, 1987), chain produc-
tions are forbidden in order to avoid infinite summations. We
explicitly allow such summations.
A WRTG G is in normal form if each produc-
tion is either a chain production or is of the
form n
w
?? ?(n1, . . . , nk) where ? ? ?(k) and
n1, . . . , nk ? N .
For WRTG G = (N,?, P, n0), s, t, u ? T?(N),
n ? N , and p ? P of the form n
w
?? u, we
obtain a derivation step from s to t by replacing
some leaf nonterminal in s labeled n with u. For-
mally, s ?pG t if there exists some v ? lv(s)
such that s(v) = n and s[u]v = t. We say this
derivation step is leftmost if, for all v? ? lv(s)
where v? < v, s(v?) ? ?. We henceforth as-
sume all derivation steps are leftmost. If, for
some m ? N, pi ? P , and ti ? T?(N) for all
1 ? i ? m, n0 ?p1 t1 . . . ?pm tm, we say
the sequence d = (p1, . . . , pm) is a derivation
of tm in G and that n0 ?? tm; the weight of d
is wt(d) = pi(p1) ? . . . ? pi(pm). The weighted
tree language recognized by G is the mapping
LG : T? ? R?+ such that for every t ? T?, LG(t)
is the sum of the weights of all (possibly infinitely
many) derivations of t in G. A weighted tree lan-
guage f : T? ? R?+ is recognizable if there is a
WRTG G such that f = LG.
We define a partial ordering  on WRTGs
such that for WRTGs G1 = (N1,?, P1, n0) and
G2 = (N2,?, P2, n0), we say G1  G2 iff
N1 ? N2 and P1 ? P2, where the weights are
preserved.
Definition 3.2 (cf. Def. 1 of (Maletti, 2008))
A weighted extended top-down tree transducer
(WXTT) is a 5-tupleM = (Q,?,?, R, q0) where:
1. Q is a finite set of states.
2. ? and ? are the ranked alphabets of in-
put and output symbols, respectively, where
(? ??) ?Q = ?.
3. R is a tuple (R?, pi), where R? is a finite set
of rules, each rule r of the form q.y ?? u for
q ? Q, y ? T?(X), and u ? T?(Q ? X).
We further require that no variable x ? X ap-
pears more than once in y, and that each vari-
able appearing in u is also in y. Moreover,
pi : R? ? R?+ is a weight function of the
rules. As for WRTGs, we refer to R as a finite
set of weighted rules, each rule r of the form
q.y
pi(r)
??? u.
A WXTT is linear (respectively, nondeleting)
if, for each rule r of the form q.y
w
?? u, each
x ? yd (y) ? X appears at most once (respec-
tively, at least once) in u. We denote the class
of all WXTTs as wxT and add the letters L and N
to signify the subclasses of linear and nondeleting
WTT, respectively. Additionally, if y is of the form
?(x1, . . . , xk), we remove the letter ?x? to signify
1060
the transducer is not extended (i.e., it is a ?tradi-
tional? WTT (Fu?lo?p and Vogler, 2009)).
For WXTT M = (Q,?,?, R, q0), s, t ? T?(Q
? T?), and r ? R of the form q.y
w
?? u, we obtain
a derivation step from s to t by replacing some
leaf of s labeled with q and a tree matching y by a
transformation of u, where each instance of a vari-
able has been replaced by a corresponding subtree
of the y-matching tree. Formally, s?rM t if there
is a position v ? pos(s), a substitution mapping
? : X ? T?, and a rule q.y
w
?? u ? R such that
s(v) = (q, ?(y)) and t = s[??(u)]v, where ?? is
a substitution mapping Q ? X ? T?(Q ? T?)
defined such that ??(q?, x) = (q?, ?(x)) for all
q? ? Q and x ? X . We say this derivation step
is leftmost if, for all v? ? lv(s) where v? < v,
s(v?) ? ?. We henceforth assume all derivation
steps are leftmost. If, for some s ? T?, m ? N,
ri ? R, and ti ? T?(Q ? T?) for all 1 ? i ? m,
(q0, s) ?r1 t1 . . . ?rm tm, we say the sequence
d = (r1, . . . , rm) is a derivation of (s, tm) in M ;
the weight of d is wt(d) = pi(r1) ? . . . ? pi(rm).
The weighted tree transformation recognized by
M is the mapping ?M : T? ? T? ? R?+ , such
that for every s ? T? and t ? T?, ?M (s, t) is the
sum of the weights of all (possibly infinitely many)
derivations of (s, t) inM . The composition of two
weighted tree transformations ? : T??T? ? R?+
and ? : T??T? ? R?+ is the weighted tree trans-
formation (? ;?) : T? ? T? ?R?+ where for every
s ? T? and u ? T?, (? ;?)(s, u) =
?
t?T?
?(s, t)
? ?(t, u).
3.2 Applicable classes
We now consider transducer classes where recog-
nizability is preserved under application. Table 1
presents known results for the top-down tree trans-
ducer classes described in Section 3.1. Unlike
the string case, preservation of recognizability is
not universal or symmetric. This is important for
us, because we can only construct an application
WRTG, i.e., a WRTG representing the result of ap-
plication, if we can ensure that the language gen-
erated by application is in fact recognizable. Of
the types under consideration, only wxLNT and
wLNT preserve forward recognizability. The two
classes marked as open questions and the other
classes, which are superclasses of wNT, do not or
are presumed not to. All subclasses of wxLT pre-
serve backward recognizability.7 We do not con-
sider cases where recognizability is not preserved
in the remainder of this paper. If a transducer M
of a class that preserves forward recognizability is
applied to a WRTG G, we can call the forward ap-
7Note that the introduction of weights limits recognizabil-
ity preservation considerably. For example, (unweighted) xT
preserves backward recognizability.
plication WRTG M(G). and ifM preserves back-
ward recognizability, we can call the backward ap-
plication WRTG M(G)/.
Now that we have explained the application
problem in the context of weighted tree transduc-
ers and determined the classes for which applica-
tion is possible, let us consider how to build for-
ward and backward application WRTGs. Our ba-
sic approach mimics that taken for WSTs by us-
ing an embed-compose-project strategy. As in
string world, if we can embed the input in a trans-
ducer, compose with the given transducer, and
project the result, we can obtain the application
WRTG. Embedding a WRTG in a wLNT is a triv-
ial operation?if the WRTG is in normal form and
chain production-free,8 for every production of the
form n
w
?? ?(n1, . . . , nk), create a rule of the form
n.?(x1, . . . , xk)
w
?? ?(n1.x1, . . . , nk.xk). Range
projection of a wxLNT is also trivial?for every
q ? Q and u ? T?(Q ? X) create a production
of the form q
w
?? u? where u? is formed from u
by replacing all leaves of the form q.x with the
leaf q, i.e., removing references to variables, and
w is the sum of the weights of all rules of the form
q.y ?? u in R.9 Domain projection for wxLT is
best explained by way of example. The left side of
a rule is preserved, with variables leaves replaced
by their associated states from the right side. So,
the rule q1.?(?(x1), x2)
w
?? ?(q2.x2, ?(?, q3.x1))
would yield the production q1
w
?? ?(?(q3), q2) in
the domain projection. However, a deleting rule
such as q1.?(x1, x2)
w
?? ?(q2.x2) necessitates the
introduction of a new nonterminal ? that can gen-
erate all of T? with weight 1.
The only missing piece in our embed-compose-
project strategy is composition. Algorithm 1,
which is based on the declarative construction of
Maletti (2006), generates the syntactic composi-
tion of a wxLT and a wLNT, a generalization
of the basic composition construction of Baker
(1979). It calls Algorithm 2, which determines
the sequences of rules in the second transducer
that match the right side of a single rule in the
first transducer. Since the embedded WRTG is of
type wLNT, it may be either the first or second
argument provided to Algorithm 1, depending on
whether the application is forward or backward.
We can thus use the embed-compose-project strat-
egy for forward application of wLNT and back-
ward application of wxLT and wxLNT. Note that
we cannot use this strategy for forward applica-
8Without loss of generality we assume this is so, since
standard algorithms exist to remove chain productions
(Kuich, 1998; E?sik and Kuich, 2003; Mohri, 2009) and con-
vert into normal form (Alexandrakis and Bozapalidis, 1987).
9Finitely many such productions may be formed.
1061
tion of wxLNT, even though that class preserves
recognizability.
Algorithm 1 COMPOSE
1: inputs
2: wxLTM1 = (Q1,?,?, R1, q10)
3: wLNTM2 = (Q2,?,?, R2, q20)
4: outputs
5: wxLTM3 = ((Q1?Q2),?,?, R3, (q10 , q20)) such
thatM3 = (?M1 ; ?M2).
6: complexity
7: O(|R1|max(|R2|size(u?), |Q2|)), where u? is the
largest right side tree in any rule in R1
8: Let R3 be of the form (R?3, pi)
9: R3 ? (?, ?)
10: ?? {(q10 , q20)} {seen states}
11: ?? {(q10 , q20)} {pending states}
12: while ? 6= ? do
13: (q1, q2)?any element of ?
14: ?? ? \ {(q1, q2)}
15: for all (q1.y
w1??? u) ? R1 do
16: for all (z, w2) ? COVER(u,M2, q2) do
17: for all (q, x) ? yd (z)? ((Q1?Q2)?X) do
18: if q 6? ? then
19: ?? ? ? {q}
20: ?? ? ? {q}
21: r ? ((q1, q2).y ?? z)
22: R?3 ? R
?
3 ? {r}
23: pi(r)? pi(r) + (w1 ? w2)
24: return M3
4 Application of tree transducer cascades
What about the case of an input WRTG and a cas-
cade of tree transducers? We will revisit the three
strategies for accomplishing application discussed
above for the string case.
In order for offline composition to be a viable
strategy, the transducers in the cascade must be
closed under composition. Unfortunately, of the
classes that preserve recognizability, only wLNT
is closed under composition (Ge?cseg and Steinby,
1984; Baker, 1979; Maletti et al, 2009; Fu?lo?p and
Vogler, 2009).
However, the general lack of composability of
tree transducers does not preclude us from con-
ducting forward application of a cascade. We re-
visit the bucket brigade approach, which in Sec-
tion 2 appeared to be little more than a choice of
composition order. As discussed previously, ap-
plication of a single transducer involves an embed-
ding, a composition, and a projection. The embed-
ded WRTG is in the class wLNT, and the projection
forms another WRTG. As long as every transducer
in the cascade can be composed with a wLNT
to its left or right, depending on the application
type, application of a cascade is possible. Note
that this embed-compose-project process is some-
what more burdensome than in the string case. For
strings, application is obtained by a single embed-
ding, a series of compositions, and a single projec-
Algorithm 2 COVER
1: inputs
2: u ? T?(Q1 ?X)
3: wTM2 = (Q2,?,?, R2, q20)
4: state q2 ? Q2
5: outputs
6: set of pairs (z, w) with z ? T?((Q1 ? Q2) ? X)
formed by one or more successful runs on u by rules
in R2, starting from q2, and w ? R?+ the sum of the
weights of all such runs.
7: complexity
8: O(|R2|size(u))
9: if u(?) is of the form (q1, x) ? Q1 ?X then
10: zinit ? ((q1, q2), x)
11: else
12: zinit ? ?
13: ?last ? {(zinit, {((?, ?), q2)}, 1)}
14: for all v ? pos(u) such that u(v) ? ?(k) for some
k ? 0 in prefix order do
15: ?v ? ?
16: for all (z, ?, w) ? ?last do
17: for all v? ? lv(z) such that z(v?) = ? do
18: for all (?(v, v?).u(v)(x1, . . . , xk)
w?
??h)?R2
do
19: ?? ? ?
20: Form substitution mapping ? : (Q2 ? X)
? T?((Q1 ? Q2 ?X) ? {?}).
21: for i = 1 to k do
22: for all v?? ? pos(h) such that
h(v??) = (q?2, xi) for some q
?
2 ? Q2 do
23: ??(vi, v?v??)? q?2
24: if u(vi) is of the form
(q1, x) ? Q1 ?X then
25: ?(q?2, xi)? ((q1, q
?
2), x)
26: else
27: ?(q?2, xi)? ?
28: ?v ? ?v ? {(z[?(h)]v? , ?
?, w ? w?)}
29: ?last ? ?v
30: Z ? {z | (z, ?, w) ? ?last}
31: return {(z,
X
(z,?,w)??last
w) | z ? Z}
tion, whereas application for trees is obtained by a
series of (embed, compose, project) operations.
4.1 On-the-fly algorithms
We next consider on-the-fly algorithms for ap-
plication. Similar to the string case, an on-the-
fly approach is driven by a calling algorithm that
periodically needs to know the productions in a
WRTG with a common left side nonterminal. The
embed-compose-project approach produces an en-
tire application WRTG before any inference al-
gorithm is run. In order to admit an on-the-fly
approach we describe algorithms that only gen-
erate those productions in a WRTG that have a
given left nonterminal. In this section we ex-
tend Definition 3.1 as follows: a WRTG is a 6-
tuple G = (N,?, P, n0,M,G) where N,?, P,
and n0 are defined as in Definition 3.1, and either
M = G = ?,10 orM is a wxLNT and G is a nor-
mal form, chain production-free WRTG such that
10In which case the definition is functionally unchanged
from before.
1062
type preserved? source
w[x]T No See w[x]NT
w[x]LT OQ (Maletti, 2009)
w[x]NT No (Ge?cseg and Steinby, 1984)
wxLNT Yes (Fu?lo?p et al, 2010)
wLNT Yes (Kuich, 1999)
(a) Preservation of forward recognizability
type preserved? source
w[x]T No See w[x]NT
w[x]LT Yes (Fu?lo?p et al, 2010)
w[x]NT No (Maletti, 2009)
w[x]LNT Yes See w[x]LT
(b) Preservation of backward recognizability
Table 1: Preservation of forward and backward recognizability for various classes of top-down tree
transducers. Here and elsewhere, the following abbreviations apply: w = weighted, x = extended LHS, L
= linear, N = nondeleting, OQ = open question. Square brackets include a superposition of classes. For
example, w[x]T signifies both wxT and wT.
Algorithm 3 PRODUCE
1: inputs
2: WRTG Gin = (Nin,?, Pin, n0,M,G) such
that M = (Q,?,?, R, q0) is a wxLNT and
G = (N,?, P, n?0,M
?, G?) is a WRTG in normal
form with no chain productions
3: nin ? Nin
4: outputs
5: WRTG Gout = (Nout, ?, Pout, n0,M,G), such that
Gin  Gout and
(nin
w
?? u) ? Pout? (nin
w
?? u) ?M(G).
6: complexity
7: O(|R||P |size(y?)), where y? is the largest left side tree
in any rule in R
8: if Pin contains productions of the form nin
w
?? u then
9: return Gin
10: Nout ? Nin
11: Pout ? Pin
12: Let nin be of the form (n, q), where n ? N and q ? Q.
13: for all (q.y
w1??? u) ? R do
14: for all (?, w2) ? REPLACE(y,G, n) do
15: Form substitution mapping ? : Q ? X ?
T?(N ?Q) such that, for all v ? yd (y) and q? ?
Q, if there exist n? ?N and x ?X such that ?(v)
= n? and y(v) = x, then ?(q?, x) = (n?, q?).
16: p? ? ((n, q)
w1?w2????? ?(u))
17: for all p ? NORM(p?, Nout) do
18: Let p be of the form n0
w
?? ?(n1, . . . , nk) for
? ? ?(k).
19: Nout ? Nout ? {n0, . . . , nk}
20: Pout ? Pout ? {p}
21: return CHAIN-REM(Gout)
G M(G).. In the latter case, G is a stand-in for
M(G)., analogous to the stand-ins for WSAs and
WSTs described in Section 2.
Algorithm 3, PRODUCE, takes as input a
WRTG Gin = (Nin,?, Pin, n0,M,G) and a de-
sired nonterminal nin and returns another WRTG,
Gout that is different from Gin in that it has more
productions, specifically those beginning with nin
that are in M(G).. Algorithms using stand-ins
should call PRODUCE to ensure the stand-in they
are using has the desired productions beginning
with the specific nonterminal. Note, then, that
PRODUCE obtains the effect of forward applica-
Algorithm 4 REPLACE
1: inputs
2: y ? T?(X)
3: WRTG G = (N,?, P, n0,M,G) in normal form,
with no chain productions
4: n ? N
5: outputs
6: set ? of pairs (?, w) where ? is a mapping
pos(y) ? N and w ? R?+ , each pair indicating
a successful run on y by productions in G, starting
from n, and w is the weight of the run.
7: complexity
8: O(|P |size(y))
9: ?last ? {({(?, n)}, 1)}
10: for all v ? pos(y) such that y(v) 6? X in prefix order
do
11: ?v ? ?
12: for all (?, w) ? ?last do
13: ifM 6= ? and G 6= ? then
14: G? PRODUCE(G, ?(v))
15: for all (?(v) w
?
?? y(v)(n1, . . . , nk)) ? P do
16: ?v ? ?v?{(??{(vi, ni), 1 ? i ? k}, w?w?)}
17: ?last ? ?v
18: return ?last
Algorithm 5 MAKE-EXPLICIT
1: inputs
2: WRTG G = (N,?, P, n0,M,G) in normal form
3: outputs
4: WRTG G? = (N ?,?, P ?, n0,M,G), in normal form,
such that ifM 6= ? andG 6= ?, LG? = LM(G). , and
otherwise G? = G.
5: complexity
6: O(|P ?|)
7: G? ? G
8: ?? {n0} {seen nonterminals}
9: ?? {n0} {pending nonterminals}
10: while ? 6= ? do
11: n?any element of ?
12: ?? ? \ {n}
13: ifM 6= ? and G 6= ? then
14: G? ? PRODUCE(G?, n)
15: for all (n w?? ?(n1, . . . , nk)) ? P ? do
16: for i = 1 to k do
17: if ni 6? ? then
18: ?? ? ? {ni}
19: ?? ? ? {ni}
20: return G?
1063
g0
g0
w1??? ?(g0, g1)
g0
w2??? ? g1
w3??? ?
(a) Input WRTG G
a0
a0.?(x1, x2)
w4??? ?(a0.x1, a1.x2)
a0.?(x1, x2)
w5??? ?(a2.x1, a1.x2)
a0.?
w6??? ? a1.?
w7??? ? a2.?
w8??? ?
(b) First transducerMA in the cascade
b0
b0.?(x1, x2)
w9??? ?(b0.x1, b0.x2)
b0.?
w10??? ?
(c) Second transducerMB in the cascade
g0a0
w1?w4????? ?(g0a0, g1a1)
g0a0
w1?w5????? ?(g0a2, g1a1)
g0a0
w2?w6????? ? g1a1
w3?w7????? ?
(d) Productions ofMA(G). built as a consequence
of building the completeMB(MA(G).).
g0a0b0
g0a0b0
w1?w4?w9??????? ?(g0a0b0, g1a1b0)
g0a0b0
w2?w6?w10???????? ? g1a1b0
w3?w7?w10???????? ?
(e) CompleteMB(MA(G).).
Figure 2: Forward application through a cascade
of tree transducers using an on-the-fly method.
tion in an on-the-fly manner.11 It makes calls to
REPLACE, which is presented in Algorithm 4, as
well as to a NORM algorithm that ensures normal
form by replacing a single production not in nor-
mal form with several normal-form productions
that can be combined together (Alexandrakis and
Bozapalidis, 1987) and a CHAIN-REM algorithm
that replaces a WRTG containing chain productions
with an equivalent WRTG that does not (Mohri,
2009).
As an example of stand-in construction, con-
sider the invocation PRODUCE(G1, g0a0), where
G1 = ({g0a0}, {?, ?, ?, ?}, ?, g0a0, MA, G), G
is in Figure 2a,12 and MA is in 2b. The stand-in
WRTG that is output contains the first three of the
four productions in Figure 2d.
To demonstrate the use of on-the-fly application
in a cascade, we next show the effect of PRO-
DUCE when used with the cascadeG?MA ?MB ,
where MB is in Figure 2c. Our driving al-
gorithm in this case is Algorithm 5, MAKE-
11Note further that it allows forward application of class
wxLNT, something the embed-compose-project approach did
not allow.
12By convention the initial nonterminal and state are listed
first in graphical depictions of WRTGs and WXTTs.
rJJ.JJ(x1, x2, x3) ?? JJ(rDT.x1, rJJ.x2, rVB.x3)
rVB.VB(x1, x2, x3) ?? VB(rNNPS.x1, rNN.x3, rVB.x2)
t.?gentle? ?? ?gentle?
(a) Rotation rules
iVB.NN(x1, x2) ?? NN(INS iNN.x1, iNN.x2)
iVB.NN(x1, x2) ?? NN(iNN.x1, iNN.x2)
iVB.NN(x1, x2) ?? NN(iNN.x1, iNN.x2, INS)
(b) Insertion rules
t.VB(x1, x2, x3) ?? X(t.x1, t.x2, t.x3)
t.?gentleman? ?? j1
t.?gentleman? ?? EPS
t.INS ?? j1
t.INS ?? j2
(c) Translation rules
Figure 3: Example rules from transducers used
in decoding experiment. j1 and j2 are Japanese
words.
EXPLICIT, which simply generates the full ap-
plication WRTG using calls to PRODUCE. The
input to MAKE-EXPLICIT is G2 = ({g0a0b0},
{?, ?}, ?, g0a0b0,MB ,G1).13 MAKE-EXPLICIT
calls PRODUCE(G2, g0a0b0). PRODUCE then
seeks to cover b0.?(x1, x2)
w9?? ?(b0.x1, b0.x2)
with productions from G1, which is a stand-in for
MA(G).. At line 14 of REPLACE, G1 is im-
proved so that it has the appropriate productions.
The productions of MA(G). that must be built
to form the complete MB(MA(G).). are shown
in Figure 2d. The complete MB(MA(G).). is
shown in Figure 2e. Note that because we used
this on-the-fly approach, we were able to avoid
building all the productions in MA(G).; in par-
ticular we did not build g0a2
w2?w8????? ?, while a
bucket brigade approach would have built this pro-
duction. We have also designed an analogous on-
the-fly PRODUCE algorithm for backward appli-
cation on linear WTT.
We have now defined several on-the-fly and
bucket brigade algorithms, and also discussed the
possibility of embed-compose-project and offline
composition strategies to application of cascades
of tree transducers. Tables 2a and 2b summa-
rize the available methods of forward and back-
ward application of cascades for recognizability-
preserving tree transducer classes.
5 Decoding Experiments
The main purpose of this paper has been to
present novel algorithms for performing applica-
tion. However, it is important to demonstrate these
algorithms on real data. We thus demonstrate
bucket-brigade and on-the-fly backward applica-
tion on a typical NLP task cast as a cascade of
wLNT. We adapt the Japanese-to-English transla-
13Note that G2 is the initial stand-in for MB(MA(G).).,
since G1 is the initial stand-in forMA(G)..
1064
method WST wxLNT wLNT
oc
?
?
?
bb
?
?
?
otf
? ? ?
(a) Forward application
method WST wxLT wLT wxLNT wLNT
oc
?
? ? ?
?
bb
? ? ? ? ?
otf
? ? ? ? ?
(b) Backward application
Table 2: Transducer types and available methods of forward and backward application of a cascade.
oc = offline composition, bb = bucket brigade, otf = on the fly.
tion model of Yamada and Knight (2001) by trans-
forming it from an English-tree-to-Japanese-string
model to an English-tree-to-Japanese-tree model.
The Japanese trees are unlabeled, meaning they
have syntactic structure but all nodes are labeled
?X?. We then cast this modified model as a cas-
cade of LNT tree transducers. Space does not per-
mit a detailed description, but some example rules
are in Figure 3. The rotation transducer R, a sam-
ple of which is in Figure 3a, has 6,453 rules, the
insertion transducer I, Figure 3b, has 8,122 rules,
and the translation transducer, T , Figure 3c, has
37,311 rules.
We add an English syntax language model L to
the cascade of transducers just described to bet-
ter simulate an actual machine translation decod-
ing task. The language model is cast as an iden-
tity WTT and thus fits naturally into the experimen-
tal framework. In our experiments we try several
different language models to demonstrate varying
performance of the application algorithms. The
most realistic language model is a PCFG. Each
rule captures the probability of a particular se-
quence of child labels given a parent label. This
model has 7,765 rules.
To demonstrate more extreme cases of the use-
fulness of the on-the-fly approach, we build a lan-
guage model that recognizes exactly the 2,087
trees in the training corpus, each with equal
weight. It has 39,455 rules. Finally, to be ultra-
specific, we include a form of the ?specific? lan-
guage model just described, but only allow the
English counterpart of the particular Japanese sen-
tence being decoded in the language.
The goal in our experiments is to apply a single
tree t backward through the cascadeL?R?I?T ?t
and find the 1-best path in the application WRTG.
We evaluate the speed of each approach: bucket
brigade and on-the-fly. The algorithm we use to
obtain the 1-best path is a modification of the k-
best algorithm of Pauls and Klein (2009). Our al-
gorithm finds the 1-best path in a WRTG and ad-
mits an on-the-fly approach.
The results of the experiments are shown in
Table 3. As can be seen, on-the-fly application
is generally faster than the bucket brigade, about
double the speed per sentence in the traditional
LM type method time/sentence
pcfg bucket 28s
pcfg otf 17s
exact bucket >1m
exact otf 24s
1-sent bucket 2.5s
1-sent otf .06s
Table 3: Timing results to obtain 1-best from ap-
plication through a weighted tree transducer cas-
cade, using on-the-fly vs. bucket brigade back-
ward application techniques. pcfg = model rec-
ognizes any tree licensed by a pcfg built from
observed data, exact = model recognizes each of
2,000+ trees with equal weight, 1-sent = model
recognizes exactly one tree.
experiment that uses an English PCFG language
model. The results for the other two language
models demonstrate more keenly the potential ad-
vantage that an on-the-fly approach provides?the
simultaneous incorporation of information from
all models allows application to be done more ef-
fectively than if each information source is consid-
ered in sequence. In the ?exact? case, where a very
large language model that simply recognizes each
of the 2,087 trees in the training corpus is used,
the final application is so large that it overwhelms
the resources of a 4gb MacBook Pro, while the
on-the-fly approach does not suffer from this prob-
lem. The ?1-sent? case is presented to demonstrate
the ripple effect caused by using on-the fly. In the
other two cases, a very large language model gen-
erally overwhelms the timing statistics, regardless
of the method being used. But a language model
that represents exactly one sentence is very small,
and thus the effects of simultaneous inference are
readily apparent?the time to retrieve the 1-best
sentence is reduced by two orders of magnitude in
this experiment.
6 Conclusion
We have presented algorithms for forward and
backward application of weighted tree trans-
ducer cascades, including on-the-fly variants, and
demonstrated the benefit of an on-the-fly approach
to application. We note that a more formal ap-
proach to application of WTTs is being developed,
1065
independent from these efforts, by Fu?lo?p et al
(2010).
Acknowledgments
We are grateful for extensive discussions with
Andreas Maletti. We also appreciate the in-
sights and advice of David Chiang, Steve De-
Neefe, and others at ISI in the preparation of
this work. Jonathan May and Kevin Knight were
supported by NSF grants IIS-0428020 and IIS-
0904684. Heiko Vogler was supported by DFG
VO 1011/5-1.
References
Athanasios Alexandrakis and Symeon Bozapalidis.
1987. Weighted grammars and Kleene?s theorem.
Information Processing Letters, 24(1):1?4.
Brenda S. Baker. 1979. Composition of top-down and
bottom-up tree transductions. Information and Con-
trol, 41(2):186?213.
Zolta?n E?sik and Werner Kuich. 2003. Formal tree se-
ries. Journal of Automata, Languages and Combi-
natorics, 8(2):219?285.
Zolta?n Fu?lo?p and Heiko Vogler. 2009. Weighted tree
automata and tree transducers. In Manfred Droste,
Werner Kuich, and Heiko Vogler, editors, Handbook
of Weighted Automata, chapter 9, pages 313?404.
Springer-Verlag.
Zolta?n Fu?lo?p, Andreas Maletti, and Heiko Vogler.
2010. Backward and forward application of
weighted extended tree transducers. Unpublished
manuscript.
Ferenc Ge?cseg and Magnus Steinby. 1984. Tree Au-
tomata. Akade?miai Kiado?, Budapest.
Liang Huang and David Chiang. 2005. Better k-best
parsing. In Harry Bunt, Robert Malouf, and Alon
Lavie, editors, Proceedings of the Ninth Interna-
tional Workshop on Parsing Technologies (IWPT),
pages 53?64, Vancouver, October. Association for
Computational Linguistics.
Werner Kuich. 1998. Formal power series over trees.
In Symeon Bozapalidis, editor, Proceedings of the
3rd International Conference on Developments in
Language Theory (DLT), pages 61?101, Thessa-
loniki, Greece. Aristotle University of Thessaloniki.
Werner Kuich. 1999. Tree transducers and formal tree
series. Acta Cybernetica, 14:135?149.
Andreas Maletti, Jonathan Graehl, Mark Hopkins, and
Kevin Knight. 2009. The power of extended top-
down tree transducers. SIAM Journal on Comput-
ing, 39(2):410?430.
Andreas Maletti. 2006. Compositions of tree se-
ries transformations. Theoretical Computer Science,
366:248?271.
Andreas Maletti. 2008. Compositions of extended top-
down tree transducers. Information and Computa-
tion, 206(9?10):1187?1196.
Andreas Maletti. 2009. Personal Communication.
Mehryar Mohri, Fernando C. N. Pereira, and Michael
Riley. 2000. The design principles of a weighted
finite-state transducer library. Theoretical Computer
Science, 231:17?32.
Mehryar Mohri. 1997. Finite-state transducers in lan-
guage and speech processing. Computational Lin-
guistics, 23(2):269?312.
Mehryar Mohri. 2009. Weighted automata algo-
rithms. In Manfred Droste, Werner Kuich, and
Heiko Vogler, editors, Handbook of Weighted Au-
tomata, chapter 6, pages 213?254. Springer-Verlag.
Adam Pauls and Dan Klein. 2009. K-best A* parsing.
In Keh-Yih Su, Jian Su, Janyce Wiebe, and Haizhou
Li, editors, Proceedings of the Joint Conference of
the 47th Annual Meeting of the ACL and the 4th In-
ternational Joint Conference on Natural Language
Processing of the AFNLP, pages 958?966, Suntec,
Singapore, August. Association for Computational
Linguistics.
Fernando Pereira and Michael Riley. 1997. Speech
recognition by composition of weighted finite au-
tomata. In Emmanuel Roche and Yves Schabes, ed-
itors, Finite-State Language Processing, chapter 15,
pages 431?453. MIT Press, Cambridge, MA.
William A. Woods. 1980. Cascaded ATN gram-
mars. American Journal of Computational Linguis-
tics, 6(1):1?12.
Kenji Yamada and Kevin Knight. 2001. A syntax-
based statistical translation model. In Proceedings
of 39th Annual Meeting of the Association for Com-
putational Linguistics, pages 523?530, Toulouse,
France, July. Association for Computational Lin-
guistics.
1066
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 145?154,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Generic binarization for parsing and translation
Matthias Bu?chse
Technische Universita?t Dresden
matthias.buechse@tu-dresden.de
Alexander Koller
University of Potsdam
koller@ling.uni-potsdam.de
Heiko Vogler
Technische Universita?t Dresden
heiko.vogler@tu-dresden.de
Abstract
Binarization of grammars is crucial for im-
proving the complexity and performance
of parsing and translation. We present a
versatile binarization algorithm that can
be tailored to a number of grammar for-
malisms by simply varying a formal pa-
rameter. We apply our algorithm to bi-
narizing tree-to-string transducers used in
syntax-based machine translation.
1 Introduction
Binarization amounts to transforming a given
grammar into an equivalent grammar of rank 2,
i.e., with at most two nonterminals on any right-
hand side. The ability to binarize grammars is
crucial for efficient parsing, because for many
grammar formalisms the parsing complexity de-
pends exponentially on the rank of the gram-
mar. It is also critically important for tractable
statistical machine translation (SMT). Syntax-
based SMT systems (Chiang, 2007; Graehl et
al., 2008) typically use some type of synchronous
grammar describing a binary translation rela-
tion between strings and/or trees, such as syn-
chronous context-free grammars (SCFGs) (Lewis
and Stearns, 1966; Chiang, 2007), synchronous
tree-substitution grammars (Eisner, 2003), syn-
chronous tree-adjoining grammars (Nesson et al,
2006; DeNeefe and Knight, 2009), and tree-to-
string transducers (Yamada and Knight, 2001;
Graehl et al, 2008). These grammars typically
have a large number of rules, many of which have
rank greater than two.
The classical approach to binarization, as
known from the Chomsky normal form transfor-
mation for context-free grammars (CFGs), pro-
ceeds rule by rule. It replaces each rule of rank
greater than 2 by an equivalent collection of rules
of rank 2. All CFGs can be binarized in this
way, which is why their recognition problem is
cubic. In the case of linear context-free rewriting
systems (LCFRSs, (Weir, 1988)) the rule-by-rule
technique also applies to every grammar, as long
as an increased fanout it permitted (Rambow and
Satta, 1999).
There are also grammar formalisms for which
the rule-by-rule technique is not complete. In the
case of SCFGs, not every grammar has an equiva-
lent representation of rank 2 in the first place (Aho
and Ullman, 1969). Even when such a represen-
tation exists, it is not always possible to compute
it rule by rule. Nevertheless, the rule-by-rule bi-
narization algorithm of Huang et al (2009) is very
useful in practice.
In this paper, we offer a generic approach
for transferring the rule-by-rule binarization tech-
nique to new grammar formalisms. At the core of
our approach is a binarization algorithm that can
be adapted to a new formalism by changing a pa-
rameter at runtime. Thus it only needs to be im-
plemented once, and can then be reused for a va-
riety of formalisms. More specifically, our algo-
rithm requires the user to (i) encode the grammar
formalism as a subclass of interpreted regular tree
grammars (IRTGs, (Koller and Kuhlmann, 2011))
and (ii) supply a collection of b-rules, which rep-
resent equivalence of grammars syntactically. Our
algorithm then replaces, in a given grammar, each
rule of rank greater than 2 by an equivalent collec-
tion of rules of rank 2, if such a collection is li-
censed by the b-rules. We define completeness of
b-rules in a way that ensures that if any equivalent
collection of rules of rank 2 exists, the algorithm
finds one. As a consequence, the algorithm bina-
rizes every grammar that can be binarized rule by
rule. Step (i) is possible for all the grammar for-
malisms mentioned above. We show Step (ii) for
SCFGs and tree-to-string transducers.
We will use SCFGs as our running example
throughout the paper. We will also apply the algo-
145
rithm to tree-to-string transducers (Graehl et al,
2008; Galley et al, 2004), which describe rela-
tions between strings in one language and parse
trees of another, which means that existing meth-
ods for binarizing SCFGs and LCFRSs cannot be
directly applied to these systems. To our knowl-
edge, our binarization algorithm is the first to bi-
narize such transducers. We illustrate the effec-
tiveness of our system by binarizing a large tree-
to-string transducer for English-German SMT.
Plan of the paper. We start by defining IRTGs
in Section 2. In Section 3, we define the gen-
eral outline of our approach to rule-by-rule bina-
rization for IRTGs, and then extend this to an ef-
ficient binarization algorithm based on b-rules in
Section 4. In Section 5 we show how to use the
algorithm to perform rule-by-rule binarization of
SCFGs and tree-to-string transducers, and relate
the results to existing work.
2 Interpreted regular tree grammars
Grammar formalisms employed in parsing and
SMT, such as those mentioned in the introduc-
tion, differ in the the derived objects?e.g., strings,
trees, and graphs?and the operations involved in
the derivation?e.g., concatenation, substitution,
and adjoining. Interpreted regular tree grammars
(IRTGs) permit a uniform treatment of many of
these formalisms. To this end, IRTGs combine
two ideas, which we explain here.
Algebras IRTGs represent the objects and op-
erations symbolically using terms; the object in
question is obtained by interpreting each symbol
in the term as a function. As an example, Table 1
shows terms for a string and a tree, together with
the denoted object. In the string case, we describe
complex strings as concatenation (con2) of ele-
mentary symbols (e.g., a, b); in the tree case, we
alternate the construction of a sequence of trees
(con2) with the construction of a single tree by
placing a symbol (e.g., ?, ?, ?) on top of a (pos-
sibly empty) sequence of trees. Whenever a term
contains variables, it does not denote an object,
but rather a function. In the parlance of universal-
algebra theory, we are employing initial-algebra
semantics (Goguen et al, 1977).
An alphabet is a nonempty finite set. Through-
out this paper, let X = {x1, x2, . . . } be a set,
whose elements we call variables. We let Xk de-
note the set {x1, . . . , xk} for every k ? 0. Let ?
be an alphabet and V ? X . We write T?(V ) for
the set of all terms over ? with variables V , i.e.,
the smallest set T such that (i) V ? T and (ii) for
every ? ? ?, k ? 0, and t1, . . . , tk ? T , we
have ?(t1, . . . , tk) ? T . Alternatively, we view
T?(V ) as the set of all (rooted, labeled, ordered,
unranked) trees over ? and V , and draw them
as usual. By T? we abbreviate T?(?). The set
C?(V ) of contexts over ? and V is the set of all
trees over ? and V in which each variable in V
occurs exactly once.
A signature is an alphabet ? where each symbol
is equipped with an arity. We write ?|k for the
subset of all k-ary symbols of ?, and ?|k to denote
? ? ?|k. We denote the signature by ? as well.
A signature is binary if the arities do not exceed 2.
Whenever we use T?(V ) with a signature ?, we
assume that the trees are ranked, i.e., each node
labeled by ? ? ?|k has exactly k children.
Let ? be a signature. A ?-algebra A consists
of a nonempty set A called the domain and, for
each symbol f ? ? with rank k, a total function
fA : Ak ? A, the operation associated with f .
We can evaluate any term t in T?(Xk) in A, to
obtain a k-ary operation tA over the domain. In
particular, terms in T? evaluate to elements of A.
For instance, in the string algebra shown in Ta-
ble 1, the term con2(a, b) evaluates to ab, and the
term con2(con2(x2, a), x1) evaluates to a binary
operation f such that, e.g., f(b, c) = cab.
Bimorphisms IRTGs separate the finite control
(state behavior) of a derivation from its derived
object (in its term representation; generational be-
havior); the former is captured by a regular tree
language, while the latter is obtained by applying
a tree homomorphism. This idea goes back to the
tree bimorphisms of Arnold and Dauchet (1976).
Let ? be a signature. A regular tree grammar
(RTG) G over ? is a triple (Q, q0, R) where Q
is a finite set (of states), q0 ? Q, and R is a fi-
nite set of rules of the form q ? ?(q1, . . . , qk),
where q ? Q, ? ? ?|k and q, q1, . . . , qk ? Q.
We call ? the terminal symbol and k the rank
of the rule. Rules of rank greater than two are
called suprabinary. For every q ? Q we de-
fine the language Lq(G) derived from q as the set
{?(t1, . . . , tk) | q ? ?(q1, . . . , qk) ? R, tj ?
Lqj (G)}. If q = q0, we drop the superscript and
write L(G) for the tree language of G. In the lit-
erature, there is a definition of RTG which also
permits more than one terminal symbol per rule,
146
strings over ? trees over ?
example term
and denoted object
con2
a b
7? ab
?
con2
?
con0
?
con0
7?
?
? ?
domain ?? T ?? (set of sequences of trees)
signature ? {a|0 | a ? ?} ? {?|1 | ? ? ?} ?
{conk|k | 0 ? k ? K, k 6= 1} {conk|k | 0 ? k ? K, k 6= 1}
operations a : () 7? a ? : x1 7? ?(x1)
conk : (x1, . . . , xk) 7? x1 ? ? ?xk conk : (x1, . . . , xk) 7? x1 ? ? ?xk
Table 1: Algebras for strings and trees, given an alphabet ? and a maximum arity K ? N.
or none. This does not increase the generative ca-
pacity (Brainerd, 1969).
A (linear, nondeleting) tree homomorphism is a
mapping h : T?(X) ? T?(X) that satisfies the
following condition: there is a mapping g : ? ?
T?(X) such that (i) g(?) ? C?(Xk) for every
? ? ?|k, (ii) h(?(t1, . . . , tk)) is the tree obtained
from g(?) by replacing the occurrence of xj by
h(tj), and (iii) h(xj) = xj . This extends the
usual definition of linear and nondeleting homo-
morphisms (Ge?cseg and Steinby, 1997) to trees
with variables. We abuse notation and write h(?)
for g(?) for every ? ? ?.
Let n ? 1 and ?1, . . . ,?n be signatures. A
(generalized) bimorphism over (?1, . . . ,?n) is a
tuple B = (G, h1, . . . , hn) where G is an RTG
over some signature ? and hi is a tree homo-
morphism from T?(X) into T?i(X). The lan-
guage L(B) induced by B is the tree relation
{(h1(t), . . . , hn(t)) | t ? L(G)}.
An IRTG is a bimorphism whose derived trees
are viewed as terms over algebras; see Fig. 1.
Formally, an IRTG G over (?1, . . . ,?n) is a
tuple (B,A1, . . . ,An) such that B is a bimor-
phism over (?1, . . . ,?n) and Ai is a ?i-algebra.
The language L(G) induced by G is the relation
{(tA11 , . . . , tAnn ) | (t1, . . . , tn) ? L(B)}. We call
the trees in L(G) derivation trees and the terms
in L(B) semantic terms. We say that two IRTGs
G and G? are equivalent if L(G) = L(G?). IRTGs
were first defined in (Koller and Kuhlmann, 2011).
For example, Fig. 2 is an IRTG that encodes
a synchronous context-free grammar (SCFG). It
contains a bimorphism B = (G, h1, h2) consist-
ing of an RTG G with four rules and homomor-
L(G)
T?1 ? ? ? T?n
A1 ? ? ? An
h1 hn
(.)A1 (.)An
? T?
bimorphism B = (G, h1, h2)
IRTG G = (B,A1,A2)
derivation
trees
semantic
terms
derived
objects
Figure 1: IRTG, bimorphism overview.
A? ?(B,C,D)
B ? ?1, C ? ?2, D ? ?3
con3
x1 x2 x3
h1?? [ ? h27?? con
4
x3 a x1 x2
b h1?? [ ?1 h27?? b
c h1?? [ ?2 h27?? c
d h1?? [ ?3 h27?? d
Figure 2: An IRTG encoding an SCFG.
phisms h1 and h2 which map derivation trees to
trees over the signature of the string algebra in Ta-
ble 1. By evaluating these trees in the algebra,
the symbols con3 and con4 are interpreted as con-
catenation, and we see that the first rule encodes
the SCFG rule A ? ?BCD,DaBC?. Figure 3
shows a derivation tree with its two homomorphic
images, which evaluate to the strings bcd and dabc.
IRTGs can be tailored to the expressive capacity
of specific grammar formalisms by selecting suit-
able algebras. The string algebra in Table 1 yields
context-free languages, more complex string al-
147
con3
b c d
h1?? [
?
?1 ?2 ?3
h27??
con4
d a b c
Figure 3: Derivation tree and semantic terms.
A? ??(A?, D)
A? ? ???(B,C)
con2
x1 x2
h?1?? [ ?? h
?
27??
con2
con2
x2 a
x1
con2
x1 x2
h?1?? [ ??? h
?
27??
con2
x1 x2
Figure 4: Binary rules corresponding to the ?-rule
in Fig. 2.
gebras yield tree-adjoining languages (Koller and
Kuhlmann, 2012), and algebras over other do-
mains can yield languages of trees, graphs, or
other objects. Furthermore, IRTGs with n = 1 de-
scribe languages that are subsets of the algebra?s
domain, n = 2 yields synchronous languages or
tree transductions, and so on.
3 IRTG binarization
We will now show how to apply the rule-by-rule
binarization technique to IRTGs. We start in this
section by defining the binarization of a rule in an
IRTG, and characterizing it in terms of binariza-
tion terms and variable trees. We derive the actual
binarization algorithm from this in Section 4.
For the remainder of this paper, let G =
(B,A1, . . . ,An) be an IRTG over (?1, . . . ,?n)
with B = (G, h1, . . . , hn).
3.1 An introductory example
We start with an example to give an intuition of
our approach. Consider the first rule in Fig. 2,
which has rank three. This rule derives (in one
step) the fragment ?(x1, x2, x3) of the derivation
tree in Fig. 3, which is mapped to the semantic
terms h1(?) and h2(?) shown in Fig. 2. Now con-
sider the rules in Fig. 4. These rules can be used to
derive (in two steps) the derivation tree fragment ?
in Fig. 5e. Note that the terms h?1(?) and h1(?)
are equivalent in that they denote the same func-
tion over the string algebra, and so are the terms
h?2(?) and h2(?). Thus, replacing the ?-rule by
the rules in Fig. 4 does not change the language of
the IRTG. However, since the new rules are binary,
(a) con3x1 x2 x3
con4
x3 a x1 x2
(b)
con2
x1 con2
x2 x3
con2
con2
x1 x2
x3
t1 : con2
con2
x3 a
con2
x1 x2
t2 : con
2
con2
x3 con2
a x1
x2
(c)
(d)
con2
x1 x2
x1 con
2
x1 x2
x1 x2
con2
con2
x2 a
x1
x1 con
2
x1 x2
x1 x2
(e)
h1?? [ ? h27??
{x1, x2, x3}
{x1} {x2, x3}
{x2} {x3}
{x1, x2, x3}
{x1, x2}
{x1} {x2}
{x3}
? : {x1, x2, x3}
{x1, x3}
{x1} {x3}
{x2}
con2
con2
x1 x2
x3
t1 :
h?1?? [
??
???
x1 x2
x3
? :
h?27??
con2
con2
x3 a
con2
x1 x2
t2 :
Figure 5: Outline of the binarization algorithm.
parsing and translation will be cheaper.
Now we want to construct the binary rules sys-
tematically. In the example, we proceed as fol-
lows (cf. Fig. 5). For each of the terms h1(?) and
h2(?) (Fig. 5a), we consider all terms that satisfy
two properties (Fig. 5b): (i) they are equivalent
to h1(?) and h2(?), respectively, and (ii) at each
node at most two subtrees contain variables. As
Fig. 5 suggests, there may be many different terms
of this kind. For each of these terms, we ana-
lyze the bracketing of variables, obtaining what we
call a variable tree (Fig. 5c). Now we pick terms
t1 and t2 corresponding to h1(?) and h2(?), re-
spectively, such that (iii) they have the same vari-
able tree, say ? . We construct a tree ? from ? by a
simple relabeling, and we read off the tree homo-
morphisms h?1 and h?2 from a decomposition we
perform on t1 and t2, respectively; see Fig. 5, dot-
ted arrows, and compare the boxes in Fig. 5d with
the homomorphisms in Fig. 4. Now the rules in
Fig. 4 are easily extracted from ?.
These rules are equivalent to r because of (i);
they are binary because ? is binary, which in turn
holds because of (ii); finally, the decompositions
of t1 and t2 are compatible with ? because of (iii).
We call terms t1 and t2 binarization terms if they
satisfy (i)?(iii). We will see below that we can con-
148
struct binary rules equivalent to r from any given
sequence of binarization terms t1, t2, and that bi-
narization terms exist whenever equivalent binary
rules exist. The majority of this paper revolves
around the question of finding binarization terms.
Rule-by-rule binarization of IRTGs follows the
intuition laid out in this example closely: it means
processing each suprabinary rule, attempting to
replace it with an equivalent collection of binary
rules.
3.2 Binarization terms
We will now make this intuition precise. To this
end, we assume that r = q ? ?(q1, . . . , qk) is a
suprabinary rule of G. As we have seen, binariz-
ing r boils down to constructing:
? a tree ? over some binary signature ?? and
? tree homomorphisms h?1, . . . , h?n of type
h?i : T??(X)? T?i(X),
such that h?i(?) and hi(?) are equivalent, i.e., they
denote the same function over Ai. We call such a
tuple (?, h?1, . . . , h?n) a binarization of the rule r.
Note that a binarization of r need not exist. The
problem of rule-by-rule binarization consists in
computing a binarization of each suprabinary rule
of a grammar. If such a binarization does not exist,
the problem does not have a solution.
In order to define variable trees, we assume a
mapping seq that maps each finite set U of pair-
wise disjoint variable sets to a sequence over U
which contains each element exactly once. Let
t ? C?(Xk). The variable set of t is the set of
all variables that occur in t. The set S(t) of sub-
tree variables of t consists of the nonempty vari-
able sets of all subtrees of t. We represent S(t)
as a tree v(t), which we call variable tree as fol-
lows. Any two elements of S(t) are either compa-
rable (with respect to the subset relation) or dis-
joint. We extend this ordering to a tree struc-
ture by ordering disjoint elements via seq. We let
v(L) = {v(t) | t ? L} for every L ? C?(Xk).
In the example of Fig. 5, t1 and t2 have the same
set of subtree variables; it is {{x1}, {x2}, {x3},
{x1, x2}, {x1, x2, x3}}. If we assume that seq or-
ders sets of variables according to the least vari-
able index, we arrive at the variable tree in the cen-
ter of Fig. 5.
Now let t1 ? T?1(Xk), . . . , tn ? T?n(Xk).
We call the tuple t1, . . . , tn binarization terms of
r if the following properties hold: (i) hi(?) and ti
are equivalent; (ii) at each node the tree ti contains
at most two subtrees with variables; and (iii) the
terms t1, . . . , tn have the same variable tree.
Assume for now that we have found binariza-
tion terms t1, . . . , tn. We show how to construct a
binarization (?, h?1, . . . , h?n) of r with ti = h?i(?).
First, we construct ?. Since t1, . . . , tn are bi-
narization terms, they have the same variable tree,
say, ? . We obtain ? from ? by replacing every la-
bel of the form {xj} with xj , and every other label
with a fresh symbol. Because of condition (ii) in
in the definition of binarization terms, ? is binary.
In order to construct h?i(?) for each symbol ?
in ?, we transform ti into a tree t?i with labels from
C?i(X) and the same structure as ?. Then we read
off h?i(?) from the node of t?i that corresponds to
the ?-labeled node of ?. The transformation pro-
ceeds as illustrated in Fig. 6: first, we apply the
maximal decomposition operation d; it replaces
every label f ? ?i|k by the tree f(x1, . . . , xk),
represented as a box. After that, we keep applying
the merge operation  m as often as possible; it
merges two boxes that are in a parent-child rela-
tion, given that one of them has at most one child.
Thus the number of variables in any box can only
decrease. Finally, the reorder operation o orders
the children of each box according to the seq of
their variable sets. These operations do not change
the variable tree; one can use this to show that t?i
has the same structure as ?.
Thus, if we can find binarization terms, we
can construct a binarization of r. Conversely, for
any given binarization (?, h?1, . . . , h?n) the seman-
tic terms h?1(?), . . . , h?n(?) are binarization terms.
This proves the following lemma.
Lemma 1 There is a binarization of r if and only
if there are binarization terms of r.
3.3 Finding binarization terms
It remains to show how we can find binarization
terms of r, if there are any.
Let bi : T?i(Xk) ? P(T?i(Xk)) the mapping
with bi(t) = {t? ? T?i(Xk) | t and t? are equiv-
alent, and at each node t? has at most two chil-
dren with variables}. Figure 5b shows some ele-
ments of b1(h1(?)) and b2(h2(?)) for our exam-
ple. Terms t1, . . . , tn are binarization terms pre-
cisely when ti ? bi(hi(?)) and t1, . . . , tn have the
same variable tree. Thus we can characterize bi-
narization terms as follows.
Lemma 2 There are binarization terms if and
only if?i v(bi(hi(?))) 6= ?.
149
con2
con2
x3 a
con2
x1 x2
 d
con2
x1 x2
con2
x1 x2
x3 a
con2
x1 x2
x1 x2
 m
con2
x1 x2
con2
x1 a
x3
con2
x1 x2
x1 x2
 m
con2
con2
x1 a
x2
x3 con
2
x1 x2
x1 x2
 o
con2
con2
x2 a
x1
con2
x1 x2
x1 x2
x3
Figure 6: Transforming t2 into t?2.
This result suggests the following procedure
for obtaining binarization terms. First, determine
whether the intersection in Lemma 2 is empty. If
it is, then there is no binarization of r. Otherwise,
select a variable tree ? from this set. We know that
there are trees t1, . . . , tn such that ti ? bi(hi(?))
and v(ti) = ? . We can therefore select arbitrary
concrete trees ti ? bi(hi(?))? v?1(?). The terms
t1, . . . , tn are then binarization terms.
4 Effective IRTG binarization
In this section we develop our binarization algo-
rithm. Its key task is finding binarization terms
t1, . . . , tn. This task involves deciding term equiv-
alence, as ti must be equivalent to hi(?). In gen-
eral, equivalence is undecidable, so the task can-
not be solved. We avoid deciding equivalence by
requiring the user to specify an explicit approxi-
mation of bi, which we call a b-rule. This param-
eter gives rise to a restricted version of the rule-
by-rule binarization problem, which is efficiently
computable while remaining practically relevant.
Let ? be a signature. A binarization rule (b-
rule) over ? is a mapping b : ? ? P(T?(X))
where for every f ? ?|k we have that b(f) ?
C?(Xk), at each node of a tree in b(f) only two
children contain variables, and b(f) is a regular
tree language. We extend b to T?(X) by setting
b(xj) = {xj} and b(f(t1, . . . , tk)) = {t[xj/t?j |
1 ? j ? k] | t ? b(f), t?j ? b(tj)}, where [xj/t?j ]
denotes substitution of xj by t?j . Given an alge-
bra A over ?, a b-rule b over ? is called a b-rule
over A if, for every t ? T?(Xk) and t? ? b(t),
t? and t are equivalent inA. Such a b-rule encodes
equivalence in A, and it does so in an explicit and
compact way: because b(f) is a regular tree lan-
guage, a b-rule can be specified by a finite collec-
tion of RTGs, one for each symbol f ? ?. We will
look at examples (for the string and tree algebras
shown earlier) in Section 5.
From now on, we assume that b1, . . . , bn are
b-rules over A1, . . . ,An, respectively. A bina-
rization (?, h?1, . . . , h?n) of r is a binarization of r
with respect to b1, . . . , bn if h?i(?) ? bi(hi(?)).
Likewise, binarization terms t1, . . . , tn are bi-
narization terms with respect to b1, . . . , bn if
ti ? bi(hi(?)). Lemmas 1 and 2 carry over to
the restricted notions. The problem of rule-by-
rule binarization with respect to b1, . . . , bn con-
sists in computing a binarization with respect to
b1, . . . , bn for each suprabinary rule.
By definition, every solution to this restricted
problem is also a solution to the general prob-
lem. The converse need not be true. However,
we can guarantee that the restricted problem has
at least one solution whenever the general problem
has one, by requiring v(bi(hi(?)) = v(b(hi(?)).
Then the intersection in Lemma 2 is empty in the
restricted case if and only if it is empty in the gen-
eral case. We call the b-rules b1, . . . , b1 complete
on G if the equation holds for every ? ? ?.
Now we show how to effectively compute bina-
rization terms with respect to b1, . . . , bn, along the
lines of Section 3.3. More specifically, we con-
struct an RTG for each of the sets (i) bi(hi(?)),
(ii) b?i = v(bi(hi(?))), (iii)
?
i b?i, and (iv) b??i =
bi(hi(?))?v?1(?) (given ? ). Then we can select ?
from (iii) and ti from (iv) using a standard algo-
rithm, such as the Viterbi algorithm or Knuth?s
algorithm (Knuth, 1977; Nederhof, 2003; Huang
and Chiang, 2005). The effectiveness of our pro-
cedure stems from the fact that we only manipulate
RTGs and never enumerate languages.
The construction for (i) is recursive, following
the definition of bi. The base case is a language
{xj}, for which the RTG is easy. For the recursive
case, we use the fact that regular tree languages
are closed under substitution (Ge?cseg and Steinby,
1997, Prop. 7.3). Thus we obtain an RTG Gi with
L(Gi) = bi(hi(?)).
For (ii) and (iv), we need the following auxiliary
150
construction. Let Gi = (P, p0, R). We define the
mapping vari : P ? P(Xk) such that for every
p ? P , every t ? Lp(Gi) contains exactly the vari-
ables in vari(p). We construct it as follows. We
initialize vari(p) to ?unknown? for every p. For
every rule p ? xj , we set vari(p) = {xj}. For
every rule p? ?(p1, . . . , pk) such that vari(pj) is
known, we set vari(p) = ?j vari(pj). This is iter-
ated; it can be shown that vari(p) is never assigned
two different values for the same p. Finally, we set
all remaining unknown entries to ?.
For (ii), we construct an RTG G?i with L(G?i) =
b?i as follows. We let G?i = ({?vari(p)? | p ?
P}, vari(p0), R?) where R? consists of the rules
?{xj}? ? {xj} if p? xi ? R ,
?vari(p)? ? vari(p)(?U1?, . . . , ?Ul??)
if p? ?(p1, . . . , pk) ? R,
V = {vari(pj) | 1 ? j ? k} \ {?},
|V | ? 2, seq(V ) = (U1, . . . , Ul) .
For (iii), we use the standard product construc-
tion (Ge?cseg and Steinby, 1997, Prop. 7.1).
For (iv), we construct an RTG G??i such that
L(G??i ) = b??i as follows. We let G??i = (P, p0, R??),
where R?? consists of the rules
p? ?(p1, . . . , pk)
if p? ?(p1, . . . , pk) ? R,
V = {vari(pj) | 1 ? j ? k} \ {?},
if |V | ? 2, then
(vari(p), seq(V )) is a fork in ? .
By a fork (u, u1 ? ? ?uk) in ? , we mean that there
is a node labeled u with k children labeled u1 up
to uk.
At this point we have all the ingredients for our
binarization algorithm, shown in Algorithm 1. It
operates directly on a bimorphism, because all the
relevant information about the algebras is captured
by the b-rules. The following theorem documents
the behavior of the algorithm. In short, it solves
the problem of rule-by-rule binarization with re-
spect to b-rules b1, . . . , bn.
Theorem 3 Let G = (B,A1, . . . ,An) be
an IRTG, and let b1, . . . , bn be b-rules over
A1, . . . ,An, respectively.
Algorithm 1 terminates. Let B? be the
bimorphism computed by Algorithm 1 on B
and b1, . . . , bn. Then G? = (B?,A1, . . . ,An) is
equivalent to G, and G? is of rank 2 if and only
Input: bimorphism B = (G, h1, . . . , hn),
b-rules b1, . . . , bn over ?1, . . . ,?n
Output: bimorphism B?
1: B? ? (G|?2, h1, . . . , hn)
2: for rule r : q ? ?(q1, . . . , qk) of G|>2 do
3: for i = 1, . . . , n do
4: compute RTG Gi for bi(hi(?))
5: compute RTG G?i for v(bi(hi(?)))
6: compute RTG Gv for ?i L(G?i)
7: if L(Gv) = ? then
8: add r to B?
9: else
10: select t? ? L(Gv)
11: for i = 1, . . . , n do
12: compute RTG G??i for
13: b??i = bi(hi(?)) ? v?1(t?)
14: select ti ? L(G??i )
15: construct binarization for t1, . . . , tn
16: add appropriate rules to B?
Algorithm 1: Complete binarization algorithm,
whereG|?2 andG|>2 isG restricted to binary and
suprabinary rules, respectively.
if every suprabinary rule of G has a binarization
with respect to b1, . . . , bn.
The runtime of Algorithm 1 is dominated by the
intersection construction in line 6, which isO(m1 ?
. . . ?mn) per rule, where mi is the size of G?i. The
quantity mi is linear in the size of the terms on the
right-hand side of hi, and in the number of rules in
the b-rule bi.
5 Applications
Algorithm 1 implements rule-by-rule binarization
with respect to given b-rules. If a rule of the given
IRTG does not have a binarization with respect to
these b-rules, it is simply carried over to the new
grammar, which then has a rank higher than 2. The
number of remaining suprabinary rules depends
on the b-rules (except for rules that have no bi-
narization at all). The user can thus engineer the
b-rules according to their current needs, trading off
completeness, runtime, and engineering effort.
By contrast, earlier binarization algorithms for
formalisms such as SCFG and LCFRS simply at-
tempt to find an equivalent grammar of rank 2;
there is no analogue of our b-rules. The problem
these algorithms solve corresponds to the general
rule-by-rule binarization problem from Section 3.
151
NP
NP
DT
the
x1:NNP POS
?s
x2:JJ x3:NN ?? das x2 x3 der x1
Figure 7: A rule of a tree-to-string transducer.
We show that under certain conditions, our algo-
rithm can be used to solve this problem as well.
In the following two subsections, we illustrate this
for SCFGs and tree-to-string transducers, respec-
tively. In the final subsection, we discuss how to
extend this approach to other grammar formalisms
as well.
5.1 Synchronous context-free grammars
We have used SCFGs as the running example in
this paper. SCFGs are IRTGs with two interpre-
tations into the string algebra of Table 1, as illus-
trated by the example in Fig. 2. In order to make
our algorithm ready to use, it remains to specify a
b-rule for the string algeba.
We use the following b-rule for both b1 and b2.
Each symbol a ? ?i|0 is mapped to the language
{a}. Each symbol conk, k ? 2, is mapped to
the language induced by the following RTG with
states of the form [j, j?] (where 0 ? j < j? ? k)
and final state [0, k]:
[j ? 1, j]? xj (1 ? j ? k)
[j, j?]? con2([j, j??], [j??, j?])
(0 ? j < j?? < j? ? k)
This language expresses all possible ways in
which conk can be written in terms of con2.
Our definition of rule-by-rule binarization with
respect to b1 and b2 coincides with that of Huang
et al (2009): any rule can be binarized by
both algorithms or neither. For instance, for the
SCFG rule A ? ?BCDE,CEBD?, the sets
v(b1(h1(?))) and v(b2(h2(?))) are disjoint, thus
no binarization exists. Two strings of length N
can be parsed with a binary IRTG that represents
an SCFG in time O(N6).
5.2 Tree-to-string transducers
Some approaches to SMT go beyond string-to-
string translation models such as SCFG by exploit-
ing known syntactic structures in the source or tar-
get language. This perspective on translation nat-
urally leads to the use of tree-to-string transducers
NP? ?(NNP, JJ,NN)
NP
con3
NP
con3
DT
the
con0
x1 POS
?s
con0
x2 x3
h1?? [ ? h27?? con
5
das x2 x3 der x1
Figure 8: An IRTG rule encoding the rule in Fig. 7.
(Yamada and Knight, 2001; Galley et al, 2004;
Huang et al, 2006; Graehl et al, 2008). Figure 7
shows an example of a tree-to-string rule. It might
be used to translate ?the Commission?s strategic
plan? into ?das langfristige Programm der Kom-
mission?.
Our algorithm can binarize tree-to-string trans-
ducers; to our knowledge, it is the first algorithm
to do so. We model the tree-to-string transducer
as an IRTG G = ((G, h1, h2),A1,A2), where
A2 is the string algebra, but this time A1 is the
tree algebra shown in Table 1. This algebra has
operations conk to concatenate sequences of trees
and unary ? that maps any sequence (t1, . . . , tl) of
trees to the tree ?(t1, . . . , tl), viewed as a sequence
of length 1. Note that we exclude the operation
con1 because it is the identity and thus unneces-
sary. Thus the rule in Fig. 7 translates to the IRTG
rule shown in Fig. 8.
For the string algebra, we reuse the b-rule from
Section 5.1; we call it b2 here. For the tree algebra,
we use the following b-rule b1. It maps con0 to
{con0} and each unary symbol ? to {?(x1)}. Each
symbol conk, k ? 2, is treated as in the string
case. Using these b-rules, we can binarize the rule
in Fig. 8 and obtain the rules in Fig. 9. Parsing
of a binary IRTG that represents a tree-to-string
transducer is O(N3 ?M) for a string of length N
and a tree with M nodes.
We have implemented our binarization algo-
rithm and the b-rules for the string and the tree
algebra. In order to test our implementation, we
extracted a tree-to-string transducer from about a
million parallel sentences of English-German Eu-
roparl data, using the GHKM rule extractor (Gal-
ley, 2010). Then we binarized the transducer. The
results are shown in Fig. 10. Of the 2.15 million
rules in the extracted transducer, 460,000 were
suprabinary, and 67 % of these could be binarized.
Binarization took 4.4 minutes on a single core of
an Intel Core i5 2520M processor.
152
NP? ??(NNP, A?)
A? ? ???(JJ,NN)
NP
con2
NP
con2
DT
the
con0
con2
x1 POS
?s
con0
x2
h?1?? [ ?? h
?
27??
con2
con2
das x2
con2
der x1
con2
x1 x2
h?1?? [ ??? h
?
27??
con2
x1 x2
Figure 9: Binarization of the rule in Fig. 8.
 1
 1.2
 1.4
 1.6
 1.8
 2
 2.2
 2.4
ext bin
# 
ru
le
s 
(m
ill
io
ns
) rank
0
1
2
3
4
5
6-7
8-10
Figure 10: Rules of a transducer extracted from
Europarl (ext) vs. its binarization (bin).
5.3 General approach
Our binarization algorithm can be used to solve
the general rule-by-rule binarization problem for
a specific grammar formalism, provided that one
can find appropriate b-rules. More precisely,
we need to devise a class C of IRTGs over the
same sequence A1, . . . ,An of algebras that en-
codes the grammar formalism, together with b-
rules b1, . . . , bn over A1, . . . ,An that are com-
plete on every grammar in C, as defined in Sec-
tion 4.
We have already seen the b-rules for SCFGs and
tree-to-string transducers in the preceding subsec-
tions; now we have a closer look at the class C
for SCFGs. We used the class of all IRTGs with
two string algebras and in which hi(?) contains
at most one occurrence of a symbol conk for ev-
ery ? ? ?. On such a grammar the b-rules are
complete. Note that this would not be the case
if we allowed several occurrences of conk, as in
con2(con2(x1, x2), x3). This term is equivalent
to itself and to con2(x1, con2(x2, x3)), but the b-
rules only cover the former. Thus they miss one
variable tree. For the term con3(x1, x2, x3), how-
ever, the b-rules cover both variable trees.
Generally speaking, given C and b-rules
b1, . . . , bn that are complete on every IRTG in C,
Algorithm 1 solves the general rule-by-rule bina-
rization problem on C. We can adapt Theorem 3 by
requiring that G must be in C, and replacing each
of the two occurrences of ?binarization with re-
spect to b1, . . . , bn? by simply ?binarization?. If C
is such that every grammar from a given grammar
formalism can be encoded as an IRTG in C, this
solves the general rule-by-rule binarization prob-
lem of that grammar formalism.
6 Conclusion
We have presented an algorithm for binarizing
IRTGs rule by rule, with respect to b-rules that
the user specifies for each algebra. This improves
the complexity of parsing and translation with any
monolingual or synchronous grammar that can be
represented as an IRTG. A novel algorithm for
binarizing tree-to-string transducers falls out as a
special case.
In this paper, we have taken the perspective that
the binarized IRTG uses the same algebras as the
original IRTG. Our algorithm extends to gram-
mars of arbitrary fanout (such as synchronous
tree-adjoining grammar (Koller and Kuhlmann,
2012)), but unlike LCFRS-based approaches to bi-
narization, it will not increase the fanout to en-
sure binarizability. In the future, we will ex-
plore IRTG binarization with fanout increase. This
could be done by binarizing into an IRTG with
a more complicated algebra (e.g., of string tu-
ples). We might compute binarizations that are
optimal with respect to some measure (e.g., fanout
(Gomez-Rodriguez et al, 2009) or parsing com-
plexity (Gildea, 2010)) by keeping track of this
measure in the b-rule and taking intersections of
weighted tree automata.
Acknowledgments
We thank the anonymous referees for their insight-
ful remarks, and Sarah Hemmen for implementing
an early version of the algorithm. Matthias Bu?chse
was financially supported by DFG VO 1011/6-1.
153
References
Alfred V. Aho and Jeffrey D. Ullman. 1969. Syntax
directed translations and the pushdown assembler.
Journal of Computer and System Sciences, 3:37?56.
Andre? Arnold and Max Dauchet. 1976. Bi-
transduction de fore?ts. In Proc. 3rd Int. Coll. Au-
tomata, Languages and Programming, pages 74?86.
Edinburgh University Press.
Walter S. Brainerd. 1969. Tree generating regular sys-
tems. Information and Control, 14(2):217?231.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2):201?228.
Steve DeNeefe and Kevin Knight. 2009. Synchronous
tree-adjoining machine translation. In Proceedings
of EMNLP, pages 727?736.
Jason Eisner. 2003. Learning non-isomorphic tree
mappings for machine translation. In Proceedings
of the 41st ACL, pages 205?208.
Michel Galley, Mark Hopkins, Kevin Knight, and
Daniel Marcu. 2004. What?s in a translation rule?
In Proceedings of HLT/NAACL, pages 273?280.
Michael Galley. 2010. GHKM rule extractor. http:
//www-nlp.stanford.edu/?mgalley/
software/stanford-ghkm-latest.tar.
gz, retrieved on March 28, 2012.
Ferenc Ge?cseg and Magnus Steinby. 1997. Tree lan-
guages. In G. Rozenberg and A. Salomaa, editors,
Handbook of Formal Languages, volume 3, chap-
ter 1, pages 1?68. Springer-Verlag.
Daniel Gildea. 2010. Optimal parsing strategies for
linear context-free rewriting systems. In Proceed-
ings of NAACL HLT.
Joseph A. Goguen, Jim W. Thatcher, Eric G. Wagner,
and Jesse B. Wright. 1977. Initial algebra seman-
tics and continuous algebras. Journal of the ACM,
24:68?95.
Carlos Gomez-Rodriguez, Marco Kuhlmann, Giorgio
Satta, and David Weir. 2009. Optimal reduction of
rule length in linear context-free rewriting systems.
In Proceedings of NAACL HLT.
Jonathan Graehl, Kevin Knight, and Jonathan May.
2008. Training tree transducers. Computational
Linguistics, 34(3):391?427.
Liang Huang and David Chiang. 2005. Better k-best
parsing. In Proceedings of the 9th IWPT, pages 53?
64.
Liang Huang, Kevin Knight, and Aravind Joshi. 2006.
Statistical syntax-directed translation with extended
domain of locality. In Proceedings of the 7th AMTA,
pages 66?73.
Liang Huang, Hao Zhang, Daniel Gildea, and Kevin
Knight. 2009. Binarization of synchronous
context-free grammars. Computational Linguistics,
35(4):559?595.
Donald E. Knuth. 1977. A generalization of Dijkstra?s
algorithm. Information Processing Letters, 6(1):1?
5.
Alexander Koller and Marco Kuhlmann. 2011. A gen-
eralized view on parsing and translation. In Pro-
ceedings of the 12th IWPT, pages 2?13.
Alexander Koller and Marco Kuhlmann. 2012. De-
composing TAG algorithms using simple alge-
braizations. In Proceedings of the 11th TAG+ Work-
shop, pages 135?143.
Philip M. Lewis and Richard E. Stearns. 1966. Syn-
tax directed transduction. Foundations of Computer
Science, IEEE Annual Symposium on, 0:21?35.
Mark-Jan Nederhof. 2003. Weighted deductive pars-
ing and Knuth?s algorithm. Computational Linguis-
tics, 29(1):135?143.
Rebecca Nesson, Stuart M. Shieber, and Alexander
Rush. 2006. Induction of probabilistic synchronous
tree-insertion grammars for machine translation. In
Proceedings of the 7th AMTA.
Owen Rambow and Giorgio Satta. 1999. Independent
parallelism in finite copying parallel rewriting sys-
tems. Theoretical Computer Science, 223(1?2):87?
120.
David J. Weir. 1988. Characterizing Mildly Context-
Sensitive Grammar Formalisms. Ph.D. thesis, Uni-
versity of Pennsylvania.
Kenji Yamada and Kevin Knight. 2001. A syntax-
based statistical translation model. In Proceedings
of the 39th ACL, pages 523?530.
154
Proceedings of the 2010 Workshop on Applications of Tree Automata in Natural Language Processing, ACL 2010, pages 1?9,
Uppsala, Sweden, 16 July 2010. c?2010 Association for Computational Linguistics
Preservation of Recognizability for
Synchronous Tree Substitution Grammars
Zolta?n Fu?lo?p
Department of Computer Science
University of Szeged
Szeged, Hungary
Andreas Maletti
Departament de Filologies Roma`niques
Universitat Rovira i Virgili
Tarragona, Spain
Heiko Vogler
Faculty of Computer Science
Technische Universita?t Dresden
Dresden, Germany
Abstract
We consider synchronous tree substitution
grammars (STSG). With the help of a
characterization of the expressive power
of STSG in terms of weighted tree bimor-
phisms, we show that both the forward and
the backward application of an STSG pre-
serve recognizability of weighted tree lan-
guages in all reasonable cases. As a con-
sequence, both the domain and the range
of an STSG without chain rules are recog-
nizable weighted tree languages.
1 Introduction
The syntax-based approach to statistical machine
translation (Yamada and Knight, 2001) becomes
more and more competitive in machine transla-
tion, which is a subfield of natural language pro-
cessing (NLP). In this approach the full parse trees
of the involved sentences are available to the trans-
lation model, which can base its decisions on this
rich structure. In the competing phrase-based ap-
proach (Koehn et al, 2003) the translation model
only has access to the linear sentence structure.
There are two major classes of syntax-based
translation models: tree transducers and synchro-
nous grammars. Examples in the former class
are the top-down tree transducer (Rounds, 1970;
Thatcher, 1970), the extended top-down tree trans-
ducer (Arnold and Dauchet, 1982; Galley et al,
2004; Knight and Graehl, 2005; Graehl et al,
2008; Maletti et al, 2009), and the extended
multi bottom-up tree transducer (Lilin, 1981; En-
gelfriet et al, 2009; Maletti, 2010). The lat-
ter class contains the syntax-directed transduc-
tions of Lewis II and Stearns (1968), the gen-
eralized syntax-directed transductions (Aho and
Ullman, 1969), the synchronous tree substitu-
tion grammar (STSG) by Schabes (1990) and the
synchronous tree adjoining grammar (STAG) by
Abeille? et al (1990) and Shieber and Schabes
(1990). The first bridge between those two classes
were established in (Martin and Vere, 1970). Fur-
ther comparisons can be found in (Shieber, 2004)
for STSG and in (Shieber, 2006) for STAG.
One of the main challenges in NLP is the am-
biguity that is inherent in natural languages. For
instance, the sentence ?I saw the man with the
telescope? has several different meanings. Some
of them can be distinguished by the parse tree,
so that probabilistic parsers (Nederhof and Satta,
2006) for natural languages can (partially) achieve
the disambiguation. Such a parser returns a set
of parse trees for each input sentence, and in
addition, each returned parse tree is assigned a
likelihood. Thus, the result can be seen as a
mapping from parse trees to probabilities where
the impossible parses are assigned the probabil-
ity 0. Such mappings are called weighted tree lan-
guages, of which some can be finitely represented
by weighted regular tree grammars (Alexandrakis
and Bozapalidis, 1987). Those weighted tree
languages are recognizable and there exist algo-
rithms (Huang and Chiang, 2005) that efficiently
extract the k-best parse trees (i.e., those with the
highest probability) for further processing.
In this paper we consider synchronized tree sub-
stitution grammars (STSG). To overcome a techni-
cal difficulty we add (grammar) nonterminals to
them. Since an STSG often uses the nontermi-
nals of a context-free grammar as terminal sym-
bols (i.e., its derived trees contain both termi-
nal and nonterminal symbols of the context-free
grammar), we call the newly added (grammar)
nonterminals of the STSG states. Substitution does
no longer take place at synchronized nonterminals
(of the context-free grammar) but at synchronized
states (one for the input and one for the output
side). The states themselves will not appear in the
final derived trees, which yields that it is sufficient
to assume that only identical states are synchro-
1
nized. Under those conventions a rule of an STSG
has the form q ? (s, t, V, a) where q is a state,
a ? R?0 is the rule weight, s is an input tree that
can contain states at the leaves, and t is an output
tree that can also contain states. Finally, the syn-
chronization is defined by V , which is a bijection
between the state-labeled leaves of s and t. We
require that V only relates identical states.
The rules of an STSG are applied in a step-wise
manner. Here we use a derivation relation to define
the semantics of an STSG. It can be understood as
the synchronization of the derivation relations of
two regular tree grammars (Ge?cseg and Steinby,
1984; Ge?cseg and Steinby, 1997) where the syn-
chronization is done on nonterminals (or states) in
the spirit of syntax-directed transductions (Lewis
II and Stearns, 1968). Thus each sentential form
is a pair of (nonterminal-) connected trees.
An STSG G computes a mapping ?G , called
its weighted tree transformation, that assigns a
weight to each pair of input and output trees,
where both the input and output tree may not con-
tain any state. This transformation is obtained as
follows: We start with two copies of the initial
state that are synchronized. Given a connected tree
pair (?, ?), we can apply the rule q ? (s, t, V, a)
to each pair of synchronized states q. Such an ap-
plication replaces the selected state q in ? by s and
the corresponding state q in ? by t. All the re-
maining synchronized states and the synchronized
states of V remain synchronized. The result is
a new connected tree pair. This step charges the
weight a. The weights of successive applications
(or steps) are multiplied to obtain the weight of the
derivation. The weighted tree transformation ?G
assigns to each pair of trees the sum of all weights
of derivations that derive that pair.
Shieber (2004) showed that for every classical
unweighted STSG there exists an equivalent bi-
morphism (Arnold and Dauchet, 1982). The con-
verse result only holds up to deterministic rela-
belings (Ge?cseg and Steinby, 1984; Ge?cseg and
Steinby, 1997), which remove the state informa-
tion from the input and output tree. It is this dif-
ference that motivates us to add states to STSG.
We generalize the result of Shieber (2004) and
prove that every weighted tree transformation that
is computable by an STSG can also be computed
by a weighted bimorphism and vice versa.
Given an STSG and a recognizable weighted
tree language ? of input trees, we investigate un-
der which conditions the weighted tree language
obtained by applying G to ? is again recognizable.
In other words, we investigate under which condi-
tions the forward application of G preserves rec-
ognizability. The same question is investigated for
backward application, which is the corresponding
operation given a recognizable weighted tree lan-
guage of output trees. Since STSG are symmet-
ric (i.e., input and output can be exchanged), the
results for backward application can be obtained
easily from the results for forward application.
Our main result is that forward application pre-
serves recognizability if the STSG G is output-
productive, which means that each rule of G con-
tains at least one output symbol that is not a state.
Dually, backward application preserves recogniz-
ability if G is input-productive, which is the anal-
ogous property for the input side. In fact, those re-
sults hold for weights taken from an arbitrary com-
mutative semiring (Hebisch and Weinert, 1998;
Golan, 1999), but we present the results only for
probabilities.
2 Preliminary definitions
In this contribution we will work with ranked
trees. Each symbol that occurs in such a tree
has a fixed rank that determines the number of
children of nodes with this label. Formally, let
? be a ranked alphabet, which is a finite set ?
together with a mapping rk? : ? ? N that asso-
ciates a rank rk?(?) with every ? ? ?. We let
?k = {? ? ? | rk?(?) = k} be the set contain-
ing all symbols in ? that have rank k. A ?-tree
indexed by a set Q is a tree with nodes labeled by
elements of ? ? Q, where the nodes labeled by
some ? ? ? have exactly rk?(?) children and the
nodes with labels ofQ have no children. Formally,
the set T?(Q) of (term representations of) ?-trees
indexed by a set Q is the smallest set T such that
? Q ? T and
? ?(t1, . . . , tk) ? T for every ? ? ?k and
t1, . . . , tk ? T .
We generally write ? instead of ?() for all ? ? ?0.
We frequently work with the set pos(t) of po-
sitions of a ?-tree t, which is defined as fol-
lows. If t ? Q, then pos(t) = {?}, and if
t = ?(t1, . . . , tk), then
pos(t) = {?} ? {iw | 1 ? i ? k,w ? pos(ti)} .
Thus, each position is a finite (possibly empty) se-
quence of natural numbers. Clearly, each position
2
designates a node of the tree, and vice versa. Thus
we identify nodes with positions. As usual, a leaf
is a node that has no children. The set of all leaves
of t is denoted by lv(t). Clearly, lv(t) ? pos(t).
The label of a position w ? pos(t) is denoted
by t(w). Moreover, for every A ? ? ? Q, let
posA(t) = {w ? pos(t) | t(w) ? A} and
lvA(t) = posA(t) ? lv(t) be the sets of po-
sitions and leaves that are labeled with an ele-
ment of A, respectively. Let t ? T?(Q) and
w1, . . . , wk ? lvQ(t) be k (pairwise) different
leaves. We write t[w1 ? t1, . . . , wk ? tk] or just
t[wi ? ti | 1 ? i ? k] with t1, . . . , tk ? T?(Q)
for the tree obtained from t by replacing, for every
1 ? i ? k, the leaf wi with the tree ti.
For the rest of this paper, let ? and ? be two
arbitrary ranked alphabets. To avoid consistency
issues, we assume that a symbol ? that occurs in
both ? and ? has the same rank in ? and ?; i.e.,
rk?(?) = rk?(?). A deterministic relabeling is
a mapping r : ? ? ? such that r(?) ? ?k for
every ? ? ?k. For a tree s ? T?, the relabeled
tree r(s) ? T? is such that pos(r(s)) = pos(s)
and
(
r(s)
)
(w) = r(s(w)) for every w ? pos(s).
The class of tree transformations computed by de-
terministic relabelings is denoted by dREL.
A tree language (over ?) is a subset of T?. Cor-
respondingly, a weighted tree language (over ?)
is a mapping ? : T? ? R?0. A weighted tree
transformation (over ? and ?) is a mapping
? : T? ? T? ? R?0. Its inverse is the weighted
tree transformation ??1 : T??T? ? R?0, which
is defined by ??1(t, s) = ?(s, t) for every t ? T?
and s ? T?.
3 Synchronous tree substitution
grammars with states
Let Q be a finite set of states with a distinguished
initial state qS ? Q. A connected tree pair is a
tuple (s, t, V, a) where s ? T?(Q), t ? T?(Q),
and a ? R?0. Moreover, V : lvQ(s) ? lvQ(t) is
a bijective mapping such that s(u) = t(v) for ev-
ery (u, v) ? V . We will often identify V with its
graph. Intuitively, a connected tree pair (s, t, V, a)
is a pair of trees (s, t) with a weight a such that
each node labeled by a state in s has a correspond-
ing node in t, and vice versa. Such a connected
tree pair (s, t, V, a) is input-productive and output-
productive if s /? Q and t /? Q, respectively. Let
Conn denote the set of all connected tree pairs that
use the index setQ. Moreover, let Connp ? Conn
contain all connected tree pairs that are input- or
output-productive.
A synchronous tree substitution grammar G
(with states) over ?, ?, and Q (for short: STSG),
is a finite set of rules of the form q ? (s, t, V, a)
where q ? Q and (s, t, V, a) ? Connp. We call
a rule q ? (s, t, V, a) a q-rule, of which q and
(s, t, V, a) are the left-hand and right-hand side,
respectively, and a is its weight. The STSG G is
input-productive (respectively, output-productive)
if each of its rules is so. To simplify the following
development, we assume (without loss of general-
ity) that two different q-rules differ on more than
just their weight.1
To make sure that we do not account essentially
the same derivation twice, we have to use a deter-
ministic derivation mode. Since the choice is im-
material, we use the leftmost derivation mode for
the output component t of a connected tree pair
(s, t, V, a). For every (s, t, V, a) ? Conn such
that V 6= ?, the leftmost output position is the
pair (w,w?) ? V , where w? is the leftmost (i.e.,
the lexicographically smallest) position of lvQ(t).
Next we define derivations. The derivation re-
lation induced by G is the binary relation ?G
over Conn such that
? = (s1, t1, V1, a1)?G (s2, t2, V2, a2) = ?
if and only if the leftmost output position of ? is
(w,w?) ? V1 and there exists a rule
s1(w)? (s, t, V, a) ? G
such that
? s2 = s1[w ? s] and t2 = t1[w? ? t],
? V2 = (V1 \ {(w,w?)}) ? V ? where
V ? = {(ww1, w?w2) | (w1, w2) ? V }, and
? a2 = a1 ? a.
A sequence D = (?1, . . . , ?n) ? Connn is a
derivation of (s, t, V, a) ? Conn from q ? Q if
? ?1 = (q, q, {(?, ?)}, 1),
? ?n = (s, t, V, a), and
? ?i ?G ?i+1 for every 1 ? i ? n? 1.
The set of all such derivations is denoted by
DqG(s, t, V, a).
For every q ? Q, s ? T?(Q), t ? T?(Q), and
bijection V : lvQ(s)? lvQ(t), let
? qG(s, t, V ) =
?
a?R?0,D?D
q
G(s,t,V,a)
a .
1Formally, q ? (s, t, V, a) ? G and q ? (s, t, V, b) ? G
implies a = b.
3
o 1? o ?G,lo
?
e o
6?1
?
?
o e
?G,lo
?
e ?
18?1
?
?
? e
?G,lo
?
?
o o
? 36?1?
?
? ?
o o
?G,lo
?
?
o ?
? 108?1?
?
? ?
? o
?G,lo
?
?
? ?
? 324?1?
?
? ?
? ?
Figure 1: Example derivation with the STSG G of Example 1.
Finally, the weighted tree transformation com-
puted by G is the weighted tree transformation
?G : T? ? T? ? R?0 with ?G(s, t) = ?
qS
G (s, t, ?)
for every s ? T? and t ? T?. As usual, we
call two STSG equivalent if they compute the same
weighted tree transformation. We observe that
every STSG is essentially a linear, nondeleting
weighted extended top-down (or bottom-up) tree
transducer (Arnold and Dauchet, 1982; Graehl et
al., 2008; Engelfriet et al, 2009) without (both-
sided) epsilon rules, and vice versa.
Example 1. Let us consider the STSG G over
? = ? = {?, ?} and Q = {e, o} where qS = o,
rk(?) = 2, and rk(?) = 0. The STSG G consists
of the following rules where V = {(1, 2), (2, 1)}
and id = {(1, 1), (2, 2)}:
o? (?(o, e), ?(e, o), V, 1/3) (?1)
o? (?(e, o), ?(o, e), V, 1/6) (?2)
o? (?(e, o), ?(e, o), id, 1/6) (?3)
o? (?, ?, ?, 1/3) (?4)
e? (?(e, e), ?(e, e), V, 1/2) (?5)
e? (?(o, o), ?(o, o), V, 1/2) (?6)
Figure 1 shows a derivation induced by G. It can
easily be checked that ?G(s, t) = 16?3?2?3?3 where
s = ?(?(?, ?), ?) and t = ?(?, ?(?, ?)). More-
over, ?G(s, s) = ?G(s, t). If ?
q
G(s, t, ?) 6= 0 with
q ? {e, o}, then s and t have the same number
of ?-labeled leaves. This number is odd if q = o,
otherwise it is even. Moreover, at every position
w ? pos(s), the left and right subtrees s1 and s2
are interchanged in s and t (due to V in the rules
?1, ?2, ?5, ?6) except if s1 and s2 contain an even
and odd number, respectively, of ?-labeled leaves.
In the latter case, the subtrees can be interchanged
or left unchanged (both with probability 1/6).
4 Recognizable weighted tree languages
Next, we recall weighted regular tree grammars
(Alexandrakis and Bozapalidis, 1987). To keep
the presentation simple, we identify WRTG with
particular STSG, in which the input and the out-
put components are identical. More precisely, a
weighted regular tree grammar over ? and Q (for
short: WRTG) is an STSG G over ?, ?, and Q
where each rule has the form q ? (s, s, id, a)
where id is the suitable (partial) identity mapping.
It follows that s /? Q, which yields that we do not
have chain rules. In the rest of this paper, we will
specify a rule q ? (s, s, id, a) of a WRTG sim-
ply by q
a
? s. For every q ? Q, we define the
weighted tree language ?qG : T?(Q) ? R?0 gen-
erated by G from q by ?qG(s) = ?
q
G(s, s, idlvQ(s))
for every s ? T?(Q), where idlvQ(s) is the iden-
tity on lvQ(s). Moreover, the weighted tree lan-
guage ?G : T? ? R?0 generated by G is defined
by ?G(s) = ?
qS
G (s) for every s ? T?.
A weighted tree language ? : T? ? R?0 is
recognizable if there exists a WRTG G such that
? = ?G . We note that our notion of recognizabil-
ity coincides with the classical one (Alexandrakis
and Bozapalidis, 1987; Fu?lo?p and Vogler, 2009).
Example 2. We consider the WRTGK over the in-
put alphabet ? = {?, ?} and P = {p, q} with
qS = q, rk(?) = 2, and rk(?) = 0. The WRTG K
contains the following rules:
q
0.4
? ?(p, ?) q
0.6
? ? p
1
? ?(?, q) (?1??3)
Let s ? T? be such that ?K(s) 6= 0. Then s is a
thin tree with zig-zag shape; i.e., there exists n ? 1
such that pos(s) contains exactly the positions:
? (12)i for every 0 ? i ? bn?12 c, and
? (12)i1, (12)i2, and (12)i11 for every integer
0 ? i ? bn?32 c.
The integer n can be understood as the length of
a derivation that derives s from q. Some example
4
??
?
? ?
?
?
?
? ?
?
? ?
?
?
weight: 0.6 weight: 0.24 weight: 0.096
Figure 2: Example trees and their weight in ?G
where G is the WRTG of Example 2.
trees with their weights are displayed in Figure 2.
Proposition 3. For every WRTG G there is an
equivalent WRTG G? in normal form, in which the
right-hand side of every rule contains exactly one
symbol of ?.
Proof. We can obtain the statement by a trivial ex-
tension to the weighted case of the approach used
in Lemma II.3.4 of (Ge?cseg and Steinby, 1984)
and Section 6 of (Ge?cseg and Steinby, 1997).
5 STSG and weighted bimorphisms
In this section, we characterize the expressive
power of STSG in terms of weighted bimorphisms.
This will provide a conceptually clear pattern for
the construction in our main result (see Theo-
rem 6) concerning the closure of recognizable
weighted tree languages under forward and back-
ward application. For this we first recall tree ho-
momorphisms. Let ? and ? be two ranked al-
phabets. Moreover, let h : ? ? T? ? (N?)?
be a mapping such that h(?) = (s, u1, . . . , uk)
for every ? ? ?k where s ? T? and all leaves
u1, . . . , uk ? lv(s) are pairwise different. The
mapping h induces the (linear and complete) tree
homomorphism h? : T? ? T?, which is defined by
h?(?(d1, . . . , dk)) = s[u1 ? d?1, . . . , uk ? d?k]
for every ? ? ?k and d1, . . . , dk ? T? with
h(?) = (s, u1, . . . , uk) and d?i = h?(di) for ev-
ery 1 ? i ? k. Moreover, every (linear and
complete) tree homomorphism is induced in this
way. In the rest of this paper we will not distin-
guish between h and h? and simply write h instead
of h?. The homomorphism h is order-preserving
if u1 < ? ? ? < uk for every ? ? ?k where
h(?) = (s, u1, . . . , uk). Finally, we note that
every ? ? dREL can be computed by a order-
preserving tree homomorphism.
A weighted bimorphism B over ? and ? con-
sists of a WRTG K over ? and P and two tree ho-
T? R?0
T? ? T?
(hin, hout)
?K
?B
Figure 3: Illustration of the semantics of the bi-
morphism B.
momorphisms
hin : T? ? T? and hout : T? ? T? .
The bimorphism B computes the weighted tree
transformation ?B : T? ? T? ? R?0 with
?B(s, t) =
?
d?h?1in (s)?h
?1
out(t)
?K(d)
for every s ? T? and t ? T?.
Without loss of generality, we assume that ev-
ery bimorphism B is presented by an WRTG K in
normal form and an order-preserving output ho-
momorphism hout. Next, we prepare the relation
between STSG and weighted bimorphisms. Let
G be an STSG over ?, ?, and Q. Moreover, let
B be a weighted bimorphism over ? and ? con-
sisting of (i) K over ? and P in normal form,
(ii) hin, and (iii) order-preserving hout. We say
that G and B are related if Q = P and there
is a bijection ? : G ? K such that, for every
rule ? ? G with ? = (q ? (s, t, V, a)) and
?(?) = (p
a
? ?(p1, . . . , pk)) we have
? p = q,
? hin(?) = (s, u1, . . . , uk),
? hout(?) = (t, v1, . . . , vk),
? V = {(u1, v1), . . . , (uk, vk)}, and
? s(ui) = pi = t(vi) for every 1 ? i ? k.
Let G and B be related. The following three easy
statements can be used to prove that G and B are
equivalent:
1. For every derivation D ? DqG(s, t, ?, a) with
q ? Q, s ? T?, t ? T?, a ? R?0, there exists
d ? T? and a derivation D? ? D
q
K(d, d, ?, a)
such that hin(d) = s and hout(d) = t.
2. For every d ? T? and D? ? D
q
K(d, d, ?, a)
with q ? Q and a ? R?0, there exists a
derivation D ? DqG(hin(d), hout(d), ?, a).
3. The mentioned correspondence on deriva-
tions is a bijection.
Given an STSG G, we can easily construct a
weighted bimorphism B such that G and B are re-
lated, and vice versa. Hence, STSG and weighted
5
bimorphisms are equally expressive, which gener-
alizes the corresponding characterization result in
the unweighted case by Shieber (2004), which we
will state after the introduction of STSG?.
Classical synchronous tree substitution gram-
mars (STSG?) do not have states. An STSG? can
be seen as an STSG by considering every substitu-
tion site (i.e., each pair of synchronised nontermi-
nals) as a state.2 We illustrate this by means of an
example here. Let us consider the STSG? G with
the following rules:
? (S(?,B?), S(D?, ?)) with weight 0.2
? (B(?,B?), D(?,D?)) with weight 0.3
? (B(?), D(?)) with weight 0.4.
The substitution sites are marked with ?. Any
rule with root A can be applied to a substitution
site A?. An equivalent STSG G? has the rules:
?S, S? ? (S(?, ?B,D?), S(?B,D?, ?), V, 0.2)
?B,D? ? (B(?, ?B,D?), D(?, ?B,D?), V ?, 0.3)
?B,D? ? (B(?), D(?), ?, 0.4) ,
where V = {(2, 1)} and V ? = {(2, 2)}. It is easy
to see that G and G? are equivalent.
Let ? = {?, ??, ???, ?, ?} where ?, ??, ??? ? ?1
and ?, ? ? ?0 (and ?? 6= ??? and ? 6= ?). We write
?m(t) with t ? T? for the tree ?(? ? ? ?(t) ? ? ? ) con-
taining m occurrences of ? above t. STSG? have a
certain locality property, which yields that STSG?
cannot compute transformations like
?(s, t) =
?
??
??
1 if s = ??(?m(?)) = t
or s = ???(?m(?)) = t
0 otherwise
for every s, t ? T?. The non-local feature is the
correspondence between the symbols ?? and ? (in
the first alternative) and the symbols ??? and ? (in
the second alternative). An STSG that computes ?
is presented in Figure 4.
Theorem 4. Let ? be a weighted tree transforma-
tion. Then the following are equivalent.
1. ? is computable by an STSG.
2. ? is computable by a weighted bimorphism.
3. There exists a STSG? G and deterministic re-
labelings r1 and r2 such that
?(s, t) =
?
s??r?11 (s),t
??r?12 (t)
?G(s
?, t?) .
2To avoid a severe expressivity restriction, several initial
states are allowed for an STSG?.
The inverse of an STSG computable weighted
tree transformation can be computed by an STSG.
Formally, the inverse of the STSG G is the STSG
G?1 = {(t, s, V ?1, a) | (s, t, V, a) ? G}
where V ?1 is the inverse of V . Then ?G?1 = ?
?1
G .
6 Forward and backward application
Let us start this section with the definition of the
concepts of forward and backward application of a
weighted tree transformation ? : T? ? T? ? R?0
to weighted tree languages ? : T? ? R?0 and
? : T? ? R?0. We will give general definitions
first and deal with the potentially infinite sums
later. The forward application of ? to ? is the
weighted tree language ?(?) : T? ? R?0, which
is defined for every t ? T? by
(
?(?)
)
(t) =
?
s?T?
?(s) ? ?(s, t) . (1)
Dually, the backward application of ? to ? is
the weighted tree language ??1(?) : T? ? R?0,
which is defined for every s ? T? by
(
??1(?)
)
(s) =
?
t?T?
?(s, t) ? ?(t) . (2)
In general, the sums in Equations (1) and (2) can
be infinite. Let us recall the important property
that makes them finite in our theorems.
Proposition 5. For every input-productive (resp.,
output-productive) STSG G and every tree s ? T?
(resp., t ? T?), there exist only finitely many
trees t ? T? (respectively, s ? T?) such that
?G(s, t) 6= 0.
Proof sketch. If G is input-productive, then each
derivation step creates at least one input symbol.
Consequently, any derivation for the input tree s
can contain at most as many steps as there are
nodes (or positions) in s. Clearly, there are only
finitely many such derivations, which proves the
statement. Dually, we can obtain the statement for
output-productive STSG.
In the following, we will consider forward ap-
plications ?G(?) where G is an output-productive
STSG and ? is recognizable, which yields that (1)
is well-defined by Proposition 5. Similarly, we
consider backward applications ??1G (?) where G
is input-productive and ? is recognizable, which
again yields that (2) is well-defined by Proposi-
tion 5. The question is whether ?G(?) and ?
?1
G (?)
6
q0 ?
??
q1
1
?
??
q1
q0 ?
???
q2
1
?
???
q2
q1 ?
?
q1
1
?
?
q1
q2 ?
?
q2
1
?
?
q2
q1 ? ?
1
? ?
q2 ? ?
1
? ?
Figure 4: STSG computing the weighted tree transformation ? with initial state q0.
are again recognizable. To avoid confusion, we
occasionally use angled parentheses as in ?p, q?
instead of standard parentheses as in (p, q). More-
over, for ease of presentation, we identify the ini-
tial state qS with ?qS, qS?.
Theorem 6. Let G be an STSG over ?, ?, and Q.
Moreover, let ? : T? ? R?0 and ? : T? ? R?0
be recognizable weighted tree languages.
1. If G is output-productive, then ?G(?) is rec-
ognizable.
2. If G is input-productive, then ??1G (?) is rec-
ognizable.
Proof. For the first item, let K be a WRTG over
? and P such that ? = ?K. Without loss of gen-
erality, we suppose that K is in normal form.
Intuitively, we take each rule q ? (s, t, V, a)
of G and run the WRTG K with every start state p
on the input side s of the rule. In this way, we
obtain a weight b. The WRTG will reach the state
leaves of s in certain states, which we then trans-
fer to the linked states in t to obtain t?. Finally, we
remove the input side and obtain a rule ?p, q?
ab
? t?
for the WRTG L that represents the forward ap-
plication. We note that the same rule of L might
be constructed several times. If this happens, then
we replace the several copies by one rule whose
weight is the sum of the weights of all copies.
As already mentioned the initial state is ?qS, qS?.
Clearly, this approach is inspired (and made rea-
sonable) by the bimorphism characterization. We
can take the HADAMARD product of the WRTG of
the bimorphism with the inverse image of ?K un-
der its input homomorphism. Then we can simply
project to the output side. Our construction per-
forms those three steps at once. The whole process
is illustrated in Figure 5.
Formally, we construct the WRTG L over ? and
P?Qwith the following rules. Let p ? P , q ? Q,
and t? ? T?(P ? Q). Then ?p, q?
c
? t? is a rule
in L?, where
c =
?
(q?(s,t,V,a))?G
V={(u1,v1),...,(uk,vk)}
p1,...,pk?P
t?=t[vi??pi,t(vi)?|1?i?k]
b=?pK(s[ui?pi|1?i?k])
ab .
This might create infinitely many rules in L?, but
clearly only finitely many will have a weight dif-
ferent from 0. Thus, we can obtain the finite rule
set L by removing all rules with weight 0.
The main statement to prove is the following:
for every t ? T?(Q) with lvQ(t) = {v1, . . . , vk},
p, p1, . . . , pk ? P , and q ? Q
?
s?T?(Q)
u1,...,uk?lvQ(s)
?pK(s
?) ? ? qG(s, t, V ) = ?
?p,q?
L (t
?) ,
where
? V = {(u1, v1), . . . , (uk, vk)},
? s? = s[ui ? pi | 1 ? i ? k], and
? t? = t[vi ? ?pi, t(vi)? | 1 ? i ? k].
In particular, for t ? T? we obtain
?
s?T?
?pK(s) ? ?
q
G(s, t, ?) = ?
?p,q?
L (t) ,
which yields
(
?G(?K)
)
(t) =
?
s?T?
?K(s) ? ?G(s, t)
=
?
s?T?
?qSK (s) ? ?
qS
G (s, t, ?)
= ??qS,qS?L (t) = ?L(t) .
In the second item G is input-productive. Then
G?1 is output-productive and ??1G (?) = ?G?1(?).
Hence the first statement proves that ??1G (?) is
recognizable.
Example 7. As an illustration of the construction
in Theorem 6, let us apply the STSG G of Exam-
ple 1 to the WRTG K over ? and P = {p, qS, q?}
and the following rules:
qS
2
5? ?(p, q?) qS
3
5? ?
p
1
? ?(q?, qS) q?
1
? ? .
In fact, K is in normal form and is equivalent to
the WRTG of Example 2. Using the construction
in the proof of Theorem 6 we obtain the WRTG L
over ? and P ?Q with Q = {e, o}. We will only
7
o
?
?
?
o o
? 136?
?
? ?
o o
?q, o?
?
?
?
?q?, o? ?q, o?
? 136 ? 25?
?
? ?
o o
?q, o?
1
36 ?
2
5????
?
? ?
?q, o? ?q?, o?
Figure 5: Illustration of the construction in the proof of Theorem 6 using the WRTG K of Example 7:
some example rule (left), run of K on the input side of the rule (middle), and resulting rule (right).
q1
1
15??
?
q2 q3
q1
1
15??
?
q3 q2
q1
1
5?? ?
q2
1
3?? ? q3
1
5??
?
q1 q2
Figure 6: WRTG constructed in Example 7. We
renamed the states and calculated the weights.
show rules of L that contribute to ?L. To the right
of each rule we indicate from which state ofK and
which rule of G the rule was constructed.
?qS, o?
1
6 ?
2
5?? ?(?q?, o?, ?p, e?) qS, ?2
?qS, o?
1
6 ?
2
5?? ?(?p, e?, ?q?, o?) qS, ?3
?qS, o?
1
3 ?
3
5?? ? qS, ?4
?q?, o?
1
3 ?1?? ? q?, ?4
?p, e?
1
2 ?
2
5?? ?(?qS, o?, ?q?, o?) p, ?6
The initial state ofL is ?qS, o?. It is easy to see that
every t ? T? such that ?L(t) 6= 0 is thin, which
means that |pos(t) ? Nn| ? 2 for every n ? N.
7 Domain and range
Finally, let us consider the domain and range of a
weighted tree transformation ? : T??T? ? R?0.
Again, we first give general definitions and deal
with the infinite sums that might occur in them
later. The domain dom(?) of ? and the range
range(?) of ? are defined by
(
dom(?)
)
(s) =
?
u?T?
?(s, u) (3)
(
range(?)
)
(t) =
?
u?T?
?(u, t) (4)
for every s ? T? and t ? T?. Obviously,
the domain dom(?) is the range range(??1) of
the inverse of ? . Moreover, we can express the
domain dom(?) of ? as the backward applica-
tion ??1(1) where 1 is the weighted tree language
that assigns the weight 1 to each tree. Note that 1
is recognizable for every ranked alphabet.
We note that the sums in Equations (3) and (4)
might be infinite, but for input-productive (re-
spectively, output-productive) STSG G the do-
main dom(?G) (respectively, the range range(?G))
are well-defined by Proposition 5. Using those ob-
servations and Theorem 6 we can obtain the fol-
lowing statement.
Corollary 8. Let G be an STSG. If G is input-
productive, then dom(?G) is recognizable. More-
over, if G is output-productive, then range(?G) is
recognizable.
Proof. These statements follow directly from The-
orem 6 with the help of the observation that
dom(?G) = ?
?1
G (1) and range(?G) = ?G(1).
Conclusion
We showed that every output-productive STSG
preserves recognizability under forward applica-
tion. Dually, every input-productive STSG pre-
serves recognizability under backward applica-
tion. We presented direct and effective construc-
tions for these operations. Special cases of those
constructions can be used to compute the domain
of an input-productive STSG and the range of an
output-productive STSG. Finally, we presented a
characterization of the power of STSG in terms of
weighted bimorphisms.
Acknowledgements
ZOLTA?N FU?LO?P and HEIKO VOGLER were finan-
cially supported by the TA?MOP-4.2.2/08/1/2008-
0008 program of the Hungarian National Devel-
opment Agency. ANDREAS MALETTI was finan-
cially supported by the Ministerio de Educacio?n y
Ciencia (MEC) grant JDCI-2007-760.
8
References
Anne Abeille?, Yves Schabes, and Aravind K. Joshi.
1990. Using lexicalized TAGs for machine trans-
lation. In Proc. 13th CoLing, volume 3, pages 1?6.
University of Helsinki, Finland.
Alfred V. Aho and Jeffrey D. Ullman. 1969. Transla-
tions on a context-free grammar. In Proc. 1st STOC,
pages 93?112. ACM.
Athanasios Alexandrakis and Symeon Bozapalidis.
1987. Weighted grammars and Kleene?s theorem.
Inf. Process. Lett., 24(1):1?4.
Andre? Arnold and Max Dauchet. 1982. Morphismes
et bimorphismes d?arbres. Theoret. Comput. Sci.,
20(1):33?93.
Joost Engelfriet, Eric Lilin, and Andreas Maletti.
2009. Extended multi bottom-up tree transducers
? composition and decomposition. Acta Inform.,
46(8):561?590.
Zolta?n Fu?lo?p and Heiko Vogler. 2009. Weighted tree
automata and tree transducers. In Manfred Droste,
Werner Kuich, and Heiko Vogler, editors, Handbook
of Weighted Automata, chapter 9, pages 313?403.
Springer.
Michel Galley, Mark Hopkins, Kevin Knight, and
Daniel Marcu. 2004. What?s in a translation rule?
In Proc. HLT-NAACL 2004, pages 273?280. ACL.
Ferenc Ge?cseg and Magnus Steinby. 1984. Tree Au-
tomata. Akade?miai Kiado?, Budapest, Hungary.
Ferenc Ge?cseg and Magnus Steinby. 1997. Tree lan-
guages. In Grzegorz Rozenberg and Arto Salomaa,
editors, Handbook of Formal Languages, chapter 1,
pages 1?68. Springer.
Jonathan S. Golan. 1999. Semirings and their Appli-
cations. Kluwer Academic.
Jonathan Graehl, Kevin Knight, and Jonathan May.
2008. Training tree transducers. Computational
Linguistics, 34(3):391?427.
Udo Hebisch and Hanns J. Weinert. 1998. Semirings
? Algebraic Theory and Applications in Computer
Science. World Scientific.
Liang Huang and David Chiang. 2005. Better k-best
parsing. In Proc. 9th IWPT, pages 53?64. ACL.
Kevin Knight and Jonathan Graehl. 2005. An
overview of probabilistic tree transducers for natural
language processing. In Proc. 6th CICLing, volume
3406 of LNCS, pages 1?24. Springer.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proc.
HLT-NAACL 2003, pages 48?54. ACL.
Philip M. Lewis II and Richard Edwin Stearns. 1968.
Syntax-directed transductions. J. ACM, 15(3):465?
488.
Eric Lilin. 1981. Proprie?te?s de clo?ture d?une extension
de transducteurs d?arbres de?terministes. In Proc.
6th CAAP, volume 112 of LNCS, pages 280?289.
Springer.
Andreas Maletti, Jonathan Graehl, Mark Hopkins,
and Kevin Knight. 2009. The power of ex-
tended top-down tree transducers. SIAM J. Comput.,
39(2):410?430.
Andreas Maletti. 2010. Why synchronous tree substi-
tution grammars? In Proc. HLT-NAACL 2010. ACL.
to appear.
David F. Martin and Steven A. Vere. 1970. On syntax-
directed transduction and tree transducers. In Proc.
2nd STOC, pages 129?135. ACM.
Mark-Jan Nederhof and Giorgio Satta. 2006. Proba-
bilistic parsing strategies. J. ACM, 53(3):406?436.
William C. Rounds. 1970. Mappings and grammars
on trees. Math. Systems Theory, 4(3):257?287.
Yves Schabes. 1990. Mathematical and computa-
tional aspects of lexicalized grammars. Ph.D. thesis,
University of Pennsylvania.
Stuart M. Shieber and Yves Schabes. 1990. Syn-
chronous tree-adjoining grammars. In Proc. 13th
CoLing, pages 253?258. ACL.
Stuart M. Shieber. 2004. Synchronous grammars as
tree transducers. In Proc. TAG+7, pages 88?95. Si-
mon Fraser University.
Stuart M. Shieber. 2006. Unifying synchronous tree
adjoining grammars and tree transducers via bimor-
phisms. In Proc. 11th EACL, pages 377?384. ACL.
James W. Thatcher. 1970. Generalized2 sequential
machine maps. J. Comput. System Sci., 4(4):339?
367.
Kenji Yamada and Kevin Knight. 2001. A syntax-
based statistical translation model. In Proc. 39th
ACL, pages 523?530. ACL.
9
Proceedings of the 2010 Workshop on Applications of Tree Automata in Natural Language Processing, ACL 2010, pages 10?18,
Uppsala, Sweden, 16 July 2010. c?2010 Association for Computational Linguistics
A Decoder for
Probabilistic Synchronous Tree Insertion Grammars
Steve DeNeefe ? and Kevin Knight ?
USC Information Sciences Institute
4676 Admiralty Way
Marina del Rey, CA 90292, USA
{sdeneefe,knight}@isi.edu
Heiko Vogler ?
Department of Computer Science
Technische Universita?t Dresden
D-01062 Dresden
Heiko.Vogler@tu-dresden.de
Abstract
Synchronous tree insertion grammars
(STIG) are formal models for syntax-
based machine translation. We formal-
ize a decoder for probabilistic STIG; the
decoder transforms every source-language
string into a target-language tree and cal-
culates the probability of this transforma-
tion.
1 Introduction
Tree adjoining grammars (TAG) were invented in
(Joshi et al 1975) in order to better character-
ize the string sets of natural languages1. One of
TAG?s important features is the ability to introduce
two related syntactic units in a single rule, then
push those two units arbitrarily far apart in sub-
sequent derivation steps. For machine translation
(MT) between two natural languages, each being
generated by a TAG, the derivations of the two
TAG may be synchronized (Abeille et al, 1990;
Shieber and Shabes, 1990) in the spirit of syntax-
directed transductions (Lewis and Stearns, 1968);
this results in synchronous TAG (STAG). Recently,
in (Nesson et al, 2005, 2006) probabilistic syn-
chronous tree insertion grammars (pSTIG) were
discussed as model of MT; a tree insertion gram-
mar is a particular TAG in which the parsing prob-
lem is solvable in cubic-time (Schabes and Wa-
ters, 1994). In (DeNeefe, 2009; DeNeefe and
Knight 2009) a decoder for pSTIG has been pro-
posed which transforms source-language strings
into (modifications of) derivation trees of the
pSTIG. Nowadays, large-scale linguistic STAG
rule bases are available.
In an independent tradition, the automata-
theoretic investigation of the translation of trees
? financially supported by NSF STAGES project, grant
#IIS-0908532.
? financially supported by DFG VO 1011/5-1.
1see (Joshi and Shabes, 1997) for a survey
led to the rich theory of tree transducers (Ge?cseg
and Steinby, 1984, 1997). Roughly speaking, a
tree transducer is a finite term rewriting system. If
each rewrite rule carries a probablity or, in gen-
eral, a weight from some semiring, then they are
weighted tree transducers (Maletti, 2006, 2006a;
Fu?lo?p and Vogler, 2009). Such weighted tree
transducers have also been used for the specifi-
cation of MT of natural languages (Yamada and
Knight, 2001; Knight and Graehl, 2005; Graehl et
al., 2008; Knight and May 2009).
Martin and Vere (1970) and Schreiber (1975)
established the first connections between the two
traditions; also Shieber (2004, 2006) and Maletti
(2008, 2010) investigated their relationship.
The problem addressed in this paper is the
decoding of source-language strings into target-
language trees where the transformation is de-
scribed by a pSTIG. Currently, this decoding re-
quires two steps: first, every source string is
translated into a derivation tree of the underly-
ing pSTIG (DeNeefe, 2009; DeNeefe and Knight
2009), and second, the derivation tree is trans-
formed into the target tree using an embedded tree
transducer (Shieber, 2006). We propose a trans-
ducer model, called a bottom-up tree adjoining
transducer, which performs this decoding in a sin-
gle step and, simultaneously, computes the prob-
abilities of its derivations. As a basis of our ap-
proach, we present a formal definition of pSTIG.
2 Preliminaries
For two sets ? and A, we let U?(A) be the set of
all (unranked) trees over ? in which also elements
of A may label leaves. We abbreviate U?(?) by
U?. We denote the set of positions, leaves, and
non-leaves of ? ? U? by pos(?) ? N?, lv(?), and
nlv(?), resp., where ? denotes the root of ? and
w.i denotes the ith child of position w; nlv(?) =
pos(?) \ lv(?). For a position w ? pos(?), the la-
bel of ? at w (resp., subtree of ? at w) is denoted
10
by ?(w) (resp., ?|w). If additionally ? ? U?(A),
then ?[?]w denotes the tree which is obtained from
? by replacing its subtree at w by ?. For every
? ? ? ?A, the set pos?(?) is the set of all those
positions w ? pos(?) such that ?(w) ? ?. Simi-
larly, we can define lv?(?) and nlv?(?). The yield
of ? is the sequence yield(?) ? (? ? A)? of sym-
bols that label the leaves from left to right.
If we associate with ? ? ? a rank k ? N, then
we require that in every tree ? ? U?(A) every ?-
labeled position has exactly k children.
3 Probabilistic STAG and STIG
First we will define probabilistic STAG, and sec-
ond, as a special case, probabilistic STIG.
Let N and T be two disjoint sets of, resp., non-
terminals and terminals. A substitution rule r is a
tuple (?s, ?t, V,W, P radj) where
? ?s, ?t ? UN (T ) (source and target tree) and
|lvN (?s)| = |lvN (?t)|,
? V ? lvN (?s)?lvN (?t) (substitution sites), V
is a one-to-one relation, and |V | = |lvN (?s)|,
? W ? nlvN (?s)?nlvN (?t) (potential adjoin-
ing sites), and
? P radj : W ? [0, 1] (adjoining probability).
An auxiliary rule r is a tuple (?s, ?t, V,W, ?, P radj)
where ?s, ?t,W , and P radj are defined as above and
? V is defined as above except that |V | =
|lvN (?s)| ? 1 and
? ? = (?s, ?t) ? lvN (?s)? lvN (?t) and neither
?s nor ?t occurs in any element of V ; more-
over, ?s(?) = ?s(?s) and ?t(?) = ?t(?t), and
?s 6= ? 6= ?t; the node ?s (and ?t) is called
the foot-node of ?s (resp., ?t).
An (elementary) rule is either a substitution rule
or an auxiliary rule. The root-category of a rule r
is the tuple (?s(?), ?t(?)), denoted by rc(r).
A probabilistic synchronous tree ad-
joining grammar (pSTAG) is a tuple
G = (N,T, (Ss, St),S,A, P ) such that N
and T are two disjoint sets (resp., of nonterminals
and terminals), (Ss, St) ? N?N (start nontermi-
nal), S and A are finite sets of, resp., substitution
rules and auxiliary rules, and P : S ? A ? [0, 1]
such that for every (A,B) ? N ?N ,
?
r?S
rc(r)=(A,B)
P (r) = 1 and
?
r?A
rc(r)=(A,B)
P (r) = 1
assuming that in each case the number of sum-
mands is not zero. In the following, let G always
denote an arbitrary pSTAG.
Ss
? A? A
?
a
r1??
St
B
?
B? ?a
P (r1) = 1
P r1adj(a) = .9
A
A ?
b,c
?
r2??
B
B
?
B
b
c ?
P (r2) = .4
P r2adj(b) = .2
P r2adj(c) = .6
A
A A
?
d
? e
r3??
B
? B
d,e
?
P (r3) = .6
P r3adj(d) = .3
P r3adj(e) = .8
A
?
r4??
B
?
P (r4) = .1
A
?
r5??
B
?
P (r5) = .9
Figure 1: The running example pSTAG G.
In Fig. 1 we show the rules of our running ex-
ample pSTAG, where the capital Roman letters are
the nonterminals and the small Greek letters are
the terminals. The substitution site (in rule r1) is
indicated by ?, and the potential adjoining sites are
denoted2 by a, b, c, d, and e. For instance, in for-
mal notation the rules r1 and r2 are written as fol-
lows:
r1 = (Ss(?,A,A(?)), St(B(?), B, ?), {?}, {a}, P r1adj)
where ? = (2, 2) and a = (3, 1), and
r2 = (A(A, ?), B(B(?), B), ?, {b, c}, ?, P r2adj)
where b = (?, ?), c = (?, 1), and ? = (1, 2).
In the derivation relation of G we will distin-
guish four types of steps:
1. substitution of a rule at a substitution site
(substitution),
2. deciding to turn a potential adjoining site into
an activated adjoining site (activation),
3. deciding to drop a potential adjoining site,
i.e., not to adjoin, (non-adjoining) and
4. adjoining of a rule at an activated adjoining
site (adjoining).
In the sentential forms (defined below) we will
maintain for every adjoining site w a two-valued
flag g(w) indicating whether w is a potential
(g(w) = p) or an activated site (g(w) = a).
The set of sentential forms ofG is the set SF(G)
of all tuples ? = (?s, ?t, V,W, g) with
2Their placement (as left or right index) does not play a
role yet, but will later when we introduce pSTIG.
11
? ?s, ?t ? UN (T ),
? V ? lvN (?s) ? lvN (?t) is a one-to-one rela-
tion, |V | = |lvN (?s)| = |lvN (?t)|,
? W ? nlvN (?s)? nlvN (?t), and
? g : W ? {p, a}.
The derivation relation (of G) is the binary
relation ? ? SF(G) ? SF(G) such that
for every ?1 = (?1s , ?1t , V1,W1, g1) and ?2 =
(?2s , ?2t , V2,W2, g2) we have ?1 ? ?2 iff one of
the following is true:
1. (substitution) there are w = (ws, wt) ? V1
and r = (?s, ?t, V,W, P radj) ? S such that
? (?1s (ws), ?1t (wt)) = rc(r),
? ?2s = ?1s [?s]ws and ?2t = ?1t [?t]wt ,
? V2 = (V1 \ {w}) ? w.V ,3
? W2 = W1 ? w.W , and
? g2 is the union of g1 and the set of pairs
(w.u, p) for every u ?W ;
this step is denoted by ?1
w,r=? ?2;
2. (activation) there is a w ? W1 with g1(w) =
p and (?1s , ?1t , V1,W1) = (?2s , ?2t , V2,W2),
and g2 is the same as g1 except that g2(w) =
a; this step is denoted by ?1
w=? ?2;
3. (non-adjoining) there is w ? W1 with
g1(w) = p and (?1s , ?1t , V1) = (?2s , ?2t , V2),
W2 = W1 \ {w}, and g2 is g1 restricted to
W2; this step is denoted by ?1
?w=? ?2;
4. (adjoining) there are w ? W1 with g1(w) =
a, and r = (?s, ?t, V,W, ?, P radj) ? A such
that, for w = (ws, wt),
? (?1s (ws), ?1t (wt)) = rc(r),
? ?2s = ?1s [? ?s]ws where ? ?s = ?s[?1s |ws ]?s ,
?2t = ?1t [? ?t]wt where ? ?t = ?t[?1t |wt ]?t ,
? V2 is the smallest set such that (i) for
every (us, ut) ? V1 we have (u?s, u?t) ? V2
where
u?s =
{
us if ws is not a prefix of us,
ws. ?s .u if us = ws.u for some u;
and u?t is obtained in the same way from ut,
wt, and ?t, and (ii) V2 contains w.V ;
? W2 is the smallest set such that (i) for every
(us, ut) ? W1 \ {w} we have (u?s, u?t) ?
W2 where u?s and u?t are obtained in the
same way as for V2, and g2(u?s, u?t) =
g1(us, ut) and (ii) W2 contains w.W and
g2(w.u) = p for every u ?W ;
this step is denoted by ?1
w,r=? ?2.
3w.V = {(ws.vs, wt.vt) | (vs, vt) ? V }
In Fig. 2 we show a derivation of our running
example pSTAG where activated adjoining sites
are indicated by surrounding circles, the other ad-
joining sites are potential.
Ss? ?? St?
substitution of
r1 at (?, ?)
=? P (r1) = 1
Ss
? A? A
?
a ??
St
B
?
B? ?a
substitution of
r4 at (2, 2)
=? P (r4) = .1
Ss
? A
?
A
?
a ??
St
B
?
B
?
?a
activation
at a = (3, 1)
=? P r1adj(a) = .9
Ss
? A
?
A
?
a ??
St
B
?
B
?
?a
adjoining of
r2 at a = (3, 1)
=? P (r2) = .4
Ss
? A
?
A
A
?
?
b,c ??
St
B
B
?
B
?
B
?
?b
c
non-adjoining
at c = (3, 1.1)
=? 1? P r2adj(c) = .4
Ss
? A
?
A
A
?
?
b ??
St
B
B
?
B
?
B
?
?b
non-adjoining
at b = (3, 1)
=? 1? P r2adj(b) = .8
Ss
? A
?
A
A
?
?
??
St
B
B
?
B
?
B
?
?
Figure 2: An example derivation with total proba-
bility 1? .1? .9? .4? .4? .8 = .01152.
The only initial sentential form is ?in =
(Ss, St, {(?, ?)}, ?, ?). A sentential form ? is final
if it has the form (?s, ?t, ?, ?, ?). Let ? ? SF(G).
A derivation (of ?) is a sequence d of the form
?0u1?1 . . . un?n with ?0 = ?in and n ? 0,
?i?1
ui? ?i for every 1 ? i ? n (and ?n = ?). We
12
denote ?n also by last(d), and the set of all deriva-
tions of ? (resp., derivations) by D(?) (resp., D).
We call d ? D successful if last(d) is final.
The tree transformation computed by G is
the relation ?G ? UN (T ) ? UN (T ) with
(?s, ?t) ? ?G iff there is a successful derivation
of (?s, ?t, ?, ?, ?).
Our definition of the probability of a deriva-
tion is based on the following observation.4 Let
d ? D(?) for some ? = (?s, ?t, V,W, g). Then,
for every w ? W , the rule which created w and
the corresponding local position in that rule can
be retrieved from d. Let us denote this rule by
r(d, ?, w) and the local position by l(d, ?, w).
Now let d be the derivation ?0u1?1 . . . un?n.
Then the probability of d is defined by
P (d) =
?
1?i?n
Pd(?i?1
ui? ?i)
where
1. (substitution) Pd(?i?1 w,r=? ?i) = P (r)
2. (activation)
Pd(?i?1 w=? ?i) = P r
?
adj(w?) where r? =
r(d, ?i?1, w) and w? = l(d, ?i?1, w)
3. (non-adjoining)
Pd(?i?1 ?w=? ?i) = 1 ? P r
?
adj(w?) where r?
and w? are defined as in the activation case
4. (adjoining)
Pd(?i?1
w,r=? ?i) = P (r).
In order to describe the generative model of
G, we impose a deterministic strategy sel on the
derivation relation in order to obtain, for every
sentential form, a probability distribution among
the follow-up sentential forms. A deterministic
derivation strategy is a mapping sel : SF(G) ?
(N? ? N?) ? {?} such that for every ? =
(?s, ?t, V,W, g) ? SF(G), we have that sel(?) ?
V ?W if V ?W 6= ?, and sel(?) = ? otherwise.
In other words, sel chooses the next site to operate
on. Then we define ?sel in the same way as ? but
in each of the cases we require that w = sel(?1).
Moreover, for every derivation d ? D, we denote
by next(d) the set of all derivations of the form
du? where last(d) u?sel ?.
The generative model of G comprises all the
generative stories of G. A generative story is a
tree t ? UD; the root of t is labeled by ?in. Let
w ? pos(t) and t(w) = d. Then either w is a
leaf, because we have stopped the generative story
4We note that a different definition occurs in (Nesson et
al., 2005, 2006).
at w, or w has |next(d)| children, each one repre-
sents exactly one possible decision about how to
extend d by a single derivation step (where their
order does not matter). Then, for every generative
story t, we have that
?
w?lv(t)
P (t(w)) = 1 .
We note that (D,next, ?) can be considered as
a discrete Markov chain (cf., e.g. (Baier et al,
2009)) where the initial probability distribution
? : D ? [0, 1] maps d = ?in to 1, and all the
other derivations to 0.
A probabilistic synchronous tree insertion
grammar (pSTIG) G is a pSTAG except that
for every rule r = (?s, ?t, V,W, P radj) or r =
(?s, ?t, V,W, ?, P radj) we have that
? if r ? A, then |lv(?s)| ? 2 and |lv(?t)| ? 2,
? for ? = (?s, ?t) we have that ?s is either the
rightmost leaf of ?s or its leftmost one; then
we call r, resp., L-auxiliary in the source and
R-auxiliary in the source; similarly, we re-
strict ?t; the source-spine of r (target-spine
of r) is the set of prefixes of ?s (resp., of ?t)
? W ? nlvN (?s)?{L,R}?nlvN (?t)?{L,R}
where the new components are the direction-
type of the potential adjoining site, and
? for every (ws, ?s, wt, ?t) ? W , if ws lies on
the source-spine of r and r is L-auxiliary (R-
auxiliary) in the source, then ?s = L (resp.,
?s = R), and corresponding restrictions hold
for the target component.
According to the four possibilities for the foot-
node ? we call r LL-, LR-, RL-, or RR-auxiliary.
The restriction for the probability distribution P of
G is modified such that for every (A,B) ? N?N
and x, y ? {L,R}:
?
r?A, rc(r)=(A,B)
r is xy?auxiliary
P (r) = 1 .
In the derivation relation of the pSTIG G we
will have to make sure that the direction-type of
the chosen adjoining site w matches with the type
of auxiliarity of the auxiliary rule. Again we as-
sume that the data structure SF(G) is enriched
such that for every potential adjoining site w of
? ? SF(G) we know its direction-type dir(w).
We define the derivation relation of the pSTIG
G to be the binary relation ?I ? SF(G)?SF(G)
such that we have ?1 ?I ?2 iff (i) ?1 ? ?2 and
13
(ii) if adjoining takes place atw, then the used aux-
iliary rule must be dir(w)-auxiliary. Since ?I is
a subset of ?, the concepts of derivation, success-
ful derivation, and tree transformation are defined
also for a pSTIG.
In fact, our running example pSTAG in Fig. 1 is
a pSTIG, where r2 and r3 are RL-auxiliary and
every potential adjoining site has direction-type
RL; the derivation shown in Fig. 2 is a pSTIG-
derivation.
4 Bottom-up tree adjoining transducer
Here we introduce the concept of a bottom-up tree
adjoining transducer (BUTAT) which will be used
to formalize a decoder for a pSTIG.
A BUTAT is a finite-state machine which trans-
lates strings into trees. The left-hand side of each
rule is a string over terminal symbols and state-
variable combinations. A variable is either a sub-
stitution variable or an adjoining variable; a substi-
tution variable (resp., adjoining variable) can have
an output tree (resp., output tree with foot node) as
value. Intuitively, each variable value is a transla-
tion of the string that has been reduced to the cor-
responding state. The right-hand side of a rule has
the form q(?) where q is a state and ? is an output
tree (with or without foot-node); ? may contain the
variables from the left-hand side of the rule. Each
rule has a probability p ? [0, 1].
In fact, BUTAT can be viewed as the string-
to-tree version of bottom-up tree transducers (En-
gelfriet, 1975; Gecseg and Steinby, 1984,1997) in
which, in addition to substitution, adjoining is al-
lowed.
Formally, we let X = {x1, x2, . . .} and F =
{f1, f2, . . .} be the sets of substitution variables
and adjoining variables, resp. Each substitu-
tion variable (resp., adjoining variable) has rank
0 (resp., 1). Thus when used in a tree, substitu-
tion variables are leaves, while adjoining variables
have a single child.
A bottom-up tree adjoining transducer (BU-
TAT) is a tuple M = (Q,?,?, Qf , R) where
? Q is a finite set (of states),
? ? is an alphabet (of input symbols), assuming
that Q ? ? = ?,
? ? is an alphabet (of output symbols),
? Qf ? Q (set of final states), and
? R is a finite set of rules of the form
?0 q1(z1) ?1 . . . qk(zk) ?k
p? q(?) (?)
where p ? [0, 1] (probability of (?)), k ? 0,
?0, ?1, . . . , ?k ? ??, q, q1, . . . , qk ? Q,
z1, . . . , zk ? X ? F , and ? ? RHS(k)
where RHS(k) is the set of all trees over
? ? {z1, . . . , zk} ? {?} in which the nullary
? occurs at most once.
The set of intermediate results of M is the set
IR(M) = {? | ? ? U?({?}), |pos{?}(?)| ? 1}
and the set of sentential forms of M is the set
SF(M) = (? ? {q(?) | q ? Q, ? ? IR(M)})?.
The derivation relation induced by M is the bi-
nary relation ? ? SF(M) ? SF(M) such that
for every ?1, ?2 ? SF(M) we define ?1 ? ?2 iff
there are ?, ?? ? SF(M), there is a rule of the form
(?) in R, and there are ?1, . . . , ?k ? IR(M) such
that:
? for every 1 ? i ? k: if zi ? X , then ?i does
not contain ?; if zi ? F , then ?i contains ?
exactly once,
? ?1 = ? ?0 q1(?1) ?1 . . . qk(?k) ?k ??, and
? ?2 = ? q(?(?)) ??
where ? is a function that replaces variables
in a right-hand side with their values (subtrees)
from the left-hand side of the rule. Formally,
? : RHS(k) ? IR(M) is defined as follows:
(i) for every ? = ?(?1, . . . , ?n) ? RHS(k), ? ?
?, we have ?(?) = ?(?(?1), . . . , ?(?n)),
(ii) (substitution) for every zi ? X , we have
?(zi) = ?i,
(iii) (adjoining) for every zi ? F and ? ?
RHS(k), we have ?(zi(?)) = ?i[?(?)]v
where v is the uniquely determined position
of ? in ?i, and
(iv) ?(?) = ?.
Clearly, the probablity of a rule carries over to
derivation steps that employ this rule. Since, as
usual, a derivation d is a sequence of derivation
steps, we let the probability of d be the product of
the probabilities of its steps.
The string-to-tree transformation computed by
M is the set ?M of all tuples (?, ?) ? ???U? such
that there is a derivation of the form ? ?? q(?) for
some q ? Qf .
5 Decoder for pSTIG
Now we construct the decoder dec(G) for a pSTIG
G that transforms source strings directly into tar-
get trees and simultaneously computes the proba-
bility of the corresponding derivation of G. This
decoder is formalized as a BUTAT.
Since dec(G) is a string-to-tree transducer, we
14
have to transform the source tree ?s of a rule r
into a left-hand side ? of a dec(G)-rule. This is
done similarly to (DeNeefe and Knight, 2009) by
traversing ?s via recursive descent using a map-
ping ? (see an example after Theorem 1); this
creates appropriate state-variable combinations for
all substitution sites and potential adjoining sites
of r. In particular, the source component of the
direction-type of a potential adjoining site deter-
mines the position of the corresponding combina-
tion in ?. If there are several potential adjoining
sites with the same source component, then we
create a ? for every permutation of these sites. The
right-hand side of a dec(G)-rule is obtained by
traversing the target tree ?t via recursive descent
using a mapping ?? and, whenever a nonterminal
with a potential adjoining site w is met, a new po-
sition labeled by fw is inserted.5 If there is more
than one potential adjoining site, then the set of
all those sites is ordered as in the left-hand side ?
from top to bottom.
Apart from these main rules we will employ
rules which implement the decision of whether or
not to turn a potential adjoining site w into an ac-
tivated adjoining site. Rules for the first purpose
just pass the already computed output tree through
from left to right, whereas rules for the second pur-
pose create for an empty left-hand side the output
tree ?.
We will use the state behavior of dec(G) in or-
der to check that (i) the nonterminals of a substi-
tution or potential adjoining site match the root-
category of the used rule, (ii) the direction-type
of an adjoining site matches the auxiliarity of the
chosen auxiliary rule, and (iii) the decisions of
whether or not to adjoin for each rule r of G are
kept separate.
Whereas each pair (?s, ?t) in the translation of
G is computed in a top-down way, starting at the
initial sentential form and substituting and adjoin-
ing to the present sentential form, dec(G) builds
?t in a bottom-up way. This change of direction is
legitimate, because adjoining is associative (Vijay-
Shanker and Weir, 1994), i.e., it leads to the same
result whether we first adjoin r2 to r1, and then
align r3 to the resulting tree, or first adjoin r3 to
r2, and then adjoin the resulting tree to r1.
In Fig. 3 we show some rules of the decoder
of our running example pSTIG and in Fig. 4 the
5We will allow variables to have structured indices that
are not elements of N. However, by applying a bijective re-
naming, we can always obtain rules of the form (?).
derivation of this decoder which correponds to the
derivation in Fig. 2.
Theorem 1. Let G be a pSTIG over N and T .
Then there is a BUTAT dec(G) such that for ev-
ery (?s, ?t) ? UN (T ) ? UN (T ) and p ? [0, 1] the
following two statements are equivalent:
1. there is a successful derivation of
(?s, ?t, ?, ?, ?) by G with probability p,
2. there is a derivation from yield(?s) to
[Ss, St](?t) by dec(G) with probability p.
PROOF. Let G = (N,T, [Ss, St],S,A, P ) be a
pSTIG. We will construct the BUTAT dec(G) =
(Q,T,N ?T, {[Ss, St]}, R) as follows (where the
mappings ? and ?? will be defined below):
? Q = [N ?N ] ? [N ?{L,R}?N ?{L,R}]
?{[r, w] | r ? A, w is an adjoining site of r},
? R is the smallest set R? of rules such
that for every r ? S ? A of the form
(?s, ?t, V,W, P radj) or (?s, ?t, V,W, ?, P radj):
? for every ? ? ?(?), if r ? S, then the
main rule
? P (r)? [?s(?), ?t(?)]
(
??(?)
)
is in R?, and if r ? A and r is ?s?t-
auxiliary, then the main rule
? P (r)? [?s(?), ?s, ?t(?), ?t]
(
??(?)
)
is in R?, and
? for every w = (ws, ?s, wt, ?t) ? W the
rules
qw
(
fw
) P radj(w)?? [r, w]
(
fw(?)
)
with qw = [?(ws), ?s, ?t(wt), ?t] for ac-
tivation at w, and the rule
?
1?P radj(w)?? [r, w](?)
for non-adjoining at w are in R?.
We define the mapping
? : pos(?s) ? P((T ?Q(X ? F ))?)
with Q(X ? F ) = {q(z) | q ? Q, z ? X ? F}
inductively on its argument as follows. Let w ?
pos(?s) and let w have n children.
(a) Let ?s(w) ? T . Then ?(w) = {?s(w)}.
15
(b) (substitution site) Let ?s(w) ? N and let
w? ? pos(?t) such that (w,w?) ? V . Then
?(w) = {[?s(w), ?t(w?)]
(
x(w,w?)
)
}.
(c) (adjoining site) Let ?s(w) ? N and let there
be an adjoining site in W with w as first
component. Then, we define ?(w) to be the
smallest set such that for every permutation
(u1, . . . , ul) (resp., (v1, . . . , vm)) of all the L-
adjoining (resp., R-adjoining) sites inW with
w as first component, the set6
J ? ?(w.1) ? . . . ? ?(w.n) ?K
is a subset of ?(w), where J = {u?1 . . . u?l}
and K = {v?m . . . v?1} and
u?i = [r, ui]
(
fui
)
and v?j = [r, vj ]
(
fvj
)
for 1 ? i ? l and 1 ? j ? m.
(d) Let ?s(w) ? N , w 6= ?, and let w be neither
the first component of a substitution site in V
nor the first component of an adjoining site in
W . Then
?(w) = ?(w.1) ? . . . ? ?(w.n) .
(e) Let w = ?. Then we define ?(w) = {?}.
For every ? ? ?(?), we define the mapping
?? : pos(?t) ? UN?F?X(T ? {?})
inductively on its argument as follows. Let
w ? pos(?t) and let w have n children.
(a) Let ?t(w) ? T . Then ??(w) = ?t(w).
(b) (substitution site) Let ?t(w) ? N and let
w? ? pos(?s) such that (w?, w) ? V . Then
??(w) = x(w?,w).
(c) (adjoining site) Let ?t(w) ? N and let there
be an adjoining site in W with w as third
component. Then let {u1, . . . , ul} ? W be
the set of all potential adjoining sites with w
as third component, and we define
??(w) = fu1(. . . ful(?) . . .)
where ? = ?t(w)(??(w.1), . . . , ??(w.n))
and the ui?s occur in ??(w) (from the root
towards the leaves) in exactly the same order
as they occur in ? (from left to right).
(d) Let ?t(w) ? N , w 6= ?, and let w be neither
the second component of a substitution site
in V nor the third component of an adjoining
site in W . Then
??(w) = ?t(w)(??(w.1), . . . , ??(w.n)).
6using the usual concatenation ? of formal languages
(e) Let w = ?. Then ??(w) = ?.
With dec(G) constructed as shown, for each
derivation of G there is a corresponding deriva-
tion of dec(G), with the same probability, and vice
versa. The derivations proceed in opposite direc-
tions. Each sentential form in one has an equiv-
alent sentential form in the other, and each step
of the derivations correspond. There is no space
to present the full proof, but let us give a slightly
more precise idea about the formal relationship be-
tween the derivations of G and dec(G).
In the usual way we can associate a deriva-
tion tree dt with every successful derivation d of
G. Assume that last(d) = (?s, ?t, ?, ?, ?), and
let Es and Et be the embedded tree transducers
(Shieber, 2006) associated with, respectively, the
source component and the target component of
G. Then it was shown in (Shieber, 2006) that
?Es(dt) = ?s and ?Et(dt) = ?t where ?E de-
notes the tree-to-tree transduction computed by an
embedded tree transducer E. Roughly speaking,
Es and Et reproduce the derivations of, respec-
tively, the source component and the target com-
ponent of G that are prescribed by dt. Thus, for
? = (??s, ??t, V,W, g), if ?in ??G ? and ? is a prefix
of d, then there is exactly one subtree dt[(w,w?)]
of dt associated with every (w,w?) ? V ? W ,
which prescribes how to continue at (w,w?) with
the reproduction of d. Having this in mind, we ob-
tain the sentential form of the dec(G)-derivation
which corresponds to ? by applying a modifica-
tion of ? to ? where the modification amounts to
replacing x(w,w?) and f(w,w?) by ?Et(dt[(w,w?)]);
note that ?Et(dt[(w,w?)]) might contain ?. 
As illustration of the construction in Theorem 1
let us apply the mappings ? and ?? to rule r2 of
Fig. 1, i.e., to r2 = (?s, ?t, ?, {b, c}, ?, P r2adj)
with ?s = A(A, ?), ?t = B(B(?), B),
b = (?,R, ?,L), c = (?,R, 1,L), and ? = (1, 2).
Let us calculate ?(?) on ?s. Due to (c),
?(?) = J ? ?(1) ? ?(2) ?K.
Since there are no L-adjoinings at ?, we have that
J = {?}. Since there are the R-adjoinings b and c
at ?, we have the two permutations (b, c) and (c, b).
(v1, v2) = (b, c): K = {[r2, c](fc)[r2, b](fb)}
(v1, v2) = (c, b): K = {[r2, b](fb)[r2, c](fc)}
Due to (e) and (a), we have that ?(1) = {?} and
?(2) = {?}, resp. Thus, ?(?) is the set:
{? [r2, c](fc) [r2, b](fb), ? [r2, b](fb) [r2, c](fc)}.
16
r1
(r1, a)
r2
(r2,?b) (r2,?c)
r4
?
[A,B]
x(2,2)
?
[r1, a]
fa
1??
[Ss, St]
St
f
B
?
x ?a (2,2)
[A,R, B,L]
fa
.9??
[r1, a]
f
?
a
?
[r2, b]
f b
[r2, c]
fc
.4??
[A,R, B,L]
f
B
f
B
?
?
b
c
? .8??
[r2, b]
?
? .4??
[r2, c]
?
? .1??
[A,B]
B
?
Figure 3: Some rules of the running example de-
coder.
Now let ? = ? [r2, b](fb) [r2, c](fc). Let us cal-
culate ??(?) on ?t.
??(?)
(c)= fb(B(??(1), ??(2)))
(c)= fb(B(fc(B(??(1.1))), ??(2)))
(a)= fb(B(fc(B(?)), ??(2)))
(e)= fb(B(fc(B(?)), ?))
Hence we obtain the rule
? [r2, b](fb) [r2, c](fc) ?
[A,R, B,L](fb(B(fc(B(?)), ?)))
which is also shown in Fig. 3.
? ? ? ?
? ? ? ?
[r2, b]
?
? ? ? ?
[r2, b]
?
[r2, c]
?
? ? ?
? ? ?
?
[A,B]
B
?
?
[A,R, B,L]
B
B
?
?
=?
=?
=?
=?
=?
[Ss, St]
St
B
B
?
B
?
B
?
?
prob. .8
prob. .4
prob. .4
prob. .9
prob. .1
=? prob. .1
[r1, a]
B
B
?
?
(r2,?b)
(r2,?c)
(r2, bc)
(r1, a)
r4
r1
[r1, a]
B
B
?
?
Figure 4: Derivation of the decoder corresponding
to the derivation in Fig. 2.
17
References
A. Abeille, Y. Schabes, A.K. Joshi. Using lexicalized
TAGs for machine translation. In Proceedings of
the 13th International Conference on Computational
Linguistics, volume 3, pp. 1?6, Helsinki, Finland,
1990.
C. Baier, M. Gro??er, F. Ciesinski. Model checking
linear-time properties of probabilistic systems. In
Handbook of Weighted Automata, Chapter 13, pp.
519?570, Springer, 2009.
S. DeNeefe. Tree adjoining machine translation. Ph.D.
thesis proposal, Univ. of Southern California, 2009.
S. DeNeefe, K. Knight. Synchronous tree adjoining
machine translation. In Proc. of Conf. Empirical
Methods in NLP, pp. 727?736, 2009.
J. Engelfriet. Bottom-up and top-down tree transfor-
mations ? a comparison. Math. Systems Theory,
9(3):198?231, 1975.
J. Engelfriet. Tree transducers and syntax-directed se-
mantics. In CAAP 1982: Lille, France, 1982.
A. Fujiyoshi, T. Kasai. Spinal-formed context-free tree
grammars. Theory of Computing Systems, 33:59?
83, 2000.
Z. Fu?lo?p, H. Vogler. Weighted tree automata and tree
transducers. In Handbook of Weighted Automata,
Chapter 9, pp. 313?403, Spinger, 2009.
F. Ge?cseg, M. Steinby. Tree Automata. Akade?miai
Kiado?, Budapest, 1984.
F. Ge?cseg, M. Steinby. Tree languages. In Handbook
of Formal Languages, volume 3, chapter 1, pages
1?68. Springer-Verlag, 1997.
J. Graehl, K. Knight, J. May. Training tree transducers.
Computational Linguistics, 34(3):391?427, 2008
A.K. Joshi, L.S. Levy, M. Takahashi. Tree adjunct
grammars. Journal of Computer and System Sci-
ences, 10(1):136?163, 1975.
A.K. Joshi, Y. Schabes. Tree-adjoining grammars. In
Handbook of Formal Languages. Chapter 2, pp. 69?
123, Springer-Verlag, 1997.
K. Knight, J. Graehl. An overview of probabilis-
tic tree transducers for natural language processing.
In Computational Linguistics and Intelligent Text
Processing, CICLing 2005, LNCS 3406, pp. 1?24,
Springer, 2005.
K. Knight, J. May. Applications of Weighted Au-
tomata in Natural Language Processing. In Hand-
book of Weighted Automata, Chapter 14, pp. 571?
596, Springer, 2009.
P.M. Lewis, R.E. Stearns. Syntax-directed transduc-
tions. Journal of the ACM, 15:465?488, 1968.
A. Maletti. Compositions of tree series transforma-
tions. Theoret. Comput. Sci., 366:248?271, 2006.
A. Maletti. The Power of Tree Series Transducers.
Ph.D. thesis, TU Dresden, Germany, 2006.
A. Maletti. Compositions of extended top-down
tree transducers. Information and Computation,
206:1187?1196, 2008.
A. Maletti. Why synchronous tree substitution gram-
mars? in Proc. 11th Conf. North American Chap-
ter of the Association of Computational Linguistics.
2010.
D.F. Martin and S.A. Vere. On syntax-directed trans-
ductions and tree transducers. In Ann. ACM Sympo-
sium on Theory of Computing, pp. 129?135, 1970.
R. Nesson, S.M. Shieber, and A. Rush. Induction
of probabilistic synchronous tree-insertion gram-
mars. Technical Report TR-20-05, Computer Sci-
ence Group, Harvard Univeristy, Cambridge, Mas-
sachusetts, 2005.
R. Nesson, S.M. Shieber, and A. Rush. Induction of
probabilistic synchronous tree-inserting grammars
for machine translation. In Proceedings of the 7th
Conference of the Association for Machine Transla-
tion in the Americas (AMTA 2006), 2006.
Y. Schabes, R.C. Waters. Tree insertion grammars:
a cubic-time, parsable formalism that lexicalizes
context-free grammar without changing the trees
produced. Computational Linguistics, 21:479?513,
1994.
P.P. Schreiber. Tree-transducers and syntax-connected
transductions. In Automata Theory and Formal
Languages, Lecture Notes in Computer Science 33,
pp. 202?208, Springer, 1975.
S.M. Shieber. Synchronous grammars and tree trans-
ducers. In Proc. 7th Workshop on Tree Adjoin-
ing Grammars and Related Formalisms, pp. 88?95,
2004.
S.M. Shieber. Unifying synchronous tree-adjoining
grammars and tree transducers via bimorphisms. In
Proc. 11th Conf. European Chapter of ACL, EACL
06, pp. 377?384, 2006.
S.M. Shieber, Y. Schabes. Synchronous tree-adjoining
grammars. In Proceedings of the 13th Interna-
tional Conference on Computational Linguistics,
volume 3, pp. 253?258, Helsinki, Finland, 1990.
K. Vijay-Shanker, D.J. Weir. The equivalence of four
extensions of context-free grammars. Mathematical
Systems Theory, 27:511?546, 1994.
K. Yamada and K. Knight. A syntax-based statistical
translation model. In Proc. of 39th Annual Meeting
of the Assoc. Computational Linguistics, pp. 523?
530, 2001.
18
Proceedings of the 2010 Workshop on Applications of Tree Automata in Natural Language Processing, ACL 2010, pages 46?54,
Uppsala, Sweden, 16 July 2010. c?2010 Association for Computational Linguistics
n-Best Parsing Revisited?
Matthias Bu?chse and Daniel Geisler and Torsten Stu?ber and Heiko Vogler
Faculty of Computer Science
Technische Universita?t Dresden
01062 Dresden
{buechse,geisler,stueber,vogler}@tcs.inf.tu-dresden.de
Abstract
We derive and implement an algorithm
similar to (Huang and Chiang, 2005) for
finding the n best derivations in a weighted
hypergraph. We prove the correctness and
termination of the algorithm and we show
experimental results concerning its run-
time. Our work is different from the afore-
mentioned one in the following respects:
we consider labeled hypergraphs, allowing
for tree-based language models (Maletti
and Satta, 2009); we specifically handle
the case of cyclic hypergraphs; we admit
structured weight domains, allowing for
multiple features to be processed; we use
the paradigm of functional programming
together with lazy evaluation, achieving
concise algorithmic descriptions.
1 Introduction
In statistical natural language processing, proba-
bilistic models play an important role which can
be used to assign to some input sentence a set of
analyses, each carrying a probability. For instance,
an analysis can be a parse tree or a possible trans-
lation. Due to the ambiguity of natural language,
the number of analyses for one input sentence can
be very large. Some models even assign an infinite
number of analyses to an input sentence.
In many cases however, the set of analyses can
in fact be represented in a finite and compact way.
While such a representation is space-efficient, it
may be incompatible with subsequent operations.
In these cases a finite subset is used as an approx-
imation, consisting of n best analyses, i. e. n anal-
yses with highest probability. For example, this
approach has the following two applications.
(1) Reranking: when log-linear models (Och
and Ney, 2002) are employed, some features may
? This research was financially supported by DFG VO
1101/5-1.
not permit an efficient evaluation during the com-
putation of the analyses. These features are com-
puted using individual analyses from said approx-
imation, leading to a reranking amongst them.
(2) Spurious ambiguity: many models produce
analyses which may be too fine-grained for further
processing (Li et al, 2009). As an example, con-
sider context-free grammars, where several left-
most derivations may exist for the same terminal
string. The weight of the terminal string is ob-
tained by summing over these derivations. The
n best leftmost derivations may be used to approx-
imate this sum.
In this paper, we consider the case where the
finite, compact representation has the form of a
weighted hypergraph (with labeled hyperedges)
and the analyses are derivations of the hypergraph.
This covers many parsing applications (Klein and
Manning, 2001), including weighted deductive
systems (Goodman, 1999; Nederhof, 2003), and
also applications in machine translation (May and
Knight, 2006).
In the nomenclature of (Huang and Chiang,
2005), which we adopt here, a derivation of a hy-
pergraph is a tree which is obtained in the follow-
ing way. Starting from some node, an ingoing hy-
peredge is picked and recorded as the label of the
root of the tree. Then, for the subtrees, one con-
tinues with the source nodes of said hyperedge in
the same way. In other words, a derivation can be
understood as an unfolding of the hypergraph.
The n-best-derivations problem then amounts
to finding n derivations which are best with re-
spect to the weights induced by the weighted hy-
pergraph.1 Among others, weighted hypergraphs
with labeled hyperedges subsume the following
two concepts.
(I) probabilistic context-free grammars (pcfgs).
1Note that this problem is different from the n-best-
hyperpaths problem described by Nielsen et al (2005), as
already argued in (Huang and Chiang, 2005, Section 2).
46
In this case, nodes correspond to nonterminals,
hyperedges are labeled with productions, and the
derivations are exactly the abstract syntax trees
(ASTs) of the grammar (which are closely related
the parse trees). Note that, unless the pcfg is un-
ambiguous, a given word may have several cor-
responding ASTs, and its weight is obtained by
summing over the weights of the ASTs. Hence,
the n best derivations need not coincide with the
n best words (cf. application (2) above).
(II) weighted tree automata (wta) (Alexandrakis
and Bozapalidis, 1987; Berstel and Reutenauer,
1982; ?Esik and Kuich, 2003; Fu?lo?p and Vogler,
2009). These automata serve both as a tree-based
language model and as a data structure for the
parse forests obtained from that language model
by applying the Bar-Hillel construction (Maletti
and Satta, 2009). It is well known that context-free
grammars and tree automata are weakly equiv-
alent (Thatcher, 1967; ?Esik and Kuich, 2003).
However, unlike the former formalism, the latter
one has the ability to model non-local dependen-
cies in parse trees.
In the case of wta, nodes correspond to states,
hyperedges are labeled with input symbols, and
the derivations are exactly the runs of the automa-
ton. Since, due to ambiguity, a given tree may
have several accepting runs, the n best derivations
need not coincide with the n best trees. As for
the pcfgs, this is an example of spurious ambigu-
ity, which can be tackled as indicated by appli-
cation (2) above. Alternatively, one can attempt
to find an equivalent deterministic wta (May and
Knight, 2006; Bu?chse et al, 2009).
Next, we briefly discuss four known algorithms
which solve the n-best-derivations problem or
subproblems thereof.
? The Viterbi algorithm solves the 1-best-
derivation problem for acyclic hypergraphs. It is
based on a topological sort of the hypergraph.
? Knuth (1977) generalizes Dijkstra?s algorithm
(for finding the single-source shortest paths in a
graph) to hypergraphs, thus solving the case n = 1
even if the hypergraph contains cycles. Knuth as-
sumes the weights to be real numbers, and he re-
quires weight functions to be monotone and supe-
rior in order to guarantee that a best derivation ex-
ists. (The superiority property corresponds to Di-
jkstra?s requirement that edge weights?or, more
generally, cycle weights?are nonnegative.)
? Huang and Chiang (2005) show that the n-
best-derivations problem can be solved efficiently
by first solving the 1-best-derivation problem and
then extending that solution in a lazy manner.
Huang and Chiang assume weighted unlabeled hy-
pergraphs with weights computed in the reals, and
they require the weight functions to be monotone.
Moreover they assume that the 1-best-
derivation problem be solved using the Viterbi
algorithm, which implies that the hypergraph must
be acyclic. However they conjecture that their
second phase also works for cyclic hypergraphs.
? Pauls and Klein (2009) propose a variation
of the algorithm of Huang and Chiang (2005) in
which the 1-best-derivation problem is computed
via an A?-based exploration of the 1-best charts.
In this paper, we also present an algorithm
for solving the n-best-derivations problem. Ulti-
mately it uses the same algorithmic ideas as the
one of Huang and Chiang (2005); however, it is
different in the following sense:
1. we consider labeled hypergraphs, allowing
for wta to be used in parsing;
2. we specifically handle the case of cyclic
hypergraphs, thus supporting the conjecture of
Huang and Chiang; for this we impose on the
weight functions the same requirements as Knuth
and use his algorithm;
3. by using the concept of linear pre-orders (and
not only linear orders on the set of reals) our ap-
proach can handle structured weights such as vec-
tors over frequencies, probabilities, and reals;
4. we present our algorithm in the framework
of functional programming (and not in that of im-
perative programming); this framework allows to
decribe algorithms in a more abstract and concise,
yet natural way;
5. due to the lazy evaluation paradigm often
found in functional programming, we obtain the
laziness on which the algorithm of Huang and Chi-
ang (2005) is based for free;
6. exploiting the abstract level of description
(see point 4) we are able to prove the correctness
and termination of our algorithm.
At the end of this paper, we will discuss experi-
ments which have been performed with an imple-
mentation of our algorithm in the functional pro-
gramming language HASKELL.
2 The n-best-derivations problem
In this section, we state the n-best-derivations
problem formally, and we give a comprehensive
47
example. First, we introduce some basic notions.
Trees and hypergraphs The definition of
ranked trees commonly used in formal tree lan-
guage theory will serve us as the basis for defining
derivations.
A ranked alphabet is a finite set ? (of symbols)
where every symbol carries a rank (a nonnegative
integer). By ?(k) we denote the set of those sym-
bols having rank k. The set of trees over ?, de-
noted by T? , is the smallest set T such that for
every k ? N, ? ? ?(k), and ?1, . . . , ?k ? T ,
also ?(?1, . . . , ?k) ? T ;2 for ? ? ?(0) we ab-
breviate ?() by ?. For every k ? N, ? ?
?(k) and subsets T1, . . . , Tk ? T? we define
the top-concatenation (with ?) ?(T1, . . . , Tk) =
{?(?1, . . . , ?k) | ?1 ? T1, . . . , ?k ? Tk}.
A ?-hypergraph is a pair H = (V,E) where
V is a finite set (of vertices or nodes) and E ?
V ????V is a finite set (of hyperedges) such that
for every (v1 . . . vk, ?, v) ? E we have that ? ?
?(k).3 We interpret E as a ranked alphabet where
the rank of each edge is carried over from its label
in ?. The family (Hv | v ? V ) of derivations of H
is the smallest family (Pv | v ? V ) of subsets
of TE such that e(Pv1 , . . . , Pvk) ? Pv for every
e = (v1 . . . vk, ?, v) ? E.
A ?-hypergraph (V,E) is cyclic if there
are hyperedges (v11 . . . v1k1 , ?1, v
1), . . . ,
(vl1 . . . vlkl , ?l, v
l) ? E such that vj?1 occurs
in vj1 . . . v
j
kj for every j ? {2, . . . , l} and v
l occurs
in v11 . . . v1k1 . It is called acyclic if it is not cyclic.
Example 1 Consider the ranked alphabet ? =
?(0)??(1)??(2) with ?(0) = {?, ?}, ?(1) = {?},
and ?(2) = {?}, and the ?-hypergraph H =
(V,E) where
? V = {0, 1} and
? E = {(?, ?, 1), (?, ?, 1), (1, ?, 1), (11, ?, 0),
(1, ?, 0)}.
A graphical representation of this hypergraph is
shown in Fig. 1. Note that this hypergraph is cyclic
because of the edge (1, ?, 1).
We indicate the derivations of H , assuming that
e1, . . . , e5 are the edges in E in the order given
above:
2The term ?(?1, . . . , ?k) is usually understood as a string
composed of the symbol ?, an opening parenthesis, the
string ?1, a comma, and so on.
3The hypergraphs defined here are essentially nondeter-
ministic tree automata, where V is the set of states and E is
the set of transitions.
? ?
0 1 ?
? ?
e5
e3
e4
e1
e2
Figure 1: Hypergraph of Example 1.
? H1 = {e1, e2, e3(e1), e3(e2), e3(e3(e1)), . . . }
and
? H0 = e4(H1,H1) ? e5(H1) where, e. g.,
e4(H1,H1) is the top-concatenation of H1,
H1 with e4, and thus
e4(H1,H1) = {e4(e1, e1), e4(e1, e2),
e4(e1, e3(e1)), e4(e3(e1), e1), . . . } .
Next we give an example of ambiguity in hyper-
graphs with labeled hyperedges. Suppose that E
contains an additional hyperedge e6 = (0, ?, 0).
Then H0 would contain the derivations e6(e5(e1))
and e5(e3(e1)), which describe the same ?-tree,
viz. ?(?(?)) (obtained by the node-wise projec-
tion to the second component). 
In the sequel, let H = (V,E) be a ?-hypergraph.
Ordering Usually an ordering is induced on the
set of derivations by means of probabilities or,
more generally, weights. In the following, we will
abstract from the weights by using a binary rela-
tion - directly on derivations, where we will in-
terpret the fact ?1 - ?2 as ??1 is better than or
equal to ?2?.
Example 2 (Ex. 1 contd.) First we show how an
ordering is induced on derivations by means of
weights. To this end, we associate an operation
over the set R of reals with every hyperedge (re-
specting its arity) by means of a mapping ?:
?(e1)() = 4 ?(e2)() = 3
?(e3)(x1) = x1 + 1 ?(e4)(x1, x2) = x1 + x2
?(e5)(x1) = x1 + 0.5
The weight h(?) of a tree ? ? TE is obtained by
interpreting the symbols at each node using ?, e. g.
h(e3(e2)) = ?(e3)(?(e2)()) = ?(e2)() + 1 = 4.
Then the natural order ? on R induces the bi-
nary relation - over TE as follows: for every
?1, ?2 ? TE we let ?1 - ?2 iff h(?1) ? h(?2),
meaning that trees with smaller weights are con-
sidered better. (This is, e. g., the case when calcu-
lating probabilites in the image of ? log x.) Note
48
that we could just as well have defined - with the
inverted order.
Since addition is commutative, we obtain
for every ?1, ?2 ? TE that h(e4(?1, ?2)) =
h(e4(?2, ?1)) and thus e4(?1, ?2) - e4(?2, ?1) and
vice versa. Thus, for two different trees (e4(?1, ?2)
and e4(?2, ?1)) having the same weight, - should
not prefer any of them. That is, - need not be
antisymmetric.
As another example, the mapping ? could as-
sign to each symbol an operation over real-valued
vectors, where each component represents one
feature of a log-linear model such as frequencies,
probabilities, reals, etc. Then the ordering could
be defined by means of a linear combination of the
feature weights. 
We use the concept of a linear pre-order to cap-
ture the orderings which are obtained this way.
Let S be a set. A pre-order (on S) is a binary
relation - ? S ? S such that (i) s - s for ev-
ery s ? S (reflexivity) and (ii) s1 - s2 and s2 - s3
implies s1 - s3 for every s1, s2, s3 ? S (transi-
tivity). A pre-order - is called linear if s1 - s2
or s2 - s1 for every s1, s2 ? S. For instance, the
binary relation - on TE as defined in Ex. 2 is a
linear pre-order.
We will restrict our considerations to a class
of linear pre-orders which admit efficient algo-
rithms. For this, we will always assume a lin-
ear pre-order - with the following two properties
(cf. Knuth (1977)).4
SP (subtree property) For every e(?1, . . . , ?k) ?
TE and i ? {1, . . . , k} we have ?i -
e(?1, . . . , ?k).5
CP (compatibility) For every pair e(?1, . . . , ?k),
e(??1, . . . , ??k) ? TE with ?1 - ??1, . . . ,
?k - ??k we have that e(?1, . . . , ?k) -
e(??1, . . . , ??k).
It is easy to verify that the linear pre-order - of
Ex. 2 has the aforementioned properties.
In the sequel, let- be a linear pre-order
on TE fulfilling SP and CP.
4Originally, these properties were called ?superiority? and
?monotonicity? because they were viewed as properties of
the weight functions. We use the terms ?subtree property?
and ?compatibility? respectively, because we view them as
properties of the linear pre-order.
5This strong property is used here for simplicity. It suf-
fices to require that for every v ? V and pair ?, ?? ? Hv we
have ? - ?? if ? is a subtree of ??.
Before we state the n-best-derivations problem
formally, we define the operation minn, which
maps every subset T of TE to the set of all se-
quences of n best elements of T . To this end, let
T ? TE and n ? |T |. We define minn(T ) to be
the set of all sequences (?1, . . . , ?n) ? T n of pair-
wise distinct elements such that ?1 - . . . - ?n and
for every ? ? T \ {?1, . . . , ?k} we have ?n - ?.
For every n > |T | we set minn(T ) = min|T |(T ).
In addition, we set min?n(T ) =
?n
i=0 mini(T ).
n-best-derivations problem The n-best-
derivations problem amounts to the following.
Given a ?-hypergraph H = (V,E), a vertex v ?
V , and a linear pre-order - on TE fulfilling
SP and CP,
compute an element of minn(Hv).
3 Functional Programming
We will describe our main algorithm as a func-
tional program. In essence, such a program is a
system of (recursive) equations that defines sev-
eral functions (as shown in Fig. 2). As a conse-
quence the main computational paradigm for eval-
uating the application (f a) of a function f to an
argument a is to choose an appropriate defining
equation f x = r and then evaluate (f a) to r?
which is obtained from r by substituting every oc-
currence of x by a.
We assume a lazy (and in particular, call-by-
need) evaluation strategy, as in the functional pro-
gramming language HASKELL. Roughly speak-
ing, this amounts to evaluating the arguments of
a function only as needed to evaluate the its body
(i. e. for branching). If an argument occurs multi-
ple times in the body, it is evaluated only once.
We use HASKELL notation and functions for
dealing with lists, i. e. we denote the empty list by
[] and list construction by x:xs (where an ele-
ment x is prepended to a list xs), and we use the
functions head (line 01), tail (line 02), and take
(lines 03 and 04), which return the first element in
a list, a list without its first element, and a prefix
of a list, respectively.
In fact, the functions shown in Fig. 2 will be
used in our main algorithm (cf. Fig. 4). Thus,
we explain the functions merge (lines 05?07) and
e(l1, . . . ,lk) (lines 08?10) a bit more in detail.
The merge function takes a set L of pairwise
disjoint lists of derivations, each one in ascend-
ing order with respect to -, and merges them into
49
-- standard Haskell functions: list deconstructors, take operation
01 head (x:xs) = x
02 tail (x:xs) = xs
03 take n xs = [] if n == 0 or xs == []
04 take n xs = (head xs):take (n-1) (tail xs)
-- merge operation (lists in L should be disjoint)
05 merge L = [] if L \ {[]} = ?
06 merge L = m:merge ({tail l | l ? L, l != [], head l == m} ?
{l | l ? L, l != [], head l != m})
07 where m = min{head l | l ? L, l != []}
-- top concatenation
08 e(l1, . . . ,lk) = [] if li == [] for some i ? {1, . . . , k}
09 e(l1, . . . ,lk) = e(head l1, . . . , head lk):merge {e(li1, . . . ,lik) | i ? {1, . . . , k}}
10 where lij =
?
?
?
?
?
lj if j < i
tail lj if j = i
[head lj] if j > i
Figure 2: Some useful functions specified in a functional programming style.
one list with the same property (as known from the
merge sort algorithm).
Note that the minimum used in line 07 is based
on the linear pre-order -. For this reason, it
need not be uniquely determined. However, in an
implementation this function is deterministic, de-
pending on the the data structures.
The function e(l1, . . . ,lk) implements the top-
concatenation with e on lists of derivations. It is
defined for every e = (v1 . . . vk, ?, v) ? E and
takes lists l1, . . . , lk of derivations, each in as-
cending order as for merge. The resulting list is
also in ascending order.
4 Algorithm
In this section, we develop our algorithm for solv-
ing the n-best-derivations problem. We begin by
motivating our general approach, which amounts
to solving the 1-best-derivation problem first and
then extending that solution to a solution of the n-
best-derivations problem.
It can be shown that for every m ? n, the
set minn(Hv) is equal to the set of all prefixes of
length n of elements of minm(Hv). According
to this observation, we will develop a function p
mapping every v ? V to a (possibly infinite) list
such that the prefix of length n is in minn(Hv)
for every n. Then, by virtue of lazy evaluation, a
solution to the n-best-derivations problem can be
obtained by evaluating the term
take n (p v)
where take is specified in lines 03?04 of Fig. 2.
Thus, what is left to be done is to specify p appro-
priately.
4.1 A provisional specification of p
Consider the following provisional specification
of p:
p v = merge {e(p v1, . . . ,p vk) |
e = (v1 . . . vk, ?, v) ? E} (?)
where the functions merge and e(l1, . . . ,lk) are
specified in lines 05?07 and lines 08?10 of Fig. 2,
respectively. This specification models exactly the
trivial equation
Hv =
?
e=(v1...vk ,?,v)?E
e(Hv1 , . . . ,Hvk)
for every v ? V , where the union and the top-
concatenation have been implemented for lists via
the functions merge and e(l1, . . . ,lk).
This specification is adequate if H is acyclic.
For cyclic hypergraphs however, it can not even
solve the 1-best-derivation problem. To illustrate
this, we consider the hypergraph of Ex. 2 and cal-
50
culate6
take 1 (p 1)
= (head (p 1)):take 0 (tail (p 1)) (04)
= head (p 1) (03)
= head (merge {e1(), e2(), e3(p 1)}) (?)
= min{head e1(), head e2(), head e3(p 1)}
(01, 06, 07)
= min{head e1(), head e2(), e3(head (p 1))}.
(09)
Note that the infinite regress occurs because the
computation of the head element head (p 1) de-
pends on itself. This leads us to the idea of
?pulling? this head element (which is the solu-
tion to the 1-best-derivation problem) ?out? of the
merge in (?). Applying this idea to our particular
example, we reach the following equation for p 1:
p 1 = e2: merge {e1(), e3(p 1)}
because e2 is the best derivation in H1. Then, in
order to evaluate merge we have to compute
min{head e1(), head e3(p 1)}
= min{e1, e3(head (p 1))}
= min{e1, e3(e2)}.
Since h(e1) = h(e3(e2)) = 4, we can choose any
of them, say e1, and continue:
e2: merge {e1(), e3(p 1)}
= e2: e1: merge {tail e1(), e3(p 1)}
= e2: e1: e3(e2): merge {tail e3(p 1)}
= ...
Generalizing this example, the function p could
be specified as follows:
p 1 = (b 1) : merge {exp} (??)
where b 1 evaluates the 1-best derivation in H1
and exp ?somehow? calculates the next best
derivations. In the following subsection, we elabo-
rate this approach. First, we develop an algorithm
for solving the 1-best-derivation problem.
4.2 Solving the 1-best-derivation problem
Using SP and CP, it can be shown that for ev-
ery v ? V such that Hv 6= ? there is a mini-
mal derivation in Hv which does not contain any
subderivation in Hv (apart from itself). In other
words, it is not necessary to consider cycles when
solving the 1-best-derivation problem.
6Please note that e1() is an application of the function in
lines 08?10 of Fig. 2 while e1 is a derivation.
We can exploit this knowledge in a program by
keeping a set U of visited nodes, taking care not to
consider edges which lead us back to those nodes.
Consider the following function:
b v U = min{e(b v1 U?, . . . , b vk U?) |
e = (v1 . . . vk, ?, v) ? E,
{v1, . . . , vk} ? U? = ?}
where U? = U ? {v}
The argument U is the set of visited nodes. The
term b v ? evaluates to a minimal element of Hv,
or to min ? if Hv = ?. The problem of this divide-
and-conquer (or top-down) approach is that man-
aging a separate set U for every recursive call in-
curs a big overhead in the computation.
This overhead can be avoided by using a
dynamic programming (or bottom-up) approach
where each node is visited only once, and nodes
are visited in the order of their respective best
derivations.
To be more precise, we maintain a family (Pv |
v ? V ) of already found best derivations (where
Pv ? min?1(Hv) and initially empty) and a set C
of candidate derivations, where candidates for all
vertices are considered at the same time. In each
iteration, a minimal candidate with respect to - is
selected. This candidate is then declared the best
derivation of its respective node.
The following lemma shows that the bottom-up
approach is sound.
Lemma 3 Let (Pv | v ? V ) be a family such that
Pv ? min?1(Hv). We define
C = ?e=(v1...vk ,?,v)?E,
Pv=?
e(Pv1 , . . . , Pvk) .
Then (i) for every ? ? ?v?V,Pv=? Hv there is a
?? ? C such that ??  ?, and (ii) for every v ? V
and ? ? C ? Hv the following implication holds:
if ? ? ?? for every ?? ? C , then ? ? min1(Hv).
An algorithm based on this lemma is shown in
Fig. 3. Its key function iter uses the notion of ac-
cumulating parameters. The parameter q is a map-
ping corresponding to the family (Pv | v ? V ) of
the lemma, i. e., q v = Pv; the parameter c is a
set corresponding to C . We begin in line 01 with
the function q0 mapping every vertex to the empty
list. According to the lemma, the candidates then
consist of the nullary edges.
As long as there are candidates left (line 04),
in a recursive call of iter the parameter q is up-
dated with the newly found pair (v, [?]) of ver-
tex v and (list of) best derivation ? (expressed by
51
Require ?-hypergraph H = (V,E), linear pre-
order - fulfilling SP and CP.
Ensure b v ? min1(Hv) for every v ? V
such that if b v == [e(?1, . . . , ?k)] for some
e = (v1 . . . vk, ?, v) ? E, then b vi == [?i] for
every i ? {1, . . . , k}.
01 b = iter q0 {(?, ?, v) ? E | ? ? ?(0)}
02 q0 v = []
03 iter q ? = q
04 iter q c = iter (q//(v,[?])) c?
05 where
06 ? = min c and ? ? Hv
07 c? =
?
e=(v1...vk,?,v)?E
q v == []
e(q v1, . . . ,q vk)
Figure 3: Algorithm solving the 1-best-derivation
problem.
q//(v,[?])) and the candidate set is recomputed
accordingly. When the candidate set is exhausted
(line 03), then q is returned.
Correctness and completeness of the algorithm
follow from Statements (ii) and (i) of Lemma 3,
respectively. Now we show termination. In every
iteration a new next best derivation is determined
and the candidate set is recomputed. This set only
contains candidates for vertices v ? V such that
q v == []. Hence, after at most |V | iterations
the candidates must be depleted, and the algorithm
terminates.
We note that the algorithm is very similar to that
of Knuth (1977). However, in contrast to the latter,
(i) it admits Hv = ? for some v ? V and (ii) it
computes some minimal derivation instead of the
weight of some minimal derivation.
Runtime According to the literature, the run-
time of Knuth?s algorithm is in O(|E| ? log|V |)
(Knuth, 1977). This statement relies on a number
of optimizations which are beyond our scope. We
just sketch two optimizations: (i) the candidate set
can be implemented in a way which admits ob-
taining its minimum in O(log|C|), and (ii) for the
computation of candidates, each edge needs to be
considered only once during the whole run of the
algorithm.
4.3 Solving the n-best-derivations problem
Being able to solve the 1-best-derivation problem,
we can now refine our specification of p. The re-
fined algorithm is given in Fig. 4; for the func-
tions not given there, please refer to Fig. 3 (func-
tion b) and to Fig. 2 (functions merge, tail, and
the top-concatenation). In particular, line 02 of
Fig. 4 shows the general way of ?pulling out? the
head element as it was indicated in Section 4.1 via
an example. We also remark that the definition of
the top-concatenation (lines 08?09 of Fig. 2) cor-
responds to the way in which multk was sped up
in Fig. 4 of (Huang and Chiang, 2005).
Theorem 4 The algorithm in Fig. 4 is correct with
respect to its require/ensure specification and it
terminates for every input.
PROOF (SKETCH). We indicate how induction on n
can be used for the proof. If n = 0, then the statement
is trivially true. Let n > 0. If b v == [], then the
statement is trivially true as well. Now we consider the
converse case. To this end, we use the following three
auxiliary statements.
(1) take n (merge {l1, . . . ,lk}) =
take n (merge {take n l1, . . . ,take n lk}),
(2) take n e(l1, . . . ,lk) =
take n e(take n l1, . . . ,take n lk),
(3) take n (tail l) = tail (take (n+1) l).
Using these statements, line 04 of Fig. 2, and line 02
of Fig. 4, we are able to ?pull? the take of take n (p
v) ?into? the right-hand side of p v, ultimately yield-
ing terms of the form take n (p vj) in the first line
of the merge application and take (n-1) (p v?j) in
the second one.
Then we can show the following statement by induc-
tion on m (note that the n is still fixed from the outer
induction): for every m ? N we have that if the tree
in b v has at most height m, then take n (p v) ?
minn(Hv). To this end, we use the following two aux-
iliary statements.
(4) For every sequence of pairwise disjoint sub-
sets P1, . . . , Pk ?
?
v?V Hv, sequence of nat-
ural numbers n1, . . . , nk ? N, and lists l1 ?
minn1(P1), . . . , lk ? minnk(Pk) such that
nj ? n for every j ? {1, . . . , k} we have that
take n (merge {l1, . . . , lk}) ? minn(P1?. . .?Pk).
(5) For every edge e = (v1 . . . vk, ?, v) ? E, subsets
P1, . . . , Pk ?
?
v?V Hv , and lists l1 ? minn(P1), . . . ,
lk ? minn(Pk) we have that take n e(l1, . . . , lk) ?
minn(e(P1, . . . , Pk)).
Using these statements, it remains to show that
{e(?1, . . . , ?k)} ? minn?1
(
(e(Hv1 , . . . , Hvk) \
{e(?1, . . . , ?k)}) ?
?
e? 6=e e?(Hv?1 , . . . , Hv?k)
)
?
minn(Hv) where b v = [e(?1, . . . , ?k)] and ?
denotes language concatenation. This can be shown by
using the definition of minn.
Termination of the algorithm now follows from the
fact that every finite prefix of p v is well defined. 
52
Require ?-hypergraph H = (V,E), linear pre-order - fulfilling SP and CP.
Ensure
(
take n (p v)
)
? minn(Hv) for every v ? V and n ? N.
01 p v = [] if b v == []
02 p v = e(?1, . . . , ?k):merge ({tail e(p v1, . . . , p vk) | e = (v1 . . . vk, ?, v) ? E} ?
{e?(p v?1, . . . , p v?k) | e? = (v?1 . . . v?k, ??, v) ? E, e? 6= e})
if b v == [e(?1, . . . , ?k)]
Figure 4: Algorithm solving the n-best-derivations problem.
4.4 Implementation, Complexity, and
Experiments
We have implemented the algorithm (consisting
of Figs. 3 and 4 and the auxiliary functions of
Fig. 2) in HASKELL. The implementation is
rather straightforward except for the following
three points.
(1) Weights: we assume that - is defined by
means of weights (cf. Ex. 2), and that comparing
these weights is in O(1) (which often holds be-
cause of limited precision). Hence, we store with
each derivation its weight so that comparison ac-
cording to - is in O(1) as well.
(2) Memoization: we use a memoization tech-
nique to ensure that no derivation occurring in p v
is computed twice.
(3) Merge: the merge operation deserves some
consideration because it is used in a nested fash-
ion, yielding trees of merge applications. This
leads to an undesirable runtime complexity be-
cause these trees need not be balanced. Thus, in-
stead of actually computing the merge in p and in
the top-concatenation, we just return a data struc-
ture describing what should be merged. That data
structure consists of a best element and a list of
lists of derivations to be merged (cf. lines 06 and
09 in Fig. 2). We use a higher-order function to
manage these data structures on a heap, perform-
ing the merge in a nonnested way.
Runtime Here we consider the n-best part of the
algorithm, i. e. we assume the computation of the
mapping b to take constant time. Note however
that due to memoization, b is only computed once.
Then the runtime complexity of our implementa-
tion is in O
(
|E|+ |V | ?n ? log(|E|+n)
)
. This can
be seen as follows.
By line 02 in Fig. 4, the initial heaps in the
higher-order merge described under (3) have a to-
tal of |E| elements. Building these heaps is thus
in O(|E|). By line 09 in Fig. 2, each newly found
derivation spawns at most as many new candidates
n total time [s] time for n-best part [s]
1 8.713 ?
25 000 10.832 2.119
50 000 12.815 4.102
100 000 16.542 7.739
200 000 24.216 15.503
Table 1: Experimental results
on the heap as the maximum rank in ?. We assume
this to be constant. Moreover, at most n deriva-
tions are computed for each node, that is, at most
|V |?n in total. Hence, the size of the heap of a node
is in O(|E|+n). For each derivation we compute,
we have to pop the minimal element off the heap
(cf. line 07 in Fig. 2), which is in O(log(|E|+n)),
and we have to compute the union of the remaining
heap with the newly spawned candidates, which
has the same complexity.
We give another estimate for the total number
of derivations computed by the algorithm, which
is based on the following observation. When pop-
ping a new derivation ? off the heap, new next best
candidates are computed. This involves comput-
ing at most as many new derivations as the number
of nodes of ?, because for each hyperedge occur-
ring in ? we have to consider the next best alter-
native. Since we pop off at most n elements from
the heap belonging to the target node, we arrive at
the estimate d ?n, where d is the size of the biggest
derivation of said node.
A slight improvement of the runtime complex-
ity can be obtained by restricting the heap size to
n best elements, as argued by Huang and Chiang
(2005). This way, they are able to obtain the com-
plexity O(|E| + d ? n ? log n).
We have conducted experiments on an Intel
Core Duo 1200 MHz with 2 GB of RAM using
a cyclic hypergraph containing 671 vertices and
12136 edges. The results are shown in Table 1.
This table indicates that the runtime of the n-best
part is roughly linear in n.
53
References
Athanasios Alexandrakis and Symeon Bozapalidis.
1987. Weighted grammars and Kleene?s theorem.
Inform. Process. Lett., 24(1):1?4.
Jean Berstel and Christophe Reutenauer. 1982. Recog-
nizable formal power series on trees. Theoret. Com-
put. Sci., 18(2):115?148.
Matthias Bu?chse, Jonathan May, and Heiko Vogler.
2009. Determinization of weighted tree automata
using factorizations. Talk presented at FSMNLP 09
in Pretoria, South Africa.
Zolta?n ?Esik and Werner Kuich. 2003. Formal tree se-
ries. J. Autom. Lang. Comb., 8(2):219?285.
Zolta?n Fu?lo?p and Heiko Vogler. 2009. Weighted tree
automata and tree transducers. In Manfred Droste,
Werner Kuich, and Heiko Vogler, editors, Handbook
of Weighted Automata, chapter 9. Springer.
Joshua Goodman. 1999. Semiring parsing. Comp.
Ling., 25(4):573?605.
Liang Huang and David Chiang. 2005. Better k-
best parsing. In Parsing ?05: Proceedings of the
Ninth International Workshop on Parsing Technol-
ogy, pages 53?64. ACL.
Dan Klein and Christopher D. Manning. 2001. Parsing
and hypergraphs. In Proceedings of IWPT, pages
123?134.
Donald E. Knuth. 1977. A Generalization of Dijkstra?s
Algorithm. Inform. Process. Lett., 6(1):1?5, Febru-
ary.
Zhifei Li, Jason Eisner, and Sanjeev Khudanpur. 2009.
Variational decoding for statistical machine transla-
tion. In Proc. ACL-IJCNLP ?09, pages 593?601.
ACL.
Andreas Maletti and Giorgio Satta. 2009. Parsing al-
gorithms based on tree automata. In Proc. 11th Int.
Conf. Parsing Technologies, pages 1?12. ACL.
Jonathan May and Kevin Knight. 2006. A better n-best
list: practical determinization of weighted finite tree
automata. In Proc. HLT, pages 351?358. ACL.
Mark-Jan Nederhof. 2003. Weighted deductive pars-
ing and Knuth?s algorithm. Comp. Ling., 29(1):135?
143.
Lars Relund Nielsen, Kim Allan Andersen, and
Daniele Pretolani. 2005. Finding the k shortest hy-
perpaths. Comput. Oper. Res., 32(6):1477?1497.
Franz Josef Och and Hermann Ney. 2002. Discrimina-
tive training and maximum entropy models for sta-
tistical machine translation. In ACL, pages 295?302.
Adam Pauls and Dan Klein. 2009. k-best a* parsing.
In Proc. ACL-IJCNLP ?09, pages 958?966, Morris-
town, NJ, USA. ACL.
J. W. Thatcher. 1967. Characterizing derivation trees
of context-free grammars through a generalization
of finite automata theory. J. Comput. Syst. Sci.,
1(4):317?322.
54
