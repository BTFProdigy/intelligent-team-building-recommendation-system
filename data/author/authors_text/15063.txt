Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 1311?1321, Dublin, Ireland, August 23-29 2014.
Group based Self Training for E-Commerce Product Record Linkage
Wayne Xin Zhao
1,2
, Yuexin Wu
2
, Hongfei Yan
2
and Xiaoming Li
2
1
School of Information, Renmin University of China, China
2
School of Electronic Engineering and Computer Science, Peking University, China
batmanfly@gmail.com, wuyuexin@gmail.com,
yhf1029@gmail.com, lxm@pku.edu.cn
Abstract
In this paper, we study the task of product record linkage across multiple e-commerce web-
sites. We solve this task via a semi-supervised approach and adopt the self-training algorithm for
learning with little labeled data. In previous self-training algorithms, the learner tries to convert
the most confidently predicted unlabeled examples of each class into labeled training examples.
However, they evaluate the confidence of an instance only based on the individual evidence from
the instance. The correlation among data instances is rarely considered.
To address it, we develop a novel variant of the self-training algorithm by leveraging the data
characteristics for the task of product record linkage. We joint consider a candidate linked pair
and its corresponding correlated pairs as a group at the selection of pseudo labeled data. We
propose a novel confidence evaluation method for a group of instances, and incorporate it as a
re-ranking step in the self-training algorithm. We evaluate the novel self-training algorithm on
two large datasets constructed based on real e-commerce Websites. We adopt several competitive
methods as comparisons and perform extensive experiments. The results show that our method
outperforms these baselines that do not consider data correlation.
1 Introduction
Recent years have witnessed the rapid development of online e-commerce business, e.g. Amazon and
eBay, which raises the need for better storing, organizing and analyzing the large amount of product
records. An important task is how to effectively link product records across multiple databases or web-
sites. This task serves as a fundamental step for many applications. For example, it will be useful to
provide entity-oriented search and product comparison analysis in eBay, where record linkage can help
to unify the corresponding records (i.e. records from different sellers) given a product. Record linkage has
been shown to be important in many fields, including biology (Needleman and Wunsch, 1970), database
(Neiling, 2006) and text mining (Goiser and Christen, 2006; Bilenko and Mooney, 2003). In this paper,
we mainly focus on the task of product record linkage for online e-commerce websites, but our method
is easy to be extended to other data sources and tasks.
Early studies on record linkage were mainly based on the classical probabilistic approach develope-
d by Fellegi and Sunter (1969), furthermore it was improved by the application of the expectation-
maximization (EM) algorithm (Winkler, 1988) and the use of approximate string comparison algorithms
(Christen, 2006; Winkler, 2006). The early work was not flexible to incorporate rich information. The
development of machine learning techniques in the late 1990s provides a new approach for record link-
age, and it has become the mainstream methodology for this task. The task of record linkage is usually
re-casted as the record pair classification problem, i.e. whether a record pair refers to the same entity or
not (Elfeky et al., 2002; Neiling, 2006; Tejada et al., 2002; Nahm et al., 2002). Supervised methods can
also be used to learn distance measures for approximate string comparisons (Bilenko and Mooney, 2003;
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer
are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/
1311
Cohen et al., 2003). Although supervised techniques often achieve good linkage quality, they are largely
limited by the availability of the training data.
To address this problem, semi-supervised learning approaches aim to make good use of a small portion
of labeled and a large amount of unlabeled data to build a better classifier (Yarowsky, 1995). Self-training
is a commonly used algorithm for semi-supervised learning, where in each iteration the learner converts
the most confidently predicted unlabeled examples of each class into labeled training examples. It has
been successfully applied to many tasks, such as sentiment analysis (He and Zhou, 2011; Riloff et al.,
2003) and object detection from images (Rosenberg et al., 2005).
In this paper, we solve the task of product record linkage via a semi-supervised approach and adopt
the flexible self-training framework for learning with little labeled data. We propose a novel variant of
the self-training algorithm by incorporating the correlation existing in the data instances, which is rarely
studied in previous studies. To introduce our idea, we first present an illustrative example in Figure 1.
There are two databases D and D
?
, and we have three records r
1
, r
2
, r
3
? D and another three records
r
?
1
, r
?
2
, r
?
3
? D
?
. Furthermore, we assume r
1
and r
?
1
refer to the same product. We can see that r
1
is
involved in three candidate pairs, i.e. (r
1
, r
?
1
), (r
1
, r
?
2
) and (r
1
, r
?
3
). Similarly, r
?
1
is involved in three
candidate pairs, i.e. (r
?
1
, r
1
), (r
?
1
, r
2
) and (r
?
1
, r
3
). Usually, each individual database does not contain
duplicate records, once we know r
1
is linked to r
?
1
, we can infer the rest candidate pairs should not be
linked. In other words, only if we are confident that no pair in the set {(r
1
, r
?
2
), (r
1
, r
?
3
), (r
2
, r
?
1
), (r
3
, r
?
1
)}
is not linked, r
1
is likely to be linked with r
?
1
.
?
?
?
?
?
?
Figure 1: An illustrative example for correlation among record pairs. The real line denotes the real linkage
relation and the dash line denotes the candidate linkage relation.
For the task of record linkage, the number of positive instances (i.e. linked record pairs) are usually
much less than that of negative instances. We mainly consider the confidence evaluation of the candidate
positive instance. By following the above idea, given a candidate linked pair, we treat all the correlated
record pairs together as a group and evaluate the linkage confidence based on the evidence of all record
pairs in this group, i.e. group confidence evaluation. We incorporate the group confidence evaluation
into the self-training algorithm as a re-ranking step. Interestingly, once we have identified a linked pair,
the rest correlated record pairs can be naturally judged as negative instances. We evaluate the novel
self-training algorithm on two large datasets constructed based on real e-commerce Websites. We adopt
several competitive methods as comparisons and perform extensive experiments. The results show that
our method outperforms these baselines that do not consider data correlation.
2 Related Work
We have briefly described the supervised approaches for record linkage in the introduction. Now we
discuss other related studies, including unsupervised clustering techniques, genetic programming based
approaches and linking based on more complex constraints.
Unsupervised clustering techniques have been investigated both for improved blocking (Cohen and
Richman, 2002; McCallum et al., 2000) and for automatic record pair classification (Elfeky et al., 2002).
Usually, such techniques do not perform not as well as supervised approaches.
Most recently, genetic programming (GP) (Koza et al., 1999) has also been utilized to the task of
record linkage. GenLink (Isele and Bizer, 2012) is a GP-based supervised learning algorithm in order
to learn linkage rules from a set of existing reference links, which also suffers from the problem of
lack of labeled data. Ngomo and Lyko (2013) evaluated linear and boolean classifiers against classifiers
1312
computed by using genetic programming for the record linkage problem. Their experiments showed that
both approaches did not perform well on real data.
Some other studies exploit more complex constraints that include relationships between different entity
types to link all types of entities in coordination (Bhattacharya and Getoor, 2007; Dong et al., 2005; On
et al., 2007). The usage of such constraints can indeed help to get better linkage results, but is in many
cases domain-dependent. We try to develop an approach which can be applicable across domains.
In order to address the problem of limited labeled data, we mainly consider the semi-supervised ap-
proaches. There are rarely semi-supervised approaches specially for the record linkage problem. Some
studies on improving self-training algorithms are related to our work. Self-training with editing (Li and
Zhou, 2005) can help to reduce mislabeled pseudo training examples, and reserved self-training (Guan
and Yang, 2013) is designed for handling imbalanced data. We have very different focus with theirs, i.e.
incorporating the instance correlations into learning algorithms, which can applied to other self-training
variants.
3 Problem Definition
In this section, we first introduce the preliminary related to our task. Then we formally define our studied
task.
Product record. A product record r is characterized by a referred product entity e and a set of attribute
values V = {(v
i
)}
i
, where v
i
denotes the value of the ith attribute in r. We use r.e and r.V to index
the product entity and attribute value set of the record r respectively. A product record corresponds to a
unique product entity but a product entity can map to multiple product records across multiple databases.
Attribute values are represented as strings, i.e. a sequence of characters. An attribute of a product might
correspond to different descriptive text across websites.
Product record linkage. The task of product record linkage is to judge whether two product records refer
to the same product entity. Given two product records r and r
?
, we aim to judge whether r.e is the same
to r
?
.e. Usually, r and r
?
come from different product databases. Although different product databases
can have different attributes for the same product and different attribute names for the same attribute, we
make an assumption about the task: candidate record pairs share the same set of attributes. It is relatively
easy to automatically identify common attributes and align attributes (H?arder et al., 1999; Rundensteiner,
1999; Hassanzadeh et al., 2013), which is not our focus in this paper. We mainly study product record
linkage under the same set of attributes, and this assumption makes our study more focused. If r and r
?
refer to the same product entity, denoted by r ? r
?
; otherwise, we denote it by r 6? r
?
.
4 A General Machine Learning based Approach
Given a product type, as we mentioned above, we assume that it corresponds to a specific set of attributes,
and all the product records share the same set of attributes but possibly with different descriptive text for
attribute values. In this section, we further present a general supervised approach with similarity features.
4.1 Defining the similarity function
Given two product records r and r
?
, we can obtain the similarity between their descriptive text of an at-
tribute by using a similarity function. The major intuition is that if two records refer to the same product,
they should have similar text for the same attribute, i.e. the similarity function should return a large sim-
ilarity value. Let f(?, ?) denote a similarity function, which takes two text strings and returns a similarity
value within the interval [0, 1] for these two strings. As revealed in (Bilenko and Mooney, 2003), differ-
ent attributes or fields may need different similarity functions to achieve best similarity evaluation. Thus,
instead of fixing a single similarity function, we consider using the following widely used similarity
functions: 1) Exact match; 2) Cosine similarity; 3) Jaccard coefficient; 4) K-Gram similarity (Kondrak,
2005); 5) Levenshtein similarity (Levenshtein, 1966); 6) Affine Gap similarity (Needleman and Wunsch,
1970).
1313
4.2 The learning framework
Based on these similarity functions, we propose a general learning framework for product record linkage
by using similarity values of different fields as features.
Given a product type, we assume that there are A attributes and K similarity functions. For two records
r and r
?
, we can obtain a similarity feature vector x = [x
a,k
]
A
i=1
,
K
k=1
, which is indexed by an attribute and
a similarity function: x
a,k
denotes the similarity of the ath attribute between r and r
?
by using the kth
similarity function. Furthermore, each feature vector x will correspond to a unique binary label y which
indicates that r and r
?
refer to the same product entity. Given a set of record pairs and their linkage labels
{(x, y)}, we can learn a classifier which is able to predict the linkage label given the similarity feature
vector of two records. To this end, we have reformulated the task of product record linkage as a binary
classification problem. Any classifiers can be used for this task. In what follows, we will use instances
and candidate pairs alternatively.
5 Group based Self-Training
In the above, we have presented a supervised learning approach for product record linkage. The approach
is easy to apply in practice, however, the performance is largely limited by the availability of training
data. For our current task, i.e. product record linkage, the generation of labeled data becomes even much
harder: there are usually many product types and it is infeasible to create a large amount of labeled data
for each type. Although it is difficult to obtain labeled data, we can easily obtain sufficient unlabeled data.
Thus, in this paper, we study the task of product record linkage in a semi-supervised setting by leveraging
both the learning ability of the classifiers and the usefulness of the large amount of unlabeled data. We
propose a novel group based self-training algorithm for product record linkage. Before introducing our
method, we first introduce the general self-training algorithm.
5.1 The general self-training algorithm
Self-training is a semi-supervised learning algorithm. It starts training on labeled data only, after each
iteration, the most confidently predicted unlabeled samples would be incorporated as new labeled data,
i.e. pseudo labeled data, decided by confidence scores from the classifier. After several iterations, it is
expected to get a better classifier trained with both labeled data and pseudo labeled data. The general
procedure of self-training algorithm is summarized in Algorithm 1.
Algorithm 1: The general procedure of the self-training algorithm.
1 Input: labeled dataset L, unlabeled dataset U , the classifier C.
2 U
?
? S randomly selected examples from U , S is usually set to 0.5 ? |U|;
3 repeat
4 Training the classifier: Use L to train C, and label the examples in U
?
;
5 Selecting pseudo labeled data: Select T most confidently classified examples from U
?
and add them to L;
6 Filling unlabeled data: Refill U
?
with examples from U , to keep U
?
at a constant size of S examples.
7 until I iterations or U = ?;
8 return The extended labeled dataset L and the trained classifier C.
We can see that self-training is a wrapper algorithm by taking a classifier as the learning component,
and it has three major steps in an iteration: 1) training classifier; 2) selecting pseudo labeled data; and
3) filling unlabeled data. Among the three steps, the most important step is the pseudo labeled data
selection. Previously, the most commonly used method is to select the top confident instances of the
classifier, and it is easy to see that the performance of self-training relies on the learning ability of the
embedded classifier.
5.2 Group confidence evaluation
Recall that each instance is a pair of product records (r, r
?
) and their label indicates whether they should
be linked or not. Let P
L
(r, r
?
) denote the confidence that r and r
?
refer to the same product entity (linked
1314
confidence), and P
N
(r, r
?
) denote the confidence that r and r
?
refer to different product entities (non-
linked confidence). P
L
(r, r
?
) and P
N
(r, r
?
) can be estimated by the confidence scores from the classifier.
In the task of product record linkage, there are usually more negative instances, i.e. the number of non-
linked pairs is much more than that of linked pairs. Thus, we mainly study the confidence of a candidate
positive instance. The standard self-training algorithm selects top ranked positive instances according
to the confidence scores estimated by the classifier, i.e. we select the instances with large linked con-
fidence P
L
(?, ?). However, when applied to product record linkage, it ignores important characteristics
underlying the data, which will be potentially helpful to the task.
Let us examine the illustrative example in Figure 1. Recall that r
1
and r
?
1
refer to the same product,
i.e. r
1
? r
?
1
. We can see that r
1
is involved in three candidate pairs, i.e. (r
1
, r
?
1
), (r
1
, r
?
2
) and (r
1
, r
?
3
).
Similarly, r
?
1
is involved in three candidate pairs, i.e. (r
?
1
, r
1
), (r
?
1
, r
2
) and (r
?
1
, r
3
). We totally have a set
of five candidate pairs, i.e. {(r
1
, r
?
1
), (r
1
, r
?
2
), (r
1
, r
?
3
), (r
2
, r
?
1
), (r
3
, r
?
1
)}. Here we follow the assumption
of the one-to-one mapping, i.e. given two databases, a product record can link to at most one record in
the other database. By leveraging the correlation among candidate pairs, with r
1
? r
?
1
, we can infer the
rest four candidate pairs must not be linked, i.e. r
1
6? r
?
2
, r
1
6? r
?
3
, r
2
6? r
?
1
, r
3
6? r
?
1
. Next, we formally
characterize the above idea and present the algorithm. Given two databases D and D
?
, let C ? D ? D
?
denote the candidate pair set where two product records in a pair come from D and D
?
respectively.
Consider a candidate pair (r, r
?
) ? C, where r ? D, r
?
? D
?
. We consider the following two sets:
S
r
= {(r, b)|(r, b) ? C, b ? D
?
and b 6= r
?
} and S
r
?
= {(a, r
?
)|(a, r
?
) ? C, a ? D and a 6= r}.
Intuitively, if we know r ? r
?
, then all the pairs in both S
r
and S
r
?
must not be linked. Thus, we define
the conflicting set of pair (r, r
?
) as S
r,r
?
cfl
= S
r
? S
r
?
.
With the definition of the conflicting set, let us reconsider the pseudo labeled data selection. The
straightforward way is to evaluate each instance with their linked confidence P
L
() from the classifier.
However, it oversimplifies the data dependence and does not make use of the correlated characteristics.
Consider an instance, which is a record pair (r, r
?
), we can have the following two properties:
? If r ? r
?
, then ?(a, b) ? S
r,r
?
cfl
, we have a 6? b;
? If ?(a, b) ? S
r,r
?
cfl
and a ? b, then we have r 6? r
?
.
The above properties suggest that it should be helpful to consider the correlation among instances
when evaluating the confidence of a positive instance, i.e. a candidate linked record pair. Intuitively, if
two records refer to the same product entity, they should have large linked confidence and their conflicting
pairs should have large non-linked confidence. We propose to use the following method to evaluate the
linkage confidence between r and r
?
Conf(r, r
?
) = P
L
(r, r
?
)
(
?
(a,b)?S
r,r
?
cfl
P
N
(a, b)
)
1/M
, (1)
where M = |S
r,r
?
cfl
|, P
L
(?, ?) and P
N
(?, ?) are positive and negative confidence scores estimated by
the classifier respectively. Note that we take the geometric mean of the non-linked confidence of these
conflicting pairs, which is to reduce the affect of large outlier values and the varying size of the conflict
sets. We treat a candidate linked pair and all the candidate pairs in its conflicting set as a group. The group
confidence evaluation consists of two intuitions: 1) the confidence that two records should be linked; 2)
the confidence that any pair of records in the conflicting set must not be linked. We have taken these two
aspects into a unified evaluation score.
5.3 The proposed self-training algorithm
In this part, we present the novel self-training algorithm based on the group confidence evaluation. We
have the similar steps with the general self-training algorithm in Algorithm 1. The major focus is to mod-
ify the step of pseudo labeled data selection. As mentioned above, we mainly consider the confidence
evaluation of positive instances. Our method for pseudo labeled data selection is three-step process:
1315
? Select top T
?
most confidently classified positive examples by the classifier;
? Rerank these T
?
examples by the group confidence scores defined in Equation 1;
? Select top T examples from the reranked T
?
examples (T ? T
?
) as pseudo positive instances and
their corresponding conflicting instances in the conflicting sets as pseudo negative instances.
We select positive instances not only based on the instance itself but also their corresponding conflict-
ing instances: if we have high confidence about a positive instance, then the confidence of their conflict-
ing instances being negative should be high, too. Next, we present the detailed group based self-training
algorithm in Algorithm 2.
Algorithm 2: The procedure of the group based self-training algorithm.
1 Input: labeled dataset L, unlabeled dataset U , the classifier C.
2 U
?
? S randomly selected examples from U ;
3 repeat
4 Training the classifier: Use L to train C, and label the examples in U
?
;
5 Selecting pseudo labeled data selection:
? Select T
?
most confident positive examples from U
?
and add them to L;
? Calculate the group confidence scores for the T
?
examples according to Equation 1.
? Rerank these T
?
examples by their group confidence scores and add top T examples to L as the pseudo positive
instances.
? For each of the T examples, add their conflicting instances to L into as the pseudo negative instances.
Filling unlabeled data: Refill U
?
with examples from U , to keep U
?
at a constant size of S examples.
6 until I iterations or U = ?;
7 return The extended labeled dataset L and the trained classifier C.
On one hand, our group based self-training algorithm naturally exploits the correlation among data
instances and evaluate the confidence scores in a broader view, which avoids the decision conflicts caused
by the data dependence. On the other hand, we focus on evaluating the confidence of being a positive
instance, which further reduces the bias from imbalanced data distribution. Thus, it is expected to achieve
better performance in the task of product record linkage.
Most classifiers can provide the estimated confidence scores P
L
() (i.e. for a positive instance) and
P
N
() (i.e. for a negative instance): Maximum-Entropy models output the conditional probabilities of an
instance for each class (Berger et al., 1996); the Decision Tree C4.5 algorithm is also able to compute
the probability distribution over different classes for each instance (Quinlan, 1993).
6 Experiments
6.1 Construction of the test collection
We test our method on two real e-commerce datasets respectively from Jingdong
1
and eTao
2
. Jingdong is
the largest B2C e-commerce company and eTao is one of the largest product search portals in China. Due
to the extremely large product databases, it is infeasible to generate training data on each product type
for these two product databases. We consider two popular kinds of products: laptop and camera. These
two kinds of products cover a considerable amount of brands and models, especially suitable for the test
of record linkage. Both Jindong and eTao have set up specific categories for these two kinds of products
respectively, thus we can easily crawl the product records under the corresponding category label. To
generate linked record datasets, we first manually align attributes (i.e. fields) for these two kinds between
Jindong and eTao. We summarize the numbers of aligned fields and some example fields in Table 1. Not
all the records contain the information for all the fields, we set the value of the empty field to a ?NULL?
string.
1
http://www.jd.com
2
http://www.etao.com
1316
We adopt a blocking approach (Baxter et al., 2003) to automatically generate a set of candidate pairs,
i.e. a record in Jindong is to be linked with a record in eTao. This approach consider all pairwise links
between Jindong records and eTao records for the same kind of product. If there exists at least one com-
mon word in the field of brand or model between a record pair, we consider it to be a candidate pair. The
automatic method generates 20,094 candidate pairs and 12,157 candidate pairs respectively for LAPTOP
and CAMERA. Then we invite professional workers from an e-commerce company to link records across
these two product databases. Instead of examining all the candidate pairs, the labeling process adopts a
product-oriented way to generate the gold standard. Given a product record of a database, the annotator
first identifies the product entity that the record refers to, then she looks for the corresponding record in
another database. In the annotation process, Web access is available all the time. Annotators can make
use of the search engines of Jindong and eTao to accelerate the product lookup. A linked record pair is
treated as a positive instance. Finally, we identify 501 linkable products (i.e. 501 positive instances) in
LAPTOP dataset, and 478 linkable products (i.e. 478 positive instances) in CAMERA dataset. All the
other candidate pairs are automatically labeled as negative. We present the the data statistics in Table 1.
Dataset
# positive # negative
# fields Example fields
instances instances
LAPTOP 501 19593 10 OS, screen size, CPU type, ram size
CAMERA 478 11679 11 lens type, sensor type, focal length, aperture size
Table 1: Basic statistics of datasets.
6.2 Experimental setup
For each kind of product, we divide the dataset into two parts, i.e. a training set and a test set. In order
to examine different methods in a semi-supervised setting, we keep a small amount of instances in the
training set, and we assume all the methods can use of the data (without labels) in the test set. There are
more negative instances, we mainly consider the amount of positive instances, and the number of positive
instances is called as the number of seeds. We randomly generate the training set with the given number
of seeds. Once we add one positive instance into the training set, we add all the its conflicting instances
into the training set. This is to reduce the correlation between training instances and testing instances for
a fair comparison. In later experiments, given the seed number, we will generate ten random training sets
and take the average of ten runs as the final performance. In later experiments, we do not explicitly report
the number of negative instances unless needed.
We adopt three widely used evaluation metrics for the classification task: Precision, Recall and the
F-measure
3
.
We compare the following methods for the task of product record linkage:
? Supervised Classifier (SC): the standard supervised classifier, which does not consider the unlabeled
data at all.
? Traditional Self-Training (t-ST): the traditional self-training method in Algorithm 1 which adds an
equal amount of samples of each class in pseudo labeled selection at each iteration.
? Proportional Self-Training (p-ST): the traditional self-training method in Algorithm 1 but add sam-
ples according to the class distribution at each iteration.
? Simple Group Based Self-Training (s-ST): a simplified version of our approach without the group
confidence valuation, which directly selects samples of high confidence scores estimated from the
classifier together with their conflicting pairs as negative samples at each iteration.
? Group Based Self-Training (g-ST): the proposed group based self-training algorithm in Algorithm 2,
which uses the group confidence evaluation method to select pseudo positive instances.
3
http:/en.wikipedia.org/wiki/Precision and recall
1317
Recall all the methods rely on the wrapped classifier. We select two classic but very different classi-
fiers: the Maximum Entropy model (MaxEnt) and the Decision Tree C4.5 (Tree). We implement these
two classifiers using the machine learning toolkit Weka
4
. We use the six similarity functions to obtain
similarity values between two records on each field as features. All the self-training based methods run
ten iterations and at each iteration they add the same number of positive instances, i.e. 30. Differen-
t methods select pseudo negative instances differently. t-ST does not consider the correlation between
data instances, and it adds top 30 confident negative instances. p-ST adds top 30 ?
#negative instances
#positive instances
con-
fident negative instances. Both p-ST and g-ST take all the conflicting instances of the selected pseudo
positive instances as the negative instances. We present the average numbers of pseudo negative instances
at an iteration in Table 2. As will be revealed later, although p-ST adds more negative instances, g-ST
performs much better than p-ST, which indicates simply adding more negative instances might not lead
to better performance. We do not perform specific preprocessing steps to make the data balanced (e.g.
under-sampling or over-sampling), and we find the data distribution does not significantly affect the
performance of the classifiers on our dataset.
Dataset t-ST p-ST s-ST g-ST
LAPTOP 30 950 845 854
CAMERA 30 655 569 584
Table 2: Average numbers of pseudo negative instances selected at each iteration.
6.3 Results and analysis
Overall performance comparison. To test the performance under weak supervision, we first set the
seed number to 30, which nearly takes up a proportion of 5% of the labeled data. We present the results
of different methods in Table 3 and Table 4. We first examine the performance of the baselines. We can
see that semi-supervised learning is very effective to improve over the the supervised classifier when the
amount of training data is small. It is interesting to see that s-ST performs best among all the baselines.
Recall that the major difference between s-ST and other baselines is that it select the conflicting pairs
of the pseudo positive instances as the negative instances. It indicates that it is important to consider the
correlation among the data instances. In addition, Decision Tree seems to be more competitive than Max-
imum Entropy Model for product record linkage. Then we take our group based self-training algorithm
into comparison. In terms of F1 measure, we can see that it is consistently better than all the baselines
on two datasets respectively by using two different classifiers. It is worth looking into the performance
comparison on precision and recall. We can see that (1) s-ST and g-ST yield better results in terms of
precision while the other baselines yield better results in terms of recall; (2) our method g-ST largely
improves over the best baseline s-ST. It is not surprising to have these observations since that our group
evaluation method is more careful at the selection of pseudo positive instance: it considers the evidence
from the conflicting instances.
Methods MaxEnt Decision Tree
P R F1 P R F1
SC 0.246 0.910 0.382 0.301 0.931 0.454
t-ST 0.264 0.925 0.411 0.328 0.921 0.484
p-ST 0.350 0.831 0.487 0.412 0.887 0.539
s-ST 0.979 0.632 0.767 0.909 0.754 0.823
g-ST 0.936 0.742 0.826 0.912 0.843 0.876
Table 3: Results on LAPTOP dataset.
Parameter tuning. In the above, we have shown the results of different methods with 30 positive in-
stances. The number of seeds is particularly important for self-training algorithms, and we want to ex-
4
http://www.cs.waikato.ac.nz/ml/weka
1318
Methods MaxEnt Decision Tree
P R F1 P R F1
SC 0.387 0.891 0.540 0.493 0.965 0.652
t-ST 0.352 0.892 0.504 0.537 0.963 0.677
p-ST 0.501 0.871 0.626 0.573 0.942 0.700
s-ST 0.931 0.479 0.632 0.962 0.570 0.716
g-ST 0.917 0.574 0.706 0.965 0.588 0.731
Table 4: Results on CAMERA dataset.
amine how it affects the performance of these methods. By varying the number of seeds from 10 to 50
with a step of 10, we present the F1 results in Figure 2 on two datasets by using two classifiers. We can
see that our method is consistently better than baselines with the varying of the seed number. Especially,
our method still works well when there is little labeled data, i.e. #seeds = 10. With a weaker classifier,
i.e. MaxEnt, our method yields more improvement than that with Tree. Besides the seed number, there
are another two factors which potentially affect the performance: (1) the iteration number and (2) the
number of pseudo positive instances selected at each iteration. We also examine the tuning results of
these two parameters and find our method is consistently better than s-ST with the varying of these two
factors. These results show that our method is very effective and it is of high stability and practicability.
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 10  15  20  25  30  35  40  45  50
F1
# of Labeled Seeds
SC
t-ST
p-ST
s-ST
g-ST
(a) LAPTOP, MaxEnt
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 10  15  20  25  30  35  40  45  50
F1
# of Labeled Seeds
SC
t-ST
p-ST
s-ST
g-ST
(b) LAPTOP, Tree
 0.4
 0.45
 0.5
 0.55
 0.6
 0.65
 0.7
 0.75
 0.8
 10  15  20  25  30  35  40  45  50
F1
# of Labeled Seeds
SC
t-ST
p-ST
s-ST
g-ST
(c) CAMERA, MaxEnt
 0.35
 0.4
 0.45
 0.5
 0.55
 0.6
 0.65
 0.7
 0.75
 0.8
 0.85
 10  15  20  25  30  35  40  45  50
F1
# of Labeled Seeds
SC
t-ST
p-ST
s-ST
g-ST
(d) CAMERA, Tree
Figure 2: Performance comparison with varying seed numbers (i.e. # of positive instances).
1319
7 Conclusion
In this paper, we develop a novel variant of the self-training algorithm by leveraging the data characteris-
tic for the task of product record linkage. We joint consider a candidate linked pair and its corresponding
correlated pairs as a group, at the selection of pseudo labeled data. We propose a confidence evaluation
method for a group of instances, and incorporate it as a re-ranking step in the self-training algorithm. We
evaluate the novel self-training algorithm on two large datasets constructed based on real e-commerce
Websites. We adopt several competitive methods as comparisons and perform extensive experiments.
The results show that our method outperforms these baselines that do not consider data correlation. We
also carefully examine the affects of various parameters, and the tuning results indicate the stability and
robustness of our method.
The major contribution and novelty of this paper is the novel group confidence evaluation to model
the correlation existing in data. Although we develop the idea in the setting of self-training algorithms,
it will be promising to be applied in other learning algorithms, i.e. active learning.
Acknowledgements
We thank the anonymous reviewers for his/her thorough review and highly appreciate the comments.
This work was partially supported by the National Key Basic Research Program (973 Program) of China
under grant No. 2014CB340403, 2014CB340405 and NSFC Grant 61272340. Xin Zhao was supported
by MSRA PhD fellowship. Xin Zhao and Yuexin Wu contributed equally to this work and should be
considered as joint first authors. Xin Zhao is the corresponding author.
References
Rohan Baxter, Peter Christen, and Tim Churches. 2003. A comparison of fast blocking methods for record linkage.
In ACM SIGKDD, volume 3, pages 25?27. Citeseer.
Adam L Berger, Vincent J Della Pietra, and Stephen A Della Pietra. 1996. A maximum entropy approach to
natural language processing. Computational linguistics, 22(1):39?71.
Indrajit Bhattacharya and Lise Getoor. 2007. Collective entity resolution in relational data. ACM Transactions on
Knowledge Discovery from Data (TKDD), 1(1):5.
Mikhail Bilenko and Raymond J Mooney. 2003. Adaptive duplicate detection using learnable string similarity
measures. In Proceedings of the ninth ACM SIGKDD international conference on Knowledge discovery and
data mining, pages 39?48. ACM.
Peter Christen. 2006. A comparison of personal name matching: Techniques and practical issues. In Data Mining
Workshops, 2006. ICDM Workshops 2006. Sixth IEEE International Conference on, pages 290?294. IEEE.
William W Cohen and Jacob Richman. 2002. Learning to match and cluster large high-dimensional data sets for
data integration. In Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery
and data mining, pages 475?480. ACM.
William W Cohen, Pradeep D Ravikumar, Stephen E Fienberg, et al. 2003. A comparison of string distance
metrics for name-matching tasks. In IIWeb, volume 2003, pages 73?78.
Xin Dong, Alon Halevy, and Jayant Madhavan. 2005. Reference reconciliation in complex information spaces. In
Proceedings of the 2005 ACM SIGMOD international conference on Management of data, pages 85?96. ACM.
Mohamed G Elfeky, Vassilios S Verykios, and Ahmed K Elmagarmid. 2002. Tailor: A record linkage toolbox. In
Data Engineering, 2002. Proceedings. 18th International Conference on, pages 17?28. IEEE.
Ivan P Fellegi and Alan B Sunter. 1969. A theory for record linkage. Journal of the American Statistical Associa-
tion, 64(328):1183?1210.
Karl Goiser and Peter Christen. 2006. Towards automated record linkage. In Proceedings of the fifth Australasian
conference on Data mining and analystics-Volume 61, pages 23?31. Australian Computer Society, Inc.
Zhiguang Liu Xishuang Dong Yi Guan and Jinfeng Yang. 2013. Reserved self-training: A semi-supervised senti-
ment classification method for chinese microblogs.
1320
Theo H?arder, G?unter Sauter, and Joachim Thomas. 1999. The intrinsic problems of structural heterogeneity and
an approach to their solution. The VLDB Journal, 8(1):25?43.
Oktie Hassanzadeh, Ken Q Pu, Soheil Hassas Yeganeh, Ren?ee J Miller, Lucian Popa, Mauricio A Hern?andez,
and Howard Ho. 2013. Discovering linkage points over web data. Proceedings of the VLDB Endowment,
6(6):445?456.
Yulan He and Deyu Zhou. 2011. Self-training from labeled features for sentiment analysis. Information Process-
ing & Management, 47(4):606?616.
Robert Isele and Christian Bizer. 2012. Learning expressive linkage rules using genetic programming. Proceed-
ings of the VLDB Endowment, 5(11):1638?1649.
Grzegorz Kondrak. 2005. N-gram similarity and distance. In String Processing and Information Retrieval, pages
115?126. Springer.
John R Koza, Forrest H Bennett III, and Oscar Stiffelman. 1999. Genetic programming as a Darwinian invention
machine. Springer.
Vladimir I Levenshtein. 1966. Binary codes capable of correcting deletions, insertions and reversals. In Soviet
physics doklady, volume 10, page 707.
Ming Li and Zhi-Hua Zhou. 2005. Setred: Self-training with editing. In Advances in Knowledge Discovery and
Data Mining, pages 611?621. Springer.
Andrew McCallum, Kamal Nigam, and Lyle H Ungar. 2000. Efficient clustering of high-dimensional data sets
with application to reference matching. In Proceedings of the sixth ACM SIGKDD international conference on
Knowledge discovery and data mining, pages 169?178. ACM.
Un Yong Nahm, Mikhail Bilenko, and Raymond J Mooney. 2002. Two approaches to handling noisy variation in
text mining. In Proceedings of the ICML-2002 workshop on text learning (TextML2002), pages 18?27. Citeseer.
Saul B Needleman and Christian D Wunsch. 1970. A general method applicable to the search for similarities in
the amino acid sequence of two proteins. Journal of molecular biology, 48(3):443?453.
Mattis Neiling. 2006. Identification of real-world objects in multiple databases. In From Data and Information
Analysis to Knowledge Engineering, pages 63?74. Springer.
Axel-Cyrille Ngonga Ngomo and Klaus Lyko. 2013. Unsupervised learning of link specifications: Deterministic
vs. non-deterministic. Ontology Matching, page 25.
Byung-Won On, Nick Koudas, Dongwon Lee, and Divesh Srivastava. 2007. Group linkage. In Data Engineering,
2007. ICDE 2007. IEEE 23rd International Conference on, pages 496?505. IEEE.
John Ross Quinlan. 1993. C4. 5: programs for machine learning, volume 1. Morgan kaufmann.
Ellen Riloff, Janyce Wiebe, and Theresa Wilson. 2003. Learning subjective nouns using extraction pattern boot-
strapping. In Proceedings of the seventh conference on Natural language learning at HLT-NAACL 2003-Volume
4, pages 25?32. Association for Computational Linguistics.
Chuck Rosenberg, Martial Hebert, and Henry Schneiderman. 2005. Semi-supervised self-training of object detec-
tion models.
Elke Rundensteiner. 1999. Special issue on data transformation. IEEE Techn. Bull. Data Engineering, 22(1).
Sheila Tejada, Craig A Knoblock, and Steven Minton. 2002. Learning domain-independent string transformation
weights for high accuracy object identification. In Proceedings of the eighth ACM SIGKDD international
conference on Knowledge discovery and data mining, pages 350?359. ACM.
William E Winkler. 1988. Using the em algorithm for weight computation in the fellegi-sunter model of record
linkage. In Proceedings of the Section on Survey Research Methods, American Statistical Association, volume
667, page 671.
William E Winkler. 2006. Overview of record linkage and current research directions. In Bureau of the Census.
Citeseer.
David Yarowsky. 1995. Unsupervised word sense disambiguation rivaling supervised methods. In Proceedings of
the 33rd annual meeting on Association for Computational Linguistics, pages 189?196. Association for Com-
putational Linguistics.
1321
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 56?65,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Jointly Modeling Aspects and Opinions with a MaxEnt-LDA Hybrid
Wayne Xin Zhao?, Jing Jiang?, Hongfei Yan?, Xiaoming Li?
?School of Electronics Engineering and Computer Science, Peking University, China
?School of Information Systems, Singapore Management University, Singapore
{zhaoxin,yhf}@net.pku.edu.cn, jingjiang@smu.edu.cn, lxm@pku.edu.cn
Abstract
Discovering and summarizing opinions from
online reviews is an important and challeng-
ing task. A commonly-adopted framework
generates structured review summaries with
aspects and opinions. Recently topic mod-
els have been used to identify meaningful re-
view aspects, but existing topic models do
not identify aspect-specific opinion words. In
this paper, we propose a MaxEnt-LDA hy-
brid model to jointly discover both aspects
and aspect-specific opinion words. We show
that with a relatively small amount of train-
ing data, our model can effectively identify as-
pect and opinion words simultaneously. We
also demonstrate the domain adaptability of
our model.
1 Introduction
With the dramatic growth of opinionated user-
generated content, consumers often turn to online
product reviews to seek advice while companies see
reviews as a valuable source of consumer feedback.
How to automatically understand, extract and sum-
marize the opinions expressed in online reviews has
therefore become an important research topic and
gained much attention in recent years (Pang and Lee,
2008). A wide spectrum of tasks have been studied
under review mining, ranging from coarse-grained
document-level polarity classification (Pang et al,
2002) to fine-grained extraction of opinion expres-
sions and their targets (Wu et al, 2009). In partic-
ular, a general framework of summarizing reviews
of a certain product is to first identify different as-
pects (a.k.a. features) of the given product and then
extract specific opinion expressions for each aspect.
For example, aspects of a restaurant may include
food, staff, ambience and price, and opinion expres-
sions for staff may include friendly, rude, etc. Be-
cause of the practicality of this structured summary
format, it has been adopted in several previous stud-
ies (Hu and Liu, 2004; Popescu and Etzioni, 2005;
Brody and Elhadad, 2010) as well as some commer-
cial systems, e.g. the ?scorecard? feature at Bing
shopping1.
Different approaches have been proposed to iden-
tify aspect words and phrases from reviews. Previ-
ous methods using frequent itemset mining (Hu and
Liu, 2004) or supervised learning (Jin and Ho, 2009;
Jin et al, 2009; Wu et al, 2009) have the limitation
that they do not group semantically related aspect
expressions together. Supervised learning also suf-
fers from its heavy dependence on training data. In
contrast, unsupervised, knowledge-lean topic mod-
eling approach has been shown to be effective in au-
tomatically identifying aspects and their representa-
tive words (Titov and McDonald, 2008; Brody and
Elhadad, 2010). For example, words such as waiter,
waitress, staff and service are grouped into one as-
pect.
We follow this promising direction and extend ex-
isting topic models to jointly identify both aspect
and opinion words, especially aspect-specific opin-
ion words. Current topic models for opinion mining,
which we will review in detail in Section 2, still lack
this ability. But separating aspect and opinion words
can be very useful. Aspect-specific opinion words
can be used to construct a domain-dependent senti-
1http://www.bing.com/shopping
56
ment lexicon and applied to tasks such as sentiment
classification. They can also provide more informa-
tive descriptions of the product or service being re-
viewed. For example, using more specific opinion
words such as cozy and romantic to describe the am-
bience aspect in a review summary is more meaning-
ful than using generic words such as nice and great.
To the best of our knowledge, Brody and Elhadad
(2010) are the first to study aspect-specific opinion
words, but their opinion word detection is performed
outside of topic modeling, and they only consider
adjectives as possible opinion words.
In this paper, we propose a new topic modeling
approach that can automatically separate aspect and
opinion words. A novelty of this model is the inte-
gration of a discriminative maximum entropy (Max-
Ent) component with the standard generative com-
ponent. The MaxEnt component allows us to lever-
age arbitrary features such as POS tags to help sepa-
rate aspect and opinion words. Because the supervi-
sion relies mostly on non-lexical features, although
our model is no longer fully unsupervised, the num-
ber of training sentences needed is relatively small.
Moreover, training data can also come from a differ-
ent domain and yet still remain effective, making our
model highly domain adaptive. Empirical evaluation
on large review data sets shows that our model can
effectively identify both aspects and aspect-specific
opinion words with a small amount of training data.
2 Related Work
Pioneered by the work of Hu and Liu (2004), review
summarization has been an important research topic.
There are usually two major tasks involved, namely,
aspect or feature identification and opinion extrac-
tion. Hu and Liu (2004) applied frequent itemset
mining to identify product features without supervi-
sion, and considered adjectives collocated with fea-
ture words as opinion words. Jin and Ho (2009),
Jin et al (2009) and Wu et al (2009) used super-
vised learning that requires hand-labeled training
sentences to identify both aspects and opinions. A
common limitation of these methods is that they do
not group semantically related aspect expressions to-
gether. Furthermore, supervised learning usually re-
quires a large amount of training data in order to per-
form well and is not easily domain adaptable.
Topic modeling provides an unsupervised and
knowledge-lean approach to opinion mining. Titov
and McDonald (2008) show that global topic models
such as LDA (Blei et al, 2003) may not be suitable
for detecting rateable aspects. They propose multi-
grain topic models for discovering local rateable as-
pects. However, they do not explicitly separate as-
pect and opinion words. Lin and He (2009) propose
a joint topic-sentiment model, but topic words and
sentiment words are still not explicitly separated.
Mei et al (2007) propose to separate topic and sen-
timent words using a positive sentiment model and
a negative sentiment model, but both models cap-
ture general opinion words only. In contrast, we
model aspect-specific opinion words as well as gen-
eral opinion words.
Recently Brody and Elhadad (2010) propose to
detect aspect-specific opinion words in an unsuper-
vised manner. They take a two-step approach by first
detecting aspect words using topic models and then
identifying aspect-specific opinion words using po-
larity propagation. They only consider adjectives as
opinion words, which may potentially miss opinion
words with other POS tags. We try to jointly capture
both aspect and opinion words within topic models,
and we allow non-adjective opinion words.
Another line of related work is about how to in-
corporate useful features into topic models (Zhu and
Xing, 2010; Mimno and McCallum, 2008). Our
MaxEnt-LDA hybrid bears similarity to these recent
models but ours is designed for opinion mining.
3 Model Description
Our model is an extension of LDA (Blei et al, 2003)
but captures both aspect words and opinion words.
To model the aspect words, we use a modified ver-
sion of the multi-grain topic models from (Titov and
McDonald, 2008). Our model is simpler and yet still
produces meaningful aspects. Specifically, we as-
sume that there are T aspects in a given collection of
reviews from the same domain, and each review doc-
ument contains a mixture of aspects. We further as-
sume that each sentence (instead of each word as in
standard LDA) is assigned to a single aspect, which
is often true based on our observation.
To understand how we model the opinion words,
let us first look at two example review sentences
57
from the restaurant domain:
The food was tasty.
The waiter was quite friendly.
We can see that there is a strong association of
tasty with food and similarly of friendly with waiter.
While both tasty and friendly are specific to the
restaurant domain, they are each associated with
only a single aspect, namely food and staff, respec-
tively. Besides these aspect-specific opinion words,
we also see general opinion words such as great
in the sentence ?The food was great!? These gen-
eral opinion words are shared across aspects, as op-
posed to aspect-specific opinion words which are
used most commonly with their corresponding as-
pects. We therefore introduce a general opinion
model and T aspect-specific opinion models to cap-
ture these different opinion words.
3.1 Generative Process
We now describe the generative process of the
model. First, we draw several multinomial word dis-
tributions from a symmetric Dirichlet prior with pa-
rameter ?: a background model ?B, a general aspect
model ?A,g, a general opinion model ?O,g, T as-
pect models {?A,t}Tt=1 and T aspect-specific opin-
ion models {?O,t}Tt=1. All these are multinomial
distributions over the vocabulary, which we assume
has V words. Then for each review document d, we
draw a topic distribution ?d?Dir(?) as in standard
LDA. For each sentence s in document d, we draw
an aspect assignment zd,s?Multi(?d).
Now for each word in sentence s of document d,
we have several choices: The word may describe the
specific aspect (e.g. waiter for the staff aspect), or a
general aspect (e.g. restaurant), or an opinion either
specific to the aspect (e.g. friendly) or generic (e.g.
great), or a commonly used background word (e.g.
know). To distinguish between these choices, we in-
troduce two indicator variable, yd,s,n and ud,s,n, for
the nth word wd,s,n. We draw yd,s,n from a multi-
nomial distribution over {0, 1, 2}, parameterized by
pid,s,n. yd,s,n determines whether wd,s,n is a back-
ground word, aspect word or opinion word. We will
discuss how to set pid,s,n in Section 3.2. We draw
ud,s,n from a Bernoulli distribution over {0, 1} pa-
rameterized by p, which in turn is drawn from a sym-
metric Beta(?). ud,s,n determines whether wd,s,n is
general or aspect-specific. We then draw wd,s,n as
T
?
?
B
?
A,t
?
O,t
?
A,g
?
O,g
D
S
N
d,s
x
d,s,n
pi
d,s,n
y
d,s,n
w
d,s,n
u
d,s,n
z
d,s
?
d
{B,O,A}
? p ?
?
Figure 1: The plate notation of our model.
follows:
wd,s,n ?
?
??????
??????
Multi(?B) if yd,s,n = 0
Multi(?A,zd,s) if yd,s,n = 1, ud,s,n = 0
Multi(?A,g) if yd,s,n = 1, ud,s,n = 1
Multi(?O,zd,s) if yd,s,n = 2, ud,s,n = 0
Multi(?O,g) if yd,s,n = 2, ud,s,n = 1
.
Figure 1 shows our model using the plate notation.
3.2 Setting pi with a Maximum Entropy Model
A simple way to set pid,s,n is to draw it from a
symmetric Dirichlet prior. However, as suggested
in (Mei et al, 2007; Lin and He, 2009), fully un-
supervised topic models are unable to identify opin-
ion words well. An important observation we make
is that aspect words and opinion words usually play
different syntactic roles in a sentence. Aspect words
tend to be nouns while opinion words tend to be ad-
jectives. Their contexts in sentences can also be dif-
ferent. But we do not want to use strict rules to sepa-
rate aspect and opinion words because there are also
exceptions. E.g. verbs such as recommend can also
be opinion words.
In order to use information such as POS tags
to help discriminate between aspect and opinion
words, we propose a novel idea as follows: We set
pid,s,n using a maximum entropy (MaxEnt) model
applied to a feature vector xd,s,n associated with
wd,s,n. xd,s,n can encode any arbitrary features we
think may be discriminative, e.g. previous, current
and next POS tags. Formally, we have
p(yd,s,n = l|xd,s,n) = pid,s,nl =
exp (?l ? xd,s,n
)
?2
l?=0 exp
(?l? ? xd,s,n
) ,
58
where {?l}2l=0 denote the MaxEnt model weights
and can be learned from a set of training sentences
with labeled background, aspect and opinion words.
This MaxEnt-LDA hybrid model is partially in-
spired by (Mimno and McCallum, 2008).
As for the features included in x, currently we
use two types of simple features: (1) lexical features
which include the previous, the current and the next
words {wi?1, wi, wi+1}, and (2) POS tag features
which include the previous, the current and the next
POS tags {POSi?1, POSi, POSi+1}.
3.3 Inference
We use Gibbs sampling to perform model inference.
Due to the space limit, we leave out the derivation
details and only show the sampling formulas. Note
that the MaxEnt component is trained first indepen-
dently of the Gibbs sampling procedure, that is, in
Gibbs sampling, we assume that the ? parameters
are fixed.
We use w to denote all the words we observe in
the collection, x to denote all the feature vectors for
these words, and y, z and u to denote all the hidden
variables. First, given the assignment of all other
hidden variables, to sample a value for zd,s, we use
the following formula:
P (zd,s = t|z?(d,s),y,u,w,x) ?
cd(t) + ?
cd(?) + T?
?
( ?
(
cA,t(?) + V ?
)
?
(
cA,t(?) + nA,t(?) + V ?
) ?
V?
v=1
?
(
cA,t(v) + nA,t(v) + ?
)
?
(
cA,t(v) + ?
)
)
?
( ?
(
cO,t(?) + V ?
)
?
(
cO,t(?) + nO,t(?) + V ?
) ?
V?
v=1
?
(
cO,t(v) + nO,t(v) + ?
)
?
(
cO,t(v) + ?
)
)
.
Here cd(t) is the number of sentences assigned to as-
pect t in document d, and cd(?) is the number of sen-
tences in document d. cA,t(v) is the number of times
word v is assigned as an aspect word to aspect t,
and cO,t(v) is the number of times word v is assigned
as an opinion word to aspect t. cA,t(?) is the total num-
ber of times any word is assigned as an aspect word
to aspect t, and cO,t(?) is the total number of times any
word is assigned as an opinion word to aspect t. All
these counts represented by a c variable exclude sen-
tence s of document d. nA,t(v) is the number of times
word v is assigned as an aspect word to aspect t in
sentence s of document d, and similarly, nO,t(v) is the
number of times word v is assigned as an opinion
word to aspect t in sentence s of document d.
Then, to jointly sample values for yd,s,n and
ud,s,n, we have
P (yd,s,n = 0|z,y?(d,s,n),u?(d,s,n),w,x)
? exp(?0 ? xd,s,n)?
l? exp(?l? ? xd,s,n)
?
cB(wd,s,n) + ?
cB(?) + V ?
,
P (yd,s,n = l, ud,s,n = b|z,y?(d,s,n),u?(d,s,n),w,x)
? exp(?l ? xd,s,n)?
l? exp(?l? ? xd,s,n)
? g(wd,s,n, zd,s, l, b),
where the function g(v, t, l, b) (1 ? v ? V, 1 ? t ?
T, l ? {1, 2}, b ? {0, 1}) is defined as follows:
g(v, t, l, b) =
?
??????????
??????????
cA,t(v) +?
cA,t(?) +V ?
? c(0)+?c(?)+2? if l = 1, b = 0
cO,t(v) +?
cO,t(?) +V ?
? c(0)+?c(?)+2? if l = 2, b = 0
cA,g(v) +?
cA,g(?) +V ?
? c(1)+?c(?)+2? if l = 1, b = 1
cO,g(v) +?
cO,g(?) +V ?
? c(1)+?c(?)+2? if l = 2, b = 1.
.
Here the various c variables denote various counts
excluding the nth word in sentence s of document d.
Due to space limit, we do not give full explanation
here.
4 Experiment Setup
To evaluate our MaxEnt-LDA hybrid model for
jointly modeling aspect and opinion words, we used
a restaurant review data set previously used in (Ganu
et al, 2009; Brody and Elhadad, 2010) and a ho-
tel review data set previously used in (Baccianella
et al, 2009). We removed stop words and used the
Stanford POS Tagger2 to tag the two data sets. Only
reviews that have no more than 50 sentences were
used. We also kept another version of the data which
includes the stop words for the purpose of extracting
the contextual features included in x. Some details
of the data sets are given in Table 1.
For our hybrid model, we ran 500 iterations of
Gibbs sampling. Following (Griffiths and Steyvers,
2004), we fixed the Dirichlet priors as follows: ? =
2http://nlp.stanford.edu/software/tagger.shtml
59
data set restaurant hotel
#tokens 1,644,923 1,097,739
#docs 52,574 14,443
Table 1: Some statistics of the data sets.
data set #sentences #tokens
restaurant 46 634
cell phone 125 4414
DVD player 180 3024
Table 2: Some statistics of the labeled training data.
50/T , ? = 0.1 and ? = 0.5. We also experimented
with other settings of these priors and did not notice
any major difference. For MaxEnt training, we tried
three labeled data sets: one that was taken from the
restaurant data set and manually annotated by us3,
and two from the annotated data set used in (Wu et
al., 2009). Note that the latter two were used for test-
ing domain adaptation in Section 6.3. Some details
of the training sets are shown in Table 2.
In our preliminary experiments, we also tried two
variations of our MaxEnt-LDA hybrid model. (1)
The first is a fully unsupervised model where we
used a uniform Dirichlet prior for pi. We found
that this unsupervised model could not separate as-
pect and opinion words well. (2) The second is a
bootstrapping version of the MaxEnt-LDA model
where we used the predicted values of y as pseudo
labels and re-trained the MaxEnt model iteratively.
We found that this bootstrapping procedure did not
boost the overall performance much and even hurt
the performance a little in some cases. Due to the
space limit we do not report these experiments here.
5 Evaluation
In this section we report the evaluation of our
model. We refer to our MaxEnt-LDA hybrid model
as ME-LDA. We also implemented a local version
of the standard LDA method where each sentence
is treated as a document. This is the model used
in (Brody and Elhadad, 2010) to identify aspects,
and we refer to this model as LocLDA.
Food Staff Order Taking Ambience
chocolate service wait room
dessert food waiter dining
cake staff wait tables
cream excellent order bar
ice friendly minutes place
desserts attentive seated decor
coffee extremely waitress scene
tea waiters reservation space
bread slow asked area
cheese outstanding told table
Table 4: Sample aspects of the restaurant domain using
LocLDA. Note that the words in bold are opinion words
which are mixed with aspect words.
5.1 Qualitative Evaluation
For each of the two data sets, we show four sample
aspects identified by ME-LDA in Table 3 and Ta-
ble 5. Because the hotel domain is somehow similar
to the restaurant domain, we used the labeled train-
ing data from the restaurant domain also for the hotel
data set. From the tables we can see that generally
aspect words are quite coherent and meaningful, and
opinion words correspond to aspects very well. For
comparison, we also applied LocLDA to the restau-
rant data set and present the aspects in Table 4. We
can see that ME-LDA and LocLDA give similar as-
pect words. The major difference between these two
models is that ME-LDA can sperate aspect words
and opinion words, which can be very useful. ME-
LDA is also able to separate general opinion words
from aspect-specific ones, giving more informative
opinion expressions for each aspect.
5.2 Evaluation of Aspects Identification
We also quantitatively evaluated the quality of the
automatically identified aspects. Ganu et al (2009)
provide a set of annotated sentences from the restau-
rant data set, in which each sentence has been as-
signed one or more labels from a gold standard label
set S = {Staff, Food, Ambience, Price, Anecdote,
Misc}. To evaluate the quality of our aspect iden-
tification, we chose from the gold standard labels
three major aspects, namely Staff, Food and Ambi-
ence. We did not choose the other aspects because
(1) Price is often mixed with other aspects such as
Food, and (2) Anecdote and Misc do not show clear
3We randomly selected 46 sentences for manual annotation.
60
Food Staff Order Taking Ambience General
Aspect Opinion Aspect Opinion Aspect Opinion Aspect Opinion Opinion
chocolate good service friendly table seated room small good
dessert best staff attentive minutes asked dining nice well
cake great food great wait told tables beautiful nice
cream delicious wait nice waiter waited bar romantic great
ice sweet waiter good reservation waiting place cozy better
desserts hot place excellent order long decor great small
coffee amazing waiters helpful time arrived scene open bad
tea fresh restaurant rude hour rude space warm worth
bread tasted waitress extremely manager sat area feel definitely
cheese excellent waitstaff slow people finally table comfortable special
Table 3: Sample aspects and opinion words of the restaurant domain using ME-LDA.
Service Room Condition Ambience Meal General
Aspect Opinion Aspect Opinion Aspect Opinion Aspect Opinion Opinion
staff helpful room shower room quiet breakfast good great
desk friendly bathroom small floor open coffee fresh good
hotel front bed clean hotel small fruit continental nice
english polite air comfortable noise noisy buffet included well
reception courteous tv hot street nice eggs hot excellent
help pleasant conditioning large view top pastries cold best
service asked water nice night lovely cheese nice small
concierge good rooms safe breakfast hear room great lovely
room excellent beds double room overlooking tea delicious better
restaurant rude bath well terrace beautiful cereal adequate fine
Table 5: Sample aspects and opinion words of the hotel domain using ME-LDA.
patterns in either word usage or writing styles, mak-
ing it even hard for humans to identify them. Brody
and Elhadad (2010) also only used these three as-
pects for quantitative evaluation. To avoid ambigu-
ity, we used only the single-labeled sentences for
evaluation. About 83% of the labeled sentences have
a single label, which confirms our observation that a
sentence usually belongs to a single aspect.
We first ran ME-LDA and LocLDA each to get
an inferred aspect set T . Following (Brody and El-
hadad, 2010), we set the number of aspects to 14
in both models. We then manually mapped each in-
ferred aspect to one of the six gold standard aspects,
i.e., we created a mapping function f(t) : T ? S.
For sentence s of document d, we first assign it to an
inferred aspect as follows:
t? = argmax
t?T
Nd,s?
n=1
logP (wd,s,n|t).
We then assign the gold standard aspect f(t?) to this
Aspect Method Precision Recall F-1
Staff LocLDA 0.804 0.585 0.677
ME-LDA 0.779 0.540 0.638
Food LocLDA 0.898 0.648 0.753
ME-LDA 0.874 0.787 0.828
Ambience LocLDA 0.603 0.677 0.638
ME-LDA 0.773 0.558 0.648
Table 6: Results of aspects identification on restaurant.
sentence. We then calculated the F-1 score of the
three aspects: Staff, Food and Ambience. The re-
sults are shown in Table 6. Generally ME-LDA has
given competitive results compared with LocLDA.
For Food and Ambience ME-LDA outperformed Lo-
cLDA, while for Staff ME-LDA is a little worse
than LocLDA. Note that ME-LDA is not designed
to compete with LocLDA for aspect identification.
61
5.3 Evaluation of Opinion Identification
Since the major advantage of ME-LDA is its abil-
ity to separate aspect and opinion words, we further
quantitatively evaluated the quality of the aspect-
specific opinion words identified by ME-LDA.
Brody and Elhadad (2010) has constructed a gold
standard set of aspect-specific opinion words for the
restaurant data set. In this gold standard set, they
manually judged eight out of the 14 automatically
inferred aspects they had: J = {Ambiance, Staff,
Food-Main Dishes, Atmosphere-Physical, Food-
Baked Goods, Food-General, Drinks, Service}.
Each word is assigned a polarity score ranging from
-2.0 to 2.0 in each aspect. We used their gold stan-
dard words whose polarity scores are not equal to
zero. Because their gold standard only includes
adjectives, we also manually added more opinion
words into the gold standard set. To do so, we took
the top 20 opinion words returned by our method
and two baseline methods, pooled them together,
and manually judged them. We use precision at n
(P@n), a commonly used metric in information re-
trieval, for evaluation. Because top words are more
important in opinion models, we set n to 5, 10 and
20. For both ME-LDA and BL-1 below, we again
manually mapped each automatically inferred aspect
to one of the gold standard aspects.
Since LocLDA does not identify aspect-specific
opinion words, we consider the following two base-
line methods that can identify aspect-specific opin-
ion words:
BL-1: In this baseline, we start with all adjectives
as candidate opinion words, and use mutual infor-
mation (MI) to rank these candidates. Specifically,
given an aspect t, we rank the candidate words ac-
cording to the following scoring function:
ScoreBL-1(w, t) =
?
v?Vt
p(w, v) log p(w, v)p(w)p(v) ,
where Vt is the set of the top-100 frequent aspect
words from ?A,t.
BL-2: In this baseline, we first use LocLDA to learn
a topic distribution for each sentence. We then as-
sign a sentence to the aspect with the largest proba-
bility and hence get sentence clusters. We manually
map these clusters to the eight gold standard aspects.
Finally, for each aspect we rank adjectives by their
Method P@5 P@10 P@20
ME-LDA 0.825?,? 0.700? 0.569?
BL-1 0.400 0.450 0.469
BL-2 0.725 0.650 0.563
Table 7: Average P@n of aspect-specific opinion words
on restaurant. * and ? indicate that the improvement hy-
pothesis is accepted at confidence level 0.9 respectively
for BL-1 and BL-2.
frequencies in the aspect and treat these as aspect-
specific opinion words.
The basic results in terms of the average precision
at n over the eight aspects are shown in Table 7. We
can see that ME-LDA outperformed the two base-
lines consistently. Especially, for P@5, ME-LDA
gave more than 100% relative improvement over
BL-1. The absolute value of 0.825 for P@5 also
indicates that top opinion words discovered by our
model are indeed meaningful.
5.4 Evaluation of the Association between
Opinion Words and Aspects
The evaluation in the previous section shows that our
model returns good opinion words for each aspect.
It does not, however, directly judge how aspect-
specific those opinion words are. This is because the
gold standard created by (Brody and Elhadad, 2010)
also includes general opinion words. E.g. friendly
and good may both be judged to be opinion words
for the staff aspect, but the former is more specific
than the latter. We suspect that BL-2 has comparable
performance with ME-LDA for this reason. So we
further evaluated the association between opinion
words and aspects by directly looking at how easy
it is to infer the corresponding aspect by only look-
ing at an aspect-specific opinion word. We selected
four aspects for evaluation: Ambiance, Staff, Food-
Main Dishes and Atmosphere-Physical . We chose
these four aspects because they are quite different
from each other and thus manual judgments on these
four aspects can be more objective. For each aspect,
similar to the pooling strategy in IR, we pooled the
top 20 opinion words identified by BL-1, BL-2 and
ME-LDA. We then asked two human assessors to
assign an association score to each of these words
as follows: If the word is closely associated with an
aspect, a score of 2 is given; if it is marginally as-
62
Metrics Dataset BL-2 ME-LDA
nDCG@5 Restaurant 0.647 0.764
Hotel 0.782 0.820
nDCG@10 Restaurant 0.781 0.897
Hotel 0.722 0.789
Table 8: Average nDCG performance of BL-2 and ME-
LDA. Because only four aspects were used for evaluation,
we did not perform statistical significance test. We found
that in all cases ME-LDA outperformed BL-2 for either
all aspects or three out of four aspects.
sociated with an aspect, a score of 1 is given; other-
wise, 0 is given. We calculated the Kappa statistics
of agreement, and we got a quite high Kappa value
of 0.8375 and 0.7875 respectively for the restaurant
data set and the hotel data set. Then for each word
in an aspect, we took the average of the scores of
the two assessors. We used an nDCG-like metric to
compare the performance of our model and of BL-2.
The metric is defined as follows:
nDCG@k(t,M) =
?k
i=1
Score(Mt,i)
log2(i+1)
iDCG@k(t) ,
where Mt,i is the ith aspect-specific opinion word
inferred by method M for aspect t, Score(Mt,i) is
the association score of this word, and iDCG@k(t)
is the score of the ideal DCG measure at k for as-
pect t, that is, the maximum DCG score assuming
an ideal ranking. We chose k = 5 and k = 10. The
average nDCG over the four aspects are presented
in Table 8. We can see that ME-LDA outperformed
BL-2 quite a lot for the restaurant data set, which
conforms to our hypothesis that ME-LDA generates
aspect-specific opinion words of stronger associa-
tion with aspects. For the hotel data set, ME-LDA
outperformed a little. This may be due to the fact
that we used the restaurant training data for the ho-
tel data set.
6 Further Analysis of MaxEnt
In this section, we perform some further evaluation
and analysis of the MaxEnt component in our model.
6.1 Feature Selection
Previous studies have shown that simple POS fea-
tures and lexical features can be very effective for
discovering aspect words and opinion words (Hu
Methods Average F-1
LocLDA 0.690
ME-LDA + A 0.631
ME-LDA + B 0.695
ME-LDA + C 0.705
Table 9: Comparison of the average F-1 using different
feature sets for aspect identification on restaurant.
and Liu, 2004; Jin et al, 2009; Wu et al, 2009;
Brody and Elhadad, 2010). for POS features, since
we observe that aspect words tend to be nouns while
opinion words tend to be adjectives but sometimes
also verbs or other part-of-speeches, we can expect
that POS features should be quite useful. As for lexi-
cal features, words from a sentiment lexicon can also
be helpful in discovering opinion words.
However, lexical features are more diverse so pre-
sumably we need more training data in order to de-
tect useful lexical features. Lexical features are also
more domain-dependent. On the other hand, we hy-
pothesize that POS features are more effective when
the amount of training data is small and/or the train-
ing data comes from a different domain. We there-
fore compare the following three sets of features:
? A: wi?1, wi, wi+1
? B: POSi?1, POSi, POSi+1
? C: A+ B
We show the comparison of the performance in Ta-
ble 9 using the average F-1 score defined in Sec-
tion 5.2 for aspect identification, and in Table 10 us-
ing the average P@n measure defined in Section 5.3
for opinion identification. We can see that Set B
plays the most important part, which conforms to
our hypothesis that POS features are very important
in opinion mining. In addition, we can see that Set C
performs a bit better than Set B, which indicates that
some lexical features (e.g., general opinion words)
may also be helpful. Note that here the training data
is from the same domain as the test data, and there-
fore lexical features are likely to be useful.
6.2 Examine the Size of Labeled Data
As we have seen, POS features play the major role
in discriminating between aspect and opinion words.
Because there are much fewer POS features than
word features, we expect that we do not need many
63
Methods P@5 P@10 P@20
BL-2 0.725 0.650 0.563
ME-LDA + A 0.150 0.200 0.231
ME-LDA + B 0.775 0.688 0.569
ME-LDA + C 0.825 0.700 0.569
Table 10: Comparison of the average P@n using different
feature sets for opinion identification on restaurant.
Method F-1
LocalLDA 0.690
ME-LDA + 10 0.629
ME-LDA + 20 0.692
ME-LDA + 30 0.691
ME-LDA + 40 0.726
ME-LDA + 46 0.705
Table 11: Average F-1 with differen sizes of training data
on restaurant.
labeled sentences to learn the POS-based patterns.
We now examine the sensitivity of the performance
with respect to the amount of labeled data. We gen-
erated four smaller training data sets with 10, 20, 30
and 40 sentences each from the whole training data
set we have, which consists of 46 labeled sentences.
The results are shown in Table 11 and Table 12. We
can see that generally the performance stays above
BL when the number of training sentences is 20 or
more. This indicates that our model needs only a
relatively small number of high-quality training sen-
tences to achieve good results.
6.3 Domain Adaption
Since we find that the MaxEnt supervision relies
more on POS features than lexical features, we also
hypothesize that if the training sentences come from
a different domain the performance can still remain
relatively high. To test this hypothesis, we tried two
Method P@5 P@10 P@20
BL-2 0.725 0.650 0.563
ME-LDA + 10 0.700 0.563 0.488
ME-LDA + 20 0.875 0.650 0.600
ME-LDA + 30 0.825 0.700 0.569
ME-LDA + 40 0.825 0.688 0.581
ME-LDA + 46 0.825 0.700 0.569
Table 12: Average P@n of aspect-specific opinion words
with differen sizes of training data on restaurant.
Method Average F-1
restaurant + B 0.695
restaurant + C 0.705
cell phone + B 0.662
cell phone + C 0.629
DVD player + B 0.686
DVD player + C 0.635
Table 13: Average F-1 performance for domain adaption
on restaurant.
Method P@5 P@10 P@20
restaurant + B 0.775 0.688 0.569
restaurant + C 0.825 0.700 0.569
cell phone + B 0.775 0.675 0.588
cell phone + C 0.750 0.688 0.594
DVD player + B 0.775 0.713 0.575
DVD player + C 0.825 0.663 0.588
Table 14: Average P@n of aspect-specific opinion words
for domain adaption on restaurant.
quite different training data sets, one from the cell
phone domain and the other from the DVD player
domain, both used in (Wu et al, 2009).
We consider two feature sets defined in Sec-
tion 6.1 for domain adaption, namely B and C. The
results are shown in Table 13 and Table 14.
For aspect identification, using out-of-domain
training data performed worse than using in-domain
training data, but the absolute performance is still
decent. And interestingly, we can see that using B
is better than using C, indicating that lexical features
may hurt the performance in the cross-domain set-
ting. It suggests that lexical features are not easily
adaptable across domains for aspect identification.
For opinion identification, we can see that there
is no clear difference between using out-of-domain
training data and using in-domain training data,
which may indicate that our opinion identification
component is robust in domain adaption. Also, we
cannot easily tell whetherB has advantage over C for
opinion identification. One possible reason may be
that those general opinion words are useful across
domains, so lexical features may still be useful for
domain adaption.
64
7 Conclusions
In this paper, we presented a topic modeling ap-
proach that can jointly identify aspect and opinion
words, using a MaxEnt-LDA hybrid. We showed
that by incorporating a supervised, discriminative
maximum entropy model into an unsupervised, gen-
erative topic model, we could leverage syntactic fea-
tures to help separate aspect and opinion words.
We evaluated our model on two large review data
sets from the restaurant and the hotel domains. We
found that our model was competitive in identifying
meaningful aspects compared with previous mod-
els. Most importantly, our model was able to iden-
tify meaningful opinion words strongly associated
with different aspects. We also demonstrated that
the model could perform well with a relatively small
amount of training data or with training data from a
different domain.
Our model provides a principled way to jointly
model both aspects and opinions. One of the future
directions we plan to explore is to use this model
to help sentence-level extraction of specific opinions
and their targets, which previously was only tackled
in a fully supervised manner. Another direction is to
extend the model to support polarity classification.
ACKNOWLEDGMENT
The authors Xin Zhao, Hongfei Yan and Xiaom-
ing Li are partially supported by NSFC under the
grant No. 70903008 and 60933004, CNGI grant No.
2008-122, 863 Program No. 2009AA01Z143, and
the Open Fund of the State Key Laboratory of Soft-
ware Development Environment under Grant No.
SKLSDE-2010KF-03, Beihang University.
References
Stefano Baccianella, Andrea Esuli, and Fabrizio Sebas-
tiani. 2009. Multi-facet rating of product reviews. In
Proceedings of the 31st ECIR.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet alocation. Journal of Machine
Learning Research, 3.
Samuel Brody and Noemie Elhadad. 2010. An unsuper-
vised aspect-sentiment model for online reviews. In
Proceedings of Human Language Technologies: The
Annual Conference of the North American Chapter of
the Association for Computational Linguistics.
Gayatree Ganu, Noemie Elhadad, and Amelie Marian.
2009. Beyond the stars: Improving rating predictions
using review text content. In Proceedings of the 12th
International Workshop on the Web and Databases.
Thomas L. Griffiths and Mark Steyvers. 2004. Find-
ing scientific topics. Proceedings of the National
Academy of Sciences of the United States of America.
Minqing Hu and Bing Liu. 2004. Mining and summa-
rizing customer reviews. In Proceedings of the 10th
ACM SIGKDD International Conference on Knowl-
edge Discovery and Data Mining.
Wei Jin and Hung Hay Ho. 2009. A novel lexicalized
HMM-based learning framework for web opinion min-
ing. In Proceedings of the 26th International Confer-
ence on Machine Learning.
Wei Jin, Hung Hay Ho, and Rohini K. Srihari. 2009.
OpinionMiner: A novel machine learning system for
web opinion mining and extraction. In Proceedings of
the 15th ACM SIGKDD.
Chenghua Lin and Yulan He. 2009. Joint senti-
ment/topic model for sentiment analysis. In Proceed-
ing of the Eighteenth ACM Conference on Information
and Knowledge Management.
Qiaozhu Mei, Xu Ling, Matthew Wondra, Hang Su, and
ChengXiang Zhai. 2007. Topic sentiment mixture:
Modeling facets and opinions in weblogs. In Proceed-
ings of the 16th International Conference on World
Wide Web.
David Mimno and Andrew McCallum. 2008.
Topic models conditioned on arbitrary features with
dirichlet-multinomial regression. In Conference on
Uncertainty in Artificial Intelligence.
Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Foundations and Trends in Infor-
mation Retrieval, 2(1-2).
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up? Sentiment classification using
machine learning techniques. In Proceedings of the
2002 Conference on Empirical Methods in Natural
Language Processing.
Ana-Maria Popescu and Oren Etzioni. 2005. Extracting
product features and opinions from reviews. In Pro-
ceedings of the HLT-EMNLP.
Ivan Titov and Ryan McDonald. 2008. Modeling online
reviews with multi-grain topic models. In Proceeding
of the 17th International Conference on World Wide
Web.
Yuanbin Wu, Qi Zhang, Xuangjing Huang, and Lide Wu.
2009. Phrase dependency parsing for opinion mining.
In Proceedings of the 2009 Conference on Empirical
Methods in Natural Language Processing.
Jun Zhu and Eric P. Xing. 2010. Conditional topic ran-
dom fields. In Proceedings of the 27th International
Conference on Machine Learning.
65
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 433?443,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Timeline Generation through Evolutionary Trans-Temporal Summarization
Rui Yan?, Liang Kong? , Congrui Huang?, Xiaojun Wan?, Xiaoming Li\, Yan Zhang??
?School of Electronics Engineering and Computer Science, Peking University, China
?Institute of Computer Science and Technology, Peking University, China
\State Key Laboratory of Virtual Reality Technology and Systems, Beihang University, China
{r.yan,kongliang,hcr,lxm}@pku.edu.cn,
wanxiaojun@icst.pku.edu.cn,zhy@cis.pku.edu.cn
Abstract
We investigate an important and challeng-
ing problem in summary generation, i.e.,
Evolutionary Trans-Temporal Summarization
(ETTS), which generates news timelines from
massive data on the Internet. ETTS greatly
facilitates fast news browsing and knowl-
edge comprehension, and hence is a neces-
sity. Given the collection of time-stamped web
documents related to the evolving news, ETTS
aims to return news evolution along the time-
line, consisting of individual but correlated
summaries on each date. Existing summariza-
tion algorithms fail to utilize trans-temporal
characteristics among these component sum-
maries. We propose to model trans-temporal
correlations among component summaries for
timelines, using inter-date and intra-date sen-
tence dependencies, and present a novel com-
bination. We develop experimental systems to
compare 5 rival algorithms on 6 instinctively
different datasets which amount to 10251 doc-
uments. Evaluation results in ROUGE metrics
indicate the effectiveness of the proposed ap-
proach based on trans-temporal information.
1 Introduction
Along with the rapid growth of the World Wide
Web, document floods spread throughout the Inter-
net. Given a large document collection related to
a news subject (for example, BP Oil Spill), readers
get lost in the sea of articles, feeling confused and
powerless. General search engines can rank these
?Corresponding author.
news webpages by relevance to a user specified as-
pect, i.e., a query such as ?first relief effort for BP
Oil Spill?, but search engines are not quite capable
of ranking documents given the whole news subject
without particular aspects. Faced with thousands of
news documents, people usually have a myriad of in-
terest aspects about the beginning, the development
or the latest situation. However, traditional infor-
mation retrieval techniques can only rank webpages
according to their understanding of relevance, which
is obviously insufficient (Jin et al, 2010).
Even if the ranked documents could be in a satis-
fying order to help users understand news evolution,
readers prefer to monitor the evolutionary trajecto-
ries by simply browsing rather than navigate every
document in the overwhelming collection. Summa-
rization is an ideal solution to provide an abbrevi-
ated, informative reorganization for faster and bet-
ter representation of news documents. Particularly,
a timeline (see Table 1) can summarize evolutionary
news as a series of individual but correlated com-
ponent summaries (items in Table 1) and offer an
option to understand the big picture of evolution.
With unique characteristics, summarizing time-
lines is significantly different from traditional sum-
marization methods which are awkward in such sce-
narios. We first study a manual timeline of BP Oil
Spill in Mexico Gulf in Table 1 from Reuters News1
to understand why timelines generation is observ-
ably different from traditional summarization. No
traditional method has considered to partition corpus
into subsets by timestamps for trans-temporal cor-
relations. However, we discover two unique trans-
1http://www.reuters.com
433
Table 1: Part of human generated timeline about BP Oil
Spill in 2010 from Reuters News website.
April 22, 2010
The Deepwater Horizon rig, valued at more than $560 million,
sinks and a five mile long (8 km) oil slick is seen.
April 25, 2010
The Coast Guard approves a plan to have remote underwater vehi-
cles activate a blowout preventer and stop leak. Efforts to activate
the blowout preventer fail.
April 28, 2010
The Coast Guard says the flow of oil is 5,000 barrels per day (bpd)
(210,000 gallons/795,000 litres) ? five times greater than first esti-
mated. A controlled burn is held on the giant oil slick.
April 29, 2010
U.S. President Barack Obama pledges ?every single available re-
source,? including the U.S. military, to contain the spreading spill.
Obama also says BP is responsible for the cleanup. Louisiana de-
clares state of emergency due to the threat to the state?s natural
resources.
April 30, 2010
An Obama aide says no drilling will be allowed in new areas, as the
president had recently proposed, until the cause of the Deepwater
Horizon accident is known.
temporal characteristics of component summaries
from the handcrafted timeline. Individuality. The
component summaries are summarized locally: the
component item on date t is constituted by sentences
with timestamp t. Correlativeness. The compo-
nent summaries are correlative across dates, based
on the global collection. To the best of our knowl-
edge, no traditional method has examined the rela-
tionships among these timeline items.
Although it is profitable, summarizing timeline
faces with new challenges:
? The first challenge for timeline generation is
to deliver important contents and avoid information
overlaps among component summaries under the
trans-temporal scenario based on global/local source
collection. Component items are individual but not
completely isolated due to the dynamic evolution.
? As we have individuality and correlativeness
to evaluate the qualities of component summaries,
both locally and globally, the second challenge is to
formulate the combination task into a balanced op-
timization problem to generate the timelines which
satisfy both standards with maximum utilities.
We introduce a novel approach for the web min-
ing problem Evolutionary Trans-Temporal Summa-
rization (ETTS). Taking a collection relevant to a
news subject as input, the system automatically out-
puts a timeline with items of component summaries
which represent evolutionary trajectories on specific
dates. We classify sentence relationships as inter-
date and intra-date dependencies. Particularly, the
inter-date dependency calculation includes temporal
decays to project sentences from all dates onto the
same time horizon (Figure 1 (a)). Based on intra-
/inter-date sentence dependencies, we then model
affinity and diversity to compute the saliency score
of each sentence and merge local and global rank-
ings into one unified ranking framework. Finally we
select top ranked sentences. We build an experimen-
tal system on 6 real datasets to verify the effective-
ness of our methods compared with other 4 rivals.
2 Related Work
Multi-document summarization (MDS) aims to pro-
duce a summary delivering the majority of informa-
tion content from a set of documents and has drawn
much attention in recent years. Conferences such as
ACL, SIGIR, EMNLP, etc., have advanced the tech-
nology and produced several experimental systems.
Generally speaking, MDS methods can be either
extractive or abstractive summarization. Abstractive
summarization (e.g. NewsBlaster2) usually needs
information fusion, sentence compression and refor-
mulation. We focus on extraction-based methods,
which usually involve assigning saliency scores to
some units (e.g. sentences, paragraphs) of the docu-
ments and extracting the units with highest scores.
To date, various extraction-based methods have
been proposed for generic multi-document summa-
rization. The centroid-based method MEAD (Radev
et al, 2004) is an implementation of the centroid-
based method that scores sentences based on fea-
tures such as cluster centroids, position, and TF.IDF,
etc. NeATS (Lin and Hovy, 2002) adds new features
such as topic signature and term clustering to select
important content, and use MMR (Goldstein et al,
1999) to remove redundancy.
Graph-based ranking methods have been pro-
posed to rank sentences/passages based on ?votes?
or ?recommendations? between each other. Tex-
tRank (Mihalcea and Tarau, 2005) and LexPageR-
ank (Erkan and Radev, 2004) use algorithms similar
to PageRank and HITS to compute sentence impor-
tance. Wan et al have improved the graph-ranking
2http://www1.cs.columbia.edu/nlp/newsblaster/
434
algorithm by differentiating intra-document and
inter-document links between sentences (2007b),
and have proposed a manifold-ranking method to
utilize sentence-to-sentence and sentence-to-topic
relationships (Wan et al, 2007a).
ETTS seems to be related to a very recent task of
?update summarization? started in DUC 2007 and
continuing with TAC. However, update summariza-
tion only dealt with a single update and we make a
novel contribution with multi-step evolutionary up-
dates. Further related work includes similar timeline
systems proposed by (Swan and Allan, 2000) us-
ing named entities, by (Allan et al, 2001) measured
in usefulness and novelty, and by (Chieu and Lee,
2004) measured in interest and burstiness. We have
proposed a timeline algorithm named ?Evolution-
ary Timeline Summarization (ETS)? in (Yan et al,
2011b) but the refining process based on generated
component summaries is time consuming. We aim
to seek for more efficient summarizing approach.
To the best of our knowledge, neither update sum-
marization nor traditional systems have considered
the relationship among ?component summaries?, or
have utilized trans-temporal properties. ETTS ap-
proach can also naturally and simultaneously take
into account global/local summarization with biased
information richness and information novelty, and
combine both summarization in optimization.
3 Trans-temporal Summarization
We conduct trans-temporal summarization based on
the global biased graph using inter-date dependency
and local biased graph using intra-date dependency.
Each graph is the complementary graph to the other.
3.1 Global Biased Summarization
The intuition for global biased summarization is that
the selected summary should be correlative with sen-
tences from neighboring dates, especially with those
informative ones. To generate the component sum-
mary on date t, we project all sentences in the collec-
tion onto the time horizon of t to construct a global
affinity graph, using temporal decaying kernels.
3.1.1 Temporal Proximity Based Projection
Clearly, a major technical challenge in ETTS is
how to define the temporal biased projection func-
tion ?(?t), where ?t is the distance between the
Figure 1: Construct global/local biased graphs. Solid cir-
cles denote intra-date sentences on the pending date t and
dash ones represent inter-date sentences from other dates.
Figure 2: Proximity-based kernel functions, where ?=10.
pending date t and neighboring date t?, i.e., ?t =
|t? ? t|. As in (Lv and Zhai, 2009), we present 5
representative kernel functions: Gaussian, Triangle,
Cosine, Circle, and Window, shown in Figure 2. Dif-
ferent kernels lead to different projections.
1. Gaussian kernel
?(?t) = exp[??t
2
2?2 ]
2. Triangle kernel
?(?t) =
{
1? ?t? if ?t ? ?
0 otherwise
3. Cosine (Hamming) kernel
?(?t) =
{
1
2 [1 + cos(?t?pi? )] if ?t ? ?
0 otherwise
4. Circle kernel
?(?t) =
{?
1? (?t? )2 if ?t ? ?
0 otherwise
435
5. Window kernel
?(?t) =
{
1 if ?t ? ?
0 otherwise
All kernels have one parameter ? to tune, which
controls the spread of kernel curves, i.e., it restricts
the projection scope of each sentence. In general,
the optimal setting of ? may vary according to the
news set because sentences presumably would have
wider semantic scope in certain news subjects, thus
requiring a higher value of ? and vice versa.
3.1.2 Modeling Global Affinity
Given the sentence collectionC partitioned by the
timestamp set T , C = {C1, C2, . . . , C |T |}, we ob-
tain Ct = {sti|1 ? i ? |Ct|} where si is a sentence
with the timestamp t = tsi . When we generate com-
ponent summary on t, we project all sentences onto
time horizon t. After projection, all sentences are
weighted by their influence on t. We use an affinity
matrix M t with the entry of the inter-date transition
probability on date t. The sum of each row equals to
1. Note that for the global biased matrix, we mea-
sure the affinity between local sentences from t and
global sentences from other dates. Therefore, intra-
date transition probability between sentences with
the timestamp t is set to 0 for local summarization.
M ti,j is the transition probability of si to sj based
on the perspective of date t, i.e., p(si ? sj |t):
p(si ? sj |t) =
{ f(si?sj |t)?
|C| f(si?sk|t)
if ? f 6= 0
0 if tsi = tsj = t
(1)
f(si ? sj |t) is defined as the temporal weighted
cosine similarity between two sentences:
f(si ? sj |t) =
?
w?si?sj
pi(w, si|t) ? pi(w, sj |t) (2)
where the weight pi associated with term w is calcu-
lated with the temporal weighted tf.isf formula:
pi(w, s|t) =
?|t? ts| ? tf(w, s)(1 + log( |C|Nw ))??
|s|(tf(w, s)(1 + log(
|C|
Nw )))2
.
(3)
where ts is the timestamp of sentence s, and
tf(w, s) is the term frequency of w in s. ts can be
any date from T . |C| is the sentences set size and
Nw is the number of sentences containing term w.
We let p(si ? si|t)=0 to avoid self transition.
Note that although f(.) is a symmetric function,
p(si ? sj |t) is usually not equal to p(sj ? si|t),
depending on the degrees of nodes si and sj .
Now we establish the affinity matrix M ti,j and by
using the general form of PageRank, we obtain:
~? = ?M?1~?+ 1? ?|C| ~e (4)
where ~? is the selective probability of all sentence
nodes and ~e is a column vector with all elements
equaling to 1. ? is the damping factor set as 0.85.
Usually the convergence of the iteration algorithm is
achieved when difference between the scores com-
puted at two successive iterations for any sentences
falls below a given threshold (0.0001 in this study).
3.1.3 Modeling Diversity
Diversity is to reflect both biased information
richness and sentence novelty, which aims to reduce
information redundancy. However, using standard
PageRank of Equation (4) will not result in diver-
sity. The aggregational effect of PageRank assigns
high salient scores to closely connected node com-
munities (Figure 3 (b)). A greedy vertex selection
algorithm may achieve diversity by iteratively se-
lecting the most prestigious vertex and then penal-
izing the vertices ?covered? by the already selected
ones, such as Maximum Marginal Relevance and its
applications in Wan et al (2007b; 2007a). Most re-
cently diversity rank DivRank is another solution
to diversity penalization in (Mei et al, 2010).
We incorporate DivRank in our general ranking
framework, which creates a dynamicM during each
iteration, rather than a static one. After z times of
iteration, the matrix M becomes:
M (z) = ?M (z?1) ? ~?(z?1) + 1? ?|C| ~e (5)
Equation (5) raises the probability for nodes with
higher centrality and nodes already having high
weights are likely to ?absorb? the weights of its
neighbors directly, and the weights of neighbors?
neighbors indirectly. The process is to iteratively ad-
just matrix M according to ~? and then to update ~?
according to the changed M . As iteration increases
436
there emerges a rich-gets-richer phenomenon (Fig-
ure 3 (c) and (d)). By incorporating DivRank, we
obtain rank r?i and the global biased ranking score
Gi for sentence si from date t to summarize Ct.
3.2 Local Biased Summarization
Naturally, the component summary for date t should
be informative within Ct. Given the sentence col-
lection Ct = {sti|1 ? i ? |Ct|}, we build an affin-
ity matrix for Figure 1 (b), with the entry of intra-
date transition probability calculated from standard
cosine similarity. We incorporate DivRank within
local summarization and we obtain the local biased
rank and ranking score for si, denoted as r?i and Li.
3.3 Optimization of Global/Local Combination
We do not directly add the global biased ranking
score and local biased ranking score, as many previ-
ous works did (Wan et al, 2007b; Wan et al, 2007a),
because even the same ranking score gap may indi-
cate different rank gaps in two ranking lists.
Given subset Ct, let R = {ri}(i = 1,. . . ,|Ct|), ri
is the final ranking of si to estimate, optimize the
following objective cost function O(R),
O(R) =?
|Ct|?
i=1
Gi?
ri
?i
? r
?
i
Gi
?2
+ ?
|Ct|?
i=1
Li?
ri
?i
? r
?
i
Li
?2
(6)
where Gi is the global biased ranking score while Li
is the local biased ranking score. ?i is expected to
be the merged ranking score, namely sentence im-
portance, which will be defined later. Among the
two components in the objective function, the first
component means that the refined rank should not
deviate too much from the global biased rank. We
use ? ri?i ?
r?i
Gi ?
2 instead of ?ri? r?i ?2 in order to dis-
tinguish the differences between sentences from the
same rank gap. The second component is similar by
refining rank from local biased summarization.
Our goal is to find R = R? to minimize the cost
function, i.e.,R? = argmin{O(R)}. R? is the final
rank merged by our algorithm. To minimize O(R),
we compute its first-order partial derivatives.
?O(R)
?ri
= 2??i
( Gi?i
ri ? r?i ) +
2?
?i
(Li?i
ri ? r?i ) (7)
Let ?O(R)?ri = 0, we get
r?i =
??ir?i + ??ir
?
i
?Gi + ?Li
(8)
Two special cases are that if (1) ? = 0, ? 6= 0:
we obtain ri = ?ir?i /Li, indicating we only use the
local ranking score. (2) ? 6= 0, ? = 0, indicating we
ignore local ranking score and only consider global
biased summarization using inter-date dependency.
There can be many ways to calculate the sen-
tence importance ?i. Here we define ?i as the
weighted combination of itself with ranking scores
from global biased and local biased summarization:
?(z)i =
?Gi + ?Li + ??(z?1)i
?+ ? + ? . (9)
To save one parameter we let ?+?+? = 1. In the z-
th iteration, r(z)i is dependent on ?(z?1)i and ?(z)i is
indirectly dependent on r(z)i via ?(z?1)i . ?(0)i = 0.
We iteratively approximate final ?i for the ultimate
rank listR?. The expectation of stable ?i is obtained
when ?(z)i = ?(z?1)i . Final ?i is expected to satisfy
?i = ?Gi + ?Li + ??i:
?i =
?Gi + ?Li
1? ? =
?Gi + ?Li
?+ ? (10)
Final ?i is dependent only on original global/local
biased ranking scores. Equation (8) becomes more
concise with no ? or ?: r? is a weighted combina-
tion of global and local ranks by ?? (? 6= 0, ? 6= 0):
r?i =
?
?+ ? r
?
i +
?
?+ ? r
?
i
= 11 + ?/?r
?
i +
1
1 + ?/? r
?
i
(11)
4 Experiments and Evaluation
4.1 Datasets
There is no existing standard test set for ETTS meth-
ods. We randomly choose 6 news subjects with
special coverage and handcrafted timelines by ed-
itors from 10 selected news websites: these 6 test
sets consist of news datasets and golden standards to
evaluate our proposed framework empirically, which
amount to 10251 news articles. As shown in Ta-
ble 2, three of the sources are in UK, one of them
437
(a) An illustrative network. (a) PageRank on t. (b) DivRank on t (c) DivRank on t?
Figure 3: An illustration of diverse ranking in a toy graph (a). Comparing (b) from general PageRank with (c),(d) from
DivRank, we find a better diversity by selecting {1,9} in (c) rather than {1,3} in (b). Moreover, (c) and (d) reflect
temporal biased processes on t {1,9} in (c) and t? {2,12} in (d).
is in China and the rest are in the US. We choose
these sites because many of them provide timelines
edited by professional editors, which serve as refer-
ence summaries. The news belongs to different cate-
gories of Rule of Interpretation (ROI) (Kumaran and
Allan, 2004). More detailed statistics are in Table 3.
Table 2: News sources of 6 datasets
News Sources Nation News Sources Nation
BBC UK Fox News US
Xinhua China MSNBC US
CNN US Guardian UK
ABC US New York Times US
Reuters UK Washington Post US
Table 3: Detailed basic information of 6 datasets.
News Subjects #size #docs #stamps #RT AL
1.Influenza A 115026 2557 331 5 83
2.Financial Crisis 176435 2894 427 2 118
3.BP Oil Spill 63021 1468 135 6 76
4.Haiti Earthquake 12073 247 83 2 32
5.Jackson Death 37819 925 168 3 64
6.Obama Presidency 79761 2160 349 5 92
size: the whole sentence counts; #stamps: the number of timestamps;
Note average size of subsets is calculated as: avg.size=#size/#stamps;
RT: reference timelines; AL: avg. length of RT measured in sentences.
4.2 Experimental System Setups
? Preprocessing. As ETTS faces with much larger
corpus compared with traditional MDS, we apply
further data preprocessing besides stemming and
stop-word removal. We extract text snippets repre-
senting atomic ?events? from all documents with a
toolkit provided by Yan et al (2010; 2011a), by
which we attempt to assign more fine-grained and
accurate timestamps for every sentence within the
text snippets. After the snippet extraction procedure,
we filter the corpora by discarding non-event texts.
? Compression Rate and Date Selection. After
preprocessing, we obtain numerous snippets with
fine-grained timestamps, and then decompose them
into temporally tagged sentences as the global col-
lection C. We partition C according to timestamps
of sentences, i.e., C = C1 ? C2 ? ? ? ? ? C |T |.
Each component summary is generated from its cor-
responding sub-collection. The sizes of component
summaries are not necessarily equal, and moreover,
not all dates may be represented, so date selection
is also important. We apply a simple mechanism
that users specify the overall compression rate ?, and
we extract more sentences for important dates while
fewer sentences for others. The importance of dates
is measured by the burstiness, which indicates prob-
able significant occurrences (Chieu and Lee, 2004).
The compression rate on ti is set as ?i = |Ci||C| .
4.3 Evaluation Metrics
The ROUGE measure is widely used for evaluation
(Lin and Hovy, 2003): the DUC contests usually of-
ficially employ ROUGE for automatic summariza-
tion evaluation. In ROUGE evaluation, the summa-
rization quality is measured by counting the num-
ber of overlapping units, such as N-gram, word se-
quences, and word pairs between the candidate time-
lines CT and the reference timelines RT . There are
several kinds of ROUGE metrics, of which the most
important one is ROUGE-N with 3 sub-metrics:
1 ROUGE-N-R is an N-gram recall metric:
ROUGE-N-R =
?
I?RT
?
N-gram?I
Countmatch(N-gram)
?
I?RT
?
N-gram?I
Count (N-gram)
438
2 ROUGE-N-P is an N-gram precision metric:
ROUGE-N-P =
?
I?CT
?
N-gram?I
Countmatch(N-gram)
?
I?CT
?
N-gram?I
Count (N-gram)
3 ROUGE-N-F is an N-gram F1 metric:
ROUGE-N-F = 2? ROUGE-N-P? ROUGE-N-RROUGE-N-P + ROUGE-N-R
I denotes a timeline. N in these metrics stands for
the length of N-gram and N-gram?RT denotes the
N-grams in reference timelines while N-gram?CT
denotes the N-grams in the candidate timeline.
Countmatch(N-gram) is the maximum number of N-
gram in the candidate timeline and in the set of ref-
erence timelines. Count(N-gram) is the number of N-
grams in reference timelines or candidate timelines.
According to (Lin and Hovy, 2003), among all
sub-metrics, unigram-based ROUGE (ROUGE-1)
has been shown to agree with human judgment most
and bigram-based ROUGE (ROUGE-2) fits summa-
rization well. We report three ROUGE F-measure
scores: ROUGE-1, ROUGE-2, and ROUGE-W,
where ROUGE-W is based on the weighted longest
common subsequence. The weight W is set to be
1.2 in our experiments by ROUGE package (version
1.55). Intuitively, the higher the ROUGE scores, the
similar the two summaries are.
4.4 Algorithms for Comparison
We implement the following widely used sum-
marization algorithms as baseline systems. They
are designed for traditional summarization without
trans-temporal dimension. The first intuitive way to
generate timelines by these methods is via a global
summarization on collection C and then distribu-
tion of selected sentences to their source dates. The
other one is via an equal summarization on all local
sub-collections. For baselines, we average both in-
tuitions as their performance scores. For fairness we
conduct the same preprocessing for all baselines.
Random: The method selects sentences ran-
domly for each document collection.
Centroid: The method applies MEAD algorithm
(Radev et al, 2004) to extract sentences according
to the following three parameters: centroid value,
positional value, and first-sentence overlap.
GMDS: The graph-based MDS proposed by
(Wan and Yang, 2008) first constructs a sentence
connectivity graph based on cosine similarity and
then selects important sentences based on the con-
cept of eigenvector centrality.
Chieu: (Chieu and Lee, 2004) present a simi-
lar timeline system with different goals and frame-
works, utilizing interest and burstiness ranking but
neglecting trans-temporal news evolution.
ETTS: ETTS is an algorithm with optimized
combination of global/local biased summarization.
RefTL: As we have used multiple human time-
lines as references, we not only provide ROUGE
evaluations of the competing systems but also of the
human timelines against each other, which provides
a good indicator as to the upper bound ROUGE
score that any system could achieve.
4.5 Overall Performance Comparison
We use a cross validation manner among 6 datasets,
i.e., train parameters on one subject set and exam-
ine the performance on the others. After 6 training-
testing processes, we take the average F-score per-
formance in terms of ROUGE-1, ROUGE-2, and
ROUGE-W on all sets. The overall results are shown
in Figure 4 and details are listed in Tables 4?6.
Figure 4: Overall performance on 6 datasets.
From the results, we have following observations:
? Random has the worst performance as expected.
? The results of Centroid are better than those of
Random, mainly because the Centroid method takes
439
Table 4: Overall performance comparison on Influenza
A (ROI? category: Science) and Financial Crisis (ROI
category: Finance). ?=0.4, kernel=Gaussian, ?=60.
1. Influenza A 2. Financial Crisis
Systems R-1 R-2 R-W R-1 R-2 R-W
RefTL 0.491 0.114 0.161 0.458 0.112 0.159
Random 0.257 0.039 0.081 0.230 0.030 0.071
Centroid 0.331 0.050 0.114 0.305 0.041 0.108
GMDS 0.364 0.062 0.130 0.327 0.054 0.110
Chieu 0.350 0.059 0.128 0.325 0.052 0.109
ETTS 0.375 0.071 0.132 0.339 0.058 0.112
Table 5: Overall performance comparison on BP Oil
(ROI category: Accidents) and Haiti Quake (ROI cate-
gory: Disasters). ?=0.4, kernel=Gaussian, ?=30.
3. BP Oil 4. Haiti Quake
Systems R-1 R-2 R-W R-1 R-2 R-W
RefTL 0.517 0.135 0.183 0.528 0.139 0.187
Random 0.262 0.041 0.096 0.266 0.043 0.093
Centroid 0.369 0.062 0.128 0.362 0.060 0.129
GMDS 0.389 0.084 0.139 0.380 0.106 0.137
Chieu 0.384 0.083 0.139 0.383 0.110 0.138
ETTS 0.441 0.107 0.158 0.436 0.111 0.145
Table 6: Overall performance comparison on Jackson
Death (ROI category: Legal Cases) and Obama Presi-
dency (ROI category: Politics). ?=0.4, kernel=Gaussian,
?=30.
5. Jackson Death 6. Obama Presidency
Systems R-1 R-2 R-W R-1 R-2 R-W
RefTL 0.482 0.113 0.161 0.495 0.115 0.163
Random 0.232 0.033 0.080 0.254 0.039 0.084
Centroid 0.320 0.051 0.109 0.325 0.053 0.111
GMDS 0.341 0.059 0.127 0.359 0.061 0.129
Chieu 0.344 0.059 0.128 0.346 0.060 0.125
ETTS 0.358 0.061 0.130 0.369 0.074 0.133
?ROI: news categorization defined by Linguistic Data Consortium.
into account positional value and first-sentence over-
lap, which facilitate main aspects summarization.
? The GMDS system outperforms centroid-based
summarization methods. This is due to the fact that
PageRank-based framework ranks the sentence us-
ing eigenvector centrality which implicitly accounts
for information subsumption among all sentences.
Traditional MDS only consider sentence selection
from either the global or the local scope, and hence
bias occurs. Mis-selected sentences result in a low
recall. Generally the performance of global priority
intuition (i.e. only global summarization and then
distribution to temporal subsets) is better than local
priority methods (only local summarization). Proba-
ble bias is enlarged by searching for worthy sentence
in single dates. However, precision drops due to ex-
cessive choice of global timeline-worthy sentences.
Figure 5: ?/?: global/local combination.
Figure 6: ? on long topics (?1 year).
Figure 7: ? on short topics (<1 year).
? In general, the result of Chieu is better than
Centroid but unexpectedly, worse than GMDS. The
reason may be that Chieu does not capture suffi-
cient timeline attributes. The ?interest? modeled
440
in the algorithms actually performs flat clustering-
based summarization which is proved to be less use-
ful (Wang and Li, 2010). GMDS utilizes sentence
linkage, and partly captures ?correlativeness?.
? ETTS under our proposed framework outper-
forms baselines, indicating that the properties we
use for timeline generation are beneficial. We also
add a direct comparison between ETTS and ETS
(Yan et al, 2011b). We notice that both balanced
algorithms achieve comparable performance (0.386
v.s. 0.412: a gap of 0.026 in terms of ROUGE-
1), but ETTS is much faster than ETS. It is under-
standable that ETS refines timelines based on neigh-
boring component summaries iteratively while for
ETTS neighboring information is incorporated in
temporal projection and hence there is no such pro-
cedure. Furthermore, ETS has 8 free parameters to
tune while ETTS has only 2 parameters. In other
words, ETTS is more simple to control.
? The performance on intensive focused news
within short time range (|last timestamp?first times-
tamp |<1 year) is better than on long lasting news.
Having proved the effectiveness of our proposed
methods, we carry the next move to identity how
global?local combination ratio ?/? and projection
kernels take effects to enhance the quality of a sum-
mary in parameter tuning.
4.6 Parameter Tuning
Each time we tune one parameter while others are
fixed. To identify how global and local biased sum-
marization combine, we provide experiments on the
performance of varying ?/? in Figure 5. Results in-
dicate that a balance between global and local biased
summarization is essential for timeline generation
because the performance is best when ?? ? [10, 100]and outperforms global and local summarization in
isolation, i.e., when ?=0 or ? = 0 in Figure 5. Inter-
estingly, we conclude an opposite observation com-
pared with ETS. Different approaches might lead to
different optimum of global/local combination.
Another key parameter ? measures the temporal
projection influence from global collection to local
collection and hence the size of neighboring sen-
tence set. 6 datasets are classified into two groups.
Subject 1, 2, 6 are grouped as long news with a time
span of more than one year and the others are short
news. The effect of ? varies on long news sets and
short news sets. In Figure 6 ? is best around 60 and
in Figure 7 it is best at about 20?40, indicating long
news has relatively wider semantic scope.
We then examine the effect of different projection
kernels. Generally, Gaussian kernel outperforms
others and window kernel is the worst, probably be-
cause Gaussian kernel provides the best smoothing
effect with no arbitrary cutoffs. Window kernel fails
to distinguish different weights of neighboring sets
by temporal proximity, so its performance is as ex-
pected. Other 3 kernels are comparable.
4.7 Sample Output and Case Study
Sample output is presented in Table 7 and it shares
major information similarity with the human time-
line in Table 1. Besides, we notice that a dynamic
?i is reasonable. Important burstiness is worthy of
more attention. Fewer sentences are selected on the
dates when nothing new occurs.
Interesting Findings. We notice that humans have
biases to generate timelines for they have (1) pref-
erence on local occurrences and (2) different writ-
ing styles. For instance, news outlets from United
States tend to summarize reactions by US govern-
ment while UK websites tend to summarize British
affairs. Some editors favor statistical reports while
others prefer narrative style, and some timelines
have detailed explanations while others are ex-
tremely concise with no more than two sentences for
each entry. Our system-generated timelines have a
large variance among all golden standards. Proba-
bly a new evaluation metric should be introduced to
measure the quality of human generated timelines
to mitigate the corresponding biases. A third in-
teresting observation is that subjects have different
volume patterns, e.g., H1N1 has a slow start and a
bursty evolution and BP Oil has a bursty start and a
quick decay. Obama is different in nature because
the report volume is temporally stable and scattered.
5 Conclusion
We present a novel solution for the important
web mining problem, Evolutionary Trans-Temporal
Summarization (ETTS), which generates trajectory
timelines for news subjects from massive data. We
formally formulate ETTS as a combination of global
and local summarization, incorporating affinity and
441
Table 7: Selected part of timeline generated by ETTS for BP Oil.
April 20, 2010
s1: An explosion on the Deepwater Horizon offshore oil drilling rig in
the Gulf of Mexico, around 40 miles south east of Louisiana, causing
several kills and injuries.
s2: The rig was drilling in about 5,000ft (1,525m) of water, pushing
the boundaries of deepwater drilling technology.
s3: The rig is owned and operated by Transocean, a company hired by
BP to carry out the drilling work.
s4: Deepwater Horizon oil rig fire leaves 11 missing.
April 22, 2010
s1: The US Coast Guard estimates that the rig is leaking oil at the rate
of up to 8,000 barrels a day.
s2: The Deepwater Horizon sinks to the bottom of the Gulf after burn-
ing for 36 hours, raising concerns of a catastrophic oil spill.
s3: Deepwater Horizon rig sinks in 5,000ft of water.
April 23, 2010
s1: The US coast guard suspends the search for missing workers, who
are all presumed dead.
s2: The Coast Guard says it had no indication that oil was leaking from
the well 5,000ft below the surface of the Gulf.
s3: Underwater robots try to shut valves on the blowout preventer to
stop the leak, but BP abandons that failed effort two weeks later.
s4: The US Coast Guard estimates that the rig is leaking oil at the rate
of up to 8,000 barrels a day.
s5: Deepwater Horizon clean-up workers fight to prevent disaster.
April 24, 2010
s1: Oil is found to be leaking from the well.
April 26, 2010
s1: BP?s shares fall 2% amid fears that the cost of cleanup and legal
claims will hit the London-based company hard.
s2: Roughly 15,000 gallons of dispersants and 21,000ft of containment
boom are placed at the spill site.
April 27, 2010
s1: BP reports a rise in profits, due in large part to oil price increases,
as shares rise again.
s2: The US departments of interior and homeland security announce
plans for a joint investigation of the explosion and fire.
s3: Minerals Management Service (MMS) approves a plan for two re-
lief wells.
s4: BP chairman Tony Hayward says the company will take full re-
sponsibility for the spill, paying for legitimate claims and cleanup cost.
April 28, 2010
s1: The coast guard says the flow of oil is 5,000bpd, five times greater
than first estimated, after a third leak is discovered.
s2: BP?s attempts to repair a hydraulic leak on the blowout preventer
valve are unsuccessful.
s3: BP reports that its first-quarter profits more than double to ?3.65
billion following a rise in oil prices.
s4: Controlled burns begin on the giant oil slick.
diversity into a unified ranking framework. We im-
plement a system under such framework for ex-
periments on real web datasets to compare all ap-
proaches. Through our experiment we notice that
the combination plays an important role in timeline
generation, and global optimization weights slightly
higher (?/? ? [10, 100]), but auxiliary local infor-
mation does help to enhance performance in ETTS.
Acknowledgments
This work was partially supported by NSFC with
Grant No.61073082, 60933004, 70903008 and
61073081, and Xiaojun Wan was supported by
NSFC with Grant No.60873155 and Beijing Nova
Program (2008B03).
References
James Allan, Rahul Gupta, and Vikas Khandelwal. 2001.
Temporal summaries of new topics. In Proceedings of
the 24th annual international ACM SIGIR conference
on Research and development in information retrieval,
SIGIR ?01, pages 10?18.
Hai Leong Chieu and Yoong Keok Lee. 2004. Query
based event extraction along a timeline. In Proceed-
ings of the 27th annual international ACM SIGIR con-
ference on Research and development in information
retrieval, SIGIR ?04, pages 425?432.
G. Erkan and D.R. Radev. 2004. Lexpagerank: Prestige
in multi-document text summarization. In Proceed-
ings of EMNLP, volume 4.
Jade Goldstein, Mark Kantrowitz, Vibhu Mittal, and
Jaime Carbonell. 1999. Summarizing text documents:
sentence selection and evaluation metrics. In Proceed-
ings of the 22nd annual international ACM SIGIR con-
ference on Research and development in information
retrieval, pages 121?128.
Xin Jin, Scott Spangler, Rui Ma, and Jiawei Han. 2010.
Topic initiator detection on the world wide web. In
Proceedings of the 19th international conference on
WWW?10, pages 481?490.
Giridhar Kumaran and James Allan. 2004. Text clas-
sification and named entities for new event detection.
In Proceedings of the 27th annual international ACM
SIGIR?04, pages 297?304.
Chin-Yew Lin and Eduard Hovy. 2002. From single
to multi-document summarization: a prototype system
and its evaluation. In Proceedings of the 40th An-
nual Meeting on Association for Computational Lin-
guistics, ACL ?02, pages 457?464.
Chin-Yew Lin and Eduard Hovy. 2003. Automatic evalu-
ation of summaries using n-gram co-occurrence statis-
tics. In Proceedings of the Human Language Technol-
ogy Conference of the NAACL?03, pages 71?78.
442
Yuanhua Lv and ChengXiang Zhai. 2009. Positional lan-
guage models for information retrieval. In Proceed-
ings of the 32nd international ACM SIGIR conference
on Research and development in information retrieval,
SIGIR ?09, pages 299?306.
Qiaozhu Mei, Jian Guo, and Dragomir Radev. 2010. Di-
vrank: the interplay of prestige and diversity in infor-
mation networks. In Proceedings of the 16th ACM
SIGKDD?10, pages 1009?1018.
R. Mihalcea and P. Tarau. 2005. A language indepen-
dent algorithm for single and multiple document sum-
marization. In Proceedings of IJCNLP, volume 5.
D.R. Radev, H. Jing, and M. Sty. 2004. Centroid-based
summarization of multiple documents. Information
Processing and Management, 40(6):919?938.
Russell Swan and James Allan. 2000. Automatic genera-
tion of overview timelines. In Proceedings of the 23rd
annual international ACM SIGIR?00, pages 49?56.
Xiaojun Wan and Jianwu Yang. 2008. Multi-document
summarization using cluster-based link analysis. In
Proceedings of the 31st annual international ACM SI-
GIR conference on Research and development in in-
formation retrieval, SIGIR ?08, pages 299?306.
X. Wan, J. Yang, and J. Xiao. 2007a. Manifold-ranking
based topic-focused multi-document summarization.
In Proceedings of IJCAI, volume 7, pages 2903?2908.
X. Wan, J. Yang, and J. Xiao. 2007b. Single document
summarization with document expansion. In Proceed-
ings of the 22nd AAAI?07, pages 931?936.
Dingding Wang and Tao Li. 2010. Document update
summarization using incremental hierarchical cluster-
ing. In Proceedings of the 19th ACM international
conference on Information and knowledge manage-
ment, CIKM ?10, pages 279?288.
Rui Yan, Yu Li, Yan Zhang, and Xiaoming Li. 2010.
Event recognition from news webpages through latent
ingredients extraction. In Information Retrieval Tech-
nology - 6th Asia Information Retrieval Societies Con-
ference, AIRS 2010, pages 490?501.
Rui Yan, Liang Kong, Yu Li, Yan Zhang, and Xiaoming
Li. 2011a. A fine-grained digestion of news webpages
through event snippet extraction. In Proceedings of
the 20th international conference companion on world
wide web, WWW ?11, pages 157?158.
Rui Yan, Xiaojun Wan, Jahna Otterbacher, Liang Kong,
Xiaoming Li, and Yan Zhang. 2011b. Evolution-
ary timeline summarization: a balanced optimization
framework via iterative substitution. In Proceedings of
the 34th annual international ACM SIGIR conference
on Research and development in information retrieval,
SIGIR ?11.
443
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1342?1351,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Summarize What You Are Interested In:
An Optimization Framework for Interactive Personalized Summarization
Rui Yan
Department of Computer
Science and Technology,
Peking University,
Beijing 100871, China
r.yan@pku.edu.cn
Jian-Yun Nie
De?partement d?informatique
et de recherche ope?rationnelle,
Universite? de Montre?al,
Montre?al, H3C 3J7 Que?bec, Canada
nie@iro.umontreal.ca
Xiaoming Li
Department of Computer
Science and Technology,
Peking University,
Beijing 100871, China
lxm@pku.edu.cn
Abstract
Most traditional summarization methods treat
their outputs as static and plain texts, which
fail to capture user interests during summa-
rization because the generated summaries are
the same for different users. However, users
have individual preferences on a particular
source document collection and obviously a
universal summary for all users might not al-
ways be satisfactory. Hence we investigate
an important and challenging problem in sum-
mary generation, i.e., Interactive Personalized
Summarization (IPS), which generates sum-
maries in an interactive and personalized man-
ner. Given the source documents, IPS captures
user interests by enabling interactive clicks
and incorporates personalization by model-
ing captured reader preference. We develop
experimental systems to compare 5 rival al-
gorithms on 4 instinctively different datasets
which amount to 5197 documents. Evalua-
tion results in ROUGE metrics indicate the
comparable performance between IPS and the
best competing system but IPS produces sum-
maries with much more user satisfaction ac-
cording to evaluator ratings. Besides, low
ROUGE consistency among these user pre-
ferred summaries indicates the existence of
personalization.
1 Introduction
In the era of information explosion, people need new
information to update their knowledge whilst infor-
mation on Web is updating extremely fast. Multi-
document summarization has been proposed to ad-
dress such dilemma by producing a summary de-
livering the majority of information content from a
document set, and hence is a necessity.
Traditional summarization methods play an im-
portant role with the exponential document growth
on the Web. However, for the readers, the impact of
human interests has seldom been considered. Tra-
ditional summarization utilizes the same methodol-
ogy to generate the same summary no matter who is
reading. However, users may have bias on what they
prefer to read due to their potential interests: they
need personalization. Therefore, traditional summa-
rization methods are to some extent insufficient.
Topic biased summarization tries for personaliza-
tion by pre-defining human interests as several gen-
eral categories, such as health or science. Readers
are required to select their possible interests before
summary generation so that the chosen topic has
priority during summarization. Unfortunately, such
topic biased summarization is not sufficient for two
reasons: (1) interests cannot usually be accurately
pre-defined by ambiguous topic categories and (2)
user interests cannot always be foreknown. Often
users do not really know what general ideas or detail
information they are interested in until they read the
summaries. Therefore, more flexible interactions
are required to establish personalization.
Due to all the insufficiencies of existed sum-
marization approaches, we introduce a new multi-
document summarization task of Interactive Person-
alized Summarization (IPS) and a novel solution for
the task. Taking a document collection as input, the
system outputs a summary aligned both with source
corpus and with user personalization, which is cap-
tured by flexible human?system interactions. We
1342
build an experimental system on 4 real datasets to
verify the effectiveness of our methods compared
with 4 rivals. The contribution of IPS is manifold
by addressing following challenges:
? The 1st challenge for IPS is to integrate user
interests into traditional summary components. We
measure the utilities of these components and com-
bine them. We formulate the task into a balanced
optimization framework via iterative substitution to
generate summaries with maximum overall utilities.
? The 2nd challenge is to capture user inter-
ests through interaction. We develop an interactive
mechanism of ?click? and ?examine? between read-
ers and summaries and address sparse data by ?click
smoothing? under the scenario of few user clicks.
We start by reviewing previous works. In Section
3 we provide IPS overview, describe user interac-
tion and optimize component combination with per-
sonalization. We conduct empirical evaluation and
demonstrate the experimental system in Section 4.
Finally we draw conclusions in Section 5.
2 Related Work
Multi-Document Summarization (MDS) has drawn
much attention in recent years and gained emphasis
in conferences such as ACL, EMNLP and SIGIR,
etc. General MDS can either be extractive or ab-
stractive. The former assigns salient scores to se-
mantic units (e.g. sentences, paragraphs) of the doc-
uments indicating their importance and then extracts
top ranked ones, while the latter demands informa-
tion fusion(e.g. sentence compression and reformu-
lation). Here we focus on extractive summarization.
Centroid-based method is one of the most popular
extractive summarization method. MEAD (Radev
et al, 2004) and NeATS (Lin and Hovy, 2002) are
such implementations, using position and term fre-
quency, etc. MMR (Goldstein et al, 1999) algorithm
is used to remove redundancy. Most recently, the
graph-based ranking methods have been proposed to
rank sentences or passages based on the ?votes? or
?recommendations? between each other. The graph-
based methods first construct a graph representing
the sentence relationships at different granularities
and then evaluate the saliency score of the sentences
based on the graph. TextRank (Mihalcea and Tarau,
2005) and LexPageRank (Erkan and Radev, 2004)
use algorithms similar to PageRank and HITS to
compute sentence importance. Wan et al improve
the graph-ranking algorithm by differentiating intra-
document and inter-document links between sen-
tences (2007b) and incorporate cluster information
in the graph model to evaluate sentences (2008).
To date, topics (or themes, clusters) in documents
have been discovered and used for sentence selec-
tion for topic biased summarization (Wan and Yang,
2008; Gong and Liu, 2001). Wan et al have
proposed a manifold-ranking method to make uni-
form use of sentence-to-sentence and sentence-to-
topic relationships to generate topic biased sum-
maries (2007a). Leuski et al in (2003) pre-define
several topic concepts, assuming users will foresee
their interested topics and then generate the topic
biased summary. However, such assumption is not
quite reasonable because user interests may not be
forecasted, or pre-defined accurately as we have ex-
plained in last section.
The above algorithms are usually traditional ex-
tensions of generic summarizers. They do not in-
volve interactive mechanisms to capture reader in-
terests, nor do they utilize user preference for per-
sonalization in summarization. Wan et al in (2008)
have proposed a summarization biased to neighbor-
ing reading context through anchor texts. How-
ever, such scenario does not apply to contexts with-
out human-edited anchor texts like Wikipedia they
have used. Our approach can naturally and simulta-
neously take into account traditional summary ele-
ments and user interests and combine both in opti-
mization under a wider practical scenario.
3 Interactive Personalized Summarization
Personalization based on user preference can be
captured via various alternative ways, such as eye-
tracking or mouse-tracking instruments used in (Guo
and Agichtein, 2010). In this study, we utilize inter-
active user clicks/examinations for personalization.
Unlike traditional summarization, IPS supports
human?system interaction by clicking into the sum-
mary sentences and examining source contexts. The
implicit feedback of user clicks indicates what they
are interested in and the system collects preference
information to update summaries if readers wish to.
We obtain an associated tuple <q, c> between a
1343
clicked sentence q and the examined contexts c.
As q has close semantic coherence with neigh-
boring contexts due to consistency in human natural
language, we consider a window of sentences cen-
tered at the clicked sentence q as c, which is a bag of
sentences. The window size k is a parameter to set.
However, click data is often sparse: users are not
likely to click more than 1/10 of total summary sen-
tences within a single generation. We amplify these
tiny hints of user interest by click smoothing.
We change the flat summary structure into a hi-
erarchical organization by extracting important se-
mantic units (denoted as u) and establishing link-
age between them. If the clicked sentence q con-
tains u, we diffuse the click impact to the correlated
units, which makes a single click perform as multi-
ple clicks and the sparse data is smoothed.
Problem Formulation
Input: Given the sentence collection D decom-
posed by documents, D = {s1, s2, . . . , s|D|} and
the clicked sentence record Q = {q1, q2, . . . }, we
generate summaries in sentences. A user click is
associated with a tuple <q, (u), c> where the exis-
tence of u depends on whether q contains u. The
collection of semantic units is denoted as M =
{u1, u2, . . . , u|M |}.
Output: A summary S as a set of sentences
{s1, s2, . . . , s|S|} and S ? D according to the pre-
specified compression rate ? (0 < ? < 1).
After the overview and formulation of IPS prob-
lem, we move on to the major components of User
Interaction and Personalized Summarization.
3.1 User Interaction
Hypertexify Summaries. We hypertexify the sum-
mary structure by establishing linkage between se-
mantic units. There are several possible formats for
semantic units, such as words or n-grams, etc. As
single words are proved to be not illustrative of se-
mantic meanings (Zhao et al, 2011) and n-grams are
rigid in length, we choose to extract semantic units
at a phrase granularity. Among all phrases from
source texts, some are of higher importance to at-
tract user interests, such as hot concepts or popu-
lar event names. We utilize the toolkit provided by
(Zhao et al, 2011) based on graph proximity LDA
(Blei et al, 2003) to extract key phrases and their
corresponding topic. A topic T is represented by
{(u1, pi(u1, T )), (u2, pi(u2, T )), . . . }where pi(u, T )
is the probability of u belonging to topic T . We in-
vert the topic-unit representation in Table 1, where
each u is represented as a topic vector. The corre-
lation corr(.) between ui, uj is measured by cosine
similarity sim(.) on topic distribution vector ~ui, ~uj .
corr(ui, uj) = simtopic(~ui, ~uj) (1)
Table 1: Inverted representation of topic-unit vector.
~u1 pi(u1, T1) pi(u1, T2) . . . pi(u1, Tn)
~u2 pi(u2, T1) pi(u2, T2) . . . pi(u2, Tn)
... ... ... ... ...
~u|M | pi(u|M |, T1) pi(u|M |, T2) . . . pi(u|M |, Tn)
When the summary is hypertexified by established
linkage, users click into the generated summary to
examine what they are interested in. A single click
on one sentence become multiple clicks via click
smoothing when the indicative function I(u|q) = 1.
I(u|q) =
{
1 q contains u;
0 otherwise. (2)
The click smoothing brings pseudo clicks q? asso-
ciated with u? and contexts c?. The entire user feed-
back texts A from q can be written as:
A(q) = I(u|q)
|M |?
j=1
corr(u?, u)(u?+? ?c?)+? ?c (3)
where ? is the weight tradeoff between u and asso-
ciated contexts c. If I(u|q) = 0, only the examined
context c is feedbacked for user preference; other-
wise, correlative contexts with u are taken into con-
sideration, which is a process of impact diffusion.
3.2 Personalized Summarization
Traditional summarization involves two essential re-
quirements: (1) coverage: the summary should
keep alignment with the source collection, which is
proved to be significant (Li et al, 2009). (2) di-
versity: according to MMR principle (Goldstein et
al., 1999) and its applications (Wan et al, 2007b;
Wan and Yang, 2008), a good summary should be
concise and contain as few redundant sentences as
possible, i.e., two sentences providing similar infor-
mation should not both present. According to our
1344
investigation, we observe that a well generated sum-
mary should properly consider a key component of
(3) user interests, which captures user preference to
summarize what they are interested in.
All above requirements involve a measurement
of similarity between two word distributions ?1
and ?2. Cosine, Kullback-Leibler divergence DKL
and Jensen Shannon divergence DJS are all able
to measure the similarity, but (Louis and Nenkova,
2009) indicate the superiority of DJS in summa-
rization task. We also introduce a pair of decreas-
ing/increasing logistic functions, L1(x) = 1/(1 +
ex) and L2(x) = ex/(1 + ex), to map the diver-
gence into interval [0,1]. V is the vocabulary set
and tf denotes the term frequency for word w.
DJS(?1||?2) =
1
2[DKL(?1||?2)+DKL(?2||?1)]
where
DKL(?1||?2) =
?
k?V
p(w|?1)log
p(w|?1)
p(w|?2)
where
p(w|?) = tf(w,?)?
w? tf(w?,?)
.
Modeling Interest for User Utility. Given a gener-
ated summary S, users tend to scrutinize texts rele-
vant to their interests. Texts related to user implicit
feedback are collected as A = ?|Q|i=1A(qi). Intu-
itively, the smaller distance between the word distri-
bution of final summary (?S) and the word distri-
bution of user preference (?A), the higher utility of
user interests Uuser(S) will be, i.e.,
Uuser(S) = L1(DJS(?S ||?A)). (4)
We model the utility of traditional summarization
Utrad(S) using a linear interpolation controlled by
parameter ? between utility from coverage Uc(S)
and utility Ud(S) from diversity:
Utrad(S) = Uc(S) + ? ? Ud(S). (5)
Coverage Utility. The summary should share a
closer word distribution with the source collection
(Allan et al, 2001; Li et al, 2009). A good summary
focuses on minimizing the loss of main information
from the whole collection D. Utility from coverage
Uc(S) is defined as follows and for coverage utility,
smaller divergence is desired.
Uc(S) = L1(DJS(?S ||?D)). (6)
Diversity Utility. Diversity measures the novelty
degree of any sentence s compared with all other
sentences within S, i.e., the distances between all
other sentences and itself. Diversity utility Ud(S) is
an average novelty score for all sentences in S. For
diversity utility, larger distance is desired, and hence
we use the increasing function L2 as follows:
Ud(S) =
1
|S|
?
s?S
L2(DJS(?s||?(S?s))). (7)
3.3 Balanced Optimization Framework
A well generated summary S should be sufficiently
aligned with the original source corpus, and also
be optimized given the user interests. The utility
of an individual summary U(S) is evaluated by the
weighted combination of these components, con-
trolled by parameter ? for balanced weights.
U(S) = Utrad(S) + ? ? Uuser(S) (8)
Given the sentence setD and the compression rate
?, there are ??|D| out of |D| possibilities to generate
S. The IPS task is to predict the optimized sentence
subset of S? from the space of all combinations. The
objective function is as follows:
S? = argmax
S
U(S). (9)
As U(S) is measured based on preferred interests
from user interaction within a generation in our sys-
tem, we extract S iteratively to approximate S?, i.e,
maximize U(S) based on the user feedbacks from
the interaction sessions. Each session is an iteration.
We use a similar framework as we have proposed in
(Yan et al, 2011).
During every session, the top ranked sentences are
strong candidates for the summary to generate and
the rank methodology is based on the metrics U(.).
The algorithm tends to highly rank sentences which
are with both coverage utility and interest utility, and
are diversified in balance: we rank each sentence s
according to U(s) under such metrics.
Consider S(n?1) generated in the (n-1)-th session
which consists of top ?|D| ranked sentences, as well
1345
as the top ?|D| ranked sentences in the n-th iteration
(denoted by O(n)), they have an intersection set of
Z(n) = Sn?1?On. There is a substitutable sentence
set X (n) = S(n?1) ?Z(n) and a new candidate sen-
tence set Y(n) = O(n) ? Z(n). We substitute x(n)
sentences with y(n), where x(n) ? X (n) and y(n)
? Y(n). During every iteration, our goal is to find a
substitutive pair <x,y> for S:
<x,y> : X ? Y ? R.
To measure the performance of such a substitu-
tion, a discriminant utility gain function ?Ux,y
?U (n)x(n),y(n) = U(S
(n))? U(S(n?1))
= U((S(n?1) ? x(n)) ? y(n))? U(S(n?1))
(10)
is employed to quantify the penalty. Therefore, we
predict the substitutive pair by maximizing the gain
function ?Ux,y over the state set R, with a size of?Y
k=0AkXCkY , where <x,y>? R. Finally the ob-
jective function of Equation (9) changes into maxi-
mization of utility gain by substitute x? with y? during
each iteration:
< x?, y? >= argmax
x?X ,y?Y
?Ux,y. (11)
Note that the objectives of interest utility opti-
mization and traditional utility optimization are not
always the same because the word distributions in
these texts are usually different. The substitutive
pair <x,y> may perform well based on the user
preference component while not on the traditional
summary part and vice versa. There is a tradeoff
between both user optimization and traditional opti-
mization and hence we need to balance them by ?.
The objective Equation (11) is actually to maxi-
mize ?U(S) from all possible substitutive pairs be-
tween two iteration sessions to generate S. The al-
gorithm is shown in Algorithm 1. The threshold  is
set at 0.001 in this study.
4 Experiments and Evaluation
4.1 Datasets
IPS can be tested on any document set but a tiny
corpus to summarize may not cover abundant effec-
tive interests to attract user clicks indicating their
Algorithm 1 Regenerative Optimization
1: Input: D, , ?
2: for all s ? D do
3: calculate Utrad(s)
4: end for
5: S ? top ?|D| ranked sentences
6: while new generation=TRUE do
7: collect clicks and update utility from U ? to U
8: if |U(S)? U ?(S)| >  then
9: for all s ? D do
10: calculate U(s)
11: end for
12: O ? top ?|D| ranked sentences by U(s)
13: Z ? S ? O
14: X ? S ?Z , Y ? O ?Z
15: for all <x,y> pair where x ? X ,y ? Y
do
16: ?Ux,y = U((S ? x) ? y)? U(S)
17: end for
18: < x?, y? >= argmax ?Ux,y
19: S ? (S ? x?) ? y?
20: end if
21: end while
preference. Besides, the scenario of small corpus is
not quite practical for the exponential growing web.
Therefore, we test IPS on large real world datasets.
We build 4 news story sets which consist of docu-
ments and reference summaries to evaluate our pro-
posed framework empirically. We downloaded 5197
news articles from 10 selected sources. As shown in
Table 2, three of the sources are in UK, one of them
is in China and the rest are in US. We choose them
because many of these websites provide handcrafted
summaries for their special reports, which serve as
reference summaries. These events belong to differ-
ent categories of Rule of Interpretation (ROI) (Ku-
maran and Allan, 2004). Statistics are in Table 3.
4.2 Experimental System Setups
? Preprocessing. Given a collection of documents,
we first decompose them into sentences. Stop-words
are removed and words stemming is performed.
Then the word distributions can be calculated.
? User Interface Design. Users are required to
specify the overall compression rate ? and the sys-
tem extracts ?|D| sentences according to user utility
1346
Figure 1: A demonstration system for Interactive Personalized Summarization when compression rate ? is specified
(e.g. 5%). For convenience of browsing, we number the selected sentences (see in part 3). Extracted semantic units,
such as ?drilling mud?, are in bold and underlined format (see in part 1). When the user clicks a sentence (part 4), the
clicked sentence ID is kept in the click record (part 2). Mis-clicked records revocation can be operated by clicking
the deletion icon ?X? (see in part 3). Once a sentence is clicked, user can track the sentence into the popup source
document to examine the contexts. The selected sentences are highlighted in the source documents (see in part 5).
Table 2: News sources of 4 datasets
News Sources Nation News Sources Nation
BBC UK Fox News US
Xinhua China MSNBC US
CNN US Guardian UK
ABC US New York Times US
Reuters UK Washington Post US
Table 3: Detailed basic information of 4 datasets.
News Subjects #size #docs #RS Avg.L
1.Influenza A 115026 2557 5 83
2.BP Oil Spill 63021 1468 6 76
3.Haiti Earthquake 12073 247 2 32
4.Jackson Death 37819 925 3 64
#size: total sentence counts; #RS: the number of reference summaries;
Avg.L: average length of reference summary measured in sentences.
and traditional utility. User utility is obtained from
interaction. The system keeps the clicked sentence
records and calculates the user feedback by Equa-
tion (3) during every session. Consider sometimes
users click into the summary due to confusion or
mis-operations, but not their real interests. The sys-
tem supports click records revocation. More details
of the user interface is demonstrated in Figure 1.
4.3 Evaluation Metrics
We include both subjective evaluation from 3 evalu-
ators based on their personalized interests and pref-
erence, and the objective evaluation based on the
widely used ROUGE metrics (Lin and Hovy, 2003).
Evaluator Judgments
Evaluators are requested to express an opinion
over all summaries based on the sentences which
they deem to be important for the news. In general
a summary can be rated in a 5-point scale, where
?1? for ?terrible?, ?2? for ?bad?, ?3? for ?normal?,
?4? for ?good? and ?5? for ?excellent?. Evaluators
are allowed to judge at any scores between 1 and 5,
e.g. a score of ?3.3? is adopted when the evaluator
feels difficult to decide whether ?3? or ?4? is more
1347
appropriate but with preference towards ?3?.
ROUGE Evaluation
The DUC usually officially employs ROUGE
measures for summarization evaluation, which mea-
sures summarization quality by counting overlap-
ping units such as the N-gram, word sequences, and
word pairs between the candidate summary and the
reference summary. We use ROUGE-N as follows:
ROUGE-N =
?
S?{RefSum}
?
N-gram?S
Countmatch(N-gram)
?
S?{RefSum}
?
N-gram?S
Count (N-gram)
whereN stands for the length of the N-gram and N-
gram?RefSum denotes the N-grams in the reference
summaries while N-gram?CandSum denotes the N-
grams in the candidate summaries. Countmatch(N-
gram) is the maximum number of N-gram in the
candidate summary and in the set of reference sum-
maries. Count(N-gram) is the number of N-grams in
the reference summaries or candidate summary.
According to (Lin and Hovy, 2003), among all
sub-metrics in ROUGE, ROUGE-N (N=1, 2) is rela-
tively simple and works well. In this paper, we eval-
uate our experiments using all methods provided by
the ROUGE package (version 1.55) and only report
ROUGE-1, since the conclusions drawn from differ-
ent methods are quite similar. Intuitively, the higher
the ROUGE scores, the similar two summaries are.
4.4 Algorithms for Comparison
We implement the following widely used multi-
document summarization algorithms as the baseline
systems, which are all designed for traditional sum-
marization without user interaction. For fairness we
conduct the same preprocessing for all algorithms.
Random: The method selects sentences ran-
domly for each document collection.
Centroid: The method applies MEAD algorithm
(Radev et al, 2004) to extract sentences according to
the following parameters: centroid value, positional
value, and first-sentence overlap.
GMDS: The Graph-based MDS proposed by
(Wan and Yang, 2008) first constructs a sentence
connectivity graph based on cosine similarity and
then selects important sentences based on the con-
cept of eigenvector centrality.
IPSini: The initial generated summary from IPS
merely models coverage and diversity utility, which
is similar to the previous work described in (Allan et
al., 2001) with different goals and frameworks.
IPS: Our proposed algorithms with personaliza-
tion component to capture interest by user feed-
backs. IPS generates summaries via iterative sen-
tence substitutions within user interactive sessions.
RefSum: As we have used multiple reference
summaries from websites, we not only provide
ROUGE evaluations of the competing systems but
also of the reference summaries against each other,
which provides a good indicator of not only the
upper bound ROUGE score that any system could
achieve, but also human inconsistency among refer-
ence summaries, indicating personalization.
4.5 Overall Performance Comparison
We take the average ROUGE-1 performance and hu-
man ratings on all sets. The overall results are shown
in Figure 2 and details are listed in Tables 4?6.
Figure 2: Overall performance on 6 datasets.
From the results, we have following observations:
? Random has the worst performance as expected,
both in ROUGE-1 scores and human judgements.
? The ROUGE-1 and human ratings of Centroid
and GMDS are better than those of Random. This is
mainly because the Centroid based algorithm takes
into account positional value and first-sentence over-
lap, which facilitates main aspects summarization
and PageRank-based GMDS ranks the sentence us-
ing eigenvector centrality which implicitly accounts
for information subsumption among all sentences.
? In general, the GMDS system slightly outper-
forms Centroid system in ROUGE-1, but the human
judgements of GMDS and Centroid are of no signifi-
cant difference. This is probably due to the difficulty
1348
Table 4: Overall performance comparison on Influenza A.
ROI? category: Science.
Systems R-1 95%-conf. H-1 H-2 H-3
RefSum 0.491 0.44958 3.5 3.0 3.9
Random 0.257 0.75694 1.2 1.0 1.0
Centroid 0.331 0.45073 2.5 3.0 3.5
GMDS 0.364 0.33269 3.0 2.7 3.5
IPSini 0.302 0.21213 2.0 2.5 2.5
IPS 0.337 0.46757 4.8 4.5 4.5
Table 5: Overall performance comparison on BP Oil
Leak. ROI category: Accidents.
Systems R-1 95%-conf. H-1 H-2 H-3
RefSum 0.517 0.48618 4.0 3.3 3.9
Random 0.262 0.64406 1.5 1.0 1.5
Centroid 0.369 0.34743 3.2 3.0 3.5
GMDS 0.389 0.43877 3.5 3.0 3.9
IPSini 0.327 0.53722 3.0 2.5 3.0
IPS 0.372 0.35681 4.8 4.5 4.5
Table 6: Overall performance comparison on Haiti Earth-
quake. ROI category: Disasters.
Systems R-1 95%-conf. H-1 H-2 H-3
RefSum 0.528 0.30450 3.8 4.0 4.0
Random 0.266 0.75694 1.5 1.5 1.8
Centroid 0.362 0.43045 3.6 3.0 4.0
GMDS 0.380 0.33694 3.9 3.5 4.0
IPSini 0.331 0.34120 2.8 2.5 3.0
IPS 0.391 0.40069 5.0 4.7 5.0
Table 7: Overall performance comparison on Michael
Jackson Death. ROI category: Legal Cases.
Systems R-1 95%-conf. H-1 H-2 H-3
RefSum 0.482 0.47052 3.5 3.5 4.0
Random 0.232 0.52426 1.2 1.0 1.5
Centroid 0.320 0.21045 3.0 2.5 2.7
GMDS 0.341 0.30070 3.5 3.3 3.9
IPSini 0.287 0.48526 2.5 2.0 2.2
IPS 0.324 0.36897 5.0 4.5 4.8
?ROI: news categorization defined by Linguistic Data Consortium.
Available at http://www.ldc.upenn.edu/projects/tdt4/annotation
of human judgements on comparable summaries.
? The results of ROUGE-1 and ratings for IPSini
are better than Random but worse than Centroid and
GMDS. The reason in this case may be that IPSini
does not capture sufficient attributes: coverage and
diversity are merely fundamental requirements.
? Traditional summarization considers sentence
selection based on corpus only, and hence neglects
Table 8: Ratings consistency between evaluators: mean
? standard deviation over the 4 datasets.
RefSum Evaluator 1 Evaluator 2 Evaluator 3
Evaluator 1 0.35?0.09 0.30?0.33
Evaluator 2 0.50?0.14
Random Evaluator 1 Evaluator 2 Evaluator 3
Evaluator 1 0.23?0.04 0.20?0.02
Evaluator 2 0.33?0.06
Centroid Evaluator 1 Evaluator 2 Evaluator 3
Evaluator 1 0.45?0.03 0.50?0.12
Evaluator 2 0.55?0.11
GMDS Evaluator 1 Evaluator 2 Evaluator 3
Evaluator 1 0.35?0.02 0.35?0.03
Evaluator 2 0.70?0.03
IPSini Evaluator 1 Evaluator 2 Evaluator 3
Evaluator 1 0.45?0.01 0.25?0.04
Evaluator 2 0.30?0.06
IPS Evaluator 1 Evaluator 2 Evaluator 3
Evaluator 1 0.35?0.01 0.18?0.02
Evaluator 2 0.28?0.04
user interests. Many sentences are extracted due to
arbitrary assumption of reader preference, which re-
sults in a low user satisfaction. Human judgements
under our proposed IPS framework greatly outper-
form baselines, indicating that the appropriate use
of human interests for summarization are beneficial.
The ROUGE-1 performance for IPS is not as ideal
as that of GMDS. This situation may result from the
divergence between user interests and general infor-
mation provided by mass media propaganda, which
again motivates the need for personalization.
Although the high disparities between different
human evaluators have been observed in (Gong and
Liu, 2001), we still examine the consistency among
3 evaluators and their preferred summaries to prove
the motivation of personalization in our work.
4.6 Consistency Analysis for Personalization
The low ROUGE-1 scores of RefSum indicate the
inconsistency among reference summaries. We con-
duct personalization analysis from two perspectives:
(1) human rating consistency and (2) content consis-
tency among human supervised summaries.
We calculate the mean and variance of rating vari-
ations among evaluator judgements, listed in Table
1349
Table 9: Content consistency among evaluators super-
vised summaries.
Evaluator 1 Evaluator 2 Evaluator 3
Evaluator 1 0.273 0.398
Evaluator 2 0.289 0.257
Evaluator 3 0.407 0.235
RefSum 0.365 0.302 0.394
8. We see that for Random the average rating vari-
ation is 0.25, for IPS is 0.27, for IPSini is 0.33, for
RefSum is 0.38, for GMDS is 0.47 and for Centroid
is the highest, 0.50. Such phenomenon indicates
for poor generated summaries, such as Random or
IPSini, humans have consensus, but for normal sum-
maries without personalized interests, they are likely
to have disparities, surprisingly, even for RefSum.
General summaries provided by mass media satisfy
part of audiences, but obviously not all of them.
The high rating consistency of IPS indicates peo-
ple tend to favor summaries generated according to
their interests. We next examine content consistency
of these summaries with high rating consistency.
As shown in Table 9, although highly scored,
these human supervised summaries still have low
content consistency (especially Evaluator 2). The
low content consistency between RefSum and su-
pervised summaries shows reader have individual
personalization. Note that the inconsistency among
evaluators is larger than that between RefSum and
supervised summaries, indicating interests take a
high proportion in evaluator supervised summaries.
4.7 Parameter Settings
? controls coverage/diversity tradeoff. We tune ? on
IPSini and apply the optimal ? directly in IPS. Ac-
cording to the statistics in (Yan et al, 2010), the se-
mantic coherent context is about 7 sentences. There-
fore, we empirically choose k=3 for the examined
context window. The number of topics is set at
n=50. We assign an equal weight (? = 1) to seman-
tic units and examined contexts according to analog-
ical research of summarization from implicit feed-
backs via clickthrough data (Sun et al, 2005).
? is the key parameter in IPS approach, control-
ling the weight of user utility during the process of
interactive personalized summarization.
Through Figure 3, we see that when ? is small
Figure 3: ? v.s. human ratings and ROUGE scores.
(? ? [0.01, 0.1]), both human judgements and
ROUGE evaluation scores have little difference.
When ? ? [0.1, 1], ROUGE scores increase signifi-
cantly but human satisfaction shows little response.
? ? [1, 10] brings large user utility enhancement be-
cause user may find what they are interested in but
ROUGE scores start to decay. When ? ? [10, 100],
ROUGE scores drop much because the emphasized
user interests may guide the generated summaries
divergent away from the original corpus.
In Figure 4 we examine how ? attracts user clicks
and regeneration counts until satisfaction. As the re-
sult indicates, both counts increase as ? increases.
When ? is small (from 0.01 to 0.1), readers find
no more interesting aspects through clicks and re-
generations and stop due to the bad user experience.
As ? increases, the system mines more relevant sen-
tences according to personalized interests and hence
attracts user clicks and intention to regenerate.
Figure 4: ? v.s. click counts and regeneration counts.
1350
5 Conclusion
We present an important and novel summariza-
tion problem, Interactive Personalized Summariza-
tion (IPS), which generates summaries based on
human?system interaction for ?interests? and per-
sonalization. We formally formulate IPS as a combi-
nation of user utility and traditional summary utility,
such as coverage and diversity. We implement a sys-
tem under such framework for experiments on real
web datasets to compare all approaches. Through
our experiments we notice that user personalization
of interests plays an important role in summary gen-
eration, which largely increase human ratings due to
user satisfaction. Besides, our experiments indicate
the inconsistency between user preferred summaries
and reference summaries measured by ROUGE, and
hence prove the effectiveness of personalization.
Acknowledgments
This work was partially supported by HGJ 2010
Grant 2011ZX01042-001-001 and NSFC with Grant
No.61073082, 60933004. Rui Yan was supported by
the MediaTek Fellowship.
References
James Allan, Rahul Gupta, and Vikas Khandelwal. 2001.
Temporal summaries of new topics. In Proceedings of
the 24th annual international SIGIR?01, pages 10?18.
D.M. Blei, A.Y. Ng, and M.I. Jordan. 2003. Latent
dirichlet alocation. The Journal of Machine Learning
Research, 3:993?1022.
G. Erkan and D.R. Radev. 2004. Lexpagerank: Prestige
in multi-document text summarization. In Proceed-
ings of EMNLP?04, volume 4.
Jade Goldstein, Mark Kantrowitz, Vibhu Mittal, and
Jaime Carbonell. 1999. Summarizing text documents:
sentence selection and evaluation metrics. In Proceed-
ings of SIGIR?99, pages 121?128.
Yihong Gong and Xin Liu. 2001. Generic text sum-
marization using relevance measure and latent seman-
tic analysis. In Proceedings of the 24th international
ACM SIGIR conference, SIGIR ?01, pages 19?25.
Q. Guo and E. Agichtein. 2010. Ready to buy or just
browsing?: detecting web searcher goals from inter-
action data. In Proceeding of the 33rd international
ACM SIGIR conference, SIGIR?10, pages 130?137.
Giridhar Kumaran and James Allan. 2004. Text clas-
sification and named entities for new event detection.
In Proceedings of the 27th annual international ACM
SIGIR?04, pages 297?304.
Anton Leuski, Chin-Yew Lin, and Eduard Hovy. 2003.
ineats: interactive multi-document summarization. In
Proceedings of ACL?03, pages 125?128.
Liangda Li, Ke Zhou, Gui-Rong Xue, Hongyuan Zha,
and Yong Yu. 2009. Enhancing diversity, cover-
age and balance for summarization through structure
learning. In Proceedings of WWW?09, pages 71?80.
Chin-Yew Lin and Eduard Hovy. 2002. From single
to multi-document summarization: a prototype system
and its evaluation. In Proceedings of ACL?02, pages
457?464.
Chin-Yew Lin and Eduard Hovy. 2003. Automatic evalu-
ation of summaries using n-gram co-occurrence statis-
tics. In Proceedings of NAACL?03, pages 71?78.
Annie Louis and Ani Nenkova. 2009. Automatically
evaluating content selection in summarization without
human models. In EMNLP?09, pages 306?314.
R. Mihalcea and P. Tarau. 2005. A language indepen-
dent algorithm for single and multiple document sum-
marization. In Proceedings of IJCNLP, volume 5.
D.R. Radev, H. Jing, and M. Sty. 2004. Centroid-based
summarization of multiple documents. Information
Processing and Management, 40(6):919?938.
Jian-Tao Sun, Dou Shen, Hua-Jun Zeng, Qiang Yang,
Yuchang Lu, and Zheng Chen. 2005. Web-page sum-
marization using clickthrough data. In Proceedings of
SIGIR?05, pages 194?201.
Stephen Wan and Ce?cile Paris. 2008. In-browser sum-
marisation: generating elaborative summaries biased
towards the reading context. In ACL-HLT?08, pages
129?132.
Xiaojun Wan and Jianwu Yang. 2008. Multi-document
summarization using cluster-based link analysis. In
Proceedings of SIGIR?08, pages 299?306.
X. Wan, J. Yang, and J. Xiao. 2007a. Manifold-ranking
based topic-focused multi-document summarization.
In Proceedings of IJCAI, volume 7, pages 2903?2908.
X. Wan, J. Yang, and J. Xiao. 2007b. Single document
summarization with document expansion. In Proceed-
ings of the 22nd AAAI?07, pages 931?936.
Rui Yan, Yu Li, Yan Zhang, and Xiaoming Li. 2010.
Event recognition from news webpages through latent
ingredients extraction. In AIRS?10, pages 490?501.
Rui Yan, Xiaojun Wan, Jahna Otterbacher, Liang Kong,
Xiaoming Li, and Yan Zhang. 2011. Evolution-
ary timeline summarization: a balanced optimization
framework via iterative substitution. In Proceedings
of the 34th annual international ACM SIGIR?11.
Xin Zhao, Jing Jiang, Jing He, Yang Song, Palakorn
Achanauparp, Ee-Peng Lim, and Xiaoming Li. 2011.
Topical Keyphrase Extraction from Twitter. In Pro-
ceedings of ACL-HLT?11.
1351
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 800?809, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
SSHLDA: A Semi-Supervised Hierarchical Topic Model
Xian-Ling Mao??, Zhao-Yan Ming?, Tat-Seng Chua?, Si Li?, Hongfei Yan??, Xiaoming Li?
?Department of Computer Science and Technology, Peking University, China
?School of Computing, National University of Singapore, Singapore
?School of ICE, Beijing University of Posts and Telecommunications, China
{xianlingmao,lxm}@pku.edu.cn, yhf@net.pku.edu.cn
{chuats,mingzhaoyan}@nus.edu.sg, lisi@bupt.edu.cn
Abstract
Supervised hierarchical topic modeling and
unsupervised hierarchical topic modeling are
usually used to obtain hierarchical topics, such
as hLLDA and hLDA. Supervised hierarchi-
cal topic modeling makes heavy use of the in-
formation from observed hierarchical labels,
but cannot explore new topics; while unsu-
pervised hierarchical topic modeling is able
to detect automatically new topics in the data
space, but does not make use of any informa-
tion from hierarchical labels. In this paper, we
propose a semi-supervised hierarchical topic
model which aims to explore new topics auto-
matically in the data space while incorporating
the information from observed hierarchical la-
bels into the modeling process, called Semi-
Supervised Hierarchical Latent Dirichlet Al-
location (SSHLDA). We also prove that hLDA
and hLLDA are special cases of SSHLDA.We
conduct experiments on Yahoo! Answers and
ODP datasets, and assess the performance in
terms of perplexity and clustering. The ex-
perimental results show that predictive ability
of SSHLDA is better than that of baselines,
and SSHLDA can also achieve significant im-
provement over baselines for clustering on the
FScore measure.
1 Introduction
Topic models, such as latent Dirichlet alation
(LDA), are useful NLP tools for the statistical anal-
ysis of document collections and other discrete data.
?This work was done in National University of Singapore.
?Corresponding author.
Furthermore, hierarchical topic modeling is able to
obtain the relations between topics ? parent-child
and sibling relations. Unsupervised hierarchical
topic modeling is able to detect automatically new
topics in the data space, such as hierarchical La-
tent Dirichlet Allocation (hLDA) (Blei et al2004).
hLDAmakes use of nested Dirichlet Process to auto-
matically obtain a L-level hierarchy of topics. Mod-
ern Web documents, however, are not merely col-
lections of words. They are usually documents with
hierarchical labels ? such as Web pages and their
placement in hierarchical directories (Ming et al
2010). Unsupervised hierarchical topic modeling
cannot make use of any information from hierarchi-
cal labels, thus supervised hierarchical topic models,
such as hierarchical Labeled Latent Dirichlet Allo-
cation (hLLDA) (Petinot et al2011), are proposed
to tackle this problem. hLLDA uses hierarchical la-
bels to automatically build corresponding topic for
each label, but it cannot find new latent topics in the
data space, only depending on hierarchy of labels.
As we know that only about 10% of an iceberg?s
mass is seen outside while about 90% of it is unseen,
deep down in water. We think that a corpus with hi-
erarchical labels should include not only observed
topics of labels, but also there are more latent top-
ics, just like icebergs. hLLDA can make use of the
information from labels; while hLDA can explore
latent topics. How can we combine the merits of the
two types of models into one model?
An intuitive and simple combinational method is
like this: first, we use hierarchy of labels as basic hi-
erarchy, called Base Tree (BT); then we use hLDA
to build automatically topic hierarchy for each leaf
800
node in BT, called Leaf Topic Hierarchy (LTH); fi-
nally, we add each LTH to corresponding leaf in the
BT and obtain a hierarchy for the entire dataset. We
refer the method as Simp-hLDA. The performance
of the Simp-hLDA is not so good, as can be seen
from the example in Figure 3 (b). The drawbacks
are: (i) the leaves in BT do not obtain reasonable
and right words distribution, such as ?Computers &
Internet? node in Figure 3 (b), its topical words, ?the
to you and a?, is not about ?Computers & Internet?;
(ii) the non-leaf nodes in BT cannot obtain words
distribution, such as ?Health? node in Figure 3 (b);
(iii) it is a heuristic method, and thus Simp-hLDA
has no solid theoretical basis.
To tackle the above drawbacks, we explore the
use of probabilistic models for such a task where
the hierarchical labels are merely viewed as a part
of a hierarchy of topics, and the topics of a path in
the whole hierarchy generate a corresponding doc-
ument. Our proposed generative model learns both
the latent topics of the underlying data and the la-
beling strategies in a joint model, by leveraging on
the hierarchical structure of labels and Hierarchical
Dirichlet Process.
We demonstrate the effectiveness of the proposed
model on large, real-world datasets in the question
answering and website category domains on two
tasks: the topic modeling of documents, and the use
of the generated topics for document clustering. Our
results show that our joint, semi-hierarchical model
outperforms the state-of-the-art supervised and un-
supervised hierarchical algorithms. The contribu-
tions of this paper are threefold: (1) We propose a
joint, generative semi-supervised hierarchical topic
model, i.e. Semi-Supervised Hierarchical Latent
Dirichlet Allocation (SSHLDA), to overcome the
defects of hLDA and hLLDA while combining the
their merits. SSHLDA is able to not only explore
new latent topics in the data space, but also makes
use of the information from the hierarchy of ob-
served labels; (2) We prove that hLDA and hLLDA
are special cases of SSHLDA; (3) We develop a
gibbs sampling inference algorithm for the proposed
model.
The remainder of this paper is organized as fol-
lows. We review related work in Section 2. In Sec-
tion 3, we introduce some preliminaries; while we
introduce SSHLDA in Section 4. Section 5 details
a gibbs sampling inference algorithm for SSHLDA;
while Section 6 presents the experimental results.
Finally, we conclude the paper and suggest direc-
tions for future research in Section 7.
2 Related Work
There have been many variations of topic mod-
els. The existing topic models can be divided
into four categories: Unsupervised non-hierarchical
topic models, Unsupervised hierarchical topic mod-
els, and their corresponding supervised counter-
parts.
Unsupervised non-hierarchical topic models are
widely studied, such as LSA (Deerwester et al
1990), pLSA (Hofmann, 1999), LDA (Blei et al
2003), Hierarchical-concept TM (Chemudugunta et
al., 2008c; Chemudugunta et al2008b), Corre-
lated TM (Blei and Lafferty, 2006) and Concept TM
(Chemudugunta et al2008a; Chemudugunta et al
2008b) etc. The most famous one is Latent Dirichlet
Allocation (LDA). LDA is similar to pLSA, except
that in LDA the topic distribution is assumed to have
a Dirichlet prior. LDA is a completely unsupervised
algorithm that models each document as a mixture
of topics. Another famous model that not only rep-
resents topic correlations, but also learns them, is
the Correlated Topic Model (CTM). Topics in CTM
are not independent; however it is noted that only
pairwise correlations are modeled, and the number
of parameters in the covariance matrix grows as the
square of the number of topics.
However, the above models cannot capture the
relation between super and sub topics. To address
this problem, many models have been proposed
to model the relations, such as Hierarchical LDA
(HLDA) (Blei et al2004), Hierarchical Dirichlet
processes (HDP) (Teh et al2006), Pachinko Allo-
cation Model (PAM) (Li and McCallum, 2006) and
Hierarchical PAM (HPAM) (Mimno et al2007)
etc. The relations are usually in the form of a hi-
erarchy, such as the tree or Directed Acyclic Graph
(DAG). Blei et al2004) proposed the hLDA model
that simultaneously learns the structure of a topic
hierarchy and the topics that are contained within
that hierarchy. This algorithm can be used to extract
topic hierarchies from large document collections.
Although unsupervised topic models are suffi-
801
ciently expressive to model multiple topics per doc-
ument, they are inappropriate for labeled corpora be-
cause they are unable to incorporate the observed la-
bels into their learning procedure. Several modifica-
tions of LDA to incorporate supervision have been
proposed in the literature. Two such models, Su-
pervised LDA (Blei and McAuliffe, 2007; Blei and
McAuliffe, 2010) and DiscLDA (Lacoste-Julien et
al., 2008) are first proposed to model documents as-
sociated only with a single label. Another category
of models, such as the MM-LDA (Ramage et al
2009b), Author TM (Rosen-Zvi et al2004), Flat-
LDA (Rubin et al2011), Prior-LDA (Rubin et al
2011), Dependency-LDA (Rubin et al2011) and
Partially LDA (PLDA) (Ramage et al2011) etc.,
are not constrained to one label per document be-
cause they model each document as a bag of words
with a bag of labels. However, these models obtain
topics that do not correspond directly with the la-
bels. Labeled LDA (LLDA) (Ramage et al2009a)
can be used to solve this problem.
None of these non-hierarchical supervised mod-
els, however, leverage on dependency structure,
such as parent-child relation, in the label space. For
hierarchical labeled data, there are also few models
that are able to handle the label relations in data.
To the best of our knowledge, only hLLDA (Petinot
et al2011) and HSLDA (Perotte et al2011) are
proposed for this kind of data. HSLDA cannot ob-
tain a probability distribution for a label. Although
hLLDA can obtain a distribution over words for each
label, hLLDA is unable to capture the relations be-
tween parent and child node using parameters, and it
also cannot detect automatically latent topics in the
data space. In this paper, we will propose a genera-
tive topic model to tackle these problems of hLLDA.
3 Preliminaries
The nested Chinese restaurant process (nCRP) is a
distribution over hierarchical partitions (Blei et al
2004). It generalizes the Chinese restaurant process
(CRP), which is a distribution over partitions. The
CRP can be described by the following metaphor.
Imagine a restaurant with an infinite number of ta-
bles, and imagine customers entering the restaurant
in sequence. The dth customer sits at a table accord-
Table 1: Notations used in the paper.
Sym Description
V Vocabulary (word set), w is a word in V
D Document collection
Tj
The set of paths in the sub-tree whose root is the
jth leaf node in the hierarchy of observed topics
m A document m that consists of words and labels
wm The text of document m, wi is ith words in w
cm The topic set of document m
com The set of topics with observed labels for document m
cem The set of topics without labels for document m
ce?m The set of latent topics for all documents other than m
zem
The assignment of the words in the mth document
to one of the latent topics
wem
The set of the words belonging to one of the latent
topics in the the mth document
zm,n
The assignment of the nth word in the mth document
to one of the L available topics
z The set of zm,n for all words in all documents
ci A topic in the ith level in the hierarchy
? The word distribution set for Z, i.e., {?}z?c
? Dirichlet prior of ?
?ci The multinomial distribution over the sub-topics of ci?1
?ci Dirichlet prior of ?ci
? Dirichlet prior of ?
? The multinomial distribution of words
?m The distributions over topics for document m
? The set for ?m, m ? {1, ..., D}
ing to the following distribution,
p(cd = k|c1:(d?1)) ?
{ mk if k is previous occupied
? if k is a new tabel, (1)
where mk is the number of previous customers sit-
ting at table k and ? is a positive scalar. AfterD cus-
tomers have sat down, their seating plan describes a
partition of D items.
In the nested CRP, imagine now that tables are or-
ganized in a hierarchy: there is one table at the first
level; it is associated with an infinite number of ta-
bles at the second level; each second-level table is
associated with an infinite number of tables at the
third level; and so on until the Lth level. Each cus-
tomer enters at the first level and comes out at the
Lth level, generating a path with L tables as she sits
in each restaurant. Moving from a table at level l to
one of its subtables at level l+1, the customer draws
following the CRP using Formula (1). In this paper,
we will make use of nested CRP to explore latent
topics in data space.
To elaborate our model, we first define two con-
cepts. If a model can learn a distribution over words
for a label, we refer the topic with a corresponding
label as a labeled topic. If a model can learn an un-
seen and latent topic without a label, we refer the
802
Figure 1: The graphical model of SSHLDA.
topic as a latent topic.
4 The Semi-Supervised Hierarchical Topic
Model
In this section, we will introduce a semi-
supervised hierarchical topic model, i.e., the Semi-
Supervised Hierarchical Latent Dirichlet Allocation
(SSHLDA). SSHLDA is a probabilistic graphical
model that describes a process for generating a hi-
erarchical labeled document collection. Like hi-
erarchical Labeled LDA (hLLDA) (Petinot et al
2011), SSHLDA can incorporate labeled topics into
the generative process of documents. On the other
hand, like hierarchical Latent Dirichlet Allocation
(hLDA) (Blei et al2004), SSHLDA can automat-
ically explore latent topic in data space, and extend
the existing hierarchy of observed topics. SSHLDA
makes use of not only observed topics, but also la-
tent topics.
The graphical model of SSHLDA is illustrated in
Figure 1. In the model, N is the number of words in
a document, D is the total number of documents in
a collection, M is the number of leaf nodes in hier-
archical observed nodes, ci is a node in the ith level
in the hierarchical tree, ?, ? and ?ci are dirichlet
prior parameters, ?k is a distribution over words, ?
is a document-specific distribution over topics, ?ci is
a multinomial distribution over observed sub-topics
of topic ci, w is an observed word, z is the topic
assigned to w, Dirk(.) is a k-dimensional Dirichlet
distribution, Tj is a set of paths in the hierarchy of
latent topics for jth leaf node in the hierarchy of ob-
Figure 2: One illustration of SSHLDA. The tree has 5
levels. The shaded nodes are observed topics, and circled
nodes are latent topics. The latent topics are generated
automatically by SSHLDA model. After learning, each
node in this tree will obtain a corresponding probability
distribution over words, i.e. a topic.
served topics, ? is a Multi-nomial distribution over
paths in the tree. All notations used in this paper are
listed in Table 1.
SSHLDA, as shown in Figure 1, assumes the fol-
lowing generative process:
(1) For each table k ? T in the infinite tree,
(a) Draw a topic ?k ? Dir(?).
(2) For each document, m ? {1, 2, ..., D}
(a) Let c1 be the root node.
(b) For each level l ? {2, ..., L}:
(i) If nodes in this level have been observed,
draw a node cl from Mult(?cl?1 |?cl?1).
(ii) Otherwise, draw a table cl from restaurant
cl?1 using Formula (1).
(c) Draw an L-dimensional topic proportion vec-
tor ?m from Dir(?).
(d) For each word n ? {1, ..., N}:
(i) Draw z ? {1, ..., L} from Mult(?).
(ii) Draw wn from the topic associated with
restaurant cz .
As the example showed in Figure 2, we assume
that we have known a hierarchy of observed top-
ics: {A1,A2,A17,A3,A4}, and assume the height
of the desired topical tree is L = 5. All circled
nodes are latent topics, and shaded nodes are ob-
served topics. A possible generative process for a
document m can be: It starts from A1, and chooses
node A17 at level 2, and then chooses A18, A20 and
A25 in the following levels. Thus we obtain a path:
cm = {A1, A17, A18, A20, A25}. After getting the
path for m, SSHLDA generates each word from one
of topics in this set of topics cm.
803
5 Probabilistic Inference
In this section, we describe a Gibbs sampling al-
gorithm for sampling from the posterior and corre-
sponding topics in the SSHLDA model. The Gibbs
sampler provides a method for simultaneously ex-
ploring the model parameter space (the latent topics
of the whole corpus) and the model structure space
(L-level trees).
In SSHLDA, we sample the paths cm for docu-
ment m and the per-word level allocations to topics
in those paths zm,n. Thus, we approximate the pos-
terior p(cm, zm|?, ?,w,?). The hyper-parameter ?
reflects the tendency of the customers in each restau-
rant to share tables, ? denotes the expected variance
of the underlying topics (e.g., ?  1 will tend to
choose topics with fewer high-probability words),
?ci is the dirichlet prior of ?ci , and ? is the set of
?ci . wm,n denotes the nth word in the mth docu-
ment; and cm,l represents the restaurant correspond-
ing to the lth-level topic in document m; and zm,n,
the assignment of the nth word in the mth document
to one of the L available topics. All other variables
in the model, ? and ?, are integrated out. The Gibbs
sampler thus assesses the values of zm,n and cm,l.
The Gibbs sampler can be divided into two main
steps: the sampling of level allocations and the sam-
pling of path assignments.
First, given the values of the SSHLDA hidden
variables, we sample the cm,l variables which are as-
sociated with the CRP prior. Noting that cm is com-
posed of com and cem , com is the set of observed
topics for document m, and cem is the set of latent
topics for document m. The conditional distribution
for cm, the L topics associated with documentm, is:
p(cm|z,w, c?m,?)
=p(com |?)p(cem |zem ,wem , ce?m)
?p(com |?)p(wem |cem ,we?m , zem)
p(cem |ce?m) (2)
where
p(com |?) =
|com |?1
?
i=0
p(ci,m|?ci) (3)
and
p(wem |cem ,we?m , zem)
=
|cem |
?
l=1
(
?(n.cem,l,?m + |V |?)
?
w ?(nwcem,l,?m + ?)
?
?
w ?(nwcem,l,?m + n
w
cem,l,m + ?)
?(n.cem,l,?m + n
?
cem,l,m + |V |?)
)
(4)
ce?m is the set of latent topics for all documents
other than m, zem is the assignment of the words
in the mth document to one of the latent topics, and
wem is the set of the words belonging to one of the
latent topics in the the mth document. nwcem,l,?m is
the number of instances of word w that have been
assigned to the topic indexed by cem,l, not including
those in the document m.
Second, given the current state of the SSHLDA,
we sample the zm,n variables of the underlying
SSHLDA model as follows:
p(zm,n = j|z?(m,n),w, cm,?)
?
nm?n,j + ?
nm?n,. + |cm|
?
nwm,n?n,j + ?wm,n
n.?(m,n) + |V |
(5)
Having obtained the full conditional distribution,
the Gibbs sampling algorithm is then straightfor-
ward. The zm,n variables are initialized to determine
the initial state of the Markov chain. The chain is
then run for a number of iterations, each time find-
ing a new state by sampling each zm,n from the dis-
tribution specified by Equation (5). After obtain-
ing individual word assignments z, we can estimate
the topic multinomials and the per-document mixing
proportions. Specifically, the topic multinomials are
estimated as:
?cm,j,i = p(wi|zcm,j) =
? + nzwicm,j
|V |? +
?
n.zcm,j
(6)
while the per-document mixing proportions fixed
can be estimated as:
?m,j =
?+ nm.,j
|cm|?+ nm.,.
, j ? 1, ..., |cm| (7)
5.1 Relation to Existing Models
In this section, we draw comparisons with the cur-
rent state-of-the-art models for hierarchical topic
804
modeling (Blei et al2004; Petinot et al2011) and
show that at certain choices of the parameters of our
model, these methods fall out as special cases.
Our method generalises not only hierarchi-
cal Latent Dirichlet Allocation (hLDA), but also
Hierarchical Labeled Latent Dirichlet Allocation
(hLLDA). Our proposed model provides a unified
framework allowing us to model hierarchical labels
while to explore new latent topics.
Equivalence to hLDA As introduced in Section 2,
hLDA is a unsupervised hierarchical topic model. In
this case, there are no observed nodes, that is, the
corpus has no hierarchical labels. This means cm is
equal to cem,m; meanwhile the factor p(com,m|?) is
always equal to one because each document has root
node, and this allows us to rewrite Formula (2) as:
p(cm|z,w, c?m,?)
?p(wcm |c,w?m, z)p(cm|c?m) (8)
which is exactly the same as the conditional distribu-
tion for cm, the L topics associated with document
m in hLDA model. In this case, our model becomes
equivalent to the hLDA model.
Equivalence to hLLDA hLLDA is a supervised hi-
erarchical topic model, which means all nodes in hi-
erarchy are observed. In this case, cm is equal to
com,m, and this allows us to rewrite Formula (2) as:
p(cm|z,w, c?m,?) = p(cm|?) ? p(com |?) (9)
which is exactly the same as the step ? Draw a
random path assignment cm? in the generative pro-
cess for hLLDA. Consequentially, in this sense our
model is equivalent to hLLDA.
6 Experiments
We demonstrate the effectiveness of the proposed
model on large, real-world datasets in the question
answering and website category domains on two
tasks: the topic modeling of documents, and the use
of the generated topics for document clustering.
6.1 Datasets
To construct comprehensive datasets for our ex-
periments, we crawled data from two websites.
First, we crawled nearly all the questions and as-
sociated answer pairs (QA pairs) of two top cat-
Table 2: The statistics of the datasets.
Datasets #labels #paths Max level #docs
Y Ans 46 35 4 6,345,786
O Hlth 6695 6505 10 54939
O Home 2432 2364 9 24254
egories of Yahoo! Answers: Computers & Inter-
net and Health. This produced forty-three sub-
categories from 2005.11 to 2008.11, and an archive
of 6,345,786 QA documents. We refer the Yahoo!
Answer data as Y Ans.
In addition, we first crawled two categories of
Open Directory Project (ODP)?: Home and Health.
Then, we removed all categories whose number of
Web sites is less than 3. Finally, for each of Web
sites in categories, we submited the url of each Web
site to Google and used the words in the snippet and
title of the first returned result to extend the sum-
mary of the Web site. We denote the data from the
category Home as O Home, and the data from the
category Health as O Hlth.
The statistics of all datasets are summarized in Ta-
ble 2. From this table, we can see that these datasets
are very diverse: Y Ans has much fewer labels than
O Hlth and O Home, but have much more docu-
ments for each label; meanwhile the depth of hierar-
chical tree for O Hlth and O Home can reach level
9 or above.
All experiments are based on the results of models
with a burn-in of 10000 Gibbs sampling iterations,
symmetric priors ? = 0.1 and free parameter ? = 1.0;
and for ?, we can obtain the estimation of ?ci by
fixed-point iteration (Minka, 2003).
6.2 Case Study
With topic modeling, the top associated words of
topics can be used as good descriptors for topics in
a hierarchy (Blei et al2003; Blei and McAuliffe,
2010). We show in Figure 3 a pair of compara-
tive example of the proposed model and a baseline
model over Y Ans dataset. The tree-based topic vi-
sualizations of Figure 3 (a) and (b) are the results of
SSHLDA and Simp-hLDA.
We have three major observations from the exam-
ple: (i) SSHLDA is a unified and generative model,
after learning, it can obtain a hierarchy of topics;
?http://dmoz.org/
805
Figure 3: (a) A sub network discovered on Y Ans dataset using SSHLDA, and the whole tree has 74 nodes; (b) A sub
network discovered on Y Ans dataset using Simp-hLDA algorithm, and the whole tree has 89 nodes. In both figures,
the shaded and squared nodes are observed labels, not topics; the shaded and round nodes are topics with observed
labels; blue nodes are topics but without labels and the yellow node is one of leaves in hierarchy of labels. Each topic
represented by top 5 terms.
while Simp-hLDA is a heuristic method, and its re-
sult is a mixture of label nodes and topical nodes.
For example, Figure 3 (b) shows that the hierarchy
includes label nodes and topic nodes, and each of la-
beled nodes just has a label, but label nodes in Fig-
ure 3 (a) have their corresponding topics. (ii) Dur-
ing obtaining a hierarchy, SSHLDAmakes use of the
information from observed labels, thus it can gener-
ate a logical, structual hierarchy with parent-child
relations; while Simp-hLDA does not incorporate
prior information of labels into its generation pro-
cess, thus although it can obtain a hierarchy, many
parent-child pairs have not parent-child relation. For
example, in Figure 3 (b), although label ?root? is
a parent of label ?Computers & Internet?, the topi-
cal words of label ?Computers & Internet? show the
topical node is not a child of label ?root?. How-
ever, in Figure 3 (a), label ?root? and ?Computers
& Internet? has corresponding parent-child relation
between their topical words. (iii) In a hierarchy of
topics, if a topical node has correspending label, the
label can help people understand descendant topi-
cal nodes. For example, when we know node ?er-
ror files click screen virus? in Figure 3 (a) has its
label ?Computers & Internet?, we can understand
the child topic ?hard screen usb power dell? is about
?computer hardware?. However, in Figure 3 (b), the
labels in parent nodes cannot provide much informa-
tion to understand descendant topical nodes because
many label nodes have not corresponding right topi-
cal words, such as label ?Computers & Internet?, its
topical words, ?the to you and a?, do not reflect the
connotation of the label.
These observations further confirm that SSHLDA
is better than the baseline model.
6.3 Perplexity Comparison
A good topic model should be able to generalize to
unseen data. To measure the prediction ability of
our model and baselines, we compute the perplex-
ity for each document d in the test sets. Perplex-
ity, which is widely used in the language modeling
and topic modeling community, is equivalent alge-
braically to the inverse of the geometric mean per-
word likelihood (Blei et al2003). Lower perplexity
scores mean better. Our model, SSHLDA, will com-
pare with three state-of-the-art models, i.e. Simp-
hLDA, hLDA and hLLDA. Simp-hLDA has been
introduced in Section 1, and hLDA and hLLDA has
been reviewed in Section 2. We keep 80% of the data
collection as the training set and use the remaining
collection as the held-out test set. We build the mod-
806
els based on the train set and compute the preplexity
of the test set to evaluate the models. Thus, our goal
is to achieve lower perplexity score on a held-out test
set. The perplexity of M test documents is calculated
as:
perplexity(Dtest) = exp
{
?
?M
d=1
?Nd
m=1 log p(wdm)
?M
d=1 Nd
}
(10)
where Dtest is the test collection of M documents,
Nd is document length of document d and wdm is
mth word in document d.
We present the results over the O Hlth dataset in
Figure 4. We choose top 3-level labels as observed,
and assume other labels are not observed, i.e. l = 3.
From the figure, we can see that the perplexities of
SSHLDA, are lower than that of Simp-hLDA, hLDA
and hLLDA at different value of the tree height pa-
rameter, i.e. L ? {5, 6, 7, 8}. It shows that the
performance of SSHLDA is always better than the
state-of-the-art baselines, and means that our pro-
posed model can model the hierarchical labeled data
better than the state-of-the-art models. We can also
obtain similar experimental results over Y Ans and
O Home datasets, and their detailed description is
not included in this paper due to the limitation of
space.
6.4 Clustering performance
To evaluate indirectly the performance of the pro-
posed model, we compare the clustering perfor-
mance of following systems: 1) the proposed model;
2) Simp-hLDA; 3) hLDA; 4) agglomerative cluster-
ing algorithm. There are many agglomerative clus-
tering algorithms, and in this paper, we make use
of the single-linkage method in a software package
called CLUTO (Karypis, 2005) to obtain hierarchies
of clusters over our datasets, with words as features.
We refer the method as h-clustering.
Given a document collectionDSwith aH-level hi-
erarchy of labels, each label in the hierarchy and cor-
responding documents will be taken as the ground
truth of clustering algorithms. The hierarchy of la-
bels denoted as GT-tree. The process of evaluation
is as follows. First, we choose top l-level labels
in GT-tree as an observed hierarchy, i.e. Base Tree
(BT), and we need to construct a L-level hierarchy
(l < L <= H) over the documents DS using a
Figure 4: Perplexities of hLLDA, hLDA, Simp-hLDA
and SSHLDA. The results are run over the O Hlth
dataset, with the height of the hierarchy of observed la-
bels l = 3. The X-axis is the height of the whole topical
tree (L), and Y-axis is the perplexity.
model. The remaining labels in GT-tree and cor-
responding documents are the ground truth classes,
each class denoted as Ci. Then, (i) for h-clustering,
we run single-linkage method over the documents
DS. (ii) for Simp-hLDA, hLDA runs on the doc-
uments in each leaf-node in BT, and the height pa-
rameter is (L ? l) for each hLDA. After training,
each document is assigned to top-1 topic accord-
ing to the distribution over topics for the document.
Each topic and corresponding documents forms a
new cluster. (iii) for hLDA, hLDA runs on all docu-
ments in DS, and the height parameter is L. Similar
to Simp-hLDA, each document is assigned to top-
1 topic. Each topic and corresponding documents
forms a new cluster. (iv) for SSHLDA, we set height
parameter as L. After training, each document is
also assigned to top-1 topic. Topics and their cor-
responding documents form a hierarchy of clusters.
6.4.1 Evaluation Metrics
For each dataset we obtain corresponding clusters
using the various models described in previous sec-
tions. Thus we can use clustering metrics to measure
the quality of various algorithms by using a measure
that takes into account the overall set of clusters that
are represented in the new generated part of a hier-
archical tree.
One such measure is the FScore measure, intro-
807
duced by (Manning et al2008). Given a particular
class Cr of size nr and a particular cluster Si of size
ni, suppose nri documents in the cluster Si belong
to Cr, then the FScore of this class and cluster is
defined to be
F (Cr, Si) =
2?R(Cr, Si)? P (Cr, Si)
R(Cr, Si) + P (Cr, Si)
(11)
where R(Cr, Si) is the recall value defined as
nri/nr, and P (Cr, Si) is the precision value defined
as nri/ni for the classCr and the cluster Si. The FS-
core of the class Cr, is the maximum FScore value
attained at any node in the hierarchical clustering
tree T . That is,
F (Cr) = max
Si?T
F (Cr, Si). (12)
The FScore of the entire clustering solution is then
defined to be the sum of the individual class FScore
weighted according to the class size.
FScore =
c
?
r=1
nr
n
F (Cr), (13)
where c is the total number of classes. In general, the
higher the FScore values, the better the clustering
solution is.
6.4.2 Experimental Results
Each of hLDA, Simp-hLDA and SSHLDA needs
a parameter?the height of the topical tree, i.e. L;
and for Simp-hLDA and SSHLDA, they need an-
other parameter?the height of the hierarchical ob-
served labels, i.e l. The h-clustering does not have
any height parameters, thus its FScore will keep the
same values at different height of the topical tree.
With choosing the height of hierarchical labels for
O Home as 4, i.e. l = 4, the results of our model
and baselines with respect to the height of a hierar-
chy are shown in Figure 5.
From the figure, we can see that our proposed
model can achieve consistent improvement over
the baseline models at different height, i.e. L ?
{5, 6, 7, 8}. For example, the performance of
SSHLDA can reach 0.396 at height 5 while the h-
clustering, hLDA and hLLDA only achieve 0.295,
0.328 and 0.349 at the same height. The result shows
that our model can achieve about 34.2%, 20.7% and
13.5% improvements over h-clustering, hLDA and
Figure 5: FScore measures of h-clustering, hLDA,
Simp-hLDA and SSHLDA. The results are run over the
O Home dataset, with the height of the hierarchy of ob-
served labels l = 3. The X-axis is the height of the whole
topical tree (L), and Y-axis is the FScore measure.
hLLDA at height 5. The improvements are signifi-
cant by t-test at the 95% significance level. We can
also obtain similar experimental results over Y Ans
and O Hlth. However, for the same reason of limita-
tion of space, their detailed descriptions are skipped
in this paper.
7 Conclusion and Future work
In this paper, we have proposed a semi-supervised
hierarchical topic models, i.e. SSHLDA, which aims
to solve the drawbacks of hLDA and hLLDA while
combine their merits. Specially, SSHLDA incorpo-
rates the information of labels into generative pro-
cess of topic modeling while exploring latent topics
in data space. In addition, we have also proved that
hLDA and hLLDA are special cases of SSHLDA.
We have conducted experiments on the Yahoo! An-
swers and ODP datasets, and assessed the perfor-
mance in terms of Perplexity and FScore measure.
The experimental results show that the prediction
ability of SSHLDA is the best, and SSHLDA can
also achieve significant improvement over the base-
lines on Fscore measure.
In the future, we will continue to explore novel
topic models for hierarchical labeled data to further
improve the effectiveness; meanwhile we will also
apply SSHLDA to other media forms, such as im-
age, to solve related problems in these areas.
808
Acknowledgments
This work was partially supported by NSFC with Grant
No.61073082, 60933004, 70903008 and NExT Search
Centre, which is supported by the Singapore National Re-
search Foundation & Interactive Digital Media R&D Pro-
gram Office, MDA under research grant (WBS:R-252-
300-001-490).
References
D. Blei and J. Lafferty. 2006. Correlated topic mod-
els. Advances in neural information processing sys-
tems, 18:147.
D.M. Blei and J.D. McAuliffe. 2007. Supervised topic
models. In Proceeding of the Neural Information Pro-
cessing Systems(nips).
D.M. Blei and J.D. McAuliffe. 2010. Supervised topic
models. Arxiv preprint arXiv:1003.0783.
D.M. Blei, A.Y. Ng, and M.I. Jordan. 2003. Latent
dirichlet alation. The Journal of Machine Learning
Research, 3:993?1022.
D. Blei, T.L. Griffiths, M.I. Jordan, and J.B. Tenenbaum.
2004. Hierarchical topic models and the nested chi-
nese restaurant process. Advances in neural informa-
tion processing systems, 16:106.
C. Chemudugunta, A. Holloway, P. Smyth, and
M. Steyvers. 2008a. Modeling documents by com-
bining semantic concepts with unsupervised statistical
learning. The Semantic Web-ISWC 2008, pages 229?
244.
C. Chemudugunta, P. Smyth, and M. Steyvers. 2008b.
Combining concept hierarchies and statistical topic
models. In Proceeding of the 17th ACM conference on
Information and knowledge management, pages 1469?
1470. ACM.
C. Chemudugunta, P. Smyth, and M. Steyvers. 2008c.
Text modeling using unsupervised topic models and
concept hierarchies. Arxiv preprint arXiv:0808.0973.
S. Deerwester, S.T. Dumais, G.W. Furnas, T.K. Landauer,
and R. Harshman. 1990. Indexing by latent semantic
analysis. Journal of the American society for informa-
tion science, 41(6):391?407.
T. Hofmann. 1999. Probabilistic latent semantic analy-
sis. In Proc. of Uncertainty in Artificial Intelligence,
UAI?99, page 21. Citeseer.
G. Karypis. 2005. Cluto: Software for
clustering high dimensional datasets. In-
ternet Website (last accessed, June 2008),
http://glaros.dtc.umn.edu/gkhome/cluto/cluto/overview.
S. Lacoste-Julien, F. Sha, and M.I. Jordan. 2008. ndis-
clda: Discriminative learning for dimensionality re-
duction and classification. Advances in Neural Infor-
mation Processing Systems, 21.
W. Li and A. McCallum. 2006. Pachinko allocation:
Dag-structured mixture models of topic correlations.
In Proceedings of the 23rd international conference on
Machine learning, pages 577?584. ACM.
C.D. Manning, P. Raghavan, and H. Schutze. 2008. In-
troduction to information retrieval, volume 1. Cam-
bridge University Press Cambridge.
D. Mimno, W. Li, and A. McCallum. 2007. Mixtures of
hierarchical topics with pachinko allocation. In Pro-
ceedings of the 24th international conference on Ma-
chine learning, pages 633?640. ACM.
Z.Y. Ming, K. Wang, and T.S. Chua. 2010. Prototype
hierarchy based clustering for the categorization and
navigation of web collections. In Proceeding of the
33rd international ACM SIGIR, pages 2?9. ACM.
T.P. Minka. 2003. Estimating a dirichlet distribution.
Annals of Physics, 2000(8):1?13.
A. Perotte, N. Bartlett, N. Elhadad, and F. Wood. 2011.
Hierarchically supervised latent dirichlet alation.
Neural Information Processing Systems (to appear).
Y. Petinot, K. McKeown, and K. Thadani. 2011. A
hierarchical model of web summaries. In Proceed-
ings of the 49th Annual Meeting of the ACL: Human
Language Technologies: short papers-Volume 2, pages
670?675. ACL.
D. Ramage, D. Hall, R. Nallapati, and C.D. Manning.
2009a. Labeled lda: A supervised topic model for
credit attribution in multi-labeled corpora. In Proceed-
ings of the 2009 Conference on Empirical Methods in
Natural Language Processing: Volume 1-Volume 1,
pages 248?256. Association for Computational Lin-
guistics.
D. Ramage, P. Heymann, C.D. Manning, and H. Garcia-
Molina. 2009b. Clustering the tagged web. In Pro-
ceedings of the Second ACM International Conference
on Web Search and Data Mining, pages 54?63. ACM.
D. Ramage, C.D. Manning, and S. Dumais. 2011. Par-
tially labeled topic models for interpretable text min-
ing. In Proceedings of the 17th ACM SIGKDD inter-
national conference on Knowledge discovery and data
mining, pages 457?465. ACM.
M. Rosen-Zvi, T. Griffiths, M. Steyvers, and P. Smyth.
2004. The author-topic model for authors and doc-
uments. In Proceedings of the 20th conference on
Uncertainty in artificial intelligence, pages 487?494.
AUAI Press.
T.N. Rubin, A. Chambers, P. Smyth, and M. Steyvers.
2011. Statistical topic models for multi-label docu-
ment classification. Arxiv preprint arXiv:1107.2462.
Y.W. Teh, M.I. Jordan, M.J. Beal, and D.M. Blei. 2006.
Hierarchical dirichlet processes. Journal of the Amer-
ican Statistical Association, 101(476):1566?1581.
809
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 1466?1477, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Identifying Event-related Bursts via Social Media Activities
Wayne Xin Zhao?, Baihan Shu?, Jing Jiang?, Yang Song?, Hongfei Yan?? and Xiaoming Li?
?School of Electronics Engineering and Computer Science, Peking University
?School of Information Systems, Singapore Management University
{batmanfly,baihan.shu,yhf1029}@gmail.com,ysong@pku.edu.cn
jingjiang@smu.edu.sg, lxm@pku.edu.cn
Abstract
Activities on social media increase at a dra-
matic rate. When an external event happens,
there is a surge in the degree of activities re-
lated to the event. These activities may be
temporally correlated with one another, but
they may also capture different aspects of an
event and therefore exhibit different bursty
patterns. In this paper, we propose to iden-
tify event-related bursts via social media activ-
ities. We study how to correlate multiple types
of activities to derive a global bursty pattern.
To model smoothness of one state sequence,
we propose a novel function which can cap-
ture the state context. The experiments on a
large Twitter dataset shows our methods are
very effective.
1 Introduction
Online social networks (e.g., Twitter, Facebook,
Myspace) significantly influence the way we live.
Activities on social media increase at a dramatic
rate. Millions of users engage in a diverse range
of routine activities on social media such as posting
blog messages, images, videos or status messages,
as well as interacting with items generated by oth-
ers such as forwarding messages. When an event
interesting to a certain group of individuals takes
place, there is usually a surge in the degree of ac-
tivities related to the event (e.g., a sudden explosion
of tweets). Since social media activities may indi-
cate the happenings of external events, can we lever-
age on the rich social media activities to help iden-
tify meaningful external events? This is the research
problem we study in this paper. By external events,
we refer to real-world events that happen external to
the online space.
?Corresponding author.
2 4 6 8 100
2040
6080
100120
140
Time index 
 all?tweetsretweetsurl?embeddedtweetsNoise
(a) Query=?Amazon.?
2 4 6 8 100
50
100
150
Time index 
 all?tweetsretweetsurl?embeddedtweetsNoise
(b) Query=?Eclipse?.
Figure 1: The amount of activities within a 10-hour
window for two queries. Three types of activities
are considered: (1) posting a tweet (upward triangle),
(2) retweet (downward triangle), (3) posting a URL-
embedded tweet (excluding retweet) (filled circle). As
explained in Table 1, both bursts above are noisy.
Mining events from text streams is usually
achieved by detecting bursty patterns (Swan and Al-
lan, 2000; Kleinberg, 2003; Fung et al 2005). How-
ever, previous work has mostly focused on tradi-
tional text streams such as scientific publications and
news articles. There is still a lack of systematic in-
vestigations into the problem of identifying event-
related bursty patterns via social media activities.
There are at least two basic characteristics of social
media that make the problem more interesting and
challenging.
First, social media involve various types of activ-
ities taking place in real time. These activities may
be temporally correlated with one another, but they
may also capture different aspects of an event and
therefore exhibit different bursty patterns. Most of
previous methods (Swan and Allan, 2000; Klein-
berg, 2003; Fung et al 2005) deal with a single type
of textual activities. When applied to social media,
they oversimplify the complex nature of online so-
1466
Bursty Activity Time # in Sr # in Su # in St Noisy?
Sr,St 23:00?23:59, Nov. 23, 2009 108 5 147 Y
See Fig. 1(b) [Query=eclipse] major bursty reason: The tweet from Robert Pattinson ?@twilight: from rob cont .
- i hope you are looking forward to eclipse as much as i am .? has been retweeted many times.
Su,St 07:00?07:59, Jul. 25, 2009 6 122 133 Y
See Fig. 1(a) [Query=Amazon] major bursty reason: Advertisement tweets like ?@fitnessjunkies amazon.com deals
: http://tinyurl.com/lakz3h.? have been posted many times.
St,Su,Sr 09:00?9:59, Oct. 9, 2009 1562 423 2848 N
[Query=Nobel] major bursty reason: The news ?Obama won Nobel Peace Prize? flood Twitter.
Table 1: Examples of bursts. The first two bursts are judged as noise since they do not correspond to any meaningful
external events. In fact, the reasons why a burst appears in social media can be quite diverse. In this paper, we only
focus on event-related bursts. St denotes posting a tweet, Su denotes posting a url-embedded tweet, and Sr denotes
retweet.
cial activities, and therefore they may not be well
suitable to social media. Let us consider a moti-
vating example. Figure 1 shows the change of the
amount of activities of three different types over a
10-hour time window for two queries. If we consider
only the total number of tweets, we can see that for
both queries there is a burst. However, neither of the
two bursts corresponds to a real-world event. The
first burst was caused by the broadcast of an adver-
tisement from several Twitter bots, and the second
burst was caused by numerous retweets of a status
update of a movie star1. The detailed explanations
of why the two bursts are noisy are also shown in
Table 1. On the other hand, interestingly, we can see
that not all the activity streams display noisy bursty
patterns at the same time. It indicates that we may
make use of multiple views of different activities
to detect event-related bursts. The intuition is that
using multiple types of activities may help learn a
better global picture of event-related bursty patterns.
Learning may also be more resistant to noisy bursts.
Second, in social media, burst detection is chal-
lenged by irregular, unpredictable and spurious
noisy bursts. To overcome this challenge, a reason-
able assumption is that a burst corresponding to a
real event should not fluctuate too much within a
relatively short time window. To illustrate it, we
present an example in Figure 2, in which we first
use a simple threshold method to detect bursts and
then analyze the effect of local smoothness. In par-
ticular, if the amount of activities at a certain time
is above a pre-defined threshold, we set its state to
1, which indicates a bursty state. Otherwise, we set
the state to 0. Figure 2(a) shows that for the query
?Eclipse,? with a threshold of 50, the state sequence
for the time window we consider is ?0000100000.?
1The reasons for these bursts were revealed by manually
checking the tweets during the corresponding periods.
1 2 3 4 5 6 7 8 9 100
50
100
150 Correct0000000000Threshold0000100000
(a) Query=?Eclipse?.
1 2 3 4 5 6 7 8 9 100
5001000
15002000
25003000 Correct0000011111Threshold0000011111
(b) Query=?Nobel?.
Figure 2: Analysis of the effect of local smoothness on
threshold method. It shows two examples of threshold
methods for burst detection in a 10-hour window. The
red line denotes the bursty threshold. If the number of
activities is above the threshold in one time interval, the
state of this time interval is judge as bursty. Detailed de-
scriptions of these cases are shown in Table 1.
Although there is a burst in this sequence, its dura-
tion is very short. In fact, this is the first example
shown in Table 1, which is a noisy burst. In con-
trast, in Figure 2(b), the state sequence for the query
?Nobel? is ?0000011111,? in which the longer and
smoother burst corresponds to a true event. A good
function for evaluating the smoothness of a state se-
quence should be able to discriminate these cases
and model the context of state sequences effectively.
With its unique characteristics and challenges,
there is an emergent need to deeply study the prob-
lem of event-related burst detection via social me-
dia activities. In this paper, we conduct a system-
atic investigation on this problem. We formulate
this problem as burst detection from time series of
social media activities. We develop an optimiza-
tion model to learn bursty patterns based on multiple
types of activities. We propose to detect bursts by
considering both local state smoothness and correla-
tion across multiple streams. We define a function to
1467
quantitatively measure local smoothness of one sin-
gle state sequence. We systematically evaluate three
types of activities for burst detection on a large Twit-
ter dataset and analyze different properties of these
three streams for burst detection.
2 Problem Definition
Before formally introducing our problems, we first
define some basic concepts.
Activity: An activity refers to some type of action
that users perform when they are interested in some
topic or event.
Activity Stream: An activity stream of length
N and type m is a sequence of numbers
(nm1 , n
m
2 , ..., n
m
N ), where each n
m
i denotes the
amount of activities of type m that occur during the
ith time interval.
Query: A queryQ is a sequence of terms q1, ..., q|Q|
which can represent the information needs of users.
For example, an example query related to President
Obama is ?barack obama.?
Event-related Burst: Given a query Q, an event-
related burst is defined as a period [ts, te] in which
some event related with Q takes place, where ts and
te are the start timestamp and end timestamp of the
event period respectively. During the event period
the amount of activities is significantly higher than
average.
Based on these definitions, our task is to try to
identify event-related bursts via multiple social me-
dia activity streams.
3 Identifying Event-related Bursts from
Social Media
In this section, we discuss how to identify event-
related bursts via social media activities. For-
mally, given a query Q, we first build M ac-
tivity streams related with Q on T timestamps:
{(nm1 , ..., n
m
T )}
M
m=1. The definition of activity in our
methods is very general; it includes various types of
social media activities, including textual and non-
textual activities, e.g., a click on a shared photo and
a link formation between two users.
Given the input, we try to infer a state sequence
over these T timestamps: z = (z1, ..., zT ), where
zi is 1 or 0. 1 indicates a time point within a burst
while 0 indicates a non-bursty time point.
3.1 Modeling a Single Activity Stream
3.1.1 Generation function
In probability theory and statistics, the Poisson
distribution2 is a discrete probability distribution
that can measure the probability of a given number
of ?activities? occurring in a fixed time interval. We
use the Poisson distribution to study the probability
of observing the number of social media activities,
and we treat one hour as one time interval in this
paper.
Homogeneous Poisson Distribution The genera-
tive probability of the ith number in one activity
stream of type m is defined as f(nmi , i, z
m
i ) =
(?zmi
)n
m
i exp(??zmi
)
nmi !
, where ?0 is the (normal) expec-
tation of the number of activities in one time inter-
val. If one state is bursty, it would emit activities
with a faster rate and result in a larger expectation
?1. We can set ?1 = ?0 ? ?, where ? > 1.
Heterogeneous Poisson Distribution The two-state
machine in (Kleinberg, 2003) used two global refer-
ences for all the time intervals: one for bursty and
the other for non-bursty. In our experiments, we ob-
serve temporal patterns of user behaviors, i.e., activ-
ities in some hours are significantly more than those
in the others. Instead of using fixed global rates ?0
and ?1, we try to model temporal patterns of user
behaviors by parameterizing ?(?) with the time in-
dex. By following (Ihler et al 2006), we use a
set of hour-specific rates {?1,h}24h=1 and {?0,h}
24
h=1.
3
Given a time index h, we set ?0,h to be the expecta-
tion of the number of activities in hth time interval
every day, then we have ?1,h = ?0,h ? ?. In this
paper, ? is empirally set as 1.5.
3.1.2 Smoothness of a State Sequence
For burst detection, the major aim is to identify
steady and meaningful bursts and to discard tran-
sient and spurious bursts. Given a state sequence
z1z2...zT , to quantitatively measure the smoothness
and compactness of it, we introduce some measures.
One simple method is to count the number of
change in the state sequence. Formally, we use the
following formula:
g1(z) = T ?
T?1?
i=1
I(zi 6= zi+1), (1)
2http://en.wikipedia.org/wiki/Poisson distribution
3We can also make the rates both day-specific and hour-
specific, i.e., {?(?),d,h}h?{1,...,24},d?{1,...,7}.
1468
where T is length of the state sequence and I(?)
is an indicator function which returns 1 only if the
statement is true. Let us take the state sequence
?0000100000? (shown in Figure 2(a)) as an example
to see how g1 works. State changes 0pos=4 ? 1pos=5
and 1pos=5 ? 0pos=6 each incur a cost of 1, there-
fore g1(0000100000) = 10 ? 2 = 8. Similarly, we
can get g1(0000000000) = 10. There is a cost dif-
ference between these two sequences, i.e., ?g1 = 2.
Kleinberg (2003) uses state transition probabilities
to model the smoothness of state sequences. With
simple derivations, we can show that Kleinberg?s
model essentially also uses a cost function that is
linear in terms of the number of state changes in a
sequence, and therefore similar to g1.
In social media, very short noisy bursts like
?0000100000? are very frequent. To discard such
noises, we may multiply g1 by a big cost factor to
punish short-term fluctuations. However, it is not
sensitive to the state context4 and may affect the
detection of meaningful bursts. For example, state
change 0pos=4 ? 1pos=5 in ?0000111100? would
receive the same cost as that of 0pos=4 ? 1pos=5
in ?0000100000? although the later is more like a
noise.
To better measure the smoothness of a state se-
quence , we propose a novel context-sensitive func-
tion, which sums the square of the length of the max-
imum subsequences in which all states are the same.
Formally, we have
g2(z1, z2, ..., zT ) =
?
si<ei
(ei ? si + 1)
2, (2)
where si and ei are the start index and end in-
dex of the ith subsequence respectively. To define
?maximum?, we have the constraints zsi = zsi+1 =
... = zei , zsi?1 6= zsi , zei 6= zei+1. For example,
g2(0000000000)= 102 = 100, g2(0000100000)=
42 + 12 + 52 = 42, we can see that ?g2 =
100 ? 42 = 58, which is significantly larger than
?g1(= 2). g2 rewards the continunity of state se-
quences while punish the fluctuating changes, and it
is context-sensitive. State change 0pos=4 ? 1pos=5
in ?0000111100? receives a cost of 4,5 which is
4Here context refers to the window of hidden state se-
quences.
5Indeed, g2 is not designed for a single state change but for
the overall smoothness patterns, so we choose a referring se-
quence generated by making the corresponding state negative to
compute the cost, i.e., |g2(0000011110)?g2(0000001110)| =
4.
much smaller than that of 0pos=4 ? 1pos=5 in
?0000100000?. g2 is also sensitive to the po-
sition of state changes, e.g., g2(0000100000) 6=
g2(0100000000).
3.2 Burst Detection from a Single Activity
Stream
Given an activity stream (nm1 , ..., n
m
T ), we would
like to infer a state sequence over these T times-
tamps, i.e., to find out the most possible state se-
quence z = (zm1 , ..., z
m
T ) based on the data, where
zmi = 1 or 0. We formulate this problem as
an optimization problem. The cost of a state se-
quence includes two parts: generation of activities
and smoothness of the state sequence. The objective
function is to find a state sequence which incurs the
minimum cost. Formally, we define the total cost
function as
Cost(z) = ?
T?
i=1
log f(nmi , i, z
m
i )
? ?? ?
generating cost
+
(
? ?(zm1 , ..., z
m
T ) ? ?1
)
? ?? ?
smoothness cost
,
(3)
where ?1 > 0 is a scaling factor which balance these
two parts. ?(?) function is the smoothness function,
and we can set it as either g1(?) or g2(?).
To seek the optimal state sequence, we can min-
imize Equation 3. However, exact inference is hard
due to the exponential search space. Instead of ex-
amining the smoothness of the whole state sequence,
we propose to measure the smoothness of all the L-
length subsequences, so called ?local smoothness?.
The assumption is that the states in a relatively short
time window should not change too much. The new
objective function is defined as
Cost(z) = ?
T?
i=1
log f(nmi , i, z
m
i ) (4)
?
(
?
i?L
?(zmi , ..., z
m
i+L?1)
)
? ?1.
The objective function in Equation 4 can be
solved efficiently by a dynamic programming algo-
rithm shown in Algorithm 1. The time complexity
of this algorithm is O(T ? 2L). Note that the meth-
ods we present in Equation 4 and Algorithm 1 are
quite general. They are independent of the concrete
forms of f(?) and ?(?), which leaves room for flexi-
ble adaptation or extension in specific tasks. In pre-
vious methods (Kleinberg, 2003), L is often fixed as
1469
2. Indeed, as shown in Figure 2, in some cases, we
may need a longer window to infer the global pat-
terns. In our model, L can be tuned based on real
datasets. We can seek a trade-off between efficiency
and length of context windows.
Algorithm 1: Dynamic Programming for Equation 4.
d[i][s][zi...zi?L+1] denotes the minimum cost of the first1
i timestamps with the state subsequence: zi...zi?L+1 and
zi = s;
set d[0][?][?] = 0;2
set c1[i] = log f(nmi , i, z
m
i );3
set c2[i] = ?(zi, ..., zi?L+1);4
b, b?: previous and current state window are represented as5
L-bit binary numbers;
for i = 1 to T do6
for s = 0 to 1 do7
for b = 0 to 2L ? 1 do8
b? = (b << 1|s)&(1 << L? 1);9
d[i][s][b?]??10
min(d[i][s][b?], d[i?1][s][b]+c1[i]+c2[i]);
end11
end12
end13
3.3 Correlating Multiple Activity Streams
In this section, we discuss how to correlate multi-
ple activity streams to learn a global bursty patterns.
The hidden state sequences corresponding to these
activity streams are not fully independent. An ex-
ternal event may intricate surges in multiple activity
streams simultaneously.
We propose to correlate multiple activity streams
in an optimization model. The idea is that activ-
ity streams related with one query might be depen-
dent, i.e., the states of multiple activity streams on
the same timestamp tend to be the same6; if not,
it would incur a cost. To implement this idea, we
develop an optimization model. For convenience,
we call the states of each activity stream as ?local
states? while the overall states learnt from multiple
activity streams as ?global states?.
The idea is that although various activity streams
are different in the scale of frequencies, they tend to
share similar trend patterns. We incorporate the cor-
relation between local states on the same timestamp.
6In our experiments, we compute the cross correlation be-
tween different streams with a lag factor ?, we find the cross
correlation achieves maximum consistantly when ? = 0.
Formally, we have
Cost(Z) =
M?
m=1
{
?
T?
i=1
log f(nmi , i, z
m
i )
?
?
i?L
?(zmi , ..., z
m
i+L?1) ? ?1
}
+
T?
i=1
?
m1,m2
I(zm1i 6= z
m2
i ) ? ?2, (5)
where I(?) is indicator function, and ?2 is the cost
when a pair of states are different across multiple
streams on the same timestamp.
The objective function in Equation 5 can be
solved by a dynamic programming algorithm pre-
sented in Algorithm 2. The time complexity of this
algorithm is O(T ? 2M ?L+M ). Generally, L can be
set as one small value, e.g., L =2 to 6, and we can
select just a few representative activity streams, i.e.,
M =2 to 6. In this case, the algorithm can be effi-
cient.
Algorithm 2: Dynamic Programming for Equation 5.
d[i][z1i ...z
1
i?L+1; ...; z
M
i ...z
M
i?L+1] denotes the minimum1
cost of the first i timestamps with the local state
subsequence zmi ...z
m
i?L+1 in the mth stream;
set d[0][...] = 0;2
bl, bl
?
: previous and current state windows represented as3
M ? L-bit binary numbers;
c[i, bl, bl
?
] denotes all the cost in the tth timestamp;4
for i = 1 to T do5
for bl = 0 to 2M?L ? 1 do6
deriving current local state sequences bl
?
from bl;7
d[i][b?l]??8
min(d[i][bl
?
], d[i? 1][bl] + c[i, bl, bl
?
]);
end9
end10
Given M types of activity streams, we can get
M (local) state sequences {(zm1 , ..., z
m
T )}
M
m=1. The
next question is how to learn a global state sequence
(zG1 , ..., z
G
T ) based on local state sequences. Here we
give a few options:
CONJUNCT: we set a global state zi as bursty if
all local states are bursty, i.e., zGi = ?
M
m=1z
m
i .
DISJUNCT: we set a global state zi as bursty if
one of the local states is bursty, i.e., zGi = ?
M
m=1z
m
i .
BELIEF: we set a global state zi as the most con-
fident local state, i.e., zGi = argmaxmbelief(z
m
i ).
The belief(?) function can be defined as the ratio be-
tween generating costs from states zmi and 1 ? z
m
i :
belief(zmi ) =
f(nmi ,i,z
m
i )
f(nmi ,i,1?z
m
i )
.
1470
Table 2: Basic statistics of our golden test collection.
# of queries 17
Aver. # of event-related bursts per query 19
Min. bursty interval 3 hours
Max. bursty interval 163 hours
Aver. bursty interval 17.8 hours
L2G: we treat the states of one local stream as the
global states.
4 Experiments
4.1 Construction of Test Collection
We test our algorithms on a large Twitter dataset,
which contains about 200 million tweets and ranges
from July, 2009 to December 2009. We manually
constructed a list of 17 queries that have high vol-
umes of relevant tweets during this period. These
queries have a very broad coverage of topics. Exam-
ple queries are ?Barack Obama?, ?Apple?, ?Earth-
quake?, ?F1? and ?Nobel Prize?. For each query, we
invite two senior graduate students to manually iden-
tify their golden bursty intervals, and each bursty in-
terval is represented as a pair of timestamps in terms
of hours. Specifically, to generate the golden stan-
dard, given a query, the judges first manually gen-
erate a candidate list of external events7; then for
each event, they look into the tweets within the cor-
responding period and check whether there is a surge
on the frequency of tweets. If so, the judges fur-
ther determine the start timepoint and end timepoint
of it. If there is a conflict, a third judge will make
the final decision. We used Cohen?s kappa coeffi-
cient to measure the agreement of between the first
two judges, which turned out to be 0.67, indicating a
good level of agreement8. We present basic statistics
of the test collection in Table 2.
4.2 Evaluation Metrics
Before introducing our evaluation metrics, we first
define the Bursty Interval Overlap Ratio (BIOR)
BIOR(f,X ) =
?
f ??X ?l(f, f
?)
L(f)
,
f is a bursty interval, ?l(f, f ?) is the length of
overlap between f ? and f , L(f) is the length of
7We refer to some gold news resources, e.g., Google News
and Yahoo! News.
8http://en.wikipedia.org/wiki/Cohen?s kappa
Figure 3: Examples to illustrate BIOR. X0, X1
and X2 are three sets of bursty intervals. X0
and X2 consist of one interval, and X1 consists of
two intervals. BIOR(f,X0)=1, BIOR(f,X1)=0.5 and
BIOR(f,X2)=0.5.
bursty period of f . X is a set of bursty intervals,
BIOR measures the proportion of the timestamps in
f which are covered by one of bursty intervals in
X . We use BIOR to measure partial match of inter-
vals, because a system may not return all the exact
bursty intervals9. We show some examples of BIOR
in Figure 3.
We use modified Precision, Recall and F as ba-
sic measures. Given one query, P, R and F can be
defined as follows
R =
?
f?B I
(
1
|Mf |
BIOR(f,M) > 0.5
)
|B|
,
P =
1
|M|
?
f ??M
(BIOR(f ?,B)),
F =
2? P ?R
P + R
,
where M is the set of bursty intervals identified
by one candidate method, B is the set of bursty in-
tervals in golden standards, and Mf is the set of in-
tervals which overlap with f in M. We incorporate
the factor 1|Mf | in Recall to penalize the incontin-
uous coverage of the golden interval, and we also
require that the overlap ratio with penalized factor
is higher than a threshold of 0.5. Given two sets of
bursty intervals which have the same value of BIOR,
we prefer the one with fewer intervals. In Figure 3,
we can easily derive X1 and X2 have the same value
9A simple evaluation method is that we label each one hour
time slot as being part of a burst or not and compare with the
gold standard. However, in our experiments, we find that some
methods tend to break one meaningful burst into small parts and
easier to be affected by small fluctuations although they may
have a good coverage of bursty points. This is why we adopt a
different evaluation approach.
1471
Table 3: Average cross-correlation between different
streams.
St Sr Su
St 1 0.830235 0.851514
Sr 0.830235 1 0.59905
Su 0.851514 0.59905 1
of BIOR, when computing Recall, we prefer X2 to
X1 since X2 consists of only one complete inter-
val whileX1 consists of two inconsecutive intervals.
I(?) is an indicator function which returns 1 only if
the statement if true. In our experiments, we use the
average of R, P and F over all test queries.
4.3 Experiment Setup
Selecting activity streams
We consider three types of activity streams in
Twitter: 1) posting a tweet, denoted as St; 2) for-
warding a tweet (retweet), denoted as Sr; 3) post-
ing a URL-embedded tweet, denoted as Su. It is
natural to test the performance of St in discover-
ing bursty patterns, while Su and Sr measure the
influence of external events on users in Twitter in
two different aspects. Sr: An important convention
in Twitter is the ?retweeting? mechanism, through
which users can actively spread the news or related
information; Su: Another characteristic of Twitter is
that the length of tweets is limited to 140 characters,
which constrains the capacity of information. Users
often embed a URL link in the tweets to help others
know more about the corresponding information.
We compute the average cross correlation be-
tween different activity streams for these 17 queries
in our test collection, and we summarize the results
in Table 3. We can see that both Sr and Su have a
high correlation with St, and Sr has a relatively low
correlation with Su. 10
Methods for comparisons
S(?): using Equation 4 and considers a single ac-
tivity stream, namely St, Su and Sr.
MBurst(?): using Equation 5 and considers mul-
tiple activity streams.
To compare our methods with previous methods,
we adopt the following baselines:
StateMachine: This is the method proposed
in (Kleinberg, 2003). We use heterogeneous Poisson
10We also consider the frequencies of unique users by hours,
however, we find it has a extremely high correlation coefficient
with St, about 0.99, so we do not incorporate it.
function as generating functions instead of binomial
function Cnk because sometimes it is difficult to get
the exact total number n in social media.
Threshold: If we find that the count in one time
interval is higher than a predefined threshold, it is
treated as a burst. The threshold is set as 1.5 times
of the average number.
PeakFinding: This is the method proposed
in (Marcus et al 2011), which aims to automatically
discover peaks from tweets.
Binomial: This is the method proposed in (Fung et
al., 2007a), which uses a cumulative binomial distri-
bution with a base probability estimated by remov-
ing abnormal frequencies.
As for multiple-stream burst detection, to the best
of our knowledge, the only existing work is pro-
posed by (Yao et al 2010), which is supervised and
requires a considerable amount of training time, so
we do not compare our work with it. We compare
our method with the following heuristic baselines:
SimpleConjunct: we first find the optimal state se-
quences for each single activity stream. We then de-
rive a global state sequence by taking the conjunc-
tion of all local states.
SimpleDisjunct: we first find the optimal state se-
quences for each single activity stream, and then we
derive a global state sequence by take the disjunction
of all local states.
Another possible baseline is that we first merge
all the activities, then apply the single-stream algo-
rithm. However, in our data set, we find that the
number of activities in St is significantly larger than
that of the two types. St dominantly determines the
final performance, so we do not incorporate it here
as a comparison.
4.4 Experimental Results
Preliminary results on a single stream
We first examine the performance of our proposed
method on a single stream. Note that, our method
in Equation 4 has two merits: 1) the length of lo-
cal window can be tuned on different datasets; 2) a
novel state smoothness function is adopted.
We set the ? function in Equation 4 respectively
as g1 and g2, and apply our proposed methods to
three streams (St,Sr,Su) mentioned above. Note
that, when L = 2 and ? = g1, our method becomes
the algorithm in (Kleinberg, 2003). We tune the pa-
rameter ?1 in Equation 4 from 2 to 20 with a step of
2. We record the best F performance and compute
1472
the corresponding standard deviation. In Table 5, we
can observe that 1) streams St and Sr perform better
than Su; 2) the length of local window significantly
affects the performance; 3) g2 is much better than g1
in our proposed burst detection algorithm; 4) gen-
erally speaking, a longer window size (L = 3, 4)
performs better than the most common used size 2
in (Kleinberg, 2003).
We can see that our proposed method is more ef-
fective than the other baselines. The major reason is
that none of these methods consider state smooth-
ness in a systematic way. In our preliminary ex-
periments, we find that these baselines usually out-
put a lot of bursts, most of which are broken mean-
ingful bursts. To overcome this, baseline method
StateMachine (g1 + L = 2) requires larger ? and
?1, which may discard relatively small meaningful
bursts; while our proposed single stream method
(g2 + L = 3, 4) tends to identify steady and con-
secutive bursts through the help of longer context
window and context sensitive smoothness function
g2, it is more suitable to be applied to social media
for burst detection.
Compared with the other baselines, (Kleinberg,
2003) is still one good and robust baseline since it
models the state smoothness partially. These prelim-
inary findings indicate that state smoothness is very
important for burst detection, and the length of state
context window will affect the performance signifi-
cantly.
To get a deep analysis of the performance of dif-
ferent streams, we set up three classes, and each
class corresponds to a single stream. Since for each
query, we can obtain multiple results in different ac-
tivity streams, we further categorize the 17 anno-
tated queries to the stream which leads to the opti-
mal performance on that query. Interestingly, we can
see: 1) the url stream gives better performance on
queries about big companies because users in Twit-
ter usually talk about the release of new products
or important evolutionary news via url-embedded
tweets; 2) the retweet stream gives better perfor-
mance on queries which correspond to unexpected
or significant events, e.g., diasters. It is consistent
with our intuitions that users in Twitter do actively
spread such information. Combining previous anal-
ysis of Table 5, overall we find the retweet stream is
more capable to identify bursts which correspond to
significant events.
Table 4: Categorization of 17 queries according to the
optimal performance.
Streams Queries
url Apple,Microsoft,Nokia, climate
retweet bomb,crash,earthquake,typhoon,
F1,Google,Olympics
all tweet Amazon, eclipse, Lakers,
NASA, Nobel Prize, Barack Obama
Table 5: Performance (average F) on a single stream.
???? indicates that the improvement our proposed single-
stream methodg2,L=4 over all the other baselines is ac-
cepted at the confidence level of 0.95, i.e., StateMachine,
PeakingFinding, Binomial and Threshold.
? L St Sr Su
4 0.545/0.015 0.543/0.037 0.451/0.036
g2 3 0.536/0.013 0.549??/0.019 0.464/0.025
2 0.468/0.055 0.542/0.071 0.455/0.045
4 0.513/0.059 0.546/0.058 0.465/0.047
g1 3 0.469/0.055 0.542/0.071 0.455/0.045
2 0.396/0.043 0.489/0.074 0.374/0.035
StateMachine 0.396 0.489 0.374
PeakFinding 0.410 0.356 0.302
Binomial 0.315 0.420 0.341
Threshold 0.195 0.181 0.175
Preliminary results on multiple streams
After examining the basic results on a single
stream, we continue to evaluate the performance of
our proposed models on multiple activity streams.
For MBurst in Equation 5, we have three parame-
ters to set, namely L, ?1 and ?2. We do a grid search
for both ?1 and ?2 from 1 to 12 with a step of 1, and
we also examine the performance when L = 2, 3, 4.
We can see that MBurst has four candidate meth-
ods to derive global states from local states; for L2G,
we use the states of St as the final states, and we em-
pirically find that it performs best compared with the
other two streams in L2G.
Recall that our proposed single-stream method
is better than all the other single-stream baselines,
so here single-best denotes our method in Equa-
tion 4 (? = g2, L = 4) on Sr. For SimpleConjunct
and SimpleDisjunct, we first find the optimal state
sequences for each single activity stream using our
proposed method in Equation 4 (? = g2, L = 4),
and then we derive a global state sequence by take
the conjunction or disjunction of all local states re-
spectively.
Besides the best performance, we further compute
the average of the top 10 results of each method
by tuning parameters to check the average perfor-
1473
Table 6: Performance (average F) on multiple streams.
??? indicates that the improvement our proposed
multiple-stream method over our proposed single-stream
method at the confidence level of 0.9 in terms of average
performance.
Methods best average
single-best (g2 + Sr) 0.549 0.526
SimpleConjunct 0.548 -
SimpleDisjunct 0.465 -
MBurst+CONJUNCTr,t,u 0.555 0.548
MBurst+DISJUNCTr,t,u 0.576 0.570?
MBurst+BELIEFr,t,u 0.568 0.561
MBurst+L2Gr,t,u(t) 0.574 0.567
MBurst+L2Gr,t,u(r) 0.560 0.558
mance. The average performance can show the sta-
bility of models in some degree. If one model out-
puts the maximum in a very limited set of parame-
ters, it may not work well in real data, especially in
social media.
In Table 6, we can seeMBurst+DISJUNCTr,t,u
gives the best performance. MBurst performs
consistently better than single-best which is a very
strong single-stream method, especially for average
performance. MBurst+DISJUNCTr,t,u has an im-
provement of average performance over single-best
by 8.4%. And simply combining three different
streams may hit results (SimpleConjunct and Sim-
pleDisjunct). It indicates that MBurst is more sta-
ble and shows a higher performance.
For different methods to derive global bursty pat-
terns, we can see that MBurst+DISJUNCT per-
forms best while MBurst+CONJUNCT performs
worst. Interestingly, however, SimpleConjunct is
better than SimpleDisjunct, the major reason is that
MBurst performs a local-state correlation of mul-
tiple activity streams to correct possible noisy fluc-
tuations from single streams before the conjunction
or disjunction of local states. After such correlation,
the performance of each activity stream should im-
prove. To see this, we present the optimal results of a
single stream without/with local-state correlation in
Table 7. Local-state correlation significantly boosts
the performance of a single stream. Indeed, we find
that the step of local-state correlation is more impor-
tant for our multiple stream algorithm than the step
of how to derive global states based on local states.
We test our MBurst algorithm with the setting:
T = 4416, L = 4 and M = 3, and for all the test
Table 7: Comparison between the optimal results of a
single stream with/without local-state correlation.
all retweet retweet url
without 0.536 0.549 0.464
with 0.574 0.560 0.547
2 4 6 8 10 120.54
0.550.56
0.570.58
0.590.6
?1
Average
 F
 
 MBurst+orsingle?best
(a) ?2 = 4, varying ?1.
2 4 6 8 10 120.54
0.550.56
0.570.58
0.590.6
?2
Average
 F
 
 MBurst+orsingle?best
(b) ?1 = 11, varying ?2.
Figure 4: Parameter sensitivity of MBurst + DIS-
JUNCT.
queries, our algorithm can respond in 2 seconds 11,
which is efficient to be deployed in social media.
Parameter sensitivity
We have shown the performance of different pa-
rameter settings for single stream algorithm in Ta-
ble 5. Next, we check parameter sensitivity in
MBurst. In our experiments, we find a longer lo-
cal window (L = 3, 4) is better than L = 2, so
we first set L = 4, then we select parameter set-
tings of ?2 = 4 and ?1 = 11, which give best per-
formance for MBurst+DISJUNCT. We vary one
with the other fixed to see how one single parame-
ter affects the performance. The results are shown in
Figure 4, and we can see MBurst+DISJUNCT is
consistently better than single-best.
5 Related Work
Our work is related to burst detection from text
streams. Pioneered by the automaton model pro-
posed in (Kleinberg, 2003), many techniques have
been proposed for burst detection such as the ?2-
test based method (Swan and Allan, 2000), the
parameter-free method (Fung et al 2005) and mov-
ing average method (Vlachos et al 2004). Our work
is related to the applications of these burst detection
algorithms for event detection (He et al 2007; Fung
et al 2007b; Shan et al 2012; Zhao et al 2012).
11All experiments are tested in a Mac PC, 2.4GHz Intel Core
2 Duo.
1474
Some recent work try to identify hot trends (Math-
ioudakis and Koudas, 2010; Zubiaga et al 2011;
Budak et al 2011; Naaman et al 2011) or make
use of the burstiness (Sakaki et al 2010; Aramki
et al 2011; Marcus et al 2011) in social media.
However, few of these methods consider modeling
the local smoothness of one state sequence in a sys-
tematic way and often use a fixed window length of
2.
Little work considers making use of different
types of social media activities for burst detection.
(Yao et al 2010; Kotov et al 2011; Wang et al
2007; Wang et al 2009) conducted some prelim-
inary studies of mining correlated bursty patterns
from multiple sources. However, they either highly
relies on high-quality training datasets or require a
considerable amount of training time. Online social
activities are dynamic, with a large number of new
items generated continuously. In such a dynamic
setting, burst detection algorithms should effectively
collect evidence, efficiently adjust prediction models
and respond to the users as social media activities
evolve. Therefore it is not suitable to deploy such
algorithms in social media.
Our work is also similar to studies which aim
to mine and leverage knowledge from social me-
dia (Mathioudakis et al 2010; Ruiz et al 2012;
Morales et al 2012). We share the common point
with these studies that we try to utilize the under-
lying rich knowledge in social media, while our fo-
cus of this work is quite different from theirs, i.e., to
identify event-related bursts.
Another line of related research is Twitter related
studies (Kwak et al 2010; Sakaki et al 2010). Our
proposed methods can provide event-related bursts
for downstream applications.
6 Conclusion
In this paper, we propose to identify event-related
bursts via social media activities. We propose one
optimization model to correlate multiple activity
streams to learn the bursty patterns. To better mea-
sure local smoothness of the state sequence, we pro-
pose a novel state cost function. We test our meth-
ods in a large Twitter dataset. The experiment re-
sults show that our methods are both effective and
efficient. Our work can provide a preliminary un-
derstanding of the correlation between the happen-
ings of events and the degree of online social media
activities.
Finally, we present a few promising directions
which may potentially improve or enrich current
work.
1) Variable-length context. In this paper, L is a
pre-determined parameter which controls the size of
context window. It cannot be modified when the al-
gorithm runs. A large L will significantly increases
the algorithm complexity, and we may not need a
large L for all the states in a Markov chain. This
problem can be addressed by using the variable-
length hidden Markov model (Wang et al 2006),
which is able to learn the ?minimum? context length
for accurately determining each state.
2) Incorporation of more useful features. Our
current model mainly considers temporal variations
of streaming data and searches the surge patterns ex-
isting in it. In some cases, simple frequency infor-
mation may not be capable to identify all the mean-
ingful bursts. It can be potentially useful to leverage
up more features to help filter out noisy bursts, e.g.,
semantic information (Zhao et al 2010).
3) Modeling multi-modality data. We have ex-
amined our multi-stream algorithm by using three
different activity streams. These streams are textual-
based. It will be interesting to check our algorithm in
multi-modality data streams. E.g., in Facebook, we
may collect a stream consisting of the daily frequen-
cies of photo sharing and another stream consisting
of the daily frequencies of text status updates.
4) Evaluation of the identified bursts. In most
of previous work, they seldom construct a gold stan-
dard for quantitative test, instead they qualitatively
evaluate their methods. In our work, we invite hu-
man judges to generate the gold standard. It is time-
consuming, and the bias from human judges cannot
be completely eliminated although more judges can
be invited. A possible evaluation method is to exam-
ine the identified bursts in downstream applications,
e.g., event detection.
Acknowledgement
This work is partially supported by NSFC Grant
61073082, 60933004 and 70903008. Xin Zhao is
supported by Google PhD Fellowship (China). We
thank the insightful comments from Junjie Yao and
the anonymous reviewers.
1475
References
Eiji Aramki, Sachiko Maskawa, and Mizuki Morita.
2011. Twitter catches the flu: Detecting influenza epi-
demics using twitter. In Proceedings of the 2011 Con-
ference on Empirical Methods in Natural Language
Processing, pages 1568?1576, Edinburgh, Scotland,
UK., July. Association for Computational Linguistics.
Ceren Budak, Divyakant Agrawal, and Amr El Abbadi.
2011. Structural trend analysis for online social net-
works. Proc. VLDB Endow., 4, July.
Gabriel Pui Cheong Fung, Jeffrey Xu Yu, Philip S. Yu,
and Hongjun Lu. 2005. Parameter free bursty events
detection in text streams. In VLDB.
Gabriel Pui Cheong Fung, Jeffrey Xu Yu, Huan Liu, and
Philip S. Yu. 2007a. Time-dependent event hierar-
chy construction. In Proceedings of the 13th ACM
SIGKDD international conference on Knowledge dis-
covery and data mining, KDD ?07.
Gabriel Pui Cheong Fung, Jeffrey Xu Yu, Huan Liu, and
Philip S. Yu. 2007b. Time-dependent event hierarchy
construction. In SIGKDD.
Qi He, Kuiyu Chang, and Ee-Peng Lim. 2007. Analyz-
ing feature trajectories for event detection. In SIGIR.
Alexander Ihler, Jon Hutchins, and Padhraic Smyth.
2006. Adaptive event detection with time-varying
poisson processes. In Proceedings of the 12th ACM
SIGKDD international conference on Knowledge dis-
covery and data mining, KDD, pages 207?216, New
York, NY, USA. ACM.
J. Kleinberg. 2003. Bursty and hierarchical structure in
streams. Data Mining and Knowledge Discovery.
Alexander Kotov, ChengXiang Zhai, and Richard Sproat.
2011. Mining named entities with temporally corre-
lated bursts from multilingual web news streams. In
Proceedings of the fourth ACM international confer-
ence on Web search and data mining, WSDM, pages
237?246.
Haewoon Kwak, Changhyun Lee, Hosung Park, and Sue
Moon. 2010. What is Twitter, a social network or a
news media? In WWW ?10: Proceedings of the 19th
international conference on World wide web, pages
591?600.
Adam Marcus, Michael S. Bernstein, Osama Badar,
David R. Karger, Samuel Madden, and Robert C.
Miller. 2011. Twitinfo: aggregating and visualizing
microblogs for event exploration. In Proceedings of
the 2011 annual conference on Human factors in com-
puting systems, CHI ?11.
Michael Mathioudakis and Nick Koudas. 2010. Twit-
termonitor: trend detection over the twitter stream.
In Proceedings of the 2010 international conference
on Management of data, SIGMOD ?10, pages 1155?
1158.
Michael Mathioudakis, Nick Koudas, and Peter Marbach.
2010. Early online identification of attention gather-
ing items in social media. In Proceedings of the third
ACM international conference on Web search and data
mining, WSDM ?10, pages 301?310, New York, NY,
USA. ACM.
Gianmarco De Francisci Morales, Aristides Gionis, and
Claudio Lucchese. 2012. From chatter to headlines:
harnessing the real-time web for personalized news
recommendation. In WSDM, pages 153?162.
Mor Naaman, Hila Becker, and Luis Gravano. 2011. Hip
and trendy: Characterizing emerging trends on twitter.
JASIST, 62(5):902?918.
Eduardo J. Ruiz, Vagelis Hristidis, Carlos Castillo, Aris-
tides Gionis, and Alejandro Jaimes. 2012. Correlat-
ing financial time series with micro-blogging activity.
pages 513?522.
Takeshi Sakaki, Makoto Okazaki, and Yutaka Matsuo.
2010. Earthquake shakes twitter users: real-time event
detection by social sensors. WWW, pages 851?860,
New York, NY, USA. ACM.
Dongdong Shan, Wayne Xin Zhao, Rishan Chen, Shu
Baihan, Hongfei Yan, and Xiaoming Li. 2012.
Eventsearch: A system for event discovery and re-
trieval on multi-type historical data. In KDD?12, De-
mostration.
Russell Swan and James Allan. 2000. Automatic gener-
ation of overview timelines. In SIGIR.
Michail Vlachos, Christopher Meek, Zografoula Vagena,
and Dimitrios Gunopulos. 2004. Identifying similari-
ties, periodicities and bursts for online search queries.
In SIGMOD.
Yi Wang, Lizhu Zhou, Jianhua Feng, JianyongWang, and
Zhi-Qiang Liu. 2006. Mining complex time-series
data by learning markovian models. In Proceedings
of the Sixth International Conference on Data Min-
ing, ICDM, pages 1136?1140, Washington, DC, USA.
IEEE Computer Society.
Xuanhui Wang, ChengXiang Zhai, Xiao Hu, and Richard
Sproat. 2007. Mining correlated bursty topic pat-
terns from coordinated text streams. In Proceedings
of the 13th ACM SIGKDD international conference on
Knowledge discovery and data mining.
Xiang Wang, Kai Zhang, Xiaoming Jin, and Dou Shen.
2009. Mining common topics from multiple asyn-
chronous text streams. In Proceedings of the Second
ACM International Conference on Web Search and
Data Mining, WSDM, pages 192?201.
Junjie Yao, Bin Cui, Yuxin Huang, and Xin Jin. 2010.
Temporal and social context based burst detection
from folksonomies. In AAAI.
Wayne Xin Zhao, Jing Jiang, Jing He, Dongdong Shan,
Hongfei Yan, and Xiaoming Li. 2010. Context mod-
eling for ranking and tagging bursty features in text
1476
streams. In Proceedings of the 19th ACM interna-
tional conference on Information and knowledge man-
agement, CIKM ?10.
Wayne Xin Zhao, Rishan Chen, Kai Fan, Hongfei Yan,
and Xiaoming Li. 2012. A novel burst-based text
representation model for scalable event detection. In
ACL?12.
Arkaitz Zubiaga, Damiano Spina, V??ctor Fresno, and
Raquel Mart??nez. 2011. Classifying trending topics:
a typology of conversation triggers on twitter. In Pro-
ceedings of the 20th ACM international conference on
Information and knowledge management, CIKM.
1477
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1337?1347,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Mining New Business Opportunities: Identifying Trend related Products by
Leveraging Commercial Intents from Microblogs
Jinpeng Wang1, Wayne Xin Zhao1, Haitian Wei2, Hongfei Yan1 and Xiaoming Li1
1School of Electronic Engineering and Computer Science, Peking University, China
2Daton Securities Co., Ltd., No.93 Jianguo Rd, Chaoyang District, Beijing, China
JooPoo@pku.edu.cn, {batmanfly,haataa.wei,yhf1029}@gmail.com, lxm@pku.edu.cn
Abstract
Hot trends are likely to bring new business
opportunities. For example, ?Air Pollution?
might lead to a significant increase of the sales
of related products, e.g., mouth mask. For e-
commerce companies, it is very important to
make rapid and correct response to these hot
trends in order to improve product sales. In
this paper, we take the initiative to study the
task of how to identify trend related products.
The major novelty of our work is that we au-
tomatically learn commercial intents revealed
from microblogs. We carefully construct a da-
ta collection for this task and present quite a
few insightful findings. In order to solve this
problem, we further propose a graph based
method, which jointly models relevance and
associativity. We perform extensive experi-
ments and the results showed that our methods
are very effective.
1 Introduction
A trend is a hot topic (e.g., the release of a popular
movie) which is being widely discussed by the pub-
lic. Hot trends usually attract much attention from
the public, and they are likely to bring new business
opportunities. Consider the following e-shopping s-
cenario. A user in Beijing would like to buy some-
thing to reduce the health impacts of Beijing air pol-
lution1. Different from traditional e-shopping sto-
ries, in this case the user may not have a clear idea of
what she should buy, and cannot even formulate the
1See http://www.nytimes.com/2013/04/04/world/asia/two-major-
air-pollutants-increase-in-china.html to find more details about the
trending topic ?Beijing Air Pollution?.
purchase needs into a clear query. Faced with trend-
driven business opportunities, e-commerce compa-
nies typically ask workers to manually identify relat-
ed products and make heuristic rules to match user
queries (e.g., incorporating trending keywords into
related product titles).
To improve trend-driven e-commerce, in this pa-
per, we propose to study the novel task of automat-
ically identifying trend related products. Why is
it compelling to understand and study trend-driven
product purchase? Because hot trends are closely
related to business opportunities directly or indirect-
ly. As a case of direct causal relationship, the world-
wide popularity of the movie series ?Harry Potter?
created the great success of the original novels of
?Harry Potter?. As a case of indirect causal rela-
tionship, the stock rise or salary increase might exert
positive effects on product sale. Based on our empir-
ical analysis (See Section 3), a considerable propor-
tion, i.e. 50%, of hot trends discussed on the largest
Chinese microblog (i.e. Sina Weibo) indeed have
corresponding product entries in the largest Chinese
C2C e-commerce website (i.e. Taobao), which in-
dicates a strong correlation between hot trends and
product sale.
Although the task is important and emergent, it
has at least two major challenges. First of all, how to
infer users? trend-driven purchase intents promptly.
A trend usually happens unexpectedly. Without pri-
or knowledge and experiences, it is particularly diffi-
cult to make rapid response to relate the trend to can-
didate products. Our solution is to leverage trend-
related commercial intents from microblogs by min-
ing users? real-time response to a trending topic. We
adopt the solution based on two key considerations:
(1) Microblogs are fast. As previous studies showed,
1337
the first story of a trending topic indeed was usu-
ally reported in microblogs rather than traditional
news media (Sakaki et al, 2010; Kwak et al, 2010;
Leskovec et al, 2009). (2) Microblogs contain user-
s? commercial intents. The microblogging service
has become one of the most popular social network
platforms, where users may tweet about their needs
and desires (Hollerit et al, 2013). E.g., a microblog
user may complain about the air quality and evince
the desire to buy a mouth mask in a tweet. The ex-
ample indicates we can make use of tweet-level re-
latedness to capture the correlation between trends
and products.
Second, how to achieve a comprehensive cover-
age of related products but not hurting precision.
The above solution will miss the related products
which have not been discussed in microblogs. Our
idea is to take the associativity between products in-
to consideration. Our definition about associativity
is very general and can have different instantiation-
s in specific settings. For example, we can define
product associativity to be the similarity between
product descriptions, or the ratio of historical pur-
chase records in e-commerce companies. Howev-
er, one-step associativity may not fully discover the
underlying relatedness between products due to the
fact that the product associativity is indeed transi-
tive. Thus, a transitable associativity model is need-
ed.
To address these two challenges, we propose a u-
nified graph based ranking algorithm which jointly
models the above two aspects, i.e., relevance and
associativity. Given a trend, the algorithm runs in
an iterative way and seeks a trade-off between rel-
evance and associativity by propagating the scores
on the product graph. Our contribution can be sum-
marized as follows: (1) we introduce the novel task
of identifying trend related products, most of all, we
propose to leverage trend-related commercial intents
from microblogs; (2) we present insightful empiri-
cal analysis to illustrate the correlation between hot
trends and product sale (See Section 3); (3) we pro-
pose a novel graph based ranking algorithm which
jointly considers relevance and associativity; (4) we
carefully construct the test collection based on re-
al data of the largest microblog and the largest C2C
e-commerce website in China. (5) we perform ex-
tensive experiments and present some important im-
plications for practice.
To the best of our knowledge, our work was the
first to consider identifying trend related products by
leveraging commercial intents from microblogs. We
believe the current work will have important impact
on industry and inspire more follow-up research s-
tudies. The rest of this paper is organized as fol-
lows. We present the data collection and empiri-
cal analysis of the impact of hot trends on product
sale in Section 3. We present a novel graph-based
method in Section 4. Experimental setup and result-
s are discussed in Section 5 and Section 6. Finally,
the related work is discussed in Section 7. And the
conclusions and future work are given in Section 8.
2 Problem Definition
A trend is a hot topic widely discussed by the public,
e.g., the release of a hot movie. Usually, a trend e
can be described by a small set of keywords denoted
by Ke and a corresponding time span Te.
Trend-related Products Identification: Given a
trend e, we assume that the following inputs are
available: 1) tweets that contain trend keywords Ke
and 2) a product database which provides a set of
candidate products P = {p1, p2, ..., pn} with nec-
essary detailed information, e.g., titles and descrip-
tions. The objective of trend-related products iden-
tification is to identify products in P that are related
to trend e within the time span Te, denoted by PR.
For convenience, we will not explicitly mention the
time span unless needed.
To better understand the problem, we first present
an illustrative example in Table 1, which will be dis-
cussed as the running case throughout the paper. In
this example, we can see that a few users tweet their
product needs related to the trend ?Air Pollution?.
We take Taobao as the product database and present
a few related products in it.
Table 1: An illustrative example for the studied task.
Trend keywords: Air Pollution
Tweets:
What bad air! We need to buy masks ASAP!!!
I am planing to buy an air purifier. Hoping it can defend air pollution.
#air pollution I will recommend to keep some houseplants at home.
Product database: Taobao2
Related products: Mouth Mask, Air Purifier, Houseplant
2The biggest C2C e-commerce site in China, similar to eBay.
1338
3 Data and Observations
As discussed earlier, hot trends may exert positive
effects on the sale of related products. In this sec-
tion, we will construct a deep analysis on this point
by presenting quantitative answers to the following
two problems:
? Q1: What is the proportion of hot trends that
potentially lead to business opportunities, and
how is their impact on related products?
? Q2: How is the associativity between related
products?
These findings are key and fundamental to develop
our models.
3.1 Data Collection
To perform the above analysis, the key is how to con-
struct an experimental data collection which relates
hot trends to corresponding related products. We
jointly consider microblogs and e-commerce plat-
forms: we obtain hot trends in microblogs and man-
ually identify trend-related products in e-commerce
websites. In this paper, we adopt Sina Weibo3
as the microbloging platform and Taobao4 as the
e-commerce platform, which are the biggest mi-
croblogging service and the largest C2C company in
China respectively. The analysis method is general
and can equally apply to other platforms. For both t-
wo data signals, we consider a two-month time span,
i.e. from May 2013 to June 2013.
Trend detection. Since trend detection is not our
focus, we directly obtained trends from ?trending
topics? provided by the microblog platform. Our
work can be easily extended to incorporate a trend
detection component. Similar to ?trending topic-
s? in Twitter, Sina Weibo provides a public list of
top searched keywords which can be obtained by
the Weibo search API5. In the list, top 50 keyword-
s are presented and ordered by the number of be-
ing searched. Weibo classifies these keywords in-
to five categories: China, movie, business, person
and sports. We consider these keywords to be trend
keywords. These keywords are dynamically updated
and we monitor the trend lists in the considered time
3http://www.weibo.com/
4http://www.taobao.com/
5http://s.weibo.com/top/summary
span. We define the start and end time of a trend to
be the first day and the last day on the trend list re-
spectively, which spans the active interval of a trend.
We only keep the trend which has an active interval
with more than one day. For each trend, we use the
trend keywords to retrieve all related tweets in the
active interval, and use the pattern based method in
(Hollerit et al, 2013) to extract all mentioned prod-
uct keywords. We present a few example patterns
used for extracting product keywords in Table 2. Af-
ter that we can obtain a set of product keywords for
each trend.
Table 2: Example patterns for extracting product key-
words.
Patterns Example segments of tweets
?(buy) ?
?|?SLxI?
bought father a Philips PT720 (Electric Razor).
?^(use) ?^N95???$?/
use N95 (mouth mask) to reduce the impact of bad air
? ?Galaxy S4
(recommend) recommend Galaxy S4 (cell phone)
Related product identification and annotation.
For each trend, we have the product keyword set to-
gether with the trend keywords as described above.
We use these keywords to retrieve candidate prod-
ucts in the product search engine of Taobao with-
in the active interval of the trend. For each can-
didate product, we further crawl its product page
and obtain corresponding related products suggest-
ed by Taobao, which are treated as candidate, too.
We invite two senior post-graduate students major
in economics as human judges. The judge is re-
quired to make a binary decision whether a product
is related to a trend by following a detailed guide-
line compiled by a senior officer of an e-commerce
company in Beijing. For each trend, we provide the
trend keywords, product keywords in tweets, relat-
ed tweets, related news articles from China Daily6.
Web access is available during the annotation pro-
cess. Due to space limit, we do not present the an-
notation guideline here. We use Cohen?s kappa to
measure the agreement of these two judges, which
has a high value of 0.75. To speed up the work,
we further group all products which have the same
lowest categorial label (e.g., leaf label) 7, and we
6http://www.chinadaily.com.cn
7Taobao has provided a category tree for products:
http://list.taobao.com/browse/cat-0.htm).
1339
will treat a group as a product in later experiments.
We only keep the products with the same judgments
and the trends with at least one related product. We
present the statistics of data set in Table 3 8. Since
current e-commerce search engines mainly adop-
t keyword matching based retrieval method, we fur-
ther examine the performance of simply using trend
keywords as queries. We compute the percentage
of related/unrelated products with at least one trend
keyword in their description. We can see that on-
ly 29.7% related products can be found on average.
These statistics indicate that more effective methods
are needed for the current task.
Table 3: Statitics of the data set.
# business-related trends 113
average candidate products per trend 55.1
average related products per trend 7.3
average perc. of rel. prod. with trend keywords 29.7%
average perc. of unrel. prod. with trend keywords 6.3%
3.2 Observations
Now we analyze the data collection and present our
observations.
A1: First of all, it is important to find out the pro-
portion of hot trends that potentially leads to busi-
ness opportunities. Recall that each trend has a cat-
egory label and possibly a set of related products i-
dentified by the judges. We refer to a trend with
related products as a business-related trend. We
present the statistics in Fig. 1. We can see that
about 36% of all trends have corresponding relat-
ed products in Taobao, which indicates that these
trends highly relate to business. Movies and Sport-
s have higher proportions of business-related trend-
s, i.e. 81% and 52% respectively, while the other
categories have lower proportions but still with a
substantial number of business related trends. It is
noteworthy that Business has the lowest proportion,
the major reason is that trends in Business are usual-
ly general events, i.e., the release of new economic
policy, which do not directly correspond to related
products. As we discussed earlier, these trends may
have indirect impact on product sales. Currently,
we only focus on direct impact, and indirect impacts
will be considered in future work.
8The data set can be downloaded at http://sewm.pku.
edu.cn/?wjp.
Next we continue to examine the impact of hot
trends on the sale of related products. We obtain
product sales from Taobao product pages. As we can
see in Fig. 2, the average sale of related products in
all categories gradually increased with trends going
on. Interestingly, we can see that categories Movies
and China achieved very significant increase. Prod-
ucts related to Movies trends are usually related to
the movie itself, e.g., movie tickets; while prod-
ucts related to China tend to be commodities (e.g.,
the mouth masks for the trend of ?Air Pollution?)
or trending products (e.g., Shenzhou-10 Spacecraft
Model for the trend of ?the launch of Shenzhou-
10?).
business person sports China movie
# tr
end
s
0
20
40
60
80
100
120
140
160
% b
usin
ess
-rea
ted 
tren
ds
0%
20%
40%
60%
80%
100%
busi.-related trends
other trend% busi.-reated trends
Figure 1: The proportion and volume of business-related
trends in five categories.
A2: Recall we have discussed that product asso-
ciativity is useful for improving the coverage of re-
lated products. Here we would like to quantitatively
examine the associativity between related products
given a trend. For a trend, we first compute the av-
erage pairwise similarity between related products
in terms of their descriptive texts (e.g. title and de-
scription). Since there are more unrelated products,
we randomly sample an equal number of unrelated
products from the candidate products we previously
generated. Then we compute the average similarity
between a related product and an unrelated product.
We further average these values over all the trends
of each category. The average similarity of related-
related product pairs is 0.112, while the average sim-
ilarities of unrelated-unrelated and related-unrelated
1340
Days0 1 2 3 4 5 6
Gro
wth
 rat
e of
 sal
es v
olum
e
1.00
1.02
1.04
1.06
1.08
1.10
1.12
1.14
1.16
1.18
businessperson
sportsChina
movie
Figure 2: An illustrative analysis of the impact on related
products in five categories. We measure the impact by
computing the average growth ratio of sale in Taobao.
product pairs are 0.039 and 0.058 respectively.9
In summary, A1 indicates that a large proportion
of hot trends are potentially related to products and
will exert positive effects on product sale; A2 in-
dicates that there is a strong associativity between
related products, which can be utilized to improve
both precision and recall of the algorithm.
4 The Proposed Method
In this section, we present a graph based ranking al-
gorithm jointly models the relevance of a product
and the associativity between products. Recall that
we have collected a set of product keywords and a
set of candidate products for each trend. Our aim is
to re-rank these candidate products to obtain a better
ranking of related products. We adopt a biased ran-
dom walk algorithm: 1) relevance is modeled as bi-
ased restart probability and 2) associativity is mod-
eled through random walk on the product graph.
4.1 Modeling the Product Relevance
Recall that in Section 2 we use the pattern based
method to extract product keywords from tweets re-
lated to a trend. However, to stimulate the real sce-
nario that we want to identify the related products at
the beginning of a trend, we only keep the keyword-
s which were contained in tweets published in the
first three days when a trend began. These extracted
9The difference was tested to be statistically significant.
product keywords directly reveal users? commercial
intents on the trends. Instead of modeling person-
alized intents, we consider learning a unified trend-
driven intent by representing the intent as a weighted
vector over these product keywords. And the key is
how to set the keyword weight.
Keyword weighting. A good weighting method
should be able to leverage commercial interest-
s/intents of users well and emphasize the keyword-
s users really focus on. Thus we consider making
use of the retweeting (a.k.a. forwarding) mechanis-
m in microblogs. Retweet links are shown to be bet-
ter in revealing relevance and interests (Welch et al,
2011). Formally, we use the following weighting
formula for a keyword k:
Weight(k) =
?
t?Ck
log10 (#rtt + 1), (1)
where Ck is the set of all originally-written tweets
(i.e., not a retweet) that contain the keyword k in
the considered time span, and #rtt is the retweet
number of a tweet t. We further normalize and build
the weight vector over all the considered keywords,
called as intent vector. We denote the intent vector
of a trend e by ~e.
Product relevance. Having the intent vector, now
we discuss about how to define the product rele-
vance. Given a product p, we extract all the words in
the title and description parts of a product. We rep-
resent it as a vector using the widely tf-idf weighting
method. We denote the weight vector of product p
by ~p. We measure the product relevance between e
and p as rel(e, p) = ~e?~p|~e||~p| .
4.2 Modeling the Associativity between
Products
To start this part, we first present an illustrative ex-
ample in Fig. 3. We can see there are four relat-
ed products for the trend ?Air Pollution?. We as-
sume that only ?mouth mask? was mentioned in mi-
croblogs. Now we expect to mine more related prod-
ucts with ?mouth mask? as a known related produc-
t. We can compute the similarity between a pair of
products. Intuitively, if the similarity between a can-
didate product and ?mouth mask? is higher than a
predefined threshold, we can consider it to be relat-
ed, too. In this example, ?air detector? and ?air pu-
rifier? are similar to ?mouth mask? in terms of prod-
1341
Figure 3: An example to illustrate the importance of as-
sociativity. Products which were mentioned in tweets related to ?Air
Pollution? are marked in red circles while the others are marked in blue
circles. The link between products indicate the similarity between two
products. Links with weights lower than a predefined threshold are not
considered. Although ?green plants? is related to this trend, it was not
mentioned in tweets and did not have a direct link to ?mouth mask?.
uct descriptions and considered to be related, while
?Green plants? is determined to be unrelated since
it has very little overlap words with ?mouth mask?
in the description. It indicates one-step similarity
method is not able to fully capture the real associa-
tivity between products.
Thus, we propose to use the random walk method
to propagate the relatedness score on the produc-
t graph. Let P denote the number of all the can-
didate products, and rP?1 denote the relatedness s-
core vector where ri denote the relatedness score of
product pi.
We first construct the product graph. We repre-
sent each candidate product as a vertex in the graph
and built the link with the cosine similarity between
the descriptive texts of two products as the link
weight.10 We denote the similarity matrix byMP?P
and Mi,j denotes the similarity between products pi
and pj . Formally, we formulate the problem in a s-
tandard PageRank form
r(n+1) = ? ? r(n) ?M+ (1? ?) ? y, (2)
where y is the restart probability vector usually
set to be uniform. With this method, it is easy to
see that relatedness score can be propagated on the
product graph, which better captures underlying as-
sociativity between products.
10Other similarity methods can be used, e.g., co-purse history
record.
4.3 Jointly Modeling Relevance and
Associativity
Having discussed about how to model both rele-
vance and associativity, now we are ready to present
a joint model to capture these two factors. By fol-
lowing (Zhao et al, 2013), the main idea is that in-
stead of using a uniform restart distribution y, we
use an relevance biased restart distribution in E-
q. 2. We set the restart probability of a produc-
t to its corresponding relevance. Formally, we set
yi = rel(e, pi). Let us further explain the idea. At
the beginning of each iteration, each product is first
assigned to its relevance score: the more relevant it
is, the larger score it has. During the iteration, each
product begins to collect relevance evidence from its
neighbors on the product graph: the more relevan-
t neighbors it has, the larger score it obtains. And
the final score is indeed a trade-off between its own
relevance score and neighboring relevance scores it
receives. In order to obtain an ergodic walk, we add
a small value, i.e. 1e ? 4, to each entry of y and
then normalize this vector. We denote our algorithm
as JMRA (Jointly Modeling Relevance and Associa-
tivity).
To have an intuitive understanding of our algo-
rithm, let us turn to the example in Fig. 3 again. At
the beginning, only ?mouth mask? has a large rele-
vance score, with the iteration going on, the related-
ness score will be propagated between products on
the graph. Although ?green plants? has not a direct
link with ?mouth mask?, it can obtain relatedness s-
core from its neighbors, i.e. ?air detector? and ?air
purifier?. JMRA is able to discover such latent asso-
ciativity between products.
5 Experimental Setup
We use the test collection which have been described
in Section 3. The statistics of the data set is shown
in Table 3.
5.1 Evaluation Metrics
For a real product search engine, top results are par-
ticularly important, thus we adopt precision@5 and
precision@10 as the evaluation metrics. Similar to
Information Retrieval, we also consider using Mean
Average Precision (MAP) as metrics to measure the
overall quality of retrieved products.
1342
5.2 Methods to Compare
We compare the following methods for inferring re-
lating products:
SALES: we rank the candidate products by their
historical sales volume descendingly.
TREND: we use trend keywords as queries and rank
the products by their relevance.
TREND+fb: based on TREND, we further incorpo-
rate pseudo-relevance feedback (Salton, 1971;
Salton and Buckley, 1997). After some tun-
ing (See Section 6.5), top 3 search results were
used to update the query.
JMRAr: it is our method which only considers
product relevance in Section 4.1.
JMRAr + fb: we further apply pseudo-relevance
feedback to JMRAr.
JMRAr+a: it is our method which considers both
relevance and associativity in Section 4.3.
JMRAr+a+fb: we further apply pseudo-relevance
feedback to JMRAr+a.
6 Experimental Results and Analysis
In this section, we first evaluate the performance of
the proposed approach and the comparison method-
s. Next, we analyze a problem in real e-commerce
search engines, i.e., the cold start. Then, we give
a qualitative case study to further demonstrate the
effectiveness of the proposed approach. Finally,
we examine the parameter sensitivity to the perfor-
mance.
6.1 Comparison of Performance
We present the results of various methods in Table
4. We first examine the performance of baselines
SALES, TREND and TREND+fb. First, SALES has
the worst performance due to the fact that a trend
usually happen unexpectedly and historical records
may not predict it well. The second observation is
that the improvement of TREND+fb over TREND is
little. This is mainly because that very few related
products can be identified only based on trend key-
words so feedback method does not work very well
on it.
Then we compare our relevance based method-
s with the above three baselines. Note that the
major difference between JMRAr and TREND is
that JMRAr makes uses of both trend keywords
and product keywords extracted from microblogs.
We can see that JMRAr performs better than al-
l three baselines. It proves the effectiveness of lever-
aging commercial intents from microblogs. An-
other interesting point is that the relative improve-
ment JMRAr+fb over JMRAr is larger than that
TREND+fb over TREND. The reason is that pseudo
relevance feedback relies highly on top results and a
system with better search quality will benefit more
from it.
Finally, we consider evaluating our full models
which jointly consider relevance and associativity. It
is easy to see JMRAr+a yields a significant improve-
ment over JMRAr and even outperforms JMRAr+fb.
This observation supports our assumption that prod-
uct associativity is very important in this task. A-
gain, pseudo relevance feedback has also improved
JMRAr+a.
In summary, our results have shown some impor-
tant implications for trend-related product retrieval
on e-commerce search engines: 1) microblogs are
very good signals to learn users? commercial intents;
2) product associativity is particularly important; 3)
other advanced retrieval methods are potentially use-
ful, e.g., pseudo relevance feedback.
Table 4: The overall performance of all the methods.
Models P@5 P@10 MAP
SALES 0.345 0.379 0.225
TREND 0.543 0.325 0.327
TREND+fb 0.550 0.325 0.328
JMRAr 0.611 0.527 0.336
JMRAr+fb 0.661 0.552 0.348
JMRAr+a 0.733 0.609 0.392
JMRAr+a+fb 0.734 0.624 0.404
6.2 Cold Start
It is noteworthy that we have considered all the can-
didate products within the entire active interval of a
trend when constructing the test collection. This is
mainly to obtain a good coverage of related product-
s since some e-commerce companies might release
new products as the response to a trend. During the
active interval of a trend, the e-commerce companies
may make some heuristic rules to enhance the re-
trieval of related products, e.g., incorporating trend
keywords into product titles and descriptions. In the
1343
real application scenario, an effective method is ex-
pected to identify related products at the beginning
of a trend when the e-commerce workers may not
make any response to the trend. How would it be
if we do not have the manually generated trend key-
words from workers in product titles and descrip-
tions?
To answer the question, in this part, we contin-
ue to examine the impact of cold start on different
methods. We select three methods as comparisons,
i.e., TREND+fb, JMRAr + fb and JMRAr+a + fb.
We first use keyword matching methods to obtain
all the products that related to trend keywords. The
descriptive text (i.e., title and description) of these
products has been refined to match trend queries by
sellers in e-commerce websites. We further removed
all the trend keywords in the desriptive text of these
products, and gradually add the trend keywords back
to original products. In such a process, we would
like to examine how cold start affects the perfor-
mance of different methods.
0% 20% 40% 60% 80% 100%
P@
10
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
JMRAr + fbJMRAr+a + fbTREND + fb
Figure 4: The impact of cold start for different meth-
ods.
We present the results in Fig. 4. First, per-
formance of all these methods improve with the
increase of products with trend keywords. Sec-
ond, cold start does not affect the relative perfor-
mance order of different methods, i.e., TREND+fb
< JMRAr + fb < JMRAr+a + fb. Finally, all the
methods display similar impact patterns: ?signifi-
cantli increasing? ? ?stable?. An interesting ob-
servation is that JMRAr + fb and JMRAr+a + fb
have much more stable performance compared to
TREND+fb. It indicates that our methods are very
robust to the cold start, and potentially applicable in
real e-commerce search engines.
6.3 Case Study
In order to have a intuitive understanding of how
different method perform, we present a case study
in this part. We select JMRAr, JMRAr+fb and
JMRAr+a+fb as comparisons. The results are shown
in Table 5. We can see that JMRAr+a+fb have iden-
tified the most related products. We further analyze
the contribution of different factors. Compared with
JMRAr, JMRAr+fb has got one more related prod-
uct ?Houseplant? due to the reason pseudo feedback
can make use of top related search results to im-
prove the queries. In this case, ?Houseplant? is i-
dentified to be related because it is very similar to
?Air Detector? (We have presented the correspond-
ing most associative products in brackets in Table 5).
Similarly, the comparison between JMRAr+fb and
JMRAr+a+fb shows the effectiveness of product as-
sociativity.
6.4 Error Analysis
To further understand the shortcomings of the pro-
posed methods, we use the example in Table 5 for er-
ror analysis. Based on our manual inspection, errors
may arise from two major sources for our method:
? Product keyword extraction errors: we use
a pattern-based product keyword extraction
method, and it tends to incorporate some irrel-
evant words. For example, given the topic ?air
pollution?, users would talk about the impact
of ?car exhaust? on air quality and advocate to
reduce automobile usage and sale. The current
keyword extraction method might mistake the
word ?car? for a product related keyword.
? Search engine retrieval errors: in this paper,
we rely on the Taobao product search engine
for candidate product generation. It is high-
ly based on surface-form matching to retrieve
related products. Therefore, given a query
?mouth mask?, it might return some irrelevant
products, e.g., ?party mask?. Clearly, pseudo-
relevance feedback will also bring additional ir-
relevant products if top search results contain
irrelevant ones.
1344
Table 5: A qualitative comparison of three methods on the topic of ?Air Pollution?. We mark related products in bold.
Sample keywords learnt from microblogs:
air pollution, mouth mask, air, air purifier, respirator, house, mask,
warm, bus, car, purified water
JMRAr JMRAr+fb JMRAr+a+fb
Mouth Mask Mouth Mask Mouth Mask
Air Detector Air Detector Air Purifier
Air Purifier Air Purifier Air Detector
Toy House Humidifier Respirator
Respirator Respirator Oxygen Bag (Mouth Mask)
Toy Car Party Mask Humidifier
Environment-friendly Bags Toy Car Houseplant (Air Detector)
Humidifier Houseplant (Air Detector) Anti-pollution Medicine (Oxygen Bag)
Purified Water Environment-friendly bags Purified Water
Warmer Purified Water Party Mask
(a)
0 2 4 6 8 10
P@10
0.54
0.56
0.58
0.60
0.62
0.64 JMRAr + fbJMRAr+a + fb
(b)
Figure 5: Parameter sensitivity. a) The impact of the
damping factor ? and b) the impact of the number of top
products used for pseudo feedback.
To solve these problems, one promising way is to
leverage more context information about the candi-
date products and construct deep semantic analysis.
We will leave it as future work.
6.5 Parameter Sensitivity
The only parameter for JMRA is the damping fac-
tor in the random walk model, i.e., ?. Intuitively,
a larger value of ? emphasize the associativity more
while a smaller value emphasize the relevance more.
We tune this parameter at a step of 0.1 and present
the results in Fig. 5(a). We can see the performance
of JMRAr+a+fb is consistently better than that of
JMRAr+fb and peaks at around ?0.8?. It indicates
the robustness of JMRAr+a+fb and the importance
of product associtivity.
We further examine the impact of the num-
ber of top products used for pseudo feedback for
JMRAr+fb and JMRAr+a+fb. In Fig. 5(b), we can
see that both JMRAr+a+fb and JMRAr+fb achieved
their best at ?3?. It indicates that we only need to
consider very top results for pseudo feedback.
6.6 Title or Description?
In previous experiments, for each product, we used
the descriptive text in both title and description. In
this part, we consider examining the individual ef-
fect of title and description. We use JMRAr+a+fb as
the examined method since both relevance and asso-
ciativity relies on the text information.
Table 6: Evaluating the performance of JMRAr+a+fb
with different text sources.
sources P@5 P@10 MAP
title 0.690 0.591 0.364
description 0.711 0.602 0.387
title+description 0.734 0.624 0.404
As shown in Table 6, we can see that the perfor-
mance of only using description is better than that
of only using title and a combination of both parts
achieve the best. title is usually carefully compiled
by e-commerce sellers, thus it reveals the most high-
lights of the products but very short; while descrip-
tion contains more informative text but tends to in-
corporate noise. In future work, we will consider a
more principled way to combine title and descrip-
tion, e.g., weighted combination.
1345
6.7 Efficiency
Finally, we present a few discussions about the issue
of efficiency. All codes were implemented in Python
2.7, and all experiments were performed on a PC
with Intel(R) Core(TM)i5 CPU 760 @ 2.8GHz and
8GB memory.
Sicne we group products by the categorial label,
the number of candidate products is usually very s-
mall. Thus, our method JMRA runs very efficiently.
Even on an extremely large set of candidate product-
s, the iterative random walk algorithm can be easily
implemented in a distributed way (Bahmani et al,
2011) and would have very good efficiency.
7 Related Work
Our work is mainly related to the following lines:
Mining the microblogs. Microblogs have been
one of the most popular social networking platform-
s, and they have recently attracted much attention
from research communities. The studies on trend
(or event) detection (Benson et al, 2011; Weng and
Lee, 2011; Sakaki et al, 2010; Zhao et al, 2012)
tried to make use of the rapid response of microblogs
users as the signal to automatically identify exter-
nal events. Another important aspect is the content
analysis of tweets, including the recommendation of
real-time topical news (Phelan et al, 2009), senti-
ment or opinions analysis (Meng et al, 2012), event
summarization using tweets (Chakrabarti and Puner-
a, 2011; Lin et al, 2012; Zhao et al, 2013), etc. Our
work do not explicitly incorporate a trend detection
component, instead we make use of the trending top-
ics provided by the microblogs platforms. It will be
easy to incorporate other trend detection methods as
our input.
Identifying online users? commercial intents.
The identification of online users? commercial in-
tents has been quite an important research prob-
lem in the past. Most researches focus on captur-
ing commercial intention from search queries (Dai
et al, 2006; Strohmaier and Kro?ll, 2012), click-
through behaviors (Ashkan and Clarke, 2009), user-
s? mouse movements or scrolling behaviors (Guo
and Agichtein, 2010) and search logs (Strohmaier
and Kro?ll, 2012). The most related to our work is
the work in (Hollerit et al, 2013), which attempts to
detect commercial intent on twitter. But we have
very different focus. They aim to identify tweet-
level commercial intents while ours aim to identify
trend-driven commercial intents. In addition, we al-
so present how to make use of these identified intents
and our paper focuses on how to identify trend relat-
ed products for e-commerce companies to improve
service when faced with hot trends.
8 Conclusions
In this paper, we make the first attempt to identify
trend related products by leveraging commercial in-
tents from microblogs. We propose a way to con-
struct the evaluation set for this task and present
some insightful findings. We propose a graph based
method to joint model relevance and associativity.
We perform extensive experiments, including quan-
titative and qualitative analysis.
Currently, our approach is indeed a framework
to solve this task, and we may consider improving
the individual components in it, e.g. consider non-
product keywords in tweets. For future work, we
will consider incorporating a trend detection com-
ponent into our method, which can be more flexible
to adapt to various trend signals. We can also refine
the method of the product keyword extraction by us-
ing more principled solutions.
Acknowledgments
We thank the anonymous reviewers for the construc-
tive comments. The work was partially supported by
NSFC Grant 60933004, 61073082 and 61272340.
Jinpeng Wang was supported by the Singapore Na-
tional Research Foundation under its IDM Futures
Funding Initiative and administered by the Interac-
tive & Digital Media Programme Office, Media De-
velopment Authority. We thank Taobao for the ac-
cess to the product data and all Taobao data in this
paper will be only used for research purpose.
References
Azin Ashkan and Charles LA Clarke. 2009. Term-
based commercial intent analysis. In Proceedings
of the 32nd international ACM SIGIR conference on
Research and development in information retrieval,
pages 800?801. ACM.
Bahman Bahmani, Kaushik Chakrabarti, and Dong Xin.
2011. Fast personalized pagerank on mapreduce. In
1346
Proceedings of the 2011 ACM SIGMOD Internation-
al Conference on Management of data, SIGMOD ?11,
pages 973?984, New York, NY, USA. ACM.
Edward Benson, Aria Haghighi, and Regina Barzilay.
2011. Event discovery in social media feeds. In Pro-
ceedings of the 49th Annual Meeting of the Associa-
tion for Computational Linguistics: Human Language
Technologies-Volume 1, pages 389?398. Association
for Computational Linguistics.
Deepayan Chakrabarti and Kunal Punera. 2011. Event
summarization using tweets. In Proceedings of the 5th
Int?l AAAI Conference on Weblogs and Social Media
(ICWSM), July.
Honghua Kathy Dai, Lingzhi Zhao, Zaiqing Nie, Ji-Rong
Wen, Lee Wang, and Ying Li. 2006. Detecting online
commercial intention (oci). In Proceedings of the 15th
international conference on World Wide Web, pages
829?837. ACM.
Qi Guo and Eugene Agichtein. 2010. Ready to buy or
just browsing?: detecting web searcher goals from in-
teraction data. In Proceedings of the 33rd internation-
al ACM SIGIR conference on Research and develop-
ment in information retrieval, pages 130?137. ACM.
Bernd Hollerit, Mark Kro?ll, and Markus Strohmaier.
2013. Towards linking buyers and sellers: detect-
ing commercial intent on twitter. In Proceedings of
the 22nd international conference on World Wide Web
companion, pages 629?632. International World Wide
Web Conferences Steering Committee.
Haewoon Kwak, Changhyun Lee, Hosung Park, and Sue
Moon. 2010. What is Twitter, a social network or a
news media? In WWW ?10: Proceedings of the 19th
international conference on World wide web, pages
591?600. ACM.
Jure Leskovec, Lars Backstrom, and Jon Kleinberg.
2009. Meme-tracking and the dynamics of the news
cycle. In Proceedings of the 15th ACM SIGKDD inter-
national conference on Knowledge discovery and data
mining, KDD ?09, pages 497?506, New York, NY, US-
A. ACM.
Chen Lin, Chun Lin, Jingxuan Li, Dingding Wang, Yang
Chen, and Tao Li. 2012. Generating event storylines
from microblogs. In Proceedings of the 21st ACM in-
ternational conference on Information and knowledge
management, pages 175?184. ACM.
Xinfan Meng, Furu Wei, Xiaohua Liu, Ming Zhou, Sujian
Li, and Houfeng Wang. 2012. Entity-centric topic-
oriented opinion summarization in twitter. In Proceed-
ings of the 18th ACM SIGKDD international confer-
ence on Knowledge discovery and data mining, pages
379?387. ACM.
Owen Phelan, Kevin McCarthy, and Barry Smyth. 2009.
Using twitter to recommend real-time topical news. In
Proceedings of the third ACM conference on Recom-
mender systems, pages 385?388. ACM.
Takeshi Sakaki, Makoto Okazaki, and Yutaka Matsuo.
2010. Earthquake shakes twitter users: real-time event
detection by social sensors. In Proceedings of the 19th
international conference on World wide web, WWW
?10, pages 851?860, New York, NY, USA. ACM.
Gerard Salton and Chris Buckley. 1997. Readings in
information retrieval. chapter Improving retrieval per-
formance by relevance feedback, pages 355?364. Mor-
gan Kaufmann Publishers Inc., San Francisco, CA,
USA.
Gerard Salton, editor. 1971. The SMART Retrieval Sys-
tem - Experiments in Automatic Document Processing.
Prentice Hall, Englewood, Cliffs, New Jersey.
Markus Strohmaier and Mark Kro?ll. 2012. Acquiring
knowledge about human goals from search query logs.
Information Processing & Management, 48(1):63?82.
Michael J. Welch, Uri Schonfeld, Dan He, and Junghoo
Cho. 2011. Topical semantics of twitter links. In Pro-
ceedings of the fourth ACM international conference
on Web search and data mining, WSDM ?11, pages
327?336, New York, NY, USA. ACM.
Jianshu Weng and Bu-Sung Lee. 2011. Event detection
in Twitter. In Proceedings of the Fifth International
Conference on Weblogs and Social Media (ICWSM),
Menlo Park, CA, USA. AAAI.
Wayne Xin Zhao, Baihan Shu, Jing Jiang, Yang Song,
Hongfei Yan, and Xiaoming Li. 2012. Identifying
event-related bursts via social media activities. In
EMNLP-CoNLL, pages 1466?1477.
Wayne Xin Zhao, Yanwei Guo, Rui Yan, Yulan He, and
Xiaoming Li. 2013. Timeline generation with so-
cial attention. In Proceedings of the 36th international
ACM SIGIR conference on Research and development
in information retrieval, SIGIR ?13, pages 1061?1064.
1347
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 379?388,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Topical Keyphrase Extraction from Twitter
Wayne Xin Zhao? Jing Jiang? Jing He? Yang Song? Palakorn Achananuparp?
Ee-Peng Lim? Xiaoming Li?
?School of Electronics Engineering and Computer Science, Peking University
?School of Information Systems, Singapore Management University
{batmanfly,peaceful.he,songyangmagic}@gmail.com,
{jingjiang,eplim,palakorna}@smu.edu.sg, lxm@pku.edu.cn
Abstract
Summarizing and analyzing Twitter content is
an important and challenging task. In this pa-
per, we propose to extract topical keyphrases
as one way to summarize Twitter. We propose
a context-sensitive topical PageRank method
for keyword ranking and a probabilistic scor-
ing function that considers both relevance and
interestingness of keyphrases for keyphrase
ranking. We evaluate our proposed methods
on a large Twitter data set. Experiments show
that these methods are very effective for topi-
cal keyphrase extraction.
1 Introduction
Twitter, a new microblogging website, has attracted
hundreds of millions of users who publish short
messages (a.k.a. tweets) on it. They either pub-
lish original tweets or retweet (i.e. forward) oth-
ers? tweets if they find them interesting. Twitter
has been shown to be useful in a number of appli-
cations, including tweets as social sensors of real-
time events (Sakaki et al, 2010), the sentiment pre-
diction power of Twitter (Tumasjan et al, 2010),
etc. However, current explorations are still in an
early stage and our understanding of Twitter content
still remains limited. How to automatically under-
stand, extract and summarize useful Twitter content
has therefore become an important and emergent re-
search topic.
In this paper, we propose to extract keyphrases
as a way to summarize Twitter content. Tradition-
ally, keyphrases are defined as a short list of terms to
summarize the topics of a document (Turney, 2000).
It can be used for various tasks such as document
summarization (Litvak and Last, 2008) and index-
ing (Li et al, 2004). While it appears natural to use
keyphrases to summarize Twitter content, compared
with traditional text collections, keyphrase extrac-
tion from Twitter is more challenging in at least two
aspects: 1) Tweets are much shorter than traditional
articles and not all tweets contain useful informa-
tion; 2) Topics tend to be more diverse in Twitter
than in formal articles such as news reports.
So far there is little work on keyword or keyphrase
extraction from Twitter. Wu et al (2010) proposed
to automatically generate personalized tags for Twit-
ter users. However, user-level tags may not be suit-
able to summarize the overall Twitter content within
a certain period and/or from a certain group of peo-
ple such as people in the same region. Existing work
on keyphrase extraction identifies keyphrases from
either individual documents or an entire text collec-
tion (Turney, 2000; Tomokiyo and Hurst, 2003).
These approaches are not immediately applicable
to Twitter because it does not make sense to ex-
tract keyphrases from a single tweet, and if we ex-
tract keyphrases from a whole tweet collection we
will mix a diverse range of topics together, which
makes it difficult for users to follow the extracted
keyphrases.
Therefore, in this paper, we propose to study the
novel problem of extracting topical keyphrases for
summarizing and analyzing Twitter content. In other
words, we extract and organize keyphrases by top-
ics learnt from Twitter. In our work, we follow the
standard three steps of keyphrase extraction, namely,
keyword ranking, candidate keyphrase generation
379
and keyphrase ranking. For keyword ranking, we
modify the Topical PageRank method proposed by
Liu et al (2010) by introducing topic-sensitive score
propagation. We find that topic-sensitive propaga-
tion can largely help boost the performance. For
keyphrase ranking, we propose a principled proba-
bilistic phrase ranking method, which can be flex-
ibly combined with any keyword ranking method
and candidate keyphrase generation method. Ex-
periments on a large Twitter data set show that
our proposed methods are very effective in topical
keyphrase extraction from Twitter. Interestingly, our
proposed keyphrase ranking method can incorporate
users? interests by modeling the retweet behavior.
We further examine what topics are suitable for in-
corporating users? interests for topical keyphrase ex-
traction.
To the best of our knowledge, our work is the
first to study how to extract keyphrases from mi-
croblogs. We perform a thorough analysis of the
proposed methods, which can be useful for future
work in this direction.
2 Related Work
Our work is related to unsupervised keyphrase ex-
traction. Graph-based ranking methods are the
state of the art in unsupervised keyphrase extrac-
tion. Mihalcea and Tarau (2004) proposed to use
TextRank, a modified PageRank algorithm to ex-
tract keyphrases. Based on the study by Mihalcea
and Tarau (2004), Liu et al (2010) proposed to de-
compose a traditional random walk into multiple
random walks specific to various topics. Language
modeling methods (Tomokiyo and Hurst, 2003) and
natural language processing techniques (Barker and
Cornacchia, 2000) have also been used for unsuper-
vised keyphrase extraction. Our keyword extraction
method is mainly based on the study by Liu et al
(2010). The difference is that we model the score
propagation with topic context, which can lower the
effect of noise, especially in microblogs.
Our work is also related to automatic topic label-
ing (Mei et al, 2007). We focus on extracting topical
keyphrases in microblogs, which has its own chal-
lenges. Our method can also be used to label topics
in other text collections.
Another line of relevant research is Twitter-
related text mining. The most relevant work is
by Wu et al (2010), who directly applied Tex-
tRank (Mihalcea and Tarau, 2004) to extract key-
words from tweets to tag users. Topic discovery
from Twitter is also related to our work (Ramage et
al., 2010), but we further extract keyphrases from
each topic for summarizing and analyzing Twitter
content.
3 Method
3.1 Preliminaries
Let U be a set of Twitter users. Let C =
{{du,m}
Mu
m=1}u?U be a collection of tweets gener-
ated by U , where Mu is the total number of tweets
generated by user u and du,m is the m-th tweet of
u. Let V be the vocabulary. du,m consists of a
sequence of words (wu,m,1, wu,m,2, . . . , wu,m,Nu,m)
where Nu,m is the number of words in du,m and
wu,m,n ? V (1 ? n ? Nu,m). We also assume
that there is a set of topics T over the collection C.
Given T and C, topical keyphrase extraction is to
discover a list of keyphrases for each topic t ? T .
Here each keyphrase is a sequence of words.
To extract keyphrases, we first identify topics
from the Twitter collection using topic models (Sec-
tion 3.2). Next for each topic, we run a topical
PageRank algorithm to rank keywords and then gen-
erate candidate keyphrases using the top ranked key-
words (Section 3.3). Finally, we use a probabilis-
tic model to rank the candidate keyphrases (Sec-
tion 3.4).
3.2 Topic discovery
We first describe how we discover the set of topics
T . Author-topic models have been shown to be ef-
fective for topic modeling of microblogs (Weng et
al., 2010; Hong and Davison, 2010). In Twit-
ter, we observe an important characteristic of tweets:
tweets are short and a single tweet tends to be about
a single topic. So we apply a modified author-topic
model called Twitter-LDA introduced by Zhao et al
(2011), which assumes a single topic assignment for
an entire tweet.
The model is based on the following assumptions.
There is a set of topics T in Twitter, each represented
by a word distribution. Each user has her topic inter-
ests modeled by a distribution over the topics. When
a user wants to write a tweet, she first chooses a topic
based on her topic distribution. Then she chooses a
380
1. Draw ?B ? Dir(?), pi ? Dir(?)
2. For each topic t ? T ,
(a) draw ?t ? Dir(?)
3. For each user u ? U ,
(a) draw ?u ? Dir(?)
(b) for each tweet du,m
i. draw zu,m ? Multi(?u)
ii. for each word wu,m,n
A. draw yu,m,n ? Bernoulli(pi)
B. draw wu,m,n ? Multi(?B) if
yu,m,n = 0 and wu,m,n ?
Multi(?zu,m) if yu,m,n = 1
Figure 1: The generation process of tweets.
bag of words one by one based on the chosen topic.
However, not all words in a tweet are closely re-
lated to the topic of that tweet; some are background
words commonly used in tweets on different topics.
Therefore, for each word in a tweet, the user first
decides whether it is a background word or a topic
word and then chooses the word from its respective
word distribution.
Formally, let ?t denote the word distribution for
topic t and ?B the word distribution for background
words. Let ?u denote the topic distribution of user
u. Let pi denote a Bernoulli distribution that gov-
erns the choice between background words and topic
words. The generation process of tweets is described
in Figure 1. Each multinomial distribution is gov-
erned by some symmetric Dirichlet distribution pa-
rameterized by ?, ? or ?.
3.3 Topical PageRank for Keyword Ranking
Topical PageRank was introduced by Liu et al
(2010) to identify keywords for future keyphrase
extraction. It runs topic-biased PageRank for each
topic separately and boosts those words with high
relevance to the corresponding topic. Formally, the
topic-specific PageRank scores can be defined as
follows:
Rt(wi) = ?
?
j:wj?wi
e(wj , wi)
O(wj)
Rt(wj)+ (1??)Pt(wi),
(1)
where Rt(w) is the topic-specific PageRank score
of word w in topic t, e(wj , wi) is the weight for the
edge (wj ? wi), O(wj) =
?
w? e(wj , w
?) and ?
is a damping factor ranging from 0 to 1. The topic-
specific preference value Pt(w) for each word w is
its random jumping probability with the constraint
that
?
w?V Pt(w) = 1 given topic t. A large Rt(?)
indicates a word is a good candidate keyword in
topic t. We denote this original version of the Topi-
cal PageRank as TPR.
However, the original TPR ignores the topic con-
text when setting the edge weights; the edge weight
is set by counting the number of co-occurrences of
the two words within a certain window size. Tak-
ing the topic of ?electronic products? as an exam-
ple, the word ?juice? may co-occur frequently with a
good keyword ?apple? for this topic because of Ap-
ple electronic products, so ?juice? may be ranked
high by this context-free co-occurrence edge weight
although it is not related to electronic products. In
other words, context-free propagation may cause the
scores to be off-topic.
So in this paper, we propose to use a topic context
sensitive PageRank method. Formally, we have
Rt(wi) = ?
?
j:wj?wi
et(wj , wi)
Ot(wj)
Rt(wj)+(1??)Pt(wi).
(2)
Here we compute the propagation from wj to wi in
the context of topic t, namely, the edge weight from
wj to wi is parameterized by t. In this paper, we
compute edge weight et(wj , wi) between two words
by counting the number of co-occurrences of these
two words in tweets assigned to topic t. We denote
this context-sensitive topical PageRank as cTPR.
After keyword ranking using cTPR or any other
method, we adopt a common candidate keyphrase
generation method proposed by Mihalcea and Tarau
(2004) as follows. We first select the top S keywords
for each topic, and then look for combinations of
these keywords that occur as frequent phrases in the
text collection. More details are given in Section 4.
3.4 Probabilistic Models for Topical Keyphrase
Ranking
With the candidate keyphrases, our next step is to
rank them. While a standard method is to simply
aggregate the scores of keywords inside a candidate
keyphrase as the score for the keyphrase, here we
propose a different probabilistic scoring function.
Our method is based on the following hypotheses
about good keyphrases given a topic:
381
Figure 2: Assumptions of variable dependencies.
Relevance: A good keyphrase should be closely re-
lated to the given topic and also discriminative. For
example, for the topic ?news,? ?president obama? is
a good keyphrase while ?math class? is not.
Interestingness: A good keyphrase should be inter-
esting and can attract users? attention. For example,
for the topic ?music,? ?justin bieber? is more inter-
esting than ?song player.?
Sometimes, there is a trade-off between these two
properties and a good keyphrase has to balance both.
Let R be a binary variable to denote relevance
where 1 is relevant and 0 is irrelevant. Let I be an-
other binary variable to denote interestingness where
1 is interesting and 0 is non-interesting. Let k denote
a candidate keyphrase. Following the probabilistic
relevance models in information retrieval (Lafferty
and Zhai, 2003), we propose to use P (R = 1, I =
1|t, k) to rank candidate keyphrases for topic t. We
have
P (R = 1, I = 1|t, k)
= P (R = 1|t, k)P (I = 1|t, k, R = 1)
= P (I = 1|t, k, R = 1)P (R = 1|t, k)
= P (I = 1|k)P (R = 1|t, k)
= P (I = 1|k)?
P (R = 1|t, k)
P (R = 1|t, k) + P (R = 0|t, k)
= P (I = 1|k)?
1
1 + P (R=0|t,k)P (R=1|t,k)
= P (I = 1|k)?
1
1 + P (R=0,k|t)P (R=1,k|t)
= P (I = 1|k)?
1
1 + P (R=0|t)P (R=1|t) ?
P (k|t,R=0)
P (k|t,R=1)
= P (I = 1|k)?
1
1 + P (R=0)P (R=1) ?
P (k|t,R=0)
P (k|t,R=1)
.
Here we have assumed that I is independent of t and
R given k, i.e. the interestingness of a keyphrase is
independent of the topic or whether the keyphrase is
relevant to the topic. We have also assumed that R
is independent of t when k is unknown, i.e. without
knowing the keyphrase, the relevance is independent
of the topic. Our assumptions can be depicted by
Figure 2.
We further define ? = P (R=0)P (R=1) . In general we
can assume that P (R = 0)  P (R = 1) because
there are much more non-relevant keyphrases than
relevant ones, that is, ?  1. In this case, we have
logP (R = 1, I = 1|t, k) (3)
= log
(
P (I = 1|k)?
1
1 + ? ? P (k|t,R=0)P (k|t,R=1)
)
? log
(
P (I = 1|k)?
P (k|t, R = 1)
P (k|t, R = 0)
?
1
?
)
= logP (I = 1|k) + log
P (k|t, R = 1)
P (k|t, R = 0)
? log ?.
We can see that the ranking score logP (R = 1, I =
1|t, k) can be decomposed into two components, a
relevance score log P (k|t,R=1)P (k|t,R=0) and an interestingness
score logP (I = 1|k). The last term log ? is a con-
stant and thus not relevant.
Estimating the relevance score
Let a keyphrase candidate k be a sequence of
words (w1, w2, . . . , wN ). Based on an independent
assumption of words given R and t, we have
log
P (k|t, R = 1)
P (k|t, R = 0)
= log
P (w1w2 . . . wN |t, R = 1)
P (w1w2 . . . wN |t, R = 0)
=
N?
n=1
log
P (wn|t, R = 1)
P (wn|t, R = 0)
. (4)
Given the topic model ?t previously learned for
topic t, we can set P (w|t, R = 1) to ?tw, i.e. the
probability of w under ?t. Following Griffiths and
Steyvers (2004), we estimate ?tw as
?tw =
#(Ct, w) + ?
#(Ct, ?) + ?|V|
. (5)
Here Ct denotes the collection of tweets assigned to
topic t, #(Ct, w) is the number of times w appears in
Ct, and #(Ct, ?) is the total number of words in Ct.
P (w|t, R = 0) can be estimated using a smoothed
background model.
P (w|R = 0, t) =
#(C, w) + ?
#(C, ?) + ?|V|
. (6)
382
Here #(C, ?) denotes the number of words in the
whole collection C, and #(C, w) denotes the number
of times w appears in the whole collection.
After plugging Equation (5) and Equation (6) into
Equation (4), we get the following formula for the
relevance score:
log
P (k|t, R = 1)
P (k|t, R = 0)
=
?
w?k
(
log
#(Ct, w) + ?
#(C, w) + ?
+ log
#(C, ?) + ?|V|
#(Ct, ?) + ?|V|
)
=
(?
w?k
log
#(Ct, w) + ?
#(C, w) + ?
)
+ |k|?, (7)
where ? = #(C,?)+?|V|#(Ct,?)+?|V| and |k| denotes the number
of words in k.
Estimating the interestingness score
To capture the interestingness of keyphrases, we
make use of the retweeting behavior in Twitter. We
use string matching with RT to determine whether
a tweet is an original posting or a retweet. If a
tweet is interesting, it tends to get retweeted mul-
tiple times. Retweeting is therefore a stronger indi-
cator of user interests than tweeting. We use retweet
ratio |ReTweetsk||Tweetsk| to estimate P (I = 1|k). To prevent
zero frequency, we use a modified add-one smooth-
ing method. Finally, we get
logP (I = 1|k) = log
|ReTweetsk|+ 1.0
|Tweetsk|+ lavg
. (8)
Here |ReTweetsk| and |Tweetsk| denote the num-
bers of retweets and tweets containing the keyphrase
k, respectively, and lavg is the average number of
tweets that a candidate keyphrase appears in.
Finally, we can plug Equation (7) and Equa-
tion (8) into Equation (3) and obtain the following
scoring function for ranking:
Scoret(k) = log
|ReTweetsk|+ 1.0
|Tweetsk|+ lavg
(9)
+
(
?
w?k
log
#(Ct, w) + ?
#(C, w) + ?
)
+ |k|?.
#user #tweet #term #token
13,307 1,300,300 50,506 11,868,910
Table 1: Some statistics of the data set.
Incorporating length preference
Our preliminary experiments with Equation (9)
show that this scoring function usually ranks longer
keyphrases higher than shorter ones. However, be-
cause our candidate keyphrase are extracted without
using any linguistic knowledge such as noun phrase
boundaries, longer candidate keyphrases tend to be
less meaningful as a phrase. Moreover, for our task
of using keyphrases to summarize Twitter, we hy-
pothesize that shorter keyphrases are preferred by
users as they are more compact. We would there-
fore like to incorporate some length preference.
Recall that Equation (9) is derived from P (R =
1, I = 1|t, k), but this probability does not allow
us to directly incorporate any length preference. We
further observe that Equation (9) tends to give longer
keyphrases higher scores mainly due to the term
|k|?. So here we heuristically incorporate our length
preference by removing |k|? from Equation (9), re-
sulting in the following final scoring function:
Scoret(k) = log
|ReTweetsk|+ 1.0
|Tweetsk|+ lavg
(10)
+
(
?
w?k
log
#(Ct, w) + ?
#(C, w) + ?
)
.
4 Experiments
4.1 Data Set and Preprocessing
We use a Twitter data set collected from Singapore
users for evaluation. We used Twitter REST API1
to facilitate the data collection. The majority of the
tweets collected were published in a 20-week period
from December 1, 2009 through April 18, 2010. We
removed common stopwords and words which ap-
peared in fewer than 10 tweets. We also removed all
users who had fewer than 5 tweets. Some statistics
of this data set after cleaning are shown in Table 1.
We ran Twitter-LDA with 500 iterations of Gibbs
sampling. After trying a few different numbers of
1http://apiwiki.twitter.com/w/page/22554663/REST-API-
Documentation
383
topics, we empirically set the number of topics to
30. We set ? to 50.0/|T | as Griffiths and Steyvers
(2004) suggested, but set ? to a smaller value of 0.01
and ? to 20. We chose these parameter settings be-
cause they generally gave coherent and meaningful
topics for our data set. We selected 10 topics that
cover a diverse range of content in Twitter for eval-
uation of topical keyphrase extraction. The top 10
words of these topics are shown in Table 2.
We also tried the standard LDA model and the
author-topic model on our data set and found that
our proposed topic model was better or at least com-
parable in terms of finding meaningful topics. In ad-
dition to generating meaningful topics, Twitter-LDA
is much more convenient in supporting the compu-
tation of tweet-level statistics (e.g. the number of
co-occurrences of two words in a specific topic) than
the standard LDA or the author-topic model because
Twitter-LDA assumes a single topic assignment for
an entire tweet.
4.2 Methods for Comparison
As we have described in Section 3.1, there are three
steps to generate keyphrases, namely, keyword rank-
ing, candidate keyphrase generation, and keyphrase
ranking. We have proposed a context-sensitive top-
ical PageRank method (cTPR) for the first step of
keyword ranking, and a probabilistic scoring func-
tion for the third step of keyphrase ranking. We now
describe the baseline methods we use to compare
with our proposed methods.
Keyword Ranking
We compare our cTPR method with the original
topical PageRank method (Equation (1)), which rep-
resents the state of the art. We refer to this baseline
as TPR.
For both TPR and cTPR, the damping factor is
empirically set to 0.1, which always gives the best
performance based on our preliminary experiments.
We use normalized P (t|w) to set Pt(w) because our
preliminary experiments showed that this was the
best among the three choices discussed by Liu et al
(2010). This finding is also consistent with what Liu
et al (2010) found.
In addition, we also use two other baselines for
comparison: (1) kwBL1: ranking by P (w|t) = ?tw.
(2) kwBL2: ranking by P (t|w) = P (t)?
t
w?
t? P (t
?)?t?w
.
Keyphrase Ranking
We use kpRelInt to denote our relevance and inter-
estingness based keyphrase ranking function P (R =
1, I = 1|t, k), i.e. Equation (10). ? and ? are em-
pirically set to 0.01 and 500. Usually ? can be set to
zero, but in our experiments we find that our rank-
ing method needs a more uniform estimation of the
background model. We use the following ranking
functions for comparison:
? kpBL1: Similar to what is used by Liu et al
(2010), we can rank candidate keyphrases by
?
w?k f(w), where f(w) is the score assigned
to word w by a keyword ranking method.
? kpBL2: We consider another baseline ranking
method by
?
w?k log f(w).
? kpRel: If we consider only relevance but
not interestingness, we can rank candidate
keyphrases by
?
w?k log
#(Ct,w)+?
#(C,w)+? .
4.3 Gold Standard Generation
Since there is no existing test collection for topi-
cal keyphrase extraction from Twitter, we manually
constructed our test collection. For each of the 10
selected topics, we ran all the methods to rank key-
words. For each method we selected the top 3000
keywords and searched all the combinations of these
words as phrases which have a frequency larger than
30. In order to achieve high phraseness, we first
computed the minimum value of pointwise mutual
information for all bigrams in one combination, and
we removed combinations having a value below a
threshold, which was empirically set to 2.135. Then
we merged all these candidate phrases. We did not
consider single-word phrases because we found that
it would include too many frequent words that might
not be useful for summaries.
We asked two judges to judge the quality of the
candidate keyphrases. The judges live in Singapore
and had used Twitter before. For each topic, the
judges were given the top topic words and a short
topic description. Web search was also available.
For each candidate keyphrase, we asked the judges
to score it as follows: 2 (relevant, meaningful and in-
formative), 1 (relevant but either too general or too
specific, or informal) and 0 (irrelevant or meaning-
less). Here in addition to relevance, the other two
criteria, namely, whether a phrase is meaningful and
informative, were studied by Tomokiyo and Hurst
384
T2 T4 T5 T10 T12 T13 T18 T20 T23 T25
eat twitter love singapore singapore hot iphone song study win
food tweet idol road #singapore rain google video school game
dinner blog adam mrt #business weather social youtube time team
lunch facebook watch sgreinfo #news cold media love homework match
eating internet april east health morning ipad songs tomorrow play
ice tweets hot park asia sun twitter bieber maths chelsea
chicken follow lambert room market good free music class world
cream msn awesome sqft world night app justin paper united
tea followers girl price prices raining apple feature math liverpool
hungry time american built bank air marketing twitter finish arsenal
Table 2: Top 10 Words of Sample Topics on our Singapore Twitter Dateset.
(2003). We then averaged the scores of the two
judges as the final scores. The Cohen?s Kappa co-
efficients of the 10 topics range from 0.45 to 0.80,
showing fair to good agreement2. We further dis-
carded all candidates with an average score less than
1. The number of the remaining keyphrases for each
topic ranges from 56 to 282.
4.4 Evaluation Metrics
Traditionally keyphrase extraction is evaluated using
precision and recall on all the extracted keyphrases.
We choose not to use these measures for the fol-
lowing reasons: (1) Traditional keyphrase extraction
works on single documents while we study topical
keyphrase extraction. The gold standard keyphrase
list for a single document is usually short and clean,
while for each Twitter topic there can be many
keyphrases, some are more relevant and interesting
than others. (2) Our extracted topical keyphrases are
meant for summarizing Twitter content, and they are
likely to be directly shown to the users. It is there-
fore more meaningful to focus on the quality of the
top-ranked keyphrases.
Inspired by the popular nDCG metric in informa-
tion retrieval (Ja?rvelin and Keka?la?inen, 2002), we
define the following normalized keyphrase quality
measure (nKQM) for a methodM:
nKQM@K =
1
|T |
?
t?T
?K
j=1
1
log2(j+1)
score(Mt,j)
IdealScore(K,t)
,
where T is the set of topics, Mt,j is the j-
th keyphrase generated by method M for topic
2We find that judgments on topics related to social me-
dia (e.g. T4) and daily life (e.g. T13) tend to have a higher
degree of disagreement.
t, score(?) is the average score from the two hu-
man judges, and IdealScore(K,t) is the normalization
factor?score of the top K keyphrases of topic t un-
der the ideal ranking. Intuitively, ifM returns more
good keyphrases in top ranks, its nKQM value will
be higher.
We also use mean average precision (MAP) to
measure the overall performance of keyphrase rank-
ing:
MAP =
1
|T |
?
t?T
1
NM,t
|Mt|?
j=1
NM,t,j
j
1(score(Mt,j) ? 1),
where 1(S) is an indicator function which returns
1 when S is true and 0 otherwise, NM,t,j denotes
the number of correct keyphrases among the top j
keyphrases returned byM for topic t, and NM,t de-
notes the total number of correct keyphrases of topic
t returned byM.
4.5 Experiment Results
Evaluation of keyword ranking methods
Since keyword ranking is the first step for
keyphrase extraction, we first compare our keyword
ranking method cTPR with other methods. For each
topic, we pooled the top 20 keywords ranked by all
four methods. We manually examined whether a
word is a good keyword or a noisy word based on
topic context. Then we computed the average num-
ber of noisy words in the 10 topics for each method.
As shown in Table 5, we can observe that cTPR per-
formed the best among the four methods.
Since our final goal is to extract topical
keyphrases, we further compare the performance
of cTPR and TPR when they are combined with a
keyphrase ranking algorithm. Here we use the two
385
Method nKQM@5 nKQM@10 nKQM@25 nKQM@50 MAP
kpBL1 TPR 0.5015 0.54331 0.5611 0.5715 0.5984
kwBL1 0.6026 0.5683 0.5579 0.5254 0.5984
kwBL2 0.5418 0.5652 0.6038 0.5896 0.6279
cTPR 0.6109 0.6218 0.6139 0.6062 0.6608
kpBL2 TPR 0.7294 0.7172 0.6921 0.6433 0.6379
kwBL1 0.7111 0.6614 0.6306 0.5829 0.5416
kwBL2 0.5418 0.5652 0.6038 0.5896 0.6545
cTPR 0.7491 0.7429 0.6930 0.6519 0.6688
Table 3: Comparisons of keyphrase extraction for cTPR and baselines.
Method nKQM@5 nKQM@10 nKQM@25 nKQM@50 MAP
cTPR+kpBL1 0.61095 0.62182 0.61389 0.60618 0.6608
cTPR+kpBL2 0.74913 0.74294 0.69303 0.65194 0.6688
cTPR+kpRel 0.75361 0.74926 0.69645 0.65065 0.6696
cTPR+kpRelInt 0.81061 0.75184 0.71422 0.66319 0.6694
Table 4: Comparisons of keyphrase extraction for different keyphrase ranking methods.
kwBL1 kwBL2 TPR cTPR
2 3 4.9 1.5
Table 5: Average number of noisy words among the top
20 keywords of the 10 topics.
baseline keyphrase ranking algorithms kpBL1 and
kpBL2. The comparison is shown in Table 3. We
can see that cTPR is consistently better than the three
other methods for both kpBL1 and kpBL2.
Evaluation of keyphrase ranking methods
In this section we compare keypharse ranking
methods. Previously we have shown that cTPR is
better than TPR, kwBL1 and kwBL2 for keyword
ranking. Therefore we use cTPR as the keyword
ranking method and examine the keyphrase rank-
ing method kpRelInt with kpBL1, kpBL2 and kpRel
when they are combined with cTPR. The results are
shown in Table 4. From the results we can see the
following: (1) Keyphrase ranking methods kpRelInt
and kpRel are more effective than kpBL1 and kpBL2,
especially when using the nKQM metric. (2) kpRe-
lInt is better than kpRel, especially for the nKQM
metric. Interestingly, we also see that for the nKQM
metric, kpBL1, which is the most commonly used
keyphrase ranking method, did not perform as well
as kpBL2, a modified version of kpBL1.
We also tested kpRelInt and kpRel on TPR, kwBL1
and kwBL2 and found that kpRelInt and kpRel are
consistently better than kpBL2 and kpBL1. Due to
space limit, we do not report all the results here.
These findings support our assumption that our pro-
posed keyphrase ranking method is effective.
The comparison between kpBL2 with kpBL1
shows that taking the product of keyword scores is
more effective than taking their sum. kpRel and
kpRelInt also use the product of keyword scores.
This may be because there is more noise in Twit-
ter than traditional documents. Common words (e.g.
?good?) and domain background words (e.g. ?Sin-
gapore?) tend to gain higher weights during keyword
ranking due to their high frequency, especially in
graph-based method, but we do not want such words
to contribute too much to keyphrase scores. Taking
the product of keyword scores is therefore more suit-
able here than taking their sum.
Further analysis of interestingness
As shown in Table 4, kpRelInt performs better
in terms of nKQM compared with kpRel. Here we
study why it worked better for keyphrase ranking.
The only difference between kpRel and kpRelInt is
that kpRelInt includes the factor of user interests. By
manually examining the top keyphrases, we find that
the topics ?Movie-TV? (T5), ?News? (T12), ?Music?
(T20) and ?Sports? (T25) particularly benefited from
kpRelInt compared with other topics. We find that
well-known named entities (e.g. celebrities, politi-
cal leaders, football clubs and big companies) and
significant events tend to be ranked higher by kpRe-
lInt than kpRel.
We then counted the numbers of entity and event
keyphrases for these four topics retrieved by differ-
ent methods, shown in Table 6 . We can see that
in these four topics, kpRelInt is consistently better
than kpRel in terms of the number of entity and event
keyphrases retrieved.
386
T2 T5 T10 T12 T20 T25
chicken rice adam lambert north east president obama justin bieber manchester united
ice cream jack neo rent blk magnitude earthquake music video champions league
fried chicken american idol east coast volcanic ash lady gaga football match
curry rice david archuleta east plaza prime minister taylor swift premier league
chicken porridge robert pattinson west coast iceland volcano demi lovato f1 grand prix
curry chicken alexander mcqueen bukit timah chile earthquake youtube channel tiger woods
beef noodles april fools street view goldman sachs miley cyrus grand slam(tennis)
chocolate cake harry potter orchard road coe prices telephone video liverpool fans
cheese fries april fool toa payoh haiti earthquake song lyrics final score
instant noodles andrew garcia marina bay #singapore #business joe jonas manchester derby
Table 7: Top 10 keyphrases of 6 topics from cTPR+kpRelInt.
Methods T5 T12 T20 T25
cTPR+kpRel 8 9 16 11
cTPR+kpRelInt 10 12 17 14
Table 6: Numbers of entity and event keyphrases re-
trieved by different methods within top 20.
On the other hand, we also find that for some
topics interestingness helped little or even hurt the
performance a little, e.g. for the topics ?Food? and
?Traffic.? We find that the keyphrases in these top-
ics are stable and change less over time. This may
suggest that we can modify our formula to handle
different topics different. We will explore this direc-
tion in our future work.
Parameter settings
We also examine how the parameters in our model
affect the performance.
?: We performed a search from 0.1 to 0.9 with a
step size of 0.1. We found ? = 0.1 was the optimal
parameter for cTPR and TPR. However, TPR is more
sensitive to ?. The performance went down quickly
with ? increasing.
?: We checked the overall performance with
? ? {400, 450, 500, 550, 600}. We found that ? =
500 ? 0.01|V| gave the best performance gener-
ally for cTPR. The performance difference is not
very significant between these different values of ?,
which indicates that the our method is robust.
4.6 Qualitative evaluation of cTPR+kpRelInt
We show the top 10 keyphrases discovered by
cTPR+kRelInt in Table 7. We can observe that these
keyphrases are clear, interesting and informative for
summarizing Twitter topics.
We hypothesize that the following applications
can benefit from the extracted keyphrases:
Automatic generation of realtime trendy phrases:
For exampoe, keyphrases in the topic ?Food? (T2)
can be used to help online restaurant reviews.
Event detection and topic tracking: In the topic
?News? top keyphrases can be used as candidate
trendy topics for event detection and topic tracking.
Automatic discovery of important named entities:
As discussed previously, our methods tend to rank
important named entities such as celebrities in high
ranks.
5 Conclusion
In this paper, we studied the novel problem of topical
keyphrase extraction for summarizing and analyzing
Twitter content. We proposed the context-sensitive
topical PageRank (cTPR) method for keyword rank-
ing. Experiments showed that cTPR is consistently
better than the original TPR and other baseline meth-
ods in terms of top keyword and keyphrase extrac-
tion. For keyphrase ranking, we proposed a prob-
abilistic ranking method, which models both rele-
vance and interestingness of keyphrases. In our ex-
periments, this method is shown to be very effec-
tive to boost the performance of keyphrase extrac-
tion for different kinds of keyword ranking methods.
In the future, we may consider how to incorporate
keyword scores into our keyphrase ranking method.
Note that we propose to rank keyphrases by a gen-
eral formula P (R = 1, I = 1|t, k) and we have made
some approximations based on reasonable assump-
tions. There should be other potential ways to esti-
mate P (R = 1, I = 1|t, k).
Acknowledgements
This work was done during Xin Zhao?s visit to the
Singapore Management University. Xin Zhao and
Xiaoming Li are partially supported by NSFC under
387
the grant No. 60933004, 61073082, 61050009 and
HGJ Grant No. 2011ZX01042-001-001.
References
Ken Barker and Nadia Cornacchia. 2000. Using noun
phrase heads to extract document keyphrases. In Pro-
ceedings of the 13th Biennial Conference of the Cana-
dian Society on Computational Studies of Intelligence:
Advances in Artificial Intelligence, pages 40?52.
Thomas L. Griffiths and Mark Steyvers. 2004. Find-
ing scientific topics. Proceedings of the National
Academy of Sciences of the United States of America,
101(Suppl. 1):5228?5235.
Liangjie Hong and Brian D. Davison. 2010. Empirical
study of topic modeling in Twitter. In Proceedings of
the First Workshop on Social Media Analytics.
Kalervo Ja?rvelin and Jaana Keka?la?inen. 2002. Cumu-
lated gain-based evaluation of ir techniques. ACM
Transactions on Information Systems, 20(4):422?446.
John Lafferty and Chengxiang Zhai. 2003. Probabilistic
relevance models based on document and query gener-
ation. Language Modeling and Information Retrieval,
13.
Quanzhi Li, Yi-Fang Wu, Razvan Bot, and Xin Chen.
2004. Incorporating document keyphrases in search
results. In Proceedings of the 10th Americas Confer-
ence on Information Systems.
Marina Litvak and Mark Last. 2008. Graph-based key-
word extraction for single-document summarization.
In Proceedings of the Workshop on Multi-source Mul-
tilingual Information Extraction and Summarization,
pages 17?24.
Zhiyuan Liu, Wenyi Huang, Yabin Zheng, and Maosong
Sun. 2010. Automatic keyphrase extraction via topic
decomposition. In Proceedings of the 2010 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, pages 366?376.
Qiaozhu Mei, Xuehua Shen, and ChengXiang Zhai.
2007. Automatic labeling of multinomial topic mod-
els. In Proceedings of the 13th ACM SIGKDD Interna-
tional Conference on Knowledge Discovery and Data
Mining, pages 490?499.
R. Mihalcea and P. Tarau. 2004. TextRank: Bringing or-
der into texts. In Proceedings of the 2004 Conference
on Empirical Methods in Natural Language Process-
ing.
Daniel Ramage, Susan Dumais, and Dan Liebling. 2010.
Characterizing micorblogs with topic models. In Pro-
ceedings of the 4th International Conference on We-
blogs and Social Media.
Takeshi Sakaki, Makoto Okazaki, and Yutaka Matsuo.
2010. Earthquake shakes Twitter users: real-time
event detection by social sensors. In Proceedings of
the 19th International World Wide Web Conference.
Takashi Tomokiyo and Matthew Hurst. 2003. A lan-
guage model approach to keyphrase extraction. In
Proceedings of the ACL 2003 Workshop on Multi-
word Expressions: Analysis, Acquisition and Treat-
ment, pages 33?40.
Andranik Tumasjan, Timm O. Sprenger, Philipp G. Sand-
ner, and Isabell M. Welpe. 2010. Predicting elections
with Twitter: What 140 characters reveal about politi-
cal sentiment. In Proceedings of the 4th International
Conference on Weblogs and Social Media.
Peter Turney. 2000. Learning algorithms for keyphrase
extraction. Information Retrieval, (4):303?336.
Jianshu Weng, Ee-Peng Lim, Jing Jiang, and Qi He.
2010. TwitterRank: finding topic-sensitive influential
twitterers. In Proceedings of the third ACM Interna-
tional Conference on Web Search and Data Mining.
Wei Wu, Bin Zhang, and Mari Ostendorf. 2010. Au-
tomatic generation of personalized annotation tags for
twitter users. In Human Language Technologies: The
2010 Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
pages 689?692.
Xin Zhao, Jing Jiang, Jianshu Weng, Jing He, Lim Ee-
Peng, Hongfei Yan, and Xiaoming Li. 2011. Compar-
ing Twitter and traditional media using topic models.
In Proceedings of the 33rd European Conference on
Information Retrieval.
388
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 516?525,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Tweet Recommendation with Graph Co-Ranking
Rui Yan?
?Department of Computer
Science and Technology,
Peking University,
Beijing 100871, China
r.yan@pku.edu.cn
Mirella Lapata?
?Institute for Language,
Cognition and Computation,
University of Edinburgh,
Edinburgh EH8 9AB, UK
mlap@inf.ed.ac.uk
Xiaoming Li?, \
\State Key Laboratory of Software
Development Environment,
Beihang University,
Beijing 100083, China
lxm@pku.edu.cn
Abstract
As one of the most popular micro-blogging
services, Twitter attracts millions of users,
producing millions of tweets daily. Shared in-
formation through this service spreads faster
than would have been possible with tradi-
tional sources, however the proliferation of
user-generation content poses challenges to
browsing and finding valuable information. In
this paper we propose a graph-theoretic model
for tweet recommendation that presents users
with items they may have an interest in. Our
model ranks tweets and their authors simulta-
neously using several networks: the social net-
work connecting the users, the network con-
necting the tweets, and a third network that
ties the two together. Tweet and author entities
are ranked following a co-ranking algorithm
based on the intuition that that there is a mu-
tually reinforcing relationship between tweets
and their authors that could be reflected in the
rankings. We show that this framework can be
parametrized to take into account user prefer-
ences, the popularity of tweets and their au-
thors, and diversity. Experimental evaluation
on a large dataset shows that our model out-
performs competitive approaches by a large
margin.
1 Introduction
Online micro-blogging services have revolutionized
the way people discover, share, and distribute infor-
mation. Twitter is perhaps the most popular such
service with over 140 million active users as of
2012.1 Twitter enables users to send and read text-
based posts of up to 140 characters, known as tweets.
Twitter users follow others or are followed. Being a
follower on Twitter means that the user receives all
the tweets from those she follows. Common prac-
tice of responding to a tweet has evolved into a well-
defined markup culture (e.g., RT stands for retweet,
?@? followed by an identifier indicates the user).
The strict limit of 140 characters allows for quick
and immediate communication in real time, whilst
enforcing brevity. Moreover, the retweet mecha-
nism empowers users to spread information of their
choice beyond the reach of their original followers.
Twitter has become a prominent broadcast-
ing medium, taking priority over traditional news
sources (Teevan et al, 2011). Shared information
through this channel spreads faster than would have
been possible with conventional news sites or RSS
feeds and can reach a far wider population base.
However, the proliferation of user-generated con-
tent comes at a price. Over 340 millions of tweets
are being generated daily amounting to thousands
of tweets per second!2 Twitter?s own search en-
gine handles more than 1.6 billion search queries per
day.3 This enormous amount of data renders it in-
feasible to browse the entire Twitter network; even
if this was possible, it would be extremely difficult
for users to find information they are interested in.
A hypothetical tweet recommendation system could
1For details see http://blog.twitter.com/2012/03/
twitter-turns-six.html
2In fact, the peak record is 6,939 tweets per second, reported
by http://blog.twitter.com/2011/03/numbers.html.
3See http://engineering.twitter.com/2011/05/
engineering-behind-twitters-new-search.html
516
alleviate this acute information overload, e.g., by
limiting the stream of tweets to those of interest to
the user, or by discovering intriguing content outside
the user?s following network.
The tweet recommendation task is challenging for
several reasons. Firstly, Twitter does not merely
consist of a set of tweets. Rather, it contains many
latent networks including the following relationships
among users and the retweeting linkage (which in-
dicates information diffusion). Secondly, the rec-
ommendations ought to be of interest to the user
and likely to to attract user response (e.g., to be
retweeted). Thirdly, recommendations should be
personalized (Cho and Schonfeld, 2007; Yan et al,
2011), avoid redundancy, and demonstrate diversity.
In this paper we present a graph-theoretic approach
to tweet recommendation that attempts to address
these challenges.
Our recommender operates over a heterogeneous
network that connects the users (or authors) and the
tweets they produce. The user network represents
links among authors based on their following be-
havior, whereas the tweet network connects tweets
based on content similarity. A third bipartite graph
ties the two together. Tweet and author entities in
this network are ranked simultaneously following a
co-ranking algorithm (Zhou et al, 2007). The main
intuition behind co-ranking is that there is a mu-
tually reinforcing relationship between authors and
tweets that could be reflected in the rankings. Tweets
are important if they are related to other important
tweets and authored by important users who in turn
are related to other important users. The model ex-
ploits this mutually reinforcing relationship between
tweets and their authors and couples two random
walks, one on the tweet graph and one on the author
graph, into a combined one. Rather than creating a
global ranking over all tweets in a collection, we ex-
tend this framework to individual users and produce
personalized recommendations. Moreover, we in-
corporate diversity by allowing the random walk on
the tweet graph to be time-variant (Mei et al, 2010).
Experimental results on a real-world dataset con-
sisting of 364,287,744 tweets from 9,449,542 users
show that the co-ranking approach substantially im-
proves performance over the state of the art. We ob-
tain a relative improvement of 18.3% (in nDCG) and
7.8% (in MAP) over the best comparison system.
2 Related Work
Tweet Search Given the large amount of tweets
being posted daily, ranking strategies have be-
come extremely important for retrieving information
quickly. Many websites currently offer a real-time
search service which returns ranked lists of Twit-
ter posts or shared links according to user queries.
Ranking methods used by these sites employ three
criteria, namely recency, popularity and content rel-
evance (Dong et al, 2010). State-of-art tweet re-
trieval methods include a linear regression model bi-
ased towards text quality with a regularization factor
inspired by the hypothesis that documents similar
in content may have similar quality (Huang et al,
2011). Duan et al (2010) learn a ranking model us-
ing SVMs and features based on tweet content, the
relations among users, and tweet specific character-
istics (e.g., urls, number of retweets).
Tweet Recommendation Previous work has also
focused on tweet recommendation systems, assum-
ing no explicit query is provided by the users.
Collaborative filtering is perhaps the most obvious
method for recommending tweets (Hannon et al,
2010). Chen et al (2010) investigate how to se-
lect interesting URLs linked from Twitter and rec-
ommend the top ranked ones to users. Their rec-
ommender takes three dimensions into account: the
source, the content topic, and social voting. Sim-
ilarly, Abel et al (2011a; 2011b; 2011c) recom-
mend external websites linked to Twitter. Their
method incorporates user profile modeling and tem-
poral recency, but they do not utilize the social
networks among users. R. et al (2009) propose
a diffusion-based recommendation framework es-
pecially for tweets representing critical events by
constructing a diffusion graph. Hong et al (2011)
recommend tweets based on popularity related fea-
tures. Ramage et al (2010) investigate which topics
users are interested in following a Labeled-LDA ap-
proach, by deciding whether a user is in the followee
list of a given user or not. Uysal and Croft (2011) es-
timate the likelihood of a tweet being reposted from
a user-centric perspective.
Our work also develops a tweet recommendation
system. Our model exploits the information pro-
vided by the tweets and the underlying social net-
works in a unified co-ranking framework. Although
517
these sources have been previously used to search
or recommend tweets, our model considers them
simultaneously and produces a ranking that is in-
formed by both. Furthermore, we argue that the
graph-theoretic framework upon which co-ranking
operates is beneficial as it allows to incorporate per-
sonalization (we provide user-specific rankings) and
diversity (the ranking is optimized so as to avoid re-
dundancy). The co-ranking framework has been ini-
tially developed for measuring scientific impact and
modeling the relationship between authors and their
publications (Zhou et al, 2007). However, the adap-
tation of this framework to the tweet recommenda-
tion task is novel to our knowledge.
3 Tweet Recommendation Framework
Our method operates over a heterogeneous network
that connects three graphs representing the tweets,
their authors and the relationships between them.
Let G denote the heterogeneous graph with nodes V
and edges E, and G = (V,E) = (VM ?VU ,EM ?EU ?
EMU). G is divided into three subgraphs, GM, GU
and GMU . GM = (VM,EM) is a weighted undirected
graph representing the tweets and their relationships.
Let VM = {mi|mi ?VM} denote a collection of |VM|
tweets and EM the set of links representing relation-
ships between them. The latter are established by
measuring how semantically similar any two tweets
are (see Section 3.4 for details). GU = (VU ,EU) is
an unweighted directed graph representing the so-
cial ties among Twitter users. VU = {ui|ui ? VU} is
the set of users with size |VU |. Links EU among
users are established by observing their following
behavior. GMU = (VMU ,EMU) is an unweighted bi-
partite graph that ties GM and GU together and repre-
sents tweet-author relationships. The graph consists
of nodes VMU = VM ?VU and edges EMU connect-
ing each tweet with all of its authors. Typically, a
tweet m is written by only one author u. However,
because of retweeting we treat all users involved in
reposting a tweet as ?co-authors?. The three subnet-
works are illustrated in Figure 1.
The framework includes three random walks, one
on GM, one on GU and one on GMU . A random walk
on a graph is a Markov chain, its states being the
vertices of the graph. It can be described by a square
n? n matrix M, where n is the number of vertices
in the graph. M is a stochastic matrix prescribing
Figure 1: Tweet recommendation based on a co-ranking
framework including three sub-networks. The undirected
links between tweets indicate semantic correlation. The
directed links between users denotes following. A bipar-
tite graph (whose edges are shown with dashed lines) ties
the tweet and author networks together.
the transition probabilities from one vertex to the
next. The framework couples the two random walks
on GM, and GU that rank tweets and theirs authors in
isolation. and allows to obtain a more global rank-
ing by taking into account their mutual dependence.
In the following sections we first describe how we
obtain the rankings on GM and GU , and then move
on to discuss how the two are coupled.
3.1 Ranking the Tweet Graph
Popularity We rank the tweet network follow-
ing the PageRank paradigm (Brin and Page, 1998).
Consider a random walk on GM and let M be the
transition matrix (defined in Section 3.4). Fix some
damping factor ? and say that at each time step with
probability (1-?) we stick to random walking and
with probability ? we do not make a usual random
walk step, but instead jump to any vertex, chosen
uniformly at random:
m = (1??)MTm+
?
|VM|
11T (1)
Here, vector m contains the ranking scores for the
vertices in GM. The fact that there exists a unique so-
518
lution to (1) follows from the random walk M being
ergodic (? >0 guarantees irreducibility, because we
can jump to any vertex). MT is the transpose of M.
1 is the vector of |VM| entries, each being equal to
one. Let m? RVM , ||m||1 = 1 be the only solution.
Personalization The standard PageRank algo-
rithm performs a random walk, starting from any
node, then randomly selects a link from that node to
follow considering the weighted matrix M, or jumps
to a random node with equal probability. It pro-
duces a global ranking over all tweets in the col-
lection without taking specific users into account.
As there are billions of tweets available on Twit-
ter covering many diverse topics, it is reasonable
to assume that an average user will only be inter-
ested in a small subset (Qiu and Cho, 2006). We
operationalize a user?s topic preference as a vec-
tor t = [t1, t2, . . . , tn]1?n, where n denotes the num-
ber of topics, and ti represents the degree of prefer-
ence for topic i. The vector t is normalized such
that ?ni=1 ti = 1. Intuitively, such vectors will be
different for different users. Note that user prefer-
ences can be also defined at the tweet (rather than
topic) level. Although tweets can illustrate user in-
terests more directly, in most cases a user will only
respond to a small fraction of tweets. This means
that most tweets will not provide any information
relating to a user?s interests. The topic preference
vector allows to propagate such information (based
on whether a tweet has been reposted or not) to other
tweets within the same topic cluster.
Given n topics, we obtain a topic distribution ma-
trix D using Latent Dirichlet Allocation (Blei et al,
2003). Let Di j denote the probability of tweet mi to
belong to topic t j. Consider a user with a topic pref-
erence vector t and topic distribution matrix D. We
calculate the response probability r for all tweets for
this user as:
r = tDT (2)
where r=[r1, r2, . . . , rVM ]1?|VM | represents the re-
sponse probability vector and ri the probability for a
user to respond to tweet mi. We normalize r so that
?ri?r ri = 1. Now, given the observed response prob-
ability vector r = [r1,r2, . . . ,rw]1?w, where w<|VM|
for a given user and the topic distribution ma-
trix D, our task is estimate the topic preference
vector t. We do this using maximum-likelihood
estimation. Assuming a user has responded to w
tweets, we approximate t so as to maximize the ob-
served response probability. Let r(t) = tDT. As-
suming all responses are independent, the probabil-
ity for w tweets r1, r2, . . . , rw is then ?wi=1 ri(t) under
a given t. The value of t is chosen when the proba-
bility is maximized:
t = argmax
t
( w
?
i=1
ri(t)
)
(3)
In a simple random walk, it is assumed that all
nodes in the matrix M are equi-probable before the
walk. In contrast, we use the topic preference vector
as a prior on M. Let Diag(r) denote a diagonal ma-
trix whose eigenvalue is vector r. Then m becomes:
m = (1??)[Diag(r)M]Tm+?r
= (1??)[Diag(tDT)M]Tm+?tDT
(4)
Diversity We would also like our output to be
diverse without redundant information. Unfortu-
nately, equation (4) will have the opposite effect,
as it assigns high scores to closely connected node
communities. A greedy algorithm such as Maxi-
mum Marginal Relevance (Carbonell and Goldstein,
1998; Wan et al, 2007; Wan et al, 2010) may
achieve diversity by iteratively selecting the most
prestigious or popular vertex and then penalizing the
vertices ?covered? by those that have been already
selected. Rather than adopting a greedy vertex selec-
tion method, we follow DivRank (Mei et al, 2010)
a recently proposed algorithm that balances popular-
ity and diversity in ranking, based on a time-variant
random walk. In contrast to PageRank, DivRank as-
sumes that the transition probabilities change over
time. Moreover, it is assumed that the transition
probability from one state to another is reinforced by
the number of previous visits to that state. At each
step, the algorithm creates a dynamic transition ma-
trix M(.). After z iterations, the matrix becomes:
M(z) = (1??)M(z?1) ?m(z?1)+?tDT (5)
and hence, m can be calculated as:
m(z) = (1??)[Diag(tDT)M(z)]Tm+?tDT (6)
Equation (5) increases the probability for nodes
with higher popularity. Nodes with high weights are
519
likely to ?absorb? the weights of their neighbors di-
rectly, and the weights of their neighbors? neighbors
indirectly. The process iteratively adjusts the ma-
trix M according to m and then updates m according
to the changed M. Essentially, the algorithm favors
nodes with high popularity and as time goes by there
emerges a rich-gets-richer effect (Mei et al, 2010).
3.2 Ranking the Author Graph
As mentioned earlier, we build a graph of au-
thors (and obtain the affinity U) using the follow-
ing linkage. We rank the author network using
PageRank analogously to equation (1). Besides
popularity, we also take personalization into ac-
count. Intuitively, users are likely to be interested
in their friends even if these are relatively unpopu-
lar. Therefore, for each author, we include a vec-
tor p = [p1, p2, . . . , p|VU |]1?|VU | denoting their prefer-
ence for other authors. The preference factor for au-
thor u toward other authors ui is defined as:
pui =
#tweets from ui
#tweets of u
(7)
which represents the proportion of tweets inherited
from user ui. A large pui means that u is more likely
to respond to ui?s tweets.
In theory, we could also apply DivRank on the au-
thor graph. However, as the authors are unique, we
assume that they are sufficiently distinct and there is
no need to promote diversity.
3.3 The Co-Ranking Algorithm
So far we have described how we rank the network
of tweets GM and their authors GU independently
following the PageRank paradigm. The co-ranking
framework includes a random walk on GM, GU ,
and GMU . The latter is a bipartite graph representing
which tweets are authored by which users. The ran-
dom walks on GM and GU are intra-class random
walks, because take place either within the tweets?
or the users? networks. The third (combined) ran-
dom walk on GMU is an inter-class random walk. It
is sufficient to describe it by a matrix MU|VM|?|VU|
and a matrix UM|VU|?|VM|, since GMU is bipartite.
One intra-class step changes the probability distribu-
tion from (m, 0) to (Mm, 0) or from (0, u) to (0, Uu),
while one inter-class step changes the probability
distribution from (m, u) to (UMT u, MUT m). The
design of M, U, MU and UM is detailed in Sec-
tion 3.4.
The two intra-class random walks are coupled
using the inter-class random walk on the bipartite
graph. The coupling is regulated by ?, a parameter
quantifying the importance of GMU versus GM and
GU . In the extreme case, if ? is set to 0, there is no
coupling. This amounts to separately ranking tweets
and authors by PageRank. In general, ? represents
the extent to which the ranking of tweets and their
authors depend on each other.
There are two intuitions behind the co-ranking al-
gorithm: (1) a tweet is important if it associates to
other important tweets, and is authored by impor-
tant users and (2) a user is important if they asso-
ciate to other important users, and they write impor-
tant tweets. We formulate these intuitions using the
following iterative procedure:
Step 1 Compute tweet saliency scores:
m(z+1) = (1??)([Diag(r)M(z)]T)m(z)+?UMTu(z)
m(z+1) = m(z+1)/||m(z+1)|| (8)
Step 2 Compute author saliency scores:
u(z+1) = (1??)([Diag(p)U]T)u(z)+?MUTm(z)
u(z+1) = u(z+1)/||u(z+1)|| (9)
Here, m(z) and u(z) are the ranking vectors for tweets
and authors for the z-th iteration. To guarantee con-
vergence, m and u are normalized after each itera-
tion. Note that the tweet transition matrix M is dy-
namic due to the computation of diversity while the
author transition matrix U is static. The algorithm
typically converges when the difference between the
scores computed at two successive iterations for any
tweet/author falls below a threshold ? (set to 0.001
in this study).
3.4 Affinity Matrices
The co-ranking framework is controlled by four
affinity matrices: M, U, MU and UM. In this section
we explain how these matrices are defined in more
detail.
The tweet graph is an undirected weighted graph,
where an edge between two tweets mi and m j repre-
sents their cosine similarity. An adjacency matrix M
520
describes the tweet graph where each entry corre-
sponds to the weight of a link in the graph:
Mij =
F (mi,m j)
?kF (mi,mk)
, F (mi,m j) =
~mi ?~m j
||~mi||||~m j||
(10)
where F (.) is the cosine similarity and ~m is a term
vector corresponding to tweet m. We treat a tweet
as a short document and weight each term with tf.idf
(Salton and Buckley, 1988), where tf is the term fre-
quency and idf is the inverse document frequency.
The author graph is a directed graph based on the
following linkage. When ui follows u j, we add a link
from ui to u j. Let the indicator function I (ui,u j) de-
note whether ui follows u j. The adjacency matrix U
is then defined as:
Uij =
I (ui,u j)
?k I (ui,uk)
, I (ui,u j)=
{
1if ei j ? EU
0if ei j /? EU
(11)
In the bipartite tweet-author graph GMU , the
entry EMU(i, j) is an indicator function denoting
whether tweet mi is authored by user u j:
A(mi,u j) =
{
1 if ei j ? EMU
0 if ei j /? EMU
(12)
Through EMU we define MU and UM, using the
weight matrices MU= [W?ij] and UM=[W?ji], con-
taining the conditional probabilities of transitioning
from mi to u j and vice versa:
W?ij =
A(mi,u j)
?kA(mi,uk)
, W?ji =
A(mi,u j)
?kA(mk,u j)
(13)
4 Experimental Setup
Data We crawled Twitter data from 23 seed users
(who were later invited to manually evaluate the
output of our system). In addition, we collected
the data of their followees and followers by travers-
ing the following edges, and exploring all newly
included users in the same way until no new
users were added. This procedure resulted in
a relatively large dataset consisting of 9,449,542
users, 364,287,744 tweets, 596,777,491 links, and
55,526,494 retweets. The crawler monitored the
data from 3/25/2011 to 5/30/2011. We used approx-
imately one month of this data for training and the
rest for testing.
Before building the graphs (i.e., the tweet graph,
the author graph, and the tweet-author graph), the
dataset was preprocessed as follows. We removed
tweets of low linguistic quality and subsequently
discarded users without any linkage to the remain-
ing tweets. We measured linguistic quality follow-
ing the evaluation framework put forward in Pitler
et al (2010). For instance, we measured the out-of-
vocabulary word ratio (as a way of gauging spelling
errors), entity coherence, fluency, and so on. We fur-
ther removed stopwords and performed stemming.
Parameter Settings We ran LDA with 500 itera-
tions of Gibbs sampling. The number of topics n
was set to 100 which upon inspection seemed gen-
erally coherent and meaningful. We set the damp-
ing factor ? to 0.15 following the standard PageRank
paradigm. We opted for more or less generic param-
eter values as we did not want to tune our frame-
work to the specific dataset at hand. We examined
the parameter ? which controls the balance of the
tweet-author graph in more detail. We experimented
with values ranging from 0 to 0.9, with a step size
of 0.1. Small ? values place little emphasis on the
tweet graph, whereas larger values rely more heav-
ily on the author graph. Mid-range values take both
graphs into account. Overall, we observed better
performance with values larger than 0.4. This sug-
gests that both sources of information ? the content
of the tweets and their authors ? are important for
the recommendation task. All our experiments used
the same ? value which was set to 0.6.
System Comparison We compared our approach
against three naive baselines and three state-of-the-
art systems recently proposed in the literature. All
comparison systems were subject to the same fil-
tering and preprocessing procedures as our own al-
gorithm. Our first baseline ranks tweets randomly
(Random). Our second baseline ranks tweets ac-
cording to token length: longer tweets are ranked
higher (Length). The third baseline ranks tweets
by the number of times they are reposted assum-
ing that more reposting is better (RTnum). We also
compared our method against Duan et al (2010).
Their model (RSVM) ranks tweets based on tweet
content features and tweet authority features using
the RankSVM algorithm (Joachims, 1999). Our
fifth comparison system (DTC) was Uysal and Croft
521
(2011) who use a decision tree classifier to judge
how likely it is for a tweet to be reposted by a spe-
cific user. This scenario is similar to ours when rank-
ing tweets by retweet likelihood. Finally, we com-
pared against Huang et al (2011) who use weighted
linear combination (WLC) to grade the relevance of
a tweet given a query. We implemented their model
without any query-related features as in our setting
we do not discriminate tweets depending on their
relevance to specific queries.
Evaluation We evaluated system output in two
ways, i.e., automatically and in a user study. Specif-
ically, we assume that if a tweet is retweeted it is rel-
evant and is thus ranked higher over tweets that have
not been reposted. We used our algorithm to predict
a ranking for the tweets in the test data which we
then compared against a goldstandard ranking based
on whether a tweet has been retweeted or not. We
measured ranking performance using the normalized
Discounted Cumulative Gain (nDCG; Ja?rvelin and
Keka?la?inen (2002)):
nDCG(k,VU) =
1
|VU|
?
u?VU
1
Zu
k
?
i=1
2r
u
i ?1
log(1+ i)
(14)
where VU denotes users, k indicates the top-k posi-
tions in a ranked list, and Zu is a normalization factor
obtained from a perfect ranking for a particular user.
rui is the relevance score (i.e., 1: retweeted, 0: not
retweeted) for the i-th tweet in the ranking list for
user u.
We also evaluated system output in terms of Mean
Average Precision (MAP), under the assumption
that retweeted tweets are relevant and the rest irrele-
vant:
MAP =
1
|VU|
?
u?VU
1
Nu
k
?
i=1
Pui ? r
u
i (15)
where Nu is the number of reposted tweets for user u,
and Pui is the precision at i-th position for user u
(Manning et al, 2008).
The automatic evaluation sketched above does not
assess the full potential of our recommendation sys-
tem. For instance, it is possible for the algorithm to
recommend tweets to users with no linkage to their
publishers. Such tweets may be of potential interest,
however our goldstandard data can only provide in-
formation for tweets and users with following links.
System nDCG@5 nDCG@10 nDCG@25 nDCG@50 MAP
Random 0.068 0.111 0.153 0.180 0.167
Length 0.275 0.288 0.298 0.335 0.258
RTNum 0.233 0.219 0.225 0.249 0.239
RSVM 0.392 0.400 0.421 0.444 0.558
DTC 0.441 0.468 0.492 0.473 0.603
WLC 0.404 0.421 0.437 0.464 0.592
CoRank 0.519 0.546 0.550 0.585 0.617
Table 1: Evaluation of tweet ranking output produced by
our system and comparison baselines against goldstan-
dard data.
System nDCG@5 nDCG@10 nDCG@25 nDCG@50 MAP
Random 0.081 0.103 0.116 0.107 0.175
Length 0.291 0.307 0.246 0.291 0.264
RTNum 0.258 0.318 0.343 0.346 0.257
RSVM 0.346 0.443 0.384 0.414 0.447
DTC 0.545 0.565 0.579 0.526 0.554
WLC 0.399 0.447 0.460 0.481 0.506
CoRank 0.567 0.644 0.715 0.643 0.628
Table 2: Evaluation of tweet ranking output produced by
our system and comparison baselines against judgments
elicited by users.
We therefore asked the 23 users whose Twitter data
formed the basis of our corpus to judge the tweets
ranked by our algorithm and comparison systems.
The users were asked to read the systems? recom-
mendations and decide for every tweet presented to
them whether they would retweet it or not, under the
assumption that retweeting takes place when users
find the tweet interesting.
In both automatic and human-based evaluations
we ranked all tweets in the test data. Then for each
date and user we selected the top 50 ones. Our
nDCG and MAP results are averages over users and
dates.
5 Results
Our results are summarized in Tables 1 and 2. Ta-
ble 1 reports results when model performance is
evaluated against the gold standard ranking obtained
from the Twitter network. In Table 2 model per-
formance is compared against rankings elicited by
users.
As can be seen, the Random method performs
worst. This is hardly surprising as it recommends
tweets without any notion of their importance or user
interest. Length performs considerably better than
522
System nDCG@5 nDCG@10 nDCG@25 nDCG@50 MAP
PageRank 0.493 0.481 0.509 0.536 0.604
PersRank 0.501 0.542 0.558 0.560 0.611
DivRank 0.487 0.505 0.518 0.523 0.585
CoRank 0.519 0.546 0.550 0.585 0.617
Table 3: Evaluation of individual system components
against goldstandard data.
System nDCG@5 nDCG@10 nDCG@25 nDCG@50 MAP
PageRank 0.557 0.549 0.623 0.559 0.588
PersRank 0.571 0.595 0.655 0.613 0.601
DivRank 0.538 0.591 0.594 0.547 0.589
CoRank 0.637 0.644 0.715 0.643 0.628
Table 4: Evaluation of individual system components
against human judgments.
Random. This might be due to the fact that infor-
mativeness is related to tweet length. Using merely
the number of retweets does not seem to capture the
tweet importance as well as Length. This suggests
that highly retweeted posts are not necessarily in-
formative. For example, in our data, the most fre-
quently reposted tweet is a commercial advertise-
ment calling for reposting!
The supervised systems (RSVM, DTC, and
WLC) greatly improve performance over the naive
baselines. These methods employ standard machine
learning algorithms (such as SVMs, decision trees
and linear regression) on a large feature space. Aside
from the learning algorithm, their main difference
lies in the selection of the feature space, e.g., the way
content is represented and whether authority is taken
into account. DTC performs best on most evalua-
tion criteria. However, neither DTC nor RSVM, or
WLC take personalization into account. They gen-
erate the same recommendation lists for all users.
Our co-ranking algorithm models user interest with
respect to the content of the tweets and their pub-
lishers. Moreover, it attempts to create diverse out-
put and has an explicit mechanism for minimizing
redundancy. In all instances, using both DCG and
MAP, it outperforms the comparison systems. Inter-
estingly, the performance of CoRank is better when
measured against human judgments. This indicates
that users are interested in tweets that fall outside
the scope of their followers and that recommenda-
tion can improve user experience.
We further examined the contribution of the in-
dividual components of our system to the tweet
recommendation task. Tables 3 and 4 show how
the performance of our co-ranking algorithm varies
when considering only tweet popularity using the
standard PageRank algorithm, personalization (Per-
sRank), and diversity (DivRank). Note that DivRank
is only applied to the tweet graph. The PageR-
ank algorithm on its own makes good recommenda-
tions, while incorporating personalization improves
the performance substantially, which indicates that
individual users show preferences to specific topics
or other users. Diversity on its own does not seem
to make a difference, however it improves perfor-
mance when combined with personalization. Intu-
itively, users are more likely to repost tweets from
their followees, or tweets closely related to those
retweeted previously.
6 Conclusions
We presented a co-ranking framework for a tweet
recommendation system that takes popularity, per-
sonalization and diversity into account. Central to
our approach is the representation of tweets and
their users in a heterogeneous network and the abil-
ity to produce a global ranking that takes both in-
formation sources into account. Our model obtains
substantial performance gains over competitive ap-
proaches on a large real-world dataset (it improves
by 18.3% in DCG and 7.8% in MAP over the best
baseline). Our experiments suggest that improve-
ments are due to the synergy of the two information
sources (i.e., tweets and their authors). The adopted
graph-theoretic framework is advantageous in that
it allows to produce user-specific recommendations
and incorporate diversity in a unified model. Evalua-
tion with actual Twitter users shows that our recom-
mender can indeed identify interesting information
that lies outside the the user?s immediate following
network. In the future, we plan to extend the co-
ranking framework so as to incorporate information
credibility and temporal recency.
Acknowledgments This work was partially
funded by the Natural Science Foundation of China
under grant 60933004, and the Open Fund of the
State Key Laboratory of Software Development
Environment under grant SKLSDE-2010KF-03.
Rui Yan was supported by a MediaTek Fellowship.
523
References
Fabian Abel, Qi Gao, Geert-Jan Houben, and Ke Tao.
2011a. Analyzing temporal dynamics in Twitter pro-
files for personalized recommendations in the social
web. In Proceedings of the ACM Web Science Confer-
ence 2011, pages 1?8, Koblenz, Germany.
Fabian Abel, Qi Gao, Geert-Jan Houben, and Ke Tao.
2011b. Analyzing user modeling on Twitter for per-
sonalized news recommendations. User Modeling,
Adaptation and Personalization, pages 1?12.
Fabian Abel, Qi Gao, Geert-Jan Houben, and Ke Tao.
2011c. Semantic enrichment of twitter posts for user
profile construction on the social web. The Semanic
Web: Research and Applications, pages 375?389.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet alddress. Journal of Machine
Learning Research, 3:993?1022.
Sergey Brin and Lawrence Page. 1998. The anatomy
of a large-scale hypertextual web search engine. Pro-
ceedings of the 7th International Conference on World
Wide Web, 30(1-7):107?117.
Jaime Carbonell and Jade Goldstein. 1998. The use of
MMR, diversity-based reranking for reordering doc-
uments and producing summaries. In Proceedings
of the 21st Annual International ACM SIGIR Confer-
ence on Research and Development in Information Re-
trieval, pages 335?336, Melbourne, Australia.
Jilin Chen, Rowan Nairn, Les Nelson, Michael Bernstein,
and Ed Chi. 2010. Short and tweet: experiments on
recommending content from information streams. In
Proceedings of the 28th International Conference on
Human Factors in Computing Systems, pages 1185?
1194, Atlanta, Georgia.
Junghoo Cho and Uri Schonfeld. 2007. Rankmass
crawler: a crawler with high personalized pagerank
coverage guarantee. In Proceedings of the 33rd Inter-
national Conference on Very Large Data Bases, pages
375?386, Vienna, Austria.
Anlei Dong, Ruiqiang Zhang, Pranam Kolari, Jing
Bai, Fernando Diaz, Yi Chang, Zhaohui Zheng, and
Hongyuan Zha. 2010. Time is of the essence: improv-
ing recency ranking using Twitter data. In Proceed-
ings of the 19th International Conference on World
Wide Web, pages 331?340, Raleigh, North Carolina.
Yajuan Duan, Long Jiang, Tao Qin, Ming Zhou, and
Heung-Yeung Shum. 2010. An empirical study on
learning to rank of tweets. In Proceedings of the 23rd
International Conference on Computational Linguis-
tics, pages 295?303, Beijing, China.
John Hannon, Mike Bennett, and Barry Smyth. 2010.
Recommending twitter users to follow using content
and collaborative filtering approaches. In Proceedings
of the 4th ACM Conference on Recommender Systems,
pages 199?206, Barcelona, Spain.
Liangjie Hong, Ovidiu Dan, and Brian D. Davison. 2011.
Predicting popular messages in Twitter. In Proceed-
ings of the 20th International Conference Companion
on World Wide Web, pages 57?58, Hyderabad, India.
Minlie Huang, Yi Yang, and Xiaoyan Zhu. 2011.
Quality-biased ranking of short texts in microblogging
services. In Proceedings of the 5th International Joint
Conference on Natural Language Processing, pages
373?382, Chiang Mai, Thailand.
Kalervo Ja?rvelin and Jaana Keka?la?inen. 2002. Cumu-
lated gain-based evaluation of IR techniques. ACM
Transactions on Information Systems, 20:422?446.
Thorsten Joachims. 1999. Making large-scale svm learn-
ing practical. In Advances in Kernel Methods: Support
Vector Learning, pages 169?184. MIT press.
Christopher D. Manning, Prabhakar Raghavan, and Hin-
rich Schutze. 2008. Introduction to Information Re-
trieval, volume 1. Cambridge University Press.
Qiaozhu Mei, Jian Guo, and Dragomir Radev. 2010.
Divrank: the interplay of prestige and diversity in
information networks. In Proceedings of the 16th
ACM SIGKDD International Conference on Knowl-
edge Discovery and Data Mining, pages 1009?1018,
Washington, DC.
Emily Pitler, Annie Louis, and Ani Nenkova. 2010.
Automatic evaluation of linguistic quality in multi-
document summarization. In Proceedings of the 48th
Annual Meeting of the Association for Computational
Linguistics, pages 544?554, Uppsala, Sweden.
Feng Qiu and Junghoo Cho. 2006. Automatic identi-
fication of user interest for personalized search. In
Proceedings of the 15th International Conference on
World Wide Web, pages 727?736, Edinburgh, Scot-
land.
Sun Aaron R., Cheng Jiesi, Zeng, and Daniel Dajun.
2009. A novel recommendation framework for micro-
blogging based on information diffusion. In Pro-
ceedings of the 19th Annual Workshop on Information
Technologies and Systems, pages 199?216, Phoenix,
Arizona.
Daniel Ramage, Susan Dumais, and Dan Liebling. 2010.
Characterizing microblogs with topic models. In In-
ternational AAAI Conference on Weblogs and Social
Media, pages 130?137. The AAAI Press.
Gerard Salton and Christopher Buckley. 1988. Term-
weighting approaches in automatic text retrieval. In-
formation Processing and Management, 24(5):513?
523.
Jaime Teevan, Daniel Ramage, and Meredith Ringel Mor-
ris. 2011. #Twittersearch: a comparison of microblog
search and web search. In Proceedings of the 4th ACM
524
International Conference on Web Search and Data
Mining, pages 35?44, Hong Kong, China.
Ibrahim Uysal and W. Bruce Croft. 2011. User oriented
tweet ranking: a filtering approach to microblogs.
In Proceedings of the 20th ACM International Con-
ference on Information and Knowledge Management,
pages 2261?2264, Glasgow, Scotland.
Xiaojun Wan, Jianwu Yang, and Jianguo Xiao. 2007.
Single document summarization with document ex-
pansion. In Proceedings of the 22nd Conference
on Artificial Intelligence, pages 931?936, Vancouver,
British Columbia.
Xiaojun Wan, Huiying Li, and Jianguo Xiao. 2010.
Cross-language document summarization based on
machine translation quality prediction. In Proceed-
ings of the 48th Annual Meeting of the Association for
Computational Linguistics, pages 917?926, Uppsala,
Sweden.
Rui Yan, Jian-Yun Nie, and Xiaoming Li. 2011. Sum-
marize what you are interested in: An optimiza-
tion framework for interactive personalized summa-
rization. In Proceedings of the 2011 Conference on
Empirical Methods in Natural Language Processing,
pages 1342?1351. Association for Computational Lin-
guistics.
Ding Zhou, Sergey A. Orshanskiy, Hongyuan Zha, and
C. Lee Giles. 2007. Co-ranking authors and docu-
ments in a heterogeneous network. In Proceedings of
the 7th IEEE International Conference on Data Min-
ing, pages 739?744. IEEE.
525
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 43?47,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
A Novel Burst-based Text Representation Model
for Scalable Event Detection
Wayne Xin Zhao?, Rishan Chen?, Kai Fan?, Hongfei Yan?? and Xiaoming Li??
?School of Electronics Engineering and Computer Science, Peking University, China
?State Key Laboratory of Software, Beihang University, China
{batmanfly,tsunamicrs,fankaicn,yhf1029}@gmail.com, lxm@pku.edu.cn
Abstract
Mining retrospective events from text streams
has been an important research topic. Classic
text representation model (i.e., vector space
model) cannot model temporal aspects of doc-
uments. To address it, we proposed a novel
burst-based text representation model, de-
noted as BurstVSM. BurstVSM corresponds
dimensions to bursty features instead of terms,
which can capture semantic and temporal in-
formation. Meanwhile, it significantly reduces
the number of non-zero entries in the repre-
sentation. We test it via scalable event de-
tection, and experiments in a 10-year news
archive show that our methods are both effec-
tive and efficient.
1 Introduction
Mining retrospective events (Yang et al, 1998; Fung
et al, 2007; Allan et al, 2000) has been quite an im-
portant research topic in text mining. One standard
way for that is to cluster news articles as events by
following a two-step approach (Yang et al, 1998):
1) represent document as vectors and calculate simi-
larities between documents; 2) run the clustering al-
gorithm to obtain document clusters as events.1 Un-
derlying text representation often plays a critical role
in this approach, especially for long text streams. In
this paper, our focus is to study how to represent
temporal documents effectively for event detection.
Classical text representation methods, i.e., Vector
SpaceModel (VSM), have a few shortcomings when
dealing with temporal documents. The major one is
that it maps one dimension to one term, which com-
pletely ignores temporal information, and therefore
VSM can never capture the evolving trends in text
streams. See the example in Figure 1, D1 and D2
?Corresponding author.
1Post-processing may be also needed on the preliminary
document clusters to refine the results.
!" !#
$%&'
()*+*
,-./01#223 ,-./01#224
Figure 1: A motivating example. D1 and D2 are news
articles about U.S. presidential election respectively in
years 2004 and 2008.
may have a high similarity based on VSM due to the
presence of some general terms (e.g., ?election?) re-
lated to U.S. presidential election, although general
terms correspond to events in different periods (i.e.,
November 2004 and November 2008). Temporal
information has to be taken into consideration for
event detection. Another important issue is scala-
bility, with the increasing of the number in the text
stream, the size of the vocabulary, i.e., the number
of dimensions in VSM, can be very large, which re-
quires a considerable amount of space for storage
and time for downstream processing.
To address these difficulties, in this paper, we pro-
pose a burst based text representation method for
scalable event detection. The major novelty is to nat-
urally incorporate temporal information into dimen-
sions themselves instead of using external time de-
caying functions (Yang et al, 1998). We instantiate
this idea by using bursty features as basic representa-
tion units of documents. In this paper, bursty feature
refers to a sudden surge of the frequency of a single
term in a text stream, and it is represented as the term
itself together with the time interval during which
the burst takes place. For example, (Olympic,
Aug-08-2008, Aug-24-2008)
2 can be regarded
as a bursty feature. We also call the term in a bursty
2Beijing 2008 Olympic Games
43
feature its bursty term. In our model, each dimen-
sion corresponds to a bursty feature, which contains
both temporal and semantic information. Bursty fea-
tures capture and reflect the evolving topic trends,
which can be learnt by searching surge patterns in
stream data (Kleinberg, 2003). Built on bursty fea-
tures, our representation model can well adapt to text
streams with complex trends, and therefore provides
a more reasonable temporal document representa-
tion. We further propose a split-cluster-merge algo-
rithm to generate clusters as events. This algorithm
can run a mutli-thread mode to speed up processing.
Our contribution can be summarized as two as-
pects: 1) we propose a novel burst-based text rep-
resentation model, to our best knowledge, it is the
first work which explicitly incorporates temporal in-
formation into dimensions themselves; 2) we test
this representation model via scalable event detec-
tion task on a very large news corpus, and extensive
experiments show the proposed methods are both ef-
fective and efficient.
2 Burst-based Text Representation
In this section, we describe the proposed burst-based
text representation model, denoted as BurstVSM. In
BurstVSM, each document is represented as one
vector as in VSM, while the major novelty is that one
dimension is mapped to one bursty feature instead
of one term. In this paper, we define a bursty fea-
ture f as a triplet (wf , tfs , t
f
e ), where w is the bursty
term and ts and te are the start and end timestamps
of the bursty interval (period). Before introducting
BurstVSM, we first discuss how to identify bursty
features from text streams.
2.1 Burst Detection Algorithm
We follow the batch mode two-state automaton
method from (Kleinberg, 2003) for bursty feature
detection.3 In this model, a stream of documents
containing a term w are assumed to be generated
from a two-state automaton with a low frequency
state q0 and a high frequency state q1. Each state
has its own emission rate (p0 and p1 respectively),
and there is a probability for changing state. If an
interval of high states appears in the optimal state
sequence of some term, this term together with this
interval is detected as a bursty feature. To obtain
all bursty features in text streams, we can perform
burst detection on each term in the vocabulary. In-
stead of using a fixed p0 and p1 in (Kleinberg, 2003),
by following the moving average method (Vlachos
3The news articles in one day is treated as a batch.
et al, 2004) ,we parameterize p0 and p1 with the
time index for each batch, formally, we have p0(t)
and p1(t) for the tth batch. Given a term w, we
use a sliding window of length L to estimate p0(t)
and p1(t) for the tth batch as follows: p0(t) =?
j?Wt
Nj,w?
j?Wt
Nj
and p1(t) = p0(t) ? s, where Nj,w and
Nj are w ?s document frequency and the total num-
ber of documents in jth batch respectively. s is a
scaling factor lager than 1.0, indicating state q1 has
a faster rate, and it is empirically set as 1.5. Wt is a
time interval [max(t?L/2, 0), min(t+L/2, N)], and
the length of moving window L is set as 180 days.
All the other parts remain the same as in (Kleinberg,
2003). Our detection method is denoted as TVBurst.
2.2 Burst based text representation models
We apply TVBurst to all the terms in our vocabu-
lary to identify a set of bursty features, denoted as
B. Given B, a document di(t) with timestamp t is
represented as a vector of weights in bursty feature
dimensions:
di(t) = (di,1(t), di,2(t), ..., di,|B|(t)).
We define the jth weight of di as follows
di,j =
?
tf-idfi,wBj , if t ? [t
Bj
s , t
Bj
e ] ,
0, otherwise.
When the timestamp of di is in the bursty inter-
val of Bj and contains bursty term wBj , we set up
the weight using common used tf-idf method. In
BurstVSM, each dimension is mapped to one bursty
feature, and it considers both semantic and temporal
information. One dimension is active only when the
document falls in the corresponding bursty interval.
Usually, a document vector in BurstVSM has only
a few non-zero entries, which makes computation of
document similarities more efficient in large datasets
compared with traditional VSM.
The most related work to ours is the boostVSM
introduced by (He et al, 2007b), it proposes to
weight different term dimensions with correspond-
ing bursty scores. However, it is still based on term
dimensions and fails to deal with terms with mul-
tiple bursts. Suppose that we are dealing with a
text collection related with U.S. presidential elec-
tions, Fig. 2 show sample dimensions for these three
methods. In BurstVSM, one term with multiple
bursts will be naturally mapped to different dimen-
sions. For example, two bursty features ( presiden-
tial, Nov., 2004) and ( presidential, Nov., 2008 ) cor-
respond to different dimensions in BurstVSM, while
44
Figure 2: One example for comparisons of different rep-
resentation methods. Terms in red box correspond to
multiple bursty periods.
Table 1: Summary of different representation models.
Here dimension reduction refers to the reduction of non-
zero entries in representation vector.
semantic temporal dimension trend
information information reduction modeling
VSM ? ? ? bad
boostVSM ? partially ? moderate
BurstVSM ? ? ? good
VSM and boostVSM cannot capture such temporal
differences. Some methods try to design time de-
caying functions (Yang et al, 1998), which decay
the similarity with the increasing of time gap be-
tween two documents. However, it requires efforts
for function selection and parameters tuning. We
summarize these discussions in Table 1.
3 split-cluster-merge algorithm for event
detection
In this section, we discuss how to cluster documents
as events. Since each document can be represented
as a burst-based vector, we use cosine function to
compute document similarities. Due to the large size
of our news corpus, it is infeasible to cluster all the
documents straightforward. We develop a heuristic
clustering algorithm for event detection, denoted as
split-cluster-merge, which includes three main steps,
namely split, cluster and merge. The idea is that we
first split the dataset into small parts, then cluster
the documents of each part independently and finally
merge similar clusters from two consecutive parts.
In our dataset, we find that most events last no more
than one month, so we split the dataset into parts by
months. After splitting, clustering can run in paral-
lel for different parts (we useCLUTO4 as the cluster-
ing tool), which significantly reduces total time cost.
For merge, we merge clusters in consecutive months
with an empirical threshold of 0.5. The final clusters
4www.cs.umn.edu/k?arypis/cluto
are returned as identified events.
4 Evaluation
4.1 Experiment Setup
We used a subset of 68 millon deduplicated
timestamped web pages generated from this
archive (Huang et al, 2008). Since our major focus
is to detect events from news articles, we only keep
the web pages with keyword ?news? in URL field.
The final collection contains 11, 218, 581 articles
with total 1, 730, 984, 304 tokens ranging from 2000
to 2009. We run all the experiments on a 64-bit linux
server with four Quad-Core AMD Opteron(tm) Pro-
cessors and 64GB of RAM. For split-cluster-merge
algorithm, we implement the cluster step in a multi-
thread mode, so that different parts can be processed
in parallel.
4.2 Construction of test collection
We manually construct the test collection for event
detection. To examine the effectiveness of event de-
tection methods in different grains, we consider two
type of events in terms of the number of relevant
documents, namely significant events and moder-
ate events. A significant event is required to have
at least 300 relevant docs, and a moderate event is
required to have 10 ? 100 relevant docs. 14 grad-
uate students are invited to generate the test collec-
tion, starting with a list of 100 candidate seed events
by referring to Xinhua News.5 For one target event,
the judges first construct queries with temporal con-
straints to retrieve candidate documents and then
judge wether they are relevant or not. Each doc-
ument is assigned to three students, and we adopt
the majority-win strategy for the final judgment. Fi-
nally, by removing all candidate seed events which
neither belong to significant events nor moderate
events, we derive a test collection consisting of 24
significant events and 40 moderate events.6
4.3 Evaluation metrics and baselines
Similar to the evaluation in information retrieval ,
given a target event, we evaluate the quality of the
returned ?relevant? documents by systems. We use
average precision, average recall and mean average
precision(MAP) as evaluation metrics. A difference
is that we do not have queries, and the output of a
system is a set of document clusters. So for a sys-
tem, given an event in golden standard, we first se-
lect the cluster (the system generates) which has the
5http://news.xinhuanet.com/english
6For access to the code and test collection, contact Xin Zhao
via batmanfly@gmail.com.
45
Table 2: Results of event detection. Our proposed method is better than all the other baselines at confidence level 0.9.
Signifcant Events Moderate Events
P R F MAP P R F MAP
timemines-?2(nouns) 0.52 0.2 0.29 0.11 0.22 0.27 0.24 0.09
timemines-?2(NE) 0.61 0.18 0.28 0.08 0.27 0.25 0.26 0.13
TVBurst+boostVSM 0.67 0.44 0.53 0.31 0.22 0.39 0.28 0.13
swan+BurstVSM 0.74 0.56 0.64 0.48 0.39 0.54 0.45 0.38
kleiberg+BurstVSM 0.68 0.63 0.65 0.52 0.35 0.53 0.42 0.36
TVBurst+BurstVSM 0.78 0.69 0.73 0.63 0.4 0.61 0.48 0.39
Table 3: Comparisons of average intra-class and inter-
class similarity.
Significant Events Moderate Events
Methods Intra Inter Intra Inter
TVBurst+boostVSM 0.234 0.132 0.295 0.007
TVBurst+BurstVSM 0.328 0.014 0.480 0.004
most relevant documents, then sort the documents
in the descending order of similarities with the clus-
ter centroid and finally compute P, R ,F and MAP in
this cluster. We perform Wilcoxon signed-rank test
for significance testing.
We used the event detection method in (Swan
and Allan, 2000) as baseline, denoted as timemines-
?2. As (Swan and Allan, 2000) suggested, we
tried two versions: 1) using all nouns and 2) us-
ing all named entities. Recall that BurstVSM re-
lies on bursty features as dimensions, we tested dif-
ferent burst detection algorithms in our proposed
BurstVSM model, including swan (Swan and Al-
lan, 2000), kleinberg (Kleinberg, 2003) and our pro-
posed TVBurst algorithm.
4.4 Experiment results
Preliminary results. In Table 2, we can see that 1)
BurstVSM with any of these three burst detection al-
gorithms is significantly better than timemines-?2,
suggesting our event detection method is very ef-
fective; 2) TVBurst with BurstVSM gives the best
performance, which suggests using moving average
base probability will improve the performance of
burst detection. We use TVBurst as the default burst
detection algorithm in later experiments.
Then we compare the performance of differ-
ent text representation models for event detection,
namely BurstVSM and boostVSM (He et al, 2007b;
He et al, 2007a).7 For different representation mod-
els, we use split-cluster-merge as clustering algo-
rithm. Table 2 shows that BurstVSM is much ef-
fecitve than boostVSM for event detection. In fact,
we empirically find boostVSM is appropriate for
7We use the same parameter settings in the original paper.
Table 4: Comparisons of observed runtime and storage.
boostVSM BurstVSM
Aver. # of non-zero entries per doc 149 14
File size for storing vectors (gigabytes) 3.74 0.571
Total # of merge 10,265,335 9,801,962
Aver. cluster cost per month (sec.) 355 55
Total merge cost (sec.) 2,441 875
Total time cost (sec.) 192,051 4,851
clustering documents in a coarse grain (e.g., in topic
level) but not for event detection.
Intra-class and inter-class similarities. In our
methods, event detection is treated as document
clustering. It is very important to study how similari-
ties affect the performance of clustering. To see why
our proposed representation methods are better than
boostVSM, we present the average intra-class simi-
larity and inter-class similarity for different events in
Table 3.8 We can see BurstVSM results in a larger
intra-class similarity and a smaller inter-class simi-
larity than boostVSM.
Analysis of the space/time complexity. We fur-
ther analyze the space/time complexity of different
representation models. In Table 4. We can see that
BurstVSM has much smaller space/time cost com-
pared with boostVSM, and meanwhile it has a better
performance for event detection (See Table 2). In
burst-based representation, one document has fewer
non-zero entries.
Acknowledgement. The core idea of this work
is initialized and developped by Kai Fan. This
work is partially supported by HGJ 2010 Grant
2011ZX01042-001-001, NSFC Grant 61073082 and
60933004. Xin Zhao is supported by Google PhD
Fellowship (China). We thank the insightful com-
ments from Junjie Yao, Jing Liu and the anony-
mous reviewers. We have developped an online Chi-
nese large-scale event search engine based on this
work, visit http://sewm.pku.edu.cn/eventsearch for
more details.
8For each event in our golden standard, we have two clus-
ters: relevant documents and non-relevant documents(within
the event period).
46
References
James Allan, Victor Lavrenko, and Hubert Jin. 2000.
First story detection in TDT is hard. In Proceedings
of the ninth international conference on Information
and knowledge management.
Gabriel Pui Cheong Fung, Jeffrey Xu Yu, Huan Liu, and
Philip S. Yu. 2007. Time-dependent event hierarchy
construction. In SIGKDD.
Q. He, K. Chang, and E. P. Lim. 2007a. Using burstiness
to improve clustering of topics in news streams. In
ICDM.
Qi He, Kuiyu Chang, Ee-Peng Lim, and Jun Zhang.
2007b. Bursty feature representation for clustering
text streams. In SDM.
L. Huang, L. Wang, and X. Li. 2008. Achieving both
high precision and high recall in near-duplicate detec-
tion. In CIKM.
J. Kleinberg. 2003. Bursty and hierarchical structure in
streams. Data Mining and Knowledge Discovery.
Russell Swan and James Allan. 2000. Automatic gener-
ation of overview timelines. In SIGIR.
Michail Vlachos, Christopher Meek, Zografoula Vagena,
and Dimitrios Gunopulos. 2004. Identifying similari-
ties, periodicities and bursts for online search queries.
In SIGMOD.
Yiming Yang, Tom Pierce, and Jaime Carbonell. 1998.
A study of retrospective and on-line event detection.
In SIGIR.
47
