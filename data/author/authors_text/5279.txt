Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 72?80,
Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
Effective Use of Linguistic and Contextual Information
for Statistical Machine Translation
Libin Shen and Jinxi Xu and Bing Zhang and
Spyros Matsoukas and Ralph Weischedel
BBN Technologies
Cambridge, MA 02138, USA
{lshen,jxu,bzhang,smatsouk,weisched}@bbn.com
Abstract
Current methods of using lexical features
in machine translation have difficulty in
scaling up to realistic MT tasks due to
a prohibitively large number of parame-
ters involved. In this paper, we propose
methods of using new linguistic and con-
textual features that do not suffer from
this problem and apply them in a state-of-
the-art hierarchical MT system. The fea-
tures used in this work are non-terminal
labels, non-terminal length distribution,
source string context and source depen-
dency LM scores. The effectiveness of
our techniques is demonstrated by signif-
icant improvements over a strong base-
line. On Arabic-to-English translation,
improvements in lower-cased BLEU are
2.0 on NIST MT06 and 1.7 on MT08
newswire data on decoding output. On
Chinese-to-English translation, the im-
provements are 1.0 on MT06 and 0.8 on
MT08 newswire data.
1 Introduction
Linguistic and context features, especially sparse
lexical features, have been widely used in re-
cent machine translation (MT) research. Unfor-
tunately, existing methods of using such features
are not ideal for large-scale, practical translation
tasks.
In this paper, we will propose several prob-
abilistic models to effectively exploit linguistic
and contextual information for MT decoding, and
these new features do not suffer from the scalabil-
ity problem. Our new models are tested on NIST
MT06 and MT08 data, and they provide signifi-
cant improvement over a strong baseline system.
1.1 Previous Work
The ideas of using labels, length preference and
source side context in MT decoding were explored
previously. Broadly speaking, two approaches
were commonly used in existing work.
One is to use a stochastic gradient descent
(SGD) or Perceptron like online learning algo-
rithm to optimize the weights of these features
directly for MT (Shen et al, 2004; Liang et al,
2006; Tillmann and Zhang, 2006). This method is
very attractive, since it opens the door to rich lex-
ical features. However, in order to robustly opti-
mize the feature weights, one has to use a substan-
tially large development set, which results in sig-
nificantly slower tuning. Alternatively, one needs
to carefully select a development set that simulates
the test set to reduce the risk of over-fitting, which
however is not always realistic for practical use.
A remedy is to aggressively limit the feature
space, e.g. to syntactic labels or a small fraction
of the bi-lingual features available, as in (Chiang
et al, 2008; Chiang et al, 2009), but that reduces
the benefit of lexical features. A possible generic
solution is to cluster the lexical features in some
way. However, how to make it work on such a
large space of bi-lingual features is still an open
question.
The other approach is to estimate a single score
or likelihood of a translation with rich features,
for example, with the maximum entropy (Max-
Ent) method as in (Carpuat and Wu, 2007; Itty-
cheriah and Roukos, 2007; He et al, 2008). This
method avoids the over-fitting problem, at the ex-
pense of losing the benefit of discriminative train-
ing of rich features directly for MT. However, the
feature space problem still exists in these pub-
lished models.
He et al (2008) extended the WSD-like ap-
proached proposed in (Carpuat and Wu, 2007) to
hierarchical decoders. In (He et al, 2008), lexical
72
features were limited on each single side due to the
feature space problem. In order to further reduce
the complexity of MaxEnt training, they ?trained
a MaxEnt model for each ambiguous hierarchical
LHS? (left-hand side or source side) of translation
rules. Different target sides were treated as possi-
ble labels. Therefore, the sample sets of each indi-
vidual MaxEnt model were very small, while the
number of features could easily exceed the number
of samples. Furthermore, optimizing individual
MaxEnt models in this way does not lead to global
maximum. In addition, MaxEnt models trained on
small sets are unstable.
The MaxEnt model in (Ittycheriah and Roukos,
2007) was optimized globally, so that it could bet-
ter employ the distribution of the training data.
However, one has to filter the training data ac-
cording to the test data to get competitive perfor-
mance with this model 1. In addition, the filtering
method causes some practical issues. First, such
methods are not suitable for real MT tasks, espe-
cially for applications with streamed input, since
the model has to be retrained with each new input
sentence or document and training is slow. Fur-
thermore, the model is ill-posed. The translation
of a source sentence depends on other source sen-
tences in the same batch with which the MaxEnt
model is trained. If we add one more sentence to
the batch, translations of other sentences may be-
come different due to the change of the MaxEnt
model.
To sum up, the existing models of employing
rich bi-lingual lexical information in MT are im-
perfect. Many of them are not ideal for practical
translation tasks.
1.2 Our Approach
As for our approach, we mainly use simple proba-
bilistic models, i.e. Gaussian and n-gram models,
which are more robust and suitable for large-scale
training of real data, as manifested in state-of-the-
art systems of speech recognition. The unique
contribution of our work is to design effective and
efficient statistical models to capture useful lin-
guistic and context information for MT decoding.
Feature functions defined in this way are robust
and ideal for practical translation tasks.
1According to footnote 2 of (Ittycheriah and Roukos,
2007), test set adaptation by test set sampling of the train-
ing corpus showed an advantage of more than 2 BLEU points
over a general system trained on all data.
1.2.1 Features
In this paper, we will introduce four new linguistic
and contextual feature functions. Here, we first
provide a high-level description of these features.
Details of the features are discussed in Section 2.
The first feature is based on non-terminal labels,
i.e. POS tags of the head words of target non-
terminals in transfer rules. This feature reduces
the ambiguity of translation rules. The other bene-
fit is that POS tags help to weed out bad target side
tree structures, as an enhancement to the target de-
pendency language model.
The second feature is based on the length dis-
tribution of non-terminals. In English as well as
in other languages, the same deep structure can
be represented in different syntactic structures de-
pending on the complexity of its constituents. We
model such preferences by associating each non-
terminal of a transfer rule with a probability distri-
bution over its length. Similar ideas were explored
in (He et al, 2008). However their length features
only provided insignificant improvement of 0.1
BLEU point. A crucial difference of our approach
is how the length preference is modeled. We ap-
proximate the length distribution of non-terminals
with a smoothed Gaussian, which is more robust
and gives rise to much larger improvement consis-
tently.
The third feature utilizes source side context in-
formation, i.e. the neighboring words of an input
span, to influence the selection of the target trans-
lation for a span. While the use of context infor-
mation has been explored in MT, e.g. (Carpuat
and Wu, 2007) and (He et al, 2008), the specific
technique we used by means of a context language
model is rather different. Our model is trained on
the whole training data, and it is not limited by the
constraint of MaxEnt training.
The fourth feature exploits structural informa-
tion on the source side. Specifically, the decoder
simultaneously generates both the source and tar-
get side dependency trees, and employs two de-
pendency LMs, one for the source and the other
for the target, for scoring translation hypotheses.
Our intuition is that the likelihood of source struc-
tures provides another piece of evidence about the
plausibility of a translation hypothesis and as such
would help weed out bad ones.
73
1.2.2 Baseline System and Experimental
Setup
We take BBN?s HierDec, a string-to-dependency
decoder as described in (Shen et al, 2008), as our
baseline for the following two reasons:
? It provides a strong baseline, which ensures
the validity of the improvement we would ob-
tain. The baseline model used in this paper
showed state-of-the-art performance at NIST
2008 MT evaluation.
? The baseline algorithm can be easily ex-
tended to incorporate the features proposed
in this paper. The use of source dependency
structures is a natural extension of the string-
to-tree model to a tree-to-tree model.
To ensure the generality of our results, we tested
the features on two rather different language pairs,
Arabic-to-English and Chinese-to-English, using
two metrics, IBM BLEU (Papineni et al, 2001)
and TER (Snover et al, 2006). Our experiments
show that each of the first three features: non-
terminal labels, length distribution and source side
context, improves MT performance. Surprisingly,
the source dependency feature does not produce
an improvement.
2 Linguistic and Context Features
2.1 Non-terminal Labels
In the original string-to-dependency model (Shen
et al, 2008), a translation rule is composed of a
string of words and non-terminals on the source
side and a well-formed dependency structure on
the target side. A well-formed dependency struc-
ture could be either a single-rooted dependency
tree or a set of sibling trees. As in the Hiero system
(Chiang, 2007), there is only one non-terminal X
in the string-to-dependency model. Any sub de-
pendency structure can be used to replace a non-
terminal in a rule.
For example, we have a source sentence in Chi-
nese as follows.
? jiantao zhuyao baohan liang fangmian
The literal translation for individual words is
? ?review? ?mainly? ?to consist of? ?two? ?part?
The reference translation is
? the review mainly consists of two parts
A single source word can be translated into
many English words. For example, jiantao can
be translated into a review, the review, reviews,
the reviews, reviewing, reviewed, etc. Suppose
we have source-string-to-target-dependency trans-
lation rules as shown in Figure 1. Since there is
no constraint on substitution, any translation for
jiantao could replace the X-1 slot.
One way to alleviate this problem is to limit the
search space by using a label system. We could
assign a label to each non-terminal on the target
side of the rules. Furthermore, we could assign a
label to the whole target dependency structure, as
shown in Figure 2. In decoding, each target de-
pendency sub-structure would be associated with
a label. Whenever substitution happens, we would
check whether the label of the sub-structure and
the label of the slot are the same. Substitutions
with unmatched labels would be prohibited.
In practice, we use a soft constraint by penaliz-
ing substitutions with unmatched labels. We intro-
duce a new feature: the number of times substitu-
tions with unmatched labels appear in the deriva-
tion of a translation hypothesis.
Obviously, to implement this feature we need to
associate a label with each non-terminal in the tar-
get side of a translation rule. The labels are gen-
erated during rule extraction. When we create a
rule from a training example, we replace a sub-
tree or dependency structure with a non-terminal
and associate it with the POS tag of the head word
if the non-terminal corresponds to a single-rooted
tree on the target side. Otherwise, it is assigned
the generic label X. (In decoding, all substitutions
of X are considered unmatched ones and incur a
penalty.)
2.2 Length Distribution
In English, the length of a phrase may determine
the syntactic structure of a sentence. For example,
possessive relations can be represented either as
?A?s B? or ?B of A?. The former is preferred if A
is a short phrase (e.g. ?the boy?s mother?) while
the latter is preferred if A is a complex structure
(e.g. ?the mother of the boy who is sick?).
Our solution is to build a model of length prefer-
ence for each non-terminal in each translation rule.
To address data sparseness, we assume the length
distribution of each non-terminal in a transfer rule
is a Gaussian, whose mean and variance can be
estimated from the training data. In rule extrac-
74
the
X
X
jiantao
reviews
the
X
X
jiantao
review
X X?1 baohan
X
X?1
consists
X?2
X?2
ofmainly
zhuyao
Figure 1: Translation rules with one label X
the
X jiantao
reviews
the
X jiantao
review
X X?1 baohan
consists
NNS VBZNN
NN?1 NNS?2
X?2
ofmainly
zhuyao
Figure 2: Translation rules with multiple labels
tion, each time a translation rule is generated from
a training example, we can record the length of the
source span corresponding to a non-terminal. In
the end, we have a frequency histogram for each
non-terminal in each translation rule. From the
histogram, a Gaussian distribution can be easily
computed.
In practice, we do not need to collect the fre-
quency histogram. Since all we need to know are
the mean and the variance, it is sufficient to col-
lect the sum of the length and the sum of squared
length.
Let r be a translation rule that occurs N
r
times
in training. Let x be a specific non-terminal in that
rule. Let l(r, x, i) denote the length of the source
span corresponding to non-terminal x in the i-th
occurrence of rule r in training. Then, we can
compute the following quantities.
m
r,x
=
1
N
r
N
r
?
i=1
l(r, x, i) (1)
s
r,x
=
1
N
r
N
r
?
i=1
l(r, x, i)
2
, (2)
which can be subsequently used to estimate the
mean ?
r,x
and variance ?2
r,x
of x?s length distri-
bution in rule r as follows.
?
r,x
= m
r,x
(3)
?
2
r,x
= s
r,x
?m
2
r,x
(4)
Since many of the translation rules have few oc-
currences in training, smoothing of the above esti-
mates is necessary. A common smoothing method
is based on maximum a posteriori (MAP) estima-
tion as in (Gauvain and Lee, 1994).
m?
r,x
=
N
r
N
r
+ ?
m
r,x
+
?
N
r
+ ?
m?
r,x
s?
r,x
=
N
r
N
r
+ ?
s
r,x
+
?
N
r
+ ?
s?
r,x
,
where ? stands for an MAP distribution and ? rep-
resents a prior distribution. m?
r,x
and s?
r,x
can
be obtained from a prior Gaussian distribution
N (??
r,x
, ??
r,x
) via equations (3) and (4), and ? is
a weight of smoothing.
There are many ways to approximate the prior
distribution. For example, we can have one prior
for all the non-terminals or one for individual non-
terminal type. In practice, we assume ??
r,x
= ?
r,x
,
and approximate ??
r,x
as (?2
r,x
+ s
r,x
)
1
2 .
In this way, we do not change the mean, but
relax the variance with s
r,x
. We tried differ-
ent smoothing methods, but the performance did
not change much, therefore we kept this simplest
setup. We also tried the Poisson distribution, and
the performance is similar to Gaussian distribu-
tion, which is about 0.1 point lower in BLEU.
When a rule r is applied during decoding, we
compute a penalty for each non-terminal x in r
according to
P (l | r, x) =
1
?
r,x
?
2pi
e
?
(l??
r,x
)
2
2?
2
r,x
,
where l is length of source span corresponding to
x.
Our method to address the problem of length
bias in rule selection is very different from the
maximum entropy method used in existing stud-
ies, e.g. (He et al, 2008).
75
2.3 Context Language Model
In the baseline string-to-dependency system, the
probability a translation rule is selected in decod-
ing does not depend on the sentence context. In
reality, translation is highly context dependent. To
address this defect, we introduce a new feature,
called context language model. The motivation of
this feature is to exploit surrounding words to in-
fluence the selection of the desired transfer rule for
a given input span.
To illustrate the problem, we use the same ex-
ample mentioned in Section 2.1. Suppose the
source span for rule selection is zhuyao baohan,
whose literal translation is mainly and to consist
of. There are many candidate translations for this
phrase, for example, mainly consist of, mainly
consists of, mainly including, mainly includes, etc.
The surrounding words can help to decide which
translation is more appropriate for zhuyao bao-
han. We compare the following two context-based
probabilities:
? P ( jiantao | mainly consist )
? P ( jiantao | mainly consists )
Here, jiantao is the source word preceding the
source span zhuyao baohan.
In the training data, jiantao is usually trans-
lated into the review, third-person singular, then
the probability P ( jiantao | mainly consists ) will
be higher than P ( jiantao | mainly consist ), since
we have seen more context events like the former
in the training data.
Now we introduce context LM formally. Let the
source words be f
1
f
2
..f
i
..f
j
..f
n
. Suppose source
sub-string f
i
..f
j
is translated into e
p
..e
q
. We can
define tri-gram probabilities on the left and right
sides of the source span:
? left : P
L
(f
i?1
|e
p
, e
p+1
)
? right : P
R
(f
j+1
|e
q
, e
q?1
)
In our implementation, the left and right context
LMs are estimated from the training data as part
of the rule extraction procedure. When we exact a
rule, we collect two 3-gram events, one for the left
side and the other for the right side.
In decoding, whenever a partial hypothesis is
generated, we calculate the context LM scores
based on the leftmost two words and the rightmost
two words of the hypothesis as well as the source
context. The product of the left and right context
LM scores is used as a new feature in the scoring
function.
Please note that our approach is very different
from other approaches to context dependent rule
selection such as (Ittycheriah and Roukos, 2007)
and (He et al, 2008). Instead of using a large num-
ber of fine grained features with weights optimized
using the maximum entropy method, we treat con-
text dependency as an ngram LM problem, and it
is smoothed with Witten-Bell discounting. The es-
timation of the context LMs is very efficient and
robust.
The benefit is two fold. The estimation of the
context LMs is very efficient. It adds only one new
weight to the scoring function.
2.4 Source Dependency Language Model
The context LM proposed in the previous sec-
tion only employs source words immediately be-
fore and after the current source span in decod-
ing. To exploit more source context, we use a
source side dependency language model as an-
other feature. The motivation is to take advantage
of the long distance dependency relations between
source words in scoring a translation theory.
We extended string-to-dependency rules in
the baseline system to dependency-to-dependency
rules. In each dependency-to-dependency rule, we
keep record of the source string as well as the
source dependency structure. Figure 3 shows ex-
amples of dependency-to-dependency rules.
We extended the string-to-dependency decod-
ing algorithm in the baseline to accommodate
dependency-to-dependency theories. In decoding,
we build both the source and the target depen-
dency structures simultaneously in chart parsing
over the source string. Thus, we can compute the
source dependency LM score in the same way we
compute the target side score, using a procedure
described in (Shen et al, 2008).
We introduce two new features for the source
side dependency LM as follows, in a way similar
to the target side.
? Source dependency LM score
? Discount on ill-formed source dependency
structures
The source dependency LM is trained on the
source side of the bi-lingual training data with
Witten-Bell smoothing. The source dependency
LM score represents the likelihood of the source
76
X?1 X?2zhuyao
baohan
the
X
X
jiantao
reviews
the
X
X
jiantao
review
X
X X?1
consists
X?2ofmainly
Figure 3: Dependency-to-dependency translation rules
dependency tree generated by the decoder. The
source dependency tree with the highest score is
the one that is most likely to be generated by the
dependency model that created the source side of
the training data.
Source dependency trees are composed of frag-
ments embedded in the translation rules. There-
fore, a source dependency LM score can be
viewed as a measure whether the translation rules
are put together in a way similar to the training
data. Therefore, a source dependency LM score
serves as a feature to represent structural con-
text information that is capable of modeling long-
distance relations.
However, unlike source context LMs, the struc-
tural context information is used only when two
partial dependency structures are combined, while
source context LMs work as a look-ahead feature.
3 Experiments
We designed our experiments to show the impact
of each feature separately as well as their cumula-
tive impact:
? BASE: baseline string-to-dependency system
? SLM: baseline + source dependency LM
? CLM: baseline + context LM
? LEN: baseline + length distribution
? LBL: baseline + syntactic labels
? LBL+LEN: baseline + syntactic labels +
length distribution
? LBL+LEN+CLM: baseline + syntactic labels
+ length distribution + context LM
All the models were optimized on lower-cased
IBM BLEU with Powell?s method (Powell, 1964;
Brent, 1973) on n-best translations (Ostendorf et
al., 1991), but evaluated on both IBM BLEU and
TER. The motivation is to detect if an improve-
ment is artificial, i.e., specific to the tuning met-
ric. For both Arabic-to-English and Chinese-to-
English MT, we tuned on NIST MT02-05 and
tested on MT06 and MT08 newswire sets.
The training data are different from what was
usd at MT06 or MT08. Our Arabic-to-English
data contain 29M Arabic words and 38M En-
glish words from 11 corpora: LDC2004T17,
LDC2004T18, LDC2005E46, LDC2006E25,
LDC2006G05, LDC2005E85, LDC2006E36,
LDC2006E82, LDC2006E95, Sakhr-A2E and
Sakhr-E2A. The Chinese-to-English data contain
107M Chinese words and 132M English words
from eight corpora: LDC2002E18, LDC2005T06,
LDC2005T10, LDC2006E26, LDC2006G05,
LDC2002L27, LDC2005T34 and LDC2003E07.
They are available under the DARPA GALE
program. Traditional 3-gram and 5-gram string
LMs were trained on the English side of the
parallel data plus the English Gigaword corpus
V3.0 in a way described in (Bulyko et al, 2007).
The target dependency LMs were trained on the
English side of the parallel training data. For that
purpose, we parsed the English side of the parallel
data. Two separate models were trained: one for
Arabic from the Arabic training data and the other
for Chinese from the Chinese training data.
To compute the source dependency LM for
Chinese-to-English MT, we parsed the Chinese
side of the Chinese-to-English parallel data. Due
to the lack of a good Arabic parser compatible
with the Sakhr tokenization that we used on the
source side, we did not test the source dependency
LM for Arabic-to-English MT.
When extracting rules with source dependency
structures, we applied the same well-formedness
constraint on the source side as we did on the tar-
get side, using a procedure described by (Shen
et al, 2008). Some candidate rules were thrown
away due to the source side constraint. On the
77
Model
MT06 MT08
BLEU TER BLEU TER
lower mixed lower mixed lower mixed lower mixed
Decoding (3-gram LM)
BASE 48.75 46.74 43.43 45.79 49.58 47.46 42.80 45.08
CLM 49.44 47.36 42.96 45.22 49.73 47.53 42.64 44.92
LEN 49.37 47.28 43.01 45.35 50.29 48.19 42.32 44.45
LBL 49.33 47.07 43.09 45.53 50.46 48.19 42.27 44.57
LBL+LEN 49.91 47.70 42.59 45.17 51.10 48.85 41.88 44.16
LBL+LEN+CLM 50.75 48.51 42.13 44.50 51.24 49.10 41.63 43.80
Rescoring (5-gram LM)
BASE 51.24 49.23 42.08 44.42 51.23 49.11 42.01 44.15
CLM 51.57 49.54 41.74 43.88 51.44 49.37 41.63 43.74
LEN 52.05 50.01 41.50 43.72 51.88 49.89 41.51 43.47
LBL 51.80 49.69 41.54 43.76 51.93 49.86 41.27 43.33
LBL+LEN 51.90 49.76 41.41 43.70 52.42 50.29 40.93 43.00
LBL+LEN+CLM 52.61 50.51 40.77 43.03 52.60 50.56 40.69 42.81
Table 1: BLEU and TER percentage scores on MT06 and MT08 Arabic-to-English newswire sets.
other hand, one string-to-dependency rule may
split into several dependency-to-dependency rules
due to different source dependency structures. The
size of the dependency-to-dependency rule set is
slightly smaller than the size of the string-to-
dependency rule set.
Tables 1 and 2 show the BLEU and TER per-
centage scores on MT06 and MT08 for Arabic-
to-English and Chinese-to-English translation re-
spectively. The context LM feature, the length
feature and the syntax label feature all produce
a small improvement for most of the conditions.
When we combined the three features, we ob-
served significant improvements over the baseline.
For Arabic-to-English MT, the LBL+LEN+CLM
system improved lower-cased BLEU by 2.0 on
MT06 and 1.7 on MT08 on decoding output.
For Chinese-to-English MT, the improvements in
lower-cased BLEU were 1.0 on MT06 and 0.8 on
MT08. After re-scoring, the improvements be-
came smaller, but still noticeable, ranging from 0.7
to 1.4. TER scores were also improved noticeably
for all conditions, suggesting there was no metric
specific over-tuning.
Surprisingly, source dependency LM did not
provide any improvement over the baseline. There
are two possible reasons for this. One is that
the source and target parse trees were generated
by two stand-alone parsers, which may cause in-
compatible structures on the source and target
sides. By applying the well-formed constraints
on both sides, a lot of useful transfer rules are
discarded. A bi-lingual parser, trained on paral-
lel treebanks recently made available to the NLP
community, may overcome this problem. The
other is that the search space of dependency-to-
dependency decoding is much larger, since we
need to add source dependency information into
the chart parsing states. We will explore tech-
niques to address these problems in the future.
4 Discussion
Linguistic information has been widely used in
SMT. For example, in (Wang et al, 2007), syntac-
tic structures were employed to reorder the source
language as a pre-processing step for phrase-based
decoding. In (Koehn and Hoang, 2007), shallow
syntactic analysis such as POS tagging and mor-
phological analysis were incorporated in a phrasal
decoder.
In ISI?s syntax-based system (Galley et al,
2006) and CMU?s Hiero extension (Venugopal et
al., 2007), non-terminals in translation rules have
labels, which must be respected by substitutions
during decoding. In (Post and Gildea, 2008; Shen
et al, 2008), target trees were employed to im-
prove the scoring of translation theories. Mar-
ton and Resnik (2008) introduced features defined
on constituent labels to improve the Hiero system
(Chiang, 2005). However, due to the limitation of
MER training, only part of the feature space could
used in the system. This problem was fixed by
78
Model
MT06 MT08
BLEU TER BLEU TER
lower mixed lower mixed lower mixed lower mixed
Decoding (3-gram LM)
BASE 37.44 35.62 54.64 56.47 33.05 31.26 56.79 58.69
SLM 37.30 35.48 54.24 55.90 33.03 31.00 56.59 58.46
CLM 37.66 35.81 53.45 55.19 32.97 31.01 55.99 57.77
LEN 38.09 36.26 53.98 55.81 33.23 31.34 56.51 58.41
LBL 38.37 36.53 54.14 55.99 33.25 31.34 56.60 58.49
LBL+LEN 38.36 36.59 53.95 55.60 33.72 31.83 56.79 58.65
LBL+LEN+CLM 38.41 36.57 53.83 55.70 33.83 31.79 56.55 58.51
Rescoring (5-gram LM)
BASE 38.91 37.04 53.65 55.45 34.34 32.32 55.60 57.60
SLM 38.27 36.38 53.64 55.29 34.25 32.28 55.35 57.21
CLM 38.79 36.88 53.09 54.80 35.01 32.98 55.39 57.28
LEN 39.22 37.30 53.34 55.06 34.65 32.70 55.61 57.51
LBL 39.11 37.30 53.61 55.29 35.02 33.00 55.39 57.48
LBL+LEN 38.91 37.17 53.56 55.27 35.03 33.08 55.47 57.46
LBL+LEN+CLM 39.58 37.62 53.21 54.94 35.72 33.63 54.88 56.98
Table 2: BLEU and TER percentage scores on MT06 and MT08 Chinese-to-English newswire sets.
Chiang et al (2008), which used an online learn-
ing method (Crammer and Singer, 2003) to handle
a large set of features.
Most SMT systems assume that translation
rules can be applied without paying attention to
the sentence context. A few studies (Carpuat and
Wu, 2007; Ittycheriah and Roukos, 2007; He et
al., 2008; Hasan et al, 2008) addressed this de-
fect by selecting the appropriate translation rules
for an input span based on its context in the in-
put sentence. The direct translation model in (It-
tycheriah and Roukos, 2007) employed syntactic
(POS tags) and context information (neighboring
words) within a maximum entropy model to pre-
dict the correct transfer rules. A similar technique
was applied by He et al (2008) to improve the Hi-
ero system.
Our model differs from previous work on the
way in which linguistic and contextual informa-
tion is used.
5 Conclusions and Future Work
In this paper, we proposed four new linguistic
and contextual features for hierarchical decoding.
The use of non-terminal labels, length distribution
and context LM features gave rise to significant
improvement on Arabic-to-English and Chinese-
to-English translation on NIST MT06 and MT08
newswire data over a state-of-the-art string-to-
dependency baseline. Unlike previous work, we
employed robust probabilistic models to capture
useful linguistic and contextual information. Our
methods are more suitable for practical translation
tasks.
In future, we will continue this work in two
directions. We will employ a Gaussian model
to unify various linguistic and contextual fea-
tures. We will also improve the dependency-to-
dependency method with a better bi-lingual parser.
Acknowledgments
This work was supported by DARPA/IPTO Con-
tract No. HR0011-06-C-0022 under the GALE
program.
References
R. P. Brent. 1973. Algorithms for Minimization With-
out Derivatives. Prentice-Hall.
I. Bulyko, S. Matsoukas, R. Schwartz, L. Nguyen, and
J. Makhoul. 2007. Language model adaptation in
machine translation from speech. In Proceedings of
the 32nd IEEE International Conference on Acous-
tics, Speech, and Signal Processing (ICASSP).
M. Carpuat and D. Wu. 2007. Context-dependent
phrasal translation lexicons for statistical machine
translation. In Proceedings of Machine Translation
Summit XI.
79
D. Chiang, Y. Marton, and P. Resnik. 2008. On-
line large-margin training of syntactic and structural
translation features. In Proceedings of the 2008
Conference of Empirical Methods in Natural Lan-
guage Processing.
D. Chiang, K. Knight, and W. Wang. 2009. 11,001
new features for statistical machine translation. In
Proceedings of the 2009 Human Language Technol-
ogy Conference of the North American Chapter of
the Association for Computational Linguistics.
D. Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings
of the 43th Annual Meeting of the Association for
Computational Linguistics (ACL).
D. Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2).
K. Crammer and Y. Singer. 2003. Ultraconservative
online algorithms for multiclass problems. Journal
of Machine Learning Research, 3:951?991.
M. Galley, J. Graehl, K. Knight, D. Marcu, S. DeNeefe,
W. Wang, and I. Thayer. 2006. Scalable infer-
ence and training of context-rich syntactic models.
In COLING-ACL ?06: Proceedings of 44th Annual
Meeting of the Association for Computational Lin-
guistics and 21st Int. Conf. on Computational Lin-
guistics.
J.-L. Gauvain and Chin-Hui Lee. 1994. Maximum a
posteriori estimation for multivariate gaussian mix-
tureobservations of markov chains. IEEE Transac-
tions on Speech and Audio Processing, 2(2).
S. Hasan, J. Ganitkevitch, H. Ney, and J. Andre?s-Ferrer.
2008. Triplet lexicon models for statistical machine
translation. In Proceedings of the 2008 Conference
of Empirical Methods in Natural Language Process-
ing.
Z. He, Q. Liu, and S. Lin. 2008. Improving statistical
machine translation using lexicalized rule selection.
In Proceedings of COLING ?08: The 22nd Int. Conf.
on Computational Linguistics.
A. Ittycheriah and S. Roukos. 2007. Direct translation
model 2. In Proceedings of the 2007 Human Lan-
guage Technology Conference of the North Ameri-
can Chapter of the Association for Computational
Linguistics.
P. Koehn and H. Hoang. 2007. Factored translation
models. In Proceedings of the 2007 Conference of
Empirical Methods in Natural Language Process-
ing.
P. Liang, A. Bouchard-Co?te?, D. Klein, and B. Taskar.
2006. An end-to-end discriminative approach to ma-
chine translation. In COLING-ACL ?06: Proceed-
ings of 44th Annual Meeting of the Association for
Computational Linguistics and 21st Int. Conf. on
Computational Linguistics.
Y. Marton and P. Resnik. 2008. Soft syntactic con-
straints for hierarchical phrased-based translation.
In Proceedings of the 46th Annual Meeting of the
Association for Computational Linguistics (ACL).
M. Ostendorf, A. Kannan, S. Austin, O. Kimball,
R. Schwartz, and J. R. Rohlicek. 1991. Integra-
tion of diverse recognition methodologies through
reevaluation of nbest sentence hypotheses. In Pro-
ceedings of the DARPA Workshop on Speech and
Natural Language.
K. Papineni, S. Roukos, and T. Ward. 2001. Bleu: a
method for automatic evaluation of machine transla-
tion. IBM Research Report, RC22176.
M. Post and D. Gildea. 2008. Parsers as language
models for statistical machine translation. In The
Eighth Conference of the Association for Machine
Translation in the Americas.
M. J. D. Powell. 1964. An efficient method for finding
the minimum of a function of several variables with-
out calculating derivatives. The Computer Journal,
7(2).
L. Shen, A. Sarkar, and F. J. Och. 2004. Discriminative
reranking for machine translation. In Proceedings of
the 2004 Human Language Technology Conference
of the North American Chapter of the Association
for Computational Linguistics.
L. Shen, J. Xu, and R. Weischedel. 2008. A New
String-to-Dependency Machine Translation Algo-
rithm with a Target Dependency Language Model.
In Proceedings of the 46th Annual Meeting of the
Association for Computational Linguistics (ACL).
M. Snover, B. Dorr, R. Schwartz, L. Micciulla, and
J. Makhoul. 2006. A study of translation edit rate
with targeted human annotation. In Proceedings of
Association for Machine Translation in the Ameri-
cas.
C. Tillmann and T. Zhang. 2006. A discrimina-
tive global training algorithm for statistical mt. In
COLING-ACL ?06: Proceedings of 44th Annual
Meeting of the Association for Computational Lin-
guistics and 21st Int. Conf. on Computational Lin-
guistics.
A. Venugopal, A. Zollmann, and S. Vogel. 2007.
An efficient two-pass approach to synchronous-cfg
driven statistical mt. In Proceedings of the 2007 Hu-
man Language Technology Conference of the North
American Chapter of the Association for Computa-
tional Linguistics.
C. Wang, M. Collins, and P. Koehn. 2007. Chinese
syntactic reordering for statistical machine transla-
tion. In Proceedings of the 2007 Conference of Em-
pirical Methods in Natural Language Processing.
80
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 708?717,
Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
Discriminative Corpus Weight Estimation for Machine Translation
Spyros Matsoukas and Antti-Veikko I. Rosti and Bing Zhang
BBN Technologies, 10 Moulton Street, Cambridge, MA 02138
{smatsouk,arosti,bzhang}@bbn.com
Abstract
Current statistical machine translation
(SMT) systems are trained on sentence-
aligned and word-aligned parallel text col-
lected from various sources. Translation
model parameters are estimated from the
word alignments, and the quality of the
translations on a given test set depends
on the parameter estimates. There are
at least two factors affecting the parame-
ter estimation: domain match and training
data quality. This paper describes a novel
approach for automatically detecting and
down-weighing certain parts of the train-
ing corpus by assigning a weight to each
sentence in the training bitext so as to op-
timize a discriminative objective function
on a designated tuning set. This way, the
proposed method can limit the negative ef-
fects of low quality training data, and can
adapt the translation model to the domain
of interest. It is shown that such discrim-
inative corpus weights can provide sig-
nificant improvements in Arabic-English
translation on various conditions, using a
state-of-the-art SMT system.
1 Introduction
Statistical machine translation (SMT) systems rely
on a training corpus consisting of sentences in
the source language and their respective reference
translations to the target language. These paral-
lel sentences are used to perform automatic word
alignment, and extract translation rules with asso-
ciated probabilities. Typically, a parallel training
corpus is comprised of collections of varying qual-
ity and relevance to the translation problem of in-
terest. For example, an SMT system applied to
broadcast conversational data may be trained on
a corpus consisting mostly of United Nations and
newswire data, with only a very small amount of
in-domain broadcast news/conversational data. In
this case, it would be desirable to down-weigh the
out-of-domain data relative to the in-domain data
during the rule extraction and probability estima-
tion. Similarly, it would be good to assign a lower
weight to data of low quality (e.g., poorly aligned
or incorrectly translated sentences) relative to data
of high quality.
In this paper, we describe a novel discrimina-
tive training method that can be used to estimate a
weight for each sentence in the training bitext so as
to optimize an objective function ? expected trans-
lation edit rate (TER) (Snover et al, 2006) ? on a
held-out development set. The training bitext typ-
ically consists of millions of (parallel) sentences,
so in order to ensure robust estimation we express
each sentence weight as a function of sentence-
level features, and estimate the parameters of this
mapping function instead. Sentence-level fea-
tures may include the identifier of the collection or
genre that the sentence belongs to, the number of
tokens in the source or target side, alignment infor-
mation, etc. The mapping from features to weights
can be implemented via any differentiable func-
tion, but in our experiments we used a simple per-
ceptron. Sentence weights estimated in this fash-
ion are applied directly to the phrase and lexical
counts unlike any previously published method to
the author?s knowledge. The tuning framework is
developed for phrase-based SMT models, but the
tuned weights are also applicable to the training of
a hierarchical model. In cases where the tuning set
used for corpus weight estimation is a close match
to the test set, this method yields significant gains
in TER, BLEU (Papineni et al, 2002), and ME-
TEOR (Lavie and Agarwal, 2007) scores over a
state-of-the-art hierarchical baseline.
The paper is organized as follows. Related work
on data selection, data weighting, and model adap-
tation is presented in Section 2. The corpus weight
708
approach and estimation algorithm are described
in Section 3. Experimental evaluation of the ap-
proach is presented in Sections 4 and 5. Section 6
concludes the paper with a few directions for fu-
ture work.
2 Related Work
Previous work related to corpus weighting may
be split into three categories: data selection, data
weighting, and translation model adaptation. The
first two approaches may improve the quality
of the word alignment and prevent phrase-pairs
which are less useful for the domain to be learned.
The model adaptation, on the other hand, may
boost the weight of the more relevant phrase-
pairs or introduce translations for unseen source
phrases.
Resnik and Smith (2003) mined parallel text
from the web using various filters to identify likely
translations. The filtering may be viewed as a
data selection where poor quality translation are
discarded before word alignment. Yasuda et al
(2008) selected subsets of an existing parallel cor-
pus to match the domain of the test set. The dis-
carded sentence pairs may be valid translations
but they do not necessarily improve the translation
quality on the test domain. Mandal et al (2008)
used active learning to select suitable training data
for human translation. Hildebrand et al (2005) se-
lected comparable sentences from parallel corpora
using information retrieval techniques.
Lu et al (2007) proposed weighting compara-
ble portions of the parallel text before word align-
ment based on information retrieval. The relevant
portions of the parallel text were given a higher in-
teger weight in GIZA++ word alignment. Similar
effect may be achieved by replicating the relevant
subset in the training data.
Lu et al (2007) also proposed training adapted
translation models which were interpolated with a
model trained on the entire parallel text. Snover
et al (2008) used cross-lingual information re-
trieval to identify possible bias-rules to improve
the coverage on the source side. These rules may
cover source phrases for which no translations
were learned from the available parallel text.
Koehn and Schroeder (2007) described a pro-
cedure for domain adaptation that was using two
translation models in decoding, one trained on
in-domain data and the other on out-of-domain
data. Phrase translation scores from the two mod-
els where combined in a log-linear fashion, with
weights estimated based on minimum error rate
training (Och, 2003) on a designated tuning set.
The method described in this paper can also be
viewed as data filtering or (static) translation adap-
tation, but it has the following advantages over
previously published techniques:
1. The estimated corpus weights are discrim-
inative and are computed so as to directly
optimize an MT performance metric on a
pre-defined development set. Unlike the do-
main adaptation technique in (Koehn and
Schroeder, 2007), which also estimates the
adaptation parameters discriminatively, our
proposed method does not require a man-
ual specification of the in-domain and out-
of-domain training data collections. Instead,
it automatically determines which collections
are most relevant to the domain of interest,
and increases their weight while decreasing
the weight assigned to less relevant collec-
tions.
2. All sentences in the parallel corpus can in-
fluence the translation model, as opposed
to filtering/discarding data. However, the
proposed method can still assign very low
weights to parts of the corpus, if it determines
that it helps improve MT performance.
3. The framework used for estimating the cor-
pus weights can be easily extended to support
discriminative alignment link-level weights,
thus allowing the system to automatically
identify which portions of the training sen-
tences are most useful.
Naturally, as with any method, the proposed
technique has certain limitations. Specifically, it
is only concerned with influencing the translation
rule probabilities via the corpus weights; it does
not change the set of rules extracted. Thus, it is
unable to add new translation rules as in Snover
et al (2008). Also, it can potentially lead to pa-
rameter over-fitting, especially if the function that
maps sentence features to weights is complex and
based on a large number of parameters, or if the
development set used for estimating the mapping
function does not match the characteristics of the
test set.
709
3 Corpus Weights Estimation
3.1 Feature Extraction
The purpose of feature extraction is to identify,
for each sentence in the parallel training data, a
set of features that can be useful in estimating a
weight that is correlated with quality or relevance
to the MT task at hand. Starting from sentence-
aligned, word-aligned parallel training data, one
could extract various types of sentence-level fea-
tures. For example, we could specify features that
describe the two sides of the parallel data or the
alignment between them, such as collection id,
genre id, number of source tokens, number of tar-
get tokens, ratio of number of source and target
tokens, number of word alignment links, fraction
of source tokens that are unaligned, and fraction
of target tokens that are unaligned. Additionally,
we could include information retrieval (IR) related
features that reflect the relevance of a training sen-
tence to the domain of interest, e.g., by measur-
ing vector space model (VSM) distance of the sen-
tence to the current tuning set, or its log likelihhod
with respect to an in-domain language model.
Note that the collection and genre identifiers
(ids) mentioned above are bit vectors. Each col-
lection in the training set is mapped to a number.
A collection may consist of sentences from multi-
ple genres (e.g., newswire, web, broadcast news,
broadcast conversations). Genres are also mapped
to a unique number across the whole training set.
Then, given a sentence in the training bitext, we
can extract a binary vector that contains two non-
zero bits, one indicating the collection id, and an-
other denoting the genre id.
It is worth mentioning that in the experiments
reported later in this paper we made use of only the
collection and genre ids as features, although the
framework supports general sentence-level fea-
tures.
3.2 Mapping Features to Weights
As mentioned previously, one way to map a fea-
ture vector to a weight is to use a perceptron.
A multi-layer neural network may also be used,
but at the expense of slower training. In this
work, all of the experiments carried out made use
of a perceptron mapping function. However, it
is also possible to cluster the training sentences
into classes by training a Gaussian mixture model
(GMM) on their respective feature vectors1. Then,
given a feature vector we can compute the (poste-
rior) probability that it was generated by one of
the N Gaussians in the GMM, and use this N-
dimensional vector of posteriors as input to the
perceptron. This is similar to having a neural net-
work with a static hidden layer and Gaussian acti-
vation functions.
Given the many choices available in mapping
features to weights, we will describe the mapping
function in general terms. Let f
i
be the n ? 1
feature vector corresponding to sentence i. Let
?(x;?) denote a function Rn ? (0, 1) that is pa-
rameterized in terms of the parameter vector ? and
maps a feature vector x to a scalar weight in (0, 1).
The goal of the automatic corpus weight estima-
tion procedure is to estimate the parameter vector
? so as to optimize an objective function on a de-
velopment set.
3.3 Training with Weighted Corpora
Once the sentence features have been mapped to
weights, the translation rule extraction and prob-
ability estimation can proceed as usual, but with
weighted counts. For example, let w
i
= ?(f
i
;?)
be the weight assigned to sentence i. Let (s, t) be
a source-target phrase pair that can be extracted
from the corpus, and A(s) and B(t) indicating the
sets of sentences that s and t occur in. Then,
P (s|t) =
?
j?A(s)?B(t)
w
j
c
j
(s, t)
?
j?B(t)
w
j
c
j
(t)
(1)
where c
j
(?) denotes the number of occurrences of
the phrase (or phrase-pair) in sentence j.
3.4 Optimizing the Mapping Function
Estimation of the parameters ? of the mapping
function ? can be performed by directly optimiz-
ing a suitable objective function on a development
set. Ideally, we would like to estimate the param-
eters of the mapping function so as to directly op-
timize an automatic MT performance evaluation
metric, such as TER or BLEU on the full transla-
tion search space. However, this is extremely com-
putationally intensive for two reasons: (a) opti-
mizing in the full translation search space requires
a new decoding pass for each iteration of opti-
mization; and (b) a direct optimization of TER or
1Note that in order to train such a GMM it may be nec-
essary to first apply a decorrelating, dimensionality reducing,
transform (e.g., principal component analysis) to the features.
710
BLEU requires the use of a derivative free, slowly
converging optimization method such as MERT
(Och, 2003), because these objective functions are
not differentiable.
In our case, for every parameter vector update
we need to essentially retrain the translation model
(reestimate the phrase and lexical translation prob-
abilities based on the updated corpus weights), so
the cost of each iteration is significantly higher
than in a typical MERT application. For these rea-
sons, in this work we chose to minimize the ex-
pected TER over a translation N-best on a desig-
nated tuning set, which is a continuous and differ-
entiable function and can be optimized with stan-
dard gradient descent methods in a small number
of iterations. Note, that using expected TER is not
the only option here; any criterion that can be ex-
pressed as a continuous function of the phrase or
lexical translation probabilities can be used to op-
timize ?.
Given an N-best of translation hypotheses over
a development set of S sentences, we can define
the expected TER as follows
T =
?
S
s=1
?
N
s
j=1
p
sj

sj
?
S
s=1
r
s
(2)
where N
s
is the number of translation hypothe-
ses available for segment s; 
sj
is the minimum
raw edit distance between hypothesis j of seg-
ment s (or h
sj
, for short) and the reference transla-
tion(s) corresponding to segment s; r
s
is the aver-
age number of reference translation tokens in seg-
ment s, and p
sj
is the posterior probability of hy-
pothesis h
sj
in the N-best. The latter is computed
as follows
p
sj
=
e
?L
sj
?
N
s
k=1
e
?L
sk
(3)
where L
sj
is the total log likelihood of hypothe-
sis h
sj
, and ? is a tunable scaling factor that can
be used to change the dynamic range of the likeli-
hood scores and hence the distribution of posteri-
ors over the N-best. The hypothesis likelihood L
sj
is typically computed as a dot product of a decod-
ing weight vector and a vector of various ?feature?
scores, such as log phrase translation probability,
log lexical translation probability, log n-gram lan-
guage model probability, and number of tokens in
the hypothesis. However, in order to simplify this
presentation we will assume that it contains a sin-
gle translation model score, the log phrase transla-
tion probability of source given target. This score
is a sum of log conditional probabilities, similar
to the one defined in Equation 1. Therefore, L
sj
is indirectly a function of the training sentence
weights.
In order to minimize the expected TER T , we
need to compute the derivative of T with respect
to the mapping function parameters ?. Using the
chain rule, we get equations (4)-(8), where the
summation in Equation 6 is over all source-target
phrase pairs in the derivation of hypothesis h
sm
, ?
is the decoding weight assigned to the log phrase
translation score, and the summation in Equation
7 is over all training sentences2.
Thus, in order to compute the derivative of
the objective function we first need to calculate
? lnP (s
k
|t
k
)
??
for every phrase pair (s
k
, t
k
) in the
translation N-best based on Equations 7 and 8,
which requires time proportional to the number of
occurrences of these phrases in the parallel train-
ing data. After that, we can compute ?Lsm
??
for
each hypothesis h
sm
, based on Equation 6. Fi-
nally, we calculate ? ln psj
??
and ?T
??
based on Equa-
tions 5 and 4, respectively.
3.5 Implementation Issues
In our system, the corpus weights were trained
based on N-best translation hypotheses generated
by a phrase-based MT system on a designated tun-
ing set. Each translation hypothesis in the N-best
has a score that is a (linear) function of the fol-
lowing log translation probabilities: target phrase
given source phrase, source phrase given target
phrase, and lexical smoothing term. Additionally,
each hypothesis specifies information about its
derivation, i.e., which source-target phrase pairs it
consists of. Therefore, given an N-best, we can
identify the set of unique phrase pairs and use this
information in order to perform a filtered accumu-
lation of the statistics needed for calculating the
derivative in Equation 8. This reduces the storage
needed for the sufficient statistics significantly.
Minimization of the expected TER of the N-
best hypotheses was performed using the limited-
memory BFGS algorithm (Liu and Nocedal,
1989). Typically, the parameter vector ? required
about 30 iterations of LBFGS to converge.
Since the N-best provides only a limited repre-
sentation of the MT hypothesis search space, we
regenerated the N-best after every 30 iterations
2In the general case where L
sj
includes other translation
scores, e.g., lexical translation probabilities, the derivative
?L
sm
??
will have to include additional terms.
711
?T
??
=
S
?
s=1
N
s
?
j=1
?T
? ln p
sj
? ln p
sj
??
=
(
1
?
S
s=1
r
s
)
S
?
s=1
N
s
?
j=1
p
sj

sj
? ln p
sj
??
(4)
? ln p
sj
??
=
N
s
?
m=1
? ln p
sj
?L
sm
?L
sm
??
= ?
(
?L
sj
??
?
N
s
?
m=1
p
sm
?L
sm
??
)
(5)
?L
sm
??
=
?
(s
k
,t
k
)?h
sm
?L
sm
? lnP (s
k
|t
k
)
? lnP (s
k
|t
k
)
??
=
?
(s
k
,t
k
)?h
sm
?
? lnP (s
k
|t
k
)
??
(6)
? lnP (s
k
|t
k
)
??
=
?
i
? lnP (s
k
|t
k
)
?w
i
?w
i
??
(7)
? lnP (s
k
|t
k
)
?w
i
=
?
j?A(s
k
)?B(t
k
)
? (j ? i) c
j
(s
k
, t
k
)
?
j?A(s
k
)?B(t
k
)
w
j
c
j
(s
k
, t
k
)
?
?
j?B(t
k
)
? (j ? i) c
j
(t
k
)
?
j?B(t
k
)
w
j
c
j
(t
k
)
(8)
?(x) =
{
1 x = 0
0 x 6= 0
(9)
of LBFGS training, merging new hypotheses with
translations from previous iterations. The overall
training procedure is described in more detail be-
low:
1. Initialize parameter vector ? to small random
values, so that all training sentences receive
approximately equal weights.
2. Initialize phrase-based MT decoding weights
to previously tuned values.
3. Perform weighted phrase rule extraction as
described in Equation 1, to estimate the
phrase and lexical translation probabilities.
4. Decode the tuning set, generating N-best.
5. Merge N-best hypotheses from previous iter-
ations to current N-best.
6. Tune decoding weights so as to minimize
TER on merged N-best, using a derivative
free optimization method. In our case, we
used Powell?s algorithm (Powell, 1964) mod-
ified by Brent as described in (Brent, 1973) 3.
7. Identify set of unique source-target phrase
pairs in merged N-best.
8. Extract sufficient statistics from training data
for all phrases identified in step 7.
3This method was first used for N-best based parameter
optimization in (Ostendorf et al, 1991).
9. Run the LBFGS algorithm to minimize the
expected TER in the merged N-best, using
the derivative equations described previously.
10. Assign a weight to each training sentence
based on the ? values optimized in 9.
11. Go to step 3.
Typically, the corpus weights converge in about
4-5 main iterations. The calculation of the deriva-
tive is parallelized to speed up computation, re-
quiring about 10 minutes per iteration of LBFGS.
4 Experimental Setup
In this section we describe the setup that was used
for all experiments reported in this paper. Specif-
ically, we provide details about the training data,
development sets, and MT systems (phrase-based
and hierarchical).
4.1 Training Data
All MT training experiments made use of an
Arabic-English corpus of approximately 200 mil-
lion tokens (English side). Most of the collections
in this corpus are available through the Linguis-
tic Data Consortium (LDC) and are regularly part
of the resources specified for the constrained data
track of the NIST MT evaluation4.
4For a list of the NIST MT09 constrained train-
ing condition resources, see http://www.itl.
nist.gov/iad/mig/tests/mt/2009/MT09_
ConstrainedResources.pdf
712
The corpus includes data from multiple gen-
res, as shown in Table 1. The ?Sakhr? newswire
collection is a set of Arabic-to-English and
English-to-Arabic data provided by Sakhr Soft-
ware, totaling about 30.8 million tokens, and
is only available to research teams participat-
ing in the Defense Advanced Research Projects
Agency (DARPA) Global Autonomous Language
Exploitation (GALE) program. The ?LDC Giga-
word (ISI)? collection was produced by automati-
cally detecting and extracting portions of parallel
text from the monolingual LDC Arabic and En-
glish Gigaword collections, using a method devel-
oped at the Information Sciences Institute (ISI) of
the University of Southern California.
Data Origin Style Size(K tokens)
LDC pre-GALE
U. Nations 118049
Newswire 2700
Treebank 685
LDC post-GALE
Newswire 14344
Treebank 292
Web 478
Broad. News 573
Broad. Conv. 1003
Web-found text Lexicons 436Quran 406
Sakhr Newswire 30790
LDC Gigaword Newswire 29169(ISI)
Table 1: Composition of the Arabic-English par-
allel corpus used for MT training.
It is easy to see that most of the parallel train-
ing data are either newswire or from United Na-
tions. The amount of web text or broadcast
news/conversations is only a very small fraction
of the total corpus. In total, there are 31 collec-
tions in the training bitext. Some collections (es-
pecially those released recently by LDC for the
GALE project) consist of data from multiple gen-
res. The total number of unique genres (or data
types) in the training set is 10.
Besides the above bitext, we also used approxi-
mately 8 billion words of English text for language
model (LM) training (3.7B words from the LDC
Gigaword corpus, 3.3B words of web-downloaded
text, and 1.1B words of data from CNN archives).
This data was used to train two language mod-
els: an entropy-pruned trigram LM, used in decod-
ing, and an unpruned 5-gram LM used in N-best
rescoring. Kneser-Ney smoothing was applied to
the n-grams in both cases.
4.2 Development Sets
The development sets used for tuning and testing
the corpus weights and other MT settings were
comprised of documents from previous Arabic-
English NIST MT evaluation sets and from GALE
development/evaluation sets.
Specifically, the newswire Tune and Test sets
consist of documents from the following col-
lections: the newswire portion of NIST MT04,
MT05, MT06, and MT08 evaluation sets, the
GALE Phase 1 (P1) and Phase 2 (P2) evaluation
sets, and the GALE P2 and P3 development sets.
The web Tune and Test sets are made of docu-
ments from NIST MT06 and MT08, the GALE P1
and P2 evaluation sets, the GALE P2 and P3 devel-
opment sets, and a held-out portion of the GALE
year 1 quarter 4 web training data release.
The audio Tune and Test sets consist of roughly
equal parts of news and conversations broadcast
from November 2005 through May 2007 by ma-
jor Arabic-speaking television and radio stations
(e.g., Al-Jazeera, Al-Arabiya, Syrian TV), totaling
approximately 14 hours of speech. The audio was
processed through automated speech recognition
(ASR) in order to produce (errorful) transcripts
that were used as input to all MT decoding experi-
ments reported in this paper. However, the corpus
weight estimation was carried out based on N-best
MT of the Arabic audio reference transcriptions
(i.e., the transcripts had no speech recognition er-
rors, and contained full punctuation).
It is important to note that some of the docu-
ments in the above devsets have multiple reference
translations (usually 4), while others have only
one. Most of the documents in the newswire sets
have 4 references, but unfortunately the web and
audio sets have, on average, less than 2 reference
translations per segment. More details are listed in
Table 2.
Another important note is that, although the au-
dio sets consist of both broadcast news (BN) and
broadcast conversations (BC), we did not perform
BN or BC-specific tuning. Corpus weights and
MT decoding parameters were optimized based on
a single Tune set, on a mix of BN and BC data.
However, when we report speech translation re-
sults in later sections, we break down the perfor-
713
Genre Tune Test#segs #tokens #refs/seg #segs #tokens #refs/seg
Newswire 1994 72359 3.94 3149 115700 3.67
Web 3278 99280 1.69 4425 125795 2.08
Audio BN 897 32990 1.00 1530 53067 1.00
Audio BC 765 24607 1.00 1416 44435 1.00
Table 2: Characteristics of the tuning (Tune) and validation (Test) sets used for development on Arabic
newswire, web, and audio. The audio sets include material from both broadcast news and broadcast
conversations.
mance by genre.
4.3 MT Systems
Experiments were performed using two types of
statistical MT systems: a phrase-based system,
similar to Pharaoh (Koehn, 2004), and a state-
of-the-art, hierarchical string-to-dependency-tree
system, similar to (Shen et al, 2008).
The phrase-based MT system employs a pruned
3-gram LM in decoding, and can optionally gen-
erate N-best unique translation hypotheses which
are used to estimate the corpus weights, as de-
scribed in Section 3.
The hierarchical MT system performs decoding
with the same 3-gram LM, generates N-best of
unique translation hypotheses, and then rescores
them using a large, unpruned 5-gram LM in order
to select the best scoring translation. It is worth
mentioning that this hierarchical MT system pro-
vides a very strong baseline; it achieves a case-
sensitive BLEU score of 52.20 on the newswire
portion of the NIST MT08 evaluation set, which
is similar to the score of the second-best system
that participated in the unconstrained data track of
the NIST MT08 evaluation.
Both types of models were trained on the same
word alignments generated by GIZA++ (Och and
Ney, 2003).
5 Results
In this section we report results on the Arabic
newswire, web, and audio development sets, us-
ing both phrase-based and hierarchical MT sys-
tems, in terms of TER, BLEU5, and METEOR
(Lavie and Agarwal, 2007). Whenever corpus
weights are used, they were estimated on the des-
ignated Tune set using the phrase-based MT sys-
5The brevity penalty was calculated using the formula in
the original IBM paper, rather than the more recent definition
implemented in the NIST mteval-v11b.pl script.
tem. Only the collection and genre ids were used
as sentence features in order to estimate the corpus
weights. As mentioned in Section 4.1, the train-
ing bitext consists of 31 collections and 10 gen-
res, so each training sentence was assigned a 41-
dimensional binary vector indicating its particu-
lar collection/genre combination. That vector was
then mapped into a single weight using a percep-
tron.
5.1 Phrase-based MT
Results using the phrase-based MT system are
shown in Table 3. In all cases, the decoding
weights were optimized so as to minimize TER
on the designated Tune set. On newswire, the
discriminative corpus weights provide 0.8% abso-
lute gain in TER, in both Tune and Test sets. On
web, the TER gain is 0.9% absolute on Tune and
0.5% on Test. On the audio Test set, the TER gain
is 0.5% on BN and 1.4% on BC. Significant im-
provements were also obtained in the BLEU and
METEOR scores, on all sets and conditions.
5.2 Hierarchical MT
Results using the hierarchical MT system are
shown in Table 4. The hierarchical system
used different tuning criteria in each genre. On
newswire, the decoding weights were optimized
so as to maximize BLEU, while on web and audio
the tuning was based on 0.5TER+0.5(1?BLEU)
(referred to as TERBLEU in what follows). Note
that these were the criteria for tuning the decoding
weights; whenever corpus weights were used, they
were taken from the phrase-based system.
It is interesting to see that gains from discrimi-
native corpus weights carry over to the more pow-
erful hierarchical MT system. On newswire Test,
the gain in BLEU is 0.8; on web Test, the gain in
TERBLEU is 0.3. On the audio Test set, the cor-
pus weights provide 0.7 and 0.75 TERBLEU re-
duction on BN and BC, respectively. As with the
714
Set Corpus Weights Newswire WebTER BLEU MTR TER BLEU MTR
Tune No 42.3 48.2 67.5 60.0 21.9 51.3Yes 41.5 49.6 68.7 59.1 22.8 52.3
Test No 43.2 46.2 66.5 58.6 24.2 52.2Yes 42.4 47.5 67.8 58.1 25.4 52.9
(a) Results on Arabic text.
Set Corpus Weights BN BCTER BLEU MTR TER BLEU MTR
Tune No 56.0 22.9 55.5 57.3 21.7 55.0Yes 55.0 25.0 57.1 56.1 23.6 56.4
Test No 53.0 25.3 57.7 55.9 22.9 55.4Yes 52.5 26.6 58.8 54.5 24.7 56.8
(b) Results on Arabic audio.
Table 3: Phrase-based trigram decoding results on the Arabic text and audio development sets. Decoding
weights were optimized on the Tune set in order to directly minimize TER. Corpus weights were also
optimized on Tune set, but based on expected TER.
phrase-based system, all metrics improve from the
use of corpus weights, in all sets/conditions.
6 Conclusions
We have described a novel approach for estimat-
ing a weight for each sentence in a parallel train-
ing corpus so as to optimize MT performance of a
phrase-based statistical MT system. The sentence
weights influence MT performance by being ap-
plied to the phrase and lexical counts during trans-
lation rule extraction and probability estimation.
In order to ensure robust training of the weights,
we expressed them as a function of sentence-level
features. Then, we defined the process for opti-
mizing the parameters of that function based on
the expected TER of a translation hypothesis N-
best on a designated tuning set.
The proposed technique was evaluated in the
context of Arabic-English translation, on multiple
conditions. It was shown that encouraging results
were obtained by just using collection and genre
ids as features. Interestingly, the discriminative
corpus weights were found to be generally appli-
cable and provided gains in a state-of-the-art hi-
erarchical string-to-dependency-tree MT system,
even though they were trained using the phrase-
based MT system.
Next step is to include other sentence-level fea-
tures, as described in Section 3.1. Finally, the
technique described in this paper can be extended
to address the estimation of weights at the align-
ment link level, based on link-level features. We
believe that this will have a larger impact on the
lexical and phrase translation probabilities, since
there is a large number of parallel training sen-
tences that are partially correct, i.e., they contain
parts that are aligned and translated correctly, and
parts that are wrong. The current procedure tries
to assign a single weight to such sentences, so
there is no way to distinguish between the ?good?
and ?bad? portions of each sentence. Pushing the
weight estimation at the alignment link level will
alleviate this problem and will make the discrimi-
native training more targeted.
Acknowledgments
This work was supported by DARPA/IPTO Con-
tract No. HR0011-06-C-0022 under the GALE
program.
References
Richard P. Brent. 1973. Algorithms for Minimization
Without Derivatives. Prentice-Hall.
Almut Silja Hildebrand, Matthias Eck, Stephan Vogel,
and Alex Waibel. 2005. Adaptation of the transla-
715
Set Corpus Weights Newswire WebTER BLEU MTR TER BLEU MTR
Tune No 39.5 54.4 70.3 58.2 25.2 53.8Yes 38.8 55.6 71.2 58.0 25.5 54.0
Test No 40.7 52.1 69.3 57.0 28.3 54.7Yes 40.1 52.9 69.8 56.6 28.5 55.0
(a) Results on Arabic text.
Set Corpus Weights BN BCTER BLEU MTR TER BLEU MTR
Tune No 54.9 27.3 58.0 55.8 26.1 57.4Yes 53.6 28.2 59.0 54.9 26.9 58.0
Test No 51.6 29.9 60.0 54.4 27.6 57.7Yes 50.7 30.4 60.7 53.2 27.9 58.7
(b) Results on Arabic audio.
Table 4: Hierarchical 5-gram rescoring results on the Arabic text and audio development sets. Decod-
ing/rescoring weights were optimized on the Tune set in order to directly maximize BLEU (for newswire)
or minimize TERBLEU (for web and audio). Corpus weights were the same as the ones used in the cor-
responding phrase-based decodings.
tion model for statistical machine translation based
on information retrieval. In Proceedings of the 10th
Annual Conference of European Association for Ma-
chine Translation, pages 133?142.
Philipp Koehn and Josh Schroeder. 2007. Experiments
in domain adaptation for statistical machine trans-
lation. In Proceedings of the Second Workshop on
Statistical Machine Translation, pages 224?227.
Philipp Koehn. 2004. Pharaoh: a beam search de-
coder for phrase-based statistical machine transla-
tion models. In Proceedings of the 6th Conference
of the Association for Machine Translation in the
Americas.
Alon Lavie and Abhaya Agarwal. 2007. METEOR:
An automatic metric for MT evaluation with high
levels of correlation with human judgments. In Pro-
ceedings of the Second Workshop on Statistical Ma-
chine Translation, pages 228?231.
Dong C. Liu and Jorge Nocedal. 1989. On the limited
memory BFGS method for large scale optimization.
Mathematical Programming, 45:503?528.
Yajuan Lu, Jin Huang, and Qun Liu. 2007. Improving
statistical machine translation performance by train-
ing data selection and optimization. In Proceedings
of the 2007 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning, pages 343?350.
Arindam Mandal, Dimitra Vergyri, Wen Wang, Jing
Zheng, Andreas Stolcke, Gokhan Tur, Dilek
Hakkani-Tu?r, and Necip Fazil Ayan. 2008. Effi-
cient data selection for machine translation. In Pro-
ceedings of the Second IEEE/ACL Spoken Language
Technology Workshop, pages 261?264.
Franz J. Och and Hermann Ney. 2003. A systematic
comparison of various statistical alignment models.
Computational Linguistics, 29(1):19?51.
F. J. Och. 2003. Minimum error rate training in sta-
tistical machine translation. In Proceedings of the
41st Annual Meeting of the Association for Compu-
tational Linguistics, pages 160?167.
M. Ostendorf, A. Kannan, S. Austin, O. Kimball,
R. Schwartz, and J. R. Rohlicek. 1991. Integra-
tion of diverse recognition methodologies through
reevaluation of nbest sentence hypotheses. In Pro-
ceedings of the DARPA Workshop on Speech and
Natural Language, pages 83?87.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proceedings
of the 40th Annual Meeting of the Association for
Computational Linguistics, pages 311?318.
M. J. D. Powell. 1964. An efficient method for finding
the minimum of a function of several variables with-
out calculating derivatives. The Computer Journal,
pages 155?162.
Philip Resnik and Noah A. Smith. 2003. The web
as a parallel corpus. Computational Linguistics,
29(3):349?380.
716
Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008.
A new string-to-dependency machine translation al-
gorithm with a target dependency language model.
In Proceedings of the 46th Annual Meeting of the
Association for Computational Linguistics: Human
Language Technologies, pages 577?585.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciula, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings of the 7th Conference of the Associa-
tion for Machine Translation in the Americas, pages
223?231.
Matthew Snover, Bonnie Dorr, and Richard Schwartz.
2008. Language and translation model adaptation
using comparable corpora. In Proceedings of the
2008 Conference on Empirical Methods in Natural
Language Processing, pages 857?866.
Keiji Yasuda, Ruiqiang Zhang, Hirofumi Yamamoto,
and Eiichiro Sumita. 2008. Method of selecting
training data to build a compact and efficient trans-
lation model. In Proceedings of the Third Interna-
tional Joint Conference on Natural Language Pro-
cessing, volume II, pages 655?660.
717
Proceedings of NAACL HLT 2007, pages 228?235,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
Combining Outputs from Multiple Machine Translation Systems
Antti-Veikko I. Rosti   and Necip Fazil Ayan

and Bing Xiang   and
Spyros Matsoukas   and Richard Schwartz   and Bonnie J. Dorr

  BBN Technologies, 10 Moulton Street, Cambridge, MA 02138

arosti,bxiang,smatsouk,schwartz  @bbn.com

Institute for Advanced Computer Studies, University of Maryland, College Park, MD 20742

nfa,bonnie  @umiacs.umd.edu
Abstract
Currently there are several approaches to
machine translation (MT) based on differ-
ent paradigms; e.g., phrasal, hierarchical
and syntax-based. These three approaches
yield similar translation accuracy despite
using fairly different levels of linguistic
knowledge. The availability of such a
variety of systems has led to a growing
interest toward finding better translations
by combining outputs from multiple sys-
tems. This paper describes three differ-
ent approaches to MT system combina-
tion. These combination methods oper-
ate on sentence, phrase and word level
exploiting information from  -best lists,
system scores and target-to-source phrase
alignments. The word-level combination
provides the most robust gains but the
best results on the development test sets
(NIST MT05 and the newsgroup portion
of GALE 2006 dry-run) were achieved by
combining all three methods.
1 Introduction
In recent years, machine translation systems based
on new paradigms have emerged. These systems
employ more than just the surface-level information
used by the state-of-the-art phrase-based translation
systems. For example, hierarchical (Chiang, 2005)
and syntax-based (Galley et al, 2006) systems have
recently improved in both accuracy and scalability.
Combined with the latest advances in phrase-based
translation systems, it has become more attractive
to take advantage of the various outputs in forming
consensus translations (Frederking and Nirenburg,
1994; Bangalore et al, 2001; Jayaraman and Lavie,
2005; Matusov et al, 2006).
System combination has been successfully ap-
plied in state-of-the-art speech recognition evalua-
tion systems for several years (Fiscus, 1997). Even
though the underlying modeling techniques are sim-
ilar, many systems produce very different outputs
with approximately the same accuracy. One of the
most successful approaches is consensus network
decoding (Mangu et al, 2000) which assumes that
the confidence of a word in a certain position is
based on the sum of confidences from each system
output having the word in that position. This re-
quires aligning the system outputs to form a con-
sensus network and ? during decoding ? simply
finding the highest scoring path through this net-
work. The alignment of speech recognition outputs
is fairly straightforward due to the strict constraint in
word order. However, machine translation outputs
do not have this constraint as the word order may be
different between the source and target languages.
MT systems employ various re-ordering (distortion)
models to take this into account.
Three MT system combination methods are pre-
sented in this paper. They operate on the sentence,
phrase and word level. The sentence-level combi-
nation is based on selecting the best hypothesis out
of the merged N-best lists. This method does not
generate new hypotheses ? unlike the phrase and
word-level methods. The phrase-level combination
228
is based on extracting sentence-specific phrase trans-
lation tables from system outputs with alignments
to source and running a phrasal decoder with this
new translation table. This approach is similar to
the multi-engine MT framework proposed in (Fred-
erking and Nirenburg, 1994) which is not capable of
re-ordering. The word-level combination is based
on consensus network decoding. Translation edit
rate (TER) (Snover et al, 2006) is used to align
the hypotheses and minimum Bayes risk decoding
under TER (Sim et al, 2007) is used to select the
alignment hypothesis. All combination methods use
weights which may be tuned using Powell?s method
(Brent, 1973) on  -best lists. Both sentence and
phrase-level combination methods can generate  -
best lists which may also be used as new system out-
puts in the word-level combination.
Experiments on combining six machine transla-
tion system outputs were performed. Three sys-
tems were phrasal, two hierarchical and one syntax-
based. The systems were evaluated on NIST MT05
and the newsgroup portion of the GALE 2006 dry-
run sets. The outputs were evaluated on both TER
and BLEU. As the target evaluation metric in the
GALE program was human-mediated TER (HTER)
(Snover et al, 2006), it was found important to im-
prove both of these automatic metrics.
This paper is organized as follows. Section 2
describes the evaluation metrics and a generic dis-
criminative optimization technique used in tuning of
the various system combination weights. Sentence,
phrase and word-level system combination methods
are presented in Sections 3, 4 and 5. Experimental
results on Arabic and Chinese to English newswire
and newsgroup test data are presented in Section 6.
2 Evaluation Metrics and Discriminative
Tuning
The official metric of the 2006 DARPA GALE
evaluation was human-mediated translation edit rate
(HTER). HTER is computed as the minimum trans-
lation edit rate (TER) between a system output and
a targeted reference which preserves the meaning
and fluency of the sentence (Snover et al, 2006).
The targeted reference is generated by human post-
editors who make edits to a reference translation so
as to minimize the TER between the reference and
the MT output without changing the meaning of the
reference. Computing the HTER is very time con-
suming due to the human post-editing. It is desir-
able to have an automatic evaluation metric that cor-
relates well with the HTER to allow fast evaluation
of the MT systems during development. Correla-
tions of different evaluation metrics have been stud-
ied (Snover et al, 2006) but according to various
internal HTER experiments it is not clear whether
TER or BLEU correlates better. Therefore it is prob-
ably safest to try and not degrade either.
The TER of a translation   is computed as

 	
 
ffProceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 312?319,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Improved Word-Level System Combination for Machine Translation
Antti-Veikko I. Rosti and Spyros Matsoukas and Richard Schwartz
BBN Technologies, 10 Moulton Street
Cambridge, MA 02138
 
arosti,smatsouk,schwartz  @bbn.com
Abstract
Recently, confusion network decoding has
been applied in machine translation system
combination. Due to errors in the hypoth-
esis alignment, decoding may result in un-
grammatical combination outputs. This pa-
per describes an improved confusion net-
work based method to combine outputs from
multiple MT systems. In this approach, ar-
bitrary features may be added log-linearly
into the objective function, thus allowing
language model expansion and re-scoring.
Also, a novel method to automatically se-
lect the hypothesis which other hypotheses
are aligned against is proposed. A generic
weight tuning algorithm may be used to op-
timize various automatic evaluation metrics
including TER, BLEU and METEOR. The
experiments using the 2005 Arabic to En-
glish and Chinese to English NIST MT eval-
uation tasks show significant improvements
in BLEU scores compared to earlier confu-
sion network decoding based methods.
1 Introduction
System combination has been shown to improve
classification performance in various tasks. There
are several approaches for combining classifiers. In
ensemble learning, a collection of simple classifiers
is used to yield better performance than any single
classifier; for example boosting (Schapire, 1990).
Another approach is to combine outputs from a few
highly specialized classifiers. The classifiers may
be based on the same basic modeling techniques
but differ by, for example, alternative feature repre-
sentations. Combination of speech recognition out-
puts is an example of this approach (Fiscus, 1997).
In speech recognition, confusion network decoding
(Mangu et al, 2000) has become widely used in sys-
tem combination.
Unlike speech recognition, current statistical ma-
chine translation (MT) systems are based on various
different paradigms; for example phrasal, hierarchi-
cal and syntax-based systems. The idea of combin-
ing outputs from different MT systems to produce
consensus translations in the hope of generating bet-
ter translations has been around for a while (Fred-
erking and Nirenburg, 1994). Recently, confusion
network decoding for MT system combination has
been proposed (Bangalore et al, 2001). To generate
confusion networks, hypotheses have to be aligned
against each other. In (Bangalore et al, 2001), Lev-
enshtein alignment was used to generate the net-
work. As opposed to speech recognition, the word
order between two correct MT outputs may be dif-
ferent and the Levenshtein alignment may not be
able to align shifted words in the hypotheses. In
(Matusov et al, 2006), different word orderings are
taken into account by training alignment models by
considering all hypothesis pairs as a parallel corpus
using GIZA++ (Och and Ney, 2003). The size of
the test set may influence the quality of these align-
ments. Thus, system outputs from development sets
may have to be added to improve the GIZA++ align-
ments. A modified Levenshtein alignment allowing
shifts as in computation of the translation edit rate
(TER) (Snover et al, 2006) was used to align hy-
312
potheses in (Sim et al, 2007). The alignments from
TER are consistent as they do not depend on the test
set size. Also, a more heuristic alignment method
has been proposed in a different system combina-
tion approach (Jayaraman and Lavie, 2005). A full
comparison of different alignment methods would
be difficult as many approaches require a significant
amount of engineering.
Confusion networks are generated by choosing
one hypothesis as the ?skeleton?, and other hypothe-
ses are aligned against it. The skeleton defines the
word order of the combination output. Minimum
Bayes risk (MBR) was used to choose the skeleton
in (Sim et al, 2007). The average TER score was
computed between each system?s   -best hypothesis
and all other hypotheses. The MBR hypothesis is
the one with the minimum average TER and thus,
may be viewed as the closest to all other hypothe-
ses in terms of TER. This work was extended in
(Rosti et al, 2007) by introducing system weights
for word confidences. However, the system weights
did not influence the skeleton selection, so a hypoth-
esis from a system with zero weight might have been
chosen as the skeleton. In this work, confusion net-
works are generated by using the   -best output from
each system as the skeleton, and prior probabili-
ties for each network are estimated from the average
TER scores between the skeleton and other hypothe-
ses. All resulting confusion networks are connected
in parallel into a joint lattice where the prior proba-
bilities are also multiplied by the system weights.
The combination outputs from confusion network
decoding may be ungrammatical due to alignment
errors. Also the word-level decoding may break
coherent phrases produced by the individual sys-
tems. In this work, log-posterior probabilities are
estimated for each confusion network arc instead of
using votes or simple word confidences. This allows
a log-linear addition of arbitrary features such as
language model (LM) scores. The LM scores should
increase the total log-posterior of more grammatical
hypotheses. Powell?s method (Brent, 1973) is used
to tune the system and feature weights simultane-
ously so as to optimize various automatic evaluation
metrics on a development set. Tuning is fully auto-
matic, as opposed to (Matusov et al, 2006) where
global system weights were set manually.
This paper is organized as follows. Three evalu-
ation metrics used in weights tuning and reporting
the test set results are reviewed in Section 2. Sec-
tion 3 describes confusion network decoding for MT
system combination. The extensions to add features
log-linearly and improve the skeleton selection are
presented in Sections 4 and 5, respectively. Section
6 details the weights optimization algorithm and the
experimental results are reported in Section 7. Con-
clusions and future work are discussed in Section 8.
2 Evaluation Metrics
Currently, the most widely used automatic MT eval-
uation metric is the NIST BLEU-4 (Papineni et al,
2002). It is computed as the geometric mean of  -
gram precisions up to  -grams between the hypoth-
esis  and reference  as follows

	
 (1)
ffProceedings of the Third Workshop on Statistical Machine Translation, pages 183?186,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Incremental Hypothesis Alignment for Building Confusion Networks with
Application to Machine Translation System Combination
Antti-Veikko I. Rosti and Bing Zhang and Spyros Matsoukas and Richard Schwartz
BBN Technologies, 10 Moulton Street, Cambridge, MA 02138
 
arosti,bzhang,smatsouk,schwartz  @bbn.com
Abstract
Confusion network decoding has been the
most successful approach in combining out-
puts from multiple machine translation (MT)
systems in the recent DARPA GALE and
NIST Open MT evaluations. Due to the vary-
ing word order between outputs from differ-
ent MT systems, the hypothesis alignment
presents the biggest challenge in confusion
network decoding. This paper describes an
incremental alignment method to build confu-
sion networks based on the translation edit rate
(TER) algorithm. This new algorithm yields
significant BLEU score improvements over
other recent alignment methods on the GALE
test sets and was used in BBN?s submission to
the WMT08 shared translation task.
1 Introduction
Confusion network decoding has been applied in
combining outputs from multiple machine transla-
tion systems. The earliest approach in (Bangalore
et al, 2001) used edit distance based multiple string
alignment (MSA) (Durbin et al, 1988) to build the
confusion networks. The recent approaches used
pair-wise alignment algorithms based on symmetric
alignments from a HMM alignment model (Matusov
et al, 2006) or edit distance alignments allowing
shifts (Rosti et al, 2007). The alignment method
described in this paper extends the latter by incre-
mentally aligning the hypotheses as in MSA but also
allowing shifts as in the TER alignment.
The confusion networks are built around a ?skele-
ton? hypothesis. The skeleton hypothesis defines
the word order of the decoding output. Usually, the
1-best hypotheses from each system are considered
as possible skeletons. Using the pair-wise hypoth-
esis alignment, the confusion networks are built in
two steps. First, all hypotheses are aligned against
the skeleton independently. Second, the confusion
networks are created from the union of these align-
ments. The incremental hypothesis alignment algo-
rithm combines these two steps. All words from the
previously aligned hypotheses are available, even if
not present in the skeleton hypothesis, when align-
ing the following hypotheses. As in (Rosti et al,
2007), confusion networks built around all skeletons
are joined into a lattice which is expanded and re-
scored with language models. System weights and
language model weights are tuned to optimize the
quality of the decoding output on a development set.
This paper is organized as follows. The incre-
mental TER alignment algorithm is described in
Section 2. Experimental evaluation comparing the
incremental and pair-wise alignment methods are
presented in Section 3 along with results on the
WMT08 Europarl test sets. Conclusions and future
work are presented in Section 4.
2 Incremental TER Alignment
The incremental hypothesis alignment is based on
an extension of the TER algorithm (Snover et al,
2006). The extension allows using a confusion net-
work as the reference. First, the algorithm finds the
minimum edit distance between the hypothesis and
the reference network by considering all word arcs
between two consecutive nodes in the reference net-
work as possible matches for a hypothesis word at
183
1 2 3 4 5 6I (3)
NULL (2)
like (3)
NULL (2)
big blue (1)
balloons (2)
blue (1) kites (1)
Figure 1: Network after pair-wise TER alignment.
that position. Second, shifts of blocks of words that
have an exact match somewhere else in the network
are tried in order to find a new hypothesis word or-
der with a lower TER. Each shifted block is con-
sidered a single edit. These two steps are executed
iteratively as a greedy search. The final alignment
between the re-ordered hypothesis and the reference
network may include matches, substitutions, dele-
tions, and insertions.
The confusion networks are built by creating a
simple confusion network from the skeleton hypoth-
esis. If the skeleton hypothesis has   words, the
initial network has   arcs and   nodes. Each
arc has a set of system specific confidence scores.
The score for the skeleton system is set to  and
the confidences for other systems are set to zeros.
For each non-skeleton hypothesis, a TER alignment
against the current network is executed as described
above. Each match found will increase the system
specific word arc confidence by 
	 where 
is the rank of the hypothesis in that system?s   -best
list. Each substitution will generate a new word arc
at the corresponding position in the network. The
word arc confidence for the system is set to 
	
and the confidences for other systems are set to ze-
ros. Each deletion will generate a new NULL word
arc unless one exists at the corresponding position
in the network. The NULL word arc confidences are
adjusted as in the case of a match or a substitution
depending on whether the NULL word arc exists or
not. Finally, each insertion will generate a new node
and two word arcs at the corresponding position in
the network. The first word arc will have the in-
serted word with the confidence set as in the case
of a substitution and the second word arc will have
a NULL word with confidences set by assuming all
previously aligned hypotheses and the skeleton gen-
erated the NULL word arc.
After all hypotheses have been added into the con-
fusion network, the system specific word arc confi-
dences are scaled to sum to one over all arcs between
1 2 3 4 5 6I (3) like (3)
kites (1)
NULL (2) NULL (1)
big (1) blue (2)
balloons (2)
Figure 2: Network after incremental TER alignment.
each set of two consecutive nodes. Other scores for
the word arc are set as in (Rosti et al, 2007).
2.1 Benefits over Pair-Wise TER Alignment
The incremental hypothesis alignment guarantees
that insertions between a hypothesis and the cur-
rent confusion network are always considered when
aligning the following hypotheses. This is not the
case in any pair-wise hypothesis alignment algo-
rithm. During the pair-wise hypothesis alignment,
an identical word in two hypotheses may be aligned
as an insertion or a substitution in a different posi-
tion with respect to the skeleton. This will result in
undesirable repetition and lower confidence for that
word in the final confusion network. Also, multiple
insertions are not handled implicitly.
For example, three hypotheses ?I like balloons?,
?I like big blue balloons?, and ?I like blue kites?
might be aligned by the pair-wise alignment, assum-
ing the first as the skeleton, as follows:
I like NULL balloons NULL
I like big blue balloons NULL
I like NULL balloons NULL
I like NULL blue kites
which results in the confusion network shown in
Figure 1. The number of hypotheses proposing each
word is shown in parentheses. The alignment be-
tween the skeleton and the second hypothesis has
two consecutive insertions ?big blue? which are not
available for matching when the third hypothesis is
aligned against the skeleton. Therefore, the word
?blue? appears twice in the confusion network. If
many hypotheses have multiple insertions at the
same location with respect to the skeleton, they have
to be treated as phrases or a secondary alignment
process has to be applied.
Assuming the same hypotheses as above, the in-
cremental hypothesis alignment may yield the fol-
lowing alignment:
184
System TER BLEU MTR
worst 53.26 33.00 63.15
best 42.30 48.52 67.71
syscomb pw 39.85 52.00 68.73
syscomb giza 40.01 52.24 68.68
syscomb inc 39.25 52.73 68.97
oracle 21.68 64.14 78.18
Table 1: Results on the Arabic GALE Phase 2 system
combination tuning set with four reference translations.
I like NULL NULL balloons
I like big blue balloons
I like NULL blue kites
which results in the confusion network shown in
Figure 2. In this case the word ?blue? is available
for matching when the third hypothesis is aligned.
It should be noted that the final confusion network
depends on the order in which the hypotheses are
added. The experiments so far have indicated that
different alignment order does not have a significant
influence on the final combination results as mea-
sured by the automatic evaluation metrics. Usually,
aligning the system outputs in the decreasing order
of their TER scores on the development set yields
the best scores.
2.2 Confusion Network Oracle
The extended TER algorithm can also be used to
estimate an oracle TER in a confusion network by
aligning the reference translations against the con-
fusion network. The oracle hypotheses can be ex-
tracted by finding a path with the maximum number
of matches. These hypotheses give a lower bound
on the TER score for the hypotheses which can be
generated from the confusion networks.
3 Experimental Evaluation
The quality of the final combination output depends
on many factors. Combining very similar outputs
does not yield as good gains as combining out-
puts from diverse systems. It is also important that
the development set used to tune the combination
weights is as similar to the evaluation set as possi-
ble. This development set should be different from
the one used to tune the individual systems to avoid
bias toward any system that may be over-tuned. Due
System TER BLEU MTR
worst 59.09 20.74 57.24
best 48.18 31.46 62.61
syscomb pw 46.31 33.02 63.18
syscomb giza 46.03 33.39 63.21
syscomb inc 45.45 33.90 63.45
oracle 27.53 49.10 71.81
Table 2: Results on the Arabic GALE Phase 2 evaluation
set with one reference translation.
to the tight schedule for the WMT08, there was no
time to experiment with many configurations. As
more extensive experiments have been conducted in
the context of the DARPA GALE program, results
on the Arabic GALE Phase 2 evaluation setup are
first presented. The translation quality is measured
by three MT evaluation metrics: TER (Snover et al,
2006), BLEU (Papineni et al, 2002), and METEOR
(Lavie and Agarwal, 2007).
3.1 Results on Arabic GALE Outputs
For the Arabic GALE Phase 2 evaluation, nine sys-
tems were combined. Five systems were phrase-
based, two hierarchical, one syntax-based, and one
rule-based. All statistical systems were trained on
common parallel data, tuned on a common genre
specific development set, and a common English to-
kenization was used. The English bi-gram and 5-
gram language models used in the system combina-
tion were trained on about 7 billion words of English
text. Three iterations of bi-gram decoding weight
tuning were performed followed by one iteration of
5-gram re-scoring weight tuning. All weights were
tuned to minimize the sum of TER and 1-BLEU.
The final 1-best outputs were true-cased and deto-
kenized before scoring.
The results on the newswire system combination
development set and the GALE Phase 2 evaluation
set are shown in Tables 1 and 2. The first two
rows show the worst and best scores from the in-
dividual systems. The scores may be from different
systems as the best performing system in terms of
TER was not necessarily the best performing system
in terms of the other metrics. The following three
rows show the scores of three combination outputs
where the only difference was the hypothesis align-
ment method. The first, syscomb pw, corresponds
185
BLEU
System de-en fr-en
worst 11.84 16.31
best 28.30 33.13
syscomb 29.05 33.63
Table 3: NIST BLEU scores on the German-English (de-
en) and French-English (fr-en) Europarl test2008 set.
to the pair-wise TER alignment described in (Rosti
et al, 2007). The second, syscomb giza, cor-
responds to the pair-wise symmetric HMM align-
ments from GIZA++ described in (Matusov et al,
2006). The third, syscomb inc, corresponds to
the incremental TER alignment presented in this pa-
per. Finally, oracle corresponds to an estimate of
the lower bound on the translation quality obtained
by extracting the TER oracle output from the con-
fusion networks generated by the incremental TER
alignment. It is unlikely that there exists a set of
weights that would yield the oracle output after de-
coding, though. The incremental TER alignment
yields significant improvements over all individual
systems and the combination outputs using the pair-
wise alignment methods.
3.2 Results on WMT08 Europarl Outputs
On the WMT08 shared translation task, transla-
tions for two language pairs and two tasks were
provided for the system combination experiments.
Twelve systems participated in the German-English
and fourteen in the French-English translation tasks.
The translations of the Europarl test (test2008) were
provided as the development set outputs and the
translations of the News test (newstest2008) were
provided as the evaluation set outputs. An English
bi-gram, 4-gram, and true-caser language models
were trained by using all English text available for
the WMT08 shared task, including Europarl mono-
lingual and news commentary parallel training sets.
The outputs were tokenized and lower-cased before
combination, and the final combination output was
true-cased and detokenized.
The results on the Europarl test set for both lan-
guage pairs are shown in table 3. The first two rows
have the NIST BLEU scores of the worst and the
best individual systems. The last row, syscomb,
corresponds to the system combination using the in-
cremental TER alignment. The improvements in the
NIST BLEU scores are fairly modest which is prob-
ably due to low diversity of the system outputs. It is
also unlikely that these weights are optimal for the
out-of-domain News test set outputs.
4 Conclusions
This paper describes a novel hypothesis alignment
algorithm for building confusion networks from
multiple machine translation system outputs. The al-
gorithm yields significant improvements on the Ara-
bic GALE evaluation set outputs and was used in
BBN?s submission to the WMT08 shared translation
task. The hypothesis alignment may benefit from
using stemming and synonymy in matching words.
Also, special handling of punctuation may improve
the alignment further. The future work will inves-
tigate the influence of better alignment to the final
combination outputs.
Acknowledgments
This work was supported by DARPA/IPTO Contract
No. HR0011-06-C-0022 under the GALE program.
References
S. Bangalore, G. Bordel, and G. Riccardi. 2001. Com-
puting consensus translation from multiple machine
translation systems. In Proc. ASRU, pages 351?354.
R. Durbin, S.R. Eddy, A. Krogh, and G. Mitchison. 1988.
Biological Sequence Analysis: Probabilistic Models of
Proteins and Nucleic Acids. Cambridge Univ. Press.
A. Lavie and A. Agarwal. 2007. METEOR: An auto-
matic metric for MT evaluation with high levels of cor-
relation with human judgments. In Proc. ACL/WMT,
pages 228?231.
E. Matusov, N. Ueffing, and H. Ney. 2006. Computing
consensus translation from multiple machine transla-
tion systems using enhanced hypotheses alignment. In
Proc. EACL, pages 33?40.
K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2002.
BLEU: a method for automatic evaluation of machine
translation. In Proc. ACL, pages 311?318.
A.-V.I. Rosti, S. Matsoukas, and R. Schwartz. 2007.
Improved word-level system combination for machine
translation. In Proc. ACL 2007, pages 312?319.
M. Snover, B. Dorr, R. Schwartz, L. Micciula, and
J. Makhoul. 2006. A study of translation edit rate with
targeted human annotation. In Proc. AMTA, pages
223?231.
186
Proceedings of the Fourth Workshop on Statistical Machine Translation , pages 61?65,
Athens, Greece, 30 March ? 31 March 2009. c?2009 Association for Computational Linguistics
Incremental Hypothesis Alignment with Flexible Matching for Building
Confusion Networks: BBN System Description for WMT09 System
Combination Task
Antti-Veikko I. Rosti and Bing Zhang and Spyros Matsoukas and Richard Schwartz
BBN Technologies, 10 Moulton Street, Cambridge, MA 02138
 
arosti,bzhang,smatsouk,schwartz  @bbn.com
Abstract
This paper describes the incremental hy-
pothesis alignment algorithm used in the
BBN submissions to the WMT09 system
combination task. The alignment algo-
rithm used a sentence specific alignment
order, flexible matching, and new shift
heuristics. These refinements yield more
compact confusion networks compared to
using the pair-wise or incremental TER
alignment algorithms. This should reduce
the number of spurious insertions in the
system combination output and the sys-
tem combination weight tuning converges
faster. System combination experiments
on the WMT09 test sets from five source
languages to English are presented. The
best BLEU scores were achieved by comb-
ing the English outputs of three systems
from all five source languages.
1 Introduction
Machine translation (MT) systems have different
strengths and weaknesses which can be exploited
by system combination methods resulting in an
output with a better performance than any indi-
vidual MT system output as measured by auto-
matic evaluation metrics. Confusion network de-
coding has become the most popular approach to
MT system combination. The first confusion net-
work decoding method (Bangalore et al, 2001)
was based on multiple string alignment (MSA)
(Durbin et al, 1988) borrowed from biological
sequence analysis. However, MSA does not al-
low re-ordering. The translation edit rate (TER)
(Snover et al, 2006) produces an alignment be-
tween two strings and allows shifts of blocks of
words. The availability of the TER software has
made it easy to build a high performance system
combination baseline (Rosti et al, 2007).
The pair-wise TER alignment originally de-
scribed by Sim et al (2007) has various limita-
tions. First, the hypotheses are aligned indepen-
dently against the skeleton which determines the
word order of the output. The same word from
two different hypotheses may be inserted in differ-
ent positions w.r.t. the skeleton and multiple inser-
tions require special handling. Rosti et al (2008)
described an incremental TER alignment to miti-
gate these problems. The incremental TER align-
ment used a global order in which the hypotheses
were aligned. Second, the TER software matches
words with identical surface strings. The pair-
wise alignment methods proposed by Ayan et al
(2008), He et al (2008), and Matusov et al (2006)
are able to match also synonyms and words with
identical stems. Third, the TER software uses a set
of heuristics which is not always optimal in de-
termining the block shifts. Karakos et al (2008)
proposed using inversion transduction grammars
to produce different pair-wise alignments.
This paper is organized as follows. A refined
incremental alignment algorithm is described in
Section 2. Experimental evaluation comparing
the pair-wise and incremental TER alignment al-
gorithms with the refined alignment algorithm on
WMT09 system combination task is presented in
Section 3. Conclusions and future work are pre-
sented in Section 4.
2 Incremental Hypothesis Alignment
with Flexible Matching
2.1 Sentence Specific Alignment Order
Rosti et al (2008) proposed incremental hypothe-
sis alignment using a system specific order. This
is not likely to be optimal since one MT system
may have better output on one sentence and worse
on another. More principled approach is similar to
MSA where the order is determined by the edit
distance of the hypothesis from the network for
61
17
0
1NULL(6.2e-7)
9
NULL(0.9999)
2cereal
NULL
3
thomas
4
jefferson
edison
5
says
6
eat
7
your
8
NULL
vegetables
NULL
10
eat
11
your 12cereal
NULL
13
thomas 14
edison
jefferson
15
says 16vegetables
NULL
NULL
(a) Alignment using the standard TER shift heuristics.
150
1NULL(0.5)
8
NULL(0.5)
2
thomas
3jefferson
edison
4
says
5
eat
6
your
7
vegetables
cereal NULL
9
eat
10
your
11
cereal
vegetables
12
thomas 13edison
jefferson
14
says
NULL
(b) Alignment using the modified shift heuristics.
Figure 1: Combined confusion networks using different shift heuristics. The initial NULL arcs include
the prior probability estimates in parentheses.
each sentence. The TER scores of the remaining
unaligned hypotheses using the current network as
the reference are computed. The hypothesis with
the lowest edit cost w.r.t. the network is aligned.
Given  systems, this increases the number of
alignments performed from  to 	
 .
2.2 Flexible Matching
The TER software assigns a zero cost for match-
ing tokens and a cost of one for all errors includ-
ing insertions, deletions, substitutions, and block
shifts. Ayan et al (2008) modified the TER soft-
ware to consider substitutions of synonyms with
a reduced cost. Recently, Snover et al (2009)
extended the TER algorithm in a similar fashion
to produce a new evaluation metric, TER plus
(TERp), which allows tuning of the edit costs in
order to maximize correlation with human judg-
ment. The incremental alignment with flexible
matching uses WordNet (Fellbaum, 1998) to find
all possible synonyms and words with identical
stems in a set of hypotheses. Substitutions involv-
ing synonyms and words with identical stems are
considered with a reduced cost of 0.2.
2.3 Modified Shift Heuristics
The TER is computed by trying shifts of blocks of
words that have an exact match somewhere else in
the reference in order to find a re-ordering of the
hypothesis with a lower edit distance to the refer-
ence. Karakos et al (2008) showed that the shift
heuristics in TER do not always yield an optimal
alignment. Their example used the following two
hypotheses:
1. thomas jefferson says eat your vegetables
2. eat your cereal thomas edison says
A system combination lattice using TER align-
ment is shown in Figure 1(a). The blocks
?eat your? are shifted when building both con-
fusion networks. Using the second hypothe-
sis as the skeleton seems to give a better align-
ment. The lower number of edits also results in a
higher skeleton prior shown between nodes 0 and
9. There are obviously some undesirable paths
through the lattice but it is likely that a language
model will give a higher score to the reasonable
hypotheses.
Since the flexible matching allows substitutions
with a reduced cost, the standard TER shift heuris-
tics have to be modified. A block of words may
have some words with identical matches and other
words with synonym matches. In TERp, synonym
and stem matches are considered as exact matches
for the block shifts, otherwise the TER shift con-
straints are used. In the flexible matching, the shift
heuristics were modified to allow any block shifts
62
that do not increase the edit cost. A system combi-
nation lattice using the modified shift heuristics is
shown in Figure 1(b). The optimal shifts of blocks
?eat your cereal? and ?eat your vegetables? were
found and both networks received equal skeleton
priors. TERp would yield this alignment only
if these blocks appear in the paraphrase table or
if ?cereal? and ?vegetables? are considered syn-
onyms. This example is artificial and does not
guarantee that optimal shifts are always found.
3 Experimental Evaluation
System combination experiments combining the
English WMT09 translation task outputs were per-
formed. A total of 96 English outputs were pro-
vided including primary, contrastive, and  -best
outputs. Only the primary  -best outputs were
combined due to time constraints. The numbers
of primary systems per source language were: 3
for Czech, 15 for German, 9 for Spanish, 15 for
French, and 3 for Hungarian. The English bigram
and 5-gram language models were interpolated
from four LM components trained on the English
monolingual Europarl (45M tokens) and News
(510M tokens) corpora, and the English sides of
the News Commentary (2M tokens) and Giga-
FrEn (683M tokens) parallel corpora. The interpo-
lation weights were tuned to minimize perplexity
on news-dev2009 set. The system combination
weights ? one for each system, LM weight, and
word and NULL insertion penalties ? were tuned
to maximize the BLEU (Papineni et al, 2002)
score on the tuning set (newssyscomb2009).
Since the system combination was performed on
tokenized and lower cased outputs, a trigram-
based true caser was trained on all News training
data. The tuning may be summarized as follows:
1. Tokenize and lower case the outputs;
2. Align hypotheses incrementally using each
output as a skeleton;
3. Join the confusion networks into a lattice
with skeleton specific prior estimates;
4. Extract a  -best list from the lattice given
the current weights;
5. Merge the  -best list with the hypotheses
from the previous iteration;
6. Tune new weights given the current merged
 -best list;
7. Iterate 4-6 three times;
8. Extract a  -best list from the lattice given
the best decoding weights and re-score hy-
potheses with a 5-gram;
9. Tune re-scoring weights given the final  -
best list;
10. Extract  -best hypotheses from the  -best
list given the best re-scoring weights, re-case,
and detokenize.
After tuning the system combination weights, the
outputs on a test set may be combined using the
same steps excluding 4-7 and 9. The hypothesis
scores and tuning are identical to the setup used in
(Rosti et al, 2007).
Case insensitive TER and BLEU scores for the
combination outputs using the pair-wise and in-
cremental TER alignment as well as the flexible
alignment on the tuning (dev) and test sets are
shown in Table 1. Only case insensitive scores
are reported since the re-casers used by different
systems are very different and some are trained
using larger resources than provided for WMT09.
The scores of the worst and best individual sys-
tem outputs are also shown. The best and worst
TER and BLEU scores are not necessarily from
the same system output. Both incremental
and flexible alignments used sentence spe-
cific alignment order. Combinations using the in-
cremental and flexible hypothesis alignment algo-
rithms consistently outperform the ones using the
pair-wise TER alignment. The flexible alignment
is slightly better than the incremental alignment on
Czech, Spanish, and Hungarian, and significantly
better on French to English test set scores.
Since the test sets for each language pair consist
of translations of the same documents, it is pos-
sible to combine outputs from many source lan-
guages to English. There were a total of 46 En-
glish primary  -best system outputs. Using all 46
outputs would have required too much memory in
tuning, so a subset of 11 outputs was chosen. The
11 outputs consist of google, uedin, and uka
outputs on all languages. Case insensitive TER
and BLEU scores for the xx-en combination are
shown in Table 2. In addition to incremental
and flexible alignment methods which used
sentence specific alignment order, scores for in-
cremental TER alignment with a fixed alignment
order used in the BBN submissions to WMT08
63
dev cz-en de-en es-en fr-en hu-en
System TER BLEU TER BLEU TER BLEU TER BLEU TER BLEU
worst 67.30 17.63 82.01 6.83 65.64 19.74 69.19 15.21 78.70 10.33
best 58.16 23.12 57.24 23.20 53.02 29.48 49.78 32.27 66.77 13.59
pairwise 59.60 24.01 56.35 26.04 53.11 29.49 51.03 31.65 69.58 14.60
incremental 59.22 24.31 55.73 26.73 53.05 29.72 50.72 32.09 70.15 14.85
flexible 59.38 24.18 55.51 26.71 52.62 30.24 50.22 32.58 69.83 14.88
test cz-en de-en es-en fr-en hu-en
System TER BLEU TER BLEU TER BLEU TER BLEU TER BLEU
worst 67.74 16.37 82.39 6.81 65.44 19.04 71.44 14.49 81.21 9.90
best 59.53 21.18 59.41 21.30 53.34 28.69 51.33 31.14 68.32 12.75
pairwise 61.02 21.25 58.75 23.41 53.65 28.15 53.17 29.83 71.50 13.39
incremental 60.63 21.67 58.13 23.96 53.47 28.38 52.51 30.45 71.69 13.60
flexible 60.34 21.87 58.05 23.86 53.13 28.57 51.98 31.30 71.17 13.84
Table 1: Case insensitive TER and BLEU scores on newssyscomb2009 (dev) and newstest2009
(test) for five source languages.
(Rosti et al, 2008) are marked as incr-wmt08.
The sentence specific alignment order yields about
a half BLEU point gain on the tuning set and a
one BLEU point gain on the test set. All system
combination experiments yield very good BLEU
gains on both sets. The scores are also signifi-
cantly higher than any combination from a single
source language. This shows that the outputs from
different source languages are likely to be more di-
verse than outputs from different MT systems on a
single language pair. The combination is not guar-
anteed to be the best possible as the set of outputs
was chosen arbitrarily.
The compactness of the confusion networks
may be measured by the average number of
nodes and arcs per segment. All xx-en con-
fusion networks for newssyscomb2009 and
newstest2009 after the incremental TER
alignment had on average 44.5 nodes and 112.7
arcs per segment. After the flexible hypothesis
alignment, there were on average 41.1 nodes and
104.6 arcs per segment. The number of NULL
word arcs may also be indicative of the alignment
quality. The flexible hypothesis alignment reduced
the average number of NULL word arcs from 29.0
to 24.8 per segment. The rate of convergence in
the  -best list based iterative tuning may be mon-
itored by the number of new hypotheses in the
merged  -best lists from iteration to iteration. By
the third tuning iteration, there were 10% fewer
new hypotheses in the merged  -best list when
using the flexible hypothesis alignment.
xx-en dev test
System TER BLEU TER BLEU
worst 74.21 12.80 75.84 12.05
best 49.78 32.27 51.33 31.14
pairwise 46.10 35.95 47.77 33.53
incr-wmt08 44.58 36.84 46.60 33.61
incremental 44.59 37.30 46.42 34.61
flexible 44.54 37.38 45.82 34.48
Table 2: Case insensitive TER and BLEU
scores on newssyscomb2009 (dev) and
newstest2009 (test) for xx-en combination.
4 Conclusions
This paper described a refined incremental hy-
pothesis alignment algorithm used in the BBN
submissions to the WMT09 system combination
task. The new features included sentence specific
alignment order, flexible matching, and modified
shift heuristics. The refinements yield more com-
pact confusion networks which should allow fewer
spurious insertions in the output and faster conver-
gence in tuning. The future work will investigate
tunable edit costs and methods to choose an opti-
mal subset of outputs for combination.
Acknowledgments
This work was supported by DARPA/IPTO Con-
tract No. HR0011-06-C-0022 under the GALE
program.
64
References
Necip Fazil Ayan, Jing Zheng, and Wen Wang. 2008.
Improving alignments for better confusion networks
for combining machine translation systems. In Pro-
ceedings of the 22nd International Conference on
Computational Linguistics (Coling 2008), pages 33?
40.
Srinivas Bangalore, German Bordel, and Giuseppe Ric-
cardi. 2001. Computing consensus translation from
multiple machine translation systems. In Proceed-
ings of the Automatic Speech Recognition and Un-
derstanding Workshop (ASRU), pages 351?354.
Richard Durbin, Sean R. Eddy, Anders Krogh, and
Graeme Mitchison. 1988. Biological Sequence
Analysis: Probabilistic Models of Proteins and Nu-
cleic Acids. Cambridge University Press.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. MIT Press.
Xiaodong He, Mei Yang, Jianfeng Gao, Patrick
Nguyen, and Robert Moore. 2008. Indirect-HMM-
based hypothesis alignment for combining outputs
from machine translation systems. In Proceedings
of the 2008 Conference on Empirical Methods in
Natural Language Processing, pages 98?107.
Damianos Karakos, Jason Eisner, Sanjeev Khundan-
pur, and Markus Dreyer. 2008. Machine trans-
lation system combination using ITG-based align-
ments. In Proceedings of ACL-08: HLT, pages 81?
84.
Evgeny Matusov, Nicola Ueffing, and Hermann Ney.
2006. Computing consensus translation from multi-
ple machine translation systems using enhanced hy-
potheses alignment. In Proceedings of the 11th Con-
ference of the European Chapter of the Association
for Computational Linguistics, pages 33?40.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proceedings
of the 40th Annual Meeting of the Association for
Computational Linguistics, pages 311?318.
Antti-Veikko I. Rosti, Spyros Matsoukas, and Richard
Schwartz. 2007. Improved word-level system com-
bination for machine translation. In Proceedings of
the 45th Annual Meeting of the Association of Com-
putational Linguistics, pages 312?319.
Antti-Veikko I. Rosti, Bing Zhang, Spyros Matsoukas,
and Richard Schwartz. 2008. Incremental hypothe-
sis alignment for building confusion networks with
application to machine translation system combina-
tion. In Proceedings of the Third Workshop on Sta-
tistical Machine Translation, pages 183?186.
Khe Chai Sim, William J. Byrne, Mark J.F. Gales,
Hichem Sahbi, and Phil C. Woodland. 2007. Con-
sensus network decoding for statistical machine
translation system combination. In Proceedings of
the 32nd IEEE International Conference on Acous-
tics, Speech, and Signal Processing, pages 105?108.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciula, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings of the 7th Conference of the Associa-
tion for Machine Translation in the Americas, pages
223?231.
Matthew Snover, Nitin Madnani, Bonnie Dorr, and
Richard Schwartz. 2009. Fluency, adequacy, or
HTER? Exploring different human judgments with
a tunable MT metric. In Proceedings of the Fourth
Workshop on Statistical Machine Translation.
65
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 616?625,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Statistical Machine Translation with a Factorized Grammar
Libin Shen and Bing Zhang and Spyros Matsoukas and
Jinxi Xu and Ralph Weischedel
Raytheon BBN Technologies
Cambridge, MA 02138, USA
{lshen,bzhang,smatsouk,jxu,weisched}@bbn.com
Abstract
In modern machine translation practice, a sta-
tistical phrasal or hierarchical translation sys-
tem usually relies on a huge set of trans-
lation rules extracted from bi-lingual train-
ing data. This approach not only results in
space and efficiency issues, but also suffers
from the sparse data problem. In this paper,
we propose to use factorized grammars, an
idea widely accepted in the field of linguis-
tic grammar construction, to generalize trans-
lation rules, so as to solve these two prob-
lems. We designed a method to take advantage
of the XTAG English Grammar to facilitate
the extraction of factorized rules. We experi-
mented on various setups of low-resource lan-
guage translation, and showed consistent sig-
nificant improvement in BLEU over state-of-
the-art string-to-dependency baseline systems
with 200K words of bi-lingual training data.
1 Introduction
A statistical phrasal (Koehn et al, 2003; Och and
Ney, 2004) or hierarchical (Chiang, 2005; Marcu
et al, 2006) machine translation system usually re-
lies on a very large set of translation rules extracted
from bi-lingual training data with heuristic methods
on word alignment results. According to our own
experience, we obtain about 200GB of rules from
training data of about 50M words on each side. This
immediately becomes an engineering challenge on
space and search efficiency.
A common practice to circumvent this problem
is to filter the rules based on development sets in the
step of rule extraction or before the decoding phrase,
instead of building a real distributed system. How-
ever, this strategy only works for research systems,
for which the segments for translation are always
fixed.
However, do we really need such a large rule set
to represent information from the training data of
much smaller size? Linguists in the grammar con-
struction field already showed us a perfect solution
to a similar problem. The answer is to use a fac-
torized grammar. Linguists decompose lexicalized
linguistic structures into two parts, (unlexicalized)
templates and lexical items. Templates are further
organized into families. Each family is associated
with a set of lexical items which can be used to lex-
icalize all the templates in this family. For example,
the XTAG English Grammar (XTAG-Group, 2001),
a hand-crafted grammar based on the Tree Adjoin-
ing Grammar (TAG) (Joshi and Schabes, 1997) for-
malism, is a grammar of this kind, which employs
factorization with LTAG e-tree templates and lexical
items.
Factorized grammars not only relieve the burden
on space and search, but also alleviate the sparse
data problem, especially for low-resource language
translation with few training data. With a factored
model, we do not need to observe exact ?template
? lexical item? occurrences in training. New rules
can be generated from template families and lexical
items either offline or on the fly, explicitly or im-
plicitly. In fact, the factorization approach has been
successfully applied on the morphological level in
previous study on MT (Koehn and Hoang, 2007). In
this work, we will go further to investigate factoriza-
tion of rule structures by exploiting the rich XTAG
English Grammar.
We evaluate the effect of using factorized trans-
lation grammars on various setups of low-resource
language translation, since low-resource MT suffers
greatly on poor generalization capability of trans-
616
lation rules. With the help of high-level linguis-
tic knowledge for generalization, factorized gram-
mars provide consistent significant improvement
in BLEU (Papineni et al, 2001) over string-to-
dependency baseline systems with 200K words of
bi-lingual training data.
This work also closes the gap between compact
hand-crafted translation rules and large-scale unor-
ganized automatic rules. This may lead to a more ef-
fective and efficient statistical translation model that
could better leverage generic linguistic knowledge
in MT.
In the rest of this paper, we will first provide a
short description of our baseline system in Section 2.
Then, we will introduce factorized translation gram-
mars in Section 3. We will illustrate the use of the
XTAG English Grammar to facilitate the extraction
of factorized rules in Section 4. Implementation de-
tails are provided in Section 5. Experimental results
are reported in Section 6.
2 A Baseline String-to-Tree Model
As the baseline of our new algorithm, we use a
string-to-dependency system as described in (Shen
et al, 2008). There are several reasons why we take
this model as our baseline. First, it uses syntactic
tree structures on the target side, which makes it easy
to exploit linguistic information. Second, depen-
dency structures are relatively easier to implement,
as compared to phrase structure grammars. Third,
a string-to-dependency system provides state-of-the-
art performance on translation accuracy, so that im-
provement over such a system will be more convinc-
ing.
Here, we provide a brief description of the base-
line string-to-dependency system, for the sake of
completeness. Readers can refer to (Shen et al,
2008; Shen et al, 2009) for related information.
In the baseline string-to-dependency model, each
translation rule is composed of two parts, source and
target. The source sides is a string rewriting rule,
and the target side is a tree rewriting rule. Both
sides can contain non-terminals, and source and tar-
get non-terminals are one-to-one aligned. Thus, in
the decoding phase, non-terminal replacement for
both sides are synchronized.
Decoding is solved with a generic chart parsing
algorithm. The source side of a translation rule is
used to detect when this rule can be applied. The tar-
get side of the rule provides a hypothesis tree struc-
ture for the matched span. Mono-lingual parsing can
be viewed as a special case of this generic algorithm,
for which the source string is a projection of the tar-
get tree structure.
Figure 1 shows three examples of string-to-
dependency translation rules. For the sake of con-
venience, we use English for both source and target.
Upper-cased words represent source, while lower-
cased words represent target. X is used for non-
terminals for both sides, and non-terminal alignment
is represented with subscripts.
In Figure 1, the top boxes mean the source side,
and the bottom boxes mean the target side. As for
the third rule, FUN Q stands for a function word in
the source language that represents a question.
3 Translation with a Factorized Grammar
We continue with the example rules in Figure 1.
Suppose, we have ?... HATE ... FUN Q? in a given
test segment. There is no rule having both HATE
and FUN Q on its source side. Therefore, we have
to translate these two source words separately. For
example, we may use the second rule in Figure 1.
Thus, HATE will be translated into hates, which is
wrong.
Intuitively, we would like to have translation rule
that tell us how to translate X1 HATE X2 FUN Q
as in Figure 2. It is not available directly from the
training data. However, if we obtain the three rules
in Figure 1, we are able to predict this missing rule.
Furthermore, if we know like and hate are in the
same syntactic/semantic class in the source or target
language, we will be very confident on the validity
of this hypothesis rule.
Now, we propose a factorized grammar to solve
this generalization problem. In addition, translation
rules represented with the new formalism will be
more compact.
3.1 Factorized Rules
We decompose a translation rule into two parts,
a pair of lexical items and an unlexicalized tem-
plate. It is similar to the solution in the XTAG En-
glish Grammar (XTAG-Group, 2001), while here we
617
X1  LIKE  X2
likes
X1 X2
X1  HATE  X2
hates
X1 X2
X1  LIKE X2  FUN_Q
like
does X1 X2
Figure 1: Three examples of string-to-dependency translation rules.
X1  V  X2
VBZ
X1 X2
X1  V  X2
VBZ
X1 X2
X1  V  X2  FUN_Q
VB
does X1 X2
Figure 3: Templates for rules in Figure 1.
X1  HATE  X2  FUN_Q
hate
does X1 X2
Figure 2: An example of a missing rule.
work on two languages at the same time.
For each rule, we first detect a pair of aligned head
words. Then, we extract the stems of this word pair
as lexical items, and replace them with their POS
tags in the rule. Thus, the original rule becomes an
unlexicalized rule template.
As for the three example rules in Figure 1, we will
extract lexical items (LIKE, like), (HATE, hate) and
(LIKE, like) respectively. We obtain the same lexical
items from the first and the third rules.
The resultant templates are shown in Figure 3.
Here, V represents a verb on the source side, VB
stands for a verb in the base form, and VBZ means
a verb in the third person singular present form as
in the Penn Treebank representation (Marcus et al,
1994).
In the XTAG English Grammar, tree templates for
transitive verbs are grouped into a family. All transi-
tive verbs are associated with this family. Here, we
assume that the rule templates representing struc-
tural variations of the same word class can also be
organized into a template family. For example, as
shown in Figure 4, templates and lexical items are
associated with families. It should be noted that
a template or a lexical item can be associated with
more than one family.
Another level of indirection like this provides
more generalization capability. As for the missing
618
X1  V  X2
VBZ
X1 X2
Family Transitive_3
X1  V  X2  FUN_Q
VB
does X1 X2
X1  V  FUN_Past
VBD
X1
Family Intransitive_2
( LIKE, like ) ( HATE, hate ) ( OPEN, open ) ( HAPPEN, happen )
Figure 4: Templates and lexical items are associated with families.
rule in Figure 2, we can now generate it by replac-
ing the POS tags in the second template of Figure
4 with lexical items (HATE, hate) with their correct
inflections. Both the template and the lexical items
here are associated with the family Transitive 3..
3.2 Statistical Models
Another level of indirection also leads to a desirable
back-off model. We decompose a rule R into to two
parts, its template PR and its lexical items LR. As-
suming they are independent, then we can compute
Pr(R) as
Pr(R) = Pr(PR)Pr(LR), or
Pr(R) =
?
F Pr(PR|F )Pr(LR|F )Pr(F ), (1)
if they are conditionally independent for each fam-
ily F . In this way, we can have a good estimate for
rules that do not appear in the training data. The
second generative model will also be useful for un-
supervised learning of families and related probabil-
ities.
In this paper, we approximate families by using
target (English) side linguistic knowledge as what
we will explain in Section 4, so this changes the def-
inition of the task. In short, we will be given a list of
families. We will also be given an association table
B(L,F ) for lexical items L and families F , such
that B(L,F ) = true if and only L is associated
with F , but we do not know the distributions.
Let S be the source side of a rule or a rule tem-
plate, T the target side of a rule of a rule template.
We define Prb, the back-off conditional model of
templates, as follows.
Prb(PS |PT , L) =
?
F :B(L,F ) #(PS , PT , F )
?
F :B(L,F ) #(PT , F )
, (2)
where # stands for the count of events.
Let P and L be the template and lexical items of
R respectively. Let Prt be the MLE model obtained
from the training data. The smoothed probability is
then defined as follows.
Pr(RS |RT ) = (1 ? ?)Prt(RS |RT )
+?Prb(PS |PT , L), (3)
where ? is a parameter. We fix it to 0.1 in later ex-
periments. Conditional probability Pr(RT |RS) is
defined in a similar way.
3.3 Discussion
The factorized models discussed in the previous sec-
tion can greatly alleviate the sparse data problem,
especially for low-resource translation tasks. How-
ever, when the training data is small, it is not easy to
619
learn families. Therefore, to use unsupervised learn-
ing with a model like (1) somehow reduces a hard
translation problem to another one of the same diffi-
culty, when the training data is small.
However, in many cases, we do have extra infor-
mation that we can take advantage of. For example,
if the target language has rich resources, although
the source language is a low-density one, we can ex-
ploit the linguistic knowledge on the target side, and
carry it over to bi-lingual structures of the translation
model. The setup of X-to-English translation tasks
is just like this. This will be the topic of the next
section. We leave unsupervised learning of factor-
ized translation grammars for future research.
4 Using A Mono-Lingual Grammar
In this section, we will focus on X-to-English trans-
lation, and explain how to use English resources to
build a factorized translation grammar. Although we
use English as an example, this approach can be ap-
plied to any language pairs that have certain linguis-
tic resources on one side.
As shown in Figure 4, intuitively, the families
are intersection of the word families of the two lan-
guages involved, which means that they are refine-
ment of the English word families. For example,
a sub-set of the English transitive families may be
translated in the same way, so they share the same
set of templates. This is why we named the two fam-
ilies Transitive 3 and Intransitive 2 in Figure 4.
Therefore, we approximate bi-lingual families
with English families first. In future, we can use
them as the initial values for unsupervised learning.
In order to learn English families, we need to take
away the source side information in Figure 4, and
we end up with a template?family?word graph as
shown in Figure 5. We can learn this model on large
mono-lingual data if necessary.
What is very interesting is that there already exists
a hand-crafted solution for this model. This is the
XTAG English Grammar (XTAG-Group, 2001).
The XTAG English Grammar is a large-scale En-
glish grammar based on the TAG formalism ex-
tended with lexicalization and unification-based fea-
ture structures. It consists of morphological, syn-
tactic, and tree databases. The syntactic database
contains the information that we have represented
in Figure 5 and many other useful linguistic annota-
tions, e.g. features.
The XTAG English grammar contains 1,004 tem-
plates, organized in 53 families, and 221 individual
templates. About 30,000 lexical items are associ-
ated with these families and individual templates 1.
In addition, it also has the richest English morpho-
logical lexicon with 317,000 inflected items derived
from 90,000 stems. We use this resource to predict
POS tags and inflections of lexical items.
In our applications, we select all the verb fami-
lies plus one each for nouns, adjectives and adverbs.
We use the families of the English word as the fam-
ilies of bi-lingual lexical items. Therefore, we have
a list of about 20 families and an association table
as described in Section 3.2. Of course, one can use
other linguistic resources if similar family informa-
tion is provided, e.g. VerbNet (Kipper et al, 2006)
or WordNet (Fellbaum, 1998).
5 Implementation
Nowadays, machine translation systems become
more and more complicated. It takes time to write
a decoder from scratch and hook it with various
modules, so it is not the best solution for research
purpose. A common practice is to reduce a new
translation model to an old one, so that we can use
an existing system, and see the effect of the new
model quickly. For example, the tree-based model
proposed in (Carreras and Collins, 2009) used a
phrasal decoder for sub-clause translation, and re-
cently, DeNeefe and Knight (2009) reduced a TAG-
based translation model to a CFG-based model by
applying all possible adjunction operations offline
and stored the results as rules, which were then used
by an existing syntax-based decoder.
Here, we use a similar method. Instead of build-
ing a new decoder that uses factorized grammars,
we reduce factorized rules to baseline string-to-
dependency rules by performing combination of
templates and lexical items in an offline mode. This
is similar to the rule generation method in (DeNeefe
and Knight, 2009). The procedure is as follows.
In the rule extraction phase, we first extract all the
string-to-dependency rules with the baseline system.
1More information about XTAG is available online at
http://www.cis.upenn.edu/?xtag .
620
VBZ
X1 X2
Family Transitive
VB
does X1 X2
VBD
X1
Family Intransitive
like hate open happen
Figure 5: Templates, families, and words in the XTAG English Grammar.
For each extracted rule, we try to split it into various
?template?lexical item? pairs by choosing different
aligned words for delexicalization, which turns rules
in Figure 1 into lexical items and templates in Fig-
ure 3. Events of templates and lexical items are
counted according to the family of the target En-
glish word. If an English word is associated with
more than one family, the count is distributed uni-
formly among these families. In this way, we collect
sufficient statistics for the back-off model in (2).
For each family, we keep the top 200 most fre-
quent templates. Then, we apply them to all the
lexical items in this families, and save the gener-
ated rules. We merge the new rules with the original
one. The conditional probabilities for the rules in the
combined set is smoothed according to (2) and (3).
Obviously, using only the 200 most frequent tem-
plates for each family is just a rough approxima-
tion. An exact implementation of a new decoder for
factorized grammars can make better use of all the
templates. However, the experiments will show that
even an approximation like this can already provide
significant improvement on small training data sets,
i.e. with no more than 2M words.
Since we implement template application in an of-
fline mode, we can use exactly the same decoding
and optimization algorithms as the baseline. The de-
coder is a generic chart parsing algorithm that gen-
erates target dependency trees from source string in-
put. The optimizer is an L-BFGS algorithm that
maximizes expected BLEU scores on n-best hy-
potheses (Devlin, 2009).
6 Experiments on Low-Resource Setups
We tested the performance of using factorized gram-
mars on low-resource MT setups. As what we noted
above, the sparse data problem is a major issue when
there is not enough training data. This is one of the
cases that a factorized grammar would help.
We did not tested on real low-resource languages.
Instead, we mimic the low-resource setup with two
of the most frequently used language pairs, Arabic-
to-English and Chinese-to-English, on newswire
and web genres. Experiments on these setups will
be reported in Section 6.1. Working on a language
which actually has more resources allows us to study
the effect of training data size. This will be reported
in Section 6.2. In Section 6.3, we will show exam-
ples of templates learned from the Arabic-to-English
training data.
6.1 Languages and Genres
The Arabic-to-English training data contains about
200K (target) words randomly selected from an
LDC corpus, LDC2006G05 A2E set, plus an
Arabic-English dictionary with about 89K items.
We build our development sets from GALE P4 sets.
There are one tune set and two test sets for the MT
systems 2. TEST-1 has about 5000 segments and
TEST-2 has about 3000 segments.
2One of the two test sets will later be used to tune an MT
combination system.
621
MODEL
TUNE TEST-1 TEST-2
BLEU %BL MET BLEU %BL MET BLEU %BL MET
Arabic-to-English newswire
baseline 21.07 12.41 43.77 19.96 11.42 42.79 21.09 11.03 43.74
factorized 21.70 13.17 44.85 20.52 11.70 43.83 21.36 11.77 44.72
Arabic-to-English web
baseline 10.26 5.02 32.78 9.40 4.87 31.26 14.11 7.34 35.93
factorized 10.67 5.34 33.83 9.74 5.20 32.52 14.66 7.69 37.11
Chinese-to-English newswire
baseline 13.17 8.04 44.70 19.62 9.32 48.60 14.53 6.82 45.34
factorized 13.91 8.09 45.03 20.48 9.70 48.61 15.16 7.37 45.31
Chinese-to-English web
baseline 11.52 5.96 42.18 11.44 6.07 41.90 9.83 4.66 39.71
factorized 11.98 6.31 42.84 11.72 5.88 42.55 10.25 5.34 40.34
Table 1: Experimental results on Arabic-to-English / Chinese-to-English newswire and web data. %BL stands for
BLEU scores for documents whose BLEU scores are in the bottom 75% to 90% range of all documents. MET stands
for METEOR scores.
The Chinese-to-English training data contains
about 200K (target) words randomly selected from
LDC2006G05 C2E set, plus a Chinese-English dic-
tionary (LDC2002L27) with about 68K items. The
development data setup is similar to that of Arabic-
to-English experiments.
Chinese-to-English translation is from a morphol-
ogy poor language to a morphology rich language,
while Arabic-to-English translation is in the oppo-
site direction. It will be interesting to see if factor-
ized grammars help on both cases. Furthermore, we
also test on two genres, newswire and web, for both
languages.
Table 1 lists the experimental results of all the four
conditions. The tuning metric is expected BLEU.
We are also interested in the BLEU scores for doc-
uments whose BLEU scores are in the bottom 75%
to 90% range of all documents. We mark it as %BL
in the table. This metric represents how a system
performances on difficult documents. It is important
to certain percentile evaluations. We also measure
METEOR (Banerjee and Lavie, 2005) scores for all
systems.
The system using factorized grammars shows
BLEU improvement in all conditions. We measure
the significance of BLEU improvement with paired
bootstrap resampling as described by (Koehn, 2004).
All the BLEU improvements are over 95% confi-
dence level. The new system also improves %BL
and METEOR in most of the cases.
6.2 Training Data Size
The experiments to be presented in this section
are designed to measure the effect of training data
size. We select Arabic web for this set of experi-
ments. Since the original Arabic-to-English train-
ing data LDC2006G05 is a small one, we switch to
LDC2006E25, which has about 3.5M target words
in total. We randomly select 125K, 250K, 500K, 1M
and 2M sub-sets from the whole data set. A larger
one always includes a smaller one. We still tune on
expected BLEU, and test on BLEU, %BL and ME-
TEOR.
The average BLEU improvement on test sets is
about 0.6 on the 125K set, but it gradually dimin-
ishes. For better observation, we draw the curves of
BLEU improvement along with significance test re-
sults for each training set. As shown in Figure 6 and
7, more improvement is observed with fewer train-
ing data. This fits well with fact that the baseline MT
model suffers more on the sparse data problem with
smaller training data. The reason why the improve-
ment diminishes on the full data set could be that the
rough approximation with 200 most frequent tem-
plates cannot fully take advantage of this paradigm,
which will be discussed in the next section.
622
MODEL SIZE
TUNE TEST-1 TEST-2
BLEU %BL MET BLEU %BL MET BLEU %BL MET
Arabic-to-English web
baseline
125K
8.54 2.96 28.87 7.41 2.82 26.95 11.29 5.06 31.37
factorized 8.99 3.44 30.40 7.92 3.57 28.63 12.04 6.06 32.87
baseline
250K
10.18 4.70 32.21 8.94 4.35 30.31 13.71 6.93 35.14
factorized 10.57 4.96 33.22 9.34 4.78 31.51 14.02 7.28 36.25
baseline
500K
12.18 5.84 35.59 10.82 5.77 33.62 16.48 8.30 38.73
factorized 12.40 6.01 36.15 11.14 5.96 34.38 16.76 8.53 39.27
baseline
1M
13.95 7.17 38.49 12.48 7.12 36.56 18.86 10.00 42.18
factorized 14.14 7.41 38.99 12.66 7.34 37.14 19.11 10.29 42.56
baseline
2M
15.74 8.38 41.15 14.18 8.17 39.26 20.96 11.95 45.18
factorized 15.92 8.81 41.51 14.34 8.25 39.68 21.42 12.05 45.51
baseline
3.5M
16.95 9.76 43.03 15.47 9.08 41.28 22.83 13.24 47.05
factorized 17.07 9.99 43.18 15.49 8.77 41.41 22.72 13.10 47.23
Table 2: Experimental results on Arabic web. %BL stands for BLEU scores for documents whose BLEU scores are
in the bottom 75% to 90% range of all documents. MET stands for METEOR scores.
-0.4
-0.2
 0
 0.2
 0.4
 0.6
 0.8
 1
 100000  1e+06
BL
EU
 im
pro
vem
ent
data size in logscale
TEST-1
Figure 6: BLEU Improvement with 95% confidence
range by using factorized grammars on TEST-1.
6.3 Example Templates
Figure 8 lists seven Arabic-to-English templates
randomly selected from the transitive verb family.
TMPL 151 is an interesting one. It helps to alleviate
the pronoun dropping problem in Arabic. However,
we notice that most of the templates in the 200 lists
are rather simple. More sophisticated solutions are
needed to go deep into the list to find out better tem-
plates in future.
It will be interesting to find an automatic or
semi-automatic way to discover source counterparts
of target treelets in the XTAG English Grammar.
-0.4
-0.2
 0
 0.2
 0.4
 0.6
 0.8
 1
 100000  1e+06
BL
EU
 im
pro
vem
ent
data size in logscale
TEST-2
Figure 7: BLEU Improvement with 95% confidence
range by using factorized grammars on TEST-2.
Generic rules like this will be very close to hand-
craft translate rules that people have accumulated for
rule-based MT systems.
7 Conclusions and Future Work
In this paper, we proposed a novel statistical ma-
chine translation model using a factorized structure-
based translation grammar. This model not only al-
leviates the sparse data problem but only relieves the
burden on space and search, both of which are im-
minent issues for the popular phrasal and/or hierar-
chical MT systems.
623
VVB
TMPL_1
X1  V
VBD
X1
TMPL_121
TMPL_31
V  X1
for
VBG
X1
TMPL_151
TMPL_61
V  X1
VBN
by
X1
TMPL_181
TMPL_91
X1  V
VBD
X1
the
V  X1
VBD
he X1
X1  V  X2
VBZ
X1 X2
Figure 8: Randomly selected Arabic-to-English templates from the transitive verb family.
We took low-resource language translation, espe-
cially X-to-English translation tasks, for case study.
We designed a method to exploit family informa-
tion in the XTAG English Grammar to facilitate the
extraction of factorized rules. We tested the new
model on low-resource translation, and the use of
factorized models showed significant improvement
in BLEU on systems with 200K words of bi-lingual
training data of various language pairs and genres.
The factorized translation grammar proposed here
shows an interesting way of using richer syntactic
resources, with high potential for future research.
In future, we will explore various learning meth-
ods for better estimation of families, templates and
lexical items. The target linguistic knowledge that
we used in this paper will provide a nice starting
point for unsupervised learning algorithms.
We will also try to further exploit the factorized
representation with discriminative learning. Fea-
tures defined on templates and families will have
good generalization capability.
Acknowledgments
This work was supported by DARPA/IPTO Contract
HR0011-06-C-0022 under the GALE program3. We
thank Aravind Joshi, Scott Miller, Richard Schwartz
and anonymous reviewers for valuable comments.
3Distribution Statement ?A? (Approved for Public Release,
Distribution Unlimited). The views, opinions, and/or find-
ings contained in this article/presentation are those of the au-
thor/presenter and should not be interpreted as representing the
official views or policies, either expressed or implied, of the De-
fense Advanced Research Projects Agency or the Department of
Defense.
624
References
Satanjeev Banerjee and Alon Lavie. 2005. Meteor: An
automatic metric for mt evaluation with improved cor-
relation with human judgments. In Proceedings of the
43th Annual Meeting of the Association for Computa-
tional Linguistics (ACL), pages 101?104, Ann Arbor,
MI.
Xavier Carreras and Michael Collins. 2009. Non-
projective parsing for statistical machine translation.
In Proceedings of the 2009 Conference of Empirical
Methods in Natural Language Processing, pages 200?
209, Singapore.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings of
the 43th Annual Meeting of the Association for Com-
putational Linguistics (ACL), pages 263?270, Ann Ar-
bor, MI.
Steve DeNeefe and Kevin Knight. 2009. Synchronous
tree adjoining machine translation. In Proceedings of
the 2009 Conference of Empirical Methods in Natural
Language Processing, pages 727?736, Singapore.
Jacob Devlin. 2009. Lexical features for statistical ma-
chine translation. Master?s thesis, Univ. of Maryland.
Christiane Fellbaum, editor. 1998. WordNet: an elec-
tronic lexical database. The MIT Press.
Aravind K. Joshi and Yves Schabes. 1997. Tree-
adjoining grammars. In G. Rozenberg and A. Salo-
maa, editors, Handbook of Formal Languages, vol-
ume 3, pages 69?124. Springer-Verlag.
Karin Kipper, Anna Korhonen, Neville Ryant, and
Martha Palmer. 2006. Extensive classifications of en-
glish verbs. In Proceedings of the 12th EURALEX In-
ternational Congress.
P. Koehn and H. Hoang. 2007. Factored translation mod-
els. In Proceedings of the 2007 Conference of Empiri-
cal Methods in Natural Language Processing.
Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003.
Statistical phrase based translation. In Proceedings
of the 2003 Human Language Technology Conference
of the North American Chapter of the Association for
Computational Linguistics, pages 48?54, Edmonton,
Canada.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Proceedings of
the 2004 Conference of Empirical Methods in Natu-
ral Language Processing, pages 388?395, Barcelona,
Spain.
Daniel Marcu, Wei Wang, Abdessamad Echihabi, and
Kevin Knight. 2006. SPMT: Statistical machine
translation with syntactified target language phrases.
In Proceedings of the 2006 Conference of Empirical
Methods in Natural Language Processing, pages 44?
52, Sydney, Australia.
M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz.
1994. Building a large annotated corpus of En-
glish: The Penn Treebank. Computational Linguistics,
19(2):313?330.
Franz J. Och and Hermann Ney. 2004. The alignment
template approach to statistical machine translation.
Computational Linguistics, 30(4).
Kishore Papineni, Salim Roukos, and Todd Ward. 2001.
Bleu: a method for automatic evaluation of machine
translation. IBM Research Report, RC22176.
Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008. A
new string-to-dependency machine translation algo-
rithm with a target dependency language model. In
Proceedings of the 46th Annual Meeting of the Associ-
ation for Computational Linguistics (ACL).
Libin Shen, Jinxi Xu, Bing Zhang, Spyros Matsoukas,
and Ralph Weischedel. 2009. Effective Use of Lin-
guistic and Contextual Information for Statistical Ma-
chine Translation. In Proceedings of the 2009 Confer-
ence of Empirical Methods in Natural Language Pro-
cessing, pages 72?80, Singapore.
XTAG-Group. 2001. A lexicalized tree adjoining gram-
mar for english. Technical Report 01-03, IRCS, Univ.
of Pennsylvania.
625
2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 49?59,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Machine Translation of Arabic Dialects
Rabih Zbib, Erika Malchiodi, Jacob Devlin, David Stallard, Spyros Matsoukas,
Richard Schwartz, John Makhoul, Omar F. Zaidan?, Chris Callison-Burch?
Raytheon BBN Technologies, Cambridge MA
?Microsoft Research, Redmond WA
?Johns Hopkins University, Baltimore MD
Abstract
Arabic Dialects present many challenges for
machine translation, not least of which is the
lack of data resources. We use crowdsourc-
ing to cheaply and quickly build Levantine-
English and Egyptian-English parallel cor-
pora, consisting of 1.1M words and 380k
words, respectively. The dialectal sentences
are selected from a large corpus of Arabic web
text, and translated using Amazon?s Mechan-
ical Turk. We use this data to build Dialec-
tal Arabic MT systems, and find that small
amounts of dialectal data have a dramatic im-
pact on translation quality. When translating
Egyptian and Levantine test sets, our Dialec-
tal Arabic MT system performs 6.3 and 7.0
BLEU points higher than a Modern Standard
Arabic MT system trained on a 150M-word
Arabic-English parallel corpus.
1 Introduction
The Arabic language is a well-known example of
diglossia (Ferguson, 1959), where the formal vari-
ety of the language, which is taught in schools and
used in written communication and formal speech
(religion, politics, etc.) differs significantly in its
grammatical properties from the informal varieties
that are acquired natively, which are used mostly for
verbal communication. The spoken varieties of the
Arabic language (which we refer to collectively as
Dialectal Arabic) differ widely among themselves,
depending on the geographic distribution and the
socio-economic conditions of the speakers, and they
diverge from the formal variety known as Mod-
ern Standard Arabic (MSA) (Embarki and Ennaji,
2011). Significant differences in the phonology,
morphology, lexicon and even syntax render some
of these varieties mutually incomprehensible.
The use of Dialectal Arabic has traditionally been
confined to informal personal speech, while writ-
ing has been done almost exclusively using MSA
(or its ancestor Classical Arabic). This situation is
quickly changing, however, with the rapid prolifer-
ation of social media in the Arabic-speaking part
of the world, where much of the communication
is composed in dialect. The focus of the Arabic
NLP research community, which has been mostly on
MSA, is turning towards dealing with informal com-
munication, with the introduction of the DARPA
BOLT program. This new focus presents new chal-
lenges, the most obvious of which is the lack of di-
alectal linguistic resources. Dialectal text, which is
usually user-generated, is also noisy, and the lack
of standardized orthography means that users often
improvise spelling. Dialectal data also includes a
wider range of topics than formal data genres, such
as newswire, due to its informal nature. These chal-
lenges require innovative solutions if NLP applica-
tions are to deal with Dialectal Arabic effectively.
In this paper:
? We describe a process for cheaply and quickly
developing parallel corpora for Levantine-
English and Egyptian-English using Amazon?s
Mechanical Turk crowdsourcing service (?3).
? We use the data to perform a variety of machine
translation experiments showing the impact of
morphological analysis, the limited value of
adding MSA parallel data, the usefulness of
cross-dialect training, and the effects of trans-
lating from dialect to MSA to English (?4).
We find that collecting dialect translations has a low
cost ($0.03/word) and that relatively small amounts
of data has a dramatic impact on translation quality.
When trained on 1.5M words of dialectal data, our
system performs 6.3 to 7.0 BLEU points higher than
when it is trained on 100 times more MSA data from
a mismatching domain.
49
2 Previous Work
Existing work on natural language processing of Di-
alectal Arabic text, including machine translation, is
somewhat limited. Previous research on Dialectal
Arabic MT has focused on normalizing dialectal in-
put words into MSA equivalents before translating
to English, and they deal with inputs that contain
a limited fraction of dialectal words. Sawaf (2010)
normalized the dialectal words in a hybrid (rule-
based and statistical) MT system, by performing a
combination of character- and morpheme-level map-
pings. They then translated the normalized source
to English using a hybrid MT or alternatively a
Statistical MT system. They tested their method
on proprietary test sets, observing about 1 BLEU
point (Papineni et al, 2002) increase on broadcast
news/conversation and about 2 points on web text.
Salloum and Habash (2011) reduced the proportion
of dialectal out-of-vocabulary (OOV) words also by
mapping their affixed morphemes to MSA equiva-
lents (but did not perform lexical mapping on the
word stems). They allowed for multiple morpho-
logical analyses, passing them on to the MT system
in the form of a lattice. They tested on a subset of
broadcast news and broadcast conversation data sets
consisting of sentences that contain at least one re-
gion marked as non-MSA, with an initial OOV rate
against an MSA training corpus of 1.51%. They
obtained a 0.62 BLEU point gain. Abo Bakr et
al. (2008) suggested another hybrid system to map
Egyptian Arabic to MSA, using morphological anal-
ysis on the input and an Egyptian-MSA lexicon.
Other work that has focused on tasks besides MT
includes that of Chiang et al (2006), who built a
parser for spoken Levantine Arabic (LA) transcripts
using an MSA treebank. They used an LA-MSA
lexicon in addition to morphological and syntac-
tic rules to map the LA sentences to MSA. Riesa
and Yarowsky (2006) built a statistical morphologi-
cal segmenter for Iraqi and Levantine speech tran-
scripts, and showed that they outperformed rule-
based segmentation with small amounts of training.
Some tools exist for preprocessing and tokenizing
Arabic text with a focus on Dialectal Arabic. For ex-
ample, MAGEAD (Habash and Rambow, 2006) is a
morphological analyzer and generator that can ana-
lyze the surface form of MSA and dialect words into
their root/pattern and affixed morphemes, or gener-
ate the surface form in the opposite direction.
Amazon?s Mechanical Turk (MTurk) is becom-
ing an essential tool for creating annotated resources
for computational linguistics. Callison-Burch and
Dredze (2010) provide an overview of various tasks
for which MTurk has been used, and offer a set of
best practices for ensuring high-quality data.
Zaidan and Callison-Burch (2011a) studied the
quality of crowdsourced translations, by quantifying
the quality of non-professional English translations
of 2,000 Urdu sentences that were originally trans-
lated by the LDC. They demonstrated a variety of
mechanisms that increase the translation quality of
crowdsourced translations to near professional lev-
els, with a total cost that is less than one tenth the
cost of professional translation.
Zaidan and Callison-Burch (2011b) created the
Arabic Online Commentary (AOC) dataset, a 52M-
word monolingual dataset rich in dialectal content.
Over 100k sentences from the AOC were annotated
by native Arabic speakers on MTurk to identify the
dialect level (and dialect itself) in each, and the col-
lected labels were used to train automatic dialect
identification systems. Although a large number
of dialectal sentences were identified (41% of sen-
tences), none were passed on to a translation phase.
3 Data Collection and Annotation
Following Zaidan and Callison-Burch (2011a,b), we
use MTurk to identify Dialectal Arabic data and to
create a parallel corpus by hiring non-professional
translators to translate the sentences that were la-
beled as being dialectal. We had Turkers perform
three steps for us: dialect classification, sentence
segmentation, and translation.
Since Dialectal Arabic is much less common in
written form than in spoken form, the first challenge
is to simply find instances of written Dialectal Ara-
bic. We draw from a large corpus of monolingual
Arabic text (approximately 350M words) that was
harvested from the web by the LDC, largely from
weblog and online user groups.1 Before present-
ing our data to annotators, we filter it to identify
1Corpora: LDC2006E32, LDC2006E77, LDC2006E90,
LDC2007E04, LDC2007E44, LDC2007E102, LDC2008E41,
LDC2008E54, LDC2009E14, LDC2009E93.
50
M
ag
hr
eb
i
E
gy
Ir
aq
i
G
ul
f
Ot
he
r
L
ev
Figure 1: One possible breakdown of spoken Arabic into
dialect groups: Maghrebi, Egyptian, Levantine, Gulf and
Iraqi. Habash (2010) gives a breakdown along mostly
the same lines. We used this map as an illustration for
annotators in our dialect classification task (Section 3.1),
with Arabic names for the dialects instead of English.
segments most likely to be dialectal (unlike Zaidan
and Callison-Burch (2011b), who did no such pre-
filtering). We eliminate documents with a large per-
centage of non-Arabic or MSA words. We then
retain documents that contain some number of di-
alectal words, using a set of manually selected di-
alectal words that was assembled by culling through
the transcripts of the Levantine Fisher and Egyp-
tian CallHome speech corpora. After filtering, the
dataset contained around 4M words, which we used
as a starting point for creating our Dialectal Arabic-
English parallel corpus.
3.1 Dialect Classification
To refine the document set beyond our keyword fil-
tering heuristic and to label which dialect each doc-
ument is written in, we hire Arabic annotators on
MTurk to perform classification similar to Zaidan
and Callison-Burch (2011b). Annotators were asked
to classify the filtered documents for being in MSA
or in one of four regional dialects: Egyptian, Lev-
antine, Gulf/Iraqi or Maghrebi, and were shown the
map in Figure 1 to explain what regions each of the
dialect labels corresponded to. We allowed an addi-
tional ?General? dialect option for ambiguous docu-
ments. Unlike Zaidan and Callison-Burch, our clas-
sification was applied to whole documents (corre-
sponding to a user online posting) instead of individ-
ual sentences. To perform quality control, we used
a set of documents for which correct labels were
known. We presented these 20% of the time, and
Dialect Classification HIT $10,064
Sentence Segmentation HIT $1,940
Translation HIT $32,061
Total cost $44,065
Num words translated 1,516,856
Cost per word 2.9 cents/word
Table 1: The total costs for the three MTurk subtasks in-
volved with the creation of our Dialectal Arabic-English
parallel corpus.
eliminated workers who did not correctly classify
them (2% of labels).
Identifying the dialect of a text snippet can be
challenging in the absence of phonetic cues. We
therefore required 3 classifications from different
workers for every document, and accepted a dialect
label if at least two of them agreed. The dialect dis-
tribution of the final output was: 43% Gulf/Iraqi,
28% Levantine, 11% Egyptian, and 16% could not
be classified. MSA and the other labels accounted
for 2%. We decided to translate only the Levantine
and Egyptian documents, since the pool of MTurk
workers contained virtually no workers from Iraq or
the Gulf region.
3.2 Sentence Segmentation
Since the data we annotated was mostly user-
generated informal web content, the existing punc-
tuation was often insufficient to determine sentence
boundaries. Since sentence boundaries are impor-
tant for correct translation, we segmented passages
into individual sentences using MTurk. We only re-
quired sentences longer than 15 words to be seg-
mented, and allowed Turkers to split and rejoin at
any point between the tokens. The instructions were
simply to ?divide the Arabic text into individual sen-
tences, where you believe it would be appropriate
to insert a period.? We also used a set of correctly
segmented passages for quality control, and scored
Turkers using a metric based on the precision and
recall of correct segmentation points. The rejection
rate was 1.2%.
3.3 Translation to English
Following Zaidan and Callison-Burch (2011a), we
hired non-professional translators on MTurk to
translate the Levantine and Egyptian sentences into
51
Sentence Arabic English
Data Set Pairs Tokens Tokens
MSA-150MW 8.0M 151.4M 204.4M
Dialect-1500KW 180k 1,545,053 2,257,041
MSA-1300KW 71k 1,292,384 1,752,724
MSA-Web-Tune 6,163 145,260 184,185
MSA-Web-Test 5,454 136,396 172,357
Lev-Web-Tune 2,600 20,940 27,399
Lev-Web-Test 2,600 21,092 27,793
Egy-Web-Test 2,600 23,671 33,565
E-Facebook-Tune 3,351 25,130 34,753
E-Facebook-Test 3,188 25,011 34,244
Table 2: Statistics about the training/tuning/test datasets
used in our experiments. The token counts are calculated
before MADA segmentation.
English. Among several quality control measures,
we rendered the Arabic sentences as images to pre-
vent Turkers from simply copying the Arabic text
into translation software. We still spot checked the
translations against the output of Google Translate
and Bing Translator. We also rejected gobbledygook
garbage translations that have a high percentage of
words not found in an English lexicon.
We quantified the quality of an individual Turker?s
translations in two ways: first by asking native Ara-
bic speaker judges to score a sample of the Turker?s
translations, and second by inserting control sen-
tences for which we have good reference translations
and measuring the Turker?s METEOR (Banerjee and
Lavie, 2005) and BLEU-1 scores (Papineni et al,
2002).2 The rejection rate of translation assignments
was 5%. We promoted good translators to a re-
stricted access ?preferred worker queue?. They were
paid at a higher rate, and were required to translate
control passages only 10% of the time as opposed
to 20% for general Turkers, thus providing us with a
higher translation yield for unseen data.
Worker turnout was initially slow, but increased
quickly as our reputation for being reliable payers
was established; workers started translating larger
volumes and referring their acquaintances. We had
121 workers who each completed 20 or more trans-
lation assignments. We eventually reached and sus-
tained a rate of 200k words of acceptable quality
2BLEU-1 provided a more reliable correlation with human
judgment in this case that the regular BLEU score (which uses
n-gram orders 1, . . . , 4), given the limited size of the sample
measured.
translated per week. Unlike Zaidan and Callison-
Burch (2011a), who only translated 2,000 Urdu sen-
tences, we translated sufficient volumes of Dialectal
Arabic to train machine translation systems. In total,
we had 1.1M words of Levantine and 380k words of
Egyptian translated into English, corresponding to
about 2.3M words on the English side.
Table 1 outlines the costs involved with creating
our parallel corpus. The total cost was $44k, or
$0.03/word ? an order of magnitude cheaper than
professional translation.
4 Experiments in Dialectal Arabic-English
Machine Translation
We performed a set of experiments to contrast sys-
tems trained using our dialectal parallel corpus with
systems trained on a (much larger) MSA-English
parallel corpus. All experiments use the same meth-
ods for training, decoding and parameter tuning, and
we only varied the corpora used for training, tun-
ing and testing. The MT system we used is based
on a phrase-based hierarchical model similar to that
of Shen et al (2008). We used GIZA++ (Och and
Ney, 2003) to align sentences and extract hierar-
chical rules. The decoder used a log-linear model
that combines the scores of multiple feature scores,
including translation probabilities, smoothed lexi-
cal probabilities, a dependency tree language model,
in addition to a trigram English language model.
Additionally, we used 50,000 sparse, binary-valued
source and target features based on Chiang et al
(2009). The English language model was trained on
7 billion words from the Gigaword and from a web
crawl. The feature weights were tuned to maximize
the BLEU score on a tuning set using the Expected-
BLEU optimization procedure (Devlin, 2009).
The Dialectal Arabic side of our corpus consisted
of 1.5M words (1.1M Levantine and 380k Egyp-
tian). Table 2 gives statistics about the various
train/tune/test splits we used in our experiments.
Since the Egyptian set was so small, we split it only
to training/test sets, opting not to have a tuning set.
The MSA training data we used consisted of Arabic-
English corpora totaling 150M tokens (Arabic side).
The MSA train/tune/test sets were constructed for
the DARPA GALE program.
We report translation quality in terms of BLEU
52
Simple Segment MADA Segment
Training Tuning BLEU OOV BLEU OOV ?BLEU ?OOV
MSA-Web-Test
MSA-150MW MSA-Web 26.21 1.69% 27.85 0.48% +1.64 -1.21%
MSA-1300KW 21.24 7.20% 25.23 1.95% +3.99 -5.25%
Egyptian-Web-Test
Dialect-1500KW Levantine-Web 18.55 6.31% 20.66 2.85% +2.11 -3.46%
Levantine-Web-Test
Dialect-1500KW Levantine-Web 17.00 6.22% 19.29 2.96% +2.29 -3.26%
Table 3: Comparison of the effect of morphological segmentation when translating MSA web text and Dialectal
Arabic web text. The morphological segmentation uniformly improves translation quality, but the improvements are
more dramatic for MSA than for Dialectal Arabic when comparing similarly-sized training corpora.
Training Tuning BLEU OOV BLEU OOV BLEU OOV
Egyptian-Web-Test Levantine-Web-Test MSA-Web-Test
MSA-150MW MSA-Web 14.76 4.42% 11.83 5.53% 27.85 0.48%
MSA-150MW Lev-Web 14.34 4.42% 12.29 5.53% 24.63 0.48%
MSA-150MW+Dial-1500KW 20.09 2.04% 19.11 2.27% 24.30 0.45%
Dialect-1500KW 20.66 2.85% 19.29 2.96% 15.53 3.70%
Egyptian-360KW 19.04 4.62% 11.21 9.00% - -
Levantine-360KW 14.05 7.11% 16.36 5.24% - -
Levantine-1100KW 17.79 4.83% 19.29 3.31% - -
Table 4: A comparison of translation quality of Egyptian, Levantine, andMSAweb text, using various training corpora.
The highest BLEU scores are achieved using the full set of dialectal data (which combines Levantine and Egyptian),
since the Egyptian alone is sparse. For Levantine, adding Egyptian has no effect. In both cases, adding MSA to the
dialectal data results in marginally worse translations.
score.3 In addition, we also report the OOV rate of
the test set relative to the training corpus in each ex-
perimental setups.
4.1 Morphological Decomposition
Arabic has a complex morphology compared to En-
glish. Preprocessing the Arabic source by morpho-
logical segmentation has been shown to improve the
performance of Arabic MT (Lee, 2004; Habash and
Sadat, 2006) by decreasing the size of the source vo-
cabulary, and improving the quality of word align-
ments. The morphological analyzers that underlie
most segmenters were developed for MSA, but the
different dialects of Arabic share many of the mor-
phological affixes of MSA, and it is therefore not
unreasonable to expect MSA segmentation to also
improve Dialect Arabic to English MT. To test this,
3We also computed TER (Snover et al, 2006) andMETEOR
scores, but omit them because they demonstrated similar trends.
we ran experiments using the MADA morpholog-
ical analyzer (Habash and Rambow, 2005). Table
3 shows the effect of applying segmentation to the
text, for both MSA and Dialectal Arabic. The BLEU
score improves uniformly, although the improve-
ments are most dramatic for smaller datasets, which
is consistent with previous work (Habash and Sadat,
2006). Morphological segmentation gives a smaller
gain on dialectal input, which could be due to two
factors: the segmentation accuracy likely decreases
since we are using an unmodified MSA segmenter,
and there is higher variability in the written form of
dialect compared to MSA. Given the significant, al-
beit smaller gain on dialectal input, we use MADA
segmentation in all our experiments.
4.2 Effect of Dialectal Training Data Size
We next examine how the size of the dialectal train-
ing data affects MT performance, and whether it is
useful to combine it with MSA training data. We
53
oh
 ti
me
 (s
pa
ce
 om
itt
ed
). 
Ap
pe
are
d w
ith
in
 a 
po
em
.
11
yA
zm
n

?
lik
e y
ou
 (c
or
ru
pti
on
 of
 M
SA
 m
vl
k)
.
10
m
tlk
"#
$
by
 m
uc
h (
co
rru
pti
on
 of
 M
SA
 bk
vy
r).
11
bk
ty
r
&'$
()
I m
iss
 yo
u (
sp
ok
en
 to
 a 
fe
ma
le)
 ?
Eg
yp
tia
n.
14
w
H
$t
yn
y
/0
'$1
2?
Th
e l
as
t n
am
e (
Al
-N
a'o
om
) o
f a
 fo
ru
m 
ad
mi
n.
16
A
ln
E
w
m
?:;
0<?
a l
oo
ot 
(c
or
ru
pti
on
 of
 M
SA
 kv
yr
A
).
17
kt
yy
yr
&''
'$?
rea
lly
/fo
r r
ea
l ?
Le
va
nti
ne
.
31
E
nj
d
DE
0F
En
gli
sh
 E
qu
iva
len
t
Co
un
t
TL
Ar
ab
ic
Table 5: The most frequent OOV?s (with counts ? 10) of the dialectal test sets against the MSA training data.
Source (EGY):  ? ? ??	
?   ? ! !
Transliteration: Ant btEml lh AElAn wlA Ayh?!!
MSA-Sys. Output: You are working for a declarationand not?
Dial-Sys. Output: You are making the advertisementfor him or what?
Reference: Are you promoting it or what?!!
Source (EGY):  01?. ??78 6 35 34? ?
 9:;? <=>
Transliteration: nfsY Atm}n Elyh bEd mA $Af
AlSwrh dy
MSA-Sys. Output: Myself feel to see this image.
Dial-Sys. Output: I wish to check on him afterhe saw this picture.
Reference: I wish to be sure that he is fineafter he saw this images
Source (LEV):  ?0??? E7770 ?F? G7H
Transliteration: lhyk Aljw ktyyyr kwwwl
MSA-Sys. Output: God you the atmosphere.
Dial-Sys. Output: this is why the weather is so cool
Reference: This is why the weather is so cool
Source (LEV):  ?L M
 G3 0?;
Transliteration: Twl bAlk Em nmzH
MSA-Sys. Output: Do you think about a joke long.
Dial-Sys. Output: Calm down we are kidding
Reference: calm down, we are kidding
Figure 2: Examples of improvement in MT output when
training on our Dialectal Arabic-English parallel corpus
instead of an MSA-English parallel corpus.
Source (EGY):   	
 	  ? 
Transliteration: qAltlp Tb tEAlY nEd ,
MSA-Sys. Output: Medicine almighty promise.
Dial-Sys. Output: She said, OK, come and then
Reference: She told him, OK, lets count them ,
Source (LEV):  "#$%& 
#'01 ?-%. ! -,%+? ?? ?2 
Transliteration: fbqrA w>HyAnA bqDyhA Em
>tslY mE rfqAty
MSA-Sys. Output: I read and sometimes with gowith my uncle.
Dial-Sys. Output: So I read, and sometimes I spendtrying to make my self comfortwith my friends
Reference: So i study and sometimes I spendthe time having fun with my friends
Source (LEV):  ?@ ?< ??' => +? &#:9? B:C12D E?
?? %$?+G 
Transliteration: Allh ysAmHkn hlq kl wAHd TAlb
qrb bykwn bdw Erws
MSA-Sys. Output: God now each student near the
Bedouin bride.
Dial-Sys. Output: God forgive you, each one is aclose student would want the bride
Reference: God forgive you. Is every oneasking to be close, want a bride!
Figure 3: Examples of ambiguous words that are trans-
lated incorrectly by the MSA-English system, but cor-
rectly by the Dialectal Arabic-English system.
54
!"
!#
!$
!%
"&
""
&' "&&' #&&' %&&' !(&&'
!
"#
$
%
!"#$%&'()*#"+"+,(-./0(/1(2/*345
)*+,-./0123
-./0123
Egyptian web test
!"
!#
!$
!%
"&
""
&' "&&' #&&' %&&' !(&&'
!
"#
$
!"#$%&'()*#"+"+,(-./0(/1(2/*345
)*+,-./0123
-./0123
Levantine web test
Figure 4: Learning curves showing the effects of increas-
ing the size of dialectal training data, when combined
with the 150M-word MSA parallel corpus, and when
used alone. Adding the MSA training data is only use-
ful when the dialectal data is scarce (200k words).
started with a baseline system trained on the 150M-
word MSA parallel corpus, and added various sized
portions of the dialect parallel corpus to it. Figure 4
shows the resulting learning curve, and compares it
to the learning curve for a system trained solely on
the dialectal parallel corpus. When only 200k words
of dialectal data are available, combining it with the
150M-word MSA corpus results in improved BLEU
scores, adding 0.8?1.5 BLEU points. When 400k
words or more of dialectal data are available, the
MSA training data ceases to provide any gain, and
in fact starts to hurt the performance.
The performance of a system trained on the 1.5M-
word dialectal data is dramatically superior to a sys-
tem that uses only the 150M-word MSA data: +6.32
BLEU points on the Egyptian test set, or 44% rela-
tive gain, and +7.00 BLEU points on the Levantine
test set, or 57% relative gain (fourth line vs. second
line of Table 4). In Section 4.4, we show that those
gains are not an artifact of the similarity between test
and training datasets, or of using the same translator
pool to translate both sets.
Inspecting the difference in the outputs of the Di-
alectal vs. MSA systems, we see that the improve-
ment in score is a reflection of a significant improve-
ment in the quality of translations. Figure 2 shows
a few examples of sentences whose translations im-
prove significantly using the Dialectal system. Fig-
ure 3 shows a particularly interesting category of ex-
amples. Many words are homographs, with different
meanings (and usually different pronunciations) in
MSA vs. one or more dialects. The bolded tokens
in the sentences in Figure 3 are examples of such
words. They are translated incorrectly by the MSA
system, while the dialect system translates them cor-
rectly.4 If we examine the most frequent OOVwords
against the MSA training data (Table 5), we find a
number of corrupted MSA words and names, but
that a majority of OOVs are dialect words.
4.3 Cross-Dialect Training
Since MSA training data appeared to have little ef-
fect when translating dialectal input, we next inves-
tigated the effect of training data from one dialect on
translating the input of another dialect. We trained a
system with the 360k-word Egyptian training subset
of our dialectal parallel corpus, and another system
with a similar amount of Levantine training data. We
used each system to translate the test set of the other
dialect. As expected, a system performs better when
it translates a test set in the same dialect that it was
trained on (Table 4).
That said, since the Egyptian training set is so
small, adding the (full) Levantine training data im-
proves performance (on the Egyptian test set) by
1.62 BLEU points, compared to using only Egyp-
tian training data. In fact, using the Levantine
training data by itself outperforms the MSA-trained
system on the Egyptian test set by more than 3
BLEU points. (For the Levantine test set, adding
the Egyptian training data has no affect, possibly
due to the small amount of Egyptian data.) This
may suggest that the mismatch between dialects is
less severe than the mismatch between MSA and
dialects. Alternatively, the differences may be due
to the changes in genre from the MSA parallel cor-
pus (which is mainly formal newswire) to the news-
groups and weblogs that mainly comprise the dialec-
tal corpus.
4The word nfsY of Figure 2 (first word of second example)
is also a homograph, as it means myself in MSA and I wish in
Dialectal Arabic.
55
Training Tuning BLEU OOV
MSA-150MW Levantine-Web 13.80 4.16%
MSA-150MW+Dialect-1500KW 16.71 2.43%
Dialect-1500KW 15.75 3.79%
MSA-150MW Egyptian-Facebook 15.80 4.16%
MSA-150MW+Dialect-1500KW 18.50 2.43%
Dialect-1500KW 17.90 3.79%
Dialect-1000KW (random selection) Egyptian-Facebook 17.09 4.64%
Dialect-1000KW (no Turker overlap) 17.10 4.60%
Table 6: Results on a truly independent test set, consisting of data harvested from Egyptian Facebook pages that are
entirely distinct from the our dialectal training set. The improvements over the MSA baseline are still considerable:
+2.9 BLEU points when no Facebook data is available for tuning and +2.7 with a Facebook tuning set.
4.4 Validation on Independent Test Data
To eliminate the possibility that the gains are solely
due to similarity between the test/training sets in the
dialectal data, we ran experiments using the same
dialectal training data, but using truly independent
test/tuning data sets selected at random from a larger
set of monolingual data that we collected from pub-
lic Egyptian Facebook pages. This data consists of
a set of original user postings and the subsequent
comments on each, giving the data a more conversa-
tional style than our other test sets. The postings
deal with current Egyptian political affairs, sports
and other topics. The test set we selected consisted
of 25,011 words (3,188 comments and 427 postings
from 86 pages), and the tuning set contained 25,130
words (3,351 comments and 415 conversations from
58 pages). We obtained reference translations for
those using MTurk as well.
Table 6 shows that using the 1.5M-word dialect
parallel corpus for training yields a 2 point BLEU
improvement over using the 150M-word MSA cor-
pus. Adding the MSA training data does yield an
improvement, though of less than a single BLEU
point. It remains true that training on 1.5M words
of dialectal data is better than training on 100 times
more MSA parallel data. The system performance
is sensitive to the tuning set choice, and improves
when it matches the test set in genre and origin.
To eliminate another potential source of artificial
bias, we also performed an experiment where we
removed any training translation contributed by a
Turker who translated any sentence in the Egyptian
Facebook set, to eliminate translator bias. For this,
we were left with 1M words of dialect training data.
This gave the same BLEU score as when training
with a randomly selected subset of the same size
(bottom part of Table 6).
4.5 Mapping from Dialectal Arabic to MSA
Before Translating to English
Given the large amount of linguistic resources that
have been developed for MSA over the past years,
and the extensive research that was conducted on
machine translation from MSA to English and other
languages, an obvious research question is whether
Dialectal Arabic is best translated to English by first
pivoting through MSA, rather than directly. The
proximity of Dialectal Arabic to MSA makes the
mapping in principle easier than general machine
translation, and a number of researchers have ex-
plored this direction (Salloum and Habash, 2011).
In this scenario, the dialectal source would first be
automatically transformed to MSA, using either a
rule-based or statistical mapping module.
The Dialectal Arabic-English parallel corpus we
created presents a unique opportunity to compare
the MSA-pivoting approach against direct transla-
tion. First, we collected equivalent MSA data for
the Levantine Web test and tuning sets, by asking
Turkers to transform dialectal passages to valid and
fluent MSA. Turkers were shown example transfor-
mations, and we encouraged fewer changes where
applicable (e.g. morphological rather than lexical
mapping), but allowed any editing operation in gen-
eral (deletion, substitution, reordering). Sample sub-
missions were independently shown to native Ara-
bic speaking judges, who confirmed they were valid
MSA. A lowOOV rate also indicated the correctness
of the mappings. By manually transforming the test
56
Training BLEU OOV BLEU OOV ?BLEU ?OOV
Direct dialect trans Map to MSA then trans
MSA-150MW 12.29 5.53% 14.59 1.53% +2.30 -4.00%
MSA-150MW+Dialect-200KW 15.37 3.59% 15.53 1.22% +0.16 -2.37%
MSA-150MW+Dialect-400KW 16.62 3.06% 16.25 1.13% -0.37 -1.93%
MSA-150MW+Dialect-800KW 17.83 2.63% 16.69 1.04% -1.14 -1.59%
MSA-150MW+Dialect-1500KW 19.11 2.27% 17.20 0.98% -1.91 -1.29%
Table 7: A comparison of the effectiveness of performing Levantine-to-MSA mapping before translating into English,
versus translating directly from Levantine into English. The mapping from Levantine to MSA was done manually, so it
is an optimistic estimate of what might be done automatically. Although initially helpful to the MSA baseline system,
the usefulness of pivoting through MSA drops as more dialectal data is added, eventually hurting performance.
dialectal sentence into MSA, we establish an opti-
mistic estimate of what could be done automatically.
Table 7 compares direct translation versus piv-
oting to MSA before translating, using the base-
line MSA-English MT system.5 The performance
of the system improves by 2.3 BLEU points with
dialect-to-MSA pivoting, compared to attempting to
translate the untransformed dialectal input directly.
As we add more dialectal training data, the BLEU
score when translating the untransformed dialect
test set improves rapidly (as seen previously in the
MSA+Dialect learning curve in Figure 4), while the
improvement is less rapid when the text is first trans-
formed to MSA. Direct translation becomes a better
option than mapping to MSA once 400k words of di-
alectal data are added, despite the significantly lower
OOV rate with MSA-mapping. This indicates that
simple vocabulary coverage is not sufficient, and
data domain mismatch, quantified by more complex
matching patterns, is more important.
5 Conclusion
We have described a process for building a Dialec-
tal Arabic-English parallel corpus, by selecting pas-
sages with a relatively high percentage of non-MSA
words from a monolingual Arabic web text corpus,
then using crowdsourcing to classify them by di-
alect, segment them into individual sentences and
translate them to English. The process was success-
fully scaled to the point of reaching and sustaining a
rate of 200k translated words per week, at 1/10 the
cost of professional translation. Our parallel corpus,
consisting of 1.5M words, was produced at a total
5The systems in each column of the table are tuned consis-
tently, using their corresponding tuning sets.
cost of $40k, or roughly $0.03/word.
We used the parallel corpus we constructed to
analyze the behavior of a Dialectal Arabic-English
MT system as a function of the size of the dialec-
tal training corpus. We showed that relatively small
amounts of training data render larger MSA corpora
from different data genres largely ineffective for this
test data. In practice, a system trained on the com-
bined Dialectal-MSA data is likely to give the best
performance, since informal Arabic data is usually
a mixture of Dialectal Arabic and MSA. An area of
future research is using the output of a dialect clas-
sifier, or other features to bias the translation model
towards the Dialectal or the MSA parts of the data.
We also validated the models built from the di-
alectal corpus by using them to translate an inde-
pendent data set collected from Egyptian Facebook
public pages. We finally investigated using MSA
as a ?pivot language? for Dialectal Arabic-English
translation, by simulating automatic dialect-to-MSA
mapping using MTurk. We obtained limited gains
from mapping the input to MSA, even when the
mapping is of good quality, and only at lower train-
ing set sizes. This suggests that the mismatch be-
tween training and test data is an important aspect of
the problem, beyond simple vocabulary coverage.
The aim of this paper is to contribute to setting
the direction of future research on Dialectal Arabic
MT. The gains we observed from using MSA mor-
phological segmentation can be further increased
with dialect-specific segmenters. Input preprocess-
ing can also be used to decrease the noise of the
user-generated data. Topic adaptation is another im-
portant problem to tackle if the large MSA linguistic
resources already developed are to be leveraged for
Dialectal Arabic-English MT.
57
Acknowledgments
This work was supported in part by DARPA/IPTO
Contract No. HR0011-12-C-0014 under the BOLT
Program, and in part by the EuroMatrixPlus project
funded by the European Commission (7th Frame-
work Programme). The views expressed are those
of the authors and do not reflect the official policy
or position of the Department of Defense or the U.S.
Government. Distribution Statement A (Approved
for Public Release, Distribution Unlimited).
References
Hitham M. Abo Bakr, Khaled Shaalan, and Ibrahim
Ziedan. 2008. A hybrid approach for converting writ-
ten Egyptian colloquial dialect into diacritized Arabic.
In The 6th International Conference on Informatics
and Systems, INFOS2008, Cairo, Egypt.
Satanjeev Banerjee and Alon Lavie. 2005. Meteor:
An automatic metric for MT evaluation with improved
correlation with human judgments. In In Proc. of ACL
2005 Workshop on Intrinsic and Extrinsic Evaluation
Measures for MT and/or Summarization, Ann Arbor,
Michigan.
Chris Callison-Burch and Mark Dredze. 2010. Creating
speech and language data with Amazon?s Mechanical
Turk. In Proceedings of the NAACL HLT 2010 Work-
shop on Creating Speech and Language Data with
Amazon?s Mechanical Turk, pages 1?12, Los Angeles,
June.
David Chiang, Mona Diab, Nizar Habash, Owen Ram-
bow, and Safiullah Shareef. 2006. Parsing Arabic di-
alects. In Proceedings of the Conference of the Eu-
ropean Chapter of the Association for Computational
Linguistics, Trento, Italy.
David Chiang, Kevin Knight, and Wei Wang. 2009.
11,001 new features for statistical machine translation.
In NAACL ?09: Proceedings of the 2009 Human Lan-
guage Technology Conference of the North American
Chapter of the Association for Computational Linguis-
tics, Boulder, Colorado.
Jacob Devlin. 2009. Lexical features for statistical ma-
chine translation. Master?s thesis, University of Mary-
land, December.
Mohamed Embarki and Moha Ennaji, editors. 2011.
Modern Trends in Arabic Dialectology. The Red Sea
Press.
Charles A. Ferguson. 1959. Diglossia. Word, 15:325?
340.
Nizar Habash and Owen Rambow. 2005. Arabic tok-
enization, part-of-speech tagging and morphological
disambiguation in one fell swoop. In Proceedings of
the 43th Annual Meeting of the Association for Com-
putational Linguistics (ACL), Ann Arbor, Michigan.
Nizar Habash and Owen Rambow. 2006. MAGEAD: A
morphological analyzer and generator for the Arabic
dialects. In Proceedings of the 44th Annual Meeting of
the Association for Computational Linguistics (ACL),
Sydney, Australia.
Nizar Habash and Fatiha Sadat. 2006. Arabic prepro-
cessing schemes for statistical machine translation. In
Proceedings of the 2006 Human Language Technol-
ogy Conference of the North American Chapter of the
Association for Computational Linguistics, New York,
New York.
Nizar Y. Habash. 2010. Introduction to Arabic Natural
Language Processing. Morgan & Claypool.
Young-Suk Lee. 2004. Morphological analysis for
statistical machine translation. In HLT-NAACL ?04:
Proceedings of HLT-NAACL 2004, Boston, Mas-
sachusetts.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19?51.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic evalua-
tion of machine translation. In Proceedings of the 40th
Annual Meeting of the Association for Computational
Linguistics (ACL), Philadelphia, PA.
Jason Riesa and David Yarowsky. 2006. Minimally
supervised morphological segmentation with applica-
tions to machine translation. In Proceedings of the 7th
Conf. of the Association for Machine Translation in the
Americas (AMTA 2006), Cambridge, MA.
Wael Salloum and Nizar Habash. 2011. Dialectal to stan-
dard Arabic paraphrasing to improve Arabic-English
statistical machine translation. In Proceedings of the
2011 Conference of Empirical Methods in Natural
Language Processing, Edinburgh, Scotland, UK.
Hassan Sawaf. 2010. Arabic dialect handling in hybrid
machine translation. In Proceedings of the 9th Conf. of
the Association for Machine Translation in the Ameri-
cas (AMTA 2010), Denver, Colorado.
Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008. A
new string-to-dependency machine translation algo-
rithm with a target dependency language model. In
Proceedings of the 46th Annual Meeting of the As-
sociation for Computational Linguistics (ACL), pages
577?585, Columbus, Ohio.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea
Micciulla, and Ralph Weischedel. 2006. A study of
translation error rate with targeted human annotation.
In Proceedings of the 7th Conf. of the Association for
Machine Translation in the Americas (AMTA 2006),
pages 223?231, Cambridge, MA.
58
Omar F. Zaidan and Chris Callison-Burch. 2011a.
The Arabic online commentary dataset: an annotated
dataset of informal Arabic with high dialectal content.
In Proceedings of the 49th Annual Meeting of the As-
sociation for Computational Linguistics: Human Lan-
guage Technologies, pages 37?41, Portland, Oregon,
June.
Omar F. Zaidan and Chris Callison-Burch. 2011b.
Crowdsourcing translation: Professional quality from
non-professionals. In Proceedings of the 49th Annual
Meeting of the Association for Computational Lin-
guistics: Human Language Technologies, pages 1220?
1229, Portland, Oregon, June.
59
2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 528?532,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Trait-Based Hypothesis Selection For Machine Translation
Jacob Devlin and Spyros Matsoukas
Raytheon BBN Technologies, 10 Moulton St, Cambridge, MA 02138, USA
{jdevlin,smatsouk}@bbn.com
Abstract
In the area of machine translation (MT) sys-
tem combination, previous work on generat-
ing input hypotheses has focused on varying a
core aspect of the MT system, such as the de-
coding algorithm or alignment algorithm. In
this paper, we propose a new method for gen-
erating diverse hypotheses from a single MT
system using traits. These traits are simple
properties of the MT output such as ?aver-
age output length? and ?average rule length.?
Our method is designed to select hypotheses
which vary in trait value but do not signif-
icantly degrade in BLEU score. These hy-
potheses can be combined using standard sys-
tem combination techniques to produce a 1.2-
1.5 BLEU gain on the Arabic-English NIST
MT06/MT08 translation task.
1 Introduction
In Machine Translation (MT), the output from mul-
tiple decoding systems can be used to create a new
output which is better than any single input system,
using a procedure known as system combination.
Normally, the input systems are generated by
varying some important aspect of the MT system,
such as the alignment algorithm (Xu and Rosti,
This work was supported by DARPA/I2O Contract No.
HR0011-12-C-0014 under the BOLT program (Approved for
Public Release, Distribution Unlimited). The views, opinions,
and/or findings contained in this article are those of the au-
thor and should not be interpreted as representing the official
views or policies, either expressed or implied, of the Defense
Advanced Research Projects Agency or the Department of De-
fense.
2010) or tokenization algorithm (de Gispert et al,
2009). Unfortunately, creating novel algorithms to
perform some important aspect of MT decoding is
obviously quite challenging. Thus, it is difficult to
increase the number of input systems in a meaning-
ful way.
In this paper, we show it is possible to create
diverse input hypotheses for combination without
making any algorithmic changes. Instead, we use
traits, which are very simple attributes of the MT
output, such as ?output length? and ?average rule
length.? Our basic procedure is to intelligently se-
lect hypotheses from our decoding forest which vary
in trait value, but have minimal BLEU degradation
compared to our baseline. We then combine these
to produce a substantial gain. Note that all of the
hypotheses are generated from a single decode of a
single input system.
Additionally, our method is completely compati-
ble with multi-system combination, since our proce-
dure can be applied to each input system, and then
these systems can be combined as normal.
Methods for automatically creating diverse hy-
potheses from a single system have been explored
in speech recognition (Siohan et al, 2005), but we
know of no analogous work applied to machine
translation. Our procedure does share some surface
similarities with techniques such as variational de-
coding (VD) (Li et al, 2009), but the goal in those
techniques is to find output which is consistent with
the entire forest, rather than to select hypotheses
with particular attributes. In fact, VD can be applied
in conjunction by running VD on the rescored forest
528
for each trait condition.1
2 Description of MT System
Our machine translation system is a string-to-
dependency hierarchical decoder based on (Shen et
al., 2008) and (Chiang, 2007). Bottom-up chart
parsing is performed to produce a shared forest of
derivations. The decoder uses a log-linear transla-
tion model, so the score of derivation d is defined
as:
Sd(~w) =
m?
i=1
wi
?
r?R(d)
Fri (1)
where R(d) is the set of translation rules that make
up derivation d, m is the number of features, Fri
is the score of the ith feature in rule r, and wi is
the weight of feature i. This weight vector is opti-
mized discriminatively to maximize BLEU score on
a tuning set, using the Expected-BLEU optimization
procedure (Rosti et al, 2010).
Our decoder uses all of the standard statistical MT
features, such as the language model, rule probabil-
ities, and lexical probabilities. Additionally, we use
50,000 sparse, binary-valued features such as ?Is the
bi-gram ?united states? present in the output??, based
on (Chiang et al, 2009). We use a 3-gram LM for
decoding and a 5-gram LM for rescoring.
3 Trait Features
An MT trait represents a high-level property of the
MT output.
The traits used in this paper are:
? Null Source Words ? The percentage of source
content words which align to null, i.e., are not
translated.
? Source Reorder ? The percentage of source
terminals/non-terminals which cross alignment
links inside their decoding rule.
? Ngram Frequency ? The percentage of target 3-
grams which are seen more than 10 times in the
monolingual training.
? Rule Frequency ? The percentage of rules
which are seen more than 3 times in the par-
allel training.
1We do not use VD here because we have not found it to be
beneficial to our system.
? Rule Length ? The average number of target
words per rule.
? Output Length ? The ratio of the number of tar-
get words in the MT output divided by the num-
ber of source words in the input.
? High Lex Prob ? The percentage of source
words which have a lexical translation proba-
bility greater than 0.1.
Each trait can be represented as the ratio of two lin-
ear decoding features. For example, for the Output
Length trait, the ?numerator? feature is the number
of target words in the hypothesis, while the ?denom-
inator? feature is the number of source words in the
input sentence. We can sum these feature scores
over a test set, and the resulting quotient is the Out-
put Length for that set.
Intuitively, each trait is associated with a par-
ticular tradeoff, such as fluency/adequacy or preci-
sion/recall. For example, when MT performance is
maximized, shorter output tends to have higher pre-
cision but lower recall than longer output. For the
Ngram Frequency trait, a greater percentage of high-
frequency n-grams tends to result in more fluent but
less adequate output. Similar intuitive justifications
should be evident for the remaining traits.
4 Hypothesis Generation
The main goal of this work is to generate additional
hypotheses which vary in trait values, while mini-
mizing degradation to the BLEU score. So, imagine
that we have some baseline MT output. Then, we
want to generate a second set of hypotheses which
have maximal BLEU score, subject to the constraint
that the output must be 5% shorter.2
The question then becomes how to figure out
which 5% of words should be removed. Rather than
attempting to do this with a new algorithm, we sim-
ply let our existing MT models do it for us, using
our standard optimization procedure. This is the es-
sential purpose of the trait features ? using the Out-
put Length feature, the optimizer has a ?knob? with
which it can control the trait value independently
of everything else.3 Thus, the new hypotheses that
2Note that the trait value is always aggregated over the entire
set, and not computed sentence-by-sentence.
3A feature representing the number of words already exists
in our baseline system, but no such feature exists for the other 6
529
we select are ?optimal? in terms of our existing MT
model probabilities, but have trait values which vary
from the baseline in a precise way.
4.1 Optimization Function
Our normal optimization procedure uses n-
best-based Expected-BLEU tuning (Rosti et al,
2010), which is a differentiable approximation of
Maximum-BLEU tuning. To ?target? a particular
trait value, we add a second term equal to the
squared error between the current trait value and
the target trait value. Our modified optimization
function which we seek to maximize is then:
Obj(~w) = ExpBLEU(~w)? ?
(
N(~w)
D(~w)
? ??
)2
where ~w is the MT feature weight vector, ? is the
weight of the trait term, ? is the baseline value of
the trait, and ? is the ?target? trait multiplier, N(~w)
is the expected-value of the numerator feature, and
D(~w) is the expected value of the denominator fea-
ture.
To give an example, imagine that for our baseline
tune set the Output Length ratio is 1.2, and we want
to create a hypothesis set with 5% fewer words. In
that case, we would set ? = 1.2 and ? = 0.95, so the
target trait value is 1.14. We fix the free parameter
? to 10, which forces the optimized trait value to be
very close to the target.4
The trait-value functions N(~w) and D(~w) are
computed as standard expected value functions, e.g.:
N(~w) =
?
i
?
j
pij(~w)Nij
where pij(~w) is the posterior probability of the jth
hypothesis of sentence i, and Nij is the value of the
numerator feature for hypothesis ij.5
4.2 Meta-Optimization
It is somewhat problematic to use a fixed multiplier
? on all of the traits, since on some traits it may
cause a larger degradation than others. So, we take
the reverse approach ? for some targeted BLEU loss
traits.
4Note that theExpBLEU(~w) is raw BLEU not BLEU per-
centage, i.e., it?s 0.4528 not 45.28
5pij(~w) is computed the same way as in ExpBLEU(~w).
See (Rosti et al, 2010) for details.
?, we find the maximum (or minimum) value of ?
which causes a loss no greater than ?, as computed
on a held-out portion of the tune set.6 Here, we find
the maximum and minimum trait value for ? = 0.5
and ? = 2.0, resulting in 4 sets of weights per trait.
We can find the optimal ? for each ? by perform-
ing a binary search on ? , where we run our optimiza-
tion procedure and then compute the BLEU loss at
each iteration.
4.3 Forest-Based Optimization
Since we have 7 traits, and we generate 4 sets of
weights per trait, we have 28 ?systems? to combine.
Obviously, running 28 full decodes on each new test
sentence is highly undesirable.
We resolve this issue by using our baseline deriva-
tion forest for both optimization and hypothesis gen-
eration. We perform a single round of decoding to
generate a forest, and then perform iterative n-best
optimization by rescoring the forest rather than re-
decoding from scratch. 7 We constrain the 50,000
sparse feature weights to be fixed at their baseline
values, to prevent over-fitting.
Once the weight sets are generated, the hypothe-
ses for each trait condition can be generated by
rescoring the forest inside of the decoder. Therefore,
all 28 trait hypotheses can be generated for almost
no cost over a single decode.
It should be noted we have found it beneficial to
relax our MT pruning parameters in order to cre-
ate a larger forest. This results in decoding which
is roughly 2x-3x as slow as the baseline, and re-
quires storing the larger forest in memory. However,
we have found that the procedure still works well
even with the standard pruning parameters. Addi-
tionally, we are investigating methods for diversify-
ing the forest with less of a slowdown to decoding.
5 Combination
Once the different trait hypotheses have been gen-
erated, system combination can be performed using
any method.
Here, we use a confusion network decoder based
on (Rosti et al, 2010). The basic procedure is to
6For example if the held-out baseline BLEU is 40.0 and ? =
0.5, the BLEU after trait optimization can be no less than 39.5.
7Forest-based optimization such as (Pauls et al, 2009) could
be used instead.
530
select one hypothesis as the ?skeleton? and then in-
crementally align the remaining hypotheses to create
a confusion network. The confusion network is de-
coded using an arc-level confidence score for each
input system and a language model, the weights for
which are estimated discriminatively to maximize
BLEU.
6 Results
We present MT results in Table 1. Our experimen-
tal setup is compatible with the NIST MT08 con-
strained track. We trained our translation model on
35 million words of parallel data and our language
model on 3.8 billion words of monolingual data. We
use a portion of MT02-05 for tuning the MT baseline
and the trait systems, and another portion of MT02-
05 for tuning system combination.
We present results on Arabic-English MT06-
newswire and MT08-eval. The systems were tuned
and evaluated using IBM-BLEU. Our baseline sys-
tem is 1.5 BLEU better than the best result from the
NIST M08 evaluation.
For the Trait Feats condition, we simply added the
numerator and denominator features for all 7 traits to
the baseline system and re-optimized.8 Somewhat
surprisingly, this produces an 0.5-0.7 BLEU gain on
its own. In this condition, although we do not target
any particular trait values, the optimizer will natu-
rally fine-tune the trait values to whatever is optimal
for BLEU score. For example, the MT08 baseline
value of Source Reorder is 0.307, while for the Trait
Feats it is 0.330, so the system determined it is ?op-
timal? to have 7.5% (0.330/0.307) more re-ordering
than the baseline.
For the Trait Comb condition, we generated 28
trait hypothesis sets using the decoding forest from
the Trait Feats condition. We combined these with
the Trait Feats output using consensus network de-
coding. This produces an additional 0.8 BLEU gain,
resulting in a 1.2-1.5 BLEU gain over the baseline.
We also present another condition, n-best Comb,
where we perform confusion network combination
on the 28-best hypotheses from Trait Feats. This
represents the simplest and most trivial method of
hypothesis selection. We observe no gain in BLEU
on this condition. Other simple methods of hy-
8Including the 50k sparse features.
potheses selection, such as optimizing systems to be
?different? from one another (i.e., have high inter-
system TER), also produced no gain over the single
system. We include these results simply to demon-
strate that it is not trivial to select hypotheses from a
single system which produce a significant improve-
ment in from system combination.
MT06 nw MT08 eval
BLEU Len BLEU Len
Baseline 55.11 99.1% 46.75 96.1%
Trait Feats 55.79? 99.3% 47.23? 96.0%
+n-best Comb 55.65 99.3% 47.24 96.2%
+Trait Comb 56.65?? 99.3% 48.00?? 96.2%
Table 1: Results on Arabic-English MT. ? = Significant
improvement at 95% confidence, as defined by (Koehn,
2004). ?? = Significant improvement at 99.9% confi-
dence. BLEU = IBM-BLEU score. Len = Hypothesis-
to-reference length ratio.
7 Conclusions and Future Work
We demonstrated a method of intelligently select-
ing hypotheses from a decoding forest which can be
combined with the baseline hypotheses to produce
a significant gain in BLEU score. In the future, we
plan to explore more trait types and alternate meth-
ods of system combination.
One possible application of this work is in fielded
translation systems. Because our method produces
high-quality complementary hypotheses at a low
computational cost, the system could present these
to the user as alternate translations. Going further,
a user could prefer a particular output type, such as
the fluency-tuned condition, and set that to be their
default translation.
The major open question is how our trait-based
combination interacts with multi-system combina-
tion. Imagine there are three different types of de-
coders which can be combined to produce some gain
in the baseline condition. If you independently im-
prove all three using trait-based combination, will
the relative gain from multi-system combination be
reduced? Or can you jointly combine all of the trait
hypotheses and get an even greater relative gain? We
plan to thoroughly explore this in the future.
531
References
D. Chiang, K. Knight, and W. Wang. 2009. 11,001 new
features for statistical machine translation. In NAACL,
pages 218?226.
D. Chiang. 2007. Hierarchical phrase-based translation.
Computational Linguistics, 33(2):201?228.
A. de Gispert, S. Virpioja, M. Kurimo, and W. Byrne.
2009. Minimum Bayes risk combination of translation
hypotheses from alternative morphological decompo-
sitions. In NAACL, pages 73?76.
P. Koehn. 2004. Pharaoh: A beam search decoder
for phrase-based statistical machine translation mod-
els. In AMTA, pages 115?124.
Z. Li, J. Eisner, and S. Khudanpur. 2009. Variational
decoding for statistical machine translation. In ACL,
pages 593?601.
A. Pauls, J. DeNero, and D. Klein. 2009. Consensus
training for consensus decoding in machine transla-
tion. In EMNLP, pages 1418?1427.
A. Rosti, B. Zhang, S. Matsoukas, and R. Schwartz.
2010. BBN system description for WMT10 system
combination task. In WMT/MetricsMATR, pages 321?
326.
L. Shen, J. Xu, and R. Weischedel. 2008. A new string-
to-dependency machine translation algorithm with a
target dependency language model. In ACL-HLT,
pages 577?585.
O. Siohan, B. Ramabhadran, and B. Kingsbury. 2005.
Constructing ensembles of ASR systems using ran-
domized decision trees. In ICASSP.
J. Xu and A. Rosti. 2010. Combining unsupervised and
supervised alignments for MT: An empirical study. In
EMNLP, pages 667?673.
532
Proceedings of NAACL-HLT 2013, pages 612?616,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Systematic Comparison of Professional and Crowdsourced Reference
Translations for Machine Translation
Rabih Zbib, Gretchen Markiewicz, Spyros Matsoukas,
Richard Schwartz, John Makhoul
Raytheon BBN Technologies
Cambridge, MA 02138, USA
{rzbib,gmarkiew,smatsouk,schwartz,makhoul}@bbn.com
Abstract
We present a systematic study of the effect of
crowdsourced translations on Machine Trans-
lation performance. We compare Machine
Translation systems trained on the same data
but with translations obtained using Amazon?s
Mechanical Turk vs. professional translations,
and show that the same performance is ob-
tained from Mechanical Turk translations at
1/5th the cost. We also show that adding a Me-
chanical Turk reference translation of the de-
velopment set improves parameter tuning and
output evaluation.
1 Introduction
Online crowdsourcing services have been shown to
be a cheap and effective data annotation resource
for various Natural Language Processing (NLP)
tasks (Callison-Burch and Dredze, 2010; Zaidan and
Callison-Burch, 2011a; Zaidan and Callison-Burch,
2011b). The resulting quality of annotations is high
enough to be used for training statistical NLP mod-
els, with a saving in cost and time of up to an or-
der of magnitude. Statistical Machine Translation
(SMT) is one of the NLP tasks that can benefit from
crowdsourced annotations. With appropriate quality
control mechanisms, reference translations collected
by crowdsourcing have been successfully used for
training and evaluating SMT systems (Zbib et al,
2012; Zaidan and Callison-Burch, 2011b).
In this work, we used Amazon?s Mechanical Turk
(MTurk) to obtain alternative reference translations
of four Arabic-English parallel corpora previously
released by the Linguistic Data Consortium (LDC)
for the DARPA BOLT program. This data, totaling
over 500K Arabic tokens, was originally collected
from web discussion forums and translated profes-
sionally to English. We used alternative MTurk
translations of the same data to train and evalua-
tion MT systems; and conducted the first systematic
study that quantifies the effect of the reference trans-
lation process on MT output. We found that:
? Mechanical Turk can be used to translate
enough data for training an MT system at
1/10th the price of professional translation, and
at a much faster rate.
? Training MT systems on MTurk reference
translations gives the same performance as
training with professional translations at 20%
of the cost.
? A second translation of the development set ob-
tained via MTurk improves parameter tuning
and output evaluation.
2 Previous Work
There have been several publications on crowd-
sourcing data annotation for NLP. Callison-Burch
and Dredze (2010) give an overview of the NAACL-
2010 Workshop on using Mechanical Turk for data
annotation. They describe tasks for which MTurk
can be used, and summarize a set of best practices.
They also include references to the workshop con-
tributions.
Zaidan and Callison-Burch (2011a) created a
monolingual Arabic data set rich in dialectal con-
tent from user commentaries on newspaper web-
sites. They hired native Arabic speakers on MTurk
612
to identify the dialect level and used the collected la-
bels to train automatic dialect identification systems.
They did not translate the collected data, however.
Zaidan and Callison-Burch (2011b) obtained mul-
tiple translations of the NIST 2009 Urdu-English
evaluation set using MTurk. They trained a statis-
tical model on a set of features to select among the
multiple translations. They showed that the MTurk
translations selected by their model approached the
range of quality of professional translations, and that
the selected MTurk translations can be used reliably
to score the outputs of different MT systems submit-
ted to the NIST evaluation. Unlike our work, they
did not investigate the use of crowdsourced trans-
lations for training or parameter tuning. Zbib et al
(2012) trained a Dialectal Arabic to English MT sys-
tem using Mechanical Turk translations. But the
data they translated on MTurk does not have profes-
sional translations to conduct the systematic com-
parison we do in this paper.
It is well known that scoring MT output against
multiple references improves MT scores such as
BLEU significantly, since it increases the chance of
matching n-grams between the MT output and the
references. Tuning system parameter with multi-
ple references also improves machine translation for
the same reason Madnani et al (2007) and Madnani
et al (2008) showed that tuning on additional ref-
erences obtained by automatic paraphrasing helps
when only few tuning references are available.
3 Data Translation
The data we used are Arabic-English parallel cor-
pora released by the LDC for the DARPA BOLT
Phase 1 program1. The data was collected from
Egyptian online discussion forums, and consists of
separate discussion threads, each composed of an
initial user posting and multiple reply postings. The
data tends to be bimodal: the first posting in the
thread is often formal and expressed in Modern
Standard Arabic, while the subsequent threads use
a less formal style, and contain colloquial Egyptian
dialect. The data was manually segmented into sen-
tence units, and translated professionally.
We used non-professional translators hired on
MTurk to get second translations. We used several
1Corpora: LDC2012E15, LDC2012E19, LDC2012E55
measures to control the quality of translations and
detect cheaters. Those include the rendering of Ara-
bic sentences as images, comparing the output to
Google Translate and Bing Translator, and other au-
tomatic checks. The quality of individual worker?s
translations was quantified by asking a native Ara-
bic speaker judge to score a sample of the Turker?s
translations. The translation task unit (aka Human
Intelligence Task or HIT) consisted of a sequence
of contiguous sentences from a discussion thread
amounting to between 40 and 60 words. The in-
structions were simply to translate the Arabic source
fully and accurately, and to take surrounding sen-
tence segments into account to help resolve ambigu-
ities. The HIT rewards were set to 2.5? per word.
At the end of the effort, we had 26 different work-
ers translate 567K Arabic tokens in 4 weeks. The
resulting translations were less fluent than their pro-
fessional counterparts, and 10% shorter on average.
The following section presents results of MT exper-
iments using the MTurk translations.
4 MT Experiments
The MT system used is based on a string-to-
dependency-tree hierarchical model of Shen et
al. (2008). Sentence alignment was done using
GIZA++ (Och and Ney, 2003). Decoder fea-
tures include translation probabilities, smoothed lex-
ical probabilities, and a dependency tree language
model. Additionally, we used 50,000 sparse, binary-
valued source and target features based on Chiang
et al (2009). The English language model was
trained on 7 billion words from the LDC Gigaword
corpus and from a web crawl. We used expected
BLEU maximization (Devlin, 2009) to tune feature
weights.
We defined a tuning set (3581 segments, 43.3K
tokens) and a test set (4166 segments, 47.7K to-
kens) using LDC2012E30, the corpus designated
as a development set by the LDC, augmented with
around 50K Words held out from LDC2012E15
and LDC2012E19, to make a development set large
enough to tune the large number of feature weights2.
The remaining data was used for training. We de-
fined three nested training sets containing 100K,
200K and 400K Arabic tokens respectively, with
2Only full forum threads were held out
613
Training Web-forum Only Newswire(10MW)+Web-forum
100KW 200KW 400KW 0KW 100KW 200KW 400KW
Prof. refs 17.71 20.23 22.61 22.82 24.05 24.85 25.19
MTurk refs 16.41 18.43 20.08 22.82 23.79 24.20 24.51
Two Training refs 19.03 21.19 23.06 22.82 24.26 25.19 25.38
Add?l Training data - 19.80 21.53 22.82 - 24.31 25.16
Table 1: Comparison of the effect of web forum training data when using professional and MTurk reference transla-
tions. All results use professional references for the tuning and test sets.
two versions of each set: one with the professional
reference translations for the target, and the other
with the same source data, but the MTurk transla-
tions. We defined two versions of the test and tuning
sets similarly. We report translation results in terms
of lower-case BLEU scores (Papineni et al, 2002).
4.1 Training Data References
We first study the effect of training data refer-
ences, varying the amount of training data and type
of translations, while using the same professional
translation references for tuning and scoring. The
first set of baseline experiments were trained on
web forum data only, using professional transla-
tions. The first line of Table 1 shows that doubling of
the training data adds 2.5 then 2.3 BLEU points. We
repeated the experiments, but with MTurk training
references, and saw that the scores are lower by 1.3-
2.5 BLEU points, depending on the size of training
data, and that the gain obtained from doubling the
training data decreases to 2.0 and 1.6 BLEU points.
The lower MT scores and slower learning curve of
the MTurk systems are both due to the lower quality
of the translations, and to the mismatch with the pro-
fessional development set translations (we discuss
this issue further in ?4.3). However, by interpolation
of the MT scores, we find that the same MT perfor-
mance can be obtained by using twice the amount of
MTurk translated data as professional data. Consid-
ering that the MTurk translations is 10 times cheaper
than professional translations (2.5? versus 25-30?),
this constitutes a cost ratio of 5x.
We repeated the above experiments, but this time
added 10 million words of parallel data from the
NIST MT 2012 corpora (mostly news) for training.
We weighted the web forum part of the training data
by a factor of 5. Note from the results in the right
half of Table 1 that the newswire data improves the
BLEU score by 2.5 to 6.3 BLEU points, depend-
ing on the size of the web forum data. This signif-
icant improvement is because some of the web fo-
rum user postings are formal and written in MSA
(?3). More relevant to our aims is the comparison
when we vary the web forum training references in
the presence of the newswire training. The differ-
ence between the MTurk translation systems and the
professional translation drops to 0.26-0.68 points.
We conclude that in a domain adaptation scenario,
where out-of-domain training data (i.e. newswire)
already exists, crowdsourced translations for the in-
domain (i.e. web forum) training data can be used
with little to no loss in MT performance.
4.2 More Data vs. Multiple Translations
To our knowledge no previous work has compared
using multiple reference translations for training
data versus using additional training data of the same
size. We studied this question by using both transla-
tions on the target side of the training data. Using the
MTurk translations in addition to the professional
translations in training gave a gain of 0.4 to 1.3
BLEU points (bottom half of Table 1). The gain was
smaller in the presence of the GALE newswire data.
When we compared with using the same amount of
different training data instead of multiple references,
we saw that training on new data with crowdsourced
translations is better: training on two translations of
100KW gives 19.03, compared to 19.80 when train-
ing on a single translation of 200KW. The advantage
of different-source data drops to 0.34 points when
we start with 200KW. With a larger initial corpus,
the additional source coverage of new data is not as
critical, and the advantage of more variety on the
target-side of the extracted translation rules becomes
more competitive. This coverage is even less criti-
cal in the presence of the news data, where the ad-
614
Training Tuning Test Training Data Size
100KW 200KW 400KW 400KW(no lex)
Prof. Prof. Prof. 17.71 20.23 22.61 20.01
Prof. Prof. Prof.+MTurk 22.53 25.75 28.38 25.42
Prof. Prof. (len=0.95) Prof.+MTurk 23.63 26.84 29.54 26.17
Prof. Prof.+MTurk Prof.+MTurk 25.26 28.44 30.94 27.22
MTurk MTurk MTurk 16.66 18.47 20.35 17.75
MTurk MTurk Prof.+MTurk 23.83 26.45 28.66 25.44
MTurk MTurk (len=1.05) Prof.+MTurk 23.73 26.19 28.74 25.87
MTurk Prof.+MTurk Prof.+MTurk 24.91 27.66 29.78 26.45
Table 2: Effect of Tuning and Scoring References on MT.
vantage of new web forum source data disappears
(lower-right quadrant of Table 1).
4.3 Development Data References
So far, we have focused on varying training data
conditions, and kept the tuning and evaluation con-
ditions fixed. But since we have re-translated the
tuning and test sets on MTurk as well, we can study
the effect of their reference translations on MT. As
Table 2 shows, scoring the MT output using both
reference translations, the BLEU scores increase by
over 5 points (and more for the MTurk-trained sys-
tem). This increase by itself is not remarkable. What
is important to note is that the gain obtained by dou-
bling the amount of training data is larger when mea-
sured using the multiple reference test set. We also
ran experiments with 400KW training data, but with
the lexical smoothing features (Koehn et al, 2003;
Devlin, 2009) turned off. The bigger gains show that
improvements in the MT output (from additional
training or new features) can be better measured us-
ing a second MTurk reference of the test set.
Finally, we study the effect of tuning the system
parameters using both translation references. Look-
ing at the system trained on the professional trans-
lations, we see a gain of 2.5 to 2.7 BLEU points
from adding the MTurk references to the tuning set.
But as we mentioned earlier, the MTurk transla-
tions are shorter than the professional translations
by around 10% on average. Tuning on both ref-
erences, therefore, shortens the system output by
around 5%. To neutralize the effect of length mis-
match, we compared to a fairer baseline tuned on
the professional references only, but we tuned the
output-to-reference length ratio to be 0.95 (thus pro-
ducing a shorter output). In this case, we see a gain
of 1.4 points from adding the MTurk references to
the tuning set.
We also used the multiple-reference tuning set
to retune the systems trained on MTurk transla-
tions. Comparing that to a baseline that is tuned and
scored using MTurk references only, we see a gain
of around 1%. Note, however, that in this case the
length mismach is reversed, and the output of the
multiple-reference system is around 5% longer than
that of the baseline. If we compare with a baseline
that is tuned with a length ratio of 1.05 (to produce a
longer output), we see the gain shrink only slightly.
To sum up this section, a second set of refer-
ence translations obtained via MTurk makes mea-
surements of improvement on the test set more re-
liable. Also, a second set of references for tuning
improves the output of the MT systems trained on
either professional or MTurk references.
5 Conclusion
We compared professional and crowdsourced trans-
lations of the same data for training, tuning and scor-
ing Arabic-English SMT systems. We showed that
the crowdsourced translations yield the same MT
performance as professional translations for as lit-
tle as 20% of the cost. We also showed that a sec-
ond crowsourced reference translation of the devel-
opment set alows for a more accurate evaluation of
MT output.
Acknowledgments
This work was supported in part by DARPA/IPTO
Contract No. HR0011-12-C-0014 under the BOLT
615
Program. The views expressed are those of the au-
thors and do not reflect the official policy or position
of the Department of Defense or the U.S. Govern-
ment. Distribution Statement A (Approved for Pub-
lic Release, Distribution Unlimited).
References
Chris Callison-Burch and Mark Dredze. 2010. Creating
speech and language data with Amazon?s Mechanical
Turk. In Proceedings of the NAACL HLT 2010 Work-
shop on Creating Speech and Language Data with
Amazon?s Mechanical Turk, pages 1?12, Los Angeles,
June.
David Chiang, Kevin Knight, and Wei Wang. 2009.
11,001 new features for statistical machine translation.
In NAACL ?09: Proceedings of the 2009 Human Lan-
guage Technology Conference of the North American
Chapter of the Association for Computational Linguis-
tics, Boulder, Colorado.
Jacob Devlin. 2009. Lexical features for statistical ma-
chine translation. Master?s thesis, University of Mary-
land, December.
P. Koehn, F. J. Och, and D. Marcu. 2003. Statistical
phrase-based translation. In Proceedings of the 2003
Human Language Technology Conference of the North
American Chapter of the Association for Computa-
tional Linguistics, pages 48?54, Edmonton, Canada.
Nitin Madnani, Necip Fazil, Ayan, Philip Resnik, and
Bonnie Dorr. 2007. Using paraphrases for parameter
tuning in statistical machine translation. In Proceed-
ings of the Second Workshop on Statistical Machine
Translation, pages 120?127, Prague, Czech Republic.
Association for Computational Linguistics.
Nitin Madnani, Philip Resnik, Bonnie Dorr, and Richard
Schwartz. 2008. Are multiple reference transla-
tions necessary? investigating the value of paraphrased
reference translations in parameter optimization. In
Proceedings of the 8th Conf. of the Association for
Machine Translation in the Americas (AMTA 2008),
Waikiki, Hawaii, USA.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19?51.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic evalua-
tion of machine translation. In Proceedings of the 40th
Annual Meeting of the Association for Computational
Linguistics (ACL), Philadelphia, PA.
Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008. A
new string-to-dependency machine translation algo-
rithm with a target dependency language model. In
Proceedings of the 46th Annual Meeting of the As-
sociation for Computational Linguistics (ACL), pages
577?585, Columbus, Ohio.
Omar F. Zaidan and Chris Callison-Burch. 2011a.
The Arabic online commentary dataset: an annotated
dataset of informal Arabic with high dialectal content.
In Proceedings of the 49th Annual Meeting of the As-
sociation for Computational Linguistics: Human Lan-
guage Technologies, pages 37?41, Portland, Oregon,
June.
Omar F. Zaidan and Chris Callison-Burch. 2011b.
Crowdsourcing translation: Professional quality from
non-professionals. In Proceedings of the 49th Annual
Meeting of the Association for Computational Lin-
guistics: Human Language Technologies, pages 1220?
1229, Portland, Oregon, June.
Rabih Zbib, Erika Malchiodi, Jacob Devlin, David
Stallard, Spyros Matsoukas, Richard Schwartz, John
Makhoul, Omar F. Zaidan, and Chris Callison-Burch.
2012. Machine translation of arabic dialects. In The
2012 Conference of the North American Chapter of the
Association for Computational Linguistics, Montreal,
June. Association for Computational Linguistics.
616
Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 321?326,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
BBN System Description for WMT10 System Combination Task
Antti-Veikko I. Rosti and Bing Zhang and Spyros Matsoukas and Richard Schwartz
Raytheon BBN Technologies, 10 Moulton Street, Cambridge, MA 02138, USA
{arosti,bzhang,smatsouk,schwartz}@bbn.com
Abstract
BBN submitted system combination out-
puts for Czech-English, German-English,
Spanish-English, French-English, and All-
English language pairs. All combinations
were based on confusion network decod-
ing. An incremental hypothesis alignment
algorithm with flexible matching was used
to build the networks. The bi-gram de-
coding weights for the single source lan-
guage translations were tuned directly to
maximize the BLEU score of the decod-
ing output. Approximate expected BLEU
was used as the objective function in gra-
dient based optimization of the combina-
tion weights for a 44 system multi-source
language combination (All-English). The
system combination gained around 0.4-
2.0 BLEU points over the best individual
systems on the single source conditions.
On the multi-source condition, the system
combination gained 6.6 BLEU points.
1 Introduction
The BBN submissions to the WMT10 system
combination task were based on confusion net-
work decoding. The confusion networks were
built using the incremental hypothesis alignment
algorithm with flexible matching introduced in the
BBN submission for the WMT09 system combi-
nation task (Rosti et al, 2009). This year, the
system combination weights were tuned to max-
imize the BLEU score (Papineni et al, 2002) of
the 1-best decoding output (lattice based BLEU
tuning) using downhill simplex method (Press et
al., 2007). A 44 system multi-source combina-
tion was also submitted. Since the gradient-free
optimization algorithms do not seem to be able to
handle more than 20-30 weights, a gradient ascent
to maximize an approximate expected BLEU ob-
jective was used to optimize the larger number of
weights.
The lattice based BLEU tuning may be imple-
mented using any optimization algorithm that does
not require the gradient of the objective function.
Due to the size of the lattices, the objective func-
tion evaluation may have to be distributed to mul-
tiple servers. The optimizer client accumulates the
BLEU statistics of the 1-best hypotheses from the
servers for given search weights, computes the fi-
nal BLEU score, and passes it to the optimiza-
tion algorithm which returns a new set of search
weights. The lattice based tuning explores the en-
tire search space and does not require multiple de-
coding iterations with N -best list merging to ap-
proximate the search space as in the standard min-
imum error rate training (Och, 2003). This allows
much faster turnaround in weight tuning.
Differentiable approximations of BLEU have
been proposed for consensus decoding. Tromble
et al (2008) used a linear approximation and Pauls
et al (2009) used a closer approximation called
CoBLEU. CoBLEU is based on the BLEU for-
mula but the n-gram counts are replaced by ex-
pected counts over a translation forest. Due to the
min-functions required in converting the n-gram
counts to matches and a non-differentiable brevity
penalty, a sub-gradient ascent must be used. In
this work, an approximate expected BLEU (Exp-
BLEU) defined over N -best lists was used as a
differentiable objective function. ExpBLEU uses
expected BLEU statistics where the min-function
is not needed as the statistics are computed off-
line and the brevity penalty is replaced by a dif-
ferentiable approximation. The ExpBLEU tun-
ing yields comparable results to direct BLEU tun-
ing using gradient-free algorithms on combina-
tions of small number of systems (fewer than 20-
30 weights). Results on a 44 system combination
show that the gradient based optimization is more
robust with larger number of weights.
321
This paper is organized as follows. Section
2 reviews the incremental hypothesis alignment
algorithm used to built the confusion networks.
Decoding weight optimization using direct lattice
1-best BLEU tuning and N -best list based Exp-
BLEU tuning are presented in Section 3. Exper-
imental results on combining single source lan-
guage to English outputs and all 44 English out-
puts are detailed in Section 4. Finally, Section 5
concludes this paper with some ideas for future
work.
2 Hypothesis Alignment
The confusion networks were built by using the
incremental hypothesis alignment algorithm with
flexible matching introduced in Rosti et al (2009).
The algorithm is reviewed in more detail here. It
is loosely related to the alignment performed in
the calculation of the translation edit rate (TER)
(Snover et al, 2006) which estimates the edit
distance between two strings allowing shifts of
blocks of words in addition to insertions, dele-
tions, and substitutions. Calculating an exact TER
for strings longer than a few tokens1 is not compu-
tationally feasible, so the tercom2 software uses
heuristic shift constraints and pruning to find an
upper bound of TER. In this work, the hypothe-
ses were aligned incrementally with the confusion
network, thus using tokens from all previously
aligned hypotheses in computing the edit distance.
Lower substitution costs were assigned to tokens
considered equivalent and the heuristic shift con-
straints of tercom were relaxed3.
First, tokens from all hypotheses are put into
equivalence classes if they belong to the same
WordNet (Fellbaum, 1998) synonym set or have
the same stem. The 1-best hypothesis from each
system is used as the confusion network skeleton
which defines the final word order of the decod-
ing output. Second, a trivial confusion network
is generated from the skeleton hypothesis by gen-
erating a single arc for each token. The align-
ment algorithm explores shifts of blocks of words
that minimize the edit distance between the cur-
rent confusion network and an unaligned hypothe-
1Hypotheses are tokenized and lower-cased prior to align-
ment. Tokens generally refer to words and punctuation.
2http://www.cs.umd.edu/?snover/tercom/
current version 0.7.25.
3This algorithm is not equivalent to an incremental TER-
Plus (Snover et al, 2009) due to different shift constraints and
the lack of paraphrase matching
30 1cat(1) 2sat(1) mat(1)
(a) Skeleton hypothesis.
40 1cat(1,1) 2sat(1,1) 3on(0,1)NULL(1,0) mat(1,1)
(b) Two hypotheses (insertion).
40 1cat(1,1,0)NULL(0,0,1) 2sat(1,1,1) 3on(0,1,0)NULL(1,0,1) mat(1,1,1)
(c) Three hypotheses (deletion).
40 1cat(1,1,0,1)NULL(0,0,1,0) 2sat(1,1,1,1) 3on(0,1,0,0)NULL(1,0,1,1) mat(1,1,1,0)hat(0,0,0,1)
(d) Four hypotheses (substitution).
Figure 1: Example of incrementally aligning ?cat
sat mat?, ?cat sat on mat?, ?sat mat?, and ?cat sat
hat?.
sis. Third, the hypothesis with the lowest edit dis-
tance to the current confusion network is aligned
into the network. The heuristically selected edit
costs used in the WMT10 system were 1.0 for
insertions, deletions, and shifts, 0.2 for substitu-
tions of tokens in the same equivalence class, and
1.0001 for substitutions of non-equivalent tokens.
An insertion with respect to the network always
results in a new node and two new arcs. The first
arc contains the inserted token and the second arc
contains a NULL token representing the missing
token from all previously aligned hypotheses. A
substitution/deletion results in a new token/NULL
arc or increase in the confidence of an existing to-
ken/NULL arc. The process is repeated until all
hypotheses are aligned into the network.
For example, given the following hypotheses
from four systems: ?cat sat mat?, ?cat sat on mat?,
?sat mat?, and ?cat sat hat?, an initial network in
Figure 1(a) is generated. The following two hy-
potheses have a distance of one edit from the initial
network, so the second can be aligned next. Figure
1(b) shows the additional node created and the two
new arcs for ?on? and ?NULL? tokens. The third
hypothesis has deleted token ?cat? and matches the
322
?NULL? token between nodes 2 and 3 as seen in
Figure 1(c). The fourth hypothesis matches all but
the final token ?hat? which becomes a substitution
for ?mat? in Figure 1(d). The binary vectors in
the parentheses following each token show which
system generated the token aligned to that arc. If
the systems generated N -best hypotheses, a frac-
tional increment could be added to these vectors
as in (Rosti et al, 2007). Given these system spe-
cific scores are normalized to sum to one over all
arcs connecting two consecutive nodes, they may
be viewed as system specific word arc posterior
estimates. Note, for 1-best hypotheses the scores
sum to one without normalization.
Given system outputs E = {E1, . . . , ENs},
an algorithm to build a set of Ns confusion
networks C = {C1, . . . , CNs} may be written
as:
for n = 1 to Ns do
Cn ? Init(En) {initialize confusion net-
work from the skeleton}
E ? ? E ? En {set of unaligned hypotheses}
while E ? 6= ? do
Em ? argminE?E ? Dist(E,Cn)
{compute edit distances}
Cn ? Align(Em, Cn) {align closest hy-
pothesis}
E ? ? E ? ? Em {update set of unaligned
hypotheses}
end while
end for
The set of Ns confusion networks are expanded to
separate paths with distinct bi-gram contexts and
connected in parallel into a big lattice with com-
mon start and end nodes with NULL token arcs.
A prior probability estimate is assigned to the sys-
tem specific word arc confidences connecting the
common start node and the first node in each sub-
network. A heuristic prior is estimated as:
pn =
1
Z
exp(?100
en
Nn
) (1)
where en is the total cost of aligning all hypothe-
ses when using system n as the skeleton, Nn is
the number of nodes in the confusion network be-
fore bi-gram expansion, and Z is a scaling factor
to guarantee pn sum to one. This gives a higher
prior for a network with fewer alignment errors
and longer expected decoding output.
3 Weight Optimization
Standard search algorithms may be used to find N -
best hypotheses from the final lattice. The score
for arc l is computed as:
sl = log
( Ns?
n=1
?nsnl
)
+ ?L(wl|wP (l)) + ?S(wl)
(2)
where ?n are the system weights constrained to
sum to one, snl are the system specific arc pos-
teriors, ? is a language model (LM) scaling fac-
tor, L(wl|wP (l)) is the bi-gram log-probability for
the token wl on the arc l given the token wP (l)
on the arc P (l) preceding the arc l, ? is the word
insertion scaling factor, and S(wl) is zero if wl
is a NULL token and one otherwise. The path
with the highest total score under summation is
the 1-best decoding output. The decoding weights
? = {?1, . . . , ?Ns , ?, ?} are tuned to optimize two
objective functions described next.
3.1 Lattice Based BLEU Optimization
Powell?s method (Press et al, 2007) on N -best
lists was used in system combination weight tun-
ing in Rosti et al (2007). This requires multiple
decoding iterations and merging the N -best lists
between tuning runs to approximate the full search
space as in Och (2003). To speed up the tuning
process, a distributed optimization method can be
used. The lattices are divided into multiple chunks
each of which are loaded into memory by a server.
A client runs the optimization algorithm relying
on the servers for parallelized objective function
evaluation. The client sends a new set of search
weights to the servers which decode the chunks
of lattices and return the 1-best hypothesis BLEU
statistics back to the client. The client accumulates
the BLEU statistics from all servers and computes
the final BLEU score used as the objective func-
tion by the optimization algorithm. Results similar
to Powell?s method can be obtained with fewer it-
erations by using the downhill simplex method in
multi-dimensions (Amoeba) (Press et al, 2007).
To enforce the sum to one constraint of the sys-
tem weights ?n, the search weights are restricted
to [0, 1] by assigning a large penalty if any cor-
responding search weight breaches the limits and
these restricted search weights are scaled to sum
to one before the objective function evaluation.
After optimizing the bi-gram decoding weights
directly on the lattices, a 300-best list are gener-
323
ated. The 300-best hypotheses are re-scored using
a 5-gram LM and another set of re-scoring weights
are tuned on the development set using the stan-
dard N -best list based method. Multiple random
restarts may be used in both lattice and N-best list
based optimization to decrease chances of finding
a local minimum. Twenty sets of initial weights
(the weights from the previous tuning and 19 ran-
domly perturbed weights) were used in all experi-
ments.
3.2 Approximate Expected BLEU
Optimization
The gradient-free optimization algorithms like
Powell?s method and downhill simplex work well
for up to around 20-30 weights. When the number
of weights is larger, the algorithms often get stuck
in local optima even if multiple random restarts
are used. The BLEU score for a 1-best output is
defined as follows:
BLEU =
4?
n=1
(?
i m
n
i?
i h
n
i
) 1
4
?
(
1 ?
?
i ri
?
i h
1
i
)
(3)
where mni is the number of n-gram matches be-
tween the hypothesis and reference for segment
i, hni is the number of n-grams in the hypothesis,
ri is the reference length (or the reference length
closest to the hypothesis if multiple references are
available), and ?(x) = min(1.0, ex) is the brevity
penalty. The first term in Equation 3 is a harmonic
mean of the n-gram precisions up to n = 4. The
selection of 1-best hypotheses is discrete and the
brevity penalty is not continuous, so the BLEU
score is not differentiable and gradient based op-
timization cannot be used. Given a posterior dis-
tribution over all possible decoding outputs could
be defined, an expected BLEU could be optimized
using gradient ascent. However, this posterior dis-
tribution can only be approximated by expensive
sampling methods.
A differentiable objective function over N -best
lists to approximate the BLEU score can be de-
fined using expected BLEU statistics and a con-
tinuous approximation of the brevity penalty. The
posterior probability for hypothesis j of segment i
is simply the normalized decoder score:
pij =
e?Sij
?
k e?Sik
(4)
where ? is a posterior scaling factor and Sij is the
total score of hypothesis j of segment i. The pos-
terior scaling factor controls the shape of the pos-
terior distribution: ? > 1.0 moves the probability
mass toward the 1-best hypothesis and ? < 1.0
flattens the distribution. The BLEU statistics in
Equation 3 are replaced by the expected statistics;
for example, m?ni =
?
j pijmij , and the brevity
penalty ?(x) is approximated by:
?(x) =
ex ? 1
e1000x + 1
+ 1 (5)
ExpBLEU has a closed form solution for the gra-
dient, provided the total decoder score is differen-
tiable.
The penalty used to restrict the search weights
corresponding to the system weights ?n in
gradient-free BLEU tuning is not differentiable.
For expected BLEU tuning, the search weights ?n
are unrestricted but the system weights are ob-
tained by a sigmoid transform and normalized to
sum to one:
?n =
?(?n)
?
m ?(?m)
(6)
where ?(?n) = 1/(1 + e??n).
The expected BLEU tuning is performed on N -
best lists in similar fashion to direct BLEU tuning.
Tuned weights from one decoding iteration are
used to generate a new N -best list, the new N -best
list is merged with the N -best list from the previ-
ous tuning run, and a new set of weights are op-
timized using limited memory Broyden-Fletcher-
Goldfarb-Shanno method (lBFGS) (Liu and No-
cedal, 1989). Since the posterior distribution is
affected by the size of the N -best list and differ-
ent decoding weights, the posterior scaling factor
can be set for each tuning run so that the perplex-
ity of the posterior distribution given the merged
N -best list is constant. A target perplexity of 5.0
was used in the experiments. Four iterations of
bi-gram decoding weight tuning were performed
using 300-best lists. The final 300-best list was re-
scored with a 5-gram and another set of re-scoring
weights was tuned on the development set.
4 Experimental Evaluation
System outputs for all language pairs with En-
glish as the target were combined. Unpruned En-
glish bi-gram and 5-gram language model com-
ponents were trained using the WMT10 corpora:
EuroParl, GigaFrEn, NewsCommentary,
and News. Additional six Gigaword v4 com-
ponents were trained: AFP, APW, XIN+CNA,
324
tune cz-en de-en es-en fr-en
System TER BLEU TER BLEU TER BLEU TER BLEU
worst 68.99 13.85 68.45 15.07 60.86 21.02 71.17 15.00
best 56.77 22.84 57.76 25.05 51.81 30.10 53.66 28.64
syscomb 57.31 25.11 54.97 27.75 50.46 31.54 51.35 31.16
test cz-en de-en es-en fr-en
System TER BLEU TER BLEU TER BLEU TER BLEU
worst 68.65 14.29 67.50 15.66 60.52 21.86 68.36 16.82
best 56.13 23.56 58.12 24.34 51.45 30.56 52.16 29.79
syscomb 56.89 25.12 55.60 26.38 50.33 31.59 51.36 30.16
Table 1: Case insensitive TER and BLEU scores on syscombtune (tune) and syscombtest (test)
for combinations of outputs from four source languages.
LTW, NYT, and Headlines+Datelines. In-
terpolation weights for the ten components
were tuned so as to minimize perplexity on
the newstest2009-ref.en development set.
The LMs used modified Kneser-Ney smoothing.
On the multi-source condition (xx-en) another
LM was trained from the system outputs and in-
terpolated with the general LM using an interpola-
tion weight 0.3 for the LM trained on the system
outputs. This LM is referred to as biasLM later.
A tri-gram true casing model was trained using all
available English data. This model was used to
restore the case of the lower-case system combi-
nation output.
All six 1-best system outputs on cz-en, 16
outputs on de-en, 8 outputs on es-en, and
14 outputs on fr-en were combined. The lat-
tice based BLEU tuning was used to optimize the
bi-gram decoding weights and N-best list based
BLEU tuning was used to optimize the 5-gram re-
scoring weights. Results for these single source
language experiments are shown in Table 1. The
gains on syscombtune were similar to those on
syscombtest for all but French-English. The
tuning set contained only 455 segments but ap-
peared to be well matched with the larger (2034
segments) test set. The characteristics of the indi-
vidual system outputs were probably different for
the tuning and test sets on French-English transla-
tion. In our experience, optimizing system com-
bination weights using the ExpBLEU tuning for
a small number of systems yields similar results
to lattice based BLEU tuning. The lattice based
BLEU tuning is faster as there is no need for mul-
tiple decoding and tuning iterations. Using the bi-
asLM on the single source combinations did not
xx-en tune test
System TER BLEU TER BLEU
worst 71.17 13.85 68.65 14.29
best 51.81 30.10 51.45 30.56
lattice 43.15 35.72 43.79 35.29
expBLEU 44.07 36.91 44.35 36.62
+biasLM 43.63 37.61 44.50 37.12
Table 2: Case insensitive TER and BLEU scores
on syscombtune (tune) and syscombtest
(test) for xx-en combination. Combinations us-
ing lattice BLEU tuning, expected BLEU tuning,
and after adding the system output biased LM are
shown.
yield any gains. The output for these conditions
probably did not contain enough data for biasLM
training given the small tuning set and small num-
ber of systems.
Finally, experiments combining all 44 1-best
system outputs were performed to produce a
multi-source combination output. The first experi-
ment used the lattice based BLEU tuning and gave
a 5.6 BLEU point gain on the tuning set as seen in
Table 2. The ExpBLEU tuning gave an additional
1.2 point gain which suggests that the direct lattice
based BLEU tuning got stuck in a local optimum.
Using the system output biased LM gave an addi-
tional 0.7 point gain. The gains on the test set were
similar and the best combination gave a 6.6 point
gain over the best individual system.
5 Conclusions
The BBN submissions for WMT10 system com-
bination task were described in this paper. The
combination was based on confusion network de-
325
coding. The confusion networks were built us-
ing an incremental hypothesis alignment algo-
rithm with flexible matching. The bi-gram de-
coding weights for the single source conditions
were optimized directly to maximize the BLEU
scores of the 1-best decoding outputs and the 5-
gram re-scoring weights were tuned on 300-best
lists. The BLEU gains over the best individual
system outputs were around 1.5 points on cz-en,
2.0 points on de-en, 1.0 points on es-en, and
0.4 points on fr-en. The system combination
weights on xx-en were tuned to maximize Exp-
BLEU, and a system output biased LM was used.
The BLEU gain over the best individual system
was 6.6 points. Future work will investigate tuning
of the edit costs used in the alignment. A lattice
based ExpBLEU tuning will be investigated. Also,
weights for more complicated functions with addi-
tional features may be tuned using ExpBLEU.
Acknowledgments
This work was supported by DARPA/IPTO Con-
tract No. HR0011-06-C-0022 under the GALE
program.
References
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. MIT Press.
Dong C. Liu and Jorge Nocedal. 1989. On the limited
memory method for large scale optimization. Math-
ematical Programming, 45(3):503?528.
Franz J. Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of the
41st Annual Meeting of the Association for Compu-
tational Linguistics, pages 160?167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proceedings
of the 40th Annual Meeting of the Association for
Computational Linguistics, pages 311?318.
Adam Pauls, John DeNero, and DanKlein. 2009. Con-
sensus training for consensus decoding in machine
translation. In Proceedings of the 2009 Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 1418?1427.
William H. Press, Saul A. Teukolsky, William T. Vet-
terling, and Brian P. Flannery. 2007. Numerical
recipes: the art of scientific computing. Cambridge
University Press, 3rd edition.
Antti-Veikko I. Rosti, Spyros Matsoukas, and Richard
Schwartz. 2007. Improved word-level system com-
bination for machine translation. In Proceedings of
the 45th Annual Meeting of the Association of Com-
putational Linguistics, pages 312?319.
Antti-Veikko I. Rosti, Bing Zhang, Spyros Matsoukas,
and Richard Schwartz. 2009. Incremental hy-
pothesis alignment with flexible matching for build-
ing confusion networks: BBN system description
for WMT09 system combination task. In Proceed-
ings of the Fourth Workshop on Statistical Machine
Translation, pages 61?65.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciula, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings of the 7th Conference of the Associa-
tion for Machine Translation in the Americas, pages
223?231.
Matthew Snover, Nitin Madnani, Bonnie Dorr, and
Richard Schwartz. 2009. Fluency, adequacy, or
HTER? exploring different human judgments with
a tunable MT metric. In Proceedings of the Fourth
Workshop on Statistical Machine Translation, pages
259?268.
Roy W. Tromble, Shankar Kumar, Franz Och, and
Wolfgang Macherey. 2008. Lattice minimum
bayes-risk decoding for statistical machine transla-
tion. In Proceedings of the 2008 Conference on Em-
pirical Methods in Natural Language Processing,
pages 620?629.
326
Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 428?437,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
Decision Trees for Lexical Smoothing in Statistical Machine
Translation
Rabih Zbib
?
and Spyros Matsoukas and Richard Schwartz and John Makhoul
BBN Technologies, 10 Moulton Street, Cambridge, MA 02138, USA
? Massachusetts Institute of Technology, 77 Massachusetts Ave., Cambridge, MA 02139, USA
Abstract
We present a method for incorporat-
ing arbitrary context-informed word at-
tributes into statistical machine trans-
lation by clustering attribute-qualified
source words, and smoothing their
word translation probabilities using bi-
nary decision trees. We describe two
ways in which the decision trees are
used in machine translation: by us-
ing the attribute-qualified source word
clusters directly, or by using attribute-
dependent lexical translation probabil-
ities that are obtained from the trees,
as a lexical smoothing feature in the de-
coder model. We present experiments
using Arabic-to-English newswire data,
and using Arabic diacritics and part-of-
speech as source word attributes, and
show that the proposed method im-
proves on a state-of-the-art translation
system.
1 Introduction
Modern statistical machine translation (SMT)
models, such as phrase-based SMT or hierar-
chical SMT, implicitly incorporate source lan-
guage context. It has been shown, however,
that such systems can still benefit from the
explicit addition of lexical, syntactic or other
kinds of context-informed word features (Vick-
rey et al, 2005; Gimpel and Smith, 2008;
Brunning et al, 2009; Devlin, 2009). But the
benefit obtained from the addition of attribute
information is in general countered by the in-
crease in the model complexity, which in turn
results in a sparser translation model when es-
timated from the same corpus of data. The
increase in model sparsity usually results in a
deterioration of translation quality.
In this paper, we present a method for using
arbitrary types of source-side context-informed
word attributes, using binary decision trees to
deal with the sparsity side-effect. The deci-
sion trees cluster attribute-dependent source
words by reducing the entropy of the lexi-
cal translation probabilities. We also present
another method where, instead of clustering
the attribute-dependent source words, the de-
cision trees are used to interpolate attribute-
dependent lexical translation probability mod-
els, and use those probabilities to compute a
feature in the decoder log-linear model.
The experiments we present in this paper
were conducted on the translation of Arabic-
to-English newswire data using a hierarchical
system based on (Shen et al, 2008), and using
Arabic diacritics (see section 2.3) and part-of-
speech (POS) as source word attributes. Pre-
vious work that attempts to use Arabic dia-
critics in machine translation runs against the
sparsity problem, and appears to lose most of
the useful information contained in the dia-
critics when using partial diacritization (Diab
et al, 2007). Using the methods proposed
in this paper, we manage to obtain consistent
improvements from diacritics against a strong
baseline. The methods we propose, though,
are not restrictive to Arabic-to-English trans-
lation. The same techniques can also be used
with other language pairs and arbitrary word
attribute types. The attributes we use in the
described experiments are local; but long dis-
tance features can also be used.
In the next section, we review relevant pre-
vious work in three areas: Lexical smoothing
and lexical disambiguation techniques in ma-
chine translation; using decision trees in nat-
ural language processing, and especially ma-
chine translation; and Arabic diacritics. We
present a brief exposition of Arabic orthogra-
428
phy, and refer to previous work on automatic
diacritization of Arabic text. Section 3 de-
scribes the procedure for constructing the deci-
sion trees, and the two methods for using them
in machine translation. In section 4 we de-
scribe the experimental setup and present ex-
perimental results. Finally, section 5 concludes
the paper and discusses future directions.
2 Previous Work
2.1 Lexical Disambiguation and
Lexical Smoothing
Various ways have been proposed to improve
the lexical translation choices of SMT systems.
These approaches typically incorporate local
context information, either directly or indi-
rectly.
The use of Word Sense Disambiguation
(WSD) has been proposed to enhance ma-
chine translation by disambiguating the source
words (Cabezas and Resnick, 2005; Carpuat
and Wu, 2007; Chan et al, 2007). WSD
usually requires that the training data be la-
beled with senses, which might not be avail-
able for many languages. Also, WSD is tra-
ditionally formulated as a classification prob-
lem, and therefore does not naturally lend it-
self to be integrated into the generative frame-
work of machine translation. Carpuat and Wu
(2007) formulate the SMT lexical disambigua-
tion problem as a WSD task. Instead of learn-
ing from word sense corpora, they use the SMT
training data, and use local context features to
enhance the lexical disambiguation of phrase-
based SMT.
Sarikaya et al (2007) incorporate context
more directly by using POS tags on the target
side to model word context. They augmented
the target words with POS tags of the word
itself and its surrounding words, and used the
augmented words in decoding and for language
model rescoring. They reported gains on Iraqi-
Arabic-to-English translation.
Finally, using word-to-word context-free lex-
ical translation probabilities has been shown
to improve the performance of machine trans-
lation systems, even those using much more
sophisticated models. This feature, usually
called lexical smoothing, has been used in
phrase-based systems (Koehn et al, 2003).
Och et al (2004) also found that including
IBM Model 1 (Brown et al, 1993) word prob-
abilities in their log-linear model works better
than most other higher-level syntactic features
at improving the baseline. The incorporation
of context on the source or target side en-
hances the gain obtained from lexical smooth-
ing. Gimpel and Smith (2008) proposed us-
ing source-side lexical features in phrase-based
SMT by conditioning the phrase probabilities
on those features. They used word context,
syntactic features or positional features. The
features were added as components into the
log-linear decoder model, each with a tunable
weight. Devlin (2009) used context lexical fea-
tures in a hierarchical SMT system, interpolat-
ing lexical counts based on multiple contexts.
It also used target-side lexical features.
The work in the paper incorporates con-
text information based on the reduction of the
translation probability entropy.
2.2 Decision Trees
Decision trees have been used extensively in
various areas of machine learning, typically
as a way to cluster patterns in order to im-
prove classification (Duda et al, 2000). They
have, for instance, been long used success-
fully in speech recognition to cluster context-
dependent phoneme model states (Young et
al., 1994).
Decision trees have also been used in ma-
chine translation, although to a lesser extent.
In this respect, our work is most similar to
(Brunning et al, 2009), where the authors ex-
tended word alignment models for IBM Model
1 and Hidden Markov Model (HMM) align-
ments. They used decision trees to cluster the
context-dependent source words. Contexts be-
longing to the same cluster were grouped to-
gether during Expectation Maximization (EM)
training, thus providing a more robust proba-
bility estimate. While Brunning et al (2009)
used the source context clusters for word align-
ments, we use the attribute-dependent source
words directly in decoding. The approach we
propose can be readily used with any align-
ment model.
Stroppa et al (2007) presented a general-
ization of phrase-based SMT (Koehn et al,
2003) that also takes into account source-
side context information. They conditioned
the target phrase probability on the source
429
phrase as well as source phrase context, such
as bordering words, or part-of-speech of bor-
dering words. They built a decision tree for
each source phrase extracted from the train-
ing data. The branching of the tree nodes
was based on the different context features,
branching on the most class-discriminative fea-
tures first. Each node is associated with the
set of aligned target phrases and correspond-
ing context-conditioned probabilities. The de-
cision tree thus smoothes the phrase probabil-
ities based on the different features, allowing
the model to back off to less context, or no
context at all depending on the presence of
that context-dependent source phrase in the
training data. The model, however, did not
provide for a back-off mechanism if the phrase
pair was not found in the extracted phrase ta-
ble. The method presented in this paper differs
in various aspects. We use context-dependent
information at the source word level, rather
than the phrase level, thus making it readily
applicable to any translation model and not
just phrase-based translation. By incorporat-
ing context at the word level, we can decode
directly with attribute-augmented source data
(see section 3.2).
2.3 Arabic Diacritics
Since an important part of the experiments
described in this paper use diacritized Arabic
source, we present a brief description of Arabic
orthography, and specifically diacritics.
The Arabic script, like that of most other
Semitic languages, only represents consonants
and long vowels using letters
1
. Short vowels
can be written as small marks written above
or below the preceding consonant, called di-
acritics. The diacritics are, however, omit-
ted from written text, except in special cases,
thus creating an additional level of lexical am-
biguity. Readers can usually guess the cor-
rect pronunciation of words in non-diacritized
text from the sentence and discourse context.
Grammatical case on nouns and adjectives are
also marked using diacritics at the end of
words. Arabic MT systems use undiacritized
text, since most available Arabic data is undi-
acritized.
1
Such writing systems are sometimes referred to as
Abjads (See Daniels, Peter T., et al eds. The World's
Writing Systems Oxford. (1996), p.4.)
Automatic diacritization of Arabic has been
done with high accuracy, using various genera-
tive and discriminative modeling techniques.
For example, Ananthakrishnan et al (2005)
used a generative model that incorporates
word level n-grams, sub-word level n-grams
and part-of-speech information to perform di-
acritization. Nelken and Shieber (2005) mod-
eled the generative process of dropping dia-
critics using weighted transducers, then used
Viterbi decoding to find the most likely gener-
ator. Zitouni et al (2006) presented a method
based on maximum entropy classifiers, us-
ing features like character n-grams, word n-
grams, POS and morphological segmentation.
Habash and Rambow (2007) determined vari-
ous morpho-syntactic features of the word us-
ing SVM classifiers, then chose the correspond-
ing diacritization. The experiments in this
paper use the automatic diacritizer by Sakhr
Software. The diacritizer determines word di-
acritics through rule-based morphological and
syntactic analysis. It outputs a diacritization
for both the internal stem and case ending
markers of the word, with an accuracy of 97%
for stem diacritization and 91% for full dia-
critization (i.e., including case endings).
There has been work done on using dia-
critics in Automatic Speech Recognition, e.g.
(Vergyri and Kirchhoff, 2004). However, the
only previous work on using diacritization for
MT is (Diab et al, 2007), which used the di-
acritization system described in (Habash and
Rambow, 2007). It investigated the effect
of using full diacritization as well as partial
diacritization on MT results. The authors
found that using full diacritics deteriorates MT
performance. They used partial diacritiza-
tion schemes, such as diacritizing only passive
verbs, keeping the case endings diacritics, or
only gemination diacritics. They also saw no
gain in most configurations. The authors ar-
gued that the deterioration in performance is
caused by the increase in the size of the vo-
cabulary, which in turn makes the translation
model sparser; as well as by errors during the
automatic diacritization process.
430
3 Decision Trees for Source Word
Attributes
3.1 Growing the Decision Tree
In this section, we describe the procedure
for growing the decision trees using context-
informed source word attributes.
The attribute-qualified source-side of the
parallel training data is first aligned to the
target-side data. If S is the set of attribute-
dependent forms of source word s, and tj is a
target word aligned to si ? S, then we define:
p (tj |si) =
count(si,tj)
count(si)
(1)
where count(si, tj) is the count of alignment
links between si and tj .
A separate binary decision tree is grown for
each source word. We start by including all the
attribute-dependent forms of the source word
at the root of the tree. We split the set of at-
tributes at each node into two child nodes, by
choosing the splitting that maximizes the re-
duction in weighted entropy of the probability
distribution in (1). In other words, at node n,
we choose the partition (S?1 , S
?
2) such that:
(S?1 , S
?
2) =
argmax
(S1,S2)
S1?S2=S
{h(S)? (h(S1) + h(S2))}
(2)
where h(S) is the entropy of the probabil-
ity distribution p(tj |si ? S), weighted by the
number of samples in the training data of the
source words in S. We only split a node if the
entropy is reduced by more than a threshold
?h. This step is repeated recursively until the
tree cannot be grown anymore.
Weighting the entropy by the source word
counts gives more weight to the context-
dependent source words with a higher number
of samples in the training data, sine the lex-
ical translation probability estimates for fre-
quent words can be trusted better. The ratio-
nale behind the splitting criterion used is that
the split that reduces the entropy of the lexical
translation probability distribution the most
is also the split that best separates the list of
forms of the source word in terms of the target
word translation. For a source word that has
multiple meanings, depending on its context,
the decision tree will tend to implicitly sepa-
rate those meanings using the information in
the lexical translation probabilities.
Although we describe this method as grow-
ing one decision tree for each word, and using
one attribute type at a time, a decision tree
can clearly be constructed for multiple words,
and more than one attribute type can be used
in the same decision tree.
3.2 Trees for Source Word Clustering
The source words could be augmented to ex-
plicitly incorporate the word attributes (dia-
critics or other attribute types). The aug-
mented source will be less ambiguous if the
attributes do in fact contain disambiguating
information. This, in principle, helps machine
translation performance. The flip side is that
the resulting increase in vocabulary size in-
creases the translation model sparsity, usually
with a detrimental effect on translation.
To mitigate the effect of the increase in vo-
cabulary, decision trees can be use to cluster
the attribute-augmented source words. More
specifically, a decision tree is grown for each
source word as described in the previous sec-
tion, using a predefined entropy threshold ?h.
When the tree cannot be expanded anymore,
its leaf nodes will contain a multi-set parti-
tioning of the list of attribute-dependent forms
of that source word. Each of the clusters is
treated as an equivalence class, and all forms
in that class are mapped to a unique form (e.g.
an arbitrarily chosen member of the cluster).
The mappings are used to map the tokens in
the parallel training data before alignment is
run on the mapped data. The test data is
also mapped consistently. This clustering pro-
cedure will only keep the attribute-dependent
forms of the source words that decrease the un-
certainty in the translation probabilities, and
are thus useful for translation.
The experiments we report on use diacritics
as an attribute type. The various diacritized
forms of a source word are thus used to train
the decision trees. The resulting clusters are
used to map the data into a subset of the vo-
cabulary that is used in translation training
and decoding (see section 4.2 for results). Di-
acritics are obviously specific to Arabic. But
this method can be used with other attribute
types, by first appending the source words with
431
{sijo
na,s
ijni}sjn
{sijo
na,s
ijni,
sajo
na,s
ajon
u,sa
jana
} {saj
ana
}
{saj
ona
,sajo
nu}
Figure 1: Decision tree for source word sjn using
diacritics as an attribute.
their context (e.g. attach to each source word
its part-of-speech tag or context), and then
training decision trees and mapping the source
side of the data.
Figure 1 shows an example of a decision
tree for the Arabic word sjn
2
using diacritics
as a source attribute. The root contains the
various diacritized forms (sijona `prison AC-
CUSATIVE', sijoni `prison DATIVE', sajona
`imprisonment ACCUSATIVE.', sajoni `im-
prisonment ACCUSATIVE.', sajana `he im-
prisoned '). The leaf nodes contain the
attribute-dependent clusters.
3.3 Trees for Lexical Smoothing
As mentioned in section 2.1, lexical smoothing,
computed from word-to-word translation prob-
abilities, is a useful feature, even in SMT sys-
tems that use sophisticated translation mod-
els. This is likely due to the robustness of
context-free word-to-word translation proba-
bility estimates compared to the probabilities
of more complicated models. In those models,
the rules and probabilities are estimated from
much larger sample spaces.
In our system, the lexical smoothing feature
is computed as follows:
f(U)=
?
tj?T (U)
(
1?
?
si?{S(U)?NULL}
(1?p?(tj |si))
)
(3)
where U is the modeling unit specific to the
translation model used. For a phrase-based
system, U is the phrase pair, and for a hierar-
chical system U is the translation rule. S (U)
2
Examples are written using Buckwalter transliter-
ation.
sjn
{sijo
na,s
ijni,
sajo
na,s
ajon
u,sa
jana
} {s
ajan
a}
{sijo
na}
{sijo
ni}
{saj
ona
}
{saj
onu
}
{sijo
na}
{sijo
ni}Figure 2: Decision tree for source word sjn grown
fully using diacritics.
is the set of terminals on the source side of U,
and T (U) is the set of terminals on its tar-
get. The NULL term in the equation above
accounts for unaligned target words, which we
found in our experiments to be beneficial. One
way of interpreting equation (3) is that f (U)
is the probability that for each target word tj
in U, tj is a likely translation of at least one
word si on the source side. The feature value
is then used as a component in the log-linear
model, with a tunable weight.
In this work, we generalize the lexical
smoothing feature to incorporate the source
word attributes. A tree is grown for each
source word as described in section 3.1, but
using an entropy threshold ?h = 0. In other
words, the tree is grown all the way until each
leaf node contains one attribute-dependent
form of the source word. Each node in the
tree contains a cluster of attribute-dependent
forms of the source word, and a corresponding
attribute-dependent lexical translation prob-
ability distribution. The lexical translation
probability models at the root nodes are those
of the regular attribute-independent lexical
translation probabilities. The models at the
leaf nodes are the most fine-grained, since they
are conditioned on only one attribute value.
Figure 2 shows a fully grown decision tree for
the same source word as the example in Figure
1.
The lexical probability distribution at the
leafs are from sparser data than the original
distributions, and are therefore less robust. To
address this, the attribute-dependent lexical
432
smoothing feature is estimated by recursively
interpolating the lexical translation probabil-
ities up the tree. The probability distribu-
tion pn at each node n is interpolated with
the probability of its parent node as follows:
pn =
{
pn if n is root,
wnpn + (1? wn)pm otherwise
where m is the parent of n
(4)
A fraction of the parent probability mass is
thus given to the probability of the child node.
If the probability estimate of an attribute-
dependent form of a source word with a cer-
tain target word t is not reliable, or if the
probability estimate is 0 (because the source
word in this context is not aligned with t),
then the model gracefully backs off by using
the probability estimates from other attribute-
dependent lexical translation probability mod-
els of the source word.
The interpolation weight is a logistic regres-
sion function of the source word count at a
node n:
wn =
1
1 + e???? log(count(Sn))
(5)
The weight varies depending on the count
of the attribute-qualified source word in each
node, thus reflecting the confidence in the es-
timates of each node's distribution. The two
global parameters of the function, a bias ? and
a scale ? are tuned to maximize the likelihood
of a set of alignment counts from a heldout
data set of 179K sentences. The tuning is done
using Powell's method (Brent, 1973).
During decoding, we use the probability dis-
tribution at the leaves to compute the feature
value f(R) for each hierarchical rule R. We
train and decode using the regular, attribute-
independent source. The source word at-
tributes are used in the decoder only to in-
dex the interpolated probability distribution
needed to compute f (R).
4 Experiments
4.1 Experimental Setup
As mentioned before, the experiments we re-
port on use a string-to-dependency-tree hier-
archical translation system based on the model
described in (Shen et al, 2008). Forward and
Likelihood %
baseline -1.29 -
Diacs.
dec. trees
-1.25 +2.98%
POS dec.
trees
-1.24 +3.41%
Table 1: Normalized likelihood of the test set algn-
ments without decision trees, then with decision trees
using diacritics and part-of-speech respectively.
backward context-free lexical smoothing are
used as decoder features in all the experiments.
Other features such as rule probabilities and
dependency tree language model (Shen et al,
2008) are also used. We use GIZA++ (Och
and Ney, 2003) for word alignments. The de-
coder model parameters are tuned using Mini-
mum Error Rate training (Och, 2003) to max-
imize the IBM BLEU score (Papineni et al,
2002).
For training the alignments, we use 27M
words from the Sakhr Arabic-English Paral-
lel Corpus (SSUSAC27). The language model
uses 7B words from the English Gigaword and
from data collected from the web. A 3-gram
language model is used during decoding. The
decoder produces an N-best list that is re-
ranked using a 5-gram language model.
We tune and test on two separate data sets
consisting of documents from the following col-
lections: the newswire portion of NIST MT04,
MT05, MT06, and MT08 evaluation sets, the
GALE Phase 1 (P1) and Phase 2 (P2) evalu-
ation sets, and the GALE P2 and P3 develop-
ment sets. The tuning set contains 1994 sen-
tences and the test set contains 3149 sentences.
The average length of sentences is 36 words.
Most of the documents in the two data sets
have 4 reference translations, but some have
only one. The average number of reference
translations per sentence is 3.94 for the tun-
ing set and 3.67 for the test set.
In the next section, we report on measure-
ments of the likelihood of test data, and de-
scribe the translation experiments in detail.
4.2 Results
In order to assess whether the decision trees
are in fact helpful in decreasing the uncer-
tainty in the lexical translation probabilities
433
54.254.354.454.554.654.754.854.955
MT Score  in BLEU
5454.154.254.354.454.554.654.754.854.955
0
25
50
100
Entr
opy T
hresh
old
Figure 3: BLEU scores of the clustering experiments
as a function of the entropy threshold on tuning set.
on unseen data, we compute the likelihood
of the test data with respect to these prob-
abilities with and without the decision tree
splitting. We align the test set with its ref-
erence using GIZA++, and then obtain the
link count l_count(si, tj) for each alignment
link i = (si,ti) in the set of alignment links I.
We calculate the normalized likelihood of the
alignments:
L = log
?
?
(
?
i
p(ti | si)
l_count(si,ti)
) 1
|I|
?
?
=
1
|I|
?
i?I
l_count(si, ti) log p? (ti | si) (6)
where p? (ti | si) is the probability for the word
pair (ti, si) in equation (4). If the same in-
stance of source word si is aligned to two tar-
get words ti and tj , then these two links are
counted separately. If a source in the test set
is out-of-vocabulary, or if a word pair (ti, si)
is aligned in the test alignment but not in the
training alignments (and thus has no probabil-
ity estimate), then it is ignored in the calcula-
tion of the log-likelihood.
Table 1 shows the likelihood for the baseline
case, where one lexical translation probability
distribution is used per source word. It also
shows the likelihoods calculated using the lex-
ical distributions in the leaf nodes of the de-
cision trees, when either diacritics or part-of-
speech are used as an attribute type. The table
shows an increase in the likelihood of 2.98% us-
ing diacritics, and 3.41% using part-of-speech.
The translation result tables present MT
scores in two different metrics: Translation
Edit Rate (Snover et al, 2006) and IBM
TER BLEU
Test
baseline 40.14 52.05
full diacritics 40.31 52.39
+0.17 +0.34
dec. trees, diac (?h = 50) 39.75 52.60
-0.39 +0.55
Table 2: Results of experiments using decision trees
to cluster source words.
BLEU. The reader is reminded that a higher
BLEU score and a lower TER are desired. The
tables also show the difference in scores be-
tween the baseline and each experiment. It is
worth noting that the gains reported are rela-
tive to a strong baseline that uses a state-of-
the-art system with many features, and a fairly
large training corpus.
The decision tree clustering experiment as
described in section 3.2 depends on a global
parameter, namely the threshold in entropy re-
duction ?h. We tune this parameter manually
on a tuning set. Figure 3 shows the BLEU
scores as a function of the threshold value, with
diacritics as an attribute type. The most gain
is obtained for an entropy threshold of 50.
The fully diacritized data has an average of
1.78 diacritized forms per source word. The av-
erage weighted by the number of occurrences is
6.28, which indicates that words with more di-
acritized forms tend to occur more frequently.
After clustering using a value of ?h = 50,
the average number of diacritized forms be-
comes 1.11, and the occurrence weighted av-
erage becomes 3.69. The clustering proce-
dure thus seems to eliminate most diacritized
forms, which likely do not contain helpful dis-
ambiguating information.
Table 2 lists the detailed results of experi-
ments using diacritics. In the first experiment,
we show that using full diacritization results in
a small gain on the BLEU score and no gain on
TER, which is somewhat consistent with the
result obtained by Diab et al (2007). The next
experiment shows the results of clustering the
diacritized source words using decision trees
for the entropy threshold of 50. The TER loss
of the full diacritics becomes a gain, and the
BLEU gain increases. This confirms our spec-
ulation that the use of fully diacritized data in-
434
TER BLEU
Test
baseline 40.14 52.05
dec. trees, diacs 39.75 52.55
-0.39 +0.50
dec. trees, POS 40.05 52.40
-0.09 +0.35
dec. trees, diacs, no interpolation 39.98 52.09
-0.16 +0.04
Table 3: Results of experiments using the word attribute-dependent lexical smoothing feature.
creases the model sparsity, which undoes most
of the benefit obtained from the disambiguat-
ing information that the diacritics contain. Us-
ing the decision trees to cluster the diacritized
source data prunes diacritized forms that do
not decrease the entropy of the lexical trans-
lation probability distributions. It thus finds
a sweet-spot between the negative effect of in-
creasing the vocabulary size and the positive
effect of disambiguation.
In our experiments, using diacritics with
case endings gave consistently better score
than using diacritics with no case endings, de-
spite the fact that they result in a higher vo-
cabulary size. One possible explanation is that
diacritics not only help in lexical disambigua-
tion, but they might also be indirectly help-
ing in phrase reordering, since the diacritics on
the final letter indicate the word's grammatical
function.
The results from using decision trees to in-
terpolate attribute-dependent lexical smooth-
ing features are summarized in table 3. In
the first experiment, we show the results of
using diacritics to estimate the interpolated
lexical translation probabilities. The results
show a gain of +0.5 BLEU points and 0.39
TER points. The gain is statistically signifi-
cant with a 95% confidence level. Using part-
of-speech as an attribute gives a smaller, but
still statistically significant gain. We also ran
a control experiment, where we used diacritic-
dependent lexical translation probabilities ob-
tained from the decision trees, but did not per-
form the probability interpolation of equation
(4). The gains mostly disappear, especially on
BLEU, showing the importance of the inter-
polation step for the proper estimation of the
lexical smoothing feature.
5 Conclusion and Future Directions
We presented in this paper a new method for
incorporating explicit context-informed word
attributes into SMT using binary decision
trees. We reported on experiments on Arabic-
to-English translation using diacritized Ara-
bic and part-of-speech as word attributes, and
showed that the use of these attributes in-
creases the likelihood of source-target word
pairs of unseen data. We proposed two spe-
cific ways in which the results of the decision
tree training process are used in machine trans-
lation, and showed that they result in better
translation results.
For future work, we plan on using multi-
ple source-side attributes at the same time.
Different attributes could have different dis-
ambiguating information, which could pro-
vide more benefit than using any of the at-
tributes alone. We also plan on investigat-
ing the use of multi-word trees; trees for word
clusters can for instance be grown instead
of growing a separate tree for each source
word. Although the experiments presented
in this paper use local word attributes, noth-
ing in principle prevents this method from be-
ing used with long-distance sentence context,
or even with document-level or discourse-level
features. Our future plans include the investi-
gation of using such features as well.
Acknowledgment
This work was supported by DARPA/IPTO
Contract No. HR0011-06-C-0022 under the
GALE program.
The views, opinions, and/or findings con-
tained in this article are those of the author
and should not be interpreted as representing
the official views or policies, either expressed
435
or implied, of the Defense Advanced Research
Projects Agency or the Department of Defense.
A pproved for Public Release, Distribution Un-
limited.
References
S. Ananthakrishnan, S. Narayanan, and S. Ban-
galore. 2005. Automatic diacritization of ara-
bic transcripts for automatic speech recognition.
Kanpur, India.
R. Brent. 1973. Algorithms for Minimization
Without Derivatives. Prentice-Hall.
P. Brown, V. Della Pietra, S. Della Pietra, and
R. L. Mercer. 1993. The mathematics of sta-
tistical machine translation: Parameter estima-
tion. Computational Linguistics, 19:263311.
J. Brunning, A. de Gispert, and W. Byrne. 2009.
Context-dependent alignment models for statis-
tical machine translation. In NAACL '09: Pro-
ceedings of the 2009 Human Language Technol-
ogy Conference of the North American Chapter
of the Association for Computational Linguis-
tics, pages 110118.
C. Cabezas and P. Resnick. 2005. Using WSD
techniques for lexical selection in statistical ma-
chine translation. In Technical report, Insti-
tute for Advanced Computer Studies (CS-TR-
4736, LAMP-TR-124, UMIACS-TR-2005-42),
College Park, MD.
M. Carpuat and D. Wu. 2007. Improving statis-
tical machine translation using word sense dis-
ambiguation. In EMNLP-CoNLL-2007: Pro-
ceedings of the 2007 Joint Conference on Em-
pirical Methods in Natural Language Processing
and Computational Natural Language Learning,
Prague, Czech Republic.
Y. Chan, H. Ng, and D. Chiang. 2007. Word
sense disambiguation improves statistical ma-
chine translation. In Proceedings of the 45th
Annual Meeting of the Association for Compu-
tational Linguistics (ACL).
J. Devlin. 2009. Lexical features for statistical
machine translation. Master's thesis, University
of Maryland, December 2009.
M. Diab, M. Ghoneim, and N. Habash. 2007. Ara-
bic diacritization in the context of statistical ma-
chine translation. InMT Summit XI, pages 143
149, Copenhagen, Denmark.
R. O. Duda, P. E. Hart, and D. G. Stork. 2000.
Pattern Classification. Wiley-Interscience Pub-
lication.
K. Gimpel and N. A. Smith. 2008. Rich source-
side context for statistical machine translation.
In StatMT '08: Proceedings of the Third Work-
shop on Statistical Machine Translation, pages
917, Columbus, Ohio.
N. Habash and O. Rambow. 2007. Arabic diacriti-
zation through full morphological tagging. In
Proceedings of the 2007 Human Language Tech-
nology Conference of the North American Chap-
ter of the Association for Computational Lin-
guistics, pages 5356, Rochester, New York.
P. Koehn, F. J. Och, and D. Marcu. 2003. Statis-
tical phrase-based translation. In Proceedings of
the 2003 Human Language Technology Confer-
ence of the North American Chapter of the As-
sociation for Computational Linguistics, pages
4854, Edmonton, Canada.
R. Nelken and S. M. Shieber. 2005. Arabic dia-
critization using weighted finite-state transduc-
ers. In Proceedings of the 2005 ACL Workshop
on Computational Approaches to Semitic Lan-
guages, Ann Arbor, Michigan.
F. J. Och and H. Ney. 2003. A systematic com-
parison of various statistical alignment models.
Computational Linguistics, 29(1):1951.
F. J. Och, D. Gildea, S. Khudanpur, A. Sarkar,
K. Yamada, A. Fraser, S. Kumar, L. Shen,
D. Smith, K. Eng, V. Jain, Z. Jin, and D. R.
Radev. 2004. A smorgasbord of features for sta-
tistical machine translation. In HLT-NAACL,
pages 161168.
F. J. Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings
of the 41st Annual Meeting of the Association
for Computational Linguistics (ACL), Sapporo,
Japan.
K. Papineni, S. Roukos, T. Ward, and W. Zhu.
2002. Bleu: a method for automatic evalua-
tion of machine translation. In Proceedings of
the 40th Annual Meeting of the Association for
Computational Linguistics (ACL), Philadelphia,
PA.
Ruhi Sarikaya, Yonggang Deng, and Yuqing Gao.
2007. Context dependent word modeling for sta-
tistical machine translation using part-of-speech
tags. In Proceedings of INTERSPEECH 2007fs,
Antwerp, Belgium.
L. Shen, J. Xu, and R. Weischedel. 2008. A new
string-to-dependency machine translation algo-
rithm with a target dependency language model.
In Proceedings of the 46th Annual Meeting of
the Association for Computational Linguistics
(ACL), Columbus, Ohio.
M. Snover, B. Dorr, R. Schwartz, J. Makhoul, and
L. Micciulla. 2006. A study of translation error
436
rate with targeted human annotation. In Pro-
ceedings of the 7th Conf. of the Association for
Machine Translation in the Americas (AMTA
2006), pages 223231, Cambridge, MA.
N. Stroppa, A. van den Bosch, and A Way.
2007. Exploiting source similarity for SMT us-
ing context-informed features. In Proceedings of
the 11th International Conference on Theoreti-
cal and Methodological Issues in Machine Trans-
lation (TMI-07), pages 231240.
D. Vergyri and K. Kirchhoff. 2004. Automatic
diacritization of arabic for acoustic modeling in
speech recognition. In Semitic '04: Proceedings
of the Workshop on Computational Approaches
to Arabic Script-based Languages, pages 6673,
Geneva, Switzerland.
D. Vickrey, L. Biewald, M. Teyssier, and D. Koller.
2005. Word-sense disambiguation for machine
translation. In HLT '05: Proceedings of the
conference on Human Language Technology and
Empirical Methods in Natural Language Pro-
cessing, Vancouser, BC, Canada.
S.J. Young, J.J. Odell, and P.C. Woodland. 1994.
Tree-based state tying for high accuracy acoustic
modelling. In HLT'94: Proceedings of the Work-
shop on Human Language Technology, pages
307312.
I. Zitouni, J. S. Sorensen, and Ruhi Sarikaya. 2006.
Maximum entropy based restoration of arabic
diacritics. In Proceedings of the 21st Interna-
tional Conference on Computational Linguistics
and the 44th annual meeting of the Association
for Computational Linguistics, pages 577584,
Sydney, Australia.
437
Proceedings of the 6th Workshop on Statistical Machine Translation, pages 159?165,
Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational Linguistics
Expected BLEU Training for Graphs: BBN System Description for
WMT11 System Combination Task
Antti-Veikko I. Rosti? and Bing Zhang and Spyros Matsoukas and Richard Schwartz
Raytheon BBN Technologies, 10 Moulton Street, Cambridge, MA 02138, USA
{arosti,bzhang,smatsouk,schwartz}@bbn.com
Abstract
BBN submitted system combination outputs
for Czech-English, German-English, Spanish-
English, and French-English language pairs.
All combinations were based on confusion
network decoding. The confusion networks
were built using incremental hypothesis align-
ment algorithm with flexible matching. A
novel bi-gram count feature, which can penal-
ize bi-grams not present in the input hypothe-
ses corresponding to a source sentence, was
introduced in addition to the usual decoder
features. The system combination weights
were tuned using a graph based expected
BLEU as the objective function while incre-
mentally expanding the networks to bi-gram
and 5-gram contexts. The expected BLEU
tuning described in this paper naturally gen-
eralizes to hypergraphs and can be used to
optimize thousands of weights. The com-
bination gained about 0.5-4.0 BLEU points
over the best individual systems on the official
WMT11 language pairs. A 39 system multi-
source combination achieved an 11.1 BLEU
point gain.
1 Introduction
The confusion networks for the BBN submissions
to the WMT11 system combination task were built
using incremental hypothesis alignment algorithm
?This work was supported by DARPA/I2O Contract No.
HR0011-06-C-0022 under the GALE program (Approved for
Public Release, Distribution Unlimited). The views, opinions,
and/or findings contained in this article/presentation are those of
the author/presenter and should not be interpreted as represent-
ing the official views or policies, either expressed or implied,
of the Defense Advanced Research Projects Agency or the De-
partment of Defense.
with flexible matching (Rosti et al, 2009). A novel
bi-gram count feature was used in addition to the
standard decoder features. The N-best list based ex-
pected BLEU tuning (Rosti et al, 2010), similar to
the one proposed by Smith and Eisner (2006), was
extended to operate on word lattices. This method is
closely related to the consensus BLEU (CoBLEU)
proposed by Pauls et al (2009). The minimum oper-
ation used to compute the clipped counts (matches)
in the BLEU score (Papineni et al, 2002) was re-
placed by a differentiable function, so there was
no need to use sub-gradient ascent as in CoBLEU.
The expected BLEU (xBLEU) naturally generalizes
to hypergraphs by simply replacing the forward-
backward algorithm with inside-outside algorithm
when computing the expected n-gram counts and
sufficient statistics for the gradient.
The gradient ascent optimization of the xBLEU
appears to be more stable than the gradient-free di-
rect 1-best BLEU tuning or N -best list based min-
imum error rate training (Och, 2003), especially
when tuning a large number of weights. On the of-
ficial WMT11 language pairs with up to 30 weights,
there was no significant benefit from maximizing
xBLEU. However, on a 39 system multi-source
combination (43 weights total), it yielded a signif-
icant gain over gradient-free BLEU tuning and N -
best list based expected BLEU tuning.
2 Hypothesis Alignment and Features
The incremental hypothesis alignment with flexible
matching (Rosti et al, 2009) produces a confusion
network for each system output acting as a skele-
ton hypothesis for the ith source sentence. A con-
fusion network is a graph where all paths visit all
159
vertices. Consecutive vertices are connected by one
or more edges representing alternatives. Each edge
l is associated with a token and a set of scores. A to-
ken may be a word, punctuation symbol, or special
NULL token indicating a deletion in the alignment.
The set of scores includes a vector ofNs system spe-
cific confidences, siln, indicating whether the token
was aligned from the output of the system n.1 Other
scores may include a language model (LM) score
as well as non-NULL and NULL token indicators
(Rosti et al, 2007). As Rosti et al (2010) described,
the networks for all skeletons are connected to a start
and end vertex with NULL tokens in order to form
a joint lattice with multiple parallel networks. The
edges connecting the start vertex to the initial ver-
tices in each network have a heuristic prior estimated
from the alignment statistics at the confidence cor-
responding to the skeleton system. The edges con-
necting the final vertices of each network to the end
vertex have all system confidences set to one, so the
final edge does not change the score of any path.
A single word confidence is produced from the
confidence vector by taking an inner product with
the system weights ?n which are constrained to sum
to one,2
?
n ?n = 1. The total edge score is pro-
duced by a log-linear interpolation of the word con-
fidence with other features film:
sil = log
( Ns?
n=1
?nsiln
)
+
?
m
?mfilm (1)
The usual features film include the LM score as well
as non-NULL and NULL token indicators. Based
on an analysis of the system combination outputs, a
large number of bi-grams not present in any input
hypothesis are often produced, some of which are
clearly ungrammatical despite the LM. These novel
bi-grams are due to errors in hypothesis alignment
and the confusion network structure where any word
from the incoming edges of a vertex can be followed
by any word from the outgoing edges. After expand-
ing and re-scoring the joint lattice with a bi-gram, a
new feature indicating the presence of a novel bi-
gram may be added on the edges. A negative weight
1The confidences are binary when aligning 1-best outputs.
More elaborate confidences may be estimated fromN -best lists;
see for example Rosti et al (2007).
2See (Rosti et al, 2010) for a differentiable constraint.
for this feature discourages novel bi-grams in the
output during decoding.
3 Weight Optimization
The most common objective function used in ma-
chine translation is the BLEU-N score (Papineni et
al., 2002) defined as follows:3
BLEU =
N?
n=1
(?
i m
n
i?
i h
n
i
) 1
N
?
(
1?
?
i ri
?
i h
1
i
)
(2)
where N is the maximum n-gram order (typically
N = 4), mni is the number of n-gram matches
(clipped counts) between the hypothesis ei and ref-
erence e?i for segment i, hni is the number of n-grams
in the hypothesis, ri is the reference length,4 and
?(x) = min(1.0, ex) is the brevity penalty. Using
gn to represent an arbitrary n-gram, cign to repre-
sent the count of gn in hypothesis ei, and c?ign to
represent the count of gn in reference e?i, the BLEU
statistics can be defined as follows:
mni =
?
gn
min(cign , c?ign) (3)
hni =
?
gn
cign (4)
The unigram count h1i is simply the hypothesis
length and higher order n-gram counts can be ob-
tained by hni = h
n?1
i ? 1. The reference n-gram
counts for each sentence can be stored in an n-gram
trie for efficient scoring.5
The BLEU score is not differentiable due to the
minimum operations on the matches mni and brevity
penalty ?(x). Therefore gradient-free optimization
algorithms, such as Powell?s method or downhill
simplex (Press et al, 2007), are often employed in
weight tuning (Och, 2003). System combination
weights tuned using the downhill simplex method
to directly optimize 1-best BLEU score of the de-
coder outputs served as the first baseline in the ex-
periments. The distributed optimization approach
used here was first described in (Rosti et al, 2010).
3Superscripts indicate the n-gram order in all variables in
this paper. They are used as exponents only for the constant e.
4If multiple references are available, ri is the reference
length closest to the hypothesis length, h1i .
5If multiple references are available, the maximum n-gram
counts are stored.
160
A set of system combination weights was first tuned
for unpruned lattices re-scored with a bi-gram LM.
Another set of re-scoring weights was tuned for 300-
best lists re-scored with a 5-gram LM.
3.1 Graph expected BLEU
Gradient-free optimization algorithms work well
with a relatively small number of weights. Weight
optimization for a 44 system combination in Rosti
et al (2010) was shown to be unstable with down-
hill simplex algorithm. Instead, an N-best list based
expected BLEU tuning with gradient ascent yielded
better results. This served as the second baseline in
the experiments. The objective function is defined
by replacing the n-gram statistics with expected n-
gram counts and matches as in (Smith and Eisner,
2006), and brevity penalty with a differentiable ap-
proximation:
?(x) =
ex ? 1
1 + e1000x
+ 1 (5)
An N-best list represents a subset of the search space
and multiple decoding iterations with N-best list
merging is required to improve convergence. In this
work, expected BLEU tuning is extended for lat-
tices by replacing the minimum operation in n-gram
matches with another differentiable approximation.
The expected n-gram statistics for path j, which cor-
respond to the standard statistics in Equations 3 and
4, are defined as follows:
m?ni =
?
gn
?
( ?
j?Ji
Pijcijgn , c?ign
)
(6)
h?ni =
?
gn
?
j?Ji
Pijcijgn (7)
where Ji is the set of all paths in a lattice or all
derivations in a hypergraph for the ith source sen-
tence, Pij is the posterior of path j, and cijgn is
the count of n-grams gn in hypothesis eij on path
j. The path posterior and approximate minimum are
defined by:
Pij =
?
l?j e
?sil
?
j??Ji
?
l?j? e?sil
(8)
?(x, c) =
x? c
1 + e1000(x?c)
+ c (9)
where sil is the total score on edge l defined in Equa-
tion 1 and ? is an edge score scaling factor. The
scaling factor affects the shape of the edge posterior
distribution; ? > 1.0 makes the edge posteriors on
the 1-best path higher than edge posteriors on other
paths and ? < 1.0 makes the posteriors on all paths
more uniform.
The graph expected BLEU can be factored as
xBLEU = ePB where:
P =
1
N
N?
n=1
(
log
?
i
m?ni ? log
?
i
h?ni
)
(10)
B = ?
(
1?
?
i ri
?
i h?
1
i
)
(11)
and ri is the reference length.6 This objective func-
tion is closely related to CoBLEU (Pauls et al,
2009). Unlike CoBLEU, xBLEU is differentiable
and standard gradient ascent algorithms can be used
to find weights that maximize the objective.
Note, the expected counts can be expressed in
terms of edge posteriors as:
?
j?Ji
Pijcijgn =
?
l?Li
pil?(c
n
il, g
n) (12)
where Li is the set of all edges for the ith sentence,
pil is the edge posterior, ?(x, c) is the Kronecker
delta function which is 1 if x = c and 0 if x 6= c, and
cnil is the n-gram context of edge l. The edge posteri-
ors can be computed via standard forward-backward
algorithm for lattices or inside-outside algorithm for
hypergraphs. As with the BLEU statistics, only ex-
pected unigram counts h?1i need to be accumulated
for the hypothesis n-gram counts in Equation 7 as
h?ni = h?
n?1
i ? 1 for n > 1. Also, the expected
n-gram counts for each graph can be stored in an
n-gram trie for efficient gradient computation.
3.2 Gradient of graph expected BLEU
The gradient of the xBLEU with respect to weight ?
can be factored as:
?xBLEU
??
=
?
i
?
l?Li
?sil
??
?
j?Ji
?xBLEU
? logPij
? logPij
?sil
(13)
where the gradient of the log-path-posterior with re-
spect to the edge score is given by:
? logPij
?sil
= ?
(
?(l ? j)? pil
)
(14)
6If multiple reference are available, ri is the reference length
closest to the expected hypothesis length h?1i .
161
?xBLEU
??
= ?eP
(
B
N
N?
n=1
?
i
(
m?nik ?m
n
ik
mn
?
h?nik ? h
n
ik
hn
))
+ C??(1? C)
?
i
h?1ik ? h
1
ik
h1
(15)
and ?(l ? j) is one if edge l is on path j, and zero
otherwise. Using the factorization xBLEU = ePB,
Equation 13 can be expressed using sufficient statis-
tics as shown in Equation 15, where ??(x) is the
derivative of ?(x) with respect to x, mn =
?
i m?
n
i ,
hn =
?
i h?
n
i , C =
?
ri/
?
i h?
1
i , and the remaining
sufficient statistics are given by:
??ign = ?
?
( ?
j?Ji
Pijcijgn , c?ign
)
mnik =
(
?
l?Li
pil
?sil
??
)(
?
j?Ji
Pij
?
gn
??igncijgn
)
m?nik =
?
l?Li
?sil
??
?
j:l?Ji
Pij
?
gn
??igncijgn
hnik =
(
?
l?Li
pil
?sil
??
)(
?
j?Ji
Pij
?
gn
cijgn
)
h?nik =
?
l?Li
?sil
??
?
j:l?Ji
Pij
?
gn
cijgn
where ??(x, c) is the derivative of ?(x, c) with re-
spect to x, and the parentheses in the equations for
mnik and h
n
ik signify that the second terms do not de-
pend on the edge l.
3.3 Forward-backward algorithm under
expectation semiring
The sufficient statistics for graph expected BLEU
can be computed using expectation semirings (Li
and Eisner, 2009). Instead of computing single
forward/backward or inside/outside scores, addi-
tional n-gram elements are tracked for matches and
counts. For example in a bi-gram graph, the ele-
ments for edge l are represented by a 5-tuple7 sl =
?pl, r1lh, r
2
lh, r
1
lm, r
2
lm? where pl = e
?sil and:
rnlh =
?
gn
?(cnil, g
n)e?sil (16)
rnlm =
?
gn
??igne
?sil (17)
Assuming the lattice is topologically sorted, the for-
ward algorithm8 under expectation semiring for a 3-
7The sentence index i is dropped for brevity.
8For inside-outside algorithm, see (Li and Eisner, 2009).
tuple9 sl = ?pl, r1lh, r
1
lm? is defined by:
?0 = ?1, 0, 0? (18)
?v =
?
l?Iv
?u(l) ? sl (19)
where Iv is the set of all edges with target vertex
v and u(l) is the source vertex for edge l, and the
operations are defined by:
s1 ? s2 = ?p1 + p2, r
1
1h + r
1
2h, r
1
1m + r
1
2m?
s1 ? s2 = ?p1p2, p1r
1
2h + p2r
1
1h, p1r
1
2m + p2r
1
1m?
The backward algorithm for ?u can be implemented
via the forward algorithm in reverse through the
graph. The sufficient statistics for the gradient can
be accumulated during the backward pass noting
that:
?
j?Ji
Pij
?
gn
??igncijgn =
rnm(?0)
p(?0)
(20)
?
j?Ji
Pij
?
gn
cijgn =
rnh(?0)
p(?0)
(21)
where rnm(?) and r
n
h(?) extract the nth order r ele-
ments from the tuple for matches and counts, respec-
tively, and p(?) extracts the p element. The statistics
for the paths traveling via edge l can be computed
by:
?
j:l?Ji
Pij
?
gn
??igncijgn =
rnm(?u ? sl ? ?v)
p(?0)
(22)
?
j:l?Ji
Pij
?
gn
cijgn =
rnh(?u ? sl ? ?v)
p(?0)
(23)
where the u and v subscripts in ?u and ?v are the
start and end vertices for edge l. To avoid under-
flow, all the computations can be carried out in log
domain.
9A 3-tuple for uni-gram counts is used as an example in or-
der to save space. In a 5-tuple for bi-gram counts, all r elements
are computed independently of other r elements with the same
operations. Similarly, tri-gram counts require 7-tuples and four-
gram counts require 9-tuples.
162
tune cz-en de-en es-en fr-en
System TER BLEU TER BLEU TER BLEU TER BLEU
worst 66.03 18.09 69.03 16.28 60.56 21.02 62.75 21.83
best 53.75 28.36 58.39 24.28 50.26 30.55 50.48 30.87
latBLEU 53.99 29.25 56.70 26.49 48.34 34.55 48.90 33.90
nbExpBLEU 54.43 29.04 56.36 27.33 48.44 34.73 48.58 34.23
latExpBLEU 53.89 29.37 56.24 27.36 48.27 34.93 48.53 34.24
test cz-en de-en es-en fr-en
System TER BLEU TER BLEU TER BLEU TER BLEU
worst 65.35 17.69 69.03 15.83 61.22 19.79 62.36 21.36
best 52.21 29.54 58.00 24.16 50.15 30.14 50.15 30.32
latBLEU 52.80 29.89 55.87 26.22 48.29 33.91 48.51 32.93
nbExpBLEU 52.97 29.93 55.77 26.52 48.39 33.86 48.25 32.94
latExpBLEU 52.68 29.99 55.74 26.62 48.30 34.10 48.17 32.91
Table 1: Case insensitive TER and BLEU scores on newssyscombtune (tune) and newssyscombtest (test)
for combinations of outputs from four source languages. Three tuning methods were used: lattice BLEU (latBLEU),
N-best list based expected BLEU (nbExpBLEU), and lattice expected BLEU (latExpBLEU).
3.4 Entropy on a graph
Expanding the joint lattice to n-gram orders above
n = 2 is often impractical without pruning. If the
edge posteriors are not reliable, which is usually
the case for unoptimized weights, pruning might re-
move good quality paths from the graph. As a com-
promise, an incremental expansion strategy may be
adopted by first expanding and re-scoring the lattice
with a bi-gram, optimizing weights for xBLEU-2,
and then expanding and re-scoring the lattice with
a 5-gram. Pruning should be more reliable with the
edge posteriors computed using the tuned bi-gram
weights. A second set of weights may be tuned with
the 5-gram graph to maximize xBLEU-4.
When the bi-gram weights are tuned, it may be
beneficial to increase the edge score scaling factor
to focus the edge posteriors to the 1-best path. On
the other hand, a lower scaling factor may be bene-
ficial when tuning the 5-gram weights. Rosti et al
(2010) determined the scaling factor automatically
by fixing the perplexity of the merged N -best lists
used in tuning. Similar strategy may be adopted in
incremental n-gram expansion of the lattices.
Entropy on a graph can also be computed using
the expectation semiring formalism (Li and Eisner,
2009) by defining sl = ?pl, rl? where pl = e?sil and
rl = log pl. The entropy is given by:
Hi = log p(?0)?
r(?0)
p(?0)
(24)
where p(?0) and r(?0) extract the p and r elements
from the 2-tuple ?0, respectively. The average target
entropy over all sentences was set manually to 3.0
in the experiments based on the tuning convergence
and size of the pruned 5-gram lattices.
4 Experimental Evaluation
System outputs for all language pairs with En-
glish as the target were combined (cz-en,
de-en, es-en, and fr-en). Unpruned English
bi-gram and 5-gram language model compo-
nents were trained using the WMT11 corpora:
EuroParl, GigaFrEn, UNDoc Es, UNDoc Fr,
NewsCommentary, News2007, News2008,
News2009, News2010, and News2011.
Additional six Gigaword v4 components in-
cluded: AFP, APW, XIN+CNA, LTW, NYT, and
Headlines+Datelines. The total number
of words used to train the LMs was about 6.4
billion. Interpolation weights for the sixteen
components were tuned to minimize perplexity on
the newstest2010-ref.en development set.
The modified Kneser-Ney smoothing (Chen and
163
Goodman, 1998) was used in training. Experiments
using a LM trained on the system outputs and inter-
polated with the general LM were also conducted.
The interpolation weights between 0.1 and 0.9 were
tried, and the weight yielding the highest BLEU
score on the tuning set was selected. A tri-gram true
casing model was trained on all the LM training
data. This model was used to restore the case of the
lower-case system combination output.
All twelve 1-best system outputs on cz-en, 26
outputs on de-en, 16 outputs on es-en, and 24
outputs on fr-en were combined. Three different
weight optimization methods were tried. First, lat-
tice based 1-best BLEU optimization of the bi-gram
decoding weights followed by N-best list based
BLEU optimization of 5-gram re-scoring weights
using 300-best lists, both using downhill simplex.
Second, N-best list based expected BLEU optimiza-
tion of the bi-gram and 5-gram weights using 300-
best lists with merging between bi-gram decoding
iterations. Third, lattice based expected BLEU opti-
mization of bi-gram and 5-gram decoding weights.
The L-BFGS (Liu and Nocedal, 1989) algorithm
was used in gradient ascent. Results for all four sin-
gle source experiments are shown in Table 1, includ-
ing case insensitive TER (Snover et al, 2006) and
BLEU scores for the worst and best systems, and
the system combination outputs for the three tuning
methods. The gains on tuning and test sets were con-
sistent, though relatively smaller on cz-en due to
a single system (online-B) dominating the other
systems by about 5-6 BLEU points. The tuning
method had very little influence on the test set scores
apart from de-en where the lattice BLEU opti-
mization yields slightly lower BLEU scores. This
seems to suggest that the gradient free optimization
is not as stable with a larger number of weights.10
The novel bi-gram feature did not have significant
influence on the TER or BLEU scores, but the num-
ber of novel bi-grams was reduced by up to 100%.
Finally, experiments combining 39 system out-
puts by taking the top half of the outputs from each
language pair were performed. The selection was
based on case insensitive BLEU scores on the tun-
ing set. Table 2 shows the scores for seven combi-
10A total number of 30 weights, 26 system and 4 feature
weights, were tuned for de-en.
xx-en tune test
System TER BLEU TER BLEU
worst 62.81 21.19 62.92 20.29
best 51.11 30.87 50.80 30.32
latBLEU 40.95 40.75 41.06 39.81
+biasLM 41.18 40.90 41.16 39.90
nbExpBLEU 40.81 41.36 41.05 40.15
+biasLM 40.72 41.99 40.65 40.89
latExpBLEU 40.57 41.68 40.62 40.60
+biasLM 40.42 42.23 40.52 41.38
-nBgF 40.85 41.41 40.88 40.55
Table 2: Case insensitive TER and BLEU scores on
newssyscombtune (tune) and newssyscombtest
(test) for xx-en combination. Combinations using lat-
tice BLEU tuning (latBLEU), N-best list based expected
BLEU tuning (nbExpBLEU), and lattice expected BLEU
tuning (latExpBLEU) with and without the system out-
put biased LM (biasLM) are shown. Final row, marked
nBgF, corresponds to the above tuning without the novel
bi-gram feature.
nations using the three tuning methods with or with-
out the system output biased LM, and finally without
the novel bi-gram count feature. There is a clear ad-
vantage from the expected BLEU tuning on the tun-
ing set, and lattice tuning yields better scores than
N-best list based tuning. The difference between
latBLEU and nbExpBLEU without biasLM is
not quite as large on the test set but latExpBLEU
yields significant gains over both. The biasLM also
yields significant gains on all but latBLEU tuning.
Finally, removing the novel bi-gram count feature
results in a significant loss, probably due to the large
number of input hypotheses. The number of novel
bi-grams in the test set output was reduced to zero
when using this feature.
5 Conclusions
The BBN submissions for WMT11 system combi-
nation task were described in this paper together
with a differentiable objective function, graph ex-
pected BLEU, which scales well for a large number
of weights and can be generalized to hypergraphs.
System output biased language model and a novel
bi-gram count feature also gave significant gains on
a 39 system multi-source combination.
164
References
Stanley F. Chen and Joshua Goodman. 1998. An empir-
ical study of smoothing techniques for language mod-
eling. Technical Report TR-10-98, Computer Science
Group Harvard University.
Zhifei Li and Jason Eisner. 2009. First- and second-order
expectation semirings with applications to minimum-
risk training on translation forests. In Proceedings of
the 2009 Conference on Empirical Methods in Natural
Language Processing, pages 40?51.
Dong C. Liu and Jorge Nocedal. 1989. On the limited
memory BFGS method for large scale optimization.
Mathematical Programming, 45(3):503?528.
Franz J. Och. 2003. Minimum error rate training in sta-
tistical machine translation. In Proceedings of the 41st
Annual Meeting of the Association for Computational
Linguistics, pages 160?167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic eval-
uation of machine translation. In Proceedings of the
40th Annual Meeting of the Association for Computa-
tional Linguistics, pages 311?318.
Adam Pauls, John DeNero, and Dan Klein. 2009. Con-
sensus training for consensus decoding in machine
translation. In Proceedings of the 2009 Conference on
Empirical Methods in Natural Language Processing,
pages 1418?1427.
William H. Press, Saul A. Teukolsky, William T. Vetter-
ling, and Brian P. Flannery. 2007. Numerical recipes:
the art of scientific computing. Cambridge University
Press, 3rd edition.
Antti-Veikko I. Rosti, Spyros Matsoukas, and Richard
Schwartz. 2007. Improved word-level system combi-
nation for machine translation. In Proceedings of the
45th Annual Meeting of the Association of Computa-
tional Linguistics, pages 312?319.
Antti-Veikko I. Rosti, Bing Zhang, Spyros Matsoukas,
and Richard Schwartz. 2009. Incremental hypothe-
sis alignment with flexible matching for building con-
fusion networks: BBN system description for WMT09
system combination task. In Proceedings of the Fourth
Workshop on Statistical Machine Translation, pages
61?65.
Antti-Veikko I. Rosti, Bing Zhang, Spyros Matsoukas,
and Richard Schwartz. 2010. BBN system description
for WMT10 system combination task. In Proceedings
of the Fifth Workshop on Statistical Machine Transla-
tion, pages 321?326.
David A. Smith and Jason Eisner. 2006. Minimum risk
annealing for training log-linear models. In Proceed-
ings of the 21st International Conference on Computa-
tional Linguistics and 44th Annual Meeting of the As-
sociation for Computational Linguistics, pages 787?
794.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciula, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings of the 7th Conference of the Associa-
tion for Machine Translation in the Americas, pages
223?231.
165
Proceedings of the 7th Workshop on Statistical Machine Translation, pages 191?199,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
Review of Hypothesis Alignment Algorithms for MT System Combination
via Confusion Network Decoding
Antti-Veikko I. Rostia?, Xiaodong Heb, Damianos Karakosc, Gregor Leuschd?, Yuan Caoc,
Markus Freitage, Spyros Matsoukasf , Hermann Neye, Jason R. Smithc and Bing Zhangf
aApple Inc., Cupertino, CA 95014
arosti@apple.com
bMicrosoft Research, Redmond, WA 98052
xiaohe@microsoft.com
cJohns Hopkins University, Baltimore, MD 21218
{damianos,yuan.cao,jrsmith}@jhu.edu
dSAIC, Monheimsallee 22, D-52062 Aachen, Germany
gregor.leusch@saic.com
eRWTH Aachen University, D-52056 Aachen, Germany
{freitag,ney}@cs.rwth-aachen.de
fRaytheon BBN Technologies, 10 Moulton Street, Cambridge, MA 02138
{smatsouk,bzhang}@bbn.com
Abstract
Confusion network decoding has proven to
be one of the most successful approaches
to machine translation system combination.
The hypothesis alignment algorithm is a cru-
cial part of building the confusion networks
and many alternatives have been proposed in
the literature. This paper describes a sys-
tematic comparison of five well known hy-
pothesis alignment algorithms for MT sys-
tem combination via confusion network de-
coding. Controlled experiments using identi-
cal pre-processing, decoding, and weight tun-
ing methods on standard system combina-
tion evaluation sets are presented. Transla-
tion quality is assessed using case insensitive
BLEU scores and bootstrapping is used to es-
tablish statistical significance of the score dif-
ferences. All aligners yield significant BLEU
score gains over the best individual system in-
cluded in the combination. Incremental indi-
rect hidden Markov model and a novel incre-
mental inversion transduction grammar with
flexible matching consistently yield the best
translation quality, though keeping all things
equal, the differences between aligners are rel-
atively small.
?The work reported in this paper was carried out while the
authors were at Raytheon BBN Technologies and
?RWTH Aachen University.
1 Introduction
Current machine translation (MT) systems are based
on different paradigms, such as rule-based, phrase-
based, hierarchical, and syntax-based. Due to the
complexity of the problem, systems make various
assumptions at different levels of processing and
modeling. Many of these assumptions may be
suboptimal and complementary. The complemen-
tary information in the outputs from multiple MT
systems may be exploited by system combination.
Availability of multiple system outputs within the
DARPA GALE program as well as NIST Open MT
and Workshop on Statistical Machine Translation
evaluations has led to extensive research in combin-
ing the strengths of diverse MT systems, resulting in
significant gains in translation quality.
System combination methods proposed in the lit-
erature can be roughly divided into three categories:
(i) hypothesis selection (Rosti et al, 2007b; Hilde-
brand and Vogel, 2008), (ii) re-decoding (Frederking
and Nirenburg, 1994; Jayaraman and Lavie, 2005;
Rosti et al, 2007b; He and Toutanova, 2009; De-
vlin et al, 2011), and (iii) confusion network de-
coding. Confusion network decoding has proven to
be the most popular as it does not require deep N -
best lists1 and operates on the surface strings. It has
1N -best lists of around N = 10 have been used in confu-
sion network decoding yielding small gains over using 1-best
191
also been shown to be very successful in combining
speech recognition outputs (Fiscus, 1997; Mangu et
al., 2000). The first application of confusion net-
work decoding in MT system combination appeared
in (Bangalore et al, 2001) where a multiple string
alignment (MSA), made popular in biological se-
quence analysis, was applied to the MT system out-
puts. Matusov et al (2006) proposed an alignment
based on GIZA++ Toolkit which introduced word
reordering not present in MSA, and Sim et al (2007)
used the alignments produced by the translation edit
rate (TER) (Snover et al, 2006) scoring. Extensions
of the last two are included in this study together
with alignments based on hidden Markov model
(HMM) (Vogel et al, 1996) and inversion transduc-
tion grammars (ITG) (Wu, 1997).
System combinations produced via confusion net-
work decoding using different hypothesis alignment
algorithms have been entered into open evalua-
tions, most recently in 2011 Workshop on Statistical
Machine Translation (Callison-Burch et al, 2011).
However, there has not been a comparison of the
most popular hypothesis alignment algorithms us-
ing the same sets of MT system outputs and other-
wise identical combination pipelines. This paper at-
tempts to systematically compare the quality of five
hypothesis alignment algorithms. Alignments were
produced for the same system outputs from three
common test sets used in the 2009 NIST Open MT
Evaluation and the 2011 Workshop on Statistical
Machine Translation. Identical pre-processing, de-
coding, and weight tuning algorithms were used to
quantitatively evaluate the alignment quality. Case
insensitive BLEU score (Papineni et al, 2002) was
used as the translation quality metric.
2 Confusion Network Decoding
A confusion network is a linear graph where all
paths visit all nodes. Two consecutive nodes may be
connected by one or more arcs. Given the arcs repre-
sent words in hypotheses, multiple arcs connecting
two consecutive nodes can be viewed as alternative
words in that position of a set of hypotheses encoded
by the network. A special NULL token represents
a skipped word and will not appear in the system
combination output. For example, three hypotheses
outputs (Rosti et al, 2011).
?twelve big cars?, ?twelve cars?, and ?dozen cars?
may be aligned as follows:
twelve big blue cars
twelve NULL NULL cars
dozen NULL blue cars
This alignment may be represented compactly as the
confusion network in Figure 1 which encodes a total
of eight unique hypotheses.
40 1twelve(2)dozen(1) 2big(1)NULL(2) 3blue(2)NULL(1) cars(3)
Figure 1: Confusion network from three strings ?twelve
big blue cars?, ?twelve cars?, and ?dozen blue cars? us-
ing the first as the skeleton. The numbers in parentheses
represent counts of words aligned to the corresponding
arc.
Building confusion networks from multiple ma-
chine translation system outputs has two main prob-
lems. First, one output has to be chosen as the skele-
ton hypothesis which defines the final word order of
the system combination output. Second, MT system
outputs may have very different word orders which
complicates the alignment process. For skeleton se-
lection, Sim et al (2007) proposed choosing the out-
put closest to all other hypotheses when using each
as the reference string in TER. Alternatively, Ma-
tusov et al (2006) proposed leaving the decision to
decoding time by connecting networks built using
each output as a skeleton into a large lattice. The
subnetworks in the latter approach may be weighted
by prior probabilities estimated from the alignment
statistics (Rosti et al, 2007a). Since different align-
ment algorithm produce different statistics and the
gain from the weights is relatively small (Rosti et al,
2011), weights for the subnetworks were not used
in this work. The hypothesis alignment algorithms
used in this work are briefly described in the follow-
ing section.
The confusion networks in this work were repre-
sented in a text lattice format shown in Figure 2.
Each line corresponds to an arc, where J is the arc
index, S is the start node index, E is the end node in-
dex, SC is the score vector, and W is the word label.
The score vector has as many elements as there are
input systems. The elements correspond to each sys-
tem and indicate whether a word from a particular
192
J=0 S=0 E=1 SC=(1,1,0) W=twelve
J=1 S=0 E=1 SC=(0,0,1) W=dozen
J=2 S=1 E=2 SC=(1,0,0) W=big
J=3 S=1 E=2 SC=(0,1,1) W=NULL
J=4 S=2 E=3 SC=(1,0,1) W=blue
J=5 S=2 E=3 SC=(0,1,0) W=NULL
J=6 S=3 E=4 SC=(1,1,1) W=cars
Figure 2: A lattice in text format representing the con-
fusion network in Figure 1. J is the arc index, S and E
are the start and end node indexes, SC is a vector of arc
scores, and W is the word label.
system was aligned to a given link2. These may be
viewed as system specific word confidences, which
are binary when aligning 1-best system outputs. If
no word from a hypothesis is aligned to a given link,
a NULL word token is generated provided one does
not already exist, and the corresponding element in
the NULL word token is set to one. The system
specific word scores are kept separate in order to
exploit system weights in decoding. Given system
weights wn, which sum to one, and system specific
word scores snj for each arc j (the SC elements), the
weighted word scores are defined as:
sj =
Ns?
n=1
wnsnj (1)
where Ns is the number of input systems. The hy-
pothesis score is defined as the sum of the log-word-
scores along the path, which is linearly interpolated
with a logarithm of the language model (LM) score
and a non-NULL word count:
S(E|F ) =
?
j?J (E)
log sj + ?SLM (E) + ?Nw(E)
(2)
where J (E) is the sequence of arcs generating the
hypothesis E for the source sentence F , SLM (E)
is the LM score, and Nw(E) is the number of
non-NULL words. The set of weights ? =
{w1, . . . , wNs , ?, ?} can be tuned so as to optimize
an evaluation metric on a development set.
Decoding with an n-gram language model re-
quires expanding the lattice to distinguish paths with
2A link is used as a synonym to the set of arcs between two
consecutive nodes. The name refers to the confusion network
structure?s resemblance to a sausage.
unique n-gram contexts before LM scores can be as-
signed the arcs. Using long n-gram context may re-
quire pruning to reduce memory usage. Given uni-
form initial system weights, pruning may remove
desirable paths. In this work, the lattices were ex-
panded to bi-gram context and no pruning was per-
formed. A set of bi-gram decoding weights were
tuned directly on the expanded lattices using a dis-
tributed optimizer (Rosti et al, 2010). Since the
score in Equation 2 is not a simple log-linear inter-
polation, the standard minimum error rate training
(Och, 2003) with exact line search cannot be used.
Instead, downhill simplex (Press et al, 2007) was
used in the optimizer client. After bi-gram decod-
ing weight optimization, another set of 5-gram re-
scoring weights were tuned on 300-best lists gener-
ated from the bi-gram expanded lattices.
3 Hypothesis Alignment Algorithms
Two different methods have been proposed for
building confusion networks: pairwise and incre-
mental alignment. In pairwise alignment, each
hypothesis corresponding to a source sentence is
aligned independently with the skeleton hypothe-
sis. This set of alignments is consolidated using the
skeleton words as anchors to form the confusion net-
work (Matusov et al, 2006; Sim et al, 2007). The
same word in two hypotheses may be aligned with a
different word in the skeleton resulting in repetition
in the network. A two-pass alignment algorithm to
improve pairwise TER alignments was introduced in
(Ayan et al, 2008). In incremental alignment (Rosti
et al, 2008), the confusion network is initialized by
forming a simple graph with one word per link from
the skeleton hypothesis. Each remaining hypothesis
is aligned with the partial confusion network, which
allows words from all previous hypotheses be con-
sidered as matches. The order in which the hypothe-
ses are aligned may influence the alignment qual-
ity. Rosti et al (2009) proposed a sentence specific
alignment order by choosing the unaligned hypoth-
esis closest to the partial confusion network accord-
ing to TER. The following five alignment algorithms
were used in this study.
193
3.1 Pairwise GIZA++ Enhanced Hypothesis
Alignment
Matusov et al (2006) proposed using the GIZA++
Toolkit (Och and Ney, 2003) to align a set of tar-
get language translations. A parallel corpus where
each system output acting as a skeleton appears as
a translation of all system outputs corresponding to
the same source sentence. The IBM Model 1 (Brown
et al, 1993) and hidden Markov model (HMM) (Vo-
gel et al, 1996) are used to estimate the alignment.
Alignments from both ?translation? directions are
used to obtain symmetrized alignments by interpo-
lating the HMM occupation statistics (Matusov et
al., 2004). The algorithm may benefit from the fact
that it considers the entire test set when estimating
the alignment model parameters; i.e., word align-
ment links from all output sentences influence the
estimation, whereas other alignment algorithms only
consider words within a pair of sentences (pairwise
alignment) or all outputs corresponding to a single
source sentence (incremental alignment). However,
it does not naturally extend to incremental align-
ment. The monotone one-to-one alignments are then
transformed into a confusion network. This aligner
is referred to as GIZA later in this paper.
3.2 Incremental Indirect Hidden Markov
Model Alignment
He et al (2008) proposed using an indirect hidden
Markov model (IHMM) for pairwise alignment of
system outputs. The parameters of the IHMM are
estimated indirectly from a variety of sources in-
cluding semantic word similarity, surface word sim-
ilarity, and a distance-based distortion penalty. The
alignment between two target language outputs are
treated as the hidden states. A standard Viterbi al-
gorithm is used to infer the alignment. The pair-
wise IHMM was extended to operate incrementally
in (Li et al, 2009). Sentence specific alignment or-
der is not used by this aligner, which is referred to
as iIHMM later in this paper.
3.3 Incremental Inversion Transduction
Grammar Alignment with Flexible
Matching
Karakos et al (2008) proposed using inversion trans-
duction grammars (ITG) (Wu, 1997) for pairwise
alignment of system outputs. ITGs form an edit
distance, invWER (Leusch et al, 2003), that per-
mits properly nested block movements of substrings.
For well-formed sentences, this may be more nat-
ural than allowing arbitrary shifts. The ITG algo-
rithm is very expensive due to its O(n6) complexity.
The search algorithm for the best ITG alignment, a
best-first chart parsing (Charniak et al, 1998), was
augmented with an A? search heuristic of quadratic
complexity (Klein and Manning, 2003), resulting in
significant reduction in computational complexity.
The finite state-machine heuristic computes a lower
bound to the alignment cost of two strings by allow-
ing arbitrary word re-orderings. The ITG hypothesis
alignment algorithm was extended to operate incre-
mentally in (Karakos et al, 2010) and a novel ver-
sion where the cost function is computed based on
the stem/synonym similarity of (Snover et al, 2009)
was used in this work. Also, a sentence specific
alignment order was used. This aligner is referred
to as iITGp later in this paper.
3.4 Incremental Translation Edit Rate
Alignment with Flexible Matching
Sim et al (2007) proposed using translation edit rate
scorer3 to obtain pairwise alignment of system out-
puts. The TER scorer tries to find shifts of blocks
of words that minimize the edit distance between
the shifted reference and a hypothesis. Due to the
computational complexity, a set of heuristics is used
to reduce the run time (Snover et al, 2006). The
pairwise TER hypothesis alignment algorithm was
extended to operate incrementally in (Rosti et al,
2008) and also extended to consider synonym and
stem matches in (Rosti et al, 2009). The shift
heuristics were relaxed for flexible matching to al-
low shifts of blocks of words as long as the edit dis-
tance is decreased even if there is no exact match in
the new position. A sentence specific alignment or-
der was used by this aligner, which is referred to as
iTER later in this paper.
3.5 Incremental Translation Edit Rate Plus
Alignment
Snover et al (2009) extended TER scoring to con-
sider synonyms and paraphrase matches, called
3http://www.cs.umd.edu/?snover/tercom/
194
TER-plus (TERp). The shift heuristics in TERp
were also relaxed relative to TER. Shifts are allowed
if the words being shifted are: (i) exactly the same,
(ii) synonyms, stems or paraphrases of the corre-
sponding reference words, or (iii) any such combina-
tion. Xu et al (2011) proposed using an incremental
version of TERp for building consensus networks. A
sentence specific alignment order was used by this
aligner, which is referred to as iTERp later in this
paper.
4 Experimental Evaluation
Combination experiments were performed on (i)
Arabic-English, from the informal system combi-
nation track of the 2009 NIST Open MT Evalua-
tion4; (ii) German-English from the system com-
bination evaluation of the 2011 Workshop on Sta-
tistical Machine Translation (Callison-Burch et al,
2011) (WMT11) and (iii) Spanish-English, again
from WMT11. Eight top-performing systems (as
evaluated using case-insensitive BLEU) were used
in each language pair. Case insensitive BLEU scores
for the individual system outputs on the tuning and
test sets are shown in Table 1. About 300 and
800 sentences with four reference translations were
available for Arabic-English tune and test sets, re-
spectively, and about 500 and 2500 sentences with a
single reference translation were available for both
German-English and Spanish-English tune and test
sets. The system outputs were lower-cased and to-
kenized before building confusion networks using
the five hypothesis alignment algorithms described
above. Unpruned English bi-gram and 5-gram lan-
guage models were trained with about 6 billion
words available for these evaluations. Multiple com-
ponent language models were trained after dividing
the monolingual corpora by source. Separate sets
of interpolation weights were tuned for the NIST
and WMT experiments to minimize perplexity on
the English reference translations of the previous
evaluations, NIST MT08 and WMT10. The sys-
tem combination weights, both bi-gram lattice de-
coding and 5-gram 300-best list re-scoring weights,
were tuned separately for lattices build with each hy-
pothesis alignment algorithm. The final re-scoring
4http://www.itl.nist.gov/iad/mig/tests/
mt/2009/ResultsRelease/indexISC.html
outputs were detokenized before computing case in-
sensitive BLEU scores. Statistical significance was
computed for each pairwise comparison using boot-
strapping (Koehn, 2004).
Decode Oracle
Aligner tune test tune test
GIZA 60.06 57.95 75.06 74.47
iTER 59.74 58.63? 73.84 73.20
iTERp 60.18 59.05? 76.43 75.58
iIHMM 60.51 59.27?? 76.50 76.17
iITGp 60.65 59.37?? 76.53 76.05
Table 2: Case insensitive BLEU scores for NIST MT09
Arabic-English system combination outputs. Note, four
reference translations were available. Decode corre-
sponds to results after weight tuning and Oracle corre-
sponds to graph TER oracle. Dagger (?) denotes statisti-
cally significant difference compared to GIZA and double
dagger (?) compared to iTERp and the aligners above it.
The BLEU scores for Arabic-English system
combination outputs are shown in Table 2. The first
column (Decode) shows the scores on tune and test
sets for the decoding outputs. The second column
(Oracle) shows the scores for oracle hypotheses ob-
tained by aligning the reference translations with the
confusion networks and choosing the path with low-
est graph TER (Rosti et al, 2008). The rows rep-
resenting different aligners are sorted according to
the test set decoding scores. The order of the BLEU
scores for the oracle translations do not always fol-
low the order for the decoding outputs. This may be
due to differences in the compactness of the confu-
sion networks. A more compact network has fewer
paths and is therefore less likely to contain signif-
icant parts of the reference translation, whereas a
reference translation may be generated from a less
compact network. On Arabic-English, all incremen-
tal alignment algorithms are significantly better than
the pairwise GIZA, incremental IHMM and ITG
with flexible matching are significantly better than
all other algorithms, but not significantly different
from each other. The incremental TER and TERp
were statistically indistinguishable. Without flexi-
ble matching, iITG yields a BLEU score of 58.85 on
test. The absolute BLEU gain over the best individ-
ual system was between 6.2 and 7.6 points on the
test set.
195
Arabic German Spanish
System tune test tune test tune test
A 48.84 48.54 21.96 21.41 27.71 27.13
B 49.15 48.97 22.61 21.80 28.42 27.90
C 49.30 49.50 22.77 21.99 28.57 28.23
D 49.38 49.59 22.90 22.41 29.00 28.41
E 49.42 49.75 22.90 22.65 29.15 28.50
F 50.28 50.69 22.98 22.65 29.53 28.61
G 51.49 50.81 23.41 23.06 29.89 29.82
H 51.72 51.74 24.28 24.16 30.55 30.14
Table 1: Case insensitive BLEU scores for the individual system outputs on the tune and test sets for all three source
languages.
Decode Oracle
Aligner tune test tune test
GIZA 25.93 26.02 37.32 38.22
iTERp 26.46 26.10 38.16 38.76
iTER 26.27 26.39? 37.00 37.66
iIHMM 26.34 26.40? 37.87 38.48
iITGp 26.47 26.50? 37.99 38.60
Table 3: Case insensitive BLEU scores for WMT11
German-English system combination outputs. Note, only
a single reference translation per segment was available.
Decode corresponds to results after weight tuning and
Oracle corresponds to graph TER oracle. Dagger (?)
denotes statistically significant difference compared to
iTERp and GIZA.
The BLEU scores for German-English system
combination outputs are shown in Table 3. Again,
the graph TER oracle scores do not follow the same
order as the decoding scores. The scores for GIZA
and iTERp are statistically indistinguishable, and
iTER, iIHMM, and iITGp are significantly better
than the first two. However, they are not statistically
different from each other. Without flexible match-
ing, iITG yields a BLEU score of 26.47 on test. The
absolute BLEU gain over the best individual system
was between 1.9 and 2.3 points on the test set.
The BLEU scores for Spanish-English system
combination outputs are shown in Table 4. All align-
ers but iIHMM are statistically indistinguishable and
iIHMM is significantly better than all other align-
ers. Without flexible matching, iITG yields a BLEU
score of 33.62 on test. The absolute BLEU gain over
the best individual system was between 3.5 and 3.9
Decode Oracle
Aligner tune test tune test
iTERp 34.20 33.61 50.45 51.28
GIZA 34.02 33.62 50.23 51.20
iTER 34.44 33.79 50.39 50.39
iITGp 34.41 33.85 50.55 51.33
iIHMM 34.61 34.05? 50.48 51.27
Table 4: Case insensitive BLEU scores for WMT11
Spanish-English system combination outputs. Note, only
a single reference translation per segment was available.
Decode corresponds to results after weight tuning and
Oracle corresponds to graph TER oracle. Dagger (?)
denotes statistically significant difference compared to
aligners above iIHMM.
points on the test set.
5 Error Analysis
Error analysis was performed to better understand
the gains from system combination. Specifically, (i)
how the different types of translation errors are af-
fected by system combination was investigated; and
(ii) an attempt to quantify the correlation between
the word agreement that results from the different
aligners and the translation error, as measured by
TER (Snover et al, 2006), was made.
5.1 Influence on Error Types
For each one of the individual systems, and for each
one of the three language pairs, the per-sentence er-
rors that resulted from that system, as well as from
each one of the the different aligners studied in this
paper, were computed. The errors were broken
196
down into insertions/deletions/substitutions/shifts
based on the TER scorer.
The error counts at the document level were ag-
gregated. For each document in each collection, the
number of errors of each type that resulted from each
individual system as well as each system combina-
tion were measured, and their difference was com-
puted. If the differences are mostly positive, then
it can be said (with some confidence) that system
combination has a significant impact in reducing the
error of that type. A paired Wilcoxon test was per-
formed and the p-value that quantifies the probabil-
ity that the measured error reduction was achieved
under the null hypothesis that the system combina-
tion performs as well as the best system was com-
puted.
Table 5 shows all conditions under consideration.
All cases where the p-value is below 10?2 are con-
sidered statistically significant. Two observations
are in order: (i) all alignment schemes significantly
reduce the number of substitution/shift errors; (ii)
in the case of insertions/deletions, there is no clear
trend; there are cases where the system combination
increases the number of insertions/deletions, com-
pared to the individual systems.
5.2 Relationship between Word Agreement
and Translation Error
This set of experiments aimed to quantify the rela-
tionship between the translation error rate and the
amount of agreement that resulted from each align-
ment scheme. The amount of system agreement at
a level x is measured by the number of cases (con-
fusion network arcs) where x system outputs con-
tribute the same word in a confusion network bin.
For example, the agreement at level 2 is equal to 2
in Figure 1 because there are exactly 2 arcs (with
words ?twelve? and ?blue?) that resulted from the
agreement of 2 systems. Similarly, the agreement at
level 3 is 1, because there is only 1 arc (with word
?cars?) that resulted from the agreement of 3 sys-
tems. It is hypothesized that a sufficiently high level
of agreement should be indicative of the correctness
of a word (and thus indicative of lower TER). The
agreement statistics were grouped into two values:
the ?weak? agreement statistic, where at most half
of the combined systems contribute a word, and the
?strong? agreement statistic, where more than half
non-NULL words NULL words
weak strong weak strong
Arabic 0.087 -0.068 0.192 0.094
German 0.117 -0.067 0.206 0.147
Spanish 0.085 -0.134 0.323 0.102
Table 6: Regression coefficients of the ?strong? and
?weak? agreement features, as computed with a gener-
alized linear model, using TER as the target variable.
of the combined systems contribute a word. To sig-
nify the fact that real words and ?NULL? tokens
have different roles and should be treated separately,
two sets of agreement statistics were computed.
A regression with a generalized linear model
(glm) that computed the coefficients of the agree-
ment quantities (as explained above) for each align-
ment scheme, using TER as the target variable, was
performed. Table 6 shows the regression coeffi-
cients; they are all significant at p-value < 0.001.
As is clear from this table, the negative coefficient of
the ?strong? agreement quantity for the non-NULL
words points to the fact that good aligners tend to
result in reductions in translation error. Further-
more, increasing agreements on NULL tokens does
not seem to reduce TER.
6 Conclusions
This paper presented a systematic comparison of
five different hypothesis alignment algorithms for
MT system combination via confusion network de-
coding. Pre-processing, decoding, and weight tun-
ing were controlled and only the alignment algo-
rithm was varied. Translation quality was compared
qualitatively using case insensitive BLEU scores.
The results showed that confusion network decod-
ing yields a significant gain over the best individ-
ual system irrespective of the alignment algorithm.
Differences between the combination output using
different alignment algorithms were relatively small,
but incremental alignment consistently yielded bet-
ter translation quality compared to pairwise align-
ment based on these experiments and previously
published literature. Incremental IHMM and a novel
incremental ITG with flexible matching consistently
yield highest quality combination outputs. Further-
more, an error analysis shows that most of the per-
197
Language Aligner ins del sub shft
GIZA 2.2e-16 0.9999 2.2e-16 2.2e-16
iHMM 2.2e-16 0.433 2.2e-16 2.2e-16
Arabic iITGp 0.8279 2.2e-16 2.2e-16 2.2e-16
iTER 4.994e-07 3.424e-11 2.2e-16 2.2e-16
iTERp 2.2e-16 1 2.2e-16 2.2e-16
GIZA 7.017e-12 2.588e-06 2.2e-16 2.2e-16
iHMM 6.858e-07 0.4208 2.2e-16 2.2e-16
German iITGp 0.8551 0.2848 2.2e-16 2.2e-16
iTER 0.2491 1.233e-07 2.2e-16 2.2e-16
iTERp 0.9997 0.007489 2.2e-16 2.2e-16
GIZA 2.2e-16 0.8804 2.2e-16 2.2e-16
iHMM 2.2e-16 1 2.2e-16 2.2e-16
Spanish iITGp 2.2e-16 0.9999 2.2e-16 2.2e-16
iTER 2.2e-16 1 2.2e-16 2.2e-16
iTERp 3.335e-16 1 2.2e-16 2.2e-16
Table 5: p-values which show which error types are statistically significantly improved for each language and aligner.
formance gains from system combination can be at-
tributed to reductions in substitution errors and word
re-ordering errors. Finally, better alignments of sys-
tem outputs, which tend to cause higher agreement
rates on words, correlate with reductions in transla-
tion error.
References
Necip Fazil Ayan, Jing Zheng, and Wen Wang. 2008.
Improving alignments for better confusion networks
for combining machine translation systems. In Proc.
Coling, pages 33?40.
Srinivas Bangalore, German Bordel, and Giuseppe Ric-
cardi. 2001. Computing consensus translation from
multiple machine translation systems. In Proc. ASRU,
pages 351?354.
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. 1993. The mathematics
of statistical machine translation: Parameter estima-
tion. Computational Linguistics, 19(2):263?311.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Omar F. Zaidan. 2011. Findings of the 2011
workshop on statistical machine translation. In Proc.
WMT, pages 22?64.
Eugene Charniak, Sharon Goldwater, and Mark Johnson.
1998. Edge-based best-first chart parsing. In Proc.
Sixth Workshop on Very Large Corpora, pages 127?
133. Morgan Kaufmann.
Jacob Devlin, Antti-Veikko I. Rosti, Shankar Ananthakr-
ishnan, and Spyros Matsoukas. 2011. System combi-
nation using discriminative cross-adaptation. In Proc.
IJCNLP, pages 667?675.
Jonathan G. Fiscus. 1997. A post-processing system to
yield reduced word error rates: Recognizer output vot-
ing error reduction (ROVER). In Proc. ASRU, pages
347?354.
Robert Frederking and Sergei Nirenburg. 1994. Three
heads are better than one. In Proc. ANLP, pages 95?
100.
Xiaodong He and Kristina Toutanova. 2009. Joint opti-
mization for machine translation system combination.
In Proc. EMNLP, pages 1202?1211.
Xiaodong He, Mei Yang, Jianfeng Gao, Patrick Nguyen,
and Robert Moore. 2008. Indirect-hmm-based hy-
pothesis alignment for combining outputs from ma-
chine translation systems. In Proc. EMNLP, pages 98?
107.
Almut S. Hildebrand and Stephan Vogel. 2008. Combi-
nation of machine translation systems via hypothesis
selection from combined n-best lists. In AMTA, pages
254?261.
Shyamsundar Jayaraman and Alon Lavie. 2005. Multi-
engine machine translation guided by explicit word
matching. In Proc. EAMT.
Damianos Karakos, Jason Eisner, Sanjeev Khudanpur,
and Markus Dreyer. 2008. Machine translation sys-
tem combination using ITG-based alignments. In
Proc. ACL, pages 81?84.
Damianos Karakos, Jason R. Smith, and Sanjeev Khu-
danpur. 2010. Hypothesis ranking and two-pass ap-
proaches for machine translation system combination.
In Proc. ICASSP.
198
Dan Klein and Christopher D. Manning. 2003. A*
parsing: Fast exact Viterbi parse selection. In Proc.
NAACL, pages 40?47.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Proc. EMNLP,
pages 388?395.
Gregor Leusch, Nicola Ueffing, and Hermann Ney. 2003.
A novel string-to-string distance measure with appli-
cations to machine translation evaluation. In Proc. MT
Summit 2003, pages 240?247, September.
Chi-Ho Li, Xiaodong He, Yupeng Liu, and Ning Xi.
2009. Incremental hmm alignment for mt system com-
bination. In Proc. ACL/IJCNLP, pages 949?957.
Lidia Mangu, Eric Brill, and Andreas Stolcke. 2000.
Finding consensus in speech recognition: Word error
minimization and other applications of confusion net-
works. Computer Speech and Language, 14(4):373?
400.
Evgeny Matusov, Richard Zens, and Hermann Ney.
2004. Symmetric word alignments for statistical ma-
chine translation. In Proc. COLING, pages 219?225.
Evgeny Matusov, Nicola Ueffing, and Hermann Ney.
2006. Computing consensus translation from multiple
machine translation systems using enhanced hypothe-
ses alignment. In Proc. EACL, pages 33?40.
Franz J. Och and Hermann Ney. 2003. A systematic
comparison of various statistical alignment models.
Computational Linguistics, 29(1):19?51.
Franz J. Och. 2003. Minimum error rate training in sta-
tistical machine translation. In Proc. ACL, pages 160?
167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic eval-
uation of machine translation. In Proc. ACL, pages
311?318.
William H. Press, Saul A. Teukolsky, William T. Vetter-
ling, and Brian P. Flannery. 2007. Numerical recipes:
the art of scientific computing. Cambridge University
Press, 3rd edition.
Antti-Veikko I. Rosti, Spyros Matsoukas, and Rirchard
Schwartz. 2007a. Improved word-level system com-
bination for machine translation. In Proc. ACL, pages
312?319.
Antti-Veikko I. Rosti, Bing Xiang, Spyros Matsoukas,
Richard Schwartz, Necip Fazil Ayan, and Bonnie J.
Dorr. 2007b. Combining outputs from multiple
machine translation systems. In Proc. NAACL-HLT,
pages 228?235.
Antti-Veikko I. Rosti, Bing Zhang, Spyros Matsoukas,
and Richard Schwartz. 2008. Incremental hypothesis
alignment for building confusion networks with appli-
cation to machine translation system combination. In
Proceedings of the Third Workshop on Statistical Ma-
chine Translation, pages 183?186.
Antti-Veikko I. Rosti, Bing Zhang, Spyros Matsoukas,
and Richard Schwartz. 2009. Incremental hypothesis
alignment with flexible matching for building confu-
sion networks: BBN system description for WMT09
system combination task. In Proc. WMT, pages 61?
65.
Antti-Veikko I. Rosti, Bing Zhang, Spyros Matsoukas,
and Richard Schwartz. 2010. BBN system descrip-
tion for WMT10 system combination task. In Proc.
WMT, pages 321?326.
Antti-Veikko I. Rosti, Evgeny Matusov, Jason Smith,
Necip Fazil Ayan, Jason Eisner, Damianos Karakos,
Sanjeev Khudanpur, Gregor Leusch, Zhifei Li, Spy-
ros Matsoukas, Hermann Ney, Richard Schwartz, Bing
Zhang, and Jing Zheng. 2011. Confusion network de-
coding for MT system combination. In Joseph Olive,
Caitlin Christianson, and John McCary, editors, Hand-
book of Natural Language Processing and Machine
Translation: DARPA Global Autonomous Language
Exploitation, pages 333?361. Springer.
Khe Chai Sim, William J. Byrne, Mark J.F. Gales,
Hichem Sahbi, and Phil C. Woodland. 2007. Con-
sensus network decoding for statistical machine trans-
lation system combination. In Proc. ICASSP.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciula, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proc. AMTA, pages 223?231.
Matthew Snover, Nitin Madnani, Bonnie Dorr, and
Richard Schwartz. 2009. Fluency, adequacy or
HTER? exploring different human judgments with a
tunable MT metric. In Proc. WMT, pages 259?268.
Stephan Vogel, Hermann Ney, and Christoph Tillman.
1996. HMM-based word alignment in statistical trans-
lation. In Proc. ICCL, pages 836?841.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377?403, Septem-
ber.
Daguang Xu, Yuan Cao, and Damianos Karakos. 2011.
Description of the JHU system combination scheme
for WMT 2011. In Proc. WMT, pages 171?176.
199
