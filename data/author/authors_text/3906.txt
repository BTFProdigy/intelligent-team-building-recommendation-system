Keyword-based Document Clustering 
 
Seung-Shik Kang 
School of Computer Science, Kookmin University & AITrc 
Chungnung-dong, Songbuk-gu, Seoul 136-702, Korea 
sskang@kookmin.ac.kr 
 
Abstract1  
Document clustering is an aggregation of 
related documents to a cluster based on the 
similarity evaluation task between documents and 
the representatives of clusters. Terms and their 
discriminating features of terms are the clue to 
the clustering and the discriminating features are 
based on the term and document frequencies. 
Feature selection method on the basis of 
frequency statistics has a limitation to the 
enhancement of the clustering algorithm because 
it does not consider the contents of the cluster 
objects. In this paper, we adopt a content-based 
analytic approach to refine the similarity 
computation and propose a keyword-based 
clustering algorithm. Experimental results show 
that content-based keyword weighting 
outperforms frequency-based weighting method. 
Keywords: Document Clustering, Weighting 
Scheme, Feature Selection 
 
1 Introduction 
Document clustering is an aggregation of 
documents by discriminating the relevant documents 
from the irrelevant documents. The relevance 
determination criteria of any two documents is a 
similarity measure and the representatives of the 
documents [1,2,3,4]. There are some similarity 
measures such as Dice coefficient, Jaccard?s 
coefficient, and cosine measure. These similarity 
measures require that the documents are represented 
in document vectors and the similarity of two 
documents is calculated from the operation of 
document vectors. 
In general, the representatives of a document or a 
cluster are document vectors that consist of <term, 
weight> pairs and the document similarities are 
determined by the terms and their weighting values 
that are extracted from the document [7,9]. In the 
previous studies on the document clustering, we 
focused on the clustering algorithm, but the document 
                                                          
This work was supported by the Korea Science and Engineering 
Foundation(KOSEF) through the Advanced Information 
Technology Research Center(AITrc). 
representation methodology was not the important 
issue. Document vectors are simply constructed from 
the term frequency (TF) and the inverted document 
frequency (IDF). This representation of term weighting 
method starts from the precondition that terms or 
keywords representing the document are calculated by 
TF-IDF. Term weighting method by TF-IDF is 
generally used to construct a document vector, but we 
cannot say that it is the best way of representing a 
document. So, we suppose that there is a limitation to 
improve the accuracy of the clustering system only by 
improving the clustering algorithm without changing 
the document/cluster representation method. 
Also, document clustering requires a large amount of 
memory spaces to keep the representatives of 
documents/clusters and the similarity measures [6, 8, 
10]. Given N documents to be clustered, N ? N 
similarity matrix is needed to store document similarity 
measures. Also, the recursive iteration of similarity 
calculation and reconstructing the representative of the 
clusters need a huge number of computations. 
In this paper, we propose a new clustering method 
that is based on the keyword weighting approach. The 
clustering algorithm starts from the seed documents 
and the cluster is expanded by the keyword relationship. 
The evolution of the cluster stops when no more 
documents are added to the cluster and irrelevant 
documents are removed from the cluster candidates. 
2 Keyword-based Weighting Scheme 
In general, the construction of a document vector 
depends on the term frequency and document 
frequency. If keywords are determined by frequency 
information of the document, we are apt to generate an 
error that nouns are often used regardless of substance 
of the document and the words of a high frequency are 
extracted. The clustering method, which is focused on 
similarity calculation considers the whole words except 
stopwords as the representative of the document, and 
constitutes a document vector that is calculated by the 
weight value from the term frequency and document 
frequency. 
It is common that terms and their weight values 
represent a document and <term, weight> pairs are the 
unique elements of the document vector. When we 
construct a document vector, term frequency and 
document frequency are the most important features to 
calculate the weight of a term. As for the terms and 
their weight values, the weight value of a term means 
a ranking score just as an importance factor to the 
document. So, the term weighting can be seen as an 
evaluation of the term as a keyword or a stopword to 
the document. The weighting function w(t) from a 
term to its weight is described in expression (1). 
 
w: term ? weight       (1) 
 
 w(t) = 0, if t is a stopword 
      1, if t is a keyword 
      a, otherwise  0 ? a ? 1 
 
For the weighting scheme of terms, there are two 
points of views as the representation of a document: 
 
(1) a discriminative value that distinguishes or 
characterizes the document from others; 
(2) an importance measure as a keyword or a 
stopword. 
 
Frequency-based term weighting (FBW) is a 
statistical measure of terms in an inter-document 
relationship. This weighting scheme is a very efficient 
method for distinguishing and characterizing a 
document from others, and it performs well for the 
applications of document classification or clustering 
in the information retrieval system. The only 
evaluation measure to characterize a document in 
frequency-based weighting scheme is a frequency 
statistics, but term frequencies are not the best 
measures to characterize the document by terms. 
Another weighting scheme is a keyword-based term 
weighting (KBW) method that is based on the 
keyword importance factors in a document. It is an 
analytic approach that analyses the contents of a 
document to get a keyword list from the document. 
The weight value of a word is calculated by the 
importance factors as a keyword in a document. The 
weight value of a word is a combination value of 
keyword-weighting factors and the terms are ordered 
by the keyword ranking score. The ranking scores in 
this weighting scheme are calculated from the analysis 
results of the document. Keyword-based term 
weighting will be a good solution to overcome the 
limitation of the frequency-based weighting scheme. 
Keywords in a text are the terms that represent a 
document and the candidate keywords are extracted 
from the analysis results of the document. Keyword 
ranking method depends on several factors of a term 
such as the type of a document, the location and the 
role of words in a sentence or a paragraph [5]. 
Thematic words of a document are representative 
terms for the document. Thematic words are extracted 
from a text by analysing the contents of the text, but 
keyword extraction depends on the type of text. 
Keywords are easily found in the title or an abstract in 
a research paper that consists of a title, abstract, body, 
experiment, and conclusion. Also, newspaper article 
contains a keyword in the title or the first part of the 
text. There are some clues of determining a keyword 
and we may classify them as word level, sentence level, 
paragraph level, and text level features. Word-level 
features are the type of part-of-speech and case-role 
information. The part-of-speech of Korean noun is 
divided into common noun, compound noun, proper 
noun, and numeral. 
Syntactic or sentence-level features are the type of a 
phrase or a clause, sentence location, and sentence type. 
From the rhetoric word in a sentence, the importance of 
the sentence is computed and the terms in a sentence 
are affected by the type of a sentence. Also, the 
weighting scheme of a term in the subjective clause is 
not the equal to the same term that appeared in an 
auxiliary clause or in a modifying clause. Basic term 
weight is assigned by the type of a term and 
recomputed by the features that it accompanies in the 
text. That is, the weight value of a term is also 
determined by the characteristics of word, sentence, 
phrase, and clause where the term is extracted. 
3 Keyword-based Document Clustering 
Keyword-based document clustering creates a 
cluster by the keywords of each document. Suppose 
that C is a set of clusters that is finally created by the 
clustering algorithm. If n is the number of clusters in C, 
then C is a set of clusters , , ? , C . 1C 2C n
 
C = { C , , ? , C } 1 2C n
 
Each cluster  is initialised by document d that is 
not assigned to the existing clusters, and d is a seed 
document of . When a new cluster is created, 
expansion and reduction steps are repeated until it 
reaches a stable state from the start state. In each 
evolution steps for cluster ,  is the j-th state of 
. 
iC
iC
iC
j
iC
iC
j
iC : the j-th state of a cluster C  i
 
The characteristic vector of a cluster is a set of 
<keyword, weight value> pairs that represents the 
cluster. If  is a keyword set of a document  
and 
i
 is a keyword set of cluster , then 
i
 
is the j-th state of cluster . Figure 1 shows a 
keyword-based clustering algorithm for the cluster . 
Given the keyword sets for each document, cluster  
is created by the self-expanding algorithm. 
DK D
j
CK
iC
iC
CK iC
iC
 
3.1 Cluster Initialisation 
The first step of the clustering algorithm is a creation 
and initialisation of a new cluster. A document  is 
selected that does not belong to any other cluster, and it 
is assigned to a new cluster  that is an initial state 
D
0
iC
of cluster .  iC
}
D
?=
C=
  all
s =
(if
for
+j
0
 
}{0 DCi =  
 
At this time, a document  that is the first document 
in the new cluster is called a seed document (or an 
initialisation document). The seed document is 
randomly selected among the documents that do not 
belong to the clusters  ~ . Keyword set 
 of a document D is a set of keywords k
D
1C
0
iC
1?iC
DK 1, k2,?, 
kn that are extracted from document . The initial 
state of keyword set  is initialised by . 
D
K DK
 
DC KK i =0  
 
DK  = { k | k is a keyword that is extracted from D } 
 
 
{0 DCi =
0
 
C KK i =
1
 
=iC  { Dx | document Dx, where   xDKk ?
0for }  that such 
iC
Kkk ??
1=j  
{do  
j
ixD
j
C CDKK xi ?  where,
jj+1
 
iiC  
begin for  x
j
iCD ?
j
 
      ),( Cx iKDsim
      )thresholds <
          C  }{11 x
j
i
j
i DC ?= ++
end  
1=j  
)()(} cumentisDeleteDowhile  
j
ii CC =  
 
Figure 1. Keyword-based clustering algorithm 
 
3.2 Expanding the Cluster 
In the initialisation step of the cluster, a new 
cluster , an initial state of cluster C , is 
established as the seed document, and the keyword set 
i
is initialised by the key word set of the seed 
document. In the expanding step of the cluster, the 
cluster is expanded by adding more related documents 
to the cluster, that include the keywords of the seed 
document as the related documents of the seed 
document. That is, adding the total documents that 
appear each keyword of 
i
(the keyword extracted 
from the seed document) to the clusterC that is the 
next state of cluster C  expands the cluster. 
iC i
0
CK
0
CK
1
i
i
}0
iC
j
C
j
iC
j
iC
1+i
|xD
K?
j
CK
xD?
0
C
 where,
j
i
j
i
C
j
i
 {1   ,i KkkC ?=  
 
The cluster expansion is performed by the iteration 
of keyword expansion and cluster expansion. More 
documents are added to a cluster by the similarity 
evaluation between the keyword set and the document. 
If a new document is added to a cluster, then the 
keywords in the added document are also added to the 
keyword set of the cluster. The first expansion is 
performed by the keyword set extracted from the seed 
document. The second expansion is performed by new 
keywords that are added to a cluster as a result of the 
first expansion. And the i-th expansion is performed by 
the (i-1)-th state of the keyword set.  
The number of iterations is decided through the 
experiment. When a cluster is expanded from  to 
, the keyword set 
i
is also expanded to a new 
keyword set 
i
 that appears in the total documents 
of the cluster . The keyword set 
i
 of  is a 
union of the total keyword sets of . 
0
iC
j
iC
1
iC K
1
CK1
iC K
j
iC
 
xD
i
C DK xi ?=   
 
The keyword set 
i
 of the cluster  is used to 
calculate the characteristic vector of each cluster. The 
characteristic vector is constituted the weight value 
calculated by term frequency (TF) and inverted 
document frequency (IDF) of the keywords and this is 
used to calculate the similarity measure between a 
document and the cluster. 
3.3 Cluster Reduction and Completion 
This step is to produce a complete cluster by 
removing the documents that are not related to the 
cluster. For the cluster C , documents of a low 
similarity to the cluster are removed, that are not 
related to a cluster C  through the similarity 
computation with the cluster . The result of cluster 
reduction is a filtering of documents that are not related 
to the cluster, and the cluster  is generated as a 
next step of the cluster C . Ultimately, the cluster 
 is completed that consists of the related documents 
after filtering the non-related documents. If a cluster 
 is completed, the next cluster C  is created 
through the same process. Clustering is terminated if 
all the documents are clustered or no more clusters are 
created. 
j
i
1+j
iC
iC
iC
  
Figure 2. Overall architecture of keyword-
based clustering 
4 Design and Implementation 
The structure of a keyword-based clustering system 
is shown in Figure 2. At first, keywords are extracted 
from each input document and the weight values of 
them are computed. Keywords and their scores are 
stored in an inverted-file structure. Inverted-file 
structure is a good for the expansion of the cluster and 
adding the documents that includes a keyword to the 
initial cluster. Figure 3 shows an example of the 
operation of the document clustering system: 
initialization, expansion, reduction, and completion of 
clusters. 
A new cluster is created and it includes a seed 
document D. An initial set of keywords for the initial 
state of a cluster is a keyword set KD of document D. 
 
KD = { T1,D, T2,D, ?, Ti,D ?, Tn,D } 
 
For the terms in KD, documents that contain the same 
term are added as a candidate document in the cluster. 
Let the candidate documents be D1a, D1b, ? , D2a, D2b, 
?, Dna, Dnb, ?. then Dxy, is a document that is 
expanded by term Tx. Keyword set of the cluster is 
reconstructed by new set of documents. 
In each step of the cluster expansion, the number of 
keywords that are used for the expansion, and the 
threshold of the weight value are decided through 
experiments considering the maximum number of 
document candidates in a cluster. Also, <keyword, 
weight> pairs as an intermediate representative of the 
cluster are much important factor of the cluster 
expansion. 
 
 aaa DnDD
TTT ,,2,1 ,, L
bbb DnDD
TTT ,,2,1 ,,, L
zzz DnDD
TTT ,,2,1 ,,, L
Input Document 
Keyword Extraction 
create inverted-file
create clusterCreate Inverted-File 
Init. Cluster Keyword set 
T1,D, 
T2,D, 
? 
Tn,D, 
 
D 
Create a Cluster 
Expand Cluster 
expand cluster 
Reduce/Complete Cluster 
D1a, D1b, ?
D2a, D2b, ?
, 
, 
Dna, Dnb, ?
,
Clusters 
complete cluster 
result 
D1A, D1B, ? 
D2A, D2B, ? 
, 
, 
DnA, DnB, ? 
 
Figure 3. Example of keyword-based 
clustering 
 
Now, a new keyword set that is limited to the cluster 
candidates is constructed to get cluster documents. 
Through the similarity calculation between the 
document and the candidate centroid of the cluster, 
relevant documents are selected to be a member of the 
cluster. Through the iterations on keyword selection 
and the reconstruction of the related documents, a new 
cluster is completed that reaches in a stable status with 
a strong relationship between keyword set and 
document set. 
5 The Experiments 
We implemented our clustering algorithm and 
applied it to the clustering of similar documents. The 
test documents for the experiment are collected from 
the three days of newspaper articles. The total number 
of articles is 383 and average 132 terms are extracted 
from the articles. We performed a document clustering 
by applying the difference criteria for term selection: 1) 
frequency-based term selection; 2) percentage-based 
keyword selection; and 3) keyword selection by 
absolute number of keywords. Figure 4 shows the 
result of similarity clustering by frequency-based term 
selection. In this experiment, three types of term 
selection are performed.  
 
- all terms are used to the clustering 
- terms with more than frequency 2 
- terms with more than frequency 3 
 
In each experiment, we varied the similarity decision 
ratio by the percentage of term matches. Figure 4 
shows that term selection by frequency 2 or 3 is not 
good for the representation of a document. 
 
0.6
0.7
0.8
0.9
40% 50% 60% 70% 80% 90%
te rm  m a tc h  ra t io
f
-
m
e
a
s
u
r
e F req. 2 Freq. 3
All terms
 
 
Figure 4. Frequency-based keyword selection 
 
 
0.6
0.7
0.8
0.9
40% 50% 60% 70% 80% 90%
term match ratio
f
-
mea
s
ure
10%
20%
30%
40%
50%
60%
70%
80%
90%
100%
 
Figure 5. Percentage-based keyword 
selection 
 
In the experiment of percentage-based keyword 
selection, terms of high weight values are selected for 
the similarity calculation of the document. All the 
curves in Figure 5 are a similar shape, except for 10 % 
selection. In case of 10% selection, we guess that less 
than 10% of keywords are not sufficient for the 
similarity decision and auxiliary keywords are also 
needed for the accuracy. Another point in this 
experiment is that 30%~60% keyword selection 
resulted better than the selection of all terms. 
We compared the F1-measure for the selection of 
maximum keywords. All the experiments in Figure 6 
resulted better than the experiment of using all the 
terms in the document. Also, 30~70 keywords with 
60%~70% match ratio resulted a good performance for 
the comparison of document similarity. 
 
0.6
0.7
0.8
0.9
40% 50% 60% 70% 80% 90%
term match ratio
f
-
m
e
a
su
re
20
30
40
50
60
70
80
90
100
Al terms
 
 
Figure 6. Keyword selection by maximum 
6 Conclusion 
It is common that clustering algorithm is based on 
the similarity computation by frequency-based 
statistics to aggregate the related documents. This 
metric is an important factor for term weighting. We 
proposed a term weighting method that is based on the 
keyword features and we tried to complement the 
drawback of frequency-based metric. Based on the 
keyword weighting scheme, documents of the same 
keywords are grouped into a cluster candidate and a 
new cluster is created by removing irrelevant 
documents. We performed an experiment for the 
clustering of similar documents and the results showed 
that keyword-based weighting scheme is better than the 
frequency-based method.  
Our keyword-based algorithm is using 30%~60% of 
terms for a clustering and the similarity matrix is not a 
necessity that it will be good for the clustering of a 
huge number of documents. We also expect that this 
algorithm will be good for the topic tracking of special 
events. In the experiment, we randomly selected a seed 
document and it is a bit sensitive for the seed document. 
So, our next research will be focused on minimizing 
the effect of the seed document by getting 
representative keywords before starting the clustering. 
References  
[1] Anderberg, M. R., ?Cluster Analysis for Applications?, 
New York: Academic, 1973. 
[2] Can, F., and E. A. Ozkarahan, ?Dynamic Cluster 
Maintenance?, Information Processing & Management, 
Vol. 25, pp.275-291, 1989. 
[3] Dubes, R., and A. K. Jain, ?Clustering Methodologies in 
Exploratory Data Analysis?, Advances in Computers, 
Vol. 19, pp.113-227, 1980. 
[4] Frakes, W. B. and R. Baeza-Yates, Information Retrieval, 
Prentice Hall, 1992. 
[5] Kang, S. S., H. G. Lee, S. H. Son, G. C. Hong, and B. J. 
Moon, ?Term Weighting Method by Postposition and 
Compound Noun Recognition?, Proceedings of 13th 
Conference on Korean Language Computing, pp.196-
198, 2001. 
[6] Murtagh, F., ?Complexities of Hierarchic Clustering 
Algorithms: State of the Art?, Computational Statistics 
Quarterly, Vol. 1, pp.101-113, 1984. 
[7] Perry, S. A., and P. Willett, ?A Review of the Use of 
Inverted Files for Best Match Searching in Information 
Retrieval Systems?, Journal of Information Science, Vol. 
6, pp.59-66, 1983. 
[8] Sibson, R. ?SLINK: an Optimally Efficient Algorithm 
for the Single-Link Cluster Method?, Computer Journal, 
Vol. 16, pp.328-342, 1973. 
[9] Willett, P., ?Document Clustering Using an Inverted File 
Approach?, Journal of Information Science, Vol. 2, 
pp.223-231, 1980. 
[10] Willett, P., ?Recent Trends in Hierarchic Document 
Clustering: A Critical Review?, Information Processing 
and Management, Vol. 24, No.5, pp.577- 597, 1988. 
 
Data-driven Language Independent Word Segmentation Using 
Character-Level Information 
Dong-Hee Lim, Seung-Shik Kang 
School of Computer Science, Kookmin Univerity 
861-1 Chongnung-dong, Songbuk-gu, Seoul 136-702, Korea 
{nlp, HTUsskang}@cs.kookmin.ac.krUTH
Abstract
This paper presents a data-driven language 
independent word segmentation system that 
has been trained for Chinese corpus at the 
second Chinese word segmentation bakeoff. 
The system consists of a base segmentation 
algorithm and the refining procedures for 
the undecided character sequences. It does 
not use any lexicon and the base 
segmentation is simply done by character 
bigram and HMM-model is applied for the 
remaining character sequences. As a final 
step, high-frequency character trigram 
modifies the error-prone parts of the text.T1T
1 Introduction
We participated in the closed track of the second 
Chinese word segmentation bakeoff for the 
training corpus of HK (City University of Hong 
Kong, PK (Beijing University), and MS 
(Microsoft Research). Our system is independent 
of the corpus or the language that we also 
registered for AS (Academia Sinica) track, but 
failed to generate a result because of the code set 
problem. AS uses two-byte space characters 
instead of a blank(0x20), and more 0x0A is used 
in AS that is regarded as EOF in Windows 
environment.  
The result of our system is not a top-level 
system when compared to other systems. 
However, our approach is quite acceptable 
because the data-driven methods can contribute 
to improving the accuracy of other word 
segmentation systems because we did not 
performed a tuning the system to fix the 
frequently repeating error patterns. 
                                                     
This work was supported by the Korea Science and 
Engineering Foundation(KOSEF) through Advaned 
Information Technology Research Center(AITrc). 
2 Bigram and trigram data 
We extracted a character bigram data from the 
training corpus. In the previous studies, 
Shim(1996) and Kang(2001) constructed space 
generation probability for each adjacent two 
characters XY. They are inside probability 
?X_Y?, left-side probability ?_XY?, and 
right-side probability ?XY_?.T 2 T That is, they 
ignored ?space information?. In our bigram data, 
inside and outside space information is extracted 
from the training corpus, together with the 
character pairs. We call it ?extended bigram data? 
and it has eight types of frequency data. For 
example, XY consists of the frequencies of 
?0X0Y0?, ?0X0Y1?, ?0X1Y0?, ?0X1Y1?, 
?1X0Y0?, ?1X0Y1?, ?1X1Y0?, and ?1X1Y1?.T3T
From the frequencies of the extended bigram data, 
we compute the space generation probability of 
Pt=000(CiCi+1) and left/right/inside probabilities 
are also computed from the extended bigram 
data.
Pt=000(CiCi+1) = Ft=000(CiCi+1) / ? 
 

111
000
1)Ft(CiCi
t
t
Extended bigram data is more sophisticated than 
the basic bigram data that the accuracy is better 
than that of the basic bigram data. 
3 Segmentation algorithm 
3.1 Base Algorithm 
The base segmentation algorithm is a HMM 
model together with the space-insertion 
probability. HMM model chooses the 
                                                     
T
2
T Lee(2002) used bigram and trigram data for HMM 
model which requires a more memory space. 
T
3
T ?0? is a non-space tag and ?1? is a space tag. 
158
segmentation with the higheset probability. 
Given a sentence of n characters, S = c1c2...cn, has 
a segmentation of m words, then segmentation 
probability is estimated as P(T,S) = P(t1,n, c1,n)
??   nii iiiii ttcctp0 121 ),|( . Our starting point 
of the word segmentation was the high precision 
ratio for the Korean language. We first tried to 
simply applying the extended bigram data with 
an appropriate threshold. However, it is supposed 
that there is a limitation of this approach because 
of the low recall ratio. It caused an adoption of 
HMM model together with the extended bigram 
data. Table 1 shows the results of HMM with 
extended bigram data. 
Table 1. Results of HMM with extended bigram 
Testing
data Recall Precision F-measure
HK 0.924 0.921 0.923 
PK 0.902 0.919 0.910 
MS 0.942 0.939 0.940 
3.2 Postprocessing by trigram data 
Extended bigram data in Section 2 consists of 2 
adjacent characters and 3 space information 
(2C3S). In contrast, we may extract trigram data 
that is constructed by 3 characters and 2 space 
information(3C2S). This 3C2S trigram data has a 
form of ?X0Y0Z?, ?X0Y1Z?, ?X1Y0Z?, and 
?X1Y1Z?. That is 3 character sequence XYZ has 
4 frequency data. We supposed that ?there are 
frequent 3-character sequences that are biased to 
one of the spacing pattern?.  
We verified this supposition by improving the 
accuracy of the word segmentation result. Table 
2 shows the final result of the postprocessing. 
Postprocessing by trigram data got an increase of 
both recall and precision. When compared to the 
base segmentation results of Table 1, F-measures 
are increased by 0.3%, 0.4%, and 0.8%, 
respectively. 
As an improvement of the system performance, 
character trigram data has been extracted from 
the training corpus. 
Table 2. Final segmentation resultsT4T
Testing
data Recall Precision F-measure
HK 0.926 0.925 0.926 
PK 0.904 0.925 0.914 
MS 0.947 0.949 0.948 
4 Pure data-driven method without 
using HMM 
Only after submitting the results for bakeoff 2005, 
we noticed that the accuracy of HMM model is 
low. It is not clear what the problem is and there 
is a possibility of the implementation error. So, 
we looked for a pure data-driven method without 
using HMM model. The first step in the base 
segmentation is to apply extended bigram with 
no space information. In this step, only the spaces 
with high confidence are fixed and others are 
marked as ?undecided?.T 5 T  In the second step, 
extended bigram with space information is 
applied. Two more postprocessing modules are 
added for refinements. One of them is to adopt 
the word-length feature by using the fact that 
average length of Chinese word is 1.6 characters. 
The other is to construct ?error dictionary? for the 
training data. Error dictionary is constructed by 
running training data and comparing the 
differences. The context information of error 
dictionary is four characters (left two and right 
two characters). The new approach got a better 
result than that of the final result of bakeoff 2005 
as shown in Table 3. 
Table 3. Pure data-driven method without HMM 
Testng
data Recall Precision F-measure
HK 0.933 0.921 0.927 
PK 0.912 0.929 0.920 
MS 0.952 0.953 0.952 
                                                     
T
4
T The final results in Table 2 are a bit higher than the 
bakeoff 2005 results. F-measures of bakeoff 2005 
results are 0.921, 0.912, and 0.947, respectively. The 
reason was not identified. Table 1 and Table 2 are 
computed by the evaluation program ?score.txt? in the 
website of SIGHAN bakeoff 2005.
T
5
T If space generation probability is higher than 0.7, 
space is inserted. With less than 0.3, space is not 
inserted, and ?undecided? mark for the range 
0.3~0.7,  
159
5 Conclusion
We presented our word segmentation method for 
the closed track of bakeoff 2005. Our approach is 
data-driven and language independent. That is, 
our method is purely statistical method that no 
language dependent features are applied for 
tuning or improving the accuracy. Word 
segmentation system for bakeoff 2005 applied 
HMM model together with extended bigram and 
trigram data. The results show that word 
segmentation problem can be solved with no 
lexicons or language-dependent resources.
One of the good point of our approach is that 
data-driven language independent approach is 
quite acceptable for the word segmentation 
problem. We also expect that our data-driven 
method would be a good solution for the 
enhancement of word segmentation systems as a 
postprocessing module.  
References 
Chen, A., Chinese Word Segmentation Using 
Minimal Linguistic Knowledge, SIGHAN 
2003, pp.148-151, 2003. 
Gao, J., M. Li, and C.N. Huang, Improved 
Source-Channel Models for Chinese Word 
Segmentation, ACL 2003. 
Kang, S. S. and C. W. Woo, Automatic 
Segmentation of Words using Syllable Bigram 
Statistics, Proceedings of NLPRS'2001, 
pp.729-732, 2001. 
Lee D. G, S. Z. Lee, ???GH. C. Rim, H. S. Lim, 
Automatic Word Spacing Using Hidden 
Markov Model for Refining Korean Text 
Corpora, Proc. of the 3rd Workshop on Asian 
Language Resources and International 
Standardization, pp.51-57, 2002. 
Maosong, S., S. Dayang, and B. K. Tsou, 
Chinese Word Segmentation without Using 
Lexicon and Hand-crafted Training Data, 
Proceedings of the 17PthP International 
Conference on Computational Linguistics 
(Coling?98), pp.1265-1271, 1998. 
Asahara, M., C. L. Go, X. Wang, and Y. 
Matsumoto, Combining Segmenter and 
Chunker for Chinese Word Segmentation, 
Proceedings of the 2PndP SIGHAN Workshop on 
Chinese Language Processing, pp144-147, 
2003. 
Nakagawa, T., Chinese and Japanese Word 
Segmentation Using Word-Level and 
Character-Level Information, COLING?04., 
pp.466-472, 2004. 
Ng, H.T. and J.K. Low, Chinese Part-of-Speech 
Tagging: One-at-a-Time or All-at-Once? 
Word-Based or Character-Based, EMNLP?04. 
Shim, K. S., Automated Word-Segmentation for 
Korean using Mutual Information of Syllables, 
Journal of KISS: Software and Applications, 
pp.991-1000, 1996. 
Sproat, R. and T. Emerson, The First 
International Chinese Word Segmentation 
Bakeoff, SIGHAN 2003. 
160
Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing, pages 197?200,
Sydney, July 2006. c?2006 Association for Computational Linguistics
N-gram Based Two-Step Algorithm for Word Segmentation 
 
Dong-Hee Lim 
Dept. of Computer Science 
Kookmin University 
Seoul 136-702, Korea 
nlp@cs.kookmin.ac.kr
 
 
Kyu-Baek, Hwang 
School of Computing 
Soongsil University 
Seoul 156-743, Korea 
kbhwang@ssu.ac.kr 
 
 
Seung-Shik Kang 
Dept. of Computer Science 
Kookmin University 
Seoul 136-702, Korea 
sskang@kookmin.ac.kr
 
Abstract 
This paper describes an n-gram based 
reinforcement approach to the closed 
track of word segmentation in the third 
Chinese word segmentation bakeoff. 
Character n-gram features of unigram, 
bigram, and trigram are extracted from 
the training corpus and its frequencies are 
counted. We investigated a step-by-step 
methodology by using the n-gram statis-
tics. In the first step, relatively definite 
segmentations are fixed by the tight 
threshold value. The remaining tags are 
decided by considering the left or right 
space tags that are already fixed in the 
first step. Definite and loose segmenta-
tion are performed simply based on the 
bigram and trigram statistics. In order to 
overcome the data sparseness problem of 
bigram data, unigram is used for the 
smoothing. 
1 Introduction 
Word segmentation has been one of the very 
important problems in the Chinese language 
processing. It is a necessary in the information 
retrieval system for the Korean language (Kang 
and Woo, 2001; Lee et al 2002). Though Korean 
words are separated by white spaces, many web 
users often do not set a space in a sentence when 
they write a query at the search engine. Another 
necessity of automatic word segmentation is the 
index term extraction from a sentence that in-
cludes word spacing errors. 
The motivation of this research is to investi-
gate a practical word segmentation system for the 
Korean language. While we develop the system, 
we found that ngram-based algorithm was ex-
actly applicable to the Chinese word segmenta-
tion and we have participated the bakeoff (Kang 
and Lim, 2005). The bakeoff result is not satis-
fiable, but it is acceptable because our method is 
language independent that does not consider the 
characteristics of the Chinese language. We do 
not use any language dependent features except 
the average length of Chinese words.  
Another advantage of our approach is that it 
can express the ambiguous word boundaries that 
are error-prone. So, there are a good possibility 
of improving the performance if language de-
pendent functionalities are added such as proper 
name, numeric expression recognizer, and the 
postprocessing of single character words.1
2 N-gram Features 
The n-gram features in this work are similar to 
the previous one in the second bakeoff. The basic 
segmentation in (Kang and Lim, 2005) has per-
formed by bigram features together with space 
tags, and the trigram features has been used as a 
postprocessing of correcting the segmentation 
errors. Trigrams for postprocessing are the ones 
that are highly biased to one type of the four tag 
features of ?AiBjC?.2 In addition, unigram fea-
tures are used for smoothing the bigram, where 
bigram is not found in the training corpora. In 
this current work, we extended the n-gram fea-
tures to a trigram.  
 
(a) trigram: AiBjC 
(b) bigram: iAjBk
(c) unigram: iAj
 
In the above features, AB and ABC are a 
Chinese character sequence of bigram and tri-
gram, respectively. The subscripts i, j, and k 
                                                          
1 Single character words in Korean are not so common, 
compared to the Chinese language. We can control the 
occurrence of them through an additional processing. 
2 We applied the trigrams for error correction in which one 
of the trigram feature occupies 95% or more. 
197
denote word space tags, where the tags are 
marked as 1(space tag) and 0(non-space tag). For 
the unigram iAj, four types of tag features are 
calculated in the training corpora and their fre-
quencies are stored. In the same way, eight types 
of bigram features and four types of trigram 
features are constructed. If we take all the inside 
and outside space-tags of ABC, there are sixteen 
types of trigram features hAiBjCk for h,i,j,k = 0 or 
1. It will cause a data sparseness problem, espe-
cially for small-sized training corpora. In order to 
avoid the data sparseness problem, we ignored 
the outside-space tags h and k and constructed 
four types of trigram features of AiBjC.  
Table 1 shows the number of n-gram features 
for each corpora. The total number of unique 
trigrams for CITYU corpus is 1,341,612 in which 
104,852 trigrams occurred more than three times. 
It is less than one tenth of the total number of 
trigrams. N-gram feature is a compound feature 
of <character, space-tag> combination. Trigram 
classes are distinguished by the space-tag context, 
trigram class hAiBjCk  is named as t4-trigram or 
C3T4.3 It is simplified into four classes of C3T2 
trigrams of AiBjC, in consideration of the mem-
ory space savings and the data sparseness prob-
lem. 
 
Table 1. The number of n-gram features 
Trigram Bigram Unigram
  
freq?1 freq?2 freq?3 freq?4 freq?1 freq?1
cityu 1341612 329764 165360 104852 404411 5112
ckip 2951274 832836 444012 296372 717432 6121
msra 986338 252656 132456 86391 303443 4767
upuc 463253 96860 45775 28210 177140 4293
3 Word Segmentation Algorithm 
Word segmentation is defined as to choose the 
best tag-sequence for a sentence. 
 
 
 
where 
 
 
                                                          
                                                          
3 ?Cn? refers to the number of characters and ?Tn? refers to 
the number of spae-tag. According to this notation, iAjBk 
and iAj are expressed as C2T3 and C1T2, respectively. 
More specifically at each character position, the 
algorithm determines a space-tag ?0? or ?1? by 
using the word spacing features. 
3.1 The Features 
We investigated a two step algorithm of de-
termining space tags in each character position of 
a sentence using by context dependent n-gram 
features. It is based on the assumption that space 
tags depend on the left and right context of 
characters together with the space tags that it 
accompanies. Let tici be a current <space tag, 
character> pair in a sentence.4  
 
? ti-2ci-2 ti-1ci-1 tici ti+1ci+1 ti+2ci+2 ? 
 
In our previous work of (Lim and Kang, 2005), 
n-gram features (a) and (b) are used. These fea-
tures are used to determine the space tag ti. In this 
work, core n-gram feature is a C3T2 classes of 
trigram features ci-2ti-1ci-1tici, ci-1ticiti+1ci+1. In 
addition, a simple character trigram with no 
space tag ?ticici+1ci+2? is added. 
 
(a) unigram:  
ti-1ci-1ti, ticiti+1
(b) bigram:  
ti-2ci-2ti-1ci-1ti, ti-1ci-1ticiti+1, ticiti+1ci+1ti+2
(c) trigram:  
ci-2ti-1ci-1tici, ci-1ticiti+1ci+1, ticici+1ci+2
 
Extended n-gram features with space tags are 
effective when left or right tags are fixed. Sup-
pose that ti-1 and ti+1 are definitely set to 0 in a 
bigram context ?ti-1ci-1ticiti+1?, then a feature 
?0ci-1tici0?(ti = 0 or 1) is applied, instead of a 
simple feature ?ci-1tici?. However, none of the 
space tags are fixed in the beginning that simple 
character n-gram features with no space tag are 
used.5
3.2 Two-step Algorithm 
The basic idea of our method is a cross checking 
the n-gram features in the space position by using 
three trigram features. For a character sequence 
?ci-2ci-1ticici+1ci+2?, we can set a space mark ?1? to 
ti, if P(ti=1) is greater than P(ti=0) in all the three 
trigram features ci-2ci-1tici, ci-1ticici+1, and tici-
ci+1ci+2. Because no space tags are determined in 
)|(maxarg? STPT
T ??
=
nn ,c,,ccStttT KK 2121  and ,,, ==
4 Tag ti is located before the character, not after the character 
that is common in other tagging problem like POS-tagging. 
5 Simple n-grams with no space tags are calculated from the 
extended n-grams. 
198
the beginning, word segmentation is performed 
in two steps. In the first step, simple n-gram 
features are applied with strong threshold values 
(tlow1 and thigh1 in Table 2). The space tags with 
high confidence are determined and the remain-
ing space tags will be set in the next step. 
 
Table 2. Strong and weak threshold values6
  tlow1 thigh1 tlow2 thigh2 tfinal
cityu 0.36 0.69 0.46 0.51 0.48
ckip 0.37 0.69 0.49 0.51 0.49
msra 0.33 0.68 0.46 0.47 0.46
upuc 0.38 0.69 0.45 0.47 0.47
 
In the second step, extended bigram features 
are applied if any one of the left or right space 
tags is fixed in the first step. Otherwise, simple 
bigram probability will be applied, too. In this 
step, extended bigram features are applied with 
weak threshold values tlow2 and thigh2. The space 
tags are determined by the final threshold tfinal, if 
it was not determined by weak threshold values. 
Considering the fact that average length of Chi-
nese words is about 1.6, the threshold values are 
lowered or highered.7
In the final step, error correction is performed 
by 4-gram error correction dictionary. It is con-
structed by running the training corpus and 
comparing the result to the answer. Error correc-
tion data format is 4-gram. If a 4-gram ci-2ci-1cici+1 
is found in a sentence, then tag ti is modified 
unconditionally as is specified in the 4-gram 
dictionary. 
4 Experimental Results 
We evaluated our system in the closed task on all 
four corpora. Table 3 shows the final results in 
bakeoff 2006. We expect that Roov will be im-
proved if any unknown word processing is per-
formed. Riv can also be improved if lexicon is 
applied to correct the segmentation errors. 
 
Table 3. Final results in bakeoff 2006 
  R P F Roov Riv
cityu 0.950  0.949 0.949  0.638 0.963
ckip 0.937  0.933 0.935  0.547 0.954
msra 0.933  0.939 0.936  0.526 0.948
upuc 0.915  0.896 0.905  0.565 0.949
                                                          
6 Threshold values are optimized for each training corpus. 
7 The average length of Korean words is 3.2 characters. 
4.1 Step-by-step Analysis 
In order to analyze the effectiveness of each step, 
we counted the number of space positions for 
sentence by sentence. If the number of characters 
in a sentence is n, then the number of words 
positions is (n-1) because we ignored the first tag 
t0 for c0. Table 4 shows the number of space 
positions in four test corpora. 
 
Table 4. The number of space positions 
  # of space positions # of spaces 
# of non- 
spaces 
cityu 356,791 212,662 144,129 
ckip 135,123   80,387  54,736 
msra 168,236   95,995   72,241 
upuc 251,418 149,747 101,671 
 
As we expressed in section 3, we assumed that 
trigram with space tag information will deter-
mine most of the space tags. Table 5 shows the 
application rate with strong threshold values. As 
we expected, around 93.8%~95.9% of total space 
tags are set in step-1 with the error rate 
1.5%~2.8%. 
 
Table 5. N-gram results with strong threshold 
 # of applied (%) # of errors (%)
cityu 342,035 (95.9%) 5,024 (1.5%) 
ckip 128,081 (94.8%) 2,818 (2.2%) 
msra 160,437 (95.4%) 3,155 (2.0%) 
upuc 235,710 (93.8%) 6,601 (2.8%) 
 
Table 6 shows the application rate of n-gram 
with weak threshold values in step-2. The space 
tags that are not determined in step-1 are set in 
the second step. The error rate in step-2 is 
24.3%~30.1%. 
 
Table 6. N-gram results with weak threshold 
 # of applied (%) # of errors (%) 
cityu 14,756 (4.1%) 3,672 (24.9%) 
ckip   7,042 (5.2%) 1,710 (24.3%) 
msra   7,799 (4.6%) 2,349 (30.1%) 
upuc 15,708 (6.3%) 4,565 (29.1%) 
 
199
4.2 4-gram Error Correction 
We examined the effectiveness of 4-gram error 
correction. The number of 4-grams that is ex-
tracted from training corpora is about 10,000 to 
15,000. We counted the number of space tags 
that are modified by 4-gram error correction 
dictionary. Table 7 shows the number of modi-
fied space tags and the negative effects of 4-gram 
error correction. Table 8 shows the results before 
error correction. When compared with the final 
results in Table 3, F-measure is slightly lower 
than the final results. 
 
Table 7. Modified space tags by error correction 
 # of modified space tags (%) 
Modification 
errors (%) 
cityu 418 (0.1%)   47 (11.2%) 
ckip 320 (0.2%)   94 (29.4%) 
msra 778 (0.5%) 153 (19.7%) 
upuc 178 (0.1%)   61 (34.3%) 
 
Table 8. Results before error correction 
  R P F 
cityu 0.948  0.947  0.948  
ckip 0.935  0.931  0.933  
msra 0.930  0.930  0.930  
upuc 0.915  0.895  0.905  
 
5 Conclusion 
We described a two-step word segmentation 
algorithm as a result of the closed track in bake-
off 2006. The algorithm is based on the cross 
validation of the word spacing probability by 
using n-gram features of <character, space-tag>. 
One of the advantages of our system is that it can 
show the self-confidence score for ambiguous or 
feature-conflict cases. We have not applied any 
language dependent resources or functionalities 
such as lexicons, numeric expressions, and 
proper name recognition. We expect that our 
approach will be helpful for the detection of 
error-prone tags and the construction of error 
correction dictionaries when we develop a prac-
tical system. Furthermore, the proposed algo-
rithm has been applied to the Korean language 
and we achieved a good improvement on proper 
names, though overall performance is similar to 
the previous method.  
Acknowledgements 
This work was supported by the Korea Science 
and Engineering Foundation(KOSEF) through 
Advaned Information Technology Research 
Center(AITrc). 
References 
Asahara, M., C. L. Go, X. Wang, and Y. Matsumoto, 
Combining Segmenter and Chunker for Chinese 
Word Segmentation, Proceedings of the 2nd 
SIGHAN Workshop on Chinese Language Proc-
essing, pp.144-147, 2003. 
Chen, A., Chinese Word Segmentation Using Mini-
mal Linguistic Knowledge, SIGHAN 2003, 
pp.148-151, 2003. 
Gao, J., M. Li, and C.N. Huang, Improved 
Source-Channel Models for Chinese Word Seg-
mentation, ACL 2003, pp.272-279, 2003. 
Kang, S. S. and C. W. Woo, Automatic Segmentation 
of Words using Syllable Bigram Statistics, Pro-
ceedings of NLPRS'2001, pp.729-732, 2001. 
Kang, S. S. and D. H. Lim, Data-driven Language 
Independent Word Segmentation Using Charac-
ter-Level Information, Proceedings of the 4th 
SIGHAN Workshop on Chinese Language Proc-
essing, pp.158-160, 2005. 
Lee D. G, S. Z. Lee, and H. C. Rim, H. S. Lim, 
Automatic Word Spacing Using Hidden Markov 
Model for Refining Korean Text Corpora, Proc. of 
the 3rd Workshop on Asian Language Resources 
and International Standardization, pp.51-57, 2002. 
Maosong, S., S. Dayang, and B. K. Tsou, Chinese 
Word Segmentation without Using Lexicon and 
Hand-crafted Training Data, Proceedings of the 
17th International Conference on Computational 
Linguistics (Coling?98), pp.1265-1271, 1998. 
Nakagawa, T., Chinese and Japanese Word Segmen-
tation Using Word-Level and Character-Level In-
formation, COLING?04., pp.466-472, 2004. 
Ng, H.T. and J.K. Low, Chinese Part-of-Speech Tag-
ging: One-at-a-Time or All-at-Once? Word-Based 
or Character-Based, EMNLP?04, pp.277-284, 
2004. 
Shim, K. S., Automated Word-Segmentation for 
Korean using Mutual Information of Syllables, 
Journal of KISS: Software and Applications, 
pp.991-1000, 1996. 
Sproat, R. and T. Emerson, The First International 
Chinese Word Segmentation Bakeoff, SIGHAN 
2003. 
200
