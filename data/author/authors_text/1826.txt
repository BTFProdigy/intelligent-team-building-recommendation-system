BLEU in characters:
towards automatic MT evaluation in languages
without word delimiters
Etienne Denoual
etienne.denoual@atr.jp
ATR ? Spoken language communication research labs
Keihanna gakken tosi, 619-0288 Kyoto, Japan
Yves Lepage
yves.lepage@atr.jp
Abstract
Automatic evaluation metrics for Ma-
chine Translation (MT) systems, such
as BLEU or NIST, are now well estab-
lished. Yet, they are scarcely used for
the assessment of language pairs like
English-Chinese or English-Japanese,
because of the word segmentation prob-
lem. This study establishes the equiv-
alence between the standard use of
BLEU in word n-grams and its appli-
cation at the character level. The use
of BLEU at the character level elimi-
nates the word segmentation problem:
it makes it possible to directly compare
commercial systems outputting unseg-
mented texts with, for instance, statisti-
cal MT systems which usually segment
their outputs.
1 Introduction
Automatic evaluation metrics for Machine Trans-
lation (MT) systems, such as BLEU (PAPINENI
et al, 2001) or NIST (DODDINGTON, 2002), are
now well established. They serve as quality as-
sessment methods or comparison tools and are a
fast way of measuring improvement. Although
it is claimed that such objective MT evaluation
methods are language-independent, they are usu-
ally only applied to English, as they basically rely
on word counts. In fact, the organisers of cam-
paigns like NIST (PRZYBOCKI, 2004)1, TIDES2
or IWSLT (AKIBA et al, 2004)3, prefer to evalu-
ate outputs of machine translation systems which
are already segmented into words before apply-
ing such objective evaluation methods. The con-
sequence of this state of affairs is that evaluation
campaigns of English to Japanese or English to
Chinese machine translation systems for instance,
are not, to our knowledge, widely seen or re-
ported.
2 Overview
2.1 The word segmentation problem
As statistical machine translation systems basi-
cally rely on the notion of words through their
lexicon models (BROWN et al, 1993), they are
usually capable of outputting sentences already
segmented into words when they translate into
languages like Chinese or Japanese. But this is
not necessarily the case with commercial systems.
For instance, Systran4 does not output segmented
texts when it translates into Chinese or Japanese.
As such, comparing systems that translate into
languages where words are not an immediate
given in unprocessed texts, is still hindered by
the human evaluation bottleneck. To compare the
performance of different systems, segmentation
has to be performed beforehand.
1http://www.nist.gov/speech/tests/mt-
/doc/mt04 evalplan.v2.1.pdf
2http://www.nist.gov/speech/tests/mt-
/mt tides01 knight.pdf
3http://www.slt.atr.jp/IWSLT2004-
/archives/000619.html
4http://www.systranbox.com/systran-
/box.
79
One can always apply standard word segmen-
tation tools (for instance, The Peking Univer-
sity Segmenter for Chinese (DUAN et al, 2003)
or ChaSen for Japanese (MATSUMOTO et al,
1999)), and then apply objective MT evaluation
methods. However, the scores obtained would be
biased by the error rates of the segmentation tools
on MT outputs5. Indeed, MT outputs still differ
from standard texts, and their segmentation may
lead to a different performance. Consequently, it
is difficult to directly and fairly compare scores
obtained for a system outputting non-segmented
sentences with scores obtained for a system de-
livering sentences already segmented into words.
2.2 BLEU in characters
Notwithstanding the previous issue, it is unde-
niable that methods like BLEU or NIST have
been adopted by the MT community as they
measure complementary characteristics of trans-
lations: namely fluency and adequacy (AKIBA et
al., 2004, p. 7). Although far from being per-
fect, they definitely are automatic, fast, and cheap.
For all these reasons, one cannot easily ask the
MT community to give up their practical know-
how related to such measures. It is preferable to
state an equivalence with well established mea-
sures than to merely look for some correlation
with human scores, which would indeed amount
to propose yet another new evaluation method.
Characters are always an immediate given in
any electronic text of any language, which is not
necessarily the case for words. Based on this ob-
servation, this study shows the effect of shifting
from the level of words to the level of charac-
ters, i.e., of performing all computations in char-
acters instead of words. According to what was
said above, the purpose is not to look for any
correlation with human scores, but to establish
an equivalence between BLEU scores obtained in
two ways: on characters and on words.
Intuitively a high correlation should exist. The
contrary would be surprising. However, the
equivalence has yet to be determined, along with
the corresponding numbers of characters and
words for which the best correlation is obtained.
5Such error rates are around 5% to 10% for standard
texts. An evaluation of the segmentation tool is in fact re-
quired. on MT outputs alone.
3 Experimental setup
The most popular off-the-shelf objective methods
currently seem to be BLEU and NIST. As NIST
was a modification of the original definition of
BLEU, the work reported here concentrates on
BLEU. Also, according to (BRILL and SORICUT,
2004), BLEU is a good representative of a class
of automatic evaluation methods with the focus
on precision6.
3.1 Computation of a BLEU score
For a given maximal order N , a baseline
BLEUwN score is the product of two factors:
a brevity penalty and the geometric average of
modified n-gram precisions computed for all n-
grams up to N .
BLEUwN score = BP ? N
????
N?
n=1
pn
The brevity penalty is the exponential of the
relative variation in length against the closest ref-
erence:
BP =
{
1 if |C| > |Rclosest|
e1?r/c if |C| ? |Rclosest|
where C is the candidate and Rclosest is the closest
reference to the candidate according to its length.
|S| is the length of a sentence S in words. Using a
consistent notation, we note as |S|W the number
of occurrences of the (sub)string W in the sen-
tence S, so that |S|w1...wn is the number of occur-
rences of the word n-gram w1 . . . wn in the sen-
tence S.
With the previous notations, a modified n-gram
precision for the order n is the ratio of two sums7:
pn =
?
w1...wn?C
min
(
|C|w1...wn , maxR
(
|R|w1...wn
))
?
w1...wn?C
|C|w1...wn
? the numerator gives the number of n-grams
of the candidate appearing in the references,
6ROUGE (LIN and HOVY, 2003) would be a representa-
tive of measures with the focus on recall.
7We limit ourselves to the cases where one candidate or
one reference is one sentence.
80
limited to the maximal number of occur-
rences of the n-gram considered in a single
reference8;
? the denominator gives the total number of n-
grams in the candidate.
We leave the basic definition of BLEU un-
touched. The previous formulae can be applied
to character n-grams instead of word n-grams.
In the sequel of this paper, for a given order N ,
the measure obtained using words will be called
BLEUwN , whereas the measure in characters for
a given order M will be noted BLEUcM .
3.2 The test data
We perform our study on English because a lan-
guage for which the segmentation is obvious and
undisputable is required. On Japanese or Chinese,
this would not be the case, as different segmenters
differ in their results on the same texts9.
The experiments presented in this paper rely on
a data set consisting of 510 Japanese sentences
translated into English by 4 different machine
translation systems, adding up to 2, 040 candidate
translations. For each sentence, a set of 13 refer-
ences had been produced by hand in advance.
Different BLEU scores in words and characters
were computed for each of the 2, 040 English can-
didate sentences, with their corresponding 13 ref-
erence sentences.
4 Results: equivalence BLEUwN /
BLEUcM
To investigate the equivalence of BLEUwN and
BLEUcM , we use three methods: we look for
the best correlation, the best agreement in judge-
ments between the two measures, and the best
behaviour, according to an intrinsic property of
BLEU.
4.1 Best correlation
For some given order N , our goal is to determine
the value of M for which the BLEUcM scores (in
8This operation is referred to as clipping in the original
paper (PAPINENI et al, 2001).
9Although we already applied the method in characters
on unsegmented Japanese or Chinese MT outputs, this is not
the object of the present study, which, again, is to show the
equivalence between BLEU in words and characters.
characters) are best correlated with the scores ob-
tained with BLEUwN . To this end, we compute
for all possible Ns and Ms all Pearson?s correla-
tions between scores obtained with BLEUwN and
BLEUcM . We then select for each N , that M
which gives a maximum in correlation. The re-
sults10 are shown in Table 1. For N = 4 words,
the best M is 17 characters.
4.2 Best agreement in judgement
Similar to the previous method, we compute for
all possible Ms and Ns all Kappa coefficients be-
tween BLEUwN and BLEUcM and then select,
for each given N , that M which gives a maxi-
mum. The justification for such a procedure is as
follows.
All BLEU scores fall between 0 and 1, there-
fore it is always possible to recast them on a scale
of grades. We arbitrarily chose 10 grades, rang-
ing from 0 to 9, to cover the interval [ 0 , 1 ] with
ten smaller intervals of equal size. A grade of 0
corresponds to the interval [ 0 , 0.1 [, and so on,
up to grade 9 which corresponds to [ 0.9 , 1 ]. A
sentence with a BLEU score of, say 0.435, will be
assigned a grade of 4.
By recasting BLEU scores as described above,
they become judgements into discrete grades, so
that computing two different BLEU scores first
in words and then in characters for the same
sentence, is tantamount to asking two different
judges to judge the same sentence. A well-
established technique to assess the agreement be-
tween two judges being the computation of the
Kappa coefficient, we use this technique to mea-
sure the agreement between any BLEUwN and
any BLEUcM .
The maximum in the Kappa coefficients is
reached for the values11 given in Table 1. For
N = 4 words, the best M is 18 characters.
10The average ratio M/N obtained is 4.14, which is not
that distant from the average word length in our data set:
3.84 for the candidate sentences.
Also, for N = 4, we computed all values of Ms for each
sentence length. See Table 2.
11Except for N = 3, where the value obtained (14) is
quite different from that obtained with Pearson?s correlation
(10), the values obtained with Kappa coefficients atmost dif-
fer by 1.
81
4.3 Best analogical behaviour
BLEU depends heavily on the geometric average
of modified n-gram precision scores. Therefore,
because one cannot hope to find a given n-gram in
a sentence if neither of the two included (n? 1)-
grams is found in the same sentence, the follow-
ing property holds for BLEU:
For any given N , for any given candi-
date, for any given set of references,
BLEUwN ? BLEUw(N?1)
The left graph of Figure 2 shows the correspon-
dence of BLEUw4 and BLEUw3 scores for the
data set. Indeed all points are found on the di-
agonal or below.
Using the property above, we are interested
in finding experimentally the value M such that
BLEUcM ? BLEUw(N?1) is true for almost all
values. Such a value M can then be considered to
be the equivalent in characters for the value N in
words.
Here we look incrementally for the M allowing
BLEUcM to best mimic BLEUwN , that is leaving
at least 90% of the points on or under the diag-
onal. For N = 4, as the graph in the middle of
Figure 2 illustrates, such a situation is first en-
countered for M = 18. The graph on the right
side shows the corresponding layout of the scores
for the data set. This indeed tends to confirm that
the M for which BLEUcM displays a similar be-
haviour to BLEUw4 is around 18.
5 The standard case of system
evaluation
5.1 BLEUw4 ' BLEUc18
According to the previous results, it is possible to
find some M for some given N for which there
is a high correlation, a good agreement in judge-
ment and an analogy of behaviour between mea-
sures in characters and in words. For the most
widely used value of N , 4, the corresponding val-
ues in characters were 17 according to correlation,
18 according to agreement in judgement, and 18
according to analogical behaviour. We thus de-
cide to take 18 as the number of characters cor-
responding to 4 words (see Figure 1 for plots of
scores in words against scores in characters).
5.2 Ranking systems
We recomputed the overall BLEU scores of the
four MT systems whose data we used, with the
usual BLEUw4 and its corresponding method in
characters, BLEUc18. Table 3 shows the average
values obtained on the four systems.
When going from words to characters, the val-
ues decrease by an average of 0.047. This is
explained as follows: a sentence of less than N
units, has necessarily a BLEU score of 0 for N -
grams in this unit. Table 4 shows that, in our data,
there are more sentences of less than 18 characters
(350) than sentences of less than 4 words (302).
Thus, there are more 0 scores with characters, and
this explains the decrease in system scores when
going from words to characters.
On the whole, Table 3 shows that happily
enough, shifting from words to characters in the
application of the standard BLEU measure leaves
the ranking unchanged12.
6 Conclusion
We studied the equivalence of applying the BLEU
formula in character M -grams instead of word
N -grams. Our study showed a high correlation,
a good agreement in judgement, and an analogy
of behaviour for definite corresponding values of
M and N . For the most widely used value of N ,
4, we determined a corresponding value in char-
acters of 18.
Consequently, this study paves the way to the
application of BLEU (in characters) in objec-
tive evaluation campaigns of automatic transla-
tion into languages without word delimiters, like
Chinese or Japanese, as it avoids any problem
with segmentation.
Acknowledgements
The research reported here was supported in part
by a contract with the National Institute of Infor-
mation and Communications Technology entitled
?A study of speech dialogue translation technol-
ogy based on a large corpus?.
12(ZHANG et al, 2004) reported confidence intervals of
around 2% (i.e., in this case, ?0.01) for BLEU, so that sys-
tem 2 and 3 are undistinguishable by BLEUw4.
82
References
Yasuhiro AKIBA, Marcello FEDERICO, Noriko
KANDO, Hiromi NAKAIWA, Michael PAUL, and
Jun?ichi TSUJII. 2004. Overview of the IWSLT04
evaluation campaign. In Proc. of the International
Workshop on Spoken Language Translation, pages
1?12, Kyoto, Japan.
Eric BRILL and Radu SORICUT. 2004. A unified
framework for automatic evaluation using n-gram
co-occurence statistics. In Proceedings of ACL
2004, pages 613?620, Barcelone.
Peter E. BROWN, Vincent J. DELLA PIETRA,
Stephen A. DELLA PIETRA, and Robert L. MER-
CER. 1993. The mathematics of statistical machine
translation: Parameter estimation. Computational
Linguistics, Special Issue on Using Large Corpora:
II, 19(2):263?311.
George DODDINGTON. 2002. Automatic evalua-
tion of machine translation quality using N-gram
co-occurrence statistics. In Proceedings of Human
Language Technology, pages 128?132, San Diego.
Huiming DUAN, Xiaojing BAI, Baobao CHANG, and
Shiwen YU. 2003. Chinese word segmentation at
Peking University. In Qing Ma and Fei Xia, edi-
tors, Proceedings of the Second SIGHAN Workshop
on Chinese Language Processing, pages 152?155.
Chin-Yew LIN and Eduard HOVY. 2003. Auto-
matic evaluation of summaries using n-gram co-
occurrence statistics. In Proceedings of HLT-
NAACL 2003, pages 71?78, Edmonton.
Y. MATSUMOTO, A. KITAUCHI, T. YAMASHITA,
Y. HIRANO, H. MATSUDA, and M. HASAHARA.
1999. Japanese morphological analysis system
ChaSen version 2.0. Technical report NAIST-IS-
TR99009, Nara Institute of Technology.
Kishore PAPINENI, Salim ROUKOS, Todd WARD, and
Wei-Jing ZHU. 2001. Bleu: a method for automatic
evaluation of machine translation. Research report
RC22176, IBM.
Mark PRZYBOCKI. 2004. The 2004 NIST machine
translation evaluation plan (MT-04).
Ying ZHANG, Stefan VOGEL, and Alex WAIBEL.
2004. Interpreting BLEU/NIST scores: how much
improvement do we need to have a better system?
In Proceedings of LREC 2004, volume V, pages
2051?2054, Lisbonne.
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 10
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
BLEUc18
BL
EU
w4
Figure 1: BLEUw4 in ordinates against BLEUc18 in abscissae.
83
Figure 2: On the left, experimental scores for BLEUw4 versus BLEUw3: all points are on the diagonal
or below. On the right, BLEUc18 scores versus BLEUw3: 90% of the points are on the diagonal or
below. In the middle, proportion of BLEUcM scores under BLEUw3 for M varying from 1 to 30.
Table 1: Equivalent Ns and Ms for BLEUwN and BLEUcM obtained by different methods.
BLEUw1 BLEUw2 BLEUw3 BLEUw4
Pearson?s correlation (best M ) 0.89 (5) 0.90 (8) 0.85 (10) 0.83 (17)
Kappa value (best M ) 0.17 (5) 0.29 (9) 0.34 (14) 0.35 (18)
best M for analogical behaviour
wrt to (N ? 1) (threshold = 90%) (9) (14) (18)
Table 2: Correlation of BLEUw4 scores with BLEUc18 scores by sentence length.
sentence length 4 5 6 7 8 9 10 > 10
points 12.9% 18.2% 13.6% 13.4% 7.5% 6.5% 5.0% 8.1%
average BLEUw4 score 0.188 0.300 0.252 0.364 0.345 0.318 0.321 0.015
std. dev. ?0.389 ?0.416 ?0.376 ?0.382 ?0.363 ?0.3150 ?0.346 ?0.291
local best M 16 17 16 19 17 17 16 12
Pearson?s correlation 0.827 0.795 0.797 0.824 0.899 0.894 0.952 0.919
global best M 18
Pearson?s correlation 0.788 0.794 0.779 0.805 0.883 0.871 0.929 0.861
Table 3: Overall BLEU scores for 4 different systems in BLEUw4 and BLEUc18.
system 1 system 2 system 3 system 4
overall BLEUw4 score 0.349 > 0.305 ? 0.312 > 0.232
overall BLEUc18 score 0.292 > 0.279 > 0.267 > 0.183
difference in scores ?0.057 ?0.036 ?0.045 ?0.049
Table 4: Distribution of the 510 sentences by lengths in words and characters.
length < 4 words ? 4 words total
< 18 characters 266 84 350
? 18 characters 37 123 160
total 302 208 510
84
The influence of data homogeneity on NLP system performance
Etienne Denoual
ATR Spoken Language Communication Research Labs,
2-2-2 Keihanna Science City, Kyoto 619-0288, Japan
Laboratoire CLIPS - GETA - IMAG, Universite? Joseph Fourier, Grenoble, France
etienne.denoual@atr.jp
Abstract
In this work we study the influence of
corpus homogeneity on corpus-based
NLP system performance. Experi-
ments are performed on both stochas-
tic language models and an EBMT sys-
tem translating from Japanese to En-
glish with a large bicorpus, in order
to reassess the assumption that using
only homogeneous data tends to make
system performance go up. We de-
scribe a method to represent corpus
homogeneity as a distribution of sim-
ilarity coefficients based on a cross-
entropic measure investigated in previ-
ous works. We show that beyond min-
imal sizes of training data the exces-
sive elimination of heterogeneous data
proves prejudicial in terms of both per-
plexity and translation quality : exces-
sively restricting the training data to a
particular domain may be prejudicial
in terms of In-Domain system perfor-
mance, and that heterogeneous, Out-of-
Domain data may in fact contribute to
better sytem performance.
1 Introduction
Homogeneity of large corpora is still an unclear
notion. In this study we make a link between the
notions of similarity and homogeneity : a large
corpus is made of sets of documents to which
may be assigned a score in similarity defined by
cross-entropic measures (similarity is implicitly
expressed in the data). The distribution of the
similarity scores of such subcorpora may then be
interpreted as a representation of the homogeneity
of the main corpus, which can in turn be used to
perform corpus adaptation to tune a corpus based
NLP system to a particular domain.
(Cavaglia` 2002) makes the assumption that a
corpus based NLP system generally yields bet-
ter results with homogeneous training data rather
than heterogeneous, and experiments on a text
classifier system (Rainbow1), to mixed conclu-
sions. We reassess this assumption by experi-
menting on language model perplexity, and on an
EBMT system translating from Japanese to En-
glish.
2 A framework for corpus homogeneity
2.1 Previous work on corpus similarity and
homogeneity
A range of measures for corpus similarity have
been put forward in past literature : (Kilgarriff
and Rose 98; Kilgarriff 2001) investigated on
the similarity of corpora and compared ?Known
Similarity Corpora? (KSC) using perplexity and
cross-entropy on words, word frequency mea-
sures, and a ?2-test which they found to be
the most robust. However(as acknowledged in
(Kilgarriff and Rose 98)), using KSC requires
that the two corpora chosen for comparison are
sufficiently similar that the most frequent lex-
emes in them almost perfectly overlap. How-
ever (Liebscher 2003) showed by comparing fre-
quency counts of different large Google Group
1See http://www.cs.cmu.edu/mccallum/bow .
226
corpora that it is not usually the case.
Measuring homogeneity by counting
word / lexeme frequencies introduces addi-
tional difficulties as it assumes that the word is an
obvious, well-defined unit, which is not the case
in the Chinese (Sproat and Emerson 2003) or
Japanese language (Matsumoto et al, 2002), for
instance, where word segmentation is not trivial.
(Denoual 2004) showed that similarity between
corpora could be quantified with a coefficient
based on the cross-entropies of probabilistic mod-
els built upon reference data. The approach
needed no explicit selection of features and was
language independent, as it relied on character
based models (as opposed to word based models)
thus bypassing the word segmentation issue and
making it applicable on any electronic data.
The cross-entropy HT (A) of an N-gram model
p constructed on a training corpus T , on a test
corpus A = {s1, .., sQ} of Q sentences with si =
{ci1..ci|si|} a sentence of |si| characters is:
HT (A) =
?Q
i=1[
?|si|
j=1?logpij ]?Q
i=1 |si|
(1)
where pij = p(cij |cij?N+1..cij?1).
We therefore define a scale of similarity be-
tween two corpora on which to rank any third
given one. Two reference corpora T1 and T2 are
selected by the user, and used as training sets to
compute N-gram character models. The cross-
entropies of these two reference models are es-
timated on a third test set T3, and respectively
named HT1(T3) and HT2(T3) as in the notation in
Eq. 1. Both model cross-entropies are estimated
according to the other reference , i .e ., HT1(T2)
and HT1(T1), HT2(T1) and HT2(T2) so as to ob-
tain the weights W1 and W2 of references T1 and
T2 :
W1 = HT1(T3)?HT1(T1)HT1(T2)?HT1(T1)
(2)
and :
W2 = HT2(T3)?HT2(T2)HT2(T1)?HT2(T2)
(3)
After which W1 and W2 are assumed to be
the weights of the barycentre between the user-
chosen references. Thus
I(T3) = W1W1 +W2 =
1
1 + W2W1
(4)
is defined to be the similarity coefficient between
reference sets 1 and 2, which are respectively cor-
pus T1 and corpus T2 . Given the previous as-
sumptions, I(T1) = 0 and I(T2) = 1 ; further-
more, any given corpus T3 yields a score between
the extrema I(T1) = 0 and I(T2) = 1
This framework may be applied to the quantifi-
cation of the similarity of large corpora, by pro-
jecting them to a scale defined implicitly via the
reference data selection. In this study we shall
specifically focus on a scale of similarity bounded
by a sublanguage of spoken conversation on the
one hand, and a sublanguage of written style me-
dia on the other.
We build upon this previous work in order to
represent intra-corpus homogeneity.
2.2 Representing corpus homogeneity
Corpora are collected sets of documents usually
originating from various sources. Whether a cor-
pus is homogeneous in content or not is scarcely
known besides the knowledge of the nature of
the sources. As homogeneity is multidimensional
(see (Biber 1988) and (Biber 1995) for consid-
erations on the dimensions in register variation
for instance), one cannot trivially say that a cor-
pus is homogeneous or heterogeneous : different
sublanguages show variations that are lexical, se-
mantic, syntactic, and structural (Kittredge and
Lehrberger 1982).
In this study we wish to implicitly capture such
variations by applying the previously described
similarity framework to the representation of ho-
mogeneity. Coefficients of similarity may be
computed for all smaller sets in a corpus, the dis-
tribution of which shall depict the homogeneity
of the corpus relatively to the scale defined im-
plicitly by the choice of the reference data.
Homogeneity as depicted here is relative to the
choice of reference training data, which implic-
itly embrace lexical and syntactic variations in a
sublanguage (which are by any means not unidi-
mensional, as argued previously). We focus as in
(Denoual 2004) on a scale of similarity bounded
by a sublanguage of spoken conversation on the
one hand, and a sublanguage of written style me-
dia on the other.
227
3 A study of the homogeneity of a large
bicorpus
3.1 Data
Reference data is needed to set up a scale of sim-
ilarity, and implicitly bound it.
For the sublanguage of spoken conversation we
used for both English and Japanese the SLDB
(Spontaneous Speech Database) corpus, a multi-
lingual corpus of raw transcripts of dialogues de-
scribed in (Nakamura et al, 1996).
For the sublanguage of written style media, we
used for English a part of the Calgary2 corpus,
containing several contemporary English litera-
ture pieces3, and for Japanese a corpus of col-
lected articles from the Nikkei Shinbun newspa-
per4.
The large multilingual corpus that is used in
our study is the C-STAR5 Japanese / English part
of an aligned multilingual corpus, the Basic Trav-
eller?s Expressions Corpus (BTEC).
A prerequisite of the method is that levels of
data transcriptions are strictly normalized, so that
the comparison is not made on the transcription
method but on the underlying signal itself.
3.2 Homogeneity in the BTEC
The BTEC is a collection of sentences originat-
ing from 197 sets (one set originating from one
phrasebook) of basic travel expressions. Here we
examine the distribution of the similarity coeffi-
cients assigned to its subsets.
The corpus may be segmented in a variety of
manners, however we wish to proceed in two intu-
itive ways : firstly, by keeping the original subdi-
vision, i .e ., one phrasebook per subset ; secondly,
at the level of the sentence, i .e ., one sentence per
subset . Figure 1 shows the similarity coefficient
distributions for Japanese and English at the sen-
tence and subset level, and Table 1 shows their
means and standard deviations.
The difference in means and standard deviation
2The Calgary Corpus is available via anonymous ftp at
ftp.cpcs.ucalgary.ca/pub/projects/text.compression.corpus .
3Parts are entitled book1, book2 and book3.
4The use of classical Japanese literature is not appropri-
ate as (older) copyright-free works make use of a consider-
ably different language. In order to maintain a certain homo-
geneity, we limit our study to contemporary language.
5See http://www.c-star.org .
Coefficient Japanese English
Phrasebook 0.330?0.020 0.288?0.027
Line 0.315?0.118 0.313?0.156
Table 1: Means ? standard deviations of the simi-
larity coefficient distributions in Japanese and En-
glish.
values can be explained by the fact that all phrase-
books do not have the same size in lines6. The dis-
tribution of similarity coefficients at the line level,
however similar to the distribution at the phrase-
book level, suggests in its irregularities that it is
indeed safer to use a larger unit to estimate cross-
entropies. Moreover, we wish not to tamper with
the integrity of the original subsets, that is to keep
the integrity of phrasebook contents as much as
possible.
On the phrasebook level, the similarity coef-
ficient has a low correlation on both the aver-
age phrasebook length (0.178) and the average
line length (0.278) (which does not make it a
too ?shallow? profiling method). On the other
hand, correlation is high between the coefficients
in Japanese and English (0.781), which is only to
be expected intuitively.
4 Experiments
4.1 Method
This work wishes to reassess the assumption that,
for a same amount of training data, a corpus-
based NLP system performs better when its data
tends to be homogeneous. Here we use the rep-
resentation of homogeneity defined by the sim-
ilarity coefficient scale to select data that tends
to be homogeneous to an expected task. Exper-
iments shall be performed both on randomly se-
lected data, and on data selected according to their
similarity coefficient. The closer the coefficient
of the training data is to the coefficient of the ex-
pected task, the better.
We assume that the task is sufficiently repre-
sented by a set of data from the same domain as
the large bicorpus used, the BTEC. Experiments
are performed on a test set of 510 Japanese sen-
tences which are not included in the ressource.
6The BTEC phrasebooks have an average size of 824
lines with a standard deviation in size of 594 lines.
228
0 0.2 0.4 0.6 0.8 10
100
200
300
400
500
600
700
800
Japanese BTEC Coefficients
Oc
cur
ren
ces
0 0.2 0.4 0.6 0.8 10
100
200
300
400
500
600
700
800
English BTEC Coefficients
Oc
cur
ren
ces
Line level
Phrasebook level
Line level
Phrasebook level
Figure 1: Distributions of similarity coefficients at the sentence level (thin line) and at the phrasebook
level (thick line), respectively for Japanese and English.
0 50 1000
0.2
0.4
0.6
0.8
1
BL
EU
 sc
ore
 (in
ter
pol
ate
d)
0 50 100
4
6
8
10
Percent of the original BTEC in size
NI
ST
 sc
ore
 (in
ter
pol
ate
d)
0 50 1000
0.2
0.4
0.6
0.8
1
m
W
ER
 sc
or
e (
inte
rpo
late
d)
Randomly selected data
Homogeneous data
Figure 2: BLEU, NIST and mWER scores for EBMT systems built on increasing amounts of randomly
chosen and homogeneous BTEC data.
These sentences shall first be used for language
model perplexity estimation, then as input sen-
tences for the EBMT system. The task is found to
have a coefficient of I0 = 0.331. The average co-
efficient for a BTEC phrasebook being 0.330, the
task is found to be particularly in the domain of
the ressource. We examine the influence of train-
ing data size first on language model perplexity,
then on the quality of translation from Japanese
to English by an example-based MT system.
4.1.1 Language model perplexity
Even if perplexity does not always yield a high
correlation with NLP systems performance, it is
still an indicator of language model complexity as
it gives an estimate of the average branching fac-
tor in a language model. The measure is popular
in the NLP community because admittedly, when
perplexity decreases, the performance of systems
based on stochastic models tends to increase.
We compute perplexities of character language
models built on variable amounts of training data
first randomly taken from the Japanese part of
the BTEC, and then selected around the expected
task coefficient I0 (thresholds are determined by
the amount of training data to be kept). Cross-
entropies are estimated on the test set, and all es-
timations are performed five times for the ran-
dom data selections and averaged. Figure 3
shows the character perplexity values for increas-
ing amounts of data from 0.5% to 100% of the
BTEC and interpolated. As was expected, per-
plexity decreases as training data increases and
tends to have an asymptotic behaviour when more
data is being used as training.
229
0 10 20 30 40 50 60 70 80 90 1000
5
10
15
20
25
30
Percent of the original BTEC in size
Char
acte
r per
plexi
ty (int
erpola
ted)
Randomly selected dataHomogeneous data
Figure 3: Perplexity of character language models
built on increasing amounts of randomly chosen
BTEC and homogeneous Japanese data.
While homogeneous data yield lower perplex-
ity scores for small amounts of training data (up
to 15% of the ressource - roughly 1.5 Megabytes
of data), beyond this value perplexity is slightly
higher than for a model trained on randomly se-
lected data. Except for the smaller amounts of
data, there seems to be no benefit in using homo-
geneous rather than random heterogeneous train-
ing data for model perplexity. On the contrary,
excessively restricting the domain seems to yield
higher model perplexities.
4.1.2 Automatic evaluation of the translation
quality
In this section we experiment on a Japanese to
English grammar-based EBMT system, HPATR
(described in (Imamura 2001)), which parses a bi-
corpus with grammars for both source and target
language, and translates by automatically gener-
ating transfer patterns from bilingual trees con-
structed on the parsed data. Not being a MT
system based on stochastic methods, it is used
here as a task evaluation criterion complemen-
tary to language model perplexity. Systems are
likewise constructed on variable amounts of train-
ing data, and evaluated on the previous task of
510 Japanese sentences, to be translated from
Japanese to English.
Because it is not feasible here to have humans
judge the quality of many sets of translated data,
we rely on an array of well known automatic eval-
uation measures to estimate translation quality :
? BLEU (Papineni et al 2002) is the geomet-
ric mean of the n-gram precisions in the out-
put with respect to a set of reference trans-
lations. It is bounded between 0 and 1, bet-
ter scores indicate better translations, and it
tends to be highly correlated with the fluency
of outputs ;
? NIST (Doddington 2002) is a variant of
BLEU based on the arithmetic mean of
weighted n-gram precisions in the output
with respect to a set of reference translations.
It has a lower bound of 0, no upper bound,
better scores indicate better translations, and
it tends to be highly correlated with the ade-
quacy of outputs ;
? mWER (Och 2003) or Multiple Word Error
Rate is the edit distance in words between
the system output and the closest reference
translation in a set. It is bounded between 0
and 1, and lower scores indicate better trans-
lations.
Figure 2 shows BLEU, NIST and mWER
scores for increasing amounts of data from 0.5%
to 100% of the BTEC and interpolated. As was
expected, MT quality increases as training data
increases and tends to have an asymptotic be-
haviour when more data is being used in training.
Here again except for the smaller amounts of data
(up to 3% of the BTEC in BLEU, up to 18% in
NIST and up to 2% in mWER), using the three
evaluation methods, translation quality is equal or
higher when using random heterogenous data. If
we perform a mean comparison of the 510 paired
score values assigned to sentences, for instance at
50% of training data, this difference is found to be
statistically significant between BLEU, NIST, and
mWER scores with confidence levels of 88.49%,
99.9%, and 73.24% respectively.
5 Discussion and future work
The contribution of this work is twofold :
We describe a method of representing homo-
geneity according to a cross-entropic measure of
similarity to reference sublanguages, that can be
used to profile language ressources. A corpus
is represented by the distribution of the similar-
ity coefficients of the smaller subsets it contains,
230
and atypical therefore heterogeneous data may be
characterized by the lower occurrences of their
values.
We further observe that marginalizing such
atypical data in order to restrict the domain on
which a corpus-based NLP system operates does
not yield better performance, either in terms of
perplexity when the system is based on stochastic
language models, or in terms of objective transla-
tion quality when the system is a grammar-based
EBMT system.
An objective for future work is therefore
to study corpus adaptation with Out-of-Domain
data. While (Cavaglia` 2002) also acknowledged
that for minimal sizes of training data, the best
NLP system performance is reached with ho-
mogeneous ressources, we would like to know
more precisely why and to what extent mixing
In-Domain and Out-of-Domain data yields better
accuracy. Concerning the representation of ho-
mogeneity, other experiments are needed to tackle
the multidimensionality of sublanguage varieties
less implicitly. We would like to consider multi-
ple sublanguage references to untangle the dimen-
sions of register variation in spoken and written
language.
Acknowledgements
This research was supported in part by the Na-
tional Institute of Information and Communica-
tions Technology.
References
Douglas Biber. 1988. Variation across speech and
writing. Cambridge University Press.
Douglas Biber. 1995. Dimensions in Register Varia-
tion. Cambridge University Press.
Gabriela Cavaglia`. 2002. Measuring corpus homo-
geneity using a range of measures for inter-
document distance. Proceedings of LREC, pp. 426-
431.
Etienne Denoual. 2004. A method to quantify corpus
similarity and its application to quantifying the de-
gree of literality in a document. Proceedings of the
International Workshop on Human Language Tech-
nology, Hong Kong, pp.28-31.
George Doddington. 2002. Automatic evaluation of
machine translation quality using n-gram co-
occurrence statistics. Proceedings of Human Lang.
Technol. Conf. (HLT-02), pp.138-145.
Kenji Imamura. 2001. Hierarchical Phrase Alignment
Harmonized with Parsing. Proceedings of NLPRS,
pp.377-384.
Adam Kilgarriff and Tony Rose. 1998. Measures for
corpus similarity and homogeneity. Proceedings of
the 3rd conference on Empirical Methods in Natural
Language Processing, Granada, Spain, pp. 46 - 52.
Adam Kilgarriff. 2001. Comparing corpora. Interna-
tional Journal of Corpus Linguistics 6:1, pp. 1-37.
Richard Kittredge and John Lehrberger. 1982. Sublan-
guage. Studies of language in restricted semantic
domains Walter de Gruyter, editor.
Robert A. Liebscher. 2003. New corpora, new tests,
and new data for frequency-based corpus compar-
isons. Center for Research in Language Newsletter,
15:2
Yuji Matsumoto, Akira Kitauchi, Tatsuo Yamashita,
Yoshitaka Hirano, Hiroshi Matsuda,Kazuma
Takaoka and Masayuki Asahara. 2002. Morpholog-
ical Analysis System ChaSen version 2.2.9 Manual.
Nara Institute of Science and Technology.
Atsushi Nakamura, Shoichi Matsunaga, Tohru
Shimizu, Masahiro Tonomura and Yoshinori Sag-
isaka 1996. Japanese speech databases for robust
speech recognition. Proceedings of the ICSLP?96,
Philadelphia, PA, pp.2199-2202, Volume 4
Franz Josef Och. 2003. Minimum Error Rate Training
in Statistical Machine Translation. Proceedings of
ACL 2003, pp.160-167.
Kishore Papineni, Salim Roukos, Todd Ward and Wei-
Jing Zhu. 2002. Bleu: a Method for Automatic
Evaluation of Machine Translation. Proceedings of
ACL 2002, pp.311-318.
Richard Sproat and Thomas Emerson. 2003. The First
International Chinese Word Segmentation Bakeoff.
The Second SIGHAN Workshop on Chinese Lan-
guage Processing, Sapporo, Japan.
231
Automatic generation of paraphrases
to be used as translation references
in objective evaluation measures of machine translation
Yves Lepage Etienne Denoual
ATR ? Spoken language communication research labs
Keihanna gakken tosi, 619-0288 Kyoto, Japan
{yves.lepage, etienne.denoual}@atr.jp
Abstract
We propose a method that automat-
ically generates paraphrase sets from
seed sentences to be used as refer-
ence sets in objective machine trans-
lation evaluation measures like BLEU
and NIST. We measured the quality
of the paraphrases produced in an ex-
periment, i.e., (i) their grammatical-
ity: at least 99% correct sentences;
(ii) their equivalence in meaning: at
least 96% correct paraphrases either
by meaning equivalence or entailment;
and, (iii) the amount of internal lexi-
cal and syntactical variation in a set of
paraphrases: slightly superior to that
of hand-produced sets. The paraphrase
sets produced by this method thus seem
adequate as reference sets to be used for
MT evaluation.
1 Introduction
We present and evaluate a method to automati-
cally produce paraphrases from seed sentences,
from a given linguistic resource. Lexical and syn-
tactical variation among paraphrases is handled
through commutations exhibited in proportional
analogies, while well-formedness is enforced by
filtering with sequences of characters of a cer-
tain length. In an experiment, the quality of the
paraphrases produced, i.e., (i) their grammatical-
ity, (ii) their equivalence in meaning with the seed
sentence, and, (iii) the internal lexical and syn-
tactical variation in a set of paraphrases, was as-
sessed by sampling and objective measures.
2 Motivation
Paraphrases are an important element in the eval-
uation of many natural language processing tasks.
Specifically, in the automatic evaluation of ma-
chine translation systems, the quality of transla-
tion candidates is judged against reference trans-
lations that are paraphrases in the target language.
Automatic measures like BLEU (PAPINENI et al,
2001) or NIST (DODDINGTON, 2002) do so by
counting sequences of words in such paraphrases.
It is expected that such reference sets contain
synonymous sentences (i.e., paraphrases) that ex-
plicit possible lexical and syntactical variations in
order to cope with translation variations in terms
and structures (BABYCH and HARTLEY, 2004).
In order to produce such reference sets, we pro-
pose a method to generate paraphrases from a
seed sentence where lexical and syntactical vari-
ations are handled by the use of commutations as
captured by proportional analogies whereas N -
sequences are used to enforce fluency of expres-
sion and adequacy of meaning.
3 The linguistic resource used
The linguistic resource used in the experiment
presented in this paper relies on the C-STAR col-
lection of utterances called Basic Traveler?s Ex-
pressions1. This is a multilingual resource of ex-
pressions from the travel and tourism domain that
contains 162,318 aligned translations in several
languages, among which English. The items are
quite short as the following examples show (one
line is one item in the corpus), and as the figures
in Table 1 show.
1http://www.c-star.org/.
57
Number of Avg. size ? std. dev.
6= sentences in characters in words
97,769 35.14 ? 18.81 6.86 ? 3.57
Table 1: Some statistics about the linguistic resource
Number of Avg. size ? std. dev.
6= sentences in characters in words
42,249 33.15 ? 9.31 6.44 ? 1.90
Table 2: Some statistics about the paraphrases produced
Thank you so much. Keep the change.
Bring plenty of lemon, please.
Please tell me about some interesting places
[near here.
Thank you. Please sign here.
How do you spell your name?
The quality of this resource is of at least 99%
correct sentences (p-value = 1.92%). The few in-
correct sentences contain spelling errors or slight
syntactical mistakes.
4 Our paraphrasing methodology
4.1 Our algorithm
The proposed method consists in two phases:
firstly, paraphrase detection through equality of
translation and secondly, paraphrase generation
through linguistic commutations based on the
data produced in the first phase:
? Detection: find sentences which share a
same translation in the multilingual resource
(4.2);
? Generation: produce new sentences by ex-
ploiting commutations (4.3); limit combina-
torics by contiguity constraints (4.4).
Each of the steps of the previous algorithm is
explained in details in the following sections.
4.2 Initialisation by paraphrase detection
In a first phase we initialise our data by para-
phrase detection. By definition, paraphrase is
an equivalence in meaning, thus, different sen-
tences having the same translation ought to be
considered equivalent in meaning, i.e., they are
paraphrases2. As the linguistic resource used
2This is basically the same approach as (OHTAKE and
YAMAMOTO, 2003, p. 3 and 4).
in the present experiment is a multilingual cor-
pus, we have at our disposal the corresponding
translations in different languages for each of its
sentences. For instance, the following English
sentences share a common Japanese translation
shown in bold face below. Therefore, they are
paraphrases.
A beer, please. ?????????
???????
???????????
Beer, please. ????
???????????
?????????
????????
??????????
Can I have a beer? ?????????
Give me a beer, please. ?????????
I would like beer. ?????????
I?d like a beer, please. ?????????
4.3 Commutation in proportional analogies
for paraphrase generation
In a second phase, we implement paraphrase gen-
eration. Any given sentence may share commu-
tations with other sentences of the corpus. Such
commutations are best seen in analogical relations
that explicit syntagmatic and paradigmatic varia-
tions (de SAUSSURE, 1995, part 3, chap 4). For
instance, the seed sentence
A slice of pizza, please.
58
I?d like a beer,
please. : A beer, please. ::
I?d like a slice of
pizza, please. :
A slice of pizza,
please.
I?d like a twin,
please. : A twin, please. ::
I?d like a slice of
pizza, please. :
A slice of pizza,
please.
I?d like a bottle of
red wine, please. :
A bottle of red
wine, please. ::
I?d like a slice of
pizza, please. :
A slice of pizza,
please.
Table 3: Some analogies formed with sentences of the linguistic resource that show commutations with
the sentence A slice of pizza, please.
(i) I?d like a beer,
please. : A beer, please. ::
I?d like a slice of
pizza, please. :
A slice of pizza,
please.
(ii) I?d like a beer,
please. : Can I have a beer? ::
I?d like a slice of
pizza, please. : x
(iii) I?d like a beer,
please. : Can I have a beer? ::
I?d like a slice of
pizza, please. :
Can I have a slice
of pizza?
Table 4: Generating a paraphrase for the seed sentence A slice of pizza, please. using proportional
analogies. (i) The original proportional analogy taken from Table 3. (ii) Replacing the sentence A
beer, please. with one of its paraphrases acquired during the detection phase: Can I have a beer? The
last sentence of the proportional analogy becomes unknown. (iii) Solving the analogical equation, i.e.,
generating a paraphrase of A slice of pizza, please.
enters in the analogies of Table 3. The replace-
ment of some sentences with known paraphrases
in such analogies allows us to produce new sen-
tences. This explains why we needed some para-
phrases to start with. For instance, by replacing
the sentence:
A beer, please.
with the sentence:
Can I have a beer?
in the first analogy of Table 3, one gets the fol-
lowing analogical equation, that is solved as indi-
cated.
I?d like a beer, please. : Can I have a beer? ::
I?d like a slice of pizza, please. : x
? x = Can I have a slice of pizza?
It is then legitimate to say that the produced sen-
tence:
Can I have a slice of pizza?
is a paraphrase of the seed sentence (see Table 4).
Such a method alleviates the problem of cre-
ating templates from examples which would be
used in an ulterior phase of generation (BARZI-
LAY and LEE, 2003). Here, all examples in the
corpus are potential templates in their actual raw
form, with the advantage that the choice of the
places where commutations may occur is left to
proportional analogy.
4.4 Limitation of combinatorics by
contiguity constraints
During paraphrase generation, spurious sentences
may be produced. For instance, the replacement
in the previous analogy, of the sentence:
A beer, please.
by the following paraphrase detected during the
first phase:
A bottle of beer, please.
produces the unfortunate sentence:
?A bottle of slice of pizza, please.
Moreover, as no complete and valid formalisation
of linguistic analogies has yet been proposed, the
algorithm used (LEPAGE, 1998) may deliver such
unacceptable strings as:
59
43 Could we have a table in the corner?
43 I?d like a table in the corner.
43 We would like a table in the corner.
28 Can we have a table in the corner?
5 Can I get a table in the corner?
5 In the corner, please.
4 We?d like to sit in the corner.
2 I?d like to sit in the corner.
2 I would like a table in the corner.
2 We?d like a table in the corner.
1 I?d prefer a table in the corner.
1 I prefer a table in the corner.
Table 5: Paraphrase candidates for Can we have a table in the corner? Candidates filtered out by unseen
N -sequences (N = 20) are struck out. Notice that the seed sentence itself has been generated again
by the method (4th sentence from the top). The figures on the left are the frequencies with which the
sentence has been generated.
?A slice of pizzthe, pleaset for tha, please.
In order to ensure a very high rate of well-
formedness among the sentences produced, we
require a method that extracts well-formed sen-
tences from the set of generated sentences with a
very high precision (to the possible prejudice of
the recall).
To this end, we eliminate all sentences contain-
ing sequences of characters of a given length un-
seen in the original data3. It is clear that, by ad-
equately tuning the given length, such a method
will be able to retain a satisfactory number of sen-
tences that will be undoubtedly correct, at least in
the sense of the linguistic resource.
5 Experiments
During the first phase of paraphrase detection,
26, 079 sentences (out of 97, 769) got at least one
possibly incorrect paraphrase candidate with an
average of 5.35 paraphrases by sentence. How-
ever, the distribution is not uniform: 60 sentences
get more than 100 paraphrases.
The maximum is reached with 529 paraphrases
for the sentence Sure. Such a sentence has a vari-
ety of meanings depending on the context, which
explains the high number of its possible para-
phrases as illustrated below.
3This is conform to the trend of using N -sequences to as-
sess the quality of outputs of various NLP systems like (LIN
and HOVY, 2003) for summary generation, (DODDINGTON,
2002) for machine translation, etc..
Sure. Here you are.
Sure. This way, please.
Certainly, go ahead, please.
I?m sure I will.
No, I don?t mind a bit.
Okay. I understand quite well, thank you.
Sounds fine to me.
Yes, I do.
. . .
However, such an example shows also that the
more the paraphrases obtained by this method, the
less reliable their quality.
During the second phase of paraphrase gener-
ation, the method generated 4, 495, 266 English
sentences on our linguistic resource. An inspec-
tion of a sample of 400 sentences shows that the
quality lies around 23.6% of correct sentences (p-
value = 1.19%) in syntax and meaning. The set
of paraphrase candidates obtained on an example
sentence are shown in Table 5.
To ensure fluency of expression and adequacy
of meaning, the method then filtered out any sen-
tence containing an N -sequence unseen in the
corpus (see Section 4.4). The best value for N
that allowed us to obtain a quality rate at the same
level to that of the original linguistic resource was
20.
As a final result, the number of seed sentences
for which we obtained at least one paraphrase is
16, 153. With a total number of 147, 708 para-
60
phrases generated4, the average number of para-
phrases per sentence is 8.65 with a standard de-
viation of 16.98 which means that the distribu-
tion is unbalanced. The graph on the left of Fig-
ure 1 shows the number of seed sentences with the
same number of paraphrases. while the graph on
the right shows the number of paraphrases against
the length of the seed sentence in words.
6 Quality of the generated paraphrases
6.1 Well-formedness of the generated
paraphrases
The grammatical quality of the paraphrase candi-
dates obtained was evaluated on a sample of 400
sentences: at least 99% of the paraphrases may
be considered grammatically correct (p-value =
2.22%). This quality is approximately the same
as that of the original resource: at least 99% (p-
value = 1.92%).
An overview of the errors in the generated para-
phrases suggests that they do not differ from the
ones in the original data. For instance, one notes
that an article is lacking before the noun phrase
tourist area in the following sentence:
Where is tourist area?
Although we are not able to trace the error back
to its origin, such a mistake is certainly due to a
commutation with a sentence like:
Where is information office?
that contains a similar mistake and that is found
in the original linguistic resource.
6.2 Equivalence in content between
generated paraphases and seed sentence
The semantic quality of the paraphrases produced
was also checked by hand on a sample of 470
paraphrases that were compared with their cor-
responding seed sentence. We not only checked
for strict equivalence, but also for meaning entail-
ment5.
4The same sentence may have been generated several
times for different seed sentences. Overall there were
42, 249 different sentences generated. Their lengths in char-
acters and words are given in Table 2.
5Bill Dolan, Chris Brockett, and Chris Quirk,
Microsoft Research Paraphrase Corpus, http://-
research.microsoft.com/research/nlp-
/msr paraphrase.htm.
The following three paraphrases on the left
with their corresponding seed sentences on the
right are examples that were judged to be strict
equivalences.
Can I see some ID? Could you show me
some ID?
Please exchange this. Could you exchange
this, please.
Please send it to
Japan.
Send it to Japan,
please.
The following are examples in which there is a
lack of information either in the paraphrase pro-
duced or in the seed sentence. This is precisely
what entailment is.
Coke, please. Miss, could I have a
coke?
I want to change
money. Please exchange this.
Sunny-side up, please. Fried eggs, sunny-side
up, please.
The result of the sampling is that the paraphrase
candidates can be considered valid paraphrases in
at least 94% of the cases either by equivalence
or entailment (p-value = 3.05%). The following
sentences exemplify the remaining cases where
two sentences were not judged valid paraphrases
of one another.
Do you charge extra if
I drop it off?
There will be a drop
off charge.
Here?s one for you,
sir.
You can get one here.
There it is. Yes, please sit down.
Table 6 summarises the distribution of para-
phrase candidates according to the abovemen-
tionned classification.
61
 0
 50
 100
 150
 200
 250
 300
 350
 1  10  100  1000  10000
n
u
m
be
r o
f p
ar
ap
hr
as
es
number of seed sentences (log scale)
 0
 50
 100
 150
 200
 250
 300
 350
 0  2  4  6  8  10  12  14  16  18
n
u
m
be
r o
f p
ar
ap
hr
as
es
length of seed sentences in words
Figure 1: Number of seed sentences with the same number of paraphrases (on the left). Number of
paraphrases by length of seed sentence in words (on the right).
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  2  4  6  8  10  12  14
av
er
ag
e 
BL
EU
 sc
or
es
length of seed sentence in words
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  2  4  6  8  10  12  14
av
er
ag
e 
N
IS
T 
sc
or
es
length of seed sentence in words
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  20  40  60  80  100  120
av
er
ag
e 
BL
EU
 sc
or
es
number of paraphrases / seed sentence
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  20  40  60  80  100  120
av
er
ag
e 
N
IS
T 
sc
or
es
number of paraphrases / seed sentence
Figure 2: BLEU and NIST scores by length of seed sentence (upper graphs) and by number of para-
phrases per seed sentence (lower graphs). In these graphs, each point is the score of a set of paraphrases
against the seed sentence they were produced for. Lower scores indicate a greater lexical and syntactical
variation in paraphrases. The connected points show mean values along the axis of abscissae.
62
Paraphrase Not a paraphrase
Equivalence Entailment
346 104 20
Table 6: Equivalence or entailment in meaning of the paraphrases produced, on a sample of 470 para-
phrases from various seed sentences.
7 Measure of lexical and syntactical
variation in paraphrases
7.1 Objective measures
We assessed the lexical and syntactical variation
of our paraphrases on a sample of 400 seed sen-
tences using BLEU and NIST. On the contrary to
evaluation of machine translation where the goal
is to obtain high scores in BLEU and NIST, our
goal here, when comparing a paraphrase to the
seed sentence it has been produced for, is to get
low scores. Indeed, high scores reflect some high
correlation with translation references that is a
lesser variation. As our goal is precisely to pre-
pare data for evaluation with BLEU and NIST, it
is thus to generate sets of paraphrases that would
contain as much variation as possible to express
the same meaning as the seed sentences, i.e. we
look for low scores in BLEU and NIST.
Again, all this can be done safely as long as
one is sure that the sentences compared are valid
sentences and valid paraphrases. This is the case
of our data, as we have already shown that the
paraphrases produced are 99% grammatically and
semantically correct sentences and that they are
paraphrases of their corresponding seed sentences
in 94% of the cases.
As for the meaning of BLEU and NIST, they
are supposed to measure complementary charac-
teristics of translations: namely fluency and ade-
quacy (AKIBA et al, 2004, p. 7). BLEU tends
to measure the quality in form of expression (flu-
ency), while NIST6 tends to measure quality in
meaning (adequacy).
7.2 Results
The scores in BLEU and NIST (both on a scale
from 0 to 1) shown in Figure 2 are interpreted
6Formally, NIST is an open scale. Hence, scores cannot
be directly compared for different seed sentences. We thus
normalised them by the score of the seed sentence against
itself. In this way, NIST scores become comparable for dif-
ferent seed sentences.
as a measure of the lexical and syntactical vari-
ation among paraphrases. The lower they are, the
greater the variation. The upper graphs show that
this variation depends clearly on the lengths of the
seed sentences. The shorter the seed sentence,
the greater the variation among the paraphrases
produced by this method. This is no surprise as
the detection phase introduces a bias as was men-
tionned in Section 5 with the example sentence
Sure.
The lower graphs show that the variation does
not depend on the number of paraphrases per seed
sentence. Hence, on the contrary to a method
that would produce more variations as more para-
phrases are generated, in our method, the varia-
tion is not expected to change when one produces
more and more paraphrases (however, the gram-
matical quality or the paraphrasing quality could
change). In this sense, the method is scalable, i.e.,
one could tune the number of paraphrases wished
without considerably altering the lexical and syn-
tactical variation.
7.3 Comparison with reference sets
produced by hand
We compared the lexical and syntactical variation
of our paraphrases with paraphrases created by
hand for a past MT evaluation campaign (AKIBA
et al, 2004) in two language pairs: Japanese to
English and Chinese to English.
For every reference set, we evaluated each sen-
tence against one chosen at random and left out.
The mean of all these evaluation scores gives an
indication on the overall internal lexical and syn-
tactical variation inside the reference sets. The
lower the scores, the better the lexical and syn-
tactical variation. This scheme was applied to
both reference sets created by hand, and to the
one automatically produced by our method. The
scores obtained are shown on Figure 7. Whereas
BLEU scores are comparable for all reference
sets, which indicates no notable difference in flu-
63
Average Average
BLEU NIST
Automatically produced set 0.11 0.39
Hand-produced set 1 0.10 0.49
Hand-produced set 2 0.11 0.49
Table 7: Measure of the lexical and syntactical variation of various reference sets produced by hand
and automatically produced by our method. The lower the scores, the better the lexical and syntactical
variation.
ency, NIST scores are definitely better for the au-
tomatically produced reference set: this hints at a
possibly richer lexical variation.
8 Conclusion
We reported a technique to generate paraphrases
in the view of constituting reference sets for ma-
chine translation evaluation measures like BLEU
and NIST. In an experiment with a linguistic re-
source of 97, 769 sentences we generated 8, 65
paraphrases in average for 16, 153 seed sentences.
The grammaticality was evaluated by sampling
and was shown to be of at least 99% grammati-
cally and semantically correct sentences (p-value
= 2.22%), a quality comparable to that of the orig-
inal linguistic resource. In addition, at least 96%
of the candidates (p-value = 1.92%) were correct
paraphrases either by meaning equivalence or en-
tailment.
Finally, the lexical and syntactical variation
within each paraphrase set was assessed using
BLEU and NIST against the seed sentence. It was
found that the lexical and syntactical variation did
not depend upon the number of paraphrases gen-
erated, but on the length of the seed sentence.
Going back to the view of constituting refer-
ence sets for machine translation evaluation, not
only are the paraphrase sets produced by this
method correct sentences and valid paraphrases,
but they also exhibit an internal lexical and syn-
tactical variation which was shown to be slightly
superior to that of two evaluation campaign sets
of paraphrases produced by hand.
Acknowledgements
This research was supported in part by the Na-
tional Institute of Information and Communica-
tions Technology.
References
Yasuhiro AKIBA, Marcello FEDERICO, Noriko
KANDO, Hiromi NAKAIWA, Michael PAUL, and
Jun?ichi TSUJII. 2004. Overview of the IWSLT04
evaluation campaign. In Proc. of the International
Workshop on Spoken Language Translation, pages
1?12, Kyoto, Japan.
Bogdan BABYCH and Anthony HARTLEY. 2004. Ex-
tending the BLEU MT evaluation method with fre-
quency weighting. In Proceedings of ACL 2004,
pages 621?628, Barcelone, July.
Regina BARZILAY and Lillian LEE. 2003. Learn-
ing to paraphrase: An unsupervised approach us-
ing multiple-sequence alignment. In HLT-NAACL
2003: Main Proceedings, pages 16?23.
Ferdinand de SAUSSURE. 1995. Cours de linguis-
tique ge?ne?rale. Payot, Lausanne et Paris. [1e
e?d. 1916].
George DODDINGTON. 2002. Automatic evalua-
tion of machine translation quality using N-gram
co-occurrence statistics. In Proceedings of Human
Language Technology, pages 128?132, San Diego,
March.
Yves LEPAGE. 1998. Solving analogies on words:
an algorithm. In Proceedings of COLING-ACL?98,
volume I, pages 728?735, Montre?al, August.
Chin-Yew LIN and Eduard HOVY. 2003. Auto-
matic evaluation of summaries using n-gram co-
occurrence statistics. In Proceedings of HLT-
NAACL 2003, pages 71?78, Edmonton, May.
Kiyonori OHTAKE and Kazuhide YAMAMOTO. 2003.
Applicability analysis of corpus-derived para-
phrases toward example-based paraphrasing. In
Language, Information and Computation, Proceed-
ings of 17th Pacific Asia Conference, pages 380?
391.
Kishore PAPINENI, Salim ROUKOS, Todd WARD, and
Wei-Jing ZHU. 2001. Bleu: a method for automatic
evaluation of machine translation. Research report
RC22176, IBM, September.
64
