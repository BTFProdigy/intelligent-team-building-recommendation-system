Proceedings of the NAACL HLT 2010 Workshop on Computational Approaches to Analysis and Generation of Emotion in Text, pages 140?146,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Hierarchical versus Flat Classification of Emotions in Text 
 
Diman Ghazi (a), Diana Inkpen (a), Stan Szpakowicz (a, b) 
(a) School of Information Technology and Engineering, University of Ottawa 
(b) Institute of Computer Science, Polish Academy of Sciences 
{dghaz038,diana,szpak}@site.uottawa.ca 
 
 
 
Abstract 
We explore the task of automatic classifica-
tion of texts by the emotions expressed. Our 
novel method arranges neutrality, polarity and 
emotions hierarchically. We test the method 
on two datasets and show that it outperforms 
the corresponding ?flat? approach, which does 
not take into account the hierarchical informa-
tion. The highly imbalanced structure of most 
of the datasets in this area, particularly the two 
datasets with which we worked, has a dramat-
ic effect on the performance of classification. 
The hierarchical approach helps alleviate the 
effect. 
1 Introduction 
Computational approaches to emotion analysis 
have focused on various emotion modalities, but 
there was only limited effort in the direction of 
automatic recognition of emotion in text (Aman, 
2007). 
Oleveres et al(1998), as one of the first works in 
emotion detection in text, uses a simple Natural 
Language Parser for keyword spotting, phrase 
length measurement and emoticon identification. 
They apply a rule-based expert system to construct 
emotion scores based on the parsed text and con-
textual information. However their simple word-
level analysis system is not sufficient when the 
emotion is expressed by more complicated phrases 
and sentences. 
More advanced systems for textual emotion recog-
nition performed sentence-level analysis.  Liu et al 
(2003), proposed an approach aimed at understand-
ing the underlying semantics of language using 
large-scale real-world commonsense knowledge to 
classify sentences into ?basic? emotion categories. 
They developed a commonsense affect model 
enabling the analysis of the affective qualities of 
text in a robust way.  
In SemEval 2007, one of the tasks was carried out 
in an unsupervised setting and the emphasis was on 
the study of emotion in lexical semantics (Strappa-
rava and Mihalcea, 2008; Chaumartin, 2007; Koza-
reva et al, 2007; Katz et al, 2007). Neviarouskaya 
et al(2009) applied a rule-based approach to affect 
recognition from a blog text. However, statistical 
and machine learning approaches have became a 
method of choice for constructing a wide variety of 
NLP applications (Wiebe et al, 2005). 
There has been previous work using statistical 
methods and supervised machine learning, includ-
ing (Aman, 2007; Katz et al, 2007; Alm, 2008; 
Wilson et al, 2009). Most of that research concen-
trated on feature selections and applying lexical 
semantics rather than on different learning 
schemes. In particular, only flat classification has 
been considered. 
According to Kiritchenko et al (2006), ?Hierar-
chical categorization deals with categorization 
problems where categories are organized in hierar-
chies?. Hierarchical text categorization places new 
items into a collection with a predefined hierar-
chical structure. The categories are partially or-
dered, usually from more generic to more specific.  
Koller and Sahami (1997) carried out the first 
proper study of a hierarchical text categorization 
problem in 1997. More work in hierarchical text 
categorization has been reported later. Keshtkar 
and Inkpen (2009) applied a hierarchical approach 
to mood classification: classifying blog posts into 
132 moods. The connection with our work is only 
indirect, because ? even though moods and emo-
tions may seem similar ? their hierarchy structure 
and the classification task are quite different. The 
work reported in (Kiritchenko et al, 2006) is more 
general. It explores two main aspects of hierarchic-
140
al text categorization: learning algorithms and per-
formance evaluation.  
In this paper, we extend our preliminary work 
(Ghazi et al, 2010) on hierarchical classification. 
Hierarchical classification is a new approach to 
emotional analysis, which considers the relation 
between neutrality, polarity and emotion of a text. 
The main idea is to arrange these categories and 
their interconnections into a hierarchy and leverage 
it in the classification process. 
We categorize sentences into six basic emotion 
classes; there also may, naturally, be no emotion in 
a sentence. The emotions are happiness, sadness, 
fear, anger, disgust, and surprise (Ekman, 1992). 
In one of the datasets we applied, we did consider 
the class non-emotional. 
For these categories, we have considered two 
forms of hierarchy for classification, with two or 
three levels. In the two-level method, we explore 
the effect of neutral instances on one dataset and 
the effect of polarity on the other dataset. In the 
three-level hierarchy, we consider neutrality and 
polarity together. 
Our experiments on data annotated with emotions 
show performance which exceeds that of the corre-
sponding flat approach. 
Section 2 of this paper gives an overview of the 
datasets and feature sets. Section 3 describes both 
hierarchical classification methods and their 
evaluation with respect to flat classification results. 
Section 4 discusses future work and presents a few 
conclusions. 
2 Data and Feature Sets 
2.1 Datasets 
The statistical methods typically require training 
and test corpora, manually annotated with respect 
to each language-processing task to be learned 
(Wiebe et al, 2005). One of the datasets in our 
experiments is a corpus of blog sentences anno-
tated with Ekman?s emotion labels (Aman, 2007). 
The second dataset is a sentence-annotated corpus 
resource divided into three parts for large-scale 
exploration of affect in children?s stories (Alm, 
2008). 
In the first dataset, each sentence is tagged by a 
dominant emotion in the sentence, or labelled as 
non-emotional. The dataset contains 173 weblog 
posts annotated by two judges. Table 1 shows the 
details of the dataset. 
In the second dataset, two annotators have anno-
tated 176 stories. The affects considered are the 
same as Ekman?s six emotions, except that the 
surprise class is subdivided into positive surprise 
and negative surprise. We run our experiments on 
only sentences with high agreement- sentences 
with the same affective labels annotated by both 
annotators. That is the version of the dataset which 
merged angry and disgusted instances and com-
bined the positive and negative surprise classes. 
The resulting dataset, therefore, has only five 
classes (Alm, 2008). Table 1 presents more details 
about the datasets, including the range of frequen-
cies for the class distribution (Min is the proportion 
of sentences with the most infrequent class, Max is 
the proportion for sentences with the most frequent 
class.) The proportion of the most frequent class 
also gives us a baseline for the accuracies of our 
classifiers (since the poorest baseline classifier 
could always choose the most frequent class).  
Table 1. Datasets specifications. 
 Domain Size # classes Min-Max% 
Aman?s 
Data set 
Weblogs 2090 7 6-38 % 
Alm?s 
Data set 
Stories 1207 5 9-36% 
2.2 Feature sets 
In (Ghazi et al, 2010), three sets of features ? one 
corpus-based and two lexically-based ? are com-
pared on Aman?s datasets. The first experiment is a 
corpus-based classification which uses unigrams 
(bag-of-words). In the second experiment, classifi-
cation was based on features derived from the 
Prior-Polarity lexicon1 (Wilson et al 2009); the 
features were the tokens common between the 
prior-polarity lexicon and the chosen dataset. In the 
last experiment, we used a combination of the 
emotional lists of words from Roget?s Thesaurus2 
(Aman and Szpakowicz, 2008) and WordNet Af-
fect3 (Strapparava and Valitutti, 2004); we call it 
the polarity feature set.  
                                                 
1 www.cs.pitt.edu/mpqa 
2 The 1987 Penguin?s Roget?s Thesaurus was used. 
3
 www.cse.unt.edu/~rada/affective 
text/data/WordNetAffectEmotioLists.tar.gz 
141
Based on the results and the discussion in (Ghazi et 
al., 2010), we decided to use the polarity feature 
set in our experiments. This feature set has certain 
advantages. It is quite a bit smaller than the uni-
gram features, and we have observed that they ap-
pear to be more meaningful. For example, the 
unigram features include (inevitably non-
emotional) names of people and countries. It is 
also possible to have misspelled tokens in uni-
grams, while the prior-polarity lexicon features are 
well-defined words usually considered as polar. 
Besides, lexical features are known to be more 
domain- and corpus-independent. Last but not 
least, our chosen feature set significantly outper-
forms the third set. 
2.3 Classification 
As a classification algorithm, we use the support 
vector machines (SVM) algorithm with tenfold 
cross-validation as a testing option. It is shown that 
SVM obtains good performance in text classifica-
tion: it scales well to the large numbers of features 
(Kennedy and Inkpen, 2006; Aman, 2007).  
We apply the same settings at each level of the 
hierarchy for our hierarchical approach classifica-
tion.  
In hierarchical categorization, categories are organ-
ized into levels (Kiritchenko et al, 2006). We use 
the hierarchical categories to put more knowledge 
into our classification method as the category hier-
archies are carefully composed manually to repre-
sent our knowledge of the subject. We will achieve 
that in two forms of hierarchy. A two-level hierar-
chy represents the relation of emotion and neutral-
ity in text, as well as the relation of positive and 
negative polarity. These two relations are exam-
ined in two different experiments, each on a sepa-
rate dataset. 
A three-level hierarchy is concerned with the rela-
tion between polarity and emotions along with the 
relation between neutrality and emotion. We as-
sume that, of Ekman's six emotions, happiness be-
longs to the positive polarity class, while the other 
five emotions have negative polarity. This is quite 
similar to the three-level hierarchy of affect labels 
used by Alm (2008). In her diagram, she considers 
happiness and positive surprise as positive, and the 
rest as negative emotions. She has not, however, 
used this model in the classification approach: 
classification experiments were only run at three 
separate affect levels. She also considers positive 
and negative surprise as one Surprise class. 
For each level of our proposed hierarchy, we run 
two sets of experiments. In the first set, we assume 
that all the instances are correctly classified at the 
preceding levels, so we only need to be concerned 
with local mistakes. Because we do not have to 
deal with instances misclassified at the previous 
level, we call these results reference results.  
In the second set of experiments, the methodology 
is different than in (Ghazi et al 2010). In that work 
both training and testing of subsequent levels is 
based on the results of preceding levels. A question 
arises, however: once we have good data available, 
why train on incorrect data which result from mis-
takes at the preceding level? That is why we de-
cided to train on correctly-labelled data and when 
testing, to compute global results by cumulating 
the mistakes from all the levels of the hierarchical 
classification. In other words, classification mis-
takes at one level of the hierarchy carry on as mis-
takes at the next levels. Therefore, we talk of 
global results because we compute the accuracy, 
precision, recall and F-measure globally, based on 
the results at all levels. These results characterize 
the hierarchical classification approach when test-
ing on new sentences: the classifiers are applied in 
a pipeline order: level 1, then level 2 on the results 
of the previous level (then level 3 if we are in the 
three-level setting).   
In the next section, we show the experiments and 
results on our chosen datasets. 
 
3 Results and discussions 
3.1 Two-level classification 
This section has two parts. The main goal of the 
first part is to find out how the presence of neutral 
instances affects the performance of features for 
distinguishing between emotional classes in 
Aman?s dataset. This was motivated by a similar 
work in polarity classification (Wilson et al, 
2009). 
In the second part, we discuss the effect of consid-
ering positive and negative polarity of emotions for 
five affect classes in Alm?s dataset. 
142
3.1.1 Neutral-Emotional 
At the first level, emotional versus non-emotional 
classification tries to determine whether an in-
stance is neutral or emotional. The second step 
takes all instances which level 1 classified as emo-
tional, and tries to classify them into one of Ek-
man's six emotions. Table 2 presents the result of 
experiments and, for comparison, the flat classifi-
cation results. A comparison of the results in both 
experiments with flat classification shows that in 
both cases the accuracy of two-level approach is 
significantly better than the accuracy of flat classi-
fication. 
One of the results worth discussing further is the 
precision of the non-emotional class: it increases 
while recall decreases. We will see the same pat-
tern in further experiments. This happens to the 
classes which used to dominate in flat classifica-
tion but they no longer dominate in hierarchical 
classification. Classifiers tends to give priority to a 
dominant class, so more instances are placed in 
this class; thus, classification achieves low preci-
sion and high recall. Hierarchical methods tend to 
produce higher precision. 
The difference between precision and recall of the 
happiness class in the flat approach and the two-
level approach cannot be ignored. It can be ex-
plained as follows: at the second level there are no 
more non-emotional instances, so the happiness 
class dominates, with 42% of all the instances. As 
explained before, this gives high recall and low 
precision for the happiness class. We hope to ad-
dress this big gap between precision and recall of 
the happiness class in the next experiments, three-
level classification. It separates happiness from the 
other five emotions, so it makes the number of in-
stances of each level more balanced. 
Our main focus is comparing hierarchical and flat 
classification, assuming all the other parameters 
are fixed. We mention, however, the best previous 
results achieved by Aman (2007) on the same data-
set. Her best result was obtained by combining 
corpus-based unigrams, features derived from 
emotional lists of words from Roget?s Thesaurus 
(explained in 2.2) and common words between the 
dataset and WordNetAffect. She also applied SVM 
with tenfold cross validation. The results appear in 
Table 3. 
     Table 3. Aman?s best results on her data set. 
 Precision Recall F-Measure 
happiness 0.813  0.698  0.751  
sadness  0.605  0.416  0.493  
fear  0.868  0.513  0.645  
surprise  0.723  0.409  0.522  
disgust  0.672  0.488  0.566  
anger  0.650  0.436  0.522  
non-emo 0.587  0.625  0.605  
 
 
 
Table 2. Two-level emotional classification on Aman?s dataset (the highest precision, recall, and F-measure val-
ues for each class are shown in bold). The results of the flat classification are repeated for convenience. 
Two-level classification Flat classification 
 Precision Recall F-measure Precision Recall F-measure 
1st level emo non-emo 
0.88 
0.88 
0.85 
0.81 
0.86 
0.84 
-- 
0.54 
-- 
0.87 
-- 
0.67 
2nd level 
reference results 
 
happiness 
sadness 
fear 
surprise 
disgust 
anger 
0.59 
0.77 
0.91 
0.75 
0.66 
0.72 
0.95 
0.49 
0.49 
0.32 
0.35 
0.33 
0.71 
0.60 
0.63 
0.45 
0.45 
0.46 
0.74 
0.69 
0.82 
0.64 
0.68 
0.67 
0.60 
0.42 
0.49 
0.27 
0.31 
0.26 
0.66 
0.52 
0.62 
0.38 
0.43 
0.38 
Accuracy   68.32%   61.67%  
2-level experi-
ment 
global results 
non-emo 
happiness 
sadness 
fear 
surprise 
disgust 
anger 
0.88 
0.56 
0.64 
0.75 
0.56 
0.52 
0.55 
0.81 
0.86 
0.42 
0.43 
0.29 
0.29 
0.27 
0.84 
0.68 
0.51 
0.55 
0.38 
0.37 
0.36 
0.54 
0.74 
0.69 
0.82 
0.64 
0.68 
0.67
0.87 
0.60 
0.42 
0.49 
0.27 
0.31 
0.26 
0.67 
0.66 
0.52 
0.62 
0.38 
0.43 
0.38 
Accuracy   65.50%   61.67%  
 
143
 By comparing the reference results in Table 2 with 
Aman?s result shown in Table 3, our results on two  
classes, non-emo and sadness are significantly bet-
ter. Even though recall of our experiments is high-
er for happiness class, the precision makes the F-
measure to be lower. The reason behind the differ-
ence between the precisions is the same as their 
difference between in our hierarchical and flat 
comparisons. As it was also mentioned there we 
hope to address this problem in three-level classifi-
cation. Both precision and recall of the sadness in 
our experiments is higher than Aman?s results. We 
have a higher precision for fear, but recall is 
slightly lower. For the last three classes our preci-
sion is higher while recall is significantly lower.  
 
The size of these three classes, which are the smal-
lest classes in the dataset, appears to be the reason. 
It is possible that the small set of features that we 
are using will recall fewer instances of these 
classes comparing to the bigger feature sets used 
by Aman (2007).  
3.1.2 Negative-Positive polarity 
These experiments have been run on Alm?s dataset 
with five emotion classes. This part is based on the 
assumption that the happiness class is positive and 
the remaining four classes are negative.  
 
 
At the first level, positive versus negative classifi-
cation tries to determine whether an instance bears 
a positive emotion. The second step takes all in-
stances which level 1 classified as negative, and 
tries to classify them into one of the four negative 
classes, namely sadness, fear, surprise and anger-
disgust. The results show a higher accuracy in ref-
erence results while it is slightly lower for global 
results. In terms of precision and recall, however, 
there is a high increase in precision of positive 
(happiness) class while the recall decreases. 
The results show a higher accuracy in reference 
results while it is slightly lower for global results. 
In terms of precision and recall, however, there is a 
high increase in precision of positive (happiness) 
class while the recall decreases. 
We also see a higher F-measure for all classes in 
the reference results. That confirms the consistency 
between the result in Table 2 and Table 4. 
In the global measurements, recall is higher for all 
the classes at the second level, but the F-measure is 
higher only for three classes. 
Here we cannot compare our results with the best 
previous results achieved by Alm (2008), because 
the datasets and the experiments are not the same. 
She reports the accuracy of the classification re-
sults for three sub-corpora separately. She random-
ly selected neutral instances from the annotated 
data and added them to the dataset, which makes it 
Table 4. Two-level emotional classification on Alm?s dataset (the highest precision, recall, and F-measure val-
ues for each class are shown in bold). 
Two-level classification Flat classification 
 Precision Recall F-measure Precision Recall F-measure 
1st level neg pos 
0.81 
0.84 
0.93 
0.64 
0.87 
0.72 
-- 
0.56 
-- 
0.86 
-- 
0.68 
2nd level 
reference results 
 
sadness 
fear 
surprise 
anger 
0.65 
0.59 
0.45 
0.49 
0.68 
0.40 
0.21 
0.73 
0.66 
0.47 
0.29 
0.59 
0.67 
0.59 
0.35 
0.54 
0.53 
0.38 
0.10 
0.43 
0.59 
0.46 
0.16 
0.48 
Accuracy   59.07%   57.41%  
2-level experiment 
global results 
happiness 
sadness 
fear 
surprise 
anger 
0.84 
0.55 
0.45 
0.27 
0.43 
0.64 
0.61 
0.39 
0.21 
0.68 
0.72 
0.58 
0.42 
0.19 
0.53 
0.56 
0.67 
0.59 
0.35 
0.54 
0.86 
0.53 
0.38 
0.10 
0.43 
0.68 
0.59 
0.46 
0.16 
0.48 
Accuracy   56.57%   57.41%  
 
144
different than the data set we used in our experi-
ments.  
3.2 Three-level classification 
In this approach, we go even further: we break the 
seven-class classification task into three levels. 
The first level defines whether the instance is emo-
tional. At the second level the instances defined as 
emotional by the first level will be classified on 
their polarity. At the third level, we assume that the 
instances of happiness class have positive polarity 
and the other five emotions negative polarity. That 
is why we take the negative instances from the 
second level and classify them into the five nega-
tive emotion classes. Table 5 presents the results of  
this classification. The results show that the accu-
racy of both reference results and global results are 
higher than flat classification, but the accuracy of 
the global results is not significantly better. 
At the first and second level, the F-measure of no-
emotion and happiness classes is significantly bet-
ter. At the third level, except in the class disgust, 
we see an increase in the F-measure of all classes 
in comparison with both the two-level and flat 
classification. 
 
Table 5. Three-level emotional classification on Aman?s data-
set (the highest precision, recall, and F-measure values for 
each class are shown in bold) 
?
Three-level Classification 
 Precision Recall F 
1st level emo 
non-emo 
0.88 
0.88 
0.85 
0.81 
0.86 
0.84 
2nd level 
reference results 
positive 
negative 
0.89 
0.79 
0.65 
0.94 
0.75 
0.86 
3rd level 
reference results 
 
 
sadness 
fear 
surprise 
disgust 
anger 
0.63 
0.88 
0.79 
0.42 
0.38 
0.54 
0.52 
0.37 
0.38 
0.71 
0.59 
0.65 
0.50 
0.40 
0.49 
Accuracy   65.5%  
 
3-level experi-
ment 
global results 
non-emo 
happiness 
sadness 
fear 
surprise 
disgust 
anger 
0.88 
0.77 
0.43 
0.52 
0.46 
0.31 
0.35 
0.81 
0.62 
0.49 
0.4 
0.32 
0.31 
0.55 
0.84 
0.69 
0.46 
0.45 
0.38 
0.31 
0.43 
Accuracy   62.2%  
Also, as shown by the two-level experiments, the 
results of the second level of the reference results 
approach an increase in the precision of the happi-
ness class. That makes the instances defined as 
happiness more precise. 
By comparing the results with Table 3, which is 
the best previous results, we see an increase in the 
precision of happiness class and its F-measure 
consequently; therefore in these results we get a 
higher F-measure for three classes, non-emo, sad-
ness and fear. We get the same F-measure for hap-
piness and slightly lower F-measure for surprise 
but we still have a lower F-measure for the other 
two classes, namely, disgust and anger. The other 
difference is the high increase in the recall value 
for fear. 
4 Conclusions and Future Work 
The focus of this study was a comparison of the 
hierarchical and flat classification approaches to 
emotional analysis and classification. In the emo-
tional classification we noticed that having a 
dominant class in the dataset degrades the results 
significantly. A classifier trained on imbalanced 
data gives biased results for the classes with more 
instances. Our results, based on a novel method, 
shows that the hierarchical classification approach 
is better at dealing with the highly imbalanced 
data. We also saw a considerable improvement in 
the classification results when we did not deal with 
the errors from previous steps and slightly better 
results when we evaluated the results globally. 
In the future, we will consider different levels of 
our hierarchy as different tasks which could be 
handled differently. Each of the tasks has its own 
specification. We can, therefore, definitely benefit 
from analyzing each task separately and defining 
different sets of features and classification methods 
for each task rather than using the same method for 
every task. 
145
References 
Alm, C.: ?Affect in text and speech?, PhD disserta-
tion, University of Illinois at Urbana-
Champaign, Department of Linguistics (2008) 
Aman, S.: ?Identifying Expressions of Emotion in 
Text?, Master's thesis, University of Ottawa, Ot-
tawa, Canada (2007) 
Aman, S., Szpakowicz, S.: ?Using Roget?s Thesau-
rus for Fine-grained Emotion Recognition?. 
Proc. Conf. on Natural Language Processing 
(IJCNLP), Hyderabad, India, 296-302 (2008) 
Chaumartin. F.: ?Upar7: A knowledge-based sys-
tem for headline sentiment tagging?, Proc. Se-
mEval-2007, Prague, Czech Republic, June 
(2007) 
Ekman, P.: ?An Argument for Basic Emotions?, 
Cognition and Emotion, 6, 169-200 (1992) 
Ghazi, D., Inkpen, D., Szpakowicz, S.: ?Hierar-
chical approach to emotion recognition and clas-
sification in texts?, A. Farzindar and V. Keselj 
(eds.), Proc. 23rd Canadian Conference on Ar-
tificial Intelligence, Ottawa, ON.  Lecture Notes 
in Artificial Intelligence 6085, Springer Verlag,  
40?50 (2010) 
Katz, P., Singleton, M., Wicentowski, R.: ?Swat-
mp: the semeval-2007 systems for task 5 and 
task 14?, Proc. SemEval-2007, Prague, Czech 
Republic, June (2007) 
Kennedy, A., Inkpen, D.: ?Sentiment classification 
of movie reviews using contextual valence shif-
ter?, Computational Intelligence 22. 110-125 
(2006) 
Keshtkar, F., Inkpen, D.: ?Using Sentiment Orien-
tation Features for Mood Classification in Blog 
Corpus?, IEEE International Conf. on NLP and 
KE, Dalian, China, Sep. 24-27 (2009) 
Kiritchenko, S., Matwin, S., Nock, R., Famili, 
F.: ?Learning and Evaluation in the Presence of 
Class Hierarchies: Application to Text Categori-
zation?, Lecture Notes in Artificial Intelligence 
4013, Springer, 395-406 (2006) 
Koller, D., Sahami, M.: ?Hierarchically Classify-
ing Documents Using Very Few Words?. Proc. 
International Conference on Machine Learning, 
170-178 (1997) 
Kozareva, Z., Navarro, B., Vazquez, S., Montoyo, 
A.: ?UA-ZBSA: A headline emotion classifica-
tion through web information?, Proc. SemEval-
2007, Prague, Czech Republic, June (2007) 
Liu, H., Lieberman, H., Selker, T.: ?A Model of 
Textual Affect Sensing using Real-World 
Knowledge?. In Proc. IUI 2003, 125-132 (2003) 
Neviarouskaya, A., Prendinger, H., and Ishizuka, 
M.: ?Compositionality Principle in Recognition 
of Fine-Grained Emotions from Text?, In: Pro-
ceedings of Third International Conference on 
Weblogs and Social Media (ICWSM?09), 
AAAI, San Jose, California, US, 278-281 (2009) 
Olveres, J., Billinghurst, M., Savage, J., Holden, 
A.: ?Intelligent, Expressive Avatars?. In Proc. of 
the WECC?98, 47-55 (1998) 
 Strapparava, C., Mihalcea, R.: ?SemEval-2007 
Task 14: Affective Text? (2007) 
Strapparava, C., Mihalcea, R.: ?Learning to Identi-
fy Emotions in Text?, Proc. ACM Symposium 
on Applied computing, Fortaleza, Brazil, 1556-
1560 (2008) 
Wilson, T., Wiebe, J., Hoffmann, P.: ?Recognizing 
contextual polarity: an exploration of features 
for phrase-level sentiment analysis?, Computa-
tional Linguistics 35(3), 399-433 (2009) 
 Wiebe, J., Wilson, T., Cardie, C.: ?Annotating 
Expressions of Opinions and Emotions in Lan-
guage?, Language Resources and Evaluation 39, 
165-210 (2005) 
146
Proceedings of the 3rd Workshop on Computational Approaches to Subjectivity and Sentiment Analysis, pages 70?78,
Jeju, Republic of Korea, 12 July 2012. c?2012 Association for Computational Linguistics
Prior versus Contextual Emotion of a Word in a Sentence
Diman Ghazi
EECS, University of Ottawa
dghaz038@uottawa.ca
Diana Inkpen
EECS, University of Ottawa
diana@eecs.uottawa.ca
Stan Szpakowicz
EECS, University of Ottawa &
ICS, Polish Academy of Sciences
szpak@eecs.uottawa.ca
Abstract
A set of words labelled with their prior emo-
tion is an obvious place to start on the auto-
matic discovery of the emotion of a sentence,
but it is clear that context must also be con-
sidered. No simple function of the labels on
the individual words may capture the overall
emotion of the sentence; words are interre-
lated and they mutually influence their affect-
related interpretation. We present a method
which enables us to take the contextual emo-
tion of a word and the syntactic structure of the
sentence into account to classify sentences by
emotion classes. We show that this promising
method outperforms both a method based on
a Bag-of-Words representation and a system
based only on the prior emotions of words.
The goal of this work is to distinguish auto-
matically between prior and contextual emo-
tion, with a focus on exploring features impor-
tant for this task.
1 Introduction
Recognition, interpretation and representation of af-
fect have been investigated by researchers in the
field of affective computing (Picard 1997). They
consider a wide range of modalities such as affect in
speech, facial display, posture and physiological ac-
tivity. It is only recently that there has been a grow-
ing interest in automatic identification and extraction
of sentiment, opinions and emotions in text.
Sentiment analysis is the task of identifying posi-
tive and negative opinions, emotions and evaluations
(Wilson, Wiebe, and Hoffmann, 2005). Most of the
current work in sentiment analysis has focused on
determining the presence of sentiment in the given
text, and on determining its polarity ? the positive or
negative orientation. The applications of sentiment
analysis range from classifying positive and nega-
tive movie reviews (Pang, Lee, and Vaithyanathan,
2002; Turney, 2002) to opinion question-answering
(Yu and Hatzivassiloglou, 2003; Stoyanov, Cardie,
and Wiebe, 2005). The analysis of sentiment must,
however, go beyond differentiating positive from
negative emotions to give a systematic account of
the qualitative differences among individual emo-
tion (Ortony, Collins, and Clore, 1988).
In this work, we deal with assigning fine-grained
emotion classes to sentences in text. It might seem
that these two tasks are strongly tied, but the higher
level of classification in emotion recognition task
and the presence of certain degrees of similarities
between some emotion labels make categorization
into distinct emotion classes more challenging and
difficult. Particularly notable in this regard are two
classes, anger and disgust, which human annotators
often find hard to distinguish (Aman and Szpakow-
icz, 2007). In order to recognize and analyze affect
in written text ? seldom explicitly marked for emo-
tions ? NLP researchers have come up with a variety
of techniques, including the use of machine learn-
ing, rule-based methods and the lexical approach
(Neviarouskaya, Prendinger, and Ishizuka, 2011).
There has been previous work using statistical
methods and supervised machine learning applied to
corpus-based features, mainly unigrams, combined
with lexical features (Alm, Roth, and Sproat, 2005;
Aman and Szpakowicz, 2007; Katz, Singleton, and
Wicentowski, 2007). The weakness of such methods
70
is that they neglect negation, syntactic relations and
semantic dependencies. They also require large (an-
notated) corpora for meaningful statistics and good
performance. Processing may take time, and anno-
tation effort is inevitably high. Rule-based meth-
ods (Chaumartin, 2007; Neviarouskaya, Prendinger,
and Ishizuka, 2011) require manual creation of rules.
That is an expensive process with weak guaran-
tee of consistency and coverage, and likely very
task-dependent; the set of rules of rule-based af-
fect analysis task (Neviarouskaya, Prendinger, and
Ishizuka, 2011) can differ drastically from what un-
derlies other tasks such as rule-based part-of-speech
tagger, discourse parsers, word sense disambigua-
tion and machine translation.
The study of emotions in lexical semantics was
the theme of a SemEval 2007 task (Strapparava and
Mihalcea, 2007), carried out in an unsupervised set-
ting (Strapparava and Mihalcea, 2008; Chaumartin,
2007; Kozareva et al, 2007; Katz, Singleton, and
Wicentowski, 2007). The participants were encour-
aged to work with WordNet-Affect (Strapparava and
Valitutti, 2004) and SentiWordNet (Esuli and Sebas-
tiani, 2006). Word-level analysis, however, will not
suffice when affect is expressed by phrases which re-
quire complex phrase- and sentence-level analyses:
words are interrelated and they mutually influence
their affect-related interpretation. On the other hand,
words can have more than one sense, and they can
only be disambiguated in context. Consequently, the
emotion conveyed by a word in a sentence can differ
drastically from the emotion of the word on its own.
For example, according to the WordNet-Affect lex-
icon, the word ?afraid? is listed in the ?fear? cate-
gory, but in the sentence ?I am afraid it is going to
rain.? the word ?afraid? does not convey fear.
We refer to the emotion listed for a word in an
emotion lexicon as the word?s prior emotion. A
word?s contextual emotion is the emotion of the sen-
tence in which that word appears, taking the context
into account.
Our method combines several way of tackling the
problem. First, we find keywords listed in WordNet-
Affect and select the sentences which include emo-
tional words from that lexicon. Next, we study the
syntactic structure and semantic relations in the text
surrounding the emotional word. We explore fea-
tures important in emotion recognition, and we con-
happi- sad- anger dis- sur- fear total
ness ness gust prise
398 201 252 53 71 141 1116
Table 1: The distribution of labels in the WordNet-
Affect Lexicon.
sider their effect on the emotion expressed by the
sentence. Finally, we use machine learning to clas-
sify the sentences, represented by the chosen fea-
tures, by their contextual emotion.
We categorize sentences into six basic emotions
defined by Ekman (1992); that has been the choice
of most of previous related work. These emotions
are happiness, sadness, fear, anger, disgust and sur-
prise. There also may, naturally, be no emotion in a
sentence; that is tagged as neutral/non-emotional.
We evaluate our results by comparing our method
applied to our set of features with Support Vec-
tor Machine (SVM) applied to Bag-of-Words, which
was found to give the best performance among su-
pervised methods (Yang and Liu, 1999; Pang, Lee,
and Vaithyanathan, 2002; Aman and Szpakowicz,
2007; Ghazi, Inkpen, and Szpakowicz, 2010). We
show that our method is promising and that it out-
performs both a system which works only with prior
emotions of words, ignoring context, and a system
which applies SVM to Bag-of-Words.
Section 2 of this paper describes the dataset and
resources used. Section 3 discusses the features
which we use for recognizing contextual emotion.
Experiments and results are presented in Section 4.
In Section 5, we conclude and discuss future work.
2 Dataset and Resources
Supervised statistical methods typically require
training data and test data, manually annotated
with respect to each language-processing task to be
learned. In this section, we explain the dataset and
lexicons used in our experiments.
WordNet-Affect Lexicon (Strapparava and Vali-
tutti, 2004). The first resource we require is an
emotional lexicon, a set of words which indicate
the presence of a particular emotion. In our exper-
iments, we use WordNet-Affect, which contains six
lists of words corresponding to the six basic emo-
tion categories. It is the result of assigning a variety
71
Neutral Negative Positive Both
6.9% 59.7% 31.1% 0.3%
Table 2: The distribution of labels in the Prior-Polarity
Lexicon.
of affect labels to each synset in WordNet. Table 1
shows the distribution of words in WordNet-Affect.
Prior-Polarity Lexicon (Wilson, Wiebe, and
Hoffmann, 2009). The prior-polarity subjectivity
lexicon contains over 8000 subjectivity clues col-
lected from a number of sources. To create this
lexicon, the authors began with the list of subjec-
tivity clues extracted by Riloff (2003). The list
was expanded using a dictionary and a thesaurus,
and adding positive and negative word lists from
the General Inquirer.1 Words are grouped into
strong subjective and weak subjective clues; Table 2
presents the distribution of their polarity.
Intensifier Lexicon (Neviarouskaya, Prendinger,
and Ishizuka, 2010). It is a list of 112 modifiers (ad-
verbs). Two annotators gave coefficients for inten-
sity degree ? strengthening or weakening, from 0.0
to 2.0 ? and the result was averaged.
Emotion Dataset (Aman and Szpakowicz,
2007). The main consideration in the selection of
data for emotional classification task is that the data
should be rich in emotion expressions. That is why
we chose for our experiments a corpus of blog sen-
tences annotated with emotion labels, discussed by
Aman and Szpakowicz (2007). Each sentence is
tagged by its dominant emotion, or as non-emotional
if it does not include any emotion. The annotation is
based on Ekman?s six emotions at the sentence level.
The dataset contains 4090 annotated sentences, 68%
of which were marked as non-emotional. The highly
unbalanced dataset with non-emotional sentences as
by far the largest class, and merely 3% in the fear
and surprise classes, prompted us to remove 2000 of
the non-emotional sentences. We lowered the num-
ber of non-emotional sentences to 38% of all the
sentences, and thus reduced the imbalance. Table 3
shows the details of the chosen dataset.
1www.wjh.harvard.edu/?inquirer/
hp sd ag dg sr fr ne total
536 173 179 172 115 115 800 2090
Table 3: The distribution of labels in Aman?s modified
dataset. The labels are happiness, sadness, anger, dis-
gust, surprise, fear, no emotion.
3 Features
The features used in our experiments were motivated
both by the literature (Wilson, Wiebe, and Hoff-
mann, 2009; Choi et al, 2005) and by the explo-
ration of contextual emotion of words in the anno-
tated data. All of the features are counted based on
the emotional word from the lexicon which occurs in
the sentence. For ease of description, we group the
features into four distinct sets: emotion-word fea-
tures, part-of-speech features, sentence features and
dependency-tree features.
Emotion-word features. This set of features are
based on the emotion-word itself.
? The emotion of a word according to WordNet-
Affect (Strapparava and Valitutti, 2004).
? The polarity of a word according to the prior-
polarity lexicon (Wilson, Wiebe, and Hoff-
mann, 2009).
? The presence of a word in a small list of modi-
fiers (Neviarouskaya, Prendinger, and Ishizuka,
2010).
Part-of-speech features. Based on the Stanford
tagger?s output (Toutanova et al, 2003), every word
in a sentence gets one of the Penn Treebank tags.
? The part-of-speech of the emotional word it-
self, both according to the emotion lexicon and
Stanford tagger.
? The POS of neighbouring words in the same
sentence. We choose a window of [-2,2], as it
is usually suggested by the literature (Choi et
al., 2005).
Sentence features. For now we only consider the
number of words in the sentence.
Dependency-tree features. For each emotional
word, we create features based on the parse tree and
its dependencies produced by the Stanford parser
(Marneffe, Maccartney, and Manning, 2006). The
72
dependencies are all binary relations: a grammati-
cal relation holds between a governor (head) and a
dependent (modifier).
According to Mohammad and Turney (2010),2
adverbs and adjectives are some of the most
emotion-inspiring terms. This is not surprising con-
sidering that they are used to qualify a noun or a
verb; therefore to keep the number of features small,
among all the 52 different type of dependencies, we
only chose the negation, adverb and adjective modi-
fier dependencies.
After parsing the sentence and getting the de-
pendencies, we count the following dependency-tree
Boolean features for the emotional word.
? Whether the word is in a ?neg? dependency
(negation modifier): true when there is a nega-
tion word which modifies the emotional word.
? Whether the word is in a ?amod? dependency
(adjectival modifier): true if the emotional
word is (i) a noun modified by an adjective or
(ii) an adjective modifying a noun.
? Whether the word is in a ?advmod? depen-
dency (adverbial modifier): true if the emo-
tional word (i) is a non-clausal adverb or adver-
bial phrase which serves to modify the meaning
of a word, or (ii) has been modified by an ad-
verb.
We also have several modification features based
on the dependency tree. These Boolean features cap-
ture different types of relationships involving the cue
word.3 We list the feature name and the condition on
the cue word w which makes the feature true.
? Modifies-positive: w modifies a positive word
from the prior-polarity lexicon.
? Modifies-negative: w modifies a negative word
from the prior-polarity lexicon.
? Modified-by-positive: w is the head of the de-
pendency, which is modified by a positive word
from the prior-polarity lexicon.
? Modified-by-negative: w is the head of the
dependency, which is modified by a negative
word from the prior-polarity lexicon.
2In their paper, they also explain how they created an emo-
tion lexicon by crowd-sourcing, but ? to the best of our knowl-
edge ? it is not publicly available yet.
3The terms ?emotional word? and ?cue word? are used in-
terchangeably.
hp sd ag dg sr fr ne total
part 1 196 64 64 63 36 52 150 625
part 2 51 18 22 18 9 14 26 158
part 1+ 247 82 86 81 45 66 176 783
part 2
Table 4: The distribution of labels in the portions of
Aman?s dataset used in our experiments, named part 1,
part 2 and part 1+part 2. The labels are happiness, sad-
ness, anger, disgust, surprise, fear, no emotion.
? Modifies-intensifier-strengthen: w modifies a
strengthening intensifier from the intensifier
lexicon.
? Modifies-intensifier-weaken: w modifies a
weakening intensifier from the intensifier lex-
icon.
? Modified-by-intensifier-strengthen: w is the
head of the dependency, which is modified by
a strengthening intensifier from the intensifier
lexicon.
? Modified-by-intensifier-weaken: w is the head
of the dependency, which is modified by a
weakening intensifier from the intensifiers lex-
icon.
4 Experiments
In the experiments, we use the emotion dataset pre-
sented in Section 2. Our main consideration is to
classify a sentence based on the contextual emotion
of the words (known as emotional in the lexicon).
That is why in the dataset we only choose sentences
which contain at least one emotional word accord-
ing to WordNet-Affect. As a result, the number of
sentences chosen from the dataset will decrease to
783 sentences, 625 of which contain only one emo-
tional word and 158 sentences which contain more
than one emotional word. Their details are shown in
Table 4.
Next, we represent the data with the features pre-
sented in Section 3. Those features, however, were
defined for each emotional word based on their con-
text, so we will proceed differently for sentences
with one emotional word and sentences with more
than one emotional word.
? In sentences with one emotional word, we as-
sume the contextual emotion of the emotional
73
word is the same as the emotion assigned to the
sentence by the human annotators; therefore all
the 625 sentences with one emotional word are
represented with the set of features presented
in Section 3 and the sentence?s emotion will be
considered as their contextual emotion.
? For sentences with more than one emotional
word, the emotion of the sentence depends on
all emotional words and their syntactic and se-
mantic relations. We have 158 sentences where
no emotion can be assigned to the contextual
emotion of their emotional words, and all we
know is the dominant emotion of the sentence.
We will, therefore, have two different sets of ex-
periments. For the first set of sentences, the data are
all annotated, so we will take a supervised approach.
For the second set of sentences, we combine super-
vised and unsupervised learning. We train a clas-
sifier on the first set of data and we use the model
to classify the emotional words into their contextual
emotion in the second set of data. Finally, we pro-
pose an unsupervised method to combine the con-
textual emotion of all the emotional words in a sen-
tence and calculate the emotion of the sentence.
For evaluation, we report precision, recall, F-
measure and accuracy to compare the results. We
also define two baselines for each set of experiments
to compare our results with. The experiments are
presented in the next two subsections.
4.1 Experiments on sentences with one
emotional word
In these experiments, we explain first the baselines
and then the results of our experiments on the sen-
tences with only one emotional word.
Baseline
We develop two baseline systems to assess the dif-
ficulty of our task. The first baseline labels the sen-
tences the same as the most frequent class?s emo-
tion, which is a typical baseline in machine learning
tasks (Aman and Szpakowicz, 2007; Alm, Roth, and
Sproat, 2005). This baseline will result in 31% ac-
curacy.
The second baseline labels the emotion of the sen-
tence the same as the prior emotion of the only emo-
tional word in the sentence. The accuracy of this
Precision Recall F
SVM +
Bag-of-
Words
Happiness 0.59 0.67 0.63
Sadness 0.38 0.45 0.41
Anger 0.40 0.31 0.35
Surprise 0.41 0.33 0.37
Disgust 0.51 0.43 0.47
Fear 0.55 0.50 0.52
Non-emo 0.49 0.48 0.48
Accuracy 50.72%
SVM
+ our
features
Happiness 0.68 0.78 0.73
Sadness 0.49 0.58 0.53
Anger 0.66 0.48 0.56
Surprise 0.61 0.31 0.41
Disgust 0.43 0.38 0.40
Fear 0.67 0.63 0.65
Non-emo 0.51 0.53 0.52
Accuracy 58.88%
Logistic
Regres-
sion + our
features
Happiness 0.78 0.82 0.80
Sadness 0.53 0.64 0.58
Anger 0.69 0.62 0.66
Surprise 0.89 0.47 0.62
Disgust 0.81 0.41 0.55
Fear 0.71 0.71 0.71
Non-emo 0.53 0.64 0.58
Accuracy 66.88%
Table 5: Classification experiments on the dataset with
one emotional word in each sentence. Each experiment
is marked by the method and the feature set.
experiment is 51%, remarkably higher than the first
baseline?s accuracy. The second baseline is particu-
larly designed to address the emotion of the sentence
only based on the prior emotion of the emotional
words; therefore it will allow us to assess the dif-
ference between the emotion of the sentence based
on the prior emotion of the words in the sentence
versus the case when we consider the context and its
effect on the emotion of the sentence.
Learning Experiments
In this part, we use two classification algorithms,
Support Vector Machines (SVM) and Logistic Re-
gression (LR), and two different set of features,
the set of features from Section 3 and Bag-of-
Words (unigram). Unigram models have been
widely used in text classification and shown to pro-
vide good results in sentiment classification tasks.
In general, SVM has long been a method of
choice for sentiment recognition in text. SVM has
74
been shown to give good performance in text clas-
sification experiments as it scales well to the large
numbers of features (Yang and Liu, 1999; Pang, Lee,
and Vaithyanathan, 2002; Aman and Szpakowicz,
2007). For the classification, we use the SMO al-
gorithm (Platt, 1998) from Weka (Hall et al, 2009),
setting 10-fold cross validation as a testing option.
We compare applying SMO to two sets of features,
(i) Bag-of-Words, which are binary features defin-
ing whether a unigram exists in a sentence and (ii)
our set of features. In our experiments we use uni-
grams from the corpus, selected using feature selec-
tion methods from Weka.
We also compare those two results with the third
experiment: apply SimpleLogistic (Sumner, Frank,
and Hall, 2005) from Weka to our set of features,
again setting 10-fold cross validation as a testing op-
tion. Logistic regression is a discriminative prob-
abilistic classification model which operates over
real-valued vector inputs. It is relatively slow to train
compared to the other classifiers. It also requires ex-
tensive tuning in the form of feature selection and
implementation to achieve state-of-the-art classifica-
tion performance. Logistic regression models with
large numbers of features and limited amounts of
training data are highly prone to over-fitting (Alias-
i, 2008). Besides, logistic regression is really slow
and it is known to only work on data represented
by a small set of features. That is why we do not
apply SimpleLogistic to Bag-of-Words features. On
the other hand, the number of our features is rela-
tively low, so we find logistic regression to be a good
choice of classifier for our representation method.
The classification results are shown in Table 5.
We note consistent improvement. The results of
both experiments using our set of features signifi-
cantly outperform (on the basis of a paired t-test,
p=0.005) both the baselines and SVM applied to
Bag-of-Words features. We get the best result, how-
ever, by applying logistic regression to our feature
set. The number of our features and the nature of
the features we introduce make them an appropriate
choice of data representation for logistic regression
methods.
4.2 Experiments on sentences with more than
one emotional word
In these experiments, we combine supervised and
unsupervised learning. We train a classifier on the
first set of data, which is annotated, and we use the
model to classify the emotional words in the sec-
ond group of sentences. We propose an unsuper-
vised method to combine the contextual emotion of
the emotional words and calculate the emotion of the
sentence.
Baseline
We develop two baseline systems. The first base-
line labels all the sentences the same: as the emo-
tion of the most frequent class, giving 32% accu-
racy. The second baseline labels the emotion of the
sentence the same as the most frequently occurring
prior-emotion of the emotional words in the sen-
tence. In the case of a tie, we randomly pick one
of the emotions. The accuracy of this experiment
is 45%. Again, as a second baseline we choose a
baseline that is based on the prior emotion of the
emotional words so that we can compare it with the
results based on contextual emotion of the emotional
words in the sentence.
Learning Experiments
For sentences with more than one emotional
word, we represent each emotional word and its con-
text by the set of features explained in section 3. We
do not have the contextual emotion label for each
emotional word, so we cannot train the classifier on
these data. Consequently, we train the classifier on
the part of the dataset which only includes sentences
with one emotional word. In these sentences, each
emotional word is labeled with their contextual emo-
tion ? the same as the sentence?s emotion.
Once we have the classifier model, we get the
probability distribution of emotional classes for each
emotional word (calculated by the logistic regres-
sion function learned from the annotated data). We
add up the probabilities of each class for all emo-
tional words. Finally, we select the class with the
maximum probability. The result, shown in Table 6,
is compared using supervised learning, SVM, with
Bag-of-Words features, explained in previous sec-
tion, with setting 10-fold cross validation as a testing
75
Precision Recall F
SVM +
Bag-of-
Words
Happiness 0.52 0.60 0.54
Sadness 0.35 0.33 0.34
Anger 0.30 0.27 0.29
Surprise 0.14 0.11 0.12
Disgust 0.30 0.17 0.21
Fear 0.44 0.29 0.35
Non-emo 0.23 0.35 0.28
Accuracy 36.71%
Logistic
Regres-
sion +
unsu-
pervised
+ our
features
Happiness 0.63 0.71 0.67
Sadness 0.67 0.44 0.53
Anger 0.50 0.41 0.45
Surprise 1.00 0.22 0.36
Disgust 0.80 0.22 0.34
Fear 0.60 0.64 0.62
Non-emo 0.37 0.69 0.48
Accuracy 54.43%
Table 6: Classification experiments on the dataset with
more than one emotional word in each sentence. Each
experiment is marked by the method and the feature set.
option.4
By comparing the results in Table 6, we can see
that the result of learning applied to our set of fea-
tures significantly outperforms (on the basis of a
paired t-test, p=0.005) both baselines and the result
of SVM algorithm applied to Bag-of-Words features.
4.3 Discussion
We cannot directly compare our results with the pre-
vious results achieved by Aman and Szpakowicz
(2007), because the datasets differ. F-measure, pre-
cision and recall for each class are reported on the
whole dataset, but we only used part of that dataset.
To show how hard this task is, and to see where we
stand, the best result from (Aman and Szpakowicz,
2007) is shown in Table 7.
In our experiments, we showed that our approach
and our features significantly outperform the base-
lines and the SVM result applied to Bag-of-Words.
For the final conclusion, we add one more compar-
ison. As we can see from Table 6, the accuracy
result of applying SVM to Bag-of-Words is really
low. Because supervised methods scale well on large
datasets, one reason could be the size of the data we
use in this experiment; therefore we try to compare
4Since SVM does not return a distribution probability, we
cannot apply SVM to our features in this set of experiments.
Precision Recall F
Happiness 0.813 0.698 0.751
Sadness 0.605 0.416 0.493
Anger 0.650 0.436 0.522
Surprise 0.723 0.409 0.522
Disgust 0.672 0.488 0.566
Fear 0.868 0.513 0.645
Non-emo 0.587 0.625 0.605
Table 7: Aman?s best result on the dataset explained in
Section 2.
the results of the two experiments on all 758 sen-
tences with at least one emotional word.
For this comparison, we apply SVM with Bag-of-
Words features to all of 758 sentences and we get
an accuracy of 55.17%. Considering our features
and methodology, we cannot apply logistic regres-
sion with our features to the whole dataset; therefore
we calculate its accuracy by counting the percent-
age of correctly classified instances in both parts of
the dataset, used in the two experiments, and we get
an accuracy of 64.36%. We also compare the re-
sults with the baselines. The first baseline, which
is the percentage of most frequent class (happiness
in this case), results in 31.5% accuracy. The second
baseline based on the prior emotion of the emotional
words results in 50.13% accuracy. It is notable that
the result of applying LR to our set of features is
still significantly better than the result of applying
SVM to Bag-of-Words and both baselines; this sup-
ports our earlier conclusion. It is hard to compare
the results mentioned thus far, so we have combined
all the results in Figure 1, which displays the accu-
racy obtained by each experiment.
We also looked into our results and assessed the
cases where the contextual emotion is different from
the prior emotion of the emotional word. Consider
the sentence ?Joe said it does not happen that often
so it does not bother him.? Based on the emotion
lexicon, the word ?bother? is classified as angry; so
is the emotion of the sentence if we only consider
the prior emotion of words. In our set of features,
however, we consider the negation in the sentence,
so the sentence is classified as non-emotional rather
than angry. Another interesting sentence is the rather
simple ?You look like her I guess.? Based on the lex-
icon, the word ?like? is in the happy category, while
76
Figure 1: The comparison of accuracy results of all ex-
periments for sentences with one emotional word (part
1), sentences with more than one emotional words (part
2), and sentences with at least one emotional word (part
1+part 2).
the sentence is non-emotional. In this case, the part-
of-speech features play an important role and they
catch the fact that ?like" is not a verb here; it does
not convey a happy emotion and the sentence is clas-
sified as non-emotional.
We also analyzed the errors, and we found some
common errors due to:
? complex sentences or unstructured sentences
which will cause the parser to fail or return in-
correct data, resulting in incorrect dependency-
tree information;
? limited coverage of the emotion lexicon.
These are some of the issues which we would like
to address in our future work.
5 Conclusion and Future Directions
The focus of this study was a comparison of prior
emotion of a word with its contextual emotion, and
their effect on the emotion expressed by the sen-
tence. We also studied features important in recog-
nizing contextual emotion. We experimented with
a wide variety of linguistically-motivated features,
and we evaluated the performance of these fea-
tures using logistic regression. We showed that
our approach and features significantly outperform
the baseline and the SVM result applied to Bag-of-
Words.
Even though the features we presented did quite
well on the chosen dataset, in the future we would
like to show the robustness of these features by ap-
plying them to different datasets.
Another direction for future work will be to ex-
pand our emotion lexicon using existing techniques
for automatically acquiring the prior emotion of
words. Based on the number of instances in each
emotion class, we noticed there is a tight relation
between the number of words in each emotion list
in the emotion lexicon and the number of sentences
that are derived for each emotion class. It follows
that a larger lexicon will have a greater coverage of
emotional expressions.
Last but not least, one of the weaknesses of our
approach was the fact that we could not use all the
instances in the dataset. Again, the main reason was
the low coverage of the emotion lexicon that was
used. The other reason was the limitation of our
method: we had to only choose the sentences that
have one or more emotional words. As future work,
we would like to relax the restriction by using the
root of the sentence (based on the dependency tree
result) as a cue word rather than the emotional word
from the lexicon. So, for sentences with no emo-
tional word, we can calculate all the features regard-
ing the root word rather than the emotional word.
References
Alias-i. 2008. Lingpipe 4.1.0., October.
Alm, Cecilia Ovesdotter, Dan Roth, and Richard Sproat.
2005. Emotions from Text: Machine Learning for
Text-based Emotion Prediction. In HLT/EMNLP.
Aman, Saima and Stan Szpakowicz. 2007. Identifying
expressions of emotion in text. In Proc. 10th Inter-
national Conf. Text, Speech and Dialogue, pages 196?
205. Springer-Verlag.
Chaumartin, Fran?ois-Regis. 2007. UPAR7: a
knowledge-based system for headline sentiment tag-
ging. In Proc. 4th International Workshop on Seman-
tic Evaluations, SemEval ?07, pages 422?425.
Choi, Yejin, Claire Cardie, Ellen Riloff, and Siddharth
Patwardhan. 2005. Identifying sources of opinions
with conditional random fields and extraction patterns.
In Proc. Human Language Technology and Empirical
Methods in Natural Language Processing, HLT ?05,
pages 355?362.
Ekman, Paul. 1992. An argument for basic emotions.
Cognition & Emotion, 6(3):169?200.
Esuli, Andrea and Fabrizio Sebastiani. 2006. SENTI-
WORDNET: A Publicly Available Lexical Resource
77
for Opinion Mining. In Proc. 5th Conf. on Language
Resources and Evaluation LREC 2006, pages 417?
422.
Ghazi, Diman, Diana Inkpen, and Stan Szpakowicz.
2010. Hierarchical approach to emotion recognition
and classification in texts. In Canadian Conference on
AI, pages 40?50.
Hall, Mark, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The WEKA data mining software: an update.
SIGKDD Explor. Newsl., 11:10?18, November.
Katz, Phil, Matthew Singleton, and Richard Wicen-
towski. 2007. SWAT-MP: the SemEval-2007 systems
for task 5 and task 14. In Proc. 4th International Work-
shop on Semantic Evaluations, SemEval ?07, pages
308?313.
Kozareva, Zornitsa, Borja Navarro, Sonia V?zquez, and
Andr?s Montoyo. 2007. UA-ZBSA: a headline emo-
tion classification through web information. In Proc.
4th International Workshop on Semantic Evaluations,
SemEval ?07, pages 334?337.
Marneffe, Marie-Catherine De, Bill Maccartney, and
Christopher D. Manning. 2006. Generating typed de-
pendency parses from phrase structure parses. In Proc.
LREC 2006.
Mohammad, Saif M. and Peter D. Turney. 2010. Emo-
tions evoked by common words and phrases: using
mechanical turk to create an emotion lexicon. In Proc.
NAACL HLT 2010 Workshop on Computational Ap-
proaches to Analysis and Generation of Emotion in
Text, CAAGET ?10, pages 26?34.
Neviarouskaya, Alena, Helmut Prendinger, and Mitsuru
Ishizuka. 2010. AM: textual attitude analysis model.
In Proc. NAACL HLT 2010 Workshop on Computa-
tional Approaches to Analysis and Generation of Emo-
tion in Text, pages 80?88.
Neviarouskaya, Alena, Helmut Prendinger, and Mitsuru
Ishizuka. 2011. Affect Analysis Model: novel rule-
based approach to affect sensing from text. Natural
Language Engineering, 17(1):95?135.
Ortony, Andrew, Allan Collins, and Gerald L. Clore.
1988. The cognitive structure of emotions. Cambridge
University Press.
Pang, Bo, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up?: sentiment classification using ma-
chine learning techniques. In Proc. ACL-02 confer-
ence on Empirical methods in natural language pro-
cessing - Volume 10, EMNLP ?02, pages 79?86.
Platt, John C. 1998. Sequential Minimal Optimization:
A Fast Algorithm for Training Support Vector Ma-
chines.
Riloff, Ellen. 2003. Learning extraction patterns for sub-
jective expressions. In Proc. 2003 Conf. on Empirical
Methods in Natural Language Processing, pages 105?
112.
Stoyanov, Veselin, Claire Cardie, and Janyce Wiebe.
2005. Multi-perspective question answering using the
opqa corpus. In Proc. Conference on Human Lan-
guage Technology and Empirical Methods in Natural
Language Processing, HLT ?05, pages 923?930.
Strapparava, Carlo and Rada Mihalcea. 2007. SemEval-
2007 Task 14: Affective Text. In Proc. Fourth Interna-
tional Workshop on Semantic Evaluations (SemEval-
2007), pages 70?74, Prague, Czech Republic, June.
Strapparava, Carlo and Rada Mihalcea. 2008. Learning
to identify emotions in text. In Proc. 2008 ACM sym-
posium on Applied computing, SAC ?08, pages 1556?
1560.
Strapparava, Carlo and Alessandro Valitutti. 2004.
WordNet-Affect: an Affective Extension of Word-
Net. In Proc. 4th International Conf. on Language
Resources and Evaluation, pages 1083?1086.
Sumner, Marc, Eibe Frank, and Mark A. Hall. 2005.
Speeding Up Logistic Model Tree Induction. In Proc.
9th European Conference on Principles and Practice
of Knowledge Discovery in Databases, pages 675?
683.
Toutanova, Kristina, Dan Klein, Christopher D. Man-
ning, and Yoram Singer. 2003. Feature-Rich Part-of-
Speech Tagging with a Cyclic Dependency Network.
In Proc. HLT-NAACL, pages 252?259.
Turney, Peter D. 2002. Thumbs up or thumbs down?:
semantic orientation applied to unsupervised classifi-
cation of reviews. In Proc. 40th Annual Meeting on
Association for Computational Linguistics, ACL ?02,
pages 417?424.
Wilson, Theresa, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-level
sentiment analysis. In Proc. HLT-EMNLP, pages 347?
354.
Wilson, Theresa, Janyce Wiebe, and Paul Hoffmann.
2009. Recognizing Contextual Polarity: An Explo-
ration of Features for Phrase-Level Sentiment Analy-
sis. Computational Linguistics, 35(3):399?433.
Yang, Yiming and Xin Liu. 1999. A re-examination
of text categorization methods. In Proc. 22nd an-
nual international ACM SIGIR conference on Re-
search and development in information retrieval, SI-
GIR ?99, pages 42?49.
Yu, Hong and Vasileios Hatzivassiloglou. 2003. To-
wards answering opinion questions: separating facts
from opinions and identifying the polarity of opin-
ion sentences. In Proc. 2003 conference on Empirical
methods in natural language processing, EMNLP ?03,
pages 129?136.
78
