Towards the Generations.of Rebul;tals in a Bayesian 
Argumentation System 
Natha l ie  J i tnah ,  Ingr id  Zukerman,  R ichard  McConachy  and  Sarah  George 
School  of  Computer  Sc ience and Sof tware  Eng ineer ing  
Monash  Un ivers i ty  
C layton ,  V ic tor ia  3800, AUSTRAL IA  
emaih {nj  i tnah ,  ingr id ,  r i cky ,  sarahg}@csse ,  monash ,  edu .  au 
Abst ract  
We describe a mechanism which generates rebuttals 
to a user's rejoinders in the context of arguments 
generated from Bayesian etworks. This mechanism 
is implemented in an interactive argumentation sys- 
tem. Given an argument generated by the system 
and an interpretation f a user's rejoinder, the gener- 
ation of the rebuttal takes into account the intended 
effect of the user's rejoinder, determined on a model 
of the user's beliefs, and its actual effect, determined 
on a model of the system's beliefs. We consider three 
main rebuttal strategies: refute the user's rejoinder, 
strengthen the argument goal, and dismiss the user's 
line of reasoning. 
1 In t roduct ion  
During argumentation, conversational partners of- 
ten use expressions of doubt, such as "But the vic- 
tim was stabbed", and requests for the considera- 
tion of additional facts they consider elevant, such 
as "What about the fingerprints found on the gun?". 
In this paper, we describe a mechanism which gen- 
erates rebuttals to such rejoinders in the context of 
arguments generated from Bayesian etworks (BNs) 
(Pearl, 1988). This mechanism is implemented in 
a system called BIAS (Bayesian Interactive Argu- 
mentation System). Given an argument produced 
by BIAS and a follow-up rejoinder posed by a user, 
our mechanisln generates a rebuttal on tim basis of a 
line of reasoning identified by BIAS from the user's 
rejoinder. These capabilities constitute a significant 
step towards allowing a user to interact freely with 
an argumentation system and to improve the expla- 
nation capability of Bayesian systems. 
Normal arguments are unconstrained in the sense 
that they can use whatever means are available to 
? justify a goal proposition:i,:"In'~conterast, rebuttals 
are constrained, since they must address the point 
through which the conversational partner attempted 
to undermine or question a previous argument. To 
illustrate the operation of BIAS and its rebuttal ca- 
pability, consider the exchange in Figure 1, which 
consists of a preamble that contains background in- 
formation~ followed by an argument generated by 
BIAS, a user's rejoinder and BIAS' rebuttal. 1 The 
domain of implementation is a murder investigation 
where the question under consideration (the goal 
proposition) is "Did Mr Green murder Mr Body?", 
and both the user and the system have access to evi- 
dence. After the presentation of the argument where 
BIAS contends Mr Green's possible innocence, 2 the 
user presents a rejoinder which requests that BIAS 
consider a fact that was omitted from the argument: 
The \]found gun is available only to Mr Green. BIAS 
infers from this rejoinder that the user is adding sup- 
port to Mr Green having the means to kill Mr Body, 
and hence to Mr Green's guilt, through the following 
line of reasoning, which is determined as described 
in (Zukerman et al, 2000): The gun being available 
only to Mr Green ~ The gun was fired by Mr Green 
Mr Green had the means to kill Mr Body -+ Mr 
Green killed Mr Body. BIAS finds that it does not 
share the user's belief in the rejoinder proposition, 
and that in addition, the effect of this proposition on 
the goal is rather weak. This prompts the genera- 
tion of a rebuttal of the form Deny-Dismiss-Follow, 
whereby the rejoinder proposition is denied, its effect 
on the goal proposition is dismissed, and its impli- 
cations are followed hypothetically until they break 
down due to the marginal effect of the rejoinder on 
Mr Green's guilt. 
In the next section, we present our knowledge rep- 
resentation formalism, followed by an outline of our 
procedure for determining a user's line of reasoning. 
In Section 4, we describe our algorithm for rebuttal 
generation and discuss our results. We then review 
related work and present concluding remarks. 
2 ' -  Knowledge Representat ion  
During the argumentation process, BIAS maintains 
two models of belief: a normative model and a user 
model,eaeh-of-which is-represented as a BN. The 
normative model contains information gathered i- 
rectly by BIAS from the murder scenario, while the 
user model stores propositions that are presumed to 
1The argument  and rebuttals hown in this paper are re- 
alized in English as described in (Zukerman et al, 1999). 
2The mechanism which generates this argument is de- 
scribed in (Zukerrnan et al, 1998). 
39 
Preamble: 
Mr. Body was found.dead in his bedroom, which is in .the.seecond .story. of.his.house. Bul letwounds 
were found in Mr. Body's body. The bedroom window was broken and broken glass was found inside 
the window. A gun was found on the premises, and some fingerprints were found on the gun. In 
addition, inspection of the. grounds revealed footprints in the garden and circular indentations in the 
ground outside the bedroom window. 
BIAS' argument: 
Bullets being found in Mr Body's body implies Mr Body was almost certainly shot. This implies he 
was almost certainly murdered. 
Forensics matching the bullets with the found gun implies the gun is almost certainly the murder 
weapon. Forensics matching the fingerprints,witth Mr.Gr.een implies_Mr Gregn~ probably fired the gun. 
This together with the gun almost certainly being the murder weapon implies Mr Green probably 
fired the murder weapon, which implies he very probably had the means to murder Mr Body. 
The Bayesian Times reporting Mr Body took Mr Green's girlfriend implies Mr Green and Mr Body 
possibly were enemies, which implies Mr Green possibly had a motive to murder Mr Body. 
The neighbour eporting Mr Green not being in the garden at 11 implies Mr Green very probably 
wasn't in the garden at 11. 
Forensics reporting the time of death being 11 and the forensic analysis of the time of death being 
reliable implies the time of death was probably 11, which together with Mr Green very probably not 
being in the garden at 11 implies he probably wasn't in the garden at the time of death. This implies 
he probably didn't have the opportunity to murder Mr Body. 
Even though Mr Green very probably had the means to murder Mr Body and he possibly had a 
motive to murder Mr Body, Mr Green probably not having the opportunity to murder Mr Body 
implies he probably didn't murder Mr Body. 
User 's  re jo inder:  Consider that the found gun is available only to Mr Green. 
BIAS' rebuttal: 
Actually, it is very improbable that the found gun is available only to Mr Green. However, even if it 
was available only to Mr Green, this would have only a small effect on the likelihood that Mr Green 
murdered Mr Body. This is for the following reason. 
The found gun being available only to Mr Green implies it is more likely that Mr Green fired the gun, 
making it almost certain. This implies it is more likely that he fired the murder weapon, making it 
almost certain, which implies it is even more likely that he had the means to murder Mr Body. This 
implies it is only slightly more likely that he murdered Mr Body. 
Figure 1: Sample Argument, Rejoinder and Rebuttal 
be believed by the user. These propositions may be 
obtained from a variety of sources, e.g., they may 
have been inspected by the user in the murder sce- 
nario (by means of a WWW interface), or appear in 
BIAS' previous arguments or the user's rejoinders. 
Arguments generated by BIAS are represented by 
means of an Argument Graph - a sub-network of 
the normative model BN which ideally also contains 
nodes from the user model BN. 
The interpretation process, where BIAS identifies 
the reasoning path intended by the user, takes place 
in the user model; since ,BIAS tries, to .%nake sense"- 
of what the user is saying according to the system's 
view of the user's beliefs (Zukerman et al, 2000). 
In contrast, the processes for generating the initial 
argument and the rebuttals consult both tile user 
model and the normative model to produce argu- 
ments that rely on beliefs held by both BIAS and 
tile user if possible. Further, during rebuttal gener- 
ation, the choice of a rebuttal strategy depends on 
the intended effect of the user's argument (according 
to the user model) and its actual effect (according 
to the normative model). 
3 Determin ing  a User ' s  L ine  o f  
Reason ing  
Our procedure for recognizing a user's intended line 
of reasoning from his/her rejoinder eceives two in- 
puts: a linguistic clue ("but" or "consider") and a 
rejoinder proposition (R), e.g., "but Mr Green was 
in the garden"~..It then-finds paths in the user model 
BN that connect R to the goal proposition (Zuker- 
man et al, 2000). During this process, BIAS copes 
with inference patterns that are different from its 
own by allowing inferred paths to contain a small 
"gap" composed of propositions that did not ex- 
ist previously in the user model. Figure 2(a) il- 
lustrates an Argument Graph, a rejoinder R, and 
40 
(a) Argument Graph and userPath 
M~ I ') 
A 
(c) Dismiss userPath: 
R = userVal has small effect on G 
for BIAS 
Figure 2: Sample Argument 
path R-I-M-E-A-G between them (composed of grey 
nodes). This path, called userPath, represents the 
line of reasoning intended by the user. The gap in 
this path contains nodes I and M (in italics), which 
means that the user inferred E directly from R. 
Each path is assigned a score based on the fol- 
lowing factors: the impact of R on BIAS' argu- 
ment along this path, whether path nodes are in 
the user's attentional focus, and BIAS' confidence in 
this path (determined from the information source 
of the nodes in this path, e.g., whether the user has 
seen the propositions in the path, asserted a be- 
lief about them or read them in BIAS' arguments). 
BIAS then selects the highest-scoring path. If sev- 
eral paths have a high score, the user is asked to 
choose one of them. Typically,-BIAS returns a single 
path, and sometimes it returns two or three paths. 
Hence, presenting them to the user for selection is a 
reasonable course of action. 
4 Rebut ta l  Generat ion  
Given a user's rejoinder proposition R, we consider 
three main types of rebuttals: (1) refute R, (2) dis- 
(b) Refute R: 
R = userVal has large effect on G; 
BIAS and the user disagree on R 
- i - . _@ 
(d) Strengthen G: 
R = userVal has large effect on G; 
BIAS and the user agree on R 
Graph and Rejoinder Strategies 
miss the line of reasoning intended by the user (user- 
Path), and (3) strengthen the argument goal G. Di- 
agrammatic representations of these rebuttal strate- 
gies and abridged versions of their applicability con- 
ditions appear in Figure 2(b-d). These conditions, 
which are specified in the following sections, depend 
oil (1) whether the rejoinder affects the system's ar- 
gument directly or indirectly, (2) the beliefs in R in 
the normative and user models, and (3) the impact 
of R on the goal proposition along userPath in the 
normative and user models. 
4.1 Refute  the rejoinder 
This strategy consists of generating an argument 
against he user's belief in the rejoinder proposition 
R. This strategy.isapplicable under the following 
conditions: . . . . . .  
(R1) The beliefs in R in the user model and the 
normative model differ significantly (the user's 
belief in R contradicts BIAS' belief); and 
(R2) Either 
(a) R was stated or implied in BIAS' argument 
(R appears in the Argument Graph), or 
41 
(b) The belief in R stated by the user has a 
significant.effect on. the goal G in ,the nor- 
mative model in the same direction as its 
effect on G in the user model. 
For example, if the user's rejoinder to the argu- 
ment in Figure 1 was "But Mr Green and Mr Body 
were not enemies", then conditions R1 and R2a 
would be satisfied, since the rejoinder directly con- 
tradicts what was stated by BIAS in the argument. 
If the user's rejoinder was "But the neighbour saw 
Mr Green shoot ..Mr, Body~!-i.~then :.conditions-,R1 
and R2b would be satisfied, since an inference from 
this rejoinder contradicts BIAS' belief in Mr Green's 
lack of opportunity to kill Mr Body (and conse- 
quently in Mr Green's guilt). The argument schema 
for the refute the rejoinder strategy and a sample 
rebuttal produced with this schema are shown in 
Figure 3. a The sub-argument that argues against 
the rejoinder proposition is generated by activating 
our Bayesian argument generator (Zukerman et al, 
1998) with the proposition Mr Green and Mr Body 
were enemies as the goal. In this case, the belief in 
the rejoinder node resulting from the sub-argument 
differs from that stated in the initial argument, ow- 
ing to the additional information included in the 
sub-argument. Hence, the implications from the re- 
joinder node are followed. The procedure for follow- 
ing these implications is described in Section 4.2. 
4.2 Dismiss the user's line of reasoning 
This strategy consists of showing the user how 
his/her argument fails to achieve its intended ef- 
fect. We distinguish between concessive and con- 
tradictory dismissals. The former is used when the 
system agrees with the rejoinder proposition R, and 
the latter when the system disagrees with R. This 
strategy is applicable under the following condition: 
(D) R does not significantly affect the belief in G 
in the normative model. 
This condition is illustrated by the rejoinder to the 
argument in Figure 1, "Consider that the found gun 
was available only to Mr Green", which purports to 
increase the belief in Mr Green's means to kill Mr 
Body, and hence Mr Green's guilt. However, since 
this increment is quite small, BIAS adopts the dis:. 
missal strategy, which follows the effect of the user's 
rejoinder through the user's line of reasoning, point- 
ing out how the effect of the rejoinder differs from its 
intended effect. It is worth noting that the main dif- 
ference between a dismissal and a strengthening of
the goal is that BIAS decides to generate a dismissal 
when its current beliefs are sufficient to invalidate 
the user's line of reasoning, whereas it decides to 
aThe rejoinders shown in this paper are posed by the user 
immediately after the argument in Figure 1. 
Refute R: 
t .  Deny, the behef in"R stated by the'user. 
2. Present a sub-argument for the normative 
belief in R. 
3. If R is not in the Argument Graph or the 
belief in R as a result of the sub-argument 
differs from that originally stated by BIAS, 
then follow the effect of R along userPath 
up to the first node in the Argument Graph 
... .  vchose belief.is, the ~ same.as ..that stated in 
the initial argument. 
Rejoinder: 
But Mr Green and Mr Body were not enemies. 
Rebuttal :  
Actually, it is quite likely that Mr Green and 
Mr Body were enemies. This is for the following 
reason. 
The forensic analysis of the blue paint being 
reliable and forensics having found some blue 
paint which they estimate is one week old im- 
plies a blue car was here last week. This to- 
gether with Mr Green having a blue car implies 
Mr Green's car was almost certainly here last 
week, which implies Mr Green almost certainly 
visited Mr Body last week. 
The neighbour being sober implies she is very 
probably reliable. This together with the neigh- 
bour reporting Mr Green arguing with Mr Body 
last week implies the neighbour very probably 
heard Mr Green arguing with Mr Body last 
week, which together with Mr Green almost 
certainly visiting Mr Body last week implies he 
almost certainly argued with Mr Body. 
The Bayesian Times reporting Mr Body took 
Mr Green's girlfriend implies Mr Body prob- 
ably seduced Mr Green's girlfriend. This to- 
gether with Mr Green almost certainly arguing 
with Mr Body implies Mr Green and Mr Body 
probably were enemies. 
Let's now go back to the main argument. 
Mr Green and Mr Body probably being enemies 
implies it is more likely that Mr Green had a 
motive to murder Mr Body, making it rather 
likely. This implies it is only slightly more likely 
that he murdered Mr Body. 
Figure 3: Refute the rejoinder Schema nd Example 
strengthen the goal when additional information is 
required to defeat he impact of the user's rejoinder. 
Our algorithm for dismissing the user's line of rea- 
soiling follows userPath until it reaches a point where 
the user's line of reasoning fails, i.e., it has no ef- 
fect on a proposition on userPath in tile Argument 
Graph. It is necessary for the rebuttal to reach the 
42 
Argument Graph even if the failure of the rejoinder 
occurs earlier in userPathrbecause the user's ~ejoin= . 
der refers to the argument, hence at least one propo- 
sition in the argument must be mentioned when ad- 
dressing the impact of this rejoinder. 
The user's line of reasoning may fail due to the 
following factors: (1) s/he did not consider propo- 
sitions that have a significant effect on the propo- 
sitions in userPath; or (2) his/her belief in one or 
more of the propositions /he did consider differs 
significantly from thatSn t.he normative model, .and 
this proposition has a substantial ~effect on a pr,515o--:: 
sition in userPath. Propositions of the first type are 
included in a set called SIGneighbours, and proposi- 
tions of the second type are included in DIFFneigh- 
bouts. Our dismissal algorithm calls our Bayesian ar- 
gument generator to generate sub-arguments for the 
propositions in DIFFneighbours, but simply presents 
the propositions in SIGneighbours without arguing 
for them. 
A lgor i thm DismissUserReasoning( userPath) 
Let userPath be composed of propositions 
R=Po--+ PI ~ P2--+...--+ Pr,=G. 
1. For i= l tondo:  
(a) Set SIGneighbours(Pi) to the nodes that 
are linked to Pi in the normative model but 
not in the user model and have a significant 
effect on the belief in Pi. 
(b) If the belief in Pi in the user model 
differs significantly from the belief in 
Pi in the normative model, then set 
DIFFneighbours(Pi) to the nodes that are 
linked to Pi in both the user model and the 
normative model and which have a differ- 
ent belief in the user model from that in 
the normative model. 
(c) For each node Pj E DIFFneighbours(Pi) 
generate a sub-argument for the normative 
belief in Pj. 
2. Present. the resulting rebuttal using the appro- 
priate schema, DismissContradict or Dismiss- 
Concede (Figures 4 and 5 and respectively). 
Our concessive schema differs fi'om our contradic- 
tory" schema in two respects. Firstly,, the former ac- 
knowledges the user's rejoinder, while the latter de- 
nies it. In addition, the concessive schema follows 
the user's line of reasoning-starting,from the nor- 
mative belief in the rejoinder proposition (which is 
close to the belief indicated by the user), while the 
contradictory" schema follows a hypothetical line of 
reasoning starting from the user's belief in the re- 
joinder proposition (which differs substantially from 
the normative belief). In both cases the user's line 
of reasoning fizzles out, due to its small effect on the 
DismissContradict userPath: 
...... t".-Deny -tiie~betiefdn. ~ - stated . . . . . . . . . . . . . .  -'by the user," 
and dismiss its hypothetical effect on the 
goal proposition. 
2. Present he sub-arguments for the nodes in 
DIFFneighbours. 
3. FollowPath userPath from the rejoinder 
proposition to the goal. 
Fo l lowPath  userPath 
For i :-= 0 to:.n.X-~:t.,.(whe/'e'n is-i;h&i4umber of 
nodes in userPath) do: 
1. If Pi+l is not in the Argument Graph or 
DIFFneighbours(Pi+l)?O, then present an 
implication from Pi to Pi+l which includes 
the nodes linked to Pi+l in the user model 
plus the nodes in SIGneighbours(Pi+l ). 
Else present an implication which reflects 
only the relative impact of Pi on Pi+l. 
2. If the resulting belief in Pi+l is the same 
as that stated in the initial argument, then 
stop. 
Figure 4: DismissContradict Schema and Follow- 
Path Procedure 
goal according to the normative model irrespective 
of its truth value. 
Both schemas follow userPath from tile rejoinder 
proposition to the goal using procedure FollowPath 
(Figure 4). This procedure distinguishes between 
propositions in userPath for which the main influ- 
encing factors (DIFFneighbours and SIGneighbours) 
should be presented, and those which require only 
information regarding the relative impact of the pre- 
ceding proposition in userPath. The latter proposi- 
tions are characterized as follows: (1) they appear 
in the Argument Graph; and (2) the user's beliefs in 
the nodes outside userPath that have a significant 
effect on these propositions are consistent with the 
normative beliefs in these nodes. For instance, the 
rebuttal in Figure 1~ which is generated by means of 
the DismissContradict schema, presents the relative 
influence of Mr Green fired the gun on Mr Green 
fired the murder weapon, since the user and BIAS 
hold consistent beliefs regarding the gun being the 
murder weapon. 
? " To iltustratte"t.he operation 'Of t-he dismissal algo- 
rithm, let us consider the rejoinder "But the time of 
death was 11", which yields the following line of rea- 
soning: The time of death was 11 (~ Mr Green was 
in the garden at 11) ~ Mr Green was in the .qarden 
at the time of death + Mr Green had the opportu- 
nity to kill Mr Body ---+ Mr Green killed Mr Body. 
DIFFneighbours includes only one proposition, Mr 
43 
DismissConcede userPath:  
1. Acknowledge.the,belief"in'-R stated bythe  
user, and dismiss its effect on the goal 
proposition. 
2. Present he sub-arguments for the nodes in 
DIFFneighbours. 
3. FollowPath userPath from the rejoinder 
proposition to the goal. 
Rejoinder: But the time of death was 11. 
Rebuttal: -: . . . . . . . . . .  
Indeed, it is quite likely but not entirely certain 
that the time of death was 11. However, this 
has only a small effect on the likelihood that 
Mr Green murdered Mr Body. 
I will show that Mr Green almost certainly 
wasn't in the garden at 11. 
Mr Green's witness not being related to Mr 
Green implies she is very probably reliable. 
This together with Mr Green's witness report- 
ing Mr Green being at the football at 10:30 
implies Mr Green was almost certainly at the 
football at 10:30. 
The neighbour being sober implies she is almost 
certainly reliable. This together with the neigh- 
bour reporting Mr Green not being in the gar- 
den at 11 implies the neighbour never saw Mr 
Green in the garden at 11, which together with 
Mr Green almost certainly being at the football 
at 10:30 implies he almost certainly wasn't in 
the garden at 11. 
Let's now go back to the main argument. 
Even though the time of death was probably 
11, Mr Green almost certainly not being in the 
garden at 11 implies it is only slightly less likely 
that he was in the garden at the time of death. 
This implies it is only slightly less likely that 
he had the opportunity to murder Mr Body, 
which implies it is only slightly less likely that 
he murdered Mr Body. 
Figure 5: DismissConcede Schema and Example 
Green was in the garden at 11, since the belief in it 
in the normative model differs from that in the user 
model, thereby prompting the generation of a sub- 
argument for this proposition.. This sub-argument 
is stronger than that incorporated in the initial ar- 
gument, yielding a belief in Mr  Green.being in the 
garden at 11 that is lower than the belief indicated 
in the original argument, which in turn reduces the 
belief in Mr Green being in the garden at the time of 
death, Mr Green having the opportunity to kill Mr 
Body, and Mr Green actually nmrdering Mr Body. 
The resulting rebuttal, which is presented by means 
of the DismissConcede schema, appears in Figure 5. 
4.3 Strengthen the goal 
: .:This: strategy, consist~-of.germrafing a stronger argu- 
ment for the original goal proposition G, bringing to 
bear information that did not appear in the initial 
argument (either because BIAS was unaware of it 
or because BIAS chose to exclude it from the argu- 
ment). This strategy is applicable under the follow- 
ing conditions: 
(G1) The beliefs in R in the normative and user 
models are consistent; and 
(G2) Rhas  a=Substantia\] detrimentgI effect on the 
belief in G in the normative model. This change 
in belief should be in the same direction as the 
change occurring in the user model. 
These conditions represent a situation where the 
system did not take into account a particular fact, 
but when this fact comes to its attention the sys- 
tem realizes the effect of this fact on the goal. For 
instance, if the user discovers new evidence that 
places Mr Green in the garden at 11, a rejoinder 
which presents this proposition will increase the be- 
lief in Mr Green's opportunity to kill Mr Body along 
the following line of reasoning: Mr Green was in 
the garden at 11 -+ Mr Green was in the garden at 
the time of death --+ Mr Green had the opportunity 
to kill Mr Body ~ Mr Green killed Mr Body. In 
this case, BIAS tries to strengthen the argument for 
Mr Green's innocence by arguing separately against 
propositions along this line of reasoning (other than 
the rejoinder node, which is true in this example). 
If no sub-argument can be generated for these nodes 
or the generated sub-arguments do not significantly 
affect the goal, then BIAS agrees with the user. 
Our algorithm for strengthening the goal searches 
along userPath for propositions that have been af- 
fected by the rejoinder, but that will reinforce BIAS' 
goal proposition if their belief is changed. It then 
tries to generate sub-arguments that change the be- 
liefs in these propositions. In order to localize the 
effect of the user's rejoinder, the search and sub- 
argument generation processes tart at R and pro- 
ceed towards the goal. The presentation of the re- 
buttal is also done in this order, using a procedure 
which is similar to the FollowPath procedure de- 
scribed in Section 4.2. 
A lgor i thm StrengthenGoal( userPath) 
Let userPath be composed of propositions 
R=Po---~ Pi ~ P2-+. . . -~  Pn=G. 
1. For i = 1 to n, while the belief in G is not as 
intended by BIAS, do: 
(a) Determine which belief in Pi will move the 
belief in G in the normative model in the 
direction intended by BIAS. 
(b) If this belief in Pi differs from the current 
belief in Pi, then 
44 
i. Generate a sub-argument for the de- 
sired belief in Pi. 
ii. If the sub-argument yields a significant 
change in the belief in Pi or in the be- 
lief in G then store the sub-argument in 
SubAG(P~). 
2. Present he resulting rebuttal (composed of the 
user's line of reasoning and intervening sub- 
arguments) using the StrengthenGoal schema 
in  Figure 6. 
To illustrate the operation of this algorithm, let 
us reconsider the rejoinder "Consider Mr Green was 
in the garden at 11", and let us assume that the re- 
joinder proposition is true. Inspection of the prop- 
ositions affected by this rejoinder eveals that if Mr 
Green was not in the garden at the time of death, 
then the belief in the goal would be closer to that 
intended by BIAS. However, an argument for this 
proposition cannot be generated. Hence, BIAS pro- 
ceeds to the next proposition, Mr Green had the op- 
portunity to murder Mr Body, and calls our Bayesian 
argument generator to generate an argument hat 
contradicts this proposition. The Bayesian genera- 
tor produces an argument which reduces the belief 
in this proposition. However, this belief cannot be 
reduced to the extent hat it exculpates Mr Green. 
Thus, BIAS attempts to generate an argument for 
the goal node (by trying to reduce the belief in Mr 
Green's means and motive to kill Mr Body). How- 
ever, this attempt also fails, leaving BIAS with a 
moderate belief in Mr Green's guilt. 4 
It is important o note that although BIAS' im- 
mediate objective is to strengthen its belief in the 
goal proposition, its primary purpose is to "tell the 
truth" to the best of its knowledge (which may con- 
tradict its initial beliefs), rather than to win the ar- 
gument at all costs. Our algorithm supports this 
attitude by retaining any sub-argument which has 
a significant impact on the goal or on a proposi- 
tion on userPath. We use this disjunctive condition 
on impacts in order to address a situation where a 
proposition Pj on userPath has been affected by a 
sub-argument, but does not affect the goal because 
of an inaccurate belief in aproposition Pk which ap- 
pears later on userPath:(recalt that the propositions 
are inspected from R towards the goal). However, 
S t rengthenGoa l  userPath: 
.... t-. ~Aekn0wledge~the.~etief.in'-R stated"by the" 
user, and set lastProposition to R. 
2. Until the goal proposition is reached o: 
(a) If after lastProposition there is a 
proposition Pi EuserPath for which 
a sub-argument was generated 
(SubAG(Pi)? 0), then 
i. Follow userPath from lastProposi- 
tion-to-Pi. . . . .  
ii. Present he sub-argument for Pi. 
iii. Set lastProposition to Pi. 
(b) Else follow the remainder of userPath. 
Figure 6: Strengthen the goal Schema 
5 Re la ted  Research  
Our research builds on work described in (Zukerman 
et al, 1998), which generated arguments from BNs, 
and (Zukerman et al, 1999), which enabled a user 
to explore the impact of different propositions on 
the generated arguments. The former system only 
generated arguments, while the latter received in- 
structions from a user (through a menu) about mod- 
ifications to be performed to a previously generated 
argument, e.g., including or excluding a proposition, 
and then generated a new argument in response to 
these instructions. Neither of these systems gener- 
ates rebuttals which take into account a user's in- 
tentions, as done by BIAS. 
Several researchers have dealt with different as- 
pects of argumentation; e.g., (Flowers et al, 1982; 
Quilici, 1992; Chu-Carroll and Carberry, 1995; Car- 
berry and Lambert, 1999). Like BIAS, the system 
described in Carberry and Lambert (1999) combined 
linguistic and contextual knowledge to recognize a
user's intentions from rejoinders. However, their 
system did not generate rebuttals. Chu-Carroll and 
Carberry (1995) provided a comprehensive approach 
for proposal evaluation which focused on dialogue 
strategies rather than argumentation strategies. In 
additiom they considered exchanges where each par- 
ticipant utters one or two propositions in each con- 
versational turn. In contrast, we focus on strategies 
for the generation of extended probabilistie rebuttals 
to individual rejoinders. In the future, our strategies 
once a sub-argument for Patispresented, then Pj af,: ;, ?will be~'e0mbined with.'dialbgue stra~gies in a corn- 
fects the goal. If BIAS accepted only sub-arguments 
for propositions which have a significant impact on 
the goal, then in this case it would miss the oppor- 
tunity to strengthen the goal. 
1The resulting argument has not been included owing to 
space limitations, 
plete argumentation system. 
Flowers et al (1982) presented a partial theory 
of argumentation which advocated the combination 
of distinct knowledge sources; their implementation 
focused on recognizing and providing episodic justi- 
fications to historical events. Our focus oil the gen- 
eration of rebuttals in the context of BNs allows us 
45 
to provide an operational definition for the broad 
argumentation strategies discussed in the l i terature,  
e.g., attack the main point directly or attack the- 
supporting evidence (Flowers et al, 1982). 5
The argumentation system described in (Quilici, 
1992) used a plan-based model of the user's beliefs to 
recognize the justification for-a user's proposal and 
provide its own justifications. However, the rebut- 
tals generated by this system were based on a single 
strategy: applying backwards chaining using a set of 
justification rules. This strategy is a special case of 
the more general rebuttal Schemas presented here. 
6 Conclusion and Future Work 
We have offered a mechanism for generating rebut- 
tals to a user's rejoinders in the context of argu- 
ments generated by a Bayesian argumentation sys- 
tem. We have implemented three main argumenta- 
tion strategies: refuting the rejoinder, strengthening 
the argument goal, and dismissing the user's line of 
reasoning. For each strategy we have identified ap- 
plicability conditions, proposed a procedure which 
determines the information to be included in a re- 
buttal, and defined a presentation schema. 
An interesting area of future research pertains 
to the omission of information from an argument. 
There are different ypes of information which may 
be omitted from an argument, such as (1) easily 
inferred information and information which has a 
small effect on the argument; (2) information which 
is required for representational reasons, but makes 
the resulting argument more confusing; (3) proba- 
bilistic information which, although correct, makes 
the resulting argument more tedious; and (4) pre- 
viously stated information. Our previous research 
deals with the first type of information (Zukerman 
et al, 1998), and in this paper we have identified 
some conditions for the omission of previously stated 
information when expressing the relative impact of 
a proposition. Another factor that affects the onfis- 
sion of information is the trade off between accuracy 
and conciseness. The omission of information affects 
the belief in the conclusion(s) presented in an argu- 
ment. Stating beliefs that differ from a system's own 
beliefs may cause the system to appear inconsistent 
or even deceitful, while presenting all the relevant 
factors may yield a verbose argument. A mecha- 
nism which addresses these issues will support he 
generation of better arguments and rebuttals. 
The evaluation of Chis. xeseareh,encompassessev- 
eral components: (1) the WWW interface, (2) the 
path-finding mechanism, and (3) the rebuttal- 
generation mechanism. A preliminary evaluation of 
5\Ve do not handle the "attack the claim that the evidence 
lcives support for the main point" strategy, as this involves 
inferring Conditional Probability Matrices for the user model, 
which is outside the scope of this research. - 
the path-finding mechanism has yielded encourag- 
ing results (Zuke~man e~ al.,: 2000).: Two.types of 
evaluation are envisaged for the rebuttal-generation 
mechanism. A whole-system evaluation, where users 
interact freely with BIAS, may be used to deter- 
mine whether users are satisfied with the system as 
a whole. In contrast, a specific evaluation of rebut- 
tals would be restricted to showing users rejoinder- 
rebuttal pairs (after showing an initial argument), 
and eliciting the users' reactions regarding the ap- 
propriateness of the rebuttals. 
7 Acknowledgments 
This work was supported in part by Australian Re- 
search Council grant A49927212. 
References 
Carberry, S. and Lambert, L. (1999). A pro- 
cess model for recognizing communicative acts 
and modeling negotiation subdialogues. Compu- 
tational Linguistics, 25(1):1-53. 
Chu-Carroll, J. and Carberry, S. (1995). Generat- 
ing information-sharing subdialogues in expert- 
user consultation. In IJCAI95 - Proceedings of 
the Fourteenth International Joint Conference on 
Artificial Intelligence, pages 1243-1250. 
Flowers, M., McGuire, R., and Birnbaum, L. (1982). 
Adversary arguments and the logic of personal 
attack. In Strategies for Natural Language Pro- 
cessing, pages 275-294. Lawrence Erlbaum Asso- 
ciates, Hillsdale, New Jersey. 
Pearl, J. (1988). Probab:ilistic Reasoning in Intelli- 
gent Systems. Morgan Kaufmann Publishers, San 
Mateo, California. 
Quilici, A. (1992). Arguing about planning al- 
ternatives. In COLING-92 - Pwceedings of the 
Fourteenth International Conference on Computa- 
tional Linguistics, pages 906-910, Nantes, France. 
Zukerman, I., Jitnah, N., McConachy, R., and 
George, S. (2000). Recognizing intentions from re- 
joinders in a Bayesian interactive argumentation 
system. To appear in PRICAI2000 - Proceedings 
of the Sixth Pacific Rim International Conference 
on Artificial InteUigence, Melbourne, Australia. 
Zukerman, I., McConachy, R., and Korb, K. B. 
(1998). Bayesian reasoning in an abductive mech- 
anism for argument generation-and analysis. In 
AAAI98 - Proceedings of the Fifteenth National 
Conference on Artificial Intelligence, pages 833- 
.838,, Madison~-;Wisconsin. . 
Zukerman, I., McConachy, R., K0rb, K. B., and 
Pickett, D. A. (1999). Exploratory interaction 
with a Bayesian argumentation system. In IJ- 
CAI99 - Proceedings of the Sixteenth Inter~m- 
tional Joint Conference on Artificial Intelligence, 
pages 1294-1299, Stockholm, Sweden. 
46 
Using Argumentation Strategies in Automated Argument 
Generation 
I ngr id  Zukerman,  R ichard  McConachy  and  Kev in  B .  Korb  
School  o f  Computer  Science and  Sof tware  Engineer ing 
Monash  Un ivers i ty  
C layton ,  V ic tor ia  3800, AUSTRAL IA  
emai l :  { ingr id ,  r i cky ,  korb}@csse ,  monash,  edu .  au 
Abst ract  
During argumentation, people persuade their au- 
dience using a variety of strategies, e.g., hypo- 
thetical reasoning, reasoning by cases and ordinary 
premise-to-goal rguments. In this paper, we of- 
fer an operational definition of the conditions for 
pursuing these strategies, and incorporate into a 
Bayesian argument-generation system a mechanism 
for proposing applicable argumentation strategies, 
generating specific arguments based on these strat- 
egies, and selecting a final argument. 
1 In t roduct ion  
During argumentation, people persuade their audi- 
ence using a variety of strategies, e.g., hypothetical 
reasoning, reasoning by cases and premise to goal. 
Although the use of different strategies i common in 
human argumentation, the argumentation and dis- 
course planning systems developed to date offer little 
insight into the problem of proposing different argu- 
mentation strategies and selecting among them. 
In this paper, we extend our previous work on 
argument generation (Zukerman et al, 1998; Zuker- 
man et al, 1999) to address this problem. In this 
extension, we provide an operational definition of 
promising conditions for pursuing different argumen- 
tation strategies, and incorporate the procedure for 
selecting an argumentation strategy into the content 
planning process. The integration of strategy selec- 
tion and content planning is necessary due to the 
interplay between argumentation strategy and con- 
tent: the strategy influences the content hat is rele- 
vant to an argument, while the information gathered 
early in the content planning process determines the 
applicability of the different strategies. 
The argumentation strategies discussed in this 
paper are premise to goal,.~hypothetiddl ~red~ctio" .... 
ad absurdum and inference to the best explanation) 
and reasoning by cases (exclusive and non-exclusive) 
(Figure 1). Premise to goal starts from believed 
premises and proceeds to the goal. Reductio ad ab- 
surdum assumes the negation of the goal, leading to 
an argulnent which results in a contradiction with 
a believed premise and requires the assertion of the 
Premise to goal: Corrective lenses are re- 
quired. 
"Being unable to see far objects is evidence for 
myopia, which indicates that corrective lenses 
are required." 
Reduct io  ad absurdum:  There has always 
been matter. 
"There could never have been a time when 
nothing existed, for, if there were \[hypotheti- 
cal assumption\], then nothing would exist now, 
since from nothing comes nothing." (from St. 
Thomas Aquinas) 
In ference  to the best  exp lanat ion:  Patient 
has the flu. 
"If she had the flu \[hypothetical assumption\], 
she would be tired, achy. and feverish, which 
are all true. Hence, she probably has the flu" 
Reason ing  by cases (exclusive):  There can- 
not be a utopian society. 
"Having checks on procreation leads to misery 
and vice, which in turn results in a non-utopian 
society. 
Having no checks on procreation leads to a pop- 
ulation explosion, which in turn leads to star- 
vation. This also results in a non-utopian soci- 
ety." (from Malthus, 1798) 
Reason ing  by cases (non-exclusive):  Fail- 
ing a test. 
"I don't know whether he is stupid or lazy. But 
either way, he is likely to fail the test." 
Figure 1: Examples of Argumentation Strategies 
goal to resolve this contradiction. Inference to the 
.best. explanation (Lipton,, 1991) assumes&he goal, 
leading to an argument hat supports a believed 
premise (which would be disbelieved in the absence 
~,of ,thia, assmnption)~ ~Reasoning,,by casesmnumerates 
a set of exhaustive conditions and establishes that a 
desired conclusion would follow regardless of which 
case is true. ~ In particular, we consider two types 
of reasoning-by-cases trategies: exclusive and non- 
1 Reasoning by cases is not related to case-based reasoning, 
which involves problem solving based upon some stereotypical 
case-  
55 
exclusive. The exclusive strategy is applicable to sit- choice, on argument persuasiveness. The mecha- 
uations where the belief in a proposition is unknown nisms developed by these researchers, which are ap- 
or not agreed upon by the conversational partners, plicable after an argumentation strategy has been 
In this case, two separate arguments for the goal are selected, are expected to complement our future in- 
generated, one assuming the truth of this proposi- vestigation on modeling the effect of rhetorical fac- 
tion and the other assuming its falsity. 2 The non- tots on an addressee's beliefs. 
exclusive strategy applies to situations where at least 
one of several propositions i  known to be true, but 3 The  Argument  Generat ion  Process  
it is not known which. This strategy produces epa- 
rate arguments in support o f  the goal, each of which The platform for our investigation is the argumenta- 
assumes the truth of one of these propositions, tion system NAG (Nice Argument Generator) (Zuk- 
In the following sectiiSfi,~we " discuss r~l~ged-"~~: ...... ~erma~:~ec.~etl.,.~.1998i..Zukermar~,.et. aL~A-999:)..NAG ... . . .  
search. Next, we present an overview of NAG's argu- generates nice arguments, that is, arguments that 
are both normatively correct and persuasive for a ment generation process. We then describe our pro- 
cedure for proposing argumentation strategies, and target audience. To this effect, it tests the effects 
for generating and selecting specific arguments. Fi- of prospective arguments on two models: (1) a nor- 
nally, we illustrate the operation of our mechanism mative model, which represents NAG's beliefs, and 
with an example, discuss results from our prelimi- (2) a user model, which represents a user's presumed 
beliefs. Each model incorporates a Bayesian net- nary evaluation and present concluding remarks. work (BN) (Pearl, 1988) as its main representation 
2 Re la ted  Research  formalism (BNs were chosen because of their abil- 
ity to represent normatively correct reasoning un- 
A general introduction to hypothetical reasoning, in- der uncertainty). An argument is represented as an 
cluding a discussion of counterfactual reasoning and Argument Graph, which is a network of nodes that 
modality, may be found in (Rescher, 1964). The represent propositions, and links that represent the 
use of suppositions in hypothetical reasoning to cre- inferences connecting these propositions. This Ar- 
ate reductio ad absurdum arguments i described in gument Graph is obtained from the structural in- 
(Freeman, 1991), and their use in the analysis of tersection of relevant portions of the normative and 
such arguments is discussed in (Fisher, 1988). Fis- user BNs. By considering the Argument Graph rel- 
cher also illustrates how suppositions can lead to ar- ative to both models we are able to assess both its 
guments that explain observed outcomes, a weaker normative correctness and its persuasiveness. 
version of inference to the best explanation. Condi- NAG receives as input a goal proposition to be 
tional argumentation, a weaker form of reasoning by argued for, an initial argument context, and a tar- 
cases, where not all the cases must be examined and get range for the belief to be achieved in the goal 
the beginning of each case does not have to be proven (as a result of the argument) in the user model BN 
within the argument i self, is described in Freeman's and the normative model BN. Initially, the context is 
work. These works provide theoretical insights into composed of the goal proposition and salient propo- 
the field of dialectics. However, they do not present sitions and concepts mentioned in the preceding dis- 
implementable computational mechanisms, cussion. During argument generation, the context is 
In the area of discourse planning, few systems expanded to include the current Argument Graph. 
deal with the selection of argumentation strategies. Figure 2 shows the main modules of NAG (the 
Cerbah (1992) considers three discourse strategies: modules in double boxes contain the new argulnen- 
CausaIChain, which is a special case of our premise tation strategy mechanisms). After receiving a goal 
to goal strategy'; Parallel, which assigns a paral- proposition, the Strategist activates a sequence of 
lel structure to part of the text.; and Concessive. focusing-generation-analysis cycles as follows. First: 
These strategies reflect specific patterns of argumen- the Attentional Mechanism is invoked to focus on 
ration which may be incorporated in our higher level parts of the normative and user BNs that are likely 
strategies. Elhadad (1995) considers the use of at- to be useful in the argument. This is performed by 
gumentative features at several stages of the dis- spreading activation from the initial context. This 
course planning process, but none of his stages deals process generates an initial Argument Graph, and in 
with high-level argumentation.st.mtegies...Reed and . . tater~ cycles extends-the exist ing ArgumentGraph.  
Long (1997) use ordering heuristics to model the el- The Strategist hen calls the Generator to continue 
fect of presentation order on argument persnasive- the argument building process by finding additional 
ness, and Mareu (1996) considers the effect of vari- information to incorporate in the Argument Graph 
ous stylistic factors, including ordering and lexical (Zukerman et al, 1998). The extended Argument 
2The generalization of this strategy to N propositions re- Graph is returned to the Strategist,  which invokes 
quires the presentation of 2 \" cases: in the current implemen- the Analyzer to deternfine the beliefs in the uodes 
tation, only individual proposit.ions are considered, in the Argnnlent Graph under a variety of condi- 
56 
Argument 1 H Argu~ 
Generator | , ~....A.m,al,yzer 
- - ' - ' -~- ' -~  Argument ~ Argument 
? ~ Goal Analysis ~ a  . . . . . . .  ~",.....~A n aly sis 
Argument ~"'-,.,,,,P ro p osi t ion s / /  ~k~u-,y,~ ~ 
, Mechanism, 
Figure 2: System Architecture 
tions (Section 4.1). The Analyzer uses a constrained 
Bayesian propagation scheme on the normative and 
user BNs, limiting the updates to the subnetworks 
represented in the Argument Graph. For the pur- 
poses of Bayesian updating, propositions which are 
provided in the preamble are treated as "observa- 
tions"; that is, their degrees of belief are used as 
sources during Bayesian propagation. Based c,n the 
beliefs resulting from the Bayesian propagation, the 
Strategist determines which argumentation strate- 
gies are worth pursuing (Sections 4.2 and 4.3). If no 
strategy ields a nice enough argument, i.e., the be- 
lief in the goal is outside the target range in one or 
both models, the context is expanded, and another 
generation-analysis cycle is performed: the Strate- 
gist re-activates the focusing mechanism, followed 
by the re-activation of the Generator and then the 
Analyzer. This process iterates until a successful 
Argument Graph is built, or NAG is unable to con- 
tinue, e.g., because it failed to find further evidence. 
If one or more strategies yield a nice enough ar- 
gument, the Strategist selects one of the more con- 
cise arguments (Section 4.4). The corresponding Ar- 
gument Graph and an ordering of the nodes to be 
presented are then passed to the Presenter, which 
removes easily inferred propositions from the argu- 
ment. After each removal, the Presenter activates 
the Analyzer to check whether the argument remains 
nice enough, and the Attentional Mechanism to de- 
termine whether the argument can still be followed 
by the user. After the Presenter determines that no 
more propositions can be removed from the argu- 
ment, it extracts Bayesian reasoning patterns from 
the final Argument Graph and passes them to the in- 
terface, which renders the argument in English (Zuk- 
erman et al, 1999). 
This procedure is implemented by the following 
algorithm, which is executed by the Strategist) 
aA previous version of this procedure which generates only 
premise-to-goal arguments is described in (Zukerman et al, 
1998). In this paper, we focus on Steps 4 and 5, which have 
been modified to support the consideration of different argu- 
mentation strategies during the content planning process. 
Generat ion -Ana lys i s  A lgor i thm 
1. Perform spreading activation starting from the 
items in the current context. 
2. Identify new subgoals in the current Argument 
Graph. 
3. Pass the subgoals identified in Step 2 to the 
Generator, which adds to the current Argument 
Graph new information related to these sub- 
goals. 
4. Pass the Argument Graph generated in Step 3 
to the Analyzer for evaluation under different 
conditions. 
5. If (based on the Analyzer's report) some of the 
argumentation strategies eem promising then 
(a) Inspect specific arguments based on these 
strategies, and 
(b) Pass to the Presenter the portion of the Ar- 
gument Graph corresponding to a concise 
argument which achieves the intended be- 
lief in the goal. 
6. Otherwise, add to the current context new 
nodes that were connected to the goal or be- 
came salient during this cycle, and go to Step 1. 
4 Us ing  Argumentat ion  S t ra teg ies  
During the argument generation process, the Strat- 
egist performs the following actions: (1) determine 
the potential applicabihty of the different .argumen- 
tation strategies based on the beliefs in the nodes 
in the Argument Graph, (2) propose specific candi- 
dates for each apphcable.strategy, and. (3) select a 
concise argument among these candidates. 
4.1 Ant ic ipat ing  the effect  o f  a node  
The Strategist selects an argumentation strategy 
based on the Analyzer's assessment of the effect of 
the nodes in the Argmnent Graph on the goal propo- 
sition (and vice versa). This effect is determined by 
means of a constrained Bayesian propagation scheme 
57 
2" 
in both the user model BN and the normative model 
BN. Specifically, for each node a t  the '!edge" of the 
Argument Graph, each new node (i.e., one added 
in the last generation step), and each previous node 
to which new links were added in the last step, the 
Analyzer calculates its positive and negative ffect 
on the goal, and the positive and negative ffect of 
the goal on this node. 4 The positive~negative effect 
of a node X on a node Y is the hypothetical be- 
lief in node Y after propagating a high/low belief in 
node X (which represents a true/false belief in the 
corresponding proposition). TheTositive/negative 
effect of a node on the goal is required to generate 
arguments by cases, and the positive/negative effect 
of the goal on a node is required to generate hy- 
pothetical arguments, viz reductio ad absurdum and 
inference to the best explanation. When computing 
positive/negative effects for a particular node, the 
Bayesian propagation process uses the prior beliefs 
of the other nodes in the Argument Graph. 
4.2 Determin ing  app l i cab le  a rgumentat ion  
s t ra teg ies  
After receiving the Analyzer's report, the Strategist 
checks the following conditions to determine the po- 
tential applicability of each argumentation strategy. 5 
Reductio ad absurdum - The negation of the goal 
G undermines a proposition Q which is firmly 
believed independently of the goal (i.e., P(Q) = 
High, where Q is a premise or inferred from 
premises). Hence, P(QI~G) = Low (where Q is 
temporari ly treated as if it were not a premise, 
so that its value may change when the goal is 
negated). 
Inference to the best explanation - The assertion 
of the goal G supports a proposition Q which 
is firmly believed (i.e., P(Q) = High, where 
Q is a premise or inferred from premises), but 
which would be unexplained (improbable) with- 
out supposing the truth of the goal. Hence, 
whereas P(Q\[G) = High, in the absence of in- 
formation about G, the belief in Q is low (where 
Q is temporarily treated as if it were not a 
premise). 
Reasoning by cases (exclusive) - A proposition Q 
satisfies one of the following conditions: (1) it 
has-an indeterminate l vel of belief in both the 
normative and user models (i.e., its probability 
is within an interval \[0:5=t:O\]); or (2)it has highly 
4Previous nodes with new links are reconsidered because 
their effect on the goal node (and tile goal node's effect on 
them) is more likely to have changed ue to these links than 
the effects of nodes with an unchanged local topology. 
5For clarity of presentation, these conditions and the sub- 
sequent discussion assume apositive bias, i.e., the proposition 
under consideration is believed: for a negative bias some ex- 
pressions will be altered accordingly. 
divergent levels of belief in the user model and 
the normative model.,: For either condition, the 
belief in the goal must be high both when a high 
level of belief is ascribed to Q and when a low 
level of belief is ascribed. 
Reasoning by cases (non-exclusive) - There ex- 
ists a set of propositions {Q1,..-,Q,~}, each 
of which leads to a strong belief in the goal 
(i.e., P(GIQi ) = High for i = 1 , . . . ,n ) ,  and 
the disjunction of these propositions i strongly 
bel ieved (i.e., P(Vi  Qi) :--High) "6 . . . . . . . . .  
Premise to goal- This is the default strategy and 
requires only that given the current beliefs in 
the premises, the belief in the goal will be in 
the target range in both the normative and user 
BNs. 
Since the conditions for the reasoning by cases 
strategies consider nodes in the Argument Graph 
separately, they do not guarantee that all opportu- 
nities to argue by cases will be found. For instance, 
two particular nodes may not satisfy the conditions 
for the exclusive strategy when considered separately 
(because when a node is ascribed a high or low level 
of belief, the prior beliefs of the other nodes are used 
for Bayesian propagation). However, when consid- 
ered jointly, the four permutations ofextreme beliefs 
in these nodes, viz high-high, high-low, low-high and 
low-low, may satisfy the applicability conditions of 
the exclusive strategy. At present, these opportuni- 
ties are missed by NAG. However, this may be an 
appropriate outcome, since such complex arguments 
by cases are quite rare. 
4.3 P ropos ing  specif ic a rguments  for each 
s t ra tegy  
In this step, the Strategist considers the propositions 
or sets of propositions that satisfy the conditions for 
each applicable argumentation strategy, and gener- 
ates a specific argument based on each of these prop- 
ositions (or sets of propositions). This is done as 
follows for each argumentation strategy. 
Reduct io  ad absurdum and In ference to the  
best  exp lanat ion .  For each proposition Q which 
satisfies the conditions for reductio ad absurdum, the 
Strategist extracts from the Argument Graph the 
? subgraph whicbcorresponds to the line of reasoning 
going from the goal node (which was ascribed a low 
level of belief) to Q (which has been contradicted 
6This situation may be generalized so that any Qi consists 
of a subset of propositions which lead to the goal. However, 
in the current implementation, each Qi consists of one propo- 
sition only. Further, owing to practicality considerations, at
present NAG implements a limited version of the applicability 
conditions for the non-exclusive strategy whereby only pairs 
of nodes that are relatively close to tile goal and to observable 
nodes are inspected. This last requirement is necessary inor- 
der to determine which combinations of beliefs are possible 
for the inspected pairs of nodes. 
58 
as a result of this line of reasoning). Each line of When choosing its final argument, the Strate- 
reasoning is obtained by  treating themegation of;the: .: ~. gis.t considers;only~=ice arguments, i.e., those that 
goal as a premise and ~Q as a goal. 
A similar process is applied for the inference to the 
best explanation strategy, but the goal is ascribed a 
high level of belief, and Q is expected to achieve a 
high level of belief as a result of the argument. In 
general, when using the reductio ad absurdum strat- 
egy, people identify only one target proposition to 
be contradicted when the goal is negated. In con- 
trast, for inference to the best explanation, the goal 
is often used to explain several propositions. In the 
current implementation, only one target proposition 
is being considered for both strategies. 
Reason ing  by cases (exclus ive) .  If proposition 
Q satisfies the conditions for the exclusive strategy, 
then a copy of the Argument Graph is made for the 
case where a high belief is ascribed to Q and another 
copy is made for the case where a low belief is as- 
cribed to Q. Both copies have the same structure, 
but the propagated values are different. The argu- 
ment by cases consists of a pair of Argument Graphs, 
one graph for each case. These graphs do not re- 
quire further analysis, since the results of propagat- 
ing these beliefs through the Argument Graph were 
previously returned by the Analyzer (Section 4.1), 
and according to the applicability conditions for the 
exclusive strategy, the argument for each case is suf- 
ficiently nice. 
Reasoning by cases (non-exc lus ive) .  If a set 
of nodes {Q1,---,  Qn } satisfies the applicability con- 
ditions of the non-exclusive strategy, an Argument 
Graph is generated for each of the n cases by ascrib- 
ing a high level of belief to each Qi in turn (the rest of 
the nodes retain their existing degrees of belief). If 
the Analyzer reports that the argument correspond- 
ing to each graph is sufficiently nice, an argument by 
cases is constructed by listing each graph in turn. 
Premise to goal. Finally, the Strategist considers 
a premise to goal argument by inspecting the belief 
in the goal in both the normative and user models 
after propagation from the premises (this belief was 
computed by the Analyzer). If the argument is nice 
enough, then it is retained as a possible candidate. 
If upon completion of this process, none of these 
argumentation strategies has yielded a nice enough 
argument, the reasoning context is updated with 
nodes that were connected to the goal or became 
salient during the current cycle. The Strategist then 
re-invokes the spreading activation process, and re- 
activates the Generator to expand the Argument 
Graph (Section 3). After expansion, the analysis 
and strategy proposal processes are repeated. If one 
or more candidate argunmnts were generated, the 
Strategist selects a concise argument as described in 
the next section. 
achieve a degree of belief in the goal which lies in- 
side the target range in both the user model and the 
normative model. However, we do not have a direct 
means for determining the belief in the goal in the 
user model as a result of a hypothetical rgument or 
an argument by cases. This is because the rhetori- 
cal force of these strategies affects the user's beliefs 
in a manner that deviates from the effect modeled 
by means of  Bayesian propagation, as illustrated by 
the sample arguments in Section 5. The problem of 
incorporating a model of the rhetorical force of an 
argument into a Bayesian propagation scheme is yet 
to be addressed. Nonetheless, in order to test the op- 
eration of our mechanism, we currently approximate 
the effect of an argument (regardless of its strategy) 
on the user's beliefs by performing Bayesian propa- 
gation in the user model BN. In the future, as a first 
step in modeling rhetorical factors, we intend to in- 
vestigate how the beliefs in our user models deviate 
from users' actual (reported) beliefs. 
4.4 Selecting a concise argument 
Here the Strategist removes long arguments, o that 
a final selection is made among (shorter) arguments 
of similar length. 7 NAG does not simply select the 
most concise argument, because as shown in Sec- 
tion 6, the choice of strategy has a greater influence 
on the addressee's beliefs than any (small) remaining 
differences in argument length. 
The Strategist initially performs coarse pruning 
on the Argument Graphs that were generated by the 
premise to goal or reasoning by cases trategies. This 
coarse-grained pruning examines eparately the im- 
pact of each individual line of reasoning contribut- 
ing to the belief in the goal, removing entire lines 
that are not strictly necessary to achieve a belief in 
the goal that falls inside the target range (the ar- 
guments generated using the reductio ad absurdum 
and inference to the best explanation strategies are 
not coarsely pruned, since those arguments already 
comprise a single line of reasoning). Sometimes, the 
impact of certain lines of reasoning cannot, be as- 
sessed in isolation, since two or more lines may con- 
tribute jointly towards the belief in a proposition in a 
mutually dependent manner. Often however, some 
of the contributing lines of reasoning are indepen- 
dent or nearly so, and coarse pruning can proceed. 
Next, the Strategist drops from consideration the 
arguments that a re  significa~ntly longer than the 
shortest argument (where length is measured in 
number of nodes),8 and selects one of the remaining 
7Other factors, such as the structural complexity of the 
arguments, will be considered in the future. 
SAlthough an Argument Graph is further pruned before 
presenting its corresponding argument to the user (Section 3), 
it is reasonable toconsider lhe length of each candidate graph 
59 
(A benevolent and omnipotent) God exists j< , , \  . . . . .  
God is benevolent God is omnipotent 
(2) (3) 
1 1 
God wants to prevent evil God can prevent evil 
(4) (5) 
S 
? There- is evit~in'the~world . . . . . .  - ' 
(6) 
Figure 3: Argument Graph for the Problem of Evil 
arguments according to the following order of prefer- 
ence: reasoning by cases, premise to goal, inference 
to the best explanation and reductio ad absurdum. 
This ordering is consistent with the results of our 
evaluation (Section 6). 
5 Example  - The  Prob lem o f  Ev i l  
We now illustrate our argumentation mechanism 
with "The Problem of Evil". Given a preamble 
that establishes that there is evil in the world, and 
the goal to prove that there is no God, NAG ob- 
tains the Argument Graph in Figure 3 after one 
focusing-generation cycle, and produces the Argu- 
ment Graphs corresponding to the arguments in Fig- 
ure 4 (the adverbs that indicate level of belief and 
the conjunctive xpressions are italicized in the ar- 
guments for ease of comparison).9 These arguments 
are based on a definition of God that requires God 
to be both omnipotent and benevolent. 
P remise  to goal. Bayesian propagation of the be- 
lief in node 6 results in the denial of the combination 
of nodes 4 and 5, but yields a moderate probability 
for each of these nodes and for their respective par- 
ents, node 2 and node 3. Still, the probability of 
node 1 is quite low (i.e., there is a high belief in its 
negation). 
Reduct io  ad absurdum.  The conditions for re- 
ductio ad absurdum are also met by this Argument 
Graph. That is, the negation of the goal undermines 
the belief in the premise (the existence of evil). 
Reason ing  by cases (exclusive) .  The condi- 
tions for exclusive reasoning by cases are met by 
both node 4 and node 5, since they obtain mid- 
dling degrees of belief duringpropagation. We 'illus- ' - 
trate here only the argument which hinges on node 4 
(the argument which hinges on node 5 is symmetri- 
cal). The two cases in the generated argument are: 
at this stage, because it is indicative of the length of the 
argument  obtained after finer pruning. 
9The English versions of these arguments  were hand gen- 
erated from NAG's output.  
node 4 is true or node 4 is false. The case which 
......... assumes.:the.negation.ofr~ode 4 leads:to a.straight~ 
forward argument hat achieves the goal. The case 
which asserts node 4 achieves the goal through an 
explain away relationship which involves nodes 4, 5 
and 6 (Pearl, 1988). This relationship requires that ? 
P(61-~4) > P(6) and P(61-~5) > P(6), which means 
that the negation of nodes 4 and 5 are potential ex- 
planations for node 6, and that P(416 & 5) < P(416) 
and P(516 & 4) < P(516), which means that given 
node 6, node 5 explains away node 4 and vice versa 
(Zukerman et al; "'1999)':" -'Tl~at is~ ~ ~'sei~ir/g':'the " 
proposition in node 5 in light of node 6 greatly weak- 
ens the belief in node 4. 
Reason ing  by cases (non-exc lus ive) .  The 
Strategist identifies nodes 4 and 5 as possible sources 
for a non-exclusive argument by cases, since the 
negation of each of these nodes leads to a strong 
belief in the goal, and P(-~4 V-~5) is high (because 
of their relation to node 6). The cases in the gen- 
erated argument are: node 4 is false or node 5 is 
false. 
Since all these arguments are nice, the Strategist 
retains all of them for further processing. As stated 
in Section 4.4, the arguments that are substantially 
longer than the shortest argument (in number of 
nodes) are dropped from consideration. In our ex- 
ample, the premise to goal argument is the short- 
est, as it threads a path through the 6 nodes in the 
Argument Graph; the exclusive reasoning by cases 
argument is the longest, requiring 9 nodes (3 for 
the case where node 4 is false, 5 for the case where 
node 4 is true, and 1 for stating the conclusion); the 
non-exclusive reasoning by cases argument requires 
8 nodes (3 for each case, 1 for node 6, which in- 
troduces the cases, and 1 for the conclusion); and 
the reductio ad absurdum argument requires 7 nodes 
(the 6 nodes in the Argument Graph plus 1 node for 
stating the conclusion). The exclusive reasoning by 
cases argument is dropped from consideration since 
it is 1.5 times longer than the shortest argument, 
and the non-exclusive argument is select.ed among 
the remaining arguments by applying our preference 
ordering. 
6 P re l iminary  Eva luat ion  
In order to determine the users' preferences Yor dif- 
ferent argumentation strategies, we performed a pre- 
liminary evaluation where we presented two sets 
of arguments to subjects.  One set contained the 
premise to goal and reasoning by cases arguments 
for the problem of evil shown in Figure 4. The sec- 
ond set contained a preamble which presented some 
background information, and a premise to goal, a 
reductio ad absurdum and an inference to the best 
explanation argument for a large asteroid striking 
Earth 65 million years BC (Figure 5). The argu- 
60 
Premise to goal: I 
"Evil in the world (6) implies.that God may not want to prevent evil (-~4) and that GQd:maynot ........ I 
be able to preveiat- egil (-45). God possibly not wantingto prevent evil (-~)t) im~liesthdl~ God ?ndb, 
not be benevolent (-~2). God possibly not being able to prevent evil (--,5) implies that God may 
not be omnipotent (-~3). The fact that God may not be benevolent (-~2) and ,that God may not 
be omnipotent (-~3) implies that it is very likely that God does not exist (-,1).' 
Reductio ad absurdum: 
"Assume that God exists (1). This implies that God is benevolent (2) and that God is omnipotent 
(3). God being benevolent (2) implies that God wants to prevent evil (4). God being omnipotent 
(3) implies that God can prevent evil (5).  The fact that Godwants  to prevent evil (4) and that 
God can prevent evil (5) implies that there is no evil in the world (-~6). But there is evil in the 
world (6): ,Therefore~.Go:d:doesmot exist." 
Reasoning by cases  (exclusive): 
"Consider the following cases: God wants to prevent evil (4), and God does not want to prevent 
evil (--14). 
God wants to prevent evil (4). This, together with the existence of evil (6) implies that God is 
not able to prevent evil (~5), which in turn implies that God is not omnipotent (-~3). This implies 
that God does not exist (~1). 
God does not want to prevent evil (~4). This implies that God is not benevolent (-~2), which in 
turn implies that God does not exist (~1). 
Either way, God does not exist (-~1)." 
Reasoning by cases (non-exclusive}: 
"Since there is evil in the world (6), God does not want to prevent evil (-~4) or God cannot prevent 
evil (-~5). 
God does not want to prevent evil (-,4). This implies that God is not benevolent (~2), which in 
turn implies that God does not exist (~1). 
God cannot prevent evil (-~5). This implies that God is not omnipotent (-~3), which in turn implies 
that God does not exist (-11). 
Either way, God does not exist (--1)." 
Figure 4: Arguments 
ments in each set were presented in two different 
orders. 40 subjects read the 'problem of evil' ar- 
guments, and 35 the 'asteroid' arguments. In the 
former set, the distribution of preferences was uni- 
form among the three strategies. In the latter set, 
premise to goal was preferred, followed by inference 
to the best explanation and then reductio ad absur- 
dum (these results, which were not affected by the 
order of presentation, were supported by X 2 tests 
which were significant at the 0.01 level). 
At first glance it appears that premise to goal 
is the preferred argmnentation strategy. However, 
the participants' comments indicate that further ex- 
periments are required to determine the conditions 
under which different argumentation strategies are 
appropriate. For exarnple,several participants in: 
dicated that reductio ad absurdum arguments are 
appropriate when the ensuing contradiction is com- 
pelling, which tile3" did not find to be the case in the 
asteroid example. Further. they stated that they 
liked the premise to goal argument because it con- 
tained inorc information than the other argunmnts 
(which have one line of reasoning only). However. 
for the Problem of Evil 
this additional information may be less appealing 
for arguments that are longer than one paragraph. 
7 Conc lus ion  
We have offered an operational definition of the 
conditions for pursuing three types of argumenta- 
tion strategies: hypothetical, reasoning by cases and 
premise to goal. We have also presented a mecha- 
nism that proposes applicable argumentation strate- 
gies based on these conditions, and generates specific 
arguments based on these strategies. This mecha- 
nism has been implemented in a Bayesian argument- 
generation system. Our evaluation also brings to 
notice tile need to investigate additional aspects of 
argumentation strategies. 
8 ? ~ :Acknowledgments  
This work was supported in part bv Australian Re- 
search Council grant A49531227. 
References  
Cerbah, F. (1992). Generating causal explana- 
tions: From qualitative models to natural lan- 
guage texts. In ECAI92 - Proceedings of the 
61 
Preamble :  
"Approximately 65 million years BC the dinosaurs, large reptiles that dominated the Earth for 
many millions ofwears,~becameextinct. At aboutthe sametime,-the-nnmber:of giant Sequoias in 
California greatly increased." 
P r ~  to goaT: 
"65 million years ago, dinosaurs became xtinctand giant sequoias proliferated. These events may 
have been caused by a cooling of the Earth, which in turn may have been caused by material 
obstructing the sun. 
Lots of 65 million year old iridium deposits have been found. This may have been caused by 
widespread iridium being deposited 65 million years ago, which together with the material ob- 
structing the sun may have been caused by an explosion which threw up material. This explosion 
may have been:caused.b~.a tar.ge.iridium~rich~astexoid~.~trikiag~.E~.th~65,,million.~years BC.'~ ... ...... 
\ [ ' ~ r o i d  had not struck Earth 65 million years BC, there wouldn't be a 
I large explosion that up throws material. Therefore, iridium would not have spread around, and 
I widespread 65 million year old iridium deposits would not have been found. 
I However, widespread 65 million year old iridium deposits were found. Therefore, a large iridium- 
I rich asteroid struck Earth about 65 million years BC." 
In fe rence  to the  best  exp lanat ion :  
"If an iridium-rich asteroid had struck Earth 65 million years BC, there would have been an 
explosion that throws up material, hence widespread iridium would have been deposited. Therefore, 
lots of 65 million year old iridium deposits would be found. 
Since widespread 65 million year old iridium deposits were found, then an iridium-rich asteroid 
struck Earth 65 million years BC." 
Figure 5: Arguments for an Asteroid Striking Earth 65 Million Years BC 
Tenth European Conference on Artificial Intelli- 
gence, pages 490-494, Vienna, Austria. 
Elhadad, M. (1995). Using argumentation i text 
generation. Journal of Pragmatics, 24:189-200. 
Fisher, A. (1988). The logic of real arguments. Cam- 
bridge University Press, Cambridge: England. 
Freeman, J. (1991). Dialectics and the macrostruc- 
ture of arguments: a theory of argument structure. 
Foris Publications, Berlin. 
Lipton, P. (1991). Inference to the best explanation. 
Routledge, London; New York. 
Malthus, T. (1798). Essay on the Principle of Pop- 
ulation as it affects the Future Improvement of 
Society with Remarks on the Speculations of Mr. 
Godwin, Mr. Condorcet and other Writers. 
Marcu, D. (1996). The conceptual and linguistic 
facets of persuasive arguments. In Proceedings of 
ECAI-96 Workshop - Gaps and Bridges: New Di- 
rections in Planning and NLG, pages 43-46, Bu- 
dapest, Hungary. 
Pearl, J. (1988). ~ Probabilistic .Reasoning in Intelli- 
gent Systems. Morgan Kauflnann Publishers, San 
Mateo, California. 
Reed, C. and Long, D. (1997). Content ordering 
in the generation of persuasive discourse. In IJ- 
CAI97 - Proceedings of the Fifteenth Interna- 
tional Joint Conference on Artificial Intelligence. 
pages 1022-1027, Nagoya, Japan. 
Rescher, N. (1964). Hypothetical reasoning. North 
Holland, Amsterdam. 
Zukerman, I., McConachy, R., and Korb, K. B. 
(1998). Bayesian reasoning in an abductive mech- 
anism for argument generation and analysis. In 
AAAI98 - Proceedings of the Fifteenth National 
Conference on Artificial Intelligence, pages 833- 
838, Madison, Wisconsin. 
Zukerman, I., McConachy, R., Korb, K. B., and 
Pickett, D. A. (1999). Exploratory interaction 
with a Bayesian argumentation system. In IJ- 
CAI99 - Proceedings of the Sixteenth Interna- 
tional Joint Conference on Artificial Intelligence, 
pages 1294-1299, Stockholm, Sweden. 
62 
