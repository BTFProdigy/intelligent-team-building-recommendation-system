Proceedings of SIGDIAL 2009: the 10th Annual Meeting of the Special Interest Group in Discourse and Dialogue, pages 160?169,
Queen Mary University of London, September 2009. c?2009 Association for Computational Linguistics
Speeding Up the Design of Dialogue Applications by Using Database 
Contents and Structure Information 
L. F. D?Haro, R. Cordoba, J. M. Lucas, R. Barra-Chicote, R. San-Segundo 
Speech Technology Group 
Dept. of Electronic Engineering 
Universidad Polit?cnica de Madrid, Spain 
{lfdharo, cordoba, juanmak, barra, lapiz}@die.upm.es 
 
Abstract 
Nowadays, most commercial and research 
dialogue applications for call centers are 
created using sophisticated and fully-
feature development platforms. Surpris-
ingly, most of them lack of some kind of 
acceleration strategy based on an automatic 
analysis of the contents or structure of the 
backend database. This paper describes our 
efforts to incorporate this kind of informa-
tion which continues the work done in 
(D?Haro et al 2006). Our main proposed 
strategies are: the generation of automatic 
state proposals for defining the dialogue 
flow network, the automatic selection of 
slots to be requested using mixed-initiative, 
the semi-automatic generation of SQL 
statements, and the quick generation of the 
data model of the application and the con-
nection with the database fields. Subjective 
and objective evaluations demonstrate the 
advantages of using the accelerations and 
their high acceptance, both in our current 
proposals and in previous work. 
1 Introduction 
Currently, the growing demand of automatic dia-
logue services for different domains, user profiles, 
and languages has led to the development of a 
large number of sophisticated commercial and re-
search platforms that provide all the necessary 
components for designing, executing, deploying 
and maintaining such services with minimum ef-
fort and with innovative functions that make them 
interesting for developers and final users. 
In their effort for accelerating the design, most 
commercial platforms provide several high-level 
tools to build multimodal and multilingual dia-
logue applications using widespread standards 
such as VoiceXML, CCXML, J2EE, RCP, SRGS, 
etc. In addition, they include state-of-the-art mod-
ules such as speech recognizers, high quality 
speech synthesizers, language identification capa-
bilities, etc., that guarantees user satisfaction and 
interaction. In addition, they present a very user-
friendly graphical interface that makes easy the 
development of very complex dialogues, besides 
the incorporation of predefined libraries for typical 
dialogues states such as requesting card or social 
security numbers, etc., and additional assistants for 
debugging, logging and simulate the service. 
In contrast to commercial platforms, research or 
academic platforms (e.g. CSLU-RAD 1 , Dialog-
Designer2, Olympus3, Trindi-kit 4, etc.) do not nec-
essary incorporate all the above-mentioned fea-
tures; especially because they are limited to the 
number of standards that they are able to handle 
and to the integration level with other platforms, as 
well as the number of capabilities that they can 
offer to the users and programmers. However, they 
allow more complex dialogue interactions, most of 
them are freely available as open source, and using 
third party modules it is possible to extend their 
functionalities. 
Surprisingly, these platforms do not include any 
kind of acceleration strategies based on the con-
tents or in the structure of the backend database 
that, as we will show, can provide important in-
formation for the design. Next, we will describe 
some examples of applications or dialogue systems 
that use data mining techniques or heuristic infor-
                                                 
1 http://cslu.cse.ogi.edu/toolkit/  
2 http://spokendialogue.dk/  
3 http://www.ravenclaw-olympus.org/  
4 http://www.ling.gu.se/projekt/trindi/trindikit/ 
160
mation extracted from the database contents in or-
der to create automatic dialogue services. 
In (Polifroni and Walker, 2006), different data 
mining techniques are used to automate the selec-
tion of content data to be used in system initiative 
queries and to provide summarized answers. At 
runtime, the system automatically selects the at-
tributes to constrain the prompt queries that narrow 
down best the interaction flow with the final users. 
In (Chung, 2004), the database is used together 
with a simulation system in order to generate thou-
sands of unique dialogues that can be used to train 
the speech recognizer and the understanding mod-
ule, as well as to diagnose the system behaviour 
against problematic user?s interactions or answers. 
In (Pargellis et al 2004), a complete platform to 
build voice services where the database contents 
change constantly is described. At runtime, the 
system retrieves information that the user is inter-
ested in according to his personal profile. In addi-
tion, the system is able to create automatically dy-
namic speech grammars and prompts, as well as 
the dialogue flow for presenting information to the 
user, or for solving some interaction errors through 
predefined dialogue templates. 
Finally, (Feng et al 2003) proposes a very dif-
ferent approach, not using a database but mining 
the content of corporate websites for automatically 
creating spoken and text-based dialogue applica-
tions for custom care. Although the dialogue flow 
is predefined, it is interesting to see that important 
knowledge, for the different modules of the dia-
logue system, can also be extracted and used from 
a well-designed content. 
In this work, we have solved some of the limita-
tions of current platforms by incorporating suc-
cessfully heuristic information into the different 
assistants of the platform and allowing them to 
collaborate between each other in several ways, as 
they collect the information already provided in the 
first stages of the design to improve and accelerate 
the design in the last stages. This way, the platform 
assistants classify which fields of the database 
could be relevant for the design, generate different 
kinds of automatic proposals according to the de-
sign step, reduce the information displayed to the 
designer, and accelerate different typical proce-
dures required to define the application. 
The paper is organized as follows: section 2 
provides an overview of the overall architecture of 
the platform, including a brief description of the 
main assistants and layers that makes it up. Section 
3 describes previous accelerations in the platform 
related with the current work; Sections 4, 5, and 6 
describe in detail the new strategies and the assis-
tants that include them. Section 7 describes the 
subjective and objective evaluations, and section 8 
outlines some conclusions and future work. 
2 Platform Architecture 
The Application Generation Platform (AGP), cre-
ated during the European project GEMINI, is an 
open and modular architecture made up of differ-
ent assistants and tools that simplifies the genera-
tion of multimodal and multilingual dialogue ap-
plications with a high adaptability to different 
kinds of services (see Figure 4 in Appendix A). 
The platform consists of three main layers inte-
grated into a common graphical user interface 
(GUI) that guides the designer step-by-step and 
lets him go back and forth.  
In the first layer, called Framework Layer, the 
designer specifies global aspects related to the ap-
plication and the data. This layer includes the Data 
Model Assistant (DMA), where the database struc-
ture is created, and the Data Connector Model As-
sistant (DCMA), where the application specific 
database access functions are created.  
The next layer, called Retrieval layer, includes 
the State Flow Model Assistant (SFMA) and the 
Retrieval Model Assistant (RMA). The former is 
used to create the dialogue flow at an abstract 
level, by specifying the high-level states of the dia-
logue, plus the slots to ask to the user and the tran-
sitions among states. Then, the later is used to in-
clude all the actions (e.g., variables, loops, if-
conditions, math or string operations, conditions 
for making transitions between states, calls to dia-
logs to provide/obtain information to/from the 
user) to be done in each state defined previously.  
Finally, the third layer, called Dialogs Layer, 
contains the assistants that complete the general 
flow specifying for each dialogue the details that 
are modality and language dependent. For instance, 
the prompts and grammars for each language and 
modality, the definition of user profiles, the ap-
pearance and contents of the Web pages, the error 
treatment for speech recognition errors or Internet 
access, the presentation of information on screen or 
using speech, etc., are defined. Furthermore, the 
161
VoiceXML and xHTML scripts used by the real-
time system are automatically generated. 
3 Previous Acceleration Strategies 
In (D?Haro et al 2006) and (D?Haro et al 2004), 
we described several acceleration strategies based 
on using the data model structure and applied them 
successfully to different assistants of the platform, 
with a special emphasis in the assistant for defining 
the actions to be done in each dialogue (i.e. RMA). 
The data model information was used to:  
a.) Create configurable and generic dialogue 
proposals for obtaining (called DGet) and for 
showing (called DSay) information from/to the 
user. In this case, the assistant creates a DGet or 
DSay dialogue for each class and attribute defined. 
b.) Automatically propose the actions required 
for completing the information for each state of the 
dialogue flow; basically, the assistant proposes the 
dialogues to ask information to the user, the data-
base access functions, and the dialogues to show 
information to the user. Figure 1 shows an example 
of the proposals for a banking application. In this 
example, the designer is editing a dialogue where 
given a currency name the system provides its spe-
cific information (buy and sell price, general in-
formation, etc.). Using the proposal window, all 
the designer would need to do is to select the cor-
responding DGet in the window 
(DGet_CurrencyName_IN_CLASS_Currency), 
then the database access function GetCurrencyBy-
Name, and finally the DSays that provide the de-
sired attributes from the currency. In order to pro-
vide these proposals, we use the information of the 
relationships between slots and arguments of the 
database functions and the attributes and classes in 
the data model (section 5 and 6). When there is no 
relationship specified, we apply relaxed filters such 
as matching in types, similarity in names, or same 
number of arguments and slots in the state. 
c.) Automate the process of passing information 
among actions/dialogues by proposing the vari-
ables that best match the connections or allowing 
the creation of new variables when no match ex-
ists. This is a critical aspect of dialogue applica-
tions design. Several actions and states have to be 
?connected? as they use the information from the 
preceding dialogues. In general, most current de-
sign platforms allow the same kind of functional-
ity, offering the user a selectable list of all the 
available variables in the dialogue. In other cases, 
especially considering the connections with data-
base access functions, some platforms only allow 
the designer to define the matching by modifying 
by hand the script code. In this acceleration, we 
have tried to provide a better solution by automat-
ing the connection through automatic proposals. 
The assistant detects the input/output variables re-
quired in each action and offers the most suitable 
already defined variable of a compatible type; if 
there are more than one variable to show, the assis-
tant sorts them according to the name similarity 
between variable and dialogue. If there is no com-
patible variable already defined in the system or 
the name proposed is not desired, the assistant al-
lows the creation of a new local/global variable. 
Additionally, the assistant includes a window 
where all this matching can be edited. 
Other accelerations included in this assistant 
were the quick creation of mixed-initiative dia-
logues, dialogues with over-answering (that do not 
exist in any current dialogue platform) and the 
quick definition of dialogue variables. 
 
 
Figure 1. Example of automatic dialogues and 
database access function proposals 
In the present work, the new accelerations addi-
tionally exploit the database contents and have 
been incorporated into the assistant to define the 
data model structure (section 4), into the assistant 
for defining the database access functions (section 
162
5), and into the assistant to define the states of the 
dialogue flow (section 6). The next sections de-
scribe in detail these assistants and accelerations. 
4 Strategies Applied to the Data Model 
Assistant (DMA) 
This assistant helps in the creation of the data 
model structure of the service through a visual rep-
resentation of all possible fields to be requested 
and presented to the user, which consists of object 
oriented classes and attributes. The goal with these 
classes and attributes is to provide information to 
the next assistants in the platform about which 
fields in the database are relevant for the service 
and the relationships between tables and fields. 
 
 
Figure 2. Example of class and attributes 
Each class, see Figure 2, can be characterized by 
a list of attributes, a description, and optionally a 
list of base classes (inheriting their attributes). The 
attributes may be: a) of atomic types (e.g., string, 
Boolean, float, date, etc., e.g., AvailableBalance), 
b) complex objects, obtained by embedding or re-
ferring to an existing class (e.g., AccountHolder), 
or c) lists of either atomic type items or complex 
objects (e.g., LastTransactionList). 
The main acceleration strategies, previously in-
cluded in this assistant, are: a) re-utilization of li-
braries with models created beforehand, which can 
be copied totally or partially, or used to create a 
new class by mixing them, b) automatic creation of 
a class when it is referenced as an attribute inside 
another one, and c) definition of classes inheriting 
the attributes of a base class. Since this is one of 
the first assistants of the platform, a significant 
effort was done to accelerate the creation of the 
database structure and to include information about 
the relationships between the class attributes and 
the fields and tables in the database. To start with, 
the system generates and analyzes automatically 
heuristic information from the database contents. 
Then, with this information, the system proposes 
full custom classes and attributes that the designer 
can use when creating the data structure. 
4.1 Extraction of heuristic information 
The process is done using an open SQL query to 
retrieve information of every table, field and record 
in the database. This information includes the 
name and number of the tables and fields, and the 
number of records for every table. In addition, the 
following features for each field are also gener-
ated: a) field type, b) average length, c) number of 
empty records, d) language dependent fields, and 
e) the proportion of records that are different. This 
information is shared among the assistants in order 
to simplify the design or to improve the presenta-
tion of information in the posterior assistants. For 
instance, they are used for: (a) to accelerate the 
creation of the data model structure (section 4.2), 
(b) and (e) to unify slots as mixed initiative or not 
(see section 6.1), (c) to sort by relevance the attrib-
utes displayed by the wizard when creating the da-
tabase structure (section 4.2), and (d) to not gener-
ate states for these fields in the SFMA since the 
dialogue flow in this assistant is language inde-
pendent (section 6.1). 
An important issue we found when retrieving 
the field type was that sometimes the metadata in-
formation provided by the SQL function was in-
correct due to: a) the driver for accessing the data-
base was only able to return a limited number of 
types, e.g. Boolean or dates were mapped as inte-
ger or string types respectively, b) the designer of 
the database defined the field using a generic type 
such as string or float when the visual inspection of 
the records showed that they actually corresponded 
to dates or integers, c) there were problems to map 
special types such as hyperlinks, binary, etc. into 
the types supported by our platform. 
In order to solve these problems, we imple-
mented a post-processing step based on using regu-
lar expressions to detect the following types: inte-
gers, floats, dates, strings, Boolean, mixed or 
empty fields. In general, the process is to analyze 
all non-empty records in a given field and to select 
as field type the one with more than the 90% of 
occurrences. Exceptions to this rule are: a) a nu-
meric field is considered integer if all its records 
are classified as such, if not it is classified as float, 
163
b) the empty type is assigned to fields with more 
than 95% of empty records. 
In order to analyse the performance of the post-
processing step, an objective evaluation was car-
ried out. In this evaluation, twenty-one databases, 
most of them available online, were retrieved and 
visually inspected field by field. In total, there 
were 109 tables (an average of 5 tables per data-
base), 767 fields and 610.506 records, which were 
classified by a human evaluator. 
In our results, the average recognition was 
89.6%, obtaining the best rates for dates, strings, 
and numeric quantities, which are the most com-
mon types in most databases. Analyzing in detail 
the misrecognitions, 0.9% of floats were incor-
rectly detected as integers due to values such as 
2.0, 30.0, etc. which were automatically returned 
by the database driver without the decimal part. 
Another source of errors was detecting some nu-
meric quantities due to special symbols such as 
dashes, percentages, or the euro symbol, which 
were incorrectly interpreted as a string type (3.3% 
and 1.6%). The major problems occurred for dis-
tinguishing between the String type and what we 
called Mixed type (i.e. fields containing: URLs, 
emails, long strings, etc.) since they are, in prac-
tice, the same. However, we wanted to separate 
them since for a speech recognizer they may be 
handled using different strategies (e.g. spelling, 
general grammars, etc.). The importance of these 
results is that they mean a reduction in the number 
of times the designer will need to change the pro-
posed type for a given attribute when creating the 
classes (section 4.2). 
4.2 Semi-automatic classes proposals 
After collecting all the heuristics, the assistant pro-
vides a wizard window that allows the designer to 
create the attributes for a new class from the tables 
and fields of the database or from already existing 
classes in the model. The information of the se-
lected field and table is saved in the definition of 
the class attribute allowing future assistants in the 
platform to access this information easily (section 
5.1 and 6.1). The heuristic information is used to 
set automatically the field types in the wizard, al-
though it can be edited by the designer. Besides, 
the wizard also proposes automatic alternative 
names for the new class and attributes when it de-
tects duplicated names with already defined ones. 
Finally, if the number of tables in the database is 
too high the designer can select those that will be 
really needed during the design, this way reducing 
the information displayed on the screen. In addi-
tion, it is also possible to customize the name of 
the tables in the database in order to make them 
more intuitive to the dialogue designer. 
5 Strategies Applied to the Data Connec-
tor Model Assistant (DCMA) 
This assistant allows the definition of the proto-
types (i.e. only the input and output parameters) of 
the database access functions used in the runtime 
system. The advantage of using prototypes is that 
their actual implementation is not required during 
the design of the dialogue flow. 
The main acceleration strategy, previously in-
cluded in this assistant, was the possibility of relat-
ing the input/output arguments to the attributes and 
classes of the data model. This information is used 
by the retrieval model assistant to create dialogue 
proposals and to automatically propose database 
access functions for a given dialogue in the design 
(section 3). In this work, we have introduced a new 
acceleration by incorporating a wizard window that 
allows the creation and debugging of the SQL 
statements used at run-time. 
5.1 Semi-automatic generation of SQL que-
ries 
The main motivation behind this wizard window 
was to simplify the process of creating the function 
prototypes (API), reducing the necessity of learn-
ing a new programming language (SQL), and to 
simplify the process of adding the query into the 
real-time modules and scripts. The new wizard 
semi-automatically creates the SQL statements for 
the given prototype and provides a pre-view of the 
results that the system would retrieve in the real-
time system (see Figure 5 in Appendix A). This 
new acceleration is interesting since currently few 
development platforms include such kind of wizard 
forcing the designer to use third party software. 
Besides, current wizards only provide debugging 
tools, nice GUI features or support for many DB 
standards, but no automatic query proposals. 
In order to automatically create the SQL state-
ment, the assistant uses the input arguments (de-
fined in the function prototype) as constraints for 
the WHERE clause, and the information of the 
164
output arguments as returned fields for the SE-
LECT clause. The assistant allows the inclusion of 
new input or output arguments if the function pro-
totype is not complete or if the designer wants to 
test new combinations of arguments. 
Finally, the wizard allows the designer to pre-
view the records that the proposed SQL statement 
will retrieve at real-time. In order to debug the 
query, the designer specifies, using a pop-up win-
dow, the values for the input arguments of the 
function to test the query (as acceleration, the wiz-
ard automatically proposes real values retrieved 
from the database). 
6 Strategies Applied to the State Flow 
Model Assistant (SFMA) 
This assistant is used to define the dialogue flow at 
an abstract level, i.e. specifying only the high-level 
states of the dialogue, the slots to be asked to the 
user, and the transitions between states, but not the 
specific details of each state. The flow is specified 
using a state transition network representation that 
is common in this kind of platforms and dialogue 
modelling. The GUI allows the definition of new 
states using wizard-driven steps and a drag-and-
drop interface. An important acceleration strategy 
from the previous version is the possibility of 
specifying the slots through attributes offered 
automatically from the data model. The new accel-
erations are the automatic proposal of the slots to 
be requested using system or mixed initiative dia-
logues (section 6.1) and the automatic generation 
of proposals of states for defining the dialogue 
flow (section 6.2). 
6.1 Automatic unification of slots for mixed 
initiative 
The idea of this acceleration is to allow the system 
to propose automatically when two or more slots 
must be requested one by one (using directed 
forms) or at the same time (using mixed initiative 
forms) according to the VoiceXML standard.  
This functionality is only available when the 
slots to be analyzed have been defined from a table 
and field in the backend database. In this case, the 
assistant uses the heuristics obtained for the given 
fields and applies a set of customizable rules used 
to decide which slots can be unified and which 
ones cannot. Some examples of the rules applied to 
not propose the unification are: a) one of the slots 
is defined as a string with an average length greater 
than 20 characters, an average number of words 
per record greater than 3, and the other slot is an 
integer/float number greater than 5 characters. In 
this case, the rule avoids the recognition of long 
strings, e.g. an address or name, plus the recogni-
tion of long numeric quantities, e.g. phone or ac-
count numbers, b) when two slots are defined as 
strings and the sum of the average length of both is 
greater than 20 characters; in this case, the system 
tries to avoid the recognition of very long sen-
tences. c) there are two numeric slots with a pro-
portion of different values close to one, and the 
total number of records of both fields is high (con-
figurable value), then the system determines that 
these slots have a large vocabulary and a high 
probability of misrecognition. So, in all these 
cases, the system decides that it is better to ask one 
slot at a time (system initiative). In case there are 
more than two slots, the system checks different 
combinations of the slots in order to find those that 
can be requested at the same time and leaving the 
other one to be requested alone. 
6.2 Automatic states 
In this strategy, the assistant creates automatically 
dialogue states that include the slots to be re-
quested to the user. Using the information of the 
database structure and the database access func-
tions, the wizard allows the designer to access to 
the following state proposals: 
Empty states and already created states: The 
first one allows the creation of a new empty state, 
with no defined slots inside, that the designer can 
define completely afterwards. This way, we allow 
a top-down design. The second one allows the de-
signer to re-use already defined states. 
From attributes with database dependency: 
This kind of states is created from any attribute 
defined in the database model (DMA) that refers to 
a database field only if the attribute has been used 
as an input argument of any database access func-
tion. The proposed states contain only one slot and 
its name corresponds to the name of the attribute in 
the data model. However, the designer can select 
several states to create states with multiple slots. 
From the database access functions: In this 
case, the system analyzes all the defined database 
functions containing input arguments defined as 
atomic types. Then, the system uses the name of 
the function as proposal for the name of the state, 
165
and the input arguments as slots for that state. The 
assistant allows the designer to select several of 
these proposals in order to create more complex 
states. For instance, in case there is a database ac-
cess function called convertCurrencies, which re-
ceives three input arguments (i.e. fromCurrency, 
toCurrency, and Amount), the system automati-
cally creates a new state proposal called convert-
Currencies that includes these three slots. Apply-
ing similar rules to the ones described in section 
6.1 the system would propose to request the first 
two at the same time (mixed-initiative) and the 
Amount separately (directed forms). 
From classes defined in the data model struc-
ture: In this case, the assistant creates a template 
that the designer can drag and drop into the work-
space (see Figure 6 in Appendix A). Then, a pop-
up window allows the designer to select the attrib-
utes to be used as slots. The assistant expands 
complex attributes (with inheritance and objects) 
allowing only the selection of atomic attributes.  
7 Evaluation 
With the objective of evaluating the performance 
of each of the acceleration strategies and assistants 
described above, we carried out a subjective and 
objective evaluation with 9 developers with differ-
ent experience levels and profiles (4 novices, 3 
intermediates, and 2 experts) on designing dia-
logue services. They were requested to fulfil dif-
ferent typical tasks covering each of the proposed 
accelerations and assistants to evaluate. Further 
details can be obtained in (D?Haro, 2009). 
For the subjective evaluation, the participants 
were asked to answer a questionnaire that consists 
of four questions per assistant and seventeen for 
the overall platform, with a range between 1 and 
10. This subjective evaluation confirms the de-
signer-friendliness of the assistants, as well as their 
usability, since all the assistants obtained a global 
score higher than 8.0, which is a nice result. In de-
tail, the DMA and DCMA obtained an 8.3, the 
SFMA a 9.0, the RMA an 8.6, and Diagen a 4.5. 
Regarding the acceleration strategies, see Figure 
3a, the evaluators scored the automatic states with 
9.3, the SQL generation and the unification of slots 
for mixed initiative with 9.0, and the class propos-
als with 8.9. Regarding the RMA and the accelera-
tions related with the information extracted from 
the database (see section 3), the passing of argu-
ments between actions and the proposal of dia-
logue actions obtained a 9.8 and 8.6 respectively. 
For the objective evaluation, we collected the 
metrics proposed in (Jung et al 2008): elapsed 
time, number of clicks, number of keystrokes, and 
number of corrections using the keyboard (key-
stroke errors). We compared our assistants with a 
built-in editor called Diagen, created during the 
GEMINI project and improved later on by 
(Hamerich, 2008), which features fewer accelera-
tions but generates the same information specified 
by our assistants. As accelerations, Diagen only 
provides default templates that the designer has to 
complete and a guided procedure using different 
pop-up windows to fulfil the templates. The results 
confirm that the design time can be reduced, in 
average for all the assistants and tasks, in more 
than 45%, the number of keystrokes in 81%, and 
the number of clicks in 40%. Especially relevant is 
the high reduction (85%) obtained in the RMA 
considering that it is the main task in the design. 
8 Conclusions and Future Work 
In this paper, we have described the main accelera-
tions incorporated into a complete platform for 
designing multimodal and multilingual dialogue 
applications. The proposed accelerations strategies 
are based on using information extracted from the 
contents of the backend database. The proposed 
accelerations include the creation of automatic 
state proposals, the unification of slots to be re-
quested using mixed-initiative dialogues, and the 
semi-automatic creation and debugging of SQL 
statements for accessing the database, among oth-
ers. Subjective and objective evaluations confirm 
that the proposed strategies are useful and contrib-
ute to simplify and accelerate the design. 
As future work, we propose the extraction of 
new heuristic information, the creation of new 
rules for unifying slots for mixed-initiative dia-
logues. Considering the negative values in Figure 
3b, we propose to improve the GUI for defining 
the connections among states in the SFMA, and to 
improve the DCMA by offering new automated 
methods for creating the prototypes. 
9 Acknowledgements 
This work has been supported by ROBONAUTA 
(DPI2007-66846-c02-02) and SD-TEAM 
(TIN2008-06856-C05-03).  
166
  
Figure 3. Average result for the: a) subjective evaluation for the accelerations, b) objective results 
References 
Chung, G. 2004. Developing a Flexible Spoken Dialog 
System Using Simulation. ACL 2004. 
D?Haro. L. F. 2009. Speed Up Strategies for the Crea-
tion of Multimodal and Multilingual Dialogue Sys-
tems. PhD Thesis. Univ. Polit?cnica de Madrid. 
D?Haro, L. F., Cordoba, R., et al 2008. Language 
Model Adaptation for a Speech to Sign Language 
Translation System Using Web Frequencies and a 
MAP framework. Interspeech 2008, pp. 2119-2202. 
D?Haro, L. F., Cordoba, R., et al 2006. An advanced 
platform to speed up the design of multilingual dia-
logue applications for multiple modalities Speech 
Communication Vol. 48, Issue 8, pp. 863-887. 
D?Haro, L. F., Cordoba, R., et al 2004. Strategies to 
reduce design time in multimodal/multilingual dialog 
applications. ICSLP 2004, pp IV-3057?3060. 
Feng, J., Bangalore, S., Rahim, M. 2003. WEBTALK: 
Mining Websites for Automatically Building Dialog 
Systems. ASRU 2003, pp. 168-173. 
Hamerich, S. 2008. From GEMINI to DiaGen: Improv-
ing Development of Speech Dialogues for Embedded 
Systems. 9th SIGDIAL, pp. 92-95. 
Jung, S., Lee, C., et. al. 2008. DialogStudio : A Work-
bench for Data-driven Spoken Dialogue System De-
velopment and Management. Speech Communica-
tions, 50 (8-9), pp. 683-697. 
Pargellis, A. N., Kuo, H. J., Lee, C. 2004. An automatic 
dialogue generation platform for personalized dia-
logue applications. Speech Communication Vol. 42, 
pp. 329-351. 
Polifroni, J. and Walker, M. 2006. Learning Database 
Content for Spoken Dialogue System Design. LREC 
2006, pp. 143-148. 
San-Segundo et al 2008. Speech to sign language trans-
lation system for Spanish. Speech Communication 
Vol. 50, pp.1009?1020. 
167
Appendix A. Additional Figures 
 
Figure 4. Platform architecture. In yellow colour the assistants with the new accelerations described in 
this paper. In pink colour assistants with previous accelerations (section 3) 
 
 
Figure 5. Wizard for creating and debugging the SQL statements for accessing the backend database. 
In the example, the proposed query allows the selection of all account numbers for a given customer 
(using his/her authentication code) and type of account (i.e. passbook saving accounts) 
168
 Figure 6. Workspace for creating the state transition network and pop up window with state proposals. 
In the example, the designer creates the state Transaction from the Class_Transaction template (cre-
ated in the DMA, see Figure 2) and selects as slots the TransactionAmount, CreditAccountNumber and 
DebitAccountNumber (not shown) 
169
Proceedings of the 2nd Workshop on Speech and Language Processing for Assistive Technologies, pages 84?93,
Edinburgh, Scotland, UK, July 30, 2011. c?2011 Association for Computational Linguistics
Source Language Categorization for improving a Speech into Sign Lan-
guage Translation System 
V. L?pez-Lude?a, R. San-Segundo, S. Lutfi, J.M. Lucas-Cuesta, J.D. Echevarry, B. 
Mart?nez-Gonz?lez 
Grupo de Tecnolog?a del Habla 
Universidad Polit?cnica de Madrid 
{veronicalopez|lapiz|syaheerah|juanmak|jdec|beatrizmartinez}@die.upm.es 
 
Abstract 
This paper describes a categorization mod-
ule for improving the performance of a 
Spanish into Spanish Sign Language (LSE) 
translation system. This categorization 
module replaces Spanish words with asso-
ciated tags. When implementing this mod-
ule, several alternatives for dealing with 
non-relevant words have been studied. Non-
relevant words are Spanish words not rele-
vant in the translation process. The catego-
rization module has been incorporated into 
a phrase-based system and a Statistical Fi-
nite State Transducer (SFST). The evalua-
tion results reveal that the BLEU has 
increased from 69.11% to 78.79% for the 
phrase-based system and from 69.84% to 
75.59% for the SFST. 
Keywords: Source language categoriza-
tion, Speech into Sign Language transla-
tion. Lengua de Signos Espa?ola (LSE). 
1 Introduction 
In the world, there are around 70 million people 
with hearing deficiencies (information from World 
Federation of the Deaf http://www.wfdeaf.org/). 
Deafness brings about significant communication 
problems: most deaf people are unable to use writ-
ten languages, having serious problems when ex-
pressing themselves in these languages or 
understanding written texts. They have problems 
with verb tenses, concordances of gender and 
number, etc., and they have difficulties when creat-
ing a mental image of abstract concepts. This fact 
can cause deaf people to have problems when ac-
cessing information, education, job, social relation-
ship, culture, etc. According to information from 
INE (Statistic Spanish Institute), in Spain, there are 
1,064,000 deaf people. 47% of deaf population do 
not have basic studies or are illiterate, and only 
between 1% and 3% have finished their studies (as 
opposed to 21% of Spanish hearing people). An-
other example are the figures from the National 
Deaf Children?s Society (NDCS), Cymru, reveal-
ing for the first time a shocking attainment gap 
between deaf and hearing pupils in Wales. In 2008, 
deaf pupils were 30% less likely than hearing pu-
pils to gain five A*-C grades at General Certificate 
of Secondary Education (GCSE) level, while at 
key stage 3 only 42% of deaf pupils achieved the 
core subject indicators, compared to 71% of their 
hearing counterparts. Another example is a study 
carried out in Ireland in 2006; of 330 respondents 
?38% said they did not feel confident to read a 
newspaper and more than half were not fully con-
fident in writing a letter or filling out a  form? 
(Conroy, 2006). 
Deaf people use a sign language (their mother 
tongue) for communicating and there are not 
enough sign-language interpreters and communica-
tion systems. In Spain, there is the Spanish Sign 
Language (Lengua de Signos Espa?ola LSE) that 
is the official sign language. In the USA, there are 
650,000 Deaf people (who use a sign language). 
Although there are more people with hearing defi-
ciencies, there are only 7,000 sign-language inter-
preters, i.e. a ratio of 93 deaf people to 1 
interpreter. In Finland we find the best ratio, 6 to 1, 
and in Slovakia the worst with 3,000 users to 1 
interpreter (Wheatley and Pabsch, 2010). In Spain 
this ratio is 221 to 1. This information shows the 
need to develop automatic translation systems with 
new technologies for helping hearing and Deaf 
people to communicate between themselves. 
84
Speech 
Recognition
Language 
Translation
Word 
Sequence
Translation 
Information
Acoustic 
Models
Language 
Model
Sign Animation
Sign 
Descriptions
Sign 
Sequence
Natural 
Speech
Categorization
Tagged 
Sequence
Tags
 
 
Figure 1. Spanish into LSE translation system. 
   
It is necessary to make a difference between 
?deaf? and ?Deaf?: the first one refers to non-
hearing people, and the second one refers to hear-
ing and non-hearing people who use a sign lan-
guage to communicate between them, being part of 
the ?Deaf community?. Each country has a differ-
ent sign language, but there may even be different 
sign languages in different regions. 
This paper describes a categorization module 
for improving the performance of a Speech into 
Sign Language Translation System. This system 
helps Deaf people to communicate with govern-
ment employees in a restricted domain: the re-
newal of Identity Documents and Driver?s License 
(San-Segundo et al, 2008). This system has been 
designed to translate the government employee?s 
explanations into LSE when government employ-
ees provide these face-to-face services. The system 
is made up of a speech recognizer (for decoding 
the spoken utterance into a word sequence), a natu-
ral language translator (a phrase-based system for 
converting a word sequence into a sequence of 
signs belonging to the sign language), and a 3D 
avatar animation module (for playing back the 
signs) (Figure 1). This paper proposes to include a 
fourth module named ?categorization? between the 
speech recognition and language translation mod-
ules (Figure 1). This categorization module re-
places Spanish words with associated tags as will 
be shown further. 
For the natural language translation module, 
two different statistical strategies have been ana-
lyzed: a phrase-based system (Moses) and a Statis-
tical Finite State Transducer (SFST). The proposed 
categorization module has been incorporated into 
and evaluated with both translation strategies. 
This paper is organized as follows: section 2 
describes the state of the art. Section 3 describes 
the parallel corpus used in these experiments. The 
main characteristics of the LSE are presented in 
section 4. Section 5 details the two main transla-
tion strategies considered. The categorization 
module is described in section 6. Section 7 in-
cludes the main experiments and the obtained re-
sults, and finally, sections 8 and 9 include the main 
conclusions and the future work. 
2 State of the art 
In recent years, several groups have developed 
prototypes for translating Spoken language into 
Sign Language: example-based (Morrissey, 2008), 
rule-based (Marshall and S?f?r, 2005; San-
Segundo et al 2008), full sentence (Cox et al, 
2002) or statistical approaches (Stein et al, 2006; 
Morrissey et al, 2007; Vendrame et al, 2010) 
approaches.  
Given the sparseness of data for researching 
in Sign Languages, in the last five years, several 
projects have started to generate more resources: in 
American Sign Language (Dreuw et al., 2008), 
British Sign Language (Schembri, 2008), Greek 
Sign Language (Efthimiou and Fotinea, 2008), in 
Irish Sign Language (Morrissey et al, 2010), NGS 
(German Sign Language) (Hanke et al, 2010), and 
Italian Sign Language (Geraci et al, 2010). For 
LSE, the biggest database was generated two years 
ago in a Plan Avanza project 
(www.traduccionvozlse.es) (San-Segundo et al, 
2010) and it is has been used in this work. Not only 
the data but also new practice (Forster et al, 2010) 
and new uses of traditional annotation tools (Cras-
born et al, 2010) have been developed. 
The work presented in this paper describes 
experiments with a relevant database Despite the 
small amount of data available for research into 
85
sign languages, the system presented in this paper 
demonstrates a very good performance compared 
to similar systems previously developed. The pre-
sented results are also the best results for translat-
ing Spanish into LSE using the biggest database 
that includes these languages. 
In Europe, the two main research projects in-
volving sign languages are DICTA-SIGN (Hanke 
et al, 2010; Efthimiou et al, 2010) and SIGN-
SPEAK (Dreuw et al, 2010a and 2010b), both 
financed by The European Commission within the 
Seventh Frame Program. DICTA-SIGN 
(http://www.dictasign.eu/) aims to develop the 
technologies necessary to make Web 2.0 interac-
tions in sign language possible: users sign to a 
webcam using a dictation style. The computer rec-
ognizes the signed phrases, converts them into an 
internal representation of sign language, and then it 
has an animated avatar that signs them back to the 
users. In SIGN-SPEAK 
(http://www.signspeak.eu/), the overall goal is to 
develop a new vision-based technology for recog-
nizing and translating continuous sign language 
into text. 
3 Parallel corpus 
This section describes the first Spanish-LSE paral-
lel corpus developed for language processing in 
two specific domains: the renewal of the Identity 
Document (ID) and Driver?s License (DL). This 
corpus has been obtained with the collaboration of 
Local Government Offices where these services 
are provided. Over several weeks, the most fre-
quent explanations (from the government employ-
ees) and the most frequent questions (from the 
user) were taken down. In this period, more than 
5,000 sentences were noted and analyzed. 
Not all the sentences refer to ID or DL re-
newal (Government Offices provide more ser-
vices), so sentences had to be selected manually. 
This was possible because every sentence was 
tagged with the information about the service be-
ing provided when it was collected. Finally, 1360 
sentences were collected: 1,023 pronounced by 
government employees and 337 by users. These 
sentences have been translated into LSE, both in 
text (sequence of glosses) and in video (containing 
replayed sentences by native LSE signers), and 
compiled in an excel file. Videos are not used in 
this study but they were collected for generating a 
complete parallel corpus. 
This corpus was increased to 4,080 by incor-
porating different variants for Spanish sentences 
(maintaining the LSE translation) (San-Segundo et 
al. 2010). Table 1 summarizes the main features of 
this database. 
 Spanish LSE 
Sentence pairs 4,080 
Different sentences 3,342 1,289 
Words/signs per sentence 7.7 5.7 
Running words 31,501 23,256 
Vocabulary 1,232 636 
Table 1. Main statistics of the corpus 
 
For the experiments presented in this paper, 
this database has been divided randomly into three 
sets: training (75%), development (12.5%) and test 
(12.5%). The training set was used for tuning the 
speech recognizer (vocabulary and language mod-
el) and training the translation models. The devel-
opment set was used for tuning the translation 
systems and finally, the test set was used for evalu-
ating the categorization module. 
4 Spanish Sign Language (LSE) 
Spanish Sign Language (LSE), just like other sign 
languages, has a visual-gestural channel, but it also 
has grammatical characteristics similar to spoken 
languages. Sign languages have complex gram-
mars and professional linguists have found all of 
the necessarily linguistic characteristics for classi-
fying sign languages as ?true languages?. In lin-
guistic terms, sign languages are as complex as 
spoken languages, despite the common misconcep-
tion that they are a ?simplification? of spoken lan-
guages. For example, The United Kingdom and 
USA share the same language. However, British 
Sign Language is completely different from Amer-
ican Sign Language. W. Stokoe (Stokoe, 1960) 
supports the idea that sign languages have four 
dimensions (three space dimensions plus time), 
and spoken languages have only one dimension, 
time, so it cannot say that sign languages are a 
simplification of any other language. 
One important difference between spoken 
languages and sign languages is sequentially. Pho-
nemes in spoken languages are produced in a se-
quence. On the other hand, sign languages have a 
86
large non-sequential component, because fingers, 
hands and face movements can be involved in a 
sign simultaneously, even two hands moving in 
different directions. These features give a com-
plexity to sign languages that traditional spoken 
languages do not have. This fact makes it very 
difficult to write sign languages. Traditionally, 
signs have been written using words (in capital 
letters) in Spanish (or English in the case of BSL, 
British Sign Language) with a similar meaning to 
the sign meaning. They are called glosses (i.e. 
?CAR? for the sign ?car?).  
In the last 20 years, several alternatives, based 
on specific characteristics of the signs, have ap-
peared in the international community: HamNoSys 
(Prillwitz et al 1989), SEA (Sistema de Escritura 
Alfab?tica) (Herrero, A., 2004) and SignWriting 
(http://www.signwriting.org/). HamNoSys and 
SignWriting require defining a specific picture font 
to be used by computers. SignWriting includes 
face features in the notation system but HamNoSys 
and SEA do not include them. All of these alterna-
tives are flexible enough for dealing with different 
sign languages including LSE. However, in this 
work, glosses have been considered for writing 
signs because it is the most familiar and extended 
alternative according to the Spanish Deaf Associa-
tion. These glosses include non-speech indicators 
(i.e. PAY or PAY? if the sign is localized at the 
end of an interrogative sentence) and finger spell-
ing indicators (i.e. DL-PETER that must be repre-
sented letter by letter P-E-T-E-R). 
LSE has some characteristics that differ from 
Spanish. One important difference is the order of 
arguments in sentences: LSE has a SOV (subject-
object-verb) order in contrast to SVO (subject-
verb-object) Spanish order. An example that illus-
trates this behaviour is shown below: 
Spanish: Juan ha comprado las entradas (Juan has 
bought the tickets) 
LSE: JUAN ENTRADAS COMPRAR (JUAN 
TICKETS TO-BUY) 
There are other typological differences that are not 
related to predication order: 
? Gender is not usually specified in LSE, in con-
trast to Spanish. 
? In LSE, there can be concordances between 
verbs and subject, receiver or object and even 
subject and receiver, but in Spanish there can be 
only concordance between verb and subject: 
? Spanish: Te explica (he explains to you) 
? LSE: EXPLICAR-?l-a-ti (EXPLAIN-HIM-
TO-YOU) 
? The use of classifiers is common in LSE, but 
they are not in Spanish. 
? Spanish: debe acercarse a la c?mara (you 
must approach the camera) 
? LSE: FOTO CLD_GRANDE_NO 
CLL_ACERCARSE DEBER (PHOTO 
CLD_BIG_NO CLL_APPROACH MUST) 
? Articles are used in Spanish, but not in LSE. 
? Plural can be descriptive in LSE, but not in 
Spanish. 
? In Spanish, there is a copula in non-verbal 
predications (the verb ?to be?, ser and estar in 
Spanish), but there is not in LSE. 
? There are Spanish impersonal sentences, but not 
in LSE. 
? LSE is more lexically flexible than Spanish, 
and it is perfect for generating periphrasis 
through its descriptive nature and because of 
this, LSE has fewer nouns than Spanish. (i.e. 
mud is translated into SAND+WATER) 
? To finish, LSE has less glosses per sentence 
(5.7 in our database) than Spanish (7.7 in our 
database). 
? LSE has smaller vocabulary variability. LSE 
has a vocabulary of around 10,000 signs while 
Spanish has several millions of different words. 
Good examples are the different verb conjuga-
tions. 
5 Statistical translation strategies 
In this paper, two different statistical strategies 
have been considered: a phrase-based system and a 
Statistical Finite State Transducer. The proposed 
automatic categorization has been evaluated with 
both translation strategies. This section describes 
the architectures used for the experiments. 
5.1 Phrase-based translation system 
The Phrase-based translation system is based on 
the software released at the 2009 NAACL Work-
shop on Statistical Machine Translation 
(http://www.statmt.org/wmt09/) (Figure 2). 
 
87
Word 
Alignment
GIZA++
Phrase extraction 
and scoring
Phrase-model
Parallel 
corpora
N-gram train
SRI-LM
Target lang. 
corpora
Translation
MOSES
Source lang. 
sentence
Translation 
output
Translation
Model
Target lang. 
Model
Target Language: Sign Language
Source Language: Spanish  
 
Figure 2. Phrase-based translation architecture. 
 
In this study, a phrase consists of a subse-
quence of words (in a sentence) that intends to 
have a meaning. Every sentence is split in several 
phrases automatically so this segmentation can 
have errors. But, the main target, when training a 
phrase-based model, is to split the sentence in sev-
eral phrases and to find their corresponding trans-
lations in the target language. 
The phrase model has been trained starting 
from a word alignment computed using GIZA++ 
(Och and Ney, 2003). GIZA++ is a statistical ma-
chine translation toolkit that is used to train IBM 
Models 1-5 and an HMM word alignment model. 
In this step, the alignments between words and 
signs in both directions (Spanish-LSE and LSE-
Spanish) are calculated. The ?alignment? parame-
ter has been fixed to ?target-source? as the best 
option (based on experiments over the develop-
ment set): only this target-source alignment was 
considered (LSE-Spanish). In this configuration, 
alignment is guided by signs: this means that in 
every sentence pair alignment, each word can be 
aligned to one or several signs (but not the oppo-
site), and also, it is possible that some words were 
not aligned to any sign. When combining the 
alignment points from all sentences pairs in the 
training set, it is possible to have all possible 
alignments: several words aligned to several signs. 
After the word alignment, the system per-
forms a phrase extraction process (Koehn et al 
2003) where all phrase pairs that are consistent 
with the word alignment (target-source alignment 
in our case) are collected. In the phrase extraction, 
the maximum phrase length has been fixed at 7 
consecutive words, based on development experi-
ments over the development set (see previous sec-
tion). 
Finally, the last step is phrase scoring. In this 
step, the translation probabilities are computed for 
all phrase pairs. Both translation probabilities are 
calculated: forward and backward. 
For the translation process, the Moses decoder 
has been used (Koehn, 2010). This program is a 
beam search decoder for phrase-based statistical 
machine translation models. In order to obtain a 3-
gram language model, the SRI language modeling 
toolkit has been used (Stolcke, 2002). 
5.2 Phrase-based translation system 
The translation based on SFST is carried out 
as set out in Figure 3. 
Word 
Alignment
GIZA++
Finite State 
Transducer
GIATI
Parallel 
corpora
Translation
search over the 
FST
Translation
Model
Source lang. 
sentence
Translation 
output  
Figure 3. Diagram of the FST-based translation 
module. 
The translation model consists of an SFST 
made up of aggregations: subsequences of aligned 
source and target words. The SFST is inferred 
from the word alignment (obtained with GIZA++) 
using the GIATI (Grammatical Inference and 
Alignments for Transducer Inference) algorithm 
(Casacuberta and Vidal, 2004). The SFST prob-
abilities are also trained from aligned corpora. The 
software used in this paper has been downloaded 
from 
http://prhlt.iti.es/content.php?page=software.php. 
6 Categorization module 
As it was presented in Figure 1, the categorization 
module proposed in this paper analyzes the source 
language sentence (sentence in Spanish) and re-
places Spanish words with their associated tags. 
This module uses a list of 1014 Spanish words (the 
vocabulary in this restricted domain) and the corre-
sponding tags. For every word, only one syntactic-
semantic tag is associated. In the case of homo-
nyms, the most frequent meaning has been consid-
ered for defining the syntactic-semantic tag. Figure 
4 shows an extract of the word-tag list. This list is 
composed of Spanish words and their correspond-
ing tags, including the English translation in paren-
thesis. 
88
 
Figure 4. Extract of the word-tag list. 
The categorization module executes a simple 
procedure: for all words in a Spanish sentence, the 
categorization module looks for this word in the 
list and replaces it with the associated tag. It is 
important to comment two main aspects. The first 
one is that there is a tag named ?non-relevant? 
associated to those words that are not useful for 
translating the sentence. The second one is that if 
the Spanish word is not in the list (it is an Out Of 
Vocabulary word: OOV), this word is not replaced 
with any tag: this word is kept as it is. 
In order to train the statistical translation 
modules when using the categorization module, it 
is necessary to retrain the translation models con-
sidering the tagged source language, not the origi-
nal word sentences, and using the training set. This 
way, the translation models learn the relationships 
between tags and signs. 
The main issue for implementing the catego-
rization module is to generate the list of the Span-
ish words with the associated tags. In this work, 
the categorization module considers the categories 
used in the rule-based translation system previ-
ously developed for this application domain (San-
Segundo et al, 2008). These categories were gen-
erated manually during one week, approximately. 
In this case, the natural language translation mod-
ule was implemented using a rule-based technique 
considering a bottom-up strategy. The translation 
process is carried out in two steps. In the first one, 
every word is mapped into one syntactic-pragmatic 
tag. After that, the translation module applies dif-
ferent rules that convert the tagged words into 
signs by means of grouping concepts or signs and 
defining new signs. These rules can define short 
and large scope relationships between the concepts 
or signs.  
When implementing the categorization mod-
ule, several strategies for dealing with the ?non-
relevant? words have been proposed: 
? In the first alternative, all the words are replaced 
by their tags with the exception of those words 
that they do not appear in the list (OOV words). 
As, it was commented before, they are kept as 
they are. In the word-tag list, there is a ?non-
relevant? tag mapped to words that are not rele-
vant for the translation process (named ?basura? 
(non-relevant)). This alternative will be referred 
in the experiments like ?Base categorization?. 
For example: 
o Source sentence: debes pagar  las tasas en la 
caja (you must pay the taxes in the cash desk) 
o Categorizated source sentence: DEBER 
PAGAR basura DINERO basura basura 
DINERO-CAJA (MUST PAY non-relevant 
MONEY non-relevant non-relevant CASH-
DESK) 
o Target sentence: VENTANILLA 
ESPEC?FICO CAJA TU PAGAR (WINDOW 
SPECIFIC CASH-DESK YOU PAY) 
? The second proposed alternative was not to tag 
any word in the source language but removing 
non-relevant words from the source lexicon (as-
sociated to the ?non-relevant? tag). This alterna-
tive will be referred in the experiments like 
?Non-relevant word deletion?. For example: 
o Source sentence: debes pagar las tasas en la 
caja (you must pay the taxes in the cash desk) 
o Categorizated source sentence: debes pagar 
tasas caja 
o Target sentence: VENTANILLA 
ESPEC?FICO CAJA TU PAGAR (WINDOW 
SPECIFIC CASH-DESK YOU PAY) 
? Finally, the third alternative proposes to replace 
words with tags (with the exception of OOVs) 
and to remove ?non-relevant? tags. This alterna-
tive will be referred in the experiments like 
?Categorization and non-relevant word dele-
tion?. For example: 
o Source sentence: debes pagar las tasas en la 
caja (you must pay the taxes in the cash desk) 
o Categorizated source sentence: de-
bes|DEBER pagar|PAGAR tasas|DINERO 
caja|DINERO-CAJA 
o Target sentence: VENTANILLA 
ESPEC?FICO CAJA TU PAGAR 
(WINDOW SPECIFIC CASH-DESK YOU 
PAY) 
In the next section, all the alternatives will be 
evaluated and discussed. 
word TAG (word and tag in English) 
? 
cerrado CERRAR-YA (closed CLOSE-ALREADY ) 
cerramos CERRAR (we close CLOSE ) 
cerrar CERRAR (to close CLOSE) 
cobradas COBRAR-YA (charged CHARGE-ALREADY) 
cobro COBRAR (I charge CHARGE) 
coge COGER (you get GET) 
cogido COGER-YA (gotten GET-ALREADY) 
coja COGER (you get GET) 
? 
 
89
7 Experiments and discussion 
For the experiments, the corpus (described in sec-
tion 3) was divided randomly into three sets: train-
ing (75%), development (12.5%) and test (12.5%). 
Results are compared with a baseline. This base-
line consists of training models with original 
source and target corpus without any type of fac-
torization, i.e, sentences contain words and signs 
from the original database. For example: this sen-
tence ?debes pagar las tasas en la caja? (you must 
pay the taxes in the cash desk) is translated into 
?VENTANILLA ESPEC?FICO CAJA TU 
PAGAR? (WINDOW SPECIFIC CASH-DESK 
YOU PAY). 
For evaluating the performance of the transla-
tion systems, the BLEU (BiLingual Evaluation 
Understudy) metric (Papineni et al, 2002) has 
been used. BLEU is one of the most well-known 
metric for evaluating automatic translation systems 
because this metric presents a good correlation 
with human evaluations. This metric has been also 
adopted to evaluate speech into sign language 
translation systems (Stein et al, 2006; Morrissey et 
al., 2007; Vendrame et al, 2010, San-Segundo et 
al. 2008). In order to analyze the significance of 
the differences between several systems, for every 
BLEU result, the confidence interval (at 95%) is 
also presented. This interval is calculated using the 
following formula: 
 
)1()100(96,1
n
BLEUBLEU ?
=??  
 
n is the number of signs used in evaluation, in this 
case n=2,906. An improvement between two sys-
tems is statistically significant when there is no 
overlap between the confidence intervals of both 
systems. 
Related to the speech recognizer, it is impor-
tant to comment that the Word Error Rate (WER) 
obtained in these experiments has been 4.7%. 
Table 2 compares the baseline system and the 
system with the categorization module for translat-
ing the references (Reference) and the speech rec-
ognizer outputs (ASR output) using the phrase-
based translation system. 
 
Phrase-based translation Sys-
tem BLEU ?? 
Reference 73.66 1.60 
Baseline 
ASR output 69.11 1.68 
Reference 81.91 1.40 Base categoriza-
tion ASR output 74.55 1.58 
Reference 80.02 1.45 Non-relevant 
words deletion ASR output 73.89 1.60 
Reference 84.37 1.32 Categorization 
and non-relevant 
word deletion ASR output 78.79 1.49 
Table 2. Evaluation results for the phrase-based 
translation system. 
Table 3 compares the baseline system and the 
system with the categorization module for translat-
ing the references (Reference) and the speech rec-
ognizer outputs (ASR output) using the SFST-
based translation system. 
SFST BLEU ?? 
Reference 71.17 1.65 
Baseline 
ASR output 69.84 1.67 
Reference 71.86 1.63 Base categoriza-
tion ASR output 68.73 1.69 
Reference 76.71 1.54 Non-relevant 
words deletion ASR output 72.77 1.62 
Reference 81.48 1.41 Categorization 
and non-relevant 
word deletion ASR output 75.59 1.56 
Table 3. Evaluation results for the SFST-based 
translation system. 
 
Comparing the three alternatives for dealing 
with the non-relevant words, it is shown that add-
ing tags to the words and removing ?non-relevant? 
words are complementary actions that allow reach-
ing the best results. 
In order to better understand the main causes 
of this improvement, an error analysis has been 
carried out, establishing a relationship between 
these errors and the main differences between 
Spanish and LSE.  
90
The most important type of error (35% of the 
cases) is related to the fact that in Spanish there are 
more words than signs in LSE (7.7 for Spanish and 
5.7 for LSE in this corpus). This circumstance 
provokes different types of errors: generation of 
many phrases in the same output, producing a high 
number of insertions. When dealing with long sen-
tences there is the risk that the translation model 
cannot deal properly with the big distortion. This 
produces important changes in order and some-
times the sentence is truncated producing several 
deletions.  
The second most important source of errors 
(25% of the cases) is related to the fact that when 
translating Spanish into LSE, there is a relevant 
number of words in the testing set that they do not 
appear in the training set due to the higher variabil-
ity presented in Spanish. These words are named 
Out Of Vocabulary words. For example, in Span-
ish there are many verb conjugations that are trans-
lated into the same sign sequence. So, when a new 
conjugation appears in the evaluation set, it is an 
OOV that provokes a translation error. 
Other important source of errors corresponds 
to ordering errors provoked by the different order 
in predication: LSE has a SOV (Subject-Object-
Verb) while Spanish SVO (Subject-Verb-Object). 
In this case, the frequency is close to 20% 
Finally, there are others causes of errors like 
the wrong generation of the different classifiers 
needed in LSE and not presented in Spanish (11%) 
and the existence of some deletions when translat-
ing very specific names, even when they are in the 
training set. Some of these names (i.e. ?mud? is 
translated into SAND + WATER) need some pe-
riphrasis in LSE that not always are properly gen-
erated. 
Based on this error analysis, the main causes 
of the translation errors are related to the different 
variability in the vocabulary for Spanish and LSE 
(much higher in Spanish), the different number for 
words or signs in the sentences (higher in Spanish) 
and the different predication order. 
The categorization module allows reducing 
the variability in the source language (for example, 
several verb conjugations are tagged with the same 
tag) and also the number of tokens composing the 
input sentence (when removing non-relevant 
words). Also, reducing the source language vari-
ability and the number of tokens provoke an im-
portant reduction on the number of source-target 
alignments the system has to train. When having a 
small corpus, as it is the case of many sign lan-
guages, this reduction of alignment points permits 
to obtain better training models with less data, 
improving the results. These aspects allow increas-
ing the system performance. Presumably, if there 
were a very large corpus of Spanish-to-Spanish-
Sign-Language available, the system could learn 
better translation models and the improvement 
reached with this categorization module would be 
lower. 
The evaluation results reveal that the BLEU 
has increased from 69.11% to 78.79% for the 
phrase-based system and from 69.84% to 75.59% 
for the SFST.  
8 Conclusions 
This paper describes a categorization module for 
improving a Spanish into Spanish Sign Language 
Translation System. This module allows incorpo-
rating syntactic-semantic information during the 
translation process reducing the source language 
variability and the number of words composing the 
input sentence. These two aspects reduce the trans-
lation error rate considering two statistical transla-
tion systems: phrase-based and SFST-based 
translation systems. This system is used to translate 
government employee?s explanations into LSE 
when providing a personal service for renewing the 
Identity Document and Driver?s License. 
9 Future work 
The main issue for implementing the categoriza-
tion module is to generate the list of the Spanish 
words with the associated tags. Generating this list 
manually is a subjective, slow and difficult task. 
Because of this, in the near future, authors will 
work on the possibility to define a procedure for 
calculating this list automatically. 
Acknowledgments 
The authors would like to thank the eSIGN consor-
tium for permitting the use of the eSIGN Editor 
and the 3D avatar. The authors would also like to 
thank discussions and suggestions from the col-
leagues at GTH-UPM. This work has been sup-
ported by Plan Avanza Exp N?: TSI-020100-2010-
489), INAPRA (MEC ref: DPI2010-21247-C02-
91
02), and SD-TEAM (MEC ref: TIN2008-06856-
C05-03) projects and FEDER program. 
References  
Casacuberta F., E. Vidal. 2004. ?Machine Translation 
with Inferred Stochastic Finite-State Transducers?. 
Computational Linguistics, Vol. 30, No. 2, pp. 205-
225, 2004. 
Conroy, P. 2006. Signing in and Signing Out: The Edu-
cation and Employment Experiences of Deaf Adults 
in Ireland. Research Report, Irish Deaf Society, Dub-
lin. 2006. 
Cox, S.J., Lincoln M., Tryggvason J., Nakisa M., Wells 
M., Mand Tutt, and Abbott, S., 2002 ?TESSA, a sys-
tem to aid communication with deaf people?. In 
ASSETS 2002, Edinburgh, Scotland, pp 205-212, 
2002. 
Crasborn O., Sloetjes H. 2010. ?Using ELAN for anno-
tating sign language corpora in a team setting?. In 4th 
Workshop on the Representation and Processing of 
Sign Languages: Corpora and Sign Language Tech-
nologies (CSLT 2010), Valletta, Malta, 2010. pp 61-
65 
Dreuw P., Neidle C., Athitsos V., Sclaroff S., and Ney 
H. 2008. ?Benchmark Databases for Video-Based 
Automatic Sign Language Recognition?. In Interna-
tional Conference on Language Resources and Eval-
uation (LREC), Marrakech, Morocco, May 2008. pp 
1115-1121. 
Dreuw P., Ney H., Martinez G., Crasborn O., Piater J., 
Miguel Moya J., and Wheatley M., 2010 ?The Sign-
Speak Project - Bridging the Gap Between Signers 
and Speakers?. In 4th Workshop on the Representa-
tion and Processing of Sign Languages: Corpora and 
Sign Language Technologies (CSLT 2010), Valletta, 
Malta, 2010a. pp 73-80. 
Dreuw P., Forster J., Gweth Y., Stein D., Ney H., Mar-
tinez G., Verges Llahi J., Crasborn O., Ormel E., Du 
W., Hoyoux T., Piater J., Moya Lazaro JM, and 
Wheatley M. 2010 ?SignSpeak - Understanding, 
Recognition, and Translation of Sign Languages?. In 
4th Workshop on the Representation and Processing 
of Sign Languages: Corpora and Sign Language 
Technologies (CSLT 2010), Valletta, Malta, May 
2010b. pp 65-73 
Efthimiou E., and Fotinea, E., 2008 ?GSLC: Creation 
and ?nnotation of a Greek Sign Language Corpus for 
HCI? LREC 2008. pp 1-10. 
Efthimiou E., Fotinea S., Hanke T., Glauert J., Bowden 
R., Braffort A., Collet C., Maragos P., Goudenove F. 
2010. ?DICTA-SIGN: Sign Language Recognition, 
Generation and Modelling with application in Deaf 
Communication?. In 4th Workshop on the Represen-
tation and Processing of Sign Languages: Corpora 
and Sign Language Technologies (CSLT 2010), Val-
letta, Malta, May 2010. pp 80-84. 
Forster J., Stein D., Ormel E., Crasborn O., Ney H. 
2010. ?Best Practice for Sign Language Data Collec-
tions Regarding the Needs of Data-Driven Recogni-
tion and Translation?. In 4th Workshop on the 
Representation and Processing of Sign Languages: 
Corpora and Sign Language Technologies (CSLT 
2010), Valletta, Malta, May 2010. pp 92-98. 
Geraci C., Bayley R., Branchini C., Cardinaletti A., 
Cecchetto C., Donati C., Giudice S., Mereghetti E., 
Poletti F., Santoro M., Zucchi S. 2010. ?Building a 
corpus for Italian Sign Language. Methodological is-
sues and some preliminary results?. In 4th Workshop 
on the Representation and Processing of Sign Lan-
guages: Corpora and Sign Language Technologies 
(CSLT 2010), Valletta, Malta, May 2010. pp 98-102. 
Hanke T., K?nig L., Wagner S., Matthes S. 2010. ?DGS 
Corpus & Dicta-Sign: The Hamburg Studio Setup?. 
In 4th Workshop on the Representation and Process-
ing of Sign Languages: Corpora and Sign Language 
Technologies (CSLT 2010), Valletta, Malta, May 
2010. pp 106-110. 
Herrero, A., 2004 ?Escritura alfab?tica de la Lengua de 
Signos Espa?ola? Universidad de Alicante. Servicio 
de Publicaciones. 
Koehn P., F.J. Och D. Marcu. 2003. ?Statistical Phrase-
based translation?. Human Language Technology 
Conference 2003 (HLT-NAACL 2003), Edmonton, 
Canada, 2003. pp. 127-133. 
Koehn, Philipp. 2010. ?Statistical Machine Transla-
tion?. phD. Cambridge University Press. 
Marshall, I., S?f?r, E. (2005) ?Grammar Development 
for Sign Language Avatar-Based Synthesis?, In Pro-
ceedings HCII 2005, 11th International Conference 
on Human Computer Interaction (CD-ROM), Las 
Vegas, USA, July 2005. pp 1-10. 
Morrissey S., Way A., Stein D., Bungeroth J., and Ney 
H., 2007 ?Towards a Hybrid Data-Driven MT Sys-
tem for Sign Languages. Machine Translation Sum-
mit (MT Summit)?, Copenhagen, Denmark, 2007. pp 
329-335. 
Morrissey, S. 2008. ?Data-Driven Machine Translation 
for Sign Languages?. Thesis. Dublin City University, 
Dublin, Ireland. 
Morrissey S., Somers H., Smith R., Gilchrist S., Danda-
pat S., 2010 ?Building Sign Language Corpora for 
Use in Machine Translation?. In 4th Workshop on 
92
the Representation and Processing of Sign Lan-
guages: Corpora and Sign Language Technologies 
(CSLT 2010), Valletta, Malta, May 2010. pp 172-
178. 
Och J., Ney. H., 2003. ?A systematic comparison of 
various alignment models?. Computational Linguis-
tics, Vol. 29, No. 1, 2003. pp. 19-51. 
Papineni K., S. Roukos, T. Ward, W.J. Zhu. 2002 
?BLEU: a method for automatic evaluation of ma-
chine translation?. 40th Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL), 
Philadelphia, PA. 2002. pp. 311-318. 
Prillwitz, S., R. Leven, H. Zienert, T. Hanke, J. 
Henning, et-al. 1989. ?Hamburg Notation System for 
Sign Languages ? An introductory Guide?. Interna-
tional Studies on Sign Language and the Communi-
cation of the Deaf, Volume 5. Institute of German 
Sign Language and Communication of the Deaf, 
University of Hamburg, 1989. 
San-Segundo R., Barra R., C?rdoba R., D?Haro L.F., 
Fern?ndez F., Ferreiros J., Lucas J.M., Mac?as-
Guarasa J., Montero J.M., Pardo J.M, 2008. ?Speech 
to Sign Language translation system for Spanish?. 
Speech Communication, Vol 50. 2008. pp. 1009-
1020. 
San-Segundo, R., Pardo, J.M., Ferreiros, F., Sama, V., 
Barra-Chicote, R., Lucas, JM., S?nchez, D., Garc?a. 
A., ?Spoken Spanish Generation from Sign Lan-
guage? Interacting with Computers, Vol. 22, No 2, 
2010. pp. 123-139. 
Schembri. A., 2008 ?British Sign Language Corpus 
Project: Open Access Archives and the Observer?s 
Paradox?. Deafness Cognition and Language Re-
search Centre, University College London. LREC 
2008. pp 1-5. 
Stein, D., Bungeroth, J. and Ney, H.: 2006 ?Morpho-
Syntax Based Statistical Methods for Sign Language 
Translation?. 11th Annual conference of the Euro-
pean Association for Machine Translation, Oslo, 
Norway, June 2006. pp 223-231. 
Stolcke A., 2002. ?SRILM ? An Extensible Language 
Modelling Toolkit?. Proc. Intl. Conf. on Spoken 
Language Processing, vol. 2, Denver USA, 2002. pp. 
901-904, 
Stokoe W., Sign Language structure: an outline of the 
visual communication systems of the American deaf, 
Studies in Linguistics, Buffalo University Paper 8, 
1960.  
Vendrame M., Tiotto G., 2010. ATLAS Project: Fore-
cast in Italian Sign Language and Annotation of Cor-
pora. In 4th Workshop on the Representation and 
Processing of Sign Languages: Corpora and Sign 
Language Technologies (CSLT 2010), Valletta, Mal-
ta, May 2010. pp 239-243. 
Wheatley, M., Annika Pabsch, 2010. ?Sign Language in 
Europe?. In 4th Workshop on the Representation and 
Processing of Sign Languages: Corpora and Sign 
Language Technologies. LREC. Malta 2010. pp 251-
255. 
 
93
